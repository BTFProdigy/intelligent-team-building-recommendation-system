Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?48,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning to recognize features of valid textual entailments
Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe,
Daniel Cer, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{wcmac, grenager, mcdm, cerd, manning}@cs.stanford.edu
Abstract
This paper advocates a new architecture for tex-
tual inference in which finding a good alignment is
separated from evaluating entailment. Current ap-
proaches to semantic inference in question answer-
ing and textual entailment have approximated the
entailment problem as that of computing the best
alignment of the hypothesis to the text, using a lo-
cally decomposable matching score. We argue that
there are significant weaknesses in this approach,
including flawed assumptions of monotonicity and
locality. Instead we propose a pipelined approach
where alignment is followed by a classification
step, in which we extract features representing
high-level characteristics of the entailment prob-
lem, and pass the resulting feature vector to a statis-
tical classifier trained on development data. We re-
port results on data from the 2005 Pascal RTE Chal-
lenge which surpass previously reported results for
alignment-based systems.
1 Introduction
During the last five years there has been a surge in
work which aims to provide robust textual inference
in arbitrary domains about which the system has no
expertise. The best-known such work has occurred
within the field of question answering (Pasca and
Harabagiu, 2001; Moldovan et al, 2003); more re-
cently, such work has continued with greater focus
in addressing the PASCAL Recognizing Textual En-
tailment (RTE) Challenge (Dagan et al, 2005) and
within the U.S. Government AQUAINT program.
Substantive progress on this task is key to many
text and natural language applications. If one could
tell that Protestors chanted slogans opposing a free
trade agreement was a match for people demonstrat-
ing against free trade, then one could offer a form of
semantic search not available with current keyword-
based search. Even greater benefits would flow to
richer and more semantically complex NLP tasks.
Because full, accurate, open-domain natural lan-
guage understanding lies far beyond current capa-
bilities, nearly all efforts in this area have sought
to extract the maximum mileage from quite lim-
ited semantic representations. Some have used sim-
ple measures of semantic overlap, but the more in-
teresting work has largely converged on a graph-
alignment approach, operating on semantic graphs
derived from syntactic dependency parses, and using
a locally-decomposable alignment score as a proxy
for strength of entailment. (Below, we argue that
even approaches relying on weighted abduction may
be seen in this light.) In this paper, we highlight the
fundamental semantic limitations of this type of ap-
proach, and advocate a multi-stage architecture that
addresses these limitations. The three key limita-
tions are an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
evaluation of entailment.
We focus on the PASCAL RTE data, examples
from which are shown in table 1. This data set con-
tains pairs consisting of a short text followed by a
one-sentence hypothesis. The goal is to say whether
the hypothesis follows from the text and general
background knowledge, according to the intuitions
of an intelligent human reader. That is, the standard
is not whether the hypothesis is logically entailed,
but whether it can reasonably be inferred.
2 Approaching a robust semantics
In this section we try to give a unifying overview
to current work on robust textual inference, to
present fundamental limitations of current meth-
ods, and then to outline our approach to resolving
them. Nearly all current textual inference systems
use a single-stage matching/proof process, and differ
41
ID Text Hypothesis Entailed
59 Two Turkish engineers and an Afghan translator kidnapped
in December were freed Friday.
translator kidnapped in Iraq no
98 Sharon warns Arafat could be targeted for assassination. prime minister targeted for assassination no
152 Twenty-five of the dead were members of the law enforce-
ment agencies and the rest of the 67 were civilians.
25 of the dead were civilians. no
231 The memorandum noted the United Nations estimated that
2.5 million to 3.5 million people died of AIDS last year.
Over 2 million people died of AIDS last
year.
yes
971 Mitsubishi Motors Corp.?s new vehicle sales in the US fell
46 percent in June.
Mitsubishi sales rose 46 percent. no
1806 Vanunu, 49, was abducted by Israeli agents and convicted
of treason in 1986 after discussing his work as a mid-level
Dimona technician with Britain?s Sunday Times newspaper.
Vanunu?s disclosures in 1968 led experts
to conclude that Israel has a stockpile of
nuclear warheads.
no
2081 The main race track in Qatar is located in Shahaniya, on the
Dukhan Road.
Qatar is located in Shahaniya. no
Table 1: Illustrative examples from the PASCAL RTE data set, available at http://www.pascal-network.org/Challenges/RTE.
Though most problems shown have answer no, the data set is actually balanced between yes and no.
mainly in the sophistication of the matching stage.
The simplest approach is to base the entailment pre-
diction on the degree of semantic overlap between
the text and hypothesis using models based on bags
of words, bags of n-grams, TF-IDF scores, or some-
thing similar (Jijkoun and de Rijke, 2005). Such
models have serious limitations: semantic overlap is
typically a symmetric relation, whereas entailment
is clearly not, and, because overlap models do not
account for syntactic or semantic structure, they are
easily fooled by examples like ID 2081.
A more structured approach is to formulate the
entailment prediction as a graph matching problem
(Haghighi et al, 2005; de Salvo Braz et al, 2005).
In this formulation, sentences are represented as nor-
malized syntactic dependency graphs (like the one
shown in figure 1) and entailment is approximated
with an alignment between the graph representing
the hypothesis and a portion of the corresponding
graph(s) representing the text. Each possible align-
ment of the graphs has an associated score, and the
score of the best alignment is used as an approxi-
mation to the strength of the entailment: a better-
aligned hypothesis is assumed to be more likely to
be entailed. To enable incremental search, align-
ment scores are usually factored as a combination
of local terms, corresponding to the nodes and edges
of the two graphs. Unfortunately, even with factored
scores the problem of finding the best alignment of
two graphs is NP-complete, so exact computation is
intractable. Authors have proposed a variety of ap-
proximate search techniques. Haghighi et al (2005)
divide the search into two steps: in the first step they
consider node scores only, which relaxes the prob-
lem to a weighted bipartite graph matching that can
be solved in polynomial time, and in the second step
they add the edges scores and hillclimb the align-
ment via an approximate local search.
A third approach, exemplified by Moldovan et al
(2003) and Raina et al (2005), is to translate de-
pendency parses into neo-Davidsonian-style quasi-
logical forms, and to perform weighted abductive
theorem proving in the tradition of (Hobbs et al,
1988). Unless supplemented with a knowledge
base, this approach is actually isomorphic to the
graph matching approach. For example, the graph
in figure 1 might generate the quasi-LF rose(e1),
nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2),
dobj(e1, x3), percent(x3), num(x3, x4), 46(x4).
There is a term corresponding to each node and arc,
and the resolution steps at the core of weighted ab-
duction theorem proving consider matching an indi-
vidual node of the hypothesis (e.g. rose(e1)) with
something from the text (e.g. fell(e1)), just as in
the graph-matching approach. The two models be-
come distinct when there is a good supply of addi-
tional linguistic and world knowledge axioms?as in
Moldovan et al (2003) but not Raina et al (2005).
Then the theorem prover may generate intermedi-
ate forms in the proof, but, nevertheless, individ-
ual terms are resolved locally without reference to
global context.
Finally, a few efforts (Akhmatova, 2005; Fowler
et al, 2005; Bos and Markert, 2005) have tried to
42
translate sentences into formulas of first-order logic,
in order to test logical entailment with a theorem
prover. While in principle this approach does not
suffer from the limitations we describe below, in
practice it has not borne much fruit. Because few
problem sentences can be accurately translated to
logical form, and because logical entailment is a
strict standard, recall tends to be poor.
The simple graph matching formulation of the
problem belies three important issues. First, the
above systems assume a form of upward monotonic-
ity: if a good match is found with a part of the text,
other material in the text is assumed not to affect
the validity of the match. But many situations lack
this upward monotone character. Consider variants
on ID 98. Suppose the hypothesis were Arafat tar-
geted for assassination. This would allow a perfect
graph match or zero-cost weighted abductive proof,
because the hypothesis is a subgraph of the text.
However, this would be incorrect because it ignores
the modal operator could. Information that changes
the validity of a proof can also exist outside a match-
ing clause. Consider the alternate text Sharon denies
Arafat is targeted for assassination.1
The second issue is the assumption of locality.
Locality is needed to allow practical search, but
many entailment decisions rely on global features of
the alignment, and thus do not naturally factor by
nodes and edges. To take just one example, drop-
ping a restrictive modifier preserves entailment in a
positive context, but not in a negative one. For exam-
ple, Dogs barked loudly entails Dogs barked, but No
dogs barked loudly does not entail No dogs barked.
These more global phenomena cannot be modeled
with a factored alignment score.
The last issue arising in the graph matching ap-
proaches is the inherent confounding of alignment
and entailment determination. The way to show that
one graph element does not follow from another is
to make the cost of aligning them high. However,
since we are embedded in a search for the lowest
cost alignment, this will just cause the system to
choose an alternate alignment rather than recogniz-
ing a non-entailment. In ID 152, we would like the
hypothesis to align with the first part of the text, to
1This is the same problem labeled and addressed as context
in Tatu and Moldovan (2005).
be able to prove that civilians are not members of
law enforcement agencies and conclude that the hy-
pothesis does not follow from the text. But a graph-
matching system will to try to get non-entailment
by making the matching cost between civilians and
members of law enforcement agencies be very high.
However, the likely result of that is that the final part
of the hypothesis will align with were civilians at
the end of the text, assuming that we allow an align-
ment with ?loose? arc correspondence.2 Under this
candidate alignment, the lexical alignments are per-
fect, and the only imperfect alignment is the subject
arc of were is mismatched in the two. A robust in-
ference guesser will still likely conclude that there is
entailment.
We propose that all three problems can be re-
solved in a two-stage architecture, where the align-
ment phase is followed by a separate phase of en-
tailment determination. Although developed inde-
pendently, the same division between alignment and
classification has also been proposed by Marsi and
Krahmer (2005), whose textual system is developed
and evaluated on parallel translations into Dutch.
Their classification phase features an output space
of five semantic relations, and performs well at dis-
tinguishing entailing sentence pairs.
Finding aligned content can be done by any search
procedure. Compared to previous work, we empha-
size structural alignment, and seek to ignore issues
like polarity and quantity, which can be left to a
subsequent entailment decision. For example, the
scoring function is designed to encourage antonym
matches, and ignore the negation of verb predicates.
The ideas clearly generalize to evaluating several
alignments, but we have so far worked with just
the one-best alignment. Given a good alignment,
the determination of entailment reduces to a simple
classification decision. The classifier is built over
features designed to recognize patterns of valid and
invalid inference. Weights for the features can be
hand-set or chosen to minimize a relevant loss func-
tion on training data using standard techniques from
machine learning. Because we already have a com-
plete alignment, the classifier?s decision can be con-
2Robust systems need to allow matches with imperfect arc
correspondence. For instance, given Bill went to Lyons to study
French farming practices, we would like to be able to conclude
that Bill studied French farming despite the structural mismatch.
43
ditioned on arbitrary global features of the aligned
graphs, and it can detect failures of monotonicity.
3 System
Our system has three stages: linguistic analysis,
alignment, and entailment determination.
3.1 Linguistic analysis
Our goal in this stage is to compute linguistic rep-
resentations of the text and hypothesis that contain
as much information as possible about their seman-
tic content. We use typed dependency graphs, which
contain a node for each word and labeled edges rep-
resenting the grammatical relations between words.
Figure 1 gives the typed dependency graph for ID
971. This representation contains much of the infor-
mation about words and relations between them, and
is relatively easy to compute from a syntactic parse.
However many semantic phenomena are not repre-
sented properly; particularly egregious is the inabil-
ity to represent quantification and modality.
We parse input sentences to phrase structure
trees using the Stanford parser (Klein and Manning,
2003), a statistical syntactic parser trained on the
Penn TreeBank. To ensure correct parsing, we pre-
process the sentences to collapse named entities into
new dedicated tokens. Named entities are identi-
fied by a CRF-based NER system, similar to that
described in (McCallum and Li, 2003). After pars-
ing, contiguous collocations which appear in Word-
Net (Fellbaum, 1998) are identified and grouped.
We convert the phrase structure trees to typed de-
pendency graphs using a set of deterministic hand-
coded rules (de Marneffe et al, 2006). In these rules,
heads of constituents are first identified using a mod-
ified version of the Collins head rules that favor se-
mantic heads (such as lexical verbs rather than aux-
iliaries), and dependents of heads are typed using
tregex patterns (Levy and Andrew, 2006), an exten-
sion of the tgrep pattern language. The nodes in the
final graph are then annotated with their associated
word, part-of-speech (given by the parser), lemma
(given by a finite-state transducer described by Min-
nen et al (2001)) and named-entity tag.
3.2 Alignment
The purpose of the second phase is to find a good
partial alignment between the typed dependency
graphs representing the hypothesis and the text. An
alignment consists of a mapping from each node
(word) in the hypothesis graph to a single node in
the text graph, or to null.3 Figure 1 gives the align-
ment for ID 971.
The space of alignments is large: there are
O((m + 1)n) possible alignments for a hypothesis
graph with n nodes and a text graph with m nodes.
We define a measure of alignment quality, and a
procedure for identifying high scoring alignments.
We choose a locally decomposable scoring function,
such that the score of an alignment is the sum of
the local node and edge alignment scores. Unfor-
tunately, there is no polynomial time algorithm for
finding the exact best alignment. Instead we use an
incremental beam search, combined with a node or-
dering heuristic, to do approximate global search in
the space of possible alignments. We have exper-
imented with several alternative search techniques,
and found that the solution quality is not very sensi-
tive to the specific search procedure used.
Our scoring measure is designed to favor align-
ments which align semantically similar subgraphs,
irrespective of polarity. For this reason, nodes re-
ceive high alignment scores when the words they
represent are semantically similar. Synonyms and
antonyms receive the highest score, and unrelated
words receive the lowest. Our hand-crafted scor-
ing metric takes into account the word, the lemma,
and the part of speech, and searches for word relat-
edness using a range of external resources, includ-
ing WordNet, precomputed latent semantic analysis
matrices, and special-purpose gazettes. Alignment
scores also incorporate local edge scores, which are
based on the shape of the paths between nodes in
the text graph which correspond to adjacent nodes
in the hypothesis graph. Preserved edges receive the
highest score, and longer paths receive lower scores.
3.3 Entailment determination
In the final stage of processing, we make a deci-
sion about whether or not the hypothesis is entailed
by the text, conditioned on the typed dependency
graphs, as well as the best alignment between them.
3The limitations of using one-to-one alignments are miti-
gated by the fact that many multiword expressions (e.g. named
entities, noun compounds, multiword prepositions) have been
collapsed into single nodes during linguistic analysis.
44
rose
sales
Mitsubishi
percent
46
nsubj dobj
nn num
Alignment
rose ? fell
sales ? sales
Mitsubishi ? Mitsubishi Motors Corp.
percent ? percent
46 ? 46
Alignment score: ?0.8962
Features
Antonyms aligned in pos/pos context ?
Structure: main predicate good match +
Number: quantity match +
Date: text date deleted in hypothesis ?
Alignment: good score +
Entailment score: ?5.4262
Figure 1: Problem representation for ID 971: typed dependency graph (hypothesis only), alignment, and entailment features.
Because we have a data set of examples that are la-
beled for entailment, we can use techniques from su-
pervised machine learning to learn a classifier. We
adopt the standard approach of defining a featural
representation of the problem and then learning a
linear decision boundary in the feature space. We
focus here on the learning methodology; the next
section covers the definition of the set of features.
Defined in this way, one can apply any statistical
learning algorithm to this classification task, such
as support vector machines, logistic regression, or
naive Bayes. We used a logistic regression classifier
with a Gaussian prior parameter for regularization.
We also compare our learning results with those
achieved by hand-setting the weight parameters for
the classifier, effectively incorporating strong prior
(human) knowledge into the choice of weights.
An advantage to the use of statistical classifiers
is that they can be configured to output a proba-
bility distribution over possible answers rather than
just the most likely answer. This allows us to get
confidence estimates for computing a confidence
weighted score (see section 5). A major concern in
applying machine learning techniques to this clas-
sification problem is the relatively small size of the
training set, which can lead to overfitting problems.
We address this by keeping the feature dimensional-
ity small, and using high regularization penalties in
training.
4 Feature representation
In the entailment determination phase, the entail-
ment problem is reduced to a representation as a
vector of 28 features, over which the statistical
classifier described above operates. These features
try to capture salient patterns of entailment and
non-entailment, with particular attention to contexts
which reverse or block monotonicity, such as nega-
tions and quantifiers. This section describes the most
important groups of features.
Polarity features. These features capture the pres-
ence (or absence) of linguistic markers of negative
polarity contexts in both the text and the hypothesis,
such as simple negation (not), downward-monotone
quantifiers (no, few), restricting prepositions (with-
out, except) and superlatives (tallest).
Adjunct features. These indicate the dropping or
adding of syntactic adjuncts when moving from the
text to the hypothesis. For the common case of
restrictive adjuncts, dropping an adjunct preserves
truth (Dogs barked loudly |= Dogs barked), while
adding an adjunct does not (Dogs barked 6|= Dogs
barked today). However, in negative-polarity con-
texts (such as No dogs barked), this heuristic is
reversed: adjuncts can safely be added, but not
dropped. For example, in ID 59, the hypothesis
aligns well with the text, but the addition of in Iraq
indicates non-entailment.
We identify the ?root nodes? of the problem: the
root node of the hypothesis graph and the corre-
sponding aligned node in the text graph. Using de-
pendency information, we identify whether adjuncts
have been added or dropped. We then determine
the polarity (negative context, positive context or
restrictor of a universal quantifier) of the two root
nodes to generate features accordingly.
Antonymy features. Entailment problems might
involve antonymy, as in ID 971. We check whether
an aligned pairs of text/hypothesis words appear to
be antonymous by consulting a pre-computed list
of about 40,000 antonymous and other contrasting
pairs derived from WordNet. For each antonymous
pair, we generate one of three boolean features, in-
dicating whether (i) the words appear in contexts of
matching polarity, (ii) only the text word appears in
a negative-polarity context, or (iii) only the hypoth-
esis word does.
45
Modality features. Modality features capture
simple patterns of modal reasoning, as in ID 98,
which illustrates the heuristic that possibility does
not entail actuality. According to the occurrence
(or not) of predefined modality markers, such as
must or maybe, we map the text and the hypoth-
esis to one of six modalities: possible, not possi-
ble, actual, not actual, necessary, and not necessary.
The text/hypothesis modality pair is then mapped
into one of the following entailment judgments: yes,
weak yes, don?t know, weak no, or no. For example:
(not possible |= not actual)? ? yes
(possible |= necessary)? ? weak no
Factivity features. The context in which a verb
phrase is embedded may carry semantic presuppo-
sitions giving rise to (non-)entailments such as The
gangster tried to escape 6|= The gangster escaped.
This pattern of entailment, like others, can be re-
versed by negative polarity markers (The gangster
managed to escape |= The gangster escaped while
The gangster didn?t manage to escape 6|= The gang-
ster escaped). To capture these phenomena, we
compiled small lists of ?factive? and non-factive
verbs, clustered according to the kinds of entail-
ments they create. We then determine to which class
the parent of the text aligned with the hypothesis
root belongs to. If the parent is not in the list, we
only check whether the embedding text is an affir-
mative context or a negative one.
Quantifier features. These features are designed
to capture entailment relations among simple sen-
tences involving quantification, such as Every com-
pany must report |= A company must report (or
The company, or IBM). No attempt is made to han-
dle multiple quantifiers or scope ambiguities. Each
quantifier found in an aligned pair of text/hypothesis
words is mapped into one of five quantifier cate-
gories: no, some, many, most, and all. The no
category is set apart, while an ordering over the
other four categories is defined. The some category
also includes definite and indefinite determiners and
small cardinal numbers. A crude attempt is made to
handle negation by interchanging no and all in the
presence of negation. Features are generated given
the categories of both hypothesis and text.
Number, date, and time features. These are de-
signed to recognize (mis-)matches between num-
bers, dates, and times, as in IDs 1806 and 231. We
do some normalization (e.g. of date representations)
and have a limited ability to do fuzzy matching. In
ID 1806, the mismatched years are correctly iden-
tified. Unfortunately, in ID 231 the significance of
over is not grasped and a mismatch is reported.
Alignment features. Our feature representation
includes three real-valued features intended to rep-
resent the quality of the alignment: score is the
raw score returned from the alignment phase, while
goodscore and badscore try to capture whether the
alignment score is ?good? or ?bad? by computing
the sigmoid function of the distance between the
alignment score and hard-coded ?good? and ?bad?
reference values.
5 Evaluation
We present results based on the First PASCAL RTE
Challenge, which used a development set contain-
ing 567 pairs and a test set containing 800 pairs.
The data sets are balanced to contain equal num-
bers of yes and no answers. The RTE Challenge
recommended two evaluation metrics: raw accuracy
and confidence weighted score (CWS). The CWS is
computed as follows: for each positive integer k up
to the size of the test set, we compute accuracy over
the k most confident predictions. The CWS is then
the average, over k, of these partial accuracies. Like
raw accuracy, it lies in the interval [0, 1], but it will
exceed raw accuracy to the degree that predictions
are well-calibrated.
Several characteristics of the RTE problems
should be emphasized. Examples are derived from a
broad variety of sources, including newswire; there-
fore systems must be domain-independent. The in-
ferences required are, from a human perspective,
fairly superficial: no long chains of reasoning are
involved. However, there are ?trick? questions ex-
pressly designed to foil simplistic techniques. The
definition of entailment is informal and approx-
imate: whether a competent speaker with basic
knowledge of the world would typically infer the hy-
pothesis from the text. Entailments will certainly de-
pend on linguistic knowledge, and may also depend
on world knowledge; however, the scope of required
46
Algorithm RTE1 Dev Set RTE1 Test Set
Acc CWS Acc CWS
Random 50.0% 50.0% 50.0% 50.0%
Jijkoun et al 05 61.0% 64.9% 55.3% 55.9%
Raina et al 05 57.8% 66.1% 55.5% 63.8%
Haghighi et al 05 ? ? 56.8% 61.4%
Bos & Markert 05 ? ? 57.7% 63.2%
Alignment only 58.7% 59.1% 54.5% 59.7%
Hand-tuned 60.3% 65.3% 59.1% 65.0%
Learning 61.2% 74.4% 59.1% 63.9%
Table 2: Performance on the RTE development and test sets.
CWS stands for confidence weighted score (see text).
world knowledge is left unspecified.4
Despite the informality of the problem definition,
human judges exhibit very good agreement on the
RTE task, with agreement rate of 91?96% (Dagan
et al, 2005). In principle, then, the upper bound
for machine performance is quite high. In practice,
however, the RTE task is exceedingly difficult for
computers. Participants in the first PASCAL RTE
workshop reported accuracy from 49% to 59%, and
CWS from 50.0% to 69.0% (Dagan et al, 2005).
Table 2 shows results for a range of systems and
testing conditions. We report accuracy and CWS on
each RTE data set. The baseline for all experiments
is random guessing, which always attains 50% accu-
racy. We show comparable results from recent sys-
tems based on lexical similarity (Jijkoun and de Ri-
jke, 2005), graph alignment (Haghighi et al, 2005),
weighted abduction (Raina et al, 2005), and a mixed
system including theorem proving (Bos and Mark-
ert, 2005).
We then show results for our system under several
different training regimes. The row labeled ?align-
ment only? describes experiments in which all fea-
tures except the alignment score are turned off. We
predict entailment just in case the alignment score
exceeds a threshold which is optimized on devel-
opment data. ?Hand-tuning? describes experiments
in which all features are on, but no training oc-
curs; rather, weights are set by hand, according to
human intuition. Finally, ?learning? describes ex-
periments in which all features are on, and feature
weights are trained on the development data. The
4Each RTE problem is also tagged as belonging to one of
seven tasks. Previous work (Raina et al, 2005) has shown that
conditioning on task can significantly improve accuracy. In this
work, however, we ignore the task variable, and none of the
results shown in table 2 reflect optimization by task.
figures reported for development data performance
therefore reflect overfitting; while such results are
not a fair measure of overall performance, they can
help us assess the adequacy of our feature set: if
our features have failed to capture relevant aspects
of the problem, we should expect poor performance
even when overfitting. It is therefore encouraging
to see CWS above 70%. Finally, the figures re-
ported for test data performance are the fairest ba-
sis for comparison. These are significantly better
than our results for alignment only (Fisher?s exact
test, p < 0.05), indicating that we gain real value
from our features. However, the gain over compara-
ble results from other teams is not significant at the
p < 0.05 level.
A curious observation is that the results for hand-
tuned weights are as good or better than results for
learned weights. A possible explanation runs as fol-
lows. Most of the features represent high-level pat-
terns which arise only occasionally. Because the
training data contains only a few hundred exam-
ples, many features are active in just a handful of
instances; their learned weights are therefore quite
noisy. Indeed, a feature which is expected to fa-
vor entailment may even wind up with a negative
weight: the modal feature weak yes is an example.
As shown in table 3, the learned weight for this fea-
ture was strongly negative ? but this resulted from
a single training example in which the feature was
active but the hypothesis was not entailed. In such
cases, we shouldn?t expect good generalization to
test data, and human intuition about the ?value? of
specific features may be more reliable.
Table 3 shows the values learned for selected fea-
ture weights. As expected, the features added ad-
junct in all context, modal yes, and text is factive
were all found to be strong indicators of entailment,
while date insert, date modifier insert, widening
from text to hyp all indicate lack of entailment. Inter-
estingly, text has neg marker and text & hyp diff po-
larity were also found to disfavor entailment; while
this outcome is sensible, it was not anticipated or
designed.
6 Conclusion
The best current approaches to the problem of tex-
tual inference work by aligning semantic graphs,
47
Feature class & condition weight
Adjunct added adjunct in all context 1.40
Date date mismatch 1.30
Alignment good score 1.10
Modal yes 0.70
Modal no 0.51
Factive text is factive 0.46
. . . . . . . . .
Polarity text & hyp same polarity ?0.45
Modal don?t know ?0.59
Quantifier widening from text to hyp ?0.66
Polarity text has neg marker ?0.66
Polarity text & hyp diff polarity ?0.72
Alignment bad score ?1.53
Date date modifier insert ?1.57
Modal weak yes ?1.92
Date date insert ?2.63
Table 3: Learned weights for selected features. Positive weights
favor entailment. Weights near 0 are omitted. Based on training
on the PASCAL RTE development set.
using a locally-decomposable alignment score as a
proxy for strength of entailment. We have argued
that such models suffer from three crucial limita-
tions: an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
entailment determination.
We have described a system which extends
alignment-based systems while attempting to ad-
dress these limitations. After finding the best align-
ment between text and hypothesis, we extract high-
level semantic features of the entailment problem,
and input these features to a statistical classifier to
make an entailment decision. Using this multi-stage
architecture, we report results on the PASCAL RTE
data which surpass previously-reported results for
alignment-based systems.
We see the present work as a first step in a promis-
ing direction. Much work remains in improving the
entailment features, many of which may be seen as
rough approximations to a formal monotonicity cal-
culus. In future, we aim to combine more precise
modeling of monotonicity effects with better mod-
eling of paraphrase equivalence.
Acknowledgements
We thank Anna Rafferty, Josh Ainslie, and partic-
ularly Roger Grosse for contributions to the ideas
and system reported here. This work was supported
in part by the Advanced Research and Development
Activity (ARDA)?s Advanced Question Answering
for Intelligence (AQUAINT) Program.
References
E. Akhmatova. 2005. Textual entailment resolution via atomic
propositions. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment, 2005.
J. Bos and K. Markert. 2005. Recognising textual entailment
with logical inference. In EMNLP-05.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL
recognising textual entailment challenge. In Proceedings of
the PASCAL Challenges Workshop on Recognising Textual
Entailment.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In LREC 2006.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for semantic entail-
ment and question-answering. In Proceedings of the Twenti-
eth National Conference on Artificial Intelligence (AAAI).
C. Fellbaum. 1998. WordNet: an electronic lexical database.
MIT Press.
A. Fowler, B. Hauser, D. Hodges, I. Niles, A. Novischi, and
J. Stephan. 2005. Applying COGEX to recognize textual
entailment. In Proceedings of the PASCAL Challenges Work-
shop on Recognising Textual Entailment.
A. Haghighi, A. Ng, and C. D. Manning. 2005. Robust textual
inference via graph matching. In EMNLP-05.
J. R. Hobbs, M. Stickel, P. Martin, and D. D. Edwards. 1988.
Interpretation as abduction. In 26th Annual Meeting of the
Association for Computational Linguistics: Proceedings of
the Conference, pages 95?103, Buffalo, New York.
V. Jijkoun and M. de Rijke. 2005. Recognizing textual entail-
ment using lexical similarity. In Proceedings of the PAS-
CAL Challenge Workshop on Recognising Textual Entail-
ment, 2005, pages 73?76.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Meeting of the Associa-
tion of Computational Linguistics.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
LREC 2006.
E. Marsi and E. Krahmer. 2005. Classification of semantic re-
lations by humans and machines. In Proceedings of the ACL
2005 Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In Proceedings of CoNLL 2003.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morpho-
logical processing in English. In Natural Language Engi-
neering, volume 7(3), pages 207?233.
D. Moldovan, C. Clark, S. Harabagiu, and S. Maiorano. 2003.
COGEX: A logic prover for question answering. In NAACL-
03.
M. Pasca and S. Harabagiu. 2001. High performance ques-
tion/answering. In SIGIR-01, pages 366?374.
R. Raina, A .Ng, and C. D. Manning. 2005. Robust textual
inference via learning and abductive reasoning. In Proceed-
ings of the Twentieth National Conference on Artificial Intel-
ligence (AAAI).
M. Tatu and D. Moldovan. 2005. A semantic approach to rec-
ognizing textual entailment. In HLT/EMNLP 2005, pages
371?378.
48
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
Proceedings of the Third Workshop on Statistical Machine Translation, pages 26?34,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Regularization and Search for Minimum Error Rate Training
Daniel Cer, Daniel Jurafsky, and Christopher D. Manning
Stanford University
Stanford, CA 94305
cerd,jurafsky,manning@stanford.edu
Abstract
Minimum error rate training (MERT) is a
widely used learning procedure for statistical
machine translation models. We contrast three
search strategies for MERT: Powell?s method,
the variant of coordinate descent found in the
Moses MERT utility, and a novel stochastic
method. It is shown that the stochastic method
obtains test set gains of +0.98 BLEU on MT03
and +0.61 BLEU on MT05. We also present
a method for regularizing the MERT objec-
tive that achieves statistically significant gains
when combined with both Powell?s method
and coordinate descent.
1 Introduction
Och (2003) introduced minimum error rate training
(MERT) as an alternative training regime to the con-
ditional likelihood objective previously used with
log-linear translation models (Och & Ney, 2002).
This approach attempts to improve translation qual-
ity by optimizing an automatic translation evalua-
tion metric, such as the BLEU score (Papineni et al,
2002). This is accomplished by either directly walk-
ing the error surface provided by an evaluation met-
ric w.r.t. the model weights or by using gradient-
based techniques on a continuous approximation of
such a surface. While the former is piecewise con-
stant and thus cannot be optimized using gradient
techniques, Och (2003) provides an approach that
performs such training efficiently.
In this paper we explore a number of variations on
MERT. First, it is shown that performance gains can
be had by making use of a stochastic search strategy
as compare to that obtained by Powell?s method and
coordinate descent. Subsequently, results are pre-
sented for two regularization strategies1. Both allow
coordinate descent and Powell?s method to achieve
performance that is on par with stochastic search.
In what follows, we briefly review minimum er-
ror rate training, introduce our stochastic search and
regularization strategies, and then present experi-
mental results.
2 Minimum Error Rate Training
Let F be a collection of foreign sentences to be
translated, with individual sentences f0, f1, . . . ,
fn. For each fi, the surface form of an indi-
vidual candidate translation is given by ei with
hidden state hi associated with the derivation of
ei from fi. Each ei is drawn from E , which
represents all possible strings our translation sys-
tem can produce. The (ei,hi, fi) triples are con-
verted into vectors of m feature functions by
? : E ?H ?F ? Rm whose dot product with the
weight vector w assigns a score to each triple.
The idealized translation process then is to find the
highest scoring pair (ei,hi) for each fi, or rather
(ei,hi) = argmax(e?E,h?H)w ??(e,h, f).
The aggregate argmax for the entire data set F is
given by equation (1)2. This gives Ew which repre-
sents the set of translations selected by the model for
data set F when parameterized by the weight vec-
tor w. Let?s assume we have an automated mea-
sure of translation quality ` that maps the collec-
1While we prefer the term regularization, the strategies pre-
sented here could also be referred to as smoothing methods.
2Here, the translation of the entire data set is treated as a
single structured prediction problem using the feature function
vector ?(E,H,F) =Pni ?(ei,hi, fi)
26
id Translation log(PTM(f |e)) log(PLM(e)) BLEU-2
e1 This is it -1.2 -0.1 29.64
e2 This is small house -0.2 -1.2 63.59
e3 This is miniscule building -1.6 -0.9 31.79
e4 This is a small house -0.1 -0.9 100.00
ref This is a small house
Table 1: Four hypothetical translations and their corresponding log model scores from a translation model PTM (f |e)
and a language model PLM (e), along with their BLEU-2 scores according to the given reference translation. The
MERT error surface for these translations is given in figure 1.
tion of translations Ew onto some real valued loss,
` : En ? R. For instance, in the experiments that
follow, the loss corresponds to 1 minus the BLEU
score assigned to Ew for a given collection of refer-
ence translations.
(Ew,Hw) = argmax
(E?En,H?Hn)
w ??(E,H,F) (1)
Using n-best lists produced by a decoder to ap-
proximate En and Hn, MERT searches for the
weight vector w? that minimizes the loss `. Let-
ting E?w denote the result of the translation argmax
w.r.t. the approximate hypothesis space, the MERT
search is then expressed by equation (2). Notice the
objective function being optimized is equivalent to
the loss assigned by the automatic measure of trans-
lation quality, i.e. O(w) = `(E?w).
w? = argmin
w
`(E?w) (2)
After performing the parameter search, the de-
coder is then re-run using the weights w? to produce
a new set of n-best lists, which are then concate-
nated with the prior n-best lists in order to obtain a
better approximation of En and Hn. The parameter
search given in (2) can then be performed over the
improved approximation. This process repeats un-
til either no novel entries are produced for the com-
bined n-best lists or the weights change by less than
some ? across iterations.
Unlike the objective functions associated with
other popular learning algorithms, the objective O
is piecewise constant over its entire domain. That
is, while small perturbations in the weights, w, will
change the score assigned by w ??(e,h, f) to each
triple, (e,h, f), such perturbations will generally not
change the ranking between the pair selected by the
argmax, (e?,h?) = argmaxw ??(e,h, f), and any
given competing pair (e?,h?). However, at certain
critical points, the score assigned to some compet-
ing pair (e?,h?) will exceed that assigned to the
prior winner (e?wold ,h?wold). At this point, the pairreturned by argmaxw ??(e,h, f) will change and
loss ` will be evaluated using the newly selected e?.
Figure 1: MERT objective for the translations given
in table 1. Regions are labeled with the translation
that dominates within it, i.e. argmaxw ??(e, f),
and with their corresponding objective values,
1? `(argmaxw ??(e, f)).
This is illustrated in figure (1), which plots the
MERT objective function for a simple model with
two parameters, wtm & wlm, and for which the
space of possible translations, E , consists of the four
sentences given in table 13. Here, the loss ` is de-
3For this example, we ignore the latent variables, h, associ-
27
fined as 1.0?BLEU-2(e). That is, ` is the differ-
ence between a perfect BLEU score and the BLEU
score calculated for each translation using unigram
and bi-gram counts.
The surface can be visualized as a collection of
plateaus that all meet at the origin and then extend
off into infinity. The latter property illustrates that
the objective is scale invariant w.r.t. the weight vec-
tor w. That is, since any vector w? = ?w ??>0 will
still result in the same relative rankings of all pos-
sible translations according to w ??(e,h, f), such
scaling will not change the translation selected by
the argmax. At the boundaries between regions, the
objective is undefined, as 2 or more candidates are
assigned identical scores by the model. Thus, it is
unclear what should be returned by the argmax for
subsequent scoring by `.
Since the objective is piecewise constant, it can-
not be minimized using gradient descent or even the
sub-gradient method. Two applicable methods in-
clude downhill simplex and Powell?s method (Press
et al, 2007). The former attempts to find a lo-
cal minimum in an n dimensional space by itera-
tively shrinking or growing an n+ 1 vertex simplex4
based on the objective values of the current vertex
points and select nearby points. In contrast, Pow-
ell?s method operates by starting with a single point
in weight space, and then performing a series of line
minimizations until no more progress can be made.
In this paper, we focus on line minimization based
techniques, such as Powell?s method.
2.1 Global minimum along a line
Even without gradient information, numerous meth-
ods can be used to find, or approximately find, local
minima along a line. However, by exploiting the fact
that the underlying scores assigned to competing hy-
potheses, w ??(e,h, f), vary linearly w.r.t. changes
in the weight vector, w, Och (2003) proposed a strat-
egy for finding the global minimum along any given
search direction.
The insight behind the algorithm is as follows.
Let?s assume we are examining two competing
ated with the derivation of each e from the foreign sentence f .
If included, such variables would only change the graph in that
multiple different derivations would be possible for each ej . If
present, the graph could then include disjoint regions that all
map to the same ej and thus the same objective value.
4A simplex can be thought of as a generalization of a triangle
to arbitrary dimensional spaces.
Figure 2: Illustration of how the model score assigned
to each candidate translation varies during a line search
along the coordinate direction wlm with a starting point
of (wtm, wlm) = (1.0, 0.5). Each plotted line corre-
sponds to the model score for one of the translation candi-
dates. The vertical bands are labeled with the hypothesis
that dominates in that region. The transitions between
bands result from the dotted intersections between 1-best
lines.
translation/derivation pairs, (e1,h1) & (e2,h2).
Further, let?s say the score assigned by the
model to (e1,h1) is greater than (e2,h2), i.e.
w ??(e1,h1, f) > w ??(e2,h2, f). Since the
scores of the two vary linearly along any search
direction, d, we can find the point at which the
model?s relative preference for the competing
pairs switches as p = w??(e1,h1,f)?w??(e2,h2,f)d??(e2,h2,f)?d??(e1,h1,f) .
At this particular point, we have the equality
(pd+w) ??(e1,h1, f) = (pd+w) ??(e2,h2, f),
or rather the point at which the scores assigned
by the model to the candidates intersect along
search direction d5. Such points correspond to
the boundaries between adjacent plateaus in the
objective, as prior to the boundary the loss function
` is computed using the translation, e1, and after the
boundary it is computed using e2.
To find the global minimum for a search direc-
tion d, we move along d and for each plateau we
5Notice that, this point only exists if the slopes of the
candidates? model scores along d are not equivalent, i.e. if
d ??(e2,h2, f) 6= d ??(e1,h1, f).
28
Translation m b 1-best
e1 -0.1 -1.25 (0.86,+?]
e2 -1.2 -0.8 (-0.83,0.88)
e3 -0.9 -2.05 n/a
e4 -0.9 -0.55 [??,-0.83]
Table 2: Slopes, m, intercepts, b, and 1-best ranges
for the 4 translations given in table 1 during a line
search along the coordinate wlm, with a starting point of
(wtm, wlm) = (1.0, 0.5). This line search in illustrated
in figure(2).
identify all the points at which the score assigned
by the model to the current 1-best translation inter-
sects the score assigned to competing translations.
At the closest such intersection, we have a new 1-
best translation. Moving to the plateau associated
with this new 1-best, we then repeat the search for
the nearest subsequent intersection. This continues
until we know what the 1-best translations are for all
points along d. The global minimum can then be
found by examining ` once for each of these.
Let?s return briefly to our earlier example given in
table 1. Starting at position (wtm, wlm) = (1.0, 0.5)
and searching along the wlm coordinate, i.e.
(dtm, dlm) = (0.0, 1.0), table 2 gives the line
search slopes, m = d ??(e,h, f), and intercepts,
b = w ??(e,h, f), for each of the four candidate
translations. Using the procedure just described, we
can then find what range of values along d each
candidate translation is assigned the highest rela-
tive model score. Figure 2 illustrates how the score
assigned by the model to each of the translations
changes as we move along d. Each of the banded re-
gions corresponds to a plateau in the objective, and
each of the top most line intersections represents the
transition from one plateau to the next. Note that,
while the surface that is defined by the line segments
with the highest classifier score for each region is
convex, this is not a convex optimization problem as
we are optimizing over the loss ` rather than classi-
fier score.
Pseudocode for the line search is given in algo-
rithm 1. Letting n denote the number of foreign sen-
tences, f , in a dataset, and having m denote the size
of the individual n-best lists, |l|, the time complexity
of the algorithm is given by O(nm2). This is seen
in that each time we check for the nearest intersec-
tion to the current 1-best for some n-best list l, we
Algorithm 1 Och (2003)?s line search method to
find the global minimum in the loss, `, when start-
ing at the point w and searching along the direction
d using the candidate translations given in the col-
lection of n-best lists L.
Input: L, w, d, `
I ? {}
for l ? L do
for e ? l do
m{e} ? e.features ? d
b{e} ? e.features ? w
end for
bestn ? argmaxe?l m{e} {b{e} breaks ties}
loop
bestn+1 = argmine?l max
(
0, b{bestn}?b{e}m{e}?m{bestn}
)
intercept ? max
(
0, b{bestn}?b{bestn+1}m{bestn+1}?m{bestn}
)
if intercept > 0 then
add(I, intercept)
else
break
end if
end loop
end for
add(I, max(I) + 2?)
ibest = argmini?I eval`(L,w + (i? ?) ? d)
return w + (ibest ? ?) ? d
must calculate its intersection with all other candi-
date translations that have yet to be selected as the
1-best. And, for each of the n n-best lists, this may
have to be done up to m? 1 times.
2.2 Search Strategies
In this section, we review two search strategies that,
in conjunction with the line search just described,
can be used to drive MERT. The first, Powell?s
method, was advocated by Och (2003) when MERT
was first introduced for statistical machine transla-
tion. The second, which we call Koehn-coordinate
descent (KCD)6, is used by the MERT utility pack-
aged with the popular Moses statistical machine
translation system (Koehn et al, 2007).
6Moses uses David Chiang?s CMERT package. Within the
source file mert.c, the function that implements the overall
search strategy, optimize koehn(), is based on Philipp Koehn?s
Perl script for MERT optimization that was distributed with
Pharaoh.
29
2.2.1 Powell?s Method
Powell?s method (Press et al, 2007) attempts to
efficiently search the objective by constructing a set
of mutually non-interfering search directions. The
basic procedure is as follows: (i) A collection of
search directions is initialized to be the coordinates
of the space being searched; (ii) The objective is
minimized by looping through the search directions
and performing a line minimization for each; (iii) A
new search direction is constructed that summarizes
the cumulative direction of the progress made dur-
ing step (ii) (i.e., dnew = wpreii ?wpostii). After
a line minimization is performed along dnew, it is
used to replace one of the existing search directions.
(iv) The process repeats until no more progress can
be made. For a quadratic function of n variables,
this procedure comes with the guarantee that it will
reach the minimum within n iterations of the outer
loop. However, since Powell?s method is usually ap-
plied to non-quadratic optimization problems, a typ-
ical implementation will forego the quadratic con-
vergence guarantees in favor of a heuristic scheme
that allows for better navigation of complex sur-
faces.
2.2.2 Koehn?s Coordinate Descent
KCD is a variant of coordinate descent that, at
each iteration, moves along the coordinate which al-
lows for the most progress in the objective. In or-
der to determine which coordinate this is, the rou-
tine performs a trial line minimization along each. It
then updates the weight vector with the one that it
found to be most successful. While much less so-
phisticated that Powell, our results indicate that this
method may be marginally more effective at opti-
mizing the MERT objective7.
3 Extensions
In this section we present and motivate two novel
extensions to MERT. The first is a stochastic alterna-
tive to the Powell and KCD search strategies, while
the second is an efficient method for regularizing the
objective.
7While we are not aware of any previously published results
that demonstrate this, it is likely that we were not the first to
make this discovery as even though Moses? MERT implemen-
tation includes a vestigial implementation of Powell?s method,
the code is hardwired to call optimize koehn rather than the rou-
tine for Powell.
3.1 Random Search Directions
One significant advantage of Powell?s algorithm
over coordinate descent is that it can optimize along
diagonal search directions in weight space. That is,
given a model with a dozen or so features, it can
explore gains that are to be had by simultaneously
varying two or more of the feature weights. In gen-
eral, the diagonals that Powell?s method constructs
allow it to walk objective functions more efficiently
than coordinate descent (Press et al, 2007). How-
ever, given that we have a line search algorithm
that will find the global minima along any given
search direction, diagonal search may be of even
more value. That is, similar to ridge phenomenon
that arise in traditional hill climbing search, it is pos-
sible that there are points in the objective that are the
global minimum along any given coordinate direc-
tion, but are not the global minimum along diagonal
directions.
However, one substantial disadvantage for Pow-
ell is that the assumptions it uses to build up the di-
agonal search directions do not hold in the present
context. Specifically, the search directions are built
up under the assumption that near a minimum the
surface looks approximately quadratic and that we
are performing local line minimizations within such
regions. However, since we are performing global
line minimizations, it is possible for the algorithm to
jump from the region around one minima to another.
If Powell?s method has already started to tune its
search directions for the prior minima, it will likely
be less effective in its efforts to search the new re-
gion. To this extent, coordinate descent will be more
robust than Powell as it has no assumptions that are
violated when such a jump occurs.
One way of salvaging Powell?s algorithm in this
context would be to incorporate additional heuris-
tics that detect when the algorithm has jumped from
the region around one minima to another. When
this occurs, the search directions could be reset to
the coordinates of the space. However, we opt for a
simpler solution, which like Powell?s algorithm per-
forms searches along diagonals in the space, but that
like coordinate descent is sufficiently simple that the
algorithm will not be confused by sudden jumps be-
tween regions.
Specifically, the search procedure chooses di-
rections at random such that each component
30
Figure 3: Regularization during line search - using, from left to right: (i) the maximum loss of adjacent plateaus, (ii)
the average loss of adjacent plateaus, (iii) no regularization. Each set of bars represents adjacent plateaus along the line
being searched, with the height of the bars representing their associated loss. The vertical lines indicate the surrogate
loss values used for the center region under each of the schemes (i-iii).
is distributed according to a Gaussian8, d s.t.
di ? N(0, 1). This allows the procedure to mini-
mize along diagonal search directions, while making
essentially no assumptions regarding the character-
istics of the objective or the relationship between a
series of sequential line minimizations. In the results
that follow, we show that, perhaps surprisingly, this
simple procedure outperforms both KCD and Pow-
ell?s method.
3.2 Regularization
One potential drawback of MERT, as it is typically
implemented, is that it attempts to find the best pos-
sible set of parameters for a training set without
making any explicit efforts to find a set of param-
eters that can be expected to generalize well. For
example, let?s say that for some objective there is
a very deep but narrow minima that is surrounded
on all sides by very bad objective values. That
is, the BLEU score at the minima might be 39.1
while all surrounding plateaus have a BLEU score
that is < 10. Intuitively, such a minima would be a
very bad solution, as the resulting parameters would
likely exhibit very poor generalization to other data
sets. This could be avoided by regularizing the sur-
face in order to eliminate such spurious minima.
One candidate for performing such regularization
is the continuous approximation of the MERT objec-
tive, O = Epw(`). Och (2003) claimed that this ap-
proximation achieved essentially equivalent perfor-
mance to that obtained when directly using the loss
as the objective, O = `. However, Zens et al (2007)
found that O = Epw(`) achieved substantially better
test set performance than O = `, even though it per-
forms slightly worse on the data used to train the
parameters. Similarly, Smith and Eisner (2006) re-
ported test set gains for the related technique of min-
imum risk annealing, which incorporates a temper-
8However, we speculate that similar results could be ob-
tained using a uniform distribution over (?1, 1)
ature parameter that trades off between the smooth-
ness of the objective and the degree it reflects the
underlying piecewise constant error surface. How-
ever, the most straightforward implementation of
such methods requires a loss that can be applied at
the sentence level. If the evaluation metric of inter-
est does not have this property (e.g. BLEU), the loss
must be approximated using some surrogate, with
successful learning then being tied to how well the
surrogate captures the critical properties of the un-
derlying loss.
The techniques of Zens et al (2007) & Smith
and Eisner (2006) regularize by implicitly smooth-
ing over nearby plateaus in the error surface. We
propose an alternative scheme that operates directly
on the piecewise constant objective and that miti-
gates the problem of spurious local minima by ex-
plicitly smoothing over adjacent plateaus during the
line search. That is, when assessing the desirabil-
ity of any given plateau, we examine a fixed win-
dow w of adjacent plateaus along the direction be-
ing searched and combine their evaluation scores.
We explore two combination methods, max and
average. The former, max, assigns each plateau an
objective value that is equal to the maximum objec-
tive value in its surrounding window, while average
assigns a plateau an objective value that is equal to
its window?s average. Figure 3 illustrates both meth-
ods for regularizing the plateaus and contrasts them
with the case where no regularization is used. No-
tice that, while both methods discount spurious pits
in the objective, average still does place some value
on isolated deep plateaus, and max discounts them
completely.
Note that one potential weakness of this scheme
is the value assigned by the regularized objective
to any given point differs depending on the direc-
tion being searched. As such, it has the potential to
wreak havoc on methods such as Powell?s, which ef-
fectively attempt to learn about the curvature of the
31
objective from a sequence of line minimizations.
4 Experiments
Three sets of experiments were performed. For the
first set, we compare the performance of Powell?s
method, KCD, and our novel stochastic search strat-
egy. We then evaluate the performance of all three
methods when the objective is regularized using the
average of adjacent plateaus for window sizes vary-
ing from 3 to 7. Finally, we repeat the regularization
experiment, but using the maximum objective value
from the adjacent plateaus. These experiments were
performed using the Chinese English evaluation data
provided for NIST MT eval 2002, 2003, and 2005.
MT02 was used as a dev set for MERT learning,
while MT03 and MT05 were used as our test sets.
For all experiments, MERT training was per-
formed using n-best lists from the decoder of size
100. During each iteration, the MERT search was
performed once with a starting point of the weights
used to generate the most recent set of n-best lists
and then 5 more times using randomly selected start-
ing points9. Of these, we retain the weights from
the search that obtained the lowest objective value.
Training continued until either decoding produced
no novel entries for the combined n-best lists or none
of the parameter values changed by more than 1e-5
across subsequent iterations.
4.1 System
Experiments were run using a right-to-left beam
search decoder that achieves a matching BLEU
score to Moses (Koehn et al, 2007) over a variety
of data sets. Moreover, when using the same under-
lying model, the two decoders only produce transla-
tions that differ by one or more words 0.2% of the
time. We made use of a stack size of 50 as it al-
lowed for faster experiments while only performing
modestly worse than a stack of 200. The distortion
limit was set to 6. And, we retrieved 20 translation
options for each unique source phrase.
Our phrase table was built using 1, 140, 693 sen-
tence pairs sampled from the GALE Y2 training
9Only 5 random restarts were used due to time constraints.
Ideally, a sizable number of random restarts should be used in
order to minimize the degree to which the results are influenced
by some runs receiving starting points that are better in general
or perhaps better/worse w.r.t. their specific optimization strat-
egy.
Method Dev Test Test
MT02 MT03 MT05
KCD 30.967 30.778 29.580
Powell 30.638 30.692 29.780
Random 31.681 31.754 30.191
Table 3: BLEU scores obtained by models trained using
the three different parameter search strategies: Powell?s
method, KCD, and stochastic search.
data. The Chinese data was word segmented us-
ing the GALE Y2 retest release of the Stanford
CRF segmenter (Tseng et al, 2005). Phrases were
extracted using the typical approach described in
Koehn et al (2003) of running GIZA++ (Och &
Ney, 2003) in both directions and then merging
the alignments using the grow-diag-final heuristic.
From the merged alignments we also extracted a bi-
directional lexical reordering model conditioned on
the source and the target phrases (Tillmann, 2004)
(Koehn et al, 2007). A 5-gram language model
was created using the SRI language modeling toolkit
(Stolcke, 2002) and trained using the Gigaword cor-
pus and English sentences from the parallel data.
5 Results
As illustrated in table 3, Powell?s method and KCD
achieve a very similar level of performance, with
KCD modestly outperforming Powell on the MT03
test set while Powell modestly outperforms coordi-
nate descent on the MT05 test set. Moreover, the
fact that Powell?s algorithm did not perform better
than KCD on the training data10, and in fact actually
performed modestly worse, suggests that Powell?s
additional search machinery does not provide much
benefit for MERT objectives.
Similarly, the fact that the stochastic search ob-
tains a much higher dev set score than either Pow-
ell or KCD indicates that it is doing a better job
of optimizing the objective than either of the two
alternatives. These gains suggest that stochastic
search does make better use of the global minimum
line search than the alternative methods. Or, alter-
natively, it strengthens the claim that the method
succeeds at combining one of the critical strengths
10This indicates that Powell failed to find a deeper minima
in the objective, since recall that the unregularized objective is
equivalent to the model?s dev set performance.
32
Method Window Dev Test Test
Avg MT02 MT03 MT05
Coordinate none 30.967 30.778 29.580
3 31.665 31.675 30.266
5 31.317 31.229 30.182
7 31.205 31.824 30.149
Powell none 30.638 30.692 29.780
3 31.333 31.412 29.890
5 31.748 31.777 30.334
7 31.249 31.571 30.161
Random none 31.681 31.754 30.191
3 31.548 31.778 30.263
5 31.336 31.647 30.415
7 30.501 29.336 28.372
Method Window Dev Test Test
Max MT02 MT03 MT05
Coordinate none 30.967 30.778 29.580
3 31.536 31.927 30.334
5 31.484 31.702 29.687
7 31.627 31.294 30.199
Powell none 30.638 30.692 29.780
3 31.428 30.944 29.598
5 31.407 31.596 30.090
7 30.870 30.911 29.620
Random none 31.681 31.754 30.191
3 31.179 30.898 29.529
5 30.903 31.666 29.963
7 31.920 31.906 30.674
Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum
loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization
is used. Statistically significant test set gains, p < 0.01, over the respective baselines are in bold face.
of Powell?s method, diagonal search, with coordi-
nate descent?s robustness to the sudden jumps be-
tween regions that result from global line minimiza-
tion. Using an approximate randomization test for
statistical significance (Riezler & Maxwell, 2005),
and with KCD as a baseline, the gains obtained
by stochastic search on MT03 are statistically sig-
nificant (p = 0.002), as are the gains on MT05
(p = 0.005).
Table 4 indicates that performing regularization
by either averaging or taking the maximum of adja-
cent plateaus during the line search leads to gains for
both Powell?s method and KCD. However, no reli-
able additional gains appear to be had when stochas-
tic search is combined with regularization.
It may seem surprising that the regularization
gains for Powell & KCD are seen not only in the test
sets but on the dev set as well. That is, in typical ap-
plications, regularization slightly decreases perfor-
mance on the data used to train the model. However,
this trend can in part be accounted for by the fact that
during training, MERT is using n-best lists for objec-
tive evaluations rather than the more expensive pro-
cess of running the decoder for each point that needs
to be checked. As such, during each iteration of
training, the decoding performance of the model ac-
tually represents its generalization performance rel-
ative to what was learned from the n-best lists cre-
ated during prior iterations. Moreover, better gen-
eralization from the prior n-best lists can also help
drive subsequent learning as there will then be more
high quality translations on the n-best lists used for
future iterations of learning. Additionally, regular-
ization can reduce search errors by reducing the risk
of getting stuck in spurious low loss pits that are in
otherwise bad regions of the space.
6 Conclusions
We have presented two methods for improving the
performance of MERT. The first is a novel stochas-
tic search strategy that appears to make better use of
Och (2003)?s algorithm for finding the global min-
imum along any given search direction than either
coordinate descent or Powell?s method. The sec-
ond is a simple regularization scheme that leads to
performance gains for both coordinate descent and
Powell?s method. However, no further gains are ob-
tained by combining the stochastic search with reg-
ularization of the objective.
One quirk of the regularization scheme presented
here is that the regularization applied to any given
point in the objective varies depending upon what
direction the point is approached from. We are
currently looking at other similar regularization
schemes that maintain consistent objective values
regardless of the search direction.
Acknowledgments
We extend our thanks to our three anonymous reviewers,
33
particularly for the depth of analysis provided. This paper
is based on work funded in part by the Defense Advanced
Research Projects Agency through IBM.
References
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,
Federico, M., Bertoldi, N., Cowan, B., Shen, W.,
Moran, C., Zens, R., Dyer, C., Bojar, O., Con-
stantin, A., & Herbst, E. (2007). Moses: Open
source toolkit for statistical machine translation.
In ACL.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical
phrase-based translation. In HLT-NAACL.
Och, F.-J. (2003). Minimum error rate training in
statistical machine translation. In ACL.
Och, F. J., & Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In ACL.
Och, F. J., & Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29, 19?51.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J.
(2002). Bleu: a method for automatic evaluation
of machine translation. In ACL.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., &
Flannery, B. P. (2007). Numerical recipes 3rd edi-
tion: The art of scientific computing. Cambridge
University Press.
Riezler, S., & Maxwell, J. T. (2005). On some pit-
falls in automatic evaluation and significance test-
ing for mt. In ACL.
Smith, D. A., & Eisner, J. (2006). Minimum risk
annealing for training log-linear models. In ACL.
Stolcke, A. (2002). Srilm ? an extensible language
modeling toolkit. In ICSLP.
Tillmann, C. (2004). A unigram orientation model
for statistical machine translation. In ACL.
Tseng, H., Chang, P., Andrew, G., Jurafsky, D.,
& Manning, C. (2005). A conditional random
field word segmenter for sighan bakeoff 2005. In
SIGHAN Workshop on Chinese Language Pro-
cessing.
Zens, R., Hasan, S., & Ney, H. (2007). A system-
atic comparison of training criteria for statistical
machine translation. In EMNLP.
34
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393?1398,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Bilingual Word Embeddings for Phrase-Based Machine Translation
Will Y. Zou?, Richard Socher, Daniel Cer, Christopher D. Manning
Department of Electrical Engineering? and Computer Science Department
Stanford University, Stanford, CA 94305, USA
{wzou,danielcer,manning}@stanford.edu, richard@socher.org
Abstract
We introduce bilingual word embeddings: se-
mantic embeddings associated across two lan-
guages in the context of neural language mod-
els. We propose a method to learn bilingual
embeddings from a large unlabeled corpus,
while utilizing MT word alignments to con-
strain translational equivalence. The new em-
beddings significantly out-perform baselines
in word semantic similarity. A single semantic
similarity feature induced with bilingual em-
beddings adds near half a BLEU point to the
results of NIST08 Chinese-English machine
translation task.
1 Introduction
It is difficult to recognize and quantify semantic sim-
ilarities across languages. The Fr-En phrase-pair
{?un cas de force majeure?, ?case of absolute neces-
sity?}, Zh-En phrase pair {??????,?persist in a
stubborn manner?} are similar in semantics. If co-
occurrences of exact word combinations are rare in
the training parallel text, it can be difficult for classi-
cal statistical MT methods to identify this similarity,
or produce a reasonable translation given the source
phrase.
We introduce an unsupervised neural model
to learn bilingual semantic embedding for words
across two languages. As an extension to their
monolingual counter-part (Turian et al, 2010;
Huang et al, 2012; Bengio et al, 2003), bilin-
gual embeddings capture not only semantic infor-
mation of monolingual words, but also semantic re-
lationships across different languages. This prop-
erty allows them to define semantic similarity met-
rics across phrase-pairs, making them perfect fea-
tures for machine translation.
To learn bilingual embeddings, we use a new ob-
jective function which embodies both monolingual
semantics and bilingual translation equivalence. The
latter utilizes word alignments, a natural sub-task
in the machine translation pipeline. Through large-
scale curriculum training (Bengio et al, 2009), we
obtain bilingual distributed representations which
lie in the same feature space. Embeddings of di-
rect translations overlap, and semantic relationships
across bilingual embeddings were further improved
through unsupervised learning on a large unlabeled
corpus.
Consequently, we produce for the research com-
munity a first set of Mandarin Chinese word embed-
dings with 100,000 words trained on the Chinese
Gigaword corpus. We evaluate these embedding
on Chinese word semantic similarity from SemEval-
2012 (Jin and Wu, 2012). The embeddings sig-
nificantly out-perform prior work and pruned tf-idf
base-lines. In addition, the learned embeddings
give rise to 0.11 F1 improvement in Named Entity
Recognition on the OntoNotes dataset (Hovy et al,
2006) with a neural network model.
We apply the bilingual embeddings in an end-to-
end phrase-based MT system by computing seman-
tic similarities between phrase pairs. On NIST08
Chinese-English translation task, we obtain an im-
provement of 0.48 BLEU from a competitive base-
line (30.01 BLEU to 30.49 BLEU) with the Stanford
Phrasal MT system.
1393
2 Review of prior work
Distributed word representations are useful in NLP
applications such as information retrieval (Pas?ca et
al., 2006; Manning et al, 2008), search query ex-
pansions (Jones et al, 2006), or representing se-
mantics of words (Reisinger et al, 2010). A num-
ber of methods have been explored to train and ap-
ply word embeddings using continuous models for
language. Collobert et al (2008) learn embed-
dings in an unsupervised manner through a con-
trastive estimation technique. Mnih and Hinton (
2008), Morin and Bengio ( 2005) proposed efficient
hierarchical continuous-space models. To system-
atically compare embeddings, Turian et al (2010)
evaluated improvements they bring to state-of-the-
art NLP benchmarks. Huang et al (2012) intro-
duced global document context and multiple word
prototypes. Recently, morphology is explored to
learn better word representations through Recursive
Neural Networks (Luong et al, 2013).
Bilingual word representations have been ex-
plored with hand-designed vector space mod-
els (Peirsman and Pado? , 2010; Sumita, 2000),
and with unsupervised algorithms such as LDA and
LSA (Boyd-Graber and Resnik, 2010; Tam et al,
2007; Zhao and Xing, 2006). Only recently have
continuous space models been applied to machine
translation (Le et al, 2012). Despite growing in-
terest in these models, little work has been done
along the same lines to train bilingual distributioned
word represenations to improve machine translation.
In this paper, we learn bilingual word embeddings
which achieve competitive performance on seman-
tic word similarity, and apply them in a practical
phrase-based MT system.
3 Algorithm and methods
3.1 Unsupervised training with global context
Our method starts with embedding learning formu-
lations in Collobert et al (2008). Given a context
window c in a document d, the optimization mini-
mizes the following Context Objective for a word w
in the vocabulary:
J (c,d)CO =
?
wr?VR
max(0, 1? f(cw, d) + f(cwr , d))
(1)
Here f is a function defined by a neural network.
wr is a word chosen in a random subset VR of the
vocabulary, and cwr is the context window contain-
ing word wr. This unsupervised objective func-
tion contrasts the score between when the correct
word is placed in context with when a random word
is placed in the same context. We incorporate the
global context information as in Huang et al (2012),
shown to improve performance of word embed-
dings.
3.2 Bilingual initialization and training
In the joint semantic space of words across two lan-
guages, the Chinese word ???? is expected to be
close to its English translation ?government?. At the
same time, when two words are not direct transla-
tions, e.g. ?lake? and the Chinese word ??? (deep
pond), their semantic proximity could be correctly
quantified.
We describe in the next sub-sections the methods
to intialize and train bilingual embeddings. These
methods ensure that bilingual embeddings retain
their translational equivalence while their distribu-
tional semantics are improved during online training
with a monolingual corpus.
3.2.1 Initialization by MT alignments
First, we use MT Alignment counts as weighting
to initialize Chinese word embeddings. In our ex-
periments, we use MT word alignments extracted
with the Berkeley Aligner (Liang et al, 2006) 1.
Specifically, we use the following equation to com-
pute starting word embeddings:
Wt-init =
S
?
s=1
Cts + 1
Ct + S
Ws (2)
In this equation, S is the number of possible tar-
get language words that are aligned with the source
word. Cts denotes the number of times when word t
in the target and word s in the source are aligned in
the training parallel text; Ct denotes the total num-
ber of counts of word t that appeared in the target
language. Finally, Laplace smoothing is applied to
this weighting function.
1On NIST08 Zh-En training data and data from GALE MT
evaluation in the past 5 years
1394
Single-prototype English embeddings by Huang
et al (2012) are used to initialize Chinese em-
beddings. The initialization readily provides a set
(Align-Init) of benchmark embeddings in experi-
ments (Section 4), and ensures translation equiva-
lence in the embeddings at start of training.
3.2.2 Bilingual training
Using the alignment counts, we form alignment
matrices Aen?zh and Azh?en. For Aen?zh, each
row corresponds to a Chinese word, and each col-
umn an English word. An element aij is first as-
signed the counts of when the ith Chinese word is
aligned with the jth English word in parallel text.
After assignments, each row is normalized such that
it sums to one. The matrix Azh?en is defined sim-
ilarly. Denote the set of Chinese word embeddings
as Vzh, with each row a word embedding, and the
set of English word embeddings as Ven. With the
two alignment matrices, we define the Translation
Equivalence Objective:
JTEO-en?zh = ?Vzh ?Aen?zhVen?2 (3)
JTEO-zh?en = ?Ven ?Azh?enVzh?2 (4)
We optimize for a combined objective during train-
ing. For the Chinese embeddings we optimize for:
JCO-zh + ?JTEO-en?zh (5)
For the English embeddings we optimize for:
JCO-en + ?JTEO-zh?en (6)
During bilingual training, we chose the value of ?
such that convergence is achieved for both JCO and
JTEO. A small validation set of word similarities
from (Jin and Wu, 2012) is used to ensure the em-
beddings have reasonable semantics. 2
In the next sections, ?bilingual trained? embed-
dings refer to those initialized with MT alignments
and trained with the objective defined by Equa-
tion 5. ?Monolingual trained? embeddings refer to
those intialized by alignment but trained without
JTEO-en?zh.
2In our experiments, ? = 50.
3.3 Curriculum training
We train 100k-vocabulary word embeddings using
curriculum training (Turian et al, 2010) with Equa-
tion 5. For each curriculum, we sort the vocabu-
lary by frequency and segment the vocabulary by a
band-size taken from {5k, 10k, 25k, 50k}. Separate
bands of the vocabulary are trained in parallel using
minibatch L-BFGS on the Chinese Gigaword cor-
pus 3. We train 100,000 iterations for each curricu-
lum, and the entire 100k vocabulary is trained for
500,000 iterations. The process takes approximately
19 days on a eight-core machine. We show visual-
ization of learned embeddings overlaid with English
in Figure 1. The two-dimensional vectors for this vi-
sualization is obtained with t-SNE (van der Maaten
and Hinton, 2008). To make the figure comprehen-
sible, subsets of Chinese words are provided with
reference translations in boxes with green borders.
Words across the two languages are positioned by
the semantic relationships implied by their embed-
dings.
Figure 1: Overlaid bilingual embeddings: English words
are plotted in yellow boxes, and Chinese words in green;
reference translations to English are provided in boxes
with green borders directly below the original word.
4 Experiments
4.1 Semantic Similarity
We evaluate the Mandarin Chinese embeddings with
the semantic similarity test-set provided by the or-
3Fifth Edition. LDC catelog number LDC2011T13. We only
exclude cna cmn, the Traditional Chinese segment of the cor-
pus.
1395
Table 1: Results on Chinese Semantic Similarity
Method Sp. Corr. K. Tau
(?100) (?100)
Prior work (Jin and Wu, 2012) 5.0
Tf-idf
Naive tf-idf 41.5 28.7
Pruned tf-idf 46.7 32.3
Word Embeddings
Align-Init 52.9 37.6
Mono-trained 59.3 42.1
Biling-trained 60.8 43.3
ganizers of SemEval-2012 Task 4. This test-set con-
tains 297 Chinese word pairs with similarity scores
estimated by humans.
The results for semantic similarity are shown in
Table 1. We show two evaluation metrics: Spear-
man Correlation and Kendall?s Tau. For both, bilin-
gual embeddings trained with the combined objec-
tive defined by Equation 5 perform best. For pruned
tf-idf, we follow Reisinger et al (2010; Huang et
al. (2012) and count word co-occurrences in a 10-
word window. We use the best results from a
range of pruning and feature thresholds to compare
against our method. The bilingual and monolingual
trained embeddings4 out-perform pruned tf-idf by
14.1 and 12.6 Spearman Correlation (?100), respec-
tively. Further, they out-perform embeddings initial-
ized from alignment by 7.9 and 6.4. Both our tf-idf
implementation and the word embeddings have sig-
nificantly higher Kendall?s Tau value compared to
Prior work (Jin and Wu, 2012). We verified Tau cal-
culations with original submissions provided by the
authors.
4.2 Named Entity Recognition
We perform NER experiments on OntoNotes (v4.0)
(Hovy et al, 2006) to validate the quality of the
Chinese word embeddings. Our experimental set-
up is the same as Wang et al (2013). With em-
beddings, we build a naive feed-forward neural net-
work (Collobert et al, 2008) with 2000 hidden neu-
rons and a sliding window of five words. This naive
setting, without sequence modeling or sophisticated
4Due to variations caused by online minibatch L-BFGS, we
take embeddings from five random points out of last 105 mini-
batch iterations, and average their semantic similarity results.
Table 2: Results on Named Entity Recognition
Embeddings Prec. Rec. F1 Improve
Align-Init 0.34 0.52 0.41
Mono-trained 0.54 0.62 0.58 0.17
Biling-trained 0.48 0.55 0.52 0.11
Table 3: Vector Matching Alignment AER (lower is bet-
ter)
Embeddings Prec. Rec. AER
Mono-trained 0.27 0.32 0.71
Biling-trained 0.37 0.45 0.59
join optimization, is not competitive with state-of-
the-art (Wang et al, 2013). Table 2 shows that the
bilingual embeddings obtains 0.11 F1 improvement,
lagging monolingual, but significantly better than
Align-Init (as in Section3.2.1) on the NER task.
4.3 Vector matching alignment
Translation equivalence of the bilingual embeddings
is evaluated by naive word alignment to match word
embeddings by cosine distance.5 The Alignment Er-
ror Rates (AER) reported in Table 3 suggest that
bilingual training using Equation 5 produces embed-
dings with better translation equivalence compared
to those produced by monolingual training.
4.4 Phrase-based machine translation
Our experiments are performed using the Stan-
ford Phrasal phrase-based machine translation sys-
tem (Cer et al, 2010). In addition to NIST08 train-
ing data, we perform phrase extraction, filtering
and phrase table learning with additional data from
GALE MT evaluations in the past 5 years. In turn,
our baseline is established at 30.01 BLEU and rea-
sonably competitive relative to NIST08 results. We
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the decoder.
In the phrase-based MT system, we add one fea-
ture to bilingual phrase-pairs. For each phrase, the
word embeddings are averaged to obtain a feature
vector. If a word is not found in the vocabulary, we
disregard and assume it is not in the phrase; if no
word is found in a phrase, a zero vector is assigned
5This is evaluated on 10,000 randomly selected sentence
pairs from the MT training set.
1396
Table 4: NIST08 Chinese-English translation BLEU
Method BLEU
Our baseline 30.01
Embeddings
Random-Init Mono-trained 30.09
Align-Init 30.31
Mono-trained 30.40
Biling-trained 30.49
to it. We then compute the cosine distance between
the feature vectors of a phrase pair to form a seman-
tic similarity feature for the decoder.
Results on NIST08 Chinese-English translation
task are reported in Table 46. An increase of
0.48 BLEU is obtained with semantic similarity
with bilingual embeddings. The increase is modest,
just surpassing a reference standard deviation 0.29
BLEU Cer et al (2010)7 evaluated on a similar sys-
tem. We intend to publish further analysis on statis-
tical significance of this result as an appendix. From
these suggestive evidence in the MT results, random
initialized monolingual trained embeddings add lit-
tle gains to the baseline. Bilingual initialization and
training seem to be offering relatively more consis-
tent gains by introducing translational equivalence.
5 Conclusion
In this paper, we introduce bilingual word embed-
dings through initialization and optimization con-
straint using MT alignments The embeddings are
learned through curriculum training on the Chinese
Gigaword corpus. We show good performance on
Chinese semantic similarity with bilingual trained
embeddings. When used to compute semantic simi-
larity of phrase pairs, bilingual embeddings improve
NIST08 end-to-end machine translation results by
just below half a BLEU point. This implies that se-
mantic embeddings are useful features for improv-
ing MT systems. Further, our results offer sugges-
tive evidence that bilingual word embeddings act as
high-quality semantic features and embody bilingual
translation equivalence across languages.
6We report case-insensitive BLEU
7With 4-gram BLEU metric from Table 4
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA,
or the US government. We thank John Bauer and
Thang Luong for helpful discussions.
References
A. Klementiev, I. Titov and B. Bhattarai. 2012. Induc-
ing Crosslingual Distributed Representation of Words.
COLING.
Y. Bengio, J. Louradour, R. Collobert and J. Weston.
2009. Curriculum Learning. ICML.
Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of
Machine Learning Research.
Y. Bengio and Y. LeCunn. 2007. Scaling learning algo-
rithms towards AI. Large-Scale Kernal Machines.
J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment
analysis across languages: multilingual supervised la-
tent dirichlet alocation. EMNLP.
D. Cer, M. Galley, D. Jurafsky and C. Manning. 2010.
Phrasal: A Toolkit for Statistical Machine Translation
with Facilities for Extraction and Incorporation of Ar-
bitrary Model Features. In Proceedings of the North
American Association of Computational Linguistics -
Demo Session (NAACL-10).
D. Cer, C. Manning and D. Jurafsky. 2010. The Best
Lexical Metric for Phrase-Based Statistical MT Sys-
tem Optimization. NAACL.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. ICML.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proceedings of the Fourth Workshop on
Statistical Machine Translation.
M. Galley, P. Chang, D. Cer, J. R. Finkel and C. D. Man-
ning. 2008. NIST Open Machine Translation 2008
Evaluation: Stanford University?s System Description.
Unpublished working notes of the 2008 NIST Open
Machine Translation Evaluation Workshop.
S. Green, S. Wang, D. Cer and C. Manning. 2013. Fast
and adaptive online training of feature-rich translation
models. ACL.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N.
Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath
1397
and B. Kingsbury. 2012. Deep Neural Networks for
Acoustic Modeling in Speech Recognition. IEEE Sig-
nal Processing Magazine.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R.
Weischedel. 2006. OntoNotes: the 90% solution.
NAACL-HLT.
E. H. Huang, R. Socher, C. D. Manning and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. ACL.
P. Jin and Y. Wu. 2012. SemEval-2012 Task 4: Eval-
uating Chinese Word Similarity. Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation. Association for Computational Linguistics.
R. Jones. 2006. Generating query substitutions. In Pro-
ceedings of the 15th international conference on World
Wide Web.
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical
Phrase-Based Translation. HLT.
H. Le, A. Allauzen and F. Yvon 2012. Continuous space
translation models with neural networks. NAACL.
P. Liang, B. Taskar and D. Klein. 2006. Alignment by
agreement. NAACL.
M. Luong, R. Socher and C. Manning. 2013. Better
word representations with recursive neural networks
for morphology. CONLL.
L. van der Maaten and G. Hinton. 2008. Visualizing data
using t-SNE. Journal of Machine Learning Research.
A. Maas and R. E. Daly and P. T. Pham and D. Huang and
A. Y. Ng and C. Potts. 2011. Learning word vectors
for sentiment analysis. ACL.
C. Manning and P. Raghavan and H. Schtze. 2008. Intro-
duction to Information Retrieval. Cambridge Univer-
sity Press, New York, NY, USA.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky and S.
Khudanpur. 2010. Recurrent neural network based
language model. INTERSPEECH.
T. Mikolov, K. Chen, G. Corrado and J. Dean. 2013. Ef-
ficient Estimation of Word Representations in Vector
Space. arXiv:1301.3781v1.
A. Mnih and G. Hinton. 2008. A scalable hierarchical
distributed language model. NIPS.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. AISTATS.
F. Och. 2003. Minimum error rate training in statistical
machine translation. ACL.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits and A. Jain.
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. ACL.
Y. Peirsman and S. Pado?. 2010. Cross-lingual induction
of selectional preferences with bilingual vector spaces.
ACL.
J. Reisinger and R. J. Mooney. 2010. Multi-prototype
vector-space models of word meaning. NAACL.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Comput. Surv., 34:1-47, March.
R. Socher, J. Pennington, E. Huang, A. Y. Ng and
C. D. Manning. 2011. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
EMNLP.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
NIPS.
E. Sumita. 2000. Lexical transfer using a vector-space
model. ACL.
Y. Tam, I. Lane and T. Schultz. 2007. Bilingual-LSA
based LM adaptation for spoken language translation.
ACL.
S. Tellex and B. Katz and J. Lin and A. Fernandes and
G. Marton. 2003. Quantitative evaluation of passage
retrieval algorithms for question answering. In Pro-
ceedings of the 26th Annual International ACM SIGIR
Conference on Search and Development in Informa-
tion Retrieval, pages 41-47. ACM Press.
J. Turian and L. Ratinov and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. ACL.
M. Wang, W. Che and C. D. Manning. 2013. Joint Word
Alignment and Bilingual Named Entity Recognition
Using Dual Decomposition. ACL.
K. Yamada and K. Knight. 2001. A Syntax-based Statis-
tical Translation Model. ACL.
B. Zhao and E. P. Xing 2006. BiTAM: Bilingual topic
AdMixture Models for word alignment. ACL.
1398
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 555?563,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Best Lexical Metric for
Phrase-Based Statistical MT System Optimization
Daniel Cer, Christopher D. Manning and Daniel Jurafsky
Stanford University
Stanford, CA 94305, USA
Abstract
Translation systems are generally trained to
optimize BLEU, but many alternative metrics
are available. We explore how optimizing
toward various automatic evaluation metrics
(BLEU, METEOR, NIST, TER) affects the re-
sulting model. We train a state-of-the-art MT
system using MERT on many parameteriza-
tions of each metric and evaluate the result-
ing models on the other metrics and also us-
ing human judges. In accordance with popular
wisdom, we find that it?s important to train on
the same metric used in testing. However, we
also find that training to a newer metric is only
useful to the extent that the MT model?s struc-
ture and features allow it to take advantage of
the metric. Contrasting with TER?s good cor-
relation with human judgments, we show that
people tend to prefer BLEU and NIST trained
models to those trained on edit distance based
metrics like TER or WER. Human prefer-
ences for METEOR trained models varies de-
pending on the source language. Since using
BLEU or NIST produces models that are more
robust to evaluation by other metrics and per-
form well in human judgments, we conclude
they are still the best choice for training.
1 Introduction
Since their introduction, automated measures of ma-
chine translation quality have played a critical role
in the development and evolution of SMT systems.
While such metrics were initially intended for eval-
uation, popular training methods such as minimum
error rate training (MERT) (Och, 2003) and mar-
gin infused relaxed algorithm (MIRA) (Crammer
and Singer, 2003; Watanabe et al, 2007; Chiang et
al., 2008) train translation models toward a specific
evaluation metric. This makes the quality of the re-
sulting model dependent on how accurately the au-
tomatic metric actually reflects human preferences.
The most popular metric for both comparing sys-
tems and tuning MT models has been BLEU. While
BLEU (Papineni et al, 2002) is relatively simple,
scoring translations according to their n-gram over-
lap with reference translations, it still achieves a rea-
sonable correlation with human judgments of trans-
lation quality. It is also robust enough to use for au-
tomatic optimization. However, BLEU does have a
number of shortcomings. It doesn?t penalize n-gram
scrambling (Callison-Burch et al, 2006), and since
it isn?t aware of synonymous words or phrases, it can
inappropriately penalize translations that use them.
Recently, there have been efforts to develop bet-
ter evaluation metrics. Metrics such as Translation
Edit Rate (TER) (Snover et al, 2006; Snover et al,
2009) and METEOR1 (Lavie and Denkowski, 2009)
perform a more sophisticated analysis of the trans-
lations being evaluated and the scores they produce
tend to achieve a better correlation with human judg-
ments than those produced by BLEU (Snover et al,
2009; Lavie and Denkowski, 2009; Przybocki et al,
2008; Snover et al, 2006).
Their better correlations suggest that we might
obtain higher quality translations by making use of
these new metrics when training our models. We ex-
pect that training on a specific metric will produce
the best performing model according to that met-
1METEOR: Metric for Evaluation of Translation with Ex-
plicit ORdering.
555
ric. Doing better on metrics that better reflect human
judgments seems to imply the translations produced
by the model would be preferred by human judges.
However, there are four potential problems. First,
some metrics could be susceptible to systematic ex-
ploitation by the training algorithm and result in
model translations that have a high score according
to the evaluation metric but that are of low qual-
ity.2 Second, other metrics may result in objective
functions that are harder to optimize. Third, some
may result in better generalization performance at
test time by not encouraging overfitting of the train-
ing data. Finally, as a practical concern, metrics used
for training cannot be too slow.
In this paper, we systematically explore these four
issues for the most popular metrics available to the
MT community. We examine how well models per-
form both on the metrics on which they were trained
and on the other alternative metrics. Multiple mod-
els are trained using each metric in order to deter-
mine the stability of the resulting models. Select
models are scored by human judges in order to deter-
mine how performance differences obtained by tun-
ing to different automated metrics relates to actual
human preferences.
The next sections introduce the metrics and our
training procedure. We follow with two sets of core
results, machine evaluation in section 5, and human
evaluation in section 6.
2 Evaluation Metrics
Designing good automated metrics for evaluating
machine translations is challenging due to the vari-
ety of acceptable translations for each foreign sen-
tence. Popular metrics produce scores primarily
based on matching sequences of words in the system
translation to those in one or more reference trans-
lations. The metrics primarily differ in how they ac-
count for reorderings and synonyms.
2.1 BLEU
BLEU (Papineni et al, 2002) uses the percentage
of n-grams found in machine translations that also
occur in the reference translations. These n-gram
precisions are calculated separately for different n-
2For example, BLEU computed without the brevity penalty
would likely result in models that have a strong preference for
generating pathologically short translations.
gram lengths and then combined using a geometric
mean. The score is then scaled by a brevity penalty
if the candidate translations are shorter than the ref-
erences, BP = min(1.0, e1?len(R)/len(T )). Equa-
tion 1 gives BLEU using n-grams up to lengthN for
a corpus of candidate translations T and reference
translations R. A variant of BLEU called the NIST
metric (Doddington, 2002) weights n-gram matches
by how informative they are.
BLEU:N =
(
N?
n=1
n-grams(T
?
R)
n-grams(T )
) 1
N
BP (1)
While easy to compute, BLEU has a number of
shortcomings. Since the order of matching n-grams
is ignored, n-grams in a translation can be randomly
rearranged around non-matching material or other
n-gram breaks without harming the score. BLEU
also does not explicitly check whether information
is missing from the candidate translations, as it only
examines what fraction of candidate translation n-
grams are in the references and not what fraction
of references n-grams are in the candidates (i.e.,
BLEU ignores n-gram recall). Finally, the metric
does not account for words and phrases that have
similar meanings.
2.2 METEOR
METEOR (Lavie and Denkowski, 2009) computes
a one-to-one alignment between matching words in
a candidate translation and a reference. If a word
matches multiple other words, preference is given to
the alignment that reorders the words the least, with
the amount of reordering measured by the number of
crossing alignments. Alignments are first generated
for exact matches between words. Additional align-
ments are created by repeatedly running the align-
ment procedure over unaligned words, first allowing
for matches between word stems, and then allow-
ing matches between words listed as synonyms in
WordNet. From the final alignment, the candidate
translation?s unigram precision and recall is calcu-
lated, P = matcheslength trans andR =
matches
length ref . These two
are then combined into a weighted harmonic mean
(2). To penalize reorderings, this value is then scaled
by a fragmentation penalty based on the number of
chunks the two sentences would need to be broken
556
into to allow them to be rearranged with no crossing
alignments, P?,? = 1 ? ?
(
chunks
matches
)?
.
F? =
PR
?P + (1 ? ?)R
(2)
METEOR?,?,? = F? ? P?,? (3)
Equation 3 gives the final METEOR score as the
product of the unigram harmonic mean, F?, and the
fragmentation penalty, P?,? . The free parameters ?,
?, and ? can be used to tune the metric to human
judgments on a specific language and variation of
the evaluation task (e.g., ranking candidate transla-
tions vs. reproducing judgments of translations ade-
quacy and fluency).
2.3 Translation Edit Rate
TER (Snover et al, 2006) searches for the shortest
sequence of edit operations needed to turn a can-
didate translation into one of the reference transla-
tions. The allowable edits are the insertion, dele-
tion, and substitution of individual words and swaps
of adjacent sequences of words. The swap opera-
tion differentiates TER from the simpler word error
rate (WER) metric (Nie?en et al, 2000), which only
makes use of insertions, deletions, and substitutions.
Swaps prevent phrase reorderings from being exces-
sively penalized. Once the shortest sequence of op-
erations is found,3 TER is calculated simply as the
number of required edits divided by the reference
translation length, or average reference translation
length when multiple are available (4).
TER =
min edits
avg ref length
(4)
TER-Plus (TERp) (Snover et al, 2009) extends
TER by allowing the cost of edit operations to be
tuned in order to maximize the metric?s agreement
with human judgments. TERp also introduces three
new edit opertions: word stem matches, WordNet
synonym matches, and multiword matches using a
table of scored paraphrases.
3Since swaps prevent TER from being calculated exactly us-
ing dynamic programming, a beam search is used and this can
overestimate the number of required edits.
3 MERT
MERT is the standard technique for obtaining a ma-
chine translation model fit to a specific evaluation
metric (Och, 2003). Learning such a model cannot
be done using gradient methods since the value of
the objective function only depends on the transla-
tion model?s argmax for each sentence in the tun-
ing set. Typically, this optimization is performed as
a series of line searches that examines the value of
the evaluation metric at critical points where a new
translation argmax becomes preferred by the model.
Since the model score assigned to each candidate
translation varies linearly with changes to the model
parameters, it is possible to efficiently find the global
minimum along any given search direction with only
O(n2) operations when n-best lists are used.
Using our implementation of MERT that allows
for pluggable optimization metrics, we tune mod-
els to BLEU:N for N = 1 . . . 5, TER, two con-
figurations of TERp, WER, several configurations
of METEOR, as well as additive combinations of
these metrics. The TERp configurations include
the default configuration of TERp and TERpA:
the configuration of TERp that was trained to
match human judgments for NIST Metrics MATR
(Matthew Snover and Schwartz, 2008; Przybocki et
al., 2008). For METEOR, we used the standard ME-
TEOR English parameters (? = 0.8, ? = 2.5, ? =
0.4), and the English parameters for the rankingME-
TEOR (? = 0.95, ? = 0.5, ? = 0.5),4 which
was tuned to maximize the metric?s correlation with
WMT-07 human ranking judgements (Agarwal and
Lavie, 2008). The default METEOR parameters fa-
vor longer translations than the other metrics, since
high ? values place much more weight on unigram
recall than precision. Since this may put models
tuned to METEOR at a disadvantage when being
evaluated by the other metrics, we also use a variant
of the standard English model and of ranking ME-
TEOR with ? set to 0.5, as this weights both recall
and precision equally.
For each iteration of MERT, 20 random restarts
were used in addition to the best performing point
discovered during earlier iterations of training.5
4Agarwal and Lavie (2008) report ? = 0.45, however the
0.8.2 release of METEOR uses ? = 0.5 for ranking English.
5This is not necessarily identical with the point returned by
the most recent MERT iteration, but rather can be any point
557
Since MERT is known to be sensitive to what restart
points are provided, we use the same series of ran-
dom restart points for each model. During each it-
eration of MERT, the random seed is based on the
MERT iteration number. Thus, while a different set
of random points is selected during each MERT iter-
ation, on any given iteration all models use the same
set of points. This prevents models from doing better
or worse just because they received different starting
points. However, it is still possible that certain ran-
dom starting points are better for some evaluation
metrics than others.
4 Experiments
Experiments were run using Phrasal (Cer et al,
2010), a left-to-right beam search decoder that
achieves a matching BLEU score to Moses (Koehn
et al, 2007) on a variety of data sets. During de-
coding we made use of a stack size of 100, set the
distortion limit to 6, and retrieved 20 translation op-
tions for each unique source phrase.
Using the selected metrics, we train both Chi-
nese to English and Arabic to English models.6 The
Chinese to English models are trained using NIST
MT02 and evaluated on NIST MT03. The Arabic
to English experiments use NIST MT06 for train-
ing and GALE dev07 for evaluation. The resulting
models are scored using all of the standalone metrics
used during training.
4.1 Arabic to English
Our Arabic to English system was based on a well
ranking 2009 NIST submission (Galley et al, 2009).
The phrase table was extracted using all of the al-
lowed resources for the constrained Arabic to En-
glish track. Word alignment was performed using
the Berkeley cross-EM aligner (Liang et al, 2006).
Phrases were extracted using the grow heuristic
(Koehn et al, 2003). However, we threw away all
phrases that have a P (e|f) < 0.0001 in order to re-
duce the size of the phrase table. From the aligned
data, we also extracted a hierarchical reordering
model that is similar to popular lexical reordering
models (Koehn et al, 2007) but that models swaps
containing more than just one phrase (Galley and
returned during an earlier iteration of MERT.
6Given the amount of time required to train a TERpAmodel,
we only present TERpA results for Chinese to English.
Manning, 2008). A 5-gram language model was cre-
ated with the SRI language modeling toolkit (Stol-
cke, 2002) using all of the English material from
the parallel data employed to train the phrase table
as well as Xinhua Chinese English Parallel News
(LDC2002E18).7 The resulting decoding model has
16 features that are optimized during MERT.
4.2 Chinese to English
For our Chinese to English system, our phrase ta-
ble was built using 1,140,693 sentence pairs sam-
pled from the GALE Y2 training data. The Chinese
sentences were word segmented using the 2008 ver-
sion of Stanford Chinese Word Segmenter (Chang et
al., 2008; Tseng et al, 2005). Phrases were extracted
by running GIZA++ (Och and Ney, 2003) in both
directions and then merging the alignments using
the grow-diag-final heuristic (Koehn et al, 2003).
From the merged alignments we also extracted a
bidirectional lexical reordering model conditioned
on the source and the target phrases (Koehn et al,
2007). A 5-gram language model was created with
the SRI language modeling toolkit (Stolcke, 2002)
and trained using the Gigaword corpus and English
sentences from the parallel data. The resulting de-
coding model has 14 features to be trained.
5 Results
As seen in tables 1 and 2, the evaluation metric we
use during training has a substantial impact on per-
formance as measured by the various other metrics.
There is a clear block structure where the best class
of metrics to train on is the same class that is used
during evaluation. Within this block structure, we
make three primary observations. First, the best
performing model according to any specific metric
configuration is usually not the model we trained to
that configuration. In the Chinese results, the model
trained on BLEU:3 scores 0.74 points higher on
BLEU:4 than the model actually trained to BLEU:4.
In fact, the BLEU:3 trained model outperforms all
other models on BLEU:N metrics. For the Arabic
results, training on NIST scores 0.27 points higher
7In order to run multiple experiments in parallel on the com-
puters available to us, the system we use for this work differs
from our NIST submission in that we remove the Google n-
gram language model. This results in a performance drop of
less than 1.0 BLEU point on our dev data.
558
Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER TERpA METR METR-r METR METR-r
? = 0.5 ? = 0.5
BLEU:1 75.98 55.39 40.41 29.64 21.60 11.94 78.07 78.71 68.28 73.63 41.98 59.63 42.46 60.02
BLEU:2 76.58 57.24 42.84 32.21 24.09 12.20 77.09 77.63 67.16 72.54 43.20 60.91 43.59 61.56
BLEU:3 76.74 57.46 43.13 32.52 24.44 12.22 76.53 77.07 66.81 72.01 42.94 60.57 43.40 60.88
BLEU:4 76.24 56.86 42.43 31.80 23.77 12.14 76.75 77.25 66.78 72.01 43.29 60.94 43.10 61.27
BLEU:5 76.39 57.14 42.93 32.38 24.33 12.40 75.42 75.77 65.86 70.29 43.02 61.22 43.57 61.43
NIST 76.41 56.86 42.34 31.67 23.57 12.38 75.20 75.72 65.78 70.11 43.11 61.04 43.78 61.84
TER 73.23 53.39 39.09 28.81 21.18 12.73 71.33 71.70 63.92 66.58 38.65 55.49 41.76 59.07
TERp 72.78 52.90 38.57 28.32 20.76 12.68 71.76 72.16 64.26 66.96 38.51 56.13 41.48 58.73
TERpA 71.79 51.58 37.36 27.23 19.80 12.54 72.26 72.56 64.58 67.30 37.86 55.10 41.16 58.04
WER 74.49 54.59 40.30 29.88 22.14 12.64 71.85 72.34 63.82 67.11 39.76 57.29 42.37 59.97
METR 73.33 54.35 40.28 30.04 22.39 11.53 84.74 85.30 71.49 79.47 44.68 62.14 42.99 60.73
METR-r 74.20 54.99 40.91 30.66 22.98 11.74 82.69 83.23 70.49 77.77 44.64 62.25 43.44 61.32
METR:0.5 76.36 56.75 42.48 31.98 24.00 12.44 74.94 75.32 66.09 70.14 42.75 60.98 43.86 61.38
METR-r:0.5 76.49 56.93 42.36 31.70 23.68 12.21 77.04 77.58 67.12 72.23 43.26 61.03 43.63 61.67
Combined Models
BLEU:4-TER 75.32 55.98 41.87 31.42 23.50 12.62 72.97 73.38 64.46 67.95 41.50 59.11 43.50 60.82
BLEU:4-2TERp 75.22 55.76 41.57 31.11 23.25 12.64 72.48 72.89 64.17 67.43 41.12 58.82 42.73 60.86
BLEU:4+2MTR 75.77 56.45 42.04 31.47 23.48 11.98 79.96 80.65 68.85 74.84 44.06 61.78 43.70 61.48
Table 1: Chinese to English test set performance on MT03 using models trained using MERT on MT02. In each column,
cells shaded blue are better than average and those shaded red are below average. The intensity of the shading indicates
the degree of deviation from average. For BLEU, NIST, and METEOR, higher is better. For edit distance metrics like
TER and WER, lower is better.
Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER METR METR-r METR METR-r
? = 0.5 ? = 0.5
BLEU:1 79.90 65.35 54.08 45.14 37.81 10.68 46.19 61.04 49.98 49.74 67.79 49.19 68.12
BLEU:2 80.03 65.84 54.70 45.80 38.47 10.75 45.74 60.63 49.24 50.02 68.00 49.71 68.27
BLEU:3 79.87 65.71 54.59 45.67 38.34 10.72 45.86 60.80 49.18 49.87 68.32 49.61 67.67
BLEU:4 80.39 66.14 54.99 46.05 38.70 10.82 45.25 59.83 48.69 49.65 68.13 49.66 67.92
BLEU:5 79.97 65.77 54.64 45.76 38.44 10.75 45.66 60.55 49.11 49.89 68.33 49.64 68.19
NIST 80.41 66.27 55.22 46.32 38.98 10.96 44.11 57.92 47.74 48.88 67.85 49.88 68.52
TER 79.69 65.52 54.44 45.55 38.23 10.75 43.36 56.12 47.11 47.90 66.49 49.55 68.12
TERp 79.27 65.11 54.13 45.35 38.12 10.75 43.36 55.92 47.14 47.83 66.34 49.43 67.94
WER 79.42 65.28 54.30 45.51 38.27 10.78 43.44 56.13 47.13 47.82 66.33 49.38 67.88
METR 75.52 60.94 49.84 41.17 34.12 9.93 52.81 70.08 55.72 50.92 68.55 48.47 66.89
METR-r 77.42 62.91 51.67 42.81 35.61 10.24 49.87 66.26 53.17 50.95 69.29 49.29 67.89
METR:0.5 79.69 65.14 53.94 45.03 37.72 10.72 45.80 60.44 49.34 49.78 68.31 49.23 67.72
METR-r:0.5 79.76 65.12 53.82 44.88 37.57 10.67 46.53 61.55 50.17 49.66 68.57 49.58 68.25
Combined Models
BLEU:4-TER 80.37 66.31 55.27 46.36 39.00 10.96 43.94 57.46 47.46 49.00 67.10 49.85 68.41
BLEU:4-2TERp 79.65 65.53 54.54 45.75 38.48 10.80 43.42 56.16 47.15 47.90 65.93 49.09 67.90
BLEU:4+2METR 79.43 64.97 53.75 44.87 37.58 10.63 46.74 62.03 50.35 50.42 68.92 49.70 68.37
Table 2: Arabic to English test set performance on dev07 using models trained using MERT on MT06. As above, in each
column, cells shaded blue are better than average and those shaded red are below average. The intensity of the shading
indicates the degree of deviation from average.
on BLEU:4 than training on BLEU:4, and outper-
forms all other models on BLEU:N metrics.
Second, the edit distance based metrics (WER,
TER, TERp, TERpA)8 seem to be nearly inter-
changeable. While the introduction of swaps al-
lows the scores produced by the TER metrics to
achieve better correlation with human judgments,
our models are apparently unable to exploit this dur-
ing training. This maybe due to the monotone na-
8In our implementation of multi-reference WER, we use the
length of the references that result in the lowest sentence level
WER to divide the edit costs. In contrast, TER divides by the
average reference length. This difference can sometimes result
in WER being lower than the corresponding TER. Also, as can
be seen in the Arabic to English results, TERp scores sometimes
differ dramatically from TER scores due to normalization and
tokenization differences (e.g., TERp removes punctuation prior
to scoring, while TER does not).
ture of the reference translations and the fact that
having multiple references reduces the need for re-
orderings. However, it is possible that differences
between training to WER and TER would become
more apparent using models that allow for longer
distance reorderings or that do a better job of cap-
turing what reorderings are acceptable.
Third, with the exception of BLEU:1, the perfor-
mance of the BLEU, NIST, and the METEOR ?=.5
models appears to be more robust across the other
evaluation metrics than the standardMETEOR,ME-
TEOR ranking, and edit distance based models
(WER, TER, TERp, an TERpA). The latter mod-
els tend to do quite well on metrics similar to what
they were trained on, while performing particularly
poorly on the other metrics. For example, on Chi-
nese, the TER and WER models perform very well
559
on other edit distance based metrics, while perform-
ing poorly on all the other metrics except NIST.
While less pronounced, the same trend is also seen
in the Arabic data. Interestingly enough, while the
TER, TERp and standard METEOR metrics achieve
good correlations with human judgments, models
trained to them are particularly mismatched in our
results. The edit distance models do terribly on ME-
TEOR and METEOR ranking, while METEOR and
METEOR ranking models do poorly on TER, TERp,
and TERpA.
Training Itr MERT Training Itr MERT
Metric Time Metric Time
BLEU:1 13 21:57 NIST 15 78:15
BLEU:2 15 32:40 TER 7 21:00
BLEU:3 19 45:08 TERp 9 19:19
BLEU:4 10 24:13 TERpA 8 393:16
BLEU:5 16 46:12 WER 13 33:53
BL:4-TR 9 21:07 BL:4-2TRp 8 22:03
METR 12 39:16 METR 0.5 18 42:04
METR R 12 47:19 METR R:0.5 13 25:44
Table 3: Chinese to English MERT iterations and training
times, given in hours:mins and excluding decoder time.
5.1 Other Results
On the training data, we see a similar block struc-
ture within the results, but there is a different pattern
among the top performers. The tables are omitted,
but we observe that, for Chinese, the BLEU:5 model
performs best on the training data according to all
higher order BLEU metrics (4-7). On Arabic, the
BLEU:6 model does best on the same higher order
BLEU metrics (4-7). By rewarding higher order n-
gram matches, these objectives actually find minima
that result in more 4-gram matches than the mod-
els optimized directly to BLEU:4. However, the fact
that this performance advantage disappears on the
evaluation data suggests these higher order models
also promote overfitting.
Models trained on additive metric blends tend
to smooth out performance differences between
the classes of metrics they contain. As expected,
weighting the metrics used in the additive blends re-
sults in models that perform slightly better on the
type of metric with the highest weight.
Table 3 reports training times for select Chinese
to English models. Training to TERpA is very com-
putationally expensive due to the implementation of
the paraphrase table. The TER family of metrics
tends to converge in fewer MERT iterations than
those trained on other metrics such as BLEU, ME-
TEOR or even WER. This suggests that the learning
objective provided by these metrics is either easier to
optimize or they more easily trap the search in local
minima.
5.2 Model Variance
One potential problem with interpreting the results
above is that learning with MERT is generally as-
sumed to be noisy, with different runs of the al-
gorithm possibly producing very different models.
We explore to what extent the results just presented
were affected by noise in the training procedure. We
perform multiple training runs using select evalua-
tion metrics and examining how consistent the re-
sulting models are. This also allows us to deter-
mine whether the metric used as a learning criteria
influences the stability of learning. For these experi-
ments, Chinese to English models are trained 5 times
using a different series of random starting points. As
before, 20 random restarts were used during each
MERT iteration.
In table 4, models trained to BLEU andMETEOR
are relatively stable, with the METEOR:0.5 trained
models being the most stable. The edit distance
models, WER and TERp, vary more across train-
ing runs, but still do not exceed the interesting cross
metric differences seen in table 1. The instability of
WER and TERp, with TERp models having a stan-
dard deviation of 1.3 in TERp and 2.5 in BLEU:4,
make them risky metrics to use for training.
6 Human Evaluation
The best evaluation metric to use during training is
the one that ultimately leads to the best translations
according to human judges. We perform a human
evaluation of select models using Amazon Mechan-
ical Turk, an online service for cheaply performing
simple tasks that require human intelligence. To use
the service, tasks are broken down into individual
units of work known as human intelligence tasks
(HITs). HITs are assigned a small amount of money
that is paid out to the workers that complete them.
For many natural language annotation tasks, includ-
ing machine translation evaluation, it is possible to
obtain annotations that are as good as those pro-
560
Train\Eval ? BLEU:1 BLEU:3 BLEU:4 BLEU:5 TERp WER METEOR METEOR:0.5
BLEU:1 0.17 0.56 0.59 0.59 0.36 0.58 0.42 0.24
BLEU:3 0.38 0.41 0.38 0.32 0.70 0.49 0.44 0.33
BLEU:4 0.27 0.29 0.29 0.27 0.67 0.50 0.41 0.29
BLEU:5 0.17 0.14 0.19 0.21 0.67 0.75 0.34 0.17
TERp 1.38 2.66 2.53 2.20 1.31 1.39 1.95 1.82
WER 0.62 1.37 1.37 1.25 1.31 1.21 1.10 1.01
METEOR 0.80 0.56 0.48 0.44 3.71 2.69 0.69 1.10
METEOR:0.5 0.32 0.11 0.09 0.11 0.23 0.12 0.07 0.11
Table 4: MERT model variation for Chinese to English. We train five models to each metric listed above. The
collection of models trained to a given metric is then evaluated using the other metrics. We report the resulting
standard devation for the collection on each of the metrics. The collection with the lowest varience is bolded.
Model Pair % Preferred p-value
Chinese
METR R vs. TERp 60.0 0.0028
BLEU:4 vs. TERp 57.5 0.02
NIST vs. TERp 55.0 0.089
NIST vs. TERpA 55.0 0.089
BLEU:4 vs. TERpA 54.5 0.11
BLEU:4 vs. METR R 54.5 0.11
METR:0.5 vs. METR 54.5 0.11
METR:0.5 vs. METR R 53.0 0.22
METR vs. BLEU:4 52.5 0.26
BLEU:4 vs. METR:0.5 52.5 0.26
METR vs. TERp 52.0 0.31
NIST vs. BLEU:4 52.0 0.31
BLEU:4 vs. METR R:0.5 51.5 0.36
WER vs. TERp 51.5 0.36
TERpA vs. TERp 50.5 0.47
Arabic
BLEU:4 vs. METR R 62.0 < 0.001
NIST vs. TERp 56.0 0.052
BLEU:4 vs. METR:0.5 55.5 0.069
BLEU:4 vs. METR 54.5 0.11
METR R:0.5 vs METR R 54.0 0.14
NIST vs. BLEU:4 51.5 0.36
WER vs. TERp 51.5 0.36
METR:0.5 vs METR 51.5 0.36
TERp vs. BLEU:4 51.0 0.42
BLEU:4 vs. METR R:0.5 50.5 0.47
Table 5: Select pairwise preference for models trained to
different evaluation metrics. For A vs. B, preferred indi-
cates how often A was preferred to B. We bold the better
training metric for statistically significant differences.
duced by experts by having multiple workers com-
plete each HIT and then combining their answers
(Snow et al, 2008; Callison-Burch, 2009).
We perform a pairwise comparison of the trans-
lations produced for the first 200 sentences of our
Chinese to English test data (MT03) and our Arabic
to English test data (dev07). The HITs consist of a
pair of machine translated sentences and a single hu-
man generated reference translation. The reference
is chosen at random from those available for each
sentence. Capitalization of the translated sentences
is restored using an HMM based truecaser (Lita et
al., 2003). Turkers are instructed to ?. . . select the
machine translation generated sentence that is eas-
iest to read and best conveys what is stated in the
reference?. Differences between the two machine
translations are emphasized by being underlined and
bold faced.9 The resulting HITs are made available
only to workers in the United States, as pilot experi-
ments indicated this results in more consistent pref-
erence judgments. Three preference judgments are
obtained for each pair of translations and are com-
bined using weighted majority vote.
As shown in table 5, in many cases the quality of
the translations produced by models trained to dif-
ferent metrics is remarkably similar. Training to the
simpler edit distance metric WER produces transla-
tions that are as good as those from models tuned to
the similar but more advanced TERp metric that al-
lows for swaps. Similarly, training to TERpA, which
makes use of both a paraphrase table and edit costs
9We emphasize relative differences between the two trans-
lations rather than the difference between each translation and
the reference in order to avoid biasing evaluations toward edit
distance metrics.
561
tuned to human judgments, is no better than TERp.
For the Chinese to English results, there is a sta-
tistically significant human preference for transla-
tions that are produced by training to BLEU:4 and
a marginally significant preferences for training to
NIST over the default configuration of TERp. This
contrasts sharply with earlier work showing that
TER and TERp correlate better with human judge-
ments than BLEU (Snover et al, 2009; Przybocki
et al, 2008; Snover et al, 2006). While it is as-
sumed that, by using MERT, ?improved evaluation
measures lead directly to improved machine trans-
lation quality? (Och, 2003), these results show im-
proved correlations with human judgments are not
always sufficient to establish that tuning to a metric
will result in higher quality translations. In the Ara-
bic results, we see a similar pattern where NIST is
preferred to TERp, again with marginal signficance.
Strangely, however, there is no real difference be-
tween TERp vs. BLEU:4.
For Arabic, training to rankingMETEOR is worse
than BLEU:4, with the differences being very sig-
nificant. The Arabic results also trend toward sug-
gesting that BLEU:4 is better than either standard
METEOR and METEOR ? 0.5. However, for the
Chinese models, training to standard METEOR and
METEOR ? 0.5 is about as good as training to
BLEU:4. In both the Chinese and Arabic results, the
METEOR ? 0.5 models are at least as good as those
trained to standard METEOR and METEOR rank-
ing. In contrast to the cross evaluation metric results,
where the differences between the ? 0.5 models and
the standard METEOR models were always fairly
dramatic, the human preferences suggest there is of-
ten not much of a difference in the true quality of the
translations produced by these models.
7 Conclusion
Training to different evaluation metrics follows the
expected pattern whereby models perform best on
the same type of metric used to train them. How-
ever, models trained using the n-gram based metrics,
BLEU and NIST, are more robust to being evaluated
using the other metrics.
Edit distance models tend to do poorly when eval-
uated on other metrics, as do models trained using
METEOR. However, training models to METEOR
can be made more robust by setting ? to 0.5, which
balances the importance the metric assigns to preci-
sion and recall.
The fact that the WER, TER and TERp models
perform very similarly suggests that current phrase-
based translation systems lack either the features or
the model structure to take advantage of swap edit
operations. The situation might be improved by us-
ing a model that does a better job of both captur-
ing the structure of the source and target sentences
and their allowable reorderings, such as a syntac-
tic tree-to-string system that uses contextually rich
rewrite rules (Galley et al, 2006), or by making use
of larger more fine grained feature sets (Chiang et
al., 2009) that allow for better discrimination be-
tween hypotheses.
Human results indicate that edit distance trained
models such as WER and TERp tend to pro-
duce lower quality translations than BLEU or NIST
trained models. Tuning to METEOR works reason-
ably well for Chinese, but is not a good choice for
Arabic. We suspect that the newer RYPT metric
(Zaidan and Callison-Burch, 2009), which directly
makes use of human adequacy judgements of sub-
strings, would obtain better human results than the
automated metrics presented here. However, like
other metrics, we expect performance gains still will
be sensitive to how the mechanics of the metric inter-
act with the structure and feature set of the decoding
model being used.
BLEU and NIST?s strong showing in both the ma-
chine and human evaluation results indicates that
they are still the best general choice for training
model parameters. We emphasize that improved
metric correlations with human judgments do not
imply that models trained to a metric will result in
higher quality translations. We hope future work
on developing new evaluation metrics will explicitly
explore the translation quality of models trained to
them.
Acknowledgements
The authors thank Alon Lavie for suggesting set-
ting ? to 0.5 when training to METEOR. This work
was supported by the Defense Advanced Research
Projects Agency through IBM. The content does
not necessarily reflect the views of the U.S. Gov-
ernment, and no official endorsement should be in-
ferred.
562
References
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In StatMT workshop at ACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In EACL.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP.
Daniel Cer, Michel Galley, Christopher D. Manning, and
Dan Jurafsky. 2010. Phrasal: A statistical machine
translation toolkit for exploring new model features.
In NAACL.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In StatMT
workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR,
3:951?991.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In HLT.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL.
Michel Galley, Spence Green, Daniel Cer, Pi-Chuan
Chang, and Christopher D. Manning. 2009. Stanford
university?s arabic-to-english statistical machine trans-
lation system for the 2009 NIST evaluation. In NIST
Open Machine Translation Evaluation Meeting.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. tRuEcasIng. In ACL.
Bonnie Dorr Matthew Snover, Nitin Madnani and
Richard Schwartz. 2008. TERp system description.
In MetricsMATR workshop at AMTA.
Sonja Nie?en, Franz Josef Och, and Hermann Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In LREC.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
M. Przybocki, K. Peterson, and S. Bronsart. 2008.
Official results of the ?Metrics for MAchine TRans-
lation? Challenge (MetricsMATR08). Techni-
cal report, NIST, http://nist.gov/speech/
tests/metricsmatr/2008/results/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER?: exploring different human judgments with a
tunable MT metric. In StatMT workshop at EACL).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good? Eval-
uating non-expert annotations for natural language
tasks. In EMNLP.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In ICSLP.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher D. Manning. 2005. A con-
ditional random field word segmenter. In SIGHAN.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In EMNLP, pages 52?61, August.
563
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Phrasal: A Toolkit for Statistical Machine Translation
with Facilities for Extraction and Incorporation of Arbitrary Model Features
Daniel Cer, Michel Galley, Daniel Jurafsky and Christopher D. Manning
Stanford University
Stanford, CA 94305, USA
Abstract
We present a new Java-based open source
toolkit for phrase-based machine translation.
The key innovation provided by the toolkit
is to use APIs for integrating new fea-
tures (/knowledge sources) into the decod-
ing model and for extracting feature statis-
tics from aligned bitexts. The package in-
cludes a number of useful features written to
these APIs including features for hierarchi-
cal reordering, discriminatively trained linear
distortion, and syntax based language models.
Other useful utilities packaged with the toolkit
include: a conditional phrase extraction sys-
tem that builds a phrase table just for a spe-
cific dataset; and an implementation of MERT
that allows for pluggable evaluation metrics
for both training and evaluation with built in
support for a variety of metrics (e.g., TERp,
BLEU, METEOR).
1 Motivation
Progress in machine translation (MT) depends crit-
ically on the development of new and better model
features that allow translation systems to better iden-
tify and construct high quality machine translations.
The popular Moses decoder (Koehn et al, 2007)
was designed to allow new features to be defined us-
ing factored translation models. In such models, the
individual phrases being translated can be factored
into two or more abstract phrases (e.g., lemma, POS-
tags) that can be translated individually and then
combined in a seperate generation stage to arrive at
the final target translation. While greatly enriching
the space of models that can be used for phrase-
based machine translation, Moses only allows fea-
tures that can be defined at the level of individual
words and phrases.
The Phrasal toolkit provides easy-to-use APIs
for the development of arbitrary new model fea-
tures. It includes an API for extracting feature
statistics from aligned bitexts and for incor-
porating the new features into the decoding
model. The system has already been used to
develop a number of innovative new features
(Chang et al, 2009; Galley and Manning, 2008;
Galley and Manning, 2009; Green et al, 2010) and
to build translation systems that have placed well
at recent competitive evaluations, achieving second
place for Arabic to English translation on the NIST
2009 constrained data track.1
We implemented the toolkit in Java because it of-
fers a good balance between performance and de-
veloper productivity. Compared to C++, develop-
ers using Java are 30 to 200% faster, produce fewer
defects, and correct defects up to 6 times faster
(Phipps, 1999). While Java programs were histori-
cally much slower than similar programs written in
C or C++, modern Java virtual machines (JVMs) re-
sult in Java programs being nearly as fast as C++
programs (Bruckschlegel, 2005). Java also allows
for trivial code portability across different platforms.
In the remainder of the paper, we will highlight
various useful capabilities, components and model-
ing features included in the toolkit.
2 Toolkit
The toolkit provides end-to-end support for the cre-
ation and evaluation of machine translation models.
Given sentence-aligned parallel text, a new transla-
tion system can be built using a single command:
java edu.stanford.nlp.mt.CreateModel \
(source.txt) (target.txt) \
(dev.source.txt) (dev.ref) (model_name)
Running this command will first create word
level alignments for the sentences in source.txt
and target.txt using the Berkeley cross-EM aligner
1http://www.itl.nist.gov/iad/mig/tests
/mt/2009/ResultsRelease/currentArabic.html
9
Figure 1: Chinese-to-English translation using discontinuous phrases.
(Liang et al, 2006).2 From the word-to-word
alignments, the system extracts a phrase ta-
ble (Koehn et al, 2003) and hierarchical reorder-
ing model (Galley and Manning, 2008). Two n-
gram language models are trained on the tar-
get.txt sentences: one over lowercased target sen-
tences that will be used by the Phrasal decoder
and one over the original source sentences that
will be used for truecasing the MT output. Fi-
nally, the system trains the feature weights for the
decoding model using minimum error rate train-
ing (Och, 2003) to maximize the system?s BLEU
score (Papineni et al, 2002) on the development
data given by dev.source.txt and dev.ref. The toolkit
is distributed under the GNU general public license
(GPL) and can be downloaded from http://
nlp.stanford.edu/software/phrasal.
3 Decoder
Decoding Engines The package includes two de-
coding engines, one that implements the left-to-
right beam search algorithm that was first intro-
duced with the Pharaoh machine translation system
(Koehn, 2004), and another that provides a recently
developed decoding algorithm for translating with
discontinuous phrases (Galley and Manning, 2010).
Both engines use features written to a common but
extensible feature API, which allows features to be
written once and then loaded into either engine.
Discontinuous phrases provide a mechanism for
systematically translating grammatical construc-
tions. As seen in Fig. 1, using discontinuous phrases
allows us to successfully capture that the Chinese
construction? X? can be translated as when X.
Multithreading The decoder has robust support
for multithreading, allowing it to take full advantage
of modern hardware that provides multiple CPU
cores. As shown in Fig. 2, decoding speed scales
well when the number of threads being used is in-
creased from one to four. However, increasing the
2Optionally, GIZA++ (Och and Ney, 2003) can also be used
to create the word-to-word alignments.
1 2 3 4 5 6 7 8
15
25
35
Cores
tra
n
la
tio
ns
 p
er
 m
in
u
te
Figure 2: Multicore translations per minute on a sys-
tem with two Intel Xeon L5530 processors running at
2.40GHz.
threads past four results in only marginal additional
gains as the cost of managing the resources shared
between the threads is starting to overwhelm the
value provided by each additional thread. Moses
also does not run faster with more than 4-5 threads.3
Feature API The feature API was designed to
abstract away complex implementation details of
the underlying decoding engine and provide a sim-
ple consistent framework for creating new decoding
model features. During decoding, as each phrase
that is translated, the system constructs a Featuriz-
able object. As seen in Table 1, Featurizable objects
specify what phrase was just translated and an over-
all summary of the translation being built. Code that
implements a feature inspects the Featurizable and
returns one or more named feature values. Prior to
translating a new sentence, the sentence is passed to
the active features for a decoding model, so that they
can perform any necessary preliminary analysis.
Comparison with Moses Credible research into
new features requires baseline system performance
that is on par with existing state-of-the-art systems.
Seen in Table 2, Phrasal meets the performance of
Moses when using the exact same decoding model
feature set as Moses and outperforms Moses signifi-
cantly when using its own default feature set.4
3http://statmt.org/moses
/?n=Moses.AdvancedFeatures (April 6, 2010)
4Phrasal was originally written to replicate Moses as it was
implemented in 2007 (release 2007-05-29), and the current ver-
10
Featurizable
Last Translated Phrase Pair
Source and Target Alignments
Partial Translation
Source Sentence
Current Source Coverage
Pointer to Prior Featurizable
Table 1: Information passed to features in the form of a
Featurizable object for each translated phrase.
System Features MT06 (tune) MT03 MT05
Moses Moses 34.23 33.72 32.51
Phrasal Moses 34.25 33.72 32.49
Phrasal Default 35.02 34.98 33.21
Table 2: Comparison of two configurations of Phrasal
to Moses on Chinese-to-English. One Phrasal configura-
tion uses the standard Moses feature set for single factor
phrase-based translation with distance and phrase level
msd-bidirectional-fe reordering features. The other uses
the default configuration of Phrasal, which replaces the
phrase level msd-bidirectional-fe feature with a heirarchi-
cal reordering feature.
4 Features
The toolkit includes the basic eight phrase-based
translation features available in Moses as well as
Moses? implementation of lexical reordering fea-
tures. In addition to the common Moses features, we
also include innovative new features that improve
translation quality. One of these features is a hier-
archical generalization of the Moses lexical reorder-
ing model. Instead of just looking at the reorder-
ing relationship between individual phrases, the new
feature examines the reordering of blocks of ad-
jacent phrases (Galley and Manning, 2008) and im-
proves translation quality when the material being
reordered cannot be captured by single phrase. This
hierarchical lexicalized reordering model is used by
default in Phrasal and is responsible for the gains
shown in Table 2 using the default features.
To illustrate how Phrasal can effectively be used
to design rich feature sets, we present an overview
of various extensions that have been built upon the
sion still almost exactly replicates this implementation when
using only the baseline Moses features. To ensure this con-
figuration of the decoder is still competitive, we compared it
against the current Moses implementation (release 2009-04-
13) and found that the performance of the two systems is still
close. Tthe current Moses implementation obtains slightly
lower BLEU scores, respectively 33.98 and 32.39 on MT06 and
MT05.
Phrasal feature API. These extensions are currently
not included in the release:
Target Side Dependency Language Model The
n-gram language models that are traditionally used
to capture the syntax of the target language do a
poor job of modeling long distance syntactic rela-
tionships. For example, if there are a number of
intervening words between a verb and its subject,
n-gram language models will often not be of much
help in selecting the verb form that agrees with the
subject. The target side dependency language model
feature captures these long distance relationships by
providing a dependency score for the target transla-
tions produced by the decoder. This is done using
an efficient quadratic time algorithm that operates
within the main decoding loop rather than in a sepa-
rate reranking stage (Galley and Manning, 2009).
Discriminative Distortion The standard distor-
tion cost model used in phrase-based MT systems
such as Moses has two problems. First, it does not
estimate the future cost of known required moves,
thus increasing search errors. Second, the model pe-
nalizes distortion linearly, even when appropriate re-
orderings are performed. To address these problems,
we used the Phrasal feature API to design a new
discriminative distortion model that predicts word
movement during translation and that estimates fu-
ture cost. These extensions allow us to triple the
distortion limit and provide a statistically significant
improvement over the baseline (Green et al, 2010).
Discriminative Reordering with Chinese Gram-
matical Relations During translation, a source
sentence can be more accurately reordered if the
system knows something about the syntactic rela-
tionship between the words in the phrases being re-
ordered. The discriminative reordering with Chinese
grammatical relations feature examines the path be-
tween words in a source-side dependency tree and
uses it to evaluate the appropriateness of candidate
phrase reorderings (Chang et al, 2009).
5 Other components
Training Decoding Models The package includes
a comprehensive toolset for training decoding mod-
els. It supports MERT training using coordinate de-
scent, Powell?s method, line search along random
search directions, and downhill Simplex. In addi-
tion to the BLEU metric, models can be trained
11
to optimize other popular evaluation metrics such
as METEOR (Lavie and Denkowski, 2009), TERp
(Snover et al, 2009), mWER (Nie?en et al, 2000),
and PER (Tillmann et al, 1997). It is also possible
to plug in other new user-created evaluation metrics.
Conditional Phrase Table Extraction Rather
than first building a massive phrase table from a par-
allel corpus and then filtering it down to just what
is needed for a specific data set, our toolkit sup-
ports the extraction of just those phrases that might
be used on a given evaluation set. In doing so, it
dramatically reduces the time required to build the
phrase table and related data structures such as re-
ordering models.
Feature Extraction API In order to assist in the
development of new features, the toolkit provides
an API for extracting feature statistics from a word-
aligned parallel corpus. This API ties into the condi-
tional phrase table extraction utility, and thus allows
for the extraction of just those feature statistics that
are relevant to a given data set.
6 Conclusion
Phrasal is an open source state-of-the-art Java-
based machine translation system that was designed
specifically for research into new decoding model
features. The system supports traditional phrase-
based translation as well as translation using discon-
tinuous phrases. It includes a number of new and
innovative model features in addition to those typi-
cally found in phrase-based translation systems. It is
also packaged with other useful components such as
tools for extracting feature statistics, building phrase
tables for specific data sets, and MERT training rou-
tines that support a number of optimization tech-
niques and evaluation metrics.
Acknowledgements
The Phrasal decoder has benefited from the help-
ful comments and code contributions of Pi-Chuan
Chang, Spence Green, Karthik Raghunathan,
Ankush Singla, and Huihsin Tseng. The software
presented in this paper is based on work work was
funded by the Defense Advanced Research Projects
Agency through IBM. The content does not neces-
sarily reflect the views of the U.S. Government, and
no official endorsement should be inferred.
References
Thomas Bruckschlegel. 2005. Microbenchmarking C++,
C#, and Java. C/C++ Users Journal.
P. Chang, H. Tseng, D. Jurafsky, and C.D. Manning.
2009. Discriminative reordering with Chinese gram-
matical relations features. In SSST Workshop at
NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Michel Galley and Christopher Manning. 2010. Improv-
ing phrase-based machine translation with discontigu-
ous phrases. In NAACL.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In In NAACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Sonja Nie?en, Franz Josef Och, and Hermann Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In LREC.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Geoffrey Phipps. 1999. Comparing observed bug and
productivity rates for java and C++. Softw. Pract. Ex-
per., 29(4):345?358.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER?: exploring dif-
ferent human judgments with a tunable MT metric. In
SMT workshop at EACL.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In In Eurospeech.
12
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311?321,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Adaptive Online Training of Feature-Rich Translation Models
Spence Green, Sida Wang, Daniel Cer, and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,sidaw,danielcer,manning}@stanford.edu
Abstract
We present a fast and scalable online
method for tuning statistical machine trans-
lation models with large feature sets. The
standard tuning algorithm?MERT?only
scales to tens of features. Recent discrimi-
native algorithms that accommodate sparse
features have produced smaller than ex-
pected translation quality gains in large
systems. Our method, which is based on
stochastic gradient descent with an adaptive
learning rate, scales to millions of features
and tuning sets with tens of thousands of
sentences, while still converging after only
a few epochs. Large-scale experiments on
Arabic-English and Chinese-English show
that our method produces significant trans-
lation quality gains by exploiting sparse fea-
tures. Equally important is our analysis,
which suggests techniques for mitigating
overfitting and domain mismatch, and ap-
plies to other recent discriminative methods
for machine translation.
1 Introduction
Sparse, overlapping features such as words and n-
gram contexts improve many NLP systems such as
parsers and taggers. Adaptation of discriminative
learning methods for these types of features to sta-
tistical machine translation (MT) systems, which
have historically used idiosyncratic learning tech-
niques for a few dense features, has been an active
research area for the past half-decade. However, de-
spite some research successes, feature-rich models
are rarely used in annual MT evaluations. For exam-
ple, among all submissions to theWMT and IWSLT
2012 shared tasks, just one participant tuned more
than 30 features (Hasler et al, 2012a). Slow uptake
of these methods may be due to implementation
complexities, or to practical difficulties of configur-
ing them for specific translation tasks (Gimpel and
Smith, 2012; Simianer et al, 2012, inter alia).
We introduce a new method for training feature-
rich MT systems that is effective yet comparatively
easy to implement. The algorithm scales to millions
of features and large tuning sets. It optimizes a lo-
gistic objective identical to that of PRO (Hopkins
and May, 2011) with stochastic gradient descent, al-
though other objectives are possible. The learning
rate is set adaptively using AdaGrad (Duchi et al,
2011), which is particularly effective for the mixture
of dense and sparse features present in MT models.
Finally, feature selection is implemented as efficient
L1 regularization in the forward-backward splitting
(FOBOS) framework (Duchi and Singer, 2009). Ex-
periments show that our algorithm converges faster
than batch alternatives.
To learn good weights for the sparse features,
most algorithms?including ours?benefit from
more tuning data, and the natural source is the train-
ing bitext. However, the bitext presents two prob-
lems. First, it has a single reference, sometimes of
lower quality than the multiple references in tun-
ing sets from MT competitions. Second, large bi-
texts often comprise many text genres (Haddow and
Koehn, 2012), a virtue for classical dense MT mod-
els but a curse for high dimensional models: bitext
tuning can lead to a significant domain adaptation
problem when evaluating on standard test sets. Our
analysis separates and quantifies these two issues.
We conduct large-scale translation quality exper-
iments on Arabic-English and Chinese-English. As
baselines we use MERT (Och, 2003), PRO, and
the Moses (Koehn et al, 2007) implementation
of k-best MIRA, which Cherry and Foster (2012)
recently showed to work as well as online MIRA
(Chiang, 2012) for feature-rich models. The first
experiment uses standard tuning and test sets from
the NIST OpenMT competitions. The second ex-
periment uses tuning and test sets sampled from the
large bitexts. The new method yields significant
improvements in both experiments. Our code is
included in the Phrasal (Cer et al, 2010) toolkit,
which is freely available.
311
2 Adaptive Online Algorithms
Machine translation is an unusual machine learning
setting because multiple correct translations exist
and decoding is comparatively expensive. When we
have a large feature set and therefore want to tune
on a large data set, batch methods are infeasible.
Online methods can converge faster, and in practice
they often find better solutions (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia).
Recall that stochastic gradient descent (SGD),
a fundamental online method, updates weights w
according to
wt = wt?1 ? ??`t(wt?1) (1)
with loss function1 `t(w) of the tth example,
(sub)gradient of the loss with respect to the param-
eters ?`t(wt?1), and learning rate ?.
SGD is sensitive to the learning rate ?, which is
difficult to set in an MT system that mixes frequent
?dense? features (like the language model) with
sparse features (e.g., for translation rules). Further-
more, ? applies to each coordinate in the gradient,
an undesirable property in MT where good sparse
features may fire very infrequently. We would in-
stead like to take larger steps for sparse features and
smaller steps for dense features.
2.1 AdaGrad
AdaGrad is a method for setting an adaptive learn-
ing rate that comes with good theoretical guaran-
tees. The theoretical improvement over SGD is
most significant for high-dimensional, sparse fea-
tures. AdaGrad makes the following update:
wt = wt?1 ? ??1/2t ?`t(wt?1) (2)
??1t = ??1t?1 +?`t(wt?1)?`t(wt?1)>
=
t?
i=1
?`i(wi?1)?`i(wi?1)> (3)
A diagonal approximation to ? can be used for a
high-dimensional vector wt. In this case, AdaGrad
is simple to implement and computationally cheap.
Consider a single dimension j, and let scalars vt =
wt,j , gt = ?j`t(wt?1), Gt = ?ti=1 g2i , then theupdate rule is
vt = vt?1 ? ? G?1/2t gt (4)
Gt = Gt?1 + g2t (5)
Compared to SGD, we just need to storeGt = ??1t,jjfor each dimension j.
1We specify the loss function for MT in section 3.1.
2.2 Prior Online Algorithms in MT
AdaGrad is related to two previous online learning
methods for MT.
MIRA Chiang et al (2008) described an adaption
of MIRA (Crammer et al, 2006) to MT. MIRA
makes the following update:
wt = arg min
w
1
2??w ? wt?1?
2
2 + `t(w) (6)
The first term expresses conservativity: the weight
should change as little as possible based on a sin-
gle example, ensuring that it is never beneficial to
overshoot the minimum.
The relationship to SGD can be seen by lineariz-
ing the loss function `t(w) ? `t(wt?1) + (w ?
wt?1)>?`t(wt?1) and taking the derivative of (6).
The result is exactly (1).
AROW Chiang (2012) adapted AROW (Cram-
mer et al, 2009) to MT. AROW models the current
weight as a Gaussian centered at wt?1 with covari-
ance ?t?1, and does the following update upon
seeing training example xt:
wt,?t =
arg min
w,?
1
?DKL(N (w,?)||N (wt?1,?t?1))
+ `t(w) +
1
2?x
>
t ?xt (7)
The KL-divergence term expresses a more general,
directionally sensitive conservativity. Ignoring the
third term, the ? that minimizes the KL is actu-
ally ?t?1. As a result, the first two terms of (7)
generalize MIRA so that we may be more conser-
vative in some directions specified by ?. To see
this, we can write out the KL-divergence between
two Gaussians in closed form, and observe that the
terms involving w do not interact with the terms
involving ?:
wt = arg min
w
1
2? (w ? wt?1)
>??1t?1(w ? wt?1)
+ `t(w) (8)
?t = arg min
?
1
2? log
( |?t?1|
|?|
)
+ 12?Tr
(
??1t?1?
)
+ 12?x
>
t ?xt (9)
The third term in (7), called the confidence term,
gives us adaptivity, the notion that we should have
smaller variance in the direction v as more data xt
312
is seen in direction v. For example, if ? is diagonal
and xt are indicator features, the confidence term
then says that the weight for a rarer feature should
have more variance and vice-versa. Recall that for
generalized linear models?`t(w) ? xt; if we sub-
stitute xt = ?t?`t(w) into (9), differentiate and
solve, we get:
??1t = ??1t?1 + xtx>t
= ??10 +
t?
i=1
?2i?`i(wi?1)?`i(wi?1)>
(10)
The precision ??1t generally grows as more datais seen. Frequently updated features receive an espe-
cially high precision, whereas the model maintains
large variance for rarely seen features.
If we substitute (10) into (8), linearize the loss
`t(w) as before, and solve, then we have the lin-
earized AROW update
wt = wt?1 ? ??t?`t(wt?1) (11)
which is also an adaptive update with per-coordinate
learning rates specified by ?t (as opposed to ?1/2tin AdaGrad).
2.3 Comparing AdaGrad, MIRA, AROW
Compare (3) to (10) and observe that if we set
??10 = 0 and ?t = 1, then the only differencebetween the AROW update (11) and the AdaGrad
update (2) is a square root. Under a constant gradi-
ent, AROW decays the step size more aggressively
(1/t) compared to AdaGrad (1/?t), and it is sensi-
tive to the specification of ??10 .Informally, SGD can be improved in the conser-
vativity direction using MIRA so the updates do
not overshoot. Second, SGD can be improved in
the adaptivity direction using AdaGrad where the
decaying stepsize is more robust and the adaptive
stepsize allows better weight updates to features
differing in sparsity and scale. Finally, AROW com-
bines both adaptivity and conservativity. For MT,
adaptivity allows us to deal withmixed dense/sparse
features effectively without specific normalization.
Why do we choose AdaGrad over AROW?
MIRA/AROW requires selecting the loss function
`(w) so that wt can be solved in closed-form, by
a quadratic program (QP), or in some other way
that is better than linearizing. This usually means
choosing a hinge loss. On the other hand, Ada-
Grad/linearized AROW only requires that the gradi-
ent of the loss function can be computed efficiently.
Algorithm 1 Adaptive online tuning for MT.
Require: Tuning set {fi, e1:ki }i=1:M1: Set w0 = 02: Set t = 1
3: repeat
4: for i in 1 . . .M in random order do
5: Decode n-best list Ni for fi6: Sample pairs {dj,+, dj,?}j=1:s from Ni7: Compute Dt = {?(dj,+)? ?(dj,?)}j=1:s8: Set gt = ?`(Dt; wt?1)}
9: Set ??1t = ??1t?1 + gtg>t . Eq. (3)
10: Update wt = wt?1 ? ??1/2t gt . Eq. (2)11: Regularize wt . Eq. (15)12: Set t = t+ 1
13: end for
14: until convergence
Linearized AROW, however, is less robust than Ada-
Grad empirically2 and lacks known theoretical guar-
antees. Finally, by using AdaGrad, we separate
adaptivity from conservativity. Our experiments
suggest that adaptivity is actually more important.
3 Adaptive Online MT
Algorithm 1 shows the full algorithm introduced in
this paper. AdaGrad (lines 9?10) is a crucial piece,
but the loss function, regularization technique, and
parallelization strategy described in this section are
equally important in the MT setting.
3.1 Pairwise Logistic Loss Function
Algorithm 1 lines 5?8 describe the gradient com-
putation. We cast MT tuning as pairwise ranking
(Herbrich et al, 1999, inter alia), which Hopkins
and May (2011) applied to MT. The pairwise ap-
proach results in simple, convex loss functions suit-
able for online learning. The idea is that for any
two derivations, the ranking predicted by the model
should be consistent with the ranking predicted by
a gold sentence-level metric G like BLEU+1 (Lin
and Och, 2004).
Consider a single source sentence f with asso-
ciated references e1:k. Let d be a derivation in an
n-best list of f that has the target e = e(d) and the
feature map ?(d). Let M(d) = w ? ?(d) be the
model score. For any derivation d+ that is better
than d? under G, we desire pairwise agreement
such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
2According to experiments not reported in this paper.
313
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011).
We compute difference vectors Dt = {x1:s+ } (Al-gorithm 1 line 7) from s pairs (d+, d?) for source
sentence ft. We use the familiar logistic loss:
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(12)
Choosing the hinge loss instead of the logistic
loss results in the 1-class SVM problem. The 1-
class separation problem is equivalent to the binary
classification problem with x+ = ?(d+)? ?(d?)
as positive data and x? = ?x+ as negative data,
which may be plugged into an existing logistic re-
gression solver.
We find that Algorithm 1 works best with mini-
batches instead of single examples. In line 4 we
simply partition the tuning set so that i becomes a
mini-batch of examples.
3.2 Updating and Regularization
Algorithm 1 lines 9?11 compute the adaptive learn-
ing rate, update the weights, and apply regulariza-
tion. Section 2.1 explained the AdaGrad learn-
ing rate computation. To update and regularize
the weights we apply the Forward-Backward Split-
ting (FOBOS) (Duchi and Singer, 2009) framework,
which separates the two operations. The two-step
FOBOS update is
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (13)
wt = arg min
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(14)
where (13) is just an unregularized gradient descent
step and (14) balances the regularization term r(w)
with staying close to the gradient step.
Equation (14) permits efficient L1 regulariza-
tion, which is well-suited for selecting good features
from exponentially many irrelevant features (Ng,
2004). It is well-known that feature selection is very
important for feature-rich MT. For example, sim-
ple indicator features like lexicalized re-ordering
classes are potentially useful yet bloat the the fea-
ture set and, in the worst case, can negatively impact
Algorithm 2 ?Stale gradient? parallelization
method for Algorithm 1.
Require: Tuning set {fi, e1:ki }i=1:M1: Initialize threadpool p1, . . . , pj2: Set t = 1
3: repeat
4: for i in 1 . . .M in random order do
5: Wait until any thread p is idle
6: Send (fi, e1:ki , t) to p . Alg. 1 lines 5?87: while ? p? done with gradient gt? do . t? ? t8: Update wt = wt?1 ? ?gt? . Alg. 1 lines 9?119: Set t = t+ 1
10: end while
11: end for
12: until convergence
search. Some of the features generalize, but many
do not. This was well understood in previous work,
so heuristic filtering was usually applied (Chiang
et al, 2009, inter alia). In contrast, we need only
select an appropriate regularization strength ?.
Specifically, when r(w) = ??w?1, the closed-
form solution to (14) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(15)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls
below the threshold ?t?1?. It is straightforward to
adapt this to AdaGrad with diagonal ? by setting
each dimension of ?t?1,j = ??
1
2
t,jj and by takingelement-wise products.
We find that?`t?1(wt?1) only involves several
hundred active features for the current example
(or mini-batch). However, naively following the
FOBOS framework requires updating millions of
weights. But a practical benefit of FOBOS is that
we can do lazy updates on just the active dimensions
without any approximations.
3.3 Parallelization
Algorithm 1 is inherently sequential like standard
online learning. This is undesirable in MT where
decoding is costly. We therefore parallelize the algo-
rithm with the ?stale gradient? method of Langford
et al (2009) (Algorithm 2). A fixed threadpool of
workers computes gradients in parallel and sends
them to a master thread, which updates a central
weight vector. Crucially, the weight updates need
not be applied in order, so synchronization is unnec-
essary; the workers only idle at the end of an epoch.
The consequence is that the update in line 8 of Al-
gorithm 2 is with respect to gradient gt? with t? ? t.
Langford et al (2009) gave convergence results for
314
stale updating, but the bounds do not apply to our
setting since we use L1 regularization. Neverthe-
less, Gimpel et al (2010) applied this framework
to other non-convex objectives and obtained good
empirical results.
Our asynchronous, stochastic method has practi-
cal appeal for MT. During a tuning run, the online
method decodes the tuning set under many more
weight vectors than a MERT-style batch method.
This characteristic may result in broader exploration
of the search space, and make the learner more ro-
bust to local optima local optima (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia). The
adaptive algorithm identifies appropriate learning
rates for the mixture of dense and sparse features.
Finally, large data structures such as the language
model (LM) and phrase table exist in shared mem-
ory, obviating the need for remote queries.
4 Experiments
We built Arabic-English and Chinese-English MT
systems with Phrasal (Cer et al, 2010), a phrase-
based system based on alignment templates (Och
and Ney, 2004). The corpora3 in our experiments
(Table 1) derive from several LDC sources from
2012 and earlier. We de-duplicated each bitext ac-
cording to exact string match, and ensured that no
overlap existed with the test sets. We produced
alignments with the Berkeley aligner (Liang et al,
2006b) with standard settings and symmetrized via
the grow-diag heuristic.
For each language we used SRILM (Stolcke,
2002) to estimate 5-gram LMs with modified
Kneser-Ney smoothing. We included the monolin-
gual English data and the respective target bitexts.
4.1 Feature Templates
The baseline ?dense? model contains 19 features:
the nine Moses baseline features, the hierarchical
lexicalized re-ordering model of Galley and Man-
ning (2008), the (log) count of each rule, and an
indicator for unique rules.
To the dense features we add three high di-
mensional ?sparse? feature sets. Discrimina-
3We tokenized the English with packages from the Stan-
ford Parser (Klein and Manning, 2003) according to the Penn
Treebank standard (Marcus et al, 1993), the Arabic with the
Stanford Arabic segmenter (Green and DeNero, 2012) accord-
ing to the Penn Arabic Treebank standard (Maamouri et al,
2008), and the Chinese with the Stanford Chinese segmenter
(Chang et al, 2008) according to the Penn Chinese Treebank
standard (Xue et al, 2005).
Bilingual Monolingual
Sentences Tokens Tokens
Ar-En 6.6M 375M 990MZh-En 9.3M 538M
Table 1: Bilingual and monolingual corpora used
in these experiments. The monolingual English
data comes from the AFP and Xinhua sections of
English Gigaword 4 (LDC2009T13).
tive phrase table (PT): indicators for each rule
in the phrase table. Alignments (AL): indica-
tors for phrase-internal alignments and deleted
(unaligned) source words. Discriminative re-
ordering (LO): indicators for eight lexicalized re-
ordering classes, including the six standard mono-
tone/swap/discontinuous classes plus the two sim-
pler Moses monotone/non-monotone classes.
4.2 Tuning Algorithms
The primary baseline is the dense feature set tuned
with MERT (Och, 2003). The Phrasal implemen-
tation uses the line search algorithm of Cer et al
(2008), uniform initialization, and 20 random start-
ing points.4 We tuned according to BLEU-4 (Pap-
ineni et al, 2002).
We built high dimensional baselines with two dif-
ferent algorithms. First, we tuned with batch PRO
using the default settings in Phrasal (L2 regulariza-
tion with ?=0.1). Second, we ran the k-best batch
MIRA (kb-MIRA) (Cherry and Foster, 2012) imple-
mentation in Moses. We did implement an online
version of MIRA, and in small-scale experiments
found that the batch variant worked just as well.
Cherry and Foster (2012) reported the same result,
and their implementation is available in Moses. We
ran their code with standard settings.
Moses5 also contains the discriminative phrase
table implementation of (Hasler et al, 2012b),
which is identical to our implementation using
Phrasal. Moses and Phrasal accept the same phrase
table and LM formats, so we kept those data struc-
tures in common. The two decoders also use the
same multi-stack beam search (Och and Ney, 2004).
For our method, we used uniform initialization,
16 threads, and a mini-batch size of 20. We found
that ?=0.02 and ?=0.1 worked well on development
sets for both languages. To compute the gradients
4Other system settings for all experiments: distortion limit
of 5, a maximum phrase length of 7, and an n-best size of 200.
5v1.0 (28 January 2013)
315
Model #features Algorithm Tuning Set MT02 MT03 MT04 MT09
Dense 19 MERT MT06 45.08 51.32 52.26 51.42 48.44
Dense 19 This paper MT06 44.19 51.42 52.52 50.16 48.13
+PT 151k kb-MIRA MT06 42.08 47.25 48.98 47.08 45.64
+PT 23k PRO MT06 44.31 51.06 52.18 50.23 47.52
+PT 50k This paper MT06 50.61 51.71 52.89 50.42 48.74
+PT+AL+LO 109k PRO MT06 44.87 51.25 52.43 50.05 47.76
+PT+AL+LO 242k This paper MT06 57.84 52.45 53.18 51.38 49.37
Dense 19 MERT MT05/6/8 49.63 51.60 52.29 51.73 48.68
+PT+AL+LO 390k This paper MT05/6/8 58.20 53.61 54.99 52.79 49.94
(Chiang, 2012)* 10-20k MIRA MT04/6 ? ? ? ? 45.90
(Chiang, 2012)* 10-20k AROW MT04/6 ? ? ? ? 47.60
#sentences 728 663 1,075 1,313
Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets
each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213
sentences. Bold indicates statistical significance relative to the best baseline in each block at p < 0.001;
bold-italic at p < 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005).
(*) Chiang (2012) used a similar-sized bitext, but two LMs trained on twice as much monolingual data.
Model #features Algorithm Tuning Set MT02 MT03 MT04
Dense 19 MERT MT06 33.90 35.72 33.71 34.26
Dense 19 This paper MT06 32.60 36.23 35.14 34.78
+PT 105k kb-MIRA MT06 29.46 30.67 28.96 30.05
+PT 26k PRO MT06 33.70 36.87 34.62 34.80
+PT 66k This paper MT06 33.90 36.09 34.86 34.73
+PT+AL+LO 148k PRO MT06 34.81 36.31 33.81 34.41
+PT+AL+LO 344k This paper MT06 38.99 36.40 35.07 34.84
Dense 19 MERT MT05/6/8 32.36 35.69 33.83 34.33
+PT+AL+LO 487k This paper MT05/6/8 37.64 37.81 36.26 36.15
#sentences 878 919 1,597
Table 3: Zh-En results [BLEU-4 % uncased] for the NIST tuning experiment. MT05/6/8 has 4,103
sentences. OpenMT 2009 did not include Zh-En, hence the asymmetry with Table 2.
we sampled 15 derivation pairs for each tuning ex-
ample and scored them with BLEU+1.
4.3 NIST OpenMT Experiment
The first experiment evaluates our algorithm when
tuning and testing on standard test sets, each with
four references. When we add features, our algo-
rithm tends to overfit to a standard-sized tuning set
like MT06. We thus concatenated MT05, MT06,
and MT08 to create a larger tuning set.
Table 2 shows the Ar-En results. Our algorithm
is competitive with MERT in the low dimensional
?dense? setting, and compares favorably to PRO
with the PT feature set. PRO does not benefit
from additional features, whereas our algorithm im-
proves with both additional features and data. The
underperformance of kb-MIRA may result from
a difference between Moses and Phrasal: Moses
MERT achieves only 45.62 on MT09. Moses PRO
with the PT feature set is slightly worse, e.g., 44.52
on MT09. Nevertheless, kb-MIRA does not im-
prove significantly over MERT, and also selects an
unnecessarily large model.
The full feature set PT+AL+LO does help. With
the PT feature set alne, our algorithm tuned on
MT05/6/8 scores well below the best model, e.g.
316
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 45.08 4 39.28 51.42
+PT 72k This paper MT05/6/8 51.29 4 39.50 50.60
+PT 79k This paper bitext5k 44.79 1 43.85 45.73
+PT+AL+LO 647k This paper bitext15k 45.68 1 43.93 45.24
Table 4: Ar-En results [BLEU-4 % uncased] for the bitext tuning experiment. Statistical significance is
relative to the Dense baseline. We include MT04 for comparison to the NIST genre.
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 33.90 4 33.44 34.26
+PT 97k This paper MT05/6/8 34.45 4 35.08 35.19
+PT 67k This paper bitext5k 36.26 1 36.01 33.76
+PT+AL+LO 536k This paper bitext15k 37.57 1 36.30 34.05
Table 5: Zh-En results [BLEU-4 % uncased] for the bitext tuning experiment.
48.56 BLEU on MT09. For Ar-En, our algorithm
thus has the desirable property of benefiting from
more and better features, and more data.
Table 3 shows Zh-En results. Somewhat sur-
prisingly our algorithm improves over MERT in
the dense setting. When we add the discrimina-
tive phrase table, our algorithm improves over kb-
MIRA, and over batch PRO on two evaluation sets.
With all features and the MT05/6/8 tuning set, we
improve significantly over all other models. PRO
learns a smaller model with the PT+AL+LO fea-
ture set which is surprising given that it applies L2
regularization (AdaGrad uses L1). We speculate
that this may be an consequence of stochastic learn-
ing. Our algorithm decodes each example with
a new weight vector, thus exploring more of the
search space for the same tuning set.
4.4 Bitext Tuning Experiment
Tables 2 and 3 show that adding tuning examples
improves translation quality. Nevertheless, even
the larger tuning set is small relative to the bitext
from which rules were extracted. He and Deng
(2012) and Simianer et al (2012) showed significant
translation quality gains by tuning on the bitext.
However, their bitexts matched the genre of their
test sets. Our bitexts, like those of most large-scale
systems, do not. Domain mismatch matters for the
dense feature set (Haddow and Koehn, 2012). We
show that it also matters for feature-rich MT.
Before aligning each bitext, we randomly sam-
pled and sequestered 5k and 15k sentence tuning
sets, and a 5k test set. We prevented overlap be-
DA DB |A| |B| |A ?B|
MT04 MT06 70k 72k 5.9k
MT04 MT568 70k 96k 7.6k
MT04 bitext5k 70k 67k 4.4k
MT04 bitext15k 70k 310k 10.5k
5ktest bitext5k 82k 67k 5.6k
5ktest bitext15k 82k 310k 14k
Table 6: Number of overlapping phrase table (+PT)
features on various Zh-En dataset pairs.
tween the tuning sets and the test set. We then
tuned a dense model with MERT on MT06, and
feature-rich models on both MT05/6/8 and the bi-
text tuning set. Table 4 shows the Ar-En results.
When tuned on bitext5k the translation quality gains
are significant for bitext5k-test relative to tuning on
MT05/6/8, which has multiple references. However,
the bitext5k models do not generalize as well to the
NIST evaluation sets as represented by the MT04
result. Table 5 shows similar trends for Zh-En.
5 Analysis
5.1 Feature Overlap Analysis
How many sparse features appear in both the tun-
ing and test sets? In Table 6, A is the set of phrase
table features that received a non-zero weight when
tuned on datasetDA (same forB). ColumnDA lists
several Zh-En test sets used and column DB lists
tuning sets. Our experiments showed that tuning
on MT06 generalizes better to MT04 than tuning
317
on bitext5k, whereas tuning on bitext5k general-
izes better to bitext5k-test than tuning on MT06.
These trends are consistent with the level of fea-
ture overlap. Phrase table features in A ? B are
overwhelmingly short, simple, and correct phrases,
suggesting L1 regularization is effective for feature
selection. It is also important to balance the number
of features with how well weights can be learned
for those features, as tuning on bitext15k produced
higher coverage for MT04 but worse generalization
than tuning on MT06.
5.2 Domain Adaptation Analysis
To understand the domain adaptation issue we com-
pared the non-zero weights in the discriminative
phrase table (PT) for Ar-En models tuned on bi-
text5k and MT05/6/8. Table 7 illustrates a statisti-
cal idiosyncrasy in the data for the American and
British spellings of program/programme. The mass
is concentrated along the diagonal, probably be-
cause MT05/6/8 was prepared by NIST, an Amer-
ican agency, while the bitext was collected from
many sources including Agence France Presse.
Of course, this discrepancy is consequential for
both dense and feature-rich models. However, we
observe that the feature-rich models fit the tuning
data more closely. For example, the MT05/6/8
model learns rules like l .?A 	KQK. 	?? 	?JK
 ? program
includes, l .?A 	KQK. ? program of, and l .?A 	KQ. ? @ ? 	Y 	?A 	K?
program window. Crucially, it does not learn the
basic rule l .?A 	KQK. ? program.
In contrast, the bitext5k model contains ba-
sic rules such l .?A 	KQK. ? programme, l .?A 	KQ. ? @ @ 	Y?
? this programme, and l .?A 	KQ. ? @ ?? 	X ? that pro-
gramme. It also contains more elaborate rules such
as l .?A 	KQ. ? @ HA? 	? 	K I	KA? ? programme expenses
were and ????A?? @ ?J
KA 	? 	?? @ HCgQ?@ l .?@QK.?manned
space flight programmes. We observed similar
trends for ?defense/defence?, ?analyze/analyse?, etc.
This particular genre problem could be addressed
with language-specific pre-processing, but our sys-
tem solves it in a data-driven manner.
5.3 Re-ordering Analysis
We also analyzed re-ordering differences. Arabic
matrix clauses tend to be verb-initial, meaning that
the subject and verb must be swapped when translat-
ing to English. To assess re-ordering differences?
if any?between the dense and feature-rich models,
we selected all MT09 segments that began with one
# bitext5k # MT05/6/8
programme 185 0
program 19 449
PT rules w/ programme 353 79
PT rules w/ program 9 31
Table 7: Top: comparison of token counts in two
Ar-En tuning sets for programme and program. Bot-
tom: rule counts in the discriminative phrase table
(PT) for models tuned on the two tuning sets. Both
spellings correspond to the Arabic l .?A 	KQK. .
of seven common verbs: ?A? qaal ?said?, hQ?? SrH
?declared?, PA ?@ ashaar ?indicated?, 	?A? kaan ?was?,
Q?
	
X dhkr ?commented?, 	?A 	?@ aDaaf ?added?, 	???@
acln ?announced?. We compared the output of the
MERT Dense model to our method with the full
feature set, both tuned on MT06. Of the 208 source
segments, 32 of the translation pairs contained dif-
ferent word order in the matrix clause. Our feature-
rich model was correct 18 times (56.3%), Dense
was correct 4 times (12.5%), and neither method
was correct 10 times (31.3%).
(1) ref: lebanese prime minister , fuad siniora ,
announced
a. and lebanese prime minister fuad siniora
that
b. the lebanese prime minister fouad siniora
announced
(2) ref: the newspaper and television reported
a. she said the newspaper and television
b. television and newspaper said
In (1) the dense model (1a) drops the verb while the
feature-rich model correctly re-orders and inserts
it after the subject (1b). The coordinated subject
in (2) becomes an embedded subject in the dense
output (2a). The feature-rich model (2b) performs
the correct re-ordering.
5.4 Runtime Comparison
Table 8 compares our method to standard implemen-
tations of the other algorithms. MERT parallelizes
easily but runtime increases quadratically with n-
best list size. PRO runs (single-threaded) L-BFGS
to convergence on every epoch, a potentially slow
procedure for the larger feature set. Moreover, both
318
epochs min.
MERT Dense 22 180
PRO +PT 25 35
kb-MIRA* +PT 26 25
This paper +PT 10 10
PRO +PT+AL+LO 13 150
This paper +PT+AL+LO 5 15
Table 8: Epochs to convergence (?epochs?) and
approximate runtime per epoch in minutes (?min.?)
for selected Zh-En experiments tuned on MT06.
All runs executed on the same dedicated system
with the same number of threads. (*) Moses and
kb-MIRA are written in C++, while all other rows
refer to Java implementations in Phrasal.
the Phrasal and Moses PRO implementations use
L2 regularization, which regularizes every weight
on every update. kb-MIRA makes multiple passes
through the n-best lists during each epoch. The
Moses implementation parallelizes decoding but
weight updating is sequential.
The core of our method is an inner product be-
tween the adaptive learning rate vector and the gra-
dient. This is easy to implement and is very fast
even for large feature sets. Since we applied lazy
regularization, this inner product usually involves
hundred-dimensional vectors. Finally, our method
does not need to accumulate n-best lists, a practice
that slows down the other algorithms.
6 Related Work
Our work relates most closely to that of Hasler et al
(2012b), who tuned models containing both sparse
and dense features with Moses. A discriminative
phrase table helped them improve slightly over a
dense, online MIRA baseline, but their best results
required initialization with MERT-tuned weights
and re-tuning a single, shared weight for the dis-
criminative phrase table with MERT. In contrast,
our algorithm learned good high dimensional mod-
els from a uniform starting point.
Chiang (2012) adapted AROW to MT and ex-
tended previous work on online MIRA (Chiang et
al., 2008; Watanabe et al, 2007). It was not clear if
his improvements came from the novel Hope/Fear
search, the conservativity gain from MIRA/AROW
by solving the QP exactly, adaptivity, or sophis-
ticated parallelization. In contrast, we show that
AdaGrad, which ignores conservativity and only
capturing adaptivity, is sufficient.
Simianer et al (2012) investigated SGD with a
pairwise perceptron objective. Their best algorithm
used iterative parameter mixing (McDonald et al,
2010), which we found to be slower than the stale
gradient method in section 3.3. They regularized
once at the end of each epoch, whereas we regular-
ized each weight update. An empirical comparison
of these two strategies would be an interesting fu-
ture contribution.
Watanabe (2012) investigated SGD and even ran-
domly selected pairwise samples as we did. He
considered both softmax and hinge losses, observ-
ing better results with the latter, which solves a QP.
Their parallelization strategy required a line search
at the end of each epoch.
Many other discriminative techniques have been
proposed based on: ramp loss (Gimpel, 2012);
hinge loss (Cherry and Foster, 2012; Haddow et
al., 2011; Arun and Koehn, 2007); maximum en-
tropy (Xiang and Ittycheriah, 2011; Ittycheriah and
Roukos, 2007; Och and Ney, 2002); perceptron
(Liang et al, 2006a); and structured SVM (Till-
mann and Zhang, 2006). These works use radically
different experimental setups, and to our knowl-
edge only (Cherry and Foster, 2012) and this work
compare to at least two high dimensional baselines.
Broader comparisons, though time-intensive, could
help differentiate these methods.
7 Conclusion and Outlook
We introduced a new online method for tuning
feature-rich translation models. The method is
faster per epoch than MERT, scales to millions of
features, and converges quickly. We used efficient
L1 regularization for feature selection, obviating
the need for the feature scaling and heuristic filter-
ing common in prior work. Those comfortable with
implementing vanilla SGD should find our method
easy to implement. Even basic discriminative fea-
tures were effective, so we believe that our work
enables fresh approaches to more sophisticated MT
feature engineering.
Acknowledgments We thank John DeNero for helpful com-
ments on an earlier draft. The first author is supported by a
National Science Foundation Graduate Research Fellowship.
We also acknowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily
reflect the view of the DARPA or the US government.
319
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In MT Summit XI.
L. Bottou and O. Bousquet. 2011. The tradeoffs of
large scale learning. In Optimization for Machine
Learning, pages 351?368. MIT Press.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regu-
larization and search for minimum error rate training.
In WMT.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
P-C. Chang, M. Galley, and C. D. Manning. 2008.
Optimizing Chinese word segmentation for machine
translation performance. In WMT.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
HLT-NAACL.
D. Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In HLT-
NAACL.
K. Gimpel, D. Das, and N. A. Smith. 2010. Distributed
asynchronous online learning for natural language
processing. In CoNLL.
K. Gimpel. 2012. Discriminative Feature-Rich Mod-
eling for Syntax-Based Machine Translation. Ph.D.
thesis, Language Technologies Institute, Carnegie
Mellon University.
S. Green and J. DeNero. 2012. A class-based agree-
ment model for generating accurately inflected trans-
lations. In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect
of out-of-domain data on SMT systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleR-
ank training for phrase-basedmachine translation. In
WMT.
E. Hasler, P. Bell, A. Ghoshal, B. Haddow, P. Koehn,
F. McInnes, et al 2012a. The UEDIN systems for
the IWSLT 2012 evaluation. In IWSLT.
E. Hasler, B. Haddow, and P. Koehn. 2012b. Sparse
lexicalised features and topic adaptation for SMT. In
IWSLT.
X. He and L. Deng. 2012. Maximum expected BLEU
training of phrase and lexicon translation models. In
ACL.
R. Herbrich, T. Graepel, and K. Obermayer. 1999.
Support vector learning for ordinal regression. In
ICANN.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In HLT-NAACL.
D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
J. Langford, A. J. Smola, and M. Zinkevich. 2009.
Slow learners are fast. In NIPS.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In HLT-NAACL.
P. Liang, A. Bouchard-C?t?, D. Klein, and B. Taskar.
2006a. An end-to-end discriminative approach to
machine translation. In ACL.
P. Liang, B. Taskar, and D. Klein. 2006b. Alignment
by agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-
ing the Arabic Treebank: A collaborative effort to-
ward new annotation guidelines. In LREC.
320
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
R.McDonald, K. Hall, andG.Mann. 2010. Distributed
training strategies for the structured perceptron. In
NAACL-HLT.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization (MTSE).
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-
scale discriminative training in SMT. In ACL.
A Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In ICSLP.
C. Tillmann and T. Zhang. 2006. A discriminative
global training algorithm for statistical MT. In ACL-
COLING.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In EMNLP-CoNLL.
T. Watanabe. 2012. Optimized online rank learning
for machine translation. In HLT-NAACL. Associa-
tion for Computational Linguistics.
B. Xiang and A. Ittycheriah. 2011. Discriminative
feature-tied mixture modeling for statistical machine
translation. In ACL-HLT.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207?238.
321
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385?393,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Daniel Cer
Stanford University
Stanford, CA 94305, USA
danielcer@stanford.edu
Mona Diab
Center for Computational Learning Systems
Columbia University
mdiab@ccls.columbia.edu
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
Abstract
Semantic Textual Similarity (STS) measures
the degree of semantic equivalence between
two texts. This paper presents the results of
the STS pilot task in Semeval. The training
data contained 2000 sentence pairs from pre-
viously existing paraphrase datasets and ma-
chine translation evaluation resources. The
test data also comprised 2000 sentences pairs
for those datasets, plus two surprise datasets
with 400 pairs from a different machine trans-
lation evaluation corpus and 750 pairs from a
lexical resource mapping exercise. The sim-
ilarity of pairs of sentences was rated on a
0-5 scale (low to high similarity) by human
judges using Amazon Mechanical Turk, with
high Pearson correlation scores, around 90%.
35 teams participated in the task, submitting
88 runs. The best results scored a Pearson
correlation>80%, well above a simple lexical
baseline that only scored a 31% correlation.
This pilot task opens an exciting way ahead,
although there are still open issues, specially
the evaluation metric.
1 Introduction
Semantic Textual Similarity (STS) measures the
degree of semantic equivalence between two sen-
tences. STS is related to both Textual Entailment
(TE) and Paraphrase (PARA). STS is more directly
applicable in a number of NLP tasks than TE and
PARA such as Machine Translation and evaluation,
Summarization, Machine Reading, Deep Question
Answering, etc. STS differs from TE in as much as
it assumes symmetric graded equivalence between
the pair of textual snippets. In the case of TE the
equivalence is directional, e.g. a car is a vehicle, but
a vehicle is not necessarily a car. Additionally, STS
differs from both TE and PARA in that, rather than
being a binary yes/no decision (e.g. a vehicle is not a
car), STS incorporates the notion of graded semantic
similarity (e.g. a vehicle and a car are more similar
than a wave and a car).
STS provides a unified framework that allows for
an extrinsic evaluation of multiple semantic compo-
nents that otherwise have tended to be evaluated in-
dependently and without broad characterization of
their impact on NLP applications. Such components
include word sense disambiguation and induction,
lexical substitution, semantic role labeling, multi-
word expression detection and handling, anaphora
and coreference resolution, time and date resolution,
named-entity handling, underspecification, hedging,
semantic scoping and discourse analysis. Though
not in the scope of the current pilot task, we plan to
explore building an open source toolkit for integrat-
ing and applying diverse linguistic analysis modules
to the STS task.
While the characterization of STS is still prelim-
inary, we observed that there was no comparable
existing dataset extensively annotated for pairwise
semantic sentence similarity. We approached the
construction of the first STS dataset with the fol-
lowing goals: (1) To set a definition of STS as a
graded notion which can be easily communicated to
non-expert annotators beyond the likert-scale; (2) To
gather a substantial amount of sentence pairs from
diverse datasets, and to annotate them with high
quality; (3) To explore evaluation measures for STS;
(4) To explore the relation of STS to PARA and Ma-
chine Translation Evaluation exercises.
385
In the next section we present the various sources
of the STS data and the annotation procedure used.
Section 4 investigates the evaluation of STS sys-
tems. Section 5 summarizes the resources and tools
used by participant systems. Finally, Section 6
draws the conclusions.
2 Source Datasets
Datasets for STS are scarce. Existing datasets in-
clude (Li et al, 2006) and (Lee et al, 2005). The
first dataset includes 65 sentence pairs which cor-
respond to the dictionary definitions for the 65
word pairs in Similarity(Rubenstein and Goode-
nough, 1965). The authors asked human informants
to assess the meaning of the sentence pairs on a
scale from 0.0 (minimum similarity) to 4.0 (maxi-
mum similarity). While the dataset is very relevant
to STS, it is too small to train, develop and test typ-
ical machine learning based systems. The second
dataset comprises 50 documents on news, ranging
from 51 to 126 words. Subjects were asked to judge
the similarity of document pairs on a five-point scale
(with 1.0 indicating ?highly unrelated? and 5.0 indi-
cating ?highly related?). This second dataset com-
prises a larger number of document pairs, but it goes
beyond sentence similarity into textual similarity.
When constructing our datasets, gathering natu-
rally occurring pairs of sentences with different de-
grees of semantic equivalence was a challenge in it-
self. If we took pairs of sentences at random, the
vast majority of them would be totally unrelated, and
only a very small fragment would show some sort of
semantic equivalence. Accordingly, we investigated
reusing a collection of existing datasets from tasks
that are related to STS.
We first studied the pairs of text from the Recog-
nizing TE challenge. The first editions of the chal-
lenge included pairs of sentences as the following:
T: The Christian Science Monitor named a US
journalist kidnapped in Iraq as freelancer Jill
Carroll.
H: Jill Carroll was abducted in Iraq.
The first sentence is the text, and the second is
the hypothesis. The organizers of the challenge an-
notated several pairs with a binary tag, indicating
whether the hypothesis could be entailed from the
text. Although these pairs of text are interesting we
decided to discard them from this pilot because the
length of the hypothesis was typically much shorter
than the text, and we did not want to bias the STS
task in this respect. We may, however, explore using
TE pairs for STS in the future.
Microsoft Research (MSR) has pioneered the ac-
quisition of paraphrases with two manually anno-
tated datasets. The first, called MSR Paraphrase
(MSRpar for short) has been widely used to evaluate
text similarity algorithms. It contains 5801 pairs of
sentences gleaned over a period of 18 months from
thousands of news sources on the web (Dolan et
al., 2004). 67% of the pairs were tagged as para-
phrases. The inter annotator agreement is between
82% and 84%. Complete meaning equivalence is
not required, and the annotation guidelines allowed
for some relaxation. The pairs which were anno-
tated as not being paraphrases ranged from com-
pletely unrelated semantically, to partially overlap-
ping, to those that were almost-but-not-quite seman-
tically equivalent. In this sense our graded annota-
tions enrich the dataset with more nuanced tags, as
we will see in the following section. We followed
the original split of 70% for training and 30% for
testing. A sample pair from the dataset follows:
The Senate Select Committee on Intelligence
is preparing a blistering report on prewar
intelligence on Iraq.
American intelligence leading up to the
war on Iraq will be criticized by a powerful
US Congressional committee due to report
soon, officials said today.
In order to construct a dataset which would reflect
a uniform distribution of similarity ranges, we sam-
pled the MSRpar dataset at certain ranks of string
similarity. We used the implementation readily ac-
cessible at CPAN1 of a well-known metric (Ukko-
nen, 1985). We sampled equal numbers of pairs
from five bands of similarity in the [0.4 .. 0.8] range
separately from the paraphrase and non-paraphrase
pairs. We sampled 1500 pairs overall, which we split
50% for training and 50% for testing.
The second dataset from MSR is the MSR Video
Paraphrase Corpus (MSRvid for short). The authors
showed brief video segments to Annotators from
Amazon Mechanical Turk (AMT) and were asked
1http://search.cpan.org/?mlehmann/
String-Similarity-1.04/Similarity.pm
386
Figure 1: Video and corresponding descriptions from
MSRvid
Figure 2: Definition and instructions for annotation
to provide a one-sentence description of the main ac-
tion or event in the video (Chen and Dolan, 2011).
Nearly 120 thousand sentences were collected for
2000 videos. The sentences can be taken to be
roughly parallel descriptions, and they included sen-
tences for many languages. Figure 1 shows a video
and corresponding descriptions.
The sampling procedure from this dataset is sim-
ilar to that for MSRpar. We construct two bags of
data to draw samples. The first includes all possible
pairs for the same video, and the second includes
pairs taken from different videos. Note that not all
sentences from the same video were equivalent, as
some descriptions were contradictory or unrelated.
Conversely, not all sentences coming from different
videos were necessarily unrelated, as many videos
were on similar topics. We took an equal number of
samples from each of these two sets, in an attempt to
provide a balanced dataset between equivalent and
non-equivalent pairs. The sampling was also done
according to string similarity, but in four bands in the
[0.5 .. 0.8] range, as sentences from the same video
had a usually higher string similarity than those in
the MSRpar dataset. We sampled 1500 pairs overall,
which we split 50% for training and 50% for testing.
Given the strong connection between STS sys-
tems and Machine Translation evaluation metrics,
we also sampled pairs of segments that had been
part of human evaluation exercises. Those pairs in-
cluded a reference translation and a automatic Ma-
chine Translation system submission, as follows:
The only instance in which no tax is levied is
when the supplier is in a non-EU country and
the recipient is in a Member State of the EU.
The only case for which no tax is still
perceived ?is an example of supply in the
European Community from a third country.
We selected pairs from the translation shared task
of the 2007 and 2008 ACL Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al,
2007; Callison-Burch et al, 2008). For consistency,
we only used French to English system submissions.
The training data includes all of the Europarl human
ranked fr-en system submissions from WMT 2007,
with each machine translation being paired with the
correct reference translation. This resulted in 729
unique training pairs. The test data is comprised of
all Europarl human evaluated fr-en pairs from WMT
2008 that contain 16 white space delimited tokens or
less.
In addition, we selected two other datasets that
were used as out-of-domain testing. One of them
comprised of all the human ranked fr-en system
submissions from the WMT 2007 news conversa-
tion test set, resulting in 351 unique system refer-
ence pairs.2 The second set is radically different as
it comprised 750 pairs of glosses from OntoNotes
4.0 (Hovy et al, 2006) and WordNet 3.1 (Fellbaum,
1998) senses. The mapping of the senses of both re-
sources comprised 110K sense pairs. The similarity
between the sense pairs was generated using simple
word overlap. 50% of the pairs were sampled from
senses which were deemed as equivalent senses, the
rest from senses which did not map to one another.
3 Annotation
In this first dataset we defined a straightforward lik-
ert scale ranging from 5 to 0, but we decided to pro-
vide definitions for each value in the scale (cf. Fig-
ure 2). We first did pilot annotations of 200 pairs se-
2At the time of the shared task, this data set contained dupli-
cates resulting in 399 sentence pairs.
387
lected at random from the three main datasets in the
training set. We did the annotation, and the pairwise
Pearson ranged from 84% to 87% among ourselves.
The agreement of each annotator with the average
scores of the other was between 87% and 89%.
In the future, we would like to explore whether
the definitions improve the consistency of the tag-
ging with respect to a likert scale without defini-
tions. Note also that in the assessment of the qual-
ity and evaluation of the systems performances, we
just took the resulting SS scores and their averages.
Using the qualitative descriptions for each score in
analysis and evaluation is left for future work.
Given the good results of the pilot we decided to
deploy the task in Amazon Mechanical Turk (AMT)
in order to crowd source the annotation task. The
turkers were required to have achieved a 95% of ap-
proval rating in their previous HITs, and had to pass
a qualification task which included 6 example pairs.
Each HIT included 5 pairs of sentences, and was
paid at 0.20$ each. We collected 5 annotations per
HIT. In the latest data collection, each HIT required
114.9 second for completion.
In order to ensure the quality, we also performed
post-hoc validation. Each HIT contained one pair
from our pilot. After the tagging was completed
we checked the correlation of each individual turker
with our scores, and removed annotations of turkers
which had low correlations (below 50%). Given the
high quality of the annotations among the turkers,
we could alternatively use the correlation between
the turkers itself to detect poor quality annotators.
4 Systems Evaluation
Given two sentences, s1 and s2, an STS system
would need to return a similarity score. Participants
can also provide a confidence score indicating their
confidence level for the result returned for each pair,
but this confidence is not used for the main results.
The output of the systems performance is evaluated
using the Pearson product-moment correlation co-
efficient between the system scores and the human
scores, as customary in text similarity (Rubenstein
and Goodenough, 1965). We calculated Pearson for
each evaluation dataset separately.
In order to have a single Pearson measure for each
system we concatenated the gold standard (and sys-
tem outputs) for all 5 datasets into a single gold stan-
dard file (and single system output). The first ver-
sion of the results were published using this method,
but the overall score did not correspond well to the
individual scores in the datasets, and participants
proposed two additional evaluation metrics, both of
them based on Pearson correlation. The organizers
of the task decided that it was more informative, and
on the benefit of the community, to also adopt those
evaluation metrics, and the idea of having a single
main evaluation metric was dropped. This decision
was not without controversy, but the organizers gave
more priority to openness and inclusiveness and to
the involvement of participants. The final result ta-
ble thus included three evaluation metrics. For the
future we plan to analyze the evaluation metrics, in-
cluding non-parametric metrics like Spearman.
4.1 Evaluation metrics
The first evaluation metric is the Pearson correla-
tion for the concatenation of all five datasets, as de-
scribed above. We will use overall Pearson or sim-
ply ALL to refer to this measure.
The second evaluation metric normalizes the out-
put for each dataset separately, using the linear least
squares method. We concatenated the system results
for five datasets and then computed a single Pear-
son correlation. Given Y = {yi} and X = {xi}
(the gold standard scores and the system scores,
respectively), we transform the system scores into
X ? = {x?i} in order to minimize the squared error?
i (yi ? x
?
i)
2. The linear transformation is given by
x?i = xi ? ?1 + ?2, where ?1 and ?2 are found an-
alytically. We refer to this measure as Normalized
Pearson or simply ALLnorm. This metric was sug-
gested by one of the participants, Sergio Jimenez.
The third evaluation metric is the weighted mean
of the Pearson correlations on individual datasets.
The Pearson returned for each dataset is weighted
according to the number of sentence pairs in that
dataset. Given ri the five Pearson scores for
each dataset, and ni the number of pairs in each
dataset, the weighted mean is given as
?
i=1..5(ri ?
ni)/
?
i=1..5 ni We refer to this measure as weighted
mean of Pearson or Mean for short.
4.2 Using confidence scores
Participants were allowed to include a confidence
score between 1 and 100 for each of their scores.
We used weighted Pearson to use those confidence
388
scores3. Table 2 includes the list of systems which
provided a non-uniform confidence. The results
show that some systems were able to improve their
correlation, showing promise for the usefulness of
confidence in applications.
4.3 The Baseline System
We produced scores using a simple word overlap
baseline system. We tokenized the input sentences
splitting at white spaces, and then represented each
sentence as a vector in the multidimensional to-
ken space. Each dimension had 1 if the token was
present in the sentence, 0 otherwise. Similarity of
vectors was computed using cosine similarity.
We also run a random baseline several times,
yielding close to 0 correlations in all datasets, as ex-
pected. We will refer to the random baseline again
in Section 4.5.
4.4 Participation
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 35
teams participated, submitting 88 system runs (cf.
first column of Table 1). Due to lack of space we
can?t detail the full names of authors and institutions
that participated. The interested reader can use the
name of the runs to find the relevant paper in these
proceedings.
There were several issues in the submissions. The
submission software did not ensure that the nam-
ing conventions were appropriately used, and this
caused some submissions to be missed, and in two
cases the results were wrongly assigned. Some par-
ticipants returned Not-a-Number as a score, and the
organizers had to request whether those where to be
taken as a 0 or as a 5.
Finally, one team submitted past the 120 hour
deadline and some teams sent missing files after the
deadline. All those are explicitly marked in Table 1.
The teams that included one of the organizers are
also explicitly marked. We want to stress that in
these teams the organizers did not allow the devel-
opers of the system to access any data or informa-
tion which was not available for the rest of partic-
ipants. One exception is weiwei, as they generated
3http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
the 110K OntoNotes-WordNet dataset from which
the other organizers sampled the surprise data set.
After the submission deadline expired, the orga-
nizers published the gold standard in the task web-
site, in order to ensure a transparent evaluation pro-
cess.
4.5 Results
Table 1 shows the results for each run in alphabetic
order. Each result is followed by the rank of the sys-
tem according to the given evaluation measure. To
the right, the Pearson score for each dataset is given.
In boldface, the three best results in each column.
First of all we want to stress that the large majority
of the systems are well above the simple baseline,
although the baseline would rank 70 on the Mean
measure, improving over 19 runs.
The correlation for the non-MT datasets were re-
ally high: the highest correlation was obtained was
for MSRvid (0.88 r), followed by MSRpar (0.73 r)
and On-WN (0.73 r). The results for the MT evalu-
ation data are lower, (0.57 r) for SMT-eur and (0.61
r) for SMT-News. The simple token overlap base-
line, on the contrary, obtained the highest results
for On-WN (0.59 r), with (0.43 r) on MSRpar and
(0.40 r) on MSRvid. The results for MT evaluation
data are also reversed, with (0.40 r) for SMT-eur and
(0.45 r) for SMT-News.
The ALLnorm measure yields the highest corre-
lations. This comes at no surprise, as it involves a
normalization which transforms the system outputs
using the gold standard. In fact, a random base-
line which gets Pearson correlations close to 0 in all
datasets would attain Pearson of 0.58914.
Although not included in the results table for lack
of space, we also performed an analysis of confi-
dence intervals. For instance, the best run according
to ALL (r = .8239) has a 95% confidence interval of
[.8123,.8349] and the second a confidence interval
of [.8016,.8254], meaning that the differences are
not statistically different.
5 Tools and resources used
The organizers asked participants to submit a de-
scription file, special emphasis on the tools and re-
sources that they used. Table 3 shows in a simpli-
4We run the random baseline 10 times. The mean is reported
here. The standard deviation is 0.0005
389
Run ALL Rank ALLnrm Rank Mean Rank MSRpar MSRvid SMT-eur On-WN SMT-news
00-baseline/task6-baseline .3110 87 .6732 85 .4356 70 .4334 .2996 .4542 .5864 .3908
aca08ls/task6-University Of Sheffield-Hybrid .6485 34 .8238 15 .6100 18 .5166 .8187 .4859 .6676 .4280
aca08ls/task6-University Of Sheffield-Machine Learning .7241 17 .8169 18 .5750 38 .5166 .8187 .4859 .6390 .2089
aca08ls/task6-University Of Sheffield-Vector Space .6054 48 .7946 44 .5943 27 .5460 .7241 .4858 .6676 .4280
acaputo/task6-UNIBA-DEPRI .6141 46 .8027 38 .5891 31 .4542 .7673 .5126 .6593 .4636
acaputo/task6-UNIBA-LSARI .6221 44 .8079 30 .5728 40 .3886 .7908 .4679 .6826 .4238
acaputo/task6-UNIBA-RI .6285 41 .7951 43 .5651 45 .4128 .7612 .4531 .6306 .4887
baer/task6-UKP-run1 .8117 4 .8559 4 .6708 4 .6821 .8708 .5118 .6649 .4672
baer/task6-UKP-run2 plus postprocessing smt twsi .8239 1 .8579 2 .6773 1 .6830 .8739 .5280 .6641 .4937
baer/task6-UKP-run3 plus random .7790 8 .8166 19 .4320 71 .6830 .8739 .5280 -.0620 -.0520
croce/task6-UNITOR-1 REGRESSION BEST FEATURES .7474 13 .8292 12 .6316 10 .5695 .8217 .5168 .6591 .4713
croce/task6-UNITOR-2 REGRESSION ALL FEATURES .7475 12 .8297 11 .6323 9 .5763 .8217 .5102 .6591 .4713
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS .6289 40 .8150 21 .5939 28 .4686 .8027 .4574 .6591 .4713
csjxu/task6-PolyUCOMP-RUN1 .6528 31 .7642 59 .5492 51 .4728 .6593 .4835 .6196 .4290
danielcer/stanford fsa? .6354 38 .7212 70 .4848 66 .3795 .5350 .4377 .6052 .4164
danielcer/stanford pdaAll? .4229 77 .7160 72 .5044 62 .4409 .4698 .4558 .6468 .4769
danielcer/stanford rte? .5589 55 .7807 55 .4674 67 .4374 .8037 .3533 .3077 .3235
davide buscaldi/task6-IRIT-pg1 .4280 76 .7379 65 .5009 63 .4295 .6125 .4952 .5387 .3614
davide buscaldi/task6-IRIT-pg3 .4813 68 .7569 61 .5202 58 .4171 .6728 .5179 .5526 .3693
davide buscaldi/task6-IRIT-wu .4064 81 .7287 69 .4898 65 .4326 .5833 .4856 .5317 .3480
demetrios glinos/task6-ATA-BASE .3454 83 .6990 81 .2772 87 .1684 .6256 .2244 .1648 .0988
demetrios glinos/task6-ATA-CHNK .4976 64 .7160 73 .3215 86 .2312 .6595 .1504 .2735 .1426
demetrios glinos/task6-ATA-STAT .4165 79 .7129 75 .3312 85 .1887 .6482 .2769 .2950 .1336
desouza/task6-FBK-run1 .5633 54 .7127 76 .3628 82 .2494 .6117 .1495 .4212 .2439
desouza/task6-FBK-run2 .6438 35 .8080 29 .5888 32 .5128 .7807 .3796 .6228 .5474
desouza/task6-FBK-run3 .6517 32 .8106 25 .6077 20 .5169 .7773 .4419 .6298 .6085
dvilarinoayala/task6-BUAP-RUN-1 .4997 63 .7568 62 .5260 56 .4037 .6532 .4521 .6050 .4537
dvilarinoayala/task6-BUAP-RUN-2 -.0260 89 .5933 89 .1016 89 .1109 .0057 .0348 .1788 .1964
dvilarinoayala/task6-BUAP-RUN-3 .6630 25 .7474 64 .5105 59 .4018 .6378 .4758 .5691 .4057
enrique/task6-UNED-H34measures .4381 75 .7518 63 .5577 48 .5328 .5788 .4785 .6692 .4465
enrique/task6-UNED-HallMeasures .2791 88 .6694 87 .4286 72 .3861 .2570 .4086 .6006 .5305
enrique/task6-UNED-SP INIST .4680 69 .7625 60 .5615 47 .5166 .6303 .4625 .6442 .4753
georgiana dinu/task6-SAARLAND-ALIGN VSSIM .4952 65 .7871 50 .5065 60 .4043 .7718 .2686 .5721 .3505
georgiana dinu/task6-SAARLAND-MIXT VSSIM .4548 71 .8258 13 .5662 43 .6310 .8312 .1391 .5966 .3806
jan snajder/task6-takelab-simple .8133 3 .8635 1 .6753 2 .7343 .8803 .4771 .6797 .3989
jan snajder/task6-takelab-syntax .8138 2 .8569 3 .6601 5 .6985 .8620 .3612 .7049 .4683
janardhan/task6-janardhan-UNL matching .3431 84 .6878 84 .3481 83 .1936 .5504 .3755 .2888 .3387
jhasneha/task6-Penn-ELReg .6622 27 .8048 34 .5654 44 .5480 .7844 .3513 .6040 .3607
jhasneha/task6-Penn-ERReg .6573 28 .8083 28 .5755 37 .5610 .7857 .3568 .6214 .3732
jhasneha/task6-Penn-LReg .6497 33 .8043 36 .5699 41 .5460 .7818 .3547 .5969 .4137
jotacastillo/task6-SAGAN-RUN1 .5522 57 .7904 47 .5906 29 .5659 .7113 .4739 .6542 .4253
jotacastillo/task6-SAGAN-RUN2 .6272 42 .8032 37 .5838 34 .5538 .7706 .4480 .6135 .3894
jotacastillo/task6-SAGAN-RUN3 .6311 39 .7943 45 .5649 46 .5394 .7560 .4181 .5904 .3746
Konstantin Z/task6-ABBYY-General .5636 53 .8052 33 .5759 36 .4797 .7821 .4576 .6488 .3682
M Rios/task6-UOW-LEX PARA .6397 36 .7187 71 .3825 80 .3628 .6426 .3074 .2806 .2082
M Rios/task6-UOW-LEX PARA SEM .5981 49 .6955 82 .3473 84 .3529 .5724 .3066 .2643 .1164
M Rios/task6-UOW-SEM .5361 59 .6287 88 .2567 88 .2995 .2910 .1611 .2571 .2212
mheilman/task6-ETS-PERP .7808 7 .8064 32 .6305 11 .6211 .7210 .4722 .7080 .5149
mheilman/task6-ETS-PERPphrases .7834 6 .8089 27 .6399 7 .6397 .7200 .4850 .7124 .5312
mheilman/task6-ETS-TERp .4477 73 .7291 68 .5253 57 .5049 .5217 .4748 .6169 .4566
nitish aggarwal/task6-aggarwal-run1? .5777 52 .8158 20 .5466 52 .3675 .8427 .3534 .6030 .4430
nitish aggarwal/task6-aggarwal-run2? .5833 51 .8183 17 .5683 42 .3720 .8330 .4238 .6513 .4499
nitish aggarwal/task6-aggarwal-run3 .4911 67 .7696 57 .5377 53 .5320 .6874 .4514 .5827 .2818
nmalandrakis/task6-DeepPurple-DeepPurple hierarchical .6228 43 .8100 26 .5979 23 .5984 .7717 .4292 .6480 .3702
nmalandrakis/task6-DeepPurple-DeepPurple sigmoid .5540 56 .7997 41 .5558 50 .5960 .7616 .2628 .6016 .3446
nmalandrakis/task6-DeepPurple-DeepPurple single .4918 66 .7646 58 .5061 61 .4989 .7092 .4437 .4879 .2441
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach? .3880 82 .6706 86 .4111 76 .3427 .3549 .4271 .5298 .4034
rada/task6-UNT-CombinedRegression .7418 14 .8406 7 .6159 14 .5032 .8695 .4797 .6715 .4033
rada/task6-UNT-IndividualDecTree .7677 9 .8389 9 .5947 25 .5693 .8688 .4203 .6491 .2256
rada/task6-UNT-IndividualRegression .7846 5 .8440 6 .6162 13 .5353 .8750 .4203 .6715 .4033
sbdlrhmn/task6-sbdlrhmn-Run1 .6663 23 .7842 53 .5376 54 .5440 .7335 .3830 .5860 .2445
sbdlrhmn/task6-sbdlrhmn-Run2 .4169 78 .7104 77 .4986 64 .4617 .4489 .4719 .6353 .4353
sgjimenezv/task6-SOFT-CARDINALITY .7331 15 .8526 5 .6708 3 .6405 .8562 .5152 .7109 .4833
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION .7107 19 .8397 8 .6486 6 .6316 .8237 .4320 .7109 .4833
siva/task6-DSS-alignheuristic .5253 60 .7962 42 .6030 21 .5735 .7123 .4781 .6984 .4177
siva/task6-DSS-average .5490 58 .8047 35 .5943 26 .5020 .7645 .4875 .6677 .4324
siva/task6-DSS-wordsim .5130 61 .7895 49 .5287 55 .3765 .7761 .4161 .5728 .3964
skamler /task6-EHU-RUN1v2?? .3129 86 .6935 83 .3889 79 .3605 .5187 .2259 .4098 .3465
sokolov/task6-LIMSI-cosprod .6392 37 .7344 67 .3940 78 .3948 .6597 .0143 .4157 .2889
sokolov/task6-LIMSI-gradtree .6789 22 .7377 66 .4118 75 .4848 .6636 .0934 .3706 .2455
sokolov/task6-LIMSI-sumdiff .6196 45 .7101 78 .4131 74 .4295 .5724 .2842 .3989 .2575
spirin2/task6-UIUC-MLNLP-Blend .4592 70 .7800 56 .5782 35 .6523 .6691 .3566 .6117 .4603
spirin2/task6-UIUC-MLNLP-CCM .7269 16 .8217 16 .6104 17 .5769 .8203 .4667 .5835 .4945
spirin2/task6-UIUC-MLNLP-Puzzle .3216 85 .7857 51 .4376 69 .5635 .8056 .0630 .2774 .2409
sranjans/task6-sranjans-1 .6529 30 .8018 39 .6249 12 .6124 .7240 .5581 .6703 .4533
sranjans/task6-sranjans-2 .6651 24 .8128 22 .6366 8 .6254 .7538 .5328 .6649 .5036
sranjans/task6-sranjans-3 .5045 62 .7846 52 .5905 30 .6167 .7061 .5666 .5664 .3968
tiantianzhu7/task6-tiantianzhu7-1 .4533 72 .7134 74 .4192 73 .4184 .5630 .2083 .4822 .2745
tiantianzhu7/task6-tiantianzhu7-2 .4157 80 .7099 79 .3960 77 .4260 .5628 .1546 .4552 .1923
tiantianzhu7/task6-tiantianzhu7-3 .4446 74 .7097 80 .3740 81 .3411 .5946 .1868 .4029 .1823
weiwei/task6-weiwei-run1?? .6946 20 .8303 10 .6081 19 .4106 .8351 .5128 .7273 .4383
yeh/task6-SRIUBC-SYSTEM1? .7513 11 .8017 40 .5997 22 .6084 .7458 .4688 .6315 .3994
yeh/task6-SRIUBC-SYSTEM2? .7562 10 .8111 24 .5858 33 .6050 .7939 .4294 .5871 .3366
yeh/task6-SRIUBC-SYSTEM3? .6876 21 .7812 54 .4668 68 .4791 .7901 .2159 .3843 .2801
ygutierrez/task6-UMCC DLSI-MultiLex .6630 26 .7922 46 .5560 49 .6022 .7709 .4435 .4327 .4264
ygutierrez/task6-UMCC DLSI-MultiSem .6529 29 .8115 23 .6116 16 .5269 .7756 .4688 .6539 .5470
ygutierrez/task6-UMCC DLSI-MultiSemLex .7213 18 .8239 14 .6158 15 .6205 .8104 .4325 .6256 .4340
yrkakde/task6-yrkakde-DiceWordnet .5977 50 .7902 48 .5742 39 .5294 .7470 .5531 .5698 .3659
yrkakde/task6-yrkakde-JaccNERPenalty .6067 47 .8078 31 .5955 24 .5757 .7765 .4989 .6257 .3468
Table 1: The first row corresponds to the baseline. ALL for overall Pearson, ALLnorm for Pearson after normaliza-
tion, and Mean for mean of Pearsons. We also show the ranks for each measure. Rightmost columns show Pearson for
each individual dataset. Note: ? system submitted past the 120 hour window, ? post-deadline fixes, ? team involving
one of the organizers.
390
Run ALL ALLw MSRpar MSRparw MSRvid MSRvidw SMT-eur SMT-eurw On-WN On-WNw SMT-news SMT-newsw
davide buscaldi/task6-IRIT-pg1 .4280 .4946 .4295 .4082 .6125 .6593 .4952 .5273 .5387 .5574 .3614 .4674
davide buscaldi/task6-IRIT-pg3 .4813 .5503 .4171 .4033 .6728 .7048 .5179 .5529 .5526 .5950 .3693 .4648
davide buscaldi/task6-IRIT-wu .4064 .4682 .4326 .4035 .5833 .6253 .4856 .5138 .5317 .5189 .3480 .4482
enrique/task6-UNED-H34measures .4381 .2615 .5328 .4494 .5788 .4913 .4785 .4660 .6692 .6440 .4465 .3632
enrique/task6-UNED-HallMeasures .2791 .2002 .3861 .3802 .2570 .2343 .4086 .4212 .6006 .5947 .5305 .4858
enrique/task6-UNED-SP INIST .4680 .3754 .5166 .5082 .6303 .5588 .4625 .4801 .6442 .5761 .4753 .4143
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach .3880 .3636 .3427 .3498 .3549 .3353 .4271 .3989 .5298 .4619 .4034 .3228
tiantianzhu7/task6-tiantianzhu7-1 .4533 .5442 .4184 .4241 .5630 .5630 .2083 .4220 .4822 .5031 .2745 .3536
tiantianzhu7/task6-tiantianzhu7-2 .4157 .5249 .4260 .4340 .5628 .5758 .1546 .4776 .4552 .4926 .1923 .3362
tiantianzhu7/task6-tiantianzhu7-3 .4446 .5229 .3411 .3611 .5946 .5899 .1868 .4769 .4029 .4365 .1823 .4014
Table 2: Results according to weighted correlation for the systems that provided non-uniform confidence alongside
their scores.
fied way the tools and resources used by those par-
ticipants that did submit a valid description file. In
the last row, the totals show that WordNet was the
most used resource, followed by monolingual cor-
pora and Wikipedia. Acronyms, dictionaries, mul-
tilingual corpora, stopword lists and tables of para-
phrases were also used.
Generic NLP tools like lemmatization and PoS
tagging were widely used, and to a lesser extent,
parsing, word sense disambiguation, semantic role
labeling and time and date resolution (in this or-
der). Knowledge-based and distributional methods
got used nearly equally, and to a lesser extent, align-
ment and/or statistical machine translation software,
lexical substitution, string similarity, textual entail-
ment and machine translation evaluation software.
Machine learning was widely used to combine and
tune components. Several less used tools were also
listed but were used by three or less systems.
The top scoring systems tended to use most of
the resources and tools listed (UKP, Takelab), with
some notable exceptions like Sgjimenez which was
based on string similarity. For a more detailed anal-
ysis, the reader is directed to the papers of the par-
ticipants in this volume.
6 Conclusions and Future Work
This paper presents the SemEval 2012 pilot eval-
uation exercise on Semantic Textual Similarity. A
simple definition of STS beyond the likert-scale was
set up, and a wealth of annotated data was pro-
duced. The similarity of pairs of sentences was
rated on a 0-5 scale (low to high similarity) by hu-
man judges using Amazon Mechanical Turk. The
dataset includes 1500 sentence pairs from MSRpar
and MSRvid (each), ca. 1500 pairs from WMT,
and 750 sentence pairs from a mapping between
OntoNotes and WordNet senses. The correlation be-
tween non-expert annotators and annotations from
the authors is very high, showing the high quality of
the dataset. The dataset was split 50% as train and
test, with the exception of the surprise test datasets:
a subset of WMT from a different domain and the
OntoNotes-WordNet mapping. All datasets are pub-
licly available.5
The exercise was very successful in participation
and results. 35 teams participated, submitting 88
runs. The best results scored a Pearson correlation
over 80%, well beyond a simple lexical baseline
with 31% of correlation. The metric for evaluation
was not completely satisfactory, and three evalua-
tion metrics were finally published. We discuss the
shortcomings of those measures.
There are several tasks ahead in order to make
STS a mature field. The first is to find a satisfac-
tory evaluation metric. The second is to analyze the
definition of the task itself, with a thorough analysis
of the definitions in the likert scale.
We would also like to analyze the relation be-
tween the STS scores and the paraphrase judgements
in MSR, as well as the human evaluations in WMT.
Finally, we would also like to set up an open frame-
work where NLP components and similarity algo-
rithms can be combined by the community. All in
all, we would like this dataset to be the focus of the
community working on algorithmic approaches for
semantic processing and inference at large.
Acknowledgements
We would like to thank all participants, specially (in al-
phabetic order) Yoan Gutierrez, Michael Heilman, Ser-
gio Jimenez, Nitin Madnami, Diana McCarthy and Shru-
tiranjan Satpathy for their contributions on evaluation
metrics. Eneko Agirre was partially funded by the
5http://www.cs.york.ac.uk/semeval-2012/
task6/
391
A
cr
on
ym
s
D
ic
ti
on
ar
ie
s
D
is
tr
ib
ut
io
na
lt
he
sa
ur
us
M
on
ol
in
gu
al
co
rp
or
a
M
ul
ti
li
ng
ua
lc
or
po
ra
S
to
p
w
or
ds
Ta
bl
es
of
pa
ra
ph
ra
se
s
W
ik
ip
ed
ia
W
or
dN
et
A
li
gn
m
en
t
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
em
m
at
iz
er
L
ex
ic
al
S
ub
st
it
ut
io
n
M
ac
hi
ne
L
ea
rn
in
g
M
T
ev
al
ua
ti
on
M
W
E
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
S
em
an
ti
c
R
ol
e
L
ab
el
in
g
S
M
T
S
tr
in
g
si
m
il
ar
it
y
S
yn
ta
x
Te
xt
ua
le
nt
ai
lm
en
t
T
im
e
an
d
da
te
re
so
lu
ti
on
W
or
d
S
en
se
D
is
am
bi
gu
at
io
n
O
th
er
aca08ls/task6-University Of Sheffield-Hybrid x x x x x x x
aca08ls/task6-University Of Sheffield-Machine Learning x x x x x x x
aca08ls/task6-University Of Sheffield-Vector Space x x x x x
baer/task6-UKP-run1 x x x x x x x x x x x x x x
baer/task6-UKP-run2 plus postprocessing smt twsi x x x x x x x x x x x x x x
baer/task6-UKP-run3 plus random x x x x x x x x x x x x x x
croce/task6-UNITOR-1 REGRESSION BEST FEATURES x x x x x x
croce/task6-UNITOR-2 REGRESSION ALL FEATURES x x x x x x
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS x x x x x x
csjxu/task6-PolyUCOMP-RUN x x x x
danielcer/stanford fsa x x x x x x x
danielcer/stanford pdaAll x x x x x x x
danielcer/stanford rte x x x x x x x x
davide buscaldi/task6-IRIT-pg1 x x x x x
davide buscaldi/task6-IRIT-pg3 x x x x x
davide buscaldi/task6-IRIT-wu x x x x x
demetrios glinos/task6-ATA-BASE x x x x x x x
demetrios glinos/task6-ATA-CHNK x x x x x x x
demetrios glinos/task6-ATA-STAT x x x x x x x
desouza/task6-FBK-run1 x x x x x x x x x x x x x
desouza/task6-FBK-run2 x x x x x x x x
desouza/task6-FBK-run3 x x x x x x
dvilarinoayala/task6-BUAP-RUN-1 x x
dvilarinoayala/task6-BUAP-RUN-2 x
dvilarinoayala/task6-BUAP-RUN-3 x x
jan snajder/task6-takelab-simple x x x x x x x x x x x x x
jan snajder/task6-takelab-syntax x x x x x x x x x
janardhan/task6-janardhan-UNL matching x x x x x x
jotacastillo/task6-SAGAN-RUN1 x x x x x x x x
jotacastillo/task6-SAGAN-RUN2 x x x x x x x x
jotacastillo/task6-SAGAN-RUN3 x x x x x x x x
Konstantin Z/task6-ABBYY-General
M Rios/task6-UOW-LEX PARA x x x x x x x x
M Rios/task6-UOW-LEX PARA SEM x x x x x x x x
M Rios/task6-UOW-SEM x x x x x x x
mheilman/task6-ETS-PERP x x x x x x x
mheilman/task6-ETS-PERPphrases x x x x x x x x x
mheilman/task6-ETS-TERp x x x x x x x
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach x x x x x x x x x x
rada/task6-UNT-CombinedRegression x x x x x x x x x
rada/task6-UNT-IndividualDecTree x x x x x x x x x
rada/task6-UNT-IndividualRegression x x x x x x x x x
sgjimenezv/task6-SOFT-CARDINALITY x x x
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION x x x
skamler /task6-EHU-RUN1v2 x x x x x
sokolov/task6-LIMSI-cosprod x x x x
sokolov/task6-LIMSI-gradtree x x x x
sokolov/task6-LIMSI-sumdiff x x x x
spirin2/task6-UIUC-MLNLP-Blend x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-CCM x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-Puzzle x x x x x x x x x x x
sranjans/task6-sranjans-1 x x x x x x x x
sranjans/task6-sranjans-2 x x x x x x x x x x x
sranjans/task6-sranjans-3 x x x x x x x x x x x
tiantianzhu7/task6-tiantianzhu7-1 x x x x
tiantianzhu7/task6-tiantianzhu7-2 x x x
tiantianzhu7/task6-tiantianzhu7-3 x x x x
weiwei/task6-weiwei-run1 x x x x x x
yeh/task6-SRIUBC-SYSTEM1 x x x x x x x
yeh/task6-SRIUBC-SYSTEM2 x x x x x x x
yeh/task6-SRIUBC-SYSTEM3 x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiLex x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSem x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSemLex x x x x x x x x
yrkakde/task6-yrkakde-DiceWordnet x x x
Total 8 6 10 33 5 5 9 20 47 7 31 37 49 13 13 4 7 12 43 9 4 13 17 10 5 15 25
Table 3: Resources and tools used by the systems that submitted a description file. Leftmost columns correspond to
the resources, and rightmost to tools, in alphabetic order.
392
European Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 270082
(PATHS project) and the Ministry of Economy under
grant TIN2009-14715-C04-01 (KNOW2 project). Daniel
Cer gratefully acknowledges the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Language
Translation (BOLT) program through IBM. The STS an-
notations were funded by an extension to DARPA GALE
subcontract to IBM # W0853748 4911021461.0 to Mona
Diab. Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of the
author(s) and do not necessarily reflect the view of the
DARPA, AFRL, or the US government.
References
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 136?158.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, StatMT ?08, pages 70?106.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meetings of the Asso-
ciation for Computational Linguistics (ACL).
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING
04: Proceedings of the 20th international conference
on Computational Linguistics, page 350.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Michael D. Lee, Brandon Pincombe, and Matthew Welsh.
2005. An empirical evaluation of models of text doc-
ument similarity. In Proceedings of the 27th Annual
Conference of the Cognitive Science Society, pages
1254?1259, Mahwah, NJ.
Y. Li, D. McLean, Z. A. Bandar, J. D. O?Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?
1150, August.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
E. Ukkonen. 1985. Algorithms for approximate string
matching. Information and Contro, 64:110?118.
393
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 648?654,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Stanford: Probabilistic Edit Distance Metrics for STS
Mengqiu Wang and Daniel Cer?
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{mengqiu,danielcer}@cs.stanford.edu
Abstract
This paper describes Stanford University?s
submission to SemEval 2012 Semantic Tex-
tual Similarity (STS) shared evaluation task.
Our proposed metric computes probabilistic
edit distance as predictions of semantic sim-
ilarity. We learn weighted edit distance in
a probabilistic finite state machine (pFSM)
model, where state transitions correspond to
edit operations. While standard edit dis-
tance models cannot capture long-distance
word swapping or cross alignments, we rectify
these shortcomings using a novel pushdown
automaton extension of the pFSM model. Our
models are trained in a regression framework,
and can easily incorporate a rich set of lin-
guistic features. The performance of our edit
distance based models is contrasted with an
adaptation of the Stanford textual entailment
system to the STS task. Our results show that
the most advanced edit distance model, pPDA,
outperforms our entailment system on all but
one of the genres included in the STS task.
1 Introduction
We describe a probabilistic edit distance based met-
ric, which was originally designed for evaluating
machine translation quality, for computing seman-
tic textual similarity (STS). This metric models
weighted edit distance in a probabilistic finite state
machine (pFSM), where state transitions correspond
to edit operations. The weights of the edit op-
erations are automatically learned in a regression
framework. One of the major contributions of this
? Daniel Cer is one of the organizers for the STS task. The
STS test set data was not used in any way for the development
or training of the systems described in this paper.
paper is a novel extension of the pFSM model into a
probabilistic Pushdown Automaton (pPDA), which
enhances traditional edit-distance models with the
ability to model phrase shift and word swapping.
Furthermore, we give a new log-linear parameteri-
zation to the pFSM model, which allows it to easily
incorporate rich linguistic features. We contrast the
performance of our probabilistic edit distance metric
with an adaptation of the Stanford textual entailment
system to the STS task.
2 pFSMs for Semantic Textual Similarity
We start off by framing the problem of semantic tex-
tual similarity in terms of weighted edit distance cal-
culated using probabilistic finite state machines (pF-
SMs). A FSM defines a language by accepting a
string of input tokens in the language, and reject-
ing those that are not. A probabilistic FSM defines
the probability that a string is in a language, extend-
ing on the concept of a FSM. Commonly used mod-
els such as HMMs, n-gram models, Markov Chains
and probabilistic finite state transducers all fall in
the broad family of pFSMs (Knight and Al-Onaizan,
1998; Eisner, 2002; Kumar and Byrne, 2003; Vi-
dal et al, 2005). Unlike all the other applications
of FSMs where tokens in the language are words, in
our language tokens are edit operations. A string of
tokens that our FSM accepts is an edit sequence that
transforms one side of the sentence pair (denoted as
s1) into the other side (s2).
Our pFSM has a unique start and stop state, and
one state per edit operation (i.e., Insert, Delete, Sub-
stitution). The probability of an edit sequence e is
generated by the model is the product of the state
transition probabilities in the pFSM, formally de-
648
Figure 1: This diagram illustrates an example sentence pair from the statistical machine translation subtask of STS.
The three rows below are the best state transition (edit) sequences that transforms REF to SYS, according to the basic
pFSM model, the extended pPDA model, and pPDA model with synonym and paraphrase linguistic features. The
corresponding alignments generated by the models (pFSM, pPDA, pPDA+f ) are shown with different styled lines,
with later models in the order generating strictly more alignments than earlier ones. The gold human evaluation score
is 6.5, and model predictions are: pPDA+f 5.5, pPDA 4.3, pFSM 3.1.
scribed as:
w(e | s1,s2) = ?
|e|
i=1 exp ? ? f(ei?1,ei,s1,s2)
Z
(1)
We featurize each of the state changes with a log-
linear parameterization; f is a set of binary feature
functions defined over pairs of neighboring states
(by the Markov assumption) and the input sentences,
and ? are the associated feature weights; Z is a parti-
tion function. In this basic pFSM model, the feature
functions are simply identity functions that emit the
current state, and the state transition sequence of the
previous state and the current state.
The feature weights are then automatically
learned by training a global regression model where
the human judgment score for each sentence pair is
the regression target (y?). Since the ?gold? edit se-
quence are not given at training or prediction time,
we treat the edit sequences as hidden variables and
sum over them in our model. We introduce a new
regression variable y ? R which is the log-sum of
the unnormalized weights (Eqn. (1)) of all edit se-
quences, formally expressed as:
y = log
?
e??e?
|e? |
?
i=1
exp ? ? f(ei?1,ei,s1,s2) (2)
e? is the set of all possible alignments. The sum
over an exponential number of edit sequences in e?
is solved efficiently using a forward-backward style
dynamic program. Any edit sequence that does not
lead to a complete transformation of the sentence
pair has a probability of zero in our model. Our
regression target then seeks to minimize the least
squares error with respect to y?, plus a L2-norm regu-
larizer term parameterized by ? :
?
? = min
?
{
?
s1i ,s2i
[y?i ? (
y
|s1i |+ |s2i |
+?)]2 +????2}
(3)
The |s1i |+ |s2i | is a length normalization term for
the ith training instance, and ? is a scaling con-
stant whose value is to be learned. At test time,
y/(|s1|+ |s2|) + ? is computed as the predicted
score.
We replaced the standard substitution edit oper-
ation with three new operations: Sword for same
word substitution, Slemma for same lemma substitu-
tion, and Spunc for same punctuation substitution. In
other words, all but the three matching-based substi-
tutions are disallowed. The start state can transition
into any of the edit states with a constant unit cost,
and each edit state can transition into any other edit
state if and only if the edit operation involved is valid
at the current edit position (e.g., the model cannot
transition into Delete state if it is already at the end
649
of s1; similarly it cannot transition into Slemma unless
the lemma of the two words under edit in s1 and s2
match). When the end of both sentences are reached,
the model transitions into the stop state and ends
the edit sequence. The first row in Figure 1 start-
ing with pFSM shows a state transition sequence for
an example sentence pair. 1 There exists a one-to-
one correspondence between substitution edits and
word alignments. Therefore this example state tran-
sition sequence correctly generates an alignment for
the word 43 and people.
2.1 pPDA Extension
A shortcoming of edit distance models is that they
cannot handle long-distance word swapping ? a
pervasive phenomenon found in most natural lan-
guages. 2 Edit operations in standard edit distance
models need to obey strict incremental order in
their edit position, in order to admit efficient dy-
namic programming solutions. The same limitation
is shared by our pFSM model, where the Markov
assumption is made based on the incremental or-
der of edit positions. Although there is no known
solution to the general problem of computing edit
distance where long-distance swapping is permit-
ted (Dombb et al, 2010), approximate algorithms do
exist. We present a simple but novel extension of the
pFSM model to a probabilistic pushdown automa-
ton (pPDA), to capture non-nested word swapping
within limited distance, which covers a majority of
word swapping in observed in real data (Wu, 2010).
A pPDA, in its simplest form, is a pFSM where
each control state is equipped with a stack (Esparza
and Kucera, 2005). The addition of stacks for each
transition state endows the machine with memory,
extending its expressiveness beyond that of context-
free formalisms. By construction, at any stage in a
normal edit sequence, the pPDA model can ?jump?
forward within a fixed distance (controlled by a max
distance parameter) to a new edit position on either
side of the sentence pair, and start a new edit subse-
quence from there. Assuming the jump was made on
1It is safe to ignore the second and third row in Figure 1 for
now, their explanations are forthcoming in Section 2.1.
2The edit distance algorithm described in Cormen et
al. (2001) can only handle adjacent word swapping (transpo-
sition), but not long-distance swapping.
the s2 side, 3 the machine remembers its current edit
position in s2 as Jstart , and the destination position
on s2 after the jump as Jlanding.
We constrain our model so that the only edit op-
erations that are allowed immediately following a
?jump? are from the set of substitution operations
(e.g., Sword). And after at least one substitution
has been made, the device can now ?jump? back to
Jstart , remembering the current edit position as Jend .
Another constraint here is that after the backward
?jump?, all edit operations are permitted except for
Delete, which cannot take place until at least one
substitution has been made. When the edit sequence
advances to position Jlanding, the only operation al-
lowed at that point is another ?jump? forward opera-
tion to position Jend , at which point we also clear all
memory about jump positions and reset.
An intuitive explanation is that when pPDA
makes the first forward jump, a gap is left in s2 that
has not been edited yet. It remembers where it left
off, and comes back to it after some substitutions
have been made to complete the edit sequence. The
second row in Figure 1 (starting with pPDA) illus-
trates an edit sequence in a pPDA model that in-
volves three ?jump? operations, which are annotated
and indexed by number 1-3 in the example. ?Jump
1? creates an un-edited gap between word 43 and
western, after two substitutions, the model makes
?jump 2? to go back and edit the gap. The only edit
permitted immediately after ?jump 2? is deleting the
comma in s1, since inserting the word 43 in s2 before
any substitution is disallowed. Once the gap is com-
pleted, the model resumes at position Jend by making
?jump 3?, and completes the jump sequence.
The ?jumps? allowed the model to align words
such as western India, in addition to the alignments
of 43 people found by the pFSM. In practice, we
found that our extension gives a big boost to model
performance (cf. Section 4), with only a modest in-
crease in computation time. 4
3Recall that we transform s1 into s2, and thus on the s2 side,
we can only insert but not delete. The argument applies equally
to the case where the jump was made on the other side.
4The length of the longest edit sequence with jumps only
increased by 0.5 ?max(|s1|, |s2|) in the worst case, and by and
large swapping is rare in comparison to basic edits.
650
Figure 2: Stanford Entailment Recognizer: The pipelined approach used by the Stanford entailment recognizer to
analyze sentence pairs and determine whether or not an entailment relationship is present. The entailment recognizer
first obtains dependency parses for both the passage and the hypothesis. These parses are then aligned based upon
lexical and structural similarity between the two dependency graphs. From the aligned graphs, features are extracted
that suggest the presence or absence of an entailment relationship. Figure courtesy of (Pado et al, 2009).
2.2 Parameter Estimation
Since the least squares operator preserves convexity,
and the inner log-sum-exponential function is con-
vex, the resulting objective function is also convex.
For parameter learning, we used the limited mem-
ory quasi-newton method (Liu and Nocedal, 1989)
to find the optimal feature weights and scaling con-
stant for the objective. We initialized ? = 0?, ? = 0,
and ? = 5. We also threw away features occurring
fewer than five times in training corpus. Gradient
calculation was similar to other pFSM models, such
as HMMs, we omitted the details here, for brevity.
2.3 Rich Linguistic Features
We add new substitution operations beyond those
introduced in Section 2, to capture synonyms and
paraphrase in the sentence pair. Synonym rela-
tions are defined according to WordNet (Miller et
al., 1990), and paraphrase matches are given by a
lookup table. To better take advantage of paraphrase
information at the multi-word phrase level, we ex-
tended our substitution operations to match longer
phrases by adding one-to-many and many-to-many
bigram block substitutions. In our experiments on
machine translation evaluation task, which our met-
ric was originally developed for, we found that most
of the gain came from unigrams and bigrams, with
little to no additional gains from trigrams. There-
fore, we limited our experiments to bigram pFSM
and pPDA models, and pruned the paraphrase table
adopted from TERplus 5 to unigrams and bigrams,
resulting in 2.5 million paraphrase pairs. Trained on
all available training data, the resulting pPDA model
has a total of 218 features.
2.4 Model Configuration
We evaluate both the pFSM and pPDA models with
the addition of rich linguistic features, as described
in the previous section. For pPDA model, the jump
distance is set to five. For each model, we experi-
mented with two different training schemes. In the
5Available from www.umiacs.umd.edu/~snover/terp.
651
HYP: Virus was infected.
REF: No one was infected by the virus.
no entailment no entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment entailment
Figure 3: Semantic similarity as determined by mutual textual entailment. Figure courtesy of (Pado et al, 2009).
first scheme, we train a separate model for each sec-
tion of the training dataset (i.e., MSRpar, MSRvid,
and SMTeuroparl), and use that model to test on
their respective test set. For the two unseen test
sets (SMTnews and OnWN), we used a joint model
trained on all of the available training data. We re-
fer to this scheme as Indi henceforth. In the second
scheme, we used the joint model trained on all train-
ing data to make preditions for all test sets (we refer
to this scheme as All). Our official submission con-
tains two runs ? pFSM with scheme Indi, and pPDA
with scheme All.
3 Textual Entailment for STS
We contrast the performance of the probabilistic edit
distance metrics with an adaptation of the Stanford
Entailment Recognizer to the STS task. In this sec-
tion, we review the textual entailment task, the op-
eration of the Stanford Entailment Recognizer, and
describe how we adapted our entailment system to
the STS task.
3.1 Recognizing Textual Entailment
The Recognizing Textual Entailment (RTE) task
(Dagan et al, 2005) involves determining whether
the meaning of one text can be inferred from an-
other. The text providing the ground truth for the
evaluation is known as the passage while the text
being tested for entailment is known as the the hy-
pothesis. A passage entails a hypothesis if a casual
speaker would consider the inference to be correct.
This intentionally side-steps strict logical entailment
and implicitly brings in all of the world knowledge
speakers use to interpret language.
The STS task and RTE differ in two significant
ways. First, the RTE task is one directional. If a
hypothesis sentence is implied by a passage, the in-
verse does not necessarily hold (e.g., ?John is out-
side in the snow without a coat.? casually implies
?John is cold?, but not vice versa). Second, the RTE
task forces systems to make a boolean choice about
entailment, rather than the graded scale of semantic
relatedness implied by STS.
3.2 Textual Entailment System Description
Shown in Figure 2, the Stanford entailment sys-
tem uses a linguistically rich multi-stage annotation
pipeline. Incoming sentence pairs are first depen-
dency parsed. The dependency parse trees are then
transformed into semantic graphs containing addi-
tional annotations such as named entities and coref-
erence. The two semantic graphs are then aligned
based upon structural overlap and lexical semantic
similarity using a variety of word similarity metrics
based on WordNet, vector space distributional sim-
ilarity as calculated by InfoMap, and a specialized
module for matching ordinal values. The system
then supplies the aligned semantic graphs as input to
a number of feature producing modules. Some mod-
ules produce gross aggregate scores, such as return-
ing the alignment quality between the two sentences.
Others look for specific phenomena that suggest the
presence or absence of an entailment relationship,
such as a match or mismatch in polarity (e.g., ?died?
vs. ?didn?t die?), tense, quantification, and argument
structure. The resulting features are then passed on
to a down stream classifier to predict whether or not
an entailment relationship exists.
3.3 Adapting RTE to STS
In order to adapt our entailment recognition sys-
tem to STS, we follow the same approach Pado
et al (2009) used to successfully adapt the entail-
ment system to machine translation evaluation. As
shown in Figure 3, for each pair of sentences pre-
sented to the system, we run the entailment system
in both directions and extract features that describe
whether the first sentence entails the second and vice
versa for the opposite direction. This setup effec-
tively treats the STS task as a bidirectional variant
of the RTE task. The extracted bidirectional entail-
ment features are then passed on to a support vec-
652
Models All MSRpar MSRvid SMTeuro OnWn SMTnews
pFSMIndi 0.6354(38) 0.3795 0.5350 0.4377 - -
pFSMAll 0.3727 0.3769 0.4569 0.4256 0.6052 0.4164
pPDAIndi 0.6808 0.4244 0.5051 0.4554 - -
pPDAAll 0.4229(77) 0.4409 0.4698 0.4558 0.6468 0.4769
Entailment 0.5589(55) 0.4374 0.8037 0.3533 0.3077 0.3235
Table 1: Absolute score prediction results on STS12 test set. Numbers in this table are Pearson correlation scores. Best
result on each test set is highlighted in bold. Numbers in All column that has superscript are the official submissions.
Their relative rank among 89 systems in shown in parentheses.
tor machine regression (SVR) model, which predicts
the STS score for the sentence pair. As in Pado et
al. (2009), we augment the bidirectional entailment
features with sentence level BLEU scores, in order
to improve robustness over noisy non-grammatical
data. We trained the SVR model using libSVM over
all of the sentence pairs in the STS training set. The
model uses a Gaussian kernel with ? = 0.125, an
SVR ?-loss of 0.25, and margin violation cost, C, of
2.0. These hyperparameters were selected by cross
validation over the training set.
4 Results
From Table 1, we can see that the pPDA model
performed better than the pFSM model on all test
sets except the MSRvid section. This result clearly
demonstrates the power of the pPDA extension
in modeling long-distance word swapping. The
MSRvid test set has the shortest overall sentence
length (13, versus 35 forMSRpar), and therefore it is
not too surprising that long distance word swapping
did not help much here. Furthermore, the pPDA
model shows a much more pronounced performance
gain than pFSM when tested on unseen datasets
(OnWn and SMTnews), suggesting that the pPDA
model is more robust across domain. A second ob-
servation is that the Indi training scheme seems to
work better than the All approach, which shows hav-
ing more training data does not compensate the dif-
ferent characteristics of each training portion. Our
best metric on all test set is the pPDAIndi model,
with a Pearson?s correlation score of 0.6808. If
interpolated into the official submitted runs rank-
ing, it would be placed at the 22nd place among
89 runs. Among the three official runs submitted
to the shared task (pPDAAll, pFSMIndi and En-
tailment), pFSMIndi performs the best, placed at
38th place among 89 runs. Since our metrics were
originally designed for statistical machine transla-
tion (MT) evaluation, we found that on the unseen
SMTNews test set, which consists of news conversa-
tion sentence pairs from the MT domain, our pPDA
model placed at a much higher position (13 among
89 runs).
In comparison to results on MT evaluation
task (Wang and Manning, 2012), we found that the
pPDA and pFSM models work less well on STS.
Whereas in MT evaluation it is common to have
access to thousands of training examples, there is
an order of magnitude less available training data
in STS. Therefore, learning hundreds of feature pa-
rameters in our models from such few examples are
likely to be ill-posed.
Overall, the RTE system did not perform as well
as the regression based models except for MSRvid
domain , which has the shortest overall sentence
length. Our qualitative evaluation suggests that
MSRvid domain seems to exhibit the least degree of
lexical divergence between the sentence pairs, thus
making this task easier than other domains (the me-
dian score of all 89 official systems for MSRvid
is 0.7538, while the median for MSRpar and SM-
Teuroparl is 0.5128 and 0.4437, respectively). The
relative rank of RTE for MSRvid is 21 among 89,
whereas the pFSM and pPDA systems ranked 80 and
83, respectively. The low performance of pFSM and
pPDA on this task significantly affected the ranking
of these two systems on the ALL evaluation measure.
We do not have a clear explanation why RTE system
thrives on this easier task while pPDA and pFSM
suffers. In the future, we aim to gain a better under-
standing of the characteristics of the two different
systems, and explore combination techniques.
653
5 Conclusion
We describe a metric for computing sentence level
semantic textual similarity, which is based on a
probabilistic finite state machine model that com-
putes weighted edit distance. Our model admits a
rich set of linguistic features, and can be trained to
learn feature weights automatically by optimizing
a regression objective. A novel pushdown automa-
ton extension was also presented for capturing long-
distance word swapping. Our models outperformed
Stanford textual entailment system on all but one of
the genres on the STS task.
Acknowledgements
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181 and the support of the
DARPA Broad Operational Language Translation
(BOLT) program through IBM. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
2001. Introduction to Algorithms, Second Edition.
MIT Press.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL recognising textual entailment challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
Y. Dombb, O. Lipsky, B. Porat, E. Porat, and A. Tsur.
2010. The approximate swap and mismatch edit dis-
tance. Theoretical Computer Science, 411(43).
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proceedings of ACL.
J. Esparza and A. Kucera. 2005. Quantitative analysis
of probabilistic pushdown automata: Expectations and
variances. In Proceedings of the 20th Annual IEEE
Symposium on Logic in Computer Science.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Proceedings of AMTA.
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In Proceed-
ings of HLT/NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. J. Miller. 1990. WordNet: an on-line lexical
database. International Journal of Lexicography, 3(4).
S. Pado, D. Cer, M. Galley, D. Jurafsky, and C. Man-
ning. 2009. Measuring machine translation quality as
semantic equivalence: A metric based on entailment
features. Machine Translation, 23:181?193.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005. Probabilistic finite-state
machines part I. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1013?1025.
M. Wang and C. Manning. 2012. SPEDE: Probabilistic
edit distance metrics for sentence level MT evaluation.
In Proceedings of WMT.
D. Wu, 2010. CRC Handbook of Natural Language Pro-
cessing, chapter How to Select an Answer String?,
pages 367?408. CRC Press.
654
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 32?43, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
*SEM 2013 shared task: Semantic Textual Similarity
Eneko Agirre
University of the Basque Country
e.agirre@ehu.es
Daniel Cer
Stanford University
danielcer@stanford.edu
Mona Diab
George Washington University
mtdiab@gwu.edu
Aitor Gonzalez-Agirre
University of the Basque Country
agonzalez278@ikasle.ehu.es
Weiwei Guo
Columbia University
weiwei@cs.columbia.edu
Abstract
In Semantic Textual Similarity (STS), sys-
tems rate the degree of semantic equivalence,
on a graded scale from 0 to 5, with 5 be-
ing the most similar. This year we set up
two tasks: (i) a core task (CORE), and (ii)
a typed-similarity task (TYPED). CORE is
similar in set up to SemEval STS 2012 task
with pairs of sentences from sources related
to those of 2012, yet different in genre from
the 2012 set, namely, this year we included
newswire headlines, machine translation eval-
uation datasets and multiple lexical resource
glossed sets. TYPED, on the other hand, is
novel and tries to characterize why two items
are deemed similar, using cultural heritage
items which are described with metadata such
as title, author or description. Several types of
similarity have been defined, including simi-
lar author, similar time period or similar lo-
cation. The annotation for both tasks lever-
ages crowdsourcing, with relative high inter-
annotator correlation, ranging from 62% to
87%. The CORE task attracted 34 participants
with 89 runs, and the TYPED task attracted 6
teams with 14 runs.
1 Introduction
Given two snippets of text, Semantic Textual Simi-
larity (STS) captures the notion that some texts are
more similar than others, measuring the degree of
semantic equivalence. Textual similarity can range
from exact semantic equivalence to complete un-
relatedness, corresponding to quantified values be-
tween 5 and 0. The graded similarity intuitively cap-
tures the notion of intermediate shades of similarity
such as pairs of text differ only in some minor nu-
anced aspects of meaning only, to relatively impor-
tant differences in meaning, to sharing only some
details, or to simply being related to the same topic,
as shown in Figure 1.
One of the goals of the STS task is to create a
unified framework for combining several semantic
components that otherwise have historically tended
to be evaluated independently and without character-
ization of impact on NLP applications. By providing
such a framework, STS will allow for an extrinsic
evaluation for these modules. Moreover, this STS
framework itself could in turn be evaluated intrin-
sically and extrinsically as a grey/black box within
various NLP applications such as Machine Trans-
lation (MT), Summarization, Generation, Question
Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of NLP
tasks. STS is different from TE inasmuch as it as-
sumes bidirectional graded equivalence between the
pair of textual snippets. In the case of TE the equiv-
alence is directional, e.g. a car is a vehicle, but a ve-
hicle is not necessarily a car. STS also differs from
both TE and Paraphrasing (in as far as both tasks
have been defined to date in the literature) in that,
rather than being a binary yes/no decision (e.g. a ve-
hicle is not a car), we define STS to be a graded sim-
ilarity notion (e.g. a vehicle and a car are more sim-
ilar than a wave and a car). A quantifiable graded
bidirectional notion of textual similarity is useful for
a myriad of NLP tasks such as MT evaluation, infor-
mation extraction, question answering, summariza-
tion, etc.
32
? (5) The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
? (4) The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade Kabul.
The US army invaded Kabul on May 7th last year, 2010.
? (3) The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a suspect.
?He is not a suspect anymore.? John said.
? (2) The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
? (1) The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
? (0) The two sentences are on different topics.
John went horse back riding at dawn with a whole group of friends.
Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.
Figure 1: Annotation values with explanations and examples for the core STS task.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al, 2012). In addition, we held
a DARPA sponsored workshop at Columbia Uni-
versity1. In 2013, STS was selected as the official
Shared Task of the *SEM 2013 conference. Ac-
cordingly, in STS 2013, we set up two tasks: The
core task CORE, which is similar to the 2012 task;
and a pilot task on typed-similarity TYPED between
semi-structured records.
For CORE, we provided all the STS 2012 data
as training data, and the test data was drawn from
related but different datasets. This is in contrast
to the STS 2012 task where the train/test data
were drawn from the same datasets. The 2012
datasets comprised the following: pairs of sentences
from paraphrase datasets from news and video elic-
itation (MSRpar and MSRvid), machine transla-
tion evaluation data (SMTeuroparl, SMTnews) and
pairs of glosses (OnWN). The current STS 2013
dataset comprises the following: pairs of news head-
lines, SMT evaluation sentences (SMT) and pairs of
glosses (OnWN and FNWN).
The typed-similarity pilot task TYPED attempts
1http://www.cs.columbia.edu/?weiwei/
workshop/
to characterize, for the first time, the reason and/or
type of similarity. STS reduces the problem of judg-
ing similarity to a single number, but, in some appli-
cations, it is important to characterize why and how
two items are deemed similar, hence the added nu-
ance. The dataset comprises pairs of Cultural Her-
itage items from Europeana,2 a single access point
to millions of books, paintings, films, museum ob-
jects and archival records that have been digitized
throughout Europe. It is an authoritative source of
information coming from European cultural and sci-
entific institutions. Typically, the items comprise
meta-data describing a cultural heritage item and,
sometimes, a thumbnail of the item itself.
Participating systems in the TYPED task need to
compute the similarity between items, using the tex-
tual meta-data. In addition to general similarity, par-
ticipants need to score specific kinds of similarity,
like similar author, similar time period, etc. (cf. Fig-
ure 3).
The paper is structured as follows. Section 2 re-
ports the sources of the texts used in the two tasks.
Section 3 details the annotation procedure. Section
4 presents the evaluation of the systems, followed
by the results of CORE and TYPED tasks. Section 6
draws on some conclusions and forward projections.
2http://www.europeana.eu/
33
Figure 2: Annotation instructions for CORE task
year dataset pairs source
2012 MSRpar 1500 news
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 news
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2013 TYPED 1500 Cultural Heritage items
Table 1: Summary of STS 2012 and 2013 datasets.
2 Source Datasets
Table 1 summarizes the 2012 and 2013 datasets.
2.1 CORE task
The CORE dataset comprises pairs of news head-
lines (HDL), MT evaluation sentences (SMT) and
pairs of glosses (OnWN and FNWN).
For HDL, we used naturally occurring news head-
lines gathered by the Europe Media Monitor (EMM)
engine (Best et al, 2005) from several different news
sources. EMM clusters together related news. Our
goal was to generate a balanced data set across the
different similarity ranges, hence we built two sets
of headline pairs: (i) a set where the pairs come
from the same EMM cluster, (ii) and another set
where the headlines come from a different EMM
cluster, then we computed the string similarity be-
tween those pairs. Accordingly, we sampled 375
headline pairs of headlines that occur in the same
EMM cluster, aiming for pairs equally distributed
between minimal and maximal similarity using sim-
ple string similarity. We sample another 375 pairs
from the different EMM cluster in the same manner.
The SMT dataset comprises pairs of sentences
used in machine translation evaluation. We have two
different sets based on the evaluation metric used:
an HTER set, and a HYTER set. Both metrics use
the TER metric (Snover et al, 2006) to measure the
similarity of pairs. HTER typically relies on several
(1-4) reference translations. HYTER, on the other
hand, leverages millions of translations. The HTER
set comprises 150 pairs, where one sentence is ma-
chine translation output and the corresponding sen-
tence is a human post-edited translation. We sam-
ple the data from the dataset used in the DARPA
GALE project with an HTER score ranging from 0
to 120. The HYTER set has 600 pairs from 3 sub-
sets (each subset contains 200 pairs): a. reference
34
Figure 3: Annotation instructions for TYPED task
vs. machine translation. b. reference vs. Finite State
Transducer (FST) generated translation (Dreyer and
Marcu, 2012). c. machine translation vs. FST gen-
erated translation. The HYTER data set is used in
(Dreyer and Marcu, 2012).
The OnWN/FnWN dataset contains gloss pairs
from two sources: OntoNotes-WordNet (OnWN)
and FrameNet-WordNet (FnWN). These pairs are
sampled based on the string similarity ranging from
0.4 to 0.9. String similarity is used to measure the
similarity between a pair of glosses. The OnWN
subset comprises 561 gloss pairs from OntoNotes
4.0 (Hovy et al, 2006) and WordNet 3.0 (Fellbaum,
1998). 370 out of the 561 pairs are sampled from the
110K sense-mapped pairs as made available from
the authors. The rest, 291 pairs, are sampled from
unmapped sense pairs with a string similarity rang-
ing from 0.5 to 0.9. The FnWN subset has 189
manually mapped pairs of senses from FrameNet 1.5
(Baker et al, 1998) to WordNet 3.1. They are ran-
domly selected from 426 mapped pairs. In combi-
nation, both datasets comprise 750 pairs of glosses.
2.2 Typed-similarity TYPED task
This task is devised in the context of the PATHS
project,3 which aims to assist users in accessing
digital libraries looking for items. The project
tests methods that offer suggestions about items that
might be useful to recommend, to assist in the inter-
pretation of the items, and to support the user in the
discovery and exploration of the collections. Hence
the task is about comparing pairs of items. The pairs
are generated in the Europeana project.
A study in the PATHS project suggested that users
would be interested in knowing why the system is
suggesting related items. The study suggested seven
similarity types: similar author or creator, similar
people involved, similar time period, similar loca-
3http://www.paths-project.eu
35
Figure 4: TYPED pair on our survey. Only general and author similarity types are shown.
tion, similar event or action, similar subject and sim-
ilar description. In addition, we also include general
similarity. Figure 3 shows the definition of each sim-
ilarity type as provided to the annotators.
The dataset is generated in semi-automatically.
First, members of the project manually select 25
pairs of items for each of the 7 similarity types (ex-
cluding general similarity), totalling 175 manually
selected pairs. After removing duplicates and clean-
ing the dataset, we got 163 pairs. Second, we use
these manually selected pairs as seeds to automat-
ically select new pairs as follows: Starting from
those seeds, we use the Europeana API to get similar
items, and we repeat this process 5 times in order to
diverge from the original items (we stored the vis-
ited items to avoid looping). Once removed from
the seed set, we select the new pairs following two
approaches:
? Distance 1: Current item and similar item.
? Distance 2: Current item and an item that is
similar to a similar item (twice removed dis-
tance wise)
This yields 892 pairs for Distance 1 and 445 of
Distance 2. We then divide the data into train and
test, preserving the ratios. The train data contains
82 manually selected pairs, 446 pairs with similarity
distance 1 and 222 pairs with similarity distance 2.
The test data follows a similar distribution.
Europeana items cannot be redistributed, so we
provide their urls and a script which uses the official
36
Europeana API to access and extract the correspond-
ing metadata in JSON format and a thumbnail. In
addition, the textual fields which are relevant for the
task are made accessible in text files, as follows:
? dcTitle: title of the item
? dcSubject: list of subject terms (from some vo-
cabulary)
? dcDescription: textual description of the item
? dcCreator: creator(s) of the item
? dcDate: date(s) of the item
? dcSource: source of the item
3 Annotation
3.1 CORE task
Figure 1 shows the explanations and values for
each score between 5 and 0. We use the Crowd-
Flower crowd-sourcing service to annotate the
CORE dataset. Annotators are presented with the
detailed instructions given in Figure 2 and are asked
to label each STS sentence pair on our 6 point scale
using a dropdown box. Five sentence pairs at a time
are presented to annotators. Annotators are paid
0.20 cents per set of 5 annotations and we collect
5 separate annotations per sentence pair. Annota-
tors are restricted to people from the following coun-
tries: Australia, Canada, India, New Zealand, UK,
and US.
To obtain high quality annotations, we create a
representative gold dataset of 105 pairs that are man-
ually annotated by the task organizers. During an-
notation, one gold pair is included in each set of 5
sentence pairs. Crowd annotators are required to
rate 4 of the gold pairs correct to qualify to work
on the task. Gold pairs are not distinguished in any
way from the non-gold pairs. If the gold pairs are
annotated incorrectly, annotators are told what the
correct annotation is and they are given an explana-
tion of why. CrowdFlower automatically stops low
performing annotators ? those with too many incor-
rectly labeled gold pairs ? from working on the task.
The distribution of scores in the headlines HDL
dataset is uniform, as in FNWN and OnWN, al-
though the scores are slightly lower in FNWN and
slightly higher in OnWN. The scores for SMT are
not uniform, with most of the scores uniformly dis-
tributed between 3.5 and 5, a few pairs between 2
and 3.5, and nearly no pairs with values below 2.
3.2 TYPED task
The dataset is annotated using crowdsourcing. The
survey contains the 1500 pairs of the dataset (750 for
train and 750 for test), plus 20 gold pairs for quality
control. Each participant is shown 4 training gold
questions at the beginning, and then one gold every
2 or 4 questions depending on the accuracy. If accu-
racy dropped to less than 66.7% percent the survey
is stopped and the answers from that particular an-
notator are discarded. Each annotator is allowed to
rate a maximum of 20 pairs to avoid getting answers
from people that are either tired or bored. To ensure
a good comprehension of the items, the task is re-
stricted to only accept annotators from some English
speaking countries: UK, USA, Australia, Canada
and New Zealand.
Participants are asked to rate the similarity be-
tween pairs of cultural heritage items from rang-
ing from 5 to 0, following the instructions shown
in Figure 3. We also add a ?Not Applicable? choice
for cases in which annotators are not sure or didn?t
know. For those cases, we calculate the similarity
score using the values of the rest of the annotators (if
none, we convert it to 0). The instructions given to
the annotators are the ones shown in Figure 3. Fig-
ure 4 shows a pair from the dataset, as presented to
annotators.
The similarity scores for the pairs follow a similar
distribution in all types. Most of the pairs have a
score between 4 and 5, which can amount to as much
as 50% of all pairs in some types.
3.3 Quality of annotation
In order to assess the annotation quality, we measure
the correlation of each annotator with the average of
the rest of the annotators. We then averaged all the
correlations. This method to estimate the quality is
identical to the method used for evaluation (see Sec-
tion 4.1) and it can be thus used as the upper bound
for the systems. The inter-tagger correlation in the
CORE dataset for each of dataset is as follows:
? HDL: 85.0%
? FNWN: 69.9%
? OnWN: 87.2%
? SMT: 65.8%
For the TYPED dataset, the inter-tagger correla-
tion values for each type of similarity is as follows:
? General: 77.0%
37
? Author: 73.1%
? People Involved: 62.5%
? Time period: 72.0%
? Location: 74.3%
? Event or Action: 63.9%
? Subject: 74.5%
? Description: 74.9%
In both datasets, the correlation figures are high,
confirming that the task is well designed. The weak-
est correlations in the CORE task are SMT and
FNWN. The first might reflect the fact that some
automatically produced translations are confusing
or difficult to understand, and the second could be
caused by the special style used to gloss FrameNet
concepts. In the TYPED task the weakest correla-
tions are for the People Involved and Event or Action
types, as they might be the most difficult to spot.
4 Systems Evaluation
4.1 Evaluation metrics
Evaluation of STS is still an open issue. STS ex-
periments have traditionally used Pearson product-
moment correlation, or, alternatively, Spearman
rank order correlation. In addition, we also need a
method to aggregate the results from each dataset
into an overall score. The analysis performed in
(Agirre and Amigo?, In prep) shows that Pearson and
averaging across datasets are the best suited com-
bination in general. In particular, Pearson is more
informative than Spearman, in that Spearman only
takes the rank differences into account, while Pear-
son does account for value differences as well. The
study also showed that other alternatives need to be
considered, depending on the requirements of the
target application.
We leave application-dependent evaluations for
future work, and focus on average weighted Pear-
son correlation. When averaging, we weight each
individual correlation by the size of the dataset.
In addition, participants in the CORE task are al-
lowed to provide a confidence score between 1 and
100 for each of their scores. The evaluation script
down-weights the pairs with low confidence, follow-
ing weighted Pearson.4 In order to compute sta-
tistical significance among system results, we use
4http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
a one-tailed parametric test based on Fisher?s z-
transformation (Press et al, 2002, equation 14.5.10).
4.2 The Baseline Systems
For the CORE dataset, we produce scores using a
simple word overlap baseline system. We tokenize
the input sentences splitting at white spaces, and
then represent each sentence as a vector in the mul-
tidimensional token space. Each dimension has 1
if the token is present in the sentence, 0 otherwise.
Vector similarity is computed using the cosine sim-
ilarity metric. We also run two freely available sys-
tems, DKPro (Bar et al, 2012) and TakeLab (S?aric? et
al., 2012) from STS 2012,5 and evaluate them on the
CORE dataset. They serve as two strong contenders
since they ranked 1st (DKPro) and 2nd (TakeLab) in
last year?s STS task.
For the TYPED dataset, we first produce XML
files for each of the items, using the fields as pro-
vided to participants. Then we run named entity
recognition and classification (NERC) and date de-
tection using Stanford CoreNLP. This is followed by
calculating the similarity score for each of the types
as follows.
? General: cosine similarity of TF-IDF vectors of
tokens from all fields.
? Author: cosine similarity of TF-IDF vectors for
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF-IDF vectors of loca-
tion/date/people recognized by NERC in all
fields.
? Events: cosine similarity of TF-IDF vectors of
verbs in all fields.
? Subject and description: cosine similarity of
TF-IDF vectors of respective fields.
IDF values are calculated from a subset of the
Europeana collection (Culture Grid collection). We
also run a random baseline several times, yielding
close to 0 correlations in all datasets, as expected.
4.3 Participation
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 34
teams participated in the CORE task, submitting 89
5Code is available at http://www-nlp.stanford.
edu/wiki/STS
38
Team and run Head. OnWN FNWN SMT Mean # Team and run Head. OnWN FNWN SMT Mean #
baseline-tokencos .5399 .2828 .2146 .2861 .3639 73 KnCe2013-all .3475 .3505 .1073 .1551 .2639 86
DKPro .7347 .7345 .3405 .3256 .5652 - KnCe2013-diff .4028 .3537 .1284 .1804 .2934 84
TakeLab-best .6559 .6334 .4052 .3389 .5221 - KnCe2013-set .0462 -.1526 .0376 -.0605 -.0397 90
TakeLab-sts12 .4858 .6334 .2693 .2787 .4340 - LCL Sapienza-ADW1 .6943 .4661 .3571 .3311 .4880 43
aolney-w3c3 .5248 .4701 .1777 .2744 .3986 67 LCL Sapienza-ADW2 .6520 .5280 .3598 .3681 .5019 32
BGU-1 .5075 .3252 .0768 .1843 .3181 81 LCL Sapienza-ADW3 .6205 .5108 .4462 .3838 .4996 34
BGU-2 .3608 .3777 -.0173 .0698 .2363 88 LIPN-tAll .7063 .6937 .4037 .3005 .5425 16
BGU-3 .3591 .3360 .0072 .2122 .2748 85 LIPN-tSp .5791 .7199 .3522 .3721 .5261 24
BUAP-RUN1 .5005 .2579 .1766 .2322 .3234 78 MayoClinicNLP-r1wtCDT .6584 .7775 .3735 .3605 .5649 6
BUAP-RUN2 .4860 .2872 .2082 .2117 .3216 79 MayoClinicNLP-r2CDT .6827 .6612 .3960 .3946 .5572 8
BUAP-RUN3 .4817 .2711 .2511 .1990 .3156 82 MayoClinicNLP-r3wtCD .6440 .8295 .3202 .3561 .5671 5
CFILT-1 .5336 .2381 .2261 .2906 .3531 75 NTNU-RUN1 .7279 .5952 .3215 .4015 .5519 9
CLaC-RUN1 .6774 .7667 .3793 .3068 .5511 10 NTNU-RUN2 .5909 .1634 .3650 .3786 .3946 68
CLaC-RUN2 .6921 .7366 .3793 .3375 .5587 7 NTNU-RUN3 .7274 .5882 .3115 .4035 .5498 12
CLaC-RUN3 .5276 .6495 .4158 .3082 .4755 47 PolyUCOMP-RUN1 .5176 .1517 .2496 .2914 .3284 77
CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36 SOFTCARDINALITY-run1 .6410 .7360 .3442 .3035 .5273 23
CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33 SOFTCARDINALITY-run2 .6713 .7412 .3838 .2981 .5402 18
CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30 SOFTCARDINALITY-run3 .6603 .7401 .3347 .2900 .5294 22
CPN-combined.RandSubSpace .6771 .5135 .3314 .3369 .4939 39 sriubc-System1? .6083 .2915 .2790 .3065 .4011 66
CPN-combined.SVM .6685 .5096 .3621 .3408 .4939 38 sriubc-System2? .6359 .3664 .2713 .3476 .4420 57
CPN-individual.RandSubSpace .6771 .5484 .3314 .2769 .4826 45 sriubc-System3? .5443 .2843 .2705 .3275 .3842 70
DeepPurple-length .6542 .5105 .2507 .2803 .4598 56 SXUCFN-run1 .6806 .5355 .3181 .3980 .5198 27
DeepPurple-linear .6878 .5105 .2693 .2787 .4721 50 SXUCFN-run2 .4881 .6146 .4237 .3844 .4797 46
DeepPurple-lineara .6227 .5105 .3265 .2952 .4607 55 SXUCFN-run3 .6761 .6481 .3025 .4003 .5458 14
deft-baseline .6532 .8431 .5083 .3265 .5795 3 SXULLL-1 .4840 .7146 .0415 .1543 .3944 69
deft-baseline2 .5706 .8111 .5503 .3325 .5495 13 UCam-A .5510 .3099 .2385 .1171 .3200 80
DLS@CU-char .3867 .2386 .3726 .3337 .3309 76 UCam-B .6399 .4440 .3995 .3400 .4709 53
DLS@CU-charSemantic .4669 .4165 .3859 .3411 .4056 64 UCam-C .4962 .5639 .1724 .3006 .4207 62
DLS@CU-charWordSemantic .4921 .3769 .4647 .3492 .4135 63 UCSP-NC? .1736 .0853 .1151 .1658 .1441 89
ECNUCS-Run1 .5656 .2083 .1725 .2949 .3533 74 UMBC EBIQUITY-galactus .7428 .7053 .5444 .3705 .5927 2
ECNUCS-Run2 .7120 .5388 .2013 .2504 .4720 51 UMBC EBIQUITY-ParingWords .7642 .7529 .5818 .3804 .6181 1
ECNUCS-Run3 .6799 .5284 .2203 .3595 .4967 35 UMBC EBIQUITY-saiyan .7838 .5593 .5815 .3563 .5683 4
HENRY-run1 .7601 .4631 .3516 .2801 .4917 41 UMCC DLSI-1 .5841 .4847 .2917 .2855 .4352 58
HENRY-run2 .7645 .4631 .3905 .3593 .5229 26 UMCC DLSI-2 .6168 .5557 .3045 .3407 .4833 44
HENRY-run3 .7103 .3934 .3364 .3308 .4734 48 UMCC DLSI-3 .3846 .1342 -.0065 .2736 .2523 87
IBM EG-run2 .7217 .6110 .3364 .3460 .5365 19 UNIBA-2STEPSML .4255 .4801 .1832 .2710 .3673 71
IBM EG-run5 .7410 .5987 .4133 .3426 .5452 15 UNIBA-DSM PERM .6319 .4910 .2717 .3155 .4610 54
IBM EG-run6 .7447 .6257 .4381 .3275 .5502 11 UNIBA-STACKING .6275 .4658 .2111 .2588 .4293 61
ikernels-sys1 .7352 .5432 .3842 .3180 .5188 28 Unimelb NLP-bahar .7119 .3490 .3813 .3507 .4733 49
ikernels-sys2 .7465 .5572 .3875 .3409 .5339 21 Unimelb NLP-concat .7085 .6790 .3374 .3230 .5415 17
ikernels-sys3 .7395 .4228 .3596 .3294 .4919 40 Unimelb NLP-stacking .7064 .6140 .1865 .3144 .5091 29
INAOE-UPV-run1 .6392 .3249 .2711 .3491 .4332 59 Unitor-SVRegressor run1 .6353 .5744 .3521 .3285 .4941 37
INAOE-UPV-run2 .6390 .3260 .2662 .3457 .4319 60 Unitor-SVRegressor run2 .6511 .5610 .3580 .3096 .4902 42
INAOE-UPV-run3 .6468 .6295 .4090 .3047 .5085 31 Unitor-SVRegressor run3 .6027 .5489 .3269 .3192 .4716 52
KLUE-approach 1 .6521 .6507 .3996 .3367 .5254 25 UPC-AE .6092 .5679 -.1268 .2090 .4037 65
KLUE-approach 2 .6510 .6869 .4189 .3360 .5355 20 UPC-AED .4136 .4770 -.0852 .1662 .3050 83
UPC-AED T .5119 .6386 -.0464 .1235 .3671 72
Table 2: Results on the CORE task. The first rows on the left correspond to the baseline and to two publicly available
systems, see text for details. Note: ? signals team involving one of the organizers, ? for systems submitting past the
120 hour window.
system runs. For the TYPED task, 6 teams partici-
pated, submitting 14 system runs.6
Some submissions had minor issues: one team
had a confidence score of 0 for all items (we re-
placed them by 100), and another team had a few
Not-a-Number scores for the SMT dataset, which
we replaced by 5. One team submitted the results
past the 120 hours. This team, and the teams that in-
6Due to lack of space we can?t detail the full names of au-
thors and institutions that participated.The interested reader can
use the name of the runs in Tables 2 and 3 to find the relevant
paper in these proceedings.
cluded one of the organizers, are explicitly marked.
We want to stress that in these teams the organizers
did not allow the developers of the system to access
any data or information which was not available for
the rest of participants. After the submission dead-
line expired, the organizers published the gold stan-
dard in the task website, in order to ensure a trans-
parent evaluation process.
4.4 CORE Task Results
Table 2 shows the results of the CORE task, with
runs listed in alphabetical order. The correlation in
39
Team and run General Author People involved Time Location Event Subject Description Mean #
baseline .6691 .4278 .4460 .5002 .4835 .3062 .5015 .5810 .4894 8
BUAP-RUN1 .6798 .6166 .0670 .2761 .0163 .1612 .5167 .5283 .3577 14
BUAP-RUN2 .6745 .6093 .1285 .3721 .0163 .1660 .5094 .5546 .3788 13
BUAP-RUN3 .6992 .6345 .1055 .1461 .0000 -.0668 .3729 .5120 .3004 15
BUT-1 .3686 .7468 .3920 .5725 .3604 .2906 .2270 .5882 .4433 9
ECNUCS-Run1 .6040 .7362 .3663 .4685 .3844 .4057 .5229 .6027 .5113 5
ECNUCS-Run2 .6064 .5684 .3663 .4685 .3844 .4057 .5563 .6027 .4948 7
PolyUCOMP-RUN1 .4888 .6940 .3223 .3820 .3621 .1625 .3962 .4816 .4112 12
PolyUCOMP-RUN2 .4893 .6940 .3253 .3777 .3628 .1968 .3962 .4816 .4155 11
PolyUCOMP-RUN3 .4915 .6940 .3254 .3737 .3667 .2207 .3962 .4816 .4187 10
UBC UOS-RUN1? .7256 .4568 .4467 .5762 .4858 .3090 .5015 .5810 .5103 6
UBC UOS-RUN2? .7457 .6618 .6518 .7466 .7244 .6533 .7404 .7751 .7124 4
UBC UOS-RUN3? .7461 .6656 .6544 .7411 .7257 .6545 .7417 .7763 .7132 3
Unitor-SVRegressor lin .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341 2
Unitor-SVRegressor rbf .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620 1
Table 3: Results on TYPED task. The first row corresponds to the baseline. Note: ? signals team involving one of the
organizers.
each dataset is given, followed by the mean cor-
relation (the official measure), and the rank of the
run. The baseline ranks 73. The highest correla-
tions are for OnWN (84%, by deft) and HDL (78%,
by UMBC), followed by FNWN (58%, by UMBC)
and SMT (40%, by NTNU). This fits nicely with the
inter-tagger correlations (respectively 87, 85, 70 and
65, cf. Section 3). It also shows that the systems get
close to the human correlations in the OnWN and
HDL dataset, with bigger differences for FNWN and
SMT.
The result of the best run (by UMBC) is signif-
icantly different (p-value < 0.05) than all runs ex-
cept the second best. The second best run is only
significantly different to the runs ranking 7th and
below, and the third best to the 14th run and be-
low. The difference between consecutive runs was
not significant. This indicates that many system runs
performed very close to each other.
Only 13 runs included non-uniform confidence
scores. In 10 cases the confidence value allowed
to improve performance, sometimes as much as .11
absolute points. For instance, SXUCFN-run3 im-
proves from .4773 to .5458. The most notable ex-
ception is MayoClinicNLP-r2CDT, which achieves
a mean correlation of .5879 instead of .5572 if they
provide uniform confidence values.
The Table also shows the results of TakeLab
and DKPro. We train the DKPro and TakeLab-
sts12 models on all the training and test STS 2012
data. We additionally train another variant sys-
tem of TakeLab, TakeLab-best, where we use tar-
geted training where the model yields the best per-
formance for each test subset as follows: (1) HDL
is trained on MSRpar 2012 data; (2) OnWN is
trained on all 2012 data; (3) FnWN is trained on
2012 OnWN data; (4) SMT is trained on 2012 SM-
Teuroparl data. Note that Takelab-best is an upper
bound, as the best combination is selected on the
test dataset. TakeLab-sts12, TakeLab-best, DKPro
rank as 58th, 27th and 6th in this year?s system sub-
missions, respectively. The different results yielded
from TakeLab depending on the training data sug-
gests that some STS systems are quite sensitive to
the source of the sentence pairs, indicating that do-
main adaptation techniques could have a role in this
task. On the other hand, DKPro performed ex-
tremely well when trained on all available training,
with no special tweaking for each dataset.
4.5 TYPED Task Results
Table 3 shows the results of TYPED task. The
columns show the correlation for each type of sim-
ilarity, followed by the mean correlation (the offi-
cial measure), and the rank of the run. The best sys-
tem (from Unitor) is best in all types. The baseline
ranked 8th, but the performance difference with the
best system is quite significant. The best result is
significantly different (p-value < 0.02) to all runs.
The second and third best runs are only significantly
different from the run ranking 5th and below. Note
that in this dataset the correlations of the best system
are higher than the inter-tagger correlations. This
might indicate that the task has been solved, in the
sense that the features used by the top systems are
enough to characterize the problem and reach hu-
man performance, although the correlations of some
40
A
cr
on
ym
s
D
is
tr
ib
ut
io
na
lm
em
or
y
D
is
tr
ib
ut
io
na
lt
he
sa
ur
us
M
on
ol
in
gu
al
co
rp
or
a
M
ul
ti
li
ng
ua
lc
or
po
ra
O
pi
ni
on
an
d
S
en
ti
m
en
t
Ta
bl
es
of
pa
ra
ph
ra
se
s
W
ik
ip
ed
ia
W
ik
ti
on
ar
y
W
or
d
em
be
dd
in
gs
W
or
dN
et
C
or
re
fe
re
nc
e
D
ep
en
de
nc
y
pa
rs
e
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
D
A
L
em
m
at
iz
er
L
ex
ic
al
S
ub
st
it
ut
io
n
L
og
ic
al
in
fe
re
nc
e
M
et
ap
ho
r
or
M
et
on
ym
y
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
R
O
U
G
E
pa
ck
ag
e
S
co
pi
ng
S
ea
rc
h
en
gi
ne
S
em
an
ti
c
R
ol
e
L
ab
el
in
g
S
tr
in
g
si
m
il
ar
it
y
S
yn
ta
x
Te
xt
ua
le
nt
ai
lm
en
t
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
W
or
d
S
en
se
D
is
am
bi
gu
at
io
n
aolney-w3c3 x x x
BGU-1 x x x x x x x
BGU-2 x x x x x x x
BGU-3 x x x x x x x
CFILT-APPROACH x x x x x
CLaC-Run1 x x x x x x x x
CLaC-Run2 x x x x x x x x
CLaC-Run3 x x x x x x x x
CNGL-LPSSVR x x x x x
CNGL-LPSSVRTL x x x x x
CNGL-LSSVR x x x x x
CPN-combined.RandSubSpace x x x x x x x x
CPN-combined.SVM x x x x x x x x
CPN-individual.RandSubSpace x x x x x x x x
DeepPurple-length x x x x x x x
DeepPurple-linear x x x x x x x
DeepPurple-lineara x x x x x x x
deft-baseline x x x x
deft-baseline x x x x x x
DLS@CU-charSemantic x x x x
DLS@CU-charWordSemantic x x x x x x
DLS@CU-charWordSemantic x x x
ECNUCS-Run1 x x x x x x x
ECNUCS-Run2 x x x x x x x
ECNUCS-Run3 x x x x x x x
HENRY-run1 x x x x x x x x x
HENRY-run2 x x x x x x x x
IBM EG-run2 x x x x x x
IBM EG-run5 x x x x x x
IBM EG-run6 x x x x x
ikernels-sys1 x x x x x x x x x x x
ikernels-sys2 x x x x x x x x x x x
ikernels-sys3 x x x x x x x x x x x
INAOE-UPV-run1 x x x x x x x
INAOE-UPV-run2 x x x x x x x
INAOE-UPV-run3 x x x x x x x
KLUE-approach 1 x x x x x x x
KLUE-approach 2 x x x x x x
KnCe2013-all x x x x x x x x
KnCe2013-div x x x x x x x x
KnCe2013-div x x x x x x x x
LCL Sapienza-ADW1 x x x
LCL Sapienza-ADW2 x x x
LCL Sapienza-ADW3 x x x
LIPN-tAll x x x x x x x x x x
LIPN-tSp x x x x x x x x x x
MayoClinicNLP-r1wtCDT x x x x x x x x x x x x
MayoClinicNLP-r2CDT x x x x x x x x x x x x
MayoClinicNLP-r3wtCD x x x x x x x x x x x x
NTNU-RUN1 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN2 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN3 x x x x x x x x x x x x x x x x x x x x x x x
PolyUCOMP-RUN1 x x x x
SOFTCARDINALITY-run1 x
SOFTCARDINALITY-run2 x x x
SOFTCARDINALITY-run3 x x x
SXUCFN-run1 x x x
SXUCFN-run2 x x x
SXUCFN-run3 x x x
SXULLL-1 x x
UCam-A x x x x
UCam-B x x x x
UCam-C x x x x
UCSP-NC x x x x x
UMBC EBIQUITY-galactus x x x x x x x
UMBC EBIQUITY-ParingWords x x x x x x
UMBC EBIQUITY-saiyan x x x x x x x
UMCC DLSI-1 x x x x x x x x x x
UMCC DLSI-2 x x x x x x x x x x
UMCC DLSI-3 x x x x x x x x x
UNIBA-2STEPSML x x x x x x x x x x x
UNIBA-DSM PERM x x x x x x
UNIBA-STACKING x x x x x x x x x x x
Unimelb NLP-bahar x x
Unimelb NLP-concat x x x x x x x x x x
Unimelb NLP-stacking x x x x x x x x x x
Unitor-SVRegressor run1 x x x x x x
Unitor-SVRegressor run2 x x x x x x
Unitor-SVRegressor run3 x x x x x x
Total 11 2 12 54 12 5 11 36 7 3 54 3 3 48 40 2 67 14 3 3 10 24 55 3 3 4 9 6 34 9 13 6 6
Table 4: CORE task: Resources and tools used by the systems that submitted a description file. Leftmost columns
correspond to the resources, and rightmost to tools, in alphabetic order.
41
types could be too low for practical use.
5 Tools and resources used
The organizers asked participants to submit a de-
scription file, making special emphasis on the tools
and resources that were used. Tables 4 and 5 show
schematically the tools and resources as reported by
some of the participants for the CORE and TYPED
tasks (respectively). In the last row, the totals show
that WordNet and monolingual corpora were the
most used resources for both tasks, followed by
Wikipedia and the use of acronyms (for CORE and
TYPED tasks respectively). Dictionaries, multilin-
gual corpora, opinion and sentiment analysis, and
lists and tables of paraphrases are also used.
For CORE, generic NLP tools such as lemmati-
zation and PoS tagging are widely used, and to a
lesser extent, distributional similarity, knowledge-
based similarity, syntactic analysis, named entity
recognition, lexical substitution and time and date
resolution (in this order). Other popular tools are
Semantic Role Labeling, Textual Entailment, String
Similarity, Tree Kernels and Word Sense Disam-
biguation. Machine learning is widely used to com-
bine and tune components (and so, it is not men-
tioned in the tables). Several less used tools are
also listed but are used by three or less systems.
The top scoring systems use most of the resources
and tools listed (UMBC EBIQUITY-ParingWords,
MayoClinicNLP-r3wtCD). Other well ranked sys-
tems like deft-baseline are only based on distribu-
tional similarity. Although not mentioned in the
descriptions files, some systems used the publicly
available DKPro and Takelab systems.
For the TYPED task, the most used tools are lem-
matizers, Named Entity Recognizers, and PoS tag-
gers. Distributional and Knowledge-base similarity
is also used, and at least four systems used syntactic
analysis and time and date resolution.7
6 Conclusions and Future Work
We presented the 2013 *SEM shared task on Seman-
tic Textual Similarity.8 Two tasks were defined: a
7For a more detailed analysis, the reader is directed to the
papers in this volume.
8All annotations, evaluation scripts and system outputs are
available in the website for the task9. In addition, a collabora-
tively maintained site10, open to the STS community, contains
A
cr
on
ym
s
M
on
ol
in
gu
al
co
rp
or
a
W
ik
ip
ed
ia
W
or
dN
et
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
em
m
at
iz
er
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
S
yn
ta
x
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
BUT-1 x x x x x x x
PolyUCOMP-RUN2 x x x x
ECNUCS-Run1 x x x
ECNUCS-Run2 x x x x x x x
PolyUCOMP-RUN1 x x x x
PolyUCOMP-RUN3 x x x x
UBC UOS-RUN1 x x x x x x x x x x x
UBC UOS-RUN2 x x x x x x x x x x x x
UBC UOS-RUN3 x x x x x x x x x x x x
Unitor-SVRegressor lin x x x x x x x
Unitor-SVRegressor rbf x x x x x x x
Total 4 7 3 7 7 4 11 3 11 11 4 4 2
Table 5: TYPED task: Resources and tools used by
the systems that submitted a description file. Leftmost
columns correspond to the resources, and rightmost to
tools, in alphabetic order.
core task CORE similar to the STS 2012 task, and
a new pilot on typed-similarity TYPED. We had 34
teams participate in both tasks submitting 89 system
runs for CORE and 14 system runs for TYPED, in
total amounting to a 103 system evaluations. CORE
uses datasets which are related to but different from
those used in 2012: news headlines, MT evalua-
tion data, gloss pairs. The best systems attained
correlations close to the human inter tagger corre-
lations. The TYPED task characterizes, for the first
time, the reasons why two items are deemed simi-
lar. The results on TYPED show that the training
data provided allowed systems to yield high corre-
lation scores, demonstrating the practical viability
of this new task. In the future, we are planning on
adding more nuanced evaluation data sets that in-
clude modality (belief, negation, permission, etc.)
and sentiment. Also given the success rate of the
TYPED task, however, the data in this pilot is rel-
atively structured, hence in the future we are inter-
ested in investigating identifying reasons why two
pairs of unstructured texts as those present in CORE
are deemed similar.
Acknowledgements
We are grateful to the OntoNotes team for sharing OntoNotes
to WordNet mappings (Hovy et al 2006). We thank Lan-
guage Weaver, INC, DARPA and LDC for providing the SMT
data. This work is also partially funded by the Spanish Ministry
of Education, Culture and Sport (grant FPU12/06243). This
a comprehensive list of evaluation tasks, datasets, software and
papers related to STS.
42
work was partially funded by the DARPA BOLT and DEFT pro-
grams.
We want to thank Nikolaos Aletras, German Rigau and
Mark Stevenson for their help designing, annotating and col-
lecting the typed-similarity data. The development of the
typed-similarity dataset was supported by the PATHS project
(http://paths-project.eu) funded by the European Community?s
Seventh Framework Program (FP7/2007-2013) under grant
agreement no. 270082. The tasks were partially financed by
the READERS project under the CHIST-ERA framework (FP7
ERA-Net). We thank Europeana and all contributors to Euro-
peana for sharing their content through the API.
References
Eneko Agirre and Enrique Amigo?. In prep. Exploring
evaluation measures for semantic textual similarity. In
Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In COLING ?98
Proceedings of the 17th international conference on
Computational linguistics - Volume 1.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, in conjunction with the
1st Joint Conference on Lexical and Computational
Semantics.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo Gar-
cia, and David Horby. 2005. Europe media monitor -
system description. In EUR Report 22173-En, Ispra,
Italy.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation evalua-
tion. In Human Language Technologies: Conference
of the North American Chapter of the Association of
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2002. Numerical Recipes: The Art of Sci-
entific Computing V 2.10 With Linux Or Single-Screen
License. Cambridge University Press.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
43
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 148?153,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Feature-Rich Phrase-based Translation: Stanford University?s Submissionto the WMT 2013 Translation Task
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt*, John Bauer
Sida Wang, Natalia Silveira?, Julia Neidert and Christopher D. Manning
Computer Science Department, Stanford University
*Center for East Asian Studies, Stanford University
?Department of Linguistics, Stanford University
{spenceg,cerd,kreschke,robvoigt,horatio,sidaw,natalias,jneid,manning}@stanford.edu
Abstract
We describe the Stanford University NLP
Group submission to the 2013 Workshop
on Statistical Machine Translation Shared
Task. We demonstrate the effectiveness of a
new adaptive, online tuning algorithm that
scales to large feature and tuning sets. For
both English-French and English-German,
the algorithm produces feature-rich mod-
els that improve over a dense baseline and
compare favorably to models tuned with
established methods.
1 Introduction
Green et al (2013b) describe an online, adaptive
tuning algorithm for feature-rich translation mod-
els. They showed considerable translation quality
improvements over MERT (Och, 2003) and PRO
(Hopkins and May, 2011) for two languages in a
research setting. The purpose of our submission to
the 2013 Workshop on Statistical Machine Trans-
lation (WMT) Shared Task is to compare the algo-
rithm to more established methods in an evaluation.
We submitted English-French (En-Fr) and English-
German (En-De) systems, each with over 100k fea-
tures tuned on 10k sentences. This paper describes
the systems and also includes new feature sets and
practical extensions to the original algorithm.
2 Translation Model
Our machine translation (MT) system is Phrasal
(Cer et al, 2010), a phrase-based system based on
alignment templates (Och and Ney, 2004). Like
many MT systems, Phrasal models the predictive
translation distribution p(e|f ;w) directly as
p(e|f ;w) = 1Z(f) exp
[
w>?(e, f)
]
(1)
where e is the target sequence, f is the source se-
quence, w is the vector of model parameters, ?(?)
is a feature map, and Z(f) is an appropriate nor-
malizing constant. For many years the dimension
of the feature map ?(?) has been limited by MERT,
which does not scale past tens of features.
Our submission explores real-world translation
quality for high-dimensional feature maps and as-
sociated weight vectors. That case requires a more
scalable tuning algorithm.
2.1 Online, Adaptive Tuning Algorithm
FollowingHopkins andMay (2011) we castMT tun-
ing as pairwise ranking. Consider a single source
sentence f with associated references e1:k. Let d
be a derivation in an n-best list of f that has the
target e = e(d) and the feature map ?(d). Define
the linear model scoreM(d) = w ? ?(d). For any
derivation d+ that is better than d? under a gold
metric G, we desire pairwise agreement such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011). Suppose that we sample
s pairs for source sentence ft to compute a set of
difference vectors Dt = {x1:s+ }. Then we optimize
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(2)
which is the familiar logistic loss. Hopkins and
May (2011) optimize (2) in a batch algorithm
that alternates between candidate generation (i.e.,
n-best list or lattice decoding) and optimization
(e.g., L-BFGS). We instead use AdaGrad (Duchi
148
et al, 2011), a variant of stochastic gradient de-
scent (SGD) in which the learning rate is adapted
to the data. Informally, AdaGrad scales the weight
updates according to the geometry of the data ob-
served in earlier iterations. Consider a particu-
lar dimension j of w, and let scalars vt = wt,j ,
gt = ?j`t(wt?1), and Gt = ?ti=1 g2i . The Ada-Grad update rule is
vt = vt?1 ? ? G?1/2t gt (3)
Gt = Gt?1 + g2t (4)
In practice,Gt is a diagonal approximation. IfGt =
I , observe that (3) is vanilla SGD.
In MT systems, the feature map may generate
exponentially many irrelevant features, so we need
to regularize (3). The L1 norm of the weight vec-
tor is known to be an effective regularizer in such
a setting (Ng, 2004). An efficient way to apply
L1 regularization is the Forward-Backward split-
ting (FOBOS) framework (Duchi and Singer, 2009),
which has the following two-step update:
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (5)
wt = argmin
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(6)
where (5) is just an unregularized gradient descent
step and (6) balances the regularization term r(w)
with staying close to the gradient step.
For L1 regularization we have r(w) = ?||w||1
and the closed-form solution to (6) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(7)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls below
the threshold ?t?1?.
Online algorithms are inherently sequential; this
algorithm is no exception. If we want to scale the
algorithm to large tuning sets, then we need to par-
allelize the weight updates. Green et al (2013b)
describe the parallelization technique that is imple-
mented in Phrasal.
2.2 Extensions to (Green et al, 2013b)
Sentence-Level Metric We previously used the
gold metric BLEU+1 (Lin and Och, 2004), which
smoothes bigram precisions and above. This metric
worked well with multiple references, but we found
that it is less effective in a single-reference setting
like WMT. To make the metric more robust, Nakov
et al (2012) extended BLEU+1 by smoothing both
the unigram precision and the reference length. We
found that this extension yielded a consistent +0.2
BLEU improvement at test time for both languages.
Subsequent experiments on the data sets of Green
et al (2013b) showed that standard BLEU+1 works
best for multiple references.
Custom regularization parameters Green et al
(2013b) showed that large feature-rich models over-
fit the tuning sets. We discovered that certain fea-
tures caused greater overfitting than others. Custom
regularization strengths for each feature set are one
solution to this problem. We found that technique
largely fixed the overfitting problem as shown by
the learning curves presented in section 5.1.
Convergence criteria Standard MERT imple-
mentations approximate tuning BLEU by re-
ranking the previous n-best lists with the updated
weight vector. This approximation becomes infeasi-
ble for large tuning sets, and is less accurate for algo-
rithms like ours that do not accumulate n-best lists.
We approximate tuning BLEU by maintaining the
1-best hypothesis for each tuning segment. At the
end of each epoch, we compute corpus-level BLEU
from this hypothesis set. We flush the set of stored
hypotheses before the next epoch begins. Although
memory-efficient, we find that this approximation
is less dependable as a convergence criterion than
the conventional method. Whereas we previously
stopped the algorithm after four iterations, we now
select the model according to held-out accuracy.
3 Feature Sets
3.1 Dense Features
The baseline ?dense? model has 19 features: the
nine Moses (Koehn et al, 2007) baseline features, a
hierarchical lexicalized re-ordering model (Galley
and Manning, 2008), the (log) bitext count of each
translation rule, and an indicator for unique rules.
The final dense feature sets for each language
differ slightly. The En-Fr system incorporates a
second language model. The En-De system adds a
future cost component to the linear distortion model
(Green et al, 2010).The future cost estimate allows
the distortion limit to be raised without a decrease
in translation quality.
149
3.2 Sparse Features
Sparse features do not necessarily fire on each hy-
pothesis extension. Unlike prior work on sparseMT
features, our feature extractors do not filter features
based on tuning set counts. We instead rely on the
regularizer to select informative features.
Several of the feature extractors depend on
source-side part of speech (POS) sequences and
dependency parses. We created those annotations
with the Stanford CoreNLP pipeline.
Discriminative Phrase Table A lexicalized in-
dicator feature for each rule in a derivation. The
feature weights can be interpreted as adjustments
to the associated dense phrase table features.
Discriminative Alignments A lexicalized indi-
cator feature for the phrase-internal alignments in
each rule in a derivation. For one-to-many, many-to-
one, and many-to-many alignments we extract the
clique of aligned tokens, perform a lexical sort, and
concatenate the tokens to form the feature string.
Discriminative Re-ordering A lexicalized indi-
cator feature for each rule in a derivation that ap-
pears in the following orientations: monotone-with-
next, monotone-with-previous, non-monotone-
with-next, non-monotone-with-previous. Green
et al (2013b) included the richer non-monotone
classes swap and discontinuous. However, we found
that these classes yielded no significant improve-
ment over the simpler non-monotone classes. The
feature weights can be interpreted as adjustments
to the generative lexicalized re-ordering model.
Source Content-Word Deletion Count-based
features for source content words that are ?deleted?
in the target. Content words are nouns, adjectives,
verbs, and adverbs. A deleted source word is ei-
ther unaligned or aligned to one of the 100 most
frequent target words in the target bitext. For each
deleted word we increment both the feature for the
particular source POS and an aggregate feature for
all parts of speech. We add similar but separate
features for head content words that are either un-
aligned or aligned to frequent target words.
Inverse Document Frequency Numeric fea-
tures that compare source and target word frequen-
cies. Let idf(?) return the inverse document fre-
quency of a token in the training bitext. Suppose
a derivation d = {r1, r2, . . . , rn} is composed of
n translation rules, where e(r) is the target side of
the rule and f(r) is the source side. For each rule
Bilingual Monolingual
Sentences Tokens Tokens
En-Fr 5.0M 289M 1.51B
En-De 4.4M 223M 1.03B
Table 1: Gross corpus statistics after data selection
and pre-processing. The En-Fr monolingual counts
include French Gigaword 3 (LDC2011T10).
r that translates j source tokens to i target tokens
we compute
q =
?
i
idf(e(r)i)?
?
j
idf(f(r)j) (8)
We add two numeric features, one for the source and
another for the target. When q > 0 we increment
the target feature by q; when q < 0 we increment
the target feature by |q|. Together these features
penalize asymmetric rules that map rare words to
frequent words and vice versa.
POS-based Re-ordering The lexicalized dis-
criminative re-ordering model is very sparse, so we
added re-ordering features based on source parts of
speech. When a rule is applied in a derivation, we
extract the associated source POS sequence along
with the POS sequences from the previous and next
rules. We add a ?with-previous? indicator feature
that is the conjunction of the current and previous
POS sequences; the ?with-next? indicator feature is
created analogously. This feature worked well for
En-Fr, but not for En-De.
4 Data Preparation
Table 1 describes the pre-processed corpora from
which our systems are built.
4.1 Data Selection
We used all of the monolingual and parallel En-
De data allowed in the constrained condition. We
incorporated all of the French monolingual data,
but sampled a 5M-sentence bitext from the approx-
imately 40M available En-Fr parallel sentences.
To select the sentences we first created a ?target?
corpus by concatenating the tuning and test sets
(newstest2008?2013). Then we ran the feature
decay algorithm (FDA) (Bi?ici and Yuret, 2011),
which samples sentences that most closely resem-
ble the target corpus. FDA is a principled method
for reducing the phrase table size by excluding less
relevant training examples.
150
4.2 Tokenization
We tokenized the English (source) data according
to the Penn Treebank standard (Marcus et al, 1993)
with Stanford CoreNLP. The French data was to-
kenized with packages from the Stanford French
Parser (Green et al, 2013a), which implements a
scheme similar to that used in the French Treebank
(Abeill? et al, 2003).
German is more complicated due to pervasive
compounding. We first tokenized the data with the
same English tokenizer. Then we split compounds
with the lattice-based model (Dyer, 2009) in cdec
(Dyer et al, 2010). To simplify post-processing we
added segmentation markers to split tokens, e.g.,
?berschritt? ?ber #schritt.
4.3 Alignment
We aligned both bitexts with the Berkeley Aligner
(Liang et al, 2006) configured with standard set-
tings. We symmetrized the alignments according
to the grow-diag heuristic.
4.4 Language Modeling
We estimated unfiltered 5-gram language models
using lmplz (Heafield et al, 2013) and loaded them
with KenLM (Heafield, 2011). For memory effi-
ciency and faster loading we also used KenLM to
convert the LMs to a trie-based, binary format. The
German LM included all of the monolingual data
plus the target side of the En-De bitext. We built
an analogous model for French. In addition, we
estimated a separate French LM from the Gigaword
data.1
4.5 French Agreement Correction
In French verbs must agree in number and person
with their subjects, and adjectives (and some past
participles) must agree in number and gender with
the nouns they modify. On their own, phrasal align-
ment and target side language modeling yield cor-
rect agreement inflection most of the time. For
verbs, we find that the inflections are often accurate:
number is encoded in the English verb and subject,
and 3rd person is generally correct in the absence
of a 1st or 2nd person pronoun. However, since En-
glish does not generally encode gender, adjective
inflection must rely on language modeling, which
is often insufficient.
1The MT system learns significantly different weights for
the two LMs: 0.086 for the primary LM and 0.044 for the
Gigaword LM.
To address this problem we apply an automatic
inflection correction post-processing step. First, we
generate dependency parses of our system?s out-
put using BONSAI (Candito and Crabb?, 2009),
a French-specific extension to the Berkeley Parser
(Petrov et al, 2006). Based on these dependencies,
we match adjectives with the nouns they modify
and past participles with their subjects. Then we
use Lefff (Sagot, 2010), a machine-readable French
lexicon, to determine the gender and number of the
noun and to choose the correct inflection for the
adjective or participle.
Applied to our 3,000 sentence development set,
this correction scheme produced 200 corrections
with perfect accuracy. It produces a slight (?0.014)
drop in BLEU score. This arises from cases where
the reference translation uses a synonymous but
differently gendered noun, and consequently has
different adjective inflection.
4.6 German De-compounding
Split German compounds must be merged after
translation. This process often requires inserting
affixes (e.g., s, en) between adjacent tokens in the
compound. Since the German compounding rules
are complex and exception-laden, we rely on a dic-
tionary lookup procedure with backoffs. The dic-
tionary was constructed during pre-processing. To
compound the final translations, we first lookup
the compound sequence?which is indicated by
segmentation markers?in the dictionary. If it is
present, then we use the dictionary entry. If the com-
pound is novel, then for each pair of words to be
compounded, we insert the suffix most commonly
appended in compounds to the first word of the pair.
If the first word itself is unknown in our dictionary,
we insert the suffix most commonly appended after
the last three characters. For example, words end-
ing with ung most commonly have an s appended
when they are used in compounds.
4.7 Recasing
Phrasal includes an LM-based recaser (Lita et al,
2003), which we trained on the target side of the
bitext for each language. On the newstest2012 de-
velopment data, the German recaser was 96.8% ac-
curate and the French recaser was 97.9% accurate.
5 Translation Quality Experiments
During system development we tuned on
newstest2008?2011 (10,570 sentences) and tested
151
#iterations #features tune newstest2012 newstest2013?
Dense 10 20 30.26 31.12 ?
Feature-rich 11 207k 32.29 31.51 29.00
Table 2: En-Fr BLEU-4 [% uncased] results. The tuning set is newstest2008?2011. (?) newstest2013 is
the cased score computed by the WMT organizers.
#iterations #features tune newstest2012 newstest2013?
Dense 10 19 16.83 18.45 ?
Feature-rich 13 167k 17.66 18.70 18.50
Table 3: En-De BLEU-4 [% uncased] results.
on newstest2012 (3,003 sentences). We compare
the feature-rich model to the ?dense? baseline.
The En-De system parameters were: 200-best
lists, a maximum phrase length of 8, and a distortion
limit of 6 with future cost estimation. The En-Fr
system parameters were: 200-best lists, a maximum
phrase length of 8, and a distortion limit of 5.
The online tuning algorithm used a default learn-
ing rate ? = 0.03 and a mini-batch size of 20. We
set the regularization strength ? to 10.0 for the dis-
criminative re-ordering model, 0.0 for the dense
features, and 0.1 otherwise.
5.1 Results
Tables 2 and 3 show En-Fr and En-De results, re-
spectively. The ?Feature-rich? model, which con-
tains the full complement of dense and sparse fea-
tures, offers ameager improvement over the ?Dense?
baseline. This result contrasts with the results
of Green et al (2013b), who showed significant
translation quality improvements over the same
dense baseline for Arabic-English and Chinese-
English. However, they had multiple target refer-
ences, whereas the WMT data sets have just one.
We speculate that this difference is significant. For
example, consider a translation rule that rewrites
to a 4-gram in the reference. This event can in-
crease the sentence-level score, thus encouraging
the model to upweight the rule indicator feature.
More evidence of overfitting can be seen in Fig-
ure 1, which shows learning curves on the devel-
opment set for both language pairs. Whereas the
dense model converges after just a few iterations,
the feature-rich model continues to creep higher.
Separate experiments on a held-out set showed that
generalization did not improve after about eight
iterations.
6 Conclusion
We submitted a feature-rich MT system to WMT
2013. While sparse features did offer a measur-
able improvement over a baseline dense feature set,
the gains were not as significant as those shown
by Green et al (2013b). One important difference
between the two sets of results is the number of ref-
erences. Their NIST tuning and test sets had four
references; the WMT data sets have just one. We
speculate that sparse features tend to overfit more
in this setting. Individual features can greatly in-
fluence the sentence-level metric and thus become
large components of the gradient. To combat this
phenomenon we experimented with custom reg-
ularization strengths and a more robust sentence-
level metric. While these two improvements greatly
reduced the model size relative to (Green et al,
2013b), a generalization problem remained. Nev-
ertheless, we showed that feature-rich models are
now competitive with the state-of-the-art.
Acknowledgments This work was supported by the Defense
Advanced Research Projects Agency (DARPA) Broad Opera-
tional Language Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of DARPA or the US government.
References
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building
a treebank for French, chapter 10. Kluwer.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
M. Candito and B. Crabb?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT.
152
ll l l l l
l l l l
l
l
l l
l l
l l
l l
29
30
31
32
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(a) En-Fr tuning
l
l
l l l l l l
l l
l
l
l l l
l l
l l l
7.5
10.0
12.5
15.0
17.5
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(b) En-De tuning
Figure 1: BLEU-4 [% uncased] Learning curves on newstest2008?2011 with loess trend lines.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In NAACL.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In HLT-NAACL.
S. Green, M-C. de Marneffe, and C. D. Manning.
2013a. Parsing models for identifying multiword
expressions. Computational Linguistics, 39(1):195?
227.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL.
B. Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In LREC.
153
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 320?328,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Positive Diversity Tuning
for Machine Translation System Combination
Daniel Cer, Christopher D. Manning and Daniel Jurafsky
Stanford University
Stanford, CA 94305, USA
{danielcer,manning,jurafsky}@stanford.edu
Abstract
We present Positive Diversity Tuning, a
newmethod for tuningmachine translation
models specifically for improved perfor-
mance during system combination. Sys-
tem combination gains are often limited
by the fact that the translations produced
by the different component systems are
too similar to each other. We propose a
method for reducing excess cross-system
similarity by optimizing a joint objective
that simultaneously rewards models for
producing translations that are similar to
reference translations, while also punish-
ing them for translations that are too sim-
ilar to those produced by other systems.
The formulation of the Positive Diversity
objective is easy to implement and allows
for its quick integration with most machine
translation tuning pipelines. We find that
individual systems tuned on the same data
to Positive Diversity can be even more
diverse than systems built using different
data sets, while still obtaining good BLEU
scores. When these individual systems are
used together for system combination, our
approach allows for significant gains of 0.8
BLEU even when the combination is per-
formed using a small number of otherwise
identical individual systems.
1 Introduction
The best performing machine translation sys-
tems are typically not individual decoders but
rather are ensembles of two ormore systemswhose
output is then merged using system combination
algorithms. Since combining multiple distinct
equally good translation systems reliably produces
gains over any one of the systems in isolation, it is
widely used in situations where high quality is es-
sential.
Exploiting system combination brings signifi-
cant cost: Macherey and Och (2007) showed that
successful system combination requires the con-
struction of multiple systems that are simultane-
ously diverse and well-performing. If the systems
are not distinct enough, they will bring very lit-
tle value during system combination. However,
if some of the systems produce diverse transla-
tions but achieve lower overall translation quality,
their contributions risk being ignored during sys-
tem combination.
Prior work has approached the need for diverse
systems by using different system architectures,
model components, system build parameters, de-
coder hyperparameters, as well as data selection
and weighting (Macherey and Och, 2007; DeNero
et al, 2010; Xiao et al, 2013). However, during
tuning, each individual system is still just trained to
maximize its own isolated performance on a tune
set, or at best an error-driven reweighting of the
tune set, without explicitly taking into account the
diversity of the resulting translations. Such tuning
does not encourage systems to rigorously explore
model variations that achieve both good translation
quality and diversity with respect to the other sys-
tems. It is reasonable to suspect that this results in
individual systems that under exploit the amount
of diversity possible, given the characteristics of
the individual systems.
For better system combination, we propose
building individual systems to attempt to simulta-
neously maximize the overall quality of the indi-
vidual systems and the amount of diversity across
systems. We operationalize this problem formu-
lation by devising a new heuristic measure called
Positive Diversity that estimates the potential use-
fulness of individual systems during system com-
bination. We find that optimizing systems toward
Positive Diversity leads to significant performance
gains during system combination even when the
combination is performed using a small number of
320
otherwise identical individual translation systems.
The remainder of this paper is organized as fol-
lows. Section 2 and 3 briefly review the tuning
of individual machine translation systems and how
system combination merges the output of multiple
systems into an improved combined translation.
Section 4 introduces our Positive Diversity mea-
sure. Section 5 introduces an algorithm for training
a collection of translation systems toward Positive
Diversity. Experiments are presented in sections 6
and 7. Sections 8 and 9 conclude with discussions
of prior work and directions for future research.
2 Tuning Individual Translation Systems
Machine translation systems are tuned toward
somemeasure of the correctness of the translations
produced by the system according to one or more
manually translated references. As shown in equa-
tion (1), this can be written as finding parameter
values? that produce translations sys? that in turn
achieve a high score on some correctness measure:
argmax
?
Correctness(ref[],sys?) (1)
The correctness measure that systems are typi-
cally tuned toward is BLEU (Papineni et al, 2002),
which measures the fraction of the n-grams that
are both present in the reference translations and
the translations produced by a system. The BLEU
score is computed as the geometric mean of the
resulting n-gram precisions scaled by a brevity
penalty.
The most widely used machine translation
tuning algorithm, minimum error rate training
(MERT) (Och, 2003), attempts to maximize the
correctness objective directly. Popular alternatives
such as pairwise ranking objective (PRO) (Hop-
kins and May, 2011), MIRA (Chiang et al, 2008),
and RAMPION (Gimpel and Smith, 2012) use sur-
rogate optimization objectives that indirectly at-
tempt to maximize the correctness function by us-
ing it to select targets for training discriminative
classification models. In practice, either optimiz-
ing correctness directly or optimizing a surrogate
objective that uses correctness to choose optimiza-
tion targets results in roughly equivalent transla-
tion performance (Cherry and Foster, 2012).
Even when individual systems are being built
to be used in a larger combined system, they are
still usually tuned to maximize their isolated in-
dividual system performance rather than to maxi-
mize the potential usefulness of their contribution
during system combination.1 To our knowledge,
no effort has been made to explicitly tune toward
criteria that attempts to simultaneously maximize
the translation quality of individual systems and
their mutual diversity. This is unfortunate since the
most valuable component systems for system com-
bination should not only obtain good translation
performance, but also produce translations that are
different from those produced by other systems.
3 System Combination
Similar to speech recognition?s Recognizer Out-
put Voting Error Reduction (ROVER) algorithm
(Fiscus, 1997), machine translation system com-
bination typically operates by aligning the transla-
tions produced by two or more individual transla-
tion systems and then using the alignments to con-
struct a search space that allows new translations to
be pieced together by picking and choosing parts
of the material from the original translations (Ban-
galore et al, 2001; Matusov et al, 2006; Rosti et
al., 2007a; Rosti et al, 2007b; Karakos et al, 2008;
Heafield and Lavie, 2010a).2 The alignment of the
individual system translations can be performed
using alignment driven evaluation metrics such as
invWER, TERp, METEOR (Leusch et al, 2003;
Snover et al, 2009; Denkowski and Lavie, 2011).
The piecewise selection of material from the orig-
inal translations is performed using the combina-
tion model?s scoring features such as n-gram lan-
guage models, confidence models over the indi-
vidual systems, and consensus features that score a
combined translation using n-gramsmatches to the
individual system translations (Rosti et al, 2007b;
Zhao and He, 2009; Heafield and Lavie, 2010b).
Both system confidence model features and n-
gram consensus features score contributions based
in part on how confident the system combination
model is in each individual machine translation
system. This means that little or no gains will typ-
ically be seen when combining a good system with
poor performing systems even if the systems col-
1The exception being Xiao et al (2013)?s work using
boosting for error-driven reweighting of the tuning set
2Other system combination techniques exist such as can-
didate selection systems, whereby the combination model at-
tempts to find the best single candidate produced by one of the
translation engines (Paul et al, 2005; Nomoto, 2004; Zwarts
and Dras, 2008), decoder chaining (Aikawa and Ruopp,
2009), re-decoding informed by the decoding paths taken
by other systems (Huang and Papineni, 2007), and decoding
model combination (DeNero et al, 2010).
321
Input : systems [], tune(), source, refs [], ?, EvalMetric (), SimMetric ()
Output: models []
// start with an empty set of translations from prior iterations
other_sys []? []
for i? 1 to len(systems []) do
// new Positive Diversity measure using prior translations
PD?,i()? new PD(?, EvalMetric(), SimMetric(), refs [], other_sys [])
// tune a new model to fit PD?,i
// e.g., using MERT, PRO, MIRA, RAMPION, etc.
models [i]? tune(systems [i], source, PD?,i())
// Save translations from tuned modeli for use during
// the diversity computation for subsequent systems
push(other_sys [], translate(systems [i],models [i], source))
end
returnmodels []
Algorithm 1: Positive Diversity Tuning (PDT)
lectively produce very diverse translations.3
The requirement that the systems used for sys-
tem combination be both of high quality and di-
verse can be and often is met by building several
different systems using different system architec-
tures, model components or tuning data. However,
as will be shown in the next few sections, by ex-
plicitly optimizing an objective that targets both
translation quality and diversity, it is possible to
obtain meaningful system combination gains even
using a single system architecture with identical
model components and the same tuning set.
4 Positive Diversity
We propose Positive Diversity as a heuristic
measurement of the value of potential contribu-
tions from an individual system to system combi-
nation. As given in equation (2), PositiveDiversity
is defined as the correctness of the translations pro-
duced by a systemminus a penalty term that scores
how similar the systems translations are with those
produced by other systems:
PD? = ? Correctness(ref[],sys?)?
(1? ?) Similarity(other_sys[],sys?)
(2)
The hyperparameter ? explicitly trades-off the
preference for a well performing individual sys-
3The machine learning theory behind boosting suggests
that it should be possible to combine a very large number of
poor performing systems into a single good system. However,
for machine translation, using a very large number of individ-
ual systems brings with it difficult computational challenges.
tem with system combination diversity. Higher
? values result in a Positive Diversity metric that
mostly favors good quality translations. However,
even for large ? values, if two translations are of
approximately the same quality, the Positive Di-
versity metric will prefer the one that is the most
diverse given the translations being produced by
other systems.
The Correctness() and Similarity()
measures are any function that can score transla-
tions from a single system against other transla-
tions. This includes traditionalmachine translation
evaluation metrics (e.g, BLEU, TER, METEOR)
as well as any other measure of textual similarity.
For the remainder of this paper, we use BLEU to
measure both correctness and the similarity of the
translations produced by the individual systems.
When tuning individual translation systems toward
Positive Diversity, our task is then to maximize
equation (3) rather than equation (1):
argmax? ? BLEU(ref[],sys)?
(1? ?) BLEU(other_sys[],sys) (3)
Since this learning objective is simply the differ-
ence between two BLEU scores, it should be easy
to integrate into most existing machine translation
tuning pipelines that are already designed to op-
timize performance on translation evaluation met-
rics.
322
PDT Individual System Diversity
System \ Iteration 1 2 3 4 5 6 7 8 9 10
? = 0.95 36.6 32.0 19.0 13.6 11.9 8.2 15.9 8.7 7.3 2.3
? = 0.97 32.9 21.7 17.7 10.4 2.7 7.4 2.3 7.3 2.1 2.9
? = 0.99 23.9 13.1 7.9 2.3 3.2 2.6 2.2 1.5 3.4 0.7
Table 1: Diversity scores for PDT individual systems onBOLT dev12 dev. Individual systems are tuned to
Positive Diversity on GALE dev10 web tune. A system?s diversity score is measured as its 1.0?BLEU
score on the translations produced by PDT systems from earlier iterations. Higher scores mean more
diversity.
Diversity of Baseline System vs. Individual PDT Systems Available at Iteration i
PDT Systems \ Iteration 0 1 2 3 4 5 6 7 8 9 10
? = 0.95 27.3 20.4 16.8 14.9 12.8 11.4 9.4 8.6 8.3 8.1 7.9
? = 0.97 28.4 21.3 15.8 14.7 13.3 13.0 12.5 12.2 10.3 10.0 9.7
? = 0.99 27.5 22.6 18.5 17.1 16.8 15.9 15.4 14.6 14.3 13.5 13.4
Table 2: Diversity scores of a baseline system tuned to BOLT dev12 tune, a different tuning set than what
was used for the PDT individual systems. The baseline system diversity is scored against all of the PDT
individual systems available at iteration i for a given ? value and over translations of BOLT dev12 dev.
5 Tuning to Positive Diversity
To tune a collection of machine transla-
tion systems using Positive Diversity, we pro-
pose a staged process, whereby systems are
tuned one-by-one to maximize equation (2)
using the translations produced by previously
trained systems to compute the diversity term,
Similarity(other_sys[], sys?).
As shown in Algorithm 1, Positive Diversity
Tuning (PDT) takes as input: a list of machine
translation systems, systems[]; a tuning proce-
dure for training individual systems, tune(); a
tuning data set with source and reference trans-
lations, source and refs; a hyperparameter ?
to adjust the trade-off between fitting the refer-
ence translations and diversity between the sys-
tems; and metrics to measure correctness and
cross-system similarity, Correctness() and
Similarity().
The list of systems can contain any translation
system that can be parameterized using tune().
This can be a heterogeneous collection of substan-
tially different systems (e.g., phrase-based, hier-
archical, syntactic, or tunable hybrid systems) or
even multiple copies of a single machine transla-
tion system. In all cases, systems later in the list
will be trained to produce translations that both fit
the references and are encouraged to be distinct
from the systems earlier in the list.
During each iteration, the system constructs a
new Positive Diversity measure PD?,i using the
translations produced during prior iterations of
training. This PD?,i measure is then given to
tune() as the the training criteria for modeli
of systemi. The function tune() is any al-
gorithm that allows a translation system?s perfor-
mance to be fit to an evaluation metric. This
includes both minimum error rate training algo-
rithms (MERT) that attempt to directly optimize a
system?s performance on a metric, as well as other
techniques such as Pairwaise Ranking Objective
(PRO),MIRA, and RAMPION that optimize a sur-
rogate loss based on the preferences of an evalua-
tion metric.
After training a model for each system, the re-
sulting model-system pairs can be combined using
any arbitrary system combination strategy.
6 Experiments
Experiments are performed using a single
phrase-based Chinese-to-English translation sys-
tem, built with the Stanford Phrasal machine trans-
lation toolkit (Cer et al, 2010). The system was
built using all of the parallel data available for
Phase 2 of the DARPA BOLT program. The Chi-
nese data was segmented to the Chinese Tree-
Bank (CTB) standard using a maximum match
word segmenter, trained on the output of a CRF
segmenter (Xiang et al, 2013). The bitext was
word aligned using the Berkeley aligner (Liang et
al., 2006). Standard phrase-pair extraction heuris-
323
BLEU scores from individual systems
tuned during iteration i of PDT
PDT System 0 1 2 3 4 5 6 7 8 9 10
? = 0.95 16.2 16.0 15.7 15.9 16.1 16.1 15.9 15.4 16.1 15.9 16.2
? = 0.97 16.4 15.8 15.8 15.9 16.0 16.2 16.1 16.2 16.2 16.4 16.1
? = 0.99 16.3 16.1 16.2 15.9 16.3 16.4 16.4 16.3 16.4 16.5 16.3
Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE
dev10 web tune. Scores report individual system performance before system combination.
tics were used to extract a phrase-table over word
alignments symmetrized using grow-diag (Koehn
et al, 2003). We made use of a hierarchical re-
ordering model (Galley and Manning, 2008) as
well as a 5-gram languagemodel trained on the tar-
get side of the bi-text and smoothed usingmodified
Kneeser-Ney (Chen and Goodman, 1996).
Individual PDT systems were tuned on the
GALE dev10 web tune set using online-PRO
(Green et al, 2013; Hopkins and May, 2011)
to the Positive Diversity Tuning criterion.4 The
Multi-EngineMachine Translation (MEMT) pack-
age was used for system combination (Heafield
and Lavie, 2010a). We used BOLT dev12 dev as
a development test set to explore different ? pa-
rameterizations of the Positive Diversity criteria.
7 Results
Table 1 illustrates the amount of diversity
achieved by individual PDT systems on the BOLT
dev12 dev evaluation set for ? values 0.95, 0.97,
and 0.99.5 Using different tuning sets is one of the
common strategies for producing diverse compo-
nent systems for system combination. Thus, as a
baseline, Table 2 gives the diversity of a system
tuned to BLEU using a different tuning set, BOLT
dev12 tune, with respect to the PDT systems avail-
able at each iteration. As in Table 1, the diver-
sity computation is performed using translations of
BOLT dev12 dev.
Like the cross-system diversity term in the for-
mulation of Positive Diversity using BLEU in
4Preliminary experiments performed using MERT to train
the individual systems produced similar results to those seen
here. However, we switched to online-PRO since it dramat-
ically reduced the amount time required to train each indi-
vidual system. We expect similar results when using other
tuning algorithms for the individual systems, such as MIRA
or RAMPION.
5Due to time constraints, wewere not able to try additional
? values. Given that our results suggest the lowest ? value
from the ones we tried works best (i.e., ? = 0.95), it would
be worth trying additional smaller ? values such as 0.90
equation (3), we measure the diversity of trans-
lations produced by an individual system as the
negative BLEU score of the translations with re-
spect to the translations from systems built during
prior iterations. For clarity of presentation, these
diversity scores are reported as 1.0?BLEU. Using
1.0?BLEU to score cross-system diversity, means
that the reported numbers can be roughly inter-
preted as the fraction of n-grams from the individ-
ual systems built during iteration i that have not
been previously produced by other systems built
during any iteration < i.6
In our experiments, we find that for ? ? 0.97,
during the first three iterations of PDT, there is
more diversity among the PDT systems tuned on a
single data set (GALE dev10 web tune) than there
is between systems tuned on different datasets
(BOLT dev12 tune vs. GALE dev10 wb tune). This
is significant since using different tuning sets is a
common strategy for increasing diversity during
system combination. These results suggest PDT
is better at producing additional diversity than us-
ing different tuning sets. The PDT systems also
achieve good coverage of the n-grams present in
the baseline system that was tuned using different
data. At iteration 10 and using ? = 0.95, the base-
line systems receive a diversity score of only 7.9%
when measured against the PDT systems.7
As PDT progresses, it becomes more difficult to
tune systems to produce high quality translations
that are substantially different from those already
being produced by other systems. This is seen in
the per iteration diversity scores, whereby during
iteration 5, the individual PDT translation systems
have a 1.0?BLEU diversity score with prior sys-
tems ranging from 11.9%, when using an ? value
6This intuitive interpretation assumes a brevity penalty
that is approximately 1.0.
7For this diversity score, the brevity penalty is 1.0, mean-
ing the diversity score is based purely on the n-grams present
in the baseline system that are not present in translations pro-
duced by one or more of the PDT systems
324
Figure 1: System combination BLEU score achieved using Positive Diversity Tuning with the ? values
0.95, 0.97, and 0.99. Four iterations of PDT with ? = 0.95 results in a 0.8 BLEU gain over the initial
BLEU tuned system. We only examine combinations of up to 6 systems (i.e., iterations 0-5), as the time
required to tune MEMT increases dramatically as additional systems are added.
of 0.95, to 3.2% when using an ? value of 0.99.
A diversity score of 3.2% when using ? = 0.99
suggests that by iteration 5, very high ? values
put insufficient pressure on learning to find mod-
els that produce diverse translations. When using
an ? of 0.95, a sizable amount of diversity still ex-
ists across the systems translations all the way to
iteration 7. By iteration 10, only a small amount
of additional diversity is contributed by each addi-
tional system for all of the alpha values (< 3%).8
Table 3 shows the BLEU scores obtained on the
BOLT dev12 dev evaluation set by the individual
systems tuned during each iteration of PDT. The
0th iteration for each ? value has an empty set of
translations for the diversity term. This means the
resulting systems are effectively tuned to just max-
imize BLEU. Differences in system performance
during this iteration are only due to differences in
the random seeds used during training. Starting at
iteration 1, the individual systems are optimized to
produce translations that both score well on BLEU
8We speculate that if heterogeneous translation systems
were used with PDT, it could be possible to run with higher ?
values and still obtain diverse translations after a large number
of PDT iterations
and are diverse from the systems produced dur-
ing prior iterations. It is interesting to note that
the systems trained during these subsequent itera-
tions obtain BLEU scores that are usually competi-
tive with those obtained by the iteration 0 systems.
Taken together with the diversity scores in Table
1, this strongly suggests that PDT is succeeding
at increasing diversity while still producing high
quality individual translation systems.
Figure 1 graphs the system combination BLEU
score achieved by using varying numbers of Pos-
itive Diversity Tuned translation systems and dif-
ferent ? values to trade-off translation quality with
translation diversity. After running 4 iterations of
PDT, the best configuration, ? = 0.95, achieves a
BLEU score that is 0.8 BLEU higher than the cor-
responding BLEU trained iteration 0 system.9
From the graph, it appears that PDT perfor-
mance initially increases as additional systems are
added to the system combination and then later
plateaus or even drops after too many systems are
included. The combinations using PDT systems
9Recall that the iteration 0 system is effectively just tuned
to maximize BLEU since we have an empty set of translations
from other systems that are used to compute diversity
325
built with higher ? values reach the point of di-
minishing returns faster than combinations using
systems built with lower alpha values. For in-
stance, ? = 0.99 plateaus on iteration 2, while
? = 0.95 peaks on iteration 4. It might be pos-
sible to identify the point at which additional sys-
tems will likely not be useful by using the diversity
scores in Table 1. Scoring about 10% or less on
the 1?BLEUdiversitymeasure, with respect to the
other systems being used within the system combi-
nation, seems to suggest the individual system will
not be very helpful to add into the combination.
8 Related Work
While the idea of encouraging diversity in indi-
vidual systems that will be used for system combi-
nation has been proven effective in speech recogni-
tion and document summarization (Hinton, 2002;
Breslin and Gales, 2007; Carbonell and Goldstein,
1998; Goldstein et al, 2000), there has only been
a modest amount of prior work exploring such
approaches for machine translation. Prior work
within machine translation has investigated adapt-
ing machine learning techniques for building en-
sembles of classifiers to translation system tuning,
encouraging diversity by varying both the hyper-
parameters and the data used to build the individual
systems, and chaining together individual transla-
tion systems.
Xiao et al (2013) explores using boosting to
train an ensemble of machine translation systems.
Following the standard Adaboost algorithm, each
system was trained in sequence on an error-driven
reweighting of the tuning set that focuses learning
on the material that is the most problematic for the
current ensemble. They found that using a single
system to tune a large number of decoding mod-
els to different Adaboost guided weightings of the
tuning data results in significant gains during sys-
tem combination.
Macherey and Och (2007) investigated system
combination using automatic generation of diverse
individual systems. They programmatically gener-
ated variations of systems using different build and
decoder hyperparameters such as choice of word-
alignment algorithm, distortion limit, variations of
model feature function weights, and the set of lan-
guage models used. Then, in a process similar to
forward feature selection, they constructed a com-
bined system by iteratively adding the individual
automatically generated system that produced the
largest increase in quality when used in conjunc-
tion with the systems already selected for the com-
bined system. They also explored producing varia-
tion by using different samplings of the the training
data. The individual and combined systems pro-
duced by sampling the training data were inferior
to systems that used all of the available data. How-
ever, the experiments facilitated insightful analysis
on what properties an individual system must have
in order to be useful during system combination.
They found that in order to be useful within a com-
bination, individual systems need to produce trans-
lations of similar quality to other individual sys-
tems within the system combination while also be-
ing as uncorrelated as possible from the other sys-
tems. The Positive Diversity Tuning method in-
troduced in our work is an explicit attempt to build
individual translation systems that meet this crite-
ria, while being less computationally demanding
than the diversity generating techniques explored
by Macherey and Och (2007).
Aikawa and Ruopp (2009) investigated build-
ing machine translations systems specifically for
use in sequential combination with other systems.
They constructed chains of systems whereby the
output of one decoder is feed as input to the next
decoder in the pipeline. The downstream systems
are built and tuned to correct errors produced by
the preceding system. In this approach, the down-
stream decoder acts as a machine learning based
post editing system.
9 Conclusion
We have presented Positive Diversity as a new
way of jointly measuring the quality and diversity
of the contribution of individual machine transla-
tion systems to system combination. This method
heuristically assesses the value of individual trans-
lation systems by measuring their similarity to the
reference translations as well as their dissimilarity
from the other systems being combined. We op-
erationalize this metric by reusing existing tech-
niques from machine translation evaluation to as-
sess translation quality and the degree of similar-
ity between systems. We also give a straightfor-
ward algorithm for training a collection of individ-
ual systems to optimize Positive Diversity. Our
experimental results suggest that tuning to Positive
Diversity leads to improved cross-system diversity
and system combination performance even when
combining otherwise identical machine translation
326
systems.
The Positive Diversity Tuning method explored
in this work can be used to tune individual systems
for any ensemble in which individual models can
be fit to multiple extrinsic loss functions. Since
Hall et al (2011) demonstrated the general purpose
application of multiple extrinsic loss functions to
training structured prediction models, Positive Di-
versity Tuning could be broadly useful within nat-
ural language processing and for other machine
learning tasks.
In future work within machine translation, it
may prove fruitful to examine more sophisticated
measures of dissimilarity. For example, one could
imagine a metric that punishes instances of simi-
lar material in proportion to some measure of the
expected diversity of the material. It might also be
useful to explore joint rather than sequential train-
ing of the individual translation systems.
Acknowledgments
We thank the reviewers and the members of the Stan-
ford NLP group for their helpful comments and sugges-
tions. This work was supported by the Defense Advanced
Research Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM and a fel-
lowship to one of the authors from the Center for Advanced
Study in the Behavioral Sciences. Any opinions, findings,
and conclusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily reflect
the view of DARPA or the US government.
References
TakakoAikawa andAchimRuopp. 2009. Chained sys-
tem: A linear combination of different types of sta-
tistical machine translation systems. In Proceedings
of MT Summit XII.
S. Bangalore, G. Bordel, and Giuseppe Riccardi. 2001.
Computing consensus translation from multiple ma-
chine translation systems. In ASRU.
C. Breslin and M. J F Gales. 2007. Complementary
system generation using directed decision trees. In
ICASSP.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In SIGIR.
Daniel Cer, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2010. Phrasal: A statis-
tical machine translation toolkit for Exploring new
model features. In NAACL/HLT.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In ACL.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL/HLT.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
John DeNero, Shankar Kumar, Ciprian Chelba, and
Franz Och. 2010. Model combination for machine
translation. In NAACL/HLT.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: AutomaticMetric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation.
J.G. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recognizer output voting
error reduction (ROVER). In ASRU.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
NAACL/HLT.
J. Goldstein, V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In ANLP/NAACL
Workshop on Automatic Summarization.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D. Manning. 2013. Fast and adaptive online
training of feature-rich translation models. In (to ap-
pear) ACL.
Keith Hall, Ryan McDonald, and Slav Petrov. 2011.
Training structured prediction models with extrinsic
loss functions. In Domain Adaptation Workshop at
NIPS.
Kenneth Heafield and Alon Lavie. 2010a. CMUmulti-
enginemachine translation forWMT2010. InWMT.
Kenneth Heafield and Alon Lavie. 2010b. Voting on n-
grams for machine translation system combination.
In AMTA.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Comput., 14(8):1771?1800, August.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
Fei Huang and Kishore Papineni. 2007. Hierarchi-
cal system combination for machine translation. In
EMNLP-CoNLL.
327
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
ACL/HLT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003. A novel string-to-string distancemeasure with
applications to machine translation evaluation. In
MT Summit.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL/HLT.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems. In
EMNLP/CoNLL.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In EMNLP.
Tadashi Nomoto. 2004. Multi-engine machine transla-
tion with voted language model. In ACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.
Michael Paul, Takao Doi, Youngsook Hwang, Kenji
Imamura, Hideo Okuma, and Eiichiro Sumita. 2005.
Nobody is perfect: ATR?s hybrid approach to spoken
language translation. In IWSLT.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple ma-
chine translation systems. In NAACL/HLT.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In ACL.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER?: exploring different human judgments with
a tunable MT metric. InWMT.
Bing Xiang, Xiaoqiang Luo, and Bowen Zhou. 2013.
Enlisting the ghost: Modeling empty categories for
machine translation. In ACL.
Tong Xiao, Jingbo Zhu, and Tongran Liu. 2013. Bag-
ging and boosting statistical machine translation sys-
tems. Artif. Intell., 195:496?527, February.
Yong Zhao and Xiaodong He. 2009. Using n-gram
based features for machine translation system com-
bination. In NAACL/HLT.
Simon Zwarts and Mark Dras. 2008. Choosing the
right translation: A syntactically informed classifi-
cation approach. In CoLING.
328
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 114?121,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Phrasal: A Toolkit for New Directions in Statistical Machine Translation
Spence Green, Daniel Cer, and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,danielcer,manning}@stanford.edu
Abstract
We present a new version of Phrasal, an
open-source toolkit for statistical phrase-
based machine translation. This revision
includes features that support emerging re-
search trends such as (a) tuning with large
feature sets, (b) tuning on large datasets like
the bitext, and (c) web-based interactive ma-
chine translation. A direct comparison with
Moses shows favorable results in terms of
decoding speed and tuning time.
1 Introduction
In the early part of the last decade, phrase-based ma-
chine translation (MT) (Koehn et al., 2003) emerged
as the preeminent design of statistical MT systems.
However, most systems were proprietary or closed-
source, so progress was initially constrained by
the high engineering barrier to entry into the field.
Then Moses (Koehn et al., 2007) was released.
What followed was a flowering of work on all as-
pects of the translation problem, from rule extrac-
tion to deployment issues. Other toolkits appeared
including Joshua (Post et al., 2013), Jane (Wuebker
et al., 2012), cdec (Dyer et al., 2010) and the first
version of our package, Phrasal (Cer et al., 2010), a
Java-based, open source package.
This paper presents a completely re-designed
release of Phrasal that lowers the barrier to entry
into several exciting areas of MT research. First,
Phrasal exposes a simple yet flexible feature API for
building large-scale, feature-rich systems. Second,
Phrasal provides multi-threaded decoding and on-
line tuning for learning feature-rich models on very
large datasets, including the bitext. Third, Phrasal
supplies the key ingredients for web-based, inter-
active MT: an asynchronous RESTful JSON web
service implemented as a J2EE servlet, integrated
pre- and post-processing, and fast search.
Revisions to Phrasal were guided by several de-
sign choices. First, we optimized the system for
multi-core architectures, eschewing distributed in-
frastructure like Hadoop and MapReduce. While
?scaling-out? with distributed infrastructure is the
conventional industry and academic choice, we find
that ?scaling-up? on a single large-node is an at-
tractive yet overlooked alternative (Appuswamy et
al., 2013). A single ?scale-up? node is usually
competitive in terms of cost and performance, and
multi-core code has fewer dependencies in terms
of software and expertise. Second, Phrasal makes
extensive use of Java interfaces and reflection. This
is especially helpful in the feature API. A feature
function can be added to the system by simply im-
plementing an interface and specifying the class
name on the decoder command line. There is no
need to modify or recompile anything other than
the new feature function.
This paper presents a direct comparison of
Phrasal and Moses that shows favorable results
in terms of decoding speed and tuning time. An
indirect comparison via the WMT2014 shared
task (Neidert et al., 2014) showed that Phrasal
compares favorably to Moses in an evaluation
setting. The source code is freely available at:
http://nlp.stanford.edu/software/phrasal/
2 Standard System Pipeline
This section describes the steps required to build
a phrase-based MT system from raw text. Each
step is implemented as a stand-alone executable.
For convenience, the Phrasal distribution includes
a script that coordinates the steps.
2.1 Prerequisites
Phrasal assumes offline preparation of word align-
ments and at least one target-side language model.
Word Alignment The rule extractor can accom-
modate either unsymmetrized or symmetrized
alignments. Unsymmetrized alignments can be
produced with either GIZA++ or the Berkeley
Aligner (Liang et al., 2006). Phrasal then applies
symmetrization on-the-fly using heuristics such as
grow-diag or grow-diag-final. If the alignments are
symmetrized separately, then Phrasal accepts align-
114
ments in the i-j Pharaoh format, which indicates
that source token i is aligned to target token j.
Language Modeling Phrasal can load any n-
gram language model saved in the ARPA format.
There are two LM loaders. The Java-based loader is
used by default and is appropriate for small-scale ex-
periments and pure-Java environments. The C++
KenLM (Heafield, 2011) loader
1
is best for large-
scale LMs such as the unfiltered models produced
by lmplz (Heafield et al., 2013). Profiling shows
that LM queries often account for more than 50% of
the CPU time in a Phrasal decoding run, so we de-
signed the Phrasal KenLM loader to execute queries
mostly in C++ for efficiency. The KenLM bind-
ing efficiently passes full strings to C++ via JNI.
KenLM then iterates over the string, returning a
score and a state length. Phrasal can load multiple
language models, and includes native support for
the class-based language models that have become
popular in recent evaluations (Wuebker et al., 2012;
Ammar et al., 2013; Durrani et al., 2013).
2.2 Rule Extraction
The next step in the pipeline is extraction of a phrase
table. Phrasal includes a multi-threaded version
of the rule extraction algorithm of Och and Ney
(2004). Phrase tables can be filtered to a specific
data set?as is common in research environments.
When filtering, the rule extractor lowers memory
utilization by splitting the data into arbitrary-sized
chunks and extracting rules from each chunk.
The rule extractor includes a feature API that is
independent of the decoder feature API. This al-
lows for storage of static rule feature values in the
phrase table. Static rule features are useful in two
cases. First, if a feature value depends on bitext
statistics, which are not accessible during tuning
or decoding, then that feature should be stored in
the phrase table. Examples are the standard phrase
translation probabilities, and the dense rule count
and rule uniqueness indicators described by Green
et al. (2013). Second, if a feature depends only
on the rule and is unlikely to change, then it may
be more efficient to store that feature value in the
phrase table. An example is a feature template that
indicates inclusion in a specific data domain (Dur-
rani et al., 2013). Rule extractor feature templates
must implement the FeatureExtractor inter-
face and are loaded via reflection.
1
Invoked by prefixing the LM path with the ?kenlm:?.
The rule extractor can also create lexicalized re-
ordering tables. The standard phrase orientation
model (Tillmann, 2004) and the hierarchical model
of Galley and Manning (2008) are available.
2.3 Tuning
Once a language model has been estimated and a
phrase table has been extracted, the next step is to
estimate model weights. Phrasal supports tuning
over n-best lists, which permits rapid experimenta-
tion with different error metrics and loss functions.
Lattice-based tuning, while in principle more pow-
erful, requires metrics and losses that factor over
lattices, and in practice works no better than n-best
tuning (Cherry and Foster, 2012).
Tuning requires a parallel set {(f
t
, e
t
)}
T
t=1
of
source sentences f
t
and target references e
t
.
2
Phrasal follows the log-linear approach to phrase-
based translation (Och and Ney, 2004) in which
the predictive translation distribution p(e|f ;w) is
modeled directly as
p(e|f ;w) =
1
Z(f)
exp
[
w
>
?(e, f)
]
(1)
where w ? R
d
is the vector of model parameters,
?(?) ? R
d
is a feature map, and Z(f) is an appro-
priate normalizing constant.
MT differs from other machine learning settings
in that it is not common to tune to log-likelihood
under (1). Instead, a gold error metric G(e
?
, e) is
chosen that specifies the similarity between a hy-
pothesis e
?
and a reference e, and that error is min-
imized over the tuning set. Phrasal includes Java
implementations of BLEU (Papineni et al., 2002),
NIST, and WER, and bindings for TER (Snover et
al., 2006) and METEOR (Denkowski and Lavie,
2011). The error metric is incorporated into a loss
function ` that returns the loss at either the sentence-
or corpus- level.
For conventional corpus-level (batch) tuning,
Phrasal includes multi-threaded implementations
of MERT (Och, 2003) and PRO (Hopkins and
May, 2011). The MERT implementation uses the
line search of Cer et al. (2008) to directly min-
imize corpus-level error. The PRO implementa-
tion uses a pairwise logistic loss to minimize the
number of inversions in the ranked n-best lists.
These batch implementations accumulate n-best
lists across epochs.
2
For simplicity, we assume one reference, but the multi-
reference case is analogous.
115
Online tuning is faster and more scalable than
batch tuning, and sometimes leads to better solu-
tions for non-convex settings like MT (Bottou and
Bousquet, 2011). Weight updates are performed
after each tuning example is decoded, and n-best
lists are not accumulated. Consequently, online tun-
ing is preferable for large tuning sets, or for rapid
iteration during development. Phrasal includes the
AdaGrad-based (Duchi et al., 2011) tuner of Green
et al. (2013). The regularization options are L
2
,
efficient L
1
for feature selection (Duchi and Singer,
2009), or L
1
+ L
2
(elastic net). There are two on-
line loss functions: a pairwise (PRO) objective and
a listwise minimum expected error objective (Och,
2003). These online loss functions require sentence-
level error metrics, several of which are available in
the toolkit: BLEU+1 (Lin and Och, 2004), Nakov
BLEU (Nakov et al., 2012), and TER.
2.4 Decoding
The Phrasal decoder can be invoked either program-
matically as a Java object or as a standalone appli-
cation. In both cases the decoder is configured via
options that specify the language model, phrase
table, weight vector w, etc. The decoder is multi-
threaded, with one decoding instance per thread.
Each decoding instance has its own weight vector,
so in the programmatic case, it is possible to decode
simultaneously under different weight vectors.
Two search procedures are included. The default
is the phrase-based variant of cube pruning (Huang
and Chiang, 2007). The standard multi-stack beam
search (Och and Ney, 2004) is also an option. Ei-
ther procedure can be configured in one of several
recombination modes. The ?Pharaoh? mode only
considers linear distortion, source coverage, and
target LM history. The ?Exact? mode considers
these states in addition to any feature that declares
recombination state (see section 3.3).
The decoder includes several options for deploy-
ment environments such as an unknown word API,
pre-/post-processing APIs, and both full and prefix-
based force decoding.
2.5 Evaluation and Post-processing
All of the error metrics available for tuning can
also be invoked for evaluation. For significance
testing, the toolkit includes an implementation of
the permutation test of Riezler and Maxwell (2005),
which was shown to be less susceptible to Type-I
error than bootstrap re-sampling (Koehn, 2004).
r : s(r,w)
r ? R axiom
d : w(d) r : s(r,w)
d
?
: s(d
?
,w)
r /? cov(d) item
|cov(d)| = |s| goal
Table 1: Phrase-based MT as deductive inference.
This notation can be read as follows: if the an-
tecedents on the top are true, then the consequent
on the bottom is true subject to the conditions on
the right. The new item d
?
is creating by appending
r to the ordered sequence of rules that define d.
Phrasal also includes two truecasing packages.
The LM-based truecaser (Lita et al., 2003) requires
an LM estimated from cased, tokenized text. A
subsequent detokenization step is thus necessary. A
more convenient alternative is the CRF-based post-
processor that can be trained to invert an arbitrary
pre-processor. This post-processor can perform
truecasing and detokenization in one pass.
3 Feature API
Phrasal supports dynamic feature extraction dur-
ing tuning and decoding. In the API, feature tem-
plates are called featurizers. There are two types
with associated interfaces: RuleFeaturizer
and DerivationFeaturizer. One way to il-
lustrate these two featurizers is to consider phrase-
based decoding as a deductive system. Let r =
?f, e? be a rule in a set R, which is conventionally
called the phrase table. Let d = {r
i
}
N
i=1
be an
ordered sequence of derivation N rules called a
derivation, which specifies a translation for some
source input sequence s (which, by some abuse of
notation, is equivalent to f in Eq. (1)). Finally,
define functions cov(d) as the source coverage set
of d as a bit vector and s(?, w) as the score of a rule
or derivation under w.
3
The expression r /? cov(d)
means that r maps to an empty/uncovered span in
cov(d). Table 1 shows the deductive system.
3.1 Dynamic Rule Features
RuleFeaturizers are invoked when scoring axioms,
which do not require any derivation context. The
static rule features described in section 2.2 also
contribute to axiom scoring, and differ only from
RuleFeaturizers in that they are stored permanently
in the phrase table. In contrast, RuleFeaturizers
3
Note that s(d,w) = w
>
?(d) in the log-linear formulation
of MT (see Eq. (1)).
116
Listing 1: A RuleFeaturizer, which depends
only on a translation rule.
public class WordPenaltyFeaturizer
implements RuleFeaturizer {
@Override
public List<FeatureValue>
ruleFeaturize(Featurizable f) {
List<FeatureValue> features =
Generics.newLinkedList();
// Extract single feature
features.add(new FeatureValue(
"WordPenalty", f.targetPhrase.size()));
return features;
}
}
are extracted during decoding. An example feature
template is the word penalty, which is simply the
dimension of the target side of r (Listing 1).
Featurizable wraps decoder state from
which features can be extracted. RuleFeaturizers
are extracted during each phrase table query and
cached, so they can be simply efficiently retrieved
during decoding.
Once the feature is compiled, it is simply speci-
fied on the command-line when the decoder is exe-
cuted. No other configuration is required.
3.2 Derivation Features
DerivationFeaturizers are invoked when scoring
items, and thus depend on some derivation context.
An example is the LM, which requires the n-gram
context from d to score r when creating the new
hypothesis d
?
(Listing 2).
The LM featurizer first looks up the recombi-
nation state of the derivation, which contains the
n-gram context. Then it queries the LM by passing
the rule and context, and sets the new state as the
result of the LM query. Finally, it returns a feature
?LM? with the value of the LM query.
3.3 Recombination State
Listing 2 shows a state lookup during feature ex-
traction. Phrase-based MT feature design differs
significantly from that of convex classifiers in terms
of the interaction with inference. For example, in
a maximum entropy classifier inference is exact,
so a good optimizer can simply nullify bad fea-
tures to retain baseline accuracy. In contrast, MT
feature templates affect search through both future
cost heuristics and recombination state. Bad fea-
tures can introduce search errors and thus decrease
Listing 2: A DerivationFeaturizer, which
must lookup and save recombination state for ex-
traction.
public class NGramLanguageModelFeaturizer
extends DerivationFeaturizer {
@Override
public List<FeatureValue> featurize(
Featurizable f) {
// Get recombination state
LMState priorState = f.prior.getState(this);
// LM query
LMState state = lm.score(f.targetPhrase, priorState);
List<FeatureValue> features =
Generics.newLinkedList();
// Extract single feature
features.add(
new FeatureValue("LM", state.getScore()));
// Set new recombination state
f.setState(this, state);
return features;
}
}
accuracy, sometimes catastrophically.
The feature API allows DerivationFeaturizers
to explicitly declare recombination state via the
FeaturizerState interface.4 The interface re-
quires a state equality operator and a hash code
function. Then the search procedure will only re-
combine derivations with equal states. For example,
the state of the n-gram LM DerivationFeaturizer
(Listing 2) is the n-1 gram context, and the hash-
code is a hash of that context string. Only deriva-
tions for which the equality operator of LMState
returns true can be recombined.
4 Web Service
Machine translation output is increasingly uti-
lized in computer-assisted translation (CAT) work-
benches. To support deployment, Phrasal includes
a lightweight J2EE servlet that exposes a REST-
ful JSON API for querying a trained system. The
toolkit includes a standalone servlet container, but
the servlet may also be incorporated into a J2EE
server. The servlet requires just one input param-
eter: the Phrasal configuration file, which is also
used for tuning and decoding. Consequently, after
running the standard pipeline, the trained system
can be deployed with one command.
4
To control future cost estimation, the designer would need
to write a new heuristic that considers perhaps a subset of
the full feature map. There is a separate API for future cost
heuristics.
117
4.1 Standard Web Service
The standard web service supports two types of
requests. The first is TranslationRequest,
which performs full decoding on a source input.
The JSON message structure is:
Listing 3: TranslationRequest message.
TranslationRequest {
srcLang :(string),
tgtLang :(string),
srcText :(string),
tgtText :(string),
limit :(integer),
properties :(object)
}
The srcLang and tgtLang fields are ignored by
the servlet, but can be used by a middleware proxy
to route requests to Phrasal servlet instances, one
per language pair. The srcText field is the source
input, and properties is a Javascript associa-
tive array that can contain key/value pairs to pass
to the feature API. For example, we often use the
properties field to pass domain information
with each request.
Phrasal will perform full decoding and respond
with the message:
Listing 4: TranslationReply message,
which is returned upon successful processing of
TranslationRequest.
TranslationReply {
resultList :[
{tgtText :(string),
align :(string),
score :(float)
},...]
}
resultList is a ranked n-best list of transla-
tions, each with target tokens, word alignments,
and a score.
The second request type is RuleRequest,
which enables phrase table queries. These requests
are processed very quickly since decoding is not
required. The JSON message structure is:
Listing 5: RuleRequest message, which
prompts a direct lookup into the phrase table.
RuleRequest {
srcLang :(string),
tgtLang :(string),
srcText :(string),
limit :(integer),
properties :(object)
}
limit is the maximum number of translations to
return. The response message is analogous to that
for TranslationRequest, so we omit it.
4.2 Interactive Machine Translation
Interactive machine translation (Bisbey and Kay,
1972) pairs human and machine translators in hopes
of increasing the throughput of high quality trans-
lation. It is an old idea that is again in focus. One
challenge is to present relevant machine suggestions
to humans. To that end, Phrasal supports context-
sensitive translation queries via prefix decod-
ing. Consider again the TranslationRequest
message. When the tgtText field is empty, the
source input is decoded from scratch. But when
this field contains a prefix, Phrasal returns transla-
tions that begin with the prefix. The search proce-
dure force decodes the prefix, and then completes
the translation via conventional decoding. Conse-
quently, if the user has typed a partial translation,
Phrasal can suggest completions conditioned on
that prefix. The longer the prefix, the faster the de-
coding, since the user prefix constrains the search
space. This feature allows Phrasal to produce in-
creasingly precise suggestions as the user works.
5 Experiments
We compare Phrasal and Moses by restricting an
existing large-scale system to a set of common fea-
tures. We start with the Arabic?English system of
Green et al. (2014), which is built from 6.6M paral-
lel segments. The system includes a 5-gram English
LM estimated from the target-side of the bitext and
990M English monolingual tokens. The feature set
is their dense baseline, but without lexicalized re-
ordering and the two extended phrase table features.
This leaves the nine baseline features also imple-
mented by Moses. We use the same phrase table,
phrase table query limit (20), and distortion limit
(5) for both decoders. The tuning set (mt023568)
contains 5,604 segments, and the development set
(mt04) contains 1,075 segments.
We ran all experiments on a dedicated server with
16 physical cores and 128GB of memory.
Figure 1 shows single-threaded decoding time
of the dev set as a function of the cube pruning
pop limit. At very low limits Moses is faster than
Phrasal, but then slows sharply. In contrast, Phrasal
scales linearly and is thus faster at higher pop limits.
Figure 2 shows multi-threaded decoding time of
the dev set with the cube pruning pop limit fixed
at 1,200. Here Phrasal is initially faster, but Moses
becomes more efficient at four threads. There are
two possible explanations. First, profiling shows
that LM queries account for approximately 75%
118
l
l
l
l l
l
l l
l
l
l l
l l
l l
l
l
l
l100200 500 1000 1500 2000Pop LimitTime (seconds) Systeml PhrasalMoses
Figure 1: Development set decoding time as a
function of the cube pruning pop limit.
of the Phrasal CPU-time. KenLM is written in
C++, and Phrasal queries it via JNI. It appears
as though multi-threading across this boundary is
a source of inefficiency. Second, we observe that
the Java parallel garbage collector (GC) runs up to
seven threads, which become increasingly active
as the number of decoder threads increases. These
and other Java overhead threads must be scheduled,
limiting gains as the number of decoding threads
approaches the number of physical cores.
Finally, Figure 3 shows tuning BLEU as a func-
tion of wallclock time. For Moses we chose the
batch MIRA implementation of Cherry and Fos-
ter (2012), which is popular for tuning feature-rich
systems. Phrasal uses the online tuner with the ex-
pected BLEU objective (Green et al., 2014). Moses
achieves a maximum BLEU score of 47.63 after
143 minutes of tuning, while Phrasal reaches this
level after just 17 minutes, later reaching a maxi-
mum BLEU of 47.75 after 42 minutes. Much of
the speedup can be attributed to phrase table and
LM loading time: the Phrasal tuner loads these data
structures just once, while the Moses tuner loads
them every epoch. Of course, this loading time be-
comes more significant with larger-scale systems.
6 Conclusion
We presented a revised version of Phrasal, an open-
source, phrase-based MT toolkit. The revisions
support new directions in MT research including
feature-rich models, large-scale tuning, and web-
l
l
l
l
l
l l l l l l l l l l l
50100150200 4 8 12 16ThreadsTime (seconds) Systeml PhrasalMoses
Figure 2: Development set decoding time as a
function of the threadpool size.
l
l
llll
lllllllllllllllllll45464748 0 100 200Time (minutes)Approx. BLEU?4 Systeml PhrasalMoses
Figure 3: Approximate BLEU-4 during tuning
as a function of time over 25 tuning epochs. The
horizontal axis is accumulated time, while each
point indicates BLEU at the end of an epoch.
based interactive MT. A direct comparison with
Moses showed favorable performance on a large-
scale translation system.
Acknowledgments We thank Michel Galley for previous con-
tributions to Phrasal. The first author is supported by aNational
Science Foundation Graduate Research Fellowship. This work
was supported by the Defense Advanced Research Projects
Agency (DARPA) Broad Operational Language Translation
(BOLT) program through IBM. Any opinions, findings, and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the view
of DARPA or the US government.
119
References
W. Ammar, V. Chahuneau, M. Denkowski, G. Hanne-
man, W. Ling, A. Matthews, et al. 2013. The CMU
machine translation systems at WMT 2013: Syntax,
synthetic translation options, and pseudo-references.
In WMT.
R. Appuswamy, C. Gkantsidis, D. Narayanan, O. Hod-
son, and A. Rowstron. 2013. Nobody ever got fired
for buying a cluster. Technical report, Microsoft Cor-
poration, MSR-TR-2013-2.
R. Bisbey and Kay. 1972. The MIND translation sys-
tem: a study in man-machine collaboration. Techni-
cal Report P-4786, Rand Corp., March.
L. Bottou and O. Bousquet. 2011. The tradeoffs of
large scale learning. In Optimization for Machine
Learning, pages 351?368. MIT Press.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regu-
larization and search for minimum error rate training.
In WMT.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
M. Denkowski and A. Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation
of machine translation systems. In WMT.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
N. Durrani, B. Haddow, K. Heafield, and P. Koehn.
2013. Edinburgh?s machine translation systems for
European language pairs. In WMT.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al. 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014. An em-
pirical comparison of features and tuning for phrase-
based machine translation. In WMT.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In EMNLP.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
J. Neidert, S. Schuster, S. Green, K. Heafield, and C. D.
Manning. 2014. Stanford University?s submissions
to the WMT 2014 translation task. In WMT.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
M. Post, J. Ganitkevitch, L. Orland, J. Weese, Y. Cao,
and C. Callison-Burch. 2013. Joshua 5.0: Sparser,
better, faster, server. In WMT.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
120
C. Tillmann. 2004. A unigram orientation model for
statistical machine translation. In NAACL.
J. Wuebker, M. Huck, S. Peitz, M. Nuhn, M. Freitag,
J. T. Peter, S. Mansour, and H. Ney. 2012. Jane 2:
Open source phrase-based and hierarchical statisti-
cal machine translation. InCOLING:Demonstration
Papers.
121
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 466?476,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
An Empirical Comparison of Features and Tuning
for Phrase-based Machine Translation
Spence Green, Daniel Cer, and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,danielcer,manning}@stanford.edu
Abstract
Scalable discriminative training methods
are now broadly available for estimating
phrase-based, feature-rich translation mod-
els. However, the sparse feature sets typi-
cally appearing in research evaluations are
less attractive than standard dense features
such as language and translation model
probabilities: they often overfit, do not gen-
eralize, or require complex and slow fea-
ture extractors. This paper introduces ex-
tended features, which are more specific
than dense features yet more general than
lexicalized sparse features. Large-scale ex-
periments show that extended features yield
robust BLEU gains for both Arabic-English
(+1.05) and Chinese-English (+0.67) rel-
ative to a strong feature-rich baseline. We
also specialize the feature set to specific
data domains, identify an objective function
that is less prone to overfitting, and release
fast, scalable, and language-independent
tools for implementing the features.
1 Introduction
Scalable discriminative algorithm design for ma-
chine translation (MT) has lately been a booming
enterprise. There are now algorithms for every taste:
probabilistic and distribution-free, online and batch,
regularized and unregularized. Technical differ-
ences aside, the papers that apply these algorithms
to phrase-based translation often share a curious
empirical characteristic: the algorithms support ex-
tra features, but the features do not significantly
improve translation. For example, Hopkins and
May (2011) showed that PRO with some simple ad
hoc features only exceeds the baseline on one of
three language pairs. Gimpel and Smith (2012b)
observed a similar result for both PRO and their
ramp-loss algorithm. Cherry and Foster (2012)
found that, at least in the batch case, many algo-
rithms produce similar results, and features only
significantly increased quality for one of three lan-
guage pairs. Only recently did Cherry (2013) and
Green et al. (2013b) identify certain features that
consistently reduce error.
These empirical results suggest that feature de-
sign and model fitting, the subjects of this paper,
warrant a closer look. We introduce an effective
extended feature set for phrase-based MT and iden-
tify a loss function that is less prone to overfitting.
Extended features share three attractive characteris-
tics with the standard Moses dense features (Koehn
et al., 2007): ease of implementation, language in-
dependence, and independence from ancillary cor-
pora like treebanks. In our experiments, they do
not overfit and can be extracted efficiently during
decoding. Because all feature weights are tuned
on the development set, the new feature templates
are amenable to feature augmentation (Daum? III,
2007), a simple domain adaptation technique that
we show works surprisingly well for MT.
Extended features are designed according to a
principle rather than a rule: they should fire less
than standard dense features, which are general, but
more than so-called sparse features, which are very
specific?they are usually lexicalized?and thus
prone to overfitting. This principle is motivated
by analysis, which shows how expressive models
can be a mixed blessing in the translation setting.
It is obvious that features allow the model to fit
the tuning data more tightly. For example, sparse
lexicalized features could reduce tuning error by
learning that the references prefer U.S. over United
States, a minor lexical distinction. Reference choice
should matter more than in the dense case, an issue
that we quantify. We also show that frequency cut-
offs, which are a crude but common form of feature
selection, are unnecessary and even detrimental
when features follow this principle.
We report large-scale translation quality experi-
ments relative to both dense and feature-rich base-
lines. Our best feature set, which includes domain
adaptation features, yields an average+1.05 BLEU
improvement for Arabic-English and +0.67 for
466
Chinese-English. In addition to the extended fea-
ture set, we show that an online variant of expected
error (Och, 2003) is significantly faster to compute,
less prone to overfitting, and nearly as effective as a
pairwise loss. We release all software?feature ex-
tractors, and fast word clustering and data selection
packages?used in our experiments.
1
2 Phrase-based Models and Learning
The log-linear approach to phrase-based translation
(Och and Ney, 2004) directly models the predictive
translation distribution
p(e|f ;w) =
1
Z(f)
exp
[
w
>
?(e, f)
]
(1)
where e is the target string, f is the source string,
w ? R
d
is the vector of model parameters, ?(?) ?
R
d
is a feature map, and Z(f) is an appropriate
normalizing constant. Assume that there is also a
function ?(e, f) ? R
d
that produces a recombina-
tion map for the features. That is, each coordinate
in ? represents the state of the corresponding co-
ordinate in ?. For example, suppose that ?
j
is the
log probability produced by the n-gram language
model (LM). Then ?
j
would be the appropriate LM
history. Recall that recombination collapses deriva-
tions with equivalent recombination maps during
search and thus affects learning. This issue signifi-
cantly influences feature design.
To learn w, we follow the online procedure of
Green et al. (2013b), who calculate gradient steps
with AdaGrad (Duchi et al., 2011) and perform fea-
ture selection via L
1
regularization in the FOBOS
(Duchi and Singer, 2009) framework. This proce-
dure accommodates any loss function for which a
subgradient can be computed. Green et al. (2013b)
used a PRO objective (Hopkins and May, 2011)
with a logistic (surrogate) loss function. However,
later results showed overfitting (Green et al., 2013a),
and we found that their online variant of PRO tends
to produce short translations like its batch counter-
part (Nakov et al., 2013). Moreover, PRO requires
sampling, making it slow to compute.
To address these shortcomings, we explore an
online variant of expected error (Och, 2003, Eq.7).
Let E
t
= {e
i
}
n
i=1
be a scored n-best list of trans-
lations at time step t for source input f
t
. Let G(e)
be a gold error metric that evaluates each candi-
date translation with respect to a set of one or more
1http://nlp.stanford.edu/software/phrasal
references. The smooth loss function is
`
t
(w
t?1
) = E
p(e|f
t
;w
t?1
)
[G(e)]
=
1
Z
?
e
?
?E
t
exp
(
w
>
?(e
?
, f)
)
?G(e
?
)
(2)
with normalization constant Z =
?
e
?
?E
t
exp
(
w
>
?(e
?
, f)
)
. The gradient g
t
for coordinate j is:
g
t
= E[G(e)?
j
(e, f
t
)]?
E[G(e)]E[?
j
(e, f
t
)] (3)
To our knowledge, we are the first to experiment
with the online version of this loss.
2
When G(e) is
sentence-level BLEU+1 (Lin and Och, 2004)?the
setting in our experiments?this loss is also known
as expected BLEU (Cherry and Foster, 2012). How-
ever, other metrics are possible.
3 Extended Phrase-based Features
We divide our feature templates into five categories,
which are well-known sources of error in phrase-
based translation. The features are defined over
derivations d = {r
i
}
D
i=1
, which are ordered se-
quences of rules r from the translation model. De-
fine functions f(?) to be the source string of a rule
or derivation and e(?) to be the target string. Local
features can be extracted from individual rules and
do not declare any state in the recombination map,
thus for all local features i we have ?
i
= 0. Non-
local features are defined over partial derivations
and declare some state, either a real-valued param-
eter or an index indicating a categorical value like
an n-gram context.
For each language, the extended feature tem-
plates require unigram counts and a word-to-class
mapping ? : w 7? c for word w ? V and class
c ? C. These can be extracted from any monolin-
gual data; our experiments simply use both sides of
the unaligned parallel training data.
The features are language-independent, but we
will use Arabic-English as a running example.
3.1 Lexical Choice
Lexical choice features make more specific distinc-
tions between target words than the dense transla-
tion model features (Koehn et al., 2003).
2
Gao and He (2013) used stochastic gradient descent and
expected BLEU to learn phrase table feature weights, but not
the full translation model w.
467
Lexicalized rule indicator (Liang et al., 2006a)
Some rules occur frequently enough that we can
learn rule-specific weights that augment the dense
translation model features. For example, our model
learns the following rule indicator features and
weights:
H
.
AJ
.
?

@? reasons -0.022
H
.
AJ
.
?

@? reasons for 0.002
H
.
AJ
.
?

@? the reasons for 0.016
These translations are all correct depending on con-
text. When the plural noun H
.
AJ
.
?

@ ?reasons? appears
in a construct state (iDafa) the preposition for is
unrealized. Moreover, depending on the context,
the English translation might also require the deter-
miner the, which is also unrealized. The weights
reflect that H
.
AJ
.
?

@ ?reasons? often appears in con-
struct and boost insertion of necessary target terms.
To prevent overfitting, this template only fires an
indicator for rules that occur more than 50 times
in the parallel training data (this is different from
frequency filtering on the tuning data; see section
6.1). The feature is local.
Class-based rule indicator Word classes ab-
stract over lexical items. For each rule r, a pro-
totype that abstracts over many rules can be built
by concatenating {?(w) : w ? f(r)} with
{?(w) : w ? e(r)}. For example, suppose
that Arabic class 492 consists primarily of Arabic
present tense verbs and class 59 contains English
auxiliaries. Then the model might penalize a rule
prototype like 492>59_59, which drops the verb.
This template fires an indicator for each rule proto-
type and is local.
Target unigram class (Ammar et al., 2013) Tar-
get lexical items with similar syntactic and semantic
properties may have very different frequencies in
the training data. These frequencies will influence
the dense features. For example, in one of our En-
glish class mappings the following words map to
the same class:
word class freq.
surface-to-surface 0 269
air-to-air 0 98
ground-to-air 0 63
The classes capture common linguistic attributes of
these words, which is the motivation for a full class-
based LM. Learning unigram weights directly is
surprisingly effective and does not require building
another LM. This template fires a separate indicator
for each class {?(w) : w ? e(r)} and is local.
3.2 Word Alignments
Word alignment features allow the model to recog-
nize fine-grained phrase-internal information that
is largely opaque in the dense model.
Lexicalized alignments (Liang et al., 2006a)
Consider the internal alignments of the rule:
sunday ,
??K


1
YgB@ 2
Alignment 1 ???K


?day?? ,? is incorrect and align-
ment 2 is correct. The dense translation model
features might assign this rule high probability if
alignment 1 is a common alignment error. Lexical-
ized alignment features allow the model to compen-
sate for these events. This feature fires an indicator
for each alignment in a rule?including multiword
cliques?and is local.
Class-based alignments Like the class-based
rule indicator, this feature template replaces each
lexical itemwith its word class, resulting in an align-
ment prototype. This feature fires an indicator for
each alignment in a rule after mapping lexical items
to classes. It is local.
Source class deletion Phrase extraction algo-
rithms often use a ?grow? symmetrization step (Och
and Ney, 2003) to add alignment points. Sometimes
this procedure can produce a rule that deletes im-
portant source content words. This feature template
allows the model to penalize these rules by firing
an indicator for the class of each unaligned source
word. The feature is local.
Punctuation ratio Languages use different types
and ratios of punctuation (Salton, 1958). For ex-
ample, quotation marks are not commonly used in
Arabic, but they are conventional in English. Fur-
thermore, spurious alignments often contain punc-
tuation. To control these two phenomena, this fea-
ture template returns the ratio of target punctuation
tokens to source punctuation tokens for each deriva-
tion. Since the denominator is constant, this feature
can be computed incrementally as a derivation is
constructed. It is local.
Function word ratio Words can also be spuri-
ously aligned to non-punctuation, non-digit func-
tion words such as determiners and particles. Fur-
thermore, linguistic differences may account for
468
differences in function word occurrences. For ex-
ample, English has a broad array of modal verbs
and auxiliaries not found in Arabic. This feature
template takes the 25 most frequent words in each
language (according to the unigram counts), and
computes the ratio between target and source func-
tion words for each derivation. As before the de-
nominator is constant, so the feature can be com-
puted efficiently. It is local.
3.3 Phrase Boundaries
The LM and hierarchical reordering model are the
only dense features that cross phrase boundaries.
Target-class bigramboundary Wehave already
added target class unigrams. We find that both lexi-
calized and class-based bigrams cause overfitting,
therefore we restrict to bigrams that straddle phrase
boundaries. The feature template fires an indicator
for the concatenation of the word classes on either
side of each boundary. This feature is non-local
and its recombination state ? is the word class at
the right edge of the partial derivation.
3.4 Derivation Quality
To satisfy strong features like the LM, or hard con-
straints like the distortion limit, the phrase-based
model can build derivations from poor translation
rules. For example, a derivation consisting mostly
of unigram rules may miss idiomatic usage that
larger rules can capture. All of these feature tem-
plates are local.
Source dimension (Hopkins and May, 2011) An
indicator feature for the source dimension of the
rule: |f(r)|.
Target dimension (Hopkins and May, 2011) An
indicator for the target dimension: |e(r)|.
Rule shape (Hopkins and May, 2011) The
conjunction of source and target dimension:
|f(r)|_|e(r)|.
3.5 Reordering
Lexicalized reordering models score the orientation
of a rule in an alignment grid. We use the same
baseline feature extractor as Moses, which has three
classes: monotone, swap, and discontinuous. We
also add the non-monotone class, which is a con-
junction of swap and discontinuous, for a total of
eight orientations.
3
3
Each class has ?with-previous? and ?with-next? special-
izations.
Algorithm (implementation) #threads Time
Brown (wcluster) 1 1023.39
Clark (cluster_neyessen) 1 890.11
Och (mkcls) 1 199.04
PredictiveFull (this paper) 8 3.27
Predictive (this paper) 8 2.42
Table 1: Wallclock time (min.sec) to generate a
mapping from a vocabulary of 63k English words
(3.7M tokens) to 512 classes. All experiments were
run on the same server, which had eight physical
cores. Our Java implementation is multi-threaded;
the C++ baselines are single-threaded.
Lexicalized rule orientation (Liang et al.,
2006a) For each rule, the template fires an indi-
cator for the concatenation of the orientation class,
each element in f(r), and each element in e(r). To
prevent overfitting, this template only fires for rules
that occur more than 50 times in the training data.
The feature is non-local and its recombination state
? is the rule orientation.
Class-based rule orientation For each rule, the
template fires an indicator for the concatenation
of the orientation class, each element in {?(w) :
w ? f(r)}, and each element in {?(w) : w ?
e(r)}. The feature is non-local and its recombina-
tion state ? is the rule orientation.
Signed linear distortion The dense feature set
includes a simple reordering cost model. Assume
that [r] returns the index of the leftmost source index
in f(d) and [[r]] returns the rightmost index. Then
the linear distortion is:
? = [r
1
] +
D
?
i=2
|[[r
i?1
]] + 1? [r
i
]| (4)
This score does not distinguish between left and
right distortion. To correct this issue, this feature
template fires an indicator for each signed com-
ponent in the sum, for each positive and negative
component. The feature is non-local and its recom-
bination state ? is the signed distortion.
3.6 Feature Dependencies
While unigram counts are trivial to compute, the
same is not necessarily true of the word-to-class
mapping ?. Standard algorithms run in O(n
2
),
where n = |V |. Table 1 shows an evaluation of
standard implementations of several popular algo-
rithms: Brown et al. (1992) implemented by Liang
469
(2005); Clark (2003) without the morphological
prior, which increases training time dramatically;
and the implementation of Och (1999) that comes
with the GIZA++ word aligner. The latter has
been used recently for MT features (Ammar et al.,
2013; Cherry, 2013; Yu et al., 2013). In a broad
survey, Christodoulopoulos et al. (2010) found that
for several downstream tasks, most word clustering
algorithms?including Brown and Clark?result in
similar task accuracy. For our large-scale setting,
the primary issue is then the time to estimate ?.
For large corpora the existing implementations
may require days or weeks, making our feature set
less practical than the traditional dense MT features.
Consequently, we re-implemented the predictive
one-sided class model of Whittaker and Woodland
(2001) with the parallelized clustering algorithm of
Uszkoreit and Brants (2008) (Predictive), which
was originally developed for very large scale lan-
guage modeling. Our implementation uses multiple
threads on a single processor instead ofMapReduce.
We also added two extensions that are useful for
translation features. First, we map all digits to 0.
This reduces sparsity while retaining useful patterns
such as 0000 (e.g., years) and 0th (e.g., ordinals).
Second, we mapped all words occurring fewer than
? times to an <unk> token. In our experiment,
these two changes reduce the vocabulary size by
71.1%. They also make the mapping ? more ro-
bust to unseen events during translation decoding.
For a conservative comparison to the other three
algorithms, we include results without these two
extensions (PredictiveFull).
4
4 Domain Adaptation Features
Feature augmentation is a simple yet effective do-
main adaptation technique (Daum? III, 2007). Sup-
pose that the source data comes fromM domains.
Then for each original feature ?
i
, we addM addi-
tional features, one for each domain. The original
feature ?
i
can be interpreted as a prior over theM
domains (Finkel and Manning, 2009, fn.2).
Most of the extended features are defined over
rules, so the critical issue is how to identify in-
domain rules. The trick is to know which training
sentence pairs are in-domain. Then we can annotate
all rules extracted from these instances with domain
4
For the baselines the training settings are the suggested
defaults: Brown, default; Clark, 10 iterations, frequency cutoff
? = 5; Och, 10 iterations. Our implementation: PredictiveFull,
30 iterations, ? = 0; Predictive, 30 iterations, ? = 5.
labels. The in-domain rule sets need not be disjoint
since some rules might be useful across domains.
This paper explores the following approach: we
choose one of theM domains as the default. Next,
we collect some source sentences for each of the
M ? 1 remaining domains. Using these examples
we then identify in-domain sentence pairs in the bi-
text via data selection, in our case the feature decay
algorithm (Bi?ici and Yuret, 2011). Finally, our rule
extractor adds domain labels to all rules extracted
from each selected sentence pair. Crucially, these
labels do not influence which rules are extracted
or how they are scored. The resulting phrase table
contains the same rules, but with a few additional
annotations.
Our method assumes domain labels for each
source input to be decoded. Our experiments utilize
gold, document-level labels, but accurate sentence-
level domain classifiers exist (Wang et al., 2012).
4.1 Augmentation of Extended Features
Irvine et al. (2013) showed that lexical selection is
the most quantifiable and perhaps most common
source of error in phrase-based domain adaptation.
Our development experiments seemed to confirm
this hypothesis as augmentation of the class-based
and non-lexical (e.g., Rule shape) features did not
reduce error. Therefore, we only augment the lex-
icalized features: rule indicators and orientations,
and word alignments.
4.2 Domain-Specific Feature Templates
In-domain Rule Indicator (Durrani et al., 2013)
An indicator for each rule that matches the input do-
main. This template fires a generic in-domain indi-
cator and a domain-specific indicator (e.g., the fea-
tures might be indomain and indomain-nw).
The feature is local.
Adjacent Rule Indicator Indicators for adjacent
in-domain rules. This template also fires both
generic and domain-specific features. The feature
is non-local and the state is a boolean indicating if
the last rule in a partial derivation is in-domain.
5 Experiments
We evaluate and analyze our feature set under a vari-
ety of large-scale experimental conditions including
multiple domains and references. To our knowl-
edge, the only language pairs with sufficient re-
search resources to support this protocol are Arabic-
English (Ar-En) and Chinese-English (Zh-En). The
470
Bilingual Monolingual
#Seg. #Tok. #Tok.
Ar-En 6.6M 375M
990M
Zh-En 9.3M 538M
Table 2: Bilingual and monolingual training cor-
pora. The monolingual English data comes from
the AFP and Xinhua sections of English Gigaword
4 (LDC2009T13).
training corpora
5
come from several Linguistic
Data Consortium (LDC) sources from 2012 and
earlier (Table 2). The test, development, and tuning
corpora
6
come from the NIST OpenMT andMetric-
sMATR evaluations (Table 3). Extended features
benefit from more tuning data, so we concatenated
five NIST data sets to build one large tuning set.
Observe that all test data come from later epochs
than the tuning and development data.
From these data we built phrase-based MT sys-
tems with Phrasal (Green et al., 2014).
7
We aligned
the parallel corpora with the Berkeley aligner
(Liang et al., 2006b) with standard settings and
symmetrized via the grow-diag heuristic. We cre-
ated separate English LMs for each language pair by
concatenating the monolingual Gigaword data with
the target-side of the respective bitexts. For each
corpus we estimated unfiltered 5-gram language
models with lmplz (Heafield et al., 2013).
For each condition we ran the learning algorithm
for 25 epochs
8
and selected the model according
to the maximum uncased, corpus-level BLEU-4
(Papineni et al., 2002) score on the dev set.
5.1 Results
We evaluate the new feature set relative to two base-
lines. Dense is the same baseline as Green et al.
5
We tokenized the English with Stanford CoreNLP ac-
cording to the Penn Treebank standard (Marcus et al., 1993),
the Arabic with the Stanford Arabic segmenter (Monroe et
al., 2014) according to the Penn Arabic Treebank standard
(Maamouri et al., 2008), and the Chinese with the Stanford
Chinese segmenter (Chang et al., 2008) according to the Penn
Chinese Treebank standard (Xue et al., 2005).
6
Data sources: tune, MT023568; dev, MT04; dev-dom,
domain adaptation dev set is MT04 and all wb and bn data
from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En);
test2, Progress0809 which was revealed in the OpenMT 2012
evaluation; test3, MetricsMATR08-10.
7
System settings: distortion limit of 5, cube pruning beam
size of 1200, maximum phrase length of 7.
8
Other learning settings: 16 threads, mini-batch size of 20;
L
1
regularization strength ? = 0.001; learning rate ?
0
= 0.02;
initialization of LM to 0.5, word penalty to -1.0, and all other
dense features to 0.2; initialization of extended features to 0.0.
#Seg. #Ref. Domains
Ar-En Zh-En
tune 5,604 5,900 4 nw,wb,bn
dev 1,075 1,597 4 nw
dev-dom 2,203 2,317 1 nw,wb,bn
test1 1,313 820 4 nw,wb
test2 1,378 1,370 4 nw,wb
test3 628 613 1 nw,wb,bn
Table 3: Development, test, and tuning data. Do-
main abbreviations: broadcast news (bn), newswire
(nw), and web (wb).
(2013b); these dense features are included in all of
the models that follow. Sparse is their best feature-
rich model, which adds lexicalized rule indicators,
alignments, orientations, and source deletions with-
out bitext frequency filtering.
We do not perform a full ablation study. Both
the approximate search and the randomization of
the order of tuning instances make the contribu-
tions of each individual template differ from run to
run. Resource constraints prohibit multiple large-
scale runs for each incremental feature. Instead,
we divide the extended feature set into two parts,
and report large-scale results. Ext includes all ex-
tended features except for the the filtered lexicalized
feature templates. Ext+Filt adds those filtered
lexicalized templates: rule indicators and orienta-
tions, and word alignments (section 3).
Table 4 shows translation quality results. The
new feature set significantly exceeds the baseline
Dense model for both language pairs. An interest-
ing result is that the new extended features alone
match the strong Sparse baseline. The class-based
features, which are more general, should clearly
be preferred to the sparse features when decoding
out-of-domain data (so long as word mappings are
trained for that data). The increased runtime per
iteration comes not from feature extraction but from
larger inner products as the model size increases.
Next, we add the domain features from section
4.2. We marked in-domain sentence pairs by con-
catenating the tuning data with additional bn and
wb monolingual in-domain data from several LDC
sources.
9
The FDA selection size was set to 20
times the number of in-domain examples for each
genre. Newswire was selected as the default domain
since most of the bitext comes from that domain.
The bottom rows of Tables 4a and 4b compare
9
Catalog: LDC2007T24, LDC2008T08, LDC2008T18,
LDC2012T16, LDC2013T01, LDC2013T05, LDC2013T14.
471
Model #features Epochs Min. / Epoch tune dev test1 test2 test3
Dense (D) 18 24 3 49.52 50.25 47.98 43.41 27.56
D+Sparse 48,597 24 8 56.51 52.98 49.55 45.40 29.02
D+Ext 62,931 16 11 57.83 54.33 49.66 45.66 29.15
D+Ext+Filt 94,606 17 14 59.13 55.35 50.02 46.24 29.59
D+Ext+Filt+Dom 123,353 22 18 59.97 29.20
?
50.45 46.24 30.84
(a) Ar-En.
Model #features Epochs Min. / Epoch tune dev test1 test2 test3
Dense (D) 18 17 3 32.82 34.96 26.61 26.72 10.19
D+Sparse 55,024 17 8 38.91 36.68 27.86 28.41 10.98
D+Ext 67,936 16 13 40.96 37.19 28.27 28.40 10.72
D+Ext+Filt 100,275 17 14 41.38 37.36 28.68 28.90 11.24
D+Ext+Filt+Dom 126,014 17 14 41.70 17.20
?
28.71 28.96 11.67
(b) Zh-En.
Table 4: Translation quality results (uncased BLEU-4%). Per-epoch times are in minutes (Min.). Statistical
significance relative to D+Sparse, the strongest baseline: bold (p < 0.001) and bold-italic (p < 0.05).
Significance is computed by the permutation test of Riezler and Maxwell (2005).
?
The dev score of
Ext+Filt+Dom is the dev-dom data set from Table 3, so it is not comparable with the other rows.
Ext+Filt+Dom to the baselines and other feature
sets. The gains relative to Sparse are statistically
significant for all six test sets.
A crucial result is that with domain features accu-
racy relative to Ext+Filt never decreases: a single
domain-adapted system is effective across domains.
Irvine et al. (2013) showed that when models from
multiple domains are interpolated, scoring errors
affecting lexical selection?the model could have
generated the correct target lexical item but did
not?increase significantly. We do not observe that
behavior, at least from the perspective of BLEU.
Table 5 separates out per-domain results. The
web data appears to be the hardest domain. That is
sensible given that broadcast news transcripts are
more similar to newswire, the default domain, than
web data. Moreover, inspection of the bitext sources
revealed very little web data, so our automatic data
selection is probably less effective. Accuracy on
newswire actually increases slightly.
6 Analysis
6.1 Learning
Loss Function In a now classic empirical com-
parison of batch tuning algorithms, Cherry and Fos-
ter (2012) showed that PRO and expected BLEU
Ar-En test1 test2 test3
nw wb nw wb bn nw wb
EF 59.78 39.55 51.69 38.80 30.39 37.59 20.58
EFD 60.21 40.38 51.76 38.77 31.63 38.18 22.37
Zh-En
EF 34.56 21.94 17.38 12.07 3.04 17.42 12.83
EFD 34.87 21.82 17.96 12.66 3.01 17.74 13.80
Table 5: Per-domain results (uncased BLEU-4 %).
Here bold simply indicates the maximum in each
column. Model abbreviations: EF is Ext+Filt and
EFD is Ext+Filt+Dom.
yielded similar translation quality results. In con-
trast, Table 6a shows significant differences be-
tween these loss functions. First, expected BLEU
can be computed faster since it is linear in the n-
best list size, whereas exact computation of the PRO
objective is O(n
2
) (thus sampling is often used). It
also converges faster. Second, PRO tends to select
larger models.
10
Finally, PRO seems to overfit on
the tuning set, since there are no gains on test1.
Feature Selection A common yet crude method
of feature selection is frequency cutoffs on the
10
PRO L
1
regularization strength of ? = 0.01, above which
model size decreases but translation quality degrades.
472
Loss #epochs Min./Epoch #feat. tune test1
EB 17 14 94,606 59.13 50.02
PRO 14 25 181,542 61.20 50.09
(a) PRO vs. expected BLEU (EB) for Ext+Filt.
Feature Selection #features tune test1
L
1
94,606 59.13 50.02
Freq. cutoffs 23,617 56.84 49.79
(b) Feature selection for Ext+Filt.
Model #refs tune test1
Dense 4 49.52 47.98
Dense 1 49.34 47.78
Ext+Filt 4 59.13 50.02
Ext+Filt 1 55.39 48.88
(c) Single- vs. multiple-reference tuning.
Table 6: Ar-En learning comparisons.
tuning data. Only features that fire more than
some threshold are admitted into the feature set.
Table 6b shows that for our new feature set, L
1
regularization?which simply requires setting a reg-
ularization strength parameter?is more effective
than frequency cutoffs.
References FewMT data sets supply multiple ref-
erences. Even when they do, those references are
but a sample from a larger pool of possible trans-
lations. This observation has motivated attempts
at generating lattices of translations for evaluation
(Dreyer and Marcu, 2012; Bojar et al., 2013). But
evaluation is only part of the problem. Table 6c
shows that the Dense model, which has only a
few features to describe the data, is little affected
by the elimination of references. In contrast, the
feature-rich model degrades significantly. This may
account for the underperformance of features in
single-reference settings like WMT (Durrani et al.,
2013; Green et al., 2013a). The next section ex-
plores the impact of references further.
6.2 Reference Variance
We took the Dense Ar-En output for the dev
data, which has four references, and computed the
sentence-level BLEU+1 with respect to each refer-
ence. Figure 1a shows a point for each of the 1,075
translations. The horizontal axis is the minimum
score with respect to any reference and the verti-
cal axis is the maximum (BLEU has a maximum
value of 1.0). Ideally, from the perspective of learn-
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l ll ll
l
l
l
l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l l
ll
l
l l
l l
l
l
l
ll
l
ll
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
lll l
l
ll
l
l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
ll l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
lll
l
l
ll l
l
l
lll l
l
l
l l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l ll
ll
l l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l l
l
l
l
l l
ll
l
l
l l
l
l
l
l
l
l
l l
l
ll
l
l l
l l
l
l
l
l
l
l l
l
l
l
l
l
l
ll l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
ll
l
l
ll
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l
ll
l
l
ll
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll l
ll
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
ll
ll
l
l0255075100 0 25 50 75 100MinimumMaximum
(a) Maximum vs. minimum BLEU+1 (%)
l l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l l
l
l
l
ll
l
l
l
l
l
l
l ll
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l ll l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
lll
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l ll
l
l
l l
l
l
l
l
l
l
l
ll
l
l
l
l
l l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
ll
ll
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
lll
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l l
l
l
l
l l
l
l
l
lll
l
l
l ll
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l ll
l
l
l
ll l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
ll
l
l
l
l
l0255075100 0 25 50 75 100MaximumAll References
(b) BLEU+1 (%) according to all four references vs.
maximum
Figure 1: Reference choice analysis for Ar-En
Dense output on the dev set.
ing, the scores should cluster around the diagonal:
the references should yield similar scores. This is
hardly the case. The mean difference isM = 18.1
BLEU, with a standard deviation SD = 11.5.
Figure 1b shows the same data set, but with the
maximum on the horizontal axis and the multiple-
reference score on the vertical axis. Assuming
a constant brevity penalty, the maximum lower-
bounds themultiple-reference score since BLEU ag-
gregates n-grams across references. The multiple-
reference score is an ?easier? target since the model
has more opportunities to match n-grams.
Consider again the single-reference condition
and one of the pathological cases at the top of Fig-
ure 1a. Suppose that the low-scoring reference is
observed in the single-reference condition. The
more expressive feature-rich model has a greater
capacity to fit that reference when, under another
473
reference, it would have matched the translation
exactly and incurred a low loss.
Nakov et al. (2012) suggested extensions to
BLEU+1 that were subsequently found to improve
accuracy in the single-reference condition (Gimpel
and Smith, 2012a). Repeating the min/max calcula-
tions with the most effective extensions (according
to Gimpel and Smith (2012a)) we observe lower
variance (M = 17.32, SD = 10.68). These exten-
sions are very simple, so a more sophisticated noise
model is a promising future direction.
7 Related Work
We review work on phrase-based discriminative fea-
ture sets that influence decoder search, and domain
adaptation with features.
11
7.1 Feature Sets
Variants of some extended features are scattered
throughout previous work: unfiltered lexicalized
rule indicators and alignments (Liang et al., 2006a);
rule shape (Hopkins and May, 2011); rule orien-
tation (Liang et al., 2006b; Cherry, 2013); target
unigram class (Ammar et al., 2013). We found
that other prior features did not improve translation:
higher-order target lexical n-grams (Liang et al.,
2006a; Watanabe et al., 2007; Gimpel and Smith,
2012b), higher-order target class n-grams (Ammar
et al., 2013), target word insertion (Watanabe et al.,
2007; Chiang et al., 2009), and many other unpub-
lished ideas transmitted through received wisdom.
To our knowledge, Yu et al. (2013) were the first
to experiment with non-local (derivation) features
for phrase-based MT. They added discriminative
rule features conditioned on target context. This is
a good idea that we plan to explore. However, they
do not mention if their non-local features declare
recombination state. Our empirical experience is
that non-local features are less effective when they
do not influence recombination.
Liang et al. (2006a) proposed replacing lexical
items with supervised part-of-speech (POS) tags to
reduce sparsity. This is a natural idea that lay dor-
mant until recently. Ammar et al. (2013) incorpo-
rated unigram and bigram target class features. Yu
et al. (2013) used word classes as backoff features to
reduce overfitting. Wuebker et al. (2013) replaced
all lexical items in the bitext and monolingual data
with classes, and estimated the dense feature set.
11
Space limitations preclude discussion of re-ranking fea-
tures.
Then they added these dense class-based features
to the baseline lexicalized system. Finally, Cherry
(2013) experimented with class-based hierarchical
reordering features. However, his features used a
bespoke representation rather than the simple full
rule string that we use.
7.2 Domain Adaptation with Features
Both Clark et al. (2012) and Wang et al. (2012) aug-
mented the baseline dense feature set with domain
labels. They each showed modest improvements
for several language pairs. However, neither incor-
porated a notion of a default prior domain.
Liu et al. (2012) investigated local adaption of
the log-linear scores by selecting comparable bitext
examples for a given source input. After selecting
a small local corpus, their algorithm then performs
several online update steps?starting from a glob-
ally tuned weight vector?prior to decoding the
input. The resulting model is effectively a locally
weighted, domain-adapted classifier.
Su et al. (2012) proposed domain adaptation
via monolingual source resources much as we use
in-domain monolingual corpora for data selection.
They labeled each bitext sentence with a topic using
a Hidden Topic Markov Model (HTMM) Gruber
et al. (2007). Source topic information was then
mixed into the translation model dense feature cal-
culations. This work follows Chiang et al. (2011),
who present a similar technique but using the same
gold NIST labels that we use. Hasler et al. (2012)
extended these ideas to a discriminative sparse fea-
ture set by augmenting both rule and unigram align-
ment features with HTMM topic information.
8 Conclusion
This paper makes four major contributions. First,
we introduced extended features for phrase-based
MT that exceeded both dense and feature-rich base-
lines. Second, we specialized the features to source
domains, further extending the gains. Third, we
showed that online expected BLEU is faster and
more stable than online PRO for extended fea-
tures. Finally, we released fast, scalable, language-
independent tools for implementing the feature set.
Our work should help practitioners quickly estab-
lish higher baselines on the way to more targeted
linguistic features. However, our analysis showed
that reference choice may restrain otherwise justifi-
able enthusiasm for feature-rich MT.
474
AcknowledgmentsWe thank John DeNero for comments on
an earlier version of this work. The first author is supported by
a National Science Foundation Graduate Research Fellowship.
This work was supported by the Defense Advanced Research
Projects Agency (DARPA) Broad Operational Language Trans-
lation (BOLT) program through IBM. Any opinions, findings,
and conclusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily reflect the
view of DARPA or the US government.
References
W. Ammar, V. Chahuneau, M. Denkowski, G. Hanne-
man, W. Ling, A. Matthews, et al. 2013. The CMU
machine translation systems at WMT 2013: Syntax,
synthetic translation options, and pseudo-references.
In WMT.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
O. Bojar, M. Mach??ek, A. Tamchyna, and D. Zeman.
2013. Scratching the surface of possible translations.
In I. Habernal and V.Matou?ek, editors, Text, Speech,
and Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 465?474. Springer Berlin Hei-
delberg.
P-C. Chang, M. Galley, and C. D. Manning. 2008.
Optimizing Chinese word segmentation for machine
translation performance. In WMT.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
C. Cherry. 2013. Improved reordering for phrase-based
translation using sparse features. In HLT-NAACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
HLT-NAACL.
D. Chiang, S. DeNeefe, and M. Pust. 2011. Two easy
improvements to lexical weighting. In ACL.
C. Christodoulopoulos, S. Goldwater, andM. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In EMNLP.
J. H. Clark, A. Lavie, and C. Dyer. 2012. One system,
many domains: Open-domain statistical machine
translation via feature augmentation. In AMTA.
H. Daum? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
M. Dreyer and D. Marcu. 2012. HyTER: Meaning-
equivalent semantics for translation evaluation. In
NAACL.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
N. Durrani, B. Haddow, K. Heafield, and P. Koehn.
2013. Edinburgh?s machine translation systems for
European language pairs. In WMT.
J. R. Finkel and C. D. Manning. 2009. Hierarchical
bayesian domain adaptation. In HLT-NAACL.
J. Gao and X. He. 2013. Training MRF-based phrase
translation models using gradient ascent. In NAACL.
K. Gimpel and N. A. Smith. 2012a. Addendum to
structured ramp loss minimization for machine trans-
lation. Technical report, Language Technologies In-
stitute, Carnegie Mellon University.
K. Gimpel and N. A. Smith. 2012b. Structured ramp
loss minimization for machine translation. In HLT-
NAACL.
S. Green, D. Cer, K. Reschke, R. Voigt, J. Bauer,
S. Wang, and others. 2013a. Feature-rich phrase-
based translation: Stanford University?s submission
to the WMT 2013 translation task. In WMT.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014. Phrasal: A
toolkit for new directions in statistical machine trans-
lation. In WMT.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
topic markov models. In AISTATS.
E. Hasler, B. Haddow, and P. Koehn. 2012. Sparse
lexicalised features and topic adaptation for SMT. In
IWSLT.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
A. Irvine, J. Morgan, M. Carpuat, H. Daum? III, and
D. Munteanu. 2013. Measuring machine translation
errors in new domains. TACL, 1.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, A. Bouchard-C?t?, D. Klein, and B. Taskar.
2006a. An end-to-end discriminative approach to
machine translation. In ACL.
P. Liang, B. Taskar, and D. Klein. 2006b. Alignment
by agreement. In NAACL.
475
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. Liu, H. Cao, T. Watanabe, T. Zhao, M. Yu, and
C. Zhu. 2012. Locally training the log-linear model
for SMT. In EMNLP-CoNLL.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-
ing the Arabic Treebank: A collaborative effort to-
ward new annotation guidelines. In LREC.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
W. Monroe, S. Green, and C. D. Manning. 2014. Word
segmentation of informal Arabic with domain adap-
tation. In ACL, Short Papers.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
P. Nakov, F. Guzm?n, and S. Vogel. 2013. A tale about
PRO and monsters. In ACL, Short Papers.
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. Compu-
tational Linguistics, 29(1):19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
G. Salton. 1958. The use of punctuation patterns in ma-
chine translation. Mechanical Translation, 5(1):16?
24, July.
J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong, and
Q. Liu. 2012. Translation model adaptation for sta-
tistical machine translation with monolingual topic
information. In ACL.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling
in machine translation. In ACL-HLT.
W. Wang, K. Macherey, W. Macherey, F. J. Och, and
P. Xu. 2012. Improved domain adaptation for statis-
tical machine translation. In AMTA.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In EMNLP-CoNLL.
E. W. D. Whittaker and P. C. Woodland. 2001. Effi-
cient class-based language modelling for very large
vocabularies. In ICASSP.
J. Wuebker, S. Peitz, F. Rietig, and H. Ney. 2013.
Improving statistical machine translation with word
class models. In EMNLP.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207?238.
H. Yu, L. Huang, H. Mi, and K. Zhao. 2013. Max-
violation perceptron and forced decoding for scalable
MT training. In EMNLP.
476
