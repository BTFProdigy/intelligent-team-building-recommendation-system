Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 407?414,
New York, June 2006. c?2006 Association for Computational Linguistics
Language Model Information Retrieval with Document Expansion
Tao Tao, Xuanhui Wang, Qiaozhu Mei, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana Champaign
Abstract
Language model information retrieval de-
pends on accurate estimation of document
models. In this paper, we propose a docu-
ment expansion technique to deal with the
problem of insufficient sampling of docu-
ments. We construct a probabilistic neigh-
borhood for each document, and expand
the document with its neighborhood infor-
mation. The expanded document provides
a more accurate estimation of the docu-
ment model, thus improves retrieval ac-
curacy. Moreover, since document expan-
sion and pseudo feedback exploit different
corpus structures, they can be combined to
further improve performance. The experi-
ment results on several different data sets
demonstrate the effectiveness of the pro-
posed document expansion method.
1 Introduction
Information retrieval with statistical language mod-
els (Lafferty and Zhai, 2003) has recently attracted
much more attention because of its solid theoreti-
cal background as well as its good empirical per-
formance. In this approach, queries and documents
are assumed to be sampled from hidden generative
models, and the similarity between a document and
a query is then calculated through the similarity be-
tween their underlying models.
Clearly, good retrieval performance relies on the
accurate estimation of the query and document mod-
els. Indeed, smoothing of document models has
been proved to be very critical (Chen and Good-
man, 1998; Kneser and Ney, 1995; Zhai and Laf-
ferty, 2001b). The need for smoothing originated
from the zero count problem: when a term does not
occur in a document, the maximum likelihood esti-
mator would give it a zero probability. This is un-
reasonable because the zero count is often due to in-
sufficient sampling, and a larger sample of the data
would likely contain the term. Smoothing is pro-
posed to address the problem.
While most smoothing methods utilize the global
collection information with a simple interpolation
(Ponte and Croft, 1998; Miller et al, 1999; Hiemstra
and Kraaij, 1998; Zhai and Lafferty, 2001b), sev-
eral recent studies (Liu and Croft, 2004; Kurland and
Lee, 2004) have shown that local corpus structures
can be exploited to improve retrieval performance.
In this paper, we further study the use of local cor-
pus structures for document model estimation and
propose to use document expansion to better exploit
local corpus structures for estimating document lan-
guage models.
According to statistical principles, the accuracy of
a statistical estimator is largely determined by the
sampling size of the observed data; a small data
set generally would result in large variances, thus
can not be trusted completely. Unfortunately, in re-
trieval, we often have to estimate a model based on a
single document. Since a document is a small sam-
ple, our estimate is unlikely to be very accurate.
A natural improvement is to enlarge the data sam-
ple, ideally in a document-specific way. Ideally, the
enlarged data sample should come from the same
original generative model. In reality, however, since
407
the underlying model is unknown to us, we would
not really be able to obtain such extra data. The
essence of this paper is to use document expansion
to obtain high quality extra data to enlarge the sam-
ple of a document so as to improve the accuracy
of the estimated document language model. Docu-
ment expansion was previously explored in (Sing-
hal and Pereira, 1999) in the context of the vec-
tor space retrieval model, mainly involving selecting
more terms from similar documents. Our work dif-
fers from this previous work in that we study doc-
ument expansion in the language modeling frame-
work and implement the idea quite differently.
Our main idea is to augment a document prob-
abilistically with potentially all other documents in
the collection that are similar to the document. The
probability associated with each neighbor document
reflects how likely the neighbor document is from
the underlying distribution of the original document,
thus we have a ?probabilistic neighborhood?, which
can serve as ?extra data? for the document for es-
timating the underlying language model. From the
viewpoint of smoothing, our method extends the ex-
isting work on using clusters for smoothing (Liu and
Croft, 2004) to allow each document to have its own
cluster for smoothing.
We evaluated our method using six representative
retrieval test sets. The experiment results show that
document expansion smoothing consistently outper-
forms the baseline smoothing methods in all the data
sets. It also outperforms a state-of-the-art cluster-
ing smoothing method. Analysis shows that the
improvement tends to be more significant for short
documents, indicating that the improvement indeed
comes from the improved estimation of the docu-
ment language model, since a short document pre-
sumably would benefit more from the neighborhood
smoothing. Moreover, since document expansion
and pseudo feedback exploit different corpus struc-
tures, they can be combined to further improve per-
formance. As document expansion can be done in
the indexing stage, it is scalable to large collections.
2 Document Expansion Retrieval Model
2.1 The KL-divergence retrieval model
We first briefly review the KL-divergence retrieval
model, on which we will develop the document
expansion technique. The KL-divergence model
is a representative state-of-the-art language model-
ing approach for retrieval. It covers the basic lan-
guage modeling approach (i.e., the query likelihood
method) as a special case and can support feedback
more naturally.
In this approach, a query and a document are as-
sumed to be generated from a unigram query lan-
guage model ?Q and a unigram document languagemodel ?D, respectively. Given a query and a docu-ment, we would first compute an estimate of the cor-
responding query model (??Q) and document model
(??D), and then score the document w.r.t. the querybased on the KL-divergence of the two models (Laf-
ferty and Zhai, 2001):
D(??Q || ??d) =
?
w?V
p(w|??Q) ? log
p(w|??Q)
p(w|??d)
.
where V is the set of all the words in our vocabulary.
The documents can then be ranked according to the
ascending order of the KL-divergence values.
Clearly, the two fundamental problems in such a
model are to estimate the query model and the doc-
ument model, and the accuracy of our estimation of
these models would affect the retrieval performance
significantly. The estimation of the query model
can often be improved by exploiting the local cor-
pus structure in a way similar to pseudo-relevance
feedback (Lafferty and Zhai, 2001; Lavrenko and
Croft, 2001; Zhai and Lafferty, 2001a). The esti-
mation of the document model is most often done
through smoothing with the global collection lan-
guage model (Zhai and Lafferty, 2001b), though re-
cently there has been some work on using clusters
for smoothing (Liu and Croft, 2004). Our work is
mainly to extend the previous work on document
smoothing and improve the accuracy of estimation
by better exploiting the local corpus structure. We
now discuss all these in detail.
2.2 Smoothing of document models
Given a document d, the simplest way to estimate
the document language model is to treat the docu-
ment as a sample from the underlying multinomial
word distribution and use the maximum likelihood
estimator: P (w|??d) = c(w,d)|d| , where c(w, d) isthe count of word w in document d, and |d| is the
408
length of d. However, as discussed in virtually all
the existing work on using language models for re-
trieval, such an estimate is problematic and inaccu-
rate; indeed, it would assign zero probability to any
word not present in document d, causing problems
in scoring a document with query likelihood or KL-
divergence (Zhai and Lafferty, 2001b). Intuitively,
such an estimate is inaccurate because the document
is a small sample.
To solve this problem, many different smoothing
techniques have been proposed and studied, usually
involving some kind of interpolation of the maxi-
mum likelihood estimate and a global collection lan-
guage model (Hiemstra and Kraaij, 1998; Miller et
al., 1999; Zhai and Lafferty, 2001b). For exam-
ple, Jelinek-Mercer(JM) and Dirichlet are two com-
monly used smoothing methods (Zhai and Lafferty,
2001b). JM smoothing uses a fixed parameter ? to
control the interpolation:
P (w|??d) = ?
c(w, d)
|d| + (1 ? ?)P (w|?C),
while the Dirichlet smoothing uses a document-
dependent coefficient (parameterized with ?) to con-
trol the interpolation:
P (w|??d) =
c(w, d) + ?P (w|?C)
|d| + ? .
Here P (w|?C) is the probability of word w given bythe collection language model ?C , which is usuallyestimated using the whole collection of documents
C , e.g., P (w|?C) =
P
d?C c(d,w)
P
d?C |d|
.
2.3 Cluster-based document model (CBDM)
Recently, the cluster structure of the corpus has been
exploited to improve language models for retrieval
(Kurland and Lee, 2004; Liu and Croft, 2004). In
particular, the cluster-based language model pro-
posed in (Liu and Croft, 2004) uses clustering infor-
mation to further smooth a document model. It di-
vides all documents into K different clusters (K =
1000 in their experiments). Both cluster informa-
tion and collection information are used to improve
the estimate of the document model:
P (w|??d) = ?
c(w, d)
|d| + (1 ? ?)
?[?P (w|?Ld) + (1 ? ?)P (w|?C )],
where ?Ld stands for document d?s cluster modeland ? and ? are smoothing parameters. In this
clustering-based smoothing method, we first smooth
a cluster model with the collection model using
Dirichlet smoothing, and then use smoothed cluster
model as a new reference model to further smooth
the document model using JM smoothing; empirical
results show that the added cluster information in-
deed enhances retrieval performance (Liu and Croft,
2004).
2.4 Document expansion
From the viewpoint of data augmentation, the
clustering-based language model can be regarded as
?expanding? a document with more data from the
cluster that contains the document. This is intu-
itively better than simply expanding every document
with the same collection language model as in the
case of JM or Dirichlet smoothing. Looking at it
from this perspective, we see that, as the ?extra data?
for smoothing a document model, the cluster con-
taining the document is often not optimal. Indeed,
the purpose of clustering is to group similar doc-
uments together, hence a cluster model represents
well the overall property of all the documents in the
cluster. However, such an average model is often not
accurate for smoothing each individual document.
We illustrate this problem in Figure 1(a), where we
show two documents d and a in cluster D. Clearly
the generative model of cluster D is more suitable
for smoothing document a than document d. In gen-
eral, the cluster model is more suitable for smooth-
ing documents close to the centroid, such as a, but is
inaccurate for smoothing a document at the bound-
ary, such as d.
To achieve optimal smoothing, each document
should ideally have its own cluster centered on the
document, as shown in Figure 1(b). This is pre-
cisely what we propose ? expanding each document
with a probabilistic neighborhood around the doc-
ument and estimate the document model based on
such a virtual, expanded document. We can then ap-
ply any simple interpolation-based method (e.g., JM
or Dirichlet) to such a ?virtual document? and treat
the word counts given by this ?virtual document? as
if they were the original word counts.
The use of neighborhood information is worth
more discussion. First of all, neighborhood is not a
409
cluster D
d d
d?s neighbors
(a) (b)
a
Figure 1: Clusters, neighborhood, and document ex-
pansion
clearly defined concept. In the narrow sense, only
a few documents close to the original one should
be included in the neighborhood, while in the wide
sense, the whole collection can be potentially in-
cluded. It is thus a challenge to define the neighbor-
hood concept reasonably. Secondly, the assumption
that neighbor documents are sampled from the same
generative model as the original document is not
completely valid. We probably do not want to trust
them so much as the original one. We solve these
two problems by associating a confidence value with
every document in the collection, which reflects our
belief that the document is sampled from the same
underlying model as the original document. When a
document is close to the original one, we have high
confidence, but when it is farther apart, our confi-
dence would fade away. In this way, we construct
a probabilistic neighborhood which can potentially
include all the documents with different confidence
values. We call a language model based on such a
neighborhood document expansion language model
(DELM).
Technically, we are looking for a new enlarged
document d? for each document d in a text collec-
tion, such that the new document d? can be used
to estimate the hidden generative model of d more
accurately. Since a good d? should presumably be
based on both the original document d and its neigh-
borhood N(d), we define a function ?:
d? = ?(d,N(d)). (1)
The precise definition of the neighborhood con-
cept N(d) relies on the distance or similarity be-
tween each pair of documents. Here, we simply
choose the commonly used cosine similarity, though
other choices may also be possible. Given any two
document models X and Y , the cosine similarity is
d
Figure 2: Normal distribution of confidence values.
defined as:
sim(X,Y ) =
?
i xi ? yi
?
?
i(xi)2 ?
?
i(yi)2
.
To model the uncertainty of neighborhood, we as-
sign a confidence value ?d(b) to every document b inthe collection to indicate how strongly we believe b
is sampled from d?s hidden model. In general, ?d(b)can be set based on the similarity of b and d ? the
more similar b and d are, the larger ?d(b) wouldbe. With these confidence values, we construct a
probabilistic neighborhood with every document in
it, each with a different weight. The whole problem
is thus reduced to how to define ?d(b) exactly.Intuitively, an exponential decay curve can help
regularize the influence from remote documents. We
therefore want ?d(b) to satisfy a normal distributioncentered around d. Figure 2 illustrates the shape
of this distribution. The black dots are neighbor-
hood documents centered around d. Their proba-
bility values are determined by their distances to the
center. We fortunately observe that the cosine sim-
ilarities, which we use to decide the neighborhood,
are roughly of this decay shape. We thus use them
directly without further transformation because that
would introduce unnecessary parameters. We set
?d(b) by normalizing the cosine similarity scores :
?d(b) =
sim(d, b)
?
b??C?{d} sim(d, b?)
.
Function ? serves to balance the confidence be-
tween d and its neighborhood N(d) in the model es-
timation step. Intuitively, a shorter document is less
sufficient, hence needs more help from its neighbor-
hood. Conversely, a longer one can rely more on
itself. We use a parameter ? to control this balance.
Thus finally, we obtain a pseudo document d? with
410
the following pseudo term count:
c(w, d?) = ?c(w, d) + (1 ? ?)
?
?
b?C?{d}
(?d(b) ? c(w, b)),
We hypothesize that, in general, ?d can be estimatedmore accurately from d? rather than d itself because
d? contains more complete information about ?d.This hypothesis can be tested by by comparing the
retrieval results of applying any smoothing method
to d with those of applying the same method to d?.
In our experiments, we will test this hypothesis with
both JM smoothing and Dirichlet smoothing.
Note that the proposed document expansion tech-
nique is quite general. Indeed, since it transforms
the original document to a potentially better ?ex-
panded document?, it can presumably be used to-
gether with any retrieval method, including the vec-
tor space model. In this paper, we focus on evalu-
ating this technique with the language modeling ap-
proach.
Because of the decay shape of the neighborhood
and for the sake of efficiency, we do not have to ac-
tually use all documents in C?{d}. Instead, we can
safely cut off the documents on the tail, and only use
the top M closest neighbors for each document. We
show in the experiment section that the performance
is not sensitive to the choice of M when M is suf-
ficiently large (for example 100). Also, since doc-
ument expansion can be done completely offline, it
can scale up to large collections.
3 Experiments
We evaluate the proposed method over six repre-
sentative TREC data sets (Voorhees and Harman,
2001): AP (Associated Press news 1988-90), LA
(LA Times), WSJ (Wall Street Journal 1987-92),
SJMN (San Jose Mercury News 1991), DOE (De-
partment of Energy), and TREC8 (the ad hoc data
used in TREC8). Table 1 shows the statistics of these
data.
We choose the first four TREC data sets for per-
formance comparison with (Liu and Croft, 2004).
To ensure that the comparison is meaningful, we use
identical sources (after all preprocessing). In addi-
tion, we use the large data set TREC8 to show that
our algorithm can scale up, and use DOE because its
#document queries #total qrel
AP 242918 51-150 21819
LA 131896 301-400 2350
WSJ 173252 51-100 and 151-200 10141
SJMN 90257 51-150 4881
TREC8 528155 401-450 4728
DOE 226087 DOE queries 2047
Table 1: Experiment data sets
documents are usually short, and our previous expe-
rience shows that it is a relatively difficult data set.
3.1 Neighborhood document expansion
Our model boils down to a standard query likelihood
model when no neighborhood document is used. We
therefore use two most commonly used smoothing
methods, JM and Dirichlet , as our baselines. The re-
sults are shown in Table 2, where we report both the
mean average precision (MAP) and precision at 10
documents. JM and Dirichlet indicate the standard
language models with JM smoothing and Dirichlet
smoothing respectively, and the other two are the
ones combined with our document expansion. For
both baselines, we tune the parameters (? for JM,
and ? for Dirichlet) to be optimal. We then use the
same values of ? or ? without further tuning for the
document expansion runs, which means that the pa-
rameters may not necessarily optimal for the docu-
ment expansion runs. Despite this disadvantage, we
see that the document expansion runs significantly
outperform their corresponding baselines, with more
than 15% relative improvement on AP. The parame-
ters M and ? were set to 100 and 0.5, respectively.
To understand the improvement in more detail, we
show the precision values at different levels of recall
for the AP data in Table 3. Here we see that our
method significantly outperforms the baseline at ev-
ery precision point.
In our model, we introduce two additional param-
eters: M and ?. We first examine M here, and then
study ? in Section 3.3. Figure 3 shows the perfor-
mance trend with respect to the values of M . The
x-axis is the values of M , and the y-axis is the non-
interpolated precision averaging over all 50 queries.
We draw two conclusions from this plot: (1) Neigh-
borhood information improves retrieval accuracy;
adding more documents leads to better retrieval re-
sults. (2) The performance becomes insensitive to
411
Data JM DELM+JM (impr. %) Dirichlet DELM + Diri.(impr. %)
AP AvgPrec 0.2058 0.2405 (16.8%***) 0.2168 0.2505 (15.5%***)
P@10 0.3990 0.4444 (11.4%***) 0.4323 0.4515 (4.4%**)
DOE AvgPrec 0.1759 0.1904 (8.3%***) 0.1804 0.1898 (5.2%**)
P@10 0.2629 0.2943 (11.9%*) 0.2600 0.2800 (7.7%*)
TREC8 AvgPrec 0.2392 0.2539 (6.01%**) 0.2567 0.2671 (4.05%*)
P@10 0.4300 0.4460 (3.7%) 0.4500 0.4740 (5.3%*)
Table 2: Comparisons with baselines. *,**,*** indicate that we accept the improvement hypothesis by
Wilcoxon test at significance level 0.1, 0.05, 0.01 respectively.
AP, TREC queries 51-150
Dirichlet DELM+Diri Improvement(%)
Rel. 21819 21819
Rel.Retr. 10126 10917 7.81% ***
Prec.
0.0 0.6404 0.6605 3.14% *
0.1 0.4333 0.4785 10.4% ***
0.2 0.3461 0.3983 15.1% ***
0.3 0.2960 0.3496 18.1% ***
0.4 0.2436 0.2962 21.6% ***
0.5 0.2060 0.2418 17.4% ***
0.6 0.1681 0.1975 17.5% ***
0.7 0.1290 0.1580 22.5% ***
0.8 0.0862 0.1095 27.0% **
0.9 0.0475 0.0695 46.3% **
1.0 0.0220 0.0257 16.8%
ave. 0.2168 0.2505 15.5% ***
Table 3: PR curve on AP data. *,**,*** indicate that
we accept the improvement hypothesis by Wilcoxon
test at significant level 0.1, 0.05, 0.01 respectively.
M when M is sufficiently large, namely 100. The
reason is twofold: First, since the neighborhood is
centered around the original document, when M is
large, the expansion may be evenly magnified on all
term dimensions. Second, the exponentially decay-
ing confidence values reduce the influence of remote
documents.
3.2 Comparison with CBDM
In this section, we compare the CBDM method us-
ing the model performing the best in (Liu and Croft,
2004)1. Furthermore, we also set Dirichlet prior pa-
rameter ? = 1000, as mentioned in (Liu and Croft,
2004), to rule out any potential influence of Dirichlet
smoothing.
Table 4 shows that our model outperforms CBDM
in MAP values on four data sets; the improvement
1We use the exact same data, queries, stemming and all
other preprocessing techniques. The baseline results in (Liu and
Croft, 2004) are confirmed.
0.17
0.18
0.19
0.2
0.21
0.22
0.23
0.24
0.25
0.26
0.27
0 100 200 300 400 500 600 700 800
a
ve
ra
ge
 p
re
ce
sio
n
M : the number of  neighborhood documents
AP
DOE
TREC8
Figure 3: Performance change with respect to M
CBDM DELM+Diri. improvement(%)
AP 0.2326 0.2505 7.7%
LA 0.2590 0.2655 2.5%
WSJ 0.3006 0.3113 3.6%
SJMN 0.2171 0.2266 4.3%
Table 4: Comparisons with CBDM.
presumably comes from a more principled way of
exploiting corpus structures. Given that clustering
can at least capture the local structure to some ex-
tent, it should not be very surprising that the im-
provement of document expansion over CBDM is
much less than that over the baselines.
Note that we cannot fulfill Wilcoxon test because
of the lack of the individual query results of CBDM.
3.3 Impact on short documents
Document expansion is to solve the insufficient sam-
pling problem. Intuitively, a short document is less
sufficient than a longer one, hence would need more
?help? from its neighborhood. We design experi-
ments to test this hypothesis.
Specifically, we randomly shrink each document
in AP88-89 to a certain percentage of its original
length. For example, a shrinkage factor of 30%
means each term has 30% chance to stay, or 70%
chance to be filtered out. In this way, we reduce the
original data set to a new one with the same number
412
average doc length 30% 50% 70% 100%
baseline 0.1273 0.1672 0.1916 0.2168
document expansion 0.1794 0.2137 0.2307 0.2505
optimal ? 0.2 0.3 0.3 0.4
improvement(%) 41% 28% 20% 16%
Table 5: Impact on short documents (in MAP)
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
a
ve
ra
ge
 p
re
cis
io
n
alpha
30%
50%
70%
100%
Figure 4: Performance change with respect to ?
of documents but a shorter average document length.
Table 5 shows the experiment results over docu-
ment sets with different average document lengths.
The results indeed support our hypothesis that doc-
ument expansion does help short documents more
than longer ones. While we can manage to improve
41% on a 30%-length corpus, the same model only
gets 16% improvement on the full length corpus.
To understand how ? affects the performance we
plot the sensitivity curves in Figure 4. The curves all
look similar, but the optimal points slightly migrate
when the average document length becomes shorter.
A 100% corpus gets optimal at ? = 0.4, but 30%
corpus has to use ? = 0.2 to obtain its optimum.
(All optimal ? values are presented in the fourth row
of Table 5.)
3.4 Further improvement with pseudo
feedback
Query expansion has been proved to be an effec-
tive way of utilizing corpus information to improve
the query representation (Rocchio, 1971; Zhai and
Lafferty, 2001a). It is thus interesting to examine
whether our model can be combined with query ex-
pansion to further improve the retrieval accuracy.
We use the model-based feedback proposed in (Zhai
and Lafferty, 2001a) and take top 5 returned docu-
ments for feedback. There are two parameters in the
model-based pseudo feedback process: the noisy pa-
DELM pseudo DELM+pseudo Impr.(%)
AP 0.2505 0.2643 0.2726 3.14%*
LA 0.2655 0.2769 0.2901 4.77%
TREC8 0.2671 0.2716 0.2809 3.42%**
DOE 0.1898 0.1918 0.2046 6.67%***
Table 6: Combination with pseudo feed-
back.*,**,*** indicate that we accept the improve-
ment hypothesis by Wilcoxon test at significant
level 0.1, 0.05, 0.01 respectively.
pseu. inter. combined (%) z-score
AP 0.2643 0.2450 0.2660 (0.64%) -0.2888
LA 0.2769 0.2662 0.2636 (-0.48%) -1.0570
TREC8 0.2716 0.2702 0.2739 (0.84%) -1.6938
Table 7: Performance of the interpolation algorithm
combined with the pseudo feedback.
rameter ? and the interpolation parameter ?2. We fix
? = 0.9 and tune ? to optimal, and use them directly
in the feedback process combined with our models.
(It again means that ? is probably not optimal in our
results.) The combination is conducted in the fol-
lowing way: (1) Retrieve documents by our DELM
method; (2) Choose top 5 document to do the model-
based feedback; (3) Use the expanded query model
to retrieve documents again with DELM method.
Table 6 shows the experiment results (MAP); in-
deed, by combining DELM with pseudo feedback,
we can obtain significant further improvement of
performance.
As another baseline, we also tested the algorithm
proposed in (Kurland and Lee, 2004). Since the al-
gorithm overlaps with pseudo feedback process, it is
not easy to further combine them. We implement its
best-performing algorithm, ?interpolation? (labeled
as inter. ), and show the results in Table 7. Here,
we use the same three data sets as used in (Kurland
and Lee, 2004). We tune the feedback parameters to
optimal in each experiment. The second last column
in Table 7 shows the performance of combination of
the ?interpolation? model with the pseudo feedback
and its improvement percentage. The last column is
the z-scores of Wilcoxon test. The negative z-scores
indicate that none of the improvement is significant.
2 (Zhai and Lafferty, 2001a) uses different notations. We
change them because ? has already been used in our own
model.
413
4 Conclusions
In this paper, we proposed a novel document expan-
sion method to enrich the document sample through
exploiting the local corpus structure. Unlike pre-
vious cluster-based models, we smooth each doc-
ument using a probabilistic neighborhood centered
around the document itself.
Experiment results show that (1) The proposed
document expansion method outperforms both the
?no expansion? baselines and the cluster-based mod-
els. (2) Our model is relatively insensitive to the set-
ting of parameter M as long as it is sufficiently large,
while the parameter ? should be set according to the
document length; short documents need a smaller
? to obtain more help from its neighborhood. (3)
Document expansion can be combined with pseudo
feedback to further improve performance. Since any
retrieval model can be presumably applied on top of
the expanded documents, we believe that the pro-
posed technique can be potentially useful for any re-
trieval model.
5 Acknowledgments
This work is in part supported by the National Sci-
ence Foundation under award number IIS-0347933.
We thank Xiaoyong Liu for kindly providing us sev-
eral processed data sets for our performance com-
parison. We thank Jing Jiang and Azadeh Shakery
for helping improve the paper writing, and thank the
anonymous reviewers for their useful comments.
References
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-cal Report TR-10-98, Harvard University.
D. Hiemstra and W. Kraaij. 1998. Twenty-one at trec-7:
Ad-hoc and cross-language track. In Proc. of Seventh
Text REtrieval Conference (TREC-7).
R. Kneser and H. Ney. 1995. Improved smoothing for m-
gram languagemodeling. In Proceedings of the Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Oren Kurland and Lillian Lee. 2004. Corpus structure,language models, and ad hoc information retrieval. In
SIGIR ?04: Proceedings of the 27th annual interna-
tional conference on Research and development in in-
formation retrieval, pages 194?201. ACM Press.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-tion for information retrieval. In Proceedings of SI-
GIR?2001, pages 111?119, Sept.
John Lafferty and ChengXiang Zhai. 2003. Probabilistic
relevance models based on document and query gen-eration.
Victor Lavrenko and Bruce Croft. 2001. Relevance-based language models. In Proceedings of SI-
GIR?2001, Sept.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-basedretrieval using language models. In SIGIR ?04: Pro-
ceedings of the 27th annual international conference
on Research and development in information retrieval,pages 186?193. ACM Press.
D. H. Miller, T. Leek, and R. Schwartz. 1999. A hid-den markov model information retrieval system. In
Proceedings of the 1999 ACM SIGIR Conference on
Research and Development in Information Retrieval,pages 214?221.
J. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
the ACM SIGIR, pages 275?281.
J. Rocchio. 1971. Relevance feedback in information re-trieval. In The SMART Retrieval System: Experiments
in Automatic Document Processing, pages 313?323.Prentice-Hall Inc.
Amit Singhal and Fernando Pereira. 1999. Documentexpansion for speech retrieval. In SIGIR ?99: Pro-
ceedings of the 22nd annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 34?41. ACM Press.
E. Voorhees and D. Harman, editors. 2001. Proceedings
of Text REtrieval Conference (TREC1-9). NIST Spe-cial Publications. http://trec.nist.gov/pubs.html.
Chengxiang Zhai and John Lafferty. 2001a. Model-based feedback in the KL-divergence retrieval model.
In Tenth International Conference on Information and
Knowledge Management (CIKM 2001), pages 403?410.
Chengxiang Zhai and John Lafferty. 2001b. A study
of smoothing methods for language models applied toad hoc information retrieval. In Proceedings of SI-
GIR?2001, pages 334?342, Sept.
414
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Named Entity Transliteration with Comparable Corpora
Richard Sproat, Tao Tao, ChengXiang Zhai
University of Illinois at Urbana-Champaign, Urbana, IL, 61801
rws@uiuc.edu, {taotao,czhai}@cs.uiuc.edu
Abstract
In this paper we investigate Chinese-
English name transliteration using compa-
rable corpora, corpora where texts in the
two languages deal in some of the same
topics ? and therefore share references
to named entities ? but are not transla-
tions of each other. We present two dis-
tinct methods for transliteration, one ap-
proach using phonetic transliteration, and
the second using the temporal distribu-
tion of candidate pairs. Each of these ap-
proaches works quite well, but by com-
bining the approaches one can achieve
even better results. We then propose a
novel score propagation method that uti-
lizes the co-occurrence of transliteration
pairs within document pairs. This prop-
agation method achieves further improve-
ment over the best results from the previ-
ous step.
1 Introduction
As part of a more general project on multilin-
gual named entity identification, we are interested
in the problem of name transliteration across lan-
guages that use different scripts. One particular is-
sue is the discovery of named entities in ?compara-
ble? texts in multiple languages, where by compa-
rable we mean texts that are about the same topic,
but are not in general translations of each other.
For example, if one were to go through an English,
Chinese and Arabic newspaper on the same day,
it is likely that the more important international
events in various topics such as politics, business,
science and sports, would each be covered in each
of the newspapers. Names of the same persons,
locations and so forth ? which are often translit-
erated rather than translated ? would be found in
comparable stories across the three papers.1 We
wish to use this expectation to leverage translit-
eration, and thus the identification of named enti-
ties across languages. Our idea is that the occur-
rence of a cluster of names in, say, an English text,
should be useful if we find a cluster of what looks
like the same names in a Chinese or Arabic text.
An example of what we are referring to can be
found in Figure 1. These are fragments of two
stories from the June 8, 2001 Xinhua English and
Chinese newswires, each covering an international
women?s badminton championship. Though these
two stories are from the same newswire source,
and cover the same event, they are not translations
of each other. Still, not surprisingly, a lot of the
names that occur in one, also occur in the other.
Thus (Camilla) Martin shows up in the Chinese
version as ??? ma-er-ting; Judith Meulendijks
is ??????? yu mo-lun-di-ke-si; and Mette
Sorensen is ?????mai su-lun-sen. Several
other correspondences also occur. While some of
the transliterations are ?standard? ? thus Martin
is conventionally transliterated as ??? ma-er-
ting ? many of them were clearly more novel,
though all of them follow the standard Chinese
conventions for transliterating foreign names.
These sample documents illustrate an important
point: if a document in language L1 has a set of
names, and one finds a document in L2 containing
a set of names that look as if they could be translit-
erations of the names in the L1 document, then
this should boost one?s confidence that the two sets
of names are indeed transliterations of each other.
We will demonstrate that this intuition is correct.
1Many names, particularly of organizations, may be trans-
lated rather than transliterated; the transliteration method we
discuss here obviously will not account for such cases, though
the time correlation and propagation methods we discuss will
still be useful.
73
Dai Yun Nips World No. 1 Martin to Shake off Olympic
Shadow . . . In the day?s other matches, second seed Zhou Mi
overwhelmed Ling Wan Ting of Hong Kong, China 11-4, 11-
4, Zhang Ning defeat Judith Meulendijks of Netherlands 11-
2, 11-9 and third seed Gong Ruina took 21 minutes to elimi-
nate Tine Rasmussen of Denmark 11-1, 11-1, enabling China
to claim five quarterfinal places in the women?s singles.
? ? ? ? ? ?  ? ? ? ? ? ? ? ? ? ? ?
. . . ??????,????????4???,?? . . .
? ? ? ? ? ? ? ? ? ? ? ? 11:1? ? ? ? ? ?
?? ????,??????11:2?11:9????? ?
???????,??????11:4?11:1? ???
??????
Figure 1: Sample from two stories about an inter-
national women?s badminton championship.
2 Previous Work
In previous work on Chinese named-entity
transliteration ? e.g. (Meng et al, 2001; Gao
et al, 2004), the problem has been cast as the
problem of producing, for a given Chinese name,
an English equivalent such as one might need in
a machine translation system. For example, for
the name ??????wei wei-lian-mu-si, one
would like to arrive at the English name V(enus)
Williams. Common approaches include source-
channel methods, following (Knight and Graehl,
1998) or maximum-entropy models.
Comparable corpora have been studied exten-
sively in the literature (e.g.,(Fung, 1995; Rapp,
1995; Tanaka and Iwasaki, 1996; Franz et al,
1998; Ballesteros and Croft, 1998; Masuichi et al,
2000; Sadat et al, 2003)), but transliteration in the
context of comparable corpora has not been well
addressed.
The general idea of exploiting frequency corre-
lations to acquire word translations from compara-
ble corpora has been explored in several previous
studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka
and Iwasaki, 1996)).Recently, a method based on
Pearson correlation was proposed to mine word
pairs from comparable corpora (Tao and Zhai,
2005), an idea similar to the method used in (Kay
and Roscheisen, 1993) for sentence alignment. In
our work, we adopt the method proposed in (Tao
and Zhai, 2005) and apply it to the problem of
transliteration. We also study several variations of
the similarity measures.
Mining transliterations from multilingual web
pages was studied in (Zhang and Vines, 2004);
Our work differs from this work in that we use
comparable corpora (in particular, news data) and
leverage the time correlation information naturally
available in comparable corpora.
3 Chinese Transliteration with
Comparable Corpora
We assume that we have comparable corpora, con-
sisting of newspaper articles in English and Chi-
nese from the same day, or almost the same day. In
our experiments we use data from the English and
Chinese stories from the Xinhua News agency for
about 6 months of 2001.2 We assume that we have
identified names for persons and locations?two
types that have a strong tendency to be translit-
erated wholly or mostly phonetically?in the En-
glish text; in this work we use the named-entity
recognizer described in (Li et al, 2004), which
is based on the SNoW machine learning toolkit
(Carlson et al, 1999).
To perform the transliteration task, we propose
the following general three-step approach:
1. Given an English name, identify candi-
date Chinese character n-grams as possible
transliterations.
2. Score each candidate based on how likely the
candidate is to be a transliteration of the En-
glish name. We propose two different scoring
methods. The first involves phonetic scoring,
and the second uses the frequency profile of
the candidate pair over time. We will show
that each of these approaches works quite
well, but by combining the approaches one
can achieve even better results.
3. Propagate scores of all the candidate translit-
eration pairs globally based on their co-
occurrences in document pairs in the compa-
rable corpora.
The intuition behind the third step is the following.
Suppose several high-confidence name transliter-
ation pairs occur in a pair of English and Chi-
nese documents. Intuitively, this would increase
our confidence in the other plausible translitera-
tion pairs in the same document pair. We thus pro-
pose a score propagation method to allow these
high-confidence pairs to propagate some of their
2Available from the LDC via the English Gigaword
(LDC2003T05) and Chinese Gigaword (LDC2003T09) cor-
pora.
74
scores to other co-occurring transliteration pairs.
As we will show later, such a propagation strat-
egy can generally further improve the translitera-
tion accuracy; in particular, it can further improve
the already high performance from combining the
two scoring methods.
3.1 Candidate Selection
The English named entity candidate selection pro-
cess was already described above. Candidate Chi-
nese transliterations are generated by consulting
a list of characters that are frequently used for
transliterating foreign names. As discussed else-
where (Sproat et al, 1996), a subset of a few hun-
dred characters (out of several thousand) tends to
be used overwhelmingly for transliterating foreign
names into Chinese. We use a list of 495 such
characters, derived from various online dictionar-
ies. A sequence of three or more characters from
the list is taken as a possible name. If the character
??? occurs, which is frequently used to represent
the space between parts of an English name, then
at least one character to the left and right of this
character will be collected, even if the character in
question is not in the list of ?foreign? characters.
Armed with the English and Chinese candidate
lists, we then consider the pairing of every En-
glish candidate with every Chinese candidate. Ob-
viously it would be impractical to do this for all of
the candidates generated for, say, an entire year:
we consider as plausible pairings those candidates
that occur within a day of each other in the two
corpora.
3.2 Candidate scoring based on
pronunciation
We adopt a source-channel model for scoring
English-Chinese transliteration pairs. In general,
we seek to estimate P (e|c), where e is a word in
Roman script, and c is a word in Chinese script.
Since Chinese transliteration is mostly based on
pronunciation, we estimate P (e?|c?), where e? is
the pronunciation of e and c? is the pronunciation
of c. Again following standard practice, we de-
compose the estimate of P (e?|c?) as P (e?|c?) =
?
i P (e?i|c?i). Here, e?i is the ith subsequence of
the English phone string, and c?i is the ith subse-
quence of the Chinese phone string. Since Chi-
nese transliteration attempts to match the syllable-
sized characters to equivalent sounding spans of
the English language, we fix the c?i to be syllables,
and let the e?i range over all possible subsequences
of the English phone string. For training data we
have a small list of 721 names in Roman script and
their Chinese equivalent.3 Pronunciations for En-
glish words are obtained using the Festival text-to-
speech system (Taylor et al, 1998); for Chinese,
we use the standard pinyin transliteration of the
characters. English-Chinese pairs in our training
dictionary were aligned using the alignment algo-
rithm from (Kruskal, 1999), and a hand-derived
set of 21 rules-of-thumb: for example, we have
rules that encode the fact that Chinese /l/ can cor-
respond to English /r/, /n/ or /er/; and that Chinese
/w/ may be used to represent /v/. Given that there
are over 400 syllables in Mandarin (not count-
ing tone) and each of these syllables can match
a large number of potential English phone spans,
this is clearly not enough training data to cover all
the parameters, and so we use Good-Turing esti-
mation to estimate probabilities for unseen corre-
spondences. Since we would like to filter implau-
sible transliteration pairs we are less lenient than
standard estimation techniques in that we are will-
ing to assign zero probability to some correspon-
dences. Thus we set a hard rule that for an En-
glish phone span to correspond to a Chinese sylla-
ble, the initial phone of the English span must have
been seen in the training data as corresponding to
the initial of the Chinese syllable some minimum
number of times. For consonant-initial syllables
we set the minimum to 4. We omit further details
of our estimation technique for lack of space. This
phonetic correspondence model can then be used
to score putative transliteration pairs.
3.3 Candidate Scoring based on Frequency
Correlation
Names of the same entity that occur in different
languages often have correlated frequency patterns
due to common triggers such as a major event.
Thus if we have comparable news articles over a
sufficiently long time period, it is possible to ex-
ploit such correlations to learn the associations of
names in different languages. The idea of exploit-
ing frequency correlation has been well studied.
(See the previous work section.) We adopt the
method proposed in (Tao and Zhai, 2005), which
3The LDC provides a much larger list of transliterated
Chinese-English names, but we did not use this here for two
reasons. First, we have found it it be quite noisy. Secondly,
we were interested in seeing how well one could do with a
limited resource of just a few hundred names, which is a more
realistic scenario for languages that have fewer resources than
English and Chinese.
75
works as follows: We pool all documents in a sin-
gle day to form a large pseudo-document. Then,
for each transliteration candidate (both Chinese
and English), we compute its frequency in each
of those pseudo-documents and obtain a raw fre-
quency vector. We further normalize the raw fre-
quency vector so that it becomes a frequency dis-
tribution over all the time points (days). In order
to compute the similarity between two distribution
vectors, The Pearson correlation coefficient was
used in (Tao and Zhai, 2005); here we also consid-
ered two other commonly used measures ? cosine
(Salton and McGill, 1983), and Jensen-Shannon
divergence (Lin, 1991), though our results show
that Pearson correlation coefficient performs bet-
ter than these two other methods.
3.4 Score Propagation
In both scoring methods described above, scoring
of each candidate transliteration pair is indepen-
dent of the other. As we have noted, document
pairs that contain lots of plausible transliteration
pairs should be viewed as more plausible docu-
ment pairs; at the same time, in such a situation we
should also trust the putative transliteration pairs
more. Thus these document pairs and translitera-
tion pairs mutually ?reinforce? each other, and this
can be exploited to further optimize our translit-
eration scores by allowing transliteration pairs to
propagate their scores to each other according to
their co-occurrence strengths.
Formally, suppose the current generation of
transliteration scores are (ei, ci, wi) i = 1, ..., n,
where (ei, ci) is a distinct pair of English and Chi-
nese names. Note that although for any i 6= j, we
have (ei, ci) 6= (ej , cj), it is possible that ei = ej
or ci = cj for some i 6= j. wi is the transliteration
score of (ei, ci).
These pairs along with their co-occurrence re-
lation computed based on our comparable cor-
pora can be formally represented by a graph as
shown in Figure 2. In such a graph, a node repre-
sents (ei, ci, wi). An edge between (ei, ci, wi) and
(ej , cj , wj) is constructed iff (ei, ci) and (ej , cj)
co-occur in a certain document pair (Et, Ct), i.e.
there exists a document pair (Et, Ct), such that
ei, ej ? Et and ci, cj ? Ct. Given a node
(ei, ci, wi), we refer to all its directly-connected
nodes as its ?neighbors?. The documents do not
appear explicitly in the graph, but they implicitly
affect the graph?s topology and the weight of each
edge. Our idea of score propagation can now be
formulated as the following recursive equation for
w1
w4
w2
w3
w5
w6
w7
(e4, c4)
(e3, c3)
(e5, c5)
(e5, c5)
(e2, c2)
(e7, c7)
(e6, c6)
Figure 2: Graph representing transliteration pairs
and cooccurence relations.
updating the scores of all the transliteration pairs.
w(k)i = ?? w
(k?1)
i + (1 ? ?) ?
n
?
j 6=i,j=1
(w(k?1)j ? P (j|i)),
where w(k)i is the new score of the pair (ei, ci)
after an iteration, while w(k?1)i is its old score
before updating; ? ? [0, 1] is a parameter to
control the overall amount of propagation (when
? = 1, no propagation occurs); P (j|i) is the con-
ditional probability of propagating a score from
node (ej , cj , wj) to node (ei, ci, wi).
We estimate P (j|i) in two different ways: 1)
The number of cooccurrences in the whole collec-
tion (Denote as CO). P (j|i) = C(i,j)?
j? C(i,j?)
, where
C(i, j) is the cooccurrence count of (ei, ci) and
(ej , cj); 2) A mutual information-based method
(Denote as MI). P (j|i) = MI(i,j)?
j? MI(i,j?)
, where
MI(i, j) is the mutual information of (ei, ci) and
(ej , cj). As we will show, the CO method works
better. Note that the transition probabilities be-
tween indirect neighbors are always 0. Thus prop-
agation only happens between direct neighbors.
This formulation is very similar to PageRank,
a link-based ranking algorithm for Web retrieval
(Brin and Page, 1998). However, our motivation
is propagating scores to exploit cooccurrences, so
we do not necessarily want the equation to con-
verge. Indeed, our results show that although the
initial iterations always help improve accuracy, too
many iterations actually would decrease the per-
formance.
4 Evaluation
We use a comparable English-Chinese corpus to
evaluate our methods for Chinese transliteration.
We take one day?s worth of comparable news arti-
cles (234 Chinese stories and 322 English stories),
generate about 600 English names with the entity
recognizer (Li et al, 2004) as described above, and
76
find potential Chinese transliterations also as pre-
viously described. We generated 627 Chinese can-
didates. In principle, all these 600 ? 627 pairs are
potential transliterations. We then apply the pho-
netic and time correlation methods to score and
rank all the candidate Chinese-English correspon-
dences.
To evaluate the proposed transliteration meth-
ods quantitatively, we measure the accuracy of the
ranked list by Mean Reciprocal Rank (MRR), a
measure commonly used in information retrieval
when there is precisely one correct answer (Kan-
tor and Voorhees, 2000). The reciprocal rank is
the reciprocal of the rank of the correct answer.
For example, if the correct answer is ranked as the
first, the reciprocal rank would be 1.0, whereas if
it is ranked the second, it would be 0.5, and so
forth. To evaluate the results for a set of English
names, we take the mean of the reciprocal rank of
each English name.
We attempted to create a complete set of an-
swers for all the English names in our test set,
but a small number of English names do not seem
to have any standard transliteration according to
the resources that we consulted. We ended up
with a list of about 490 out of the 600 English
names judged. We further notice that some an-
swers (about 20%) are not in our Chinese candi-
date set. This could be due to two reasons: (1) The
answer does not occur in the Chinese news articles
we look at. (2) The answer is there, but our candi-
date generation method has missed it. In order to
see more clearly how accurate each method is for
ranking the candidates, we also compute the MRR
for the subset of English names whose transliter-
ation answers are in our candidate list. We dis-
tinguish the MRRs computed on these two sets of
English names as ?AllMRR? and ?CoreMRR?.
Below we first discuss the results of each of the
two methods. We then compare the two methods
and discuss results from combining the two meth-
ods.
4.1 Phonetic Correspondence
We show sample results for the phonetic scoring
method in Table 1. This table shows the 10 high-
est scoring transliterations for each Chinese char-
acter sequence based on all texts in the Chinese
and English Xinhua newswire for the 13th of Au-
gust, 2001. 8 out of these 10 are correct. For all
the English names the MRR is 0.3, and for the
?paris ??? pei-lei-si 3.51
iraq ??? yi-la-ke 3.74
staub ??? si-ta-bo 4.45
canada ?? jia-na-da 4.85
belfast ????? bei-er-fa-si-te 4.90
fischer ??? fei-she-er 4.91
philippine ??? fei-lu?-bin 4.97
lesotho ?? lai-suo-two 5.12
?tirana ??? tye-lu-na 5.15
freeman ??? fu-li-man 5.26
Table 1: Ten highest-scoring matches for the Xin-
hua corpus for 8/13/01. The final column is the
?log P estimate for the transliteration. Starred
entries are incorrect.
core names it is 0.89. Thus on average, the cor-
rect answer, if it is included in our candidate list,
is ranked mostly as the first one.
4.2 Frequency correlation
Similarity AllMRR CoreMRR
Pearson 0.1360 0.3643
Cosine 0.1141 0.3015
JS-div 0.0785 0.2016
Table 2: MRRs of the frequency correlation meth-
ods.
We proposed three similarity measures for the
frequency correlation method, i.e., the Cosine,
Pearson coefficient, and Jensen-Shannon diver-
gence. In Table 2, we show their MRRs. Given
that the only resource the method needs is compa-
rable text documents over a sufficiently long pe-
riod, these results are quite encouraging. For ex-
ample, with Pearson correlation, when the Chinese
transliteration of an English name is included in
our candidate list, the correct answer is, on aver-
age, ranked at the 3rd place or better. The results
thus show that the idea of exploiting frequency
correlation does work. We also see that among
the three similarity measures, Pearson correlation
performs the best; it performs better than Cosine,
which is better than JS-divergence.
Compared with the phonetic correspondence
method, the performance of the frequency correla-
tion method is in general much worse, which is not
surprising, given the fact that terms may be corre-
lated merely because they are topically related.
77
4.3 Combination of phonetic correspondence
and frequency correlation
Method AllMRR CoreMRR
Phonetic 0.2999 0.8895
Freq 0.1360 0.3643
Freq+PhoneticFilter 0.3062 0.9083
Freq+PhoneticScore 0.3194 0.9474
Table 3: Effectiveness of combining the two scor-
ing methods.
Since the two methods exploit complementary
resources, it is natural to see if we can improve
performance by combining the two methods. In-
deed, intuitively the best candidate is the one that
has a good pronunciation alignment as well as a
correlated frequency distribution with the English
name. We evaluated two strategies for combining
the two methods. The first strategy is to use the
phonetic model to filter out (clearly impossible)
candidates and then use the frequency correlation
method to rank the candidates. The second is to
combine the scores of these two methods. Since
the correlation coefficient has a maximum value
of 1, we normalize the phonetic correspondence
score by dividing all scores by the maximum score
so that the maximum normalized value is also 1.
We then take the average of the two scores and
rank the candidates based on their average scores.
Note that the second strategy implies the applica-
tion of the first strategy.
The results of these two combination strategies
are shown in Table 3 along with the results of the
two individual methods. We see that both com-
bination strategies are effective and the MRRs of
the combined results are all better than those of the
two individual methods. It is interesting to see that
the benefit of applying the phonetic correspon-
dence model as a filter is quite significant. Indeed,
although the performance of the frequency corre-
lation method alone is much worse than that of the
phonetic correspondence method, when working
on the subset of candidates passing the phonetic
filter (i.e., those candidates that have a reasonable
phonetic alignment with the English name), it can
outperform the phonetic correspondence method.
This once again indicates that exploiting the fre-
quency correlation can be effective. When com-
bining the scores of these two methods, we not
only (implicitly) apply the phonetic filter, but also
exploit the discriminative power provided by the
phonetic correspondence scores and this is shown
to bring in additional benefit, giving the best per-
formance among all the methods.
4.4 Error Analysis
From the results above, we see that the MRRs for
the core English names are substantially higher
than those for all the English names. This means
that our methods perform very well whenever we
have the answer in our candidate list, but we have
also missed the answers for many English names.
The missing of an answer in the candidate list is
thus a major source of errors. To further under-
stand the upper bound of our method, we manu-
ally add the missing correct answers to our can-
didate set and apply all the methods to rank this
augmented set of candidates. The performance is
reported in Table 4 with the corresponding perfor-
mance on the original candidate set. We see that,
Method ALLMRR
Original Augmented
Phonetic 0.2999 0.7157
Freq 0.1360 0.3455
Freq+PhoneticFilter 0.3062 0.6232
Freq+PhoneticScore 0.3194 0.7338
Table 4: MRRs on the augmented candidate list.
as expected, the performance on the augmented
candidate list, which can be interpreted as an up-
per bound of our method, is indeed much better,
suggesting that if we can somehow improve the
candidate generation method to include the an-
swers in the list, we can expect to significantly im-
prove the performance for all the methods. This
is clearly an interesting topic for further research.
The relative performance of different methods on
this augmented candidate list is roughly the same
as on the original candidate list, except that the
?Freq+PhoneticFilter? is slightly worse than that
of the phonetic method alone, though it is still
much better than the performance of the frequency
correlation alone. One possible explanation may
be that since these names do not necessarily oc-
cur in our comparable corpora, we may not have
sufficient frequency observations for some of the
names.
78
Method AllMRR CoreMRR
init. CO MI init. CO MI
Freq+PhoneticFilter 0.3171 0.3255 0.3255 0.9058 0.9372 0.9372
Freq+PhoneticScore 0.3290 0.3373 0.3392 0.9422 0.9659 0.9573
Table 5: Effectiveness of score propagation.
4.5 Experiments on score propagation
To demonstrate that score propagation can further
help transliteration, we use the combination scores
in Table 3 as the initial scores, and apply our prop-
agation algorithm to iteratively update them. We
remove the entries when they do not co-occur with
others. There are 25 such English name candi-
dates. Thus, the initial scores are actually slightly
different from the values in Table 3. We show
the new scores and the best propagation scores in
Table 5. In the table, ?init.? refers to the initial
scores. and ?CO? and ?MI? stand for best scores
obtained using either the co-occurrence or mutual
information method. While both methods result
in gains, CO very slightly outperforms the MI ap-
proach. In the score propagation process, we in-
troduce two additional parameters: the interpola-
tion parameter ? and the number of iterations k.
Figure 3 and Figure 4 show the effects of these
parameters. Intuitively, we want to preserve the
initial score of a pair, but add a slight boost from
its neighbors. Thus, we set ? very close to 1 (0.9
and 0.95), and allow the system to perform 20 it-
erations. In both figures, the first few iterations
certainly leverage the transliteration, demonstrat-
ing that the propagation method works. However,
we observe that the performance drops when more
iterations are used, presumably due to noise intro-
duced from more distantly connected nodes. Thus,
a relatively conservative approach is to choose a
high ? value, and run only a few iterations. Note,
finally, that the CO method seems to be more sta-
ble than the MI method.
5 Conclusions and Future Work
In this paper we have discussed the problem of
Chinese-English name transliteration as one com-
ponent of a system to find matching names in com-
parable corpora. We have proposed two methods
for transliteration, one that is more traditional and
based on phonetic correspondences, and one that
is based on word distributions and adopts meth-
ods from information retrieval. We have shown
 0.76
 0.78
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 0  2  4  6  8  10  12  14  16  18  20
M
RR
 v
al
ue
s
number of iterations
alpha=0.9, MI
alpha=0.9, CO
alpha=0.95, MI
alpha=0.95, CO
Figure 3: Propagation: Core items
that both methods yield good results, and that even
better results can be achieved by combining the
methods. We have further showed that one can
improve upon the combined model by using rein-
forcement via score propagation when translitera-
tion pairs cluster together in document pairs.
The work we report is ongoing. We are inves-
tigating transliterations among several language
pairs, and are extending these methods to Ko-
rean, Arabic, Russian and Hindi ? see (Tao et al,
2006).
6 Acknowledgments
This work was funded by Dept. of the Interior con-
tract NBCHC040176 (REFLEX). We also thank
three anonymous reviewers for ACL06.
References
Lisa Ballesteros and W. Bruce Croft. 1998. Resolv-
ing ambiguity for cross-language retrieval. In Re-
search and Development in Information Retrieval,
pages 64?71.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks and ISDN Systems, 30:107?
117.
79
 0.28
 0.29
 0.3
 0.31
 0.32
 0.33
 0.34
 0  2  4  6  8  10  12  14  16  18  20
M
RR
 v
al
ue
s
number of iterations
alpha=0.9, MI
alpha=0.9, CO
alpha=0.95, MI
alpha=0.95, CO
Figure 4: Propagation: All items
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC CS Dept.
Martin Franz, J. Scott McCarley, and Salim Roukos.
1998. Ad hoc and multilingual information retrieval
at IBM. In Text REtrieval Conference, pages 104?
115.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proceedings of ACL 1995,
pages 236?243.
W. Gao, K.-F. Wong, and W. Lam. 2004. Phoneme-
based transliteration of foreign names for OOV
problem. In IJCNLP, pages 374?381, Sanya,
Hainan.
P. Kantor and E. Voorhees. 2000. The TREC-5 confu-
sion track: Comparing retrieval methods for scanned
text. Information Retrieval, 2:165?176.
M. Kay and M. Roscheisen. 1993. Text translation
alignment. Computational Linguistics, 19(1):75?
102.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. CL, 24(4).
J. Kruskal. 1999. An overview of sequence compar-
ison. In D. Sankoff and J. Kruskal, editors, Time
Warps, String Edits, and Macromolecules, chapter 1,
pages 1?44. CSLI, 2nd edition.
X. Li, P. Morie, and D. Roth. 2004. Robust reading:
Identification and tracing of ambiguous names. In
NAACL-2004.
J. Lin. 1991. Divergence measures based on the shan-
non entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
H. Masuichi, R. Flournoy, S. Kaufmann, and S. Peters.
2000. A bootstrapping method for extracting bilin-
gual text pairs.
H.M. Meng, W.K Lo, B. Chen, and K. Tang. 2001.
Generating phonetic cognates to handle named enti-
ties in English-Chinese cross-languge spoken doc-
ument retrieval. In Proceedings of the Automatic
Speech Recognition and Understanding Workshop.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of ACL 1995, pages
320?322.
Fatiha Sadat, Masatoshi Yoshikawa, and Shunsuke Ue-
mura. 2003. Bilingual terminology acquisition from
comparable corpora and phrasal translation to cross-
language information retrieval. In ACL ?03, pages
141?144.
G. Salton and M. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A
stochastic finite-state word-segmentation algorithm
for Chinese. CL, 22(3).
K. Tanaka and H. Iwasaki. 1996. Extraction of lexical
translation from non-aligned corpora. In Proceed-
ings of COLING 1996.
Tao Tao and ChengXiang Zhai. 2005. Mining compa-
rable bilingual text corpora for cross-language infor-
mation integration. In KDD?05, pages 691?696.
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard
Sproat, and ChengXiang Zhai. 2006. Unsupervised
named entity transliteration using temporal and pho-
netic correlation. In EMNLP 2006, Sydney, July.
P. Taylor, A. Black, and R. Caley. 1998. The archi-
tecture of the Festival speech synthesis system. In
Proceedings of the Third ESCA Workshop on Speech
Synthesis, pages 147?151, Jenolan Caves, Australia.
Ying Zhang and Phil Vines. 2004. Using the web for
automated translation extraction in cross-language
information retrieval. In SIGIR ?04, pages 162?169.
80
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 250?257,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Named Entity Transliteration Using Temporal and Phonetic
Correlation
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat and ChengXiang Zhai
University of Illinois at Urbana-Champaign
{syoon9,afister2,rws}@uiuc.edu, {taotao,czhai}@cs.uiuc.edu
Abstract
In this paper we investigate unsuper-
vised name transliteration using compara-
ble corpora, corpora where texts in the two
languages deal in some of the same top-
ics ? and therefore share references to
named entities ? but are not translations
of each other. We present two distinct
methods for transliteration, one approach
using an unsupervised phonetic translit-
eration method, and the other using the
temporal distribution of candidate pairs.
Each of these approaches works quite
well, but by combining the approaches
one can achieve even better results. We
believe that the novelty of our approach
lies in the phonetic-based scoring method,
which is based on a combination of care-
fully crafted phonetic features, and empiri-
cal results from the pronunciation errors of
second-language learners of English. Un-
like previous approaches to transliteration,
this method can in principle work with any
pair of languages in the absence of a train-
ing dictionary, provided one has an esti-
mate of the pronunciation of words in text.
1 Introduction
As a part of a on-going project on multilingual
named entity identification, we investigate unsu-
pervised methods for transliteration across lan-
guages that use different scripts. Starting from
paired comparable texts that are about the same
topic, but are not in general translations of each
other, we aim to find the transliteration correspon-
dences of the paired languages. For example, if
there were an English and Arabic newspaper on
the same day, each of the newspapers would likely
contain articles about the same important inter-
national events. From these comparable articles
across the two languages, the same named enti-
ties such as persons and locations would likely be
found. For at least some of the English named
entities, we would therefore expect to find Ara-
bic equivalents, many of which would in fact be
transliterations.
The characteristics of transliteration differ ac-
cording to the languages involved. In particular,
the exact transliteration of say, an English name
is highly dependent on the language since this will
be influenced by the difference in the phonological
systems of the language pairs. In order to show the
reliability of a multi-lingual transliteration model,
it should be tested with a variety of different lan-
guages. We have tested our transliteration meth-
ods with three unrelated target languages ? Ara-
bic, Chinese and Hindi, and a common source lan-
guage ? English. Transliteration from English to
Arabic and Chinese is complicated (Al-Onaizan
and Knight, 2002). For example, while Arabic or-
thography has a conventional way of writing long
vowels using selected consonant symbols ? ba-
sically <w>, <y> and <?>, in ordinary text
short vowels are rarely written. When transliter-
ating English names there is the option of repre-
senting the vowels as either short (i.e. unwrit-
ten) or long (i.e. written with one of the above
three mentioned consonant symbols). For exam-
ple London is transliterated as     lndn, with no
vowels; Washington often as  
	   wSnjTwn,
with <w> representing the final <o>. Transliter-
ations in Chinese are very different from the orig-
inal English pronunciation due to the limited syl-
lable structure and phoneme inventory of Chinese.
For example, Chinese does not allow consonant
clusters or coda consonants except [n, N], and this
results in deletion, substitution of consonants or
insertion of vowels. Thus while a syllable initial
/d/ may surface as in Baghdad  ba-ge-da,
note that the syllable final /d/ is not represented.
250
Hindi transliteration is not well-studied, but it is
in principle easier than Arabic and Chinese since
Hindi phonotactics is much more similar to that of
English.
2 Previous Work
Named entity transliteration is the problem of pro-
ducing, for a name in a source language, a set
of one or more transliteration candidates in a tar-
get language. Previous work ? e.g. (Knight and
Graehl, 1998; Meng et al, 2001; Al-Onaizan and
Knight, 2002; Gao et al, 2004) ? has mostly as-
sumed that one has a training lexicon of translit-
eration pairs, from which one can learn a model,
often a source-channel or MaxEnt-based model.
Comparable corpora have been studied exten-
sively in the literature ? e.g.,(Fung, 1995; Rapp,
1995; Tanaka and Iwasaki, 1996; Franz et al,
1998; Ballesteros and Croft, 1998; Masuichi et
al., 2000; Sadat et al, 2004), but transliteration
in the context of comparable corpora has not been
well addressed. The general idea of exploiting
time correlations to acquire word translations from
comparable corpora has been explored in several
previous studies ? e.g., (Fung, 1995; Rapp, 1995;
Tanaka and Iwasaki, 1996). Recently, a Pearson
correlation method was proposed to mine word
pairs from comparable corpora (Tao and Zhai,
2005); this idea is similar to the method used in
(Kay and Roscheisen, 1993) for sentence align-
ment. In our work, we adopt the method proposed
in (Tao and Zhai, 2005) and apply it to the problem
of transliteration; note that (Tao and Zhai, 2005)
compares several different metrics for time corre-
lation, as we also note below ? and see (Sproat et
al., 2006).
3 Transliteration with Comparable
Corpora
We start from comparable corpora, consisting of
newspaper articles in English and the target lan-
guages for the same time period. In this paper, the
target languages are Arabic, Chinese and Hindi.
We then extract named-entities in the English text
using the named-entity recognizer described in (Li
et al, 2004), which is based on the SNoW machine
learning toolkit (Carlson et al, 1999). To perform
transliteration, we use the following general ap-
proach: 1 Extract named entities from the English
corpus for each day; 2 Extract candidates from the
same day?s newspapers in the target language; 3
For each English named entity, score and rank the
target-language candidates as potential transliter-
ations. We apply two unsupervised methods ?
time correlation and pronunciation-based methods
? independently, and in combination.
3.1 Candidate scoring based on
pronunciation
Our phonetic transliteration score uses a standard
string-alignment and alignment-scoring technique
based on (Kruskal, 1999) in that the distance is de-
termined by a combination of substitution, inser-
tion and deletion costs. These costs are computed
from a language-universal cost matrix based on
phonological features and the degree of phonetic
similarity. (Our technique is thus similar to other
work on phonetic similarity such as (Frisch, 1996)
though details differ.) We construct a single cost
matrix, and apply it to English and all target lan-
guages. This technique requires the knowledge of
the phonetics and the sound change patterns of the
language, but it does not require a transliteration-
pair training dictionary. In this paper we assume
the WorldBet transliteration system (Hieronymus,
1995), an ASCII-only version of the IPA.
The cost matrix is constructed in the following
way. All phonemes are decomposed into stan-
dard phonological features. However, phonolog-
ical features alone are not enough to model the
possible substution/insertion/deletion patterns of
languages. For example, /h/ is more frequently
deleted than other consonants, whereas no single
phonological feature allows us to distinguish /h/
from other consonants. Similarly, stop and frica-
tive consonants such as /p, t, k, b, d, g, s, z/ are
frequently deleted when they appear in the coda
position. This tendency is very salient when the
target languages do not allow coda consonants or
consonant clusters. So, Chinese only allows [n,
N] in coda position, and stop consonants in coda
position are frequently lost; Stanford is translit-
erated as sitanfu, with the final /d/ lost. Since
phonological features do not consider the posi-
tion in the syllable, this pattern cannot be cap-
tured by conventional phonological features alone.
To capture this, an additional feature ?deletion
of stop/fricative consonant in the coda position?
is added. We base these observations, and the
concomitant pseudofeatures on pronunciation er-
ror data of learners of English as a second lan-
guage, as reported in (Swan and Smith, 2002). Er-
251
rors in second language pronunciation are deter-
mined by the difference in the phonological sys-
tem of learner?s first and second language. The
same substitution/deletion/insertion patterns in the
second language learner?s errors appear also in
the transliteration of foreign names. For exam-
ple, if the learner?s first language does not have
a particular phoneme found in English, it is sub-
stituted by the most similar phoneme in their first
language. Since Chinese does not have /v/, it is
frequently substituted by /w/ or /f/. This sub-
stitution occurs frequently in the transliteration
of foreign names in Chinese. Swan & Smith?s
study covers 25 languages, and includes Asian
languages such as Thai, Korean, Chinese and
Japanese, European languages such as German,
Italian, French, and Polish and Middle Eastern
languages such as Arabic and Farsi. Frequent sub-
stitution/insertion/deletion patterns of phonemes
are collected from these data. Some examples are
presented in Table 1.
Twenty phonological features and 14 pseud-
ofeatures are used for the construction of the cost
matrix. All features are classified into 5 classes.
There are 4 classes of consonantal features ?
place, manner, laryngeality and major (conso-
nant, sonorant, syllabicity), and a separate class
of vocalic features. The purpose of these classes
is to define groups of features which share the
same substitution/insertion/deletion costs. For-
mally, given a class C, and a cost CC , for each
feature f ? C, CC defines the cost of substitut-
ing a different value for f than the one present in
the source phoneme. Among manner features, the
feature continuous is classified separately, since
the substitution between stop and fricative con-
sonants is very frequent; but between, say, nasals
and fricatives such substitution is much less com-
mon. The cost for frequent sound change pat-
terns should be low. Based on our intuitions, our
pseudofeatures are classified into one or another
of the above-mentioned five classes. The substitu-
tion/deletion/insertion cost for a pair of phonemes
is the sum of the individual costs of the features
which are different between the two phonemes.
For example, /n/ and /p/ are different in sonorant,
labial and coronal features. Therefore, the substi-
tution cost of /n/ for /p/ is the sum of the sonorant,
labial and coronal cost (20+10+10 = 40). Features
and associated costs are shown in Table 2. Sam-
ple substitution, insertion, and deletion costs for
/g/ are presented in Table 3.
The resulting cost matrix based on these prin-
ciples is then used to calculate the edit distance
between two phonetic strings. Pronunciations for
English words are obtained using the Festival text-
to-speech system (Taylor et al, 1998), and the tar-
get language words are automatically converted
into their phonemic level transcriptions by various
language-dependent means. In the case of Man-
darin Chinese this is based on the standard pinyin
transliteration system. For Arabic this is based
on the orthography, which works reasonably well
given that (apart from the fact that short vowels
are no represented) the script is fairly phonemic.
Similarly, the pronunciation of Hindi can be rea-
sonably well-approximated based on the standard
Devanagari orthographic representation. The edit
cost for the pair of strings is normalized by the
number of phonemes. The resulting score ranges
from zero upwards; the score is used to rank can-
didate transliterations, with the candidate having
the lowest cost being considered the most likely
transliteration. Some examples of English words
and the top three ranking candidates among all of
the potential target-language candidates are given
in Table 4.1 Starred entries are correct.
3.2 Candidate scoring based on time
correlation
Names of the same entity that occur in different
languages often have correlated frequency patterns
due to common triggers such as a major event. For
example, the 2004 tsunami disaster was covered
in news articles in many different languages. We
would thus expect to see a peak of frequency of
names such as Sri Lanka, India, and Indonesia in
news articles published in multiple languages in
the same time period. In general, we may expect
topically related names in different languages to
tend to co-occur together over time. Thus if we
have comparable news articles over a sufficiently
long time period, it is possible to exploit such cor-
relations to learn the associations of names in dif-
ferent languages.
The idea of exploiting time correlation has been
well studied. We adopt the method proposed in
(Tao and Zhai, 2005) to represent the source name
and each name candidate with a frequency vector
and score each candidate by the similarity of the
1We describe candidate selection for each of the target
languages later.
252
Input Output Position
D D, d, z everywhere
T T, t, s everywhere
N N, n, g everywhere
p/t/k deletion coda
Table 1: Substitution/insertion/deletion patterns for phonemes based on English second-language
learner?s data reported in (Swan and Smith, 2002). Each row shows an input phoneme class, possi-
ble output phonemes (including null), and the positions where the substitution (or deletion) is likely to
occur.
Class Feature Cost
Major features and Consonant Del consonant 20
sonorant
consonant deletion
Place features and Vowel Del coronal 10
vowel del/ins
stop/fricative consonant del at coda position
h del/ins
Manner features nasal 5
dorsal feature for palatal consonants
Vowel features and Exceptions vowel height 3
vowel place
exceptional
Manner/ Laryngeal features continuous 1.5
voicing
Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional
denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between
these two sounds actually occur frequently in second-language error data.
two frequency vectors. This is very similar to the
case in information retrieval where a query and a
document are often represented by a term vector
and documents are ranked by the similarity be-
tween their vectors and the query vector (Salton
and McGill, 1983). But the vectors are very dif-
ferent and should be constructed in quite differ-
ent ways. Following (Tao and Zhai, 2005), we
also normalize the raw frequency vector so that
it becomes a frequency distribution over all the
time points. In order to compute the similarity be-
tween two distribution vectors ~x = (x1, ..., xT )
and ~y = (y1, ..., yT ), the Pearson correlation co-
efficient was used in (Tao and Zhai, 2005). We
also consider two other commonly used measures
? cosine (Salton and McGill, 1983), and Jensen-
Shannon divergence (Lin, 1991), though our re-
sults show that Pearson correlation coefficient per-
forms better than these two other methods. Since
the time correlation method and the phonetic cor-
respondence method exploit distinct resources, it
makes sense to combine them. We explore two ap-
proaches to combining these two methods, namely
score combination and rank combination. These
will be defined below in Section 4.2.
4 Experiments
We evaluate our algorithms on three compara-
ble corpora: English/Arabic, English/Chinese, and
English/Hindi. Data statistics are shown in Ta-
ble 5.
From each data set in Table 5, we picked out all
news articles from seven randomly selected days.
We identified about 6800 English names using the
entity recognizer from (Carlson et al, 1999), and
chose the most frequent 200 names as our English
named entity candidates. Note that we chose the
most frequent names because the reliability of the
statistical correlation depends on the size of sam-
ple data. When a name is rare in a collection,
253
Source Target Cost Target Cost
g g 0 r 40.5
kh 2.5 e 44.5
cCh 5.5 del 24
tsh 17.5 ins 20
N 26.5
Table 3: Substitution/deletion/insertion costs for /g/.
English Candidate
Script Worldbet
Philippines 1       
 
 f l b y n
*2     
	    
 
 f l b y n y t
3            f l b y n a
Megawati *1 
 
 
  m h a f th
2          m i j a w a t a
3        m a k w z a
English Candidate
Script Romanization Worldbet
Belgium *1 