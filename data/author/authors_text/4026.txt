Incremental Parsing, or Incremental Grammar??
Matthew Purver? and Ruth Kempson?
Departments of ?Computer Science and ?Philosophy,
King?s College London, Strand,
London WC2R 2LS,
UK
{matthew.purver, ruth.kempson}@kcl.ac.uk
Abstract
Standard grammar formalisms are defined with-
out reflection of the incremental and serial na-
ture of language processing, and incremental-
ity must therefore be reflected by independently
defined parsing and/or generation techniques.
We argue that this leads to a poor setup for
modelling dialogue, with its rich speaker-hearer
interaction, and instead propose context-based
parsing and generation models defined in terms
of an inherently incremental grammar formal-
ism (Dynamic Syntax), which allow a straight-
forward model of otherwise problematic dia-
logue phenomena such as shared utterances, el-
lipsis and alignment.
1 Introduction
Despite increasing psycholinguistic evidence of
incrementality in language processing, both in
parsing (see e.g. (Crocker et al, 2000)) and in
production (Ferreira, 1996), there is almost uni-
versal agreement that this should not be re-
flected in grammar formalisms which constitute
the underlying model of language (for rare ex-
ceptions, see (Hausser, 1989; Kempson et al,
2001)). Constraint-based grammar formalisms
are accordingly defined neutrally between either
of these applications, with parsing/generation
systems (whether incremental or not) defined
as independent architectures manipulating the
same underlying system.1 Such assumptions
however lead to formal architectures that are
relatively poorly set up for modelling dialogue,
for they provide no basis for the very rich de-
gree of interaction between participants in dia-
logue. A common phenomenon in dialogue is
? Related papers from the point of view of generation
rather than parsing, and from the point of view of align-
ment rather than incrementality, are to be presented at
INLG?04 and Catalog?04 respectively.
1Authors vary as to the extent to which these archi-
tectures might be defined to be reversible. See (Neu-
mann, 1994).
that of shared utterances (Clark and Wilkes-
Gibbs, 1986), with exchange of roles of parser
and producer midway through an utterance:2
(1)
Daniel: Why don?t you stop mumbling
and
Marc: Speak proper like?
Daniel: speak proper?
(2) Ruth: What did Alex . . .Hugh: Design? A kaleidoscope.
Such utterances clearly show the need for a
strictly incremental model: however, they are
particularly problematic for any overall archi-
tecture in which parsing and generation are in-
dependently defined as applications of a use-
neutral grammar formalism which yields as out-
put the set of well-formed strings, for in these
types of exchange, the string uttered by one and
parsed by the other need not be a wellformed
string in its own right, so will not fall within
the set of data which the underlying formalism
is set up to capture. Yet, with the transition be-
tween interlocutors seen as a shift from one sys-
tem to another, each such substring will have to
be characterised independently. Many other di-
alogue phenomena also show the need for inter-
action between the parsing and generation pro-
cesses, among them cross-speaker ellipsis (e.g.
simple bare fragment answers to wh-questions),
and alignment (Pickering and Garrod, 2004), in
which conversational participants mirror each
other?s patterns at many levels (including lexi-
cal choice and syntactic structure).
The challenge of being able to model these
phenomena, problematic for theorists but ex-
tremely easy and natural for dialogue partici-
pants themselves, has recently been put out by
Pickering and Garrod (2004) as a means of eval-
uating both putative grammar formalisms and
2Example (1) from the BNC, file KNY (sentences
315?317).
models of language use. In response to this chal-
lenge, we suggest that an alternative means of
evaluating parsing implementations is by eval-
uation of paired parsing-generation models and
the dialogue model that results. As an illustra-
tion of this, we show how if we drop the assump-
tion that grammar formalisms are defined neu-
trally between parsing and production in favour
of frameworks in which the serial nature of lan-
guage processing is a central design feature (as
in Dynamic Syntax: (Kempson et al, 2001)),
then we can define a model in which incremental
sub-systems of parsing and generation are nec-
essarily tightly coordinated, and can thus pro-
vide a computational model of dialogue which
directly corresponds with currently emerging
psycholinguistic results (Branigan et al, 2000).
In particular, by adding a shared model of con-
text to previously defined word-by-word incre-
mental parsing and generation models, we show
how the switch in speaker/hearer roles during
a shared utterance can be seen as a switch be-
tween processes which are directed by different
goals, but which share the same incrementally
built data structures. We then show how this
inherent incrementality and structure/context
sharing also allows a straightforward model of
cross-speaker ellipsis and alignment.
2 Background
Dynamic Syntax (DS) (Kempson et al, 2001) is
a parsing-directed grammar formalism in which
a decorated tree structure representing a se-
mantic interpretation for a string is incremen-
tally projected following the left-right sequence
of the words. Importantly, this tree is not a
model of syntactic structure, but is strictly se-
mantic, being a representation of the predicate-
argument structure of the sentence. In DS,
grammaticality is defined as parsability (the
successful incremental construction of a tree-
structure logical form, using all the information
given by the words in sequence): there is no cen-
tral use-neutral grammar of the kind assumed
by most approaches to parsing and/or gener-
ation. The logical forms are lambda terms of
the epsilon calculus (see (Meyer-Viol, 1995) for
a recent development), so quantification is ex-
pressed through terms of type e whose complex-
ity is reflected in evaluation procedures that ap-
ply to propositional formulae once constructed,
and not in the tree itself. With all quantification
expressed as type e terms, the standard grounds
for mismatch between syntactic and semantic
analysis for all NPs is removed; and, indeed, all
syntactic distributions are explained in terms of
this incremental and monotonic growth of par-
tial representations of content. Hence the claim
that the model itself constitutes a NL grammar
formalism.
Parsing (Kempson et al, 2001) defines pars-
ing as a process of building labelled semantic
trees in a strictly left-to-right, word-by-word in-
cremental fashion by using computational ac-
tions and lexical actions defined (for some natu-
ral language) using the modal tree logic LOFT
(Blackburn and Meyer-Viol, 1994). These ac-
tions are defined as transition functions be-
tween intermediate states, which monotonically
extend tree structures and node decorations.
Words are specified in the lexicon to have as-
sociated lexical actions: the (possibly partial)
semantic trees are monotonically extended by
applying these actions as each word is consumed
from the input string. Partial trees may be un-
derspecified: tree node relations may be only
partially specified; node decorations may be de-
fined in terms of unfulfilled requirements and
metavariables; and trees may lack a full set of
scope constraints. Anaphora resolution is a fa-
miliar case of update: pronouns are defined to
project metavariables that are substituted from
context as part of the construction process. Rel-
ative to the same tree-growth dynamics, long-
distance dependency effects are characterised
through restricted licensing of partial trees with
relation between nodes introduced with merely
a constraint on some fixed extension (following
D-Tree grammar formalisms (Marcus, 1987)),
an underspecification that gets resolved within
the left-to-right construction process.3 Quanti-
fying terms are also built up using determiner
and noun to yield a partially specified term e.g.
(, y, Man?(y)) with a requirement for a scope
statement. These scope statements, of the form
x < y (?the term binding x is to be evaluated
as taking scope over the term binding y?), are
added to a locally dominating type-t-requiring
node. Generally, they are added to an accu-
mulating set following the serial order of pro-
cessing in determining the scope dependency,
but indefinites (freer in scope potential) are as-
signed a metavariable as first argument, allow-
3In this, the system is also like LFG, modelling long-
distance dependency in the same terms as functional un-
certainty (Kaplan and Zaenen, 1989), differing from that
concept in the dynamics of update internal to the con-
struction of a single tree.
Figure 1: Parsing ?john likes mary? . . . . . . and generating ?john likes mary?
{}
{john?} {?}
john
{}
{john?} {}
{like?} {?}
likes
{like?(mary?)(john?),?}
{john?} {like?(mary?)}
{like?} {mary?}
mary
{}
{john?} {?}
FAIL FAIL
john
likes mary
{}
{john?} {}
{like?} {?}
FAIL
likes
mary
{like?(mary?)(john?),?}
{john?} {like?(mary?)}
{like?} {mary?}
mary
ing selection from any term already added, in-
cluding temporally-sorted variables associated
with tense/modality specifications. The gen-
eral mechanism is the incremental analogue of
quantifier storage; and once a propositional for-
mula of type t has been derived at a node with
some collection of scope statements, these are
jointly evaluated to yield fully expanded terms
that reflect all relative dependencies within the
restrictor of the terms themselves. For example,
parsing A man coughed yields the pair Si < x,
Cough?(, x, Man?(x)) (Si the index of evalua-
tion), then evaluated as Man?(a) ? Cough?(a)
where a = (, x, Man?(x) ? Cough?(x)).4
Once all requirements are satisfied and all
partiality and underspecification is resolved,
trees are complete, parsing is successful and the
input string is said to be grammatical. Central
to the formalism is the incremental and mono-
tonic growth of labelled partial trees: the parser
state at any point contains all the partial trees
which have been produced by the portion of the
string so far consumed and which remain can-
didates for completion.5
4For formal details of this approach to quantification
see (Kempson et al, 2001) chapter 7; for an early imple-
mentation see (Kibble et al, 2001).
5Figure 1 assumes, simplistically, that linguistic
names correspond directly to scopeless names in the log-
Generation (Otsuka and Purver, 2003;
Purver and Otsuka, 2003) (hereafter O&P)
give an initial method of context-independent
tactical generation based on the same incre-
mental parsing process, in which an output
string is produced according to an input
semantic tree, the goal tree. The generator
incrementally produces a set of corresponding
output strings and their associated partial trees
(again, on a left-to-right, word-by-word basis)
by following standard parsing routines and
using the goal tree as a subsumption check.
At each stage, partial strings and trees are
tentatively extended using some word/action
pair from the lexicon; only those candidates
which produce trees which subsume the goal
tree are kept, and the process succeeds when
a complete tree identical to the goal tree is
produced. Generation and parsing thus use
the same tree representations and tree-building
actions throughout.
3 Contextual Model
The current proposed model (and its imple-
mentation) is based on these earlier definitions
but modifies them in several ways, most signif-
icantly by the addition of a model of context:
ical language that decorate the tree.
while they assume some notion of context they
give no formal model or implementation.6 The
contextual model we now assume is made up not
only of the semantic trees built by the DS pars-
ing process, but also the sequences of words and
associated lexical actions that have been used
to build them. It is the presence of (and as-
sociations between) all three, together with the
fact that this context is equally available to both
parsing and generation processes, that allow our
straightforward model of dialogue phenomena.7
For the purposes of the current implementa-
tion, we make a simplifying assumption that
the length of context is finite and limited to the
result of some immediately previous parse (al-
though information that is independently avail-
able can be represented in the DS tree format,
so that, in reality, larger and only partially or-
dered contexts are no doubt possible): context
at any point is therefore made up of the trees
and word/action sequences obtained in parsing
the previous sentence and the current (incom-
plete) sentence.
Parsing in Context A parser state is there-
fore defined to be a set of triples ?T, W, A?,
where T is a (possibly partial) semantic tree,8
W the sequence of words and A the sequence
of lexical and computational actions that have
been used in building it. This set will initially
contain only a single triple ?Ta, ?, ?? (where Ta
is the basic axiom taken as the starting point of
the parser, and the word and action sequences
are empty), but will expand as words are con-
sumed from the input string and the corre-
sponding actions produce multiple possible par-
tial trees. At any point in the parsing process,
the context for a particular partial tree T in
6There are other departures in the treatment of linked
structures (for relatives and other modifiers) and quan-
tification, and more relevantly to improve the incremen-
tality of the generation process: we do not adopt the
proposal of O&P to speed up generation by use of a re-
stricted multiset of lexical entries selected on the basis
of goal tree features, which prevents strictly incremental
generation and excludes modification of the goal tree.
7In building n-tuples of trees corresponding to
predicate-argument structures, the system is similar to
LTAG formalisms (Joshi and Kulick, 1997). However,
unlike LTAG systems (see e.g. (Stone and Doran, 1997)),
both parsing and generation are not head-driven, but
fully (word-by-word) incremental. This has the ad-
vantage of allowing fully incremental models for all
languages, matching psycholinguistic observations (Fer-
reira, 1996).
8Strictly speaking, scope statements should be in-
cluded in these n-tuples ? for now we consider them as
part of the tree.
this set can then be taken to consist of: (a) a
similar triple ?T0, W0, A0? given by the previous
sentence, where T0 is its semantic tree repre-
sentation, W0 and A0 the sequences of words
and actions that were used in building it; and
(b) the triple ?T, W, A? itself. Once parsing is
complete, the final parser state, a set of triples,
will form the new starting context for the next
sentence. In the simple case where the sentence
is unambiguous (or all ambiguity has been re-
moved) this set will again have been reduced
to a single triple ?T1, W1, A1?, corresponding to
the final interpretation of the string T1 with its
sequence of words W1 and actions A1, and this
replaces ?T0, W0, A0? as the new context; in the
presence of persistent ambiguity there will sim-
ply be more than one triple in the new context.9
Generation in Context A generator state
is now defined as a pair (Tg, X) of a goal tree
Tg and a set X of pairs (S, P ), where S is a
candidate partial string and P is the associated
parser state (a set of ?T, W, A? triples). Ini-
tially, the set X will usually contain only one
pair, of an empty candidate string and the stan-
dard initial parser state, (?, {?Ta, ?, ??}). How-
ever, as both parsing and generation processes
are strictly incremental, they can in theory start
from any state. The context for any partial tree
T is defined exactly as for parsing: the previ-
ous sentence triple ?T0, W0, A0?; and the cur-
rent triple ?T, W, A?. Generation and parsing
are thus very closely coupled, with the central
part of both processes being a parser state: a set
of tree/word-sequence/action-sequence triples.
Essential to this correspondence is the lack of
construction of higher-level hypotheses about
the state of the interlocutor. All transitions
are defined over the context for the individ-
ual (parser or generator). In principle, con-
texts could be extended to include high-level
hypotheses, but these are not essential and are
not implemented in our model (see (Millikan,
2004) for justification of this stance).
4 Shared Utterances
One primary evidence for this close coupling
and sharing of structures and context is the ease
with which shared utterances can be expressed.
O&P suggest an analysis of shared utterances,
9The current implementation of the formalism does
not include any disambiguation mechanism. We simply
assume that selection of some (minimal) context and at-
tendant removal of any remaining ambiguity is possible
by inference.
Figure 2: Transition from hearer to speaker: ?What did Alex . . . / . . . design??
Pt =
?
{+Q}
{WH} {alex?}{?Ty(e ? t),?}
, {what, did, alex}, {a1, a2, a3}
?
Gt =
(
{+Q, design?(WH)(alex?)}
{alex?} {design(WH)}
{WH}{design?}
,
(
?,
?
{+Q}
{WH} {alex?}{?Ty(e ? t),?}
, {what, did, alex}, {a1, a2, a3}
?)
)
G1 =
(
{+Q, design?(WH)(alex?)}
{alex?} {design?(WH)}
{WH}{design?}
,
(
{design},
?
{+Q}
{WH}{alex?} {?Ty(e ? t)}
{?}{design?}
, {. . . , design}, {. . . , a4}
?)
)
and this can now be formalised given the cur-
rent model. As the parsing and generation pro-
cesses are both fully incremental, they can start
from any state (not just the basic axiom state
?Ta, ?, ??). As they share the same lexical en-
tries, the same context and the same semantic
tree representations, a model of the switch of
roles now becomes relatively straightforward.
Transition from Hearer to Speaker Nor-
mally, the generation process begins with
the initial generator state as defined above:
(Tg, {(?, P0)}), where P0 is the standard initial
?empty? parser state {?Ta, ?, ??}. As long as a
suitable goal tree Tg is available to guide gen-
eration, the only change required to generate a
continuation from a heard partial string is to
replace P0 with the parser state (a set of triples
?T, W, A?) as produced from that partial string:
we call this the transition state Pt. The initial
hearer A therefore parses as usual until transi-
tion,10 then given a suitable goal tree Tg, forms
a transition generator state Gt = (Tg, {(?, Pt)}),
from which generation can begin directly ? see
figure 2.11 Note that the context does not
change between processes.
For generation to begin from this transition
state, the new goal tree Tg must be subsumed
by at least one of the partial trees in Pt (i.e.
the proposition to be expressed must be sub-
sumed by the incomplete proposition that has
been built so far by the parser). Constructing
10We have little to say about exactly when transitions
occur. Presumably speaker pauses and the availability
to the hearer of a possible goal tree both play a part.
11Figure 2 contains several simplifications to aid read-
ability, both to tree structure details and by show-
ing parser/generator states as single triples/pairs rather
than sets thereof.
Tg prior to the generation task will often be a
complex process involving inference and/or ab-
duction over context and world/domain knowl-
edge ? Poesio and Rieser (2003) give some idea
as to how this inference might be possible ? for
now, we make the simplifying assumption that
a suitable propositional structure is available.
Transition from Speaker to Hearer At
transition, the initial speaker B?s generator
state G?t contains the pair (St, P ?t), where St is
the partial string output so far, and P ?t is the
corresponding parser state, the transition state
as far as B is concerned.12 In order for B to
interpret A?s continuation, B need only use P ?t
as the initial parser state which is extended as
the string produced by A is consumed.
As there will usually be multiple possible par-
tial trees at the transition point, A may con-
tinue in a way that does not correspond to B?s
initial intentions ? i.e. in a way that does not
match B?s initial goal tree. For B to be able
to understand such continuations, the genera-
tion process must preserve all possible partial
parse trees (just as the parsing process does),
whether they subsume the goal tree or not, as
long as at least one tree in the current state does
subsume the goal tree. A generator state must
therefore rule out only pairs (S, P ) for which P
contains no trees which subsume the goal tree,
rather than thinning the set P directly via the
subsumption check as proposed by O&P.
It is the incrementality of the underlying
grammar formalism that allows this simple
switch: the parsing process can begin directly
12Of course, if both A and B share the same lexical
entries and communication is perfect, Pt = P ?t , but we
do not have to assume that this is the case.
from a state produced by an incomplete gener-
ation process, and vice versa, as their interme-
diate representations are necessarily the same.
5 Cross-Speaker Ellipsis
This inherent close coupling of the two incre-
mental processes, together with the inclusion
of tree-building actions in the model of con-
text, also allows a simple analysis of many cross-
speaker elliptical phenomena.
Fragments Bare fragments (3) may be anal-
ysed as taking a previous structure from con-
text as a starting point for parsing (or genera-
tion). WH -expressions are analysed as partic-
ular forms of metavariables, so parsing a wh-
question yields a type-complete but open for-
mula, which the term presented by a subsequent
fragment can update:
(3) A: What did you eat for breakfast?B: Porridge.
Parsing the fragment involves constructing an
unfixed node, and merging it with the contex-
tually available structure, so characterising the
wellformedness/interpretation of fragment an-
swers to questions without any additional mech-
anisms: the term (, x, porridge?(x)) stands in a
licensed growth relation from the metavariable
WH provided by the lexical actions of what.
Functional questions (Ginzburg and Sag,
2000) with their fragment answers (4) pose
no problem. As the wh-question contains a
metavariable, the scope evaluation cannot be
completed; completion of structure and evalu-
ation of scope can then be effected by merg-
ing in the term the answer provides, identifying
any introduced metavariable in this context (the
genitive imposes narrow scope of the introduced
epsilon term):
(4) A: Who did every student ignore?B: Their supervisor.
{Si < x}
{(?, x, stud?(x))} {}
{WH,?} {ignr?}
? {Si < x, x < y}
{(?, x, stud?(x))} {}
{(, y, sup?(x)(y)}{ignr?}
VP Ellipsis Anaphoric devices such as pro-
nouns and VP ellipsis are analysed as decorating
tree nodes with metavariables licensing update
from context using either established terms, or,
for ellipsis, (lexical) tree-update actions. Strict
readings of VP ellipsis result from taking a suit-
able semantic formula directly from a tree node
in context: any node n ? (T0 ? T ) of suitable
type (e ? t) with no outstanding requirements.
Sloppy readings involve re-use of actions: any
sequence of actions (a1; a2; . . . ; an) ? (A0 ? A)
can be used (given the appropriate elliptical
trigger) to extend the current tree T if this pro-
vides a formula of type e ? t.13 This latter
approach, combined with the representation of
quantified elements as terms, allows a range of
phenomena, including those which are problem-
atic for other (abstraction-based) approaches
(for discussion see (Dalrymple et al, 1991)):
(5)
A: A policeman who arrested Bill read
him his rights.
B: The policeman who arrested Tom
did too.
The actions associated with A?s use of read
him his rights in (5) include the projection of a
metavariable associated with him, and its res-
olution to the term in context associated with
Bill. B?s ellipsis allows this action sequence to
be re-used, again projecting a metavariable and
resolving it, this time (given the new context) to
the term provided by parsing Tom. This leads
to a copy of Tom within the constructed predi-
cate, and a sloppy reading.
This analysis also applies to yield parallellism
effects in scoping (Hirschbu?hler, 1982; Shieber
et al, 1996), allowing narrow scope construal
for indefinites in subject position:
(6) A: A nurse interviewed every patient.B: An orderly did too.
Resolution of the underspecification in the
scope statement associated with an indefinite
can be performed at two points: either at the
immediate point of processing the lexical ac-
tions, or at the later point of compiling the re-
sulting node?s interpretation within the emer-
gent tree.14 In (6), narrow scope can be as-
signed to the subject in A?s utterance via this
late assignment of scope; at this late point in the
13In its re-use of actions provided by context, this ap-
proach to ellipsis is essentially similar to the glue lan-
guage approach (see (Asudeh and Crouch, 2002) and
papers in (Dalrymple, 1999) but, given the lack of in-
dependent syntax /semantics vocabularies, the need for
an intermediate mapping language is removed.
14This pattern parallels expletive pronouns which
equally allow a delayed update (Cann, 2003).
parse process, the term constructed from the ob-
ject node will have been entered into the set of
scope statements, allowing the subject node to
be dependent on the following quantified expres-
sion. The elliptical word did in B?s utterance
will then license re-use of these late actions, re-
peating the procedures used in interpreting A?s
antecedent and so determining scope of the new
subject relative to the object.
Again, these analyses are possible because
parsing and generation processes share incre-
mentally built structures and contextual pars-
ing actions, with this being ensured by the in-
crementality of the grammar formalism itself.
6 Alignment & Routinization
The parsing and generation processes must both
search the lexicon for suitable entries at ev-
ery step (i.e. when parsing or generating each
word). For generation in particular, this is a
computationally expensive process in principle:
every possible word/action pair must be tested ?
the current partial tree extended and the result
checked for goal tree subsumption. As proposed
by O&P (though without formal definitions or
implementation) our model of context now al-
lows a strategy for minimising this effort: as
it includes previously used words and actions,
a subset of such actions can be re-used in ex-
tending the current tree, avoiding full lexical
search altogether. High frequency of elliptical
constructions is therefore expected, as ellipsis
licenses such re-use; the same can be said for
pronouns, as long as they (and their correspond-
ing actions) are assumed to be pre-activated or
otherwise readily available from the lexicon.
As suggested by O&P, this can now lead di-
rectly to a model of alignment phenomena, char-
acterisable as follows. For the generator, if there
is some action a ? (A0 ?A) suitable for extend-
ing the current tree, a can be re-used, generat-
ing the word w which occupies the correspond-
ing position in the sequence W0 or W . This re-
sults in lexical alignment ? repeating w rather
than choosing an alternative word from the lex-
icon. Alignment of syntactic structure (e.g. pre-
serving double-object or full PP forms in the use
of a verb such as give rather than shifting to
the semantically equivalent form (Branigan et
al., 2000)) also follows in virtue of the procedu-
ral action-based specification of lexical content.
A word such as give has two possible lexical
actions a? and a?? despite semantic equivalence
of output, corresponding to the two alternative
forms. A previous use will cause either a? or a??
to be present in (A0 ? A); re-use of this action
will cause the same form to be repeated.15
A similar definition holds for the parser: for a
word w presented as input, if w ? (W0?W ) then
the corresponding action a in the sequence A0 or
A can be used without consulting the lexicon.
Words will therefore be interpreted as having
the same sense or reference as before, modelling
the semantic alignment described by (Garrod
and Anderson, 1987). These characterisations
can also be extended to sequences of words ?
a sub-sequence (a1; a2; . . . ; an) ? (A0 ? A) can
be re-used by a generator, producing the cor-
responding word sequence (w1; w2; . . . ; wn) ?
(W0 ? W ); and similarly the sub-sequence of
words (w1; w2; . . . ; wn) ? (W0 ? W ) will cause
the parser to use the corresponding action se-
quence (a1; a2; . . . ; an) ? (A0 ? A). This will
result in sequences or phrases being repeatedly
associated by both parser and generator with
the same sense or reference, leading to what
Pickering and Garrod (2004) call routinization
(construction and re-use of word sequences with
consistent meanings).
It is notable that these various patterns of
alignment, said by Pickering and Garrod (2004)
to be alignment across different levels, are ex-
pressible without invoking distinct levels of syn-
tactic or lexical structure, since context, content
and lexical actions are all defined in terms of the
same tree configurations.
7 Summary
The inherent left-to-right incrementality and
monotonicity of DS as a grammar formalism al-
lows both parsing and generation processes to
be not only incremental but closely coupled,
sharing structures and context. This enables
shared utterances, cross-speaker elliptical phe-
nomena and alignment to be modelled straight-
forwardly. A prototype system has been im-
plemented in Prolog which reflects the model
given here, demonstrating shared utterances
and alignment phenomena in simple dialogue
sequences. The significance of this direct re-
flection of psycholinguistic data is to buttress
the DS claim that the strictly serial incremen-
tality of processing is not merely essential to
the modelling of natural-language parsing, but
15Most frameworks would have to reflect this via pref-
erences defined over syntactic rules or parallelisms with
syntactic trees in context, both problematic.
to the design of the underlying grammar formal-
ism itself.
Acknowledgements
This paper is an extension of joint work on the
DS framework with Wilfried Meyer-Viol, on ex-
pletives and on defining a context-dependent
formalism with Ronnie Cann, and on DS gen-
eration with Masayuki Otsuka. Each has pro-
vided ideas and input without which the cur-
rent results would have differed, although any
mistakes here are ours. Thanks are also due to
the ACL reviewers. This work was supported
by the ESRC (RES-000-22-0355) and (for the
second author) by the Leverhulme Trust.
References
A. Asudeh and R. Crouch. 2002. Derivational
parallelism and ellipsis parallelism. In Pro-
ceedings of WCCFL 21.
P. Blackburn and W. Meyer-Viol. 1994. Lin-
guistics, logic and finite trees. Bulletin of the
IGPL, 2:3?31.
H. Branigan, M. Pickering, and A. Cleland.
2000. Syntactic co-ordination in dialogue.
Cognition, 75:13?25.
R. Cann. 2003. Semantic underspecification
and the interpretation of copular clauses in
English. In Where Semantics Meets Pragmat-
ics. University of Michigan.
H. H. Clark and D. Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition,
22:1?39.
M. Crocker, M. Pickering, and C. Clifton, ed-
itors. 2000. Architectures and Mechanisms
in Sentence Comprehension. Cambridge Uni-
versity Press.
M. Dalrymple, S. Shieber, and F. Pereira. 1991.
Ellipsis and higher-order unification. Linguis-
tics and Philosophy, 14(4):399?452.
M. Dalrymple, editor. 1999. Syntax and Se-
mantics in Lexical Functional Grammar: The
Resource-Logic Approach. MIT Press.
V. Ferreira. 1996. Is it better to give than to
donate? Syntactic flexibility in language pro-
duction. Journal of Memory and Language,
35:724?755.
S. Garrod and A. Anderson. 1987. Saying what
you mean in dialogue. Cognition, 27:181?218.
J. Ginzburg and I. A. Sag. 2000. Interrogative
Investigations. CSLI Publications.
R. Hausser. 1989. Computation of Language.
Springer-Verlag.
P. Hirschbu?hler. 1982. VP deletion and across-
the-board quantifier scope. In Proceedings of
NELS 12.
A. Joshi and S. Kulick. 1997. Partial proof trees
as building blocks for a categorial grammar.
Linguistics and Philosophy, 20:637?667.
R. Kaplan and A. Zaenen. 1989. Long-
distance dependencies, constituent structure,
and functional uncertainty. In M. Baltin and
A. Kroch, editors, Alternative Conceptions of
Phrase Structure, pages 17?42. University of
Chicago Press.
R. Kempson, W. Meyer-Viol, and D. Gabbay.
2001. Dynamic Syntax: The Flow of Lan-
guage Understanding. Blackwell.
R. Kibble, W. Meyer-Viol, D. Gabbay, and
R. Kempson. 2001. Epsilon terms: a la-
belled deduction account. In H. Bunt and
R. Muskens, editors, Computing Meaning.
Kluwer Academic Publishers.
M. Marcus. 1987. Deterministic parsing and
description theory. In P. Whitelock et al, ed-
itor, Linguistic Theory and Computer Appli-
cations, pages 69?112. Academic Press.
W. Meyer-Viol. 1995. Instantial Logic. Ph.D.
thesis, University of Utrecht.
R. Millikan. 2004. The Varieties of Meaning.
MIT Press.
G. Neumann. 1994. A Uniform Computa-
tional Model for Natural Language Parsing
and Generation. Ph.D. thesis, Universita?t des
Saarlandes.
M. Otsuka and M. Purver. 2003. Incremental
generation by incremental parsing. In Pro-
ceedings of the 6th CLUK Colloquium.
M. Pickering and S. Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behav-
ioral and Brain Sciences, forthcoming.
M. Poesio and H. Rieser. 2003. Coordination in
a PTT approach to dialogue. In Proceedings
of the 7th Workshop on the Semantics and
Pragmatics of Dialogue (DiaBruck).
M. Purver and M. Otsuka. 2003. Incremental
generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceed-
ings of the 9th European Workshop in Natural
Language Generation (ENLG-2003).
S. Shieber, F. Pereira, and M. Dalrymple. 1996.
Interactions of scope and ellipsis. Linguistics
and Philosophy, 19:527?552.
M. Stone and C. Doran. 1997. Sentence plan-
ning as description using tree-adjoining gram-
mar. In Proceedings of the 35th Annual Meet-
ing of the ACL, pages 198?205.
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 74?81,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Incrementality, Speaker-Hearer Switching
and the Disambiguation Challenge
Ruth Kempson, Eleni Gregoromichelaki
King?s College London
{ruth.kempson, eleni.gregor}@kcl.ac.uk
Yo Sato
University of Hertfordshire
y.sato@herts.ac.uk
Abstract
Taking so-called split utterances as our
point of departure, we argue that a new
perspective on the major challenge of dis-
ambiguation becomes available, given a
framework in which both parsing and gen-
eration incrementally involve the same
mechanisms for constructing trees reflect-
ing interpretation (Dynamic Syntax: (Cann
et al, 2005; Kempson et al, 2001)). With
all dependencies, syntactic, semantic and
pragmatic, defined in terms of incremental
progressive tree growth, the phenomenon
of speaker/hearer role-switch emerges as
an immediate consequence, with the po-
tential for clarification, acknowledgement,
correction, all available incrementally at
any sub-sentential point in the interpreta-
tion process. Accordingly, at all interme-
diate points where interpretation of an ut-
terance subpart is not fully determined for
the hearer in context, uncertainty can be
resolved immediately by suitable clarifica-
tion/correction/repair/extension as an ex-
change between interlocutors. The result
is a major check on the combinatorial ex-
plosion of alternative structures and inter-
pretations at each choice point, and the ba-
sis for a model of how interpretation in
context can be established without either
party having to make assumptions about
what information they and their interlocu-
tor share in resolving ambiguities.
1 Introduction
A major characteristic of dialogue is effortless
switching between the roles of hearer and speaker.
Dialogue participants seamlessly shift between
parsing and generation bi-directionally across any
syntactic dependency, without any indication of
there being any problem associated with such
shifts (examples from Howes et al (in prep)):
(1) Conversation from A and B, to C:
A: We?re going
B: to Bristol, where Jo lives.
(2) A smelling smoke comes into the kitchen:
A: Have you burnt
B the buns. Very thoroughly.
A: But did you burn
B: Myself? No. Luckily.
(3) A: Are you left or
B: Right-handed.
Furthermore, in no case is there any guarantee that
the way the shared utterance evolves is what ei-
ther party had in mind to say at the outset, indeed
obviously not, as otherwise the exchange risks be-
ing otiose. This flexibility provides a vehicle for
ongoing clarification, acknowledgement, correc-
tions, repairs etc. ((6)-(7) from (Mills, 2007)):
(4) A: I?m seeing Bill.
B: The builder?
A: Yeah, who lives with Monica.
(5) A: I saw Don
B: John?
A: Don, the guy from Bristol.
(6) A: I?m on the second switch
B: Switch?
A: Yeah, the grey thing
(7) A: I?m on the second row third on the left.
B: What?
A: on the left
The fragmental utterances that constitute such in-
cremental, joint contributions have been analysed
as falling into discrete structural types according
to their function, in all cases resolved to propo-
sitional types by combining with appropriate ab-
stractions from context (Ferna?ndez, 2006; Purver,
2004). However, any such fragment and their
resolution may occur as mid-turn interruptions,
well before any emergent propositional structure
is completed:
74
(8) A: They X-rayed me, and took a urine
sample, took a blood sample.
Er, the doctor ...
B: Chorlton?
A: Chorlton, mhm, he examined me, erm,
he, he said now they were on about a slight
[shadow] on my heart. [BNC: KPY
1005-1008]
The advantage of such ongoing, incremental, joint
conversational contributions is the effective nar-
rowing down of the search space out of which
hearers select (a) interpretations to yield some
commonly shared understanding, e.g. choice
of referents for NPs, and, (b) restricted struc-
tural frames which allow (grammatical) context-
dependent fragment resolution, i.e. exact speci-
fications of what contextually available structures
resolve elliptical elements. This seems to pro-
vide an answer as to why such fragments are so
frequent and undemanding elements of dialogue,
forming the basis for the observed coordination
between participants: successive resolution at sub-
sentential stages yields a progressively jointly es-
tablished common ground, that can thereafter be
taken as a secure, albeit individual, basis for filter-
ing out interpretations inconsistent with such con-
firmed knowledge-base (see (Poesio and Rieser,
2008; Ginzburg, forthcmg) etc). All such dialogue
phenomena, illustrated in (1)-(8), jointly and in-
crementally achieved, we address with the general
term split utterances.
However, such exchanges are hard to model
within orthodox grammatical frameworks, given
that usually it is the sentence/proposition that is
taken as the unit of syntactic/semantic analysis;
and they have not been addressed in detail within
such frameworks, being set aside as deviant, given
that such grammars in principle do not specify
a concept of grammaticality that relies on a de-
scription of the context of occurrence of a certain
structure (however, see Poesio and Rieser (2008)
for German completions). In so far as fragment
utterances are now being addressed, the pressure
of compatibility with sentence-based grammars
is at least partly responsible for analyses of e.g.
clarificatory-request fragments as sentential in na-
ture (Ginzburg and Cooper, 2004). But such anal-
yses fail to provide a basis for incrementally re-
solved clarification requests such as the interrup-
tion in (8) where no sentential basis is yet avail-
able over which to define the required abstraction
of contextually provided content.
In the psycholinguistic literature, on the other
hand, there is broad agreement that incrementality
is a crucial feature of parsing with semantic inter-
pretation taking place as early as possible at the
sub-sentential level (see e.g. (Sturt and Crocker,
1996)). Nonetheless, this does not, in and of it-
self, provide a basis for explaining the ease and
frequency of split utterances in dialogue: the inter-
active coordination between the parsing and pro-
duction activities, one feeding the other, remains
as a challenge.
In NLP modelling, parsing and generation algo-
rithms are generally dissociated from the descrip-
tion of linguistic entities and rules, i.e. the gram-
mar formalisms, which are considered either to be
independent of processing (?process-neutral?) or
to require some additional generation- or parsing-
specific mechanisms to be incorporated. However,
this point of view creates obstacles for a success-
ful account of data as in (1)-(8). Modelling those
would require that, for the current speaker, the ini-
tiated generation mechanism has to be displaced
mid-production without the propositional genera-
tion task having been completed. Then the parsing
mechanism, despite being independent of, indeed
in some sense the reverse of, the generation com-
ponent, has to take over mid-sentence as though, in
some sense there had been parsing involved up to
the point of switchover. Conversely, for the hearer-
turned-speaker, it would be necessary to somehow
connect their parse with what they are now about
to produce in order to compose the meaning of the
combined sentence. Moreover, in both directions
of switch, as (2) shows, this is not a phenomenon
of both interlocutors intending to say the same
sentence: as (3) shows, even the function of the
utterance (e.g. question/answer) can alter in the
switch of roles and such fragments can play two
roles (e.g. question/completion) at the same time
(e.g. (2)). Hence the grammatical integration of
such joint contributions must be flexible enough
to allow such switches which means that such
fragment resolutions must occur before the com-
putation of intentions at the pragmatic level. So
the ability of language users to successfully pro-
cess such utterances, even at sub-sentential levels,
means that modelling their grammar requires fine-
grained grammaticality definitions able to char-
acterise and integrate sub-sentential fragments in
turns jointly constructed by speaker and hearer.
75
This can be achieved straightforwardly if fea-
tures like incrementality and context-dependent
processing are built into the grammar architecture
itself. The modelling of split utterances then be-
comes straightforward as each successive process-
ing step exploits solely the grammatical apparatus
to succeed or fail. Such a view notably does not in-
voke high-level decisions about speaker/hearer in-
tentions as part of the mechanism itself. That this
is the right view to take is enhanced by the fact that
as all of (1)-(8) show, neither party in such role-
exchanges can definitively know in advance what
will emerge as the eventual joint proposition. If,
to the contrary, generation decisions are modelled
as involving intentions for whole utterances, there
will be no the basis for modelling how such in-
complete strings can be integrated in suitable con-
texts, with joint propositional structures emerging
before such joint intentions have been established.
An additional puzzle, equally related to both
the challenges of disambiguation and the status
of modelling speaker?s intentions as part of the
mechanism whereby utterance interpretation takes
place, is the common occurrence of hearers NOT
being constrained by any check on consistency
with speaker intentions in determining a putative
interpretation, failing to make use of well estab-
lished shared knowledge:
(9) A: I?m going to cook salmon, as John?s
coming.
B: What? John?s a vegetarian.
A: Not my brother. John Smith.
(10) A: Why don?t you have cheese and noodles?
B: Beef? You KNOW I?m a vegetarian
Such examples are problematic for any account
that proposes that interpretation mechanisms for
utterance understanding solely depend on selec-
tion of interpretations which either the speaker
could have intended (Sperber and Wilson, 1986;
Carston, 2002), or ones which are compati-
ble with checking consistency with the com-
mon ground/plans established between speaker
and hearer (Poesio and Rieser, 2008; Ginzburg,
forthcmg), mutual knowledge, etc. (Clark, 1996;
Brennan and Clark, 1996). To the contrary, the
data in (9)-(10) tend to show that the full range
of interpretations computable by the grammar has
in principle to be available at all choice points for
construal, without any filter based on plausibility
measures, thus leaving the disambiguation chal-
lenge still unresolved.
In this paper we show how with speaker and
hearer in principle using the same mechanisms for
construal, equally incrementally applied, such dis-
ambiguation issues can be resolved in a timely
manner which in turn reduces the multiplication
of structural/interpretive options. As we shall see,
what connects our diverse examples, and indeed
underpins the smooth shift in the joint endeav-
our of conversation, lies in incremental, context-
dependent processing and bidirectionality, essen-
tial ingredients of the Dynamic Syntax (Cann et al,
2005) dialogue model.
2 Incrementality in Dynamic Syntax
Dynamic Syntax (DS) is a procedure-oriented
framework, involving incremental processing, i.e.
strictly sequential, word-by-word interpretation of
linguistic strings. The notion of incrementality
in DS is closely related to another of its features,
the goal-directedness of BOTH parsing and gener-
ation. At each stage of processing, structural pre-
dictions are triggered that could fulfill the goals
compatible with the input, in an underspecified
manner. For example, when a proper name like
Bob is encountered sentence-initially in English,
a semantic predicate node is predicted to follow
(?Ty(e ? t)), amongst other possibilities.
By way of introducing the reader to the DS
devices, let us look at some formal details with
an example, Bob saw Mary. The ?complete? se-
mantic representation tree resulting after the com-
plete processing of this sentence is shown in Fig-
ure 2 below. A DS tree is formally encoded with
the tree logic LOFT (Blackburn and Meyer-Viol
(1994)), we omit these details here) and is gen-
erally binary configurational, with annotations at
every node. Important annotations here, see the
(simplified) tree below, are those which represent
semantic formulae along with their type informa-
tion (e.g. ?Ty(x)?) based on a combination of the
epsilon and lambda calculi1.
Such complete trees are constructed, starting
from a radically underspecified annotation, the ax-
iom, the leftmost minimal tree in Figure 2, and
going through monotonic updates of partial, or
structurally underspecified, trees. The outline of
this process is illustrated schematically in Figure
2. Crucial for expressing the goal-directedness
are requirements, i.e. unrealised but expected
1These are the adopted semantic representation languages
in DS but the computational formalism is compatible with
other semantic-representation formats
76
0?Ty(t),
?
7?
1
?Ty(t)
?Ty(e),? ?Ty(e? t)
7?
2
?Ty(t)
Ty(e),Bob? ?Ty(e? t),?
7?
3
?Ty(t)
Ty(e),
Bob? ?Ty(e? t)
?Ty(e),
?
Ty(e? (e? t)),
See?
7?
0(gen)/4
Ty(t),?
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 2: Monotonic tree growth in DS
Ty(t),
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 1: A DS complete tree
node/tree specifications, indicated by ??? in front
of annotations. The axiom says that a proposition
(of type t, Ty(t)) is expected to be constructed.
Furthermore, the pointer, notated with ??? indi-
cates the ?current? node in processing, namely the
one to be processed next, and governs word order.
Updates are carried out by means of applying
actions, which are divided into two types. Compu-
tational actions govern general tree-constructional
processes, such as moving the pointer, introducing
and updating nodes, as well as compiling interpre-
tation for all non-terminal nodes in the tree. In our
example, the update of (1) to (2) is executed via
computational actions specific to English, expand-
ing the axiom to the subject and predicate nodes,
requiring the former to be processed next by the
position of the ?. Construction of only weakly
specified tree relations (unfixed nodes) can also be
induced, characterised only as dominance by some
current node, with subsequent update required. In-
dividual lexical items also provide procedures for
building structure in the form of lexical actions,
inducing both nodes and annotations. For exam-
ple, in the update from (2) to (3), the set of lexical
actions for the word see is applied, yielding the
predicate subtree and its annotations. Thus partial
trees grow incrementally, driven by procedures as-
sociated with particular words as they are encoun-
tered.
Requirements embody structural predictions as
mentioned earlier. Thus unlike the conven-
tional bottom-up parsing,2 the DS model takes
the parser/generator to entertain some predicted
goal(s) to be reached eventually at any stage of
processing, and this is precisely what makes the
formalism incremental. This is the characteri-
sation of incrementality adopted by some psy-
cholinguists under the appellation of connected-
ness (Sturt and Crocker, 1996; Costa et al, 2002):
an encountered word always gets ?connected? to a
larger, predicted, tree.
Individual DS trees consist of predicates and
their arguments. Complex structures are obtained
via a general tree-adjunction operation licensing
the construction of so-called LINKed trees, pairs
of trees where sharing of information occurs. In
its simplest form this mechanism is the same one
which provides the potential for compiling in-
2The examples in (1)-(8) also suggest the implausibility
of purely bottom-up or head-driven parsing being adopted di-
rectly, because such strategies involve waiting until all the
daughters are gathered before moving on to their projection.
In fact, the parsing strategy adopted by DS is somewhat sim-
ilar to mixed parsing strategies like the left-corner or Earley
algorithm to a degree. These parsing strategic issues are more
fully discussed in Sato (forthcmg).
77
A consultant, a friend of Jo?s, is retiring: Ty(t), Retire?((?, x, Consultant?(x) ? Friend?(Jo?)(x)))
Ty(e), (?, x, Consultant?(x) ? Friend?(Jo?)(x)) Ty(e? t), Retire?
Ty(e), (?, x, Friend?(Jo?)(x))
Ty(cn), (x, Friend?(Jo?)(x))
x Friend?(Jo?)
Jo? Friend?
Ty(cn? e), ?P.?, P
Figure 3: Apposition in DS
terpretation for apposition constructions as can
be seen in Figure (3)3. The assumption in the
construction of such LINKed structures is that at
any arbitrary stage of development, some type-
complete subtree may constitute the context for
the subsequent parsing of the following string as
an adjunct structure candidate for incorporation
into the primary tree, hence the obligatory sharing
of information in the resulting semantic represen-
tation.
More generally, context in DS is defined as the
storage of parse states, i.e., the storing of par-
tial tree, word sequence parsed to date, plus the
actions used in building up the partial tree. For-
mally, a parse state P is defined as a set of triples
?T, W, A?, where: T is a (possibly partial) tree;
W is the associated sequence of words; A is the
associated sequence of lexical and computational
actions. At any point in the parsing process, the
context C for a particular partial tree T in the set
P can be taken to consist of: a set of triples P ? =
{. . . , ?Ti, Wi, Ai?, . . .} resulting from the previ-
ous sentence(s); and the triple ?T, W, A? itself,
the subtree currently being processed. Anaphora
and ellipsis construal generally involve re-use of
formulae, structures, and actions from the set C.
Grammaticality of a string of words is then de-
fined relative to its context C, a string being well-
formed iff there is a mapping from string onto
completed tree with no outstanding requirements
given the monotonic processing of that string rela-
tive to context. All fragments illustrated above are
processed by means of either extending the current
3Epsilon terms, like ?, x, Consultant?(x), stand for wit-
nesses of existentially quantified formulae in the epsilon cal-
culus and represent the semantic content of indefinites in DS.
Defined relative to the equivalence ?(?, x, ?(x)) = ?x?(x),
their defining property is their reflection of their contain-
ing environment, and accordingly they are particularly well-
suited to expressing the growth of terms secured by such ap-
positional devices.
tree, or constructing LINKed structures and trans-
fer of information among them so that one tree
provides the context for another, and are licensed
as wellformed relative to that context. In particu-
lar, fragments like the doctor in (8) are licensed by
the grammar because they occur at a stage in pro-
cessing at which the context contains an appropri-
ate structure within which they can be integrated.
The definite NP is taken as an anaphoric device,
relying on a substitution process from the context
of the partial tree to which the node it decorates is
LINKed to achieve the appropriate construal and
tree-update:
(11) The?parse? tree licensing production of the
doctor: LINK adjunction
?Ty(t)
Chorlton? ?Ty(e? t)
(Doctor?(Chorlton?)),?
3 Bidirectionality in DS
Crucially, for our current concern, this architec-
ture allows a dialogue model in which generation
and parsing function in parallel, following exactly
the same procedure in the same order. See Fig (2)
for a (simplified) display of the transitions manip-
ulated by a parse of Bob saw Mary, as each word
is processed and integrated to reach the complete
tree. Generation of this utterance from a complete
tree follows precisely the same actions and trees
from left to right, although the complete tree is
available from the start (this is why the complete
tree is marked ?0? for generation): in this case the
eventual message is known by the speaker, though
of course not by the hearer. What generation in-
volves in addition to the parse steps is reference
78
to this complete tree to check whether each pu-
tative step is consistent with it in order not to be
deviated from the legitimate course of action, that
is, a subsumption check. The trees (1-3) are li-
censed because each of these subsumes (4). Each
time then the generator applies a lexical action, it
is licensed to produce the word that carries that ac-
tion under successful subsumption check: at Step
(3), for example, the generator processes the lex-
ical action which results in the annotation ?See??,
and upon success and subsumption of (4) license
to generate the word see at that point ensues.
For split utterances, two more assumptions are
pertinent. On the one hand, speakers may have
initially only a partial structure to convey: this is
unproblematic, as all that is required by the for-
malism is monotonicity of tree growth, the check
being one of subsumption which can be carried
out on partial trees as well. On the other hand,
the utterance plan may change, even within a sin-
gle speaker. Extensions and clarifications in DS
can be straightforwardly generated by appending
a LINKed structure projecting the added material
to be conveyed (preserving the monotonicity con-
straint)4.
(12) I?m going home, with my brother, maybe
with his wife.
Such a model under which the speaker and
hearer essentially follow the same sets of actions,
updating incrementally their semantic representa-
tions, allows the hearer to ?mirror? the same series
of partial trees, albeit not knowing in advance what
the content of the unspecified nodes will be.
4 Parser/generator implementation
The process-integral nature of DS emphasised
thus far lends itself to the straightforward imple-
mentation of a parsing/generating system, since
the ?actions? defined in the grammar directly pro-
vide a major part of its implementation. By now it
should also be clear that the DS formalism is fully
bi-directional, not only in the sense that the same
grammar can be used for generation and parsing,
but also because the two sets of activities, conven-
tionally treated as ?reverse? processes, are mod-
elled to run in parallel. Therefore, not only can the
same sets of actions be used for both processes,
4Revisions however will involve shifting to a previous
partial tree as the newly selected context: I?m going home,
to my brother, sorry my mother.
but also a large part of the parsing and generation
algorithms can be shared.
This design architecture and a prototype im-
plementation are outlined in (Purver and Otsuka,
2003), and the effort is under way to scale up the
DS parsing/generating system incorporating the
results in (Gargett et al, 2008; Gregoromichelaki
et al, to appear).5 The parser starts from the axiom
(step 0 in Fig.2), which ?predicts? a proposition to
be built, and follows the applicable actions, lexi-
cal or general, to develop a complete tree. Now,
as has been described in this paper, the genera-
tor follows exactly the same steps: the axiom is
developed through successive updates into a com-
plete tree. The only material difference from ?
or rather in addition to? parsing is the complete
tree (Step 0(gen)/4), given from the very start of
the generation task, which is then referred to at
each tree update for subsumption check. The main
point is that despite the obvious difference in their
purposes ?outputting a string from a meaning ver-
sus outputting a meaning from a string? parsing
and generation indeed share the direction of pro-
cessing in DS. Moreover, as no intervening level
of syntactic structure over the string is ever com-
puted, the parsing/generation tasks are more effi-
ciently incremental in that semantic interpretation
is directly imposed at each stage of lexical integra-
tion, irrespective of whether some given partially
developed constituent is complete.
To clarify, see the pseudocode in the Prolog
format below, which is a close analogue of the
implemented function that both does parsing and
generation of a word (context manipulation is
ignored here for reasons of space). The plus
and minus signs attached to a variable indicate it
must/needn?t be instantiated, respectively. In ef-
fect, the former corresponds to the input, the latter
to the output.
(13) parse gen word(
+OldMeaning,?Word,?NewMeaning):-
apply lexical actions(+OldMeaning, ?Word,
+LexActions, ?IntermediateMeaning ),
apply computational actions(
+IntermediateMeaning, +CompActions,
?NewMeaning )
OldMeaning is an obligatory input item, which
corresponds to the semantic structure con-
structed so far (which might be just structural
tree information initially before any lexical
5The preliminary results are described in (Sato,
forthcmg).
79
input has been processed thus advocating a
strong predictive element even compared to
(Sturt and Crocker, 1996). Now notice that
the other two variables ?corresponding to the
word and the new (post-word) meaning? may
function either as the input or output. More
precisely, this is intended to be a shorthand
for either (+OldMeaning,+Word,?NewMeaning)
i.e. Word as input and NewMeaning as out-
put, or (+OldMeaning,?Word,+NewMeaning), i.e.
NewMeaning as input and Word as output, to repeat,
the former corresponding to parsing and the latter
to generation.
In either case, the same set of two sub-
procedures, the two kinds of actions described in
(13), are applied sequentially to process the input
to produce the output. These procedures corre-
spond to an incremental ?update? from one par-
tial tree to another, through a word. The whole
function is then recursively applied to exhaust the
words in the string, from left to right, either in
parsing or generation. Thus there is no differ-
ence between the two in the order of procedures
to be applied, or words to be processed. Thus it is
a mere switch of input/output that shifts between
parsing and generation.6
4.1 Split utterances in Dynamic Syntax
Split utterances follow as an immediate conse-
quence of these assumptions. For the dialogues in
(1)-(8), therefore, while A reaches a partial tree of
what she has uttered through successive updates
as described above, B as the hearer, will follow
the same updates to reach the same representation
of what he has heard. This provides him with the
ability at any stage to become the speaker, inter-
rupting to continue A?s utterance, repair, ask for
clarification, reformulate, or provide a correction,
as and when necessary7. According to our model
of dialogue, repeating or extending a constituent
of A?s utterance by B is licensed only if B, the
hearer turned now speaker, entertains a message
6Thus the parsing procedure is dictated by the grammar to
a large extent, but importantly, not completely. More specif-
ically, the grammar formalism specifies the state paths them-
selves, but not how the paths should be searched. The DS ac-
tions are defined in conditional terms, i.e. what to do as and
when a certain condition holds. If a number of actions can be
applied at some point during a parse, i.e. locally ambiguity
is encountered, then it is up to a particular implementation
of the parser to decide which should be traversed first. The
current implementation includes suggestions of search strate-
gies.
7The account extends the implementation reported in
(Purver et al, 2006)
to be conveyed that matches or extends the parse
tree of what he has heard in a monotonic fashion.
In DS, this message is a semantic representation
in tree format and its presence allows B to only ut-
ter the relevant subpart of A?s intended utterance.
Indeed, this update is what B is seeking to clarify,
extend or acknowledge. In DS, B can reuse the
already constructed (partial) parse tree in his con-
text, rather than having to rebuild an entire propo-
sitional tree or subtree.
The fact that the parsing formalism integrates
a strong element of predictivity, i.e. the parser
is always one step ahead from the lexical in-
put, allows a straightforward switch from pars-
ing to generation thus resulting in an explana-
tion of the facility with which split utterances oc-
cur (even without explicit reasoning processes).
Moreover, on the one hand, because of incremen-
tality, the issue of interpretation-selection can be
faced at any point in the process, with correc-
tions/acknowledgements etc. able to be provided
at any point; this results in the potential exponen-
tial explosion of interpretations being kept firmly
in check. And, structurally, such fragments can
be integrated in the current partial tree represen-
tation only (given the position of the pointer) so
there is no structural ambiguity multiplication. On
the other hand, for any one of these intermedi-
ate check points, bidirectionality entails that con-
sistency checking remains internal to the individ-
ual interlocutors? system, the fact of their mir-
roring each other resulting at their being at the
same point of tree growth. This is sufficient to en-
sure that any inconsistency with their own parse
recognised by one party as grounds for correc-
tion/repair can be processed AS a correction/repair
by the other party without requiring any additional
metarepresentation of their interlocutors? informa-
tion state (at least for these purposes). This allows
the possibility of building up apparently complex
assumptions of shared content, without any neces-
sity of constructing hypotheses of what is enter-
tained by the other, since all context-based selec-
tions are based on the context of the interlocutor
themselves. This, in its turn, opens up the possi-
bility of hearers constructing interpretations based
on selections made that transparently violate what
is knowledge shared by both parties, for no pre-
sumption of common ground is essential as input
to the interpretation process (see, e.g. (9)-(10)).
80
5 Conclusion
It is notable that, from this perspective, no pre-
sumption of common ground or hypothesis as to
what the speaker could have intended is necessary
to determine how the hearer selects interpretation.
All that is required is a concept of system-internal
consistency checking, the potential for clarifica-
tion in cases of uncertainty, and reliance at such
points on disambiguation/correction/repair by the
other party. The advantage of such a proposal, we
suggest, is the provision of a fully mechanistic ac-
count for disambiguation (cf. (Pickering and Gar-
rod, 2004)). The consequence of such an analysis
is that language use is essentially interactive (see
also (Ginzburg, forthcmg; Clark, 1996)): the only
constraint as to whether some hypothesised in-
terpretation assigned by either party is confirmed
turns on whether it is acknowledged or corrected
(see also (Healey, 2008)).
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962,
the EU ITALK project (FP7-214668) and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex
Davies, Arash Eshghi, Jonathan Ginzburg, Pat Healey, Greg
James Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Bulletin of the
IGPL, 2:3?31.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 22:482?1493.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Robyn Carston. 2002. Thoughts and Utterances: The
Pragmatics of Explicit Communication. Blackwell.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
Patrick Sturt, and Giovanni Soda. 2002. Enhanc-
ing first-pass attachment prediction. In ECAI 2002:
508-512.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Andrew Gargett, Eleni Gregoromichelaki, Chris
Howes, and Yo Sato. 2008. Dialogue-grammar cor-
respondence in dynamic syntax. In Proceedings of
the 12th SEMDIAL (LONDIAL).
Jonathan Ginzburg and Robin Cooper. 2004. Clarifi-
cation, ellipsis, and the nature of contextual updates
in dialogue. Linguistics and Philosophy, 27(3):297?
365.
Jonathan Ginzburg. forthcmg. Semantics for Conver-
sation. CSLI.
Eleni Gregoromichelaki, Yo Sato, Ruth Kempson, An-
drew Gargett, and Christine Howes. to appear. Dia-
logue modelling and the remit of core grammar. In
Proceedings of IWCS 2009.
Patrick Healey. 2008. Interactive misalignment: The
role of repair in the development of group sub-
languages. In R. Cooper and R. Kempson, editors,
Language in Flux. College Publications.
Christine Howes, Patrick G. T. Healey, and Gregory
Mills. in prep. a: An experimental investigation
into. . . b: . . . split utterances.
Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
Gregory J. Mills. 2007. Semantic co-ordination in di-
alogue: the role of direct interaction. Ph.D. thesis,
Queen Mary University of London.
Martin Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences.
Massimo Poesio and Hannes Rieser. 2008. Comple-
tions, coordination, and alignment in dialogue. Ms.
Matthew Purver and Masayuki Otsuka. 2003. Incre-
mental generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceedings of
the 9th European Workshop in Natural Language
Generation (ENLG), pages 79?86.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computa-
tion, 4(2-3):289?326.
Matthew Purver. 2004. The Theory and Use of Clari-
fication Requests in Dialogue. Ph.D. thesis, Univer-
sity of London, forthcoming.
Yo Sato. forthcmg. Local ambiguity, search strate-
gies and parsing in dynamic syntax. In Eleni Gre-
goromichelaki and Ruth Kempson, editors, Dynamic
Syntax: Collected Papers. CSLI.
Dan Sperber and Deirdre Wilson. 1986. Relevance:
Communication and Cognition. Blackwell.
Patrick Sturt and Matthew Crocker. 1996. Monotonic
syntactic processing: a cross-linguistic study of at-
tachment and reanalysis. Language and Cognitive
Processes, 11:448?494.
81
Proceedings of the 8th International Conference on Computational Semantics, pages 128?139,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Dialogue Modelling and the Remit
of Core Grammar
Eleni Gregoromichelaki
?
, Yo Sato
?
, Ruth Kempson
?
Andrew Gargett
?
, Christine Howes
?
?
King?s College London,
?
Queen Mary University of London
1 Introduction
In confronting the challenge of providing formal models of dialogue, with
its plethora of fragments and rich variation in modes of context-dependent
construal, it might seem that linguists face two types of methodological
choice: either (a) conversation employs dialogue-specific mechanisms, for
which a grammar specific to such activity must be constructed; or (b) vari-
ation arises due to independent parsing/production systems which invoke
a process-neutral grammar. However, as dialogue research continues to de-
velop, there are intermediate possibilities, and in this paper we discuss the
approach developed within Dynamic Syntax (DS, Kempson et al 2001,
Cann et al 2005), a grammar framework within which, not only the parser,
but indeed ?syntax? itself are just a single mechanism allowing the pro-
gressive construction of semantic representations in context. Here we take
as a case study the set of phenomena classifiable as clarifications, reformu-
lations, fragment requests and corrections accompanied by extensions, and
argue that though these may seem to be uniquely constitutive of dialogue,
they are grounded in the mechanisms of apposition equivalently usable in
monologue for presenting reformulations, extensions, self-corrections etc.
2 Background
The data we focus on are non-repetitive fragment forms of acknowledge-
ments, clarifications and corrections (henceforth, A female, B male):
(1) A: Bob left.
B: (Yeah,) the accounts guy.
(2)
A: They X-rayed me, and took a urine sample, took a blood sample.
Er, the doctor
B: Chorlton?
A: Chorlton, mhm, he examined me, erm, he, he said now they
were on about a slight [shadow] on my heart.
128
(3) A: Bob left.
B: Rob?
A: (No,) (Bob,) the accounts guy.
Even though in the literature the fragments in (2)-(3) might be characterised
as illustrating distinct construction-types, in our view, they all illustrate how
speakers and hearers may contribute, in some sense to be made precise, to
the joint enterprise of establishing some shared communicative content, in
what might be loosely called split utterances. Even (1), an acknowledgement,
can be seen this way upon analysis: B?s addition is similar in structure to
an afterthought extension that might have been added by A herself to A?s
fully sentential utterance. It can be seen in (2) that such joint construction
of content can proceed incrementally: the clarification request in the form of
a reformulation is provided by B and resolved by A within the construction
of a single proposition. In (3) the fragment reply can be taken to involve
correction, in the sense that, according to the DS analysis of B?s fragment
question, he has provided content construable as equivalent to that derived
by processing Rob left? (see Kempson et al (2007)). Nevertheless such
corrections can also incorporate extensions in the above sense, enabling a
single conjoined propositional content to be derived in a single step.
It might seem that such illustration of diversity of fragment usage is am-
ple evidence of the need for conversation-specific rules. Indeed, Ferna?ndez
(2006) presents a thorough taxonomy, as well as detailed formal modelling
of Non-Sentential Utterances (NSUs), referring to contributions such as (1)
as repeated acknowledgements involving reformulation. Ferna?ndez models
such constructions via type-specific ?accommodation rules? which make a
constituent of the antecedent utterance ?topical?. The semantic effect of
acknowledgement is then derived by applying an appropriately defined ut-
terance type for such fragments to the newly constructed context. A distinct
form of contextual accommodation is employed to model so-called helpful
rejection fragments, as in (3) (without the reformulation), whereby a wh-
question is accommodated in the context by abstracting over the content
of one of the sub-constituents of the previous utterance. The content of
the correction is derived by applying this wh-question in the context to the
content of the fragment (see also Schlangen (2003) for another classification
and analysis).
In contrast, the alternative explored here is whether phenomena such
as (1)-(2), both of which are non-repetitive next-speaker contributions, can
be handled uniformly using the mechanisms for structure-building made
available in the core grammar, without recourse to construction-specific ex-
tensions of that grammar and contextual accommodation rules. This is
because, in our view, the range of interpretations these fragments receive
in actual dialogue seem to form continua with no well-defined boundaries
and mixing of functions (see also comments in Schlangen (2003)). Thus we
129
propose that the grammar itself simply provides mechanisms for process-
ing/integrating such fragments in the current structure while their precise
contribution to the interaction can be calculated by pragmatic inferencing
if needed (as in e.g. Schlangen (2003)) or, as seems most often to be the
case, be left underspecified without disruption to the dialogue.
One bonus of the stance taken here is the promise it offers for elucidating
the grammar-parser contribution to the disambiguation task. Part of the
challenge of modelling dialogue is the apparent multiplicity of interpretive
and structural options opened up during processing by the recurrent, of-
ten overlapping fragments as seen in (2) above. Thus, it might seem that
the rich array of elliptical fragments available in dialogue adds to its com-
plexity. However, an alternative point of view is to see such phenomena as
providing a window on how interlocutors exploit the incrementality afforded
by the grammar. The reliance of fragments on context for interpretation,
when employed incrementally, enables the hearer to immediately respond to
a previous utterance at any relevant point, in a constrained manner, with-
out ?recovering? a propositional unit. Three features of the Dynamic Syntax
model of dialogue (Purver et al (2006)), presented below, provide the flex-
ible control required for such processing: (a) word-by-word incrementality
(b) interaction with contextually provided information at every step of the
construction process (c) tight coordination of parsing and production.
3 Dynamic Syntax: A Sketch
Dynamic Syntax (DS ) is a parsing-based framework, involving strictly se-
quential interpretation of linguistic strings. The model is implemented via
goal-directed growth of tree structures and their annotations formalised us-
ing LOFT (Blackburn and Meyer-Viol (1994)), with modal operators ???, ???
to define concepts of mother and daughter, and their iterated counterparts,
??
?
?, ??
?
?, to define the notions be dominated by and dominate. Under-
specification and update are core aspects of the grammar itself and involve
strictly monotonic information growth for any dimension of tree structures
and annotations. Underspecification is employed at all levels of tree rela-
tions (mother, daughter etc.), as well as formulae and type values, each
having an associated requirement that drives the goal-directed process of
update. For example, an underspecified subject node of a tree may have a
requirement expressed in DS with the node annotation ?Ty(e), for which
the only legitimate updates are logical expressions of type entity (Ty(e));
but requirements may also take a modal form, e.g. ????Ty(e ? t), a con-
straint that the mother node be annotated with a formula of predicate type.
Requirements are essential to the dynamics informing the DS account: all
requirements must be satisfied if the construction process is to lead to a
successful outcome.
130
Semantic structure is built from lexical and general computational ac-
tions. Computational actions govern general tree-constructional processes,
such as introducing and updating nodes, as well as compiling interpretation
for all non-terminal nodes in the tree. Construction of only weakly spec-
ified tree relations (unfixed nodes) can also be induced, characterised only
as dominance by some current node, with subsequent update required. In-
dividual lexical items also provide procedures for building structure in the
form of lexical actions, inducing both nodes and annotations. Thus partial
trees grow incrementally, driven by procedures associated with particular
words as they are encountered, with a pointer, ?, recording the parser?s
progress (unlike van Leusen and Muskens (2003), partial trees are part of
the model and, unlike in other frameworks, incrementality is word-by-word
rather than sentence-by-sentence).
Complete individual trees are taken to correspond to predicate-argument
structures (with an event term associated with tense, suppressed in this
paper). The epsilon calculus (see e.g. Meyer-Viol (1995)) provides the se-
mantic representation language. Complex structures are obtained via a gen-
eral tree-adjunction operation licensing the construction of so-called linked
trees, hosting information that is eventually transferred onto the tree from
which the link is made (Kempson et al2001). Structures projected as such
paired trees range over restrictive relatives, nonrestrictive relatives, condi-
tionals, topic structures and appositions as here. As the semantic represen-
tations employ the epsilon calculus, eventual compound epsilon terms (e.g.
?, x, P (x)) are constructed incrementally through link-adjunction:
(4) A consultant, a friend of Jo?s, is retiring
Ty(t),Retire
?
((?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)))
Ty(e), (?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)) Ty(e? t), Retire
?
Ty(e), (?, x, Friend
?
(Jo
?
)(x))
Ty(cn), (x,Friend
?
(Jo
?
)(x))
x Friend
?
(Jo
?
)
Jo
?
Friend
?
Ty(cn? e), ?P.?, P
Underspecification of content as well as structure are central to facilitat-
ing successful linguistic interaction, our primary concern here. Pronouns,
the prototypical case, contribute a place-holding metavariable, noted as e.g.
U, plus an associated requirement for update by an appropriate term value:
??x.Fo(x). Equally, definite NPs contribute place-holders plus a constraint
providing a restriction/?presupposition? on the kind of entity picked out,
e.g., the man contributes the annotation U
Man
?
(U)
, T y(e). The subscript
specification is shorthand for a transition to a linked tree whose root node is
131
annotated with a formula Man
?
(U)
1
. The update of metavariables can be
accomplished if the context contains an appropriate term for substitution:
context involves storage of parse states, i.e., storing of partial tree, word se-
quence to date, plus the actions used in building up the partial tree (Purver
et al2006).
Scope dependencies between constructed terms or the index of evalua-
tion (e.g. S) are defined on completed propositional formulae, relative to
incrementally collected scope constraints (of the form x < y for constructed
terms containing variables x and y respectively). Constraints reflect on-line
processing considerations modulo over-riding lexical stipulations. For ex-
ample, proper names contribute as iota terms, i.e, epsilon terms reflecting
uniqueness in the context, ?, x,Bob
?
(x), and these project a scope depen-
dency solely on the index of evaluation reflecting their widest scope property
(cf Kamp and Reyle 1994). The structure projected from A?s utterance in
(1) is thus (5) (note that trees are the result of processing words but do
not encode the structure of strings, word order etc., only semantic content
derived in interaction with context, thus are the equivalents of DRSs):
(5)
S < x Ty(t),Leave
?
((?, x,Bob
?
(x)))
Ty(e), (?, x,Bob
?
(x)) Ty(e? t), Leave
?
The scope evaluation rule reflects the predicate-logic/epsilon-calculus equiv-
alence ?xF (x) ? F (?, x, F (x)) so evaluated terms eventually reflect their
containing structure. Hence, evaluation of (5) yields:
(6) Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
A major aspect of the DS dialogue model is that both generation and
parsing are goal-directed and incremental, and hence are governed by es-
sentially the same mechanism. Under this model, a human hearer-parser
builds a succession of partial parse trees based on what (s)he has heard
thus far. Importantly, however, unlike the conventional bottom-up parsing,
the DS model assumes a strong predictive element in parsing: a hearer is
assumed to entertain some goal to be reached eventually at any stage of
parsing. In (1), for example, as soon as the hearer encounters Bob, an un-
derspecified propositional tree is constructed, as in the first simplified and
schematised tree in Figure 1. Then the tree ?grows? monotonically, i.e. such
that at each word input, it is ?updated? to an ?incremented? tree that is
subsumed by the original tree, as depicted in the same Figure. This can be
described as a process of specifying the relevant nodes towards a complete
tree. This predictive element in DS allows a speaker-generator to be mod-
elled as doing exactly the same, i.e. going through monotonically updated
partial trees, the only difference being that (s)he also has available a more
1
These linked structures are suppressed in all diagrams.
132
fully specified goal tree representing what (s)he wishes to say, corresponding
to the rightmost tree in the Figure (with ?0? in the ?generation? row at the
bottom indicating it is entertained before utterance). Each licensed step in
generation, i.e. the utterance of a word, is governed by whatever step is
licensed by the parsing formalism, constrained via a required subsumption
relation of the goal tree. By updating their growing ?parse? tree relative
to the goal tree, speakers are licensed to produce the associated natural
language string.
Parsing: 1 2 3
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob
?
,?
?Ty(e ? t)
7?
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave
?
,?
7?
Ty(t),
Leave?(Bob?),?
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave?
Generation: 1 2 0,3
Figure 1: Parallel parsing and generation in DS
This architecture allows for a dialogue model in which generation (what
a speaker does) and parsing (what a hearer does) function in parallel. The
speaker goes through partial trees subsuming a specified goal tree, while
the hearer attempts to ?mirror? the same series of partial trees, albeit not
knowing what the content of the unspecified nodes will be. For the dialogues
in (1)-(3), therefore, B as the hearer will have the partial representation of
what he has successfully parsed, required also for generation. This provides
him with the ability at any stage to become the speaker, interrupting to
ask for clarification, reformulating, or providing a correction, as and when
necessary. As we shall see, B?s parse tree reveals where need of clarifica-
tion or miscommunication occurs, as it will be at that node from which a
sub-routine extending it takes place
2
. According to our model of dialogue,
repeating or extending a constituent of A?s utterance by B is licensed only
if B, the hearer of A turned now a speaker, entertains currently a goal tree
that matches or extends the parse tree of what he has heard in a monotonic
fashion, although he only utters the relevant subpart of A?s utterance. In-
deed, this update is what B is seeking to clarify, correct or acknowledge. In
DS, B can reuse the already constructed (partial) parse tree in his context,
rather than having to rebuild an entire propositional tree or subtree
3
.
2
The account extends the implementation reported in Purver et al (2006)
3
Given the DS concept of linked trees projecting propositional content, we anticipate
that this mechanism will be extendable to fragment construal involving inference (see e.g.
Schlangen (2003), Schlangen and Lascarides (2003))
133
4 NSU fragments in Dynamic Syntax
4.1 Non-repetitive Acknowledgement
From a DS perspective, phenomena like reformulations as in (1), or exten-
sions to what one understands of the other speaker?s utterance, (2), can
be handled with exactly the same mechanisms as the sentence-internal phe-
nomenon independently identifiable as apposition, as in (4), and equally
usable by a single individual as a means of incrementally reformulating, cor-
recting or extending what they have just uttered. The update rule for such
structures, applicable to all terms, takes the two type e terms so formed and
yields a new term whose compound content is a combination of both.
We now have the basis for analysing extensions potentially functioning
as acknowledgements which build on what has been previously said as a way
of confirming the previous utterance. Recall (1), (2). There are two ways for
fragments which reformulate an interlocutor A?s utterance to occur: (a) as
interruptions of A?s utterance with immediate confirmation of identification
of the individual concerned, see (2); (b) as confirmations/extensions of A?s
utterance after the whole of her utterance has been integrated, see (1). Both
are modelled by DS as incremental additions.
Turning to (1), B?s response (Yeah,) the accounts guy constitutes a re-
formulation of A?s utterance and an extension of A?s referring expression,
yielding a similar content as that of an appositive expression Bob, the ac-
counts guy in this case jointly constructed. B?s reformulation/extension
counts in effect as an acknowledgement in virtue of signalling successful
processing of A?s utterance without objection raised. Thus there is no need
for a separate grammatical mechanism to process these structures. In DS
terms, after processing A?s utterance, B?s context consists of the following
tree:
(7) B?s Context for producing ?Yeah?
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x)) Leave
?
B, as a speaker, can now re-use this representation as point of departure
for generating the expression the accounts guy. In this case his goal tree,
the message to be expressed, will now be annotated with a composite term
made up from both the term recovered from parsing A?s utterance and the
new addition. This requires attaching a linked tree to the correct node
and an appropriate update of the context tree (for reasons of space, the
exact structure of the linked tree is condensed below, with subscripting as
shorthand):
134
(8) B?s goal tree:
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x))) ? Leave
?
(x))
(?, x,Bob
?
(x) ?Acc.guy
?
(x))
Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
In order to license generation of the expression the accounts guy, B now
needs to verify that processing these words in the context provided by the
tree in (7) will produce a tree that matches this goal tree in (8). To achieve
this, starting from (7), a series of simulated ?parse? trees are generated
which indeed result in the requisite matching. Steps include shifting the
pointer to the appropriate node, projection of a linked tree from that node
and test-processing the words the accounts guy, each step checking against
the goal tree that a subsumption relation between the current ?parse? tree
and the goal tree is always maintained:
(9) B?s parse tree licensing production of the accounts guy: link adjunction
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
(?, x,Bob
?
(x)) Leave
?
U
Acc.guy
?
(U)
,?
The only way to update this representation relative to both the restriction
on the metavariable and monotonicity of growth on any one node in a tree
involves replacing the metavariable with (?, x,Bob
?
(x)), as this is commen-
surate with an extension of the term annotating the node from which the
link transition was constructed:
(10) Updating B?s parse tree licensing production of the accounts guy
Ty(t), Leave
?
(?, x,Bob
?
(x) ?Acc.guy
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Finally, the information is passed up to the top node of the main tree,
completing the parse tree to match B?s goal tree, (8), thus licensing the
utterance of the expression the accounts guy.
4.2 Non-repetitive Clarification
In the acknowledgement case above, the proposition relative to which the
linked structure is built is completed (with an already extended epsilon
term); but the same mechanism can be used when the interlocutor needs
clarification, prior to any such completion of the tree. In (2), B again, as
the speaker, takes as his goal tree a tree annotated with an expansion of the
term constructed from parsing A?s utterance but nevertheless picking out
the same individual. Using the very same mechanism as in (1) of building a
linked structure, B, interrupting A, provides a distinct expression, the name
Chorlton, this time before he has completed the parse tree for A?s utterance.
All that has been achieved at this point is the definite?s contribution of a
meta-variable with the restriction that the individual picked out must be a
doctor:
135
(11) A/B?s parse tree for Chorlton:
?Ty(t)
U
Doctor
?
(U)
,?
?Ty(e ? t)
(?, x, Chorlton
?
(x))
As in the acknowledgement case, but this time at the node initiating the
link transition, the only possible value to provide for the metavariable U
compatible both with its restriction and the monotoniticity constraint is
the composite term (?, x,Doctor
?
(x) ? Chorlton
?
(x)). The mechanism of
constructing paired structures involving type e terms across linked trees
is identical to that employed in B?s utterance in (1), though to a rather
different effect at this intermediate stage in the interpretation process. This
extension of the term is confirmed by A, this time replicating the composite
term which processing B?s utterance has led to. The eventual effect of the
process of inducing linked structures to be annotated by coreferential type e
terms may thus vary across monologue and different dialogue applications,
yielding different interpretations, but the mechanism is the same.
4.3 Correction
It might be argued nonetheless that correction is intrinsically a dialogue
phenomenon. Consider (3), for example. One of the possible interpretations
of (3), according to the DS analysis, is that B has offered the equivalent of the
content derived by processing Rob left?. That is, let?s assume here that B has
misheard and requests confirmation of what he has perceived A as saying.
A in turn rejects B?s understanding of her utterance and provides more
information. Presuming rejection as simple disagreement (i.e. the utterance
has been understood, but judged as incorrect), in DS terms, this means that
A has in mind a goal tree that licensed what she had produced, which is
distinct from the one derived by processing B?s clarification. As shown in
Kempson et al (2007), this means that A has been unable to process B?s
clarification request as an extension of her own context. Instead, she has to
parse the clarification by exploiting the potential for introducing an initially
structurally underspecified tree-node to accommodate the contribution of
the word Rob. Subsequently, by re-running the actions stored in context
previously by processing her own utterance of the word left, she is able to
complete the integration of the fragment in a new propositional structure.
Now, in order for A to produce the following correction, what is required
is for A to establish as the current most recent representation in context her
original goal tree. This can be monotonically achieved by recovering and
copying this original goal tree to serve as the current most immediate con-
text
4
. An option available to A at this point is to introduce, in addition
or exclusively, a reformulation of her original utterance in order to facilitate
4
Mistaken representations must be maintained in the context as they can provide an-
tecedents for subsequent anaphoric expressions.
136
identification of the named individual which proved problematic for B previ-
ously. She can answer B?s utterance of Rob? with (No,) Bob, the accounts
guy, as in (3) or simply with (No,) the accounts guy. Both are licensed
by the DS parsing mechanism without more ado. For both, the goal tree
will be as follows and it will always be the point of reference for checking
the subsumption relation relative to the simulated parsing steps described
further below:
(12) A?s goal tree
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x)))
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Under these circumstances, given the DS grammar-as-parser perspective,
several strategies are now available for the licensing of generation of the
fragment. A is licensed to repeat the name Bob by locally extending the
node in the context tree where the representation of the individual referred to
is located by using the rule of late*adjunction, a process which involves
building a node of type e from a dominating node of that type (illustrated in
Kempson et al 2007). An alternative way of licensing repetition of the word
Bob is to employ one of the strategies generally available for the parsing of
long distance dependencies i.e. constructing initial tree nodes as unfixed
(*adjunction). We show here how the latter strategy can be exploited to
license the production of the fragment by A.
(13) Parsing simulation licensing generation of Bob, the accounts guy
Step 1: *Adjunction Step 2: LINK-Adjuction + testing the accounts guy
?Ty(t)
(?, x,Bob
?
(x)),?
?Ty(t)
(?, x,Bob
?
(x)),?
U
Acc.guy
?
(U)
The only way to develop the constructed tree at Step 2 commensurate with
the goal tree (12) is to identify the value of U as (?, x,Bob
?
(x)), so this
is what is entered at the newly constructed linked tree, duly leading to
extension of the term originally given as annotating the unfixed node as
(?, x,Bob
?
(x) ? Acc.guy
?
(x)). The structure
5
derived by processing such an
extension is exactly that of (1) above (compare goal tree in (12) above
and tree in (8)). Now, as mentioned before, context, as defined in DS,
keeps track not only of tree representations and words but also of actions
contributed by the words and utilised in building up the tree representations.
Here, according to DS, production of the correction in (3) is licensed to be
5
Again note that DS trees represent derived content rather than structure over natural
language strings.
137
fragmental only because the original actions for parsing/producing the word
left are available in the context and can be recalled to complete the structure
initiated by processing/producing the name Bob. Now these stored actions
can be retrieved to develop the tree further:
(14) Parsing simulation licensing generation of Bob, the accounts guy
Step 3: test-processing stored actions for left
?Ty(t)
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) ?Ty(e),? Leave?
(?, x,Bob
?
(x))
Acc.guy
?
(?,x,Bob
?
(x))
With this partial tree being commensurate with the goal tree, all actions that
follow are general computational processes for completing the tree: unifying
the unfixed node to determine the subject argument, applying the subject
to the predicate, evaluating the quantified terms. Nothing specific to this
structure is needed. Indeed, all these mechanisms are equally applicable by
an individual speaker, perhaps more familiar as right dislocation phenomena,
but equally available incrementally:
(15) Bob left, (Bob) the accounts guy.
5 Conclusion
As these fragments and their construal have demonstrated, despite serving
distinct functions in dialogue, the mechanisms which make such diversity
possible are general strategies for tree growth. In all cases, the advantage
which use of fragments provides is a ?least effort? means of re-employing
previous content/structure/actions which constitute the minimal local con-
text. As modelled in DS, it is more economical to reuse information from
this local context rather than constructing representations afresh (via costly
processes of lexical retrieval, choice of alternative parsing strategies, etc.).
A further quandary in dialogue construal is that, despite such avenues
for economising their efforts, interlocutors are nevertheless faced with an
increasing set of interpretive options at any point during the construction
of representations. One strategy available to hearers is to delay a disam-
biguating move until further input potentially resolves the uncertainty. How-
ever, as further input is processed and parsing/interpretive options increase
rapidly, the human processor struggles. The incremental definition of the
DS formalism allows for the modelling of an alternative strategy available
to hearers: at any (sub-sentential) point they could opt to intervene imme-
diately, and make a direct appeal to the speaker for more information as
illustrated by the clause-medial fragment interruption (2). It seems clear
that the grammar should allow the resources for modelling this behaviour
without any complications.
138
The phenomena examined here are also cases where speakers? and hear-
ers? representations, despite attempts at coordination, may nevertheless sep-
arate sufficiently for them to have to seek ?repair? (see especially (3)). In
the model presented here, the dynamics of interaction allow fully incremen-
tal generation and integration of fragmental utterances so that interlocutors
can be taken to constantly provide optimal evidence of each other?s represen-
tations with necessary adjuncts being able to be incrementally introduced.
But such mechanisms apply equally within an individual utterance, with self-
correction, extension, elaboration, repetition etc. The effect is that all the de-
vices which seem so characteristic of dialogue involve mechanisms invariably
available within an individual?s core grammar. This suggests a new inverse
methodology: it is the challenge of modelling dialogue that can be used as
a point of departure for modelling grammars for individual speakers, rather
than the other, more familiar, way round (see also Ginzburg (forthcmg)).
This reversibility is, notably, straightforwardly available to grammar for-
malisms in which the incremental dynamics of information growth is the
core structural concept because emergent dialogue structure crucially ex-
hibits and interpretively relies on such incrementality.
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962 and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex Davies, Arash Eshghi,
Jonathan Ginzburg, Pat Healey, Greg Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. Linguistics, logic and finite trees.
Bulletin of the IGPL, 2:3?31, 1994.
Raquel Ferna?ndez. Non-Sentential Utterances in Dialogue: Classification, Resolu-
tion and Use. PhD thesis, King?s College London, University of London, 2006.
Jonathan Ginzburg. Semantics for Conversation. CSLI, forthcmg.
Ruth Kempson, Andrew Gargett, and Eleni Gregoromichelaki. Clarification re-
quests: An incremental account. In Proceedings of the 11th Workshop on the
Semantics and Pragmatics of Dialogue (DECALOG), 2007.
Wilfried Meyer-Viol. Instantial Logic. PhD thesis, University of Utrecht, 1995.
Matthew Purver, Ronnie Cann, and Ruth Kempson. Grammars as parsers: Meeting
the dialogue challenge. Research on Language and Computation, 4(2-3):289?326,
2006.
David Schlangen. A Coherence-Based Approach to the Interpretation of Non-
Sentential Utterances in Dialogue. PhD thesis, University of Edinburgh, 2003.
David Schlangen and Alex Lascarides. The interpretation of non-sentential utter-
ances in dialogue. In Proceedings of the 4th SIGdial Workshop on Discourse and
Dialogue, pages 62?71, Sapporo, Japan, July 2003. Association for Computa-
tional Linguistics.
Noor van Leusen and Reinhard Muskens. Construction by description in discourse
representation. In J. Peregrin, editor, Meaning: The Dynamic Turn, chapter 12,
pages 33?65. 2003.
139
