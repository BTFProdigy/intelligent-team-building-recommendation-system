Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620?629,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
Roy W. Tromble1 and Shankar Kumar2 and Franz Och2 and Wolfgang Macherey2
1Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
royt@jhu.edu
2 Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,och,wmach}@google.com
Abstract
We present Minimum Bayes-Risk (MBR) de-
coding over translation lattices that compactly
encode a huge number of translation hypothe-
ses. We describe conditions on the loss func-
tion that will enable efficient implementation
of MBR decoders on lattices. We introduce
an approximation to the BLEU score (Pap-
ineni et al, 2001) that satisfies these condi-
tions. The MBR decoding under this approx-
imate BLEU is realized using Weighted Fi-
nite State Automata. Our experiments show
that the Lattice MBR decoder yields mod-
erate, consistent gains in translation perfor-
mance over N-best MBR decoding on Arabic-
to-English, Chinese-to-English and English-
to-Chinese translation tasks. We conduct a
range of experiments to understand why Lat-
tice MBR improves upon N-best MBR and
study the impact of various parameters on
MBR performance.
1 Introduction
Statistical language processing systems for speech
recognition, machine translation or parsing typically
employ the Maximum A Posteriori (MAP) deci-
sion rule which optimizes the 0-1 loss function. In
contrast, these systems are evaluated using metrics
based on string-edit distance (Word Error Rate), n-
gram overlap (BLEU score (Papineni et al, 2001)),
or precision/recall relative to human annotations.
Minimum Bayes-Risk (MBR) decoding (Bickel and
Doksum, 1977) aims to address this mismatch by se-
lecting the hypothesis that minimizes the expected
error in classification. Thus it directly incorporates
the loss function into the decision criterion. The ap-
proach has been shown to give improvements over
the MAP classifier in many areas of natural lan-
guage processing including automatic speech recog-
nition (Goel and Byrne, 2000), machine transla-
tion (Kumar and Byrne, 2004; Zhang and Gildea,
2008), bilingual word alignment (Kumar and Byrne,
2002), and parsing (Goodman, 1996; Titov and Hen-
derson, 2006; Smith and Smith, 2007).
In statistical machine translation, MBR decoding
is generally implemented by re-ranking an N -best
list of translations produced by a first-pass decoder;
this list typically contains between 100 and 10, 000
hypotheses. Kumar and Byrne (2004) show that
MBR decoding gives optimal performance when the
loss function is matched to the evaluation criterion;
in particular, MBR under the sentence-level BLEU
loss function (Papineni et al, 2001) gives gains on
BLEU. This is despite the fact that the sentence-level
BLEU loss function is an approximation to the exact
corpus-level BLEU.
A different MBR inspired decoding approach is
pursued in Zhang and Gildea (2008) for machine
translation using Synchronous Context Free Gram-
mars. A forest generated by an initial decoding pass
is rescored using dynamic programming to maxi-
mize the expected count of synchronous constituents
in the tree that corresponds to the translation. Since
each constituent adds a new 4-gram to the existing
translation, this approach approximately maximizes
the expected BLEU.
In this paper we explore a different strategy
to perform MBR decoding over Translation Lat-
tices (Ueffing et al, 2002) that compactly encode a
huge number of translation alternatives relative to an
N -best list. This is a model-independent approach
620
in that the lattices could be produced by any statis-
tical MT system ? both phrase-based and syntax-
based systems would work in this framework. We
will introduce conditions on the loss functions that
can be incorporated in Lattice MBR decoding. We
describe an approximation to the BLEU score (Pa-
pineni et al, 2001) that will satisfy these condi-
tions. Our Lattice MBR decoding is realized using
Weighted Finite State Automata.
We expect Lattice MBR decoding to improve
upon N -best MBR primarily because lattices con-
tain many more candidate translations than the N -
best list. This has been demonstrated in speech
recognition (Goel and Byrne, 2000). We conduct
a range of translation experiments to analyze lattice
MBR and compare it with N -best MBR. An impor-
tant aspect of our lattice MBR is the linear approxi-
mation to the BLEU score. We will show that MBR
decoding under this score achieves a performance
that is at least as good as the performance obtained
under sentence-level BLEU score.
The rest of the paper is organized as follows. We
review MBR decoding in Section 2 and give the for-
mulation in terms of a gain function. In Section 3,
we describe the conditions on the gain function for
efficient decoding over a lattice. The implementa-
tion of lattice MBR with Weighted Finite State Au-
tomata is presented in Section 4. In Section 5, we in-
troduce the corpus BLEU approximation that makes
it possible to perform efficient lattice MBR decod-
ing. An example of lattice MBR with a toy lattice
is presented in Section 6. We present lattice MBR
experiments in Section 7. A final discussion is pre-
sented in Section 8.
2 Minimum Bayes Risk Decoding
Minimum Bayes-Risk (MBR) decoding aims to find
the candidate hypothesis that has the least expected
loss under the probability model (Bickel and Dok-
sum, 1977). We begin with a review of MBR decod-
ing for Statistical Machine Translation (SMT).
Statistical MT (Brown et al, 1990; Och and Ney,
2004) can be described as a mapping of a word se-
quence F in the source language to a word sequence
E in the target language; this mapping is produced
by the MT decoder ?(F ). If the reference transla-
tion E is known, the decoder performance can be
measured by the loss function L(E, ?(F )). Given
such a loss function L(E,E?) between an automatic
translation E? and the reference E, and an under-
lying probability model P (E|F ), the MBR decoder
has the following form (Goel and Byrne, 2000; Ku-
mar and Byrne, 2004):
E? = argmin
E??E
R(E?)
= argmin
E??E
?
E?E
L(E,E?)P (E|F ),
where R(E?) denotes the Bayes risk of candidate
translation E? under the loss function L.
If the loss function between any two hypotheses
can be bounded: L(E,E?) ? Lmax, the MBR de-
coder can be rewritten in terms of a gain function
G(E,E?) = Lmax ? L(E,E?):
E? = argmax
E??E
?
E?E
G(E,E?)P (E|F ). (1)
We are interested in performing MBR decoding
under a sentence-level BLEU score (Papineni et al,
2001) which behaves like a gain function: it varies
between 0 and 1, and a larger value reflects a higher
similarity. We will therefore use Equation 1 as the
MBR decoder.
We note that E represents the space of transla-
tions. For N -best MBR, this space E is the N -best
list produced by a baseline decoder. We will investi-
gate the use of a translation lattice for MBR decod-
ing; in this case, E will represent the set of candi-
dates encoded in the lattice.
In general, MBR decoding can use different
spaces for hypothesis selection and risk computa-
tion: argmax and the sum in Equation 1 (Goel,
2001). As an example, the hypothesis could be se-
lected from the N -best list while the risk is com-
puted based on the entire lattice. Therefore, the
MBR decoder can be more generally written as fol-
lows:
E? = argmax
E??Eh
?
E?Ee
G(E,E?)P (E|F ), (2)
where Eh refers to the Hypothesis space from where
the translations are chosen, and Ee refers to the Evi-
dence space that is used for computing the Bayes-
risk. We will present experiments (Section 7) to
show the relative importance of these two spaces.
621
3 Lattice MBR Decoding
We now present MBR decoding on translation lat-
tices. A translation word lattice is a compact rep-
resentation for very large N -best lists of transla-
tion hypotheses and their likelihoods. Formally,
it is an acyclic Weighted Finite State Acceptor
(WFSA) (Mohri, 2002) consisting of states and arcs
representing transitions between states. Each arc is
labeled with a word and a weight. Each path in the
lattice, consisting of consecutive transitions begin-
ning at the distinguished initial state and ending at a
final state, expresses a candidate translation. Aggre-
gation of the weights along the path1 produces the
weight of the path?s candidate H(E,F ) according
to the model. In our setting, this weight will imply
the posterior probability of the translation E given
the source sentence F :
P (E|F ) =
exp (?H(E,F ))
?
E??E exp (?H(E
?, F ))
. (3)
The scaling factor ? ? [0,?) flattens the distribu-
tion when ? < 1, and sharpens it when ? > 1.
Because a lattice may represent a number of can-
didates exponential in the size of its state set, it is of-
ten impractical to compute the MBR decoder (Equa-
tion 1) directly. However, if we can express the gain
function G as a sum of local gain functions gi, then
we now show that Equation 1 can be refactored and
the MBR decoder can be computed efficiently. We
loosely call a gain function local if it can be ap-
plied to all paths in the lattice via WFSA intersec-
tion (Mohri, 2002) without significantly multiplying
the number of states.
In this paper, we are primarily concerned with lo-
cal gain functions that weight n-grams. Let N =
{w1, . . . , w|N |} be the set of n-grams and let a local
gain function gw : E ? E ? R, for w ? N , be as
follows:
gw(E,E
?) = ?w#w(E
?)?w(E), (4)
where ?w is a constant, #w(E?) is the number of
times that w occurs in E?, and ?w(E) is 1 if w ? E
and 0 otherwise. That is, gw is ?w times the number
of occurrences of w in E?, or zero if w does not oc-
cur in E. We first assume that the overall gain func-
tion G(E,E?) can then be written as a sum of local
1using the log semiring?s extend operator
gain functions and a constant ?0 times the length of
the hypothesis E?.
G(E,E?) = ?0|E
?|+
?
w?N
gw(E,E
?) (5)
= ?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
Given a gain function of this form, we can rewrite
the risk (sum in Equation 1) as follows
?
E?E
G(E,E?)P (E|F )
=
?
E?E
(
?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
)
P (E|F )
= ?0|E
?|+
?
w?N
?w#w(E
?)
?
E?Ew
P (E|F ),
where Ew = {E ? E|?w(E) > 0} represents the
paths of the lattice containing the n-gram w at least
once. TheMBR decoder on lattices (Equation 1) can
therefore be written as
E? = argmax
E??E
{
?0|E
?|+
?
w?N
?w#w(E
?)p(w|E)
}
. (6)
Here p(w|E) =
?
E?Ew P (E|F ) is the posterior
probability of the n-gram w in the lattice. We have
thus replaced a summation over a possibly exponen-
tial number of items (E ? E) with a summation over
the number of n-grams that occur in E , which is at
worst polynomial in the number of edges in the lat-
tice that defines E . We compute the posterior proba-
bility of each n-gram w as:
p(w|E) =
?
E?Ew
P (E|F ) =
Z(Ew)
Z(E)
, (7)
where Z(E) =
?
E??E exp(?H(E
?, F )) (denomi-
nator in Equation 3) and
Z(Ew) =
?
E??Ew exp(?H(E
?, F )). Z(E) and
Z(Ew) represent the sums2 of weights of all paths
in the lattices Ew and E respectively.
4 WFSA MBR Computations
We now show how the Lattice MBR Decision Rule
(Equation 6) can be implemented using Weighted
Finite State Automata (Mohri, 1997). There are four
steps involved in decoding starting from weighted
finite-state automata representing the candidate out-
puts of a translation system. We will describe these
2in the log semiring, where log+(x, y) = log(ex + ey) is
the collect operator (Mohri, 2002)
622
steps in the setting where the evidence lattice Ee may
be different from the hypothesis lattice Eh (Equa-
tion 2).
1. Extract the set of n-grams that occur in the ev-
idence lattice Ee. For the usual BLEU score, n
ranges from one to four.
2. Compute the posterior probability p(w|E) of
each of these n-grams.
3. Intersect each n-gram w, with an appropriate
weight (from Equation 6), to an initially un-
weighted copy of the hypothesis lattice Eh.
4. Find the best path in the resulting automaton.
Computing the set of n-grams N that occur in a
finite automaton requires a traversal, in topological
order, of all the arcs in the automaton. Because the
lattice is acyclic, this is possible. Each state q in the
automaton has a corresponding set of n-grams Nq
ending there.
1. For each state q,Nq is initialized to {}, the set
containing the empty n-gram.
2. Each arc in the automaton extends each of its
source state?s n-grams by its word label, and
adds the resulting n-grams to the set of its tar-
get state. ( arcs do not extend n-grams, but
transfer them unchanged.) n-grams longer than
the desired order are discarded.
3. N is the union over all states q of Nq.
Given an n-gram, w, we construct an automaton
matching any path containing the n-gram, and in-
tersect that automaton with the lattice to find the set
of paths containing the n-gram (Ew in Equation 7).
Suppose E represent the weighted lattice, we com-
pute3: Ew = E ? (w w ??), where w = (?? w ??)
is the language that contains all strings that do not
contain the n-gram w. The posterior probability
p(w|E) of n-gram w can be computed as a ratio of
the total weights of paths in Ew to the total weights
of paths in the original lattice (Equation 7).
For each n-gram w ? N , we then construct
an automaton that accepts an input E with weight
3in the log semiring (Mohri, 2002)
equal to the product of the number of times the n-
gram occurs in the input (#w(E)), the n-gram fac-
tor ?w from Equation 6, and the posterior proba-
bility p(w|E). The automaton corresponds to the
weighted regular expression (Karttunen et al, 1996):
w?(w/(?wp(w|E)) w?)?.
We successively intersect each of these automata
with an automaton that begins as an unweighted
copy of the lattice Eh. This automaton must also
incorporate the factor ?0 of each word. This can
be accomplished by intersecting the unweighted lat-
tice with the automaton accepting (?/?0)?. The
resulting MBR automaton computes the total ex-
pected gain of each path. A path in this automa-
ton that corresponds to the word sequence E? has
cost: ?0|E?|+
?
w?N ?w#w(E)p(w|E) (expression
within the curly brackets in Equation 6).
Finally, we extract the best path from the resulting
automaton4, giving the lattice MBR candidate trans-
lation according to the gain function (Equation 6).
5 Linear Corpus BLEU
Our Lattice MBR formulation relies on the decom-
position of the overall gain function as a sum of lo-
cal gain functions (Equation 5). We here describe a
linear approximation to the log(BLEU score) (Pap-
ineni et al, 2001) which allows such a decomposi-
tion. This will enable us to rewrite the log(BLEU)
as a linear function of n-gram matches and the hy-
pothesis length. Our strategy will be to use a first
order Taylor-series approximation to what we call
the corpus log(BLEU) gain: the change in corpus
log(BLEU) contributed by the sentence relative to
not including that sentence in the corpus.
Let r be the reference length of the corpus, c0 the
candidate length, and {cn|1 ? n ? 4} the number
of n-gram matches. Then, the corpus BLEU score
B(r, c0, cn) can be defined as follows (Papineni et
al., 2001):
logB = min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0 ??n
,
? min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0
,
where we have ignored ?n, the difference between
the number of words in the candidate and the num-
4in the (max,+) semiring (Mohri, 2002)
623
ber of n-grams. If L is the average sentence length
in the corpus, ?n ? (n? 1)
c0
L .
The corpus log(BLEU) gain is defined as the
change in log(BLEU) when a new sentence?s (E?)
statistics are added to the corpus statistics:
G = logB? ? logB,
where the counts in B? are those of B plus those for
the current sentence. We will assume that the brevity
penalty (first term in the above approximation) does
not change when adding the new sentence. In exper-
iments not reported here, we found that taking into
account the brevity penalty at the sentence level can
cause large fluctuations in lattice MBR performance
on different test sets. We therefore treat only cns as
variables.
The corpus log BLEU gain is approximated by a
first-order vector Taylor series expansion about the
initial values of cn.
G ?
N?
n=0
(c?n ? cn)
? logB?
?c?n
?
?
?
?
c?n=cn
, (8)
where the partial derivatives are given by
? logB
?c0
=
?1
c0
, (9)
? logB
?cn
=
1
4cn
.
Substituting the derivatives in Equation 8 gives
G = ? logB ? ?
?c0
c0
+
1
4
4?
n=1
?cn
cn
, (10)
where each ?cn = c?n ? cn counts the statistic in
the sentence of interest, rather than the corpus as a
whole. This score is therefore a linear function in
counts of words ?c0 and n-gram matches ?cn. Our
approach ignores the count clipping present in the
exact BLEU score where a correct n-gram present
once in the reference but several times in the hypoth-
esis will be counted only once as correct. Such an
approach is also followed in Dreyer et al (2007).
Using the above first-order approximation to gain
in log corpus BLEU, Equation 9 implies that ?0, ?w
from Section 3 would have the following values:
?0 =
?1
c0
(11)
?w =
1
4c|w|
.
5.1 N-gram Factors
We now describe how the n-gram factors (Equa-
tion 11) are computed. The factors depend on
a set of n-gram matches and counts (cn; n ?
{0, 1, 2, 3, 4}). These factors could be obtained from
a decoding run on a development set. However, do-
ing so could make the performance of lattice MBR
very sensitive to the actual BLEU scores on a partic-
ular run. We would like to avoid such a dependence
and instead, obtain a set of parameters which can
be estimated from multiple decoding runs without
MBR. To achieve this, we make use of the properties
of n-gram matches. It is known that the average n-
gram precisions decay approximately exponentially
with n (Papineni et al, 2001). We now assume that
the number of matches of each n-gram is a constant
ratio r times the matches of the corresponding n? 1
gram.
If the unigram precision is p, we can obtain the
n-gram factors (n ? {1, 2, 3, 4}) (Equation 11) as a
function of the parameters p and r, and the number
of unigram tokens T :
?0 =
?1
T
(12)
?n =
1
4Tp? rn?1
We set p and r to the average values of unigram pre-
cision and precision ratio across multiple develop-
ment sets. Substituting the above factors in Equa-
tion 6, we find that the MBR decision does not de-
pend on T ; therefore any value of T can be used.
6 An Example
Figure 1 shows a toy lattice and the final MBR au-
tomaton (Section 4) for BLEU with a maximum n-
gram order of 2. We note that the MBR hypothesis
(bcde) has a higher decoder cost relative to the MAP
hypothesis (abde). However, bcde gets a higher ex-
pected gain (Equation 6) than abde since it shares
more n-grams with the Rank-3 hypothesis (bcda).
This illustrates how a lattice can help select MBR
translations that can differ from the MAP transla-
tion.
7 Experiments
We now present experiments to evaluate MBR de-
coding on lattices under the linear corpus BLEU
624
01
2
3
4
5
6
7
8
9
10
0
1
2
3
4
7
5
8
6
c/0.013
d/0.013
d/?0.008
d/?0.008
e/0.004
a/0.038
a/0.5
b/0.6
b/0.6
b/0.6
c/0.6
c/0.6
d/0.3
d/0.4
e/0.5
a/0.5
a/0.063
b/0.043
b/0.043
b/0.013
c/0.013
Figure 1: An example translation lattice with decoder
costs (top) and its MBR Automaton for BLEU-2 (bot-
tom). The bold path in the top is the MAP hypothesis
and the bold path in the bottom is the MBR hypothe-
sis. The precision parameters in Equation 12 are set to:
T = 10, p = 0.85, r = 0.72.
Dataset # of sentences
aren zhen enzh
dev1 1353 1788 1664
dev2 663 919 919
blind 1360 1357 1859
Table 1: Statistics over the development and test sets.
gain. We start with a description of the data sets
and the SMT system.
7.1 Development and Blind Test Sets
We present our experiments on the constrained data
track of the NIST 2008 Arabic-to-English (aren),
Chinese-to-English (zhen), and English-to-Chinese
(enzh) machine translation tasks.5 In all language
pairs, the parallel and monolingual data consists of
all the allowed training sets in the constrained track.
For each language pair, we use two development
sets: one for Minimum Error Rate Training (Och,
2003; Macherey et al, 2008), and the other for tun-
ing the scale factor for MBR decoding. Our devel-
opment sets consists of the NIST 2004/2003 evalu-
ation sets for both aren and zhen, and NIST 2006
(NIST portion)/2003 evaluation sets for enzh. We
report results on NIST 2008 which is our blind test
set. Statistics computed over these data sets are re-
ported in Table 1.
5http://www.nist.gov/speech/tests/mt/
7.2 MT System Description
Our phrase-based statistical MT system is similar to
the alignment template system described in Och and
Ney (2004). The system is trained on parallel cor-
pora allowed in the constrained track. We first per-
form sentence and sub-sentence chunk alignment on
the parallel documents. We then train word align-
ment models (Och and Ney, 2003) using 6 Model-1
iterations and 6 HMM iterations. An additional 2 it-
erations of Model-4 are performed for zhen and enzh
pairs. Word Alignments in both source-to-target
and target-to-source directions are obtained using
the Maximum A-Posteriori (MAP) framework (Ma-
tusov et al, 2004). An inventory of phrase-pairs
up to length 5 is then extracted from the union of
source-target and target-source alignments. Several
feature functions are then computed over the phrase-
pairs. 5-gram word language models are trained on
the allowed monolingual corpora. Minimum Error
Rate Training under BLEU is used for estimating
approximately 20 feature function weights over the
dev1 development set.
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004) using two decoding passes. The first de-
coder pass generates either a lattice or anN -best list.
MBR decoding is performed in the second pass. The
MBR scaling parameter (? in Equation 3) is tuned
on the dev2 development set.
7.3 Translation Results
We next report translation results from lattice MBR
decoding. All results will be presented on the NIST
2008 evaluation sets. We report results using the
NIST implementation of the BLEU score which
computes the brevity penalty using the shortest ref-
erence translation for each segment (NIST, 2002
2008). The BLEU scores are reported at the word-
level for aren and zhen but at the character level for
enzh. We measure statistical significance using 95%
confidence intervals computed with paired bootstrap
resampling (Koehn, 2004). In all tables, systems in a
column show statistically significant differences un-
less marked with an asterisk.
We first compare lattice MBR toN -best MBR de-
coding and MAP decoding (Table 2). In these ex-
periments, we hold the likelihood scaling factor ? a
625
BLEU(%)
aren zhen enzh
MAP 43.7 27.9 41.4
N -best MBR 43.9 28.3? 42.0
Lattice MBR 44.9 28.5? 42.6
Table 2: Lattice MBR, N -best MBR & MAP decoding.
On zhen, Lattice MBR and N -best MBR do not show
statistically significant differences.
constant; it is set to 0.2 for aren and enzh, and 0.1
for zhen. The translation lattices are pruned using
Forward-Backward pruning (Sixtus and Ortmanns,
1999) so that the average numbers of arcs per word
(lattice density) is 30. For N -best MBR, we use
N -best lists of size 1000. To match the loss func-
tion, Lattice MBR is performed at the word level for
aren/zhen and at the character level for enzh. Our
lattice MBR is implemented using the Google Open-
Fst library.6 In our experiments, p, r (Equation 12)
have values of 0.85/0.72, 0.80/0.62, and 0.63/0.48
for aren, zhen, and enzh respectively.
We note that Lattice MBR provides gains of 0.2-
1.0 BLEU points over N -best MBR, which in turn
gives 0.2-0.6 BLEU points over MAP. These gains
are obtained on top of a baseline system that has
competitive performance relative to the results re-
ported in the NIST 2008 Evaluation.7 This demon-
strates the effectiveness of lattice MBR decoding as
a realization of MBR decoding which yields sub-
stantial gains over the N -best implementation.
The gains from lattice MBR over N -best MBR
could be due to a combination of factors. These in-
clude: 1) better approximation of the corpus BLEU
score, 2) larger hypothesis space, and 3) larger evi-
dence space. We now present experiments to tease
apart these factors.
Our first experiment restricts both the hypothesis
and evidence spaces in lattice MBR to the 1000-best
list (Table 3). We compare this toN -best MBRwith:
a) sentence-level BLEU, and b) sentence-level log
BLEU.
The results show that when restricted to the 1000-
best list, Lattice MBR performs slightly better than
N -best MBR (with sentence BLEU) on aren/enzh
while N -best MBR is better on zhen. We hypothe-
6http://www.openfst.org/
7
http://www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
BLEU(%)
aren zhen enzh
Lattice MBR, Lin. Corpus BLEU 44.2 28.1 42.2
N -best MBR, Sent. BLEU 43.9? 28.3? 42.0?
N -best MBR, Sent. Log BLEU 44.0? 28.3? 41.9?
Table 3: Lattice and N-best MBR (with Sentence
BLEU/Sentence log BLEU) on a 1000-best list. In each
column, entries with an asterisk do not show statistically
significant differences.
BLEU(%)
Hyp Space Evid Space aren zhen enzh
Lattice Lattice 44.9 28.5 42.6
1000-best Lattice 44.6 28.5 42.6
Lattice 1000-best 44.1? 28.0? 42.1
1000-best 1000-best 44.2? 28.1? 42.2
Table 4: Lattice MBR with restrictions on hypothesis and
evidence spaces. In each column, entries with an asterisk
do not show statistically significant differences.
size that on aren/enzh, the linear corpus BLEU gain
(Equation 10) is better correlated to the actual cor-
pus BLEU than sentence-level BLEU while the op-
posite is true on zhen. N -best MBR gives similar
results with either sentence BLEU or sentence log
BLEU. This confirms that using a log BLEU score
does not change the outcome of MBR decoding and
further justifies our Taylor-series approximation of
the log BLEU score.
We next attempt to understand factors 2 and 3. To
do that, we carry out lattice MBR when either the
hypothesis or the evidence space in Equation 2 is re-
stricted to 1000-best hypotheses (Table 4). For com-
parison, we also include results from lattice MBR
when both hypothesis and evidence spaces are iden-
tical: either the full lattice or the 1000-best list (from
Tables 2 and 3).
These results show that lattice MBR results are
almost unchanged when the hypothesis space is re-
stricted to a 1000-best list. However, when the ev-
idence space is shrunk to a 1000-best list, there is
a significant degradation in performance; these lat-
ter results are almost identical to the scenario when
both evidence and hypothesis spaces are restricted
to the 1000-best list. This experiment throws light
on what makes lattice MBR effective over N -best
MBR. Relative to the N -best list, the translation lat-
tice provides a better estimate of the expected BLEU
score. On the other hand, there are few hypotheses
626
outside the 1000-best list which are selected by lat-
tice MBR.
Finally, we show how the performance of lattice
MBR changes as a function of the lattice density.
The lattice density is the average number of arcs per
word and can be varied using Forward-Backward
pruning (Sixtus and Ortmanns, 1999). Figure 2 re-
ports the average number of lattice paths and BLEU
scores as a function of lattice density. The results
show that Lattice MBR performance generally im-
proves when the size of the lattice is increased.
However, on zhen, there is a small drop beyond a
density of 10. This could be due to low quality (low
posterior probability) hypotheses that get included at
the larger densities and result in a poorer estimate of
the expected BLEU score. On aren and enzh, there
are some gains beyond a lattice density of 30. These
gains are relatively small and come at the expense
of higher memory usage; we therefore work with a
lattice density of 30 in all our experiments. We note
that Lattice MBR is operating over lattices which are
gigantic in comparison to the number of paths in an
N -best list. At a lattice density of 30, the lattices in
aren contain on an average about 1081 hypotheses!
7.4 Lattice MBR Scale Factor
We next examine the role of the scale factor ? in
lattice MBR decoding. The MBR scale factor de-
termines the flatness of the posterior distribution
(Equation 3). It is chosen using a grid search on the
dev2 set (Table 1). Figure 3 shows the variation in
BLEU scores on eval08 as this parameter is varied.
The results show that it is important to tune this fac-
tor. The optimal scale factor is identical for all three
language pairs. In experiments not reported in this
paper, we have found that the optimal scaling factor
on a moderately sized development set carries over
to unseen test sets.
7.5 Maximum n-gram Order
Lattice MBR Decoding (Equation 6) involves com-
puting a posterior probability for each n-gram in the
lattice. We would like to speed up the Lattice MBR
computation (Section 4) by restricting the maximum
order of the n-grams in the procedure. The results
(Table 5) show that on aren, there is no degradation
if we limit the maximum order of the n-grams to
3. However, on zhen/enzh, there is improvement by
BLEU(%)
Max n-gram order aren zhen enzh
1 38.7 26.8 40.0
2 44.1 27.4 42.2
3 44.9 28.0 42.4
4 44.9 28.5 42.6
Table 5: Lattice MBR as a function of max n-gram order.
considering 4-grams. We can therefore reduce Lat-
tice MBR computations in aren.
8 Discussion
We have presented a procedure for performing Min-
imum Bayes-Risk Decoding on translation lattices.
This is a significant development in that the MBR
decoder operates over a very large number of trans-
lations. In contrast, the current N -best implementa-
tion of MBR can be scaled to, at most, a few thou-
sands of hypotheses. If the number of hypotheses
is greater than, say 20,000, the N -best MBR be-
comes computationally expensive. The lattice MBR
technique is efficient when performed over enor-
mous number of hypotheses (up to 1080) since it
takes advantage of the compact structure of the lat-
tice. Lattice MBR gives consistent improvements in
translation performance over N -best MBR decod-
ing, which is used in many state-of-the-art research
translation systems. Moreover, we see gains on three
different language pairs.
There are two potential reasons why Lattice MBR
decoding could outperform N -best MBR: a larger
hypothesis space from which translations could be
selected or a larger evidence space for computing the
expected loss. Our experiments show that the main
improvement comes from the larger evidence space:
a larger set of translations in the lattice provides a
better estimate of the expected BLEU score. In other
words, the lattice provides a better posterior distri-
bution over translation hypotheses relative to an N -
best list. This is a novel insight into the workings
of MBR decoding. We believe this could be possi-
bly employed when designing discriminative train-
ing approaches for machine translation. More gener-
ally, we have found a component in machine transla-
tion where the posterior distribution over hypotheses
plays a crucial role.
We have shown the effect of the MBR scaling fac-
627
10 20 30 40
44
44.2
44.4
44.6
44.8
45 aren
         Lattice Density
33
85
121
161
187
208
B
L
E
U
(
%
)
10 20 30 4028.2
28.3
28.4
28.5
28.6
28.7 zhen
Lattice Density
6
22
37
49
59
65
10 20 30 4041.8
42
42.2
42.4
42.6 enzh
      Lattice Density
3
10
17
25
30
34
Figure 2: Lattice MBR vs. lattice density: aren/zhen/enzh. Each point also shows the loge(Avg. # of paths).
0 0.2 0.4 0.6 0.8 1
44
44.2
44.4
44.6
44.8 aren
Scale Factor 
B
L
E
U
(
%
)
0 0.2 0.4 0.6 0.8 127.9
28
28.1
28.2
28.3
28.4
28.5 zhen
Scale Factor
0 0.2 0.4 0.6 0.8 1
41.8
42
42.2
42.4
42.6
42.8 enzh
Scale Factor
Figure 3: Lattice MBR with various scale factors ?: aren/zhen/enzh.
tor on the performance of lattice MBR. The scale
factor determines the flatness of the posterior distri-
bution over translation hypotheses. A scale of 0.0
means a uniform distribution while 1.0 implies that
there is no scaling. This is an important parameter
that needs to be tuned on a development set. There
has been prior work in MBR speech recognition and
machine translation (Goel and Byrne, 2000; Ehling
et al, 2007) which has shown the need for tuning
this factor. Our MT system parameters are trained
with Minimum Error Rate Training which assigns a
very high posterior probability to the MAP transla-
tion. As a result, it is necessary to flatten the prob-
ability distribution so that MBR decoding can select
hypotheses other than the MAP hypothesis.
Our Lattice MBR implementation is made pos-
sible due to the linear approximation of the BLEU
score. This linearization technique has been applied
elsewhere when working with BLEU: Smith and
Eisner (2006) approximate the expectation of log
BLEU score. In both cases, a linear metric makes
it easier to compute the expectation. While we have
applied lattice MBR decoding to the approximate
BLEU score, we note that our procedure (Section 3)
is applicable to other gain functions which can be
decomposed as a sum of local gain functions. In par-
ticular, our framework might be useful with transla-
tion metrics such as TER (Snover et al, 2006) or
METEOR (Lavie and Agarwal, 2007).
In contrast to a phrase-based SMT system, a syn-
tax based SMT system (e.g. Zollmann and Venu-
gopal (2006)) can generate a hypergraph that rep-
resents a generalized translation lattice with words
and hidden tree structures. We believe that our lat-
tice MBR framework can be extended to such hy-
pergraphs with loss functions that take into account
both BLEU scores as well as parse tree structures.
Lattice and Forest based search and training pro-
cedures are not yet common in statistical machine
translation. However, they are promising because
the search space of translations is much larger than
the typical N -best list (Mi et al, 2008). We hope
that our approach will provide some insight into the
design of lattice-based search procedures along with
the use of non-linear, global loss functions such as
BLEU.
References
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J . Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.
628
Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79?85.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In SSST, NAACL-HLT
2007, pages 103?110, Rochester, NY, USA, April.
N. Ehling, R. Zens, and H. Ney. 2007. Minimum Bayes
Risk Decoding for BLEU. In ACL 2007, pages 101?
104, Prague, Czech Republic, June.
V. Goel and W. Byrne. 2000. Minimum Bayes-Risk Au-
tomatic Speech Recognition. Computer Speech and
Language, 14(2):115?135.
V. Goel. 2001. Minimum Bayes-Risk Automatic Speech
Recognition. Ph.D. thesis, Johns Hopkins University,
Baltimore, MD, USA.
J. Goodman. 1996. Parsing Algorithms and Metrics. In
ACL, pages 177?183, Santa Cruz, CA, USA.
L. Karttunen, J-p. Chanod, G. Grefenstette, and
A. Schiller. 1996. Regular Expressions for Language
Engineering. Natural Language Engineering, 2:305?
328.
P. Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In EMNLP, Barcelona,
Spain.
S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk
word alignments of bilingual texts. In EMNLP, pages
140?147, Philadelphia, PA, USA.
S. Kumar and W. Byrne. 2004. Minimum Bayes-Risk
Decoding for Statistical Machine Translation. In HLT-
NAACL, pages 169?176, Boston, MA, USA.
A. Lavie and A. Agarwal. 2007. METEOR: An Auto-
matic Metric for MT Evaluation with High Levels of
Correlation with Human Judgments. In SMT Work-
shop, ACL, pages 228?231, Prague, Czech Republic.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based Minimum Error Rate Training for Sta-
tistical Machine Translation. In EMNLP, Honolulu,
Hawaii, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Translation.
In COLING, Geneva, Switzerland.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based Trans-
lation. In ACL, Columbus, OH, USA.
M.Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(3).
M. Mohri. 2002. Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3):321?350.
NIST. 2002-2008. The NIST Machine Translation Eval-
uations. http://www.nist.gov/speech/tests/mt/.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 ? 51.
F. Och and H. Ney. 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computa-
tional Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statisti-
cal Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Technical Report RC22176 (W0109-
022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
D. Smith and J. Eisner. 2006. Minimum Risk Anneal-
ing for Training Log-Linear Models. In ACL, Sydney,
Australia.
D. Smith and N. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In EMNLP-CoNLL,
Prague, Czech Republic.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA, Boston,
MA, USA.
I. Titov and J. Henderson. 2006. Loss Minimization in
Parse Reranking. In EMNLP, Sydney, Australia.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
Word Graphs in Statistical Machine Translation. In
EMNLP, Philadelphia, PA, USA.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass De-
coding for Synchronous Context Free Grammars. In
ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
629
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 475?482, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Context-Based Morphological Disambiguation with Random Fields?
Noah A. Smith and David A. Smith and Roy W. Tromble
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,dasmith,royt}@cs.jhu.edu
Abstract
Finite-state approaches have been highly successful at describ-
ing the morphological processes of many languages. Such
approaches have largely focused on modeling the phone- or
character-level processes that generate candidate lexical types,
rather than tokens in context. For the full analysis of words
in context, disambiguation is also required (Hakkani-Tu?r et al,
2000; Hajic? et al, 2001). In this paper, we apply a novel
source-channel model to the problem of morphological disam-
biguation (segmentation into morphemes, lemmatization, and
POS tagging) for concatenative, templatic, and inflectional lan-
guages. The channel model exploits an existing morphological
dictionary, constraining each word?s analysis to be linguistically
valid. The source model is a factored, conditionally-estimated
random field (Lafferty et al, 2001) that learns to disambiguate
the full sentence by modeling local contexts. Compared with
baseline state-of-the-art methods, our method achieves statisti-
cally significant error rate reductions on Korean, Arabic, and
Czech, for various training set sizes and accuracy measures.
1 Introduction
One of the great successes in computational linguistics
has been the construction of morphological analyzers for
diverse languages. Such tools take in words and enu-
merate the possible morphological analyses?typically a
sequence of morphemes, perhaps part-of-speech tagged.
They are often encoded as finite-state transducers (Ka-
plan and Kay, 1981; Koskenniemi, 1983; Beesley and
Karttunen, 2003).
What such tools do not provide is a means to dis-
ambiguate a word in context. For languages with com-
plex morphological systems (inflective, agglutinative,
and polysynthetic languages, for example), a word form
may have many analyses. To pick the right one, we
must consider the word?s context. This problem has
been tackled using statistical sequence models for Turk-
ish (Hakkani-Tu?r et al, 2000) and Czech (Hajic? et al,
2001); their approaches (and ours) are not unlike POS
tagging, albeit with complex tags.
?This work was supported by a Fannie and John Hertz
Foundation Fellowship, a NSF Fellowship, and a NDSEG Fel-
lowship (sponsored by ARO and DOD). The views expressed
are not necessarily endorsed by sponsors. We thank Eric Gold-
lust and Markus Dreyer for Dyna language support and Jason
Eisner, David Yarowsky, and three anonymous reviewers for
comments that improved the paper. We also thank Jan Hajic?
and Pavel Krbec for sharing their Czech tagger.
In this paper, we describe context-based models for
morphological disambiguation that take full account of
existing morphological dictionaries by estimating condi-
tionally against only dictionary-accepted analyses of a
sentence (?2). These models are an instance of condi-
tional random fields (CRFs; Lafferty et al, 2001) and
include overlapping features. Our applications include
diverse disambiguation frameworks and we make use of
linguistically-inspired features, such as local lemma de-
pendencies and inflectional agreement. We apply our
model to Korean and Arabic, demonstrating state-of-the-
art results in both cases (?3). We then describe how our
model can be expanded to complex, structured morpho-
logical tagging, including an efficient estimation method,
demonstrating performance on Czech (?4).
2 Modeling Framework
Our framework is a source-channel model (Jelinek,
1976). The source (modeled probabilistically by ps) gen-
erates a sequence of unambiguous tagged morphemes
y = ?y1, y2, ...? ? Y+ (Y is the set of unambiguous
tagged morphemes in the language).1 The precise con-
tents of the tag will vary by language and corpus but
will minimally include POS. y passes through a chan-
nel (modeled by pc), which outputs x = ?x1, x2, ...? ?
(X ? {OOV})+, a sequence of surface-level words in the
language and out-of-vocabulary words (OOV; X is the
language?s vocabulary). Note that |x| may be smaller
than |y|, since some morphemes may combine to make
a word. We will denote by yi the contiguous subse-
quence of y that generates xi; ~y will refer to a dictionary-
recognized type in Y+.
At test time, we decode the observed x into the most
probable sequence of tag/morpheme pairs:
y? = argmax
y
p(y | x) = argmax
y
ps(y) ? pc(x | y) (1)
Training involves constructing ps and pc. We assume
that there exists a training corpus of text (each word xi
annotated with its correct analysis y?i ) and a morpholog-
ical dictionary. We next describe the channel model and
the source model.
1The sequence also includes segmentation markings be-
tween words, not shown to preserve clarity.
475
a. There are many kinds of trench mortars.
b. . ? 1998 1998?Sanaa accuses Riyadh of occupying border territories.
0 1NUM/1998 2PUNC/- 3NOUN_PROP/SnEA?NOUN_PROP/SanoEA? 4
IV3FS/tuIV2MS/tu
5IV3FS/taIV2MS/ta 6NOUN_PROP/tthm
IV_PASS/t~ahamIV/t~ahim 7 8 9 10 11 12 13 14
c. Klimatizovana? j??delna, sve?tla? m??stnost pro sn??dane?. Air-conditioned dining room, well-lit breakfast room.
0 1
Adj {Neu Pl Acc Pos Aff}/klimatizovan?Adj {Neu Pl Voc Pos Aff}/klimatizovan?Adj {Fem Si Voc Pos Aff}/klimatizovan?Adj {Fem Si Nom Pos Aff}/klimatizovan?Adj {Neu Pl Nom Pos Aff}/klimatizovan? 2Noun {Fem Si Nom Aff}/j?delna 3Punc/, 4
Adj {Neu Pl Pos Aff}/svetl?Adj {Fem Si Voc Pos Aff}/svetl?Adj {Neu Pl Acc Pos Aff}/svetl?Adj {Neu Pl Voc Pos Aff}/svetl?Adj {Fem Si Nom Pos Aff}/svetl? 5Noun {Fem Si Acc Aff}/m?stnostNoun {Fem Si Nom Aff}/m?stnost 6 7 8
Figure 1: Lattices for example sentences in Korean (a), Arabic (b), and Czech (c). Arabic lemmas are not shown, and some Arabic
and Czech arcs are unlabeled, for readability. The Arabic morphemes are shown in Buckwalter?s encoding. The arcs in the correct
path through each lattice are solid (incorrect arcs are dashed). Note the adjective-noun agreement in the correct path through the
Czech lattice (c). The Czech lattice has no lemma-ambiguity; this is typical in Czech (see ?4).
2.1 Morphological dictionaries and the channel
A great deal of research has gone into developing mor-
phological analysis tools that enumerate valid analyses
~y ? Y+ for a particular word x ? X. Typically these
tools are unweighted and therefore do not enable token
disambiguation.2
They are available for many languages. We will refer
to this source of categorial lexical information as a mor-
phological dictionary d that maps X ? 2Y+ . The set d(x)
is the set of analyses for word x; the set d(x) is the set of
whole-sentence analyses for sentence x = ?x1, x2, ...?.
d(x) can be represented as an acyclic lattice with a
?sausage? shape familiar from work in speech recogni-
tion (Mangu et al, 1999). Note that for languages with
bound morphemes, d(x) will consist of a set of sequences
of tokens, so a given ?link? in the sausage lattice may
contain paths of different lengths. Fig. 1 shows sausage
lattices for sentences in three languages.
In this paper, the dictionary defines the support set of
the channel model. That is, pc(x | y) > 0 if and only
if y ? d(x). This is a clean way to incorporate do-
main knowledge into the probabilistic model; this kind
of constraint has been applied in previous work at decod-
ing time (Hakkani-Tu?r et al, 2000; Hajic? et al, 2001). In
such a model, each word is independent of its neighbors
(because the dictionary ignores context).
Estimation. A unigram channel model defines
2Probabilistic modeling of what we call the morphologi-
cal channel was first carried out by Levinger et al (1995), who
used unlabeled data to estimate p(~y | x) for Hebrew, with the
support defined by a dictionary.
pc(x | y)
def
=
|x|?
i=1
p(xi | yi) (2)
The simplest estimate of this model is to make p(?, ?)
uniform over (x, ~y) such that ~y ? d(x). Doing so and
marginalizing to get p(x | ~y) makes the channel model
encode categorial information only, leaving all learning
to the source model.3
Another way to estimate this model is, of course,
from data. This is troublesome, because?modulo
optionality?x is expected to be known given ~y, result-
ing in a huge model with mostly 1-valued probabili-
ties. Our solution is to take a projection pi of ~y and let
p(? | ~y) ? p(? | pi(~y)). In this paper, pi maps the analysis
to its morphological tag (or tag sequence). We will refer
to this as the ?tag channel.?
OOV. Morphological dictionaries typically do not have
complete coverage of a language. We can augment them
in two ways using the training data. If a known word x
(one for which d(x) is non-empty) appears in the training
dataset with an analysis not in d(x), we add the entry to
the dictionary. Unknown words (those not recognized by
the dictionary) are replaced by an OOV symbol. d(OOV)
is taken to be the set of all analyses for any OOV word
seen in training. Rather than attempt to recover the mor-
pheme sequence for an OOV word, in this paper we try
only for the tag sequence, replacing all of an OOV?s mor-
phemes with the OOV symbol. Since OOV symbols ac-
count for less than 2% of words in our corpora, we leave
3Note that this makes the channel term in Eq. 1 a constant.
Then decoding means maximizing ps(y) over y ? d(x), equiv-
alently maximizing p(y | d(x)).
476
more sophisticated channel models to future work.
2.2 The source model
The source model ps defines a probability distribution
over Y+, sequences of (tag, morpheme) pairs. Our source
models can be viewed as weighted multi-tape finite-state
automata, where the weights are associated with local, of-
ten overlapping features of the path through the automa-
ton.
Estimation. We estimate the source conditionally from
annotated data. That is, we maximize
?
(x,y)?X+?Y+
p?(x,y) log ps
(
y | d(x), ~?
)
(3)
where p?(?, ?) is the empirical distribution defined by the
training data and ~? are the model parameters. In terms
of Fig. 1, our learner maximizes the weight of the correct
(solid) path through each lattice, at the expense of the
other incorrect (dashed) paths. Note that
log ps
(
y | d(x), ~?
)
= log
ps
(
y | ~?
)
?
y??d(x) ps
(
y? | ~?
) (4)
The sum in the denominator is computed using a dynamic
programming algorithm (akin to the forward algorithm);
it involves computing the sum of all paths through the
?sausage? lattice of possible analyses for x. By doing
this, we allow knowledge of the support of the channel
model to enter into our estimation of the source model. It
is important to note that the estimation of the model (the
objective function used in training, Eq. 3) is distinct from
the source-channel structure of the model (Eq. 1).
The lattice-conditional estimation approach was
first used by Kudo et al (2004) for Japanese seg-
mentation and hierarchical POS-tagging and by
Smith and Smith (2004) for Korean morphological
disambiguation. The resulting model is an instance of
a conditional random field (CRF; Lafferty et al, 2001).
When training a CRF for POS tagging, IOB chunking
(Sha and Pereira, 2003), or word segmentation (Peng
et al, 2004), one typically structures the conditional
probabilities (in the objective function) using domain
knowledge: in POS tagging, the set of allowed tags for
a word is used; in IOB chunking, the bigram ?O I? is
disallowed; and in segmentation, a lexicon is used to
enumerate the possible word boundaries.4
4This refinement is in the same vein as the move from max-
imum likelihood estimation to conditional estimation. MLE
would make the sum in the denominator of Eq. 4 Y+, which
for log-linear models is often intractable to compute (and for
sequence models may not converge). Conditional estimation
limits the sum to the subset of Y+ that is consistent with x, and
our variant further stipulates consistency with the dictionary en-
tries for x.
Our approach is the same, with two modifications.
First, we model the relationship between labels yi and
words xi in a separately-estimated channel model (?2.1).
Second, our labels are complex. Each word xi is tagged
with a sequence of one or more tagged morphemes; the
tags may include multiple fields. This leads to models
with more parameters. It also makes the dictionary es-
pecially important for limiting the size of the sum in the
denominator, since a complex label set Y could in prin-
ciple lead to a huge hypothesis space for a given sen-
tence x. Importantly, it makes training conditions more
closely match testing conditions, ruling out hypotheses a
dictionary-aware decoder would never consider.
Optimization. The objective function (Eq. 3) is con-
cave and known to have a unique global maximum. Be-
cause log-linear models and CRFs have been widely de-
scribed elsewhere (e.g., Lafferty, 2001), we note only that
we apply a standard first-order numerical optimization
method (L-BFGS; Liu and Nocedal, 1989). The struc-
ture, features, and regularization of our models will be
described in ?3 and ?4.
Prior work (morphological source models).
Hakkani-Tu?r et al (2000) described a system for Turkish
that was essentially a source model; Hajic? et al (2001)
described an HMM-based system for Czech that could
be viewed as a combined source and channel. Both
used dictionaries and estimated their (generative) models
using maximum likelihood (with smoothing).5 Given
enough data, a ML-estimated model will learn to recog-
nize a good path y, but it may not learn to discriminate
a good y from wrong alternatives per se. The generative
framework is limiting as well, disallowing the straight-
forward inclusion of arbitrary overlapping features. We
present a competitive Czech model in ?4.
3 Concatenative Models
The beauty of log-linear models is that estimation is
straightforward given any features, even ones that are
not orthogonal (i.e., ?overlap?). This permits focusing
on feature (or feature template) selection without worries
about the mathematics of training.
We consider two languages modeled by concatenative
processes with surface changes at morpheme boundaries:
Korean and Arabic.
Our model includes features for tag n-grams, mor-
pheme n-grams, and pairs of the two (possibly of differ-
ent lengths and offsets). Fig. 2 illustrates TM3, our base
model. TM3 includes feature templates for some tuples
of three or fewer elements, plus begin and end templates.
5Hajic? et al also included a rule-based system for pruning
hypotheses, which gave slight performance gains.
477
i?1
i?1
T
M
Tn
Mn
?2iT
?2iMM1
1T Ti
Mi
morpheme trigram
tag trigram
begin
features
end
features
tag/morpheme pair
tag + prev. morpheme
tag bigram
morpheme unigram
Figure 2: The base two-level trigram source model, TM3. Each
polygon corresponds to a feature template. This is a two level,
second-order Markov model (weighted finite-state machine) pa-
rameterized with overlapping features. Note that only some fea-
tures are labeled in the diagram.
A variant, TM3H, includes all of the same templates,
plus a similar set of templates that look only at head mor-
phemes. For instance, a feature fires for each trigram
of heads, even though there are (bound) morphemes be-
tween them. This increases the domain of locality for se-
mantic content-bearing morphemes. This model requires
slight changes to the dynamic programming algorithms
for inference and training (the previous two heads must
be remembered at each state).
Every instantiation of the templates seen in any lattice
d(x) built from training data is included in the model, not
just those seen in correct analyses y?.6
3.1 Experimental design
In all of our experiments, we vary the training set size
and the amount of smoothing, which is enforced by a di-
agonal Gaussian prior (L2 regularizer) with variance ?2.
The ?2 = ? case is equivalent to not smoothing. We
compare performance to the expected performance of a
randomized baseline that picks for each word token x an
analysis from d(x); this gives a measure of the amount of
ambiguity and is denoted ?channel only.? Performance
of unigram, bigram, and trigram HMMs estimated us-
ing maximum likelihood (barely smoothed, using add-
10?14) is also reported. (The unigram HMM simply
picks the most likely ~y for each x, based on training data
and is so marked.)
In the experiments in this section, we report three per-
formance measures. Tagging accuracy is the fraction
of words whose tag sequence was correctly identified
in entirety; morpheme accuracy is defined analogously.
6If we used only features observed to occur in y?, we would
not be able to learn negative weights for unlikely bits of structure
seen in the lattice d(x) but not in y?.
Lemma accuracy is the fraction of words whose lemma
was correctly identified.
3.2 Korean experiments
We applied TM3 and TM3H to Korean. The dataset is
the Korean Treebank (Han et al, 2002), with up to 90%
used for training and 10% (5K words) for test. The mor-
phological dictionary is klex (Han, 2004). There are 27
POS tags in the tag set; the corpus contains 10K word
types and 3,272 morpheme types. There are 1.7 mor-
phemes per word token on average (? = 0.75). A Ko-
rean word generally consists of a head morpheme with a
series of enclitic suffixes. In training the head-augmented
model TM3H, we assume the first morpheme of every
word is the head and lemma.
Results are shown in Tab. 1. TM3H achieved very slight
gains over TM3, and the tag channel model was helpful
only with the smaller training set. The oracle (last line
of Tab. 1) demonstrates that the coverage of the dictio-
nary remains an obstacle, particularly for recovering mor-
phemes. Another limitation is the small amount of train-
ing data, which may be masking differences among esti-
mation conditions. We report the performance of TM3H
with ?factored? estimation. This will be discussed in
detail in ?4; it means that a model containing only the
head features was trained on its own, then combined with
the independently trained TM3 model at test time. Fac-
tored training was slightly faster and did not affect per-
formance at all; accuracy scores were identical with un-
factored training.
Prior work (Korean). Similar results were presented
by Smith and Smith (2004), using a similar estimation
strategy with a model that included far more feature tem-
plates. TM3 has about a third as many parameters and
TM3H about half; performance is roughly the same (num-
bers omitted for space). Korean disambiguation results
were also reported by Cha et al (1998), who applied a
deterministic morpheme pattern dictionary to segment
words, then used a bigram HMM tagger. They also ap-
plied transformation-based learning to fix common er-
rors. Due to differences in tag set and data, we cannot
compare to that model; a bigram baseline is included.
3.3 Arabic experiments
We applied TM3 and TM3H to Arabic. The dataset is the
Arabic Treebank (Maamouri et al, 2003), with up to 90%
used for training and 10% (13K words) for test. The mor-
phological dictionary is Buckwalter?s analyzer (version
2), made available by the LDC (Buckwalter, 2004).7 This
analyzer has total coverage of the corpus; there are no
7Arabic morphological processing was also addressed by
Kiraz (2000), who gives a detailed review of symbolic work in
that area, and by Darwish (2002).
478
Korean Arabic
POS tagging morpheme lemma POS tagging morpheme lemma
accuracy accuracy accuracy accuracy accuracy accuracy
?2 32K 49K 32K 49K 32K 49K 38K 76K 114K 38K 76K 114K 38K 76K 114K
most likely ~y 86.0 86.9 87.5 88.8 95.3 95.7 84.5 87.0 88.3 83.2 86.2 87.0 37.9 39.8 40.9
channel only 62.6 62.6 70.3 70.8 86.4 86.4 43.7 43.7 43.7 41.2 41.2 41.2 27.2 27.2 27.2
bigram HMM 90.7 91.2 83.2 86.1 96.9 97.2 90.3 92.0 92.8 89.2 91.4 91.6 85.7? 87.8? 87.9?
trigram HMM 91.5 91.8 83.3 86.0 97.0 97.2 89.8 92.0 93.0 88.5 91.3 91.3 85.2? 87.8? 87.7?
TM3 ? 90.7 91.3 89.3 90.5 97.1 97.4 94.6 95.4 95.9 93.4 94.3 94.9 89.7? 90.5? 90.7?
u
n
ifo
rm
ch
an
ne
l
10 91.2 91.7 89.4 90.6 97.1 97.6 95.3 95.7 96.1 93.9 94.5 95.0 90.2? 90.6? 91.1?
1 91.5 92.2 89.4 90.6 97.1 97.5 95.2 95.7 96.0 93.9 94.5 94.7 90.0? 90.7? 91.0?
TM3H ? 91.1 91.1 89.3 90.4 97.2 97.5 95.0 95.7 96.0 94.0 94.8 95.3 93.3 93.9 94.2
(factored) 10 91.3 91.9 89.5 90.6 97.3 97.6 95.3 95.7 96.1 94.2 94.7 95.4 93.4 93.6 94.4
1 91.4 92.2 89.5 90.7 97.3 97.6 95.4 95.8 96.1 94.4 94.8 95.1 93.3 93.8 94.2
channel only 51.4 51.3 60.6 60.4 81.2 81.7 41.4 40.6 40.1 39.9 39.1 38.6 26.7? 26.5? 26.4?
bigram HMM 91.2 90.9 88.9 90.1 97.0 97.3 91.0 92.3 93.4 89.7 91.5 91.9 88.1? 89.9? 90.0?
trigram HMM 91.6 91.9 88.9 90.2 97.1 97.4 91.1 92.9 93.7 89.6 92.2 92.0 88.1? 90.6? 90.4?
TM3 ? 90.8 91.0 89.5 90.5 97.4 97.5 95.1 95.7 96.0 93.8 94.6 95.0 92.2? 93.1? 93.2?
ta
g
ch
an
ne
l
10 90.6 91.1 89.5 90.7 97.2 97.6 95.2 95.6 96.0 93.9 94.7 95.0 92.4? 93.2? 93.5?
1 90.1 90.9 89.5 90.7 97.1 97.6 94.9 95.5 95.8 93.8 94.5 94.8 92.2? 93.0? 93.1?
TM3H ? 91.0 91.0 89.4 90.5 97.2 97.6 95.1 95.8 96.0 94.0 95.1 95.4 93.3 94.3 94.4
(factored) 10 90.4 91.2 89.6 90.7 97.4 97.6 95.2 95.7 96.0 94.1 94.8 95.4 93.3 94.0 94.6
1 90.1 91.0 89.5 90.7 97.3 97.6 95.1 95.5 95.9 94.1 94.9 95.1 93.3 94.0 94.4
oracle given d(x) 95.3 95.7 90.2 91.2 98.1 98.3 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
Table 1: Korean (left, 5K test-set) and Arabic (right, 13K test-set) disambiguation. A word is marked correct only if its entire
tag (or morpheme) sequence (or lemma) was correctly identified. Morpheme and lemma accuracy do not include OOV words. The
oracle is an upper bound on accuracy given the morphological dictionary. ?These models do not explicitly predict lemmas; the
lemma is chosen arbitrarily from those that match the hypothesized tag/morpheme sequence for each word. Bold scores indicate a
significant improvement over the trigram HMM (binomial sign test, p < 0.05).
OOV words. There are 139 distinct POS tags; these con-
tain some inflectional information which we treat atom-
ically. For speed, TM3H was trained in two separate
pieces: TM3 and the lemma features added by TM3H.
Arabic has a templatic morphology in which conso-
nantal roots are transformed into surface words by the
insertion of vowels and ancillary consonants. Our sys-
tem does not model this process except through the use
of Buckwalter?s dictionary to define the set of analyses
for each word (cf., Daya et al, 2004, who modeled inter-
digitation in Hebrew). We treat the analysis of an Ara-
bic word as a sequence ~y of pairs of morphemes and
POS tags, plus a lemma. The lemma, given in the dic-
tionary, provides further disambiguation beyond the head
morpheme. The lemma is a standalone dictionary head-
word and not merely the consonantal root, as in some
other work. The ?heads? modeled by TM3H correspond
to these lemmas. There are 20K word types, and 34K
morpheme types. There are 1.7 morphemes per word to-
ken on average (? = 0.77).
Results are shown in Tab. 1. Across tasks and training
set sizes, our models reduce error rates by more than 36%
compared to the trigram HMM source with tag channel.
The TM3H model and the tag channel offer slight gains
over the base TM3 model (especially on lemmatization),
though the tag channel offers no help in POS tagging.
Prior work (Arabic). Both Diab et al (2004) and
Habash and Rambow (2005) use support-vector ma-
chines with local features; the former for tokenization,
POS tagging, and base phrase chunking; the latter for
full morphological disambiguation. Diab et al report
results for a coarsened 24-tag set, while we use the full
139 tags from the Arabic Treebank, so the systems are
not directly comparable. Habash and Rambow present
even better results on the same POS tag set. Our full dis-
ambiguation results appear to be competitive with theirs.
Khoja (2001) and Freeman (2001) describe Arabic POS
taggers and many of the issues involved in developing
them, but because tagged corpora did not yet exist, there
are no comparable quantitative results.
4 Czech: Model and Experiments
Inflective languages like Czech present a new set of chal-
lenges. Our treatment of Czech is not concatenative;
following prior work, the analysis for each word x is a
single tag/lemma pair y. Inflectional affixes in the sur-
face form are represented as features in the tag. While
lemmatization of Czech is not hard (there is little ambi-
guity), tagging is quite difficult, because morphological
tags are highly complex. Our tag set is the Prague Depen-
dency Treebank (PDT; Hajic?, 1998) set, which consists of
fifteen-field tags that indicate POS as well as inflectional
information (case, number, gender, etc.). There are over
479
full model (decoding) factored models (training)
gender
number case
POSmo
rpholo
gical tagsmorph. tag
num.gen. case POS
lemma lemm
asy
y
y
y
1
2
3
4
Figure 3: The Czech model, shown as an undirected graphi-
cal model. The structure of the full model is on the left; fac-
tored components for estimation are shown on the right. Each
of these five models contains a subset of the TM3 features. The
full model is only used to decode. The factored models make
training faster and are used for pruning.
1,400 distinct tag types in the PDT.
Czech has been treated probabilistically before, per-
haps most successfully by Hajic? et al (2001).8 In con-
trast, we estimate conditionally (rather than by maximum
likelihood for a generative HMM) and separate the train-
ing of the source and the channel. We also introduce a
novel factored treatment of the morphological tags.
4.1 Factored tags and estimation
Because Czech morphological tags are not monolithic,
the choice among them can be treated as several more or
less orthogonal decisions. The case feature of one word,
for example, is expected to be conditionally independent
of the next word?s gender, given the next word?s case.
Constraints in the language are expected to cause features
like case, number, and gender to agree locally (on words
that have such features) and somewhat independently of
each other. Coarser POS tagging may be treated as an-
other, roughly independent stream.
Log-linear models and the use of a morphological dic-
tionary make this kind of tag factoring possible. Our
approach is to separately train five log-linear models.
Each model is itself an instance of some of the templates
from TM3, modeling a projection of the full analysis.
The model and its factored components are illustrated in
Fig. 3.
POS model. The full tag is replaced by the POS tag
(the first two fields); there are 60 POS tags. The TM3
8Czech morphological processing was studied by
Petkevic? (2001), Hlava?cova? (2001) (who focuses on han-
dling OOV words), and Mra?kova? and Sedlacek (2003) (who use
partial parsing to reduce the set of possible analyses), inter alia.
feature templates are included twice: once for the full tag
and once for a coarser tag (the first PDT field, for which
there are 12 possible values).9
Gender, number, and case models. The full tag is re-
placed by the gender (or case or number) field. This
model includes bigrams and trigrams as well as field-
morpheme unigram features. These models are intended
to learn to predict local agreement.
Tag-lemma model. This model contains unigram fea-
tures of full PDT tags, both alone and with lemmas. It is
intended to learn to penalize morphological tags that are
rare, or that are rare with a particular lemma. In our for-
mulation, this is not a channel model, because it ignores
the surface word forms.
Each model is estimated independently of the others.
The lattice d(x) against which the conditional probabili-
ties are estimated contains the relevant projection of the
full morphological tags (with lemmas). To decode, we
run a Viterbi-like algorithm that uses the union of all
models? features to pick the best analysis (full morpho-
logical tags and lemmas) allowed by the dictionary.
There are two important advantages of factored train-
ing. First, each model is faster to train alone than a model
with all features merged; in fact, training the fully merged
model takes far too long to be practical. Second, factored
models can be held out at test time to measure their effect
on the system, without retraining.
Prior work (factored training). Separately training
different models that predict the same variables (e.g., x
and y) then combining them for consensus-based infer-
ence (either through a mixture or a product of proba-
bilities) is an old idea (Genest and Zidek, 1986). Re-
cent work in learning weights for the component ?ex-
pert? models has turned to cooperative techniques (Hin-
ton, 1999). Decoding that finds y (given x) to maximize
some weighted average of log-probabilities is known as
a logarithmic opinion pool (LOP). LOPs were applied
to CRFs (for named entity recognition and tagging) by
Smith et al (2005), with an eye toward regularization.
Their experts (each a CRF) contained overlapping feature
sets, and the combined model achieved much the same
effect as training a single model with smoothing. Note
that our models, unlike theirs, partition the feature space;
there is only one CRF, but some parameters are ignored
when estimating other parameters. We have not estimated
log-domain mixing coefficients?we weight all models?
contributions equally. Sutton and McCallum (2005) have
applied factored estimation to CRFs, motivated (like us)
by speed; they also describe how factored estimation
9Lemma-trigram and fine POS-unigram/lemma-bigram fea-
tures were eliminated to limit model size.
480
full morph. lemma POS OOV POS
accuracy accuracy accuracy accuracy
?2 376K 768K 376K 768K 376K 768K 376K 768K
channel only 61.4 60.3 85.1 84.2 88.5 87.2 17.8 16.4
most likely ~y 80.0 80.8 98.1 98.1 97.9 97.8 52.0 52.0
Hajic? et al HMM 88.8 89.2 97.9 97.9 95.8 95.8 52.0 52.0
+ OOV model 90.5 90.8 97.9 97.9 96.7 96.6 93.0 92.9
full ? 88.1 88.5 98.3 98.5 98.3 98.3 60.2 61.8
oracle given pruning 98.6 99.3 99.5 99.6 99.1 99.7 60.2 90.3
10 88.4 88.5 98.4 98.4 98.3 98.2 61.8 59.4
oracle given pruning 99.3 99.3 99.5 99.6 99.8 99.7 93.4 90.6
1 88.6 88.6 98.4 98.4 98.2 98.1 60.0 56.7
oracle given pruning 99.3 99.3 99.5 99.6 99.8 99.8 95.0 94.0
? POS ? 87.9 88.0? 98.2 98.2? 98.0 97.9? 55.7 51.7?
10 88.1 88.3? 98.2 98.3? 98.0 97.9? 55.4 51.6?
1 88.4 88.5? 98.2 98.2? 98.0 97.9? 55.0 51.9?
? tag-lemma ? 87.8 88.3 98.3 98.6 98.3 98.3 60.2 59.7
10 88.0 88.1 98.4 98.5 98.3 98.2 59.1 59.1
1 88.0 88.1 98.4 98.4 98.2 98.1 59.0 58.1
POS only ? 65.6? 65.5? 98.3 98.6 98.3 98.4 60.2 63.7
10 65.7? 65.5? 98.5 98.6 98.5 98.5 65.2 66.4
1 65.7? 65.5? 98.6 98.7 98.6 98.6 67.2 67.2
POS & ? 81.2 82.3 98.3 98.6 98.3 98.4 60.2 63.9
tag-lemma? 10 81.9 82.3 98.5 98.6 98.4 98.5 65.8 67.2
1 82.0 82.3 98.4 98.5 98.5 98.4 67.8 66.3
oracle given d(x) 99.8 99.8 99.5 99.6 99.9 99.9 100.0 100.0
Table 2: Czech disambiguation:
test-set (109K words) accuracy. A
word is marked correct only if its
entire morphological tag (or mor-
pheme or POS tag) was correctly
identified. Note that the full tag
is a complex, 15-field morphologi-
cal label, while ?POS? is a projec-
tion down to a tagset of size 60.
Lemma accuracy does not include
OOV words. ?The POS-only model
selects only POS, not full tags; these
measures are expected performance
if the full tag is selected randomly
from those in the dictionary that
match the selected POS. ?Required
more aggressive pruning. Bold
scores were significantly better than
the HMM of Hajic? et al (binomial
sign test, p < 0.05). Our models
were slightly but significantly worse
on full tagging, but showed signif-
icant improvements on recovering
POS tags and lemmas.
maximizes a lower bound on the unfactored objective.
Smith and Smith (2004) applied factored estimation to a
bilingual weighted grammar, driven by data limitations.
4.2 Experiments
Our corpus is the PDT (Hajic?, 1998), with up to 60% used
for training and 10% (109K words) used for test.10 The
morphological dictionary is the one packaged with the
PDT; it covers about 98% of the tokens in the corpus. The
remaining 2% have (unsurprisingly) a diverse set of 300?
400 distinct tags, depending on the training set size.11
Results are shown in Tab. 2. We compare to the HMM
of (Hajic? et al, 2001) without its OOV component.12 We
report morphological tagging accuracy on words; we also
report lemma accuracy (on non-OOV words), POS accu-
10We used less than the full corpus to keep training time
down; note that the training sets are nonetheless substantially
larger than in the Korean and Arabic experiments.
11During training, these project down to manageable num-
bers of hypotheses in the factored models. At test-time, how-
ever, Viterbi search is quite difficult when OOV symbols occur
consecutively. To handle this, we prune OOV arcs from the lat-
tices using the factored POS and inflectional models. For each
OOV, every model prunes a projection of the analysis (e.g., the
POS model prunes POS tags) until 90% of the posterior mass or
3 arcs remain (whichever is more conservative). Viterbi decod-
ing is run on a lattice containing OOV arcs consistent with the
pruned projected lattices.
12Results with the OOV component are also reported in Tab. 2,
but we cannot guarantee their experimental validity, since the
OOV component is pre-trained and may have been trained on
data in our test set.
racy on all words, and POS accuracy on OOV words. The
channel model (not shown) tended to have a small, harm-
ful effect on performance.
Without any explicit OOV treatment, our POS-only
component model significantly reduces lemma and POS
errors compared to Hajic? et al?s model. On recovering
full morphological tags, our full model is close in perfor-
mance to Hajic? et al, but still significantly worse. It is
likely that for many tasks, these performance gains are
more helpful than the loss on full tagging is harmful.
Why doesn?t our full model perform as well as Hajic? et
al.?s model? An error analysis reveals that our full model
(768K, ?2 = 1), compared to the HMM (768K) had 91%
as many number errors but 0.1% more gender and 31%
more case errors. Taking out those three models (?POS
& tag-lemma? in Fig. 2) is helpful on all measures ex-
cept full tagging accuracy, due in part to substantially
increased errors on gender (87% increase), case (54%),
and number (35%). The net effect of these components,
then, is helpful, but not quite helpful enough to match
a well-smoothed HMM on complex tagging. We com-
pared the models on the training set and found the same
pattern, demonstrating that this is not merely a matter of
over-fitting.
5 Future Work
Two clear ways to improve our models present them-
selves. The first is better OOV handling, perhaps through
an improved channel model. Possibilities include learn-
ing weights to go inside the FST-encoded dictionaries and
481
directly modeling spelling changes. The second is to turn
our factored model into a LOP. Training the mixture co-
efficients should be straightforward (if time-consuming)
with a development dataset.
A drawback of our system (especially for Czech) is
that some components (most notably, the Czech POS
model) take a great deal of time to train (up to two weeks
on 2GHz Pentium systems). Speed improvements are
expected to come from eliminating some of the over-
lapping feature templates, generalized speedups for log-
linear training, and perhaps further factoring.
6 Conclusion
We have explored morphological disambiguation of di-
verse languages using log-linear sequence models. Our
approach reduces error rates significantly on POS tag-
ging (Arabic and Czech), morpheme sequence recovery
(Korean and Arabic), and lemmatization (all three lan-
guages), compared to baseline state-of-the-art methods
For complex analysis tasks (e.g., Czech tagging), we have
demonstrated that factoring a large model into smaller
components can simplify training and achieve excel-
lent results. We conclude that a conditionally-estimated
source model informed by an existing morphological dic-
tionary (serving as an unweighted channel) is an effective
approach to morphological disambiguation.
References
K. R. Beesley and L. Karttunen. 2003. Finite State Morphol-
ogy. CSLI.
T. Buckwalter. 2004. Arabic morphological analyzer version
2.0. LDC2004L02.
J. Cha, G. Lee, and J.-H. Lee. 1998. Generalized unknown
morpheme guessing for hybrid POS tagging of Korean. In
Proc. of VLC.
K. Darwish. 2002. Building a shallow Arabic morphological
analyser in one day. In Proc. of ACL Workshop on Computa-
tional Approaches to Semitic Languages.
E. Daya, D. Roth, and S. Wintner. 2004. Learning Hebrew
roots: Machine learning with linguistic constraints. In Proc.
of EMNLP.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic tag-
ging of Arabic text: From raw text to base phrase chunks. In
Proc. of HLT-NAACL.
A. Freeman. 2001. Brill?s POS tagger and a morphology parser
for Arabic. In Proc. of ACL Workshop on Arabic Language
Processing.
C. Genest and J. V. Zidek. 1986. Combining probability distri-
butions: A critique and an annotated bibliography. Statistical
Science, 1:114?48.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-
of-speech tagging and morphological disambiguation in one
fell swoop. In Proc. of ACL.
J. Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petkevic?. 2001.
Serial combination of rules and statistics: A case study in
Czech tagging. In Proc. of ACL.
J. Hajic?. 1998. Building a syntactically annotated corpus:
The Prague Dependency Treebank. In Issues of Valency and
Meaning.
D. Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2000. Statistical
morphological disambiguation for agglutinative languages.
In Proc. of COLING.
C.-H. Han, N.-R. Han, E.-S. Ko, H. Yi, and M. Palmer. 2002.
Penn Korean Treebank: Development and evaluation. In
Proc. Pacific Asian Conf. Language and Comp.
N.-R. Han. 2004. Klex: Finite-state lexical transducer for Ko-
rean. LDC2004L01.
G. Hinton. 1999. Products of experts. In Proc. of ICANN.
J. Hlava?cova?. 2001. Morphological guesser of Czech words.
In Proc. of TSD.
F. Jelinek. 1976. Continuous speech recognition by statistical
methods. Proc. of the IEEE, 64(4):532?557.
R. M. Kaplan and M. Kay. 1981. Phonological rules and finite-
state transducers. Presented at Linguistic Society of Amer-
ica.
S. Khoja. 2001. APT: Arabic part-of-speech tagger. In Proc.
of NAACL Student Workshop.
G. Kiraz. 2000. Multitiered nonlinear morphology using mul-
titape finite automata: A case study on Syriac and Arabic.
Computational Linguistics, 26(1):77?105.
K. Koskenniemi. 1983. Two-level morphology: A general
computational model of word-form recognition and produc-
tion. Technical Report 11, University of Helsinki.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Applying
conditional random fields to Japanese morphological analy-
sis. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
M. Levinger, U. Ornan, and A. Itai. 1995. Learning morpho-
lexical probabilities from an untagged corpus with an appli-
cation to Hebrew. Computational Linguistics, 21(3):383?
404.
D. C. Liu and J. Nocedal. 1989. On the limited memory method
for large scale optimization. Mathematical Programming B,
45(3):503?28.
M. Maamouri, A. Bies, H. Jin, and T. Buckwalter. 2003. Arabic
Treebank part 1 version 2.0. LDC2003T06.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding consensus
among words: Lattice-based word error minimization. In
Proc. of ECSCT.
E. Mra?kova? and R. Sedlacek. 2003. From Czech morphol-
ogy through partial parsing to disambiguation. In Proc. of
CLITP.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random fields.
In Proc. of COLING.
V. Petkevic?. 2001. Grammatical agreement and automatic
morphological disambiguation of inflectional languages. In
Proc. of TSD.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of HLT-NAACL.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing with
factored estimation: Using English to parse Korean. In Proc.
of EMNLP.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opin-
ion pools for conditional random fields. In Proc. of ACL.
C. Sutton and A. McCallum. 2005. Cliquewise training for
undirected models. In Proc. of UAI.
482
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 423?430,
New York, June 2006. c?2006 Association for Computational Linguistics
A fast finite-state relaxation method for enforcing
global constraints on sequence decoding
Roy W. Tromble and Jason Eisner
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{royt,jason}@cs.jhu.edu
Abstract
We describe finite-state constraint relaxation, a method for ap-
plying global constraints, expressed as automata, to sequence
model decoding. We present algorithms for both hard con-
straints and binary soft constraints. On the CoNLL-2004 se-
mantic role labeling task, we report a speedup of at least 16x
over a previous method that used integer linear programming.
1 Introduction
Many tasks in natural language processing involve
sequence labeling. If one models long-distance or
global properties of labeled sequences, it can be-
come intractable to find (?decode?) the best labeling
of an unlabeled sequence.
Nonetheless, such global properties can improve
the accuracy of a model, so recent NLP papers
have considered practical techniques for decod-
ing with them. Such techniques include Gibbs
sampling (Finkel et al, 2005), a general-purpose
Monte Carlo method, and integer linear program-
ming (ILP), (Roth and Yih, 2005), a general-purpose
exact framework for NP-complete problems.
Under generative models such as hidden Markov
models, the probability of a labeled sequence de-
pends only on its local properties. The situation
improves with discriminatively trained models, such
as conditional random fields (Lafferty et al, 2001),
which do efficiently allow features that are functions
of the entire observation sequence. However, these
features can still only look locally at the label se-
quence. That is a significant shortcoming, because
in many domains, hard or soft global constraints on
the label sequence are motivated by common sense:
? For named entity recognition, a phrase that
appears multiple times should tend to get the
same label each time (Finkel et al, 2005).
? In bibliography entries (Peng and McCallum,
2004), a given field (author, title, etc.) should
be filled by at most one substring of the in-
put, and there are strong preferences on the co-
occurrence and order of certain fields.
? In seminar announcements, a given field
(speaker, start time, etc.) should appear with
at most one value in each announcement, al-
though the field and value may be repeated
(Finkel et al, 2005).
? For semantic role labeling, each argument
should be instantiated only once for a given
verb. There are several other constraints that
we will describe later (Roth and Yih, 2005).
A popular approximate technique is to hypothe-
size a list of possible answers by decoding without
any global constraints, and then rerank (or prune)
this n-best list using the full model with all con-
straints. Reranking relies on the local model being
?good enough? that the globally best answer appears
in its n-best list. Otherwise, reranking can?t find it.
In this paper, we propose ?constraint relaxation,?
a simple exact alternative to reranking. As in rerank-
ing, we start with a weighted lattice of hypotheses
proposed by the local model. But rather than restrict
to the n best of these according to the local model,
we aim to directly extract the one best according to
the global model. As in reranking, we hope that the
local constraints alone will work well, but if they do
not, the penalty is not incorrect decoding, but longer
runtime as we gradually fold the global constraints
into the lattice. Constraint relaxation can be used
whenever the global constraints can be expressed as
regular languages over the label sequence.
In the worst case, our runtime may be exponential
in the number of constraints, since we are consider-
ing an intractable class of problems. However, we
show that in practice, the method is quite effective
at rapid decoding under global hard constraints.
423
0O
1
?
O?
Figure 1: An automaton expressing the constraint that the label
sequence cannot be O?. Here ? matches any symbol except O.
The remainder of the paper is organized as fol-
lows: In ?2 we describe how finite-state automata
can be used to apply global constraints. We then
give a brute-force decoding algorithm (?3). In ?4,
we present a more efficient algorithm for the case of
hard constraints. We report results for the semantic
role labeling task in ?5. ?6 treats soft constraints.
2 Finite-state constraints
Previous approaches to global sequence labeling?
Gibbs sampling, ILP, and reranking?seem moti-
vated by the idea that standard sequence methods are
incapable of considering global constraints at all.
In fact, finite-state automata (FSAs) are powerful
enough to express many long-distance constraints.
Since all finite languages are regular, any constraint
over label sequences of bounded length is finite-
state. FSAs are more powerful than n-gram mod-
els. For example, the regular expression ??X??Y??
matches only sequences of labels that contain an X
before a Y. Similarly, the regular expression ?(O?)
requires at least one non-O label; it compiles into the
FSA of Figure 1.
Note that this FSA is in one or the other of its two
states according to whether it has encountered a non-
O label yet. In general, the current state of an FSA
records properties of the label sequence prefix read
so far. The FSA needs enough states to keep track of
whether the label sequence as a whole satisfies the
global constraint in question.
FSAs are a flexible approach to constraints be-
cause they are closed under logical operations such
as disjunction (union) and conjunction (intersec-
tion). They may be specified by regular expressions
(Karttunen et al, 1996), in a logical language (Vail-
lette, 2004), or directly as FSAs. They may also be
weighted to express soft constraints.
Formally, we pose the decoding problem in terms
of an observation sequence x ? X ? and possible la-
bel sequences y ? Y?. In many NLP tasks, X is the
set of words, and Y the tags. A lattice L: Y? 7? R
maps label sequences to weights, and is encoded as a
weighted FSA. Constraints are formally the same?
any function C: Y? 7? R is a constraint, includ-
ing weighted features from a classifier or probabilis-
tic model. In this paper we will consider only con-
straints that are weighted in particular ways.
Given a lattice L and constraints C, we seek
y?
def
= argmax
y
(
L(y) +
?
C?C
C(y)
)
. (1)
We assume the lattice L is generated by a model
M : X ? 7? (Y? 7? R). For a given observation se-
quence x, we put L = M(x). One possible model
is a finite-state transducer, where M(x) is an FSA
found by composing the transducer with x. Another
is a CRF, where M(x) is a lattice with sums of log-
potentials for arc weights.1
3 A brute-force finite-state decoder
To find the best constrained labeling in a lattice, y?,
according to (1), we could simply intersect the lat-
tice with all the constraints, then extract the best
path.
Weighted FSA intersection is a generalization of
ordinary unweighted FSA intersection (Mohri et al,
1996). It is customary in NLP to use the so-called
tropical semiring, where weights are represented by
their natural logarithms and summed rather than
multiplied. Then the intersected automaton L ? C
computes
(L ? C)(y)
def
= L(y) + C(y) (2)
To find y?, one would extract the best path in
L ? C1 ? C2 ? ? ? ? using the Viterbi algorithm, or
Dijkstra?s algorithm if the lattice is cyclic. This step
is fast if the intersected automaton is small.
The problem is that the multiple intersections in
L ? C1 ? C2 ? ? ? ? can quickly lead to an FSA with
an intractable number of states. The intersection
of two finite-state automata produces an automaton
1For example, if M is a simple linear-chain CRF, L(y) =Pn
i=1 f(yi?1, yi) + g(xi, yi). We build L = M(x) as an
acyclic FSA whose state set is Y ? {1, 2, . . . n}, with transi-
tions (y?, i? 1) ? (y, i) of weight f(y?, y) + g(xi, y).
424
with the cross product state set. That is, if F has m
states and G has n states, then F ?G has up to mn
states (fewer if some of the mn possible states do
not lie on any accepting path).
Intersection of many such constraints, even if they
have only a few states each, quickly leads to a com-
binatorial explosion. In the worst case, the size, in
states, of the resulting lattice is exponential in the
number of constraints. To deal with this, we present
a constraint relaxation algorithm.
4 Hard constraints
The simplest kind of constraint is the hard con-
straint. Hard constraints are necessarily binary?
either the labeling satisfies the constraint, or it vi-
olates it. Violation is fatal?the labeling produced
by decoding must satisfy each hard constraint.
Formally, a hard constraint is a mappingC: Y? 7?
{0,??}, encoded as an unweighted FSA. If a string
satisfies the constraint, recognition of the string will
lead to an accepting state. If it violates the con-
straint, recognition will end in a non-accepting state.
Here we give an algorithm for decoding with a set
of such constraints. Later (?6), we discuss the case
of binary soft constraints. In what follows, we will
assume that there is always at least one path in the
lattice that satisfies all of the constraints.
4.1 Decoding by constraint relaxation
Our decoding algorithm first relaxes the global con-
straints and solves a simpler problem. In particular,
we find the best labeling according to the model,
y?0
def
= argmax
y
L(y) (3)
ignoring all the constraints in C.
Next, we check whether y?0 satisifies the con-
straints. If so, then we are done?y?0 is also y?. If
not, then we reintroduce the constraints. However,
rather than include all at once, we introduce them
only as they are violated by successive solutions to
the relaxed problems: y?0, y?1, etc. We define
y?1
def
= argmax
y
(L(y) + C(y)) (4)
for some constraint C that y?0 violates. Similarly,
y?2 satisfies an additional constraint that y?1 violates,
HARD-CONSTRAIN-LATTICE(L, C):
1. y := Best-Path(L)
2. while ?C ? C such that C(y) = ??:
3. L := L ? C
4. C := C ? {C}
5. y := Best-Path(L)
6. return y
Figure 2: Hard constraints decoding algorithm.
and so on. Eventually, we find some k for which y?k
satisfies all constraints, and this path is returned.
To determine whether a labeling y satisfies a con-
straint C, we represent y as a straight-line automa-
ton and intersect with C, checking the result for non-
emptiness. This is equivalent to string recognition.
Our hope is that, although intractable in the worst
case, the constraint relaxation algorithm will operate
efficiently in practice. The success of traditional se-
quence models on NLP tasks suggests that, for nat-
ural language, much of the correct analysis can be
recovered from local features and constraints alone.
We suspect that, as a result, global constraints will
often be easy to satisfy.
Pseudocode for the algorithm appears in Figure 2.
Note that line 2 does not specify how to choose
C from among multiple violated constraints. This
is discussed in ?7. Our algorithm resembles the
method of Koskenniemi (1990) and later work. The
difference is that there lattices are unweighted and
may not contain a path that satisfies all constraints,
so that the order of constraint intersection matters.
5 Semantic role labeling
The semantic role labeling task (Carreras and
Ma`rques, 2004) involves choosing instantiations of
verb arguments from a sentence for a given verb.
The verb and its arguments form a proposition. We
use data from the CoNLL-2004 shared task?the
PropBank (Palmer et al, 2005) annotations of the
Penn Treebank (Marcus et al, 1993), with sections
15?18 as the training set and section 20 as the de-
velopment set. Unless otherwise specified, all mea-
surements are made on the development set.
We follow Roth and Yih (2005) exactly, in order
to compare system runtimes. They, in turn, follow
Hacioglu et al (2004) and others in labeling only
the heads of syntactic chunks rather than all words.
We label only the core arguments (A0?A5), treating
425
(a)
0?
1
A0
A0
2
?
?
(b) 0
1
O A0 A1 A2 A3 A4 A5
2
O
(verb p
osition
)A1A2A3A4A5 OA0
(c)
0OA0A1A2A3
Figure 4: Automata expressing NO DUPLICATE A0 (? matches
anything but A0), KNOWN VERB POSITION[2], and DISALLOW
ARGUMENTS[A4,A5].
adjuncts and references as O.
Figure 3 shows an example sentence from the
shared task. It is marked with an IOB phrase chunk-
ing, the heads of the phrases, and the correct seman-
tic role labeling. Heads are taken to be the rightmost
words of chunks. On average, there are 18.8 phrases
per proposition, vs. 23.5 words per sentence. Sen-
tences may contain multiple propositions. There are
4305 propositions in section 20.
5.1 Constraints
Roth and Yih use five global constraints on label se-
quences for the semantic role labeling task. We ex-
press these constraints as FSAs. The first two are
general, and the seven automata encoding them can
be constructed offline:
? NO DUPLICATE ARGUMENT LABELS
(Fig. 4(a)) requires that each verb have at
most one argument of each type in a given
sentence. We separate this into six individual
constraints, one for each core argument type.
Thus, we have constraints called NO DUPLI-
CATE A0, NO DUPLICATE A1, etc. Each of
these is represented as a three-state FSA.
? AT LEAST ONE ARGUMENT (Fig. 1) simply re-
quires that the label sequence is not O?. This is
a two-state automaton as described in ?2.
The last three constraints require information
about the example, and the automata must be con-
structed on a per-example basis:
? ARGUMENT CANDIDATES (Fig. 5) encodes a
set of position spans each of which must re-
ceive only a single label type. These spans were
proposed using a high-recall heuristic (Xue and
Palmer, 2004).
? KNOWN VERB POSITION (Fig. 4(b)) simply
encodes the position of the verb in question,
which must be labeled O.
? DISALLOW ARGUMENTS (Fig. 4(c)) specifies
argument types that are compatible with the
verb in question, according to PropBank.
5.2 Experiments
We implemented our hard constraint relaxation al-
gorithm, using the FSA toolkit (Kanthak and Ney,
2004) for finite-state operations. FSA is an open-
source C++ library providing a useful set of algo-
rithms on weighted finite-state acceptors and trans-
ducers. For each example we decoded, we chose a
random order in which to apply the constraints.
Lattices are generated from what amounts to a
unigram model?the voted perceptron classifier of
Roth and Yih. The features used are a subset of those
commonly applied to the task.
Our system produces output identical to that of
Roth and Yih. Table 1 shows F-measure on the core
arguments. Table 2 shows a runtime comparison.
The ILP runtime was provided by the authors (per-
sonal communication). Because the systems were
run under different conditions, the times are not di-
rectly comparable. However, constraint relaxation is
more than sixteen times faster than ILP despite run-
ning on a slower platform.
5.2.1 Comparison to an ILP solver
Roth and Yih?s linear program has two kinds of
numeric constraints. Some encode the shortest path
problem structure; the others encode the global con-
straints of ?5.1. The ILP solver works by relaxing
to a (real-valued) linear program, which may obtain
a fractional solution that represents a path mixture
instead of a path. It then uses branch-and-bound to
seek the optimal rounding of this fractional solution
to an integer solution (Gue?ret et al, 2002) that repre-
sents a single path satisfying the global constraints.
Our method avoids fractional solutions: a relaxed
solution is always a true single path, which either
426
Mr. Turner said the test will be shipped in 45 days to hospitals and clinical laboratories .
B-NP I-NP B-VP B-NP I-NP B-VP I-VP I-VP B-PP B-NP I-NP B-PP B-NP O B-NP I-NP O
Turner said test shipped in days to hospitals and laboratories .
A0 O A1 A1 A1 A1 A1 A1 A1 A1 O
Figure 3: Example sentence, with phrase tags and heads, and core argument labels. The A1 argument of ?said? is a long clause.
0
1
O A0 A1 A2 A3 A4 A5
2
A2 A3 A4 A5 O A0 A1
4
O
10
A0
16
A1
22
A2
28
A3
34
A4
40
A5
5
O
11
A0
17
A1
23
A2
29
A3
35
A4
41
A5
42
A5
43
A5
44
A5
45
A5
3
A5
46
O A0 A1 A2 A3 A4 A5
OA0A1A2A3A4A5
36
A4
37
A4
38
A4
39
A4
A4
30
A3
31
A3
32
A3
33
A3
A3
24
A2
25
A2
26
A2
27
A2
A2
18
A1
19
A1
20
A1
21
A1
A1
12
A0
13
A0
14
A0
15
A0
A0
6
O
7
O
8
O
9
O
O
Figure 5: An automaton expressing ARGUMENT CANDIDATES.
Argument Count F-measure
A0 2849 79.27
A1 4029 75.59
A2 943 55.68
A3 149 46.41
A4 147 81.82
A5 4 25.00
All 8121 74.51
Table 1: F-measure on core arguments.
satisfies or violates each global constraint. In effect,
we are using two kinds of domain knowledge. First,
we recognize that this is a graph problem, and insist
on true paths so we can use Viterbi decoding. Sec-
ond, we choose to relax only domain-specific con-
straints that are likely to be satisfied anyway (in our
domain), in contrast to the meta-constraint of inte-
grality relaxed by ILP. Thus it is cheaper on aver-
age for us to repair a relaxed solution. (Our repair
strategy?finite-state intersection in place of branch-
and-bound search?remains expensive in the worst
case, as the problem is NP-hard.)
5.2.2 Constraint violations
The y?0s, generated with only local information,
satisfy most of the global constraints most of the
time. Table 3 shows the violations by type.
The majority of best labelings according to the
local model don?t violate any global constraints?
a fact especially remarkable because there are no
label sequence features in Roth and Yih?s unigram
Constraint Violations Fraction
ARGUMENT CANDIDATES 1619 0.376
NO DUPLICATE A1 899 0.209
NO DUPLICATE A0 348 0.081
NO DUPLICATE A2 151 0.035
AT LEAST ONE ARGUMENT 108 0.025
DISALLOW ARGUMENTS 48 0.011
NO DUPLICATE A3 13 0.003
NO DUPLICATE A4 3 0.001
NO DUPLICATE A5 1 0.000
KNOWN VERB POSITION 0 0.000
Table 3: Violations of constraints by y?0 .
model. This confirms our intuition that natural lan-
guage structure is largely apparent locally. Table 4
shows the breakdown. The majority of examples are
very efficient to decode, because they don?t require
intersection of the lattice with any constraints?y?0
is extracted and is good enough. Those examples
where constraints are violated are still relatively effi-
cient because they only require a small number of in-
tersections. In total, the average number of intersec-
tions needed, even with the naive randomized con-
straint ordering, was only 0.65. The order doesn?t
matter very much, since 75% of examples have one
violation or fewer.
5.2.3 Effects on lattice size
Figure 6 shows the effect of intersection with vi-
olated constraints on the average size of lattices,
measured in arcs. The vertical bars at k = 0,
k = 1, . . . show the number of examples where con-
427
Method Total Time Per Example Platform
Brute Force Finite-State 37m25.290s 0.522s Pentium III, 1.0 GHz
ILP 11m39.220s 0.162s Xeon, 3.x GHz
Constraint Relaxation 39.700s 0.009s Pentium III, 1.0 GHz
Table 2: A comparison of runtimes for constrained decoding with ILP and FSA.
Violations Labelings Fraction Cumulative
0 2368 0.550 0.550
1 863 0.200 0.750
2 907 0.211 0.961
3 156 0.036 0.997
4 10 0.002 0.999
5 1 0.000 1.000
6?10 0 0.000 1.000
Table 4: Number of y?0 with each violation count.
 0
 500
 1000
 1500
 2000
 2500
 0  1  2  3  4  5
VerbsMean Arcs with RelaxationMean Arcs with Brute Force
Figure 6: Mean lattice size (measured in arcs) throughout de-
coding. Vertical bars show the number of examples over which
each mean is computed.
straint relaxation had to intersect k contraints (i.e.,
y? ? y?k). The trajectory ending at (for example)
k = 3 shows how the average lattice size for that
subset of examples evolved over the 3 intersections.
The X at k = 3 shows the final size of the brute-force
lattice on the same subset of examples.
For the most part, our lattices do stay much
smaller than those produced by the brute-force algo-
rithm. (The uppermost curve, k = 5, is an obvious
exception; however, that curve describes only the
seven hardest examples.) Note that plotting only the
final size of the brute-force lattice obscures the long
trajectory of its construction, which involves 10 in-
tersections and, like the trajectories shown, includes
larger intermediate automata.2 This explains the far
2The final brute-force lattice is especially shrunk by its in-
Constraint Violations Fraction
ARGUMENT CANDIDATES 90 0.0209
AT LEAST ONE ARGUMENT 27 0.0063
NO DUPLICATE A2 3 0.0007
NO DUPLICATE A0 2 0.0005
NO DUPLICATE A1 2 0.0005
NO DUPLICATE A3 1 0.0002
NO DUPLICATE A4 1 0.0002
Table 5: Violations of constraints by y?, measured over the de-
velopment set.
longer runtime of the brute-force method (Table 2).
Harder examples (corresponding to longer trajec-
tories) have larger lattices, on average. This is partly
just because it is disproportionately the longer sen-
tences that are hard: they have more opportunities
for a relaxed decoding to violate global constraints.
Hard examples are rare. The left three columns,
requiring only 0?2 intersections, constitute 96% of
examples. The vast majority can be decoded without
much more than doubling the local-lattice size.
6 Soft constraints
The gold standard labels y? occasionally violate the
hard global constraints that we are using. Counts
for the development set appear in Table 5. Counts
for violations of NO DUPLICATE A? do not include
discontinous arguments, of which there are 104 in-
stances, since we ignore them.
Because of the infrequency, the hard constraints
still help most of the time. However, on a small sub-
set of the examples, they preclude us from inferring
the correct labeling.
We can apply these constraints with weights,
rather than making them inviolable. This constitutes
a transition from hard to soft constraints. Formally,
a soft constraint C: Y? 7? R? is a mapping from a
label sequence to a non-positive penalty.
Soft constraints present new difficulty for decod-
clusion of, for example, DISALLOW ARGUMENTS, which can
only remove arcs. That constraint is rarely included in the re-
laxation lattices because it is rarely violated (see Table 3).
428
SOFT-CONSTRAIN-LATTICE(L, C):
1. (y?, Score(y?)) := (empty,??)
2. branches := [(L, C, 0)]
3. while (L, C, penalty) := Dequeue(branches):
4. L := Prune(L, Score(y?)? penalty)
5. unless Empty(L):
6. y := Best-Path(L)
7. for C ? C:
8. if C(y) < 0: (* so C(y) = wC *)
9. C := C ? {C}
10. Enqueue(branches, (L ? C, C, penalty))
11. penalty := penalty + C(y)
12. if Score(y?) < L(y) + penalty:
13. (y?, Score(y?)) := (y, L(y) + penalty)
14. return y?
Figure 7: Soft constraints decoding algorithm
ing, because instead of eliminating paths of L from
contention, they just reweight them.
In what follows, we consider only binary soft
constraints?they are either satisfied or violated, and
the same penalty is assessed whenever a violation
occurs. That is, ?C ? C,?wC < 0 such that
?y, C(y) ? {0, wC}.
6.1 Soft constraint relaxation
The decoding algorithm for soft constraints is a gen-
eralization of that for hard constraints. The differ-
ence is that, whereas with hard constraints a vio-
lation meant disqualification, here violation simply
means a penalty. We therefore must find and com-
pare two labelings: the best that satisfies the con-
straint, and the best that violates it.
We present a branch-and-bound algorithm
(Lawler and Wood, 1966), with pseudocode in
Figure 7. At line 9, we process and eliminate a
currently violated constraint C ? C by considering
two cases. On the first branch, we insist that C be
satisfied, enqueuing L ? C for later exploration. On
the second branch, we assume C is violated by all
paths, and so continue considering L unmodified,
but accept a penalty for doing so; we immediately
explore the second branch by returning to the start
of the for loop.3
Not every branch needs to be completely ex-
plored. Bounding is handled by the PRUNE func-
tion at line 4, which shrinks L by removing some
3It is possible that a future best path on the second branch
will not actually violate C, in which case we have overpenalized
it, but in that case we will also find it with correct penalty on the
first branch.
or all paths that cannot score better than Score(y?),
the score of the best path found on any branch so
far. Our experiments used almost the simplest possi-
ble PRUNE: replace L by the empty lattice if the best
path falls below the bound, else leave L unchanged.4
A similar bounding would be possible in the im-
plicit branches. If, during the for loop, we find that
the test at line 12 would fail, we can quit the for
loop and immediately move to the next branch in
the queue at line 3.
There are two factors in this algorithm that con-
tribute to avoiding consideration of all of the expo-
nential number of leaves corresponding to the power
set of constraints. First, bounding stops evaluation
of subtrees. Second, only violated constraints re-
quire branching. If a lattice?s best path satisifies a
constraint, then the best path that violates it can be
no better since, by assumption, ?y, C(y) ? 0.
6.2 Runtime experiments
Using the ten constraints from ?5.1, weighted
naively by their log odds of violation, the soft con-
straint relaxation algorithm runs in a time of 58.40
seconds. It is, as expected, slower than hard con-
straint relaxation, but only by a factor of about two.
As a side note, softening these particular con-
straints in this particular way did not improve de-
coding quality in this case. It might help to jointly
train the relative weights of these constraints and
the local model?e.g., using a perceptron algorithm
(Freund and Schapire, 1998), which repeatedly ex-
tracts the best global path (using our algorithm),
compares it to the gold standard, and adjusts the con-
straint weights. An obvious alternative is maximum-
entropy training, but the partition function would
have to be computed using the large brute-force lat-
tices, or else approximated by a sampling method.
7 Future work
For a given task, we may be able to obtain further
speedups by carefully choosing the order in which
to test and apply the constraints. We might treat this
as a reinforcement learning problem (Sutton, 1988),
4Partial pruning is also possible: by running the Viterbi ver-
sion of the forward-backward algorithm, one can discover for
each edge the weight of the best path on which it appears. One
can then remove all edges that do not appear on any sufficiently
good path.
429
where an agent will obtain rewards by finding y?
quickly. In the hard-constraint algorithm, for ex-
ample, the agent?s possible moves are to test some
constraint for violation by the current best path, or
to intersect some constraint with the current lattice.
Several features can help the agent choose the next
move. How large is the current lattice, which con-
straints does it already incorporate, and which re-
maining constraints are already known to be satis-
fied or violated by its best path? And what were the
answers to those questions at previous stages?
Our constraint relaxation method should be tested
on problems other than semantic role labeling. For
example, information extraction from bibliography
entries, as discussed in ?1, has about 13 fields to ex-
tract, and interesting hard and soft global constraints
on co-occurrence, order, and adjacency. The method
should also be evaluated on a task with longer se-
quences: though the finite-state operations we use
do scale up linearly with the sequence length, longer
sequences have more chance of violating a global
constraint somewhere in the sequence, requiring us
to apply that constraint explicitly.
8 Conclusion
Roth and Yih (2005) showed that global constraints
can improve the output of sequence labeling models
for semantic role labeling. In general, decoding un-
der such constraints is NP-complete. We exhibited
a practical approach, finite-state constraint relax-
ation, that greatly sped up decoding on this NLP task
by using familiar finite-state operations?weighted
FSA intersection and best-path extraction?rather
than integer linear programming.
We have also given a constraint relaxation algo-
rithm for binary soft constraints. This allows incor-
poration of constraints akin to reranking features, in
addition to inviolable constraints.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0347822. We thank Scott Yih for kindly providing
both the voted-perceptron classifier and runtime re-
sults for decoding with ILP, and the reviewers for
helpful comments.
References
Xavier Carreras and Llu??s Ma`rques. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In Proc.
of CoNLL, pp. 89?97.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of ACL, pp.
363?370.
Yoav Freund and Robert E. Schapire. 1998. Large margin clas-
sification using the perceptron algorithm. In Proc. of COLT,
pp. 209?217, New York. ACM Press.
Christelle Gue?ret, Christian Prins, and Marc Sevaux. 2002. Ap-
plications of optimization with Xpress-MP. Dash Optimiza-
tion. Translated and revised by Susanne Heipcke.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H. Mar-
tin, and Daniel Jurafsky. 2004. Semantic role labeling by
tagging syntactic chunks. In Proc. of CoNLL, pp. 110?113.
Stephan Kanthak and Hermann Ney. 2004. FSA: An efficient
and flexible C++ toolkit for finite state automata using on-
demand computation. In Proc. of ACL, pp. 510?517.
Lauri Karttunen, Jean-Pierre Chanod, Gregory Grefenstette,
and Anne Schiller. 1996. Regular expressions for lan-
guage engineering. Journal of Natural Language Engineer-
ing, 2(4):305?328.
Kimmo Koskenniemi. 1990. Finite-state parsing and disam-
biguation. In Proc. of COLING, pp. 229?232.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML, pp.
282?289.
Eugene L. Lawler and David E. Wood. 1966. Branch-and-
bound methods: A survey. Operations Research, 14(4):699?
719.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics,
19:313?330.
Mehryar Mohri, Fernando Pereira, and Michael Riley. 1996.
Weighted automata in text and speech processing. In A. Ko-
rnai, editor, Proc. of the ECAI 96 Workshop, pp. 46?50.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Fuchun Peng and Andrew McCallum. 2004. Accurate informa-
tion extraction from research papers using conditional ran-
dom fields. In Proc. of HLT-NAACL, pp. 329?336.
Dan Roth and Wen-tau Yih. 2005. Integer linear programming
inference for conditional random fields. In Proc. of ICML,
pp. 737?744.
Richard S. Sutton. 1988. Learning to predict by the methods of
temporal differences. Machine Learning, 3(1):9?44.
Nathan Vaillette. 2004. Logical Specification of Finite-State
Transductions for Natural Language Processing. Ph.D. the-
sis, Ohio State University.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proc. of EMNLP, pp. 88?94.
430
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1007?1016,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Learning Linear Ordering Problems for Better Translation
?
Roy Tromble
Google, Inc.
4720 Forbes Ave.
Pittsburgh, PA 15213
royt@google.com
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
jason@cs.jhu.edu
Abstract
We apply machine learning to the Lin-
ear Ordering Problem in order to learn
sentence-specific reordering models for
machine translation. We demonstrate that
even when these models are used as a mere
preprocessing step for German-English
translation, they significantly outperform
Moses? integrated lexicalized reordering
model.
Our models are trained on automatically
aligned bitext. Their form is simple but
novel. They assess, based on features of
the input sentence, how strongly each pair
of input word tokens w
i
, w
j
would like
to reverse their relative order. Combining
all these pairwise preferences to find the
best global reordering is NP-hard. How-
ever, we present a non-trivial O(n
3
) al-
gorithm, based on chart parsing, that at
least finds the best reordering within a cer-
tain exponentially large neighborhood. We
show how to iterate this reordering process
within a local search algorithm, which we
use in training.
1 Introduction
Machine translation is an important but difficult
problem. One of the properties that makes it dif-
ficult is the fact that different languages express
the same concepts in different orders. A ma-
chine translation system must therefore rearrange
the source language concepts to produce a fluent
translation in the target language.
1
This work is excerpted and adapted from the first au-
thor?s Ph.D. thesis (Tromble, 2009). Some of the ideas here
appeared in (Eisner and Tromble, 2006) without empirical
validation. The material is based in part upon work sup-
ported by the National Science Foundation under Grant No.
0347822.
Phrase-based translation systems rely heavily
on the target language model to ensure a fluent
output order. However, a target n-gram language
model alone is known to be inadequate. Thus,
translation systems should also look at how the
source sentence prefers to reorder. Yet past sys-
tems have traditionally used rather weak models of
the reordering process. They may look only at the
distance between neighboring phrases, or depend
only on phrase unigrams. The decoders also rely
on search error, in the form of limited reordering
windows, for both efficiency and translation qual-
ity.
Demonstrating the inadequacy of such ap-
proaches, Al-Onaizan and Papineni (2006)
showed that even given the words in the reference
translation, and their alignment to the source
words, a decoder of this sort charged with merely
rearranging them into the correct target-language
order could achieve a BLEU score (Papineni et
al., 2002) of at best 69%?and that only when
restricted to keep most words very close to their
source positions.
This paper introduces a more sophisticated
model of reordering based on the Linear Order-
ing Problem (LOP), itself an NP-hard permutation
problem. We apply machine learning, in the form
of a modified perceptron algorithm, to learn pa-
rameters of a linear model that constructs a matrix
of weights from each source language sentence.
We train the parameters on orderings derived from
automatic word alignments of parallel sentences.
The LOP model of reordering is a complete
ordering model, capable of assigning a different
score to every possible permutation of the source-
language sentence. Unlike the target language
model, it uses information about the relative posi-
tions of the words in the source language, as well
as the source words themselves and their parts of
speech and contexts. It is therefore a language-pair
specific model.
1007
We apply the learned LOP model as a prepro-
cessing step before both training and evaluation of
a phrase-based translation system, namely Moses.
Our methods for finding a good reordering un-
der the NP-hard LOP are themselves of interest,
adapting algorithms from natural language parsing
and developing novel dynamic programs.
Our results demonstrate a significant improve-
ment over translation using unreordered German.
Using Moses with only distance-based reordering
and a distortion limit of 6, our preprocessing im-
proves BLEU from 25.27 to 26.40. Furthermore,
that improvement is significantly greater than the
improvement Moses achieves with its lexicalized
reordering model, 25.55.
Collins et al (2005) improved German-English
translation using a statistical parser and several
hand-written rules for preprocessing the German
sentences. This paper presents a similar improve-
ment using fully automatic methods.
2 A Linear Ordering Model
This section introduces a model of word reorder-
ing for machine translation based on the Linear
Ordering Problem.
2.1 Formalization
The input sentence is w = w
1
w
2
. . . w
n
. To dis-
tinguish duplicate tokens of the same word, we as-
sume that each token is superscripted by its input
position, e.g., w = die
1
Katze
2
hat
3
die
4
Frau
5
gekauft
6
(gloss: ?the cat has the woman bought?).
For a fixedw, a permutation pi = pi
1
pi
2
. . . pi
n
is
any reordering of the tokens in w. The set ?
n
of
all such permutations has size n!. We would like to
define a scoring model that assigns a high score to
the permutationpi = die
4
Frau
5
hat
3
gekauft
6
die
1
Katze
2
(gloss: ?the woman has bought the cat?),
since that corresponds well to the desired English
order.
To construct a function that scores permutations
of w, we first construct a pairwise preference ma-
trix B
w
? R
n?n
, whose entries are
B
w
[`, r]
def
= ? ? ?(w, `, r), (1)
Here ? is a vector of weights. ? is a vector of
feature functions, each considering the entire word
sequencew, as well as any functions thereof, such
as part of speech tags.
We will hereafter abbreviate B
w
as B. Its inte-
ger indices ` and r are identified with the input to-
kensw
`
andw
r
, and it can be helpful to write them
that way; e.g., we will sometimes write B[2, 5] as
B[Katze
2
,Frau
5
].
The idea behind our reordering model is
that B[Katze
2
,Frau
5
] > B[Katze
5
,Frau
2
] ex-
presses a preference to keep Katze
2
before Frau
5
,
whereas the opposite inequality would express a
preference?other things equal?for permutations
in which their order is reversed. Thus, we define
1
score(pi)
def
=
?
i,j: 1?i<j?n
B[pi
i
, pi
j
] (2)
p(pi)
def
=
1
Z
exp(? ? score(pi)) (3)
?
pi
def
= argmax
pi??
n
score(pi) (4)
Note that i and j denote positions in pi, whereas
pi
i
, pi
j
, `, and r denote particular input tokens such
as Katze
2
and Frau
5
.
2.2 Discussion
To the extent that the costs B generally discour-
age reordering, they will particularly discourage
long-distance movement, as it swaps more pairs
of words.
We point out that our model is somewhat pecu-
liar, since it does not directly consider whether the
permutation pi keeps die
4
and Frau
5
adjacent or
even close together, but only whether their order
is reversed.
Of course, the model could be extended to con-
sider adjacency, or more generally, the three-way
cost of interposing k between i and j. See (Eis-
ner and Tromble, 2006; Tromble, 2009) for such
extensions and associated algorithms.
However, in the present paper we focus on the
model in the simple form (2) that only considers
pairwise reordering costs for all pairs in the sen-
tence. Our goal is to show that these unfamiliar
pairwise reordering costs are useful, when mod-
eled with a rich feature set via equation (1). Even
in isolation (as a preprocessing step), without con-
sidering any other kinds of reordering costs or lan-
guage model, they can achieve useful reorderings
1
For any ` < r, we may assume without loss of gener-
ality that B[r, `] = 0, since if not, subtracting B[r, `] from
bothB[`, r] andB[r, `] (exactly one of which appears in each
score(pi)) will merely reduce the scores of all permutations
by this amount, leaving equations (3) and (4) unchanged.
Thus, in practice, we take B to be an upper triangular ma-
trix. We use equation (1) only to defineB[`, r] for ` < r, and
train ? accordingly. However, we will ignore this point in our
exposition.
1008
of German that complement existing techniques
and thus improve state-of-the-art systems. Our
positive results in even this situation suggest that
in future, pairwise reordering costs should proba-
bly be integrated into MT systems.
The probabilistic interpretation (3) of the
score (2) may be useful when thus integrating our
model with language models or other reordering
models during translation, or simply when train-
ing our model to maximize likelihood or minimize
expected error. However, in the present paper we
will stick to purely discriminative training and de-
coding methods that simply try to maximize (2).
2.3 The Linear Ordering Problem
In the combinatorial optimization literature, the
maximization problem (4) (with inputB) is known
as the Linear Ordering Problem. It has numer-
ous practical applications in fields including eco-
nomics, sociology, graph theory, graph drawing,
archaeology, and task scheduling (Gr?otschel et
al., 1984). Computational studies on real data
have often used ?input-output? matrices represent-
ing resource flows among economic sectors (Schi-
avinotto and St?utzle, 2004).
Unfortunately, the problem is NP-hard. Further-
more, it is known to be APX-complete, meaning
that there is no polynomial time approximation
scheme unless P=NP (Mishra and Sikdar, 2004).
However, there are various heuristic procedures
for approximating it (Tromble, 2009). We now
give an attractive, novel procedure, which uses a
CKY-parsing-like algorithm to search various sub-
sets of ?
n
in polynomial time.
3 Local Search
?Local search? refers to any hill-climbing proce-
dure that iteratively improves a solution by mak-
ing an optimal ?local? change at each iteration.
2
In this case, we start with the identity permutation,
find a ?nearby? permutation with a better score (2),
and repeat until we have reached a local maximum
of the scoring objective.
This section describes a local search procedure
that uses a very generous definition of ?local.? At
each iteration, it finds the optimal permutation in
a certain exponentially large neighborhood N(pi)
of the current permutation pi.
2
One can introduce randomness to obtain MCMC sam-
pling or simulated annealing algorithms. Our algorithms ex-
tend naturally to allow this (cf. Tromble (2009)).
S ? S
0,n
S
i,k
? S
i,j
S
j,k
S
i?1,i
? pi
i
Figure 1: A grammar for a large neighborhood of
permutations, given one permutation pi of length
n. The S
i,k
rules are instantiated for each 0 ?
i < j < k ? n, and the S
i?1,i
rules for each
0 < i ? n.
We say that two permutations are neighbors iff
they can be aligned by an Inversion Transduction
Grammar (ITG) (Wu, 1997), which is a familiar
reordering device in machine translation. Equiva-
lently, pi
?
? N(pi) iff pi can be transformed into
pi
?
by swapping various adjacent substrings of pi,
as long as these swaps are properly nested. Zens
and Ney (2003) used a normal form to show that
the size of the ITG neighborhood N(pi) is a large
Schr?oder number, which grows exponentially in
n. Asymptotically, the ratio between the size of
the neighborhood for n + 1 and the size for n ap-
proaches 3 + 2
?
2 ? 5.8.
We show that equation (2) can be optimized
within N(pi) in O(n
3
) time, using dynamic pro-
gramming. The algorithm is based on CKY pars-
ing. However, a novelty is that the grammar
weights must themselves be computed by O(n
3
)
dynamic programming.
Our grammar is shown in Figure 1. Parsing
the ?input sentence? pi with this grammar simply
constructs all binary trees that yield the string pi.
There is essentially only one nonterminal, S, but
we split it into O(n
2
) position-specific nontermi-
nals such as S
i,j
, which can only yield the span
pi
i+1
pi
i+2
. . . pi
j
. An example parse is shown in
Figure 2.
The important point is that we will place a
score on each binary grammar rule. The score
of the rule S
i,k
? S
i,j
S
j,k
is max(0,?
i,j,k
),
where ?
i,j,k
is the benefit to swapping the sub-
strings pi
i+1
pi
i+2
. . . pi
j
and pi
j+1
pi
j+2
. . . pi
k
. The
rule is considered to be a ?swap rule? if its
score is positive, showing that a swap will be
beneficial (independent of the rest of the tree).
If the parse in Figure 2 is the parse with
the highest total score, and its swap rules are
S
0,5
? S
0,1
S
1,5
and S
3,5
? S
3,4
S
4,5
, then
our best permutation in the neighborhood of pi
must be the (linguistically desirable) permutation
die
4
Frau
5
hat
3
gekauft
6
die
1
Katze
2
, obtained from
1009
SS
0,6






H
H
H
H
H
H
S
0,5




H
H
H
H
S
0,1
die
1
S
1,5




H
H
H
H
S
1,3


H
H
S
1,2
die
4
S
2,3
Frau
5
S
3,5


H
H
S
3,4
gekauft
6
S
4,5
hat
3
S
5,6
Katze
2
Figure 2: One parse of the current permutation pi.
In this example, pi has somehow gotten the input
words into alphabetical order (owing to previous
hill-climbing steps). We are now trying to further
improve this order.
pi by two swaps.
How do we find this solution? Clearly
the benefit (positive or negative) to swapping
pi
i+1
pi
i+2
. . . pi
j
with pi
j+1
pi
j+2
. . . pi
k
is
?
i,j,k
=
j
?
`=i+1
k
?
r=j+1
B[pi
r
, pi
`
]?B[pi
`
, pi
r
] (5)
We can evaluate all O(n
3
) possible swaps in to-
tal time O(n
3
), using the dynamic programming
recurrence
?
i,j,k
= ?
i,j,k?1
+ ?
i+1,j,k
??
i+1,j,k?1
(6)
+B[pi
k
, pi
i+1
]?B[pi
i+1
, pi
k
]
with the base case ?
i,j,k
= 0 if i = j or j = k.
This gives us the weights for the grammar rules,
and then we can use weighted CKY parsing to
find the highest-scoring (Viterbi) parse in O(n
3
)
time. Extracting our new and improved permuta-
tion pi
?
? N(pi) from this parse is a simple O(n)-
time algorithm.
Figure 3 gives pseudocode for our local search
algorithm, showing how to compute the quan-
tities (6) during parsing rather than beforehand.
?[i, k] holds the weight of the best permuta-
tion (in the neighborhood) of the subsequence
pi
i+1
pi
i+1
. . . pi
k
.
3
3
The use of ? is intended to suggest an analogy to inside
probability?or more precisely, the Viterbi approximation to
inside probability (since we are maximizing rather than sum-
ming over parses).
The next two sections describe how to use our
local search algorithm to discriminatively learn the
weights of the parameters from Section 2, equa-
tion (1).
4 Features
Our objective function (2) works only to the extent
that we can derive a good pairwise preference ma-
trix B
w
. We do this by using a rich feature set in
equation (1).
We adapt the features of McDonald et al
(2005), introduced there for dependency parsing,
to the task of machine translation reordering. Be-
cause both models construct features for pairs of
words given the entire sentence, there is a close
correspondence between the two tasks, although
the output is quite different.
Each feature ?(w, `, r) in equation (1) is a bi-
nary feature that fires when (w, `, r) has some
conjunction of properties. The properties that are
considered include the words w
`
and w
r
, the parts
of speech of {w
`?1
, . . . , w
r+1
}, and the distance
r ? `. Table 1 shows the feature templates.
We also tried features based on a dependency
parse of the German, with the idea of using LOP
features to reorder the dependents of each word,
and thus model syntactic movement. This did
produce better monolingual reorderings (as in Ta-
ble 2), but it did not help final translation into En-
glish (Table 3), so we do not report the details here.
5 Learning to Reorder
Ideally, we would have a large corpus of desir-
able reorderings of source sentences?in our case,
German sentences permuted into target English
word order?from which to train the parameters of
our model. Unfortunately, the alignments between
German and English sentences are only infre-
quently one-to-one. Furthermore, human-aligned
parallel sentences are hard to come by, and never
in the quantity we would like.
Instead, we make do with automatically-
generated word alignments, and we heuristi-
cally derive an English-like word order for
the German sentence based on the alignment.
We used GIZA++ (Och and Ney, 2003) to
align approximately 751,000 sentences from the
German-English portion of the Europarl corpus
(Koehn, 2005), in both the German-to-English and
English-to-German directions. We combined the
1010
1: procedure LOCALSEARCHSTEP(B,pi, n)
2: for i? 0 to n? 1 do
3: ?[i, i+ 1]? 0
4: for k ? i+ 1 to n do
5: ?[i, i, k]? ?[i, k, k]? 0
6: end for
7: end for
8: for w ? 2 to n do
9: for i? 0 to n? w do
10: k ? i+ w
11: ?[i, k]? ??
12: for j ? i+ 1 to k ? 1 do
13: ?[i, j, k]? ?[i, j, k ? 1] + ?[i+ 1, j, k]??[i+ 1, j, k ? 1] +B[pi
k
, pi
i+1
]?B[pi
i+1
, pi
k
]
14: ?[i, k]? max(?[i, k], ?[i, j] + ?[j, k] + max(0, ?[i, j, k]))
15: end for
16: end for
17: end for
18: return ?[0, n]
19: end procedure
Figure 3: Pseudocode for computing the score of the best permutation in the neighborhood of pi under
the Linear Ordering Problem specified by the matrix B. Computing the best neighbor is a simple matter
of keeping back pointers to the choices of max and ordering them as implied.
alignments using the ?grow-diag-final-and? proce-
dure provided with Moses (Koehn et al, 2007).
For each of these German sentences, we derived
the English-like reordering of it, which we call
German
?
, by the following procedure. Each Ger-
man token was assigned an integer key, namely
the position of the leftmost of the English tokens
to which it was aligned, or 0 if it was not aligned
to any English tokens. We then did a stable sort of
the German tokens based on these keys, meaning
that if two German tokens had the same key, their
order was preserved.
This is similar to the oracle ordering used by
Al-Onaizan and Papineni (2006), but differs in the
handling of unaligned words. They kept unaligned
words with the closest preceding aligned word.
4
Having found the German
?
corresponding to
each German sentence, we randomly divided
the sentences into 2,000 each for development
and evaluation, and the remaining approximately
747,000 for training.
We used the averaged perceptron algorithm
(Freund and Schapire, 1998; Collins, 2002) to
train the parameters of the model. We ran the al-
gorithm multiple times over the training sentences,
4
We tried two other methods for deriving English word
order from word alignments. The first alternative was to
align only in one direction, from English to German, with
null alignments disallowed, so that every German word was
aligned to a single English word. The second alternative
used BerkeleyAligner (Liang et al, 2006; DeNero and Klein,
2007), which shares information between the two alignment
directions to improve alignment quality. Neither alternative
produced improvements in our ultimate translation quality.
measuring the quality of the learned parameters by
reordering the held-out development set after each
iteration. We stopped when the BLEU score on
the development set failed to improve for two con-
secutive iterations, which occurred after fourteen
passes over the data.
Each perceptron update should compare the true
German
?
to the German
?
that would be predicted
by the model (2). As the latter is NP-hard to find,
we instead substitute the local maximum found by
local search as described in Section 3, starting at
the identity permutation, which corresponds to the
original German word order.
During training, we iterate the local search as
described earlier. However, for decoding, we only
do a single step of local search, thus restricting re-
orderings to the ITG neighborhood of the origi-
nal German. This restriction turns out to improve
performance slightly, even though it reduces the
quality of our approximation to the LOP prob-
lem (4). In other words, it turns out that reorder-
ings found outside the ITG neighborhood tend to
be poor German
?
even if our LOP-based objective
function thinks that they are good German
?
.
This is not to say that the gold standard German
?
is always in the ITG neighborhood of the original
German?often it is not. Thus, it might be bet-
ter in future work to still allow the local search to
take more than one step, but to penalize the second
step. In effect, score(pi) would then include a fea-
ture indicating whether pi is in the neighborhood
of the original German.
1011
t`?1
w
`
t
`
t
`+1
t
b
t
r?1
w
r
t
r
t
r+1
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ?
? ?
? ?
? ?
?
?
?
?
? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ? ?
Table 1: Feature templates forB[`, r] (w
`
is the `th
word, t
`
its part of speech tag, and b matches any
index such that ` < b < r). Each of the above
is also conjoined with the distance between the
words, r ? `, to form an additional feature tem-
plate. Distances are binned into 1, 2, 3, 4, 5, > 5,
and > 10.
The model is initialized at the start of train-
ing using log-odds of the parameters. Let ?
m
=
{(w, `, r) | ?
m
(w, `, r) = 1} be the set of word
pairs in the training data for which feature m fires.
Let
?
?
m
be the subset of ?
m
for which the words
stay in order, and
?
?
m
the subset for which the
words reverse order. Then in this model,
?
m
= log
(
?
?
?
?
?
m
?
?
?
+
1
2
)
?log
(
?
?
?
?
?
m
?
?
?
+
1
2
)
. (7)
This model is equivalent to smoothed na??ve Bayes
if converted to probabilities. The learned model
significantly outperforms it on the monolingual re-
ordering task.
Table 2 compares the model after perceptron
training to the model at the start of training,
measuring BLEU score of the predicted German
?
against the observed German
?
. In addition to these
BLEU scores, we can measure precision and re-
call of pairs of reordered words against the ob-
Ordering p
2
p
3
p
4
BLEU
German 57.4 38.3 27.7 49.65
Log-odds 57.4 38.4 27.8 49.75
Perceptron 58.6 40.3 29.8 51.51
Table 2: Monolingual BLEU score on develop-
ment data, measured against the ?true? German
?
ordering that was derived from automatic align-
ments to known English translations. The table
evaluates three candidate orderings: the original
German, German reordered using the log-odds
initialized model, and German reordered using
the perceptron-learned model. In addition to the
BLEU score, the table shows bigram, trigram, and
4-gram precisions. The unigram precisions are al-
ways 100%, because the correct words are given.
served German
?
. On the held out test set, the pre-
dicted German
?
achieves a recall of only 21%, but
a precision of 64%. Thus, the learned model is
too conservative, but makes moderately good de-
cisions when it does reorder.
6 Reordering as Preprocessing
This section describes experiments using the
model introduced in Section 2 and learned in Sec-
tion 5 to preprocess German sentences for trans-
lation into English. These experiments are similar
to those of Collins et al (2005).
We used the model learned in Section 5 to gen-
erate a German
?
ordering of the training, develop-
ment, and test sets. The training sentences are the
same that the model was trained on, and the devel-
opment set is the same that was used as the stop-
ping criterion for the perceptron. The test set was
unused in training.
We used the resulting German
?
as the input to
the Moses training pipeline. That is, Moses re-
computed alignments of the German
?
training data
to the English sentences using GIZA++, then con-
structed a phrase table. Moses used the develop-
ment data for minimum error-rate training (Och,
2003) of its small number of parameters. Finally,
Moses translated the test sentences, and we mea-
sured performance against the English reference
sentences. This is the standard Moses pipeline, ex-
cept German has been replaced by German
?
.
Table 3 shows the results of translation, both
starting with unreordered German, and starting
with German
?
, reordered using the learned Linear
Ordering Problems. Note that Moses may itself re-
1012
System Input Moses Reord. p
1
p
2
p
3
p
4
BLEU METEOR TER
baseline German Distance 59.6 31.4 18.8 11.6 25.27 54.03 60.60
(a) German Lexical 60.0 32.0 19.3 12.1 25.55 54.18 59.76
(b) German
?
Distance 60.4 32.7 20.2 12.8 26.40 54.91 58.63
(a)+(b) German
?
Lexical 59.9 32.4 20.0 12.8 26.44 54.61 59.23
Table 3: Machine translation performance of several systems, measured against a single English refer-
ence translation. The results vary both the preprocessing?either none, or reordered using the learned
Linear Ordering Problems?and the reordering model used in Moses. Performance is measured using
BLEU, METEOR (Lavie et al, 2004), and TER (Snover et al, 2006). (For TER, smaller values are
better.)
order whatever input that it receives, during trans-
lation into English. Thus, the results in the table
also vary the reordering model used in Moses, set
to either a single-parameter distance-based model,
or to the lexicalized bidirectional msd model. The
latter model has six parameters for each phrase
in the phrase table, corresponding to monotone,
swapped, or discontinuous ordering relative to the
previous phrase in either the source or target lan-
guage.
How should we understand the results? The
baseline system is Moses phrase-based translation
with no preprocessing and only a simple distance-
based reordering model. There are two ways to
improve this: (a) ask Moses to use the lexicalized
bidirectional msd reordering model that is pro-
vided with Moses and is integrated with the rest of
translation, or (b) keep the simple distance-based
model within Moses, but preprocess its training
and test data with our linear reordering model.
Note that the preprocessing in (b) will obviously
change the phrasal substrings that are learned by
Moses, for better or for worse.
First, remarkably, (b) is significantly better than
(a) on BLEU, with p < 0.0001 according to a
paired permutation test.
Second, combining (a) with (b) produced no im-
provement over (b) in BLEU score (the difference
between 26.40 and 26.44 is not significant, even
at p < 0.2, according to the same paired per-
mutation test). Lexicalized reordering in Moses
even degraded translation performance according
to METEOR and TER. The TER change is sig-
nificant according to the paired permutation test at
p < 0.001. (We did not perform a significance test
for METEOR.)
Our word-based model surpasses the lexical-
ized reordering in Moses largely because of long-
distance movement. The 518 sentences (26%) in
ll
llll
l
l
ll
l
lll
l
lll
ll
ll
lll
lllllllll
llllllllll l l ll
llllll
l
l
ll
l
l
ll
l
lll
ll
ll
lll
lllllllll
lllllllll
l l l ll
0 10 20 30 40 50
?
0.00
20
.000
0.00
2
0.00
4
0.00
6
0.00
8
0.01
0
Word Pairs Reordered
Cum
ulat
ive 
BLE
U C
han
ge
BLEU Improvement Aggregated by Amount of Reordering
vs. baseline
vs. (a)
Figure 4: Cumulative change in BLEU score of
(b) relative to the baseline and (a), aggregated by
the number of reordered word pairs in each sen-
tence. For those sentences where our model re-
orders fewer than five word pairs, the BLEU score
of translation degrades.
the test set for which our model moves a word
more than six words away from its starting posi-
tion account for more than 67% of the improve-
ment in BLEU from (a) to (b).
Figure 4 shows another view of the BLEU im-
provement. It shows that, compared to the base-
line, our preprocessing has basically no effect for
sentences where it does only a little reordering,
changing the relative order of fewer than five pairs
of words. Compared to Moses with lexicalized re-
ordering, these same sentences actually hurt per-
formance. This more than accounts for the differ-
ence between the BLEU scores of (b) and (a)+(b).
Going beyond preprocessing, our model could
also be integrated into a phrase-based decoder. We
briefly sketch that possibility here.
1013
Phrase-based decoders keep a source coverage
vector with every partial translation hypothesis.
That coverage vector allows us to incorporate the
scores from a LOP matrix B directly. Whenever
the decoder extends the hypothesis with a new
source phrase, covering w
i+1
w
i+2
. . . w
j
, it adds
j?1
?
`=i+1
j
?
r=`+1
B[`, r] +
j
?
`=i+1
?
r?U
B[`, r].
The first term represents the phrase-internal score,
and the second the score of putting the words in the
phrase before all the remaining uncovered words
U .
7 Comparison to Prior Work
Preprocessing the source language to improve
translation is a common technique. Xia and Mc-
Cord (2004) improved English-French translation
using syntactic rewrite rules derived from Slot
Grammar parses. Collins et al (2005) reported
an improvement from 25.2% to 26.8% BLEU
on German-English translation using six hand-
written rules to reorder the German sentences
based on automatically-generated phrase-structure
trees. Our work differs from these approaches in
providing an explicit model that scores all pos-
sible reorderings. In this paper, our model was
trained and used only for 1-best preprocessing, but
it could potentially be integrated into decoding as
well, where it would work together with the trans-
lation model and target language model to find a
congenial translation.
Costa-juss`a and Fonollosa (2006) improved
Spanish-English and Chinese-English translation
using a two-step process, first reordering the
source language, then translating it, both using dif-
ferent versions of a phrase-based translation sys-
tem. Many others have proposed more explicit
reordering models (Tillmann, 2004; Kumar and
Byrne, 2005; Koehn et al, 2005; Al-Onaizan and
Papineni, 2006). The primary advantage of our
model is that it directly accounts for interactions
between distant words, leading to better treatment
of long-distance movement.
Xiong et al (2006) proposed a constituent
reordering model for a bracketing transduction
grammar (BTG) (Wu, 1995), which predicts the
probability that a pair of subconstituents will re-
order when combined to form a new constituent.
The features of their model look only at the first
source and target word of each constituent, mak-
ing it something like a sparse version of our model.
However, because of the target word features, their
reordering model cannot be separated from their
translation model.
8 Conclusions and Future Work
We have presented an entirely new model of re-
ordering for statistical machine translation, based
on the Linear Ordering Problem, and shown that
it can substantially improve translation from Ger-
man to English.
The model is demonstrably useful in this pre-
processing setting?which means that it can be
very simply added as a preprocessing step to any
MT system. German-to-English is a particularly
attractive use case, because the word orders are
sufficiently different as to require a good reorder-
ing model that requires long-distance reordering.
Our preprocessing here gave us a BLEU gain
of 0.9 point over the best Moses-based result.
English-to-German would obviously be another
potential win, as would translating between En-
glish and Japanese, for example.
As mentioned in Section 6, our model could
also be integrated into a phrase-based, or a syntax-
based decoder. That possibility remains future
work, but it is likely to lead to further improve-
ments, because it allows the translation system to
consider multiple possible reorderings under the
model, as well as to tune the weight of the model
relative to the other parts of the system during
MERT.
Tromble (2009) covers this integration in more
detail, and proposes several other ways of integrat-
ing our reordering model into machine translation.
It also experiments with numerous other param-
eter estimation procedures, including some that
use the probabilistic interpretation of our model
from (3). It presents numerous additional neigh-
borhoods for search in the Linear Ordering Prob-
lem.
We mentioned several possible extensions to the
model, such as going beyond the scoring model
of equation (2), or considering syntax-based fea-
tures. Another extension would try to reorder not
words but phrases, following (Xiong et al, 2006),
or segment choice models (Kuhn et al, 2006),
which assume a single segmentation of the words
into phrases. We would have to define the pair-
wise preference matrix B over phrases rather than
1014
words (Eisner and Tromble, 2006). This would
have the disadvantage of complicating the feature
space, but might be a better fit for integration with
a phrase-based decoder.
Finally, we gave a novel algorithm for ap-
proximately solving the Linear Ordering Prob-
lem, interestingly combining dynamic program-
ming with local search. Another novel contri-
bution is that we showed how to parameterize a
function that constructs a specific Linear Order-
ing Problem instance from an input sentence w,
and showed how to learn those parameters from
a corpus of parallel sentences, using the percep-
tron algorithm. Likelihood-based training using
equation (3) would also be possible, with modifi-
cations to our algorithm, notably the use of normal
forms to avoid counting some permutations multi-
ple times (Tromble, 2009).
It would be interesting to compare the speed
and accuracy of our dynamic-programming local-
search method with an exact algorithm for solving
the LOP, such as integer linear programming with
branch and bound (cf. Charon and Hudry (2006)).
Exact solutions can generally be found in practice
for n ? 100.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
COLING-ACL, pages 529?536, Sydney, July.
Ir`ene Charon and Olivier Hudry. 2006. A branch-and-
bound algorithm to solve the linear ordering problem
for weighted tournaments. Discrete Applied Mathe-
matics, 154(15):2097?2116, October.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In ACL, pages 531?540, Ann Arbor,
Michigan, June.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8, Philadelphia, July.
Marta R. Costa-juss`a and Jos?e A. R. Fonollosa. 2006.
Statistical machine reordering. In EMNLP, pages
70?76, Sydney, July.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL,
pages 17?24, Prague, June.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal per-
mutations in machine translation. In Workshop on
computationally hard problems and joint inference
in speech and language processing, New York, June.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In COLT, pages 209?217, New York. ACM Press.
Martin Gr?otschel, Michael J?unger, and Gerhard
Reinelt. 1984. A cutting plane algorithm for
the linear ordering problem. Operations Research,
32(6):1195?1220, November?December.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In IWSLT, Pittsburgh, October.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In ACL Demo and Poster Sessions, pages 177?
180, Prague, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit
X, pages 79?86, Phuket, Thailand, September.
Roland Kuhn, Denis Yuen, Michel Simard, Patrick
Paul, George Foster, Eric Joanis, and Howard John-
son. 2006. Segment choice models: Feature-rich
models for global distortion in statistical machine
translation. In HLT-NAACL, pages 25?32, New
York, June.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In HLT-EMNLP, pages 161?168, Van-
couver, October.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The signicance of recall in automatic
metrics for MT evaluation. In Robert E. Frederking
and Kathryn B. Taylor, editors, Machine Transla-
tion: From Real Users to Research, pages 134?143.
AMTA, Springer, September?October.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL, pages 104?
111, New York, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Spanning tree methods for discrim-
inative training of dependency parsers. Technical
Report MS-CIS-05-11, UPenn CIS.
Sounaka Mishra and Kripasindhu Sikdar. 2004. On
approximability of linear ordering and related NP-
optimization problems on graphs. Discrete Applied
Mathematics, 136(2?3):249?269, February.
1015
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167, Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311?318, Philadelphia, July.
Tommaso Schiavinotto and Thomas St?utzle. 2004.
The linear ordering problem: Instances, search
space analysis and algorithms. Journal of Math-
ematical Modelling and Algorithms, 3(4):367?402,
December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In HLT-
NAACL Short Papers, pages 101?104, Boston, May.
Roy Wesley Tromble. 2009. Search and Learning for
the Linear Ordering Problem with an Application
to Machine Translation. Ph.D. thesis, Johns Hop-
kins University, Baltimore, April. http://nlp.
cs.jhu.edu/
?
royt/
Dekai Wu. 1995. An algorithm for simultaneously
bracketing parallel texts by aligning words. In ACL,
pages 244?251, Cambridge, Massachusetts, June.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Septem-
ber.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING, pages 508?514,
Geneva, August.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In COLING-ACL,
pages 521?528, Sydney, July.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL, pages 144?151, Sapporo, July.
1016
