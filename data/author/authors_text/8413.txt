Integrating Cross-Lingually Relevant News Articles and
Monolingual Web Documents in Bilingual Lexicon Acquisition
Takehito Utsuro? and Kohei Hino? and Mitsuhiro Kida?
Seiichi Nakagawa? and Satoshi Sato?
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606-8501, Japan
?Department of Information and Computer Sciences, Toyohashi University of Technology
Tenpaku-cho, Toyohashi, 441?8580, Japan
Abstract
In the framework of bilingual lexicon acquisition
from cross-lingually relevant news articles on the
Web, it is relatively harder to reliably estimate bilin-
gual term correspondences for low frequency terms.
Considering such a situation, this paper proposes to
complementarily use much larger monolingual Web
documents collected by search engines, as a resource
for reliably re-estimating bilingual term correspon-
dences. We experimentally show that, using a suf-
ficient number of monolingual Web documents, it
is quite possible to have reliable estimate of bilin-
gual term correspondences for those low frequency
terms.
1 Introduction
Translation knowledge acquisition from paral-
lel/comparative corpora is one of the most im-
portant research topics of corpus-based MT.
This is because it is necessary for an MT sys-
tem to (semi-)automatically increase its trans-
lation knowledge in order for it to be used in
the real world situation. One limitation of
the corpus-based translation knowledge acquisi-
tion approach is that the techniques of transla-
tion knowledge acquisition heavily rely on avail-
ability of parallel/comparative corpora. How-
ever, the sizes as well as the domain of existing
parallel/comparative corpora are limited, while
it is very expensive to manually collect paral-
lel/comparative corpora. Therefore, it is quite
important to overcome this resource scarcity
bottleneck in corpus-based translation knowl-
edge acquisition research.
In order to solve this problem, we proposed
an approach of taking bilingual news articles
on Web news sites as a source for translation
knowledge acquisition (Utsuro et al, 2003). In
the case of Web news sites in Japan, Japanese
as well as English news articles are updated ev-
eryday. Although most of those bilingual news
articles are not parallel even if they are from
the same site, certain portion of those bilingual
news articles share their contents or at least re-
port quite relevant topics. This characteristic
is quite important for the purpose of transla-
tion knowledge acquisition. Utsuro et al (2003)
showed that it is possible to acquire translation
knowledge of domain specific named entities,
event expressions, and collocational expressions
from the collection of bilingual news articles on
Web news sites.
Based on the results of our previous study,
this paper further examines the correlation of
term frequency and the reliability of bilingual
term correspondences estimated from bilingual
news articles. We show that, for high frequency
terms, it is relatively easier to reliably estimate
bilingual term correspondences. However, for
low frequency terms, it is relatively harder to re-
liably estimate bilingual term correspondences.
Low frequency problem of this type often hap-
pens when a sufficient number of bilingual news
articles are not available at hand.
Considering such a situation, this paper then
proposes to complementarily use much larger
monolingual Web documents collected by search
engines, as a resource for reliably re-estimating
bilingual term correspondences. Those col-
lected monolingual Web documents are re-
garded as comparable corpora. Here, a stan-
dard technique of estimating bilingual term cor-
respondences from comparable corpora is em-
ployed. In the evaluation, we show that, using
a sufficient number of monolingual Web docu-
ments, it is relatively easier to have reliable esti-
mate of bilingual term correspondences. As one
of the most remarkable experimental evalua-
tion results, we further show that, for the terms
which appear infrequently in news articles, the
accuracy of re-estimating bilingual term corre-
spondences does actually improve.
Figure 1: Translation Knowledge Acquisition
from Web News Sites: Overview
2 Estimating Bilingual Term
Correspondences from
Cross-Lingually Relevant News
Articles
2.1 Overview
Figure 1 illustrates the overview of our frame-
work of translation knowledge acquisition from
Web news sites. First, pairs of Japanese and
English news articles which report identical con-
tents or at least closely related contents are re-
trieved. In this cross-lingual retrieval process,
translation knowledge such as a bilingual dic-
tionary and an MT software is used for mea-
suring similarity of Japanese and English arti-
cles across languages. Then, by applying pre-
viously studied techniques of translation knowl-
edge acquisition from parallel/comparative cor-
pora, translation knowledge such as bilingual
term correspondences are acquired.
2.2 Cross-Language Retrieval of Rel-
evant News Articles
This section gives the overview of our frame-
work of cross-language retrieval of relevant news
articles from Web news sites (Utsuro et al,
2003). First, from Web news sites, both
Japanese and English news articles within cer-
tain range of dates are retrieved. Let dJ and
dE denote one of the retrieved Japanese and
English articles, respectively. Then, each En-
glish article dE is translated into a Japanese
document dMTJ by some commercial MT soft-
ware1. Each Japanese article dJ as well as the
Japanese translation dMTJ of each English ar-
ticle are next segmented into word sequences,
and word frequency vectors v(dJ ) and v(dMTJ )
are generated. Then, cosine similarities between
v(dJ ) and v(dMTJ ) are calculated
2 and pairs of
articles dJ and dE which satisfy certain criterion
are considered as candidates for cross-lingually
relevant article pairs.
As we describe in section 4.1, on Web news
sites in Japan, the number of articles up-
dated per day is far greater (about 4 times)
in Japanese than in English. Thus, it is
much easier to find cross-lingually relevant ar-
ticles for each English query article than for
each Japanese query article. Considering this
fact, we estimate bilingual term correspon-
dences from the results of cross-lingually re-
trieving relevant Japanese articles with English
query articles. For each English query article
diE and its Japanese translation d
MTi
J , the set
DiJ of Japanese articles that are within certain
range of dates and are with cosine similarities
higher than or equal to a certain lower bound
Ld is constructed:
DiJ =
{
dJ | cos(v(dMTiJ ), v(dJ )) ? Ld
}
(1)
2.3 Estimating Bilingual Term Cor-
respondences with Pseudo-
Parallel Corpus
This section describes the technique we apply to
the task of estimating bilingual term correspon-
dences from cross-lingually relevant news texts.
Here, we regard cross-lingually relevant news
texts as a pseudo-parallel corpus, to which stan-
dard techniques of estimating bilingual term
correspondences from parallel corpora can be
applied3.
1In this query translation process, we compared an
MT software with a bilingual lexicon. CLIR with query
translation by an MT software performed much better
than that by a bilingual lexicon. In the case of news
articles on Web news sites, it is relatively easier to find
articles in the other language which report closely related
contents, with just a few days difference of report dates.
In such a case, exact query translation by an MT soft-
ware is suitable, because exact translation is expected to
easily match the closely related articles in the other lan-
guage. As we mention in section 3.3, this is opposite to
the situation of monolingual Web documents, where it is
much less expected to find closely related documents in
the other language.
2It is also quite possible to employ weights other than
word frequencies such as tf ?idf and similarity measures
other than cosine measure such as dice or Jaccard coef-
ficients.
3We also applied another technique based on con-
textual vector similarities (Utsuro et al, 2003), which
First, we concatenate constituent Japanese
articles of DiJ into one article D
?i
J , and regard
the article pair diE and D
?i
J as a pseudo-parallel
sentence pair. Next, we collect such pseudo-
parallel sentence pairs and construct a pseudo-
parallel corpus PPCEJ of English and Japanese
articles:
PPCEJ =
{
?diE , D
?i
J ? | D
i
J = ?
}
Then, we apply standard techniques of es-
timating bilingual term correspondences from
parallel corpora (Matsumoto and Utsuro, 2000)
to this pseudo-parallel corpus PPCEJ . First,
from a pseudo-parallel sentence pair diE and D
?i
J ,
we extract monolingual (possibly compound4)
term pair tE and tJ :
r?tE , tJ ? s.t. ?diE?dJ , tE in d
i
E , tJ in dJ , (2)
cos(v(dMTiJ ), v(dJ )) ? Ld
Then, based on the contingency table of co-
occurrence document frequencies of tE and tJ
below, we estimate bilingual term correspon-
dences according to the statistical measures
such as the mutual information, the ?2 statistic,
the dice coefficient, and the log-likelihood ratio.
tJ ?tJ
tE df(tE , tJ ) = a df(tE ,?tJ ) = b
?tE df(?tE , tJ) = c df(?tE ,?tJ) = d
We compare the performance of those four
measures, where the ?2 statistic and the log-
likelihood ratio perform best, the dice coefficient
the second best, and the mutual information the
worst. In section 4.3, we show results with the
?2 statistic as the bilingual term correspondence
corrEJ(tE , tJ):
?2(tE , tJ) =
(ad ? bc)2
(a + b)(a + c)(b + d)(c + d)
3 Re-estimating Bilingual Term
Correspondences using
Monolingual Web Documents
3.1 Overview
This section illustrates the overview of the pro-
cess of re-estimating bilingual term correspon-
dences using monolingual Web documents col-
lected by search engines. Figure 2 gives its
rough idea.
has been well studied in the context of bilingual lexicon
acquisition from comparable corpora. In this method,
we regard cross-lingually relevant texts as a compara-
ble corpus, where bilingual term correspondences are es-
timated in terms of contextual similarities across lan-
guages. This technique is less effective than the one we
describe here (Utsuro et al, 2003).
4In the evaluation of this paper, we restrict English
and Japanese terms t
E
and t
J
to be up to five words
long.
Figure 2: Re-estimating Bilingual Term Corre-
spondences using Monolingual Web Documents:
Overview
Suppose that we have an English term, and
that the problem to solve here is to find its
Japanese translation. As we described in the
previous section and in Figure 1, with a cross-
lingually relevant Japanese and English news
articles database, we can have a certain num-
ber of Japanese translation candidates for the
target English term. Here, for high frequency
terms, it is relatively easier to have reliable
ranking of those Japanese translation candi-
dates. However, for low frequency terms, hav-
ing reliable ranking of those Japanese transla-
tion candidates is difficult. Especially, low fre-
quency problem of this type often happens when
we do not have large enough language resources
(in this case, cross-lingually relevant news arti-
cles).
Considering such a situation, re-estimation of
bilingual term correspondences proceeds as fol-
lows, using much larger monolingual Web doc-
uments sets that are easily accessible through
search engines. First, English pages which
contain the target English term are collected
through an English search engine. In the simi-
lar way, for each Japanese term in the Japanese
translation candidates, Japanese pages which
contain the Japanese term are collected through
a Japanese search engine. Then, texts con-
tained in those English and Japanese pages are
extracted and are regarded as comparable cor-
pora. Here, a standard technique of estimat-
ing bilingual term correspondences from com-
parable corpora (e.g., Fung and Yee (1998) and
Rapp (1999)) is employed. Contextual sim-
ilarity between the target English term and
the Japanese translation candidate is measured
across languages, and all the Japanese transla-
tion candidates are re-ranked according to the
contextual similarities.
3.2 Filtering by Hits of Search En-
gines
Before re-estimating bilingual term correspon-
dences using monolingual Web documents, we
assume there exists certain correlation between
hits of the English term tE and the Japanese
term tJ returned by search engines. Depending
on the hits h(tE) of tE , we restrict the hits h(tJ )
of tJ to be within the range of a lower bound
hL and an upper bound hU :
hL < h(tJ ) ? hU
As search engines, we used AltaVista
(http://www. altavista.com/ for En-
glish, and goo (http://www.goo.ne.jp/) for
Japanese. With a development data set con-
sisting of translation pairs of an English term
and a Japanese term, we manually constructed
the following rules for determining the lower
bound hL and the upper bound hU :
1. 0 < h(tE) ? 100
hL = 0, hU = 10, 000 ? h(tE)
2. 100 < h(tE) ? 20, 000
hL = 0.05 ? h(tE), hU = 1, 000, 000
3. 20, 000 < h(tE)
hL = 1, 000, hU = 50 ? h(tE)
In the experimental evaluation of Section 4.4,
the initial set of Japanese translation candi-
dates consists of 50 terms for each English term,
which are then reduced to on the average 24.8
terms with this filtering.
3.3 Re-estimating Bilingual Term
Correspondences based on Con-
textual Similarity
This section describes how to re-estimate bilin-
gual term correspondences using monolingual
Web documents collected by search engines.
For an English term tE and a Japanese term
tJ , let D(tE) and D(tJ) be the sets of docu-
ments returned by search engines with queries
tE and tJ , respectively. Then, for the English
term tE, translated contextual vector cvtrJ (tE)
is constructed as below: each English sen-
tence sE which contains tE is translated into
Japanese sentence strJ , then the term frequency
vectors5 v(strJ ) of Japanese translation s
tr
J are
5In the term frequency vectores, compound terms are
restricted to be up to five words long both for English
and Japanese.
Table 1: Statistics of # of Days, Articles, and
Article Sizes
total total average # average
# of # of of articles article
days articles per day size (bytes)
Eng 935 23064 24.7 3228.9
Jap 941 96688 102.8 837.7
summed up into the translated contextual vec-
tor cvtrJ(tE):
cvtrJ (tE) =
?
?s
E
in D(t
E
) s.t. t
E
in s
E
v(strJ )
The contextual vector cv(tJ ) for the Japanese
term tJ is also constructed by summing up the
term frequency vectors v(sJ) of each Japanese
sentence sJ which contains tJ :
cv(tJ ) =
?
?s
J
in D(t
J
) s.t. t
J
in s
J
v(sJ)
In the translation of English sentences into
Japanese, we evaluated an MT software and a
bilingual lexicon in terms of the performance of
re-estimation of bilingual term correspondences.
Unlike the situation of cross-lingually relevant
news articles mentioned in Section 2.2, trans-
lation by a bilingual lexicon is more effective
for monolingual Web documents. In the case of
monolingual Web documents, it is much less ex-
pected to find closely related documents in the
other language. In such cases, multiple trans-
lation rather than exact translation by an MT
software is suitable. In Section 4.4, we show
evaluation results with translation by a bilin-
gual lexicon6.
Finally, bilingual term correspondence
corrEJ(tE , tJ) is estimated in terms of co-
sine measure cos(cvtrJ (tE), cv(tJ )) between
contextual vectors cvtrJ (tE) and cv(tJ ).
4 Experimental Evaluation
4.1 Japanese-English Relevant News
Articles on Web News Sites
We collected Japanese and English news articles
from a Web news site. Table 1 shows the total
number of collected articles and the range of
dates of those articles represented as the num-
ber of days. Table 1 also shows the number of
articles updated in one day, and the average ar-
ticle size. The number of Japanese articles up-
dated in one day are far greater (about 4 times)
than that of English articles.
6Eijiro Ver.37, 850,000 entries, http://homepage3.
nifty.com/edp/.
Table 2: # of Japanese/English Articles Pairs with Similarity Values above Lower Bounds
Lower Bound Ld of Articles? Sim w/o 0.3 0.4 0.5
Difference of Dates (days) CLIR ? 2
# of English Articles 23064 6073 2392 701
# of Japanese Articles 96688 12367 3444 882
# of English-Japanese Article Pairs ? 16507 3840 918
Next, for several lower bounds Ld of the
similarity between English and Japanese arti-
cles, Table 2 shows the numbers of English and
Japanese articles as well as article pairs which
satisfy the similarity lower bound. Here, the
difference of dates of English and Japanese arti-
cles is within two days, with which it is guaran-
teed that, if exist, closely related articles in the
other language can be discovered (see Utsuro et
al. (2003) for details). Note that it can happen
that one article has similarity values above the
lower bound against more than one articles in
the other language.
According to our previous study (Utsuro et
al., 2003), cross-lingually relevant news arti-
cles are available in the direction of English-
to-Japanese retrieval for more than half of the
retrieval query English articles. Furthermore,
with the similarity lower bound Ld = 0.3, pre-
cision and recall of cross-language retrieval are
around 30% and 60%, respectively. Therefore,
with the similarity lower bound Ld = 0.3, at
least 1,800 (? 6, 073?0.5?0.6) English articles
have relevant Japanese articles in the results of
cross-language retrieval. Based on this analysis,
the next section gives evaluation results with
the similarity lower bound Ld = 0.3.
4.2 English Term List for Evaluation
For the evaluation of this paper, we first man-
ually select target English terms and their
reference Japanese translation, and examine
whether reference bilingual term correspon-
dences can be estimated by the methods pre-
sented in Sections 2 and 3. Target English terms
are selected by the following procedure.
First, from the whole English articles of Ta-
ble 1, any sequence of more than one words
whose frequency is more than or equal to 10 is
enumerated. This enumeration is easily imple-
mented and efficiently computed by employing
the technique of PrefixSpan (Pei et al, 2001).
Here, certain portion of those word sequences
are appropriate as compound terms, while the
rest are some fragments of a compound term,
or concatenation of those fragments. In or-
der to automatically select candidates for cor-
rect compound terms, we parse those word se-
Figure 3: Accuracy of Estimating Bilingual
Term Correspondences with News Articles
quences by Charniak parser7, and collect noun
phrases which consist of adjectives, nouns, and
present/past participles. For each of those word
sequences, the ?2 statistic against Japanese
translation candidates is calculated, then those
word sequences are sorted in descending order of
their ?2 statistic. Finally, among top 3,000 can-
didates for compound terms, 100 English com-
pound terms are randomly selected for the eval-
uation of this paper. Selected 100 terms satisfy
the following condition: those English terms can
be correctly translated neither by the MT soft-
ware used in Section 2.2, nor by the bilingual
lexicon used in Section 3.3.
4.3 Estimating Bilingual Term Cor-
respondences with News Articles
For the 100 English terms selected in the pre-
vious section, Japanese translation candidates
which satisfy the condition of the formula (2) in
Section 2.3 are collected, and are ranked accord-
ing to the ?2 statistic. Figure 3 plots the rate
of reference Japanese translation being within
top n candidates. In the figure, the plot labeled
as ?full? is the result with the whole articles in
Table 1. In this case, the accuracy of the top
ranked Japanese translation candidate is about
40%, and the rate of reference Japanese trans-
lation within top five candidates is about 75%.
7http://www.cs.brown.edu/people/ec/
Table 3: Statistics of Average Document Frequencies and Number of Days
Document Frequencies of target English Term # of Days
Data Set df(tE) df(tE, tJ) Eng Jap
freq=10, 13.6 days 14.9 9.1 13.6 21.9
freq=10, 20 days 14.9 9.1 21.0 78.7
freq=10, 200 days 14.9 9.1 200 581
freq=70, 600 days 37.4 24.9 600 872
full 53.9 35.6 935 941
On the other hand, other plots labeled as
?Freq=x, y days? are the results when the num-
ber of the news articles is reduced, which are
simulations for estimating bilingual term cor-
respondences for low frequency terms. Here,
the label ?Freq=x, y days? indicates that news
articles used for ?2 statistic estimation is re-
stricted to certain portion of the whole news
articles so that the following condition be satis-
fied: i) co-occurrence document frequency of a
target English term and its reference Japanese
translation is fixed to be x,8 ii) the number of
days be greater than or equal to y. For each
news articles data set, Table 3 shows document
frequencies df(tE) of a target English term tE ,
co-occurrence document frequencies df(tE, tJ )
of tE and its reference Japanese translation tJ ,
and the numbers of days for English as well as
Japanese articles. Those numbers are all aver-
aged over the 100 English terms. The number of
days for Japanese articles could be at maximum
five times larger than that for English articles,
because relevant Japanese articles are retrieved
against a query English article from the dates of
differences within two days (details are in Sec-
tions 2.2 and 4.1).
As can be seen from the plots of Figure 3,
the smaller the news articles data set, the lower
the plot is. Especially, in the case of the small-
est news articles data set, it is clear that re-
liable ranking of Japanese translation candi-
dates is difficult. This is because it is not easy
to discriminate the reference Japanese transla-
tion and the other candidates with statistics ob-
tained from such a small news articles data set.
4.4 Re-estimating Bilingual Term
Correspondences with Monolin-
gual Web Documents
For the 100 target English terms evaluated in
the previous section, this section describes the
result of applying the technique presented in
Section 3.3, i.e., re-estimating bilingual term
8When the co-occurrence document frequency of t
E
and t
J
in the whole news articles is less than x, all the
co-occurring dates are included.
Figure 4: Accuracy of Re-estimating Bilingual
Term Correspondences with Monolingual Web
Documents
correspondences with monolingual Web docu-
ments. For each of the 100 target English
terms, bilingual term correspondences are re-
estimated against candidates of Japanese trans-
lation ranked within top 50 according to the
?2 statistic. Here, as a simulation for terms
that are infrequent in news articles, 50 can-
didate terms for Japanese translation are col-
lected from the smallest data set labeled as
?Freq=10, 13.6 days?. As mentioned in Sec-
tion 3.2, those 50 candidates are reduced to on
the average 24.8 terms with the filtering by hits
of search engines. For each of an English term
tE and a Japanese term tJ , 100 monolingual
documents are collected by search engines9 10.
Figure 4 compares the plots of re-estimation
with monolingual Web documents and estima-
tion by news articles (data set ?Freq=10, 13.6
9In the result of our preliminary evaluation, accuracy
of re-estimating bilingual term correspondences did not
improve even if more than 100 documents were used.
10Alternatively, as the monolingual documents from
which contextual vectors are constructed, we evaluated
each of the short passages listed in the summary pages
returned by search engines, instead of the whole docu-
ments of the URLs listed in the summary pages. The
difference of the performance of bilingual term corre-
spondence estimation is little, while the computational
cost can reduced to almost 5%.
days?). It is clear from this result that mono-
lingual Web documents contribute to improving
the accuracy of estimating bilingual term corre-
spondences for low frequency terms.
One of the major reasons for this improve-
ment is that topics of monolingual Web doc-
uments collected through search engines are
much more diverse than those of news articles.
Such diverse topics help discriminate correct
and incorrect Japanese translation candidates.
For example, suppose that the target English
term tE is ?special anti-terrorism law? and its
reference Japanese translation is ???????
????. In the news articles we used for evalua-
tion, most articles in which tE or tJ appear have
?dispatch of Self-Defense Force for reconstruc-
tion of Iraq? as their topics. Here, Japanese
translation candidates other than ??????
????? that are highly ranked according to
the ?2 statistic are: e.g., ????? (dissolution
of the House of Representatives)? and ?????
??? (assistance for reconstruction of Iraq)?,
which frequently appear in the topic of ?dis-
patch of Self-Defense Force for reconstruction
of Iraq?.
On the other hand, in the case of monolin-
gual Web documents collected through search
engines, it can be expected that topics of docu-
ments may vary according to the query terms.
In the case of the example above, the major
topic is ?dispatch of Self-Defense Force for re-
construction of Iraq? for both of reference terms
tE and tJ , while major topics for other Japanese
translation candidates are: ?issues on Japanese
Diet? for ????? (dissolution of the House
of Representatives)? and ?issues on reconstruc-
tion of Iraq, not only in Japan, but all over the
world? for ???????? (assistance for re-
construction of Iraq)?. Those topics of incor-
rect Japanese translation candidates are differ-
ent from that of the target English term tE, and
their contextual vector similarities against the
target English term tE are relatively low com-
pared with the reference Japanese translation
tJ . Consequently, the reference Japanese trans-
lation tJ is re-ranked higher compared with the
ranking based on news articles.
5 Related Works
In large scale experimental evaluation of bilin-
gual term correspondence estimation from com-
parable corpora, it is difficult to estimate bilin-
gual term correspondences against every possi-
ble pair of terms due to its computational com-
plexity. Previous works on bilingual term cor-
respondence estimation from comparable cor-
pora controlled experimental evaluation in var-
ious ways in order to reduce this computational
complexity. For example, Rapp (1999) filtered
out bilingual term pairs with low monolingual
frequencies (those below 100 times), while Fung
and Yee (1998) restricted candidate bilingual
term pairs to be pairs of the most frequent 118
unknown words. Cao and Li (2002) restricted
candidate bilingual compound term pairs by
consulting a seed bilingual lexicon and requir-
ing their constituent words to be translation
of each other across languages. On the other
hand, in the framework of bilingual term corre-
spondences estimation of this paper, the compu-
tational complexity of enumerating translation
candidates can be easily avoided with the help of
cross-language retrieval of relevant news texts.
Furthermore, unlike Cao and Li (2002), bilin-
gual term correspondences for compound terms
are not restricted to compositional translation.
6 Conclusion
In the framework of bilingual lexicon acquisition
from cross-lingually relevant news articles on
the Web, it has been relatively harder to reliably
estimate bilingual term correspondences for low
frequency terms. This paper proposed to com-
plementarily use much larger monolingual Web
documents collected by search engines, as a re-
source for reliably re-estimating bilingual term
correspondences. We showed that, for the terms
which appear infrequently in news articles, the
accuracy of re-estimating bilingual term corre-
spondences actually improved.
References
Y. Cao and H. Li. 2002. Base noun phrase translation
using Web data and the EM algorithm. In Proc. 19th
COLING, pages 127?133.
P. Fung and L. Y. Yee. 1998. An IR approach for trans-
lating new words from nonparallel, comparable texts.
In Proc. 17th COLING and 36th ACL, pages 414?420.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge
acquisition. In R. Dale, H. Moisl, and H. Somers,
editors, Handbook of Natural Language Processing,
chapter 24, pages 563?610. Marcel Dekker Inc.
J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. 2001.
Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. Inter. Conf.
Data Mining, pages 215?224.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proc. 37th ACL, pages 519?526.
T. Utsuro, T. Horiuchi, T. Hamamoto, K. Hino, and
T. Nakayama. 2003. Effect of cross-language IR in
bilingual lexicon acquisition from comparable cor-
pora. In Proc. 10th EACL, pages 355?362.
355
356
357
358
359
360
361
362
Compiling French-Japanese Terminologies from the Web 
 
Xavier Robitaille?, Yasuhiro Sasaki?, Masatsugu Tonoike?,  
Satoshi Sato? and Takehito Utsuro? 
?Graduate School of Informatics, 
Kyoto University 
Yoshida-Honmachi, Sakyo-ku, 
Kyoto 606-8501 Japan 
?Graduate School of Engineering, 
Nagoya University 
Furo-cho, Chikusa-ku, 
Nagoya 464-8603 Japan 
{xavier, sasaki, tonoike, utsuro}@pine.kuee.kyoto-u.ac.jp, 
ssato@nuee.nagoya-u.ac.jp 
 
Abstract 
We propose a method for compiling bi-
lingual terminologies of multi-word 
terms (MWTs) for given translation pairs 
of seed terms. Traditional methods for bi-
lingual terminology compilation exploit 
parallel texts, while the more recent ones 
have focused on comparable corpora. We 
use bilingual corpora collected from the 
web and tailor made for the seed terms. 
For each language, we extract from the 
corpus a set of MWTs pertaining to the 
seed?s semantic domain, and use a com-
positional method to align MWTs from 
both sets. We increase the coverage of 
our system by using thesauri and by ap-
plying a bootstrap method. Experimental 
results show high precision and indicate 
promising prospects for future develop-
ments.  
1 Introduction 
Bilingual terminologies have been the center of 
much interest in computational linguistics. Their 
applications in machine translation have proven 
quite effective, and this has fuelled research aim-
ing at automating terminology compilation. Early 
developments focused on their extraction from 
parallel corpora (Daille et al (1994), Fung 
(1995)), which works well but is limited by the 
scarcity of such resources. Recently, the focus 
has changed to utilizing comparable corpora, 
which are easier to obtain in many domains. 
Most of the proposed methods use the fact that 
words have comparable contexts across lan-
guages. Fung (1998) and Rapp (1999) use so 
called context vector methods to extract transla-
tions of general words. Chiao and Zweigenbaum 
(2002) and D?jean and Gaussier (2002) apply 
similar methods to technical domains. Daille and 
Morin (2005) use specialized comparable cor-
pora to extract translations of multi-word terms 
(MWTs).  
These methods output a few thousand terms 
and yield a precision of more or less 80% on the 
first 10-20 candidates. We argue for the need for 
systems that output fewer terms, but with a 
higher precision. Moreover, all the above were 
conducted on language pairs including English. 
It would be possible, albeit more difficult, to ob-
tain comparable corpora for pairs such as 
French-Japanese. We will try to remove the need 
to gather corpora beforehand altogether. To 
achieve this, we use the web as our only source 
of data. This idea is not new, and has already 
been tried by Cao and Li (2002) for base noun 
phrase translation. They use a compositional 
method to generate a set of translation candidates 
from which they select the most likely translation 
by using empirical evidence from the web.  
The method we propose takes a translation 
pair of seed terms in input. First, we collect 
MWTs semantically similar to the seed in each 
language. Then, we work out the alignments be-
tween the MWTs in both sets. Our intuition is 
that both seeds have the same related terms 
across languages, and we believe that this will 
simplify the alignment process. The alignment is 
done by generating a set of translation candidates 
using a compositional method, and by selecting 
the most probable translation from that set. It is 
very similar to Cao and Li?s, except in two re-
spects. First, the generation makes use of 
thesauri to account for lexical divergence be-
tween MWTs in the source and target language. 
Second, we validate candidate translations using 
a set of terms collected from the web, rather than 
using empirical evidence from the web as a 
whole. Our research further differs from Cao and 
Li?s in that they focus only on finding valid 
translations for given base noun phrases. We at-
tempt to both collect appropriate sets of related 
MWTs and to find their respective translations. 
The initial output of the system contains 9.6 
pairs on average, and has a precision of 92%.  
We use this high precision as a bootstrap to 
augment the set of Japanese related terms, and 
obtain a final output of 19.6 pairs on average, 
with a precision of 81%. 
2 Related Term Collection 
Given a translation pair of seed terms (sf, sj), we 
use a search engine to gather a set F of French 
terms related to sf, and a set J of Japanese terms 
related to sj. The methods applied for both lan-
guages use the framework proposed by Sato and 
Sasaki (2003), outlined in Figure 1. We proceed 
in three steps: corpus collection, automatic term 
recognition (ATR), and filtering.   
2.1 Corpus Collection 
For each language, we collect a corpus C from 
web pages by selecting passages that contain the 
seed. 
Web page collection 
In French, we use Google to find relevant web 
pages by entering the following three queries: 
?sf?, ?sf est? (sf is), and ?sf sont? (sf are). In Japa-
nese, we do the same with queries ?sj?, ?sj???, 
?sj??, ?sj????, and ?sj??, where ?? toha, 
? ha, ??? toiu, and ? no are Japanese func-
tional words that are often used for defining or 
explaining a term. We retrieve the top pages for 
each query, and parse those pages looking for 
hyperlinks whose anchor text contain the seed. If 
such links exist, we retrieve the linked pages as 
well. 
Sentence extraction 
From the retrieved web pages, we remove html 
tags and other noise. Then, we keep only prop-
erly structured sentences containing the seed, as 
well as the preceding and following sentences ? 
that is, we use a window of three sentences 
around the seed. 
2.2 Automatic Term Recognition 
The next step is to extract candidate related terms 
from the corpus. Because the sentences compos-
ing the corpus are related to the seed, the same 
should be true for the terms they contain. The 
process of extracting terms is highly language 
dependent. 
French ATR 
We use the C-value method (Frantzi and 
Ananiadou (2003)), which extracts compound 
terms and ranks them according to their term-
hood. It consists of a linguistic part, followed by 
a statistical part. 
The linguistic part consists in applying a lin-
guistic filter to constrain the structure of terms 
extracted. We base our filter on a morphosyntac-
tic pattern for the French language proposed by 
Daille et al It defines the structure of multi-word 
units (MWUs) that are likely to be terms. Al-
though their work focused on MWUs limited to 
two content words (nouns, adjectives, verbs or 
adverbs), we extend our filter to MWUs of 
greater length. The pattern is defined as follows: 
( ) ( )( )+NumNounDetPrepAdjNumNoun ?  
The statistical part measures the termhood of 
each compound that matches the linguistic pat-
tern. It is given by the C-value:  
( )
( )
( )
( )
( )
??
??
?
??
??
?
?
??
?
?
?
??
?
?
?
?=?
?
?
otherwise
T
b
aaa
nestednotisaif
aa
a
a
Tb a
P
f
f)f(log
,
flog
valueC
2
2
 
where a is the candidate string, f(a) is its fre-
quency of occurrence in all the web pages re-
trieved, Ta is the set of extracted candidate terms 
that contain a, and P(Ta) is the number of these 
candidate terms. 
The nature of our variable length pattern is 
such that if a long compound matches the pat-
tern, all the shorter compounds it includes also 
match. For example, consider the N-Prep-N-
 
 
 
related term sets 
(F, J)
 the  Web ATR 
Filtering 
 
 Corpus collection 
corpora 
(Cf, Cj) 
 
term sets 
(Xf, Xj) 
seed terms
(sf, sj) 
Figure 1: Related term collection 
Prep-N structure in syst?me ? base de connais-
sances (knowledge based system). The shorter 
candidate syst?me ? base (based system) also 
matches, although we would prefer not to extract 
it. 
Fortunately, the strength of the C-value is the 
way it effectively handles nested MWTs. When 
we calculate the termhood of a string, we sub-
tract from its total frequency its frequency as a 
substring of longer candidate terms. In other 
words, a shorter compound that almost always 
appears nested in a longer compound will have a 
comparatively smaller C-value, even if its total 
frequency is higher than that of the longer com-
pound. Hence, we discard MWTs whose C-value 
is smaller than that of a longer candidate term in 
which it is nested. 
Japanese ATR 
Because compound nouns represent the bulk of 
Japanese technical MWTs, we extract them as 
candidate related terms. As opposed to Sato and 
Sasaki, we ignore single nouns. Also, we do not 
limit the number of candidates output by ATR as 
they did.  
2.3 Filtering 
Finally, from the output set of ATR, we select 
only the technical terms that are part of the 
seed?s semantic domain. Numerous measures 
have been proposed to gauge the semantic simi-
larity between two words (van Rijsbergen 
(1979)). We choose the Jaccard coefficient, 
which we calculate based on search engine hit 
counts. The similarity between a seed term s and 
a candidate term x is given by: ( )
( )xsH
xsHJac ?
?=  
where H(s ? x) is the hit count of pages contain-
ing both s and x, and H(s ? x) is the hit count of 
pages containing s or x. The latter can be calcu-
lated as follows: 
( ) ( ) ( )xsHxHsHxsH ??+=? )(  
Candidates that have a high enough coefficient 
are considered related terms of the seed.  
3 Term Alignment 
Once we have collected related terms in both 
French and Japanese, we must link the terms in 
the source language to the terms in the target 
language. Our alignment procedure is twofold. 
First, we first generate Japanese translation can-
didates for each collected French term. Second, 
we select the most likely translation(s) from the 
set of candidates. This is similar to the genera-
tion and selection procedures used in the litera-
ture (Baldwin and Tanaka (2004), Cao and Li, 
Langkilde and Knight (1998)). 
3.1 Translation Candidates Generation 
Translation candidates are generated using a 
compositional method, which can be divided in 
three steps. First, we decompose the French 
MWTs into combinations of shorter MWU ele-
ments. Second, we look up the elements in bilin-
gual dictionaries. Third, we recompose transla-
tion candidates by generating different combina-
tions of translated elements. 
Decomposition 
In accordance with Daille et al, we define the 
length of a MWU as the number of content 
words it contains. Let n be the length of the 
MWT to decompose. We produce all the combi-
nations of MWU elements of length less or equal 
to n. For example, consider the French transla-
tion of ?knowledge based system?: 
It has a length of three and yields the following 
four combinations1: 
Note the treatment given to the prepositions 
and determiners: we leave them in place when 
they are interposed between content words 
within elements, otherwise we remove them. 
Dictionary Lookup 
We look up each element in bilingual dictionar-
ies. Because some words appear in their inflected 
forms, we use their lemmata. In the example 
given above, we look up connaissance (lemma) 
rather than connaissances (inflected). Note that 
we do not lemmatize MWUs such as base de 
connaissances. This is due to the complexity of 
gender and number agreements of French com-
pounds. However, only a small part of the 
MWTs are collected in their inflected forms, and 
French-Japanese bilingual dictionaries do not 
contain that many MWTs to begin with. The per-
formance hit should therefore be minor.  
Already at this stage, we can anticipate prob-
lems arising from the insufficient coverage of 
                                                 
1 A MWT of length n produces 2n-1 combinations, 
including itself. 
syst?me ? base de connaissances
Noun Prep Noun Prep Noun 
[syst?me ? [base de [connaissances]
[syst?me]  [base de [connaissances]
[syst?me ? [base]  [connaissances]
[syst?me]  [base]  [connaissances]
French-Japanese lexicon resources. Bilingual 
dictionaries may not have enough entries, and  
existing entries may not include a great variety of 
translations for every sense. The former problem 
has no easy solution, and is one of the reasons 
we are conducting this research. The latter can be 
partially remedied by using thesauri ? we aug-
ment each element?s translation set by looking 
up in thesauri all the translations obtained with 
bilingual dictionaries. 
Recomposition 
To recompose the translation candidates, we 
simply generate all suitable combinations of 
translated elements for each decomposition. The 
word order is inverted to take into account the 
different constraints in French and Japanese. In 
the example above, if the lookup phase gave {?
? chishiki}, {?? dodai, ??? besu} and {?
? taikei, ???? shisutemu} as respective 
translation sets for syst?me, base and connais-
sance, the fourth decomposition given above 
would yield the following candidates: 
connaissance base syst?me 
?? ?? ?? 
?? ?? ????
?? ??? ?? 
?? ??? ????
If we do not find any translation for one of the 
elements, the generation fails. 
3.2 Translation Selection  
Selection consists of picking the most likely 
translation from the translation candidates we 
have generated. To discern the likely from the 
unlikely, we use the empirical evidence provided 
by the set of Japanese terms related to the seed. 
We believe that if a candidate is present in that 
set, it could well be a valid translation, as the 
French MWT in consideration is also related to 
the seed. Accordingly, our selection process con-
sists of picking those candidates for which we 
find a complete match among the related terms.  
3.3 Relevance of Compositional Methods 
The automatic translation of MWTs is no simple 
task, and it is worthwhile asking if it is best tack-
led with a compositional method. Intricate prob-
lems have been reported with the translations of 
compounds (Daille and Morin, Baldwin and Ta-
naka), notably:  
? fertility: source and target MWTs can be 
of different lengths. For example, table 
de v?rit? (truth table) contains two con-
tent words and translates into ??????
shinri ? chi ? hyo (lit. truth-value-table), 
which contains three. 
? variability of forms in the transla-
tions: MWTs can appear in many forms. 
For example, champ electromagn?tique 
(electromagnetic field) translates both 
into ???? denji? ba (lit. electromag-
netic field)???? denji?kai (lit. elec-
tromagnetic ?region?). 
? constructional variability in the trans-
lations: source and target MWTs have 
different morphological structures. For 
example, in the pair apprentissage auto-
matique??? ???  kikai ? gakushu 
(machine learning) we have (N-
Adj)?(N-N). In the pair programmation 
par contraintes???????? patan?
ninshiki (pattern recognition) we have 
(N-par-N)?(N-N). 
? non-compositional compounds: some 
compounds? meaning cannot be derived 
from the meaning of their components. 
For example, the Japanese term ???
aka?ten (failing grade, lit. ?red point?) 
translates into French as note d??chec (lit. 
failing grade) or simply ?chec (lit. fail-
ure).  
? lexical divergence: source and target 
MWTs can use different lexica to ex-
press a concept. For example, traduction 
automatique (machine translation, lit. 
?automatic translation?) translates as ?
???? kikai ? honyaku (lit. machine 
translation). 
It is hard to imagine any method that could ad-
dress all these problems accurately.  
Tanaka and Baldwin (2003) found that 48.7% 
of English-Japanese Noun-Noun compounds 
translate compositionality. In a preliminary ex-
periment, we found this to be the case for as 
much as 75.1% of the collected MWTs. If we are 
to maximize the coverage of our system, it is 
sensible to start with a compositional approach. 
We will not deal with the problem of fertility and 
non-compositional compounds in this paper. 
Nonetheless, lexical divergence and variability 
issues will be partly tackled by broader transla-
tions and related words given by thesauri. 
4 Evaluation 
4.1 Linguistic Resources 
The bilingual dictionaries used in the experi-
ments are the Crown French-Japanese Dictionary 
(Ohtsuki et al (1989)), and the French-Japanese 
Scientific Dictionary (French-Japanese Scientific 
Association (1989)). The former contains about 
50,000 entries of general usage single words. 
The latter contains about 50,000 entries of both 
single and multi-word scientific terms. These 
two complement each other, and by combining 
both entries we form our base dictionary to 
which we refer as DicFJ. 
The main thesaurus used is Bunrui Goi Hyo 
(National Institute for Japanese Language 
(2004)). It contains about 96,000 words, and 
each entry is organized in two levels: a list of 
synonyms and a list of more loosely related 
words. We augment the initial translation set by 
looking up the Japanese words given by DicFJ. 
The expanded bilingual dictionary comprised of 
the words from DicFJ combined with their syno-
nyms is denoted DicFJJ. The dictionary resulting 
of DicFJJ combined with the more loosely related 
words is denoted DicFJJ2. 
Finally, we build another thesaurus from a 
Japanese-English dictionary. We use Eijiro 
(Electronic Dictionary Project (2004)), which 
contains 1,290,000 entries. For a given Japanese 
entry, we look up its English translations. The 
Japanese translations of the English intermediar-
ies are used as synonyms/related words of the 
entry. The resulting thesaurus is expected to pro-
vide even more loosely related translations (and 
also many irrelevant ones). We denote it DicFJEJ. 
4.2 Notation 
Let F and J be the two sets of related terms col-
lected in French and Japanese. F? is the subset of 
F for which Jac?0.01: { }01.0)(' ??= fJacFfF  
F?* is the subset of valid related terms in F?, as 
determined by human evaluation. P is the set of 
all potential translation pairs among the collected 
terms (P=F?J). P? is the set of pairs containing 
either a French term or a Japanese term with 
Jac?0.01: 
( ){ }01.0)(01.0)(,' ?????= jJacfJacJjFfP  
P?* is the subset of valid translation pairs in P?, 
determined by human evaluation. These pairs 
need to respect three criteria: 1) contain valid 
terms, 2) be related to the seed, and 3) constitute 
a valid translation. M is the set of all translations 
selected by our system. M? is the subset of pairs 
in M with Jac?0.01 for either the French or the 
Japanese term. It is also the output of our system: { }01.0)(01.0)(),(' ????= jJacfJacMjfM  
M?* is the intersection of M? and P?*, or in other 
words, the subset of valid translation pairs output 
by our system. 
4.3 Baseline Method 
Our starting point is the simplest possible align-
ment, which we refer to as our baseline. It is 
worked out by using each of the aforementioned 
dictionaries independently. The output set ob-
tained using DicFJ is denoted FJ, the one using 
DicFJJ is denoted FJJ, and so on. The experiment 
is made using the eight seed pairs given in Table 
1. On average, we have |F'| =74.3, |F'*|=51.0 and 
|P'*|=24.0. Table 2 gives a summary of the key 
results. The precision and the recall are given by: 
'
'*
M
M
precision =  , 
'*
'*
P
M
recall =  
DicFJ contains only Japanese translations cor-
responding to the strict sense of French elements. 
Such a dictionary generates only a few transla-
tion candidates which tend to be correct when 
present in the target set. On the other hand, the 
lookup in DicFJJ2 and DicFJEJ interprets French 
Set |M'| |M'*| Prec. Recall 
FJ 10.5 9.6  92% 40% 
FJJ 15.3 12.6  83% 53% 
FJJ2 20.5 13.4  65% 56% 
FJEJ 30.9 14.1  46% 59% 
Table 2: Results for the baseline 
Id French Japanese (English)
1 analyse vectorielle ??????? bekutoru?kaiseki (vector analysis) 
2 circuit logique ????? ronri?kairo (logic circuit) 
3   intelligence artificielle          ????? jinko?chinou (artificial intelligence) 
4 linguistique informatique ?????? keisan?gengogaku (computational linguistics) 
5 reconnaissance des formes ??????? patan?ninshiki (pattern recognition) 
6 reconnaissance vocale ????? onsei?ninshiki (speech recognition) 
7 science cognitive ????? ninchi?kagaku (cognitive science) 
8 traduction automatique ????? kikai?honyaku (machine translation) 
Table 1: Seed pairs 
MWT elements with more laxity, generating 
more translations and thus more alignments, at 
the cost of some precision. 
4.4 Incremental Selection 
The progressive increase in recall given by the 
increasingly looser translations is in inverse pro-
portion to the decrease in precision, which hints 
that we should give precedence to the alignments 
obtained with the more accurate methods. Con-
sequently, we start by adding the alignments in 
FJ to the output set. Then, we augment it with 
the alignments from FJJ whose terms are not 
already in FJ. The resulting set is denoted FJJ'. 
We then augment FJJ' with the pairs from FJJ2 
whose terms are not in FJJ', and so on, until we 
exhaust the alignments in FJEJ.  
For instance, let FJ contain (synth?se de la 
parole? ? ? ? ? ? onsei ? gousei (speech 
synthesis)) and FJJ contain this pair plus 
(synth?se de la parole?????? onsei?kaiseki 
(speech analysis)). In the first iteration, the pair 
in FJ is added to the output set. In the second 
iteration, no pair is added because the output set 
already contains an alignment with synth?se de 
la parole. 
Table 3 gives the results for each incremental 
step. We can see an increase in precision for FJJ', 
FJJ2' and FJEJ' of respectively 5%, 9% and 8%, 
compared to FJJ, FJJ2 and FJEJ. We are effec-
tively filtering output pairs and, as expected, the 
increase in precision is accompanied by a slight 
decrease in recall.  Note that, because FJEJ is 
not a superset of FJJ2, we see an increase in both 
precision and recall in FJEJ' over FJEJ. None-
theless, the precision yielded by FJEJ' is not suf-
ficient, which is why DicFJEJ is left out in the 
next experiment. 
4.5 Bootstrapping 
The coverage of the system is still shy of the 20 
pairs/seed objective we gave ourselves. One 
cause for this is the small number of valid trans-
lation pairs available in the corpora. From an 
average of 51 valid related terms in the source 
set, only 24 have their translation in the target set. 
To counter that problem, we increase the cover-
age of Japanese related terms and hope that by 
doing so, we will also increase the coverage of 
the system as a whole.  
Once again, we utilize the high precision of 
the baseline method. The average 10.5 pairs in 
FJ include 92% of Japanese terms semantically 
similar to the seed. By inputting these terms in 
the term collection system, we collect many 
more terms, some of which are probably the 
translations of our French MWTs. 
The results for the baseline method with boot-
strapping are given in Table 4. The ones using 
incremental selection and bootstrapping are 
given in Table 5. FJ+ consists of the alignments 
given by a generation process using DicFJ and a 
selection performed on the augmented set of re-
lated terms. FJJ+ and FJJ2+ are obtained in the 
same way using DicFJJ and DicFJJ2. FJ+' contains 
the alignments from FJ, augmented with those 
from FJ+ whose terms are not in FJ. FJJ+' con-
tains FJ+', incremented with terms from FJJ. 
FJJ+'' contains FJJ+', incremented with terms 
from FJJ+, and so on.  
The bootstrap mechanism grows the target 
term set tenfold, making it very laborious to 
identify all the valid translation pairs manually. 
Consequently, we only evaluate the pairs output 
by the system, making it impossible to calculate 
recall. Instead, we use the number of valid trans-
lation pairs as a makeshift measure. 
Bootstrapping successfully allows for many 
more translation pairs to be found. FJ+, FJJ+, 
and FJJ2+ respectively contain 7.6, 8.7 and 8.5 
more valid alignments on average than FJ, FJJ 
and FJJ2. The augmented target term set is nois-
ier than the initial set, and it produces many more 
invalid alignments as well. Fortunately, the in-
cremental selection effectively filters out most of 
the unwanted, restoring the precision to accept-
able levels.  
Set |M'| |M'*| Prec. Recall 
FJJ' 14.0  12.3  88% 51% 
FJJ2' 16.1  12.8  79% 53% 
FJEJ' 29.1  15.5  53% 65% 
Table 3: Results for the incremental selection 
Set |M'| |M'*| Prec. 
FJ+' 19.5 16.1  83% 
FJJ+' 22.5 18.6  83% 
FJJ +'' 24.3 19.6  81% 
FJJ2+' 25.6 20.1  79% 
FJJ2+'' 28.6 20.6  72% 
Table 5: Results for the incremental 
selection with bootstrap expansion 
Set |M'| |M'*| Prec. 
FJ+ 20.9 16.8  80% 
FJJ+ 30.9 21.3  69% 
FJJ2+ 45.8 22.6  49% 
Table 4: Results for the baseline 
method with bootstrap expansion 
4.6 Analysis 
A comparison of all the methods is illustrated in 
the precision ? valid alignments curves of Figure 
2. The points on the four curves are taken from 
Tables 2 to 5. The gap between the dotted and 
filled curves clearly shows that bootstrapping 
increases coverage. The respective positions of 
the squares and crosses show that incremental 
selection effectively filters out erroneous align-
ments. FJJ+'', with 19.6 valid alignments and a 
precision of 81%, is at the rightmost and upper-
most position in the graph. The detailed results 
for each seed are presented in Table 6, and the 
complete output for the seed ?logic circuit? is 
given in Table 7.  
From the average 4.7 erroneous pairs/seed, 3.2 
(68%) were correct translations but were judged 
unrelated to the seed. This is not surprising, con-
sidering that our set of French related terms con-
tained only 69% (51/74.3) of valid related terms. 
Also note that, of the 24.3 pairs/seed output, 5.25 
are listed in the French-Japanese Scientific Dic-
tionary. However, only 3.9 of those pairs are in-
cluded in M'*. The others were deemed unrelated 
to the seed.  
In the output set of ?machine translation?, ?
??????? shizen ?gengo ?shori (natural lan-
guage processing) is aligned to both traitement 
du language naturel and traitement des langues 
naturelles. The system captures the term?s vari-
ability around langue/language. Lexical diver-
gence is also taken into account to some extent. 
The seed computational linguistics yields the 
alignment of langue maternelle (mother tongue) 
with ?? ?? bokoku ? go (literally [[mother-
country]-language]). The usage of thesauri en-
abled the system to include the concept of coun-
try in the translated MWT, even though it is not 
present in any of the French elements. 
5 Conclusion and future work 
We have proposed a method for compiling bilin-
gual terminologies of compositionally translated 
MWTs. As opposed to previous work, we use the 
web rather than comparable corpora as a source 
of bilingual data. Our main insight is to constrain 
source and target candidate MWTs to only those 
strongly related to the seed. This allows us to 
achieve term alignment with high precision. We 
showed that coverage reaches satisfactory levels 
by using thesauri and bootstrapping.  
Due to the difference in objectives and in cor-
pora, it is very hard to compare results: our 
method produces a rather small set of highly ac-
curate alignments, whereas extraction from com-
parable corpora generates much more candidates, 
but with an inferior precision. These two ap-
proaches have very different applications. Our 
method does however eliminate the requirement 
of comparable corpora, which means that we can 
use seeds from any domain, provided we have 
reasonably rich dictionaries and thesauri.  
Let us not forget that this article describes 
only a first attempt at compiling French-Japanese 
terminology, and that various sources of im-
provement have been left untapped. In particular, 
our alignment suffers from the fact that we do 
not discriminate between different candidate 
translations. This could be achieved by using any 
of the more sophisticated selection methods pro-
posed in the literature. Currently, corpus features 
are used solely for the collection of related terms. 
These could also be utilized in the translation 
selection, which Baldwin and Tanaka have 
shown to be quite effective. We could also make 
use of bilingual dictionary features as they did. 
Lexical context is another resource we have not 
exploited. Context vectors have successfully 
been applied in translation selection by Fung  as 
well as  Daille and Morin.  
On a different level, we could also apply the 
bootstrapping to expand the French set of related 
terms. Finally, we are investigating the possibil-
seed |F'| |F'*| |P'*| |M'| |M'*| Prec. 
1 89 40 14 26 13 50% 
2 64 55 24 14 14 100% 
3 72 59 38 40 33 83% 
4 67 49 22 23 18 78% 
5 85 70 22 21 17 81% 
6 67 50 27 22 21 95% 
7 36 27 16 20 17 85% 
8 114 58 29 28 24 86% 
avg 74.3 51.0 24.0  24.3  19.6  81% 
Table 6: Detailed results for  FJJ+'' 
70% 
80% 
90% 
100% 
25
Pr
ec
is
io
n 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
0 5 10 15 20 
Baseline 
Baseline with bootstrap
Incremental 
Incremental with bootstrap
Number of Valid Alignments
Figure 2: Precision - Valid Alignments curves 
ity of resolving the alignments in the opposite 
direction: from Japanese to French. Surely the 
constructional variability of French MWTs 
would present some difficulties, but we are con-
fident that this could be tackled using translation 
templates, as proposed by Baldwin and Tanaka. 
References 
T. Baldwin and T. Tanaka. 2004. Translation by Ma-
chine of Complex Nominals: Getting it Right. In 
Proc. of the ACL 2004 Workshop on Multiword 
Expressions: Integrating Processing, pp. 24?31, 
Barcelona, Spain.  
Y. Cao and H. Li. 2002. Base Noun Phrase Transla-
tion Using Web Data and the EM Algorithm. In 
Proc. of COLING -02, Taipei, Taiwan. 
Y.C. Chiao and P. Zweigenbaum. 2002. Looking for 
Candidate Translational Equivalents in Specialized, 
Comparable Corpora. In Proc. of COLING-02, pp. 
1208?1212. Taipei, Taiwan. 
B. Daille, E. Gaussier, and J.M. Lange. 1994. To-
wards Automatic Extraction of Monolingual and 
Bilingual Terminology. In Proc. of COLING-94, 
pp. 515?521, Kyoto, Japan. 
B. Daille and E. Morin. 2005. French-English Termi-
nology Extraction from Comparable Corpora, In 
IJCNLP-05, pp. 707?718, Jeju Island, Korea. 
H. D?jean., E. Gaussier and F. Sadat. An Approach 
Based on Multilingual Thesauri and Model Com-
bination for Bilingual Lexicon Extraction. In Proc. 
of COLING-02, pp. 218?224. Taipei, Taiwan. 
Electronic Dictionary Project. 2004. Eijiro Japanese-
English Dictionary: version 79. EDP. 
K.T. Frantzi, and S. Ananiadou. 2003. The C-
Value/NC-Value Domain Independent Method for 
Multi-Word Term Extraction. Journal of Natural 
Language Processing, 6(3), pp. 145?179. 
French Japanese Scientific Association. 1989. French-
Japanese Scientific Dictionary: 4th edition. Haku-
suisha. 
P. Fung. 1995. A Pattern Matching Method for Find-
ing Noun and Proper Noun from Noisy Parallel 
Corpora. In Proc of the ACL-95, pp. 236?243, 
Cambridge, USA. 
P. Fung. 1998. A Statiscal View on Bilingual Lexicon 
Extraction: From Parallel Corpora to Non-parallel 
Corpora. In D. Farwell, L. Gerber and L. Hovy 
eds.: Proceedings of the AMTA-98, Springer, pp. 
1?16. 
I. Langkilde and K. Knight. 1998. Generation that 
exploits corpus-based statistical knowledge. In 
COLLING/ACL-98, pp. 704?710, Montreal, Can-
ada. 
National Institute for Japanese Language. 2004. Bun-
rui Goi Hyo: revised and enlarged edition Dainip-
pon Tosho. 
T. Ohtsuki et al 1989. Crown French-Japanese Dic-
tionary: 4th edition. Sanseido. 
R. Rapp. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
Corpora. In Proc. of the ACL-99. pp. 1?17. Col-
lege Park, USA. 
S. Sato and Y. Sasaki. 2003. Automatic Collection of 
Related Terms from the Web. In ACL-03 Compan-
ion Volume to the Proc. of the Conference, pp. 
121?124, Sapporo, Japan. 
T. Tanaka and T. Baldwin. 2003. Noun-Noun Com-
pound Machine Translation: A Feasibility Study on 
Shallow Processing. In Proc. of the ACL-2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pp. 17?24. Sapporo, 
Japan. 
van Rijsbergen, C.J. 1979. Information Retrieval. 
London: Butterworths. Second Edition. 
Jac (Fr.) French term Japanese term (English) eval? 
0.100  portes logiques ?????? ronri?geeto (logic gate) 2/2/2 
0.064  fonctions logiques ????? ronri?kansuu (logic function) 2/2/2 
0.064  fonctions logiques ????? ronri?kinou (logic function) 2/2/2 
0.048  registre ? d?calage ???????? shifuto?rejisuta (shift register) 2/2/2 
0.044  simulateur de circuit ????????? kairo?shimureeta (circuit simulator) 2/2/2 
0.040  circuit combinatoire ?????? kumiawase?kairo (combinatorial circuit) 2/2/2 
0.031  nombre binaire 2??? ni?shinsuu (binary number) 2/2/2 
0.024  niveaux logiques ?????? ronri?reberu (logical level) 2/2/2 
0.020  circuit logique combinatoire ????????? kumiawase?ronri?kairo (combinatorial logic circuit) 2/2/2 
0.017  valeur logique ???? ronri?chi (logical value) 2/2/2 
0.013  tension d' alimentation ????? dengen?denatsu (supply voltage) 2/2/2 
0.011  conception de circuits ????? kairo?sekkei (circuit design) 2/2/2 
0.007  conception d' un circuit logique ???????? ronri?kairo?sekkei (logic circuit design) 2/1/2 
0.005  nombre de portes ????? geeto?suu (number of gates) 2/1/2 
? relatedness / termhood / quality of the translation, on a scale of  0 to 2 
Table 7: System output for seed pair circuit logique ????? (logic circuit) 
Effect of Domain-Specific Corpus
in Compositional Translation Estimation for Technical Terms
Masatsugu Tonoike?, Mitsuhiro Kida?,
Takehito Utsuro?
?Graduate School of Informatics,
Kyoto University
Yoshida-Honmachi, Sakyo-ku,
Kyoto 606-8501 Japan
(tonoike,kida,takagi,sasaki,
utsuro)@pine.kuee.kyoto-u.ac.jp
Toshihiro Takagi?, Yasuhiro Sasaki?,
and Satoshi Sato?
?Graduate School of Engineering,
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603 JAPAN
ssato@nuee.nagoya-u.ac.jp
Abstract
This paper studies issues on compiling
a bilingual lexicon for technical terms.
In the task of estimating bilingual term
correspondences of technical terms, it
is usually quite difficult to find an exist-
ing corpus for the domain of such tech-
nical terms. In this paper, we take an
approach of collecting a corpus for the
domain of such technical terms from
the Web. As a method of translation
estimation for technical terms, we pro-
pose a compositional translation esti-
mation technique. Through experimen-
tal evaluation, we show that the do-
main/topic specific corpus contributes
to improving the performance of the
compositional translation estimation.
1 Introduction
This paper studies issues on compiling a bilingual
lexicon for technical terms. So far, several tech-
niques of estimating bilingual term correspon-
dences from a parallel/comparable corpus have
been studied (Matsumoto and Utsuro, 2000). For
example, in the case of estimation from compa-
rable corpora, (Fung and Yee, 1998; Rapp, 1999)
proposed standard techniques of estimating bilin-
gual term correspondences from comparable cor-
pora. In their techniques, contextual similarity
between a source language term and its transla-
tion candidate is measured across the languages,
and all the translation candidates are re-ranked ac-
cording to the contextual similarities. However,
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
Figure 1: Compilation of a Domain/Topic Spe-
cific Bilingual Lexicon
there are limited number of parallel/comparable
corpora that are available for the purpose of es-
timating bilingual term correspondences. There-
fore, even if one wants to apply those existing
techniques to the task of estimating bilingual term
correspondences of technical terms, it is usually
quite difficult to find an existing corpus for the
domain of such technical terms.
Considering such a situation, we take an ap-
proach of collecting a corpus for the domain of
such technical terms from the Web. In this ap-
proach, in order to compile a bilingual lexicon
for technical terms, the following two issues have
to be addressed: collecting technical terms to be
listed as the headwords of a bilingual lexicon, and
estimating translation of those technical terms.
Among those two issues, this paper focuses on the
second issue of translation estimation of technical
terms, and proposes a method for translation es-
timation for technical terms using a domain/topic
specific corpus collected from the Web.
More specifically, the overall framework of
114
compiling a bilingual lexicon from the Web can
be illustrated as in Figure 1. Suppose that we have
sample terms of a specific domain/topic, techni-
cal terms to be listed as the headwords of a bilin-
gual lexicon are collected from the Web by the re-
lated term collection method of (Sato and Sasaki,
2003). Those collected technical terms can be di-
vided into three subsets according to the number
of translation candidates they have in an existing
bilingual lexicon, i.e., the subset XUS of terms for
which the number of translations in the existing
bilingual lexicon is one, the subset XMS of terms
for which the number of translations is more than
one, and the subset YS of terms which are not
found in the existing bilingual lexicon. (Hence-
forth, the union XUS ? XMS is denoted as XS .)
The translation estimation task here is to estimate
translations for the terms of XMS and YS . For the
terms of XMS , it is required to select an appro-
priate translation from the translation candidates
found in the existing bilingual lexicon. For ex-
ample, as a translation of the Japanese technical
term ??????, which belongs to the logic cir-
cuit field, the term ?register? should be selected
but not the term ?regista? of the football field. On
the other hand, for the terms of YS , it is required
to generate and validate translation candidates. In
this paper, for the above two tasks, we use a do-
main/topic specific corpus. Each term of XUS has
the only one translation in the existing bilingual
lexicon. The set of the translations of terms of
XUS is denoted as XUT . Then, the domain/topic
specific corpus is collected from the Web using
the terms in the set XUT . A new bilingual lexicon
is compiled from the result of translation estima-
tion for the terms of XMS and YS , as well as the
translation pairs which consist of the terms of XUS
and their translations found in the existing bilin-
gual lexicon.
For each term of XMS , from the translation can-
didates found in the existing bilingual lexicon, we
select the one which appears most frequently in
the domain/topic specific corpus. The experimen-
tal result of this translation selection process is de-
scribed in Section 5.2.
As a method of translation genera-
tion/validation for technical terms, we propose a
compositional translation estimation technique.
Compositional translation estimation of a term
can be done through the process of composi-
tionally generating translation candidates of the
term by concatenating the translation of the
constituents of the term. Here, those translation
candidates are validated using the domain/topic
specific corpus.
In order to assess the applicability of the com-
positional translation estimation technique, we
randomly pick up 667 Japanese and English tech-
nical term translation pairs of 10 domains from
existing technical term bilingual lexicons. We
then manually examine their compositionality,
and find out that 88% of them are actually com-
positional, which is a very encouraging result.
Based on this assessment, this paper proposes a
method of compositional translation estimation
for technical terms, and through experimental
evaluation, shows that the domain/topic specific
corpus contributes to improving the performance
of compositional translation estimation.
2 Collecting a Domain/Topic Specific
Corpus
When collecting a domain/topic specific corpus of
the language T , for each technical term xUT in the
set XUT , we collect the top 100 pages with search
engine queries including xUT . Our search engine
queries are designed so that documents which de-
scribe the technical term xUT is to be ranked high.
For example, an online glossary is one of such
documents. Note that queries in English and those
in Japanese do not correspond. When collect-
ing a Japanese corpus, the search engine ?goo?1
is used. Specific queries used here are phrases
with topic-marking postpositional particles such
as ?xUT ???, ?xUT ????, ?xUT ??, and an ad-
nominal phrase ?xUT ??, and ?xUT ?. When col-
lecting a English corpus, the search engine ?Ya-
hoo!?2 is used. Specific queries used here are ?xUT
AND what?s?, ?xUT AND glossary?, and ?xUT ?.
3 Compositional Translation Estimation
for Technical Terms
3.1 Overview
An example of compositional translation estima-
tion for the Japanese technical term ??????
1http://www.goo.ne.jp/
2http://www.yahoo.com/
115
? application(1)
? practical(0.3)
? applied(1.6)
? action(1)
? activity(1)
? behavior(1)
? analysis(1)
? diagnosis(1)
? assay(0.3)
? behavior analysis(10)
??Compositional generation 
of translation candidate
? applied behavior analysis(17.6)
? application behavior analysis(11)
? applied behavior diagnosis(1)
??Decompose source term into constituents  
??Translate constituents into target language      process
?? ?? ??a
?? ????b
Generated translation candidates
?(1.6?1?1)+(1.6?10)
? application(1)
? practical(0.3)
? applied(1.6)
Figure 2: Compositional Translation Estimation
for the Japanese Technical Term ????????
?? is shown in Figure 2. First, the Japanese tech-
nical term ???????? is decomposed into
its constituents by consulting an existing bilin-
gual lexicon and retrieving Japanese headwords.3
In this case, the result of this decomposition can
be given as in the cases ?a? and ?b? (in Fig-
ure 2). Then, each constituent is translated into
the target language. A confidence score is as-
signed to the translation of each constituent. Fi-
nally, translation candidates are generated by con-
catenating the translation of those constituents
without changing word order. The confidence
score of translation candidates are defined as the
product of the confidence scores of each con-
stituent. Here, when validating those translation
candidates using the domain/topic specific cor-
pus, those which are not observed in the corpus
are not regarded as candidates.
3.2 Compiling Bilingual Constituents
Lexicons
This section describes how to compile bilingual
constituents lexicons from the translation pairs of
the existing bilingual lexicon Eijiro. The under-
lying idea of augmenting the existing bilingual
lexicon with bilingual constituents lexicons is il-
lustrated with the example of Figure 3. Suppose
that the existing bilingual lexicon does not in-
clude the translation pair ?applied :???, while
it includes many compound translation pairs with
the first English word as ?applied? and the first
3Here, as an existing bilingual lexicon, we use Ei-
jiro(http://www.alc.co.jp/) and bilingual constituents lexi-
cons compiled from the translation pairs of Eijiro (details
to be described in the next section).
 
applied mathematics : ?? ??
applied science : ?? ??
applied robot : ?? ????
.
.
. frequency
? ??
applied : ?? : 40
 
Figure 3: Example of Estimating Bilingual Con-
stituents Translation Pair (Prefix)
Table 1: Numbers of Entries and Translation Pairs
in the Lexicons
lexicon # of entries # of translationEnglish Japanese pairs
Eijiro 1,292,117 1,228,750 1,671,230
P
2
232,716 200,633 258,211
B
P
38,353 38,546 112,586
B
S
22,281 20,627 71,429
Eijiro : existing bilingual lexicon
P
2
: entries of Eijiro with two constituents
in both languages
B
P
: bilingual constituents lexicon (prefix)
B
S
: bilingual constituents lexicon (suffix)
Japanese word ????.4 In such a case, we align
those translation pairs and estimate a bilingual
constituent translation pair, which is to be col-
lected into a bilingual constituents lexicon.
More specifically, from the existing bilingual
lexicon, we first collect translation pairs whose
English terms and Japanese terms consist of two
constituents into another lexicon P
2
. We compile
?bilingual constituents lexicon (prefix)? from the
first constituents of the translation pairs in P
2
and
compile ?bilingual constituents lexicon (suffix)?
from their second constituents. The numbers of
entries in each language and those of translation
pairs in those lexicons are shown in Table 1.
In the result of our assessment, only 27% of the
667 translation pairs mentioned in Section 1 can
be compositionally generated using Eijiro, while
the rate increases up to 49% using both Eijiro and
?bilingual constituents lexicons?.5
4Japanese entries are supposed to be segmented into a
sequence of words by the morphological analyzer JUMAN
(http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html)
5In our rough estimation, the upper bound of this rate
is about 80%. Improvement from 49% to 80% could be
achieved by extending the bilingual constituents lexicons
and by introducing constituent reordering rules with preposi-
tions into the process of compositional translation candidate
generation.
116
3.3 Score of Translation Pairs in the
Lexicons
This section introduces a confidence score of
translation pairs in the various lexicons presented
in the previous section. Here, we suppose that
the translation pair ?s, t? of terms s and t is used
when estimating translation from the language of
the term s to that of the term t. First, in this pa-
per, we assume that translation pairs follow cer-
tain preference rules and can be ordered as below:
1. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of two or more constituents.
2. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are high.
3. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of exactly one constituent.
4. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are not
high.
As the definition of the confidence score
q(?s, t?) of a translation pair ?s, t?, in this paper,
we use the following:
q(?s, t?) =
?
?
?
?
?
10
(compo(s)?1) (?s, t? in Eijiro)
log
10
fp(?s, t?) (?s, t? in BP )
log
10
fs(?s, t?) (?s, t? in BS)
(1)
where compo(s) denotes the word (in English) or
morpheme (in Japanese) count of s, fp(?s, t?) the
frequency of ?s, t? as the first constituent in P
2
,
and fs(?s, t?) the frequency of ?s, t? as the second
constituent in P
2
.
6
3.4 Score of Translation Candidates
Suppose that a translation candidate yt is gener-
ated from translation pairs ?s
1
, t
1
?, ? ? ? , ?sn, tn?
by concatenating t
1
, ? ? ? , tn as yt = t1 ? ? ? tn.
Here, in this paper, we define the confidence score
of yt as the product of the confidence scores of the
6It is necessary to empirically examine whether this def-
inition of the confidence score is optimal or not. However,
according to our rough qualitative examination, the results
of the confidence scoring seem stable when without a do-
main/topic specific corpus, even with minor tuning by incor-
porating certain parameters into the score.
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
Figure 4: Experimental Evaluation of Translation
Estimation for Technical Terms with/without the
Domain/Topic Specific Corpus (taken from Fig-
ure 1)
constituent translation pairs ?s
1
, t
1
?, ? ? ? , ?sn, tn?.
Q(yt) =
n
?
i=1
q(?si, ti?) (2)
If a translation candidate is generated from
more than one sequence of translation pairs, the
score of the translation candidate is defined as the
sum of the score of each sequence.
4 Translation Candidate Validation
using a Domain/Topic Specific Corpus
It is not clear whether translation candidates
which are generated by the method described in
Section 3 are valid as English or Japanese terms,
and it is not also clear whether they belong to the
domain/topic. So using a domain/topic specific
corpus collected by the method described in Sec-
tion 2, we examine whether the translation candi-
dates are valid as English or Japanese terms and
whether they belong to the domain/topic. In our
validation method, given a ranked list of trans-
lation candidates, each translation candidate is
checked whether it is observed in the corpus, and
one which is not observed in the corpus is re-
moved from the list.
5 Experiments and Evaluation
5.1 Translation Pairs for Evaluation
In our experimental evaluation, within the frame-
work of compiling a bilingual lexicon for tech-
nical terms, we evaluate the translation estima-
tion part which is indicated with bold line in Fig-
117
Table 2: Number of Translation Pairs for Evaluation
dictionaries categories |X
S
| |Y
S
|
S = English S = Japanese
|X
U
S
| |X
M
S
| C(S) |X
U
S
| |X
M
S
| C(S)
Electromagnetics 58 33 36 22 82% 32 26 76%
McGraw-Hill Electrical engineering 52 45 34 18 67% 25 27 64%
Optics 54 31 42 12 65% 22 32 65%
Iwanami Programming language 55 29 37 18 86% 38 17 100%Programming 53 29 29 24 86% 29 24 79%
Dictionary of (Computer) 100 100 91 9 46% 69 31 56%Computer
Anatomical Terms 100 100 91 9 86% 33 67 39%
Dictionary of Disease 100 100 91 9 74% 53 47 51%
250,000 Chemicals and Drugs 100 100 94 6 58% 74 26 51%
medical terms Physical Science and Statistics 100 100 88 12 64% 58 42 55%
Total 772 667 633 139 68% 433 339 57%
McGraw-Hill : Dictionary of Scientific and Technical Terms
Iwanami : Encyclopedic Dictionary of Computer Science
C(S) : for Y
S
, the rate of including correct translations within the collected domain/topic specific corpus
ure 4. In the evaluation of this paper, we sim-
ply skip the evaluation of the process of collecting
technical terms to be listed as the headwords of a
bilingual lexicon. In order to evaluate the transla-
tion estimation part, from ten categories of exist-
ing Japanese-English technical term dictionaries
listed in Table 2, terms are randomly picked up
for each of the set XUS , XMS , and YS . (Here, as
the terms of YS , these which consist of the only
one word or morpheme are excluded.) As de-
scribed in Section 1, the terms of XUT (the set
of the translations for the terms of XUS ) is used
for collecting a domain/topic specific corpus from
the Web. Translation estimation evaluation is to
be done against the set XMS and YS . For each of
the ten categories, Table 2 shows the sizes of XUS ,
XMS and YS , and for YS , the rate of including cor-
rect translation within the collected domain/topic
specific corpus, respectively.
5.2 Translation Selection from Existing
Bilingual Lexicon
For the terms of XMS , the selected translations are
judged by a human. The correct rates are 69%
from English to Japanese on the average and 75%
from Japanese to English on the average.
5.3 Compositional Translation Estimation
for Technical Terms without the
Domain/Topic Specific Corpus
Without the domain specific corpus, the cor-
rect rate of the first ranked translation candidate
is 19% on the average (both from English to
Japanese and from Japanese to English). The
rate of including correct candidate within top 10
is 40% from English to Japanese and 43% from
Japanese to English on the average. The rate of
compositionally generating correct translation us-
ing both Eijiro and the bilingual constituents lex-
icons (n = ?) is about 50% on the average (both
from English to Japanese and from Japanese to
English).
5.4 Compositional Translation Estimation
for Technical Terms with the
Domain/Topic Specific Corpus
With domain specific corpus, on the average, the
correct rate of the first ranked translation candi-
date improved by 8% from English to Japanese
and by 2% from Japanese to English. However,
the rate of including correct candidate within top
10 decreased by 7% from English to Japanese,
and by 14% from Japanese to English. This is be-
cause correct translation does not exist in the cor-
pus for 32% (from English to Japanese) or 43%
(from Japanese to English) of the 667 translation
pairs for evaluation.
For about 35% (from English to Japanese) or
30% (from Japanese to English) of the 667 trans-
lation pairs for evaluation, correct translation does
exist in the corpus and can be generated through
the compositional translation estimation process.
For those 35% or 30% translation pairs, Fig-
ure 5 compares the correct rate of the first ranked
translation pairs between with/without the do-
main/topic specific corpus. The correct rates in-
crease by 34?37% with the domain/topic specific
corpus. This result supports the claim that the do-
118
??
???
???
???
???
???
???
???
???
???
????
???
???
??
??
??
???
?
???
???
???
???
??
???
???
??
??
???
?
??
??
???
??
??
???
??
??
??
??
??
???
??
??
??
??
???
?
??
???
??
??
???
???
?
???
??
??
??
??
???
???
??
???
???
?
??
??
???
???
???
??
?
??
???
??
??
??
??
??
??
??
??
???
???
??
??
??
??
??
??
??
??
???
?
??
??
??
??
??
??
?
??????????????
???????????
(a) English to Japanese
??
???
???
???
???
???
???
???
???
???
????
???
???
??
??
??
???
?
???
???
???
???
??
???
???
??
??
???
?
??
??
???
??
??
???
??
??
??
??
??
???
??
??
??
??
???
?
??
???
??
??
???
???
?
???
??
??
??
??
???
???
??
???
???
?
??
??
???
???
???
??
?
??
???
??
??
??
??
??
??
??
??
???
???
??
??
??
??
??
??
??
??
???
?
??
??
??
??
??
??
?
??????????????
???????????
(b) Japanese to English
Figure 5: Evaluation against the Translation Pairs
whose Correct Translation Exist in the Corpus
and can be Generated Compositionally
main/topic specific corpus is effective in transla-
tion estimation of technical terms.
6 Related Works
As a related work, (Fujii and Ishikawa, 2001)
proposed a technique of compositional estima-
tion of bilingual term correspondences for the
purpose of cross-language information retrieval.
In (Fujii and Ishikawa, 2001), a bilingual con-
stituents lexicon is compiled from the translation
pairs included in an existing bilingual lexicon in
the same way as our proposed method. One of the
major differences of the technique of (Fujii and
Ishikawa, 2001) and the one proposed in this pa-
per is that in (Fujii and Ishikawa, 2001), instead of
the domain/topic specific corpus, they use a cor-
pus of the collection of the technical papers, each
of which is published by one of the 65 Japanese
associations for various technical domains. An-
other important difference is that in (Fujii and
Ishikawa, 2001), they evaluate only the perfor-
mance of cross-language information retrieval but
not that of translation estimation.
(Cao and Li, 2002) proposed a method of com-
positional translation estimation for compounds.
In the proposed method of (Cao and Li, 2002),
translation candidates of a term are composition-
ally generated by concatenating the translation
of the constituents of the term and are re-ranked
by measuring contextual similarity against the
source language term. One of the major differ-
ences of the technique of (Cao and Li, 2002) and
the one proposed in this paper is that in (Cao and
Li, 2002), they do not use the domain/topic spe-
cific corpus.
7 Conclusion
This paper proposed a method of compositional
translation estimation for technical terms using
the domain/topic specific corpus, and through
the experimental evaluation, showed that the do-
main/topic specific corpus contributes to improv-
ing the performance of compositional translation
estimation.
Future works include the followings: first, in
order to improve the proposed method with re-
spect to its coverage, for example, it is desir-
able to extend the bilingual constituents lexicons
and to introduce constituent reordering rules with
prepositions into the process of compositional
translation candidate generation. Second, we are
planning to introduce a mechanism of re-ranking
translation candidates based on the frequencies of
technical terms in the domain/topic specific cor-
pus.
References
Y. Cao and H. Li. 2002. Base noun phrase translation using
Web data and the EM algorithm. In Proc. 19th COLING,
pages 127?133.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japanese/english
cross-language information retrieval: Exploration of
query translation and transliteration. Computers and the
Humanities, 35(4):389?420.
P. Fung and L. Y. Yee. 1998. An IR approach for translating
new words from nonparallel, comparable texts. In Proc.
17th COLING and 36th ACL, pages 414?420.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge ac-
quisition. In R. Dale, H. Moisl, and H. Somers, editors,
Handbook of Natural Language Processing, chapter 24,
pages 563?610. Marcel Dekker Inc.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proc. 37th ACL, pages 519?526.
S. Sato and Y. Sasaki. 2003. Automatic collection of related
terms from the web. In Proc. 41st ACL, pages 121?124.
119
An Empirical Study on
Multiple LVCSR Model Combination by Machine Learning
Takehito Utsuro? Yasuhiro Kodama? Tomohiro Watanabe??
Hiromitsu Nishizaki?? Seiichi Nakagawa??
?Graduate School of Informatics, Kyoto University, Kyoto, 606-8501, Japan
?Sony Corporation ??Toyohashi University of Technology ??University of Yamanashi
Abstract
This paper proposes to apply machine learn-
ing techniques to the task of combining out-
puts of multiple LVCSR models. The proposed
technique has advantages over that by voting
schemes such as ROVER, especially when the
majority of participating models are not reli-
able. In this machine learning framework, as
features of machine learning, information such
as the model IDs which output the hypothe-
sized word are useful for improving the word
recognition rate. Experimental results show
that the combination results achieve a relative
word error reduction of up to 39 % against the
best performing single model and that of up to
23 % against ROVER. We further empirically
show that it performs better when LVCSR mod-
els to be combined are chosen so as to cover as
many correctly recognized words as possible,
rather than choosing models in descending or-
der of their word correct rates.
1 Introduction
Since current speech recognizers? outputs are far from
perfect and always include a certain amount of recogni-
tion errors, it is quite desirable to have an estimate of con-
fidence for each hypothesized word. This is especially
true for many practical applications of speech recogni-
tion systems such as automatic weighting of additional,
non-speech knowledge sources, keyword based speech
understanding, and recognition error rejection ? confir-
mation in spoken dialogue systems. Most of previous
works on confidence measures (e.g., (Kemp and Schaaf,
1997) ) are based on features available in a single LVCSR
model. However, it is well known that a voting scheme
such as ROVER (Recognizer output voting error reduc-
tion) for combining multiple speech recognizers? outputs
can achieve word error reduction (Fiscus, 1997; Ever-
mann and Woodland, 2000). Considering the success of
a simple voting scheme such as ROVER, it also seems
quite possible to improve reliability of previously stud-
ied features for confidence measures by simply exploit-
ing more than one speech recognizers? outputs. From this
observation, we experimentally evaluated the agreement
among the outputs of multiple Japanese LVCSR models,
with respect to whether it is effective as an estimate of
confidence for each hypothesized word.
Our previous study reported that the agreement be-
tween the outputs with two different acoustic models can
achieve quite reliable confidence, and also showed that
the proposed measure of confidence outperforms previ-
ously studied features for confidence measures such as
the acoustic stability and the hypothesis density (Kemp
and Schaaf, 1997). We also reported evaluation results
with 26 distinct acoustic models and identified the fea-
tures of acoustic models most effective in achieving high
confidence (Utsuro et al, 2002). The most remarkable
results are as follows: for the newspaper sentence ut-
terances, nearly 99% precision is achieved by decreas-
ing 94% word correct rate of the best performing single
model by only 7%. For the broadcast news speech, nearly
95% precision is achieved by decreasing 72% word cor-
rect rate of the best performing single model by only 8%.
Based on those results of our previous studies, this pa-
per proposes to apply machine learning techniques to the
task of combining outputs of multiple LVCSR models.
As a machine learning technique, the Support Vector Ma-
chine (SVM) (Vapnik, 1995) learning technique is em-
ployed. A Support Vector Machine is trained for choos-
ing the most confident one among several hypothesized
words, where, as features of SVM learning, information
such as the model IDs which output the hypothesized
word, its part-of-speech, and the number of syllables are
useful for improving the word recognition rate.
Model combination by high performance machine
learning techniques such as SVM learning has advantages
over that by voting schemes such as ROVER and oth-
ers (Fiscus, 1997; Evermann and Woodland, 2000), espe-
cially when the majority of participating models are not
reliable. In the model combination techniques based on
voting schemes, outputs of multiple LVCSR models are
combined according to simple majority vote or weighted
majority vote based on confidence of each hypothesized
word such as its likelihood. The results of model com-
bination by those voting techniques can be harmed when
the majority of participating models have quite low per-
formance and output word recognition errors with high
confidence. On the other hand, in the model combination
by high performance machine learning techniques such
as SVM learning, among those participating models, re-
liable ones and unreliable ones are easily discriminated
through the training process of machine learning frame-
work. Furthermore, depending on the features of hypoth-
esized words such as its part-of-speech and the number
of syllables, outputs of multiple models are combined in
an optimal fashion so as to minimize word recognition
errors in the combination results.
Experimental results show that model combination by
SVM achieves the followings: i.e., for the newspaper sen-
tence utterances, a relative word error reduction of 39 %
against the best performing single model and that of 23
% against ROVER; for the broadcast news speech, a rel-
ative word error reduction of 13 % against the best per-
forming single model and that of 8 % against ROVER.
We further empirically show that it performs better when
LVCSR models to be combined are chosen so as to cover
as many correctly recognized words as possible, rather
than choosing models in descending order of their word
correct rates1.
2 Specification of Japanese LVCSR
Systems
2.1 Decoders
As decoders of Japanese LVCSR systems, we use the one
named Julius, which is provided by IPA Japanese dicta-
tion free software project (Kawahara and others, 1998),
as well as the one named SPOJUS (Kai et al, 1998),
which has been developed in Nakagawa lab., Toyohashi
Univ. of Tech., Japan. Both decoders are composed of
two decoding passes, where the first pass uses the word
bigram, and the second pass uses the word trigram.
2.2 Acoustic Models
The acoustic models of Japanese LVCSR systems are
based on Gaussian mixture HMM. We evaluate phoneme-
based HMMs as well as syllable-based HMMs.
2.2.1 Acoustic Models with the Decoder JULIUS
As the acoustic models used with the decoder Julius,
we evaluate phoneme-based HMMs as well as syllable-
based HMMs. The following four types of HMMs are
evaluated: i) triphone model, ii) phonetic tied mixture
1Compared with our previous report (Utsuro et al, 2003),
the major achievement of the paper is this empirical result.
Utsuro et al (2003) examined the correlation between each
word?s confidence and the word?s features, and then introduced
the framework of combining outputs of multiple LVCSR mod-
els by SVM learning.
(PTM) triphone model, iii) monophone model, and iv)
syllable model. Every HMM phoneme model is gender-
dependent (male). For each of the four models above,
we evaluate both HMMs with and without the short pause
state, which amount to 8 acoustic models in total.
2.2.2 Acoustic Models with the Decoder SPOJUS
The acoustic models used with the decoder SPOJUS are
based on syllable HMMs, which have been developed
in Nakagawa laboratory, Toyohashi University of Tech-
nology, Japan (Nakagawa and Yamamoto, 1996). The
acoustic models are gender-dependent (male) syllable
unit HMMs. Among various combinations of features of
acoustic models2, we carefully choose 9 acoustic models
so that they include the best performing ones as well as
a sufficient number of minimal pairs which have differ-
ence in only one feature. Then, for each of the 9 models,
we evaluate both HMMs with and without the short pause
states, which amount to 18 acoustic models in total.
2.3 Language Models
As the language models, the following two types of word
bigram / trigram language models for 20k vocabulary
size are evaluated: 1) the one trained using 45 months
Mainichi newspaper articles, 2) the one trained using 5
years Japanese NHK (Japan Broadcasting Corporation)
broadcast news scripts (about 120,000 sentences).
2.4 Evaluation Data Sets
The evaluation data sets consist of newspaper sentence
utterances, which are relatively easier for speech recog-
nizers, and rather harder broadcast news speech: 1) 100
newspaper sentence utterances from 10 male speakers
consisting of 1,565 words, selected by IPA Japanese dic-
tation free software project (Kawahara and others, 1998)
from the JNAS (Japanese Newspaper Article Sentences)
speech data (Itou and others, 1998), 2) 175 Japanese
NHK broadcast news (June 1st, 1996) speech sentences
consisting of 6,813 words, uttered by 14 male speakers
(six announcers and eight reporters).
2.5 Word Recognition Rates
Word correct and accuracy rates of the individual LVCSR
models for the above two evaluation data sets are mea-
sured, where for the recognition of the newspaper sen-
tence utterances, the language model used is the one
trained using newspaper articles, and for the recognition
of the broadcast news speech, the language model used
is the one trained using broadcast news scripts. Word
recognition rates for the above two evaluation data sets
are summarized as below:
2Sampling frequencies, frame shift lengths, feature param-
eters, covariance matrices, and self loop transition / duration
control.
(a) Newspaper Sentence
(b) Broadcast News
Figure 1: Comparison among Combination by SVM /
(Weighted) Majority Votes / Individual Models
newspaper sentence utterances
decoder word correct (%) word accuracy (%)
Julius 93.0(max) to 72.7(min) 90.4(max) to 69.4(min)
SPOJUS 90.2(max) to 78.1(min) 85.3(max) to 51.0(min)
broadcast news speech
decoder word correct (%) word accuracy (%)
Julius 71.7(max) to 49.0(min) 68.8(max) to 39.7(min)
SPOJUS 70.7(max) to 55.4(min) 62.8(max) to 36.2(min)
3 Combining Outputs of Multiple LVCSR
Models by SVM
This section describes the results of applying SVM learn-
ing technique to the task of combining outputs of multiple
LVCSR models considering the confidence of each word.
We divide each of the data sets described in Section 2.4
into two halves3, where one half is used for training and
the other half for testing. A Support Vector Machine
is trained for choosing the most confident one among
several hypothesized words from the outputs of the 26
LVCSR models4. As features of the SVM learning, we
use the model IDs which output the word, the part-of-
speech of the word, and the number of syllables 5. As
3It is guaranteed that the two halves do not share speakers.
4We used SVM light (http://svmlight.joachims.
org/) as a tool for SVM learning. We compared linear and
quadratic kernels and the linear kernel performs better.
5Contribution of the parts-of-speech and the numbers of syl-
lables was slight. We also evaluated the effect of acoustic and
(a) Newspaper Sentence
(b) Broadcast News
Figure 2: Comparing Methods for Combining Outputs of
n (3 ? n ? 26) Models
classes of the SVM learning, we use whether each hy-
pothesized word is correct or incorrect. Since Support
Vector Machines are binary classifiers, we regard the dis-
tance from the separating hyperplane to each hypothe-
sized word as the word?s confidence. The outputs of the
26 LVCSR models are aligned by Dynamic Time Warp-
ing, and the most confident one among those competing
hypothesized words is chosen as the result of model com-
bination. We also require the confidence of hypothesized
words to be higher than a certain threshold, and choose
the ones with the confidence above this threshold as the
result of model combination.
The results of the performance evaluation against the
test data are shown in Figure 1. All the results in Fig-
ure 1 are the best performing ones among those for com-
bining outputs of n (3 ? n ? 26) models. The results
of model combination by SVM are indicated as ?SVM?.
As a baseline performance, that of the best performing
single model with respect to word correct rate (?Individ-
ual Model with Max Cor?) is shown. (Note that their
word recognition rates are those for the half of the whole
data set, and thus different from those in Section 2.5.)
For both speech data, model combination by SVM sig-
language scores of each hypothesized word as features of SVM,
where their contribution to improving the overall performance
was very little.
(a) Newspaper Sentence
(b) Broadcast News
Figure 3: Comparison between Maximizing Recall of
Union / Descending Order of Word Correct Rates
nificantly outperforms the best performing single model.
In terms of word accuracy rate, relative word error re-
duction are 39 % for the newspaper sentence utterances
and 13 % for the broadcast news speech. Figure 1 also
shows the performance of ROVER (Fiscus, 1997) as an-
other baseline, where ?Majority Vote? shows the perfor-
mance of the strategy of outputting no word at a tie, while
?Weighted Majority Vote? shows the performance when,
for each individual model, word correct rate for each sen-
tence is estimated and used as the weight of hypothesized
words. Model combination by SVM mostly outperforms
ROVER for both speech data. In terms of word accuracy
rate, relative word error rate reduction are 23 % for the
newspaper sentence utterances and 8 % for the broadcast
news speech6.
Figure 2 plots the changes of word accuracy rates
against the increasing number of models which partici-
pate in LVCSR model combination. Here, LVCSR mod-
els to be combined are chosen so as to cover as many cor-
rectly recognized words as possible, rather than choosing
models in descending order of their word correct rates.
(As we show later, the former outperforms the latter.) It
6Remarkable improvements are achieved especially in word
accuracy rates. This is due to the strategy of requiring the confi-
dence of hypothesized words to be higher than a certain thresh-
old, where insertion error words tend to be discarded.
is quite clear from this result that the difference of model
combination by SVM and (weighted) majority votes be-
comes much larger as more and more models participate
in model combination. This is because the majority of
participating models become unreliable in the second half
of the curves in Figure 2.
Figure 3 compares the model selection procedures, i.e.,
choosing models so as to cover as many correctly recog-
nized words as possible (indicated as ?Maximizing Recall
of Union?), and choosing models in descending order of
their word correct rates (indicated as ?Descending Order
of Word Correct Rates?). The former performs better in
the first half of the curves. This result indicates that, even
if recognition error words increase in the outputs of mod-
els participating in LVCSR model combination, it is bet-
ter to cover as many correctly recognized words as pos-
sible. This is because, in the model combination by high
performance machine learning techniques such as SVM
learning, reliable and unreliable hypothesized words are
easily discriminated through the training process.
4 Concluding Remarks
This paper proposed to apply the SVM learning technique
to the task of combining outputs of multiple LVCSR
models. The proposed technique has advantages over that
by voting schemes such as ROVER, especially when the
majority of participating models are not reliable.
References
G. Evermann and P. Woodland. 2000. Posterior probability
decoding, confidence estimation and system combination. In
Proc. NIST Speech Transcription Workshop.
J. G. Fiscus. 1997. A post-processing system to yield reduced
word error rates: Recognizer output voting error reduction
(ROVER). In Proc. ASRU, pages 347?354.
K. Itou et al 1998. The design of the newspaper-based
Japanese large vocabulary continuous speech recognition
corpus. In Proc. 5th ICSLP, pages 3261?3264.
A. Kai, Y. Hirose, and S. Nakagawa. 1998. Dealing with out-
of-vocabulary words and speech disfluencies in an n-gram
based speech understanding system. In Proc. 5th ICSLP,
pages 2427?2430.
T. Kawahara et al 1998. Sharable software repository for
Japanese large vocabulary continuous speech recognition. In
Proc. 5th ICSLP, pages 3257?3260.
T. Kemp and T. Schaaf. 1997. Estimating confidence using
word lattices. In Proc. 5th Eurospeech, pages 827?830.
S. Nakagawa and K. Yamamoto. 1996. Evaluation of segmen-
tal unit input HMM. In Proc. 21st ICASSP, pages 439?442.
T. Utsuro, T. Harada, H. Nishizaki, and S. Nakagawa. 2002.
A confidence measure based on agreement among multiple
LVCSR models ? correlation between pair of acoustic mod-
els and confidence ?. In Proc. 7th ICSLP, pages 701?704.
T. Utsuro, Y. Kodama, T. Watanabe, H. Nishizaki, and S. Nak-
agawa. 2003. Confidence of agreement among multiple
LVCSR models and model combination by SVM. In Proc.
28th ICASSP, volume I, pages 16?19.
V. N. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer-Verlag.
Answer Validation by Keyword Association
Masatsugu Tonoike, Takehito Utsuro and Satoshi Sato
Graduate school of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku 606-8501 Kyoto, JAPAN
{tonoike,utsuro,sato}@pine.kuee.kyoto-u.ac.jp
Abstract
Answer validation is a component of question
answering system, which selects reliable answer
from answer candidates extracted by certain
methods. In this paper, we propose an approach
of answer validation based on the strengths of
lexical association between the keywords ex-
tracted from a question sentence and each an-
swer candidate. The proposed answer valida-
tion process is decomposed into two steps: the
first is to extract appropriate keywords from a
question sentence using word features and the
strength of lexical association, while the second
is to estimate the strength of the association
between the keywords and an answer candidate
based on the hits of search engines. In the re-
sult of experimental evaluation, we show that a
good proportion (79%) of a multiple-choice quiz
?Who wants to be a millionaire? can be solved
by the proposed method.
1 Introduction
The technology of searching for the answer of
a question written in natural language is called
?Question Answering?(QA), and has gotten a
lot of attention recently. Research activities of
QA have been promoted through competitions
such as TREC QA Track (Voorhees, 2004) and
NTCIR QAC (Fukumoto et al, 2004). Ques-
tion answering systems can be decomposed into
two steps: first step is to collect answer can-
didates, while the second is to validate each of
those candidates. The first step of collecting an-
swer candidates has been well studied so far. Its
standard technology is as follows: first, the an-
swer type of a question, such as LOCATION or
PERSON, is identified. Then, the documents
which may contain answer candidates are re-
trieved by querying available document set with
queries generated from the question sentence.
Finally, named entities which match the answer
type of the question sentence are collected from
the retrieved documents as answer candidates.
In this paper, we focus on the second step of
how to validate an answer candidate. Several
answer validation methods have been proposed.
One of the well-known approaches is that based
on deep understanding of text (e.g. Moldovan
et al (2003)). In the approach of answer valida-
tion based on deep understanding, first a ques-
tion and the paragraph including an answer can-
didate are parsed and transformed into logical
forms. Second, the validity of the answer candi-
date is examined through logical inference. One
drawback of this approach is that it requires a
rich set of lexical knowledge such as WordNet
and world knowledge such as the inference rule
set. Consequently, this approach is computa-
tionally expensive. In contrast, in this paper,
we propose another approach of answer vali-
dation, which is purely based on the estima-
tion of the strengths of lexical association be-
tween the keywords extracted from a question
sentence and each answer candidate. One un-
derlying motivation of this paper is to exam-
ine the effectiveness of quite low level semantic
operation such as measuring lexical association
against knowledge rich NLP tasks such as an-
swer validation of question answering. Surpris-
ingly, as we show later, given multiple-choices as
answer candidates of a question, a good propor-
tion of a certain set of questions can be solved
by our method based on lexical association.
In our framework of answer validation by key-
word association (in the remaining of this paper,
we call the notion of the lexical association in-
troduced above as ?keyword association?), the
answer validation process is decomposed into
two steps: the first step is to extract appro-
priate keywords from a question sentence, while
the second step is to estimate the strength of
the association between the keywords and an
answer candidate. We propose two methods for
the keyword selection step: one is by a small
number of hand-crafted rules for determining
word weights based on word features, while the
other is based on search engine hits. In the sec-
ond step of how to validate an answer candidate,
the web is used as a knowledge base for estimat-
ing the strength of the association between the
extracted keywords and an answer candidate.
Its basic idea is as follows: the stronger the as-
sociation between the keywords and an answer
candidate, the more frequently they co-occur on
the web. In this paper, we introduce several
measures for estimating the strength of the as-
sociation, and show their effectiveness through
experimental evaluation.
In this paper, in order to concentrate on the
issue of answer validation, but not the whole QA
processes, we use an existing multiple-choice
quiz as the material for our study. The multiple-
choice quiz we used is taken from ?Who wants to
be a millionaire?. ?Who wants to be a million-
aire? is a famous TV show, which originated in
the United Kingdom and has been localized in
more than fifty countries. We used the Japanese
version, which is produced by Fuji Television
Network, Inc.. In the experimental evaluation,
about 80% of the questions of this quiz can be
solved by the proposed method of answer vali-
dation by keyword association.
Section 2 introduces the idea of question an-
swering by keyword association. Section 3 de-
scribes how to select keywords from a question
sentence. Section 4 describes how to select the
answer of multiple-choice questions. Section 5
describes how to integrate the procedures of
keyword selection and answer selection. Sec-
tion 6 presents the results of experimental eval-
uations. Section 7 compares our work with sev-
eral related works Section 8 presents our con-
clusion and future works.
2 Answer Validation by Keyword
Association
2.1 Keyword Association
Here is an example of the multiple-choice quiz.
Q1: Who is the director of ?American Graffiti??
a: George Lucas
b: Steven Spielberg
c: Francis Ford Coppola
d: Akira Kurosawa
Suppose that you do not know the correct an-
swer and try to find it using a search engine
on the Web. The simplest way is to input the
query ?American Graffiti? to the search engine
and skim the retrieved pages. This strategy as-
sumes that the correct answer may appear on
the page that includes the keyword ?American
Graffiti?. A little cleverer way is to consider the
number of pages that contain both the keyword
and a choice. This number can be estimated
Table 1: Hits of Keywords and the Choices for
the Question Q1 (X:?American Graffiti?)
Y (choice) hits(X and Y )
?George Lucas? 15,500
?Steven Spielberg? 5,220
?Francis Ford Coppola? 4,800
?Akira Kurosawa? 836
from the hits of a search engine when you in-
put a conjunct query ?American Graffiti? and
?George Lucas?. Based on this assumption, it is
reasonable to hypothesize that the choice which
has the largest hits is the answer. For the above
question Q1, this strategy works. Table 1 shows
the hits of the conjunct queries for each of the
choices. We used ?google1? as a search engine.
Here, let X be the set of keywords, Y be the
choice. Function hits is defined as follows.
hits(X) ? hits(x
1
AND x
2
AND ? ? ?AND x
n
)
where
X = {x
1
, x
2
, . . . , x
n
}
The conjunct query with ?George Lucas?, which
is the correct answer, returns the largest hits.
Here, the question Q1 can be regarded as a
question on the strength of association between
keyword and an choice, and converted into the
following form.
Q1?: Select the one that has the strongest asso-
ciation with ?American Graffiti?.
a: George Lucas
b: Steven Spielberg
c: Francis Ford Coppola
d: Akira Kurosawa
We call this association between the keyword
and the choice as keyword association.
2.2 How to Select Keywords
It is important to select appropriate keywords
from a question sentence. Consider the follow-
ing question.
Q2: Who is the original author of the famous
movie ?Lord of the Rings??
a: Elijah Wood
b: JRR Tolkien
c: Peter Jackson
d: Liv Tyler
The numbers of hits are shown in Table 2. Here,
let X be ?Lord of the Rings?, X ? be ?Lord of the
1http://www.google.com
Table 2: Hits of Keywords and the Choices
for the Question Q2 (X:?Lord of the Rings?,
X ?:?Lord of the Rings? and ?original author?)
Y (choice) hits hits
(X and Y ) (X ? and Y )
?Elijah Wood? 682,000 213
?JRR Tolkien? 652,000 702
?Peter Jackson? 1,140,000 340
?Liv Tyler? 545,000 106
Rings? and ?original author?. When you select
the title of this movie ?Lord of the Rings? as a
keyword, the choice with the maximum hits is
?Peter Jackson?, which is not the correct an-
swer ?JRR Tolkien?. However, if you select
?Lord of the Rings? and ?original author? as
keywords, this question can be solved by select-
ing the choice with maximum hits. Therefore,
it is clear from this example that how to select
appropriate keywords is important.
2.3 Forward and Backward Association
For certain questions, it is not enough to gen-
erate a conjunct query consisting of some key-
words and a choice, and then to simply select
the choice with maximum hits. This section in-
troduces more sophisticated measures for select-
ing an appropriate answer. Consider the follow-
ing question.
Q3: Where is Pyramid?
a: Canada
b: Egypt
c: Japan
d: China
The numbers of hits are shown in Table 3. In
this case, given a conjunct query consisting of
a keyword ?Pyramid? and a choice, the choice
with the maximum hits, i.e., ?Canada? is not
the correct answer ?Egypt?. Why could not this
question be solved? Let us consider the hits of
the choices alone. The hits of the atomic query
?Canada? is about seven times larger than the
hits of the atomic query ?Egypt?. With this ob-
servation, we can hypothesize that the hits of a
conjunct query ?Pyramid? and a choice are af-
fected by the hits of the choice alone. Therefore
some normalization might be required.
Based on the analysis above, we employ the
metrics proposed by Sato and Sasaki (2003).
Sato and Sasaki (2003) has proposed two met-
rics for evaluating the strength of the relation
of two terms. Suppose that X be the set of
keywords and Y be the choice. In this paper,
we call the hits of a conjunct query consisting
of keywords X and a choice Y , which is nor-
malized by the hits of X, as forward association
FA(X, Y ). We also call the hits of a conjunct
query X and Y , which is normalized by the hits
of Y , as backward association BA(X, Y ).
FA(X, Y ) = hits(X ? {Y })/hits(X)
BA(X, Y ) = hits(X ? {Y })/hits({Y })
Note that when X is fixed, FA(X, Y ) is propor-
tional to hits(X ? {Y }).
Let?s go back to Q3. In this case, the choice
with the maximum BA is correct. Some ques-
tions may solved by referring to FA, while oth-
ers may be solved only by referring to BA.
Therefore, it is inevitable to invent a mecha-
nism which switches between FA and BA.
2.4 Summary
Based on the observation of Sections 2.1 ? 2.3,
the following three questions must be addressed
by answer validation based on keyword associ-
ation.
? How to select appropriate keywords from a
question sentence.
? How to identify the correct answer consid-
ering forward and/or backward association.
? How many questions can be solved by this
strategy based on keyword association.
3 Keyword Selection
This section describes two methods for selecting
appropriate keywords from a question sentence:
one is based on the features of each word, the
other based on hits of a search engine.
First, all the nouns are extracted from the
question sentence using a Japanese morpholog-
ical analyzer JUMAN(Kurohashi and Nagao,
1999) and a Japanese parser KNP(Kurohashi,
1998). Here, when the sequence of nouns con-
stitute a compound, only the longest compound
is extracted and their constituent nouns are not
extracted. Let N denote the set of those ex-
tracted nouns and compounds, from which key-
words are selected. In the following, the search
engine ?goo2? is used for obtaining the number
of hits.
2http://www.goo.ne.jp
Table 3: Hits of Keywords and the Choices for the Question Q3
X(keyword) hits(X)
Pyramid 3,170,000
Y(choice) hits(Y ) hits(Y and X) FA(X , Y ) BA(X , Y )
Canada 100,000,000 334,000 0.105 0.00334
Egypt 14,500,000 325,000 0.103 0.0224
Japan 63,100,000 246,000 0.0776 0.00390
China 53,600,000 225,000 0.0710 0.00420
3.1 Keyword Selection Based on Word
Features
In this method, keywords are selected by the
following procedure:
1. If the question sentence contains n quota-
tions with quotation marks ??? and ???,
those n quoted strings are selected as key-
words.
2. Otherwise:
2-1. According to the rules for word
weights in Table 4, weights are as-
signed to each element of the keyword
candidate set N .
2-2. Select the keyword candidate with the
maximum weight and that with the
second maximum weight.
2-3. i. If the hits of AND search of those
two keyword candidates are 15 or
more, both are selected as key-
words.
ii. Otherwise, select the one with the
maximum weight.
Let k denote the set of the selected keywords
(k ? N), we examine the correctness of k as
follows. Let c denote a choice, cF A
1
(k) the
choice with the maximum FA(k, c), and cBA
1
(k)
the choice with the maximum BA(k, c), respec-
tively.
cF A
1
(k) = argmax
c
FA(k, c)
c
BA
1
(k) = argmax
c
BA(k, c)
Here, we regard the selected keywords k to be
correct if either cF A
1
(k) or cBA
1
(k) is correct.
Against the development set which is to be in-
troduced in Section 6.1, the correct rate of the
keywords selected by the procedure above is
84.5%.
Table 4: Rules for Word Weights
rule weight
n-th segment (1 +
0.01 ? n)
stopword 0
quoted by quotation marks?? 3
person name 3
verbal nouns (?sahen?-verb stem) 0.5
word which expresses relation 2
Katakana 2
name of an award 2
name of an era 0.5
name of a country 0.5
number 3
hits > 1000000
and consists of one character 0.9
marked by a topic maker and
name of a job 0.1
hits > 100000 0.2
hits < 10000 1.1
number of characters = 1 0.2
number of characters = 2 0.25
number of characters = 3 0.5
number of characters = 4 1.1
number of characters ? 5 1.2
3.2 Keyword Selection Based on Hits
of Search Engine
3.2.1 Basic Methods
First, we introduce several basic methods for
selecting keywords based on hits of a search en-
gine. Let 2N denote the power set of N , where a
set of keywords k is an element of 2N (k ? 2N ).
Let k? denote the selected set of keywords and c?
the selected choice.
The first method is to simply select the pair
of ?k?, c?? which gives the maximum hits as below:
?k?, c?? = argmax
c, k?2
N
hits(k ? {c})
Against the development set, the correct rate of
the choice which is selected by this method is
35.7%.
In a similar way, another method which se-
lects the maximum FA or BA can be given as
below:
?k?, c?? = argmax
c, k?2
N
FA(k ? {c})
?k?, c?? = argmax
c, k?2
N
BA(k ? {c})
Their correct rates are 71.3% and 36.1%, respec-
tively.
3.2.2 Keyword Association Ratio
Next, we introduce more sophisticated meth-
ods which use the ratio of maximum and sec-
ond maximum associations such as FA or BA.
The underlying assumption of those methods
are that: the greater those ratios are, the more
reliable is the selected choice with the maximum
FA/BA. First, we introduce two methods: FA
ratio and BA ratio.
FA ratio This is the ratio of FA of the choice
with second maximum FA over one with maxi-
mum FA. FA ratio is calculated by the follow-
ing procedure.
1. Select the choices with maximum FA and
second maximum FA.
2. Estimate the correctness of the choice with
maximum FA by the ratio of their FAs.
The set k? of keywords and the choice c? to be
selected by FA ratio are expressed as below:
k? = argmin
k?2
N
FA(k, cF A
2
(k))
FA(k, cF A
1
(k))
c? = cF A
1
(k?)
c
F A
2
(k) = arg-secondmax
c
FA(k, c)
where arg-secondmax
c
is defined as a function
which selects c with second maximum value.
Similarly, the method based on BA ratio is
given as below:
BA ratio
k? = argmin
k?2
N
BA(k, cBA
2
(k))
BA(k, cBA
1
(k))
c? = cBA
1
(k?)
c
BA
2
(k) = arg-secondmax
c
BA(k, c)
Unlike the methods based on FA ratio and
BA ratio, the following two methods consider
both FA and BA. The motivation of those two
methods is to regard the decision by FA and
BA to be reliable if FA and BA agree on se-
lecting the choice.
Table 5: Evaluation of Keyword Association
Ratios (precision/coverage)(%)
max and second max
FA BA
ratio
FA 63.1/100 70.6/95.0
BA 75.8/93.2 67.6/100
BA ratio with maximum and second max-
imum FA
k? = argmin
k?2
N
BA(k, cF A
2
(k))
BA(k, cF A
1
(k))
c? = cF A
1
(k?)
FA ratio with maximum and second max-
imum BA
k? = argmin
k?2
N
FA(k, cBA
2
(k))
FA(k, cBA
1
(k))
c? = cBA
1
(k?)
Coverages and precisions of these four methods
against the development set are shown in Ta-
ble 5. Coverage is measured as the rate of ques-
tions for which the ratio is less than or equal to
13. Precisions are measured as the rate of ques-
tions for which the selected choice c? is the cor-
rect answer, over those covered questions. The
method having the greatest precision is BA ra-
tio with maximum and second maximum FA.
In the following sections, we use this ratio as
the keyword association ratio. Table 6 farther
examines the correlation of the range of the ra-
tio and the coverage/precision. When the ratio
is less than or equal to 0.25, about 60% of the
questions are solved with the precision close to
90%. This threshold of 0.25 is used in the Sec-
tion 5 when integrating the keyword association
ratio and word weights.
4 Answer Selection
In this section, we explain a method to identify
the correct answer considering forward and/or
backward association. After selecting keywords,
the following numbers are obtained by a search
engine.
? Hits of the keywords X: hits(X)
? Hits of the choice Y : hits({Y })
3For the ratios considering both FA and BA, the
ratio greater than 1 means that FA and BA disagree on
selecting the choice.
Table 6: Evaluation of Keyword Association
Ratio: BA ratio of FA max and second-max
ratio
# of questions
coverage precision
0 18.9% (163/888) 89.6% (146/163)
? 0.01 21.5% (191/888) 89.5% (171/191)
? 0.1 40.5% (360/888) 87.5% (315/360)
? 0.25 60.4% (536/888) 86.9% (466/536)
? 0.5 78.0% (693/888) 81.6% (566/693)
? 0.75 87.2% (774/888) 78.4% (607/774)
? 1 93.2% (828/888) 75.8% (628/828)
? Hits of the conjunct query:
hits(X ? {Y })
Then for each choice Y , FA and BA are cal-
culated. As introduced in section 3, cF A
1
(k) de-
notes the choice whose FA value is highest, and
cBA
1
(k) the choice whose BA value is highest.
What has to done here is to decide which of
cF A
1
(k) and cBA
1
(k) is correct.
After manually analyzing the search engine
hits against the development set, we hand-
crafted the following rules for switching between
cF A
1
(k) and cBA
1
(k).
1. if cF A
1
(k) = cBA
1
(k) then cF A
1
(k)
2. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.8 then cBA
1
(k)
3. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.2 then cF A
1
(k)
4. else if BA(k,c
FA
1
(k))
BA(k,c
BA
1
(k))
? 0.53 then cF A
1
(k)
5. else if hits(k) ? 1300 then cBA
1
(k)
6. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.6 then cBA
1
(k)
7. else cF A
1
(k)
Table 7 shows the results of evaluating preci-
sion of answer selection methods against the de-
velopment set, when the keywords are selected
based on word weights in Section 3.1. In the
table, in addition to the result of answer selec-
tion rules above, the results with baselines of
selecting the choice with maximum FA or BA
are also shown. It is clear from the table that
the answer selection rules described here signif-
icantly outperforms those baselines.
For each of the answer selection rules, Ta-
ble 8 shows its precision. In the development
set4, there are 541 questions (about 60%) where
4Four questions are excluded because hits of the con-
junct query hits(X ? {Y }) were 0
Table 7: Precision of Answer Selection (with
keyword selection by word weights)
method precision
max FA 70.8%
max BA 67.6%
selection rule 77.3%
Table 8: Evaluation of Each Answer Selection
Rule (with keyword selection by word weights)
rule answer precision
1 cFA
1
(k) = cBA
1
(k) 88.5% (479/541)
2 ? 6 - 60.3% (207/343)
total - 77.6% (686/884)
2 cBA
1
(k) 65.3% (32/49)
3 cFA
1
(k) 61.8% (68/110)
4 cFA
1
(k) 53.6% (37/69)
5 cBA
1
(k) 60.3% (35/58)
6 cBA
1
(k) 66.7% (12/18)
7 cFA
1
(k) 59.0% (23/39)
cF A
1
(k) and cBA
1
(k) are identical, and the 88.5%
of the selected choices are correct. This re-
sult shows that more than half of the questions
cF A
1
(k) is equal to cBA
1
(k) and about 90% of
these questions can be solved. This result shows
that whether FA and BA agree or not is very
important and is crucial for reliably selecting
the answer.
5 Total Procedure of Keyword
Selection and Answer Selection
Finally, the procedures of keyword selection and
answer selection presented in the previous sec-
tions are integrated as given below:
1. If ratio ? 0.25:
Use the set of keywords selected by BA ra-
tio with maximum and second maximum
FA. The choice to be selected is the one
with maximum BA.
2. Otherwise:
Use the set of keywords selected by word
weights. Answer selection is done by the
procedure of Section4.
6 Evaluation
6.1 Data Set
In this research, we used the card game ver-
sion of ?????????? (Who wants to be a
millionaire)?, which is sold by Tomy Company,
LTD. It has 1960 questions, which are classi-
fied into fifteen levels according to the amount
of prize money. Each question has four choices.
All questions are written in Japanese. The fol-
lowings give a few examples.
10,000 yen level
[A39]???????????????
??????
(Which continent are Egypt and
Kenya located in?)
A. ?????? (Africa)
B. ??????? (Eurasia)
C. ??????? (North America)
D. ??????? (South America)
[Correct Answer: ??????]
1,000,000 yen level
[J39] ???????????????
?????????????????
(What is the name of the ship in which
Columbus was sailing when he discov-
ered a new continent?)
A. ???????? (Atlantis)
B. ???? (Argo)
C. ??????? (Santa Maria)
D. ?????? (Nautilus)
[Correct Answer: ???????]
10,000,000 yen level
[O4] ???????????????
?????????????????
??
(In which summer Olympics did the
number of participating countries first
exceed 100?)
A. ????? (Rome Olympics)
B. ???? (Tokyo Olympics)
C. ?????? (Mexico Olympics)
D. ??????? (Munich Olympics)
[Correct Answer: ??????]
We divide questions of each level into two
halves: first of which is used as the develop-
ment set and the second as the test set. We
exclude questions with superlative expressions
(e.g., Out of the following four countries, select
the one with the maximum number of states.)
or negation (e.g., Out of the following four col-
ors, which is not used in the national flag of
France.) because they are not suitable for solv-
ing by keyword association. Consequently, the
development set comprises 888 questions, while
the test set comprises 906 questions. The num-
ber of questions per prize money amount is
shown in Table 9.
Table 9: The number of questions per prize
money amount
prize money amount # of questions
(yen) full dev test
10,000 160 71 74
20,000 160 71 77
30,000 160 67 70
50,000 160 75 71
100,000 160 73 73
150,000 160 76 72
250,000 160 71 77
500,000 160 74 77
750,000 160 78 71
1,000,000 160 73 76
1,500,000 120 53 58
2,500,000 90 38 42
5,000,000 70 30 32
7,500,000 50 24 21
10,000,000 30 14 15
total 1960 888 906
We compare the questions of ?Who wants to
be a millionaire? with those of TREC 2003 QA
track and those of NTCIR4 QAC2 task. The
questions of ?Who wants to be a millionaire?
are all classified as factoid question. They cor-
respond to TREC 2003 QA track factoid com-
ponent. The questions of NTCIR4 QAC2 are
also all classified as factoid question. We com-
pare bunsetsu 5 count of the questions of ?Who
wants to be a millionaire? with word count of
the questions of TREC 2003 QA track factoid
component and bunsetsu count of the questions
of NTCIR4 QAC2 Subtask1. The questions
of ?Who wants to be a millionaire? consist of
7.24 bunsetsu on average, while those of TREC
2003 QA track factoid component consist of 7.76
words on average, and those of NTCIR4 QAC2
Subtask1 consist of 6.19 bunsetsu on average.
Therefore, it can be concluded that the ques-
tions of ?Who wants to be a millionaire? are
not shorter than those of TREC 2003 QA track
and those of NTCIR4 QAC2 task.
6.2 Results
Against the development and the test sets,
Table 10 shows the results of evaluating the
total procedure of keyword selection and an-
swer selection presented in Section 5. The ta-
ble also shows the performance of baselines:
5A bunsetsu is one of the linguistic units in Japanese.
A bunsetsu consists of one content word possibly fol-
lowed by one or more function words.
Table 10: Total Evaluation Results (preci-
sion/coverage)(%)
method dev test
K.A.R. (r ? 1) 75.8/93.2 74.6/93.6
word weights
77.3/100 73.4/100
+ answer selection
Integration 78.6/100 75.9/100
K.A.R. (r ? 0.25) 86.9/60.4 86.0/61.5
word weights (r > 0.25)
65.9/39.6 59.9/38.5
+ answer selection
K.A.R.: keyword association ratio
?
??
??
??
??
??
??
??
??
??
???
??
???
?
??
???
?
??
???
?
??
???
?
??
???
??
??
???
??
??
???
??
??
???
??
??
???
??
???
??
???
?
???
??
???
?
???
??
???
?
???
??
???
?
???
??
???
?
??
???
???
??
????????????????????????
??
??
??
??
??
??
?
Figure 1: Precision classified by prize money
amount
i.e., keyword association ratio presented in Sec-
tion 3.2.2, and word weights of Section 3.1 +
answer selection of Section 4. Integration of
keyword association ratio and word weight out-
performs those baselines. In total, about 79%
(for the development set) and 76% (for the test
set) of the questions are solved by the proposed
answer validation method based on keyword as-
sociation.
Comparing the performance of the two data
sets, word weights + answer selection has 4%
lower precision in the test set. This result indi-
cates that rules for word weights as well as an-
swer selection rules overfit to the development
set. On the other hand, the difference of the pre-
cisions of the keyword association ratio is much
less between the two data sets, indicating that
keyword association ratio has less overfit to the
development set.
Finally, the result of the experiment where
the development set was solved by the inte-
gration method was classified by prize money
amount. The result is shown in Figure 1. The
more the prize money amount is, the lower the
precision seems to be, while their precisions are
all above 60%, and their differences are less than
20% in most cases. It can be concluded that our
system can solve questions of all the levels al-
most equally.
7 Related Work
Kwok et al (2001) proposed the first automated
question-answering system which uses the web.
First, it collects documents that are related to
the question sentence using google and picks an-
swer candidates up from them. Second, it se-
lects an answer based on the frequency of can-
didates which appear near the keywords.
In the method proposed by Brill et al (2002),
answer candidates are picked up from the sum-
mary pages returned by a search engine. Then,
each answer candidate is validated by searching
for relevant documents in the TREC QA docu-
ment collection. Both methods do not consider
the number of hits returned by the search en-
gine.
Magnini et al (2002) proposed an answer val-
idation method which uses the number of search
engine hits. They formulate search engine
queries using AltaVista?s OR and NEAR oper-
ators. Major difference between the method of
Magnini et al (2002) and ours is in keyword se-
lection. In the method of Magnini et al (2002),
the initial keywords are content words extracted
from a question sentence. If the hits of keywords
is less than a threshold, the least important key-
word is removed. This procedure is repeated un-
til the hits of the keywords is over the threshold.
On the other hand, in our method, keywords
are selected so that the strength of the associ-
ation between the keyword and an answer can-
didate is maximized. Intuitively, our method of
keyword selection is more natural than that of
Magnini et al (2002), since it considers both
the question sentence and an answer candidate.
As for measures for scoring answer candidates,
Magnini et al (2002) proposed three measures,
out of which ?Corrected Conditional Probabil-
ity? performs best. In our implementation, the
performance of ?Corrected Conditional Proba-
bility? is about 5% lower than our best result.
8 Conclusion and Future Work
In this paper, we proposed an approach of an-
swer validation based on the strengths of lexi-
cal association between the keywords extracted
from a question sentence and each answer can-
didate. The proposed answer validation process
is decomposed into two steps: the first is to
extract appropriate keywords from a question
sentence using word features and the strength
of lexical association, while the second is to es-
timate the strength of the association between
the keywords and an answer candidate based on
the hits of search engines. In the result of exper-
imental evaluation, we showed that a good pro-
portion (79%) of the multiple-choice quiz ?Who
wants to be a millionaire? can be solved by the
proposed method.
Future works include the followings: first, we
are planning to examine whether the syntactic
structures of the question sentence is useful for
selecting appropriate keywords from the ques-
tion sentence. Secondly, it is interesting to see
whether the keyword selection method proposed
in this paper is also effective for other applica-
tions such as answer candidate collection of the
whole question answering process.
References
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2002.
Data-intensive question answering. In Proc. TREC
2001.
J. Fukumoto, T. Kato, and F. Masui. 2004. Question
answering challenge for five ranked answers and list
answers -overview of ntcir4 qac2 subtask 1 and 2-. In
Proc. 4th NTCIR Workshop Meeting.
Sadao Kurohashi and Makoto Nagao, 1999. Japanese
Morphological Analysis System JUMAN version 3.62
Manual.
Sadao Kurohashi, 1998. Japanese Dependency/Case
Structure Analyzer KNP version 2.0b6 Manual.
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In Proc. the 10th
WWW Conf., pages 150?161.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Is it the right answer? exploiting web redundancy for
answer validation. In Proc. 40th ACL, pages 425?432.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, and
F. Lacatusu. 2003. Lcc tools for question answering.
In Proc. TREC 2002.
S. Sato and Y. Sasaki. 2003. Automatic collection of
related terms from the web. In Proc. 41st ACL, pages
121?124.
E. M. Voorhees. 2004. Overview of the trec 2003 ques-
tion answering track. In Proc. TREC 2003.
Named Entity Chunking Techniques 
in Supervised Learning for Japanese Named Entity Recognition 
Manabu Sassano  
Fujitsu Laboratories, Ltd. 
4-4-1, Kamikodanaka, Nakahara-ku, 
Kawasaki 211-8588, Japan 
sassano(@ilab.fujitsu.(:o.j I) 
Takeh i to  Utsuro  
l )el)artment of Intbrlnation 
and Computer  Sciences, 
Toyohashi University of Technology 
lcnl)aku-cho, ~l~)yohashi 441-8580, Jat)an 
utsm'og))ics, rut. ac.j p 
Abst rac t  
This 1)aper focuses on the issue of named entity 
chunking in Japanese named entity recognition. 
We apply the SUl)ervised decision list lean> 
ing method to Japanese named entity recogni- 
tion. We also investigate and in(:ori)orate sev- 
eral named-entity noun phrase chunking tech.- 
niques and experimentally evaluate and con> 
t)are their l)erfornlanee, ill addition, we t)rot)ose 
a method for incorporating richer (:ontextua\] 
ilflbrmation as well as I)atterns of constituent 
morphenms within a named entity, which h~ve 
not 1)een considered ill previous research, and 
show that the t)roi)osed method outt)erfi)rms 
these t)revious ai)proa('hes. 
1 I n t roduct ion  
It is widely a.greed that named entity recog- 
nition is an imt)ort;ant ste t) ti)r various al)pli- 
(:ations of natural language 1)ro(:('.ssing such as 
intbnnation retrieval, maclfine translation, in- 
tbrmation extraction and natural language un- 
derstanding. In tile English language, the 
task of named entity recognition is one of the 
tasks of the Message Understanding Confer- 
once (MUC) (e..g., MUC-7 (19!)8)) and has 
be.on studied intensively. In the .}al)anese lan- 
guage~ several recent conferences, uch as MET 
(Multilingual Entity Task, MET-I (Maiorano, 
1996) and MET-2 (MUC, 1998)) and IREX (In- 
formation l{etriew~l and Extraction Exercise) 
Workshop (IREX Committee, 1999), focused on 
named entity recognition ms one of their con- 
test tasks, thus promoting research on Jat)anese 
named entity recognition. 
In Japanese named entity recognition, it is 
quite common to apply morphological analy- 
sis as a t)reprocessing step and to segment 
the sentence string into a sequence of mor- 
i)henles. Then, hand-crafted t)attern m~tching 
rules and/or statistical named entity recognizer 
are apt)lied to recognize named entities. It is 
ofl;en the case that named entities to be rec- 
ognized have different segmentation boundaries 
from those of morpheums obtained by the mor- 
phological analysis. For example, in our anal- 
ysis of the \]Ill,F,X workshop's training corpus of 
llallled entities, about half of the mtmed enti- 
ties have segmentation boundaries that al'e dif- 
ferellt \]'rein the result of morphological nalysis 
t)y a .\]al)anese lnorphological nalyzer BI~EAK- 
FAST (Sassano et al, 1997) (section 2). Thus, in 
.Japanese named entity recognition: among the 
most difficult problems is how to recognize such 
named entities that have segmentation bound- 
ary mismatch against he morphemes ot)tained 
l)y morphological nalysis. Furthermore, in al- 
most 90% of (:ases of those segmentation t)oulld- 
ary mismatches, named entities to l)e recognized 
can t)e (teconq)osed into several mort)heroes as 
their constituents. This means that the 1)roblem 
of recognizing named entities in those cases can 
be solved by incorporating techniques of base 
noun phrase chunking (Ramshaw and Marcus, 
1995). 
In this paper, we tbcus on the issue of named 
entity chunking in Japanese name.d entity recog- 
nition. First, we take a supervised learning ap- 
proach rather than a hand-crafted rule based 
approach, because the tbnner is nlore promis- 
ing than the latter with respect o the amomlt 
of  human labor if requires, as well as its adaI)t- 
abi l i ty to a new domain  or a new def init ion of  
named entities. In general, creating training 
data tbr supervised learning is somewhat easier 
than creating pattern matching rules by hand. 
Next, we apply Yarowsky's method tbr super- 
vised decision list learning I (Yarowsky, 1994) to 
1VVe choose tile decision list learning method as the 
705 
Table 1: Statistics of NE Types of IREX 
NE Type 
ORGANIZATION 
PERSON 
LOCATION 
ARTIFACT 
DATE 
TIME 
MONEY 
PERCENT 
Total 
frequency (%) 
Training 
3676 (19.7) 
3840 (20.6) 
5463 (29.2) 
747 (4.0) 
3567 (19.1) 
502 (2.7) 
390 (2.1) 
492 (2.6) 
18677 
Test 
361 (23.9) 
338 (22.4) 
413 (27.4) 
48 (3.2) 
260 (17.2) 
54 (3.5) 
15 (1.0) 
21 (1.4) 
1510 
Japanese named entity recognition, into which 
we incorporate several noun phrase chunking 
techniques (sections 3 and 4) and experimen- 
tally evaluate their performance on the IREX 
, workshop's training and test data (section 5). 
As one of those noun phrase chunking tech- 
niques, we propose a method for incorporating 
richer contextual information as well as patterns 
of constituent morphemes within a named en- 
tity, compared with those considered in tire pre- 
vious research (Sekine et al, 1998; Borthwick, 
1999), and show that the proposed method out- 
perlbrms these approaches. 
2 Japanese Named Ent i ty  
Recogn i t ion  
2.1 Task of the IREX Workshop 
The task of named entity recognition of the 
IREX workshop is to recognize ight named en- 
tity types in Table 1 (IREX Conmfittee, 1999). 
The organizer of the IREX workshop provided 
1,174 newspaper articles which include 18,677 
named entities as tire training data. In the for- 
mal run (general domain) of the workshop, the 
participating systems were requested to recog- 
nize 1,510 nanmd entities included in the held- 
out 71 newspaper articles. 
2.2 Segmentation Boundaries of 
Morphemes and Named Entities 
In the work presented here, we compare the seg- 
mentation boundaries of named entities in tire 
IREX workshop's training corpus with those of 
supervised learning technique mainly because it is easy 
to implement and quite straightibrward toextend a su- 
pervised lem'ning version to a milfimally supervised ver- 
sion (Collins and Singer, 1999; Cucerzan and Yarowsky, 
1999). We also reported in (Utsuro and Sassano, 2000) 
the experimental results of a minimally supervised ver- 
sion of Japanese named entity recognition. 
Table 2: Statistics of Boundary Match vs. Mis- 
lnatch of Morphemes (M) attd Named Entities 
(NE) 
Match/Misnmtch II freq. of NE Tags (%) 
1 M to 1 NE 10480 (56.1) 
n(> 2) Ms 
to 
1 NE 
n=2 
n=3 
n > 4 
4557 (24.4) 
1658 (8.9) 7175 
96o (5.1) (38.4) 
other boundary mismatch 1022 (5.5) 
Total J\[ 18677 
morphemes which were obtained through mor- 
phological analysis by a Japanese morphologi- 
cal attalyzer BREAKFAST (Sassano et al, 1997). 2 
Detailed statistics of the comparison are pro- 
vided in 'Fable 2. Nearly half of the named 
entities have bmmdary mismatches against he 
morI)hemes and also almost 90% of the named 
entities with boundary mismatches can be tie- 
composed into more than one morpheme. Fig-- 
ure 1 shows some examples of such cases, a
3 Chunk ing  and Tagging Named 
Ent i t ies  
In this section, we formalize the problem of 
named entity chunking in Japanese named en- 
tity recognition. We describe ~t novel tech- 
nique as well as those proposed in the previous 
works on nan ted entity recognition. The novel 
technique incorporates richer contextual infor- 
mation as well as p~tterns of constituent mor- 
phemes within ~ named entity, compared with 
the techniques proposed in previous research on 
named entity recognition and base noun phrase 
chunking. 
3.1 Task Definition 
First, we will provide out" definition of the task 
of Japanese named entity chunking. Suppose 
'~The set of part-of-speech tags of lllU.~AKFAST consists 
of about 300 tags. mmAKFaST achieves 99.6% part-of- 
speech accuracy against newspaper a ticles. 
aIn most cases of the "other boundary mismatch" in 
Table 2, one or more named entities have to be rec- 
ognized as a part of a correctly analyzed morpheme 
and those cases are not caused by errors of morpholog- 
ical analysis. One frequent example of this type is a 
Japanese verbal noun "hou-bei (visiting United States)" 
which consists of two characters "hou (visitin.q)" and "bet 
(United States)", where "bet (United States)" has to be 
recognized as <LOCATION>. \Ve believe that 1)ouudary 
mismatches ofthis type can be easily solved by employ- 
ink a supervised learning technique such as the decision 
list learning method. 
706 
'Dfl)le 3: Exmoding Schemes of Named Entity Chunldng States 
Named Entity Tag 
Mort)heine Sequence 
Illside/()utside Encoding 
Stmt/End Encoding 
<0RG> <LOC> <L0C> 
- . .  M I M M \] M 
0 0RG_I 0 L0C_I LOC_I LOC_I LOC_B 0 
0 0RG_U 0 LOC_S L0C_C LOC_E LOC_U 0 
V Mo i - l )hemes  to 1 Named Ent i ty \ ]  
<ORGANIZATION> 
.... Roshia gun -..  
( S<,s,~i,. 0 ( a,',,.j) 
<PERSON> 
.... Murayama IbIni ichi shushOUprimc \]"" 
(last nmne) (first name) ( minister" 
\[3 Morphemes to1 Named Entity\] 
<TIME> 
gozen ku .ii " ? 
(AM) (niuc) (o ?1ock) 
<ARTIFACT> 
hokubei .jiyuu-1)oueki kyoutei - - ? 
Norfl~ 
( America ) (flee trade.) (treaty) 
Figure 1: Ex~mq)les of B(mndary Mismatch of 
Morl)hemes mid Named Entities 
that a sequen('e of morl)hemes i given as 1)e- 
low: 
Left; l{,ight 
( Context ) (Named Entity) ( Context; )
? . .~ I ' _ '~ . . .~ IL  ~, i~ '~. . .M/ ' . . .~ , l / ,  ''~ ~1~". . .~, f / " . . .  
t 
(Current Position) 
Then, given tht~t the current t)osition is at 
the morpheme M .N1': the task of tanned elltity 
eh l l l l k i l lg  is to  ass ign  a, C\] luuki l lg  s ta te  (to })e de-  
scribed in Section 3.2) as well ~rs a nmned entity 
type to the morl)helne Mi NE at tim current po- 
sition, considering the patterns of surrounding 
morl)hemes. Note that in the SUl)ervised learn- 
ing phase we can use the (:lmnking iuibnnation 
on which morphemes constitute a ngune(l entity, 
and whi(-h morphemes are in the lefl;/right con- 
texts of tit(; named entity. 
3.2 Encoding Schemes of Named 
Ent i ty  Chunking States 
In this t)at)er, we evalu~te the following two 
s('hemes of encoding ctmnking states of nalned 
entities. EXalnples of these encoding s(:hemes 
are shown in Table 3. 
3.2.1 Ins ide/Outs ide  Encoding 
The Inside/Outside scheme of encoding chunk- 
ing states of base noun phrases was studied in 
Ibmlshaw and Marcus (1995). This scheme dis- 
tinguishes the tbllowing three states: 0 the 
word at the current position is outside any base 
holm phrase. I the word at the current po- 
sition is inside some base holm phrase. B the 
word at the current position marks the begin- 
ning of ~ base noml t)hrase that immediately fop 
lows another base noun phrase. We extend this 
scheme to named entity chunking by further dis- 
tinguishing each of the states I and B into eight 
named entity types. 4 Thus, this scheme distin- 
guishes 2 x 8 + 1 = 17 states. 
3.2.2 S tar t /End  Encoding 
The Start /End scheme of encoding clmnking 
states of nmned entities was employed in Sekine 
e,t al. (1998) and Borthwick (1999). This 
scheme distinguishes the, following four states 
for each named entity type: S the lllOlTt)\]lellle 
at the (:urreld; position nmrks the l)eginldng of a 
lUl.in(xt (;lltity consisting of more than one mor- 
1)\]mme. C l;he lnOrl)heme ~I; the cm'r(mt )osi -
tion marks the middle of a mmmd entity (:onsist- 
ing of more tlmn one lilOrt)hellle. E -- the illOf 
t)heme, at the current position ram:ks the ending 
of a n~mmd entity consisting of more than one 
morl)heme. U - the morpheme at the current 
t)osition is a named entity consisting of only one, 
mort)heine. The scheme ;dso considers one ad(li- 
tional state for the position outside any named 
entity: 0 t;he mort)heine at the current posi- 
tion is outside any named entity. Thus, in our 
setting, this scheme distinguishes 4 x 8 + 1 = 33 
states. 
a.3 Preced ing /Subsequent  Morphemes 
as Contextua l  Clues 
In this l)aper, we ewfluate the following two 
l l l ode ls  of  considering preceding/subsequent 
4\Ve allow the, state :c_B for a named entity tyt)e x 
only when the, morl)hcme at t, he current 1)osition marks 
the 1)egimdng ofa named entity of the type a" that im- 
mediately follows a nmned entity of the same type x. 
707 
morphemes as contextual clues to named entity 
clmnking/tagging. Here we provide a basic out- 
line of these models, and the details of how to 
incorporate them into the decision list learning 
framework will be described in Section 4.2.2. 
3.a.1 3-gram Model 
In this paper, we refer to the model used in 
Sekine et al (1998) and Borthwick (1999) as a 
3-gram model. Suppose that the current posi- 
tion is at the morpheme M0, as illustrated be- 
low. Then, when assigning a chunking state as 
well as a named entity type to the morpheme 
M0, the 3-gram model considers the preceding 
single morpheme M-1 as well as the subsequent 
single morpheme M1 as the contextual clue. 
Left Current Right 
( Context ) ( Position ) ( Context ) 
? . .  M0 M, . . .  (1) 
The major disadvantage of the 3-gram model 
is that in the training phase it does not 
take into account whether or not the l)re- 
ceding/subsequent morphemes constitute one 
named entity together with the mort)heine at 
the current position. 
a.a.2 Variable Length Model 
In order to overcome this disadvantage of the 3- 
gram model, we propose a novel model, namely 
the "Variable Length Model", which incorpo- 
rates richer contextual intbrmation as well as 
patterns of constituent morl)hemes within a 
named entity. In principle, as part of the train- 
ing phase this model considers which of the pre- 
ceding/subsequent morphenms constitute one 
named entity together with the morpheme at 
the current position. It also considers sev- 
eral morphemes in the lefl;/right contexts of the 
named entity. Here we restrict this model to ex- 
plicitly considering the cases of named entities 
of the length up to three morphenms and only 
implicitly considering those longer than three 
morphemes. We also restrict it to considering 
two morphemes in both left and right contexts 
of the named entity. 
Left 
( Context ) 
... ML2MI_'I 
ll,ight 
(Named Entity) ( Context ) 
M# . . .  ... Mm(<3 ) 
1" (2) 
(Current Position) 
4 Superv ised Learning for Japanese 
Named Ent i ty  Recogn i t ion  
This section describes how to apply tile deci- 
sion list learning method to chunking/tagging 
named entities. 
4.1 Decision List Learning 
A decision list (Rivest, 1987; Yarowsky, 1994) 
is a sorted list of decision rules, each of which 
decides the wflue of a decision D given some ev- 
idence E. Each decision rule in a decision list is 
sorted in descending order with respect o some 
preference value, and rules with higher prefer- 
ence values are applied first when applying the 
decision list to some new test; data. 
First, the random variable D representing a 
decision w, ries over several possible values, and 
the random w~riable E representing some evi- 
dence varies over '1' and '0' (where '1' denotes 
the presence of the corresponding piece of evi- 
dence, '0' its absence). Then, given some train- 
ing data in which the correct value of the deci- 
sion D is annotated to each instance, the con- 
ditional probabilities P(D = x I E = 1) of ob- 
serving the decision D = x under the condition 
of the presence of the evidence E (E = 1) are 
calculated and the decision list is constructed 
by the tbllowing procedure. 
1. For each piece of evidence, we calculate the 
Iw of likelihood ratio of the largest; condi- 
tional probability of the decision D = :rl 
(given the presence of that piece of ev- 
idence) to the second largest conditional 
probability of the decision D =x2: 
I E=I) 
l?g2 P(D=x2 I E=I )  
Then~ a decision list is constructed with 
pieces of evidence sorted in descending or- 
der with respect to their log of likelihood 
ratios, where the decision of the rule at each 
line is D = xl with the largest conditional 
probabil i ty) 
'~Yarowsky (1994) discusses everal techniques for 
avoiding the problems which arise when an observed 
count is 0. lq-om among those techniques, we employ 
tlm simplest ram, i.e., adding a small constant c~ (0.1 < 
< 0.25) to the numerator and denominator. With 
this inodification, more frcquent evidence is preferred 
when several evidence candidates exist with the same 
708 
2. The final line of a decision list; ix defined as 
% default', where the log of likelihood ratio 
is calculated D<)m the ratio of the largest; 
marginal )robability of the decision D = x t 
to the second largest marginal l)rol)at)ility 
of the decision D =x2: 
P(D =:/11) 
log~ p (D = x'2) 
The 'default' decision of this final line is 
D = Xl with the largest lnarginal probabil- 
ity. 
4.2 Decision List Learning for 
Chunking/Tagging Named Entities 
4.2.1 Decision 
For each of the two schemes of enco(li1~g chunk- 
ing states of nalned entities descrit)ed in Sec- 
tion 3.2, as the l)ossible values of the <teei- 
sion D, we consider exactly the same categories 
of chunking states as those described in Sec- 
tion 3.2. 
4.2.2 Evidence 
The evidence E used in the decision list learn- 
ing is a combination of the tbatures of preced- 
ing/subsequent inorphemes as well as the mor- 
pheme at; the current position. The following 
describes how to form the evidence E fi)r 1)oth 
the a-gram nlodel and varial)le length model. 
3-,gram Model 
The evidence E ret)resents a tut)le (F - l ,  F0, F1 ), 
where F-1 and F1 denote the features of imme- 
diately t)receding/subsequent morphemes M_~ 
and M1, respectively, F0 the featm:e of the mor- 
pheme 54o at the current position (see Fonnuta 
(1) in Section 3.3.1). The definition of the pos- 
sible values of those tbatures F_l ,  F0, and 1'~ 
are given below, where Mi denotes the roo f  
1)\]mnm itself (i.e., including its lexicM tbrm as 
well as part-of-sl)eech), C,i the character type 
(i.e., JaI)anese (hiragana or katakana), Chinese 
(kanji), numbers, English alphabets, symbols, 
and all possible combinations of these) of Mi, 
Ti the part-of-st)eech of Mi: 
F_  1 ::m_ \]~//--1 I (C -1 ,  T - l )  I T - t  Inu l l  
mlsmoothed conditional probability P(D = x \[ E = 1). 
Yarowsky's training Mgoritl,m also ditfcrs omewhat in 
his use of the ratio *'(~D=,d*~-j)' which is equivalent in 
the case of binary classifications, and also by the interpo- 
lation between the global probalfilities (used here) and 
tl,e residual prol)abilities further conditional on higher- 
ranked patterns failing to match in the list. 
17'1 ::--~ \]~/-/1 I (C , ,V ; ) I  T* Inu l \ ]  
F0 ::- M0 I(C0,T0) lT0 
As the evidence E, we consider each possible 
coml)ination of the values of those three f'ea- 
lures. 
Variable Length Model 
The evidence E rel>resents a tuple 
(FL,FNu, FIt), where FL and Fl~ denote 
the features of the morphemes ML_2ML1 and 
Mff'M~ ~ in the left/right contexts of the current 
named entity, respectively, FNE the features 
of the morphemes MN~""  " MNE " "" MNEm(_<3) 
constituting the current named entity (see 
Formula (2) in Section 3.3.2). The definition of 
the possible values of those features 1 L, FNI,:, 
and FI~ arc given below, where F NI~ denotes 
the feature of the j - th constituent morpheme 
M .NJ~ within the current nalne(1 entity, and a 
k/l NI~ is the morl)heme at the cm'ren~ i)osition: 
FL ::= M*_'2M~ ~ \ [M~ Inull 
Fu ::= M\ ]~M~IM~Inu l l  
FNE FNEFNE IFNE r.NI'2 7z~NE FNE : := i i+1 i+2 \[ * i-1 * i * i+1 
\] I~NI'A~NI'21pNE 17NE~NI,2 
? , FNE , (3) 
~NIC MN~c (Cm,: T~VJ~ , ,NI,: 
As the evidence E, we consider each possit)le 
(:oml)ination of the wfiues of those three fba- 
tures, except that the tbllowing three restric- 
tions are applied. 
1. In the cases where the current named en- 
tity consists of up to three mort)heroes , as 
the possible values of the feature FNIi in 
the definition (3), we consider only those 
which are consistent with the requirement 
that each nlort)heme M NE is a constituent 
of the cun'ent named entity. For exainple, 
suppose that the cun'ent named entity con- 
sists of three morphemes, where the cur- 
rent position is at the middle of those con- 
stituent morphemes as below: 
Left Right 
( Context ) (Named Entity) ( Context ) 
I. L M1N~'M N+~M~u I~ t~ ? " M1 Mi -'- _/l//_ 2/~//_ 1 
1" (4) 
(Current Position) 
Then, as the possible values of the feature 
FN\],;, we consider only the tbllowing ibm': 
rN .  ::= \[ F.g U.g 
709 
2. II1 the cases where the eurrellt ilalned entity 
consists of more than three morphemes, 
only the three constituent morphemes are 
regarded as within the current named en- 
tity and the rest are treated as if they 
were outside the named entity. For exam- 
pie, suppose that the current named en- 
tity consists of four morphemes as below: 
Left Right 
( Context ) (Named Entity) ( Context ) 
L L 
$ 
(Current Position) 
Iit this case, the fourth constitnent mor- 
pheme M N1c is treated as if it were in the 
right context of the current named entity 
as below: 
Left Right 
( Context ) (Named Entity) ( Context ) 
'.,~ 1. ~r.,vJ,:~,Nu ~,.,,'.r,_,C M~ZMff 
t 
(Curren~ Position) 
3. As the evidence E, among the possible 
combination of the values of three t'ea- 
tures /~,, ENId, and F/t, we only accept 
those in which the positions of the mor- 
phemes are continuous, and reject those 
discontimmus combinations. For example, 
in the case of Formula (4:) above, as the 
evidence E, we accel)t the combination 
(Mq,  M 'My , ull), while we r( iect 
(ML1, M~EM~ 1':, 1,ull). 
4.3 Procedures  for Training and 
Testing 
Next we will briefly describe the entire pro- 
cesses of learning the decision list tbr etmnk- 
ing/tagging named entities as well as applying 
it to chunking/tagging unseen named entities. 
4.3.1 Training 
In the training phase, at the positions where 
the corresponding morpheme is a constitnent of 
a named entity, as described in Section 4.2, each 
al lowable combination of features is considered 
as the evidence E. On the other hand, at the 
positions where the corresponding morpheme is
outside any named entity, the way the combi- 
nation of t~at;ures i  considered is diflbrent in 
the variable length model, in that the exception 
\]. in the previous section is no longer applied. 
Theretbre, all the possible wflues of the feature 
FNB in Definition (3) are accepted. Finally, the 
frequency of each decision D and evidence E is 
counted and the decision list is learned as de- 
scribed in Section 4.1. 
4.3.2 Testing 
When applying the decision list to chunk- 
ing/tagging nnseen amed entities, first, at each 
morpheme position, the combination of features 
is considered as in the case of the non-entity po- 
sition in the training phase. Then, the decision 
list is consulted and all the decisions of the rules 
with a log of likelihood ratio above a certain 
threshold are recorded. Finally, as in the case 
of previous research (Sekine et al, 1998; Berth- 
wick, 1999), the most appropriate sequence of 
the decisions that are consistent throughout the 
whole sequence is searched for. By consistency 
of the decisions, we mean requirements such as 
that the decision representing the beginning of 
some named entity type has to be followed by 
that representing the middle of the same entity 
type (in the case of Start /End encoding). Also, 
in our case, the appropriateness of the sequence 
of the decisions is measured by the stun of the 
log of likelihood ratios of 1;t1(; corresponding de- 
cision rules. 
5 Exper imenta l  Eva luat ion  
We experimentally evaluate the performance 
of the supervised learning tbr Japanese nalned 
entity recognition on the IREX workshop's 
training and test data. We compare the re- 
suits of the confl)inations of the two encod- 
ing schemes of named entity chunking states 
(the Inside/Outside and the Start /End encod- 
ing schemes) and the two at)preaches to contex- 
tual feature design (the 3-gram and the Variable 
Length models). For each of those combina- 
tions, we search tbr an optintal threshold of the 
log of likelihood ratio in the decision list. The 
performance of each combination measured by 
F-measure (fl = 1) is given in Table 4. 
In this ewduation, we exclude the named 
entities with "other boundary mismatch" in 
Tat)le 2. We also classify the system 
output according to the number of con- 
stitnent lnorphemes of each named entity 
and evaluate the peribnnance tbr each sub- 
set of the system output. For each sub- 
710 
3-gram 
Variable 
Length 
' l 'al)le 4: Ewduation \]{esults Measured by F-measm'e (fl = 1) 
~, Mori)lmlnes to 1 Named Entity 
- . ,  > J_ll ,,, = I,,.=21,..=311.,>_21.,,.>_31,,>4 
inside/Outside 72.9 75.9 79.7 51.4 69.4 42.5 29.2 
Start/End 72.7 76.6 79.6 43.7 68.1 37.8 29.6 
inside/Outside lJ 74.3 77.6 80.0 55.5 70.9 49.9 41.0 
Start/End t \ [72.1  77.0 75.6 51.5 67.2 48.6 43.6 
set, we compare,' the performmme of the fore' 
combinations of {3-grmn, Vm'iable Length} ? 
{Inside/Outside, S|;~n't/EIld} mM show the 
highest mrtbrmance with bold-faced font. 
Several remarkable points of these re, suits of 
1)erfbrmance omparison can be stated as below: 
? Among the four coml)inations, the Variable 
Length Model with hlside/()utside Ent:od- 
ing 1)erfi)rms best in tot~fl (n > 1) as well 
as in the recognition of named entities con- 
sisting of more thml one morl)heme (',, -- 
2, 3, n > 2, 3). 
? in the re,(:ognil;ion of ilsAll(;d elll;ities con- 
sisting of more than two mOl"l)henles (~, = 
3: ?t ~ 3, 4)~ the Vm'ial)le Lellgth Model 
l)erforlllS signific;mtly t)etter thml the 3- 
rill "t(.~ {~l'alll mo(le\]. .tn\],' result (:letu'ly SUpl)orts 
the (;l~iin that our modeli\]xg of the Vm'i- 
nl)le Length Model has an adva,ntnge in the 
recognition ()f long named entities. 
" Ill general, the Inside/Outside n(:oding 
scheme l)erfol'lns slightly t)etl;er th;m the 
Sta\]'t/l'3nd encoding s(:henm, (Well though 
the tbrmer distinguislms (:onsidera|)ly ti~wer 
sl;ates th;m the latter. 
6 Conc lus ion  
In this 1)~per, we al)plied the supervised eci- 
si(m list learning method to ,\]at)anese mmmd en- 
tity recognition, into wlfich we, incorporated sev- 
eral n(mn phrase chunking teelmiques ~md ex- 
perimentally evaluated their pertbrmance. We, 
showed that a novel technique that we proposed 
out, performed those using previously considered 
(;otd;extual fe~tu\]:es. 
7 Acknowledgments  
This research was c~rried out while the au- 
thors were visiting scholars at l)epartment of 
Computer Science, Johns Hopkins University. 
The ~mthors would like to thank Prof  David 
Yarowsky of Johns Hopkins University for in- 
valual)le sut)porl;s to this research. 
References  
A. Borthwick. 1999. A JaI)mmse named entity rec- 
ognizer constructed by a non-speaker ofJapanese. 
In Proc. of the II~EX Workshop, pages 187 193. 
54. Collins a.nd Y. Singer. 1999. Unsupervised mod- 
els of named entity classification. In P.roc. of 
the 1999 Joint SIGDAT Cm@rcncc on Empiri- 
cal Mcth, ods in Natural Languagc P~vccssing and 
Very Large Corpora, pages 100 110. 
S. Cucerzmt and D. Yarowsky. 1999. l~anguage inde- 
1)endent named entity recognition combining mor- 
1)hological mid contextual evideime, in Proc. of 
th, c 1999 Joi'nt SIGDAT Cm@rence on Empiri- 
cal Methods in Natural Language PTvccssin9 mid 
Very Large Corpora, pages 90 99. 
IREX Committee, editor. 1!)99. P~vcecdings of the 
\]REX Workshop. (in Japanese). 
S. Maiorano. 1996. The multilingual entity task 
(MET): Jalmne, se, results. In ISvc. of TIPSTEI~, 
PIH)U1/,AM P HA,5'1'; 11, pa.ges 449 45\]. 
MUC. 1998. l)'rocccdings oJ"l,h,e, 7th Message Unde'r- 
standing ConJ?rence, (MUC-7). 
L. l{alnshaw and M. Ma.rcus. 1995. Text chunking 
using trmlsforma.tion-based \] mning. In P,roc. of 
th, c 3rd Work,vh, op on l/cry Larg(: Corpora~ Im.ges 
83 -94. 
ILL. Rive, st. 1987, Le, arning decision lists. Machine 
Learning, 2:229 246. 
M. Sassano, Y. Saito, and K. Matsui. 1997. 
,J~l)alle,se morphological mmlyzer for NLP apl)li- 
(:atioils. Ill Proc. of thc, 3rd gn'ttual Meeting of 
th, c Association for Natural Language Processing, 
1)a.ges 441- 444. (in Jal)anese). 
S. Sekine, lL Grishman, and H. Shinnou. 1998. 
A decision tree method tbr tinding and ('lassit~- 
ing names in Jat)almse texts. In Proc. of the 6th 
Workshop on Very La~yc Ctnpora, pages 148-152. 
T. Utsuro mid M. Sassano. 2000. Minimally su- 
pervised ,\]almnese named e, ntity recognition: I{e- 
source, s and evahmtion. In Proc. of thc 2nd Inter- 
national Confcrcncc on Lanquaqc Resources and 
Evahtation, pages 1229 -1236. 
1). Yarowsky. 1994. Decision lists for lexical mnbi- 
guity resolution: Al)t)lication to accent restora.- 
tion in Spanish and French. In Proc. of the 32rid 
Annual Mecl, ing of ACL, 1)ages 88 -95. 
711 
Combining Outputs of Multiple Japanese Named Entity Chunkers
by Stacking
Takehito Utsuro
Department of Information
and Computer Sciences,
Toyohashi University of Technology
Tenpaku-cho, Toyohashi 441-8580, Japan
utsuro@ics.tut.ac.jp
Manabu Sassano
Fujitsu Laboratories, Ltd.
4-4-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
sassano@jp.fujitsu.com
Kiyotaka Uchimoto
Keihanna Human Info-Communications Research Center,
Communications Research Laboratory
Hikaridai Seika-cho, Kyoto 619-0289, Japan
uchimoto@crl.go.jp
Abstract
In this paper, we propose a method for
learning a classifier which combines out-
puts of more than one Japanese named
entity extractors. The proposed combi-
nation method belongs to the family of
stacked generalizers, which is in principle
a technique of combining outputs of sev-
eral classifiers at the first stage by learn-
ing a second stage classifier to combine
those outputs at the first stage. Individ-
ual models to be combined are based on
maximum entropy models, one of which
always considers surrounding contexts of
a fixed length, while the other consid-
ers those of variable lengths according to
the number of constituent morphemes of
named entities. As an algorithm for learn-
ing the second stage classifier, we employ
a decision list learning method. Experi-
mental evaluation shows that the proposed
method achieves improvement over the
best known results with Japanese named
entity extractors based on maximum en-
tropy models.
1 Introduction
In the recent corpus-based NLP research, sys-
tem combination techniques have been successfully
applied to several tasks such as parts-of-speech
tagging (van Halteren et al, 1998), base noun
phrase chunking (Tjong Kim Sang, 2000), and pars-
ing (Henderson and Brill, 1999; Henderson and
Brill, 2000). The aim of system combination is to
combine portions of the individual systems? outputs
which are partial but can be regarded as highly ac-
curate. The process of system combination can be
decomposed into the following two sub-processes:
1. Collect systems which behave as differently as
possible: it would help a lot if at least the col-
lected systems tend to make errors of differ-
ent types, because simple voting technique can
identify correct outputs.
Previously studied techniques for collecting
such systems include: i) using several exist-
ing real systems (van Halteren et al, 1998;
Brill and Wu, 1998; Henderson and Brill, 1999;
Tjong Kim Sang, 2000), ii) bagging/boosting
techniques (Henderson and Brill, 1999; Hen-
derson and Brill, 2000), and iii) switching the
data expression and obtaining several mod-
els (Tjong Kim Sang, 2000).
2. Combine the outputs of the several systems:
previously studied techniques include: i) vot-
ing techniques (van Halteren et al, 1998;
Tjong Kim Sang, 2000; Henderson and Brill,
1999; Henderson and Brill, 2000), ii) switch-
ing among several systems according to con-
fidence values they provide (Henderson and
Brill, 1999), iii) stacking techniques (Wolpert,
1992) which train a second stage classifier for
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 281-288.
                         Proceedings of the Conference on Empirical Methods in Natural
combining outputs of classifiers at the first
stage (van Halteren et al, 1998; Brill and Wu,
1998; Tjong Kim Sang, 2000).
In this paper, we propose a method for combining
outputs of (Japanese) named entity chunkers, which
belongs to the family of stacking techniques. In
the sub-process 1, we focus on models which dif-
fer in the lengths of preceding/subsequent contexts
to be incorporated in the models. As the base model
for supervised learning of Japanese named entity
chunking, we employ a model based on the maxi-
mum entropy model (Uchimoto et al, 2000), which
performed the best in IREX (Information Retrieval
and Extraction Exercise) Workshop (IREX Commit-
tee, 1999) among those based on machine learning
techniques. Uchimoto et al (2000) reported that the
optimal number of preceding/subsequent contexts to
be incorporated in the model is two morphemes to
both left and right from the current position. In this
paper, we train several maximum entropy models
which differ in the lengths of preceding/subsequent
contexts, and then combine their outputs.
As the sub-process 2, we propose to apply a stack-
ing technique which learns a classifier for com-
bining outputs of several named entity chunkers.
This second stage classifier learns rules for accept-
ing/rejecting outputs of several individual named en-
tity chunkers. The proposed method can be applied
to the cases where the number of constituent systems
is quite small (e.g., two). Actually, in the experimen-
tal evaluation, we show that the results of combining
the best performing model of Uchimoto et al (2000)
with the one which performs poorly but extracts
named entities quite different from those of the
best performing model can help improve the perfor-
mance of the best model.
2 Named Entity Chunking based on
Maximum Entropy Models
2.1 Task of the IREX Workshop
The task of named entity recognition of the IREX
workshop is to recognize eight named entity types
in Table 1 (IREX Committee, 1999). The organizer
of the IREX workshop provided 1,174 newspaper
articles which include 18,677 named entities as the
training data. In the formal run (general domain)
Table 1: Statistics of NE Types of IREX
frequency (%)
NE Type Training Test
ORGANIZATION 3676 (19.7) 361 (23.9)
PERSON 3840 (20.6) 338 (22.4)
LOCATION 5463 (29.2) 413 (27.4)
ARTIFACT 747 (4.0) 48 (3.2)
DATE 3567 (19.1) 260 (17.2)
TIME 502 (2.7) 54 (3.5)
MONEY 390 (2.1) 15 (1.0)
PERCENT 492 (2.6) 21 (1.4)
Total 18677 1510
of the workshop, the participating systems were re-
quested to recognize 1,510 named entities included
in the held-out 71 newspaper articles.
2.2 Named Entity Chunking
We first provide our definition of the task of
Japanese named entity chunking (Sekine et al,
1998; Borthwick et al, 1998; Uchimoto et al,
2000). Suppose that a sequence of morphemes is
given as below:
(
Left
Context ) (Named Entity) (
Right
Context )
? ? ?M
L
?k
? ? ?M
L
?1
M
NE
1
? ? ?M
NE
i
? ? ?M
NE
m
M
R
1
? ? ?M
R
l
? ? ?
?
(Current Position)
Given that the current position is at the morpheme
MNE
i
, the task of named entity chunking is to assign
a chunking state (to be described in section 2.3.1) to
the morpheme MNE
i
at the current position, consid-
ering the patterns of surrounding morphemes. Note
that in the supervised learning phase, we can use the
chunking information on which morphemes consti-
tute a named entity, and which morphemes are in the
left/right contexts of the named entity.
2.3 The Maximum Entropy Model
In the maximum entropy model (Della Pietra et al,
1997), the conditional probability of the output y
given the context x can be estimated as the follow-
ing p
?
(y | x) of the form of the exponential family,
where binary-valued indicator functions called fea-
ture functions f
i
(x, y) are introduced for expressing
a set of ?features?, or ?attributes? of the context x
and the output y. A parameter ?
i
is introduced for
each feature f
i
, and is estimated from a training data.
p
?
(y | x) =
exp
(
?
i
?
i
f
i
(x, y)
)
?
y
exp
(
?
i
?
i
f
i
(x, y)
)
Uchimoto et al (2000) defines the context x as the
patterns of surrounding morphemes as well as that at
the current position, and the output y as the named
entity chunking state to be assigned to the mor-
pheme at the current position.
2.3.1 Named Entity Chunking States
Uchimoto et al (2000) classifies classes of
named entity chunking states into the following 40
tags:
? Each of eight named entity types plus an ?OP-
TIONAL? type are divided into four chunking
states, namely, the beginning/middle/end of an
named entity, or an named entity consisting of
a single morpheme. This amounts to 9?4 = 36
classes.
? Three more classes are distinguished for mor-
phemes immediately preceding/following a
named entity, as well as the one between two
named entities.
? Other morphemes are assigned the class
?OTHER?.
2.3.2 Features
Following Uchimoto et al (2000), feature func-
tions for morphemes at the current position as well
as the surrounding contexts are defined. More
specifically, the following three types of feature
functions are used: 1
1. 2052 lexical items that are observed five times
or more within two morphemes from named
entities in the training corpus.
2. parts-of-speech tags of morphemes2.
3. character types of morphemes (i.e., Japanese
(hiragana or katakana), Chinese (kanji), num-
bers, English alphabets, symbols, and their
combinations).
As for the number of preceding/subsequent mor-
phemes as contextual clues, we consider the follow-
ing models:
1Minor modifications from those of Uchimoto et al (2000)
are: i) we used character types of morphemes because they are
known to be useful in the Japanese named entity chunking, and
ii) the sets of parts-of-speech tags are different.
2As a Japanese morphological analyzer, we used BREAK-
FAST (Sassano et al, 1997) with the set of about 300 part-of-
speech tags. BREAKFAST achieves 99.6% part-of-speech accu-
racy against newspaper articles.
5-gram model
This model considers the preceding two mor-
phemes M
?2
, M
?1
as well as the subsequent two
morphemes M
1
, M
2
as the contextual clue. Both in
(Uchimoto et al, 2000) and in this paper, this is the
model which performs the best among all the indi-
vidual models without system combination.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?2
M
?1
M
0
M
1
M
2
? ? ?
7-gram model
This model considers the preceding three mor-
phemes M
?3
, M
?2
, M
?1
as well as the subsequent
three morphemes M
1
, M
2
, M
3
as the contextual
clue.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?3
M
?2
M
?1
M
0
M
1
M
2
M
3
? ? ?
9-gram model
This model considers the preceding four mor-
phemes M
?4
, M
?3
, M
?2
, M
?1
as well as the subse-
quent four morphemes M
1
, M
2
, M
3
, M
4
as the con-
textual clue.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?4
? ? ?M
?1
M
0
M
1
? ? ?M
4
? ? ?
For both 7-gram and 9-gram models, we consider
the following three modifications to those models:
? with all features
? with lexical items and parts-of-speech
tags (without the character types) of
M
{(?4),?3,3,(4)}
? with only the lexical items of M
{(?4),?3,3,(4)}
In our experiments, the number of features is
13,200 for 5-gram model and 15,071 for 9-gram
model. The number of feature functions is 31,344
for 5-gram model and 35,311 for 9-gram model.
Training a variable length (5?9-gram) model,
testing with 9-gram model
The major disadvantage of the 5/7/9-gram models
is that in the training phase it does not take into ac-
count whether or not the preceding/subsequent mor-
phemes constitute one named entity together with
the morpheme at the current position. Consider-
ing this disadvantage, we examine another model,
namely, variable length model, which incorporates
variable length contextual information. In the train-
ing phase, this model considers which of the preced-
ing/subsequent morphemes constitute one named
entity together with the morpheme at the current po-
sition (Sassano and Utsuro, 2000). It also considers
several morphemes in the left/right contexts of the
named entity. Here we restrict this model to explic-
itly considering the cases of named entities of the
length up to three morphemes and only implicitly
considering those longer than three morphemes. We
also restrict it to considering two morphemes in both
left and right contexts of the named entity.
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
? ? ?MNE
i
? ? ?MNE
m(?3)
MR
1
MR
2
? ? ?
?
(Current Position)
1. In the cases where the current named entity
consists of up to three morphemes, all the con-
stituent morphemes are regarded as within the
current named entity. The following is an ex-
ample of this case, where the current named
entity consists of three morphemes, and the
current position is at the middle of those con-
stituent morphemes as below:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MR
1
MR
2
? ? ?
? (1)
(Current Position)
2. In the cases where the current named entity
consists of more than three morphemes, only
the three constituent morphemes are regarded
as within the current named entity and the rest
are treated as if they were outside the named
entity. For example, suppose that the cur-
rent named entity consists of four morphemes:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MNE
4
MR
1
MR
2
? ? ?
?
(Current Position)
In this case, the fourth constituent morpheme
MNE
4
is treated as if it were in the right context
of the current named entity as below:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MNE
4
MR
1
? ? ?
?
(Current Position)
In the testing phase, we apply this model consid-
ering the preceding four morphemes as well as the
subsequent four morphemes at every position, as in
the case of 9-gram model3.
We consider the following three modifications to
this model, where we suppose that the morpheme at
the current position be M
0
:
? with all features
? with lexical items and parts-of-speech tags
(without the character types) of M
{?4,?3,3,4}
? with only the lexical items of M
{?4,?3,3,4}
3 Learning to Combine Outputs of Named
Entity Chunkers
3.1 Data Sets
The following gives the training and test data sets
for our framework of learning to combine outputs of
named entity chunkers.
1. TrI : training data set for learning individual
named entity chunkers.
2. TrC: training data set for learning a classifier
for combining outputs of individual named en-
tity chunkers.
3. Ts: test data set for evaluating the classifier for
combining outputs of individual named entity
chunkers.
3.2 Procedure
The following gives the procedure for learning the
classifier to combine outputs of named entity chun-
kers using TrI and TrC.
1. Train the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) using TrI .
2. Apply the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) to TrC, respectively,
and obtain the list of chunked named entities
NEList
i
(TrC) for each named entity chun-
ker NEchk
i
.
3Note that, as opposed to the training phase, the length of
preceding/subsequent contexts is fixed in the testing phase of
this model. Although this discrepancy between training and
testing damages the performance of this single model (sec-
tion 4.1), it is more important to note that this model tends to
have distribution of correct/over-generated named entities dif-
ferent from that of the 5-gram model. In section 4, we exper-
imentally show that this difference is the key to improving the
named entity chunking performance by system combination.
Table 2: Examples of Event Expressions for Combining Outputs of Multiple Systems
Segment Morpheme(POS) NE Outputs ofIndividual Systems Event Expressions
System 0 System 1
.
.
.
SegEv
i
rainen
(?next year?,
temporal noun)
10gatsu
(?October?,
temporal noun)
rainen
(DATE)
10gatsu
(DATE)
rainen
-10gatsu
(DATE)
{
systems=?0?,mlength=1,
NEtag=DATE,
POS=?temporal noun?, class
NE
=?
}
{
systems=?0?,mlength=1,
NEtag=DATE,
POS=?temporal noun?, class
NE
=?
}
{
systems=?1?,mlength=2,
NEtag=DATE,
POS=?temporal noun, temporal noun?,
class
NE
=+
}
.
.
.
SegEv
i+1
seishoku
(?reproductive?, noun)
iryou
(?medical?, noun)
gijutsu
(?technology?, noun)
seishoku
-iryou
-gijutsu
(ARTIFACT)
{
systems=?0?, class
sys
=?no outputs?
}
{
systems=?1?,mlength=3,
NEtag=ARTIFACT,
POS=?noun,noun,noun?, class
NE
=?
}
nitsuite
(?about?, particle)
.
.
.
3. Align the lists NEList
i
(TrC) (i = 1, . . . , n)
of chunked named entities according to the po-
sitions of the chunked named entities in the text
TrC, and obtain the event expression TrCev
of TrC.
4. Train the classifier NEchk
cmb
for combining
outputs of individual named entity chunkers us-
ing the event expression TrCev.
The following gives the procedure for applying the
learned classifier to Ts.
1. Apply the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) to Ts, respectively,
and obtain the list of chunked named entities
NEList
i
(Ts) for each named entity chunker
NEchk
i
.
2. Align the lists NEList
i
(Ts) (i=1, . . . , n) of
chunked named entities according to the posi-
tions of the chunked named entities in the text
Ts, and obtain the event expression Tsev of
Ts.
3. Apply NEchk
comb
to Tsev and evaluate its
performance.
3.3 Data Expressions
3.3.1 Events
The event expression TrCev of TrC is obtained
by aligning the lists NEList
i
(TrC) (i =1, . . . , n)
of chunked named entities, and is represented as a
sequence of segments, where each segment is a set
of aligned named entities. Chunked named enti-
ties are aligned under the constraint that those which
share at least one constituent morpheme have to be
aligned into the same segment. Examples of seg-
ments, into which named entities chunked by two
systems are aligned, are shown in Table 2. In the
first segment SegEv
i
, given the sequence of the two
morphemes, the system No.0 decided to extract two
named entities, while the system No.1 chunked the
two morphemes into one named entity. In those
event expressions, systems indicates the list of the
indices of the systems which output the named en-
tity, mlength gives the number of the constituent
morphemes, NEtag gives one of the nine named
entity types, POS gives the list of parts-of-speech
of the constituent morphemes, and class
NE
indi-
cates whether the named entity is a correct one com-
pared against the gold standard (?+?), or the one
over-generated by the systems (???).
In the second segment SegEv
i+1
, only the sys-
tem No.1 decided to extract a named entity from
the sequence of the three morphemes. In this case,
the event expression for the system No.0 is the one
which indicates that no named entity is extracted by
the system No.0.
In the training phase, each segment SegEv
j
of
event expression constitutes a minimal unit of an
event, from which features for learning the classi-
fier are extracted. In the testing phase, the classes
of each system?s outputs are predicted against each
segment SegEv
j
.
3.3.2 Features and Classes
In principle, features for learning the classifier for
combining outputs of named entity chunkers are rep-
resented as a set of pairs of the system indices list
?p, . . . , q? and a feature expression F of the named
entity:
f =
{
?systems=?p, . . . , q?, F ?
? ? ?
?systems=?p?, . . . , q??, F ??
}
(2)
In the training phase, any possible feature of this
form is extracted from each segment SegEv
j
of
event expression. The system indices list ?p, . . . , q?
indicates the list of the systems which output the
named entity. A feature expression F of the named
entity can be any possible subset of the full feature
expression {mlength= ? ? ? , NEtag= ? ? ? , POS =
? ? ?}, or the set indicating that the system outputs no
named entity within the segment.
F =
?
?
?
?
?
?
?
any subset of
{
mlength= ? ? ? ,
NEtag= ? ? ? , POS= ? ? ?
}
{
class
sys
=?no outputs?
}
In the training and testing phases, within each
segment SegEv
j
of event expression, a class is as-
signed to each system, where each class classi
sys
for
the i-th system is represented as a list of the classes
of the named entities output by the system:
classi
sys
=
{
+/?, . . . , +/?
?no output? (i = 1, . . . , n)
3.4 Learning Algorithm
We apply a simple decision list learning method
to the task of learning a classifier for combining
outputs of named entity chunkers4. A decision
list (Yarowsky, 1994) is a sorted list of decision
rules, each of which decides the value of class given
some features f of an event. Each decision rule in
a decision list is sorted in descending order with
respect to some preference value, and rules with
higher preference values are applied first when ap-
plying the decision list to some new test data. In
this paper, we simply sort the decision list according
to the conditional probability P (class
i
| f) of the
class
i
of the i-th system?s output given a feature f .
4 Experimental Evaluation
We experimentally evaluate the performance of the
proposed system combination method using the
IREX workshop?s training and test data.
4.1 Comparison of Outputs of Individual
Systems
First, Table 3 shows the performance of the indi-
vidual models described in the section 2.3.2, where
trained with the IREX workshop?s training data, and
tested against the IREX workshop?s test data as Ts.
The 5-gram model performs the best among those
individual models.
Next, assuming that each of the models other
than the 5-gram model is combined with the 5-gram
model, Table 4 compares the named entities of their
outputs. Recall rate of the correct named entities in
the union of their outputs, as well as the overlap rate5
of the over-generated entities against those included
in the output of the 5-gram model are shown.
From the Tables 3 and 4, it is clear that the 7-gram
and 9-gram models are quite similar to the 5-gram
model both in the performance and in the distribu-
tion of correct/over-generated named entities. On
the other hand, variable length models have distri-
bution of correct/over-generated named entities a lit-
4It is quite straightforward to apply any other supervised
learning algorithms to this task.
5For a model X , the overlap rate of the over-generated enti-
ties against those included in the output of the 5-gram model is
defined as: (# of the intersection of the over-generated entities
output by the 5-gram model and those output by the model X)/
(# of the over-generated entities output by the 5-gram model).
Table 3: Performance of Individual Models against
Ts (F-measure (? = 1) (%))
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 80.78 80.81 80.71
9-gram 80.13 80.53 80.53
variable length 45.12 77.02 75.16
5-gram 81.16
Table 4: Difference between 5-gram model and
Other Individual Models (Recall of the Union /
Overlap Rate of Over-generated Entities) (%)
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 79.8/85.2 79.8/85.2 79.7/91.2
9-gram 79.7/84.7 79.7/86.1 79.5/90.7
variable
length 82.6/27.3 81.4/63.4 80.4/72.7
tle different from that of the 5-gram model. Vari-
able length models have lower performance mainly
because of the difference between the training and
testing phases with respect to the modeling of con-
text lengths. Especially, the variable length model
with ?all? features of M
{?4,?3,3,4}
has much lower
performance as well as significantly different dis-
tribution of correct/over-generated named entities.
This is because character types features are so gen-
eral that many (erroneous) named entities are over-
generated, while sometimes they contribute to find-
ing named entities that are never detected by any of
the other models.
4.2 Results of Combining System Outputs
This section reports the results of combining the out-
put of the 5-gram model with that of 7-gram models,
9-gram models, and the variable length models. As
the training data sets TrI and TrC, we evaluate the
following two assignments (a) and (b), where D
CRL
denotes the IREX workshop?s training data:
(a) TrI: D
CRL
? D200
CRL
(200 articles from D
CRL
)
TrC: D200
CRL
(b) TrI = TrC = D
CRL
We use the IREX workshop?s test data for Ts.
In the assignment (a), TrI and TrC are disjoint,
while in the assignment (b), individual named entity
chunkers are applied to their own training data, i.e.,
closed data. The assignment (b) is for the sake of
avoiding data sparseness in learning the classifier for
combining outputs of two named entity chunkers.
Table 5 shows the peformance in F-measure (? =
1) for both assignments (a) and (b). For both (a) and
Table 5: Performance of Combining 5-gram model
and Other Individual Models (against Ts, F-measure
(? = 1) (%))
(a) TrI = D
CRL
? D200
CRL
, TrC = D200
CRL
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 81.54 81.53 80.60
9-gram 81.31 81.26 80.60
variable length 83.43 81.55 81.85
(b) TrI = TrC = D
CRL
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 81.97 81.83 81.58
9-gram 81.53 81.66 81.52
variable length 84.07 83.07 82.50
(b), ?5-gram + variable length (All)? significantly
outperforms the 5-gram model, which is the best
model among all the individual models without sys-
tem combination. It is remarkable that models which
perform poorly but extract named entities quite dif-
ferent from those of the best performing model can
actually help improve the best model by the pro-
posed method. The performance for the assignment
(b) is better than that for the assignment (a). This re-
sult claims that the training data size should be larger
when learning the classifier for combining outputs of
two named entity chunkers.
In the Table 6, for the best performing result (i.e.,
5-gram + variable length (All)) as well as the con-
stituent individual models (5-gram model and vari-
able length model (All)), we classify the system
output according to the number of constituent mor-
phemes of each named entity. In the Table 7, we
classify the system output according to the named
entity types. The following summarizes several re-
markable points of these results: i) the benefit of the
system combination is more in the improvement of
precision rather than in that of recall. This means
that the proposed system combination technique is
useful for detecting over-generation of named en-
tity chunkers, ii) the combined outputs of the 5-gram
model and the variable length model improve the re-
sults of chunking longer named entities quite well
compared with shorter named entities. This is the
effect of the variable length features of the variable
length model.
Table 6: Evaluation Results of Combining System Outputs, per # of constituent morphemes
(TrI = TrC = D
CRL
, F-measure (? = 1) / Recall / Precision (%))
n Morphemes to 1 Named Entity
n ? 1 n = 1 n = 2 n = 3 n ? 4
5-gram 81.16 83.60 86.94 68.42 50.59
78.87/83.60 84.97/82.28 85.90/88.00 63.64/73.98 35.83/86.00
variable length (All) 45.12 53.77 56.63 33.74 16.78
51.50/40.15 38.69/88.14 71.37/47.93 57.34/23.91 40.00/10.62
5-gram + variable length (All) 84.07 85.06 88.96 75.19 65.96
81.45/86.86 85.12/84.99 87.42/90.56 69.93/81.30 51.67/91.18
Table 7: Evaluation Results of Combining System Outputs, per NE type
(TrI = TrC = D
CRL
, F-measure (? = 1) (Recall, Precision) (%))
ORGANI- PER- LOCA- ARTI- DATE TIME MONEY PER-
ZATION SON TION FACT CENT
67.74 81.82 77.04 30.43 91.49 93.20 92.86 87.18
5-gram (58.45) (79.88) (71.91) (29.17) (88.85) (88.89) (86.67) (80.95)
(80.53) (83.85) (82.96) (31.82) (94.29) (97.96) (100.00) (94.44)
35.48 48.45 38.47 5.80 78.60 56.90 60.61 87.18
variable length (All) (37.40) (48.52) (32.93) (22.92) (81.92) (61.11) (66.67) (80.95)
(33.75) (48.38) (46.26) (3.32) (75.53) (53.23) (55.56) (94.44)
5-gram + 72.18 84.15 79.58 38.71 92.86 93.20 92.86 87.18
variable length (All) (62.88) (81.66) (73.61) (37.50) (90.00) (88.89) (86.67) (80.95)
(84.70) (86.79) (86.61) (40.00) (95.90) (97.96) (100.00) (94.44)
5 Conclusion
This paper proposed a method for learning a classi-
fier to combine outputs of more than one Japanese
named entity chunkers. Experimental evaluation
showed that the proposed method achieved improve-
ment in F-measure over the best known results with
an ME model (Uchimoto et al, 2000), when a com-
plementary model extracted named entities quite dif-
ferently from the best performing model.
References
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Proc.
6th Workshop on VLC, pages 152?160.
E. Brill and J. Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. 17th COLING
and 36th ACL, pages 191?195.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
J. C. Henderson and E. Brill. 1999. Exploiting diversity
in natural language processing: Combining parsers. In
Proc. 1999 EMNLP and VLC, pages 187?194.
J. C. Henderson and E. Brill. 2000. Bagging and boost-
ing a treebank parser. In Proc. 1st NAACL, pages 34?
41.
IREX Committee, editor. 1999. Proceedings of the IREX
Workshop. (in Japanese).
M. Sassano and T. Utsuro. 2000. Named entity chunking
techniques in supervised learning for Japanese named
entity recognition. In Proceedings of the 18th COL-
ING, pages 705?711.
M. Sassano, Y. Saito, and K. Matsui. 1997. Japanese
morphological analyzer for NLP applications. In Proc.
3rd Annual Meeting of the Association for Natural
Language Processing, pages 441?444. (in Japanese).
S. Sekine, R. Grishman, and H. Shinnou. 1998. A deci-
sion tree method for finding and classifying names in
Japanese texts. In Proc. 6th Workshop on VLC, pages
148?152.
E. Tjong Kim Sang. 2000. Noun phrase recognition by
system combination. In Proc. 1st NAACL, pages 50?
55.
K. Uchimoto, Q. Ma, M. Murata, H. Ozaku, and H. Isa-
hara. 2000. Named entity extraction based on a maxi-
mum entropy model and transformation rules. In Proc.
38th ACL, pages 326?335.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Im-
proving data driven wordclass tagging by system com-
bination. In Proc. 17th COLING and 36th ACL, pages
491?497.
D. H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in Span-
ish and French. In Proc. 32nd ACL, pages 88?95.
Analyzing Dependencies of Japanese Subordinate Clauses 
based on Statistics of Scope Embedding Preference 
Takeh i to  Utsuro ,  Sh igeyuk i  N ish iokayama,  Masakazu  Fu j io ,  Yu j i  Matsumoto  
Graduate  Schoo l  of  In fo rmat ion  Sc ience,  Nara  Ins t i tu te  of  Sc ience and  Techno logy  
8916-5, Takayama-cho, Ikoma-shi, Nara, 630-0101, JAPAN 
E-mail: utsuro @is. aist-nara, ac.jp, URL: http://cl, aist-nara, ac.jp/-utsuro/ 
Abst ract  
This paper proposes a statistical method for 
learning dependency preference of Japanese 
subordinate clauses, in which scope embedding 
preference of subordinate clauses is exploited 
as a useful information source for disambiguat- 
ing dependencies between subordinate clauses. 
Estimated dependencies of subordinate clauses 
successfully increase the precision of an existing 
statistical dependency analyzer. 
1 In t roduct ion  
In the Japanese language, since word order in a 
sentence is relatively free compared with Euro- 
pean languages, dependency analysis has been 
shown to be practical and effective in both rule- 
based and stochastic approaches to syntactic 
analysis. In dependency analysis of a Japanese 
sentence, among various source of ambiguities 
in a sentence, dependency ambiguities of sub- 
ordinate clauses are one of the most problem- 
atic ones, partly because word order in a sen- 
tence is relatively free. In general, dependency 
ambiguities of subordinate clauses cause scope 
ambiguities of subordinate clauses, which result 
in enormous number of syntactic ambiguities of 
other types of phrases such as noun phrases. 1 
1In our preliminary corpus analysis using the stochas- 
tic dependency analyzer of Fujio and Matsumoto (1998), 
about 30% of the 210,000 sentences in EDR bracketed 
corpus (EDR, 1995) have dependency ambiguities of sub- 
ordinate clauses, for which the precision of chunk (bun- 
setsu) level dependencies is about 85.3% and that of sen- 
tence level is about 25.4% (for best one) ~ 35.8% (for 
best five), while for the rest 70% of EDR bracketed cor- 
pus, the precision of chunk (bunsetsu) level dependencies 
is about 86.7% and that of sentence l vel is about 47.5% 
(for best one) ~ 60.2% (for best five). In addition to 
that, when assuming that those ambiguities of subor- 
dinate clause dependencies are initially resolved in some 
way, the chunk level precision increases to 90.4%, and the 
sentence l vel precision to 40.6% (for best one) ~ 67.7% 
(for best five). This result of our preliminary analysis 
110 
In the Japanese linguistics, a theory of Mi- 
nami (1974) regarding scope embedding pref- 
erence of subordinate clauses is well-known. 
Minami (1974) classifies Japanese subordinate 
clauses according to the breadths of their scopes 
and claim that subordinate clauses which inher- 
ently have narrower scopes are embedded within 
the scopes of subordinate clauses which inher- 
ently have broader scopes (details are in sec- 
tion 2). By manually analyzing several raw cor- 
pora, Minami (1974) classifies various types of 
Japanese subordinate clauses into three cate- 
gories, which are totally ordered by the embed- 
ding relation of their scopes. In the Japanese 
computational linguistics community, Shirai et 
al. (1995) employed Minami (1974)'s theory on 
scope embedding preference of Japanese sub- 
ordinate clauses and applied it to rule-based 
Japanese dependency analysis. However, in 
their approach, since categories of subordinate 
clauses are obtained by manually analyzing 
a small number of sentences, their coverage 
against a large corpus such as EDR bracketed 
corpus (EDR, 1995) is quite low. 2 
In order to realize a broad coverage and high 
performance dependency analysis of Japanese 
sentences which exploits scope embedding pref- 
erence of subordinate clauses, we propose a 
corpus-based and statistical alternative to the 
rule-based manual approach (section 3). 3 
clearly shows that dependency ambiguities of subordi- 
nate clauses are among the most problematic source of 
syntactic ambiguities in a Japanese sentence. 
2In our implementation, the coverage of the categories 
of Shirai et al (1995) is only 30% for all the subordinate 
clauses included in the whole EDR corpus. 
~Previous works on statistical dependency analysis in- 
clude Fujio and Matsumoto (1998) and Haruno et al 
(1998) in Japanese analysis as well as Lafferty et al 
(1992), Eisner (1996), and Collins (1996) in English anal- 
ysis. In later sections, we discuss the advantages of our 
approach over several closely related previous works. 
Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence 
Word Segmentation 
POS (+ conjugation form) 
Tagging 
Bunsetsu Segmentation 
Tenki ga yoi kara dekakeyou 
noun case- adjective predicate- verb 
particle (base) conjunctive-particle (volitional) 
Tenki-ga yoi-kara dekakeyou 
(Chunking) 
English Translation weather subject fine because let's go out 
(Because the weather is fine, let's go out.) 
First, we formalize the problem of decid- 
ing scope embedding preference as a classifi- 
cation problem, in which various types of lin- 
guistic information of each subordinate clause 
are encoded as features and used for deciding 
which one of given two subordinate clauses has 
a broader scope than the other. As in the case of 
Shirai et al (1995), we formalize the problem of 
deciding dependency preference of subordinate 
clauses by utilizing the correlation of scope em- 
bedding preference and dependency preference 
of Japanese subordinate clauses. Then, as a sta- 
tistical earning method, we employ the decision 
list learning method of Yarowsky (1994), where 
optimal combination of those features are se- 
lected and sorted in the form of decision rules, 
according to the strength of correlation between 
those features and the dependency preference 
of the two subordinate clauses. We evaluate 
the proposed method through the experiment 
on learning dependency preference of Japanese 
subordinate clauses from the EDR bracketed 
corpus (section 4). We show that the pro- 
posed method outperforms other related meth- 
ods/models. We also evaluate the estimated e- 
pendencies of subordinate clauses in Fujio and 
Matsumoto (1998)'s framework of the statisti- 
cal dependency analysis of a whole sentence, in 
which we successfully increase the precisions of 
both chunk level and sentence level dependen- 
cies thanks to the estimated ependencies of
subordinate clauses. 
2 Ana lyz ing  Dependenc ies  between 
Japanese  Subord inate  C lauses  
based  on  Scope  Embedd ing  
Pre ference  
2.1 Dependency  Analys is  of  A 
Japanese Sentence  
First, we overview dependency analysis of a 
Japanese sentence. Since words in a Japanese 
sentence are not segmented by explicit delim- 
iters, input sentences are first word segmented, 
111 
Phrase Structure 
Scope of 
Subordin.~.ff..f.~... 
( !(ffenki-ga) (yO~:ra))  \[ (dekakeyou)) 
t 
Dependency (modification) Relation 
Figure 1: An Example of Japanese Subordinate 
Clause (taken from the Sentence of Table 1) 
part-of-speech tagged, and then chunked into a 
sequence of segments called bunsetsus. 4 Each 
chunk (bunsetsu) generally consists of a set of 
content words and function words. Then, de- 
pendency relations among those chunks are es- 
timated, where most practical dependency ana- 
lyzers for the Japanese language usually assume 
the following two constraints: 
1. Every chunk (bunsetsu) except he last one 
modifies only one posterior chunk (bun- 
setsu). 
2. No modification crosses to other modifica- 
tions in a sentence. 
Table 1 gives an example of word segmenta- 
tion, part-of-speech tagging, and bunsetsu seg- 
mentation (chunking) of a Japanese sentence, 
where the verb and the adjective are tagged 
with their parts-of-speech as well as conjuga- 
tion forms. Figure i shows the phrase structure, 
the bracketing, 5 and the dependency (modifica- 
tion) relation of the chunks (bunsetsus) within 
the sentence. 
4Word segmentation and part-of-speech tagging are 
performed by the Japanese morphological analyzer 
Chasen (Matsumoto et al, 1997), and chunking is done 
by the preprocessor used in Fujio and Matsumoto (1998). 
5The phrase structure and the bracketing are shown 
just for explanation, and we do not consider them 
but consider only dependency relations in the analysis 
throughout this paper. 
A Japanese subordinate clause is a clause whose head chunk satisfies the following properties. 
1. The 
(a) 
(b) 
2. The 
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
(g) 
(h) 
content words part of the chunk (bunsetsu) is one of the following types: 
A predicate (i.e., a verb or an adjective). 
nouns and a copula like "Noun1 dearu" (in English, "be Noun1"). 
function words part of the chunk (bunsetsu) is one of the following types: 
Null. 
Adverb type such as "Verbl ippou-de" (in English, "(subject) Verb1 ..., on the other hand,"). 
Adverbial noun type such as "Verb1 tame" (in English, "in order to Verb1"). 
FormM noun type such as "Verb1 koto" (in English, gerund "Verbl-ing"). 
Temporal noun type such as "Verb1 mae" (in English, "before (subject) Verb1 ..."). 
A predicate conjunctive particle such as "Verbl ga" (in English, "although (subject) Verbl ...,"). 
A quoting particle such as "Verbl to (iu)" (in English, "(say) that (subject) Verbl ..."). 
(a),,~(g) followed by topic marking particles and/or sentence-final particles. 
Figure 2: Definition of Japanese Subordinate Clause 
2.2 Japanese Subordinate Clause 
The following gives the definition of what we call 
a "Japanese subordinate clause" throughout this 
paper. A clause in a sentence is represented as 
a sequence of chunks. Since the Japanese lan- 
guage is a head-final language, the clause head 
is the final chunk in the sequence. A grammati- 
cal definition of a Japanese subordinate clause is 
given in Figure 2. 6 For example, the Japanese 
sentence in Table 1 has one subordinate clause, 
whose scope is indicated as the shaded rectangle 
in Figure 1. 
2.3 Scope Embedd ing  Pre ference  o f  
Subordinate Clauses 
We introduce the concept of Minami (1974)'s 
classification of Japanese subordinate clauses 
by describing the more specific classification by 
Shirai et al (1995). From 972 newspaper 
summary sentences, Shirai et al (1995) man- 
ually extracted 54 clause final function words 
of Japanese subordinate clauses and classified 
them into the following three categories accord- 
ing to the embedding relation of their scopes. 
Category  A: Seven expressions representing 
simultaneous occurrences such as "Verb1 
SThis definition includes adnominal or noun phrase 
modifying clauses "Clause1 (NP1)" (in English, rela- 
tive clauses "(NP1) that Clause1"). Since an adnom- 
inal clause does not modify any posterior subordinate 
clauses, but modifies aposterior noun phrase, we regard 
adnominal clauses only as modifees when considering de- 
pendencies between subordinate clauses. 
to-tomoni (Clause2)" and "Verbl nagara 
(Clause2)". 
Category  B: 46 expressions representing 
cause and discontinuity such as "Verb1 
te (Clause2)" (in English "Verbl and 
(Clause2)") and "Verb1 node" (in English 
"because (subject) Verb1 ...,"). 
Category  C: One expression representing in- 
dependence, "Verb1 ga" (in English, "al- 
though (subject) Verb1 ...,"). 
The category A has the narrowest scope, while 
the category C has the broadest scope, i.e., 
Category A -4 Category B -4 Category C 
where the relation '-<~ denotes the embedding 
relation of scopes of subordinate clauses. Then, 
scope embedding preference of Japanese subor- 
dinate clauses can be stated as below: 
Scope  Embedd ing  Pre ference  of  
Japanese Subordinate Clauses  
1. A subordinate clause can be embedded within 
the scope of another subordinate clause which 
inherently has a scope of the same or a broader 
breadth. 
2. A subordinate clause can not be embedded 
within the scope of another subordinate clause 
which inherently has a narrower scope. 
For example, a subordinate clause of 'Category 
B' can be embedded within the scope of another 
subordinate clause of 'Category B' or 'Category 
C', but not within that of 'Category A'. Figure 3 
112 
(a) Category A -.< Category C
Scopes  o f  Subord inate  C lauses  
Category  C 
boi l -  pol i te /past -  scorch- per fec t -po l i te /past -per iod  stir up-wi th  a l though-  comma 
( A l though ? bo i led it with st i r r ing it up, it had  got  scorched.  ) 
(b) Category C P- Category A
Scopes of Subordinate C ~  
r Catego 
c ) ) 
boil- polite scorch fear- sbj exist- polite- hot_fwe-over stir_up-with (volitional)-period although- comma 
( Although there is some fear of  its getting scorched, let's boil it with stirring it up over a hot\]we. ) 
Figure 3: Examples of Scope Embedding of Japanese Subordinate Clauses 
(a) gives an example of an anterior Japanese 
subordinate clause ( "kakimaze-nagara", Cate- 
gory A), which is embedded within the scope 
of a posterior one with a broader scope ("ni- 
mashita-ga-,", Category C). Since the poste- 
rior subordinate clause inherently has a broader 
scope than the anterior, the anterior is embed- 
ded within the scope of the posterior. On the 
other hand, Figure 3 (b) gives an example of 
an anterior Japanese subordinate clause ("ari- 
masu-ga-,', Category C), which is not embed- 
ded within the scope of a posterior one with a 
narrower scope ( "kakimaze-nagara", Category 
A). Since the posterior subordinate clause in- 
herently has a narrower scope than the anterior, 
the anterior is not embedded within the scope 
of the posterior. 
2.4 P re ference  of Dependenc ies  
between Subord inate  Clauses based 
on Scope Embedd ing  Pre ference  
Following the scope embedding preference of 
Japanese subordinate clauses proposed by Mi- 
nami (1974), Shirai et al (1995) applied it 
to rule-based Japanese dependency analysis, 
and proposed the following preference of decid- 
ing dependencies between subordinate clauses. 
Suppose that a sentence has two subordinate 
clauses Clausez and Clause2, where the head 
vp chunk of Clauses precedes that of Clause2. 
Dependency Preference of Japanese 
Subord inate  Clauses 
1. The head vp chunk of Clause1 can modify that 
of Clause2 if Clause2 inherently has a scope of 
the same or a broader breadth compared with 
that of Clause1. 
2. The head vp chunk of Clausez can not mod- 
ify that of Clause2 if Clause2 inherently has a 
narrower scope compared with that of Clause1. 
3 Learn ing  Dependency  Pre ference  
of  Japanese  Subord inate  C lauses  
As we mentioned in section 1, the rule-based 
approach of Shirai et al (1995) to analyz- 
ing dependencies of subordinate clauses using 
scope embedding preference has serious limi- 
tation in its coverage against corpora of large 
size for practical use. In order to overcome 
the limitation of the rule-based approach, in 
this section, we propose a method of learning 
dependency preference of Japanese subordinate 
clauses from a bracketed corpus. We formalize 
the problem of deciding scope embedding pref- 
erence as a classification problem, in which var- 
ious types of linguistic information of each sub- 
ordinate clause are encoded as features and used 
for deciding which one of given two subordinate 
clauses has a broader scope than the other. As 
a statistical learning method, we employ the de- 
cision list learning method of Yarowsky (1994). 
113 
Table 2: Features of Japanese Subordinate Clauses 
Feature Type # of Feat . . . .  Each Binary Feature 
Punctuation 2 with-comma, without-comma 
Grammatical adverb, adverbial-noun, formal-noun, temporal-noun, 
(some features have distinction 17 quoting-particle, copula, predicate-conjunctive-particle, 
of chunk-final/middle) topic-marking-particle, sentence-final-particle 
12 Conjugation form of 
chunk-final conjugative word 
Lexical (lexicalized forms of 
'Grammatical' features, 
with more than 
9 occurrences 
in EDR corpus) 
235 
stem, base, mizen, ren'you, rental, conditional, 
imperative, ta, tari, re, conjecture, volitional 
adverb (e.g., ippou-de, irai), adverbial-noun (e.g., tame, baai) 
topic-marking-particle (e.g., ha, mo), quoting-particle (to), 
predicate-conjunctive-particle (e.g.,ga, kara), 
temporal-noun (e.g., ima, shunkan), formal-noun (e.g., koto), 
copula (dearu), sentence-final-particle (e.g., ka, yo) 
3.1 The  Task  Def in i t ion  
Considering the dependency preference of 
Japanese subordinate clauses described in sec- 
tion 2.4, the following gives the definition of our 
task of deciding the dependency of Japanese 
subordinate clauses. Suppose that a sen- 
tence has two subordinate clauses Clause1 and 
Clause2, where the head vp chunk of Clausel 
precedes that of Clause2. Then, our task of de- 
ciding the dependency of Japanese subordinate 
clauses is to distinguish the following two cases: 
1. The head vp chunk of Clausez modifies that of 
Clause2. 
2. The head vp chunk of Clause1 does not modify 
that of Clause2, but modifies that of another 
subordinate clause or the matrix clause which 
follows Clause2. 
Roughly speaking, the first corresponds to the 
case where Clause2 inherently has a scope of the 
same or a broader breadth compared with that 
of Clause1, while the second corresponds to the 
case where Clause2 inherently has a narrower 
scope compared with that of Clause1.7 
3.2 Dec is ion  L ist  Learn ing  
A decision list (Yarowsky, 1994) is a sorted list 
of the decision rules each of which decides the 
value of a decision D given some evidence E. 
Each decision rule in a decision list is sorted 
TOur modeling is slightly different from those of other 
standard approaches to statistical dependency analy- 
sis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno 
et al, 1998) which simply distinguish the two cases: the 
case where dependency relation holds between the given 
two vp chunks or clauses, and the case where dependency 
relation does not hold. In contrast to those standard ap- 
proaches, we ignore the case where the head vp chunk 
of Clause1 modifies that of another subordinate clause 
which precedes Clause2. This is because we assume that 
this case is more loosely related to the scope mbedding 
preference ofsubordinate clauses. 
in descending order with respect o some pref- 
erence value, and rules with higher preference 
values are applied first when applying the deci- 
sion list to some new test data. 
First, let the random variable D represent- 
ing a decision varies over several possible values, 
and the random variable E representing some 
evidence varies over '1' and '0' (where '1' de- 
notes the presence of the corresponding piece 
of evidence, '0' its absence). Then, given some 
training data in which the correct value of the 
decision D is annotated to each instance, the 
conditional probabilities P(D =x \[ E= 1) of ob- 
serving the decision D = x under the condition 
of the presence of the evidence E (E = 1) are 
calculated and the decision list is constructed 
by the following procedure. 
1. For each piece of evidence, calculate the likeli- 
hood ratio of the conditional probability of a de- 
cision D = xl (given the presence of that piece 
of evidence) to the conditional probability of 
the rest of the decisions D =-,xl: 
P(D=xl I E=I )  
l?g2 P(D='~xl \ [E=I )  
Then, a decision list is constructed with pieces 
of evidence sorted in descending order with re- 
spect to their likelihood ratios, s
2. The final line of a decision list is defined as 'a 
default', where the likelihood ratio is calculated 
as the ratio of the largest marginal probability 
of the decision D = xl to the marginal proba- 
Syarowsky (1994) discusses several techniques for 
avoiding the problems which arise when an observed 
count is 0. Among those techniques, we employ the sim- 
plest one, i.e., adding a small constant c~ (0.1 < c~ < 
0.25) to the numerator and denominator. With this 
modification, more frequent evidence is preferred when 
there exist several evidences for each of which the con- 
ditional probability P(D=x \[ E=I)  equals to 1. 
114 
(a) An Example Sentence with Chunking, Bracketing, and Dependency Relations 
Subordinate Clauses 
Clause2 
~"i:" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ar i..,,:..:.,,,,,:i:,,:~:::,i::::i::ti:, :~ :~ f (~/o : ,a , ,~(  Cs..0 (~you~,ai,-taa-toi-)) (k~s~-ga)) (dete-tu u-.) ) 
I 
raise-price 
10%-~ -although 3%- emphatic_.au?iliay - comma _verb (te-form) -comma involuntary dealer-charge-of case- sbj happen-will/may- ~dod 
( I f  the tax rate is 10%, the dealers will raise price, but, because it is 3%, there will happen to be the cases that the dealers pay the tax. ) 
(b) Feature Expression of Head VP Chunk of Subordinate Clauses 
Head VP Chunk of Subordinate Clause Feature Set 
Seg 1 : "neage-suru-ga-," 
Se92 : "3%-ha-node-," 
~-z = ~f with-comma, predicate-conjunctive-particle(chunk-final), k 
predicate-conjunctive-particle(chunk-final)-"ga" } 
~'2 = I with-comma, chunk-final-conjugative-word-te-form } 
(c) Evidence-Decision Pairs for Decision List Learning 
Evidence E (E= 1) (feature names are abbreviated) 
El 
with-comma 
I 
with-comma 
re-form 
with-comma, te-form 
with-comma 
. . .  
with-comma 
. ? .  
with-comma 
? . .  
with-comma 
. . .  
with-comma 
with-comma 
pred-conj-particle(final) 
. o ?  
with-comma, pred-conj-particle(final) 
. . .  
pred-conj-particle(final)- "ga" 
. ? .  
with-comma, pred-conj-particle(final)-"ga" 
? ? .  
Decision D 
"beyond" 
"beyond" 
"beyond" 
"beyond" 
. . .  
"beyond" 
"beyond" 
. . .  
"beyond" 
? . ,  
Figure 4: An Example of Evidence-Decision Pair of Japanese Subordinate Clauses 
bility of the rest of the decisions D-=--xz: final conjugative word: used when the chunk- 
P(D=xl )  
l?g2 P(D="x l )  
The 'default' decision of this final line is D-= xz 
with the largest marginal probability. 
3.3 Feature  of  Subord inate  C lauses  
Japanese subordinate clauses defined in sec- 
tion 2.2 are encoded using the following four 
types of features: i) Punctuation: represents 
whether the head vp chunk of the subordinate 
clause is marked with a comma or not, ii) Gram- 
matical: represents parts-of-speech of function 
words of the head vp chunk of the subordi- 
nate clause, 9 iii) Conjugation form of chunk- 
9Terms of parts-of-speech tags and conjugation forms 
are borrowed from those of the Japanese morphological 
ana/ysis ystem Chasen (Matsumoto et al, 1997). 
final word is conjugative, iv) Lexical: lexicalized 
forms of 'Grammatical '  features which appear 
more than 9 times in EDR corpus. Each fea- 
ture of these four types is binary and its value 
is '1' or '0' ('1' denotes the presence of the cor- 
responding feature, '0' its absence). The whole 
feature set shown in Table 2 is designed so as to 
cover the 210,000 sentences of EDR corpus. 
3.4 Dec is ion  L ist  Learn ing  of 
Dependency  Pre ference  of  
Subord inate  C lauses  
First, in the modeling of the evidence, we con- 
sider every possible correlation (i.e., depen- 
dency) of the features of the subordinate clauses 
listed in section 3.3. Furthermore, since it is 
necessary to consider the features for both of the 
given two subordinate clauses, we consider all 
115 
the possible combination of features of the an- 
terior and posterior head vp chunks of the given 
two subordinate clauses. More specifically, let 
Seg\] and Seg2 be the head vp chunks of the 
given two subordinate clauses (Segl is the ante- 
rior and Seg2 is the posterior). Also let 9Vl and 
9r2 be the sets of features which Segl and Seg2 
have, respectively (i.e., the values of these fea- 
tures are '1'). We consider every possible subset 
F1 and F2 of ~-1 and ~2, respectively, and then 
model the evidence of the decision list learning 
method as any possible pair (F1, F2)3 ? 
Second, in the modeling of the decision, we 
distinguish the two cases of dependency rela- 
tions described in section 3.1. We name the first 
case as the decision "modify", while the second 
as the decision "beyond". 
3.5 Example  
Figure 4 illustrates an example of transforming 
subordinate clauses into feature xpression, and 
then obtaining training pairs of an evidence and 
a decision from a bracketed sentence. Figure 4 
(a) shows an example sentence which contains 
two subordinate clauses Clause1 and Clause2, 
with chunking, bracketing, and dependency re- 
lations of chunks. Both of the head vp chunks 
Segl and Seg2 of Clause1 and Clause2 modify 
the sentence-final vp chunk. As shown in Fig- 
ure 4 (b), the head vp chunks Segl and Seg2 
have feature sets ~'1 and ~'2, respectively. Then, 
every possible subsets F1 and F2 of ~1 and 
~2 are considered, n respectively, and training 
pairs of an evidence and a decision are collected 
as in Figure 4 (c). In this case, the value of the 
decision D is "beyond", because Segl modifies 
the sentence-final vp chunk, which follows Seg 2. 
1?Our formalization of the evidence of decision list 
learning has an advantage over the decision tree learn- 
ing (Quinlan, 1993) approach to feature selection of de- 
pendency analysis (Haruno et al, 1998). In the feature 
selection procedure of the decision tree learning method, 
the utility of each feature is evaluated independently, 
and thus the utility of the combination of more than one 
features is not evaluated irectly. On the other hand, in 
our formalization of the evidence of decision list learn- 
ing, we consider every possible pair of the subsets F1 and 
Fz, and thus the util ity of the combination of more than 
one features is evaluated irectly. 
lXSince the feature 'predicate-conjunctive- 
particle(chunk-final)' subsumes 'predicate-conjunctive- 
particle(chunk-final)-"ga", they are not considered 
together as one evidence. 
i . \ 
Coverage (Model (b)) . . . . . . .  " ...... \~  \ P~OurModet) --.-- "~.. \ \ 
0.5 0.55 0.6 0.65 0.7 0.75 0.8 0,85 0.9 0.95 
Lower Bound of P(DIE ) 
Figure 5: Precisions and Coverages of Deciding 
Dependency between Two Subordinate Clauses 
100 , , ,~  , , ' Our Model , ' 
~_~ Model (a) ......... ......  Model (b) ........... 
95 
90 
ii . . . . . . . .  ~ , .  
"m. .  
80 
75 . . . .  "i "": .......... '~. , 
0 20 40 60 80 100 
Coverage (%) 
Figure 6: Correlation of Coverages and Precisions 
4 Exper iments  and Evaluat ion 
We divided the 210,000 sentences of the whole 
EDR bracketed Japanese corpus into 95% train- 
ing sentences and 5~0 test sentences. Then, 
we extracted 162,443 pairs of subordinate 
clauses from the 199,500 training sentences, and 
learned a decision list for dependency prefer- 
ence of subordinate clauses from those pairs. 
The default decision in the decision list is 
D ="beyond", where the marginal probability 
P(D = "beyond") = 0.5378, i.e., the baseline 
precision of deciding dependency between two 
subordinate clauses is 53.78 %. We limit the fre- 
quency of each evidence-decision pair to be more 
than 9. The total number of obtained evidence- 
decision pairs is 7,812. We evaluate the learned 
decision list through several experiments. 12 
First, we apply the learned decision list to 
deciding dependency between two subordinate 
clauses of the 5% test sentences. We change 
the threshold of the probability P(D I E) 13 in 
12Details of the experimental evaluation will be pre- 
sented in Utsuro (2000). 
I~P( D I E) can be used equivalently to the likelihood 
116 
the decision list and plot the trade-off between 
coverage and precision. 14 As shown in the plot 
of "Our Model" in Figure 5, the precision varies 
from 78% to 100% according to the changes of 
the threshold of the probability P(D I E). 
Next, we compare our model with the other 
two models: (a) the model learned by apply- 
ing the decision tree learning method of Haruno 
et al (1998) to our task of deciding depen- 
dency between two subordinate clauses, and (b) 
a decision list whose decisions are the following 
two cases, i.e., the case where dependency rela- 
tion holds between the given two vp chunks or 
clauses, and the case where dependency relation 
does not hold. The model (b) corresponds to a 
model in which standard approaches to statis- 
tical dependency analysis (Collins, 1996; Fujio 
and Matsumoto, 1998; Haruno et al, 1998) are 
applied to our task of deciding dependency be- 
tween two subordinate clauses. Their results 
are also in Figures 5 and 6. Figure 5 shows that 
"Our Model" outperforms the other two mod- 
els in coverage. Figure 6 shows that our model 
outperforms both of the models (a) and (b) in 
coverage and precision. 
Finally, we examine whether the estimated 
dependencies of subordinate clauses improve 
the precision of Fujio and Matsumoto (1998)'s 
statistical dependency analyzer. 15 Depending 
on the threshold of P(D \[ E), we achieve 
0.8,,~1.8% improvement in chunk level precision, 
and 1.6~-4.7% improvement in sentence level, is 
5 Conc lus ion  
This paper proposed a statistical method for 
learning dependency preference of Japanese 
ratio. 
14Coverage: the rate of the pairs of subordinate clauses 
whose dependencies are decided by the decision list, 
against he total pairs of subordinate clauses, Precision: 
the rate of the pairs of subordinate clauses whose depen- 
dencies are correctly decided by the decision list, against 
those covered pairs of subordinate clauses. 
15Fujio and Matsumoto (1998)'s lexicalized depen- 
dency analyzer is similar to that of Collins (1996), where 
various features were evaluated through performance 
test and an optimal feature set was manually selected. 
16The upper bounds of the improvement in chunk level 
and sentence level precisions, which are estimated by 
providing Fujio and Matsumoto (1998)'s tatistical de- 
pendency analyzer with correct dependencies of subor- 
dinate clauses extracted from the bracketing of the EDR 
corpus, are 5.1% and 15%, respectively. 
subordinate clauses, in which scope embed- 
ding preference of subordinate clauses is ex- 
ploited. We evaluated the estimated ependen- 
cies of subordinate clauses through several ex- 
periments and showed that our model outper- 
formed other related models. 
Re ferences  
M. Collins. 1996. A new statistical parser based on 
bigram lexical dependencies. In Proceedings of the 
34th Annual Meeting of ACL, pages 184-191. 
EDR (Japan Electronic Dictionary Research Insti- 
tute, Ltd.). 1995. EDR Electronic Dictionary 
Technical Guide. 
J. Eisner. 1996. Three new probabilistic models for 
dependency parsing: An exploration. In Proceed- 
ings of the 16th COLING, pages 340-345. 
M. Fujio and Y. Matsumoto. 1998. Japanese de- 
pendency structure analysis based on lexicalized 
statistics. In Proceedings of the 3rd Conference on 
Empirical Methods in Natural Language Process- 
ing, pages 88--96. 
M. Haruno, S. Shirai, and Y. Oyama. 1998. Us- 
ing decision trees to construct a practical parser. 
In Proceedings of the 17th COLING and the 36th 
Annual Meeting of ACL, pages 505-511. 
J. Lafferty, D. Sleator, and D. Temperley. 1992. 
Grammatical trigrams: A probabilistic model of 
link grammar. In Proceedings of the AAAI Fall 
Symposium: Probabilistic Approaches to Natural 
Language, pages 89-97. 
Y. Matsumoto, A. Kitauchi, T. Yamashita, 
O. Imaichi, and T. Imamura. 1997. Japanese 
morphological nalyzer ChaSen 1.0 users manual. 
Information Science Technical Report NAIST-IS- 
TR9?007, Nara Institute of Science and Technol- 
ogy. (in Japanese). 
F. Minami. 1974. Gendai Nihongo no Kouzou. 
Taishuukan Shoten. (in Japanese). 
J. R. Quinlan. 1993. CJ.5: Programs for Machine 
Learning. Morgan Kaufmann. 
S. Shirai, S. Ikehara, A. Yokoo, and J. Kimura. 1995. 
A new dependency analysis method based on 
semantically embedded sentence structures and 
its performance on Japanese subordinate clauses. 
Transactions of Information Processing Society of 
Japan, 36(10):2353-2361. (in Japanese). 
T. Utsuro. 2000. Learning preference of depen- 
dency between Japanese subordinate clauses and 
its evaluation i  parsing. In Proceedings of the Pad 
International Conference on Language Resources 
and Evaluation. (to appear). 
D. Yarowsky. 1994. Decision lists for lexical ambi- 
guity resolution: Application to accent restora- 
tion in Spanish and French. In Proceedings of the 
32nd Annual Meeting of A CL, pages 88-95. 
117 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 353?360,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Japanese Idiom Recognition:
Drawing a Line between Literal and Idiomatic Meanings
Chikara Hashimoto? Satoshi Sato? Takehito Utsuro?
? Graduate School of
Informatics
Kyoto University
Kyoto, 606-8501, Japan
? Graduate School of
Engineering
Nagoya University
Nagoya, 464-8603, Japan
? Graduate School of Systems
and Information Engineering
University of Tsukuba
Tsukuba, 305-8573, Japan
Abstract
Recognizing idioms in a sentence is im-
portant to sentence understanding. This
paper discusses the lexical knowledge of
idioms for idiom recognition. The chal-
lenges are that idioms can be ambiguous
between literal and idiomatic meanings,
and that they can be ?transformed? when
expressed in a sentence. However, there
has been little research on Japanese idiom
recognition with its ambiguity and trans-
formations taken into account. We pro-
pose a set of lexical knowledge for idiom
recognition. We evaluated the knowledge
by measuring the performance of an idiom
recognizer that exploits the knowledge. As
a result, more than 90% of the idioms in a
corpus are recognized with 90% accuracy.
1 Introduction
Recognizing idioms in a sentence is important to
sentence understanding. Failure of recognizing id-
ioms leads to, for example, mistranslation.
In the case of the translation service of Excite1,
it sometimes mistranslates sentences that contain
idioms such as (1a), due to the recognition failure.
(1) a. Kare-wa
he-TOP
mondai-no
problem-GEN
kaiketu-ni
solving-DAT
hone-o
bone-ACC
o-tta.
break-PAST
?He made an effort to solve the problem.?
b. ?He broke his bone to the resolution of a
question.?
1http://www.excite.co.jp/world/
(1a) contains an idiom, hone-o oru (bone-ACC
break) ?make an effort.? (1b) is the mistranslation
of (1a), in which the idiom is interpreted literally.
In this paper, we discuss lexical knowledge for
idiom recognition. The lexical knowledge is im-
plemented in an idiom dictionary that is used by
an idiom recognizer we implemented. Note that
the idiom recognition we define includes distin-
guishing literal and idiomatic meanings.2 Though
there has been a growing interest in MWEs (Sag
et al, 2002), few proposals on idiom recognition
take into account ambiguity and transformations.
Note also that we tentatively define an idiom as a
phrase that is semantically non-compositional. A
precise characterization of the notion ?idiom? is
beyond the scope of the paper.3
Section 2 defines what makes idiom recognition
difficult. Section 3 discusses the classification of
Japanese idioms, the requisite lexical knowledge,
and implementation of an idiom recognizer. Sec-
tion 4 evaluates the recognizer that exploits the
knowledge. After the overview of related works
in Section 5, we conclude the paper in Section 6.
2 Two Challenges of Idiom Recognition
Two factors make idiom recognition difficult: am-
biguity between literal and idiomatic meanings
and ?transformations? that idioms could un-
dergo.4 In fact, the mistranslation in (1) is caused
by the inability of disambiguation between the two
meanings. ?Transformation? also causes mistrans-
2Some idioms represent two or three idiomatic meanings.
But those meanings in an idiom are not distinguished. We
concerned only whether a phrase is used as an idiom or not.
3For a detailed discussion of what constitutes the notion
of (Japanese) idiom, see Miyaji (1982), which details usages
of commonly used Japanese idioms.
4The term ?transformation? in the paper is not relevant to
the Chomskyan term in Generative Grammar.
353
lation. Sentences in (2) and (3a) contain an idiom,
yaku-ni tatu (part-DAT stand) ?serve the purpose.?
(2) Kare-wa
he-TOP
yaku-ni
part-DAT
tatu.
stand
?He serves the purpose.?
(3) a. Kare-wa
he-TOP
yaku-ni
part-DAT
sugoku
very
tatu.
stand
?He really serves the purpose.?
b. ?He stands enormously in part.?
Google?s translation system5 mistranslates (3a) as
in (3b), which does not make sense,6 though it suc-
cessfully translates (2). The only difference be-
tween (2) and (3a) is that bunsetu7 constituents of
the idiom are detached from each other.
3 Knowledge for Idiom Recognition
3.1 Classification of Japanese Idioms
Requisite lexical knowledge to recognize an idiom
depends on how difficult it is to recognize it. Thus,
we first classify idioms based on recognition diffi-
culty. The recognition difficulty is determined by
the two factors: ambiguity and transformability.
Consequently, we identify three classes (Figure
1).8 Class A is not transformable nor ambigu-
ous. Class B is transformable but not ambiguous.9
Class C is transformable and ambiguous. Class A
amounts to unambiguous single words, which are
easy to recognize, while Class C is the most diffi-
cult to recognize. Only Class C needs further clas-
sifications, since only Class C needs disambigua-
tion and lexical knowledge for disambiguation de-
pends on its part-of-speech (POS) and internal
structure. The POS of Class C is either verbal
or adjectival, as in Figure 1. Internal structure
represents constituent words? POS and a depen-
dency between bunsetus. The internal structure
5http://www.google.co.jp/language tools
6In fact, the idiom has no literal interpretation.
7A bunsetu is a syntactic unit in Japanese, consisting of
one independent word and more than zero ancillary words.
The sentence in (3a) consists of four bunsetu constituents.
8The blank space at the upper left in the figure implies that
there is no idiom that does not undergo any transformation
and yet is ambiguous. Actually, we have not come up with
such an example that should fill in the blank space.
9Anonymous reviewers pointed out that Class A and B
could also be ambiguous. In fact, one can devise a context
that makes the literal interpretation of those Classes possible.
However, virtually no phrase of Class A or B is interpreted
literally in real texts, and we think our generalization safely
captures the reality of idioms.
A
m
bi
gu
ou
s
U
na
m
bi
gu
o
u
s
TransformableUntransformable
Class B
yaku-ni
part-DAT
tatu
stand
?serve the purpose?
- Verbal
- Adjectival
Class C
hone-o
bone-ACC
oru
break
?make an effort?
- Verbal
- Adjectival
Class A
mizu-mo
water-TOO
sitataru
drip
?extremely handsome?
- Adnominal
- Nominal
- Adverbial
More Difficult
Figure 1: Idiom Classification based on the
Recognition Difficulty
of hone-o oru (bone-ACC bone), for instance, is
?(Noun/Particle Verb),? abbreviated as ?(N/P V).?
Then, let us give a full account of the further
classification of Class C. We exploit grammatical
differences between literal and idiomatic usages
for disambiguation. We will call the knowledge of
the differences the disambiguation knowledge.
For instance, a phrase, hone-o oru, does not al-
low passivization when used as an idiom, though
it does when used literally. Thus, (4), in which the
phrase is passivized, cannot be an idiom.
(4) hone-ga
bone-NOM
o-rareru
break-PASS
?A bone is broken.?
In this case, passivizability can be used as a dis-
ambiguation knowledge. Also, detachability of
the two bunsetu constituents can serve for disam-
biguating the idiom; they cannot be separated. In
general, usages applicable to idioms are also ap-
plicable to literal phrases, but the reverse is not
always true (Figure 2). Then, finding the disam-
Usages Applicable to Only Literal Phrases
Usages Applicable to Both
Idioms and Literal Phrases
Figure 2: Difference of Applicable Usages
biguation knowledge amounts to finding usages
applicable to only literal phrases.
Naturally, the disambiguation knowledge for an
idiom depends on its POS and internal structure.
354
As for POS, disambiguation of verbal idioms can
be performed by the knowledge of passivizability,
while that of adjectival idioms cannot. Regarding
internal structure, detachability should be anno-
tated on every boundary of bunsetus. Thus, the
number of annotations of detachability depends on
the number of bunsetus of an idiom.
There is no need for further classification of
Class A and B, since lexical knowledge for them is
invariable. The next section mentions their invari-
ableness. After all, Japanese idioms are classified
as in Figure 3. The whole picture of the subclasses
of Class C remains to be seen.
3.2 Knowledge for Each Class
What lexical knowledge is needed for each class?
Class A needs only a string information; idioms
of the class amount to unambiguous single words.
A string information is undoubtedly invariable
across all kinds of POS and internal structure.
Class B requires not only a string but also
knowledge that normalizes transformations id-
ioms could undergo, such as passivization and de-
tachment of bunsetus. We identify three types of
transformations that are relevant to idioms: 1) De-
tachment of Bunsetu Constituents, 2) Predicate?s
Change, and 3) Particle?s Change. Predicate?s
change includes inflection, attachment of a neg-
ative morpheme, a passive morpheme or modal
verbs, and so on. Particle?s change represents at-
tachment of topic or restrictive particles. (5b) is an
example of predicate?s change from (5a) by adding
a negative morpheme to a verb. (5c) is an example
of particle?s change from (5a) by adding a topic
particle to the preexsistent particle of an idiom.
(5) a. Kare-wa
he-TOP
yaku-ni
part-DAT
tatu.
stand
?He serves the purpose.?
b. Kare-wa
he-TOP
yaku-ni
part-DAT
tat-anai.
stand-NEG
?He does not serve the purpose.?
c. Kare-wa
he-TOP
yaku-ni-wa
part-DAT-TOP
tatu.
stand
?He serves the purpose.?
To normalize the transformations, we utilize a
dependency relation between constituent words,
and we call it the dependency knowledge. This
amounts to checking the presence of all the con-
stituent words of an idiom. Note that we ignore,
among constituent words, endings of a predicate
and case particles, ga (NOM) and o (ACC), since
they could change their forms or disappear.
The dependency knowledge is also invariable
across all kinds of POS and internal structure.
Class C requires the disambiguation knowl-
edge, as well as all the knowledge for Class B.
As a result, all the requisite knowledge for id-
iom recognition is summarized as in Table 1.
String Dependency Disambiguation
Class A ?
Class B ? ?
Class C ? ? ?
Table 1: Requisite Knowledge for each Class
As discussed in ?3.1, the disambiguation
knowledge for an idiom depends on which sub-
class it belongs to. A comprehensive idiom recog-
nizer calls for all the disambiguation knowledge
for all the subclasses, but we have not figured out
all of them. Then, we decided to blaze a trail to
discover the disambiguation knowledge by inves-
tigating the most commonly used idioms.
3.3 Disambiguation Knowledge for the
Verbal (N/P V) Idioms
What type of idiom is used most commonly? The
answer is the verbal (N/P V) type like hone-
o oru (bone-ACC break); it is the most abundant in
terms of both type and token. Actually, 1,834 out
of 4,581 idioms (40%) in Kindaichi and Ikeda
(1989), which is a Japanese dictionary with more
than 100,000 words, are this type.10 Also, 167,268
out of 220,684 idiom tokens in Mainichi newspa-
per of 10 years (?91??00) (76%) are this type.11
Then we discuss what can be used to disam-
biguate the verbal (N/P V) type. First, we exam-
ined literature of linguistics (Miyaji, 1982; Morita,
1985; Ishida, 2000) that observed characteristics
of Japanese idioms. Then, among the characteris-
tics, we picked those that could help with the dis-
ambiguation of the type. (6) summarizes them.
10Counting was performed automatically by means of the
morphological analyzer ChaSen (Matsumoto et al, 2000)
with no human intervention. Note that Kindaichi and Ikeda
(1989) consists of 4,802 idioms, but 221 of them were ig-
nored since they contained unknown words for ChaSen.
11We counted idiom tokens by string matching with inflec-
tion taken into account. And we referred to Kindaichi and
Ikeda (1989) for a comprehensive idiom list. Note that count-
ing was performed totally automatically.
355
Recognition
Difficulty
POS
Internal
Structure
Japanese Idioms
Class C
Verb
(N/P V)
hone-o
bone-ACC
oru
break
?make an effort?
(N/P N/P V)
mune-ni
chest-DAT
te-o
hand-ACC
ateru
put
?think over?
? ? ?
Adj
(N/P A)
atama-ga
head-NOM
itai
ache
?be in trouble?
? ? ?
Class B
yaku-ni
part-DAT
tatu
stand
?serve the purpose?
Class A
mizu-mo
water-TOO
sitataru
drip
?extremely handsome?
Figure 3: Classification of Japanese Idioms for the Recognition Task
(6) Disambiguation Knowledge for the
Verbal (N/P V) Idioms
a. Adnominal Modification Constraints
I. Relative Clause Prohibition
II. Genitive Phrase Prohibition
III. Adnominal Word Prohibition
b. Topic/Restrictive Particle Constraints
c. Voice Constraints
I. Passivization Prohibition
II. Causativization Prohibition
d. Modality Constraints
I. Negation Prohibition
II. Volitional Modality Prohibition12
e. Detachment Constraint
f. Selectional Restriction
For example, the idiom, hone-o oru, does not al-
low adnominal modification by a genitive phrase.
Thus, (7) can be interpreted only literally.
(7) kare-no
he-GEN
hone-o
bone-ACC
oru
break
?(Someone) breaks his bone.?
That is, the Genitive Phrase Prohibition, (6aII), is
in effect for the idiom. Likewise, the idiom does
not allow its case particle o (ACC) to be substi-
tuted with restrictive particles such as dake (only).
Thus, (8) represents only a literal meaning.
(8) hone-dake
bone-ONLY
oru
break
?(Someone) breaks only some bones.?
12
?Volitional Modality? represents those verbal expres-
sions of order, request, permission, prohibition, and volition.
This means the Restrictive Particle Constraint,
(6b), is also in effect. Also, (4) shows that the
Passivization Prohibition, (6cI), is in effect, too.
Note that the constraints in (6) are not always
in effect for an idiom. For instance, the Causativi-
zation Prohibition, (6cII), is invalid for the idiom,
hone-o oru. In fact, (9a) can be interpreted both
literally and idiomatically.
(9) a. kare-ni
he-DAT
hone-o
bone-ACC
or-aseru
break-CAUS
b. ?(Someone) makes him break a bone.?
c. ?(Someone) makes him make an effort.?
3.4 Implementation
We implemented an idiom dictionary based on the
outcome above and a recognizer that exploits the
dictionary. This section illustrates how they work,
and we focus on Class B and C hereafter.
The idiom recognizer looks up dependency
patterns in the dictionary that match a part of the
dependency structure of a sentence (Figure 4). A
dependency pattern is equipped with all the req-
uisite knowledge for idiom recognition. Rough
sketch of the recognition algorithm is as follows:
1. Analyze the morphology and dependency
structures of an input sentence.
2. Look up dependency patterns in the dictio-
nary that match a part of the dependency
structure of the input sentence.
3. Mark constituents of an idiom in the sentence
if any.13 Constituents that are marked are
constituent words and bunsetu constituents
that include one of those constituent words.
13As a constituent marker, we use an ID that is assigned to
each idiom in the dictionary.
356
Input
yaku-ni-wa
part-DAT-TOP
mattaku
totally
tat-anai
stand-NEG
Morphology &
Dependency
Analysis
Dependency
Matching
yaku
part
/ ni
DAT
/ wa
TOP
mattaku
totally
tatu
stand
/ nai
NEG
Output
yaku
part
/ ni
DAT
/ wa
TOP
mattaku
totally
tatu
stand
/ nai
NEG
Idiom
Recognizer
Idiom
Dictionary
? ? ?
yaku
part
/ ni
DAT
tatu
stand
? ? ?
Dependency Pattern
Figure 4: Internal Working of the Idiom Recognizer
Input Output
Idiom
Recognizer
ChaSen
Morphology
Analysis
CaboCha
Dependency
Analysis
TGrep2
Dependency
Matching
Dependency Pattern
Generator Pattern DB
Idiom
Dictionary
Figure 5: Organization of the System
As in Figure 5, we use ChaSen as a morphol-
ogy analyzer and CaboCha (Kudo and Matsumoto,
2002) as a dependency analyzer. Dependency
matching is performed by TGrep2 (Rohde, 2005),
which finds syntactic patterns in a sentence or tree-
bank. The dependency pattern is usually getting
complicated since it is tailored to the specifica-
tion of TGrep2. Thus, we developed the Depen-
dency Pattern Generator that compiles the pattern
database from a human-readable idiom dictionary.
Only the difference in treatments of Class B and
C lies in their dependency patterns. The depen-
dency pattern of Class B consists of only its depen-
dency knowledge, while that of Class C consists
of not only its dependency knowledge but also its
disambiguation knowledge (Figure 6).
The idiom dictionary consists of 100 idioms,
which are all verbal (N/P V) and belong to either
Class B or C. Among the knowledge in (6), the
Selectional Restriction has not been implemented
yet. The 100 idioms are those that are used most
frequently. To be precise, 50 idioms in Kindaichi
and Ikeda (1989) and 50 in Miyaji (1982) were
extracted by the following steps:14
1. From Miyaji (1982), 50 idioms that were
14We counted idiom tokens by string matching with inflec-
tion taken into account. Note that counting was performed
automatically without human intervention.
used most frequently in Mainichi newspaper
of 10 years (?91??00) were extracted.
2. From Kindaichi and Ikeda (1989), 50 idioms
that were used most frequently in the newspa-
per of 10 years but were not included in the
50 idioms from Miyaji (1982) were extracted.
As a result, 66 out of the 100 idioms were Class
B, and the other 34 idioms were Class C.15
4 Evaluation
4.1 Experiment Condition
We conducted an experiment to see the effective-
ness of the lexical knowledge we proposed.
As an evaluation corpus, we collected 300 ex-
ample sentences of the 100 idioms from Mainichi
newspaper of ?95: three sentences for each id-
iom. Then we added another nine sentences for
three idioms that are orthographic variants of one
of the 100 idioms. Among the three idioms, one
belonged to Class B and the other two belonged to
Class C. Thus, 67 out of the 103 idioms were Class
B and the other 36 were Class C. After all, 309
15We found that the most frequently used 100 idioms in
Kindaichi and Ikeda (1989) cover as many as 53.49% of all
tokens in Mainichi newspaper of 10 years. This implies that
our dictionary accounts for approximately half of all idiom
tokens in a corpus.
357
Dependency Pattern
Disambiguation
Knowledge
?Adnominal
Modification Cs
?Topic/Restrictive
Particle Cs
?Detachment C
?Voice Cs
?Modality Cs
Dependency
Knowledge
? Dependency of Constituents
hone
bone
/ o
ACC
oru
break
hone
bone
/ o
ACC
oru
break
Figure 6: Dependency Pattern of Class C
sentences were prepared. Table 2 shows the break-
down of them. ?Positive? indicates sentences in-
Class B Class C Total
Positive 200 66 266
Negative 1 42 43
Total 201 108 309
Table 2: Breakdown of the Evaluation Corpus
cluding a true idiom, while ?Negative? indicates
those including a literal-usage ?idiom.?
A baseline system was prepared to see the ef-
fect of the disambiguation knowledge. The base-
line system was the same as the recognizer except
that it exploited no disambiguation knowledge.
4.2 Result
The result is shown in Table 3. The left side shows
the performances of the recognizer, while the right
side shows that of the baseline. Differences of per-
formances between the two systems are marked
with bold. Recall, Precision, and F-Measure, are
calculated using the following equations.
Recall =
|Correct Outputs|
|Positive|
Precision =
|Correct Outputs|
|All Outputs|
F -Measure =
2? Precision ?Recall
Precision+Recall
As a result, more than 90% of the idioms can be
recognized with 90% accuracy. Note that the rec-
ognizer made fewer errors due to the employment
of the disambiguation knowledge.
The result shows the high performances. How-
ever, there turns out to be a long way to go to solve
the most difficult problem of idiom recognition:
drawing a line between literal and idiomatic mean-
ings. In fact, the precision of recognizing idioms
of Class C remains less than 70% as in Table 3.
Besides, the recognizer successfully rejected only
15 out of 42 negative sentences. That is, its suc-
cess rate of rejecting negative ones is only 35.71%
4.3 Discussion of the Disambiguation
Knowledge
First of all, positive sentences, i.e., sentences con-
taining true idioms, are in the blank region of Fig-
ure 2, while negative ones, i.e., those containing
literal phrases, are in both regions. Accordingly,
the disambiguation amounts to i) rejecting nega-
tive ones in the shaded region, ii) rejecting nega-
tive ones in the blank region, or iii) accepting pos-
itive ones in the blank region. i) is relatively easy
since there are visible evidences in a sentence that
tell us that it is NOT an idiom. However, ii) and
iii) are difficult due to the absence of visible evi-
dences. Our method is intended to perform i), and
thus has an obvious limitation.
Next, we look cloosely at cases of success or
failure of rejecting negative sentences. There were
15 cases where rejection succeeded, which corre-
spond to i). The disambiguation knowledge that
contributed to rejection and the number of sen-
tences it rejects are as follows.16
1. Genitive Phrase Prohibition (6aII) . . . . . . . 6
2. Relative Clause Prohibition (6aI) . . . . . . . . 5
3. Detachment Constraint (6e) . . . . . . . . . . . . . 2
4. Negation Prohibition (6dI) . . . . . . . . . . . . . . 1
This shows that the Adnominal Modification Con-
straints, 1. and 2. above, are the most effective.
There were 27 cases where rejection failed.
These are classified into two types:
16There was one case where rejection succeeded due to the
dependency analysis error.
358
Class B Class C All
Recall 0.975 (195
200
) 0.939 (62
66
) 0.966 (257
266
)
Precision 1.000 (195
195
) 0.697 (62
89
) 0.905 (257
284
)
F-Measure 0.987 0.800 0.935
Class B Class C All
0.975 (195
200
) 0.939 (62
66
) 0.966 (257
266
)
1.000 (195
195
) 0.602 ( 62
103
) 0.862 (257
298
)
0.987 0.734 0.911
Table 3: Performances of the Recognizer (left side) and the Baseline System (right side)
1. Those that could have been rejected by the
Selectional Restriction (6f) . . . . . . . . . . . . . .5
2. Those that might be beyond the current tech-
nology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1. and 2. correspond to i) and ii), respectively.
We see that the Selectional Restriction would have
been as effective as the Adnominal Modification
Constraints. A part of a sentence that the knowl-
edge could have rejected is below.
(10) basu-ga
bus-NOM
tyuu-ni
midair-DAT
ui-ta
float-PAST
?The bus floated in midair.?
An idiom, tyuu-ni uku (midair-DAT float) ?remain
to be decided,? takes as its argument something
that can be decided, i.e., ?1000:abstract? rather
than ?2:concrete? in the sense of the Goi-Taikei
ontology (Ikehara et al, 1997). Thus, (10) has no
idiomatic sense.
A simplified example of 2. is illustrated in (11).
(11) ase-o
sweat-ACC
nagasi-te
shed-and
huku-o
clothes-ACC
kiru-yorimo,
wear-rather.than,
hadaka-ga
nudity-NOM
gouriteki-da
rational-DECL
?It makes more sense to be naked than
wearing clothes in a sweat.?
The phrase ase-o nagasu (sweat-ACC shed) could
have been an idiom meaning ?work hard.? It is
contextual knowledge that prevented it from being
the idiom. Clearly, our technique is unable to han-
dle such a case, which belongs to ii), since no vis-
ible evidence is available. Dealing with that might
require some sort of machine learning technique
that exploits contextual information. Exploring
that possibility is one of our future works.
Finally, the 42 negative sentences consist of 15
sentences, which we could disambiguate, 5 sen-
tences, which Selectional Restriction could have
disambiguated, and 22, which belong to ii) and are
beyond the current technique. Thus, the real chal-
lenge lies in 7% ( 22
309
) of all idiom occurrences.
4.4 Discussion of the Dependency Knowledge
The dependency knowledge failed in only five
cases. Three of them were due to the defect
of dealing with case particles? change like omis-
sion. The other two cases were due to the noun
constituent?s incorporation into a compound noun.
(12) is a part of such a case.
(12) kaihuku-kidou-ni
recovery-orbit-DAT
nori-hajimeru
ride-begin
?(Economics) get back on a recovery track.?
The idiom, kidou-ni noru (orbit-DAT ride) ?get on
track,? has a constituent, kidou, which is incorpo-
rated into a compound noun kaihuku-kidou ?re-
covery track.? This is unexpected and cannot be
handled by the current machinery.
5 Related Work
There has been a growing awareness of Japanese
MWE problems (Baldwin and Bond, 2002). How-
ever, few attempts have been made to recognize id-
ioms in a sentence with their ambiguity and trans-
formations taken into account. In fact, most of
them only create catalogs of Japanese idiom: col-
lecting idioms as many as possible and classifying
them based on some general linguistic properties
(Tanaka, 1997; Shudo et al, 2004).
A notable exception is Oku (1990); his id-
iom recognizer takes the ambiguity and transfor-
mations into account. However, he only uses
the Genitive Phrase Prohibition, the Detachment
Constraint, and the Selectional Restriction, which
would be too few to disambiguate idioms.17 As
well, his classification does not take the recogni-
tion difficulty into account. This makes his id-
iom dictionary get bloated, since disambiguation
knowledge is given to unambiguous idioms, too.
Uchiyama et al (2005) deals with disambiguat-
ing some Japanese verbal compounds. Though
verbal compounds are not counted as idioms, their
study is in line with this study.
17We cannot compare his recognizer with ours numerically
since no disambiguation success rate is presented in Oku
(1990); only the overall performance is presented.
359
Our classification of idioms correlates loosely
with that of MWEs by Sag et al (2002). Japanese
idioms that we define correspond to lexicalized
phrases. Among lexicalized phrases, fixed expres-
sions are equal to Class A. Class B and C roughly
correspond to semi-fixed or syntactically-flexible
expressions. Note that, though the three subtypes
of lexicalized phrases are distinguished based on
what we call transformability, no distinction is
made based on the ambiguity.18
6 Conclusion
Aiming at Japanese idiom recognition with am-
biguity and transformations taken into accout, we
proposed a set of lexical knowledge for idioms and
implemented a recognizer that exploits the knowl-
edge. We maintain that requisite knowledge de-
pends on its transformability and ambiguity; trans-
formable idioms require the dependency knowl-
edge, while ambiguous ones require the disam-
biguation knowledge as well as the dependency
knowledge. As the disambiguation knowledge,
we proposed a set of constraints applicable to a
phrase when it is used as an idiom. The experi-
ment showed that more than 90% idioms could be
recognized with 90% accuracy but the success rate
of rejecting negative sentences remained 35.71%.
The experiment also revealed that, among the dis-
ambiguation knowledge, the Adnominal Modifi-
cation Constraints and the Selectional Restriction
are the most effective.
What remains to be done is two things; one is
to reveal all the subclasses of Class C and all the
disambiguation knowledge, and the other is to ap-
ply a machine learning technique to disambiguat-
ing those cases that the current technique is unable
to handle, i.e., cases without visible evidence.
In conclusion, there is still a long way to go to
draw a perfect line between literal and idiomatic
meanings, but we believe we broke new ground in
Japanese idiom recognition.
Acknowledgment A special thank goes to
Gakushu Kenkyu-sha, who permitted us to use
Gakken?s Dictionary for our research.
18The notion of decomposability of Sag et al (2002)
and Nunberg et al (1994) is independent of ambigu-
ity. In fact, ambiguous idioms are either decomposable
(hara-ga kuroi (belly-NOM black) ?black-hearted?) or non-
decomposable (hiza-o utu (knee-ACC hit) ?have a brain-
wave?). Also, unambiguous idioms are either decomposable
(hara-o yomu (belly-ACC read) ?fathom someone?s think-
ing?) or non-decomposable (saba-o yomu (chub.mackerel-
ACC read) ?cheat in counting?).
References
Timothy Baldwin and Francis Bond. 2002. Multiword
Expressions: Some Problems for Japanese NLP. In
Proceedings of the 8th Annual Meeting of the As-
sociation of Natural Language Processing, Japan,
pages 379?382, Keihanna, Japan.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei ? A Japanese Lexicon. Iwanami Shoten.
Priscilla Ishida. 2000. Doushi Kanyouku-ni taisuru
Tougoteki Sousa-no Kaisou Kankei (On the Hier-
archy of Syntactic Operations Applicable to Verb
Idioms). Nihongo Kagaku (Japanese Linguistics),
7:24?43, April.
Haruhiko Kindaichi and Yasaburo Ikeda, editors. 1989.
Gakken Kokugo Daijiten (Gakken?s Dictionary).
Gakushu Kenkyu-sha.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analyisis using Cascaded Chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 63?69.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara, 2000. Morpholog-
ical Analysis System ChaSen version 2.2.1 Manual.
Nara Institute of Science and Technology, Dec.
Yutaka Miyaji. 1982. Kanyouku-no Imi-to Youhou
(Usage and Semantics of Idioms). Meiji Shoin.
Yoshiyuki Morita. 1985. Doushikanyouku (Verb
Idioms). Nihongogaku (Japanese Linguistics),
4(1):37?44.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70:491?538.
Masahiro Oku. 1990. Nihongo-bun Kaiseki-ni-okeru
Jutsugo Soutou-no Kanyouteki Hyougen-no Atsukai
(Treatments of Predicative Idiomatic Expressions in
Parsing Japanese). Journal of Information Process-
ing Society of Japan, 31(12):1727?1734.
Douglas L. T. Rohde, 2005. TGrep2 User Manual ver-
sion 1.15. Massachusetts Institute of Technology.
http://tedlab.mit.edu/?dr/Tgrep2/.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Compu-
tational Linguistics and Intelligent Text Processing:
Third International Conference, pages 1?15.
Kosho Shudo, Toshifumi Tanabe, Masahito Takahashi,
and Kenji Yoshimura. 2004. MWEs as Non-
propositional Content Indicators. In the 2nd ACL
Workshop on Multiword Expressions: Integrating
Processing, pages 32?39.
Yasuhito Tanaka. 1997. Collecting idioms and their
equivalents. In IPSJ SIGNL 1997-NL-121.
Kiyoko Uchiyama, Timothy Baldwin, and Shun
Ishizaki. 2005. Disambiguating Japanese Com-
pound Verbs. Computer Speech and Language,
Special Issue on Multiword Expressions, 19, Issue
4:497?512.
360
Proceedings of the Fourth International Natural Language Generation Conference, pages 41?43,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Adjective-to-Verb Paraphrasing in Japanese
Based on Lexical Constraints of Verbs
Atsushi Fujita? Naruaki Masuno?? Satoshi Sato? Takehito Utsuro?
?Graduate School of Engineering, Nagoya University
??IBM Engineering & Technology Services, IBM Japan, Ltd.
?Graduate School of Systems and Information Engineering, University of Tsukuba
Abstract
This paper describes adjective-to-verb
paraphrasing in Japanese. In this para-
phrasing, generated verbs require addi-
tional suffixes according to their difference
in meaning. To determine proper suffixes
for a given adjective-verb pair, we have ex-
amined the verbal features involved in the
theory of Lexical Conceptual Structure.
1 Introduction
Textual expressions that (roughly) convey the
same meaning are called paraphrases. Since gen-
erating and recognizing paraphrases has a poten-
tial to contribute to a broad range of natural lan-
guage applications, such as MT, IE, and QA, many
researchers have done a lot of practices on auto-
matic paraphrasing in the last decade.
Most previous studies have addressed para-
phrase phenomena where the syntactic category
is not changed: e.g., noun-to-noun (?document?
? ?article?), verb-to-verb (?raise?? ?bring up?).
In these inner-categorial paraphrasing, only lim-
ited types of problems arise when replacing words
or phrases with their synonymous expressions.
On the other hand, this paper focuses on inter-
categorial paraphrasing, such as adjective-to-verb
(?attractive? ? ?attract?) that leads to novel type
of problems due to the prominent differences in
meaning and usage. In other words, calculating
those differences is more crucial to determine how
they can or cannot be paraphrased.
The aim of this study is to clarify what lexical
knowledge is required for capturing those differ-
ences, and to explore where such a knowledge can
be obtained from. Recent work in lexical seman-
tics has shown that syntactic behaviors and seman-
tic properties of words provide useful informa-
tion to explain the mechanisms of several classes
of paraphrases. More specifically, lexical proper-
ties involved in the theory of Lexical Conceptual
Structure (LCS) (Jackendoff, 1990) have seemed
to be beneficial because each verb does not func-
tion idiosyncratically. However, in the literature,
there have been less studies for other syntactic
categories than verbs. To the best of our knowl-
edge, the Meaning-Text Theory (MTT) (Mel?c?uk
and Polgue`re, 1987) is one of the very few frame-
works. In MTT, lexical properties and inter-
categorial paraphrasing are realized with a unique
semantic representation irrespective of syntactic
categories and what are called lexical functions,
e.g., S
0
(receive) = reception.
To make out how the recent advances in lexi-
cal semantics for verbs can be extended to other
syntactic categories, we assess LCS for inter-
categorial paraphrasing. We choose adjectives as
a counterpart of paraphrasing because they behave
relatively similar to verbs compared with other
categories: both adjectives and verbs have inflec-
tion and function as predicates, adnominal ele-
ments, etc. Yet, we speculate that their difference
in meaning and usage reveal intriguing generation
problems. To put it briefly, adjective-to-verb para-
phrasing in Japanese requires verbal suffixes such
as ?ta (past / attributive)? in example (1)1:
(1) s. furui otera-no jushoku-o tazune-ta.
be old temple-GEN priest-ACC to visit-PAST
I visited a priest in the old temple.
t. furubi-ta otera-no jushoku-o tazune-ta.
to olden-ATTR temple-GEN priest-ACC to visit-PAST
I visited a priest in the olden(ed) temple.
2 Preliminary investigation
To make an investigation into the variation and
distribution of required verbal suffixes, we col-
lected a set of paraphrase examples through the
following semi-automatic procedure:
Step 1. We handcrafted adjective-verb pairs
based on JCore (Sato, 2004), which classifies
Japanese words into five-levels of readability.
Our 128 pairs (for 85 adjectives) contain only
those sharing first few phonemes (reading)
1For each example, ?s? and ?t? denote an original sen-
tence and its paraphrase, respectively.
41
Table 1: Distribution of verbal suffixes used.
Verbal suffix Cadc Cpr
1
Cpr
2
ru 9 16 0
tei-ru 5 42 0
re-ru 14 8 0
re-tei-ru 2 5 0
ta 57 0 7
tei-ta 2 0 2
re-ta 6 0 1
re-tei-ta 0 0 1
both ta and tei-ru 4 0 0
both ta and ru 1 0 0
tea-ru 0 2 0
Total 100 73 11
where
ru: base form
tei: progressive / perfective
re: passive / potential
ta: past / attributive
tea: perfective
and characters (kanji), and either of adjective
or verb falls into the easiest three levels.
Step 2. Candidate paraphrases for a given sen-
tence collection are automatically generated
by replacing adjectives with their corre-
sponding verbs. Multiple candidates are gen-
erated for adjectives that correspond to mul-
tiple verbs.
Step 3. The correctness of each candidate para-
phrase is judged by two human annotators.
The basic criterion for judgement is that two
sentences are regarded as paraphrases if and
only if they share at least one interpretation.
In this step, the annotators are allowed to re-
vise candidates: (i) append verbal suffixes,
(ii) change of case markers, and (iii) insert
adverbs. Finally, candidates that both anno-
tators judge correct qualify as paraphrases.
Assuming that the variation and distribution of
verbal suffixes vary according to the usage of ad-
jectives, we separately collected paraphrase exam-
ples for adnominal and predicative usages.
Adnominal usages: For 960 sentences randomly
extracted from a one-year newspaper corpus,
Mainichi 1995, we obtained 165 examples for 142
source sentences. We then divided them into two
portions: 12 adjectives that appeared only once
and at least one examples for the other adjectives
were kept unseen (Cad
o
), while the remaining ex-
amples (Cad
c
) were used for our investigation.
Predicative usages: For 157 example sentences
within IPAL adjective dictionary (IPA, 1990), we
generated candidate paraphrases. 84 candidates
for 70 sentences qualified as paraphrases. They
are then divided into two portions according to
the tense of adjectives: Cpr
1
consists of examples
where adjectives appear in base form and Cpr
2
is
for ?ta? form (past tense).
Table 1 shows the distribution of verbal suffixes
used for given adjective-verb pairs in each portion
of example collections. We confirmed that their
distribution was fairly different. In the remaining
sections, we focus on adnominal usages because
examples of predicative usages have displayed a
degree of compositionality. Which of ?ru? or ?ta?
must be used is given by the input: if a given ad-
jective accompanies past tense, the resultant ver-
bal suffix is necessarily that for present tense fol-
lowed by ?ta.?
3 Determining verbal suffixes
The task we address here is to determine verbal
suffixes for a given input, a pair of an adnominal
usage of adjective in a certain context and a candi-
date verb given by our adjective-verb list.
From the viewpoint of language generation,
this task can be thought of as generating verbal
expressions where options are already given in
Table 1. A straightforward way for determining
verbal suffixes is to make use of lexical properties
of verbs as constraints on generation. To manifest
them, in particular aspectual properties involved
in LCS, we first designed seven types of linguis-
tic tests shown in Table 2. They are derived from
a classical analysis of verb semantics in Japanese
(Kageyama, 1996) and some ongoing projects on
constructing LCS dictionaries (Kato et al, 2005;
Takeuchi et al, 2006). We then manually ex-
amined 128 verbs in Section 2 under those tests.
To determine the word sense in which the deriva-
tive relationship hold good, example sentences in
IPAL verb dictionary (IPA, 1987) for each verb
were used. For a verb which was out of the dic-
tionary, we manually gave a sample sentence.
Since our aim is to explain why a certain ver-
bal suffix is used for a given input, we have not
feverishly applied a machine learning algorithm to
the task. Instead, we have manually created a rule-
based model shown in Table 3 using Cad
c
, where
each if-then rule assigns either of verbal suffixes in
Table 1 to a given input based on verbal features in
Table 2 and some other features below:
? D: affix pair of the adjective and the candi-
date verb: e.g., ?A shii-V mu? for ?kuyashii
(be regretful)? ? ?kuyamu (to regret)?
? N : disjunction of semantic classes in a the-
saurus (The Natural Institute for Japanese
Language, 2004) for the modified noun
? C: whether the adjective is head of clause
4 Experiment and discussion
By conducting an empirical experiment with Cad
c
and Cad
o
, we evaluate how our model (RULE)
properly determines verbal suffixes. A compar-
ison with a simple baseline model (BL) is also
done. BL selects the most frequently used suffix
(in this experiment ?ta?) for any given input.
42
Table 2: Linguistic tests for verbs derived from Lexical Conceptual Structure (Kageyama, 1996).
Label Description
Va whether the verb allows accusative case
Vb whether the verb can co-occur with a temporal adverb ?ichi-jikan (for one hour)? or its variant
Vc whether the verb can co-occur with a temporal adverb ?ichi-jikan-de (in one hour)? or its variant
Vd whether the verb can be followed by ?tearu (perfective)? when its accusative case is moved to nominative
Ve interpretation of the verb followed by ?tei-ru (progressive / perfective)?
Vf when followed by ?ta,? whether the verb can have the perfective interpretation or just past tense
Vg whether the verb can co-occur with a sort of adverb which indicates intention of the action: e.g. ?wazato (purposely)? and ?iyaiya (reluctantly)?
Table 3: The rule-set for determining verbal suf-
fixes, where ?(non)? indicates non-paraphrasable.
Order Condition (conjunction of ?feature label =? value?) Verbal suffix
1 Va=?yes?? Vb=?yes? ? Vf=?no? ? re-ru
N=?except Human (1.10)? ?
D=?A ui?V bumu? ? ?A i?V mu? ? ?A asii?V u?
2 Va=?yes?? Vb=?yes? ? Vf=?no? ? ta
Vd=?no? ?N=?Mind: mind, attitude (1.303)?
3 Va=?no? ? Vg=?yes? ta
4 Va=?no? ? Vf=?yes??D=?A i?V migakaru? ta / tei-ru
5 C=?clause? ?D=?A i?V maru? ru
6 Va=?no? ? Vf=?yes? ta
7 Va=?no? ? Vb=?yes?? Vf=?no? ta
8 Vb=?yes? ? Vf=?no? ? Vc=?yes?? tei-ru
Vd=?yes?? Ve=?progressive??N=?Subject (1.2)?
9 ? (non)
Table 4 shows the experimental results, where
recall and precision are calculated with regard
to input adjective-verb pairs. Among rules in
Table 3, rules 1 (for ?re-ru?), 3, 6, and 7 (for ?ta?
where Va=?no?) performed much better than the
other rules. This indicates that these rules and fea-
tures in their conditions properly reflect our lin-
guistic intuition. For instance, rule 6 reflects that
a change-of-state intransitive verb expresses re-
sultative meaning as adjectives when it modifies
Theme of the event via ?ta? (Kageyama, 1996)
as shown in (1), and rule 2 does that a psycho-
logical verb modifies a nouns with ?re-ru? when
the noun arouses the specific emotion, such as re-
gretting mistakes (e.g., ?kuyashii (be regretful)?
? ?kuyama-re-ru (be regretted)?). The aspectual
property captured by the tests in Table 2 is used to
classify verbs into these semantic classes.
On the other hand, the rules for the other types
are immature due to lack of examples: we cannot
find out even necessary conditions to be ?ru,? ?tei-
ru,? etc. What is required to induce proper con-
ditions for these suffixes is a larger example col-
lection and discovering another semantic property
and a set of linguistic tests for capturing it.
5 Conclusion and future work
In this paper, we focused on inter-categorial para-
phrasing and reported on our study on an issue
in adjective-to-verb paraphrasing. Two general-
purpose resources and a task-specific rule-set have
been handcrafted to generate proper verbal suf-
fixes. Although the rule-based model has achieved
better performance than a simple baseline model,
there is a plenty of room for improvement.
Table 4: Recall and precision of determining ver-
bal suffix for given adjective-verb pairs.
Cadc Cado
Verbal suffix Recall Precision Recall Precision
ta (Va=?yes?) 3/13 3/3 1/6 1/1
ta (Va=?no?) 42/44 42/63 18/18 18/29
re-ru 12/14 12/19 7/13 7/11
ru 3/9 3/6 0/2 0/5
tei-ru 1/5 1/7 2/8 2/6
ta / tei-ru 2/4 2/2 1/2 1/1
No rule for 11 inputs for 7 inputs
Total (RULE) 63/100 63/100 29/56 29/53
(63%) (63%) (52%) (55%)
BL 57/100 57/148 24/56 24/83
(57%) (39%) (43%) (29%)
Future work includes (i) to enlarge our two
resources as in (Dorr, 1997; Habash and Dorr,
2003) evolving an effective construction method,
(ii) intrinsic evaluation of those resources, and, of
course, (iii) to enhance the paraphrasing models
through further experiments with a larger test-set.
References
B. J. Dorr. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine trans-
lation. Machine Translation, 12(4):271?322.
N. Habash and B. J. Dorr. 2003. A categorial variation
database for English. In Proceedings of the 2003 Human
Language Technology Conference and the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 17?23.
IPA. 1987. IPA Lexicon of the Japanese language for com-
puters (Basic Verbs). Information-technology Promotion
Agency. (in Japanese).
IPA. 1990. IPA Lexicon of the Japanese language for com-
puters (Basic Adjectives). Information-technology Pro-
motion Agency. (in Japanese).
R. Jackendoff. 1990. Semantic structures. The MIT Press.
T. Kageyama. 1996. Verb semantics. Kurosio Publishers.
(in Japanese).
T. Kato, S. Hatakeyama, H. Sakamoto, and T. Ito. 2005.
Constructing Lexical Conceptual Structure dictionary for
verbs of Japanese origin. In Proceedings of the 11th An-
nual Meeting of the Association for Natural Language
Processing, pages 871?874. (in Japanese).
I. Mel?c?uk and A. Polgue`re. 1987. A formal lexicon in
meaning-text theory (or how to do lexica with words).
Computational Linguistics, 13(3-4):261?275.
S. Sato. 2004. Identifying spelling variations of Japanese
words. In Information Processing Society of Japan SIG
Notes, NL-161-14, pages 97?104. (in Japanese).
K. Takeuchi, K. Inui, and A. Fujita. 2006. Construction of
compositional lexical database based on Lexical Concep-
tual Structure for Japanese verbs. In T. Kageyama, editor,
Lexicon Forum No.2. Hitsuji Shobo. (in Japanese).
The Natural Institute for Japanese Language. 2004. Word
list by semantic principles, revised and enlarged edition.
Dainippon Tosho. (in Japanese).
43
A Comparative Study on Compositional Translation Estimation
using a Domain/Topic-Specific Corpus collected from the Web
Masatsugu Tonoike?, Mitsuhiro Kida?, Toshihiro Takagi?, Yasuhiro Sasaki?,
Takehito Utsuro??, Satoshi Sato? ? ?
?Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
??Graduate School of Systems and Information Engineering, University of Tsukuba
1-1-1, Tennodai, Tsukuba, 305-8573, Japan
? ? ?Graduate School of Engineering, Nagoya University
Furo-cho, Chikusa-ku, Nagoya 464-8603, Japan
Abstract
This paper studies issues related to the
compilation of a bilingual lexicon for tech-
nical terms. In the task of estimating bilin-
gual term correspondences of technical
terms, it is usually rather difficult to find
an existing corpus for the domain of such
technical terms. In this paper, we adopt
an approach of collecting a corpus for the
domain of such technical terms from the
Web. As a method of translation esti-
mation for technical terms, we employ a
compositional translation estimation tech-
nique. This paper focuses on quantita-
tively comparing variations of the compo-
nents in the scoring functions of composi-
tional translation estimation. Through ex-
perimental evaluation, we show that the
domain/topic-specific corpus contributes
toward improving the performance of the
compositional translation estimation.
1 Introduction
This paper studies issues related to the compilation
of a bilingual lexicon for technical terms. Thus
far, several techniques of estimating bilingual term
correspondences from a parallel/comparable cor-
pus have been studied (Matsumoto and Utsuro,
2000). For example, in the case of estimation from
comparable corpora, (Fung and Yee, 1998; Rapp,
1999) proposed standard techniques of estimating
bilingual term correspondences from comparable
corpora. In their techniques, contextual similarity
between a source language term and its translation
candidate is measured across the languages, and
all the translation candidates are re-ranked accord-
ing to their contextual similarities. However, there
are limited number of parallel/comparable corpora
that are available for the purpose of estimating
bilingual term correspondences. Therefore, even
if one wants to apply those existing techniques to
the task of estimating bilingual term correspon-
dences of technical terms, it is usually rather dif-
ficult to find an existing corpus for the domain of
such technical terms.
On the other hand, compositional translation es-
timation techniques that use a monolingual corpus
(Fujii and Ishikawa, 2001; Tanaka and Baldwin,
2003) are more practical. It is because collecting a
monolingual corpus is less expensive than collect-
ing a parallel/comparable corpus. Translation can-
didates of a term can be compositionally generated
by concatenating the translation of the constituents
of the term. Here, the generated translation candi-
dates are validated using the domain/topic-specific
corpus.
In order to assess the applicability of the com-
positional translation estimation technique, we
randomly pick up 667 Japanese and English tech-
nical term translation pairs of 10 domains from ex-
isting technical term bilingual lexicons. We then
manually examine their compositionality, and find
out that 88% of them are actually compositional,
which is a very encouraging result.
But still, it is expensive to collect a
domain/topic-specific corpus. Here, we adopt
an approach of using the Web, since documents
of various domains/topics are available on the
Web. When validating translation candidates
using the Web, roughly speaking, there exist the
following two approaches. In the first approach,
translation candidates are validated through
the search engine (Cao and Li, 2002). In the
second approach, a domain/topic-specific corpus
is collected from the Web in advance and fixed
11
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
web
(language T )
web
(language T )
Figure 1: Compilation of a Domain/Topic-
Specific Bilingual Lexicon using the Web
before translation estimation, then generated
translation candidates are validated against the
domain/topic-specific corpus (Tonoike et al,
2005). The first approach is preferable in terms of
coverage, while the second is preferable in terms
of computational efficiency. This paper mainly
focuses on quantitatively comparing the two
approaches in terms of coverage and precision of
compositional translation estimation.
More specifically, in compositional translation
estimation, we decompose the scoring function
of a translation candidate into two components:
bilingual lexicon score and corpus score. In this
paper, we examine variants for those components
and define 9 types of scoring functions in total.
Regarding the above mentioned two approaches
to validating translation candidates using the Web,
the experimental result shows that the second
approach outperforms the first when the correct
translation does exist in the corpus. Furthermore,
we examine the methods that combine two scor-
ing functions based on their agreement. The ex-
perimental result shows that it is quite possible to
achieve precision much higher than those of single
scoring functions.
2 Overall framework
The overall framework of compiling a bilingual
lexicon from the Web is illustrated as in Figure 1.
Suppose that we have sample terms of a specific
domain/topic, then the technical terms that are to
be listed as the headwords of a bilingual lexicon
are collected from the Web by the related term col-
lection method of (Sato and Sasaki, 2003). These
collected technical terms can be divided into three
subsets depending on the number of translation
candidates present in an existing bilingual lexicon,
i.e., the subset XUS of terms for which the number
of translations in the existing bilingual lexicon is
one, the subset XMS of terms for which the number
of translations is more than one, and the subset YS
of terms that are not found in the existing bilingual
lexicon (henceforth, the union XUS ? XMS will be
denoted as XS). Here, the translation estimation
task here is to estimate translations for the terms
of the subsets XMS and YS . A new bilingual lex-
icon is compiled from the result of the translation
estimation for the terms of the subsets XMS and
YS as well as the translation pairs that consist of
the terms of the subset XUS and their translations
found in the existing bilingual lexicon.
For the terms of the subset XMS , it is required
that an appropriate translation is selected from
among the translation candidates found in the ex-
isting bilingual lexicon. For example, as a trans-
lation of the Japanese technical term ?????,?
which belongs to the logic circuit domain, the term
?register? should be selected but not the term ?reg-
ista? of the football domain. On the other hand, for
the terms of YS , it is required that the translation
candidates are generated and validated. In this pa-
per, out of the above two tasks, we focus on the
latter of translation candidate generation and val-
idation using the Web. As we introduced in the
previous section, here we experimentally compare
the two approaches to validating translation candi-
dates. The first approach directly uses the search
engine, while the second uses the domain/topic-
specific corpus, which is collected in advance from
the Web. Here, in the second approach, we use the
term of XUS , which has only one translation in the
existing bilingual lexicon. The set of translations
of the terms of the subset XUS is denoted as XUT .
Then, in the second approach, the domain/topic-
specific corpus is collected from the Web using the
terms of the set XUT .
3 Compositional Translation Estimation
for Technical Terms
3.1 Overview
An example of compositional translation estima-
tion for the Japanese technical term ??????
?? is illustrated in Figure 2. First, the Japanese
technical term ???????? is decomposed
into its constituents by consulting an existing
bilingual lexicon and retrieving Japanese head-
12
? application(1)
? practical(0.3)
? applied(1.6)
? action(1)
? activity(1)
? behavior(1)
? analysis(1)
? diagnosis(1)
? assay(0.3)
? behavior analysis(10)
??Compositional generation 
of translation candidate
? applied behavior analysis(17.6)
? application behavior analysis(11)
? applied behavior diagnosis(1)
??Decompose source term into constituents  
??Translate constituents into target language      process
?? ?? ??a
?? ????b
Generated translation candidates
?(1.6?1?1)+(1.6?10)
? application(1)
? practical(0.3)
? applied(1.6)
Figure 2: Compositional Translation Estimation
for the Japanese Technical Term ????????
words.1 In this case, the result of this decompo-
sition can be given as in the cases ?a? and ?b?
(in Figure 2). Then, each constituent is translated
into the target language. A confidence score is as-
signed to the translation of each constituent. Fi-
nally, translation candidates are generated by con-
catenating the translation of those constituents ac-
cording to word ordering rules considering prepo-
sitional phrase construction.
3.2 Collecting a Domain/Topic-Specific
Corpus
When collecting a domain/topic-specific corpus of
the language T , for each technical term xUT in the
set XUT , we collect the top 100 pages obtained
from search engine queries that include the term
xUT . Our search engine queries are designed such
that documents that describe the technical term xUT
are ranked high. For example, an online glossary
is one such document. When collecting a Japanese
corpus, the search engine ?goo?2 is used. The spe-
cific queries that are used in this search engine
are phrases with topic-marking postpositional par-
ticles such as ?xUT ??,? ?xUT ???,? ?xUT ?,?
and an adnominal phrase ?xUT ?,? and ?xUT .?
3.3 Translation Estimation
3.3.1 Compiling Bilingual Constituents
Lexicons
This section describes how to compile bilingual
constituents lexicons from the translation pairs of
1Here, as an existing bilingual lexicon, we use Ei-
jiro(http://www.alc.co.jp/) and bilingual constituents lexicons
compiled from the translation pairs of Eijiro (details to be de-
scribed in the next section).
2http://www.goo.ne.jp/
 
applied mathematics : ?? ??
applied science : ?? ??
applied robot : ?? ????
.
.
. frequency
? ??
applied : ?? : 40
 
Figure 3: Example of Estimating Bilingual Con-
stituents Translation Pair (Prefix)
the existing bilingual lexicon Eijiro. The under-
lying idea of augmenting the existing bilingual
lexicon with bilingual constituents lexicons is il-
lustrated in Figure 3. Suppose that the existing
bilingual lexicon does not include the translation
pair ?applied : ??,? while it includes many
compound translation pairs with the first English
word ?applied? and the first Japanese word ??
?.?3 In such a case, we align those translation
pairs and estimate a bilingual constituent transla-
tion pair which is to be collected into a bilingual
constituents lexicon.
More specifically, from the existing bilingual
lexicon, we first collect translation pairs whose
English terms and Japanese terms consist of two
constituents into another lexicon P
2
. We com-
pile the ?bilingual constituents lexicon (prefix)?
from the first constituents of the translation pairs
in P
2
and compile the ?bilingual constituents lex-
icon (suffix)? from their second constituents. The
number of entries in each language and those of
the translation pairs in these lexicons are shown in
Table 1.
The result of our assessment reveals that only
48% of the 667 translation pairs mentioned in Sec-
tion 1 can be compositionally generated by using
Eijiro, while the rate increases up to 69% using
both Eijiro and ?bilingual constituents lexicons.?4
3.3.2 Score of Translation Candidates
This section gives the definition of the scores
of a translation candidate in compositional trans-
lation estimation.
First, let ys be a technical term whose transla-
tion is to be estimated. We assume that ys is de-
3Japanese entries are supposed to be segmented into a
sequence of words by the morphological analyzer JUMAN
(http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html).
4In our rough estimation, the upper bound of this rate
is approximately 80%. An improvement from 69% to 80%
could be achieved by extending the bilingual constituents lex-
icons.
13
Table 1: Numbers of Entries and Translation Pairs
in the Lexicons
lexicon # of entries # of translationEnglish Japanese pairs
Eijiro 1,292,117 1,228,750 1,671,230
P
2
217,861 186,823 235,979
B
P
37,090 34,048 95,568
B
S
20,315 19,345 62,419
B 48,000 42,796 147,848
Eijiro : existing bilingual lexicon
P
2
: entries of Eijiro with two constituents
in both languages
B
P
: bilingual constituents lexicon (prefix)
B
S
: bilingual constituents lexicon (suffix)
B : bilingual constituents lexicon (merged)
composed into their constituents as below:
ys = s1, s2, ? ? ? , sn (1)
where each si is a single word or a sequence of
words.5 For ys, we denote a generated translation
candidate as yt.
yt = t1, t2, ? ? ? , tn (2)
where each ti is a translation of si. Then the trans-
lation pair ?ys, yt? is represented as follows.
?ys, yt? = ?s1, t1?, ?s2, t2?, ? ? ? , ?sn, tn? (3)
The score of a generated translation candidate is
defined as the product of a bilingual lexicon score
and a corpus score as follows.
Q(ys, yt) = Qdict(ys, yt) ? Qcoprus(yt) (4)
Bilingual lexicon score measures appropriateness
of correspondence of ys and yt. Corpus score
measures appropriateness of the translation candi-
date yt based on the target language corpus. If a
translation candidate is generated from more than
one sequence of translation pairs, the score of the
translation candidate is defined as the sum of the
score of each sequence.
Bilingual Lexicon Score
In this paper, we compare two types of bilin-
gual lexicon scores. Both scores are defined as the
product of scores of translation pairs included in
the lexicons presented in the previous section as
follows.
5Eijiro has both single word entries and compound word
entries.
? Frequency-Length
Qdict(ys, yt) =
n
?
i=1
q(?si, ti?) (5)
The first type of bilingual lexicon scores is re-
ferred to as ?Frequency-Length.? This score is
based on the length of translation pairs and the fre-
quencies of translation pairs in the bilingual con-
stituent lexicons (prefix,suffix) BP , BS in Table 1.
In this paper, we first assume that the translation
pairs follow certain preference rules and that they
can be ordered as below:
1. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of two or more constituents.
2. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are high.
3. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of exactly one constituent.
4. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are not
high.
As the definition of the confidence score
q(?s, t?) of a translation pair ?s, t?, we use the fol-
lowing:
q(?s, t?) =
?
?
?
10
(compo(s)?1) (?s, t? in Eijiro)
log
10
fp(?s, t?) (?s, t? in BP )
log
10
fs(?s, t?) (?s, t? in BS)
(6)
, where compo(s) denotes the word count of s,
fp(?s, t?) represents the frequency of ?s, t? as the
first constituent in P
2
, and fs(?s, t?) represents the
frequency of ?s, t? as the second constituent in P
2
.
? Probability
Qdict(ys, yt) =
n
?
i=1
P (si|ti) (7)
The second type of bilingual lexicon scores is re-
ferred to as ?Probability.? This score is calcu-
lated as the product of the conditional probabili-
ties P (si|ti). P (s|t) is calculated using bilingual
lexicons in Table 1.
P (s|t) =
fprob(?s, t?)
?
s
j
fprob(?sj , t?)
(8)
14
Table 2: 9 Scoring Functions of Translation Candidates and their Components
bilingual lexicon score corpus score corpus
score ID freq-length probability probability frequency occurrence off-line on-line
(search engine)
A prune/final prune/final o
B prune/final prune/final o
C prune/final prune/final o
D prune/final prune o
E prune/final
F prune/final final prune o
G prune/final prune/final o
H prune/final final o
I prune/final final o
fprob(?s, t?) denotes the frequency of the transla-
tion pair ?s, t? in the bilingual lexicons as follows:
fprob(?s, t?) =
{
10 (?s, t? in Eijiro)
fB(?s, t?) (?s, t? in B)
(9)
Note that the frequency of a translation pair in Ei-
jiro is regarded as 106 and fB(?s, t?) denotes the
frequency of the translation pair ?s, t? in the bilin-
gual constituent lexicon B.
Corpus Score
We evaluate three types of corpus scores as fol-
lows.
? Probability: the occurrence probability of yt
estimated by the following bi-gram model
Qcorpus(yt) = P (t1) ?
n
?
i=1
P (ti+1|ti) (10)
? Frequency: the frequency of a translation
candidate in a target language corpus
Qcorpus(yt) = freq(yt) (11)
? Occurrence: whether a translation candidate
occurs in a target language corpus or not
Qcorpus(yt) =
?
?
?
?
?
1 yt occurs in a corpus
0 yt does not occur
in a corpus
(12)
6It is necessary to empirically examine whether or not the
definition of the frequency of a translation pair in Eijiro is
appropriate.
Variation of the total scoring functions
As shown in Table 2, in this paper, we examine
the 9 combinations of the bilingual lexicon scores
and the corpus scores. In the table, ?prune? indi-
cates that the score is used for ranking and pruning
sub-sequences of generated translation candidates
in the course of generating translation candidates
using a dynamic programming algorithm. ?Final?
indicates that the score is used for ranking the fi-
nal outputs of generating translation candidates.
In the column ?corpus?, ?off-line? indicates that
a domain/topic-specific corpus is collected from
the Web in advance and then generated transla-
tion candidates are validated against this corpus.
?On-line? indicates that translation candidates are
directly validated through the search engine.
Roughly speaking, the scoring function ?A? cor-
responds to a variant of the model proposed by
(Fujii and Ishikawa, 2001). The scoring func-
tion ?D? is a variant of the model proposed by
(Tonoike et al, 2005) and ?E? corresponds to the
bilingual lexicon score of the scoring function ?D?.
The scoring function ?I? is intended to evaluate the
approach proposed in (Cao and Li, 2002).
3.3.3 Combining Two Scoring Functions
based on their Agreement
In this section, we examine the method that
combines two scoring functions based on their
agreement. The two scoring functions are selected
out of the 9 functions introduced in the previous
section. In this method, first, confidence of trans-
lation candidates of a technical term are measured
by the two scoring functions. Then, if the first
ranked translation candidates of both scoring func-
tions agree, this method outputs the agreed trans-
lation candidate. The purpose of introducing this
method is to prefer precision to recall.
15
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
domain/topic
specific
corpus
(language T )
validating
translation
candidates
web
(language T )
web
(language T )
Figure 4: Experimental Evaluation of Translation
Estimation for Technical Terms with/without the
Domain/Topic-Specific Corpus (taken from Fig-
ure 1)
4 Experiments and Evaluation
4.1 Translation Pairs for Evaluation
In our experimental evaluation, within the frame-
work of compiling a bilingual lexicon for techni-
cal terms, we evaluate the translation estimation
portion that is indicated by the bold line in Fig-
ure 4. In this paper, we simply omit the evalua-
tion of the process of collecting technical terms to
be listed as the headwords of a bilingual lexicon.
In order to evaluate the translation estimation por-
tion, terms are randomly selected from the 10 cate-
gories of existing Japanese-English technical term
dictionaries listed in Table 3, for each of the sub-
sets XUS and YS (here, the terms of YS that consist
of only one word or morpheme are excluded). As
described in Section 1, the terms of the set XUT (the
set of translations for the terms of the subset XUS )
is used for collecting a domain/topic-specific cor-
pus from the Web. As shown in Table 3, size of the
collected corpora is 48MB on the average. Trans-
lation estimation evaluation is to be conducted for
the subset YS . For each of the 10 categories, Ta-
ble 3 shows the sizes of the subsets XUS and YS ,
and the rate of including correct translation within
the collected domain/topic-specific corpus for YS .
In the following, we show the evaluation results
with the source language S as English and the tar-
get language T as Japanese.
4.2 Evaluation of single scoring functions
This section gives the results of evaluating single
scoring functions A ? I listed in Table 2.
Table 4 shows three types of experimental re-
sults. The column ?the whole set YS? shows the
results against the whole set YS . The column
?generatable? shows the results against the trans-
lation pairs in YS that can be generated through
the compositional translation estimation process.
69% of the terms in ?the whole set YS? belongs
to the set ?generatable?. The column ?gene.-exist?
shows the result against the source terms whose
correct translations do exist in the corpus and that
can be generated through the compositional trans-
lation estimation process. 50% of the terms in ?the
whole set YS? belongs to the set ?gene.-exist?. The
column ?top 1? shows the correct rate of the first
ranked translation candidate. The column ?top 10?
shows the rate of including the correct candidate
within top 10.
First, in order to evaluate the effectiveness of
the approach of validating translation candidates
by using a target language corpus, we compare the
scoring functions ?D? and ?E?. The difference be-
tween them is whether or not they use a corpus
score. The results for the whole set YS show that
using a corpus score, the precision improves from
33.9% to 43.0%. This result supports the effec-
tiveness of the approach of validating translation
candidates using a target language corpus.
As can be seen from these results for the whole
set YS , the correct rate of the scoring function ?I?
that directly uses the web search engine in the cal-
culation of its corpus score is higher than those
of other scoring functions that use the collected
domain/topic-specific corpus. This is because,
for the whole set YS , the rate of including cor-
rect translation within the collected domain/topic-
specific corpus is 72% on the average, which is
not very high. On the other hand, the results of the
column ?gene.-exist? show that if the correct trans-
lation does exist in the corpus, most of the scor-
ing functions other than ?I? can achieve precisions
higher than that of the scoring function ?I?. This
result supports the effectiveness of the approach
of collecting a domain/topic-specific corpus from
the Web in advance and then validating generated
translation candidates against this corpus.
4.3 Evaluation of combining two scoring
functions based on their agreement
The result of evaluating the method that combines
two scoring functions based on their agreement is
shown in Table 5. This result indicates that com-
binations of scoring functions with ?off-line?/?on-
16
Table 3: Number of Translation Pairs for Evaluation (S=English)
dictionaries categories |Y
S
| |X
U
S
| corpus size C(S)
Electromagnetics 33 36 28MB 85%
McGraw-Hill Electrical engineering 45 34 21MB 71%
Optics 31 42 37MB 65%
Iwanami Programming language 29 37 34MB 93%Programming 29 29 33MB 97%
Dictionary of (Computer) 100 91 67MB 51%Computer
Anatomical Terms 100 91 73MB 86%
Dictionary of Disease 100 91 83MB 77%
250,000 Chemicals and Drugs 100 94 54MB 60%
medical terms Physical Science and Statistics 100 88 56MB 68%
Total 667 633 482MB 72%
McGraw-Hill : Dictionary of Scientific and Technical Terms
Iwanami : Encyclopedic Dictionary of Computer Science
C(S) : for Y
S
, the rate of including correct translations within the collected domain/topic-specific corpus
Table 4: Result of Evaluating single Scoring Functions
the whole set Y
S
(667 terms?100%) generatable (458 terms?69%) gene.-exist (333 terms?50%)
ID top 1 top 10 top 1 top 10 top 1 top 10
A 43.8% 52.9% 63.8% 77.1% 82.0% 98.5%
B 42.9% 50.7% 62.4% 73.8% 83.8% 99.4%
C 43.0% 58.0% 62.7% 84.5% 75.1% 94.6%
D 43.0% 47.4% 62.7% 69.0% 85.9% 94.6%
E 33.9% 57.3% 49.3% 83.4% 51.1% 84.1%
F 40.2% 47.4% 58.5% 69.0% 80.2% 94.6%
G 39.1% 46.8% 57.0% 68.1% 78.1% 93.4%
H 43.8% 57.3% 63.8% 83.4% 73.6% 84.1%
I 49.8% 57.3% 72.5% 83.4% 74.8% 84.1%
Table 5: Result of combining two scoring func-
tions based on their agreement
corpus combination precision recall F
?=1
A & I 88.0% 27.6% 0.420
off-line/ D & I 86.0% 29.5% 0.440
on-line F & I 85.1% 29.1% 0.434
H & I 58.7% 37.5% 0.457
A & H 86.0% 30.4% 0.450
F & H 80.6% 33.7% 0.476
off-line/ D & H 80.4% 32.7% 0.465
off-line A & D 79.0% 32.1% 0.456
A & F 74.6% 33.0% 0.457
D & F 68.2% 35.7% 0.469
line? corpus tend to achieve higher precisions than
those with ?off-line?/?off-line? corpus. This result
also shows that it is quite possible to achieve high
precisions even by combining scoring functions
with ?off-line?/?off-line? corpus (the pair ?A? and
?H?). Here, the two scoring functions ?A? and ?H?
are the one with frequency-based scoring func-
tions and that with probability-based scoring func-
tions, and hence, have quite different nature in the
design of their scoring functions.
5 Related Works
As a related work, (Fujii and Ishikawa, 2001) pro-
posed a technique for compositional estimation of
bilingual term correspondences for the purpose of
cross-language information retrieval. One of the
major differences between the technique of (Fu-
jii and Ishikawa, 2001) and the one proposed in
this paper is that in (Fujii and Ishikawa, 2001), in-
stead of a domain/topic-specific corpus, they use a
corpus containing the collection of technical pa-
pers, each of which is published by one of the
65 Japanese associations for various technical do-
mains. Another significant difference is that in
(Fujii and Ishikawa, 2001), they evaluate only the
performance of the cross-language information re-
trieval and not that of translation estimation.
(Cao and Li, 2002) also proposed a method
of compositional translation estimation for com-
pounds. In the method of (Cao and Li, 2002), the
translation candidates of a term are composition-
ally generated by concatenating the translation of
the constituents of the term and are validated di-
rectly through the search engine. In this paper,
we evaluate the approach proposed in (Cao and
Li, 2002) by introducing a total scoring function
17
that is based on validating translation candidates
directly through the search engine.
6 Conclusion
This paper studied issues related to the compila-
tion a bilingual lexicon for technical terms. In
the task of estimating bilingual term correspon-
dences of technical terms, it is usually rather dif-
ficult to find an existing corpus for the domain
of such technical terms. In this paper, we adopt
an approach of collecting a corpus for the do-
main of such technical terms from the Web. As
a method of translation estimation for technical
terms, we employed a compositional translation
estimation technique. This paper focused on quan-
titatively comparing variations of the components
in the scoring functions of compositional transla-
tion estimation. Through experimental evaluation,
we showed that the domain/topic specific corpus
contributes to improving the performance of the
compositional translation estimation.
Future work includes complementally integrat-
ing the proposed framework of compositional
translation estimation using the Web with other
translation estimation techniques. One of them is
that based on collecting partially bilingual texts
through the search engine (Nagata and others,
2001; Huang et al, 2005). Another technique
which seems to be useful is that of transliteration
of names (Knight and Graehl, 1998; Oh and Choi,
2005).
References
Y. Cao and H. Li. 2002. Base noun phrase translation using
Web data and the EM algorithm. In Proc. 19th COLING,
pages 127?133.
A. Fujii and T. Ishikawa. 2001. Japanese/english cross-
language information retrieval: Exploration of query
translation and transliteration. Computers and the Hu-
manities, 35(4):389?420.
P. Fung and L. Y. Yee. 1998. An IR approach for translating
new words from nonparallel, comparable texts. In Proc.
17th COLING and 36th ACL, pages 414?420.
F. Huang, Y. Zhang, and S. Vogel. 2005. Mining key phrase
translations from web corpora. In Proc. HLT/EMNLP,
pages 483?490.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4):599?612.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge ac-
quisition. In R. Dale, H. Moisl, and H. Somers, editors,
Handbook of Natural Language Processing, chapter 24,
pages 563?610. Marcel Dekker Inc.
M. Nagata et al 2001. Using the Web as a bilingual dictio-
nary. In Proc. ACL-2001 Workshop on Data-driven Meth-
ods in Machine Translation, pages 95?102.
J. Oh and K. Choi. 2005. Automatic extraction of english-
korean translations for constituents of technical terms. In
Proc. 2nd IJCNLP, pages 450?461.
R. Rapp. 1999. Automatic identification of word translations
from unrelated English and German corpora. In Proc.
37th ACL, pages 519?526.
S. Sato and Y. Sasaki. 2003. Automatic collection of related
terms from the web. In Proc. 41st ACL, pages 121?124.
T. Tanaka and T. Baldwin. 2003. Translation selection for
japanese-english noun-noun compounds. In Proc. Ma-
chine Translation Summit IX, pages 378?85.
M. Tonoike, M. Kida, T. Takagi, Y. Sasaki, T. Utsuro, and
S. Sato. 2005. Effect of domain-specific corpus in com-
positional translation estimation for technical terms. In
Proc. 2nd IJCNLP, Companion Volume, pages 116?121.
18
Chunking Japanese Compound Functional Expressions
by Machine Learning
Masatoshi Tsuchiya? and Takao Shime? and Toshihiro Takagi?
Takehito Utsuro?? and Kiyotaka Uchimoto?? and Suguru Matsuyoshi?
Satoshi Sato?? and Seiichi Nakagawa??
?Computer Center / ??Department of Information and Computer Sciences,
Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN
??Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
??National Institute of Information and Communications Technology,
3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
The Japanese language has various types
of compound functional expressions,
which are very important for recogniz-
ing the syntactic structures of Japanese
sentences and for understanding their
semantic contents. In this paper, we
formalize the task of identifying Japanese
compound functional expressions in a
text as a chunking problem. We apply a
machine learning technique to this task,
where we employ that of Support Vector
Machines (SVMs). We show that the pro-
posed method significantly outperforms
existing Japanese text processing tools.
1 Introduction
As in the case of other languages, the Japanese
language has various types of functional words
such as post-positional particles and auxiliary
verbs. In addition to those functional words,
the Japanese language has much more compound
functional expressions which consist of more than
one words including both content words and func-
tional words. Those single functional words as
well as compound functional expressions are very
important for recognizing the syntactic structures
of Japanese sentences and for understanding their
semantic contents. Recognition and understanding
of them are also very important for various kinds
of NLP applications such as dialogue systems, ma-
chine translation, and question answering. How-
ever, recognition and semantic interpretation of
compound functional expressions are especially
difficult because it often happens that one com-
pound expression may have both a literal (in other
words, compositional) content word usage and
a non-literal (in other words, non-compositional)
functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni)???
(tsuite)?, which consists of a post-positional par-
ticle ?? (ni)?, and a conjugated form ????
(tsuite)? of a verb ??? (tsuku)?. In the sentence
(A), the compound expression functions as a case-
marking particle and has a non-compositional
functional meaning ?about?. On the other hand,
in the sentence (B), the expression simply corre-
sponds to a literal concatenation of the usages of
the constituents: the post-positional particle ??
(ni)? and the verb ???? (tsuite)?, and has a
content word meaning ?follow?. Therefore, when
considering machine translation of those Japanese
sentences into English, it is necessary to precisely
judge the usage of the compound expression ??
(ni)??? (tsuite)?, as shown in the English trans-
lation of the two sentences in Table 1.
There exist widely-used Japanese text process-
ing tools, i.e., pairs of a morphological analysis
tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. How-
ever, they process those compound expressions
only partially, in that their morphological analy-
sis dictionaries list only limited number of com-
pound expressions. Furthermore, even if certain
expressions are listed in a morphological analysis
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman-e.html
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
25
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 2: Classification of Functional Expressions based on Grammatical Function
# of major # of
Grammatical Function Type expressions variants Example
subsequent to predicate 36 67 ????
post-positional / modifying predicate (to-naru-to)
particle subsequent to nominal 45 121 ?????
type / modifying predicate (ni-kakete-ha)
subsequent to predicate, nominal 2 3 ???
/ modifying nominal (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
dictionary, those existing tools often fail in resolv-
ing the ambiguities of their usages, such as those
in Table 1. This is mainly because the frame-
work of those existing tools is not designed so as
to resolve such ambiguities of compound (possi-
bly functional) expressions by carefully consider-
ing the context of those expressions.
Considering such a situation, it is necessary
to develop a tool which properly recognizes and
semantically interprets Japanese compound func-
tional expressions. In this paper, we apply a ma-
chine learning technique to the task of identify-
ing Japanese compound functional expressions in
a text. We formalize this identification task as a
chunking problem. We employ the technique of
Support Vector Machines (SVMs) (Vapnik, 1998)
as the machine learning technique, which has been
successfully applied to various natural language
processing tasks including chunking tasks such
as phrase chunking (Kudo and Matsumoto, 2001)
and named entity chunking (Mayfield et al, 2003).
In the preliminary experimental evaluation, we fo-
cus on 52 expressions that have balanced distribu-
tion of their usages in the newspaper text corpus
and are among the most difficult ones in terms of
their identification in a text. We show that the pro-
posed method significantly outperforms existing
Japanese text processing tools as well as another
tool based on hand-crafted rules. We further show
that, in the proposed SVMs based framework, it is
sufficient to collect and manually annotate about
50 training examples per expression.
2 Japanese Compound Functional
Expressions and their Example
Database
2.1 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) examine
450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their exam-
ple sentences. Compared with those two collec-
tions, Gendaigo Hukugouji Youreishu (National
Language Research Institute, 2001) (henceforth,
denoted as GHY) concentrates on 125 major func-
tional expressions which have non-compositional
usages, as well as their variants5 (337 expressions
in total), and collects example sentences of those
expressions. As a first step of developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants. In this paper, we take
an approach of regarding each of those variants as
a fixed expression, rather than a semi-fixed expres-
sion or a syntactically-flexible expression (Sag et
al., 2002). Then, we focus on evaluating the ef-
fectiveness of straightforwardly applying a stan-
5For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) in-
sertion/deletion/alternation of certain particles, ii) alternation
of synonymous words, iii) normal/honorific/conversational
forms, iv) base/adnominal/negative forms.
26
Table 3: Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ???? ????????????? ????
??????
functional
(to-naru-to) (The situation is serious if it is not effec-
tive against this disease.)
(???? (to-naru-to) = if)
(2) ???? ???????????????
???? ????????
content
(to-naru-to) (They think that it will become a require-
ment for him to be the president.)
(????? (to-naru-to)
= that (something) becomes ?)
(3) ????? ???????? ????? ????
??????????
functional
(ni-kakete-ha) (He has a great talent for earning money.) (?????? (ni-kakete-ha)
= for ?)
(4) ????? ???? ????? ???? content
(ni-kakete-ha) (I do not worry about it.)
( (??)??????
((?)-wo-ki-ni-kakete-ha)
= worry about ?)
(5) ??? ??????? ??? ??????
??
functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ????????????? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this
discussion.)
(???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2, according to their grammat-
ical functions, those 337 expressions in total
are roughly classified into post-positional particle
type, and auxiliary verb type. Functional expres-
sions of post-positional particle type are further
classified into three subtypes: i) those subsequent
to a predicate and modifying a predicate, which
mainly function as conjunctive particles and are
used for constructing subordinate clauses, ii) those
subsequent to a nominal, and modifying a predi-
cate, which mainly function as case-marking parti-
cles, iii) those subsequent to a nominal, and modi-
fying a nominal, which mainly function as adnom-
inal particles and are used for constructing adnom-
inal clauses. For each of those types, Table 2 also
shows the number of major expressions as well as
that of their variants listed in GHY, and an exam-
ple expression. Furthermore, Table 3 gives exam-
ple sentences of those example expressions as well
as the description of their usages.
2.2 Issues on Identifying Compound
Functional Expressions in a Text
The task of identifying Japanese compound func-
tional expressions roughly consists of detecting
candidates of compound functional expressions in
a text and of judging the usages of those can-
didate expressions. The class of Japanese com-
pound functional expressions can be regarded as
closed and their number is at most a few thousand.
27
Table 4: Examples of Detecting more than one Candidate Expression
Expression Example sentence (English translation) Usage
(9) ??? ????? ??? ???????? functional
(to-iu) (That?s why a match is not so easy.) (NP1??? (to-iu)NP2
= NP
2
called as NP
1
)
(10) ?????? ??? ?????? ???????? functional
(to-iu-mono-no) (Although he won, the score is bad.)
(???????
(to-iu-mono-no)
= although ?)
Therefore, it is easy to enumerate all the com-
pound functional expressions and their morpheme
sequences. Then, in the process of detecting can-
didates of compound functional expressions in a
text, the text are matched against the morpheme
sequences of the compound functional expressions
considered.
Here, most of the 125 major functional expres-
sions we consider in this paper are compound ex-
pressions which consist of one or more content
words as well as functional words. As we intro-
duced with the examples of Table 1, it is often
the case that they have both a compositional con-
tent word usage as well as a non-compositional
functional usage. For example, in Table 3, the
expression ????? (to-naru-to)? in the sen-
tence (2) has the meaning ? that (something) be-
comes ??, which corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ???, the verb ????, and the
post-positional particle ???, and can be regarded
as a content word usage. On the other hand, in
the case of the sentence (1), the expression ???
?? (to-naru-to)? has a non-compositional func-
tional meaning ?if?. Based on this discussion, we
classify the usages of those expressions into two
classes: functional and content. Here, functional
usages include both non-compositional and com-
positional functional usages, although most of the
functional usages of those 125 major expressions
can be regarded as non-compositional. On the
other hand, content usages include compositional
content word usages only.
More practically, in the process of detecting
candidates of compound functional expressions in
a text, it can happen that more than one can-
didate expression is detected. For example, in
Table 4, both of the candidate compound func-
tional expressions ???? (to-iu)? and ????
??? (to-iu-mono-no)? are detected in the sen-
tence (9). This is because the sequence of the two
morphemes ?? (to)? and ??? (iu)? constituting
the candidate expression ???? (to-iu)? is a sub-
sequence of the four morphemes constituting the
candidate expression ??????? (to-iu-mono-
no)? as below:
Morpheme sequence
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression??? (to-iu)
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression?????? (to-iu-mono-no)
? (to) ?? (iu) ?? (mono) ? (no)
This is also the case with the sentence (10).
Here, however, as indicated in Table 4, the sen-
tence (9) is an example of the functional usage of
the compound functional expression ???? (to-
iu)?, where the sequence of the two morphemes ?
? (to)? and ??? (iu)? should be identified and
chunked into a compound functional expression.
On the other hand, the sentence (10) is an ex-
ample of the functional usage of the compound
functional expression ??????? (to-iu-mono-
no)?, where the sequence of the four morphemes ?
? (to)?, ??? (iu)?, ??? (mono)?, and ?? (no)?
should be identified and chunked into a compound
functional expression. Actually, in the result of
our preliminary corpus study, at least in about 20%
of the occurrences of Japanese compound func-
tional expressions, more than one candidate ex-
pression can be detected. This result indicates that
it is necessary to consider more than one candidate
expression in the task of identifying a Japanese
compound functional expression, and also in the
task of classifying the functional/content usage of
a candidate expression. Thus, in this paper, based
on this observation, we formalize the task of iden-
tifying Japanese compound functional expressions
as a chunking problem, rather than a classification
problem.
28
Table 5: Number of Sentences collected from
1995 Mainichi Newspaper Texts (for 337 Expres-
sions)
# of expressions
50 ? # of sentences 187 (55%)
0 < # of sentences < 50 117 (35%)
# of sentences = 0 33 (10%)
2.3 Developing an Example Database
We developed an example database of Japanese
compound functional expressions, which is used
for training/testing a chunker of Japanese com-
pound functional expressions (Tsuchiya et al,
2005). The corpus from which we collect example
sentences is 1995 Mainichi newspaper text corpus
(1,294,794 sentences, 47,355,330 bytes). For each
of the 337 expressions, 50 sentences are collected
and chunk labels are annotated according to the
following procedure.
1. The expression is morphologically analyzed
by ChaSen, and its morpheme sequence6 is
obtained.
2. The corpus is morphologically analyzed by
ChaSen, and 50 sentences which include the
morpheme sequence of the expression are
collected.
3. For each sentence, every occurrence of the
337 expressions is annotated with one of the
usages functional/content by an annotator7.
Table 5 classifies the 337 expressions accord-
ing to the number of sentences collected from the
1995 Mainichi newspaper text corpus. For more
than half of the 337 expressions, more than 50 sen-
tences are collected, although about 10% of the
377 expressions do not appear in the whole cor-
pus. Out of those 187 expressions with more than
50 sentences, 52 are those with balanced distribu-
tion of the functional/content usages in the news-
paper text corpus. Those 52 expressions can be re-
garded as among the most difficult ones in the task
of identifying and classifying functional/content
6For those expressions whose constituent has conjugation
and the conjugated form also has the same usage as the ex-
pression with the original form, the morpheme sequence is
expanded so that the expanded morpheme sequences include
those with conjugated forms.
7For the most frequent 184 expressions, on the average,
the agreement rate between two human annotators is 0.93 and
the Kappa value is 0.73, which means allowing tentative con-
clusions to be drawn (Carletta, 1996; Ng et al, 1999). For
65% of the 184 expressions, the Kappa value is above 0.8,
which means good reliability.
usages. Thus, this paper focuses on those 52 ex-
pressions in the training/testing of chunking com-
pound functional expressions. We extract 2,600
sentences (= 52 expressions ? 50 sentences) from
the whole example database and use them for
training/testing the chunker. The number of the
morphemes for the 2,600 sentences is 92,899. We
ignore the chunk labels for the expressions other
than the 52 expressions, resulting in 2,482/701
chunk labels for the functional/content usages, re-
spectively.
3 Chunking Japanese Compound
Functional Expressions with SVMs
3.1 Support Vector Machines
The principle idea of SVMs is to find a separate
hyperplane that maximizes the margin between
two classes (Vapnik, 1998). If the classes are not
separated by a hyperplane in the original input
space, the samples are transformed in a higher di-
mensional features space.
Giving x is the context (a set of features) of
an input example; xi and yi(i = 1, ..., l, xi ?
Rn, yi?{1,?1}) indicate the context of the train-
ing data and its category, respectively; The deci-
sion function f in SVM framework is defined as:
f(x) = sgn
( l
?
i=1
?iyiK(xi,x) + b
)
(1)
where K is a kernel function, b ? R is a thresh-
old, and ?i are weights. Besides, the weights ?i
satisfy the following constraints:
0 ? ?i ? C (i = 1, ..., l) (2)
?l
i=1 ?iyi = 0 (3)
where C is a misclassification cost. The xi with
non-zero ?i are called support vectors. To train
an SVM is to find the ?i and the b by solving the
optimization problem; maximizing the following
under the constraints of (2) and (3):
L(?) =
l
?
i=1
?i?
1
2
l
?
i,j=1
?i?jyiyjK(x
i
,x
j
) (4)
The kernel function K is used to transform the
samples in a higher dimensional features space.
Among many kinds of kernel functions available,
we focus on the d-th polynomial kernel:
K(x,y) = (x ? y + 1)d (5)
29
Through experimental evaluation on chunking
Japanese compound functional expressions, we
compared polynomial kernels with d = 1, 2, and
3. Kernels with d = 2 and 3 perform best, while
the kernel with d = 3 requires much more compu-
tational cost than that with d = 2. Thus, through-
out the paper, we show results with the quadratic
kernel (d = 2).
3.2 Chunking with SVMs
This section describes details of formalizing the
chunking task using SVMs. In this paper, we use
an SVMs-based chunking tool YamCha8 (Kudo
and Matsumoto, 2001). In the SVMs-based
chunking framework, SVMs are used as classi-
fiers for assigning labels for representing chunks
to each token. In our task of chunking Japanese
compound functional expressions, each sentence
is represented as a sequence of morphemes, where
a morpheme is regarded as a token.
3.2.1 Chunk Representation
For representing proper chunks, we employ
IOB2 representation, one of those which have
been studied well in various chunking tasks of nat-
ural language processing (Tjong Kim Sang, 1999;
Kudo and Matsumoto, 2001). This method uses
the following set of three labels for representing
proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
As we described in section 2.2, given a candi-
date expression, we classify the usages of the ex-
pression into two classes: functional and content.
Accordingly, we distinguish the chunks of the two
types: the functional type chunk and the content
type chunk. In total, we have the following five la-
bels for representing those chunks: B-functional,
I-functional, B-content, I-content, and O. Ta-
ble 6 gives examples of those chunk labels rep-
resenting chunks.
Finally, as for exending SVMs to multi-class
classifiers, we experimentally compare the pair-
wise method and the one vs. rest method, where
the pairwise method slightly outperformed the one
vs. rest method. Throughout the paper, we show
results with the pairwise method.
8http://chasen.org/?taku/software/
yamcha/
3.2.2 Features
For the feature sets for training/testing of
SVMs, we use the information available in the sur-
rounding context, such as the morphemes, their
parts-of-speech tags, as well as the chunk labels.
More precisely, suppose that we identify the chunk
label ci for the i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, mi is the morpheme appearing at i-th po-
sition, Fi is the feature set at i-th position, and ci
is the chunk label for i-th morpheme. Roughly
speaking, when identifying the chunk label ci for
the i-th morpheme, we use the feature sets Fi?2,
Fi?1, Fi, Fi+1, Fi+2 at the positions i ? 2, i ? 1,
i, i + 1, i + 2, as well as the preceding two chunk
labels ci?2 and ci?1.
The detailed definition of the feature set Fi at i-
th position is given below. The feature set Fi is de-
fined as a tuple of the morpheme feature MF (mi)
of the i-th morpheme mi, the chunk candidate fea-
ture CF (i) at i-th position, and the chunk context
feature OF (i) at i-th position.
Fi = ? MF (mi), CF (i), OF (i) ?
The morpheme feature MF (mi) consists of the
lexical form, part-of-speech, conjugation type and
form, base form, and pronunciation of mi.
The chunk candidate feature CF (i) and the
chunk context feature OF (i) are defined consid-
ering the candidate compound functional expres-
sion, which is a sequence of morphemes includ-
ing the morpheme mi at the current position i. As
we described in section 2, the class of Japanese
compound functional expressions can be regarded
as closed and their number is at most a few thou-
sand. Therefore, it is easy to enumerate all the
compound functional expressions and their mor-
pheme sequences. Chunk labels other than O
should be assigned to a morpheme only when it
constitutes at least one of those enumerated com-
pound functional expressions. Suppose that a se-
quence of morphemes mj . . . mi . . . mk including
mi at the current position i constitutes a candidate
functional expression E as below:
m
j?2
m
j?1
m
j
. . . m
i
. . . m
k
m
k+1
m
k+2
candidate E of
a compound
functional expression
where the morphemes mj?2, mj?1, mk+1, and
mk+2 are at immediate left/right contexts of E.
Then, the chunk candidate feature CF (i) at i-th
position is defined as a tuple of the number of mor-
phemes constituting E and the position of mi in
E. The chunk context feature OF (i) at i-th posi-
tion is defined as a tuple of the morpheme features
30
Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features
(a) Sentence (7) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
?? (giron) (discussion) O ? ?
? (ga) (NOM) O ? ?
???(owatt) (finish) O ? ?
?? (tara) (after) O ? ?
?? (kyuukei) (break) O ? ?
? (shi) (have) O ? ?
? (te) (may) B-functional ?2, 1? ? MF (?? (kyuukei)), ?, MF (? (shi)), ?,
?? (ii) I-functional ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
(b) Sentence (8) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
??? (bag) (discussion) O ? ?
? (ha) (TOP) O ? ?
??? (ookiku) (big) O ? ?
? (te) (because) B-content ?2, 1? ? MF (? (ha)), ?, MF (??? (ookiku)), ?,
?? (ii) (nice) I-content ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
as well as the chunk candidate features at immedi-
ate left/right contexts of E.
CF (i) = ? length of E, position of m
i
in E ?
OF (i) = ? MF (m
j?2
), CF (j ? 2),
MF (m
j?1
), CF (j ? 1),
MF (m
k+1
), CF (k + 1),
MF (m
k+2
), CF (k + 2) ?
Table 6 gives examples of chunk candidate fea-
tures and chunk context features
It can happen that the morpheme at the cur-
rent position i constitutes more than one candidate
compound functional expression. For example,
in the example below, the morpheme sequences
mi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-
tute candidate expressions E
1
, E
2
, and E
3
, respec-
tively.
Morpheme sequence m
i?1
m
i
m
i+1
m
i+2
Candidate E
1
m
i?1
m
i
m
i+1
Candidate E
2
m
i?1
m
i
Candidate E
3
m
i
m
i+1
m
i+2
In such cases, we prefer the one starting with the
leftmost morpheme. If more than one candidate
expression starts with the leftmost morpheme, we
prefer the longest one. In the example above, we
prefer the candidate E
1
and construct the chunk
candidate features and chunk context features con-
sidering E
1
only.
4 Experimental Evaluation
The detail of the data set we use in the experimen-
tal evaluation was presented in section 2.3. As we
show in Table 7, performance of our SVMs-based
chunkers as well as several baselines including ex-
isting Japanese text processing tools is evaluated
in terms of precision/recall/F?=1 of identifying
functional chunks. Performance is evaluated also
in terms of accuracy of classifying detected can-
didate expressions into functional/content chunks.
Among those baselines, ?majority ( = functional)?
always assigns functional usage to the detected
candidate expressions. ?Hand-crafted rules? are
manually created 145 rules each of which has con-
ditions on morphemes constituting a compound
functional expression as well as those at immedi-
ate left/right contexts. Performance of our SVMs-
based chunkers is measured through 10-fold cross
validation.
As shown in Table 7, our SVMs-based chunkers
significantly outperform those baselines both in
F?=1 and classification accuracy9. We also evalu-
ate the effectiveness of each feature set, i.e., the
morpheme feature, the chunk candidate feature,
and the chunk context feature. The results in the
table show that the chunker with the chunk candi-
date feature performs almost best even without the
chunk context feature10.
9Recall of existing Japanese text processing tools is low,
because those tools can process only 50?60% of the whole
52 compound functional expressions, and for the remaining
40?50% expressions, they fail in identifying all of the occur-
rences of functional usages.
10It is also worthwhile to note that training the SVMs-
based chunker with the full set of features requires computa-
tional cost three times as much as training without the chunk
31
Table 7: Evaluation Results (%)
Identifying Acc. of classifying
functional chunks functional/content
Prec. Rec. F?=1 chunks
majority ( = functional) 78.0 100 87.6 78.0
Baselines Juman/KNP 89.2 49.3 63.5 55.8
ChaSen/CaboCha 89.0 45.6 60.3 53.2
hand-crafted rules 90.7 81.6 85.9 79.1
SVM morpheme 88.0 91.0 89.4 86.5
(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0
set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2
Figure 1: Change of F?=1 with Different Number
of Training Instances
For the SVMs-based chunker with the chunk
candidate feature with/without the chunk context
feature, Figure 1 plots the change of F?=1 when
training with different number of labeled chunks
as training instances. With this result, the increase
in F?=1 seems to stop with the maximum num-
ber of training instances, which supports the claim
that it is sufficient to collect and manually annotate
about 50 training examples per expression.
5 Concluding Remarks
The Japanese language has various types of com-
pound functional expressions, which are very im-
portant for recognizing the syntactic structures of
Japanese sentences and for understanding their se-
mantic contents. In this paper, we formalized
the task of identifying Japanese compound func-
tional expressions in a text as a chunking prob-
lem. We applied a machine learning technique
to this task, where we employed that of Sup-
port Vector Machines (SVMs). We showed that
the proposed method significantly outperforms ex-
isting Japanese text processing tools. The pro-
context feature.
posed framework has advantages over an approach
based on manually created rules such as the one in
(Shudo et al, 2004), in that it requires human cost
to manually create and maintain those rules. On
the other hand, in our framework based on the ma-
chine learning technique, it is sufficient to collect
and manually annotate about 50 training examples
per expression.
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistic. Computational Linguistics,
22(2):249?254.
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten.
Kuroshio Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support
vector machines. In Proc. 2nd NAACL, pages 192?199.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named entity
recognition using hundreds of thousands of features. In
Proc. 7th CoNLL, pages 184?187.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo
Hukugouji Youreishu. (in Japanese).
H. T. Ng, C. Y. Lim, and S. K. Foo. 1999. A case study on
inter-annotator agreement for word sense disambiguation.
In Proc. ACL SIGLEXWorkshop on Standardizing Lexical
Resources, pages 9?13.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc.
2nd ACL Workshop on Multiword Expressions: Integrat-
ing Processing, pages 32?39.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. 9th EACL, pages 173?179.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2005. A corpus for classifying usages of Japanese
compound functional expressions. In Proc. PACLING,
pages 345?350.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
32
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 65?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Dependency Relations of
Japanese Compound Functional Expressions
Takehito Utsuro? and Takao Shime? and Masatoshi Tsuchiya??
Suguru Matsuyoshi?? and Satoshi Sato??
?Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
?NEC Corporation
??Computer Center, Toyohashi University of Technology,
Tenpaku-cho, Toyohashi, 441?8580, JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
This paper proposes an approach of process-
ing Japanese compound functional expressions
by identifying them and analyzing their depen-
dency relations through a machine learning tech-
nique. First, we formalize the task of identify-
ing Japanese compound functional expressions
in a text as a machine learning based chunking
problem. Next, against the results of identify-
ing compound functional expressions, we apply
the method of dependency analysis based on the
cascaded chunking model. The results of ex-
perimental evaluation show that, the dependency
analysis model achieves improvements when ap-
plied after identifying compound functional ex-
pressions, compared with the case where it is ap-
plied without identifying compound functional
expressions.
1 Introduction
In addition to single functional words, the Japanese
language has many more compound functional ex-
pressions which consist of more than one word in-
cluding both content words and functional words.
They are very important for recognizing syntactic
structures of Japanese sentences and for understand-
ing their semantic content. Recognition and under-
standing of them are also very important for vari-
ous kinds of NLP applications such as dialogue sys-
tems, machine translation, and question answering.
However, recognition and semantic interpretation of
compound functional expressions are especially dif-
ficult because it often happens that one compound
expression may have both a literal (i.e. compo-
sitional) content word usage and a non-literal (i.e.
non-compositional) functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni) ???
(tsuite)?, which consists of a post-positional particle
?? (ni)?, and a conjugated form ???? (tsuite)? of
a verb ??? (tsuku)?. In the sentence (A), the com-
pound expression functions as a case-marking parti-
cle and has a non-compositional functional meaning
?about?. On the other hand, in the sentence (B), the
expression simply corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ?? (ni)? and the verb ????
(tsuite)?, and has a content word meaning ?follow?.
Therefore, when considering machine translation of
these Japanese sentences into English, it is neces-
sary to judge precisely the usage of the compound
expression ?? (ni)??? (tsuite)?, as shown in the
English translation of the two sentences in Table 1.
There exist widely-used Japanese text processing
tools, i.e. combinations of a morphological analy-
sis tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. However,
they process those compound expressions only par-
tially, in that their morphological analysis dictionar-
ies list only a limited number of compound expres-
sions. Furthermore, even if certain expressions are
listed in a morphological analysis dictionary, those
existing tools often fail in resolving the ambigui-
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
65
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
Correct English Translation:
( As a means of solving the problem, USA recommended the activity of OSCE in which Russia participates.)
(1) Correct Dependency Relation by Identifying Compound Functional Expression: ??????
with a Case Marking Particle Usage.
(2)  Incorrect Dependency Relation without Identifying Compound Functional Expression: ??????,
which Literally Consists of a Post-positional Particle ??? (with) and a Conjugation Form ????
of a Verb ???? (do).
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP as a means for solution       Russia-NOM also             participate in                                of  OSCE activity-ACC          recommended
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP with a means for Russia-NOM also             participate in                            of  OSCE activity-ACC       recommended
solution
Figure 1: Example of Improving Dependency Analysis of Compound Functional Expressions by Identifying
them before Dependency Analysis
ties of their usages, such as those in Table 1. This
is mainly because the framework of these existing
tools is not designed so as to resolve such ambigu-
ities of compound (possibly functional) expressions
by carefully considering the context of those expres-
sions.
Actually, as we introduce in the next section, as a
first step towards studying computational processing
of compound functional expressions, we start with
125 major functional expressions which have non-
compositional usages, as well as their variants (337
expressions in total). Out of those 337 expressions,
111 have both a content word usage and a functional
usage. However, the combination of JUMAN+KNP
is capable of distinguishing the two usages only for
43 of the 111 expressions, and the combination of
ChaSen+CaboCha only for 40 of those 111 expres-
sions. Furthermore, the failure in distinguishing the
two usages may cause errors of syntactic analysis.
For example, (1) of Figure 1 gives an example of
identifying a correct modifiee of the second bunsetsu
segment 5 ???????? (as a means for solu-
tion)? including a Japanese compound functional ex-
pression ???? (as)?, by appropriately detecting
the compound functional expression before depen-
dency analysis. On the other hand, (2) of Figure 1
gives an example of incorrectly indicating an erro-
neous modifiee of the third bunsetsu ????, which
actually happens if we do not identify the compound
functional expression ???? (as)? before depen-
dency analysis of this sentence.
Considering such a situation, it is necessary to
develop a tool which properly recognizes and se-
mantically interprets Japanese compound functional
expressions. This paper proposes an approach of
processing Japanese compound functional expres-
sions by identifying them and analyzing their de-
pendency relations through a machine learning tech-
nique. The overall flow of processing compound
functional expressions in a Japanese sentence is il-
5A Japanese bunsetsu segment is a phrasal unit which con-
sits of at least one content word and zero or more functional
words.
66
( As a means of solving the 
problem, USA recommended the 
activity of OSCE in which Russia 
participates.)
???????????
????????????
??????????
?????
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
morphological 
analysis
by ChaSen
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
compound
functional 
expression
Identifying
compound
functional
expression
chunking
bunsetsu
segmentation
&
dependency
analysis
bunsetsu
segment
dependency
relation
Figure 2: Overall Flow of Processing Compound Functional Expressions in a Japanese Sentence
lustrated in Figure 2. First of all, we assume a
sequence of morphemes obtained by a variant of
ChaSen with all the compound functional expres-
sions removed from its outputs, as an input to our
procedure of identifying compound functional ex-
pressions and analyzing their dependency relations.
We formalize the task of identifying Japanese com-
pound functional expressions in a text as a machine
learning based chunking problem (Tsuchiya et al,
2006). We employ the technique of Support Vec-
tor Machines (SVMs) (Vapnik, 1998) as the ma-
chine learning technique, which has been success-
fully applied to various natural language process-
ing tasks including chunking tasks such as phrase
chunking and named entity chunking. Next, against
the results of identifying compound functional ex-
pressions, we apply the method of dependency anal-
ysis based on the cascaded chunking model (Kudo
and Matsumoto, 2002), which is simple and efficient
because it parses a sentence deterministically only
deciding whether the current bunsetsu segment mod-
ifies the one on its immediate right hand side. As
we showed in Figure 1, identifying compound func-
tional expressions before analyzing dependencies in
a sentence does actually help deciding dependency
relations of compound functional expressions.
In the experimental evaluation, we focus on 59
expressions having balanced distribution of their us-
ages in the newspaper text corpus and are among the
most difficult ones in terms of their identification in
a text. We first show that the proposed method of
chunking compound functional expressions signifi-
cantly outperforms existing Japanese text processing
tools. Next, we further show that the dependency
analysis model of (Kudo and Matsumoto, 2002) ap-
plied to the results of identifying compound func-
tional expressions significantly outperforms the one
applied to the results without identifying compound
functional expressions.
2 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) exam-
ine 450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their example
sentences. Compared with those two collections,
Gendaigo Hukugouji Youreishu (National Language
Research Institute, 2001) (henceforth, denoted as
GHY) concentrates on 125 major functional expres-
sions which have non-compositional usages, as well
as their variants6, and collects example sentences of
those expressions. As we mentioned in the previous
section, as a first step towards developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants (337 expressions in to-
6For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) inser-
tion/deletion/alternation of certain particles, ii) alternation of
synonymous words, iii) normal/honorific/conversational forms,
iv) base/adnominal/negative forms.
67
(a) Classification of Compound Functional Expressions based on Grammatical Function
Grammatical Function Type # of major expressions # of variants Example
post-positional conjunctive particle 36 67 ??? (kuse-ni)
particle type case-marking particle 45 121 ??? (to-shite)
adnominal particle 2 3 ??? (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
(b) Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ??? ??????? ??? ???????????????? functional
(kuse-ni) (To my brother, (someone) gave money, while (he/she) did noth-
ing to me but just sent a letter.)
(??? (kuse-ni) = while)
(2) ??? ???? ??? ??????? content
(kuse-ni) (They all were surprised by his habit.) (???? (kuse-ni)
= by one?s habit
(3) ??? ?????????? ??? ??????? functional
(to-shite) (He is known as an expert of the problem.) (???? (to-shite)
= as ?)
(4) ??? ?????????????? ??? ???? content
(to-shite) (Please make it clear whether this is true or not.) (?? ???? (to-shite)
= make ? ?
(5) ??? ??????? ??? ???????? functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ?????????? ??? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this discussion.) (???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
Table 2: Classification and Example Usages of Compound Functional Expressions
tal). In this paper, following (Sag et al, 2002), we
regard each variant as a fixed expression, rather than
a semi-fixed expression or a syntactically-flexible
expression 7. Then, we focus on evaluating the
effectiveness of straightforwardly applying a stan-
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2 (a), according to their grammat-
ical functions, those 337 expressions in total are
roughly classified into post-positional particle type,
and auxiliary verb type. Functional expressions of
post-positional particle type are further classified
into three subtypes: i) conjunctive particle types,
which are used for constructing subordinate clauses,
ii) case-marking particle types, iii) adnominal parti-
cle types, which are used for constructing adnominal
7Compound functional expressions of auxiliary verb types
can be regarded as syntactically-flexible expressions.
clauses. Furthermore, for examples of compound
functional expressions listed in Table 2 (a), Table 2
(b) gives their example sentences as well as the de-
scription of their usages.
3 Identifying Compound Functional
Expressions by Chunking with SVMs
This section describes summaries of formalizing the
chunking task using SVMs (Tsuchiya et al, 2006).
In this paper, we use an SVMs-based chunking tool
YamCha8 (Kudo and Matsumoto, 2001). In the
SVMs-based chunking framework, SVMs are used
as classifiers for assigning labels for representing
chunks to each token. In our task of chunking
Japanese compound functional expressions, each
8http://chasen.org/?taku/software/
yamcha/
68
sentence is represented as a sequence of morphemes,
where a morpheme is regarded as a token.
3.1 Chunk Representation
For representing proper chunks, we employ IOB2
representation, which has been studied well in var-
ious chunking tasks of natural language processing.
This method uses the following set of three labels
for representing proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
Given a candidate expression, we classify the us-
ages of the expression into two classes: functional
and content. Accordingly, we distinguish the chunks
of the two types: the functional type chunk and the
content type chunk. In total, we have the follow-
ing five labels for representing those chunks: B-
functional, I-functional, B-content, I-content, and
O. Finally, as for extending SVMs to multi-class
classifiers, we experimentally compare the pairwise
method and the one vs. rest method, where the pair-
wise method slightly outperformed the one vs. rest
method. Throughout the paper, we show results with
the pairwise method.
3.2 Features
For the feature sets for training/testing of SVMs, we
use the information available in the surrounding con-
text, such as the morphemes, their parts-of-speech
tags, as well as the chunk labels. More precisely,
suppose that we identify the chunk label c
i
for the
i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, m
i
is the morpheme appearing at i-th posi-
tion, F
i
is the feature set at i-th position, and c
i
is
the chunk label for i-th morpheme. Roughly speak-
ing, when identifying the chunk label c
i
for the i-th
morpheme, we use the feature sets F
i?2
, F
i?1
, F
i
,
F
i+1
, F
i+2
at the positions i ? 2, i ? 1, i, i + 1,
i+2, as well as the preceding two chunk labels c
i?2
and c
i?1
. The detailed definition of the feature set
F
i
at i-th position is given in (Tsuchiya et al, 2006),
which mainly consists of morphemes as well as in-
formation on the candidate compound functional ex-
pression at i-th position.
4 Learning Dependency Relations of
Japanese Compound Functional
Expressions
4.1 Japanese Dependency Analysis using
Cascaded Chunking
4.1.1 Cascaded Chunking Model
First of all, we define a Japanese sen-
tence as a sequence of bunsetsu segments
B = ?b
1
, b
2
, . . . , b
m
? and its syntactic struc-
ture as a sequence of dependency patterns
D = ?Dep(1), Dep(2), . . . , Dep(m ? 1)?, where
Dep(i) = j means that the bunsetsu segment b
i
depends on (modifies) bunsetsu segment b
j
. In
this framework, we assume that the dependency
sequence D satisfies the following two constraints:
1. Japanese is a head-final language. Thus, except
for the rightmost one, each bunsetsu segment
modifies exactly one bunsetsu segment among
those appearing to its right.
2. Dependencies do not cross one another.
Unlike probabilistic dependency analysis models
of Japanese, the cascaded chunking model of Kudo
and Matsumoto (2002) does not require the proba-
bilities of dependencies and parses a sentence de-
terministically. Since Japanese is a head-final lan-
guage, and the chunking can be regarded as the cre-
ation of a dependency between two bunsetsu seg-
ments, this model simplifies the process of Japanese
dependency analysis as follows: 9
1. Put an O tag on all bunsetsu segments. The O
tag indicates that the dependency relation of the
current segment is undecided.
2. For each bunsetsu segment with an O tag, de-
cide whether it modifies the bunsetsu segment
on its immediate right hand side. If so, the O
tag is replaced with a D tag.
3. Delete all bunsetsu segments with a D tag that
immediately follows a bunsetsu segment with
an O tag.
9The O and D tags used in this section have no relation to
those chunk reppresentation tags introduced in section 3.1.
69
Initialization
?? ??? ??? ??? ?????
( He was moved by her warm heart. )
He her warm heart be moved
Input:
Tag:
?? ??? ??? ??? ?????
O O O O O
Input:
Tag:
?? ??? ??? ??? ?????
O O D D O
Deleted
Input:
Tag:
?? ??? ??? ?????
O D D O
Deleted
Input:
Tag:
?? ??? ?????
O D O
Input:
Tag:
?? ?????
O
Deleted
Input:
Tag:
?????
O
Finish
D
Deleted
Figure 3: Example of the Parsing Process with Cas-
caded Chunking Model
4. Terminate the algorithm if a single bunsetsu
segment remains, otherwise return to the step
2 and repeat.
Figure 3 shows an example of the parsing process
with the cascaded chunking model.
4.1.2 Features
As a Japanese dependency analyzer based on the
cascaded chunking model, we use the publicly avail-
able version of CaboCha (Kudo and Matsumoto,
2002), which is trained with the manually parsed
sentences of Kyoto text corpus (Kurohashi and Na-
gao, 1998), that are 38,400 sentences selected from
the 1995 Mainichi newspaper text.
The standard feature set used by CaboCha con-
sists of static features and dynamic features. Static
features are those solely defined once the pair
of modifier/modifiee bunsetsu segments is speci-
fied. For the pair of modifier/modifiee bunsetsu
segments, the following are used as static fea-
tures: head words and their parts-of-speech tags,
inflection-types/forms, functional words and their
parts-of-speech tags, inflection-types/forms, inflec-
tion forms of the words that appear at the end
of bunsetsu segments. As for features between
modifier/modifiee bunsetsu segments, the distance
of modifier/modifiee bunsetsu segments, existence
of case-particles, brackets, quotation-marks, and
punctuation-marks are used as static features. On the
other hand, dynamic features are created during the
parsing process, so that, when a certain dependency
relation is determined, it can have some influence
on other dependency relations. Dynamic features in-
clude bunsetsu segments modifying the current can-
didate modifiee (see Kudo and Matsumoto (2002)
for the details).
4.2 Coping with Compound Functional
Expressions
As we show in Figure 2, a compound functional ex-
pression is identified as a sequence of several mor-
phemes and then chunked into one morpheme. The
result of this identification process is then trans-
formed into the sequence of bunsetsu segments. Fi-
nally, to this modified sequence of bunsetsu seg-
ments, the method of dependency analysis based on
the cascaded chunking model is applied.
Here, when chunking a sequence of several mor-
phemes constituting a compound functional expres-
sion, the following two cases may exist:
(A) As in the case of the example (A) in Table 1, the
two morphemes constituting a compound func-
tional expression ?? (ni)??? (tsuite)? over-
laps the boundary of two bunsetsu segments.
In such a case, when chunking the two mor-
phemes into one morpheme corresponding to
a compound functional expression, those two
bunsetsu segments are concatenated into one
bunsetsu segment.
? ?
kare ni
(he)
???
tsuite
=?
? ????
kare ni-tsuite
(he) (about)
(B) As we show below, a compound functional ex-
pression ??? (koto)? (ga)?? (aru)? over-
laps the boundary of two bunsetsu segments,
though the two bunsetsu segments concatenat-
ing into one bunsetsu segment does include no
content words. In such a case, its immedi-
ate left bunsetsu segment (???(itt)? (ta)? in
the example below), which corresponds to the
content word part of ??? (koto)? (ga)??
(aru)?, has to be concatenated into the bunsetsu
segment ??? (koto)? (ga)?? (aru)?.
70
?? ?
itt ta
(went)
?? ?
koto ga
??
aru
=?
?? ? ?????
itt ta koto-ga-aru
(have been ?)
Next, to the compound functional expression, we
assign one of the four grammatical function types
listed in Table 2 as its POS tag. For example,
the compound functional expression ?? (ni)???
(tsuite)? in (A) above is assigned the grammatical
function type ?case-marking particle type?, while ?
?? (koto) ? (ga) ?? (aru)? in (B) is assigned
?auxiliary verb type?.
These modifications cause differences in the final
feature representations. For example, let us compare
the feature representations of the modifier bunsetsu
segments in (1) and (2) of Figure 1. In (1), the mod-
ifier bunsetsu segment is ????????? which
has the compound functional expression ?????
in its functional word part. On the other hand, in
(2), the modifier bunsetsu segment is ????, which
corresponds to the literal verb usage of a part of the
compound functional expression ?????. In the
final feature representations below, this causes the
following differences in head words and functional
words / POS of the modifier bunsetsu segments:
(1) of Figure 1 (2) of Figure 1
head word ?? (means) ?? (do)
functional word ??? (as) ? (and)
POS subsequent to nominal conjunctive
/ modifying predicate particle
5 Experimental Evaluation
5.1 Training/Test Data Sets
For the training of chunking compound functional
expressions, we collected 2,429 example sentences
from the 1995 Mainichi newspaper text corpus. For
each of the 59 compound functional expressions for
evaluation mentioned in section 1, at least 50 ex-
amples are included in this training set. For the
testing of chunking compound functional expres-
sions, as well as training/testing of learning depen-
dencies of compound functional expressions, we
used manually-parsed sentences of Kyoto text cor-
pus (Kurohashi and Nagao, 1998), that are 38,400
sentences selected from the 1995 Mainichi newspa-
per text (the 2,429 sentences above are selected so
that they are exclusive of the 37,400 sentences of
Kyoto text corpus.). To those data sets, we manually
annotate usage labels of the 59 compound functional
expressions (details in Table 3).
Usages # of
functional content total sentences
for chunker
training 1918 1165 3083 2429
Kyoto text corpus 5744 1959 7703 38400
Table 3: Statistics of Data Sets
Identifying
functional chunks
Acc. of
classifying
functional /
content
Prec. Rec. F
?=1
chunks
majority ( = functional) 74.6 100 85.5 74.6
Juman/KNP 85.8 40.5 55.0 58.4
ChaSen/CaboCha 85.2 26.7 40.6 51.1
SVM 91.4 94.6 92.9 89.3
Table 4: Evaluation Results of Chunking (%)
5.2 Chunking
As we show in Table 4, performance of our SVMs-
based chunkers as well as several baselines includ-
ing existing Japanese text processing tools is evalu-
ated in terms of precision/recall/F
?=1
of identifying
all the 5,744 functional chunks included in the test
data (Kyoto text corpus in Table 3). Performance is
evaluated also in terms of accuracy of classifying de-
tected candidate expressions into functional/content
chunks. Among those baselines, ?majority ( = func-
tional)? always assigns functional usage to the de-
tected candidate expressions. Performance of our
SVMs-based chunkers is measured through 10-fold
cross validation. Our SVMs-based chunker signif-
icantly outperforms those baselines both in F
?=1
and classification accuracy. As we mentioned in
section 1, existing Japanese text processing tools
process compound functional expressions only par-
tially, which causes damage in recall in Table 4.
5.3 Analyzing Dependency Relations
We evaluate the accuracies of judging dependency
relations of compound functional expressions by the
variant of CaboCha trained with Kyoto text cor-
pus annotated with usage labels of compound func-
tional expressions. This performance is measured
through 10-fold cross validation with the modified
version of the Kyoto text corpus. In the evaluation
phase, according to the flow of Figure 2, first we ap-
ply the chunker of compound functional expressions
trained with all the 2,429 sentences in Table 3 and
obtain the results of chunked compound functional
expressions with about 90% correct rate. Then, bun-
setsu segmentation and dependency analysis are per-
71
modifier modifiee
baselines CaboCha (w/o FE) 72.5 88.0
CaboCha (public) 73.9 87.6
chunker + CaboCha (proposed) 74.0 88.0
reference + CaboCha (proposed) 74.4 88.1
Table 5: Accuracies of Identifying Modi-
fier(s)/Modifiee (%)
formed by our variant of CaboCha, where accu-
racies of identifying modifier(s)/modifiee of com-
pound functional expressions are measured as in Ta-
ble 5 (?chunker + CaboCha (proposed)? denotes that
inputs to CaboCha (proposed) are with 90% correct
rate, while ?reference + CaboCha (proposed)? de-
notes that they are with 100% correct rate). Here,
?CaboCha (w/o FE)? denotes a baseline variant of
CaboCha, with all the compound functional expres-
sions removed from its inputs (which are outputs
from ChaSen), while ?CaoboCha (public)? denotes
the publicly available version of CaboCha, which
have some portion of the compound functional ex-
pressions included in its inputs.
For the modifier accuracy, the difference of
?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is statistically significant at a level of
0.05. Identifying compound functional expressions
typically contributes to improvements when the lit-
eral constituents of a compound functional expres-
sion include a verb. In such a case, for bunsetsu
segments which usually modifies a verb, an incor-
rect modifee candidate is removed, which results in
improvements in the modifier accuracy. The dif-
ference between ?CaoboCha (public)? and ?chunker
+ CaboCha (proposed)? is slight because the pub-
licly available version of CaboCha seems to include
compound functional expressions which are dam-
aged in identifying their modifiers with ?CaboCha
(w/o FE)?. For the modifiee accuracy, the difference
of ?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is zero. Here, more than 100 instances of
improvements like the one in Figure 1 are observed,
while almost the same number of additional fail-
ures are also observed mainly because of the sparse-
ness problem. Furthermore, in the case of the modi-
fiee accuracy, it is somehow difficult to expect im-
provement because identifying modifiees of func-
tional/content bunsetsu segments mostly depends on
features other than functional/content distinction.
6 Concluding Remarks
We proposed an approach of processing Japanese
compound functional expressions by identifying
them and analyzing their dependency relations
through a machine learning technique. This ap-
proach is novel in that it has never been applied
to any language so far. Experimental evaluation
showed that the dependency analysis model applied
to the results of identifying compound functional ex-
pressions significantly outperforms the one applied
to the results without identifying compound func-
tional expressions. The proposed framework has ad-
vantages over an approach based on manually cre-
ated rules such as the one in (Shudo et al, 2004), in
that it requires human cost to create manually and
maintain those rules. Related works include Nivre
and Nilsson (2004), which reports improvement of
Swedish parsing when multi word units are manu-
ally annotated.
References
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten. Kuroshio
Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support vec-
tor machines. In Proc. 2nd NAACL, pages 192?199.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency ana-
lyisis using cascaded chunking. In Proc. 6th CoNLL, pages
63?69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese parsed
corpus while improving the parsing system. In Proc. 1st
LREC, pages 719?724.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo Huku-
gouji Youreishu. (in Japanese).
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic
parsing. In Proc. LRECWorkshop, Methodologies and Eval-
uation of Multiword Units in Real-World Applications, pages
39?46.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc. 2nd
ACL Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 32?39.
M. Tsuchiya, T. Shime, T. Takagi, T. Utsuro, K. Uchimoto,
S. Matsuyoshi, S. Sato, and S. Nakagawa. 2006. Chunk-
ing Japanese compound functional expressions by machine
learning. In Proc. Workshop on Multi-Word-Expressions in
a Multilingual Context, pages 25?32.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
72
