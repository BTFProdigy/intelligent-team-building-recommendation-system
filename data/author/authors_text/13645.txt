Coling 2010: Poster Volume, pages 543?551,
Beijing, August 2010
Generative Alignment and Semantic Parsing
for Learning from Ambiguous Supervision
Joohyun Kim
Department of Computer Science
The University of Texas at Austin
scimitar@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
mooney@cs.utexas.edu
Abstract
We present a probabilistic generative
model for learning semantic parsers from
ambiguous supervision. Our approach
learns from natural language sentences
paired with world states consisting of
multiple potential logical meaning repre-
sentations. It disambiguates the mean-
ing of each sentence while simultane-
ously learning a semantic parser that maps
sentences into logical form. Compared
to a previous generative model for se-
mantic alignment, it also supports full
semantic parsing. Experimental results
on the Robocup sportscasting corpora in
both English and Korean indicate that
our approach produces more accurate se-
mantic alignments than existing methods
and also produces competitive semantic
parsers and improved language genera-
tors.
1 Introduction
Most approaches to learning semantic parsers that
map sentences into complete logical forms (Zelle
and Mooney, 1996; Zettlemoyer and Collins,
2005; Kate and Mooney, 2006; Wong and
Mooney, 2007b; Lu et al, 2008) require fully-
supervised corpora that provide full formal logi-
cal representations for each sentence. Such cor-
pora are expensive and difficult to construct. Sev-
eral recent projects on ?grounded? language learn-
ing (Kate and Mooney, 2007; Chen and Mooney,
2008; Chen et al, 2010; Liang et al, 2009) exploit
more easily and naturally available training data
consisting of sentences paired with world states
consisting of multiple potential semantic repre-
sentations. This setting is partially motivated by a
desire to model how children naturally learn lan-
guage in the context of a rich, ambiguous percep-
tual environment.
In particular, Chen and Mooney (2008) in-
troduced the problem of learning to sportscast
by simply observing natural language commen-
tary on simulated Robocup robot soccer games.
The training data consists of natural language
(NL) sentences ambiguously paired with logical
meaning representations (MRs) describing recent
events in the game extracted from the simulator.
Most sentences describe one of the extracted re-
cent events; however, the specific event to which
it refers is unknown. Therefore, the learner has
to figure out the correct matching (alignment) be-
tween NL and MR before inducing a semantic
parser or language generator. Based on an ap-
proach introduced by Kate and Mooney (2007),
Chen and Mooney (2008) repeatedly retrain both
a supervised semantic parser and language gener-
ator using an iterative algorithm analogous to Ex-
pectation Maximization (EM). However, this ap-
proach is somewhat ad hoc and does not exploit
a well-defined probabilistic generative model or
real EM training.
On the other hand, Liang et al (2009) in-
troduced a probabilistic generative model for
learning semantic correspondences in ambigu-
ous training data consisting of sentences paired
with observed world states. Compared to Chen
and Mooney (2008), they demonstrated improved
alignment results on Robocup sportscasting data.
However, their model only produces an NL?MR
alignment and does not learn either an effective
543
semantic parser or language generator. In addi-
tion, they use a combination of a simple Markov
model and a bag-of-words model when generating
natural language for MRs, therefore, they do not
model context-free linguistic syntax.
Motivated by the limitations of these previ-
ous methods, we propose a new generative align-
ment model that includes a full semantic pars-
ing model proposed by Lu et al (2008). Our
approach is capable of disambiguating the map-
ping between language and meanings while also
learning a complete semantic parser for mapping
sentences to logical form. Experimental results
on Robocup sportscasting show that our approach
outperforms all previous results on the NL?MR
matching (alignment) task and also produces com-
petitive performance on semantic parsing and im-
proved language generation.
2 Related Work
The conventional approach to learning seman-
tic parsers (Zelle and Mooney, 1996; Ge and
Mooney, 2005; Kate and Mooney, 2006; Zettle-
moyer and Collins, 2007; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2007b; Lu et
al., 2008) requires detailed supervision unambigu-
ously pairing each sentence with its logical form.
However, developing training corpora for these
methods requires expensive expert human labor.
Chen and Mooney (2008) presented methods
for grounded language learning from ambigu-
ous supervision that address three related tasks:
NL?MR alignment, semantic parsing, and natu-
ral language generation. They solved the prob-
lem of aligning sentences and meanings by iter-
atively retraining an existing supervised seman-
tic parser, WASP (Wong and Mooney, 2007b) or
KRISP (Kate and Mooney, 2006), or an existing
supervised natural-language generator, WASP?1
(Wong and Mooney, 2007a). During each iter-
ation, the currently trained parser (generator) is
used to produce an improved NL?MR alignment
that is used to retrain the parser (generator) in the
next iteration. However, this approach does not
use the power of a probabilistic correspondence
between an NL and MRs during training.
On the other hand, Liang et al (2009) pro-
posed a probabilistic generative approach to pro-
duce a Viterbi alignment between NL and MRs.
They use a hierarchical semi-Markov generative
model that first determines which facts to dis-
cuss and then generates words from the predi-
cates and arguments of the chosen facts. They re-
port improved matching accuracy in the Robocup
sportscasting domain. However, they only ad-
dressed the alignment problem and are unable to
parse new sentences into meaning representations
or generate natural language from logical forms.
In addition, the model uses a weak bag-of-words
assumption when estimating links between NL
segments and MR facts. Although it does use a
simple Markov model to order the generation of
the different fields of an MR record, it does not
utilize the full syntax of the NL or MR or their
relationship.
Chen et al (2010) recently reported results
on utilizing the improved alignment produced by
Liang et al (2009)?s model to initialize their own
iterative retraining method. By combining the ap-
proaches, they produced more accurate NL?MR
alignments and improved semantic parsers.
Motivated by this prior research, our approach
combines the generative alignment model of
Liang et al (2009) with the generative semantic
parsing model of Lu et al (2008) in order to fully
exploit the NL syntax and its relationship to the
MR semantics. Therefore, unlike Liang et al?s
simple Markov + bag-of-words model for gener-
ating language, it uses a tree-based model to gen-
erate grammatical NL from structured MR facts.
3 Background
This section describes existing models and algo-
rithms employed in the current research. Our
model is built on top of the generative semantic
parsing model developed by Lu et al (2008). Af-
ter learning a probabilistic alignment and parsing
model, we also used the WASP and WASP?1 sys-
tems to produce additional parsing and generation
results. In particular, since our current system is
incapable of effectively generating NL sentences
from MR logical forms, in order to demonstrate
how our matching results can aid NL generation,
we use WASP?1 to learn a generator. This follows
the experimental scheme of Chen et al (2010),
which demonstrated that an improved NL?MR
544
S
S : pass (PLAYER, PLAYER)
PLAYER
PLAYER : pink10
pink10
passes the ball to PLAYER
PLAYER : pink11
pink11
Figure 1: Sample hybrid tree from English
sportscasting dataset where (w,m) = (pink10
passes the ball to pink11, pass(pink10, pink11))
matching from Liang et al (2009) results in better
overall parsing and generation. Finally, our over-
all generative model uses the IGSL (Iterative Gen-
eration Strategy Learning) method of Chen and
Mooney (2008) to initially estimate the prior prob-
ability of each event-type generating a natural-
language comment.
3.1 Generative Semantic Parsing
Lu et al (2008) introduced a generative seman-
tic parsing model using a hybrid-tree framework.
A hybrid tree is defined over a pair, (w,m), of a
natural-language sentence and its logical meaning
representation. The tree expresses a correspon-
dence between word segments in the NL and the
grammatical structure of the MR. In a hybrid tree,
MR production rules constitute the internal nodes,
while NL words (or phrases) constitute the leaves.
A sample hybrid tree from the English Robocup
data is given in Figure 1.
A generative model based on hybrid trees is de-
fined as follows: starting from a root semantic
category, the model generates a production of the
MR grammar, and then subsequently generates a
mixed hybrid pattern of NL words and child se-
mantic categories. This process is repeated un-
til all leaves in the hybrid tree are NL words (or
phrases). Each generation step is only dependent
on the parent step, thus, generation is assumed to
be a Markov process.
Lu et al (2008)?s generative parsing model es-
timates the joint probability P (T ,w,m), which
represents the probability of generating a hybrid
tree T with NL w, and MR m. This probability
is computed as the product of the probabilities of
the steps in the generative process. Since there are
multiple ways to construct a hybrid tree given a
pair of NL and MR, the data likelihood of the pair
(w,m) given by the learned model is calculated
by summing P (T ,w,m) over all the possible hy-
brid trees for NL w and MR m.
The model is normally trained in a fully su-
pervised setting using NL?MR pairs. In order to
learn from ambiguous supervision, we extend this
model to include an additional generative process
for selecting the subset of available MRs used to
generate NL sentences.
3.2 WASP and WASP?1
WASP (Word-Alignment-based Semantic Parsing)
is a semantic parsing system that uses syntax-
based statistical machine translation techniques. It
induces a probabilistic synchronous context-free
grammar (PSCFG) for generating corresponding
NL?MR pairs. Since a PSCFG is symmetric
with respect to the two languages it generates,
the same learned model can be used for both se-
mantic parsing (mapping NL to MR) and natural
language generation (mapping MR to NL), Since
there is no prespecified formal grammar for the
NL, the WASP?1 system learns an n-gram lan-
guage model for the NL side and uses it to choose
the most probable NL translation for a given MR
using a noisy-channel model.
3.3 IGSL
Chen and Mooney (2008) introduced the IGSL
method for determining which event types a hu-
man commentator is more likely to describe in
natural language. This is sometimes called strate-
gic generation or content selection, the process of
choosing what to say; as opposed to tactical gen-
eration, which determines how to say it. IGSL
uses a method analogous to EM to train on am-
biguously supervised data and iteratively improve
probability estimates for each event type, speci-
fying how likely each MR predicate is to elicit
a comment. The algorithm alternates between
two processes: calculating the expected proba-
bility of an NL?MR matching based on the cur-
rently learned estimates, and updating the prob-
ability of each event type based on the expected
match counts. IGSL was shown to be quite effec-
tive at predicting which events in a Robocup game
545
English Korean
# of NL comments 2036 1999
# of extracted MR events 10452 10668
# of NLs w/ matching MRs 1868 1913
# of MRs w/ matching NLs 4670 4610
Avg. # of MRs per NL 2.50 2.41
Table 1: Stats for Robocup sportscasting data
a human would comment upon. In our proposed
model, we use IGSL probability scores as initial
priors for our event selection model.
4 Evaluation Dataset
In our experiments, we use the Robocup
sportscasting data produced by Chen et al (2010),
which includes both English and Korean com-
mentaries. The data was collected by having both
English and Korean speakers commentate the fi-
nal games from the RoboCup simulation soccer
league for each year from 2001 through 2004. Ta-
ble 1 presents some statistics on this sportscasting
data. To construct the ambiguous training data,
each NL commentary sentence is paired with MRs
for all extracted simulation events that occurred in
the previous 5 seconds (an average of 2.5 events).
Figure 2 shows a sample trace from the
Robocup English data. Each NL commentary sen-
tence normally has several possible MR matches
that occurred within the 5-second window, in-
dicated by edges between the NL and MR.
Bold edges represent gold standard matches con-
structed solely for evaluation purposes. Note that
not every NL has a gold matching MR. This oc-
curs because the sentence refers to unrecognized
or undetected events or situations or because the
matching MR lies outside the 5-second window.
5 Generative Model
Like Liang et al (2009)?s generative alignment
model, our model is designed to estimate P (w|s),
where w is an NL sentence and s is a world state
containing a set of possible MR logical forms that
can be matched to w. However, our approach
is intended to support both determining the most
likely match between an NL and its MR in its
world state, and semantic parsing, i.e. finding the
Natu
ral L
angu
age
Mea
ning
 Rep
rese
ntati
on
Purp
le9 p
repa
res t
o at
tack
pass
 ( Pu
rple
Play
er9 
, Pu
rple
Play
er6 
)
defe
nse 
( Pin
kPla
yer6
 , Pi
nkP
laye
r6 )
Purp
le9 p
asse
s to 
Purp
le6
Purp
le6's
 pas
s wa
s de
fend
ed b
y Pi
nk6
turn
over
 ( pu
rple
6 , p
ink6
 )
ball
stop
ped
upl
e6s
pass
was
defe
nded
by
ink6
Pink
6 ma
kes a
 sho
rt pa
ss to
 Pin
k3
kick
 ( Pi
nkP
laye
r6 )
pass
 ( Pi
nkP
laye
r6 , 
Pink
Play
er3 
)
Pink
 goa
lie n
ow h
as th
e ba
ll
play
mod
e( f
ree_
kick
_r)
pass
 ( Pi
nkP
laye
r3 , 
Pink
Play
er1 
)
Figure 2: Sample trace from Robocup English
data.
most probable mapping from a given NL sentence
to an MR logical form.
Our generative model consists of two stages:
? Event selection: P (e|s), chooses the event e
in the world state s to be described.
? Natural language generation: P (w|e), mod-
els the probability of generating natural-
language sentence w from the MR specified
by event e.
5.1 Event selection model
The event selection model specifies the probabil-
ity distribution for picking an event that is likely
to be commented upon amongst the multiple MR
logical forms in the world state s. The probabil-
ity of picking an event is assumed to depend only
on its event type as given by the predicate of its
MR. For example, the MR pass(pink10, pink11)
has event type pass and arguments pink10 and
pink11.
Our model is similar to Liang et al (2009)?s
record choice model, but we only model their no-
tion of salience, denoting that some event types
are more likely to be described than others. We do
not model their notion of coherence, which mod-
els the order of event types in the commentary. We
found that for sportscasting the order of described
events depends only on the sequence of events in
the game and does not exhibit any additional de-
tectable pattern due to linguistic preferences.
The probability of picking an event e of type te
is denoted by p(te). If there are multiple events
of type t in a world state s, then an event of type
t is selected uniformly from the set s(t) of events
546
of type t in state s. Therefore, the probability of
picking an event is given by:
P (e|s) = p(te)
1
|s(te)|
(1)
5.2 Natural language generation model
The natural-language generation model defines
the probability distribution of NL sentences given
an MR specified by the previously selected event.
We use Lu et al (2008)?s generative model for this
step, in which:
P (w|e) =
?
?T over (w,m)
P (T ,w|m) (2)
where m is the MR logical form defined by event
e and T is a hybrid tree defined over the NL?MR
pair (w,m).
The probability P (T ,w|m) is calculated using
the generative semantic parsing model of Lu et al
(2008) using the joint probability of the NL?MR
pair (w,m), i.e. the inside probability of gener-
ating (w,m). The likelihood of a sentence w is
then the sum over all possible hybrid trees defined
by the NL?MR pair (w,m). 1
The natural language generation model covers
the roles of both the field choice model and word
choice models of Liang et al (2009). Since our
event selection model only chooses an event based
on its type, the order of its arguments still needs
to be addressed. However, Lu et al?s generative
model includes ordering the MR arguments (as
specified by MR production rules) as well as the
generation of NL words and phrases to express
these arguments. Thus, it is unnecessary to sepa-
rately model argument ordering in our approach.2
1Lu et al (2008) propose 3 models for generative seman-
tic parsing: unigram, bigram, and mixgram (interpolation be-
tween the two). We used the bigram model, where the gen-
eration of a hybrid-tree component (NL word or semantic
category) depends on the previously generated component as
well as the parent MR production. The bigram model always
performed the best on all tasks in our experimental evalua-
tion.
2We also tried using a Markov model to order arguments
like Liang et al (2009), but preliminary experimental results
showed that this additional component actually decreased
performance rather than improving it.
6 Learning and Inference
This composite generative model is trained using
conventional EM methods. The process is similar
to Lu et al (2008)?s, an inside-outside style al-
gorithm using dynamic programming to generate
a hybrid tree from the NL?MR pair (w,m), ex-
cept our model?s estimation process additionally
deals with calculating expected counts under the
posterior P (e|w, s; ?) in the E-step and normaliz-
ing the counts to optimize parameters. The whole
process is quite efficient; training time takes about
30 minutes to run on sportscasts of three games in
either English or Korean.
Unfortunately, we found that EM tended to get
stuck at local maxima with respect to learning the
event-type selection probabilities, p(t). There-
fore, we also tried initializing these parameters
with the corresponding strategic generation values
learned by the IGSL method of Chen and Mooney
(2008). Since IGSL was shown to be quite effec-
tive at predicting which event types were likely to
be described, the use of IGSL priors provides a
good starting point for our event selection model.
Our model is built on top of Lu et al (2008)?s
generative semantic parsing model, which is also
trained in several steps in its best-performing ver-
sion.3 Thus, the overall model is vulnerable to
getting stuck in local optima when running EM
across these multiple steps. We also tried using
random restarts with different initialization of pa-
rameters, but initializing with IGSL priors per-
formed the best in our experimental evaluation.
7 Experimental Evaluation
We evaluated our proposed model on the Robocup
sportscasting data described in Section 4. Our ex-
perimental results cover 3 tasks: NL?MR match-
ing, semantic parsing, and tactical generation.
Following Chen and Mooney (2008), the exper-
iments were conducted using 4-fold (leave one
game out) cross validation. Since the corpus con-
tains data for four separate games, each fold uses
3 games for training and the remaining game for
3The bigram model of Lu et al (2008), which is the one
used in this paper, must be trained using parameters previ-
ously learned for the IBM Model 1 and unigram model in
order to exhibit the best performance. We followed the same
training scheme in our version.
547
testing for semantic parsing and tactical genera-
tion. Matching performance is measured in train-
ing data, since the goal is to disambiguate this
data. All results are averaged across these 4 folds.
We also use the same performance metrics
as Chen and Mooney (2008). The accuracy of
matching and semantic parsing are measured us-
ing F-measure, the harmonic mean of precision
and recall, where precision is the fraction of the
system?s annotations that are correct, and recall
is the fraction of the annotations from the gold-
standard that the system correctly produces. Gen-
eration is evaluated using BLEU score (Papineni
et al, 2002) between generated sentences and ref-
erence NL sentences in the test set. We com-
pare our results to previous results from Chen and
Mooney (2008) and Chen et al (2010) and to
matching results on Robocup data from Liang et
al. (2009).
7.1 NL?MR Matching
The goal of matching is to find the most probable
NL?MR alignment for ambiguous examples con-
sisting of an NL sentence and multiple potential
MR logical forms. In Robocup sportscasting, the
MRs for a given sentence correspond to all game
events that occur within a 5-second window prior
to the NL comment. Not all NL sentences have a
matching MR in this window, but most do. Dur-
ing testing, an NL w is matched to an MR m if
and only if the learned semantic parser produces
m as the most probable parse of w. Thus, our
model does not force every NL to match an MR.
If the most probable semantic parse of a sentence
does not match any of the possible recent events,
it is simply left unmatched. Matching is evaluated
against the gold-standard matches supplied with
the data, which are used for evaluation purposes
only. The gold matching data is never used during
training.
Table 2 shows the detailed results for both
English and Korean data.4 Our best approach
outperforms all previous methods for both En-
glish and Korean by quite large margins. Note
4Since the Korean data was not yet available for use by
either Chen and Mooney (2008) or Liang et al (2009), we
present the results reported by Chen et al (2010) for these
methods.
English Korean
Chen and Mooney (2008) 0.681 0.753
Liang et al (2009) 0.757 0.694
Chen et al (2010) 0.793 0.841
Our model 0.832 0.800
Our model w/ IGSL init 0.885 0.895
Table 2: NL?MR Matching Results (F-measure).
Results are the highest reported in the cited work.
English Korean
Chen and Mooney (2008) 0.702 0.720
Chen et al (2010) 0.803 0.812
Our learned parser 0.742 0.764
Lu et al + our matching 0.810 0.794
WASP + our matching 0.786 0.808
Lu et al + Liang et al 0.790 0.690
WASP + Liang et al 0.803 0.740
Table 3: Semantic Parsing Results (F-measure).
Results are the highest reported in the cited work.
that initializing our EM training with IGSL?s es-
timates improves performance significantly, and
this approach outperforms Chen et al (2010)?s
best method, which also uses IGSL.
In particular, our proposed model outperforms
the generative alignment model of Liang et al
(2009), indicating that the extra linguistic infor-
mation and MR grammatical structure used by Lu
et al (2008)?s generative language model make
our overall model more effective than a simple
Markov + bag-of-words model for language gen-
eration.
7.2 Semantic Parsing
Semantic parsing is evaluated by determining how
accurately NL sentences in the test set are cor-
rectly mapped to their meaning representations.
Results are presented in Table 3.5 6 For our
model, we report results using the parser learned
directly from the ambiguous supervision, as well
5The best result of Chen and Mooney (2008) is for
WASPER-GEN, and that of Chen et al (2010) is for WASPER
with Liang et al?s matching initialization for English and for
WASER-GEN-IGSL-METEOR with Liang et al?s initialization
for Korean.
6Our semantic parsing results are based on our best
matching results with IGSL initialization.
548
as results for training a supervised parser (both
WASP and Lu et al (2009)?s) on the NL?MR
matching produced by our model. We also present
results for training Lu et al?s parser and WASP on
Liang et al?s NL?MR matchings.
Our initial learned semantic parser does not per-
form better than the best results reported by Chen
et al (2010), but it is clearly better than the ini-
tial results of Chen and Mooney (2008). Train-
ing WASP and Lu et al?s supervised parser on
our method?s highly accurate set of disambiguated
NL?MR pairs improved the results. Retraining Lu
et al?s parser gave the best overall results for En-
glish, and retraining WASP gave the second high-
est results for Korean, only failing to beat the very
best results of Chen et al (2010). It is somewhat
surprising that simply retraining on the hardened
set of most probable NL?MR matches gives bet-
ter results than the parser trained using EM, which
actually exploits the uncertainty in the underly-
ing matches. Further investigations of this phe-
nomenon are indicated.
Comparing with the corresponding results for
training WASP and Lu et al?s supervised parser
on the NL?MR matchings produced by Liang et
al.?s alignment method, it is clear that our match-
ings produce more accurate semantic parsers ex-
cept when training WASP on English.
7.3 Tactical Generation
Tactical generation is evaluated based on how
well the learned model generates accurate NL sen-
tences from MR logical forms. Without integrat-
ing a language model for the NL, the existing
generative model is not very effective for tactical
generation. Lu et al (2009) introduced an effec-
tive language generator for the hybrid tree frame-
work using a Tree-CRF model; however, we did
not have access to this system. Therefore, for
tactical generation, we used the publicly avail-
able WASP?1 system (Wong and Mooney, 2007a)
trained on disambiguated NL?MR matches. This
approach also allows direct comparison with the
results of Chen and Mooney (2008) and Chen et
al. (2010), who also used WASP?1 for tactical
generation. Our objective is to show that the more
accurate matchings produced by our generative
model can improve tactical generation.
English Korean
Chen and Mooney (2008) 0.4560 0.5575
Chen et al (2010) 0.4599 0.6796
WASP?1 + Liang et al 0.4580 0.5828
WASP?1 + our matching 0.4727 0.7148
Table 4: Tactical Generation Results (BLEU
score). Results are the highest reported in the cited
work.
The results are shown in Table 4.7 8 Overall,
WASP?1 trained on the NL?MR matching from
our alignment model performs better than all pre-
vious methods. In particular, using the matchings
from our method to train WASP?1 produces bet-
ter tactical generators than using matchings from
Liang et al?s approach.
7.4 Discussion
Overall, our model performs particularly well at
matching NL and MRs under ambiguous supervi-
sion, and the difference is larger for English than
Korean. However, improved matching results do
not necessarily translate into significantly better
semantic parsers. For English, the improvement
in matching is almost 10 percentage points in F-
measure, but the semantic parsing result trained
with this more accurate matching shows only 1
point improvement.
Compared to Liang et al (2009), our more ac-
curate (i.e. higher F-measure) matchings provide
a clear improvement in both semantic parsing and
tactical generation. The only exception is English
parsing using WASP, which seems to be due to
some misleading noise in our alignments. WASP
seems to be affected more than Lu et al?s system
by such extraneous noise. However, in tactical
generation, this extraneous noise does not seem to
lead to worse performance, and our approach al-
ways gives the best results. As discussed by Chen
and Mooney (2008) and Chen et al (2010), tac-
tical generation is somewhat easier than seman-
tic parsing in that semantic parsing needs to learn
7The best result of Chen and Mooney (2008) is for
WASPER-GEN, and that of Chen et al (2010) is for WASPER
with Liang et al?s matching initialization for English and for
WASER-GEN with Liang et al initialization for Korean.
8Our generation results are based on our best matching
results with IGSL initialization.
549
to map a variety of synonymous natural-language
expressions to the same meaning representation,
while tactical generation only needs to learn one
way to produce a correct natural language descrip-
tion of an event. This difference in the nature of
semantic parsing and tactical generation may be
the cause of the different trends in the results.
8 Conclusions and Future Work
We have presented a novel generative model capa-
ble of probabilistically aligning natural-language
sentences to their correct meaning representa-
tions given the ambiguous supervision provided
by a grounded language acquisition scenario. Our
model is also capable of simultaneously learning
to semantically parse NL sentences into their cor-
responding meaning representations. Experimen-
tal results in Robocup sportscasting show that the
NL?MR matchings inferred by our model are sig-
nificantly more accurate than those produced by
all previous methods. Our approach also learns
competitive semantic parsers and improved lan-
guage generators compared to previous methods.
In particular, we showed that our alignments pro-
vide a better foundation for learning accurate se-
mantic parsers and tactical generators compared
to those of Liang et al (2009), whose genera-
tive model is limited by a simple bag-of-words as-
sumption.
In the future, we plan to test our model on
more complicated data with higher degrees of am-
biguity as well as more complex meaning repre-
sentations. One immediate direction is evaluat-
ing our approach on the datasets of weather fore-
casts and NFL football articles used by Liang et al
(2009). However, our current model does not sup-
port matching multiple meaning representations
to the same natural-language sentence, and needs
to be extended to allow multiple MRs to generate
a single NL sentence.
Acknowledgements
We thank Wei Lu and Wee Sun Lee for sharing
their software and giving helpful comments for
the paper. We also thank Percy Liang for sharing
his code and experimental results with us. Ad-
ditionally, we thank David Chen in UTCS ML
group for his comments and advice. Finally, we
thank the anonymous reviewers for their com-
ments. This work was funded by the NSF grant
IIS. 0712907X. The experiments were executed
and run on the Mastodon Cluster, provided by
NSF Grant EIA-0303609.
References
Chen, David L. and Raymond J. Mooney. 2008.
Learning to sportscast: a test of grounded language
acquisition. In ICML ?08: Proceedings of the
25th International Conference on Machine Learn-
ing, pages 128?135, New York, NY, USA. ACM.
Chen, David L., Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual
sportscaster: Using perceptual context to learn lan-
guage. Journal of Artificial Intelligence Research,
37:397?435.
Ge, Ruifang and Raymond J. Mooney. 2005. A sta-
tistical semantic parser that integrates syntax and
semantics. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 9?16, Ann Arbor, MI, July.
Kate, Rohit J. and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics (COLING/ACL-06), pages 913?920, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kate, Rohit J. and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the Twenty-Second Con-
ference on Artificial Intelligence (AAAI-07), pages
895?900, Vancouver, Canada, July.
Liang, Percy, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In ACL-IJCNLP ?09: Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1, pages 91?99, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 783?792, Morristown, NJ, USA. Association
for Computational Linguistics.
550
Lu, Wei, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In EMNLP ?09: Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 400?409, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, PA, July.
Wong, Yuk Wah and Raymond J. Mooney. 2007a.
Generation by inverting a semantic parser that uses
statistical machine translation. In Proceedings of
Human Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
172?179, Rochester, NY.
Wong, Yuk Wah and Raymond J. Mooney. 2007b.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL-07), pages 960?967,
Prague, Czech Republic, June.
Zelle, John M. and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-
96), pages 1050?1055, Portland, OR, August.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of 21st Conference on
Uncertainty in Artificial Intelligence (UAI-2005),
Edinburgh, Scotland, July.
Zettlemoyer, Luke S. and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL-07), pages 678?
687, Prague, Czech Republic, June.
551
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 433?444, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised PCFG Induction for Grounded Language Learning
with Highly Ambiguous Supervision
Joohyun Kim Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX 78701, USA
{scimitar,mooney}@cs.utexas.edu
Abstract
?Grounded? language learning employs train-
ing data in the form of sentences paired with
relevant but ambiguous perceptual contexts.
Bo?rschinger et al2011) introduced an ap-
proach to grounded language learning based
on unsupervised PCFG induction. Their ap-
proach works well when each sentence po-
tentially refers to one of a small set of pos-
sible meanings, such as in the sportscasting
task. However, it does not scale to prob-
lems with a large set of potential meanings
for each sentence, such as the navigation in-
struction following task studied by Chen and
Mooney (2011). This paper presents an en-
hancement of the PCFG approach that scales
to such problems with highly-ambiguous su-
pervision. Experimental results on the naviga-
tion task demonstrates the effectiveness of our
approach.
1 Introduction
The ultimate goal of ?grounded? language learning
is to develop computational systems that can acquire
language more like a human child. Given only su-
pervision in the form of sentences paired with rel-
evant but ambiguous perceptual contexts, a system
should learn to interpret and/or generate language
describing situations and events in the world. For
example, systems have learned to commentate sim-
ulated robot soccer games by learning from sample
sportscasts (Chen and Mooney, 2008; Liang et al
2009; Bo?rschinger et al2011), or understand nav-
igation instructions by learning from action traces
produced when following the directions (Chen and
Mooney, 2011; Tellex et al2011).
Bo?rschinger et al2011) recently introduced an
approach to grounded language learning using un-
supervised induction of probabilistic context free
grammars (PCFGs) to learn from ambiguous con-
textual supervision. Their approach first constructs
a large set of production rules from sentences paired
with descriptions of their ambiguous context, and
then trains the parameters of this grammar using
EM. Parsing a novel sentence with this grammar
gives a parse tree which contains the formal mean-
ing representation (MR) for this sentence. This ap-
proach works quite well on the sportscasting task
originally introduced by Chen and Mooney (2008).
In this task, each sentence in a natural-language
commentary describing activity in a simulated robot
soccer game is paired with the small set of actions
observed within the past 5 seconds, one of which
is usually described by the sentence. Even with this
low level of ambiguity in a constrained domain, their
method constructs a PCFG with about 33,000 pro-
ductions. More fundamentally, their approach is re-
stricted to a finite set of potential meaning represen-
tations, and the grammar size grows at least linearly
with the number of possible MRs, which in turn is
inevitably exponential in the number of objects and
actions in the domain.
The navigation task studied by Chen and Mooney
(2011) provides much more ambiguous supervision.
In this task, each instructional sentence is paired
with a formal landmarks plan (represented as a
large graph) that includes a full description of the
observed actions and world-states that result when
433
someone follows this instruction. An instruction
generally refers to a subgraph of this large graph.
Therefore, there are a combinatorial number of pos-
sible meanings to which a given sentence can refer.
Chen and Mooney (2011) circumvent this combi-
natorial problem by never explicitly enumerating the
exponential number of potential meanings for each
sentence. Their system first induces a semantic lex-
icon that maps words and short phrases to formal
representations of actions and objects in the world.
This lexicon is learned by finding words and phrases
whose occurrence highly correlates with specific ob-
served actions and objects in the simulated environ-
ment when executing the corresponding instruction.
This learned lexicon is then used to directly infer
a formal MR for observed instructional sentences
using a greedy covering algorithm. These inferred
MRs are then used to train a supervised semantic
parser capable of mapping novel sentences to their
formal meanings.
We present a novel enhancement of Bo?rschinger
et al PCFG approach that uses Chen and Mooney?s
lexicon learner to avoid a combinatorial explosion in
the number of productions. The learned lexicon is
first used to build a hierarchy of semantic lexemes
(i.e. lexicon entries) called the Lexeme Hierarchy
Graph (LHG) for each ambiguous landmarks plan
in the training data. The intuition behind utilizing
an LHG is that the MR for each lexeme constitutes a
semantic concept that corresponds to some natural-
language (NL) word or phrase. Therefore, the LHG
represents how complex semantic concepts are com-
posed of simpler semantic concepts and ultimately
connected to NL words and phrases. Bo?rschinger
et al approach instead produces NL groundings at
the level of atomic MR constituents, which causes
an explosion in the number of PCFG productions
for complex MR languages. We estimated that
Bo?rschinger et al approach would require more
than 20! (> 1018) productions for our navigation
problem.1 On the other hand, our method, which
uses correspondences from the LHG at the seman-
tic concept level, constructs a more focused PCFG
of tractable size. It then extracts the MR for a novel
1The corpus contains quite a few examples with landmarks
plans containing more than 20 actions. This results in at least
20! permutations representing possible alignments between ac-
tions and NL words.
sentence from the most-probable parse tree for the
resulting PCFG. Our approach can produce a large,
combinatorial number of different MRs for a wide
range of novel sentences by composing relevant MR
components from the resulting parse tree, whereas
Bo?rschinger et al approach is only able to output
MRs that are explicitly included as a nonterminals
in the original learned PCFG.
The remainder of the paper is organized as fol-
lows. Section 2 reviews Bo?rschinger et al PCFG
approach as well as the navigation task and data.
Section 3 describes our enhanced PCFG approach
and Section 4 presents an experimental evaluation
of it. Then, Section 5 discusses the unique aspects
of our approach and Section 6 describes additional
related work. Finally, Section 7 presents future re-
search directions and Section 8 gives our conclu-
sions.
2 Background
2.1 Existing PCFG Approach
Our approach extends that of Bo?rschinger et al
(2011), which in turn was inspired by a series of
previous techniques (Lu et al2008; Liang et al
2009; Kim and Mooney, 2010) following the idea
of constructing correspondences between NL and
MR in a single probabilistic generative framework.
Particularly, their approach automatically constructs
a PCFG that generates NL sentences from MRs,
which indicates how atomic MR constituents are
probabilistically related to NL words. The nonter-
minals in the grammar correspond to complete MRs,
MR constituents, and NL phrases. The nontermi-
nal for a composite MR generates each of its MR
constituents, and each atomic MR, x, generates an
NL phrase, Phrasex. Each Phrasex then gener-
ates a sequence of Wordx?s for describing x, and
each Wordx can generate each possible word in the
natural language. This allows the system to learn
the words and phrases used to describe each atomic
MR by properly weighting these rules. Figure 1
shows one possible derivation tree for a sample NL-
MR pair and the PCFG rules that are constructed for
it. Once a set of productions are assembled, their
probabilities are learned using the Inside-Outside al-
gorithm. Computing the most probable parse for a
novel sentence with the trained PCFG provides its
434
Figure 1: Derivation tree for the NL/MR pair: THE
PINK GOALIE PASSES THE BALL TO PINK11 /
pass(pink1, pink11). Left side shows PCFG rules
that are added for each stage (full MR to atomic
MRs, and atomic MRs to NL words ).
preferred MR interpretation in the topmost nonter-
minal.
Unfortunately, as discussed earlier, this approach
only works for finite MR languages, and the gram-
mar becomes intractably large even for finite but
complex MRs. It effectively assumes that MRs are
fairly small and includes every possible MR con-
stituent as a nonterminal in the PCFG. This is not
tractable for more complex MRs. Therefore, our ex-
tension incorporates a learned lexicon to constrain
the space of productions, thereby making the size
of the PCFG tractable for complex MRs, and even
giving it the ability to handle infinite MR languages.
Moreover, when processing novel sentences, our ap-
proach can produce a large space of novel MRs that
were not anticipated during training, which is not the
case for Bo?rschinger et al approach.
2.2 Navigation Task and Dataset
We employ the task and data introduced by Chen and
Mooney (2011) whose goal is to interpret and follow
NL navigation instructions in a virtual world. Fig-
ure 2 shows a sample execution path in a particular
virtual world. The challenge is learning to perform
this task by simply observing humans following in-
structions. Formally, given training data of the form
{(e1, a1, w1), . . . , (en, an, wn)}, where ei is an NL
instruction, ai is an observed action sequence, and
wi is the current world state (patterns of floors and
walls, positions of any objects, etc.), we want to pro-
duce the correct actions aj for a novel (ej , wj).
Figure 2: Sample virtual world from Chen and
Mooney (2011) of interconnecting hallways with
different floor and wall patterns and objects indi-
cated by letters (e.g. ?H? for hatrack).
Figure 3: Sample instruction with its constructed
landmarks plan, components in bold compose the
correct plan.
In order to learn, their system infers the intended
formal plan pi (the MR for a sentence) which pro-
duced the action sequence ai from the instruction ei.
However, there is a large space of possible plans for
any given action sequence. Chen and Mooney first
construct a formal landmarks plan, ci, for each ai,
which is a graph representing the context of every
action and the world-state encountered during the
execution of the sequence. The correct plan MR,
pi, is assumed to be a subgraph of ci, and this causes
a combinatorial matching problem between ei and
ci in order to learn the correct meaning of ei among
all the possible subgraphs of ci. The landmarks and
correct plans for a sample instruction are shown in
Figure 3, illustrating the complexity of the MRs.
Instead of directly solving the combinatorial cor-
respondence problem, they first learn a semantic lex-
435
Figure 4: An overview of Chen and Mooney
(2011)?s system. Our method replaces the plan re-
finement and semantic parser parts.
icon that maps words and short phrases to small sub-
graphs representing their inferred meanings from the
(ei, ci) pairs. The lexicon is learned by evaluating
pairs of n-grams, wj , and MR graphs, mj , and scor-
ing them based on how much more likely mj is a
subgraph of the context ci when w occurs in the
corresponding instruction ei. This process is simi-
lar to other ?cross-situational? approaches to learn-
ing word meanings (Siskind, 1996; Thompson and
Mooney, 2003). Then, a plan refinement step esti-
mates pi from ci by greedily selecting high-scoring
lexemes of the form (wj ,mj) whose words and
phrases (wj) cover the instruction ei and introduce
components (mj) from the landmarks plan ci. The
refined plans are used to construct supervised train-
ing data (ei, pi) for a supervised semantic-parser
learner. The trained semantic parser can parse a
novel instruction into a formal plan, which is finally
executed for end-to-end evaluation. Figure 4 illus-
trates the overall system.
As this figure indicates, our new PCFG method
replaces the plan refinement and semantic parser
components in their system with a unified model
that both disambiguates the training data and learns
a semantic parser. We use the landmarks plans and
the learned lexicon produced by Chen and Mooney
(2011) as inputs to our system.2
2In our experiments, we used the top 1,000 lexemes learned
by Chen and Mooney (2011).
3 Our PCFG Approach
Like Bo?rschinger et al2011), our approach learns
a semantic parser directly from ambiguous su-
pervision, specifically NL instructions paired with
their complete landmarks plans as context. Our
method incorporates the semantic lexemes as build-
ing blocks to find correspondences between NL
words and semantic concepts represented by the lex-
eme MRs, instead of building connections between
NL words and every possible MR constituent as in
Bo?rschinger et al approach. Particularly, we uti-
lize the hierarchical subgraph relationships between
the MRs in the learned semantic lexicon to produce
a smaller, more focused set of PCFG rules.3 The
intuition behind our approach is analogous to the hi-
erarchical relations between nonterminals in syntac-
tic parsing, where higher-level categories such as S,
VP, or NP are further divided into smaller categories
such as V, N, or Det, thereby forming a hierarchi-
cal structure. Inspired by this idea, we introduce a
directed acyclic graph called the Lexeme Hierarchy
Graph (LHG) which represents the hierarchical rela-
tionships between lexeme MRs. Since complex lex-
eme MRs represent complicated semantic concepts
while simple MRs represent simple concepts, it is
natural to construct a hierarchy amongst them. The
LHGs for all of the training examples are used to
construct production rules for the PCFG, which are
then parametrized using EM. Finally, a novel sen-
tence is semantically parsed by computing its most-
probable parse using the trained PCFG, and then its
MR is extracted from the resulting parse tree.
3.1 Constructing a Lexeme Hierarchy Graph
An LHG represents the hierarchy of lexical mean-
ings relevant to a particular training instance by en-
coding the subgraph relations between the MRs of
relevant lexemes. Algorithm 1 describes how an
LHG is constructed for an ambiguous training pair
of a sentence and its corresponding context, (ei, ci).
First, we obtain all relevant lexemes (wij ,m
i
j) in the
lexicon L, where the MR mij is a subgraph of the
context ci (denoted as mij ? ci). These lexemes are
3The total number of PCFG rules constructed for our navi-
gation training sets is about 18,000, while Bo?rschinger et al
method produces 33,000 rules for the much simpler sportscast-
ing domain.
436
Algorithm 1 LEXEME HIERARCHY GRAPH (LHG)
Input: Training instance (ei, ci), Lexicon L
Output: Lexeme hierarchy graph for (ei, ci)
Find relevant lexemes (wi1,m
i
1), . . . , (w
i
n,m
i
n)
s.t. mij ? ci
Create a starting node T ; MR(T )? ci
for all mij in the descending order of size do
Create a node T ij ; MR(T
i
j )? m
i
j
PLACELEXEME(T ij ,T )
end for
procedure PLACELEXEME(T ?,T )
for all children Tj of T do
if MR(T ?) ? MR(Tj) then
PLACELEXEME(T ?,Tj)
end if
end for
if T ? was not placed under any child Tj then
Add T ? as child of T
end if
end procedure
sorted in descending order based on the number of
nodes in their MRs mij . Then, after setting the con-
text ci as the MR of the root node (MR(T ) ? ci),
lexemes are inserted, in order, into the graph to cre-
ate a hierarchy of MRs, where each child?s MR is a
subgraph of the MR of each of its parents. Figure 5
illustrates a sample construction of an LHG for the
following landmarks plan (ci):
Turn(RIGHT),
Verify(side:HATRACK, front:SOFA),
Travel(steps:3),
Verify(at:EASEL)
The initial LHG may contain nodes with too many
children. This is a problem, because when we sub-
sequently extract PCFG rules, we need to add a pro-
duction for every k-permutation of the children of
each node (see Section 3.2). To reduce the branch-
ing factor in the LHG, we introduce pseudo-lexeme
nodes by repeatedly combining the two most similar
children of each node. Pseudocode for the process is
shown in Algorithm 2. The MR for a pseudo-lexeme
is the minimal graph, m?, that is a supergraph of both
of the lexeme MRs that it combines. The pair of
(a) All relevant lexemes are obtained for the training exam-
ple and ordered by the number of nodes in their MR.
(b) Lexeme MR [1] is added as a child of the top node. MR
[2] is a subgraph of [1], so it is added as its child.
(c) MR [3] is not a subgraph of [1] or [2], so it is added as a
child of the root. MR [4] is added under [3], and MR [5] is
recursively filtered down and added under [2].
Figure 5: Sample LHG construction.
437
Algorithm 2 ADDING PSEUDO LEXEMES TO LHG
Input: LHG with root T
Output: LHG with pseudo lexemes added
procedure RECONSTRUCTLHG(T )
repeat
((Ti, Tj),m?) ? pick the most similar
pair (Ti, Tj) of children of T and the minimal ex-
tension m? s.t. MR(Ti) ? m?, MR(Tj) ? m?,
m? ? MR(T )
Add child T ? of T ; MR(T ?)? m?
Move Ti and Tj to be children of T ?
until There are no more pairs to combine
for all non-leaf children Tk of T do
RECONSTRUCTLHG(Tk)
end for
end procedure
most similar children, (mi,mj), is determined by
measuring the fraction of the nodes in mi and mj
that overlap with their minimum extension m? and
is calculated as follows:
Sim(mi,mj ,m
?) =
|mi|+ |mj |
2 |m?|
where |m| is the number of nodes in the MR m.
Adding pseudo-lexemes also has another advan-
tage. They can be considered to be higher-level
semantic concepts composed of two or more sub-
concepts. These higher-level concepts will likely
occur in other training examples as well, which al-
lows for more flexible interpretations. For example,
assuming the rule A ? BCD is constructed from
an LHG, we will introduce a pseudo lexeme E and
build two rules A? BE and E ? CD. It is likely
that E also occurs in another rule constructed from
other training examples such as E ? FG. This
increases the model?s expressive power by support-
ing additional derivations such as A?? BFG, pro-
viding more flexibility when parsing novel NL sen-
tences.
3.2 Composing PCFG Rules
The next step composes PCFG rules from the LHGs
and is summarized in Figure 6. We basically fol-
low the scheme of Bo?rschinger et al2011), but
instead of generating NL words from each atomic
MR, words are generated from each lexeme MR,
Figure 6: Summary of the rule generation process.
NLs refer to the set of NL words in the corpus. Lex-
eme rules come from the schemata of Bo?rschinger
et al2011), and allow every lexeme MR to gener-
ate one or more NL words. Note that pseudo-lexeme
nodes do not produce NL words.
and smaller lexeme MRs are generated from more
complex ones as given by the LHGs. A nonterminal
Sm is generated for the MR, m, of each LHG node.
Then, for every LHG node, T , with MR, m, we add
rules of the form Sm ? Smi ...Smj , where the RHS
is some k-permutation of the nonterminals for the
MRs of the children of node T . Bo?rschinger et al
assume that every atomic MR generates at least one
NL word. However, since we do not know which
subgraph of the overall context (i.e. ci, the MR of the
root node) conveys the intended plan and is therefore
expressed in the NL instruction, we must allow each
ordered subset of the children of a node (i.e. each
k-permutation) to be a possible generation.
The rest of the process more closely follows
Bo?rschinger et al. Every MR, m, of a lexeme
node4 generates a rule Sm ? Phrasem, and ev-
ery Phrasem generates a sequence of NL words, in-
cluding one or more ?content words? (Wordm) for
expressing m and zero or more ?extraneous? words
(Word?). While Bo?rschinger et alave Wordm
generate all possible NL words (each of which are
4We exclude pseudo-lexeme nodes in this process, because
they should only generate words through generating lexemes.
438
subsequently weighted by EM training), in our ap-
proach, each Wordm only produces the NL phrase
associated with m in the lexicon, or individual words
that appear in this phrase. The words not covered
by Wordm also can be generated by Word? which
has rules for every word. Phm and PhXm ensure
that Phrasem produces at least one Wordm, where
PhXm indicates that one or more Wordm?s have
already been generated, and Phm indicates that no
Wordm has yet been generated.
3.3 Parsing Novel NL Sentences
To learn the parameters of the resulting PCFG, we
use the Inside-Outside algorithm.5 Then, the stan-
dard probabilistic CKY algorithm is used to produce
the most probable parse for novel NL sentences (Ju-
rafsky and Martin, 2000).
Bo?rschinger et al2011) simply read the MR, m,
for a sentence off the top Sm nonterminal of the
most probable parse tree. However, in our approach,
the correct MR is constructed by properly compos-
ing the appropriate subset of lexeme MRs from the
most-probable parse tree. This allows the system to
produce a wide variety of novel MRs for novel sen-
tences, as long as the correct MR is a subgraph of the
complete context (ci) for at least one of the training
sentences.
First, the parse tree is pruned to remove all sub-
trees starting with Phrasex nodes. This leaves a
tree consisting of the Root and a set of Sm nodes.
The pruned subtrees only concern generating NL
words and phrases from the selected MRs. The re-
maining tree shows which MR constituents were se-
lected from the available context, from which the
sentence is then generated. Each leaf in the pruned
tree represents an MR constituent that was used to
generate a phrase in the sentence. These are the con-
stituents we want to assemble and compose into a
final MR for the sentence.
Algorithm 3 describes the procedure for extract-
ing the final MR from the pruned parse tree. Fig-
ure 7 graphically depicts a sample trace of this algo-
rithm. The algorithm recursively traverses the parse
tree. When a leaf-node is reached, it marks all of the
nodes in its MR. After traversing all of its children,
5We used the implementation available at http://web.
science.mq.edu.au/?mjohnson/Software.htm
which was also used by Bo?rschinger et al2011).
Algorithm 3 CONSTRUCT PARSED MR RESULT
Input: Parse tree T for input NL, e, with all
Phrasex subtrees removed.
Output: Semantic parse MR, m, for e
procedure OBTAINPARSEDOUTPUT(T )
if T is a leaf then
return MR(T ) with all its nodes marked
end if
for all children Ti of T do
mi ? OBTAINPARSEDOUTPUT(Ti)
Mark the nodes in MR(T ) corresponding
to the marked nodes in mi
end for
if T is not the root then
return MR(T )
end if
return MR(T ) with unmarked nodes removed
end procedure
a node in the MR for the current parse-tree node is
marked iff its corresponding node in any of the chil-
dren?s MRs were marked. The final output is the MR
constructed by removing all of the unmarked nodes
from the MR for the root node.
4 Experimental Evaluation
For evaluation, we used the same data and method-
ology as Chen and Mooney (2011). Please see their
paper for more details.
4.1 Data
We used the English instructions and follower data
collected by MacMahon et al2006).6 This data
contains 706 route instructions for three virtual
worlds. The instructions were produced by six in-
structors for 126 unique starting and ending loca-
tion pairs spread evenly across the three worlds, and
there were 1 to 15 human followers for each instruc-
tion who executed an average of 10.4 actions per in-
struction. Each instruction is a paragraph consist-
ing of an average of 5.0 sentences, each contain-
ing an average of 7.8 words. Chen and Mooney
constructed the additional single-sentence corpus by
matching each sentence with the majority of human
6Available at http://www.cs.utexas.edu/users/
ml/clamp/navigation/
439
(a) Pruned parse tree showing only MRs for Sm
nodes
(b) Leaf nodes have all their elements marked
(c) Upper level nodes are marked according to leaf-
node markings
(d) Removing all unmarked elements for the root
node leads to the final MR output
Figure 7: Sample construction of MR output from pruned parse tree.
followers? actions. We use this single-sentence ver-
sion for training, but use both the single-sentence
and the original paragraph version for testing. Each
sentence was manually annotated with a ?gold stan-
dard? execution plan, which is used for evaluation
but not for training.
4.2 Methodology and Results
Experiments were conducted using ?leave one envi-
ronment out? cross-validation, training on two envi-
ronments and testing on the third, averaging over all
three test environments. We perform direct compar-
ison to the best results of Chen and Mooney (2011)
(referred to as CM). A Wilcoxon signed-rank test
is performed for statistical significance, and ??? de-
notes significant differences (p < .01) in the tables.
Semantic Parsing Results
We first evaluated how well our system learns to
map novel NL sentences for new test environments
into their correct MRs. Partial semantic-parsing ac-
curacy (Chen and Mooney, 2011) is calculated by
Precision Recall F1
Our system 87.58 ?65.41 ?74.81
CM ?90.22 55.10 68.37
Table 1: Test accuracy for semantic parsing.
??? denotes difference is statistically significant.
comparing the system?s MR output to the hand-
annotated gold standard. Accuracy is measured in
terms of precision, recall, and F1 for individual MR
constituents (thereby awarding partial credit for ap-
proximately correct MRs).
Table 1 demonstrates that our method outper-
forms CM by 6 points in F1. Our PCFG-based ap-
proach is able to probabilistically disambiguate the
training data as well as simultaneously learn a sta-
tistical semantic parser within a single framework.
This results in better overall performance compared
to CM, since they lose potentially useful informa-
tion, particularly during the refinement stage, due to
the separate disjoint components of the system.
440
Single-sentence Paragraph
Our system ?57.22% ?20.17%
CM 54.40% 16.18%
Table 2: Successful plan execution rates for novel
test data. ??? means statistical significance.
Navigation Plan Execution Results
Next, we test the end-to-end system by execut-
ing the parsed navigation plans for test instructions
in novel environments to see if they reach the ex-
act desired destinations in the environment. Table
2 shows the successful end-to-end navigation-task
completion rate for both single-sentences and com-
plete paragraph instructions.
Again, our system outperforms CM?s best results
since more accurate semantic parsing produces more
successful plans. However, the difference in per-
formance is smaller than that observed for semantic
parsing. This is because the redundancy in the hu-
man generated instructions allows an incorrect se-
mantic parse to be successful, as long as the errors
do not affect its ability to guide the system to the
correct destination.
5 Discussion
Our approach improves on Bo?rschinger et al
(2011)?s method in the following ways:
? The building blocks for associating NL and MR
are semantic lexemes instead of atomic MR con-
stituents. This prevents the number of constructed
PCFG rules from becoming intractably large as hap-
pens with Bo?rschinger et al approach. As previ-
ously mentioned, lexeme MRs are intuitively anal-
ogous to syntactic categories in that complex lex-
eme MRs represent complicated semantic concepts
whereas higher-level syntactic categories such as S,
VP, or NP represent complex syntactic structures.
? Our approach has the ability to produce previ-
ously unseen MRs, whereas Bo?rschinger et alan
only generate an MR if it is explicitly included in
the PCFG rules constructed from the training data.
Even though our MR parse is restricted to be a sub-
graph of some training context, ci, our model allows
for exponentially many combinations.
In addition, our approach can produce a wider
range of MR outputs than Chen and Mooney
(2011)?s even though we use their semantic lexi-
con as input. Their system deterministically builds a
supervised training set by greedily selecting high-
scoring lexemes, thus implicitly including only
high-scoring lexemes during training. On the other
hand, our probabilistic approach also considers rela-
tively low-scoring but useful lexemes, thereby utiliz-
ing more semantic concepts in the lexicon. In partic-
ular, this explains why our approach obtains higher
recall in the evaluation of semantic parsing.
Even though we have demonstrated our approach
on the specific task of following navigation in-
structions, it is straightforward to apply it to other
language-grounding tasks where NL sentences po-
tentially refer to some subset of states, events, or ac-
tions in the world, as long as this overall context can
be represented as a semantic graph or logical form.
Since the semantic lexicon is an input to our system,
other approaches to lexicon learning are also easily
incorporated.
6 Related Work
Most work on learning semantic parsers that map
natural-language sentences to formal representa-
tions of their meaning have relied upon totally su-
pervised training data consisting of NL/MR pairs
(Zelle and Mooney, 1996; Zettlemoyer and Collins,
2005; Kate and Mooney, 2006; Wong and Mooney,
2007; Zettlemoyer and Collins, 2007; Lu et al
2008; Zettlemoyer and Collins, 2009). Several re-
cent approaches have investigated grounded learn-
ing from ambiguous supervision extracted from per-
ceptual context. A number of approaches (Kate and
Mooney, 2007; Chen and Mooney, 2008; Chen et al
2010; Kim and Mooney, 2010; Bo?rschinger et al
2011) assume training data consisting of a set of sen-
tences each associated with a small set of MRs, one
of which is usually the correct meaning of the sen-
tence. Many of these approaches (Kate and Mooney,
2007; Chen and Mooney, 2008; Chen et al2010)
disambiguate the data and match NL sentences to
their correct MR by iteratively retraining a super-
vised semantic parser. Kim and Mooney (2010)
proposed a generative semantic parsing model that
first chooses which MRs to describe and then gen-
erates a hybrid tree structure (Lu et al2008) con-
taining both the MR and NL sentence. They train
441
this model on ambiguous data using EM. As pre-
viously discussed, Bo?rschinger et al2011) use a
PCFG generative model and also train it on ambigu-
ous data using EM. Liang et al2009) assume each
sentence maps to one or more semantic records (i.e.
MRs) and trains a hierarchical semi-Markov genera-
tive model using EM, and then finds a Viterbi align-
ment between NL words and records and their con-
stituents. Several recent projects (Branavan et al
2009; Vogel and Jurafsky, 2010) use NL instructions
to guide reinforcement learning from independent
exploration with delayed rewards. These systems do
not even need the ambiguous supervision obtained
from observing humans follow instructions; how-
ever, they do not learn semantic parsers that map
sentences to complex, structural representations of
their meaning.
Interpreting and executing NL navigation instruc-
tions is our primary task, and several other recent
projects have studied related problems. Shimizu and
Haas (2009) present a system that parses natural lan-
guage instructions into actions. However, they limit
the number of possible actions to only 15 and treat
the problem as a sequence labeling problem that is
solved using a CRF with supervised training. Ma-
tuszek et al2010) developed a system that learns to
map NL instructions to executable commands for a
robot navigating in an environment constructed by a
laser range finder. However, their approach has limi-
tations of ignoring any objects or other landmarks in
the environment to which the instructions can refer.
There are several recent projects (Vogel and Juraf-
sky, 2010; Kollar et al2010; Tellex et al2011)
which learn to follow instructions in more linguisti-
cally complex environments. However, they assume
predefined spatial words, direct matching between
NL words and the names of objects and other land-
marks in the MR, and/or an existing syntactic parser.
By contrast, our work does not assume any prior lin-
guistic knowledge, syntactic, lexical, or semantic,
and must learn the mapping between NL words and
phrases and the MR terms describing landmarks.
7 Future Work
In the future, we would like to develop a better lex-
icon learner since our PCFG approach critically re-
lies on the quality of the learned lexicon. Particu-
larly, we would like to investigate how syntactic in-
formation (such as part-of-speech tags induced us-
ing unsupervised learning) could be used to improve
semantic-lexicon learning. For example, some of the
current lexicon entries violate the general constraint
that nouns usually refer to objects and verbs to ac-
tions. Ideally, the lexicon learner would be able to
induce and then utilize this sort of relationship be-
tween syntax and semantics.
In addition, we want to investigate the use of dis-
criminative reranking (Collins, 2000), which has
proven effective in various other NLP tasks. We
would expect the final MR output to improve if a
discriminative model, which uses additional global
features, is used to rerank the top-k parses produced
by our generative PCFG model.
8 Conclusions
We have presented a novel method for learning a
semantic parser given only highly ambiguous su-
pervision. Our model enhances Bo?rschinger et
al. (2011)?s approach to reducing the problem of
grounded learning of semantic parsers to PCFG in-
duction. We use a learned semantic lexicon to aid
the construction of a smaller and more focused set
of PCFG productions. This allows the approach
to scale to complex MR languages that define a
large (potentially infinite) space of representations
for capturing the meaning of sentences. By contrast,
the previous PCFG approach requires a finite MR
language and its grammar grows intractably large
for even moderately complex MR languages. In ad-
dition, our algorithm for composing MRs from the
final parse tree provides the flexibility to produce a
wide range of novel MRs that were not seen during
training. Evaluations on a previous corpus of nav-
igational instructions for virtual environments has
demonstrated the effectiveness of our method com-
pared to a recent competing system.
Acknowledgments
We thank the anonymous reviewers and David Chen
for useful comments that helped improve this paper.
This work was funded by NSF grants IIS-0712907
and IIS-1016312. Experiments were performed on
the Mastodon Cluster, provided by NSF grant EIA-
0303609.
442
References
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-11), pages 1416?1425, Stroudsburg, PA,
USA. Association for Computational Linguistics.
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing
(ACL-IJCNLP), Singapore.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language ac-
quisition. In Proceedings of 25th International Con-
ference on Machine Learning (ICML-2008), Helsinki,
Finland, July.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-11),
San Francisco, CA, USA, August.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML-2000), pages 175?182, Stanford, CA, June.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall, Upper Saddle
River, NJ.
R. J. Kate and R. J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proceedings
of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL-
06), pages 913?920, Sydney, Australia, July.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervision.
In Proceedings of the Twenty-Second Conference on
Artificial Intelligence (AAAI-07), pages 895?900, Van-
couver, Canada, July.
Joohyun Kim and Raymond. J. Mooney. 2010. Genera-
tive alignment and semantic parsing for learning from
ambiguous supervision. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING-10), pages 543?551. Association for Com-
putational Linguistics.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Toward understanding natural language
directions. In Proceedings of Human Robot Interac-
tion Conference (HRI-2010).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In Joint
Conference of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), Singapore.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP-08), pages 783?792,
Morristown, NJ, USA. Association for Computational
Linguistics.
M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006.
Walk the talk: Connecting language, knowledge, and
action in route instructions. In Proceedings of the
Twenty-First National Conference on Artificial Intel-
ligence (AAAI-06), Boston, MA, July.
Cynthia Matuszek, Dieter Fox, and Karl Koscher. 2010.
Following directions using statistical machine transla-
tion. In Proceedings of the 5th ACM/IEEE interna-
tional conference on Human-robot interaction (HRI-
10), pages 251?258, New York, NY, USA. ACM.
Nobuyuki Shimizu and Andrew Haas. 2009. Learning to
follow navigational route instructions. In Proceedings
of the Twenty First International Joint Conference on
Artificial Intelligence (IJCAI-2009).
Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1):39?91, October.
Stefanie Tellex, Thomas Kolla, Steven Dickerson,
Matthew R. Walter, Ashis G. Banerjee, Seth Teller, and
Nicholas Roy. 2011. Understanding natural language
commands for robotic navigation and mobile manipu-
lation. In Proceedings of the National Conference on
Artificial Intelligence (AAAI-11), August.
Cynthia A. Thompson and Raymond J. Mooney. 2003.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence Re-
search, 18:1?44.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL-10).
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
443
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 960?967, Prague, Czech Repub-
lic, June.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In Proceedings of the Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1050?1055, Portland, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of 21st Conference on Uncertainty in Ar-
tificial Intelligence (UAI-2005), Edinburgh, Scotland,
July.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL-07), pages 678?687, Prague, Czech
Republic, June.
Luke .S. Zettlemoyer and Micheal Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP (ACL-IJCNLP-09), pages
976?984. Association for Computational Linguistics.
444
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 218?227,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adapting Discriminative Reranking to Grounded Language Learning
Joohyun Kim
Department of Computer Science
The University of Texas at Austin
Austin, TX 78701, USA
scimitar@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
Austin, TX 78701, USA
mooney@cs.utexas.edu
Abstract
We adapt discriminative reranking to im-
prove the performance of grounded lan-
guage acquisition, specifically the task of
learning to follow navigation instructions
from observation. Unlike conventional
reranking used in syntactic and semantic
parsing, gold-standard reference trees are
not naturally available in a grounded set-
ting. Instead, we show how the weak su-
pervision of response feedback (e.g. suc-
cessful task completion) can be used as
an alternative, experimentally demonstrat-
ing that its performance is comparable to
training on gold-standard parse trees.
1 Introduction
Grounded language acquisition involves learn-
ing to comprehend and/or generate language by
simply observing its use in a naturally occur-
ring context in which the meaning of a sentence
is grounded in perception and/or action (Roy,
2002; Yu and Ballard, 2004; Gold and Scassel-
lati, 2007; Chen et al, 2010). Bo?rschinger et
al. (2011) introduced an approach that reduces
grounded language learning to unsupervised prob-
abilistic context-free grammar (PCFG) induction
and demonstrated its effectiveness on the task of
sportscasting simulated robot soccer games. Sub-
sequently, Kim and Mooney (2012) extended their
approach to make it tractable for the more complex
problem of learning to follow natural-language
navigation instructions from observations of hu-
mans following such instructions in a virtual envi-
ronment (Chen and Mooney, 2011). The observed
sequence of actions provides very weak, ambigu-
ous supervision for learning instructional language
since there are many possible ways to describe the
same execution path. Although their approach im-
proved accuracy on the navigation task compared
to the original work of Chen and Mooney (2011),
it was still far from human performance.
Since their system employs a generative model,
discriminative reranking (Collins, 2000) could po-
tentially improve its performance. By training a
discriminative classifier that uses global features
of complete parses to identify correct interpreta-
tions, a reranker can significantly improve the ac-
curacy of a generative model. Reranking has been
successfully employed to improve syntactic pars-
ing (Collins, 2002b), semantic parsing (Lu et al,
2008; Ge and Mooney, 2006), semantic role la-
beling (Toutanova et al, 2005), and named entity
recognition (Collins, 2002c). Standard reranking
requires gold-standard interpretations (e.g. parse
trees) to train the discriminative classifier. How-
ever, grounded language learning does not provide
gold-standard interpretations for the training ex-
amples. Only the ambiguous perceptual context
of the utterance is provided as supervision. For
the navigation task, this supervision consists of
the observed sequence of actions taken by a hu-
man when following an instruction. Therefore, it
is impossible to directly apply conventional dis-
criminative reranking to such problems. We show
how to adapt reranking to work with such weak
supervision. Instead of using gold-standard an-
notations to determine the correct interpretations,
we simply prefer interpretations of navigation in-
structions that, when executed in the world, actu-
ally reach the intended destination. Additionally,
we extensively revise the features typically used in
parse reranking to work with the PCFG approach
to grounded language learning.
The rest of the paper is organized as fol-
lows: Section 2 reviews the navigation task and
the PCFG approach to grounded language learn-
ing. Section 3 presents our modified approach to
reranking and Section 4 describes the novel fea-
tures used to evaluate parses. Section 5 experi-
mentally evaluates the approach comparing to sev-
218
(a) Sample virtual world of hallways with varying tiles,
wallpapers, and landmark objects indicated by letters
(e.g. ?H? for hat-rack) and illustrating a sample path
taken by a human follower.
(b) A sample natural language instruction and its formal land-
marks plan for the path illustrated above. The subset corre-
sponding to the correct formal plan is shown in bold.
Figure 1: Sample virtual world and instruction.
eral baselines. Finally, Section 6 describes related
work, Section 7 discusses future work, and Sec-
tion 8 concludes.
2 Background
2.1 Navigation Task
We address the navigation learning task intro-
duced by Chen and Mooney (2011). The goal is
to interpret natural-language (NL) instructions in a
virtual environment, thereby allowing a simulated
robot to navigate to a specified location. Figure 1a
shows a sample path executed by a human follow-
ing the instruction in Figure 1b. Given no prior lin-
guistic knowledge, the task is to learn to interpret
such instructions by simply observing humans fol-
low sample directions. Formally speaking, given
training examples of the form (ei, ai, wi), where
ei is an NL instruction, ai is an executed action
sequence for the instruction, and wi is the initial
world state, we want to learn to produce an appro-
priate action sequence aj given a novel (ej , wj).
More specifically, one must learn a seman-
tic parser that produces a plan pj using a for-
mal meaning representation (MR) language intro-
duced by Chen and Mooney (2011). This plan is
then executed by a simulated robot in a virtual en-
vironment. The MARCO system, introduced by
MacMahon et al (2006), executes the formal plan,
flexibly adapting to situations encountered dur-
ing execution and producing the action sequence
aj . During learning, Chen and Mooney construct
a landmarks plan ci for each training example,
which includes the complete context observed in
the world-state resulting from each observed ac-
tion. The correct plan, pi, (which is latent and
must be inferred) is assumed to be composed from
a subset of the components in the corresponding
landmarks plan. The landmarks and correct plans
for a sample instruction are shown in Figure 1b.
2.2 PCFG Induction for Grounded Language
Learning
The baseline generative model we use for rerank-
ing employs the unsupervised PCFG induction ap-
proach introduced by Kim and Mooney (2012).
This model is, in turn, based on the earlier model
of Bo?rschinger et al (2011), which transforms
the grounded language learning into unsupervised
PCFG induction. The general approach uses
grammar-formulation rules which construct CFG
productions that form a grammar that effectively
maps NL sentences to formal meaning represen-
tations (MRs) encoded in its nonterminals. After
using Expectation-Maximization (EM) to estimate
the parameters for these productions using the am-
biguous supervision provided by the grounded-
learning setting, it produces a PCFG whose most
probable parse for a sentence encodes its correct
semantic interpretation. Unfortunately, the initial
approach of Bo?rschinger et al (2011) produces ex-
plosively large grammars when applied to more
complex problems, such as our navigation task.
Therefore, Kim and Mooney enhanced their ap-
proach to use a previously learned semantic lexi-
con to reduce the induced grammar to a tractable
size. They also altered the processes for construct-
ing productions and mapping parse trees to MRs in
order to make the construction of semantic inter-
pretations more compositional and allow the ef-
ficient construction of more complex representa-
219
Figure 2: Simplified parse for the sentence ?Turn
left and find the sofa then turn around the corner?
for Kim and Mooney?s model. Nonterminals show
the MR graph, where additional nonterminals for
generating NL words are omitted.
tions.
The resulting PCFG can be used to produce
a set of most-probable interpretations of instruc-
tional sentences for the navigation task. Our pro-
posed reranking model is used to discriminatively
reorder the top parses produced by this generative
model. A simplified version of a sample parse tree
for Kim and Mooney?s model is shown in Figure 2.
3 Modified Reranking Algorithm
In reranking, a baseline generative model is first
trained and generates a set of candidate outputs
for each training example. Next, a second con-
ditional model is trained which uses global fea-
tures to rescore the candidates. Reranking using
an averaged perceptron (Collins, 2002a) has been
successfully applied to a variety of NLP tasks.
Therefore, we modify it to rerank the parse trees
generated by Kim and Mooney (2012)?s model.
The approach requires three subcomponents: 1)
a GEN function that returns the list of top n can-
didate parse trees for each NL sentence produced
by the generative model, 2) a feature function ?
that maps a NL sentence, e, and a parse tree, y,
into a real-valued feature vector ?(e, y) ? Rd, and
3) a reference parse tree that is compared to the
highest-scoring parse tree during training.
However, grounded language learning tasks,
such as our navigation task, do not provide ref-
erence parse trees for training examples. Instead,
our modified model replaces the gold-standard ref-
erence parse with the ?pseudo-gold? parse tree
Algorithm 1 AVERAGED PERCEPTRON TRAIN-
ING WITH RESPONSE-BASED UPDATE
Input: A set of training examples (ei, y?i ),
where ei is a NL sentence and y?i =
arg maxy?GEN(ei) EXEC(y)Output: The parameter vector W? , averaged
over all iterations 1...T
1: procedure PERCEPTRON
2: Initialize W? = 0
3: for t = 1...T, i = 1...n do
4: yi = arg maxy?GEN(ei) ?(ei, y) ? W?
5: if yi 6= y?i then
6: W? = W? + ?(ei, y?i )? ?(ei, yi)
7: end if
8: end for
9: end procedure
whose derived MR plan is most successful at get-
ting to the desired goal location. Thus, the third
component in our reranking model becomes an
evaluation function EXEC that maps a parse tree
y into a real number representing the success rate
(w.r.t. successfully reaching the intended destina-
tion) of the derived MR plan m composed from
y.
Additionally, we improve the perceptron train-
ing algorithm by using multiple reference parses
to update the weight vector W? . Although
we determine the pseudo-gold reference tree to
be the candidate parse y? such that y? =
arg maxy?GEN(e) EXEC(y), it may not actually be
the correct parse for the sentence. Other parses
may contain useful information for learning, and
therefore we devise a way to update weights us-
ing all candidate parses whose successful execu-
tion rate is greater than the parse preferred by the
currently learned model.
3.1 Response-Based Weight Updates
To circumvent the need for gold-standard refer-
ence parses, we select a pseudo-gold parse from
the candidates produced by the GEN function. In a
similar vein, when reranking semantic parses, Ge
and Mooney (2006) chose as a reference parse the
one which was most similar to the gold-standard
semantic annotation. However, in the navigation
task, the ultimate goal is to generate a plan that,
when actually executed in the virtual environment,
leads to the desired destination. Therefore, the
pseudo-gold reference is chosen as the candidate
parse that produces the MR plan with the great-
220
est execution success. This requires an external
module that evaluates the execution accuracy of
the candidate parses. For the navigation task, we
use the MARCO (MacMahon et al, 2006) ex-
ecution module, which is also used to evaluate
how well the overall system learns to follow direc-
tions (Chen and Mooney, 2011). Since MARCO
is nondeterministic when executing underspecified
plans, we execute each candidate plan 10 times,
and its execution rate is the percentage of trials
in which it reaches the correct destination. When
there are multiple candidate parses tied for the
highest execution rate, the one assigned the largest
probability by the baseline model is selected. Our
modified averaged perceptron procedure with such
a response-based update is shown in Algorithm 1.
One additional issue must be addressed when
computing the output of the GEN function. The fi-
nal plan MRs are produced from parse trees using
compositional semantics (see Kim and Mooney
(2012) for details). Consequently, the n-best parse
trees for the baseline model do not necessarily pro-
duce the n-best distinct plans, since many parses
can produce the same plan. Therefore, we adapt
the GEN function to produce the n best distinct
plans rather than the n best parses. This may
require examining many more than the n best
parses, because many parses have insignificant
differences that do not affect the final plan. The
score assigned to a plan is the probability of the
most probable parse that generates that plan. In
order to efficiently compute the n best plans, we
modify the exact n-best parsing algorithm devel-
oped by Huang and Chiang (2005). The modified
algorithm ensures that each plan in the computed
n best list produces a new distinct plan.
3.2 Weight Updates Using Multiple Parses
Typically, when used for reranking, the averaged
perceptron updates its weights using the feature-
vector difference between the current best pre-
dicted candidate and the gold-standard reference
(line 6 in Algorithm 1). In our initial modified
version, we replaced the gold-standard reference
parse with the pseudo-gold reference, which has
the highest execution rate amongst all candidate
parses. However, this ignores all other candidate
parses during perceptron training. However, it is
not ideal to regard other candidate parses as ?use-
less.? There may be multiple candidate parses with
the same maximum execution rate, and even can-
didates with lower execution rates could represent
the correct plan for the instruction given the weak,
indirect supervision provided by the observed se-
quence of human actions.
Therefore, we also consider a further mod-
ification of the averaged perceptron algorithm
which updates its weights using multiple candi-
date parses. Instead of only updating the weights
with the single difference between the predicted
and pseudo-gold parses, the weight vector W? is
updated with the sum of feature-vector differences
between the current predicted candidate and all
other candidates that have a higher execution rate.
Formally, in this version, we replace lines 5?6 of
Algorithm 1 with:
1: for all y ? GEN(ei) where y 6= yi and
EXEC(y) > EXEC(yi) do
2: W? = W? + (EXEC(y)? EXEC(yi))
?(?(ei, y)? ?(ei, yi))
3: end for
where EXEC(y) is the execution rate of the MR
plan m derived from parse tree y.
In the experiments below, we demonstrate that,
by exploiting multiple reference parses, this new
update rule increases the execution accuracy of
the final system. Intuitively, this approach gathers
additional information from all candidate parses
with higher execution accuracy when learning the
discriminative reranker. In addition, as shown in
line 2 of the algorithm above, it uses the differ-
ence in execution rates between a candidate and
the currently preferred parse to weight the update
to the parameters for that candidate. This allows
more effective plans to have a larger impact on the
learned model in each iteration.
4 Reranking Features
This section describes the features ? extracted
from parses produced by the generative model and
used to rerank the candidates.
4.1 Base Features
The base features adapt those used in previous
reranking methods, specifically those of Collins
(2002a), Lu et al (2008), and Ge and Mooney
(2006), which are directly extracted from parse
trees. In addition, we also include the log prob-
ability of the parse tree as an additional feature.
Figure 3 shows a sample full parse tree from our
baseline model, which is used when explaining the
221
L1: Turn(LEFT), Verify(front : SOFA, back : EASEL),
Travel(steps : 2), Verify(at : SOFA), Turn(RIGHT)
L6: Turn()
PhraseL6
WordL6
corner
PhXL6
Word?
the
PhXL6
WordL6
around
PhXL6
WordL6
turn
PhXL6
Word?
then
L3: Travel(steps : 2),
Verify(at : SOFA), Turn(RIGHT)
L5: Travel(), Verify(at : SOFA)
PhraseL5
WordL5
sofa
PhXL5
Word?
the
PhXL5
WordL5
find
L2: Turn(LEFT),
Verify(front : SOFA)
L4: Turn(LEFT)
PhraseL4
Word?
and
PhL4
WordL4
left
PhXL4
WordL4
Turn
Figure 3: Sample full parse tree for the sentence ?Turn left and find the soft then turn around the corner?
used to explain reranking features. Nonterminals representing MR plan components are shown, which
are labeled L1 to L6 for ease of reference. Additional nonterminals such as Phrase, Ph, PhX , and
Word are subsidiary ones for generating NL words from MR nonterminals. They are also shown in
order to represent the entire process of how parse trees are constructed (for details, refer to Kim and
Mooney (2012)).
reranking features below, each illustrated by an ex-
ample.
a) PCFG Rule. Indicates whether a particular
PCFG rule is used in the parse tree: f(L1 ?
L2L3) = 1.
b) Grandparent PCFG Rule. Indicates whether
a particular PCFG rule as well as the non-
terminal above it is used in the parse tree:
f(L3 ? L5L6|L1) = 1.
c) Long-range Unigram. Indicates whether a
nonterminal has a given NL word below it
in the parse tree: f(L2 ; left) = 1 and
f(L4 ; turn) = 1.
d) Two-level Long-range Unigram. Indicates
whether a nonterminal has a child nontermi-
nal which eventually generates a NL word in
the parse tree: f(L4 ; left|L2) = 1
e) Unigram. Indicates whether a nonterminal
produces a given child nonterminal or terminal
NL word in the parse tree: f(L1 ? L2) = 1
and f(L1 ? L3) = 1.
f) Grandparent Unigram. Indicates whether
a nonterminal has a given child nontermi-
nal/terminal below it, as well as a given parent
nonterminal: f(L2 ? L4|L1) = 1
g) Bigram. Indicates whether a given bigram of
nonterminal/terminals occurs for given a par-
ent nonterminal: f(L1 ? L2 : L3) = 1.
h) Grandparent Bigram. Same as Bigram, but
also includes the nonterminal above the parent
nonterminal: f(L3 ? L5 : L6|L1) = 1.
i) Log-probability of Parse Tree. Certainty as-
signed by the base generative model.
4.2 Predicate-Only Features
The base features above generally include non-
terminal symbols used in the parse tree. In the
grounded PCFG model, nonterminals are named
after components of the semantic representations
(MRs), which are complex and numerous. There
are ' 2,500 nonterminals in the grammar con-
structed for the navigation data, most of which
are very specific and rare. This results in a very
large, sparse feature space which can easily lead
222
the reranking model to over-fit the training data
and prevent it from generalizing properly.
Therefore, we also tried constructing more gen-
eral features that are less sparse. First, we con-
struct generalized versions of the base features
in which nonterminal symbols use only predicate
names and omit their arguments. In the navigation
task, action arguments frequently contain redun-
dant, rarely used information. In particular, the
interleaving verification steps frequently include
many details that are never actually mentioned in
the NL instructions. For instance, a nonterminal
for the MR
Turn(LEFT),
Verify(at:SOFA,front:EASEL),
Travel(steps:3)
is transformed into the predicate-only form
Turn(), Verify(), Travel()
, and then used to construct more general versions
of the base features described in the previous sec-
tion. Second, another version of the base features
are constructed in which nonterminal symbols in-
clude action arguments but omit all interleaving
verification steps. This is a somewhat more con-
servative simplification of the nonterminal sym-
bols. Although verification steps sometimes help
interpret the actions and their surrounding context,
they frequently cause the nonterminal symbols to
become unnecessarily complex and specific.
4.3 Descended Action Features
Finally, another feature group which we utilize
captures whether a particular atomic action in a
nonterminal ?descends? into one of its child non-
terminals or not. An atomic action consists of a
predicate and its arguments, e.g. Turn(LEFT),
Travel(steps:2), or Verify(at:SOFA).
When an atomic action descends into lower non-
terminals in a parse tree, it indicates that it is men-
tioned in the NL instruction and is therefore im-
portant. Below are several feature types related to
descended actions that are used in our reranking
model:
a) Descended Action. Indicates whether a given
atomic action in a nonterminal descends to the
next level. In Figure 3, f(Turn(LEFT)) = 1
since it descends into L2 and L4.
b) Descended Action Unigram. Same as De-
scended Action, but also includes the current
nonterminal: f(Turn(LEFT)|L1) = 1.
c) Grandparent Descended Action Unigram.
Same as Descended Action Unigram,
but additionally includes the parent
nonterminal as well as the current one:
f(Turn(LEFT)|L2, L1) = 1.
d) Long-range Descended Action Unigram. Indi-
cates whether a given atomic action in a non-
terminal descends to a child nonterminal and
this child generates a given NL word below it:
f(Turn(LEFT) ; left) = 1
5 Experimental Evaluation
5.1 Data and Methodology
The navigation data was collected by MacMahon
et al (2006), and includes English instructions
and human follower data.1 The data contains 706
route instructions for three virtual worlds. The in-
structions were produced by six instructors for 126
unique starting and ending location pairs over the
three maps. Each instruction is annotated with 1
to 15 human follower traces with an average of
10.4 actions per instruction. Each instruction con-
tains an average of 5.0 sentences each with an av-
erage of 7.8 words. Chen and Mooney (2011)
constructed a version of the data in which each
sentence is annotated with the actions taken by
the majority of followers when responding to this
sentence. This single-sentence version is used for
training. Manually annotated ?gold standard? for-
mal plans for each sentence are used for evaluation
purposes only.
We followed the same experimental methodol-
ogy as Kim and Mooney (2012) and Chen and
Mooney (2011). We performed ?leave one en-
vironment out? cross-validation, i.e. 3 trials of
training on two environments and testing on the
third. The baseline model is first trained on data
for two environments and then used to generate
the n = 50 best plans for both training and test-
ing instructions. As mentioned in Section 3.1, we
need to generate many more top parse trees to get
50 distinct formal MR plans. We limit the num-
ber of best parse trees to 1,000,000, and even with
this high limit, some training examples were left
with less than 50 distinct plans.2 Each candidate
1Data is available at http://www.cs.utexas.
edu/users/ml/clamp/navigation/
29.6% of the examples (310 out of total 3237) produced
less than 50 distinct MR plans in the evaluation. This was
mostly due to exceeding the parse-tree limit and partly be-
cause the baseline model failed to parse some NL sentences.
223
n 1 2 5 10 25 50
Parse Accuracy F1 74.81 79.08 82.78 85.32 87.52 88.62
Plan Execution Single-sentence 57.22 63.86 70.93 76.41 83.59 87.02Paragraph 20.17 28.08 35.34 40.64 48.69 53.66
Table 1: Oracle parse and execution accuracy for single sentence and complete paragraph instructions
for the n best parses.
plan is then executed using MARCO and its rate
of successfully reaching the goal is recorded. Our
reranking model is then trained on the training
data using the n-best candidate parses. We only
retain reranking features that appear (i.e. have a
value of 1) at least twice in the training data.
Finally, we measure both parse and execution
accuracy on the test data. Parse accuracy evalu-
ates how well a system maps novel NL sentences
for new environments into correct MR plans (Chen
and Mooney, 2011). It is calculated by compar-
ing the system?s MR output to the gold-standard
MR. Accuracy is measured using F1, the harmonic
mean of precision and recall for individual MR
constituents, thereby giving partial credit to ap-
proximately correct MRs. We then execute the re-
sulting MR plans in the test environment to see
whether they successfully reach the desired des-
tinations. Execution is evaluated both for sin-
gle sentence and complete paragraph instructions.
Successful execution rates are calculated by aver-
aging 10 nondeterministic MARCO executions.
5.2 Reranking Results
Oracle results
As typical in reranking experiments, we first
present results for an ?oracle? that always returns
the best result amongst the top-n candidates pro-
duced by the baseline system, thereby providing
an upper bound on the improvements possible
with reranking. Table 1 shows oracle accuracy for
both semantic parsing and plan execution for sin-
gle sentence and complete paragraph instructions
for various values of n. For oracle parse accuracy,
for each sentence, we pick the parse that gives
the highest F1 score. For oracle single-sentence
execution accuracy, we pick the parse that gives
the highest execution success rate. These single-
sentence plans are then concatenated to produce a
complete plan for each paragraph instruction in or-
der to measure overall execution accuracy. Since
making an error in any of the sentences in an in-
struction can easily lead to the wrong final destina-
tion, paragraph-level accuracies are always much
lower than sentence-level ones. In order to bal-
ance oracle accuracy and the computational ef-
fort required to produce n distinct plans, we chose
n = 50 for the final experiments since oracle per-
formance begins to asymptote at this point.
Response-based vs. gold-standard reference
weight updates
Table 2 presents reranking results for our proposed
response-based weight update (Single) for the
averaged perceptron (cf. Section 3.1) compared
to the typical weight update method using gold-
standard parses (Gold). Since the gold-standard
annotation gives the correct MR rather than a parse
tree for each sentence, Gold selects as a single
reference parse the candidate in the top 50 whose
resulting MR is most similar to the gold-standard
MR as determined by its parse accuracy. Ge and
Mooney (2006) employ a similar approach when
reranking semantic parses.
The results show that our response-based ap-
proach (Single) has better execution accuracy
than both the baseline and the standard approach
using gold-standard parses (Gold). However,
Gold does perform best on parse accuracy since
it explicitly focuses on maximizing the accuracy
of the resulting MR. In contrast, by focusing dis-
criminative training on optimizing performance
of the ultimate end task, our response-based ap-
proach actually outperforms the traditional ap-
proach on the final task. In addition, it only uti-
lizes feedback that is naturally available for the
task, rather than requiring an expert to laboriously
annotate each sentence with a gold-standard MR.
Even though Gold captures more elements of the
gold-standard MRs, it may miss some critical MR
components that are crucial to the final naviga-
tion task. The overall result is very promising be-
cause it demonstrates how reranking can be ap-
plied to grounded language learning tasks where
gold-standard parses are not readily available.
224
Parse Acc Plan Execution
F1 Single Para
Baseline 74.81 57.22 20.17
Gold 78.26 52.57 19.33
Single 73.32 59.65 22.62
Multi 73.43 62.81 26.57
Table 2: Reranking results comparing our
response-based methods using single (Single)
or multiple (Multi) pseudo-gold parses to the
standard approach using a single gold-standard
parse (Gold). Baseline refers to Kim and
Mooney (2012)?s system. Reranking results use
all features described in Section 4. ?Single? means
the single-sentence version and ?Para? means the
full paragraph version of the corpus.
Weight update with single vs. multiple
reference parses
Table 2 also shows performance when using mul-
tiple reference parse trees to update weights (cf.
Section 3.2). Using multiple parses (Multi)
clearly performs better for all evaluation met-
rics, particularly execution. As explained in Sec-
tion 3.2, the single-best pseudo-gold parse pro-
vides weak, ambiguous feedback since it only pro-
vides a rough estimate of the response feedback
from the execution module. Using a variety of
preferable parses to update weights provides a
greater amount and variety of weak feedback and
therefore leads to a more accurate model.3
Comparison of different feature groups
Table 3 compares reranking results using the dif-
ferent feature groups described in Section 4. Com-
pared to the baseline model (Kim and Mooney,
2012), each of the feature groups Base (base
features), Pred (predicate-only and verification-
removed features), and Desc (descended action
features) helps improve the performance of plan
execution for both single sentence and complete
paragraph navigation instructions. Among them,
Desc is the most effective group of features.
Combinations of the feature groups helps fur-
3We also tried extending Gold to use multiple reference
parses in the same manner, but this actually degraded its per-
formance for all metrics. This indicates that, unlike Multi,
parses other than the best one do not have useful information
in terms of optimizing normal parse accuracy. Instead, ad-
ditional parses seem to add noise to the training process in
this case. Therefore, updating with multiple parses does not
appear to be useful in standard reranking.
Features Parse Acc Plan ExecutionF1 Single Para
Baseline 74.81 57.22 20.17
Base 71.50 60.09 23.20
Pred 71.61 60.87 24.13
Desc 73.90 61.33 25.00
Base+Pred 69.52 61.49 26.24
Base+Desc 73.66 61.72 25.58
Pred+Desc 72.56 62.36 26.04
All 73.43 62.81 26.57
Table 3: Reranking results comparing different
sets of features. Base refers to base features (cf.
Section 4.1), Pred refers to predicate-only fea-
tures and also includes features based on remov-
ing interleaving verification steps (cf. Section 4.2),
Desc refers to descended action features (cf. Sec-
tion 4.3). All refers to all the features including
Base, Pred, and Desc. All results use weight
update with multiple reference parses (cf. Sec-
tion 3.2).
ther improve the plan execution performance, and
reranking using all of the feature groups (All)
performs the best, as expected. However, since
our model is optimizing plan execution during
training, the results for parse accuracy are always
worse than the baseline model.
6 Related Work
Discriminative reranking is a common machine
learning technique to improve the output of gen-
erative models. It has been shown to be effective
for various natural language processing tasks in-
cluding syntactic parsing (Collins, 2000; Collins,
2002b; Collins and Koo, 2005; Charniak and
Johnson, 2005; Huang, 2008), semantic parsing
(Lu et al, 2008; Ge and Mooney, 2006), part-
of-speech tagging (Collins, 2002a), semantic role
labeling (Toutanova et al, 2005), named entity
recognition (Collins, 2002c). machine translation
(Shen et al, 2004; Fraser and Marcu, 2006) and
surface realization in generation (White and Ra-
jkumar, 2009; Konstas and Lapata, 2012). How-
ever, to our knowledge, there has been no pre-
vious attempt to apply discriminative reranking
to grounded language acquisition, where gold-
standard reference parses are not typically avail-
able for training reranking models.
Our use of response-based training is similar
225
to work on learning semantic parsers from execu-
tion output such as the answers to database queries
(Clarke et al, 2010; Liang et al, 2011). Although
the demands of grounded language tasks, such as
following navigation instructions, are different, it
would be interesting to try adapting these alterna-
tive approaches to such problems.
7 Future Work
In the future, we would like to explore the con-
struction of better, more-general reranking fea-
tures that are less prone to over-fitting. Since
typical reranking features rely on the combina-
tion and/or modification of nonterminals appear-
ing in parse trees, for the large PCFG?s produced
for grounded language learning, such features are
very sparse and rare. Although the current features
provide a significant increase in performance, or-
acle results imply that an even larger benefit may
be achievable.
In addition, employing other reranking method-
ologies, such as kernel methods (Collins, 2002b),
and forest reranking exploiting a packed forest of
exponentially many parse trees (Huang, 2008), is
another area of future work. We also would like
to apply our approach to other reranking algo-
rithms such as SVMs (Joachims, 2002) and Max-
Ent methods (Charniak and Johnson, 2005).
8 Conclusions
In this paper, we have shown how to adapt dis-
criminative reranking to grounded language learn-
ing. Since typical grounded language learning
problems, such as navigation instruction follow-
ing, do not provide the gold-standard reference
parses required by standard reranking models, we
have devised a novel method for using the weaker
supervision provided by response feedback (e.g.
the execution of inferred navigation plans) when
training a perceptron-based reranker. This ap-
proach was shown to be very effective compared
to the traditional method of using gold-standard
parses. In addition, since this response-based su-
pervision is weak and ambiguous, we have also
presented a method for using multiple reference
parses to perform perceptron weight updates and
shown a clear further improvement in end-task
performance with this approach.
Acknowledgments
We thank anonymous reviewers for their helpful
comments to improve this paper. This work was
funded by the NSF grant IIS-0712907 and IIS-
1016312. Experiments were performed on the
Mastodon Cluster, provided by NSF Grant EIA-
0303609.
References
Benjamin Bo?rschinger, Bevan K. Jones, and Mark
Johnson. 2011. Reducing grounded learning tasks
to grammatical inference. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 1416?1425,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), pages 173?180, Ann Arbor, MI, June.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011), San Francisco, CA, USA, August.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. Journal
of Artificial Intelligence Research, 37:397?435.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL-2010), pages 18?27, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning (ICML-2000), pages 175?182, Stanford,
CA, June.
Michael Collins. 2002a. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-02),
Philadelphia, PA, July.
Michael Collins. 2002b. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of
226
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 263?270,
Philadelphia, PA, July.
Michael Collins. 2002c. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2002), pages 489?496, Philadelphia,
PA.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (ACL-06), pages 769?776, Stroudsburg, PA,
USA. Association for Computational Linguistics.
R. Ge and R. J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING/ACL-
06), Sydney, Australia, July.
Kevin Gold and Brian Scassellati. 2007. A robot that
uses existing vocabulary to infer non-visual word
meanings from observation. In Proceedings of the
22nd national conference on Artificial intelligence -
Volume 1, AAAI?07, pages 883?888. AAAI Press.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the Eighth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-
2002), Edmonton, Canada.
Joohyun Kim and Raymond J. Mooney. 2012. Un-
supervised PCFG induction for grounded language
learning with highly ambiguous supervision. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing and Natural Lan-
guage Learning, EMNLP-CoNLL ?12.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 369?378, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL, Portland, Oregon, June.
Association for Computational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08), Honolulu, HI, October.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: connecting language,
knowledge, and action in route instructions. In pro-
ceedings of the 21st national conference on Artifi-
cial intelligence - Volume 2, AAAI?06, pages 1475?
1482. AAAI Press.
Deb Roy. 2002. Learning visually grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3):353?385.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
177?184, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL-05), pages 589?596, Ann Arbor, MI,
June.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 -
Volume 1, EMNLP ?09, pages 410?419, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Chen Yu and Dana H. Ballard. 2004. On the integra-
tion of grounding language and learning objects. In
Proceedings of the Nineteenth National Conference
on Artificial Intelligence (AAAI-04), pages 488?493.
227
