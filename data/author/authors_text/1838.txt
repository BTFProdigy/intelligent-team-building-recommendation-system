Proceedings of NAACL HLT 2007, pages 444?451,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Randomized Decoding for Selection-and-Ordering Problems
Pawan Deshpande, Regina Barzilay and David R. Karger
Computer Science and Articial Intelligence Laboratory
Massachusetts Institute of Technology
{pawand,regina,karger}@csail.mit.edu
Abstract
The task of selecting and ordering infor-
mation appears in multiple contexts in text
generation and summarization. For in-
stance, methods for title generation con-
struct a headline by selecting and order-
ing words from the input text. In this pa-
per, we investigate decoding methods that
simultaneously optimize selection and or-
dering preferences. We formalize decod-
ing as a task of finding an acyclic path
in a directed weighted graph. Since the
problem is NP-hard, finding an exact so-
lution is challenging. We describe a novel
decoding method based on a randomized
color-coding algorithm. We prove bounds
on the number of color-coding iterations
necessary to guarantee any desired likeli-
hood of finding the correct solution. Our
experiments show that the randomized de-
coder is an appealing alternative to a range
of decoding algorithms for selection-and-
ordering problems, including beam search
and Integer Linear Programming.
1 Introduction
The task of selecting and ordering information ap-
pears in multiple contexts in text generation and
summarization. For instance, a typical multidocu-
ment summarization system creates a summary by
selecting a subset of input sentences and ordering
them into a coherent text. Selection and ordering at
the word level is commonly employed in lexical re-
alization. For instance, in the task of title generation,
the headline is constructed by selecting and ordering
words from the input text.
Decoding is an essential component of the
selection-and-ordering process. Given selection and
ordering preferences, the task is to find a sequence of
elements that maximally satisfies these preferences.
One possible approach for finding such a solution
is to decompose it into two tasks: first, select a set
of words based on individual selection preferences,
and then order the selected units into a well-formed
sequence. Although the modularity of this approach
is appealing, the decisions made in the selection step
cannot be retracted. Therefore, we cannot guarantee
that selected units can be ordered in a meaningful
way, and we may end up with a suboptimal output.
In this paper, we investigate decoding methods
that simultaneously optimize selection and order-
ing preferences. We formalize decoding as find-
ing a path in a directed weighted graph.1 The
vertices in the graph represent units with associ-
ated selection scores, and the edges represent pair-
wise ordering preferences. The desired solution is
the highest-weighted acyclic path of a prespecified
length. The requirement for acyclicity is essential
because in a typical selection-and-ordering problem,
a well-formed output does not include any repeated
units. For instance, a summary of multiple docu-
ments should not contain any repeated sentences.
1We assume that the scoring function is local; that is, it is
computed by combining pairwise scores. In fact, the majority
of models that are used to guide ordering (i.e., bigrams) are local
scoring functions.
444
Since the problem is NP-hard, finding an exact
solution is challenging. We introduce a novel ran-
domized decoding algorithm2 based on the idea of
color-coding (Alon et al, 1995). Although the algo-
rithm is not guaranteed to find the optimal solution
on any single run, by increasing the number of runs
the algorithm can guarantee an arbitrarily high prob-
ability of success. The paper provides a theoretical
analysis that establishes the connection between the
required number of runs and the likelihood of find-
ing the correct solution.
Next, we show how to find an exact solution using
an integer linear programming (ILP) formulation.
Although ILP is NP-hard, this method is guaranteed
to compute the optimal solution. This allows us to
experimentally investigate the trade-off between the
accuracy and the efficiency of decoding algorithms
considered in the paper.
We evaluate the accuracy of the decoding algo-
rithms on the task of title generation. The decod-
ing algorithms introduced in the paper are compared
against beam search, a heuristic search algorithm
commonly used for selection-and-ordering and other
natural language processing tasks. Our experiments
show that the randomized decoder is an appealing al-
ternative to both beam search and ILP when applied
to selection-and-ordering problems.
2 Problem Formulation
In this section, we formally define the decoding task
for selection-and-ordering problems. First, we intro-
duce our graph representation and show an example
of its construction for multidocument summariza-
tion. (An additional example of graph construction
for title generation is given in Section 6.) Then, we
discuss the complexity of this task and its connec-
tion to classical NP-hard problems.
2.1 Graph Representation
We represent the set of selection units as the set of
vertices V in a weighted directed graph G. The
set of edges E represents pairwise ordering scores
between all pairs of vertices in V . We also add a
special source vertex s and sink vertex t. For each
vertex v in V , we add an edge from s to v and an
2The code is available at
http://people.csail.mit.edu/pawand/rand/
edge from v to t. We then define the set of all ver-
tices as V ? = V ? {s, t}, and the set of all edges as
E? = E ? {(s, v) ? v ? V } ? {(v, t) ? v ? V }.
To simplify the representation, we remove all ver-
tex weights in our graph structure and instead shift
the weight for each vertex onto its incoming edges.
For each pair of distinct vertices (v, u) ? V , we set
the weight of edge ev,u to be the sum of the loga-
rithms of the selection score for u and the pairwise
ordering score of (v, u).
We also enhance our graph representation by
grouping sets of vertices into equivalence classes.
We introduce these classes to control for redundancy
as required in many selection-and-ordering prob-
lems.3 For instance, in title generation, an equiva-
lence class may consist of morphological variants of
the same stem (i.e., examine and examination). Be-
cause a typical title is unlikely to contain more than
one word with the same stem, we can only select a
single representative from each class.
Our task is now to find the highest weighted
acyclic path starting at s and ending at t with k ver-
tices in between, such that no two vertices belong to
the same equivalence class.
2.2 Example: Decoding for Multidocument
Summarization
In multidocument summarization, the vertices in the
decoding graph represent sentences from input doc-
uments. The vertices may be organized into equiva-
lence classes that correspond to clusters of sentences
conveying similar information. The edges in the
graph represent the combination of the selection and
the ordering scores. The selection scores encode the
likelihood of a sentence to be extracted, while pair-
wise ordering scores capture coherence-based prece-
dence likelihood. The goal of the decoder is to find
the sequence of k non-redundant sentences that op-
timize both the selection and the ordering scores.
Finding an acyclic path with the highest weight will
achieve this goal.
3An alternative approach for redundancy control would be
to represent all the members of an equivalence class as a sin-
gle vertex in the graph. However, such an approach does not
allow us to select the best representative from the class. For in-
stance, one element in the equivalence class may have a highly
weighted incoming edge, while another may have a highly
weighted outgoing edge.
445
2.3 Relation to Classical Problems
Our path-finding problem may seem to be simi-
lar to the tractable shortest paths problem. How-
ever, the requirement that the path be long makes it
more similar to the the Traveling Salesman Problem
(TSP). More precisely, our problem is an instance of
the prize collecting traveling salesman problem, in
which the salesman is required to visit k vertices at
best cost (Balas, 1989; Awerbuch et al, 1995).
Since our problem is NP-hard, we might be pes-
simistic about finding an exact solution. But our
problem has an important feature: the length k of
the path we want to find is small relative to the num-
ber of vertices n. This feature distinguishes our task
from other decoding problems, such as decoding in
machine translation (Germann et al, 2001), that are
modeled using a standard TSP formulation. In gen-
eral, the connection between n and k opens up a new
range of solutions. For example, if we wanted to
find the best length-2 path, we could simply try all
subsets of 2 vertices in the graph, in all 2 possible
orders. This is a set of only O(n2) possibilities, so
we can check all to identify the best in polynomial
time.
This approach is very limited, however: in gen-
eral, its runtime of O(nk) for paths of length k
makes it prohibitive for all but the smallest values
of k. We cannot really hope to avoid the exponential
dependence on k, because doing so would give us
a fast solution to an NP-hard problem, but there is
hope of making the dependence ?less exponential.?
This is captured by the definition of xed parameter
tractability (Downey and Fellows, 1995). A prob-
lem is fixed parameter tractable if we can make the
exponential dependence on the parameter k indepen-
dent of the polynomial dependence on the problem
size n. This is the case for our problem: as we will
describe below, an algorithm of Alon et al can be
used to achieve a running time of roughly O(2kn2).
In other words, the path length k only exponentiates
a small constant, instead of the problem size n, while
the dependence on n is in fact quadratic.
3 Related Work
Decoding for selection-and-ordering problems is
commonly implemented using beam search (Banko
et al, 2000; Corston-Oliver et al, 2002; Jin and
Hauptmann, 2001). Being heuristic in nature, this
algorithm is not guaranteed to find an optimal so-
lution. However, its simplicity and time efficiency
make it a decoding algorithm of choice for a wide
range of NLP applications. In applications where
beam decoding does not yield sufficient accuracy,
researchers employ an alternative heuristic search,
A* (Jelinek, 1969; Germann et al, 2001). While in
some cases A* is quite effective, in other cases its
running time and memory requirements may equal
that of an exhaustive search. Time- and memory-
bounded modifications of A* (i.e., IDA-A*) do not
suffer from this limitation, but they are not guaran-
teed to find the exact solution. Nor do they pro-
vide bounds on the likelihood of finding the exact
solution. Newly introduced methods based on lo-
cal search can effectively examine large areas of a
search space (Eisner and Tromble, 2006), but they
still suffer from the same limitations.
As an alternative to heuristic search algorithms,
researchers also employ exact methods from com-
binatorial optimization, in particular integer linear
programming (Germann et al, 2001; Roth and Yih,
2004). While existing ILP solvers find the exact so-
lution eventually, the running time may be too slow
for practical applications.
Our randomized decoder represents an impor-
tant departure from previous approaches to decod-
ing selection-and-ordering problems. The theoreti-
cally established bounds on the performance of this
algorithm enable us to explicitly control the trade-
off between the quality and the efficiency of the de-
coding process. This property of our decoder sets it
apart from existing heuristic algorithms that cannot
guarantee an arbitrarily high probability of success.
4 Randomized Decoding with
Color-Coding
One might hope to solve decoding with a dynamic
program (like that for shortest paths) that grows an
optimal path one vertex at a time. The problem is
that this dynamic program may grow to include a
vertex already on the path, creating a cycle. One way
to prevent this is to remember the vertices used on
each partial path, but this creates a dynamic program
with too many states to compute efficiently.
Instead, we apply a color coding technique of
446
Alon et al(1995). The basic step of the algo-
rithm consists of randomly coloring the graph ver-
tices with a set of colors of size r, and using dy-
namic programming to find the optimum length-k
path without repeated colors. (Later, we describe
how to determine the number of colors r.) Forbid-
ding repeated colors excludes cycles as required, but
remembering only colors on the path requires less
state than remembering precisely which vertices are
on the path. Since we color randomly, any single it-
eration is not guaranteed to find the optimal path; in
a given coloring, two vertices along the optimal path
may be assigned the same color, in which case the
optimal path will never be selected. Therefore, the
whole process is repeated multiple times, increasing
the likelihood of finding an optimal path.
Our algorithm is a variant of the original color-
coding algorithm (Alon et al, 1995), which was de-
veloped to detect the existence of paths of length k
in an unweighted graph. We modify the original al-
gorithm to find the highest weighted path and also
to handle equivalence classes of vertices. In addi-
tion, we provide a method for determining the opti-
mal number of colors to use for finding the highest
weighted path of length k.
We first describe the dynamic programming algo-
rithm. Next, we provide a probabilistic bound on
the likelihood of finding the optimal solution, and
present a method for determining the optimal num-
ber of colors for a given value of k.
Dynamic Programming Recall that we began
with a weighted directed graphG to which we added
artificial start and end vertices s and t. We now posit
a coloring of that graph that assigns a color cv to
each vertex v aside from s and t. Our dynamic pro-
gram returns the maximum score path of length k+2
(including the artificial vertices s and t) from s to t
with no repeated colors.
Our dynamic program grows colorful paths?
paths with at most one vertex of each color. For
a given colorful path, we define the spectrum of
a path to be the set of colors (each used exactly
once) of nodes on the interior of the path?we ex-
clude the starting vertex (which will always be s)
and the ending vertex. To implement the dynamic
program, we maintain a table q[v, S] indexed by a
path-ending vertex v and a spectrum S. For vertex
v and spectrum S, entry q[v, S] contains the value
of the maximum-score colorful path that starts at s,
terminates at v, and has spectrum S in its interior.
We initialize the table with length-one paths:
q[v, ?] represents the path from s to v, whose spec-
trum is the empty set since there are no interior ver-
tices. Its value is set to the score of edge (s, v). We
then iterate the dynamic program k times in order
to build paths of length k + 1 starting at s. We ob-
serve that the optimum colorful path of length ` and
spectrum S from s to v must consist of an optimum
path from s to u (which will already have been found
by the dynamic program) concatenated to the edge
(u, v). When we concatenate (u, v), vertex u be-
comes an interior vertex of the path, and so its color
must not be in the preexisting path?s spectrum, but
joins the spectrum of the path we build. It follows
that
q[v, S] = max
(u,v)?G,cu?S,cv /?S
q[u, S?{cu}] +w(u, v)
After k iterations, for each vertex v we will have
a list of optimal paths from s to v of length k + 1
with all possible spectra. The optimum length-k+ 2
colorful path from s to t must follow the optimum
length-k + 1 path of some spectrum to some penul-
timate vertex v and then proceed to vertex t; we find
it by iterating over all such possible spectra and all
vertices v to determine argmaxv,Sq[v, S]+w(v, t).
Amplification The algorithm of Alon et al, and
the variant we describe, are somewhat peculiar in
that the probability of finding the optimal solu-
tion in one coloring iteration is quite small. But
this can easily be dealt with using a standard tech-
nique called amplication (Motwani and Raghavan,
1995). Suppose that the algorithm succeeds with
small probability p, but that we would like it to suc-
ceed with probability 1 ? ? where ? is very small.
We run the algorithm t = (1/p) ln 1/? times. The
probability that the algorithm fails every single run
is then (1 ? p)t ? e?pt = ?. But if the algorithm
succeeds on even one run, then we will find the op-
timum answer (by taking the best of the answers we
see).
No matter how many times we run the algo-
rithm, we cannot absolutely guarantee an optimal
answer. However, the chance of failure can easily be
driven to negligible levels?achieving, say, a one-in-
a-billion chance of failure requires only 20/p itera-
447
tions by the previous analysis.
Determining the number of colors Suppose that
we use r random colors and want to achieve a given
failure probability ?. The probability that the opti-
mal path has no repeated colors is:
1 ? r ? 1r ?
r ? 2
r ? ? ?
r ? (k ? 1)
r .
By the amplification analysis, the number of trials
needed to drive the failure probability to the desired
level will be inversely proportional to this quantity.
At the same time, the dynamic programming table
at each vertex will have size 2r (indexing on a bit
vector of colors used per path), and the runtime of
each trial will be proportional to this. Thus, the run-
ning time for the necessary number of trials Tr will
be proportional to
1 ? rr ? 1 ?
r
r ? 2 ? ? ?
r
r ? (k ? 1) ? 2
r
What r ? k should we choose to minimize this
quantity? To answer, let us consider the ratio:
Tr+1
Tr
=
(r + 1
r
)k
? r ? (k ? 1)r + 1 ? 2
= 2(1 + 1/r)k(1? k/(r + 1))
If this ratio is less than 1, then using r + 1 col-
ors will be faster than using r; otherwise it will be
slower. When r is very close to k, the above equa-
tion is tiny, indicating that one should increase r.
When r  k, the above equation is huge, indicating
one should decrease r. Somewhere in between, the
ratio passes through 1, indicating the optimum point
where neither increasing nor decreasing r will help.
If we write ? = k/r, and consider large k, then Tr+1Trconverges to 2e?(1??). Solving numerically to find
where this is equal to 1, we find ? ? .76804, which
yields a running time proportional to approximately
(4.5)k.
In practice, rather than using an (approximate)
formula for the optimum r, one should simply plug
all values of r in the range [k, 2k] into the running-
time formula in order to determine the best; doing
so takes negligible time.
5 Decoding with Integer Linear
Programming
In this section, we show how to formulate the
selection-and-ordering problem in the ILP frame-
work. We represent each edge (i, j) from vertex i
to vertex j with an indicator variable Ii,j that is set
to 1 if the edge is selected for the optimal path and 0
otherwise. In addition, the associated weight of the
edge is represented by a constant wi,j .
The objective is then to maximize the following
sum:
max
I
?
i?V
?
j?V
wi,jIi,j (1)
This sum combines the weights of edges selected to
be on the optimal path.
To ensure that the selected edges form a valid
acyclic path starting at s and ending at t, we intro-
duce the following constraints:
Source-Sink Constraints Exactly one edge orig-
inating at source s is selected:
?
j?V
Is,j = 1 (2)
Exactly one edge ending at sink t is selected:
?
i?V
Ii,t = 1 (3)
Length Constraint Exactly k + 1 edges are se-
lected: ?
i?V
?
j?V
Ii,j = k + 1 (4)
The k + 1 selected edges connect k + 2 vertices in-
cluding s and t.
Balance Constraints Every vertex v ? V has in-
degree equal to its out-degree:
?
i?V
Ii,v =
?
i?V
Iv,j ? v ? V ? (5)
Note that with this constraint, a vertex can have at
most one outgoing and one incoming edge.
Redundancy Constraints To control for redun-
dancy, we require that at most one representative
from each equivalence class is selected. Let Z be
a set of vertices that belong to the same equivalence
class. For every equivalence class Z, we force the
total out-degree of all vertices in Z to be at most 1.
448
s t
Figure 1: A subgraph that contains a cycle, while
satisfying constraints 2 through 5.
?
i?Z
?
j?V
Ii,j ? 1 ? Z ? V (6)
Acyclicity Constraints The constraints intro-
duced above do not fully prohibit the presence of
cycles in the selected subgraph. Figure 1 shows an
example of a selected subgraph that contains a cycle
while satisfying all the above constraints.
We force acyclicity with an additional set of vari-
ables. The variables fi,j are intended to number the
edges on the path from 1 to k+ 1, with the first edge
getting number fi,j = k + 1, and the last getting
number fi,j = 1. All other edges will get fi,j = 0.
To enforce this, we start by ensuring that only the
edges selected for the path (Ii,j = 1) get nonzero
f -values:
0 ? fi,j ? (k + 1) Ii,j ? i, j ? V (7)
When Ii,j = 0, this constraint forces fi,j = 0.
When Ii,j = 1, this allows 0 ? fi,j ? k+1. Now we
introduce additional variables and constraints. We
constrain demand variables dv by:
dv =
?
i?V
Ii,v ? v ? V ? ? {s} (8)
The right hand side sums the number of selected
edges entering v, and will therefore be either 0 or 1.
Next we add variables av and bv constrained by the
equations:
av =
?
i?V
fi,v (9)
bv =
?
i?V
fv,i (10)
Note that av sums over f values on all edges enter-
ing v. However, by the previous constraints those
f -values can only be nonzero on the (at most one)
selected edge entering v. So, av is simply the f -
value on the selected edge entering v, if one exists,
and 0 otherwise. Similarly, bv is the f -value on the
(at most one) selected edge leaving v.
Finally, we add the constraints
av ? bv = dv v 6= s (11)
bs = k + 1 (12)
at = 1 (13)
These last constraints let us argue, by induction, that
a path of length exactly k + 1 must run from s to t,
as follows. The previous constraints forced exactly
one edge leaving s, to some vertex v, to be selected.
The constraint bs = k+ 1 means that the f -value on
this edge must be k + 1. The balance constraint on
v means some edge must be selected leaving v. The
constraint av ? bv = dv means this edge must have
f -value k. The argument continues the same way,
building up a path. The balance constraints mean
that the path must terminate at t, and the constraint
that at = 1 forces that termination to happen after
exactly k + 1 edges.4
For those familiar with max-flow, our program
can be understood as follows. The variables I force
a flow, of value 1, from s to t. The variables f rep-
resent a flow with supply k + 1 at s and demand dv
at v, being forced to obey ?capacity constraints? that
let the flow travel only along edges with I = 1.
6 Experimental Set-Up
Task We applied our decoding algorithm to the task
of title generation. This task has been extensively
studied over the last six years (Banko et al, 2000; Jin
and Hauptmann, 2001). Title generation is a classic
selection-and-ordering problem: during title realiza-
tion, an algorithm has to take into account both the
likelihood of words appearing in the title and their
ordering preferences. In the previous approaches,
beam search has been used for decoding. Therefore,
it is natural to explore more sophisticated decoding
techniques like the ones described in this paper.
Our method for estimation of selection-and-
ordering preferences is based on the technique de-
scribed in (Banko et al, 2000). We compute the
4The network flow constraints allow us to remove the previ-
ously placed length constraint.
449
likelihood of a word in the document appearing in
the title using a maximum entropy classifier. Every
stem is represented by commonly used positional
and distributional features, such as location of the
first sentence that contains the stem and its TF*IDF.
We estimate the ordering preferences using a bigram
language model with Good-Turing smoothing.
In previous systems, the title length is either pro-
vided to a decoder as a parameter, or heuristics are
used to determine it. Since exploration of these
heuristics is not the focus of our paper, we provide
the decoder with the actual title length (as measured
by the number of content words).
Graph Construction We construct a decoding
graph in the following fashion. Every unique con-
tent word comprises a vertex in the graph. All the
morphological variants of a stem belong to the same
equivalence class. An edge (v, u) in the graph en-
codes the selection preference of u and the likeli-
hood of the transition from v to u.
Note that the graph does not contain any auxiliary
words in its vertices. We handle the insertion of aux-
iliary words by inserting additional edges. For every
auxiliary word x, we add one edge representing the
transition from v to u via x, and the selection pref-
erence of u. The auxiliary word set consists of 24
prepositions and articles extracted from the corpus.
Corpus Our corpus consists of 547 sections of a
commonly used undergraduate algorithms textbook.
The average section contains 609.2 words. A title,
on average, contains 3.7 words, among which 3.0 are
content words; the shortest and longest titles have 1
and 13 words respectively. Our training set consists
of the first 382 sections, the remaining 165 sections
are used for testing. The bigram language model is
estimated from the body text of all sections in the
corpus, consisting of 461,351 tokens.
To assess the importance of the acyclicity con-
straint, we compute the number of titles that have
repeated content words. The empirical findings sup-
port our assumption: 97.9% of the titles do not con-
tain repeated words.
Decoding Algorithms We consider three decod-
ing algorithms: our color-coding algorithm, ILP, and
beam search.5 The beam search algorithm can only
5The combination of the acyclicity and path length con-
straints require an exponential number of states for A* since
each state has to preserve the history information. This prevents
consider vertices which are not already in the path.6
To solve the ILP formulations, we employ a
Mixed Integer Programming solver lp solve which
implements the Branch-and-Bound algorithm. We
implemented the rest of the decoders in Python with
the Psyco speed-up module. We put substantial ef-
fort to optimize the performance of all of the al-
gorithms. The color-coding algorithm is imple-
mented using parallelized computation of coloring
iterations.
7 Results
Table 1 shows the performance of various decoding
algorithms considered in the paper. We first evalu-
ate each algorithm by the running times it requires
to find all the optimal solutions on the test set. Since
ILP is guaranteed to find the optimal solution, we
can use its output as a point of comparison. Table 1
lists both the average and the median running times.
For some of the decoding algorithms, the difference
between the two measurements is striking ? 6,536
seconds versus 57.3 seconds for ILP. This gap can be
explained by outliers which greatly increase the av-
erage running time. For instance, in the worst case,
ILP takes an astounding 136 hours to find the opti-
mal solution. Therefore, we base our comparison on
the median running time.
The color-coding algorithm requires a median
time of 9.7 seconds to find an optimal solution com-
pared to the 57.3 seconds taken by ILP. Furthermore,
as Figure 2 shows, the algorithm converges quickly:
just eleven iterations are required to find an optimal
solution in 90% of the titles, and within 35 itera-
tions all of the solutions are found. An alternative
method for finding optimal solutions is to employ a
beam search with a large beam size. We found that
for our test set, the smallest beam size that satisfies
this condition is 1345, making it twenty-three times
slower than the randomized decoder with respect to
the median running time.
Does the decoding accuracy impact the quality of
the generated titles? We can always trade speed for
accuracy in heuristic search algorithms. As an ex-
treme, consider a beam search with a beam of size
1: while it is very fast with a median running time
us from applying A* to this problem.
6Similarly, we avoid redundancy by disallowing two vertices
from the same equivalence class to belong to the same path.
450
Average (s) Median (s) ROUGE-L Optimal Solutions (%)
Beam 1 0.6 0.4 0.0234 0.0
Beam 80 28.4 19.3 0.2373 64.8
Beam 1345 368.6 224.4 0.2556 100.0
ILP 6,536.2 57.3 0.2556 100.0
Color-coding 73.8 9.7 0.2556 100.0
Table 1: Running times in seconds, ROUGE scores, and percentage of optimal solutions found for each of
the decoding algorithms.
 0
 20
 40
 60
 80
 100
 0  5  10  15  20  25  30  35
Ex
ac
t S
ol
ut
io
ns
 (%
)
Iterations
Figure 2: The proportion of exact solutions found
for each iteration of the color coding algorithm.
of less than one second, it is unable to find any of
the optimal solutions. The titles generated by this
method have substantially lower scores than those
produced by the optimal decoder, yielding a 0.2322
point difference in ROUGE scores. Even a larger
beam size such as 80 (as used by Banko et al (2000))
does not match the title quality of the optimal de-
coder.
8 Conclusions
In this paper, we formalized the decoding task for
selection-and-ordering as a problem of finding the
highest-weighted acyclic path in a directed graph.
The presented decoding algorithm employs random-
ized color-coding, and can closely approximate the
ILP performance, without blowing up the running
time. The algorithm has been tested on title genera-
tion, but the decoder is not specific to this task and
can be applied to other generation and summariza-
tion applications.
9 Acknowledgements
The authors acknowledge the support of the Na-
tional Science Foundation (CAREER grant IIS-
0448168 and grant IIS-0415865). We also would
like to acknowledge the MIT NLP group and the
anonymous reviewers for valuable comments.
References
N. Alon, R. Yuster, U. Zwick. 1995. Color-coding. Jour-
nal of the ACM (JACM), 42(4):844?856.
B. Awerbuch, Y. Azar, A. Blum, S. Vempala. 1995.
Improved approximation guarantees for minimum-
weight k-trees and prize-collecting salesmen. In Pro-
ceedings of the STOC, 277?283.
E. Balas. 1989. The prize collecting traveling salesman
problem. Networks, 19:621?636.
M. Banko, V. O. Mittal, M. J. Witbrock. 2000. Headline
generation based on statistical translation. In Proceed-
ings of the ACL, 318?325.
S. Corston-Oliver, M. Gamon, E. Ringger, R. Moore.
2002. An overview of amalgam: A machine-learned
generation module. In Proceedings of INLG, 33?40.
R. G. Downey, M. R. Fellows. 1995. Fixed-parameter
tractability and completeness II: On completeness for
W [1]. Theoretical Computer Science, 141(1?2):109?
131.
J. Eisner, R. W. Tromble. 2006. Local search with very
large-scale neighborhoods for optimal permutations
in machine translation. In Proceedings of the HLT-
NAACL Workshop on Computationally Hard Problems
and Joint Inference in Speech and Language Process-
ing.
U. Germann, M. Jahr, K. Knight, D. Marcu, K. Yamada.
2001. Fast decoding and optimal decoding for ma-
chine translation. In Proceedings of the EACL/ACL,
228?235.
F. Jelinek. 1969. A fast sequential decoding algorithm
using a stack. IBM Research Journal of Research and
Development.
R. Jin, A. G. Hauptmann. 2001. Automatic title genera-
tion for spoken broadcast news. In Proceedings of the
HLT, 1?3.
R. Motwani, P. Raghavan. 1995. Randomized Algo-
rithms. Cambridge University Press, New York, NY.
D. Roth, W. Yih. 2004. A linear programming formula-
tion for global inference in natural language tasks. In
Proceedings of the CONLL, 1?8.
451
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 544?551,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generating a Table-of-Contents
S.R.K. Branavan, Pawan Deshpande and Regina Barzilay
Massachusetts Institute of Technology
{branavan, pawand, regina}@csail.mit.edu
Abstract
This paper presents a method for the auto-
matic generation of a table-of-contents. This
type of summary could serve as an effec-
tive navigation tool for accessing informa-
tion in long texts, such as books. To gen-
erate a coherent table-of-contents, we need
to capture both global dependencies across
different titles in the table and local con-
straints within sections. Our algorithm ef-
fectively handles these complex dependen-
cies by factoring the model into local and
global components, and incrementally con-
structing the model?s output. The results of
automatic evaluation and manual assessment
confirm the benefits of this design: our sys-
tem is consistently ranked higher than non-
hierarchical baselines.
1 Introduction
Current research in summarization focuses on pro-
cessing short articles, primarily in the news domain.
While in practice the existing summarization meth-
ods are not limited to this material, they are not
universal: texts in many domains and genres can-
not be summarized using these techniques. A par-
ticularly significant challenge is the summarization
of longer texts, such as books. The requirement
for high compression rates and the increased need
for the preservation of contextual dependencies be-
tween summary sentences places summarization of
such texts beyond the scope of current methods.
In this paper, we investigate the automatic gener-
ation of tables-of-contents, a type of indicative sum-
mary particularly suited for accessing information in
long texts. A typical table-of-contents lists topics
described in the source text and provides informa-
tion about their location in the text. The hierarchical
organization of information in the table further re-
fines information access by specifying the relations
between different topics and providing rich contex-
tual information during browsing. Commonly found
in books, tables-of-contents can also facilitate access
to other types of texts. For instance, this type of
summary could serve as an effective navigation tool
for understanding a long, unstructured transcript for
an academic lecture or a meeting.
Given a text, our goal is to generate a tree wherein
a node represents a segment of text and a title that
summarizes its content. This process involves two
tasks: the hierarchical segmentation of the text, and
the generation of informative titles for each segment.
The first task can be addressed by using the hier-
archical structure readily available in the text (e.g.,
chapters, sections and subsections) or by employ-
ing existing topic segmentation algorithms (Hearst,
1994). In this paper, we take the former approach.
As for the second task, a naive approach would be to
employ existing methods of title generation to each
segment, and combine the results into a tree struc-
ture.
However, the latter approach cannot guarantee
that the generated table-of-contents forms a coher-
ent representation of the entire text. Since titles of
different segments are generated in isolation, some
of the generated titles may be repetitive. Even non-
repetitive titles may not provide sufficient informa-
tion to discriminate between the content of one seg-
544
Scientific computing
Remarkable recursive algorithm for multiplying matrices
Divide and conquer algorithm design
Making a recursive algorithm
Solving systems of linear equations
Computing an LUP decomposition
Forward and back substitution
Symmetric positive definite matrices and least squares approximation
Figure 1: A fragment of a table-of-contents generated by our method.
ment and another. Therefore, it is essential to gen-
erate an entire table-of-contents tree in a concerted
fashion.
This paper presents a hierarchical discriminative
approach for table-of-contents generation. Figure 1
shows a fragment of a table-of-contents automat-
ically generated by this algorithm. Our method
has two important points of departure from exist-
ing techniques. First, we introduce a structured dis-
criminative model for table-of-contents generation
that accounts for a wide range of phrase-based and
collocational features. The flexibility of this model
results in improved summary quality. Second, our
model captures both global dependencies across dif-
ferent titles in the tree and local dependencies within
sections. We decompose the model into local and
global components that handle different classes of
dependencies. We further reduce the search space
through incremental construction of the model?s out-
put by considering only the promising parts of the
decision space.
We apply our method to process a 1,180 page al-
gorithms textbook. To assess the contribution of our
hierarchical model, we compare our method with
state-of-the-art methods that generate each segment
title independently.1 The results of automatic eval-
uation and manual assessment of title quality show
that the output of our system is consistently ranked
higher than that of non-hierarchical baselines.
2 Related Work
Although most current research in summarization
focuses on newspaper articles, a number of ap-
proaches have been developed for processing longer
texts. Most of these approaches are tailored to a par-
1The code and feature vector data for
our model and the baselines are available at
http://people.csail.mit.edu/branavan/code/toc.
ticular domain, such as medical literature or scien-
tific articles. By making strong assumptions about
the input structure and the desired format of the out-
put, these methods achieve a high compression rate
while preserving summary coherence. For instance,
Teufel and Moens (2002) summarize scientific arti-
cles by selecting rhetorical elements that are com-
monly present in scientific abstracts. Elhadad and
McKeown (2001) generate summaries of medical ar-
ticles by following a certain structural template in
content selection and realization.
Our work, however, is closer to domain-
independent methods for summarizing long texts.
Typically, these approaches employ topic segmen-
tation to identify a list of topics described in a
document, and then produce a summary for each
part (Boguraev and Neff, 2000; Angheluta et al,
2002). In contrast to our method, these approaches
perform either sentence or phrase extraction, rather
than summary generation. Moreover, extraction for
each segment is performed in isolation, and global
constraints on the summary are not enforced.
Finally, our work is also related to research on ti-
tle generation (Banko et al, 2000; Jin and Haupt-
mann, 2001; Dorr et al, 2003). Since work in this
area focuses on generating titles for one article at a
time (e.g., newspaper reports), the issue of hierarchi-
cal generation, which is unique to our task, does not
arise. However, this is not the only novel aspect of
the proposed approach. Our model learns title gener-
ation in a fully discriminative framework, in contrast
to the commonly used noisy-channel model. Thus,
instead of independently modeling the selection and
grammaticality constraints, we learn both types of
features in a single framework. This joint training
regime supports greater flexibility in modeling fea-
ture interaction.
545
3 Problem Formulation
We formalize the problem of table-of-contents gen-
eration as a supervised learning task where the goal
is to map a tree of text segments S to a tree of titles
T . A segment may correspond to a chapter, section
or subsection.
Since the focus of our work is on the generation
aspect of table-of-contents construction, we assume
that the hierarchical segmentation of a text is pro-
vided in the input. This division can either be au-
tomatically computed using one of the many avail-
able text segmentation algorithms (Hearst, 1994), or
it can be based on demarcations already present in
the input (e.g., paragraph markers).
During training, the algorithm is provided with a
set of pairs (Si, T i) for i = 1, . . . , p, where Si is
the ith tree of text segments, and T i is the table-of-
contents for that tree. During testing, the algorithm
generates tables-of-contents for unseen trees of text
segments.
We also assume that during testing the desired
title length is provided as a parameter to the algo-
rithm.
4 Algorithm
To generate a coherent table-of-contents, we need
to take into account multiple constraints: the titles
should be grammatical, they should adequately rep-
resent the content of their segments, and the table-
of-contents as a whole should clearly convey the re-
lations between the segments. Taking a discrimina-
tive approach for modeling this task would allow us
to achieve this goal: we can easily integrate a range
of constraints in a flexible manner. Since the num-
ber of possible labels (i.e., tables-of-contents) is pro-
hibitively large and the labels themselves exhibit a
rich internal structure, we employ a structured dis-
criminative model that can easily handle complex
dependencies. Our solution relies on two orthogo-
nal strategies to balance the tractability and the rich-
ness of the model. First, we factor the model into
local and global components. Second, we incremen-
tally construct the output of each component using
a search-based discriminative algorithm. Both of
these strategies have the effect of intelligently prun-
ing the decision space.
Our model factorization is driven by the different
types of dependencies which are captured by the two
components. The first model is local: for each seg-
ment, it generates a list of candidate titles ranked by
their individual likelihoods. This model focuses on
grammaticality and word selection constraints, but it
does not consider relations among different titles in
the table-of-contents. These latter dependencies are
captured in the global model that constructs a table-
of-contents by selecting titles for each segment from
the available candidates. Even after this factoriza-
tion, the decision space for each model is large: for
the local model, it is exponential in the length of the
segment title, and for the global model it is exponen-
tial in the size of the tree.
Therefore, we construct the output for each of
these models incrementally using beam search. The
algorithm maintains the most promising partial out-
put structures, which are extended at every itera-
tion. The model incorporates this decoding pro-
cedure into the training process, thereby learning
model parameters best suited for the specific decod-
ing algorithm. Similar models have been success-
fully applied in the past to other tasks including pars-
ing (Collins and Roark, 2004), chunking (Daume?
and Marcu, 2005), and machine translation (Cowan
et al, 2006).
4.1 Model Structure
The model takes as input a tree of text segments S.
Each segment s ? S and its title z are represented
as a local feature vector ?loc(s, z). Each compo-
nent of this vector stores a numerical value. This
feature vector can track any feature of the segment s
together with its title z. For instance, the ith compo-
nent of this vector may indicate whether the bigram
(z[j]z[j+ 1]) occurs in s, where z[j] is the jth word
in z:
(?loc(s, z))i =
{
1 if (z[j]z[j + 1]) ? s
0 otherwise
In addition, our model captures dependencies
among multiple titles that appear in the same table-
of-contents. We represent a tree of segments S
paired with titles T with the global feature vector
?glob(S, T ). The components here are also numer-
ical features. For example, the ith component of the
vector may indicate whether a title is repeated in the
table-of-contents T :
546
(?glob(S, T ))i =
{
1 repeated title
0 otherwise
Our model constructs a table-of-contents in two
basic steps:
Step One The goal of this step is to generate a
list of k candidate titles for each segment s ? S.
To do so, for each possible title z, the model maps
the feature vector ?loc(s, z) to a real number. This
mapping can take the form of a linear model,
?loc(s, z) ? ?loc
where ?loc is the local parameter vector.
Since the number of possible titles is exponen-
tial, we cannot consider all of them. Instead, we
prune the decision space by incrementally construct-
ing promising titles. At each iteration j, the algo-
rithm maintains a beam Q of the top k partially gen-
erated titles of length j. During iteration j + 1, a
new set of candidates is grown by appending a word
from s to the right of each member of the beam Q.
We then sort the entries in Q: z1, z2, . . . such that
?loc(s, zi) ??loc ? ?loc(s, zi+1) ??loc, ?i. Only the
top k candidates are retained, forming the beam for
the next iteration. This process continues until a title
of the desired length is generated. Finally, the list of
k candidates is returned.
Step Two Given a set of candidate titles
z1, z2, . . . , zk for each segment s ? S, our goal is
to construct a table-of-contents T by selecting the
most appropriate title from each segment?s candi-
date list. To do so, our model computes a score for
the pair (S, T ) based on the global feature vector
?glob(S, T ):
?glob(S, T ) ? ?glob
where ?glob is the global parameter vector.
As with the local model (step one), the num-
ber of possible tables-of-contents is too large to be
considered exhaustively. Therefore, we incremen-
tally construct a table-of-contents by traversing the
tree of segments in a pre-order walk (i.e., the or-
der in which segments appear in the text). In this
case, the beam contains partially generated tables-
of-contents, which are expanded by one segment ti-
tle at a time. To further reduce the search space,
during decoding only the top five candidate titles for
a segment are given to the global model.
4.2 Training the Model
Training for Step One We now describe how the
local parameter vector ?loc is estimated from train-
ing data. We are given a set of training examples
(si, yi) for i = 1, . . . , l, where si is the ith text seg-
ment, and yi is the title of this segment.
This linear model is learned using a variant of
the incremental perceptron algorithm (Collins and
Roark, 2004; Daume? and Marcu, 2005). This on-
line algorithm traverses the training set multiple
times, updating the parameter vector ?loc after each
training example in case of mis-predictions. The al-
gorithm encourages a setting of the parameter vector
?loc that assigns the highest score to the feature vec-
tor associated with the correct title.
The pseudo-code of the algorithm is shown in Fig-
ure 2. Given a text segment s and the corresponding
title y, the training algorithm maintains a beam Q
containing the top k partial titles of length j. The
beam is updated on each iteration using the func-
tions GROW and PRUNE. For every word in seg-
ment s and for every partial title in Q, GROW cre-
ates a new title by appending this word to the title.
PRUNE retains only the top ranked candidates based
on the scoring function ?loc(s, z) ??loc. If y[1 . . . j]
(i.e., the prefix of y of length j) is not in the modi-
fied beam Q, then ?loc is updated2 as shown in line
4 of the pseudo-code in Figure 2. In addition, Q is
replaced with a beam containing only y[1 . . . j] (line
5). This process is performed |y| times. We repeat
this process for all training examples over 50 train-
ing iterations. 3
Training for Step Two To train the global param-
eter vector ?glob, we are given training examples
(Si, T i) for i = 1, . . . , p, where Si is the ith tree of
text segments, and T i is the table-of-contents for that
tree. However, we cannot directly use these tables-
of-contents for training our global model: since this
model selects one of the candidate titles zi1, . . . , z
i
k
returned by the local model, the true title of the seg-
ment may not be among these candidates. There-
fore, to determine a new target title for the segment,
we need to identify the title in the set of candidates
2If the word in the jth position of y does not occur in s, then
the parameter update is not performed.
3For decoding, ?loc is averaged over the training iterations
as in Collins and Roark (2004).
547
s ? segment text.
y ? segment title.
y[1 . . . j] ? prefix of y of length j.
Q ? beam containing partial titles.
1. for j = 1 . . . |y|
2. Q = PRUNE(GROW(s,Q))
3. if y[1 . . . j] /? Q
4. ?loc = ?loc + ?loc(s, y[1 . . . j])
?
?
z?Q
?loc(s,z)
|Q|
5. Q = {y[1 . . . j]}
Figure 2: The training algorithm for the local model.
that is closest to the true title.
We employ the L1 distance measure to compare
the content word overlap between two titles.4 For
each input (S, T ), and each segment s ? S, we iden-
tify the segment title closest in the L1 measure to the
true title y5:
z? = arg min
i
L1(zi, y)
Once all the training targets in the corpus have
been identified through this procedure, the global
linear model ?glob(S, T ) ??glob is learned using the
same perceptron algorithm as in step one. Rather
than maintaining the beam of partially generated ti-
tles, the beam Q holds partially generated tables-of-
contents. Also, the loop in line 1 of Figure 2 iterates
over segment titles rather than words. The global
model is trained over 200 iterations.
5 Features
Local Features Our local model aims to generate
titles which adequately represent the meaning of the
segment and are grammatical. Selection and contex-
tual preferences are encoded in the local features.
The features that capture selection constraints are
specified at the word level, and contextual features
are expressed at the word sequence level.
The selection features capture the position of the
word, its TF*IDF, and part-of-speech information.
In addition, they also record whether the word oc-
curs in the body of neighboring segments. We also
4This measure is close to ROUGE-1 which in addition con-
siders the overlap in auxiliary words.
5In the case of ties, one of the titles is picked arbitrarily.
Segment has the same title as its sibling
Segment has the same title as its parent
Two adjacent sibling titles have the same head
Two adjacent sibling titles start with the same word
Rank given to the title by the local model
Table 1: Examples of global features.
generate conjunctive features by combining features
of different types.
The contextual features record the bigram and tri-
gram language model scores, both for words and for
part-of-speech tags. The trigram scores are aver-
aged over the title. The language models are trained
using the SRILM toolkit. Another type of contex-
tual feature models the collocational properties of
noun phrases in the title. This feature aims to elim-
inate generic phrases, such as ?the following sec-
tion? from the generated titles.6 To achieve this ef-
fect, for each noun phrase in the title, we measure
the ratio of their frequency in the segment to their
frequency in the corpus.
Global Features Our global model describes the
interaction between different titles in the tree (See
Table 1). These interactions are encoded in three
types of global features. The first type of global
feature indicates whether titles in the tree are re-
dundant at various levels of the tree structure. The
second type of feature encourages parallel construc-
tions within the same tree. For instance, titles of ad-
joining segments may be verbalized as noun phrases
with the same head (e.g., ?Bubble sort algorithm?,
?Merge sort algorithm?). We capture this property
by comparing words that appear in certain positions
in adjacent sibling titles. Finally, our global model
also uses the rank of the title provided by the local
model. This feature enables the global model to ac-
count for the preferences of the local model in the
title selection process.
6 Evaluation Set-Up
Data We apply our method to an undergraduate al-
gorithms textbook. For detailed statistics on the data
see Table 2. We split its table-of-contents into a set
6Unfortunately, we could not use more sophisticated syntac-
tic features due to the low accuracy of statistical parsers on our
corpus.
548
Number of Titles 540
Number of Trees 39
Tree Depth 4
Number of Words 269,650
Avg. Title Length 3.64
Avg. Branching 3.29
Avg. Title Duplicates 21
Table 2: Statistics on the corpus used in the experi-
ments.
of independent subtrees. Given a table-of-contents
of depth n with a root branching factor of r, we gen-
erate r subtrees, with a depth of at most n ? 1. We
randomly select 80% of these trees for training, and
the rest are used for testing. In our experiments, we
use ten different randomizations to compensate for
the small number of available trees.
Admittedly, this method of generating training
and testing data omits some dependencies at the
level of the table-of-contents as a whole. However,
the subtrees used in our experiments still exhibit
a sufficiently deep hierarchical structure, rich with
contextual dependencies.
Baselines As an alternative to our hierarchical dis-
criminative method, we consider three baselines that
build a table-of-contents by generating a title for
each segment individually, without taking into ac-
count the tree structure, and one hierarchical gener-
ative baseline. The first method generates a title for a
segment by selecting the noun phrase from that seg-
ment with the highest TF*IDF. This simple method
is commonly used to generate keywords for brows-
ing applications in information retrieval, and has
been shown to be effective for summarizing techni-
cal content (Wacholder et al, 2001).
The second baseline is based on the noisy-channel
generative (flat generative, FG) model proposed by
Banko et al, (2000). Similar to our local model,
this method captures both selection and grammati-
cal constraints. However, these constraints are mod-
eled separately, and then combined in a generative
framework.
We use our local model (Flat Discriminative
model, FD) as the third baseline. Like the second
baseline, this model omits global dependencies, and
only focuses on features that capture relations within
individual segments.
In the hierarchical generative (HG) baseline we
run our global model on the ranked list of titles pro-
duced for each section by the noisy-channel genera-
tive model.
The last three baselines and our algorithm are pro-
vided with the title length as a parameter. In our
experiments, the algorithms use the reference title
length.
Experimental Design: Comparison with refer-
ence tables-of-contents Reference based evalu-
ation is commonly used to assess the quality of
machine-generated headlines (Wang et al, 2005).
We compare our system?s output with the table-of-
contents from the textbook using ROUGE metrics.
We employ a publicly available software package,7
with all the parameters set to default values.
Experimental Design: Human assessment The
judges were each given 30 segments randomly se-
lected from a set of 359 test segments. For each test
segment, the judges were presented with its text, and
3 alternative titles consisting of the reference and
the titles produced by the hierarchical discriminative
model, and the best performing baseline. In addi-
tion, the judges had access to all of the segments in
the book. A total of 498 titles for 166 unique seg-
ments were ranked. The system identities were hid-
den from the judges, and the titles were presented in
random order. The judges ranked the titles based on
how well they represent the content of the segment.
Titles were ranked equal if they were judged to be
equally representative of the segment.
Six people participated in this experiment. All the
participants were graduate students in computer sci-
ence who had taken the algorithms class in the past
and were reasonably familiar with the material.
7 Results
Figure 3 shows fragments of the tables-of-contents
generated by our method and the four baselines
along with the reference counterpart. These extracts
illustrate three general phenomena that we observed
in the test corpus. First, the titles produced by key-
word extraction exhibit a high degree of redundancy.
In fact, 40% of the titles produced by this method are
repeated more than once in the table-of-contents. In
7http://www.isi.edu/licensed-sw/see/rouge/
549
Reference:
hash tables
direct address tables
hash tables
collision resolution by chaining
analysis of hashing with chaining
open addressing
linear probing
quadratic probing
double hashing
Flat Generative:
linked list
worst case time
wasted space
worst case running time
to show that there are
dynamic set
occupied slot
quadratic function
double hashing
Flat Discriminative:
dictionary operations
universe of keys
computer memory
element in the list
hash table with load factor
hash table
hash function
hash function
double hashing
Keyword Extraction:
hash table
dynamic set
hash function
worst case
expected number
hash table
hash function
hash table
double hashing
Hierarchical Generative:
dictionary operations
worst case time
wasted space
worst case running time
to show that there are
collision resolution
linear time
quadratic function
double hashing
Hierarchical Discriminative:
dictionary operations
direct address table
computer memory
worst case running time
hash table with load factor
address table
hash function
quadratic probing
double hashing
Figure 3: Fragments of tables-of-contents generated by our method and the four baselines along with the
corresponding reference.
Rouge-1 Rouge-L Rouge-W Full Match
HD 0.256 0.249 0.216 13.5
FD 0.241 0.234 0.203 13.1
HG 0.139 0.133 0.117 5.8
FG 0.094 0.090 0.079 4.1
Keyword 0.168 0.168 0.157 6.3
Table 3: Title quality as compared to the reference
for the hierarchical discriminative (HD), flat dis-
criminative (FD), hierarchical generative (HG), flat
generative (FG) and Keyword models. The improve-
ment given by HD over FD in all three Rouge mea-
sures is significant at p ? 0.03 based on the Sign
test.
better worse equal
HD vs. FD 68 32 49
Reference vs. HD 115 13 22
Reference vs. FD 123 7 20
Table 4: Overall pairwise comparisons of the rank-
ings given by the judges. The improvement in ti-
tle quality given by HD over FD is significant at
p ? 0.0002 based on the Sign test.
contrast, our method yields 5.5% of the titles as du-
plicates, as compared to 9% in the reference table-
of-contents.8
Second, the fragments show that the two discrim-
inative models ? Flat and Hierarchical ? have a
number of common titles. However, adding global
dependencies to rerank titles generated by the local
model changes 30% of the titles in the test set.
Comparison with reference tables-of-contents
Table 3 shows the average ROUGE scores over
the ten randomizations for the five automatic meth-
ods. The hierarchical discriminative method consis-
tently outperforms the four baselines according to
all ROUGE metrics.
At the same time, these results also show that only
a small ratio of the automatically generated titles
are identical to the reference ones. In some cases,
the machine-generated titles are very close in mean-
ing to the reference, but are verbalized differently.
Examples include pairs such as (?Minimum Span-
ning Trees?, ?Spanning Tree Problem?) and (?Wal-
lace Tree?, ?Multiplication Circuit?).9 While mea-
sures like ROUGE can capture the similarity in the
first pair, they cannot identify semantic proximity
8Titles such as ?Analysis? and ?Chapter Outline? are re-
peated multiple times in the text.
9A Wallace Tree is a circuit that multiplies two integers.
550
between the titles in the second pair. Therefore,
we supplement the results of this experiment with
a manual assessment of title quality as described be-
low.
Human assessment We analyze the human rat-
ings by considering pairwise comparisons between
the models. Given two models, A and B, three out-
comes are possible: A is better than B, B is bet-
ter than A, or they are of equal quality. The re-
sults of the comparison are summarized in Table 4.
These results indicate that using hierarchical infor-
mation yields statistically significant improvement
(at p ? 0.0002 based on the Sign test) over a flat
counterpart.
8 Conclusion and Future Work
This paper presents a method for the automatic gen-
eration of a table-of-contents. The key strength of
our method lies in its ability to track dependencies
between generation decisions across different levels
of the tree structure. The results of automatic evalu-
ation and manual assessment confirm the benefits of
joint tree learning: our system is consistently ranked
higher than non-hierarchical baselines.
We also plan to expand our method for the task
of slide generation. Like tables-of-contents, slide
bullets are organized in a hierarchical fashion and
are written in relatively short phrases. From the
language viewpoint, however, slides exhibit more
variability and complexity than a typical table-of-
contents. To address this challenge, we will explore
more powerful generation methods that take into ac-
count syntactic information.
Acknowledgments
The authors acknowledge the support of the Na-
tional Science Foundation (CAREER grant IIS-
0448168 and grant IIS-0415865). We would also
like to acknowledge the many people who took part
in human evaluations. Thanks to Michael Collins,
Benjamin Snyder, Igor Malioutov, Jacob Eisenstein,
Luke Zettlemoyer, Terry Koo, Erdong Chen, Zo-
ran Dzunic and the anonymous reviewers for helpful
comments and suggestions. Any opinions, findings,
conclusions or recommendations expressed above
are those of the authors and do not necessarily re-
flect the views of the NSF.
References
Roxana Angheluta, Rik De Busser, and Marie-Francine
Moens. 2002. The use of topic segmentation for auto-
matic summarization. In Proceedings of the ACL-2002
Workshop on Automatic Summarization.
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statistical
translation. In Proceedings of the ACL, pages 318?
325.
Branimir Boguraev and Mary S. Neff. 2000. Discourse
segmentation in aid of document summarization. In
Proceedings of the 33rd Hawaii International Confer-
ence on System Sciences, pages 3004?3014.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the ACL, pages 111?118.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the EMNLP, pages 232?241.
Hal Daume? and Daniel Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proceedings of the ICML,
pages 169?176.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop, pages 1?8.
Noemie Elhadad and Kathleen R. McKeown. 2001. To-
wards generating patient specific summaries of med-
ical articles. In Proceedings of NAACL Workshop on
Automatic Summarization, pages 31?39.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of the ACL, pages 9?
16.
Rong Jin and Alexander G. Hauptmann. 2001. Auto-
matic title generation for spoken broadcast news. In
Proceedings of the HLT, pages 1?3.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Nina Wacholder, David K. Evans, and Judith Klavans.
2001. Automatic identification and organization of in-
dex terms for interactive browsing. In JCDL, pages
126?134.
R. Wang, J. Dunnion, and J. Carthy. 2005. Machine
learning approach to augmenting news headline gen-
eration. In Proceedings of the IJCNLP.
551
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 189?198,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Inducing Temporal Graphs
Philip Bramsen Pawan Deshpande Yoong Keok Lee Regina Barzilay
MIT CSAIL MIT CSAIL DSO National Laboratories MIT CSAIL
bramsen@mit.edu pawan@mit.edu lyoongke@dso.org.sg regina@csail.mit.edu
Abstract
We consider the problem of constructing
a directed acyclic graph that encodes tem-
poral relations found in a text. The unit of
our analysis is a temporal segment, a frag-
ment of text that maintains temporal co-
herence. The strength of our approach lies
in its ability to simultaneously optimize
pairwise ordering preferences and global
constraints on the graph topology. Our
learning method achieves 83% F-measure
in temporal segmentation and 84% accu-
racy in inferring temporal relations be-
tween two segments.
1 Introduction
Understanding the temporal flow of discourse is
a significant aspect of text comprehension. Con-
sequently, temporal analysis has been a focus of
linguistic research for quite some time. Tem-
poral interpretation encompasses levels ranging
from the syntactic to the lexico-semantic (Re-
ichenbach, 1947; Moens and Steedman, 1987)
and includes the characterization of temporal dis-
course in terms of rhetorical structure and prag-
matic relations (Dowty, 1986; Webber, 1987; Pas-
sonneau, 1988; Lascarides and Asher, 1993).
Besides its linguistic significance, temporal
analysis has important practical implications. In
multidocument summarization, knowledge about
the temporal order of events can enhance both the
content selection and the summary generation pro-
cesses (Barzilay et al, 2002). In question an-
swering, temporal analysis is needed to determine
when a particular event occurs and how events re-
late to each other. Some of these needs can be
addressed by emerging technologies for temporal
analysis (Wilson et al, 2001; Mani et al, 2003;
Lapata and Lascarides, 2004; Boguraev and Ando,
2005).
This paper characterizes the temporal flow of
discourse in terms of temporal segments and their
ordering. We define a temporal segment to be
a fragment of text that does not exhibit abrupt
changes in temporal focus (Webber, 1988). A seg-
ment may contain more than one event or state, but
the key requirement is that its elements maintain
temporal coherence. For instance, a medical case
summary may contain segments describing a pa-
tient?s admission, his previous hospital visit, and
the onset of his original symptoms. Each of these
segments corresponds to a different time frame,
and is clearly delineated as such in a text.
Our ultimate goal is to automatically construct
a graph that encodes ordering between temporal
segments. The key premise is that in a coherent
document, temporal progression is reflected in a
wide range of linguistic features and contextual
dependencies. In some cases, clues to segment or-
dering are embedded in the segments themselves.
For instance, given a pair of adjacent segments,
the temporal adverb next day in the second seg-
ment is a strong predictor of a precedence relation.
In other cases, we can predict the right order be-
tween a pair of segments by analyzing their rela-
tion to other segments in the text. The interaction
between pairwise ordering decisions can easily be
formalized in terms of constraints on the graph
topology. An obvious example of such a con-
straint is prohibiting cycles in the ordering graph.
We show how these complementary sources of in-
formation can be incorporated in a model using
global inference.
We evaluate our temporal ordering algorithm on
a corpus of medical case summaries. Temporal
189
analysis in this domain is challenging in several re-
spects: a typical summary exhibits no significant
tense or aspect variations and contains few abso-
lute time markers. We demonstrate that humans
can reliably mark temporal segments and deter-
mine segment ordering in this domain. Our learn-
ing method achieves 83% F-measure in temporal
segmentation and 84% accuracy in inferring tem-
poral relations between two segments.
Our contributions are twofold:
Temporal Segmentation We propose a fully
automatic, linguistically rich model for temporal
segmentation. Most work on temporal analysis
is done on a finer granularity than proposed here.
Our results show that the coarse granularity of our
representation facilitates temporal analysis and is
especially suitable for domains with sparse tempo-
ral anchors.
Segment Ordering We introduce a new method
for learning temporal ordering. In contrast to ex-
isting methods that focus on pairwise ordering, we
explore strategies for global temporal inference.
The strength of the proposed model lies in its abil-
ity to simultaneously optimize pairwise ordering
preferences and global constraints on graph topol-
ogy. While the algorithm has been applied at the
segment level, it can be used with other temporal
annotation schemes.
2 Related Work
Temporal ordering has been extensively studied
in computational linguistics (Passonneau, 1988;
Webber, 1988; Hwang and Schubert, 1992; Las-
carides and Asher, 1993; Lascarides and Ober-
lander, 1993). Prior research has investigated
a variety of language mechanisms and knowl-
edge sources that guide interpretation of tempo-
ral ordering, including tense, aspect, temporal ad-
verbials, rhetorical relations and pragmatic con-
straints. In recent years, the availability of an-
notated corpora, such as TimeBank (Pustejovsky
et al, 2003), has triggered the use of machine-
learning methods for temporal analysis (Mani et
al., 2003; Lapata and Lascarides, 2004; Boguraev
and Ando, 2005). Typical tasks include identifica-
tion of temporal anchors, linking events to times,
and temporal ordering of events.
Since this paper addresses temporal ordering,
we focus our discussion on this task. Existing or-
dering approaches vary both in terms of the or-
dering unit ? it can be a clause, a sentence or
an event ? and in terms of the set of ordering
relations considered by the algorithm. Despite
these differences, most existing methods have the
same basic design: each pair of ordering units (i.e.,
clauses) is abstracted into a feature vector and a
supervised classifier is employed to learn the map-
ping between feature vectors and their labels. Fea-
tures used in classification include aspect, modal-
ity, event class, and lexical representation. It is im-
portant to note that the classification for each pair
is performed independently and is not guaranteed
to yield a globally consistent order.
In contrast, our focus is on globally optimal
temporal inference. While the importance of
global constraints has been previously validated
in symbolic systems for temporal analysis (Fikes
et al, 2003; Zhou et al, 2005), existing corpus-
based approaches operate at the local level. These
improvements achieved by a global model moti-
vate its use as an alternative to existing pairwise
methods.
3 TDAG: A representation of temporal
flow
We view text as a linear sequence of temporal
segments. Temporal focus is retained within a
segment, but radically changes between segments.
The length of a segment can range from a single
clause to a sequence of adjacent sentences. Fig-
ure 1 shows a sample of temporal segments from
a medical case summary. Consider as an example
the segment S13 of this text. This segment de-
scribes an examination of a patient, encompassing
several events and states (i.e., an abdominal and
neurological examination). All of them belong to
the same time frame, and temporal order between
these events is not explicitly outlined in the text.
We represent ordering of events as a temporal
directed acyclic graph (TDAG). An example of the
transitive reduction1 of a TDAG is shown in Fig-
ure 1. Edges in a TDAG capture temporal prece-
dence relations between segments. Because the
graph encodes an order, cycles are prohibited. We
do not require the graph to be fully connected ? if
the precedence relation between two nodes is not
specified in the text, the corresponding nodes will
not be connected. For instance, consider the seg-
ments S5 and S7 from Figure 1, which describe
her previous tests and the history of eczema. Any
1The transitive reduction of a graph is the smallest graph
with the same transitive closure.
190
S1 S12 S13 S14
S2 S10 S6
S8
S4
S3 S5
S7
S9S11
S1 A 32-year-old woman was admitted to the hospital because of left subcostal pain...
S2 The patient had been well until four years earlier,
S5 Three months before admission an evaluation elsewhere included an ultrasonographic ex-
amination, a computed tomographic (CT) scan of the abdomen...
S7 She had a history of eczema and of asthma...
S8 She had lost 18 kg in weight during the preceding 18 months.
S13 On examination the patient was slim and appeared well. An abdominal examination re-
vealed a soft systolic bruit... and a neurologic examination was normal...
Figure 1: An example of the transitive reduction of a TDAG for a case summary. A sample of segments
corresponding to the nodes marked in bold is shown in the table.
order between the two events is consistent with our
interpretation of the text, therefore we cannot de-
termine the precedence relation between the seg-
ments S5 and S7.
In contrast to many existing temporal represen-
tations (Allen, 1984; Pustejovsky et al, 2003),
TDAG is a coarse annotation scheme: it does not
capture interval overlap and distinguishes only a
subset of commonly used ordering relations. Our
choice of this representation, however, is not ar-
bitrary. The selected relations are shown to be
useful in text processing applications (Zhou et al,
2005) and can be reliably recognized by humans.
Moreover, the distribution of event ordering links
under a more refined annotation scheme, such as
TimeML, shows that our subset of relations cov-
ers a majority of annotated links (Pustejovsky et
al., 2003).
4 Method for Temporal Segmentation
Our first goal is to automatically predict shifts
in temporal focus that are indicative of segment
boundaries. Linguistic studies show that speakers
and writers employ a wide range of language de-
vices to signal change in temporal discourse (Best-
gen and Vonk, 1995). For instance, the presence of
the temporal anchor last year indicates the lack of
temporal continuity between the current and the
previous sentence. However, many of these pre-
dictors are heavily context-dependent and, thus,
cannot be considered independently. Instead of
manually crafting complex rules controlling fea-
ture interaction, we opt to learn them from data.
We model temporal segmentation as a binary
classification task. Given a set of candidate bound-
aries (e.g., sentence boundaries), our task is to se-
lect a subset of the boundaries that delineate tem-
poral segment transitions. To implement this ap-
proach, we first identify a set of potential bound-
aries. Our analysis of the manually-annotated cor-
pus reveals that boundaries can occur not only be-
tween sentences, but also within a sentence, at the
boundary of syntactic clauses. We automatically
segment sentences into clauses using a robust sta-
tistical parser (Charniak, 2000). Next, we encode
each boundary as a vector of features. Given a
set of annotated examples, we train a classifier2 to
predict boundaries based on the following feature
set:
Lexical Features Temporal expressions, such
as tomorrow and earlier, are among the strongest
markers of temporal discontinuity (Passonneau,
1988; Bestgen and Vonk, 1995). In addition to
a well-studied set of domain-independent tempo-
ral markers, there are a variety of domain-specific
temporal markers. For instance, the phrase ini-
tial hospital visit functions as a time anchor in the
medical domain.
To automatically extract these expressions, we
provide a classifier with n-grams from each of the
candidate sentences preceding and following the
candidate segment boundary.
Topical Continuity Temporal segmentation is
closely related to topical segmentation (Chafe,
1979). Transitions from one topic to another may
indicate changes in temporal flow and, therefore,
2BoosTexter package (Schapire and Singer, 2000).
191
identifying such transitions is relevant for tempo-
ral segmentation.
We quantify the strength of a topic change
by computing a cosine similarity between sen-
tences bordering the proposed segmentation. This
measure is commonly used in topic segmenta-
tion (Hearst, 1994) under the assumption that
change in lexical distribution corresponds to topi-
cal change.
Positional Features Some parts of the docu-
ment are more likely to exhibit temporal change
than others. This property is related to patterns in
discourse organization of a document as a whole.
For instance, a medical case summary first dis-
cusses various developments in the medical his-
tory of a patient and then focuses on his current
conditions. As a result, the first part of the sum-
mary contains many short temporal segments. We
encode positional features by recording the rela-
tive position of a sentence in a document.
Syntactic Features Because our segment
boundaries are considered at the clausal level,
rather than at the sentence level, the syntax sur-
rounding a hypothesized boundary may be indica-
tive of temporal shifts. This feature takes into ac-
count the position of a word with respect to the
boundary. For each word within three words of
the hypothesized boundary, we record its part-of-
speech tag along with its distance from the bound-
ary. For example, NNP+1 encodes the presence
of a proper noun immediately following the pro-
posed boundary.
5 Learning to Order Segments
Our next goal is to automatically construct a graph
that encodes ordering relations between tempo-
ral segments. One possible approach is to cast
graph construction as a standard binary classifica-
tion task: predict an ordering for each pair of dis-
tinct segments based on their attributes alone. If
a pair contains a temporal marker, like later, then
accurate prediction is feasible. In fact, this method
is commonly used in event ordering (Mani et al,
2003; Lapata and Lascarides, 2004; Boguraev and
Ando, 2005). However, many segment pairs lack
temporal markers and other explicit cues for order-
ing. Determining their relation out of context can
be difficult, even for humans. Moreover, by treat-
ing each segment pair in isolation, we cannot guar-
antee that all the pairwise assignments are consis-
tent with each other and yield a valid TDAG.
Rather than ordering each pair separately, our
ordering model relies on global inference. Given
the pairwise ordering predictions of a local clas-
sifier3, our model finds a globally optimal assign-
ment. In essence, the algorithm constructs a graph
that is maximally consistent with individual order-
ing preferences of each segment pair and at the
same time satisfies graph-level constraints on the
TDAG topology.
In Section 5.2, we present three global inference
strategies that vary in their computational and lin-
guistic complexity. But first we present our under-
lying local ordering model.
5.1 Learning Pairwise Ordering
Given a pair of segments (i, j), our goal is to as-
sign it to one of three classes: forward, backward,
and null (not connected). We generate the train-
ing data by using all pairs of segments (i, j) that
belong to the same document, such that i appears
before j in the text.
The features we consider for the pairwise order-
ing task are similar to ones used in previous re-
search on event ordering (Mani et al, 2003; Lapata
and Lascarides, 2004; Boguraev and Ando, 2005).
Below we briefly summarize these features.
Lexical Features This class of features cap-
tures temporal markers and other phrases indica-
tive of order between two segments. Represen-
tative examples in this category include domain-
independent cues like years earlier and domain-
specific markers like during next visit. To automat-
ically identify these phrases, we provide a classi-
fier with two sets of n-grams extracted from the
first and the second segments. The classifier then
learns phrases with high predictive power.
Temporal Anchor Comparison Temporal an-
chors are one of the strongest cues for the order-
ing of events in text. For instance, medical case
summaries use phrases like two days before ad-
mission and one day before admission to express
relative order between events. If the two segments
contain temporal anchors, we can determine their
ordering by comparing the relation between the
two anchors. We identified a set of temporal an-
chors commonly used in the medical domain and
devised a small set of regular expressions for their
comparison.4 The corresponding feature has three
3The perceptron classifier.
4We could not use standard tools for extraction and analy-
sis of temporal anchors as they were developed on the news-
paper corpora, and are not suitable for analysis of medical
192
values that encode preceding, following and in-
compatible relations.
Segment Adjacency Feature Multiple studies
have shown that two subsequent sentences are
likely to follow a chronological progression (Best-
gen and Vonk, 1995). To encode this information,
we include a binary feature that captures the adja-
cency relation between two segments.
5.2 Global Inference Strategies for Segment
Ordering
Given the scores (or probabilities) of all pairwise
edges produced by a local classifier, our task is
to construct a TDAG. In this section, we describe
three inference strategies that aim to find a con-
sistent ordering between all segment pairs. These
strategies vary significantly in terms of linguistic
motivation and computational complexity. Exam-
ples of automatically constructed TDAGs derived
from different inference strategies are shown in
Figure 2.
5.2.1 Greedy Inference in Natural Reading
Order (NRO)
The simplest way to construct a consistent
TDAG is by adding segments in the order of their
appearance in a text. Intuitively speaking, this
technique processes segments in the same order
as a reader of the text. The motivation underly-
ing this approach is that the reader incrementally
builds temporal interpretation of a text; when a
new piece of information is introduced, the reader
knows how to relate it to already processed text.
This technique starts with an empty graph and
incrementally adds nodes in order of their appear-
ance in the text. When a new node is added, we
greedily select the edge with the highest score that
connects the new node to the existing graph, with-
out violating the consistency of the TDAG. Next,
we expand the graph with its transitive closure.
We continue greedily adding edges and applying
transitive closure until the new node is connected
to all other nodes already in the TDAG. The pro-
cess continues until all the nodes have been added
to the graph.
5.2.2 Greedy Best-first Inference (BF)
Our second inference strategy is also greedy. It
aims to optimize the score of the graph. The score
of the graph is computed by summing the scores of
text (Wilson et al, 2001).
its edges. While this greedy strategy is not guar-
anteed to find the optimal solution, it finds a rea-
sonable approximation (Cohen et al, 1999).
This method begins by sorting the edges by their
score. Starting with an empty graph, we add one
edge at a time, without violating the consistency
constraints. As in the previous strategy, at each
step we expand the graph with its transitive clo-
sure. We continue this process until all the edges
have been considered.
5.2.3 Exact Inference with Integer Linear
Programming (ILP)
We can cast the task of constructing a globally
optimal TDAG as an optimization problem. In
contrast to the previous approaches, the method
is not greedy. It computes the optimal solu-
tion within the Integer Linear Programming (ILP)
framework.
For a document with N segments, each pair of
segments (i, j) can be related in the graph in one
of three ways: forward, backward, and null (not
connected). Let si?j , si?j , and si=j be the scores
assigned by a local classifier to each of the three
relations respectively. Let Ii?j , Ii?j , and Ii=j
be indicator variables that are set to 1 if the corre-
sponding relation is active, or 0 otherwise.
The objective is then to optimize the score of a
TDAG by maximizing the sum of the scores of all
edges in the graph:
max
N
X
i=1
N
X
j=i+i
si?jIi?j + si?jIi?j + si=jIi=j (1)
subject to:
Ii?j , Ii?j , Ii=j ? {0, 1} ? i, j = 1, . . . N, i < j (2)
Ii?j + Ii?j + Ii=j = 1 ? i, j = 1, . . . N, i < j (3)
We augment this basic formulation with two more
sets of constraints to enforce validity of the con-
structed TDAG.
Transitivity Constraints The key requirement
on the edge assignment is the transitivity of the
resulting graph. Transitivity also guarantees that
the graph does not have cycles. We enforce tran-
sitivity by introducing the following constraint for
every triple (i, j, k):
Ii?j + Ij?k ? 1 ? Ii?k (4)
If both indicator variables on the left side of the
inequality are set to 1, then the indicator variable
193
on the right side must be equal to 1. Otherwise, the
indicator variable on the right can take any value.
Connectivity Constraints The connectivity
constraint states that each node i is connected to
at least one other node and thereby enforces con-
nectivity of the generated TDAG. We introduce
these constraints because manually-constructed
TDAGs do not have any disconnected nodes. This
observation is consistent with the intuition that the
reader is capable to order a segment with respect
to other segments in the TDAG.
(
i?1
?
j=1
Ii=j +
N
?
j=i+1
Ij=i) < N ? 1 (5)
The above constraint rules out edge assignments
in which node i has null edges to the rest of the
nodes.
Solving ILP Solving an integer linear program
is NP-hard (Cormen et al, 1992). Fortunately,
there exist several strategies for solving ILPs. We
employ an efficient Mixed Integer Programming
solver lp solve5 which implements the Branch-
and-Bound algorithm. It takes less than five sec-
onds to decode each document on a 2.8 GHz Intel
Xeon machine.
6 Evaluation Set-Up
We first describe the corpora used in our experi-
ments and the results of human agreement on the
segmentation and the ordering tasks. Then, we in-
troduce the evaluation measures that we use to as-
sess the performance of our model.
6.1 Corpus Characteristics
We applied our method for temporal ordering to
a corpus of medical case summaries. The medical
domain has been a popular testbed for methods for
automatic temporal analyzers (Combi and Shahar,
1997; Zhou et al, 2005). The appeal is partly due
to rich temporal structure of these documents and
the practical need to parse this structure for mean-
ingful processing of medical data.
We compiled a corpus of medical case sum-
maries from the online edition of The New Eng-
land Journal of Medicine.6 The summaries are
written by physicians of Massachusetts General
5
http://groups.yahoo.com/group/lp_solve
6
http://content.nejm.org
Hospital. A typical summary describes an admis-
sion status, previous diseases related to the cur-
rent conditions and their treatments, family his-
tory, and the current course of treatment. For
privacy protection, names and dates are removed
from the summaries before publication.
The average length of a summary is 47 sen-
tences. The summaries are written in the past
tense, and a typical summary does not include in-
stances of the past perfect. The summaries do
not follow a chronological order. The ordering of
information in this domain is guided by stylistic
conventions (i.e., symptoms are presented before
treatment) and the relevance of information to the
current conditions (i.e., previous onset of the same
disease is summarized before the description of
other diseases).
6.2 Annotating Temporal Segmentation
Our approach for temporal segmentation requires
annotated data for supervised training. We first
conducted a pilot study to assess the human agree-
ment on the task. We employed two annotators to
manually segment a portion of our corpus. The an-
notators were provided with two-page instructions
that defined the notion of a temporal segment and
included examples of segmented texts. Each an-
notator segmented eight summaries which on av-
erage contained 49 sentences. Because annotators
were instructed to consider segmentation bound-
aries at the level of a clause, there were 877 po-
tential boundaries. The first annotator created 168
boundaries, while the second ? 224 boundaries.
We computed a Kappa coefficient of 0.71 indicat-
ing a high inter-annotator agreement and thereby
confirming our hypothesis about the reliability of
temporal segmentation.
Once we established high inter-annotator agree-
ment on the pilot study, one annotator seg-
mented the remaining 52 documents in the cor-
pus.7 Among 3,297 potential boundaries, 1,178
(35.7%) were identified by the annotator as seg-
ment boundaries. The average segment length is
three sentences, and a typical document contains
around 20 segments.
6.3 Annotating Temporal Ordering
To assess the inter-annotator agreement, we asked
two human annotators to construct TDAGs from
7It took approximately 20 minutes to segment a case sum-
mary.
194
five manually segmented summaries. These sum-
maries consist of 97 segments, and their transi-
tive closure contain a total of 1,331 edges. We
computed the agreement between human judges
by comparing the transitive closure of the TDAGs.
The annotators achieved a surprisingly high agree-
ment with a Kappa value of 0.98.
After verifying human agreement on this task,
one of the annotators constructed TDAGs for an-
other 25 summaries.8 The transitive reduction of
a graph contains on average 20.9 nodes and 20.5
edges. The corpus consists of 72% forward, 12%
backward and 16% null segment edges inclusive
of edges induced by transitive closure. At the
clause level, the distribution is even more skewed
? forward edges account for 74% edges, equal for
18%, backward for 3% and null for 5%.
6.4 Evaluation Measures
We evaluate temporal segmentation by consider-
ing the ratio of correctly predicted boundaries.
We quantify the performance using F-measure, a
commonly used binary classification metric. We
opt not to use the Pk measure, a standard topical
segmentation measure, because the temporal seg-
ments are short and we are only interested in the
identification of the exact boundaries.
Our second evaluation task is concerned with
ordering manually annotated segments. In these
experiments, we compare an automatically gener-
ated TDAG against the annotated reference graph.
In essence, we compare edge assignment in the
transitive closure of two TDAGs, where each edge
can be classified into one of the three types: for-
ward, backward, or null.
Our final evaluation is performed at the clausal
level. In this case, each edge can be classified into
one of the four classes: forward, backward, equal,
or null. Note that the clause-level analysis allows
us to compare TDAGs based on the automatically
derived segmentation.
7 Results
We evaluate temporal segmentation using leave-
one-out cross-validation on our corpus of 60 sum-
maries. The segmentation algorithm achieves a
performance of 83% F-measure, with a recall of
78% and a precision of 89%.
8It took approximately one hour to build a TDAG for each
segmented document.
To evaluate segment ordering, we employ leave-
one-out cross-validation on 30 annotated TDAGs
that overall contain 13,088 edges in their transi-
tive closure. In addition to the three global in-
ference algorithms, we include a majority base-
line that classifies all edges as forward, yielding
a chronological order.
Our results for ordering the manually annotated
temporal segments are shown in Table 1. All infer-
ence methods outperform the baseline, and their
performance is consistent with the complexity of
the inference mechanism. As expected, the ILP
strategy, which supports exact global inference,
achieves the best performance ? 84.3%.
An additional point of comparison is the accu-
racy of the pairwise classification, prior to the ap-
plication of global inference. The accuracy of the
local ordering is 81.6%, which is lower than that
of ILP. The superior performance of ILP demon-
strates that accurate global inference can further
refine local predictions. Surprisingly, the local
classifier yields a higher accuracy than the two
other inference strategies. Note, however, the local
ordering procedure is not guaranteed to produce a
consistent TDAG, and thus the local classifier can-
not be used on its own to produce a valid TDAG.
Table 2 shows the ordering results at the clausal
level. The four-way classification is computed
using both manually and automatically generated
segments. Pairs of clauses that belong to the same
segment stand in the equal relation, otherwise they
have the same ordering relation as the segments to
which they belong.
On the clausal level, the difference between the
performance of ILP and BF is blurred. When eval-
uated on manually-constructed segments, ILP out-
performs BF by less than 1%. This unexpected re-
sult can be explained by the skewed distribution of
edge types ? the two hardest edge types to clas-
sify (see Table 3), backward and null, account only
for 7.4% of all edges at the clause level.
When evaluated on automatically segmented
text, ILP performs slightly worse than BF. We hy-
pothesize that this result can be explained by the
difference between training and testing conditions
for the pairwise classifier: the classifier is trained
on manually-computed segments and is tested on
automatically-computed ones, which negatively
affects the accuracy on the test set. While all
the strategies are negatively influenced by this dis-
crepancy, ILP is particularly vulnerable as it relies
195
Algorithm Accuracy
Integer Linear Programming (ILP) 84.3
Best First (BF) 78.3
Natural Reading Order (NRO) 74.3
Baseline 72.2
Table 1: Accuracy for 3-way ordering classifica-
tion over manually-constructed segments.
Algorithm Manual Seg. Automatic Seg.
ILP 91.9 84.8
BF 91.0 85.0
NRO 87.8 81.0
Baseline 73.6 73.6
Table 2: Results for 4-way ordering classification
over clauses, computed over manually and auto-
matically generated segments.
on the score values for inference. In contrast, BF
only considers the rank between the scores, which
may be less affected by noise.
We advocate a two-stage approach for temporal
analysis: we first identify segments and then order
them. A simpler alternative is to directly perform
a four-way classification at the clausal level using
the union of features employed in our two-stage
process. The accuracy of this approach, however,
is low ? it achieves only 74%, most likely due
to the sparsity of clause-level representation for
four-way classification. This result demonstrates
the benefits of a coarse representation and a two-
stage approach for temporal analysis.
8 Conclusions
This paper introduces a new method for temporal
ordering. The unit of our analysis is a temporal
segment, a fragment of text that maintains tem-
poral coherence. After investigating several infer-
ence strategies, we concluded that integer linear
programming and best first greedy approach are
valuable alternatives for TDAG construction.
In the future, we will explore a richer set of con-
straints on the topology on the ordering graph. We
will build on the existing formal framework (Fikes
et al, 2003) for the verification of ordering con-
sistency. We are also interested in expanding our
framework for global inference to other temporal
annotation schemes. Given a richer set of temporal
relations, the benefits from global inference can be
even more significant.
Algorithm Forward Backward Null
ILP 92.5 45.6 76.0
BF 91.4 42.2 74.7
NRO 87.7 43.6 66.4
Table 3: Per class accuracy for clause classifica-
tion over manually computed segments.
Acknowledgments
The authors acknowledge the support of the Na-
tional Science Foundation and National Institute
of Health (CAREER grant IIS-0448168, grant IIS-
0415865). Thanks to Terry Koo, Igor Malioutov,
Zvika Marx, Benjamin Snyder, Peter Szolovits,
Luke Zettlemoyer and the anonymous reviewers
for their helpful comments and suggestions. Any
opinions, findings, conclusions or recommenda-
tions expressed above are those of the authors and
do not necessarily reflect the views of the NSF or
NIH.
References
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23(2):123?
154.
Regina Barzilay, Noemie Elhadad, and Kathleen McK-
eown. 2002. Inferring strategies for sentence order-
ing in multidocument news summarization. Journal
of Artificial Intelligence Research, 17:35?55.
Yves Bestgen and Wietske Vonk. 1995. The role
of temporal segmentation markers in discourse pro-
cessing. Discourse Processes, 19:385?406.
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal reason-
ing. In Proceedings of IJCAI, pages 997?1003.
Wallace Chafe. 1979. The flow of thought and the
flow of language. In Talmy Givon, editor, Syntax
and Semantics: Discourse and Syntax, volume 12,
pages 159?182. Academic Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the NAACL, pages
132?139.
William Cohen, Robert Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial
Intelligence, 10:243?270.
Carlo Combi and Yuval Shahar. 1997. Temporal rea-
soning and temporal data maintenance in medicine:
Issues and challenges. Computers in Biology and
Medicine, 27(5):353?368.
196
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1992. Intoduction to Algorithms.
The MIT Press.
David R. Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: Semantics or
Pragmatics? Linguistics and Philosophy, 9:37?61.
R. Fikes, J. Jenkins, and G. Frank. 2003. A system
architecture and component library for hybrid rea-
soning. Technical report, Stanford University.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of the ACL, pages
9?16.
Chung Hee Hwang and Lenhart K. Schubert. 1992.
Tense trees as the ?fine structure? of discourse. In
Proceedings of the ACL, pages 232?240.
Mirella Lapata and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In Proceedings
of HLT-NAACL, pages 153?160.
Alex Lascarides and Nicholas Asher. 1993. Tem-
poral interpretation, discourse relations, and com-
monsense entailment. Linguistics and Philosophy,
16:437?493.
Alex Lascarides and John Oberlander. 1993. Temporal
connectives in a discourse context. In Proceeding of
the EACL, pages 260?268.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in news.
In Proceeding of HLT-NAACL, pages 55?57.
Mark Moens and Mark J. Steedman. 1987. Temporal
ontology in natural language. In Proceedings of the
ACL, pages 1?7.
Rebecca J. Passonneau. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, 14(2):44?60.
James Pustejovsky, Patrick Hanks, Roser Sauri,
Andrew See, David Day, Lissa Ferro, Robert
Gaizauskas, Marcia Lazo, Andrea Setzer, and Beth
Sundheim. 2003. The timebank corpus. Corpus
Linguistics, pages 647?656.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York, NY.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Bonnie L. Webber. 1987. The interpretation of tense
in discourse. In Proceedings of the ACL, pages 147?
154.
Bonnie L. Webber. 1988. Tense as discourse anaphor.
Computational Linguistics, 14(2):61?73.
George Wilson, Inderjeet Mani, Beth Sundheim, and
Lisa Ferro. 2001. A multilingual approach to anno-
tating and extracting temporal information. In Pro-
ceedings of the ACL 2001 Workshop on Temporal
and Spatial Information Processing, pages 81?87.
Li Zhou, Carol Friedman, Simon Parsons, and George
Hripcsak. 2005. System architecture for temporal
information extraction, representation and reason-
ing in clinical narrative reports. In Proceedings of
AMIA, pages 869?873.
197
   
 
 
   
	  
 
 	
 
		
 
	
 
 
	  
	
(a) Reference TDAG
 


 
