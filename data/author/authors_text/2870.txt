Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 563?570,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Segmented and unsegmented dialogue-act annotation with statistical
dialogue models?
Carlos D. Mart??nez Hinarejos, Ramo?n Granell, Jose? Miguel Bened??
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
Camino de Vera, s/n, 46022, Valencia
{cmartine,rgranell,jbenedi}@dsic.upv.es
Abstract
Dialogue systems are one of the most chal-
lenging applications of Natural Language
Processing. In recent years, some statis-
tical dialogue models have been proposed
to cope with the dialogue problem. The
evaluation of these models is usually per-
formed by using them as annotation mod-
els. Many of the works on annotation
use information such as the complete se-
quence of dialogue turns or the correct
segmentation of the dialogue. This in-
formation is not usually available for dia-
logue systems. In this work, we propose a
statistical model that uses only the infor-
mation that is usually available and per-
forms the segmentation and annotation at
the same time. The results of this model
reveal the great influence that the availabil-
ity of a correct segmentation has in ob-
taining an accurate annotation of the dia-
logues.
1 Introduction
In the Natural Language Processing (NLP) field,
one of the most challenging applications is dia-
logue systems (Kuppevelt and Smith, 2003). A
dialogue system is usually defined as a com-
puter system that can interact with a human be-
ing through dialogue in order to complete a spe-
cific task (e.g., ticket reservation, timetable con-
sultation, bank operations,. . . ) (Aust et al, 1995;
Hardy et al, 2002). Most dialogue system have a
characteristic behaviour with respect to dialogue
? Work partially supported by the Spanish project
TIC2003-08681-C02-02 and by Spanish Ministry of Culture
under FPI grants.
management, which is known as dialogue strat-
egy. It defines what the dialogue system must do
at each point of the dialogue.
Most of these strategies are rule-based, i.e., the
dialogue strategy is defined by rules that are usu-
ally defined by a human expert (Gorin et al, 1997;
Hardy et al, 2003). This approach is usually diffi-
cult to adapt or extend to new domains where the
dialogue structure could be completely different,
and it requires the definition of new rules.
Similar to other NLP problems (like speech
recognition and understanding, or statistical ma-
chine translation), an alternative data-based ap-
proach has been developed in the last decade (Stol-
cke et al, 2000; Young, 2000). This approach re-
lies on statistical models that can be automatically
estimated from annotated data, which in this case,
are dialogues from the task.
Statistical modelling learns the appropriate pa-
rameters of the models from the annotated dia-
logues. As a simplification, it could be considered
that each label is associated to a situation in the di-
alogue, and the models learn how to identify and
react to the different situations by estimating the
associations between the labels and the dialogue
events (words, the speaker, previous turns, etc.).
An appropriate annotation scheme should be de-
fined to capture the elements that are really impor-
tant for the dialogue, eliminating the information
that is irrelevant to the dialogue process. Several
annotation schemes have been proposed in the last
few years (Core and Allen, 1997; Dybkjaer and
Bernsen, 2000).
One of the most popular annotation schemes at
the dialogue level is based on Dialogue Acts (DA).
A DA is a label that defines the function of the an-
notated utterance with respect to the dialogue pro-
cess. In other words, every turn in the dialogue
563
is supposed to be composed of one or more ut-
terances. In this context, from the dialogue man-
agement viewpoint an utterance is a relevant sub-
sequence . Several DA annotation schemes have
been proposed in recent years (DAMSL (Core and
Allen, 1997), VerbMobil (Alexandersson et al,
1998), Dihana (Alca?cer et al, 2005)).
In all these studies, it is necessary to annotate
a large amount of dialogues to estimate the pa-
rameters of the statistical models. Manual anno-
tation is the usual solution, although is very time-
consuming and there is a tendency for error (the
annotation instructions are not usually easy to in-
terpret and apply, and human annotators can com-
mit errors) (Jurafsky et al, 1997).
Therefore, the possibility of applying statistical
models to the annotation problem is really inter-
esting. Moreover, it gives the possibility of evalu-
ating the statistical models. The evaluation of the
performance of dialogue strategies models is a dif-
ficult task. Although many proposals have been
made (Walker et al, 1997; Fraser, 1997; Stolcke
et al, 2000), there is no real agreement in the NLP
community about the evaluation technique to ap-
ply.
Our main aim is the evaluation of strategy mod-
els, which provide the reaction of the system given
a user input and a dialogue history. Using these
models as annotation models gives us a possible
evaluation: the correct recognition of the labels
implies the correct recognition of the dialogue sit-
uation; consequently this information can help the
system to react appropriately. Many recent works
have attempted this approach (Stolcke et al, 2000;
Webb et al, 2005).
However, many of these works are based on the
hypothesis of the availability of the segmentation
into utterances of the turns of the dialogue. This is
an important drawback in order to evaluate these
models as strategy models, where segmentation is
usually not available. Other works rely on a de-
coupled scheme of segmentation and DA classifi-
cation (Ang et al, 2005).
In this paper, we present a new statistical model
that computes the segmentation and the annota-
tion of the turns at the same time, using a statis-
tical framework that is simpler than the models
that have been proposed to solve both problems
at the same time (Warnke et al, 1997). The results
demonstrate that segmentation accuracy is really
important in obtaining an accurate annotation of
the dialogue, and consequently in obtaining qual-
ity strategy models. Therefore, more accurate seg-
mentation models are needed to perform this pro-
cess efficiently.
This paper is organised as follows: Section 2,
presents the annotation models (for both the un-
segmented and segmented versions); Section 3,
describes the dialogue corpora used in the ex-
periments; Section 4 establishes the experimental
framework and presents a summary of the results;
Section 5, presents our conclusions and future re-
search directions.
2 Annotation models
The statistical annotation model that we used ini-
tially was inspired by the one presented in (Stol-
cke et al, 2000). Under a maximum likeli-
hood framework, they developed a formulation
that assigns DAs depending on the conversation
evidence (transcribed words, recognised words
from a speech recogniser, phonetic and prosodic
features,. . . ). Stolcke?s model uses simple and
popular statistical models: N-grams and Hidden
Markov Models. The N-grams are used to model
the probability of the DA sequence, while the
HMM are used to model the evidence likelihood
given the DA. The results presented in (Stolcke et
al., 2000) are very promising.
However, the model makes some unrealistic as-
sumptions when they are evaluated to be used as
strategy models. One of them is that there is a
complete dialogue available to perform the DA
assignation. In a real dialogue system, the only
available information is the information that is
prior to the current user input. Although this al-
ternative is proposed in (Stolcke et al, 2000), no
experimental results are given.
Another unrealistic assumption corresponds to
the availability of the segmentation of the turns
into utterances. An utterance is defined as a
dialogue-relevant subsequence of words in the cur-
rent turn (Stolcke et al, 2000). It is clear that the
only information given in a turn is the usual in-
formation: transcribed words (for text systems),
recognised words, and phonetic/prosodic features
(for speech systems). Therefore, it is necessary to
develop a model to cope with both the segmenta-
tion and the assignation problem.
Let Ud1 = U1U2 ? ? ?Ud be the sequence of DA
assigned until the current turn, corresponding to
the first d segments of the current dialogue. Let
564
W = w1w2 . . . wl be the sequence of the words
of the current turn, where subsequences W ji =
wiwi+1 . . . wj can be defined (1 ? i ? j ? l).
For the sequence of words W , a segmentation
is defined as sr1 = s0s1 . . . sr, where s0 = 0 and
W = W s1s0+1W
s2
s1+1 . . .W
sr
sr?1+1. Therefore, the
optimal sequence of DA for the current turn will
be given by:
U? = argmax
U
Pr(U |W l1, U
d
1 ) =
argmax
Ud+rd+1
?
(sr1,r)
Pr(Ud+rd+1 |W
l
1, U
d
1 )
After developing this formula and making sev-
eral assumptions and simplifications, the final
model, called unsegmented model, is:
U? = argmax
Ud+rd+1
max
(sr1,r)
d+r?
k=d+1
Pr(Uk|U
k?1
k?n?1) Pr(W
sk?d
sk?(d+1)+1
|Uk)
This model can be easily implemented using
simple statistical models (N-grams and Hidden
Markov Models). The decoding (segmentation
and DA assignation) was implemented using the
Viterbi algorithm. A Word Insertion Penalty
(WIP) factor, similar to the one used in speech
recognition, can be incorporated into the model to
control the number of utterances and avoid exces-
sive segmentation.
When the segmentation into utterances is pro-
vided, the model can be simplified into the seg-
mented model, which is:
U? = argmax
Ud+rd+1
d+r?
k=d+1
Pr(Uk|U
k?1
k?n?1) Pr(W
sk?d
sk?(d+1)+1
|Uk)
All the presented models only take into account
word transcriptions and dialogue acts, although
they could be extended to deal with other features
(like prosody, sintactical and semantic informa-
tion, etc.).
3 Experimental data
Two corpora with very different features were
used in the experiment with the models proposed
in Section 2. The SwitchBoard corpus is com-
posed of human-human, non task-oriented dia-
logues with a large vocabulary. The Dihana corpus
is composed of human-computer, task-oriented di-
alogues with a small vocabulary.
Although two corpora are not enough to let us
draw general conclusions, they give us more reli-
able results than using only one corpus. Moreover,
the very different nature of both corpora makes
our conclusions more independent from the cor-
pus type, the annotation scheme, the vocabulary
size, etc.
3.1 The SwitchBoard corpus
The first corpus used in the experiments was the
well-known SwitchBoard corpus (Godfrey et al,
1992). The SwitchBoard database consists of
human-human conversations by telephone with no
directed tasks. Both speakers discuss about gen-
eral interest topics, but without a clear task to ac-
complish.
The corpus is formed by 1,155 conversations,
which comprise 126,754 different turns of spon-
taneous and sometimes overlapped speech, using
a vocabulary of 21,797 different words. The cor-
pus was segmented into utterances, each of which
was annotated with a DA following the simpli-
fied DAMSL annotation scheme (Jurafsky et al,
1997). The set of labels of the simplified DAMSL
scheme is composed of 42 different labels, which
define categories such as statement, backchannel,
opinion, etc. An example of annotation is pre-
sented in Figure 1.
3.2 The Dihana corpus
The second corpus used was a task-oriented cor-
pus called Dihana (Bened?? et al, 2004). It is com-
posed of computer-to-human dialogues, and the
main aim of the task is to answer telephone queries
about train timetables, fares, and services for long-
distance trains in Spanish. A total of 900 dialogues
were acquired by using the Wizard of Oz tech-
nique and semicontrolled scenarios. Therefore,
the voluntary caller was always free to express
him/herself (there were no syntactic or vocabu-
lary restrictions); however, in some dialogues, s/he
had to achieve some goals using a set of restric-
tions that had been given previously (e.g. depar-
ture/arrival times, origin/destination, travelling on
a train with some services, etc.).
These 900 dialogues comprise 6,280 user turns
and 9,133 system turns. Obviously, as a task-
565
Utterance Label
YEAH, TO GET REFERENCES AND THAT, SO, BUT, UH, I DON?T FEEL COMFORTABLE ABOUT LEAVING MY KIDS IN A BIG
DAY CARE CENTER, SIMPLY BECAUSE THERE?S SO MANY KIDS AND SO MANY <SNIFFING> <THROAT CLEARING>
Yeah, aa
to get references and that, sd
so, but, uh, %
I don?t feel comfortable about leaving my kids in a big day care center, simply because there?s so
many kids and so many <sniffing> <throat clearing> sd
I THINK SHE HAS PROBLEMS WITH THAT, TOO.
I think she has problems with that, too. sd
Figure 1: An example of annotated turns in the SwitchBoard corpus.
oriented and medium size corpus, the total number
of different words in the vocabulary, 812, is not as
large as the Switchboard database.
The turns were segmented into utterances. It
was possible for more than one utterance (with
their respective labels) to appear in a turn (on av-
erage, there were 1.5 utterances per user/system
turn). A three-level annotation scheme of the ut-
terances was defined (Alca?cer et al, 2005). These
labels represent the general purpose of the utter-
ance (first level), as well as more specific semantic
information (second and third level): the second
level represents the data focus in the utterance and
the third level represents the specific data present
in the utterance. An example of three-level anno-
tated user turns is given in Figure 2. The corpus
was annotated by means of a semiautomatic pro-
cedure, and all the dialogues were manually cor-
rected by human experts using a very specific set
of defined rules.
After this process, there were 248 different la-
bels (153 for user turns, 95 for system turns) using
the three-level scheme. When the detail level was
reduced to the first and second levels, there were
72 labels (45 for user turns, 27 for system turns).
When the detail level was limited to the first level,
there were only 16 labels (7 for user turns, 9 for
system turns). The differences in the number of
labels and in the number of examples for each la-
bel with the SwitchBoard corpus are significant.
4 Experiments and results
The SwitchBoard database was processed to re-
move certain particularities. The main adaptations
performed were:
? The interrupted utterances (which were la-
belled with ?+?) were joined to the correct
previous utterance, thereby avoiding inter-
ruptions (i.e., all the words of the interrupted
utterance were annotated with the same DA).
Table 1: SwitchBoard database statistics (mean for
the ten cross-validation partitions)
Training Test
Dialogues 1,136 19
Turns 113,370 1,885
Utterances 201,474 3,718
Running words 1,837,222 33,162
Vocabulary 21,248 2,579
? All the words were transcribed in lowercase.
? Puntuaction marks were separated from
words.
The experiments were performed using a cross-
validation approach to avoid the statistical bias
that can be introduced by the election of fixed
training and test partitions. This cross-validation
approach has also been adopted in other recent
works on this corpus (Webb et al, 2005). In our
case, we performed 10 different experiments. In
each experiment, the training partition was com-
posed of 1,136 dialogues, and the test partition
was composed of 19 dialogues. This proportion
was adopted so that our results could be compared
with the results in (Stolcke et al, 2000), where
similar training and test sizes were used. The
mean figures for the training and test partitions are
shown in Table 1.
With respect to the Dihana database, the prepro-
cessing included the following points:
? A categorisation process was performed for
categories such as town names, the time,
dates, train types, etc.
? All the words were transcribed in lowercase.
? Puntuaction marks were separated from
words.
? All the words were preceded by the speaker
identification (U for user, M for system).
566
Utterance 1st level 2nd level 3rd level
YES, TIMES AND FARES.
Yes, Acceptance Dep Hour Nil
times and fares Question Dep Hour,Fare Nil
YES, I WANT TIMES AND FARES OF TRAINS THAT ARRIVE BEFORE SEVEN.
Yes, I want times and fares of trains that arrive before seven. Question Dep Hour,Fare Arr Hour
ON THURSDAY IN THE AFTERNOON.
On thursday Answer Day Day
in the afternoon Answer Time Time
Figure 2: An example of annotated turns in the Dihana corpus. Original turns were in Spanish.
Table 2: Dihana database statistics (mean for the
five cross-validation partitions)
Training Test
Dialogues 720 180
Turns 12,330 3,083
User turns 5,024 1,256
System turns 7,206 1,827
Utterances 18,837 4,171
User utterances 7,773 1,406
System utterances 11,064 2,765
Running words 162,613 40,765
User running words 42,806 10,815
System running words 119,807 29,950
Vocabulary 832 485
User vocabulary 762 417
System vocabulary 208 174
A cross-validation approach was adopted in Di-
hana as well. In this case, only 5 different parti-
tions were used. Each of them had 720 dialogues
for training and 180 for testing. The statistics on
the Dihana corpus are presented in Table 2.
For both corpora, different N-gram models,
with N = 2, 3, 4, and HMM of one state were
trained from the training database. In the case of
the SwitchBoard database, all the turns in the test
set were used to compute the labelling accuracy.
However, for the Dihana database, only the user
turns were taken into account (because system
turns follow a regular, template-based scheme,
which presents artificially high labelling accura-
cies). Furthermore, in order to use a really sig-
nificant set of labels in the Dihana corpus, we
performed the experiments using only two-level
labels instead of the complete three-level labels.
This restriction allowed us to be more independent
from the understanding issues, which are strongly
related to the third level. It also allowed us to con-
centrate on the dialogue issues, which relate more
Table 3: SwitchBoard results for the segmented
model
N-gram Utt. accuracy Turn accuracy
2-gram 68.19% 59.33%
3-gram 68.50% 59.75%
4-gram 67.90% 59.14%
to the first and second levels.
The results in the case of the segmented ap-
proach described in Section 2 for SwitchBoard are
presented in Table 3. Two different definitions of
accuracy were used to assess the results:
? Utterance accuracy: computes the proportion
of well-labelled utterances.
? Turn accuracy: computes the proportion of
totally well-labelled turns (i.e.: if the la-
belling has the same labels in the same or-
der as in the reference, it is taken as a well-
labelled turn).
As expected, the utterance accuracy results are
a bit worse than those presented in (Stolcke et al,
2000). This may be due to the use of only the
past history and possibly to the cross-validation
approach used in the experiments. The turn accu-
racy was calculated to compare the segmented and
the unsegmented models. This was necessary be-
cause the utterance accuracy does not make sense
for the unsegmented model.
The results for the unsegmented approach for
SwitchBoard are presented in Table 4. In this case,
three different definitions of accuracy were used to
assess the results:
? Accuracy at DA level: the edit distance be-
tween the reference and the labelling of the
turn was computed; then, the number of cor-
rect substitutions (c), wrong substitutions (s),
deletions (d) and insertions (i) was com-
567
Table 4: SwitchBoard results for the unsegmented
model (WIP=50)
N-gram DA acc. Turn acc. Segm. acc.
2-gram 38.19% 39.47% 38.92%
3-gram 38.58% 39.61% 39.06%
4-gram 38.49% 39.52% 38.96%
puted, and the accuracy was calculated as
100 ? c(c+s+i+d) .
? Accuracy at turn level: this provides the pro-
portion of well-labelled turns, without taking
into account the segmentation (i.e., if the la-
belling has the same labels in the same or-
der as in the reference, it is taken as a well-
labelled turn).
? Accuracy at segmentation level: this pro-
vides the proportion of well-labelled and seg-
mented turns (i.e., the labels are the same as
in the reference and they affect the same ut-
terances).
The WIP parameter used in Table 4 was 50,
which is the one that offered the best results. The
segmentation accuracy in Table 4 must be com-
pared with the turn accuracy in Table 3. As Table 4
shows, the accuracy of the labelling decreased dra-
matically. This reveals the strong influence of the
availability of the real segmentation of the turns.
To confirm this hypothesis, similar experiments
were performed with the Dihana database. Ta-
ble 5 presents the results with the segmented cor-
pus, and Table 6 presents the results with the un-
segmented corpus (with WIP=50, which gave the
best results). In this case, only user turns were
taken into account to compute the accuracy, al-
though the model was applied to all the turns (both
user and system turns). For the Dihana corpus,
the degradation of the results of the unsegmented
approach with respect to the segmented approach
was not as high as in the SwitchBoard corpus, due
to the smaller vocabulary and complexity of the
dialogues.
These results led us to the same conclusion,
even for such a different corpus (much more la-
bels, task-oriented, etc.). In any case, these ac-
curacy figures must be taken as a lower bound on
the model performance because sometimes an in-
correct recognition of segment boundaries or dia-
logue acts does not cause an inappropriate reaction
of the dialogue strategy.
Table 5: Dihana results for the segmented model
(only two-level labelling for user turns)
N-gram Utt. accuracy Turn accuracy
2-gram 75.70% 74.46%
3-gram 76.28% 74.93%
4-gram 76.39% 75.10%
Table 6: Dihana results for the unsegmented
model (WIP=50, only two-level labelling for user
turns)
N-gram DA acc. Turn acc. Segm. acc.
2-gram 60.36% 62.86% 58.15%
3-gram 60.05% 62.49% 57.87%
4-gram 59.81% 62.44% 57.88%
An illustrative example of annotation errors in
the SwitchBoard database, is presented in Figure 3
for the same turns as in Figure 1. An error anal-
ysis of the segmented model was performed. The
results reveals that, in the case of most of the er-
rors were produced by the confusion of the ?sv?
and ?sd? classes (about 50% of the times ?sv? was
badly labelled, the wrong label was ?sd?) The sec-
ond turn in Figure 3 is an example of this type of
error. The confusions between the ?aa? and ?b?
classes were also significant (about 27% of the
times ?aa? was badly labelled, the wrong label was
?b?). This was reasonable due to the similar defini-
tions of these classes (which makes the annotation
difficult, even for human experts). These errors
were similar for all the N-grams used. In the case
of the unsegmented model, most of the errors were
produced by deletions of the ?sd? and ?sv? classes,
as in the first turn in Figure 3 (about 50% of the
errors). This can be explained by the presence of
very short and very long utterances in both classes
(i.e., utterances for ?sd? and ?sv? did not present a
regular length).
Some examples of errors in the Dihana corpus
are shown in Figure 4 (in this case, for the same
turns as those presented in Figure 2). In the seg-
mented model, most of the errors were substitu-
tions between labels with the same first level (es-
pecially questions and answers) where the second
level was difficult to recognise. The first and third
turn in Figure 4 are examples of this type of er-
ror. This was because sometimes the expressions
only differed with each other by one word, or
568
Utt Label
1 % Yeah, to get references and that, so, but, uh, I don?t
2 sd
feel comfortable about leaving my kids in a big day care center, simply because
there?s so many kids and so many <sniffing> <throat clearing>
Utt Label
1 sv I think she has problems with that, too.
Figure 3: An example of errors produced by the model in the SwitchBoard corpus
the previous segment influence (i.e., the language
model weight) was not enough to get the appro-
priate label. This was true for all the N-grams
tested. In the case of the unsegmented model, most
of the errors were caused by similar misrecogni-
tions in the second level (which are more frequent
due to the absence of utterance boundaries); how-
ever, deletion and insertion errors were also sig-
nificant. The deletion errors corresponded to ac-
ceptance utterances, which were too short (most
of them were ?Yes?). The insertion errors corre-
sponded to ?Yes? words that were placed after a
new-consult system utterance, which is the case
of the second turn presented in Figure 4. These
words should not have been labelled as a separate
utterance. In both cases, these errors were very
dependant on the WIP factor, and we had to get
an adequate WIP value which did not increase the
insertions and did not cause too many deletions.
5 Conclusions and future work
In this work, we proposed a method for simultane-
ous segmentation and annotation of dialogue ut-
terances. In contrast to previous models for this
task, our model does not assume manual utterance
segmentation. Instead of treating utterance seg-
mentation as a separate task, the proposed method
selects utterance boundaries to optimize the accu-
racy of the generated labels. We performed ex-
periments to determine the effect of the availabil-
ity of the correct segmentation of dialogue turns
in utterances in the statistical DA labelling frame-
work. Our results reveal that, as shown in previ-
ous work (Warnke et al, 1999), having the correct
segmentation is very important in obtaining accu-
rate results in the labelling task. This conclusion
is supported by the results obtained in very differ-
ent dialogue corpora: different amounts of training
and test data, different natures (general and task-
oriented), different sets of labels, etc.
Future work on this task will be carried out
in several directions. As segmentation appears
to be an important step in these tasks, it would
be interesting to obtain an automatic and accu-
rate segmentation model that can be easily inte-
grated in our statistical model. The application of
our statistical models to other tasks (like VerbMo-
bil (Alexandersson et al, 1998)) would allow us to
confirm our conclusions and compare results with
other works.
The error analysis we performed shows the need
for incorporating new and more reliable informa-
tion resources to the presented model. Therefore,
the use of alternative models in both corpora, such
as the N-gram-based model presented in (Webb et
al., 2005) or an evolution of the presented statis-
tical model with other information sources would
be useful. The combination of these two models
might be a good way to improve results.
Finally, it must be pointed out that the main task
of the dialogue models is to allow the most correct
reaction of a dialogue system given the user in-
put. Therefore, the correct evaluation technique
must be based on the system behaviour as well
as on the accurate assignation of DA to the user
input. Therefore, future evaluation results should
take this fact into account.
Acknowledgements
The authors wish to thank Nick Webb, Mark Hep-
ple and Yorick Wilks for their comments and
suggestions and for providing the preprocessed
SwitchBoard corpus. We also want to thank the
anonymous reviewers for their criticism and sug-
gestions.
References
N. Alca?cer, J. M. Bened??, F. Blat, R. Granell, C. D.
Mart??nez, and F. Torres. 2005. Acquisition and
labelling of a spontaneous speech dialogue corpus.
In Proceedings of SPECOM, pages 583?586, Patras,
Greece.
Jan Alexandersson, Bianka Buschbeck-Wolf, Tsu-
tomu Fujinami, Michael Kipp, Stephan Koch, Elis-
569
Utterance 1st level 2nd level
Yes, times Acceptance Dep Hour,Fare
and fares Question Dep Hour,Fare
Yes, I want Acceptance Dep Hour,Fare
times and fares of trains that arrive before seven. Question Dep Hour,Fare
On thursday in the afternoon Answer Time
Figure 4: An example of errors produced by the model in the Dihana corpus
abeth Maier, Norbert Reithinger, Birte Schmitz,
and Melanie Siegel. 1998. Dialogue acts in
VERBMOBIL-2 (second edition). Technical Report
226, DFKI GmbH, Saarbru?cken, Germany, July.
J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dia-
log act segmentation and classification in multiparty
meetings. In Proceedings of the International Con-
ference of Acoustics, Speech, and Signal Process-
ings, volume 1, pages 1061?1064, Philadelphia.
H. Aust, M. Oerder, F. Seide, and V. Steinbiss. 1995.
The philips automatic train timetable information
system. Speech Communication, 17:249?263.
J. M. Bened??, A. Varona, and E. Lleida. 2004. Dihana:
Dialogue system for information access using spon-
taneous speech in several environments tic2002-
04103-c03. In Reports for Jornadas de Seguimiento
- Programa Nacional de Tecnolog??as Informa?ticas,
Ma?laga, Spain.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the damsl annotation scheme. In Work-
ing Notes of AAAI Fall Symposium on Communica-
tive Action in Humans and Machines, Boston, MA,
November.
Layla Dybkjaer and Niels Ole Bernsen. 2000. The
mate workbench.
N. Fraser, 1997. Assessment of interactive systems,
pages 564?614. Mouton de Gruyter.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. In Proc. ICASSP-92, pages 517?
520.
A. Gorin, G. Riccardi, and J. Wright. 1997. How may
i help you? Speech Communication, 23:113?127.
Hilda Hardy, Kirk Baker, Laurence Devillers, Lori
Lamel, Sophie Rosset, Tomek Strzalkowski, Cris-
tian Ursu, and Nick Webb. 2002. Multi-layer di-
alogue annotation for automated multilingual cus-
tomer service. In Proceedings of the ISLE Workshop
on Dialogue Tagging for Multi-Modal Human Com-
puter Interaction, Edinburgh, Scotland, December.
Hilda Hardy, Tomek Strzalkowski, and Min Wu. 2003.
Dialogue management for an automated multilin-
gual call center. In Proceedings of HLT-NAACL
2003 Workshop: Research Directions in Dialogue
Processing, pages 10?12, Edmonton, Canada, June.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board swbd-damsl shallow- discourse-function an-
notation coders manual - draft 13. Technical Report
97-01, University of Colorado Institute of Cognitive
Science.
J. Van Kuppevelt and R. W. Smith. 2003. Current
and New Directions in Discourse and Dialogue, vol-
ume 22 of Text, Speech and Language Technology.
Springer.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modelling
for automatic tagging and recognition of conversa-
tional speech. Computational Linguistics, 26(3):1?
34.
Marilyn A. Walker, Diane Litman J., Candace A.
Kamm, and Alicia Abella. 1997. PARADISE: A
framework for evaluating spoken dialogue agents.
In Philip R. Cohen and Wolfgang Wahlster, edi-
tors, Proceedings of the Thirty-Fifth Annual Meet-
ing of the Association for Computational Linguis-
tics and Eighth Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 271?280, Somerset, New Jersey. Association
for Computational Linguistics.
V.Warnke, R. Kompe, H. Niemann, and E. No?th. 1997.
Integrated Dialog Act Segmentation and Classifica-
tion using Prosodic Features and Language Models.
In Proc. European Conf. on Speech Communication
and Technology, volume 1, pages 207?210, Rhodes.
V. Warnke, S. Harbeck, E. No?th, H. Niemann, and
M. Levit. 1999. Discriminative Estimation of Inter-
polation Parameters for LanguageModel Classifiers.
In Proceedings of the IEEE Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
525?528, Phoenix, AZ, March.
N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue
act classification using intra-utterance features. In
Proceedings of the AAAI Workshop on Spoken Lan-
guage Understanding, Pittsburgh.
S. Young. 2000. Probabilistic methods in spoken di-
alogue systems. Philosophical Trans Royal Society
(Series A), 358(1769):1389?1402.
570
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 333?336,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Simultaneous dialogue act segmentation and labelling using lexical and
syntactic features
Ramon Granell, Stephen Pulman
Oxford University Computing Laboratory,
Wolfson Building, Parks Road,
Oxford, OX1 3QD, England
ramg@comlab.ox.ac.uk
sgp@clg.ox.ac.uk
Carlos-D. Mart??nez-Hinarejos
Instituto Tecnolo?gico de Informa?tica,
Universidad Polite?cnica de Valencia,
Camino de Vera, s/n, 46022, Valencia, Spain
cmartine@dsic.upv.es
Abstract
Segmentation of utterances and annotation
as dialogue acts can be helpful for sev-
eral modules of dialogue systems. In this
work, we study a statistical machine learn-
ing model to perform these tasks simulta-
neously using lexical features and incorpo-
rating deterministic syntactic restrictions.
There is a slight improvement in both seg-
mentation and labelling due to these re-
strictions.
1 Introduction
Dialogue acts (DA) are linguistic abstractions that
are commonly accepted and employed by the the
dialogue community. In the framework of dia-
logue systems, they can be helpful to identify and
model user intentions and system answers by the
dialogue manager. Furthermore, in other dialogue
modules such as the automatic speech recognizer
or speech synthesiser, DA information may be also
used to increase their performance.
Many researchers have studied automatic DA
labelling using different techniques. However, in
most of this work it is common to assume that the
dialogue turns are already segmented into separate
utterances, where each utterance corresponds to
just one DA label, as in (Stolcke et al(2000); Ji
and Bilmes (2005); Webb et al(2005)). This is
not a realistic situation because the segmentation
of turns into utterances is not a trivial problem.
There have been many previous approaches to
segmentation of turns prior to DA labelling, be-
ginning with (Stolcke and Shriberg (1996)). Typ-
ically some combination of words and part of
speech (POS) tags is used to predict segmentation
boundaries. In this work we make use of a sta-
tistical model to solve both the DA labelling task
and the segmentation task simultaneously, follow-
ing (Ang et al(2005); Mart??nez-Hinarejos et al
(2006)). Our aim is to see whether going beyond
the word n-gram models can improve accuracy,
using syntactic information (constituent structure)
obtained from the dialogue transcriptions. We ex-
amine whether this information can improve the
segmentation of the dialogue turns into DA seg-
ments. Intuitively, it seems logical to believe that
most of these segments must coincide with partic-
ular syntactic structures, and that segment bound-
aries would respect constituent boundaries.
2 Dialogue data
The dialogue corpus used to perform the exper-
iments is the Switchboard database (SWBD). It
consists of human-human conversations by tele-
phone about generic topics. There are 1155 5-
minute conversations, comprising approximately
205000 utterances and 1.4 million words. The size
of the vocabulary is approximately 22000 words.
All this corpus has been manually annotated at
the dialogue act level using the SWBD-DAMSL
scheme, (Jurafsky et al(1997)), consisting of 42
different labels. Every dialogue turn was manu-
ally segmented into utterances. The average num-
ber of segments (utterances) per dialogue turn is
1.78 with a standard deviation of 1.41. Each utter-
ance was assigned one SWBD-DAMSL label (see
Figure 1).
3 Syntactic analysis of DA segments
An initial analysis of the syntactic structures of the
dialogue data was performed to study their possi-
ble relevance for DA segmentation.
333
- $LAUGH he waits until it gets about seventeen below up here $SEG and then he calls us , $SEG
sd sd
- he waits until it gets about seventeen below up here and then he calls us .
Figure 1: The first row is an original segmented dialogue turn, where the $SEG label indicates the end
of a DA segment. The second row contains the corresponding DA label for each segment, where ?sd?
corresponds to the SWBD-DAMSL label of Statement non-opinion. The third row is the input for the
parser.
3.1 Parsing of spontaneous dialogues
One of the main problems we face when we try
to syntactically analyse a corpus transcribed from
spontaneous speech by different people such as
SWBD corpus, is the inconsistency of annotation
conventions for spontaneous speech phenomena
and punctuation marks. This can be problematic
for parsers, as they work at the sentence level.
Some of the dialogue turns of the SWBD corpus
are not transcribed using consistent punctuation
conventions. We therefore carried out some pre-
processing so that all turns end with proper punc-
tuation marks. Additionally, the non-verbal labels
(e.g. $LAUGH, $OVERLAP, $SEG, ...) are re-
moved. In Figure 1 there is an example of this
process.
The Stanford Parser, (Klein and Manning
(2003)) was used for the syntactic analysis of the
transcriptions of SWBD dialogues. The English
grammar used to train the parser is based on the
standard LDC Penn Treebank WSJ training sec-
tions 2-21. Is is important to remark that the nature
of the training corpus (journalistic style reports)
is different from the transcriptions of spontaneous
speech conversations. We would therefore expect
a decrease in accuracy. As output of the parsing
process, a tree that contains syntactic structures
was provided (e.g. see Figure 2).
3.2 Syntactic features and segmentation
As we are interested in studying the coincidence
of syntactic structures with DA segments, we will
select two general features for each word (see Fig-
ure 3):
? Most general syntactic category that starts
with a word, (MGSS), i.e., the root of the cur-
rent subtree of the syntactic analysis, (e.g. in
Figure 2, ?CC? is the MGSS of the first word
of the second segment, ?and?).
? Most general syntactic category that ends
with a word, (MGSE), i.e., the root of the
(ROOT
(S (: -)
(S
(NP (PRP he))
(VP (VBZ waits)
(SBAR (IN until)
(S
(NP (PRP it))
(VP (VBZ gets)
(PP (IN about)
(NP (NN seventeen)))
(PP (IN below)
(ADVP (RB up) (RB here))))))))
(CC and)
(S
(ADVP (RB then))
(NP (PRP he))
(VP (VBZ calls)
(NP (PRP us))))
(. .)))
Figure 2: Example of the syntactic analysis of the
dialogue turn that appears in Figure 1.
subtree of the syntactic analysis that ends
with that word, (e.g. in Figure 2, ?S? is
the MGSE of last word of the first segment,
?here?).
Using these features, we have analysed the syn-
tactic categories of boundary words of segments.
Particularly, it seems interesting to studyMGSE of
last word of the segment and MGSS of first word
of the segment, because it indicates which syntac-
tic structure ends before the segment boundary and
which one starts after it. As there is always the be-
ginning of a segment with the first word of the turn
and the end of a segment with the last word of the
turn, we are ignoring these for the analysis, be-
cause we are looking for intra-turn segments. Re-
sults of this analysis can be seen in Table 1.
4 The model
The statistical model used to DA label and
segment the dialogues is extensively explained
in (Mart??nez-Hinarejos (2008)). Basically, it is
334
ROOT+-+: $LAUGH S+he+NP VP+waits+VBZ SBAR+until+IN S+it+NP VP+gets+VBZ
PP+about+IN NP+seventeen+PP PP+below+IN ADVP+up+RB RB+here+S $SEG
CC+and+CC S+then+ADVP NP+he+NP VP+calls+VBZ NP+us+S .+.+ROOT $SEG
Figure 3: For each word of the example turn of Figure 1, MGSS (item before the word) and MGSE (item
after the word) are obtained from the tree of Figure 2. Non-verbal labels were reincorporated.
MGSE MGSS
Occ % Cat Occ % Cat
33516 37.1 , 30318 33.5 ROOT
30640 33.9 ROOT 19988 22.1 CC
7801 8.6 : 13275 14.7 NP
7134 7.9 S 10187 11.3 S
2687 3.0 NP 3508 3.9 SBAR
2319 2.6 PRN 3421 3.8 ADVP
750 0.8 VP 2034 2.2 VP
531 0.6 ADVP 1957 2.2 INTJ
478 0.5 PP 1300 1.4 UH
465 0.5 RB 972 1.1 PP
4078 4.5 Other 3481 3.8 Other
Table 1: Occurrences and percentage of the syn-
tactic categories that correspond with the most fre-
quent MGSE of the last segment word (except last
segment) andMGSS of the first segment word (ex-
cept first segment).
based on a combination of a Hidden Markov
Model at lexical level and a Language Model (n-
gram) at DA level. The Viterbi algorithm is used
to find the most likely sequence of DA labels ac-
cording to the trained models. The segmentation
is obtained from the jumps between DAs of this
sequence.
The previous section has shown that the MGSE
and MGSS for the segments boundary words are
concentrated in a small set of categories (see Ta-
ble 1). Therefore, one quick and easy way to in-
corporate this information to the existing model is
to add some restrictions during the decoding pro-
cess, giving the model:
U? = argmax
U
max
r,sr1
r?
k=1
Pr(uk|u
k?1
k?n?1) ?
?Pr(W sksk?1+1|uk)?(xsk)
where U? is the sequence of DAs that we will get
from the annotation/segmentation process. The
search process produces a segmentation s =
(s0, s1, . . . , sr), that divides the word sequence
W into the segments W s1s0+1W
s2
s1+1 . . .W
sr
sr?1+1.
Each segment is assigned to a DA ui that forms
the DA sequence U = u1 . . . ur. xi corresponds
to the syntactic features of the i word that can be
MGSE, MGSS or both of them, and
?(xi) =
?
?
?
1 if xi ? X
0 otherwise
where X can be a subset of all the possible syn-
tactic categories that correspond to:
1. the most frequent MGSE of last segment
word, if x is MGSE.
2. the most frequent MGSS of first segment
word, if x is MGSS
3. the most frequent combinations of both pre-
vious sets.
It means that we will only allow a segment end-
ing when the MGSE of a word is in this set, or
a start of a segment when the MGSS of the fol-
lowing word is in the corresponding set or both
conditions at the same time.
5 Experiments and results
Ten cross-validation experiments were performed
for each model using, in each experiment a train-
ing partition composed of 1136 dialogues and
a test set of 19 dialogues, as in (Stolcke et al
(2000); Webb et al(2005); Mart??nez-Hinarejos
et al(2006)). The N-grams were obtained using
the SLM toolkit (Rosenfeld (1998)) with Good-
Turing discounting and the HMMs were trained
using the Baum-Welch algorithm. We use the fol-
lowing evaluation measures:
? To evaluate the labelling, we use the DA Er-
ror Rate (equivalent to Word Error Rate) and
the percentage of error labelling of whole
turns.
? For the segment evaluation, we only check
where the segments bounds are produced
(word position in the segment), making use
of F-score obtained from precision and recall.
335
The results from using different sizes for the set
X are shown for labelling performance in Tables 2
and 3, and F-score of the segmentation in Table 4.
Model/SizeX 5 10 20 All
MGSE 53.31 54.76 54.60 54.76
MGSS 53.35 52.76 54.92 54.76
Both 53.58 52.84 54.76 54.76
Table 2: DAER for models using MGSE, MGSS
and both features. SizeX indicates the size of the
set of most frequent categories accepted. Without
syntactic categories (baseline) we obtain a DAER
of 54.41.
Model/SizeX 5 10 20 All
MGSE 53.61 55.41 55.34 55.77
MGSS 53.61 53.32 55.63 55.77
Both 53.46 53.10 55.19 55.77
Table 3: Percentage of error of labelling of com-
plete turns for all the possible models. The base-
line value is 55.41.
Model / SizeX 5 10 20 All
MGSE 73.08 71.18 71.44 71.17
MGSS 73.60 73.72 71.44 71.17
Both 74.36 74.08 71.75 71.16
Table 4: F-score of segmentation. The baseline
value is 71.17.
6 Discussion and future work
In this work, we have used lexical and syntactic
features for labelling and segmenting DAs simul-
taneously. Syntactic features obtained automati-
cally were deterministically applied during the sta-
tistical decoding process. There is a slight im-
provement using syntactic information, obtaining
better results than reported in other work such
as (Mart??nez-Hinarejos et al(2006)). The F-score
of the segmentation improves 3% using the syn-
tactic features, however values are slightly worse
(2%) than results in (Stolcke and Shriberg (1996)).
As future work, we think that incorporating the
syntactic information in a non-deterministic way
might further improve the annotation and segmen-
tation scores. Furthermore, it is possible to make
use of additional information from the syntactic
structure, rather than just the boundary informa-
tion we are currently using. Finally, an evalua-
tion over different corpora must be done to check
both the performance of the proposed model and
the reusability of the syntactic sets.
Acknowledgments
This work was partially funded by the Compan-
ions project (http://www.companions-project.org)
sponsored by the European Commission as part of
the Information Society Technologies (IST) pro-
gramme under EC grant number IST-FP6-034434.
References
Ang J., Liu Y., Shriberg E. 2005. Automatic Dialog Act
Segmentation and Classification in Multiparty Meet-
ings. Proc. ICASSP, Philadelphia, USA, pp. 1061-
1064
Ji, G and Bilmes, J. 2005. Dialog act tagging using
graphical models. Proc. ICASSP, Philadelphia, USA
Jurafsky, D. Shriberg, E., Biasca, D. 1997. Switchboard
swbd-damsl shallow- discourse-function annotation
coders manual. Tech. Rep. 97-01, University of Col-
orado Institute of Cognitive Science
Klein D. and Manning, C. D. 2003. Accurate Unlex-
icalized Parsing. Proc. ACL, Sapporo, Japan, pp.
423-430
Mart??nez-Hinarejos, C. D., Granell, R., Bened??, J. M.
2006. Segmented and unsegmented dialogue-act
annotation with statistical dialogue models. Proc.
COLING/ACL Sydney, Australia, pp. 563-570
Mart??nez-Hinarejos, C. D., Bened??, J. M., Granell, R.
2008. Statistical framework for a spanish spoken
dialogue corpus. Speech Communication, vol. 50,
number 11-12, pp. 992-1008
Rosenfeld, R. 1998. The cmu-cambridge statistical
language modelling toolkit v2. Technical report,
Carnegie Mellon University
Stolcke, A. and Shriberg, E. 1996. Automatic linguis-
tic segmentation of conversational speech. Proc. of
ICSLP, Philadelphia, USA
Stolcke, A., Coccaro, N., Bates, R., Taylor, P., van
Ess-Dykema, C., Ries, K., Shriberg, E., Jurafsky,
D., Martin, R., Meteer, M. 2000. Dialogue act
modelling for automatic tagging and recognition
of conversational speech. Computational Linguistics
26 (3), 1-34
Webb, N., Hepple, M., Wilks, Y. 2005. Dialogue act
classification using intra-utterance features. Proc. of
the AAAI Workshop on Spoken Language Under-
standing. Pittsburgh, USA
336
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 341?348,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Unsupervised Classification of Dialogue Acts using a Dirichlet Process
Mixture Model
Nigel Crook, Ramon Granell, and Stephen Pulman
Oxford University Computing Laboratory
Wolfson Building
Parks Road, OXFORD, UK
nigc@comlab.ox.ac.uk
ramg@comlab.ox.ac.uk
sgp@clg.ox.ac.uk
Abstract
In recent years Dialogue Acts have be-
come a popular means of modelling the
communicative intentions of human and
machine utterances in many modern di-
alogue systems. Many of these systems
rely heavily on the availability of dialogue
corpora that have been annotated with Di-
alogue Act labels. The manual annota-
tion of dialogue corpora is both tedious
and expensive. Consequently, there is a
growing interest in unsupervised systems
that are capable of automating the annota-
tion process. This paper investigates the
use of a Dirichlet Process Mixture Model
as a means of clustering dialogue utter-
ances in an unsupervised manner. These
clusters can then be analysed in terms of
the possible Dialogue Acts that they might
represent. The results presented here are
from the application of the Dirichlet Pro-
cess Mixture Model to the Dihana corpus.
1 Introduction
Dialogue Acts (DAs) are an important contribu-
tion from discourse theory to the design of di-
alogue systems. These linguistics abstractions
are based on the illocutionary force of speech
acts (Austin, 1962) and try to capture and model
the communicative intention of human or ma-
chine utterances. In recent years, several dia-
logue systems have made use of DAs for mod-
elling discourse phenomena in either the Dialogue
Manager (Keizer et al, 2008), Automatic Speech
Recogniser (Stolcke et al, 2000) or the Auto-
matic Speech Synthesiser (Zovato and Romportl,
2008). Additionally, they have been used also in
other tasks such as summarisation, (Murray et al,
2006). Therefore, a correct DA classification of di-
alogue turns can bring benefits to the performance
of these modules and tasks.
Many machine learning approaches have been
used to automatically label DAs. They are usu-
ally based on Supervised Learning techniques
involving combinations of Ngrams and Hidden
Markov Models (Stolcke et al, 2000; Mart??nez-
Hinarejos et al, 2008), Neural Networks (Garfield
and Wermter, 2006) or Graphical Models (Ji and
Bilmes, 2005). Relatively few approaches to DA
classification have been based on unsupervised
learning methods. Some promising results were
reported by Anderach et al(Andernach et al,
1997; Andernach, 1996) who applied Kohonen
Self Organising Maps (SOMs) to the problem of
DA classification. Although the SOM is nonpara-
metric in the sense that it doesn?t require that the
number of clusters to be found in the data be a pa-
rameter of the SOM that is specified before clus-
tering begins, it?s capacity to detect clusters is lim-
ited to the size of the two-dimensional lattice onto
which the clusters are projected, and the size of
this lattice is determined prior to clustering. This
paper investigates the use of an unsupervised, non-
parametric Bayesian approach to automatic DA
labelling: namely the Dirichlet Process Mixture
Model (DPMM). Specifically, the paper reports re-
sults from applying the Chinese Restaurant Pro-
cess (CRP), a popular approach to DPMMs, to
the automatic labelling of DAs in the Dihana cor-
pus. The Dihana corpus (J.M.Bened?? et al, 2006)
has previously been used for the same task but
with a supervised learning approach (Mart??nez-
Hinarejos et al, 2008). The results reported here
indicate that, treating each utterance as a bag of
words, the CRP is capable of automatically clus-
341
tering most utterances according to speaker, level
1 and in some cases level 2 DA annotations (see
below).
2 The Dihana corpus
The Dihana corpus consists of human-computer
spoken dialogues in Spanish about queuing infor-
mation of train fares and timetables. The acquisi-
tion was performed using the Wizard of Oz (WoZ)
technique, where a human simulates the system
following a prefixed strategy. User and system
utterances are different in nature, user utterances
are completely spontaneous speech whereas sys-
tem utterances are based on pre-written patterns
that the WoZ selected according to what the user
said in the previous turn, the current dialogue state
and the WoZ strategy. There is a total of 900 dia-
logues with a vocabulary of 823 words. However,
after applying a process of name entity recognition
(cities, times, number, ...) and making the distinc-
tion between system and user words there are 964
different words. The same process of name en-
tity recognition was also used by Martinez Hinare-
jos (Mart??nez-Hinarejos et al, 2008)
2.1 Annotation scheme
Dialogues were manually annotated using a dia-
logue act annotation scheme based on three lev-
els (see Table 1). The first level corresponds to
the general intention of the speaker (speech act),
the second level represents the implicit informa-
tion that is referred to in the first level and the third
level is the specific data provided in the utterance.
Using these three levels and making the distinc-
tion between user and system labels, there are 248
different labels (153 for the user and 95 for the sys-
tem). Combining only first and second level there
are 72 labels (45 for user and 27 for system), and
with only first level there are 16 labels (7 for user
and 9 for system).
Annotation was done at utterance level. That
is, each dialogue turn was divided (segmented)
into utterances such that each one corresponds to a
unique DA label. An example of the segmentation
and annotation of two turns of a dialogue can be
seen in Figure 1
3 Dirichlet Process Mixture Models
This paper present a Dirichlet Process Mixture
Model (DPMM) (Maceachern and Mu?ller, 1998;
Escobar and West, 1995; Antoniak, 1974) for the
Level Labels
First Opening, Closing, Confirmation,
Undefined, Not-understood, Waiting,
Consult, Acceptance, Rejection
Second Departure-hour, Arrival-hour,
Fare, Origin, Destination, Day,
Train-Type, Service, Class, Trip-time
Third Departure-hour, Arrival-hour,
Fare, Origin, Destination, Day,
Train-Type, Service, Class,
Trip-time, Order-number,
Number-trains, Trip-type
Table 1: Set of dialogue act labels used in the Di-
hana corpus
automatic, unsupervised clustering of the utter-
ances in the Dihana corpus. This approach treats
each utterance as a bag of words (i.e. an unordered
collection of words) (Sebastiani, 2002). Utter-
ances are clustered according to the relative counts
of word occurrences that they contain so that utter-
ances with similar histograms of word counts will,
in general, appear in the same cluster.
Bayesian methods for unsupervised data clus-
tering divide into parametric and nonparametric
approaches. Parametric approaches to clustering
such as Finite Bayesian Mixture Models (Mclach-
lan and Peel, 2000) require prior estimation of the
number of clusters that are expected to be found
in the data. However, it is not always possible to
know this in advance and often it is necessary to
repeat a modelling experiment many times over a
range of choices of cluster numbers to find an op-
timal number of clusters. Sub-optimal choices for
the number of clusters can lead to a degradation
in the generalisation performance of the model.
Nonparametric approaches to mixture modelling,
on the other hand, do not require prior estimates
of the number of clusters in the data; this is dis-
covered automatically as the model clusters the
data. Dirichlet Processes offer one approach to de-
veloping Bayesian nonparametric mixture models.
The remainder of this section briefly introduces
DPMMs, beginning with a brief look at finite
Bayesian mixture models which will serve as use-
ful background for presenting the Chinese Restau-
rant Process, the Dirichlet Process paradigm used
in this paper.
342
Speaker Utterance Transcription
Level 1 Level 2 Level 3
S S1 Welcome to the railway information system. How may I help you?
Opening Nil Nil
U U1 Could you tell me the departure times from Valencia
Question Departure-hour Origin
U2 to Madrid .
Question Departure-hour Destination
Figure 1: An example of some turns from an annotated dialogue of DIHANA corpus.
Figure 2: A 3-simplex with two examples points
and the corresponding distributions
3.1 Finite Bayesian Mixture Models
A Dirichlet distribution is defined as a measure
on measures. Specifically, a Dirichlet distribution
defines a probability measure over the k-simplex.
The k-simplex is a convex hull constructed so that
each point on the surface of the simplex describes
a probability distribution over k outcomes:
Qk = {(x1, . . . , xk) : xi ? 0
?i ? {1 . . . k},
k
?
i=1
xi = 1}
Figure 2 shows a 3-simplex with two example
points and the corresponding distributions. The
Dirichlet distribution places a probability measure
over the k-simplex so that certain subsets of points
on the simplex (i.e. certain distributions) have
higher probabilities than others (Figure 3). The
probability measure in the Dirichlet is parame-
terised by a set of positive, non-zero concentra-
tion constants ? = {?1, . . . ?k : ?i > 0}, written
Dirichletk(?1, . . . ?k). The effects of different
values of ? for the 3-simplex are shown in Figure
3.
The probability density function of the Dirichlet
Figure 3: Three example Dirichlet Distributions
over the 3-simplex with darker regions showing
areas of high probability: (a) Dirichlet(5,5,5), (b)
Dirichlet(0.2, 5, 0.2), (c) Dirichlet(0.5,0.5,0.5).
343
distribution is given by:
Dirichletk(?1, . . . , ?k) = f(x1, . . . , xk;?1, . . . , ?k)
= ?(
?k
i=1 ?i)
?k
i=1 ?(?i)
k
?
i=1
xai?1i
where ?(x) (=
? ?
0 t(x?1)e?tdt) extends the fac-
torial function to the real numbers. Since a
draw from a Dirichlet distribution (written ? ?
Dirichletk(?)) gives a distribution, a Dirichlet
can be used as the prior for a Bayesian finite mix-
ture model:
? ? Dirichletk(?1, . . . , ?k)
? is a distribution over the k components ? of
the finite mixture model. Each component ?zi is
drawn from a base measure G0 (?zi ? G0). The
choice of distribution G0 depends on the nature
of the data to be clustered; with data that is rep-
resented using the bag of words model, G0 must
generate distributions over the word vocabulary.
Hence the Dirichlet distribution is an appropriate
choice in this case:
?zi ? Dirichletv(?1, . . . , ?v)
where v is the size of the vocabulary.
For each data point (utterance) xi a component
?zi is selected by a draw zi from the multinomial
distribution ?:
zi ? Multinomialk(?)
A suitable distribution F (?zi) is then used to draw
the data point (utterance). In the bag of words
model, the multinomial distribution is used to
draw the words for each data point xi:
xi ? Multinomialv(?zi)
A small example will illustrate this generative
process. Imagine that there are just two types
of utterances with a vocabulary consisting sim-
ply of the words A, B and C. A finite Bayesian
mixture model in this case would first draw ?
from a suitable Dirichlet distribution (e.g. ? ?
Dirichlet2(0.5, 1)) as, for example, is shown in
Figure 4(a). Next the two components ?z1 and
?z2 would be drawn from a suitable base distribu-
tion G0 (e.g. ?z1 ? Dirichlet3(1, 0.5, 0.5) and
?z2 ? Dirichlet3(0.5, 0.5, 1), see Figure 4(b)
and 4(c)). In this case, ?z1 will tend to generate
Figure 4: An example finite Bayesian mixture
model. (a) The prior distribution over components
?z1 (b) and ?z2 (c)
utterances containing more occurrences of word
A than B or C, whilst ?z2 will tend to gener-
ate utterances with more C?s than A?s or B?s. A
component zi is then selected for each utterance
(zi ? Multinomialk(?)). Note that in this ex-
ample, the distribution ? would lead to more utter-
ances generated by ?z2 than by ?z1 . Suppose that
five utterances are to be generated by this model
and that the components for each utterance are
z1 = 1, z2 = 2, z3 = 2, z4 = 1 and z5 = 2.
The words in each utterance are then generated
by repeated draws from the corresponding com-
ponent (e.g. x1 = ACAAB, x2 = ACCBCC,
x3 = CCC, x4 = CABAAC and x5 = ACC).
3.2 Dirichlet Processes
A Dirichlet Process can be thought of as an exten-
sion of a Dirichlet distribution where the dimen-
sions of the distribution are infinite. The prob-
lem with the infinite dimension Dirichlet distri-
bution, though, is that its probability mass would
be distributed across the whole of the distribution.
However, in most practical applications of mixture
modelling there will be a finite number of clusters.
The solution is to have a process which will tend
to place most of the probability mass at the be-
ginning of the infinite distribution, thereby mak-
ing it possible to assign probabilities to clusters
without restricting the number of clusters avail-
able. The GEM stick breaking construction (the
name comes from the first letters of Griffiths, En-
gen and McCloskey (Pitman, 2002)) achieves pre-
cisely this (Pitman and Yor, 1997). Starting with
344
a stick of unit length, random portions ??k are re-
peatedly broken off the stick, with each part that
is broken off representing the proportion of prob-
ability assigned to a component:
??k ? Beta(1, ?) ?k =
?k?1
i+1 (1? ??i) ? ??k
The Dirichlet Process mixture model can now
be specified as:
? ?GEM(?) ?zi ?G0 zi ? (1 . . .?)
zi ?Multinomial(?) xi ? F (?zi)
3.3 Chinese Restaurant Process
The Chinese Restaurant Process (CRP) is a popu-
lar Dirichlet Process paradigm that has been suc-
cessfully applied to many clustering problems. In
the CRP, one is asked to imagine a Chinese restau-
rant with an infinite number of tables. The cus-
tomers enter the restaurant and select, according to
a given distribution, a table at which to sit. All the
customers on the same table share the same dish.
In this paradigm, the tables represent data clusters,
the customers represent data points (xi) and the
dishes represent components (?z). As each cus-
tomer (data point) enters the restaurant the choice
of which table (cluster) and therefore which dish
(component) is determined by a draw from the fol-
lowing distribution:
?i|?1, . . . , ?i?1 ?
1
(? + i? 1)
?
?
i?1
?
j=1
??j + ?G0
?
?
where ? is the concentration parameter for the
CRP. The summation over the ??j ?s counts the
number of customers sat at each of the occupied
tables. The probability of sitting at an already oc-
cupied table, therefore, is proportional to the num-
ber of customers already sat at the table, whilst the
probability of starting a new table is proportional
to ?G0. Figure 5 illustrates four iterations of this
initial clustering process.
Once all the customers (data points) have been
placed at tables (clusters), the inference process
begins. The posterior p(?,?, z|x) cannot be cal-
culated exactly, but Gibbs sampling can be used.
Gibbs sampling for the CRP involves iteratively
removing a randomly selected customer from their
table, calculating the posterior probability distri-
bution across all the occupied tables together with
a potential new table (with a randomly drawn dish,
Figure 5: The first four steps of the initial cluster-
ing process of the CRP. The probability distribu-
tion over the tables is also shown in each case.
i.e. component), and making a draw from that dis-
tribution to determine the new table for that cus-
tomer. The posterior distribution across the tables
is calculated as follows:
?i|?1, . . . ,?i?1,x
? 1B
?
?
i?1
?
j=1
??jp(xi|?j) + ?G0p(xi|?i)
?
?
where B = ?p(xk) +
?i?1
j=1 p(xi|?i) is the nor-
malising constant. After a predetermined number
of samples, the dish (component) of each occupied
table is updated to further resemble the customers
(data points) sitting around it. In the bag of words
approach used here, this involves converting the
histogram of word counts in each customer (utter-
ance) sitting at the table into an empirical distribu-
tion H(xi), taking the average of these empirical
distributions and modifying the dish (component)
to further resemble this distribution:
?i = ?i +
?
mi
mi
?
j=1
H(xj)
where ? (0 ? ? < 1) is the learning con-
stant and mi is the number of customers around
345
table i. The inference process continues to it-
erate between Gibbs sampling and updating the
table dishes (components) until the process con-
verges. Convergence can be estimated by observ-
ing n consecutive samples in which the customer
was returned to the same table they were taken
from.
4 Results
The CRP with Gibbs sampling was used to clus-
ter both user and system utterances from the 900
dialogues in the Dihana corpus. Each utterance is
treated as an independent bag of words where all
information about the dialogue that it came from
and the context in which it was uttered is ignored
during training. Intra-cluster and inter-cluster sim-
ilarity measures were used to evaluate the resulting
clusters. Intra-cluster similarity S?i is calculated
by averaging the Euclidean distance between ev-
ery pair of data points in the cluster i:
S?i =
1
2mi
mi
?
i=1;j=1
|xi ? xj |
Inter-cluster similarity S?? is calculated by sum-
ming the Euclidean distance between the centroids
of all pairs of clusters:
S?? =
n
?
i=1;j=1
|Ci ? Cj |
where Ci is the centroid of cluster i and n is the
number of clusters.
Two classification error measures were also
used, one from the cluster (table) perspective E?,
and the other from the perspective of the Dialogue
Act (DA) annotations (first level) of the Dihana
corpus E??. The cluster classification error of ta-
ble i is calculated by summing up the occurrences
of each DA on the table, finding the DA with the
largest total and allocating that DA as the correct
classification for that table Di. The number of
false positives fpi for that table is the count of all
customers (utterances) with DA annotations not in
Di. The number of false negatives fni is the count
of utterances with label Di that occur on other ta-
bles. The cluster classification error for table i is
therefore:
E?i =
1
n(f
p
i + fni )
The DA classification error E??i measures how
well DA i has been clustered, using the size of the
Cluster
No. Ans Ask Clo Not Rej Und
1 1 5
4 2 91 2
9 2 1 9
12 7 161 1 1
13 273 26 8
14 382 12 1 5
15 6 1 909 1 327 22
17 47 39 1 1
18 73 1 3
19 1 4
20 131 115 1 3 1
22 270 29 3 3
23 135 8 2 2
25 83 31 1 4
28 247 16 1 4
29 349 6 1 12
33 13 3 5 1 4 25
41 202 45 1 2 3
46 4 1
49 6 251 1 2 4
51 124 896 1 12
53 45 477 10
Table 2: Clusters of user utterances, with the
counts for each level 1 speech act. The largest
cluster for each speech act is in bold. The abbrevi-
ations are: Und = Undefined, Ans = Answering,
Ask = Asking, Clo = Closing, Rej = Rejection,
Not = Not-understood.
DA classN ci , the size of the largest cluster of utter-
ances from that DA classM ci , and the total number
of utterances n in the corpus:
E??i =
1
n(N
c
i ?M ci )
Table 6 summarises the results from three sep-
arate runs of the CRP, each increasing in number
of epochs. It should be noted here that the Dihana
corpus has 72 DA categories, so the ideal number
of clusters discovered by the CRP would be 72. It
should also be noted that given an initial random
clustering, a good clustering algorithm will reduce
intra-cluster similarity (S??), increase inter-cluster
similarity (S??) and reduce the classification errors
(E?? and E???).
346
Epochs (K) No. Clusters S?? S?? E?? E???
0 70 99703.6 243.74 0.05303 0.00979
1000 44 14975.4 217.56 0.01711 0.00385
1500 54 10093.7 336.15 0.01751 0.00435
Figure 6: The results from three separate runs of the CRP on utterances from the Dihana corpus. Cluster
similarity measures and classification error values are shown after 0 (i.e. random clustering), 1000K, and
1500K epochs. S??, E?? and E??? are averaged values.
Level 1 Level 2 Cluster
No.
Answering Day 14
Destination 22
Fare 29
Departure-hour 28, 41
Asking Departure-hour,Fare 4
Train-type 12
Fare 49
Departure-hour 51, 53
Table 3: Clusters that have specialised on level 1
and level 2 annotations.
5 Discussion
The first row of the table in Figure 6 shows the
cluster similarity measures and classification er-
rors after 0 epochs of the inference procedure (i.e.
for a random clustering of utterances). This gives a
baseline for the measures and error values used in
subsequent runs. The second row of values shows
the results after a run of 1000K epochs of the in-
ference procedure. This run finds only 44 clusters
but has a much lower value for S?? than was found
in the random clustering, showing a significant in-
crease in the similarity between utterances within
each cluster. Surprisingly, the value for S?? is also
reduced, showing that the differentiation between
the clusters formed at this stage is even lower than
there was with the random clustering. E?? and E???
show suitable reductions indicating that the classi-
fication errors are being reduced by the inference
process. The third row of values show that after
1500K epochs 54 clusters have been found, intra-
cluster similarity is increased beyond that for the
random clustering, but the classification errors re-
main essentially the same as for the 1500K run.
Although the 1500K epoch run found only 54
clusters, it was able to clearly distinguish between
system and user utterances: with 30 clusters con-
taining system utterances only, 22 clusters con-
taining user utterances only and 2 clusters contain-
ing instances of both. Given that the system utter-
ances in the Dihana corpus are generated from a
restricted set of sentences, it is not surprising that
these were easy to cluster and differentiate from
user utterances. However, the CRP was also able
to cluster user utterances well, which is more of
a challenge. Table 2 shows the clusters that have
specialised on user utterances, with the counts of
the level 1 annotations in each case. The largest
cluster for each level 1 annotation is shown in bold
typeface. From here it can be seen that cluster 15
has specialised on bothClosing and Rejection. It is
not surprising that these fall within the same clus-
ter since the words used in each are often the same
(e.g. ?No thank you? can act as either a closing
statement or a rejection statement). Clusters 14,
22, 29, 28 and 41 have specialised to the Answer-
ing annotation, whilst clusters 4, 12 49, 51 and 53
have specialised to Asking. Table 3 shows how
each of these clusters have specialised to level 2
annotations. Cluster 14, for example, specialises
on the Answering:Day pair, whilst 22 specialises
on Answering:Destination pair.
These initial results show that, at least for the
Dihana corpus, the DPMM can successfully clus-
ter utterances into Speaker, Level 1, and Level2
classes. Whilst this looks promising, it must be
acknowledged that the Dihana corpus is restricted
to train service inquiries and it remains unclear
whether this approach will generalise to other di-
alogue corpora with a broader range of topics and
wider vocabularies. Future work will include in-
vestigating the use of ngrams of words, syntactic
features, the DAs of previous utterances and ex-
perimentation with other corpora such as Switch-
board (Godfrey et al, 1992).
Acknowledgments
This work was funded by the Companions project
(www.companions-project.org) sponsored by the
347
European Commission as part of the Information
Society Technologies (IST) programme under EC
grant number IST-FP6-034434. We thank Jeff
Bilmes (University of Washington) for many very
helpful discussions about Dirichlet processes and
their application.
References
Toine Andernach, Mannes Poel, and Etto Salomons.
1997. Finding classes of dialogue utterances with
kohonen networks. In In Daelemans, pages 85?94.
J.A. Andernach. 1996. A machine learning approach
to the classification and prediction of dialogue utter-
ances. In Proceedings of the 2nd International Con-
ference on New Methods in Language Processing,
pages 98?109.
Charles E. Antoniak. 1974. Mixtures of dirichlet pro-
cesses with applications to bayesian nonparametric
problems. The Annals of Statistics, 2(6):1152?1174.
J.L. Austin. 1962. How to do things with words. Ox-
ford: Clarendon Press.
Michael D. Escobar and Mike West. 1995. Bayesian
density estimation and inference using mixtures.
Journal of the American Statistical Association,
90(430):577?588.
Sheila Garfield and Stefan Wermter. 2006. Call clas-
sification using recurrent neural networks, support
vector machines and finite state automata. Knowl.
Inf. Syst., 9(2):131?156.
J. J. Godfrey, E. C. Holliman, and J. Mcdaniel. 1992.
SWITCHBOARD: telephone speech corpus for re-
search and development. In Proc. ICASSP, vol-
ume 1, pages 517?520 vol.1.
Gang Ji and J. Bilmes. 2005. Dialog act tagging using
graphical models. In Acoustics, Speech, and Signal
Processing, 2005. Proceedings. (ICASSP ?05). IEEE
International Conference on, volume 1, pages 33?
36.
J.M.Bened??, E.Lleida, A. Varona, M.J.Castro,
I.Galiano, R.Justo, I. Lo?pez, and A. Miguel. 2006.
Design and acquisition of a telephone spontaneous
speech dialogue corpus in spanish: Dihana. In Fifth
International Conference on Language Resources
and Evaluation (LREC), pages 1636?1639, Genova,
Italy, May.
S. Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu,
and S. Young. 2008. Modelling user behaviour
in the his-pomdp dialogue manager. In IEEE SLT,
pages 121?124, Dec.
Steven N. Maceachern and Peter Mu?ller. 1998. Esti-
mating mixture of dirichlet process models. Jour-
nal of Computational and Graphical Statistics,
7(2):223?238.
C. D. Mart??nez-Hinarejos, J. M. Bened??, and
R. Granell. 2008. Statistical framework for a span-
ish spoken dialogue corpus. Speech Communica-
tion, 50:992?1008.
Geoffrey Mclachlan and David Peel. 2000. Finite Mix-
ture Models. Wiley Series in Probability and Statis-
tics. Wiley-Interscience, October.
Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2006. Incorporating speaker and
discourse features into speech summarization. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 367?374, Morristown, NJ, USA.
Association for Computational Linguistics.
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?900.
J. Pitman. 2002. Combinatorial stochastic processes.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34(1):1?47, March.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Comput. Linguist., 26(3):339?373.
E. Zovato and J. Romportl. 2008. Speech synthesis
and emotions: a compromise between flexibility and
believability. In Proceedings of Fourth International
Workshop on Human-Computer Conversation, Bel-
lagio, Italy.
348
