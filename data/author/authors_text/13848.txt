Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 170?177,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Modeling User Satisfaction with Hidden Markov Models 
 
 
Klaus-Peter Engelbrecht, Florian G?dde, Felix Hartard, Hamed Ketabdar, Sebastian 
M?ller 
Deutsche Telekom Laboratories, Quality & Usability Lab, TU Berlin,  
Ernst-Reuter-Platz 7, 10587 Berlin, Germany 
{Klaus-Peter.Engelbrecht,Florian.Goedde, 
Hamed.Ketabdar,Sebastian.Moeller}@telekom.de 
Felix.Hartard@Berlin.de 
 
  
 
Abstract 
Models for predicting judgments about 
the quality of Spoken Dialog Systems 
have been used as overall evaluation 
metric or as optimization functions in 
adaptive systems. We describe a new 
approach to such models, using Hidden 
Markov Models (HMMs). The user?s 
opinion is regarded as a continuous 
process evolving over time. We present 
the data collection method and results 
achieved with the HMM model. 
1 Introduction 
Spoken Dialog Systems (SDSs) are now widely 
used, and are becoming more complex as a result 
of the increased solidity of advanced techniques, 
mainly in the realm of natural language 
understanding (Steimel et al 2008). At the same 
time, the evaluation of such systems increasingly 
demands for testing the entire system, as 
components for speech recognition, language 
understanding and dialog management are 
interacting more deeply. For example, the system 
might search for web content on the basis of 
meaning extracted from an n-best list, and 
generate the reply and speech recognition 
grammars depending on the content found 
(Wootton et al 2007). The performance of single 
components strongly depends on each other 
component in this case. 
While performance parameters become less 
meaningful in such a system, the system?s 
overall quality, which can only be measured by 
asking the user (Jekosch 2005), gains interest for 
the evaluation. Typically, users fill out 
questionnaires after the interaction, which cover 
various perceptional dimensions such as 
efficiency, dialog smoothness, or the overall 
evaluation of the system (Hone and Graham, 
2001; ITU-T Rec. P.851, 2003; M?ller 2005a). 
Judgments of the system?s overall quality can be 
used to compare systems with respect to a single 
measure, which however comprises all relevant 
aspects of the interaction. Thus, the complexity 
of the evaluation task is reduced. 
In addition, user simulation is increasingly 
used to address the difficulty of foreseeing all 
possible problems a user might encounter with 
the system (e.g. Ai and Weng, 2008; Engelbrecht 
et al, 2008a; Chung, 2004; L?pez-C?zar et al, 
2003). In order to evaluate results from such 
simulations, some approaches utilize prediction 
models of user judgments (e.g. Ai and Weng, 
2008; Engelbrecht et al, 2008a). 
Currently, prediction models for user 
judgments are based on the PARADISE 
framework introduced by Walker et al (1997). 
PARADISE assumes that user satisfaction 
judgments describe the overall quality of the 
system, and are causally related to task success 
and dialog costs, i.e. efficiency and quality of the 
dialog. Therefore, a linear regression function 
can be trained with interaction parameters 
describing dialog costs and task success as 
predictors, and satisfaction ratings as the target. 
The resulting equation can then be used to 
predict user satisfaction with unseen dialogs. 
In follow-up studies, it could be shown that 
such models are to some degree generalizable 
(Walker et al, 2000). However, also limitations 
of the models in predicting judgments for other 
user groups, or for systems with different levels 
of ASR performance, were reported (Walker et 
al., 1998). In the same study, prediction 
170
functions for user satisfaction were proposed to 
serve as optimization function in a system 
adapting its dialog strategy during the interaction. 
This idea is taken up by Rieser and Lemon 
(2008).  
The prediction accuracy of PARADISE 
functions typically lies around an R2 of 0.5, 
meaning that 50% of the variance in the 
judgments is explained by the model. While this 
number is not absolutely satisfying, it could be 
shown that mean values for groups of dialogs 
(e.g. with a specific system configuration) can be 
predicted more accurately than single dialogs 
with the same models (Engelbrecht and M?ller, 
2007). Low R2 for the predictions of ratings of 
individual dialogs seems to be due to inter-rater 
differences at least to some degree. Such 
differences have been described, and may 
concern the actual perception of the judged issue 
(Guski, 1999), or the way the perception is 
described by the participant (Okun and Weir, 
1990; Engelbrecht et al, 2008b) 
We have tested the PARADISE framework 
extensively, using different classifier models and 
interaction parameters. Precise and general 
models are hard to achieve, even if the set of 
parameters describing the interaction is widely 
extended (M?ller et al, 2008). In an effort to 
improve such prediction models, we developed 
two ideas: 
? Predict the distribution of ratings which 
can be expected for a representative group 
of users given the same stimulus. This 
takes into account that in most cases the 
relevant user characteristics determining 
the judgment cannot be tracked, or even 
are unknown. 
? Consider the time relations between 
events by modeling user opinion as a 
variable evolving over the course of the 
dialog. This way, time relations like co-
occurrence of events, which affect quality 
perception, attention, or memory can be 
modeled most effectively. 
In this paper, we present a new modeling 
approach considering these ideas. In Section 2, 
we introduce the topology of the model. 
Following this, we report how training data for 
the model were obtained from user tests in 
Section 3. Evaluation results are presented in 
Section 4 and discussed in Section 5, before we 
conclude with some remarks on follow-up 
research. 
2 Modeling Judgments with HMMs 
Hidden Markov Models (HMMs) are often used 
for classifying sequential stochastic processes, 
e.g. in computational linguistics or bio-
informatics. An HMM models a sequence of 
events as a sequence of states, in which each 
state emits certain symbols with some probability. 
In addition, the transitions between states are 
probabilistic. The model is defined by a set of 
state symbols, a set of emission symbols, the 
probabilities for the initial state, the state 
transition matrix, and the emission matrix. The 
transition matrix contains the probabilities for 
transitions from each state to each other state or 
itself. The emission matrix contains the 
probabilities for each emission symbol to occur 
at each state.  
While the sequence of emissions can be 
observed, the state sequence is hidden. However, 
given an emission sequence, standard algorithms 
defined for the HMM allow to calculate the 
probability of each state at each point in the 
sequence. The probability for the model to be in 
a state is dependent on the previous state and the 
emissions observed at the current state. 
As illustrated by Figure 1, the development of 
the users? opinions can be modelled as an HMM. 
The user judgment about the dialog is modelled 
as states, each state representing a specific 
judgment (think of it as ?emotional states?). A 
prediction is made at each dialog turn. In the 
model depicted, the user judgment can either be 
?bad? or ?good?. Each judgment has a 
probabilistic relation to the current events in the 
dialog. In the picture, the events are described in 
the form of understanding errors and 
confirmation types, i.e. there are two features 
which can take a number of different values, 
each with a certain probability.  
Although the judgments do not ?emit? the 
events at each turn (the causal relation is 
opposite), the probabilistic relation between them 
can be captured and evaluated with the HMM 
and the associated algorithms. 
Apart from the dialog events, the current 
judgment is also determined by the previous 
judgment. For example, we expect that the 
judgments are varying smoothly, i.e. the 
probability for a transition becomes lower with 
increasing (semantic) distance between the state 
labels. 
Although events in previous turns cannot 
impact the current judgment given this model 
topology, it is possible to incorporate dialog 
171
history by creating features with a time lag. E.g., 
a feature could represent the understanding error 
in the previous turn. Also, simultaneity of 
different events affecting the quality perception 
can be evaluated by calculating probabilities for 
each judgment given the observed combination 
of features. If the features are interacting (i.e. the 
probability of one feature changes in dependence 
of another feature), this is modelled by directly 
specifying the emission probabilities for each 
combination of features. We call this a layer of 
emissions. Additional layers with other features 
can be created. In this case, the likelihood of 
each judgment given probabilities from each 
layer can be calculated by multiplication of the 
probabilities from each layer. 
For the calculation of state probabilities, we 
can use forward recursion (Rabiner, 1989). The 
algorithm proceeds through the observed 
sequence, and at each step calculates the 
probability for each state given the probabilities 
of the observation, the probabilities of each state 
at the previous step, and the transition 
probabilities. 
 
Figure 1. Topology of an HMM to model user 
judgments (?good? or ?bad?) in their 
probabilistic relation to dialog events (error and 
confirmation strategy) and the previous rating. 
3 Data Collection 
In order to train the model, data is needed in 
which the association between dialog events and 
judgments at each turn is represented. Therefore, 
we conducted an experiment in which users 
interacted with an SDS and judged the 
interaction after each dialog turn. In addition, all 
users followed the same five dialogs, which was 
possible by using the Wizard-of-Oz technique 
(Fraser and Gilbert 1991) and relatively strict 
scenario descriptions. 
As the interactions were predefined, care was 
taken that a representative sample of relevant 
usability problems were passed in the 
interactions. Such problems were collected in a 
brainstorming session, and comprise 
understanding errors, task failure, prompt 
wording issues (such as incomplete information), 
missing control options, or not being able to 
input one of the criteria specified in the scenario 
description. As further features of the dialog, 
which might impact the quality judgment, the 
number of concepts which can be specified, as 
well as the confirmation strategy, should vary 
along the interactions. 
We then designed a dialog system with a 
consistent strategy, which however allowed for 
the quality-relevant issues mentioned above to 
occur in the dialogs. The system was derived 
from the BoRIS restaurant information system 
(M?ller 2005b), which enables the search of 
restaurants in the German city Bochum 
according to the criteria day, time, location, type 
of cuisine and pricing. For our system, we 
simplified the dialog strategy to system initiative, 
however, at the first turn, the users could say 
both day and time. If both were recognized, 
explicit confirmation was requested. If just one 
constraint was understood, confirmation was im-
plicit. The last constraint specified (pricing) was 
not confirmed, and instead the restaurant 
information was output directly. If no restaurant 
was found according to the specified criteria, the 
user could change one of the constraints. Table 1 
shows an example of a dialog with the system. 
We also reformulated some prompts to fit the 
changed dialog strategy, and partly to 
incorporate potential wording problems. Prompts 
were recorded with a non-professional speaker, 
S: Welcome to the telephone-based restaurant 
information system for Bochum. When would you 
like to eat? 
U: Tomorrow night. 
S: So you want to eat tonight? 
U: No. 
S: On which day would you like to eat? 
?. 
U: Medium pricing 
S: Unfortunately, I could not find a restaurant 
matching your query. Would you like to change 
time, location, cuisine or pricing? 
U: Yes, cuisine. 
S: Which type of cuisine are you searching for? 
U: Italian. 
S: The possible restaurants are: {name, address}  
Table 1. Example dialog with the BoRIS 
restaurant information system, version as used 
in the experiment. 
172
using high-quality audio equipment. During the 
interactions, the wizard simply replayed the 
prompt foreseen at the current state of the 
predefined dialog script. In addition to the 
foreseen prompts, the wizard had at hand no-
input and help prompts in case the user would 
behave unexpectedly. 
25 users (13 females, 12 males), recruited 
from the campus, but not all students, 
participated in the experiment. Participants were 
aged between 20 and 46 years (M=26.5; 
STD=6.6). Ratings were given on a 5-point scale, 
where the points were labeled ?bad?, ?poor?, 
?fair?, ?good?, and ?excellent?. Ratings were 
input through a number pad attached to the scale. 
Each participant rehearsed the procedure with a 
test dialog. Before the experiment, all users filled 
out a questionnaire measuring their technical 
affinity. 
As the data collected in the described experi-
ment are all needed to train the prediction model 
for as many combinations of feature values as 
possible, we conducted a second experiment to 
generate test data. For this test, we asked 17 per-
sons from our lab to conduct two dialogs with 
the system mock-up. The test setup was the same 
as in the previous experiment, except that new 
dialogs were created without particular 
requirements or restrictions. 
In both experiments, not all users behaved as 
we hoped. Therefore, not all of the predefined 
dialog scripts were judged by all participants 
(N=15?23 for training corpus, N=9?13 for test 
corpus; N: number of valid dialogs). For one 
dialog script in the training corpus, the deviating 
interactions were all equal (N=9), so 
distributions of ratings per turn could be 
calculated for comparison with the predicted 
distributions for this dialog. For the training and 
calculation of initial state probabilities, all dia-
logs in the training corpus were used. 
The model derived from the data includes five 
possible states (one for each rating). For a list of 
features annotated in the dialogs see Figure 2. 
4 Results 
In order to evaluate the modeling approach, we 
first searched for the best model given the 
training data from the first experiment. We then 
applied this model to the test data from the 
second experiment in order to evaluate the model 
accuracy given unseen data. Afterwards, we 
examined if another model trained on the 
training set can predict the test set better, i.e. we 
?optimized? the model on the test data. Finally, 
we cross-check how well the model optimized on 
the test data performs on the training data, which 
gives a glimpse at how much the model is biased 
towards the test data. 
As the criterion for the optimization, we deter-
mined the mean squared error (MSE), and 
averaged across all dialog script in the corpus on 
which the model was optimized. For each dialog 
script, all 5 probabilities (ratings ?bad? to 
?excellent?) at each dialog turn were taken into 
account, i.e. the squared prediction errors were 
added. If rate is the rating, then 
 
As this measure, in the particular way we 
applied it here, is not easily comparable to other 
results, we add two pictures illustrating the 
accuracy represented either by a rather low or by 
a rather high MSE. In addition, we report the 
mean absolute error (MAEmax) of the models in 
predicting the most likely rating at each state 
(mean rating if two ratings with equal probability) 
and the baseline performance when the 
unconditional distribution of ratings is predicted. 
We first optimized a model on the training 
data, meaning that we selected parameters, 
trained the HMM with these parameters on the 
training data and then predicted results for all 6 
dialog scripts contained in the training set (top of 
Feature Values 
understanding 
errors 
PA:PA (partially correct) 
PA:FA (failed) 
PA:IC (incorrect) 
confirmation 
strategy 
explicit 
implicit 
none 
system speech 
act 
ask for 2 constraints 
ask for 1 constraint 
ask for selection of a constraint 
provide info 
user speech act 
 
provide info 
repeat info  
confirm 
meta communication 
no-input 
contextual 
appropriateness 
(Grice?s 
maxims) 
manner 
quality 
quantity 
relevance 
task success success 
failure 
Table 2. Annotated dialog features. 
[ ]
n
ratepratep
MSE
n
turn rate
predemp
dial
? ?
= =
?
= 1
5
1
2)()(
173
Table 3). The optimized model was chosen as the 
one returning the smallest MSE (mean of all 
tasks). The best model included understanding 
errors interacting with confirmation type at each 
turn, and interacting with task success. As we 
analyzed the prediction results, we found that 
whenever the system changed from asking two 
constraints at a time to just one (which is done in 
order to avoid multiple errors in a row), the 
predictions were too positive. We therefore 
introduced a new feature, which is annotated 
whenever the system asks for a single constraint 
which has been asked in a more complex 
question before (?dummy?). In the model 
optimized on training data, this parameter was 
included on a separate feature layer. That is, this 
feature impacts quality perception independent 
of the other features? values. 
 We then used this model to predict the test 
data collected in the second experiment (top of 
Table 4). As expected, the MSE clearly increases; 
however, this was partly due to the difference in 
the sample of participants. As in the second 
experiment participants were recruited from our 
lab, their technical affinity was relatively high. 
Therefore, we retrained the HMM with only 
those 50% of the users from the training set who 
got the highest score on the technical affinity 
questionnaire. With this model, the prediction of 
test data improved.  
In a next step, we optimized the model on the 
test set meaning that we searched for the 
parameter combination achieving the best result 
on the two test dialogs. However, the model was 
still trained on the training data from the first 
experiment. As expected, the MSE could be 
improved. However, only minor changes in the 
feature configuration are necessary: Still, errors 
and confirmation type are interacting on the 
same layer. However, task success is included as 
independent variable on a second layer, and 
instead, the error in the previous turn determines 
the impact of errors and confirmation on the 
ratings. Again, we tested if the prediction can be 
 
Predicted: training dialogs 
 
Dial 1 
 
Dial 2 
 
Dial 3 
 
Dial 4 
 
Dial 5 
 
Dial 6 
 
Mean (basel.) 
Optimized on training 
dialogs 
Layer 1: Error, Confirm, Task Success 
Layer 2: Dummy 
MSE: 0.0185 0.0307 0.0166 0.0216 0.0333 0.0477 0.0281 (0.1201) 
MAEmax: 0.7000 0.5714 0.2857 0.0556 0.3636 0.3333 0.3849 (0.6167) 
Optimized on test dialogs 
 
Layer 1: Errors, Errors_lag, Confirmation 
Layer 2: TaskSuccess 
MSE: 0.0272 0.0358 0.0247 0.0374 0.0400 0.0574 0.0371 (0.1201) 
MAEmax: 0.5000 0.4286 0.4286 0.3889 0.4545 0.3333 0.4223 (0.6167) 
Number of valid dialogs (N): 22 15 23 17 17 9  
Table 3. Evaluation of predictions of training dialogs (mean squared error and mean absolute error 
in predicting the most probable state at each turn). Baseline results are given in brackets. The feature 
combinations with which results were obtained are also reported. 
 
Predicted: test dialogs Dial 1 Dial 2 Mean (baseline) 
Optimized on training dialogs Layer 1: Error, Confirm, Task Success 
Layer 2: Dummy 
MSE: 0.1039 0.0429 0.0734 (0.1583) 
MAEmax: 0.4444 0.6250 0.5347 (0.6944) 
Optimized on training dialogs (tah) Layer 1: Error, Confirm, Task Success 
Layer 2: Dummy 
MSE: 0.0957 0.0387 0.0672 (0.1636) 
MAEmax: 0.3333 0 0.1667 (0.6944) 
Optimized on test dialogs (rf) Layer 1: Errors, Errors_lag, Confirm 
Layer 2: TaskSuccess 
MSE: 0.0789 0.0349 0.0569 (0.1583) 
MAEmax: 0.4444 0.6250 0.5347 (0.6944) 
Optimized on test dialogs (tah; rf) Layer 1: Errors, Confirm 
MSE: 0.0860 0.0374 0.0617 (0.1636) 
MAEmax: 0.3333 0 0.1667 (0.6944) 
Number of valid dialogs (N): 9 13  
Table 4. Evaluation of predictions of training dialogs (tah=model trained on users with high 
technical affinity; rf=user speech act feature exclude from analysis) 
174
improved by considering differences between the 
users? technical affinity. However, repeating the 
procedure for only those users with high 
technical affinity did not improve the result this 
time. Concerning the parameters, error and 
confirmation type were confirmed to be 
significant predictors of quality judgments. The 
dummy parameter created to improve the 
accuracy on training data was not proven useful 
for the prediction of the test set ratings.  
In order to cross-check the validity of the 
model optimized on test data, we finally 
predicted the ratings of the 6 dialogs from the 
training set with the same model (bottom of 
Table 3). As can be seen, the prediction is worse 
than that from the model optimized on the 
training set. However, the quality of the 
prediction is still reasonable, showing that the 
two datasets do not demand for completely 
different models. All predictions are above the 
baseline. 
5 Discussion 
In the previous section, we presented results 
achieved with our models in terms of MSE. In 
order to gain meaning to the values of MSE, we 
added the mean absolute error of predicting the 
most probable judgment at each state. A closer 
look at the relation between MSE and MAEmax 
reveals that both measures are not strictly 
correlated (see e.g. the first two models in Table 
4). While the MSE measures the distance at each 
measurement point in the distribution, the 
MAEmax is a rough indicator of the similarity of 
the shape of the predicted and observed 
probability curves. The results for MAEmax are 
promising, as predictions of test data are in the 
range of predictions of training data and better 
than the baseline. Also, predictions made from 
participants with high technical affinity achieve 
better results on the test data in all cases, which 
was expected, but not found for the MSE results. 
Figure 2 presents examples of prediction 
results graphically. We chose one example of an 
average, and one of a relatively bad prediction, to 
allow extrapolation to other results presented. 
The pictures show that even a relatively high 
MSE corresponds to a fair quality of the 
prediction. The probability curves are mostly 
similar, mainly smoother than the observed 
probability distributions. Sometimes the 
predictions are too optimistic, however, usually 
the change in judgments is predicted, just not the 
extent of this change. We can only hypothesize 
 
 
Figure 2. Examples of predictions on test 
data made with the model, illustrating the 
meaning of MSE values. Depicted are two 
dialogs (columns) with 9 (left) and 8 (right) 
turns (rows).  For each turn, the empirical 
(solid line) and predicted (dotted line) rating 
distributions are given. Left: MSE=0.0957; 
N(emp)=9. Right: MSE=0.0349; 
N(emp)=13. 
175
about the reasons for the participants to judge the 
respective dialog worse than predicted by the 
model. A possible reason is that users more 
easily decrease their judgments when the dialog 
has a longer history of problematic situations. 
According to our data, the users were relatively 
forgiving and increased their judgments if the 
dialog went well, even if previously errors had 
occurred. However, the errors might not really be 
forgot, and be reflected in the judgment of later 
problems and errors. Unfortunately, for reasons 
of data scarcity, the wider dialog history cannot 
be considered in the models. 
Another source of prediction error might be 
the sample size available for the predicted 
dialogs. If sample size (N) and MSE values are 
compared among the dialogs, it can be observed 
that both values are correlated. This might be due 
to less smooth probability distribution curves if 
few ratings are available at each turn. While the 
curves depicted in Figure 2 are sometimes spiky, 
with increasing sample size normal distribution 
should be more likely. This might to some 
degree explain the clearly higher MSE for the test 
data predictions despite the relatively small error 
in predicting the most probable ratings.  
6 Conclusion 
In this paper, we presented a new approach to the 
prediction of user judgments about SDSs, using 
HMMs. The approach allows predicting the 
users? judgments at each step of a dialog. In 
predicting the distribution of ratings of many 
users, the approach takes into account 
differences between the users? judgment 
behaviors. This increases the usefulness of the 
model for a number of applications. E.g., in 
adaptive systems, the decision process can take 
into account differences between the users which 
cannot be attributed to user characteristics known 
to the system. If the model is applied to 
automatically generated dialogs, e.g. in the 
MeMo workbench (Engelbrecht et al, 2008a), a 
more detailed prediction of user satisfaction is 
enabled, allowing analysis on a turn-by-turn 
basis. 
In addition, the approach facilitates the 
analysis of models and features affecting the 
quality ratings, as results can be compared to the 
empirical ratings with more detail. We hope to 
gain further insight into the relations between 
interaction parameters and user judgments by 
running simulations under different assumptions 
of relations between these entities. 
A drawback of the approach is the generation 
of training data. The models presented in this 
paper cannot be assumed to be general, and in 
particular are lacking important parameters 
reflecting the timing in the dialogs. Therefore, as 
a next step the acquisition of judgments should 
be improved to be less disruptive for the 
interaction. In addition, it would be interesting to 
find a method for deriving the correct 
distributions of ratings at each dialog turn from a 
corpus of different dialogs, e.g. by grouping 
situations which are comparable. At the moment, 
we are also investigating if judgments can be 
acquired after the interactions without a loss in 
validity. 
After all, the results we achieved with the 
model suggest that HMMs are suitable for 
modeling the users? quality perception of dialogs 
with SDSs. Further research on the topic will 
hopefully show if the dialog history has to be 
considered to a wider degree than in our present 
models.  
Concerning dialog features and their relation 
to the judgments, the role of understanding errors 
in combination with the confirmation type could 
be established so far. More rich data are needed 
to work towards a general model for judgment 
predictions, including all relevant parameters. If 
judgments can be acquired after the interactions, 
we will be able to easily get the data needed for a 
better (and maybe complete) model. In any case, 
we are confident that the approach taken will 
allow a deeper analysis of the quality judgment 
process, which will enable progress by more 
analytical methods, such as formulating and 
testing hypotheses about this process. 
References 
Hua Ai, Fuliang Weng. 2008. User Simulation as 
Testing for Spoken Dialog Systems. Proc. of the 9th 
SIGdial Workshop on Discourse and Dialogue, 
Columbus, Ohio. 
Grace Chung. 2004. Developing a flexible spoken 
dialog system using simulation. Proc. of the 42nd 
Annual Meeting on Association for Computational 
Linguistics, Barcelona, Spain. 
Klaus-Peter Engelbrecht, Sebastian M?ller. 2007. 
Pragmatic Usage of Linear Regression Models for 
the Prediction of User Judgments. Proc. of 8th 
SIGdial Workshop on Discourse and Dialogue, 
Antwerp, Belgium. 
Klaus-Peter Engelbrecht, Michael Kruppa, Sebastian 
M?ller, Michael Quade. 2008a. MeMo Workbench 
176
for Semi-Automated Usability Testing. Proc. of 9th 
Interspeech, Brisbane, Australia. 
Klaus-Peter Engelbrecht, Sebastian M?ller, Robert 
Schleicher, Ina Wechsung. 2008b. Analysis of 
PARADISE Models for Individual Users of a 
Spoken Dialog System. Proc. of ESSV 2008, 
Frankfurt/Main, Germany. 
Klaus-Peter Engelbrecht, Felix Hartard, Florian 
G?dde, Sebastian M?ller. 2009. A Closer Look at 
Quality Judgments of Spoken Dialog Systems, 
submitted to Interspeech 2009. 
Norman M. Fraser, G. Nigel Gilbert. 1991. 
Simulating speech systems. Computer Speech and 
Language, 5:81?99. 
Rainer Guski. 1999. Personal and Social Variables as 
Co-determinants of Noise Annoyance. Noise & 
Health, 3:45-56. 
Kate S. Hone, Robert Graham. 2001. Subjective 
Assessment of Speech-system Interface Usability. 
Proc. of EUROSPEECH, Aalborg, Denmark. 
ITU-T Rec. P.851, 2003. Subjective Quality 
Evaluation of Telephone Services Based on Spoken 
Dialogue Systems, International 
Telecommunication Union, Geneva, Switzerland. 
Ute Jekosch. 2005. Voice and Speech Quality 
Perception. Assessment and Evaluation, Springer, 
Berlin, Germany. 
Ram?n L?pez-C?zar, ?ngel de la Torre, Jos? C. 
Segura and Antonio J. Rubio. 2003. Assessment of 
Dialogue Systems by Means of a New Simulation 
Technique. Speech Communication, 40(3):387-
407. 
Sebastian M?ller. 2005a. Perceptual Quality 
Dimensions of Spoken Dialog Systems: A Review 
and New Experimental Results, Proc. of Forum 
Acusticum, Budapest, Hungary. 
Sebastian M?ller. 2005b. Quality of Telephone-based 
Spoken Dialog Systems. Springer, New York. 
Sebastian M?ller, Klaus-Peter Engelbrecht, Robert 
Schleicher. 2008. Predicting the Quality and 
Usability of Spoken Dialogue Services, Speech 
Communication, 50:730-744. 
Morris A. Okun, Renee M. Weir. 1990. Toward a 
Judgment Model of College Satisfaction. 
Educational Psychological Review, 2(1):59-76. 
Lawrence R. Rabiner. 1989. A tutorial on HMM and 
selected applications in speech recognition. Proc. 
IEEE, 77(2):257-286. 
Verena Rieser, Oliver Lemon. 2008. Automatic 
Learning and Evaluation of User-Centered 
Objective Functions for Dialogue System 
Optimisation. Proc. of LREC'08, Marrakech, 
Morocco. 
Bernhard Steimel, Oliver Jacobs, Norbert Pfleger, 
Sebastian Paulke. 2008. Testbericht VOICE Award 
2008: Die besten deutschsprachigen 
Sprachapplikationen. Initiative Voice Business, 
Bad Homburg, Germany. 
Marilyn A. Walker, Diane J. Litman, Candace A. 
Kamm, Alicia Abella. 1997. PARADISE: A 
Framework for Evaluating Spoken Dialogue 
Agents. Proc. of ACL/EACL 35th Ann. Meeting of 
the Assoc. for Computational Linguistics, Madrid, 
Spain. 
Marilyn A. Walker, Diane J. Litman, Candace A. 
Kamm, Alicia Abella. 1998. Evaluating Spoken 
Dialog Agents with PARADISE: Two Case 
Studies. Computer Speech and Language, 12:317-
347. 
Marilyn Walker, Candace Kamm, Diane Litman. 
2000. Towards Developing General Models of 
Usability with PARADISE. Natural Language 
Engineering, 6(3-4):363-377. 
Craig Wootton, Michael McTear, Terry Anderson. 
2007. Utilizing Online Content as Domain 
Knowledge in a Multi-Domain Dynamic Dialogue 
System. Proc. of Interspeech 2007, Antwerp, 
Belgium. 
 
177
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 182?189,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Framework for Model-based Evaluation of Spoken Dialog Systems
Sebastian Mo?ller
Deutsche Telekom Laboratories
Technische Universita?t Berlin
10587 Berlin, Germany
sebastian.moeller@telekom.de
Nigel G. Ward
Computer Science Department
University of Texas at El Paso
El Paso, Texas 79968, USA
nigelward@acm.org
Abstract
Improvements in the quality, usability and ac-
ceptability of spoken dialog systems can be
facilitated by better evaluation methods. To
support early and efficient evaluation of dia-
log systems and their components, this paper
presents a tripartite framework describing the
evaluation problem. One part models the be-
havior of user and system during the interac-
tion, the second one the perception and judg-
ment processes taking place inside the user,
and the third part models what matters to sys-
tem designers and service providers. The pa-
per reviews available approaches for some of
the model parts, and indicates how anticipated
improvements may serve not only developers
and users but also researchers working on ad-
vanced dialog functions and features.
1 Introduction
Despite the utility of many spoken dialog systems
today, the user experience is seldom satisfactory.
Improving this is a matter of great intellectual in-
terest and practical importance. However improve-
ments can be difficult to evaluate effectively, and this
may be limiting the pace of innovation: today, valid
and reliable evaluations still require subjective ex-
periments to be carried out, and these are expensive
and time-consuming. Thus, the needs of system de-
velopers, of service operators, and of the final users
of spoken dialog systems argue for the development
of additional evaluation methods.
In this paper we focus on the prospects for an
early and model-based evaluation of dialog systems.
Doing evaluation as early as possible in the de-
sign and development process is critical for improv-
ing quality, reducing costs and fostering innovation.
Early evaluation renders the process more efficient
and less dependent on experience, hunches and intu-
itions. With the help of such models predicting the
outcome of user tests, the need for subjective test-
ing can be reduced, restricting it to that subset of the
possible systems which have already been vetted in
an automatic or semi-automatic way.
Several approaches have already been presented
for semi-automatic evaluation. For example, the
PARADISE framework (Walker et al, 1997) predicts
the effects of system changes, quantified in terms of
interaction parameters, on an average user judgment.
Others (Araki and Doshita, 1997; Lo?pez-Co?zar et
al., 2003; Mo?ller et al, 2006) have developed dialog
simulations to aid system optimization. However the
big picture has been missing: there has been no clear
view of how these methods relate to each other, and
how they might be improved and joined to support
efficient early evaluation.
The remainder of this paper is organized as fol-
lows. Section 2 gives a brief review of different
evaluation purposes and terminology, and outlines a
new tripartite decomposition of the evaluation prob-
lem. One part of our framework models the behav-
ior of user and system during the interaction, and
describes the impact of system changes on the inter-
action flow. The second part models the perception
and judgment processes taking place inside the user,
and tries to predict user ratings on various percep-
tual dimensions. The third part models what mat-
ters to system designers and service providers for
182
a specific application. Sections 3, 4, and 5 go into
specifics on the three parts of the framework, dis-
cussing which components are already available or
conceivable. Finally, Section 6 discusses the poten-
tial impact of the approach, and Section 7 lists the
issues to be resolved in future work.
2 Performance, Quality, Usability and
Acceptability Evaluation
Developers tend to use indices of performance to as-
sess their systems. The performance indicates the
?ability of a system to provide the function it has
been designed for? (Mo?ller, 2005). The function
and an appropriate measure for quantifying the de-
gree of fulfillment may easily be determined for cer-
tain components ? e.g. word accuracy for a speech
recognizer or concept error rate for a speech under-
standing module ? but it is harder to specify for
other components, such as a dialog manager or an
output generation module. However, definitive mea-
sures of component quality are not always neces-
sary: what matters for such a module is its contri-
bution to the quality of the entire interaction, as it is
perceived by the user.
We follow the definition of the term quality as
introduced by Jekosch (2000) and now accepted
for telephone-based spoken dialog services by the
International Telecommunication Union in ITU-T
Rec. P.851 (2003): ?Result of judgment of the per-
ceived composition of an entity with respect to its
desired composition?. Quality thus involves a per-
ception process and a judgment process, during
which the perceiving person compares the percep-
tual event with a (typically implicit) reference. It is
the comparison with a reference which associates a
user-specific value to the perceptual event. The per-
ception and the comparison processes take place in a
particular context of use. Thus, both perception and
quality should be regarded as ?events? which hap-
pen in a particular personal, spatial, temporal and
functional context.
Usability is one sub-aspect of the quality of the
system. Following the definition in ISO 9241 Part
11 (1998), usability is considered as the ?extent to
which a product can be used by specified users to
achieve specified goals with effectiveness, efficiency
and satisfaction in a specified context of use?. Us-
ability is degraded when interaction problems oc-
cur. Such problems influence the perceptual event
of the user interacting with the system, and conse-
quently the quality s/he associates with the system
as a whole. This may have consequences for the
acceptability or the system or service, that is, how
readily a customer will use the system or service.
This can be quantified, for example as the ratio of
the potential user population to the size of the target
group.
It is the task of any evaluation to quantify as-
pects of system performance, quality, usability or
acceptability. The exact target depends on the pur-
pose of the evaluation (Paek, 2007). For example,
the system developer might be most interested in
quantifying the performance of the system and its
components; s/he might further need to know how
the performance affects the quality perceived by the
user. In contrast, the service operator might instead
be most interested in the acceptability of the ser-
vice. S/he might further want to know about the
satisfaction of the user, influenced by the usability
of the system, and also by other (e.g. hedonic) as-
pects like comfort, joy-of-use, fashion, etc. Differ-
ent evaluation approaches may be complementary,
in the sense that metrics determined for one purpose
may be helpful for other purposes as well. Thus, it is
useful to describe the components of different eval-
uation approaches in a single framework.
Figure 1 summarizes our view of the evaluation
landscape. At the lower left corner is what we can
change (the dialog system), at the right is what the
service operator might be interested in (a metric for
the value of the system). In between are three com-
ponents of a model of the processes taking place in
the evaluation. The behavior model describes how
system and user characteristics determine the flow
of the interaction and translate this to quantitative
descriptors. The perception and judgment model
describes how the interaction influences the percep-
tual and quality events felt by the user, and trans-
lates these to observable user judgments. Finally the
value model associates a certain value to the qual-
ity judgments, depending on the application. The
model properties have been grouped in three layers:
aspects of the user and his/her behavior, aspects of
the system in its context-of-use, and the work of an
external observer (expert) carrying out the evalua-
183
BehaviorModel Perception and Judgment Model Value Model
UserBehavior
SystemBehavior
InteracionBehavior
Use
r La
yer
Exp
ert
Lay
er
Sys
tem
Lay
er
InteractionPhenomena
InteractionParameters
PerceptualEvent
DimensionDescriptors
QualityJudgments
Reference
QualityEvent
QualityAspects ValueDescription
PerceptualQualityDimensions
Designerand OperatorRequirements
Figure 1: Tripartite view of a model-based evaluation. Observable properties are in boxes, inferred or hidden properties
are in ovals. The layers organize the properties as mostly user-related, mostly system-related, and mostly expert-
related, and mostly system-related.
tion. They have further been classified as to whether
they are observable (boxes) or hidden from the eval-
uator (ovals).
The next three sections go through the three parts
of the model left-to-right, explaining the needs, cur-
rent status, and prospects.
3 Behavior Model
The behavior model translates the characteristics of
the system and the user into predicted interaction be-
havior. In order to be useful, the representations of
this behavior must be concise.
One way to describe dialog behavior is with in-
teraction parameters which quantify the behavior of
the user and/or the system during the interaction.
Such parameters may be measured instrumentally or
given by expert annotation. In an attempt to sys-
tematize best practice, the ITU-T has proposed a
common set of interaction parameters suitable for
the evaluation of telephone-based spoken dialog sys-
tems in ITU-T Suppl. 24 (2005). These parameters
have been developed bottom-up from a collection of
evaluation reports over the last 15 years, and include
metrics related to dialog and communication in gen-
eral, meta-communication, cooperativity, task, and
speech-input performance (Mo?ller, 2005). Unfortu-
nately, it as is yet unclear which of these parameters
relate to quality from a user?s point-of-view. In addi-
tion, some metrics are missing which address critical
aspects for the user, e.g. parameters for the quality
and attractiveness of the speech output.
Another manageable way to describe system be-
havior is to focus on interaction phenomena. Sev-
eral schemes have been developed for classifying
such phenomena, such as system errors, user errors,
points of confusion, dead time, and so on (Bernsen et
al., 1998; Ward et al, 2005; Oulasvirta et al, 2006).
Patterns of interaction phenomena may be reflected
in interaction parameter values, and may be identi-
fied on that basis. Otherwise, they have to be deter-
mined by experts and/or users, by means of obser-
vation, interviews, thinking-aloud, and other tech-
niques from usability engineering. (Using this ter-
minology we can understand the practice of usability
testing as being the identification of interaction phe-
nomena, also known as ?usability events? or ?criti-
cal incidences?, and using these to estimate specific
quality aspects or the overall value of the system.)
Obtaining the interaction parameters and classi-
fying the interaction phenomena can be done, ob-
viously, from a corpus of user-system interactions.
The challenge for early evaluation is to obtain these
without actually running user tests. Thus, we would
like to have a system behavior model and a user be-
havior model to simulate interaction behavior, and
to map from system parameters and user properties
to interaction parameters or phenomena. The value
of such models for a developer is clear: they could
184
enable estimation of how a change in the system
(e.g. a change in the vocabulary) might affect the
interaction properties. In addition to the desired ef-
fects, the side-effects of system changes are also im-
portant. Predicting such side-effects will substan-
tially decrease the risk and uncertainty involved in
dialogue design, thereby decreasing the gap between
research and commercial work on dialog system us-
ability (Heisterkamp, 2003; Pieraccini and Huerta,
2005).
Whereas modeling system behavior in response to
user input is clearly possible (since in the last resort
it is possible to fully implement the system), user be-
havior can probably not be modeled in closed form,
because it unavoidably relates to the intricacies of
the user and reflects the time-flow of the interaction.
Thus, it seems necessary to employ a simulation
of the interaction, as has been proposed by Araki
and Doshita (1997) and Lo?pez-Co?zar et al (2003),
among others.
One embodiment of this idea is the MeMo work-
bench (Mo?ller et al, 2006), which is based on
the idea of running models of the system and of
the user in a dedicated usability testing workbench.
The system model is a description of the possi-
ble tasks (system task model) plus a description of
the system?s interaction behavior (system interac-
tion model). The user model is a description of the
tasks a user would want to carry out with the sys-
tem (user task model) plus a description of the steps
s/he would take to reach the goal when faced with
the system (user interaction model). Currently the
workbench uses simple attribute-value descriptions
of tasks the system is able to carry out. From these,
user-desired tasks may be derived, given some back-
ground knowledge of the domain and possible tasks.
The system interaction model is described by a state
diagram which models interactions as paths through
a number of dialog states. The system designer pro-
vides one or several ?intended paths? through the in-
teraction, which lead easily and/or effectively to the
task goal.
The user?s interaction behavior will strongly de-
pend on the system output in the previous turn.
Thus, it is reasonable to build the user interaction
model on top of the system interaction model: The
user mainly follows the ?intended path?, but at cer-
tain points deviations from this path are generated in
a probabilistic rule-based manner. For example, the
user might deviate from the intended path, because
s/he does not understand a long system prompt, or
because s/he is irritated by a large number of op-
tions. Each deviation from the intended path has
an associated probability; these are calculated from
system characteristics (e.g. prompt length, number
of options) and user characteristics (e.g. experience
with dialog systems, command of foreign languages,
assumed task and domain knowledge).
After the models have been defined, simulations
of user-system interactions can be generated. These
interactions are logged and annotated on different
levels in order to detect interaction problems. Us-
ability predictions are obtained from the (simulated)
interaction problems. The simulations can also sup-
port reinforcement learning or other methods for au-
tomatically determining the best dialog strategy.
Building user interaction models by hand is
costly. As an alternative to explicitly defining rules
and probabilities, simulations can be based on data
sets of actual interactions, augmented with annota-
tions such as indications of the dialog state, current
subtask, inferred user state, and interaction phenom-
ena. Annotations can be generated by the dialog
participants themselves, e.g. by re-listening after the
fact (Ward and Tsukahara, 2003), or by top com-
municators, decision-makers, trend-setters, experts
in linguistics and communication, and the like. Ma-
chine learning techniques can help by providing pre-
dictions of how users tend to react in various situa-
tions from lightly annotated data.
4 Perception and Judgment Model
Once the interaction behavior is determined, the
evaluator needs to know about the impact it has on
the quality perceived by the user. As pointed out in
Section 2, the perception and judgments processes
take place in the human user and are thus hidden
from the observer. The evaluator may, however, ask
the user to describe the perceptual event and/or the
quality event, either qualitatively in an open form or
quantitatively on rating scales. Provided that the ex-
periment is properly planned and carried out, user
quality judgments can be considered as direct qual-
ity measurements, reflecting the user?s quality per-
ception.
185
Whereas user judgments on quality will reflect the
internal reference and thus depend heavily on the
specific context and application, it may be assumed
that the characteristics of the perceptual event are
more universal. For example, it is likely that sam-
ples of observers and/or users would generally agree
on whether a given system could be characterized
as responsive, smooth, or predictable, etc. regardless
of what they feel about the importance of each such
quality aspect. We may take advantage of this by
defining a small set of universal perceptual quality
dimensions, that together are sufficient for predict-
ing system value from the user?s point-of-view.
In order to quantify the quality event and to iden-
tify perceptual quality dimensions, psychometric
measurement methods are needed, e.g. interaction
experiments with appropriate measurement scales.
Several attempts have been made to come up with
a common questionnaire for user perception mea-
surement related to spoken dialog systems, for ex-
ample the SASSI questionnaire (Hone and Graham,
2000) for systems using speech input, and the ITU-
standard augmented framework for questionnaires
(ITU-T Rec. P.851, 2003) for systems with both
speech-input and speech-output capabilities. Studies
of the validity and the reliability of these question-
naires (Mo?ller et al, 2007) show that both SASSI
and P.851 can cover a large number of different qual-
ity and usability dimensions with a high validity, and
mainly with adequate reliability, although the gener-
alizability of these results remains to be shown.
On the basis of batteries of user judgments ob-
tained with these questionnaires, dimension descrip-
tors of the perceptual quality dimensions can be ex-
tracted by means of factor analysis. A summary of
such multidimensional analyses in Mo?ller (2005b)
reveals that users? perceptions of quality and usabil-
ity can be decomposed into around 5 to 8 dimen-
sions. The resulting dimensions include factors such
as overall acceptability, task effectiveness, speed,
cognitive effort, and joy-of-use. It should be noted
that most such efforts have considered task-oriented
systems, where effectiveness, efficiency, and suc-
cess are obviously important, however these dimen-
sions may be less relevant to systems designed for
other purposes, for example tutoring or ?edutain-
ment? (Bernsen et al, 2004), and additional factors
may be needed for such applications.
In order to describe the impact of the interac-
tion flow on user-perceived quality, or on some of
its sub-dimensions, we would ideally model the hu-
man perception and judgment processes. Such an
approach has the clear advantage that the resulting
model would be generic, i.e. applicable to differ-
ent systems and potentially for different user groups,
and also analytic, i.e. able to explain why certain in-
teraction characteristics have a positive or negative
impact on perceived quality. Unfortunately, the per-
ception and judgment processes involved in spoken-
dialog interaction are not yet well understood, as
compared, for example, to those involved in listen-
ing to transmitted speech samples and judging their
quality. For the latter, models are available which
estimate quality with the help of peripheral audi-
tory perception models and a signal-based compar-
ison of representations of the perceptual event and
the assumed reference (Rix et al, 2006). They are
able to estimate user judgments on ?overall quality?
with an average correlation of around 0.93, and are
widely used for planning, implementing and moni-
toring telephone networks.
For interactions with spoken dialog systems, the
situation is more complicated, as the perceptual
events depend on the interaction between user and
systems, and not on one speech signal alone. A way
out is not to worry about the perception processes,
and instead to use simple linear regression models
for predicting an average user judgment from vari-
ous interaction parameters. The most widely used
framework designed to support this sort of early
evaluation is PARADISE (Walker et al, 1997). The
target variable of PARADISE is an average of several
user judgments (labeled ?user satisfaction?) of dif-
ferent system and interaction aspects, such as system
voice, perceived system understanding, task ease,
interaction pace, or the transparency of the interac-
tion. The interaction parameters are of three types,
those relating to efficiency (including elapsed time
and the number of turns), those relating to ?dialog
quality? (including mean recognition score and the
number of timeouts and rejections), and a measure
of effectiveness (task success). The model can be
trained on data, and the results are readily inter-
pretable: they can indicate which features of the in-
teraction are most critical for improving user satis-
faction.
186
PARADISE-style models can be very helpful tools
for system developers. For example, a recent inves-
tigation showed that the model can be used to ef-
fectively determine the minimum acceptable recog-
nition rate for a smart-home system, leading to
the same critical threshold as that obtained from
user judgments (Engelbrecht and Mo?ller, 2007).
However, experience also shows that the PARADISE
framework does not reliably give valid predictions of
individual user judgments, typically covering only
around 40-50% of the variance in the data it is
trained on. The generality is also limited: cross-
system extrapolation works sometimes but other
times has low accuracy (Walker et al, 2000; Mo?ller,
2005). These limitations are easy to understand in
terms of Figure 1: over-ambitious attempts to di-
rectly relate interaction parameters to a measure of
overall system value seem unlikely to succeed in
general. Thus it seems wise to limit the scope of the
perception and judgment component to the predic-
tion of values on the perceptual quality dimensions.
In any case, there are several ways in which such
models could be improved. One issue is that a linear
combination of factors is probably not generally ad-
equate. For example, parameters like the number of
turns required to execute a specific task will have a
non-zero optimum value, at least for inexperienced
users. An excessively low number of turns will be
as sure a sign of interaction problems as an exces-
sively large number. Such non-linear effects can-
not be handled by linear models which only support
relationships like ?the-more-the-better? or ?the-less-
the-better?. Non-linear algorithms may overcome
these limitations. A second issue is that of tempo-
ral context: instead of using a single input vector
of interaction parameters for each dialog, it may be
possible to apply a sequence of feature vectors, one
for each exchange (user-system utterance pair). The
features may consist not only of numeric measures
but also of categories encoding interaction phenom-
ena. Using this input one could then perhaps use a
neural network or Hidden-Markov Model to predict
various user judgments at the end of the interaction.
5 Value Model
Even if a model can predict user judgments of ?over-
all quality? with high validity and reliability, this is
not necessarily a good indicator of the acceptability
of a service. For example, systems with a sophis-
ticated and smooth dialog flow may be unaccept-
able for frequent users because what counts for them
is effectiveness and efficiency only. Different users
may focus on different quality dimensions in differ-
ent contexts, and weight them according to the task,
context of use, price, etc.
A first step towards addressing this problem
is to define quality aspects that a system devel-
oper or service operator might be concerned about.
There can be many such, but in usability engineer-
ing they are typically categorized into ?effective-
ness?, ?efficiency? and ?satisfaction?. A more de-
tailed taxonomy of quality aspects can be found in
Mo?ller (2005). On the basis of this or other tax-
onomizations, value prediction models can be de-
veloped. For example, a system enabling 5-year
old girls to ?talk to Barbie? might ascribe little im-
portance to task completion, speech recognition ac-
curacy, or efficiency, but high importance to voice
quality, responsiveness, and unpredictability. The
value model will derive a value description which
takes such a weighting into account. A model for
systems enabling police officers on patrol to obtain
information over the telephone would have very dif-
ferent weights.
Unfortunately, there appear to be no published de-
scriptions of value prediction models, perhaps be-
cause they are very specific or even proprietary, de-
pending on a company?s business logic and cus-
tomer base. Such models probably need not be very
complex: it likely will suffice to ascribe weights to
the perceptual quality dimensions, or to quality as-
pects derived from system developer and/or service
operator requirements. Appropriate weights may be
uncovered in stakeholder workshops, where design-
ers, vendors, usability experts, marketing strategists,
user representatives and so on come together and
discuss what they desire or expect.
6 Broader Impacts
We have presented a tripartite evaluation framework
which shows the relationship between user and sys-
tem characteristics, interaction behavior, perceptual
and quality events, their descriptions, and the final
value of the system or service. In doing so, we
187
have mainly considered the needs of system devel-
opers. However, an evaluation framework that sup-
ports judgments of perceived quality could provide
additional benefits for users. We can imagine user-
specific value models, representing what is impor-
tant to specified user groups. These could be so-
licited for an entire group, or inferred from each
user?s own personal history of interactions and deci-
sions, e.g, through a personalization database avail-
able to the service operator. The models could also
be used to support system selection, or to inform
real-time system customization or adaptation.
Better evaluation will also support the needs of
the research community. With the help of model-
based evaluation, it will become easier for re-
searchers not only to do evaluation more efficiently,
but also to to produce more meaningful evaluation
results; saying not just ?this feature was useful? but
also providing quantitative statements of how much
the feature affects various interaction parameters,
and from that how much it impacts the various qual-
ity dimensions, and ultimately the value itself. This
will make evaluation more meaningful and make it
easy for others to determine when an innovation is
worth adopting, speeding technology transfer.
One might worry that a standardized framework
might only be useful for evaluating incremental im-
provements, thereby discouraging work on radically
different dialog design concepts. However well-
designed evaluation components should enable this
framework to work for systems of any type, meaning
that it may be easier to explore new regions of the
design space. In particular it may enable more ac-
curate prediction of the value of design innovations
which in isolation may not be effective, but which in
combination may be.
7 Future Work
Although examples of some model components are
available today, notably several interaction simula-
tions and the PARADISE framework for predicting
user judgments from interaction parameters, these
are limited. To realize a complete and generally use-
ful evaluation model will require considerable work,
for example, on:
? User behavior model: Of the three compo-
nents, perhaps the greatest challenges are in
the development of user behavior models. We
need to develop methods which produce simu-
lated behavior which is realistic (congruent to
the behavior of real users), and/or which pro-
duce interaction parameters and/or quality in-
dicators comparable to those obtained by sub-
jective interaction experiments. It is yet un-
clear whether realistic user behavior can also be
generated for more advanced systems and do-
mains, such as computer games, collaborative
problem solving systems, or educational sys-
tems. We also need to develop models that ac-
curately represent the behavior patterns of var-
ious user groups.
? Interaction parameters: Several quality aspects
are still not reflected in the current parameter
sets, e.g. indices for the quality of speech out-
put. Some approaches are described in Mo?ller
and Heimansberg (2006), but the predictive
power is still too limited. In addition, many pa-
rameters still have to be derived by expert an-
notation. It may be possible to automatically
infer values for some parameters from proper-
ties of the user?s and system?s speech signals,
and such analyses may be a source for new pa-
rameters, covering new quality aspects.
? Perceptual and quality events and reference:
These items are subject of ongoing research in
related disciplines, such as speech quality as-
sessment, sound quality assessment, and prod-
uct sound design. Ideas for better, more realis-
tic modeling may be derived from cooperations
with these disciplines.
? Quality judgments and dimension descriptors:
In addition to the aspects covered by the SASSI
and P.851 questionnaires, psychologists have
defined methods for assessing cognitive load,
affect, affinity towards technology, etc. Input
from such questionnaires may provide a better
basis for developing value models.
Although a full model may be out of reach for the
next decade, a more thorough understanding of hu-
man behavior, perception and judgment processes is
not only of intrinsic interest but promises benefits
enough to make this a goal worth working towards.
188
Acknowledgments
This work was supported in part by NSF Grant No.
0415150.
References
M. Araki, and S. Doshita. 1997. Automatic Evaluation
Environment for Spoken Dialogue Systems. Dialogue
Processing in Spoken Language Systems, ECAI?96
Workshop Proceedings, Springer Lecture Notes in
Artificial Intelligence No. 1236, 183-194, Springer,
Berlin.
N. O. Bernsen, H. Dybkj?r, and L. Dybkj?r. 1998. De-
signing Interactive Speech Systems: From First Ideas
to User Testing. Springer, Berlin.
N. O. Bernsen, L. Dybkj?r, L., and S. Kiilerich. 2004.
Evaluating Conversation with Hans Christian Ander-
sen. Proc. 4th Int. Conf. on Language Resources and
Evaluation (LREC 2004), 3, pp. 1011-1014, Lisbon.
K.-P. Engelbrecht, and S. Mo?ller. 2007. Using Linear Re-
gression Models for the Prediction of Data Distribu-
tions. Proc. 8th SIGdial Workshop on Discourse and
Dialogue, Antwerp, pp. 291-294.
P. Heisterkamp. 2003. ?Do not attempt to light with
match!?: Some Thoughts on Progress and Research
Goals in Spoken Dialog Systems. Proc. 8th Europ.
Conf. on Speech Communication and Technology (Eu-
rospeech 2003 ? Switzerland).
K. S. Hone, and R. Graham. 2000. Towards a Tool for the
Subjective Assessment of Speech System Interfaces
(SASSI). Natural Language Engineering, 3(3-4): 287-
303.
ITU-T Rec. P.851. 2003. Subjective Quality Eval-
uation of Telephone Services Based on Spoken
Dialogue Systems. International Telecommunication
Union, Geneva.
ITU-T Suppl. 24 to P-Series Rec. 2005. Parameters
Describing the Interaction with Spoken Dialogue
Systems. International Telecommunication Union,
Geneva.
ISO Standard 9241 Part 11. 1998. Ergonomic Require-
ments for Office Work with Visual Display Terminals
(VDTs) ? Part 11: Guidance on Usability. Interna-
tional Organization for Standardization, Geneva.
U. Jekosch. 2000. Sprache ho?ren und beur-
teilen: Ein Ansatz zur Grundlegung der
Sprachqualita?tsbeurteilung. Habilitation thesis
(unpublished), Universita?t/Gesamthochschule, Essen.
R. Lo?pez-Co?zar, A. De la Torre, J. Segura, and A. Rubio.
2003. Assessment of Dialog Systems by Means of a
New Simulation Technique. Speech Communication,
40: 387-407.
S. Mo?ller, P. Smeele, H. Boland, and J. Krebber. 2007.
Evaluating Spoken Dialogue Systems According to
De-Facto Standards: A Case Study. Computer Speech
and Language, 21: 26-53.
S. Mo?ller, R. Englert, K.-P. Engelbrecht, V. Hafner,
A. Jameson, A. Oulasvirta, A. Raake, and N. Rei-
thinger. 2006. MeMo: Towards Automatic Usability
Evaluation of Spoken Dialogue Services by User Er-
ror Simulations. Proc. 9th Int. Conf. on Spoken Lan-
guage Processing (Interspeech 2006 ? ICSLP), Pitts-
burgh PA, pp. 1786-1789.
S. Mo?ller, and J. Heimansberg. 2006. Estimation of
TTS Quality in Telephone Environments Using a
Reference-free Quality Prediction Model. Proc. 2nd
ISCA/DEGA Tutorial and Research Workshop on Per-
ceptual Quality of Systems, Berlin, pp. 56-60.
S. Mo?ller. 2005. Quality of Telephone-Based Spoken Di-
alogue Systems. Springer, New York NY.
S. Mo?ller. 2005b. Perceptual Quality Dimensions of Spo-
ken Dialogue Systems: A Review and New Exper-
imental Results. Proc. 4th European Congress on
Acoustics (Forum Acusticum Budapest 2005), Bu-
dapest, pp. 2681-2686.
A. Oulasvirta, S. Mo?ller, K.-P. Engelbrecht, and A. Jame-
son. 2006. The Relationship of User Errors to Per-
ceived Usability of a Spoken Dialogue System. Proc.
2nd ISCA/DEGA Tutorial and Research Workshop on
Perceptual Quality of Systems, Berlin, pp. 61-67.
T. Paek. 2007. Toward Evaluation that Leads to Best
Practices: Reconciling Dialog Evaluation in Research
and Industry. Bridging the Gap: Academic and Indus-
trial Research in Dialog Technologies Workshop Pro-
ceedings, Rochester, pp. 40-47.
R. Pieraccini, J. Huerta. 2005. Where Do We and Com-
mercial Spoken Dialog Systems. Proc. 6th SIGdial
Workshop on Discourse and Dialogue, Lisbon, pp. 1-
10.
A. W. Rix, J. G. Beerends, D.-S. Kim, P. Kroon, and
O. Ghitza. 2006. Objective Assessment of Speech and
Audio Quality ? Technology and Applications. IEEE
Trans. Audio, Speech, Lang. Process, 14: 1890-1901.
M. Walker, C. Kamm, and D. Litman. 2000. Towards
Developing General Models of Usability with PAR-
ADISE. Natural Language Engineering, 6: 363-377.
M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella.
1997. PARADISE: A Framework for Evaluating Spo-
ken Dialogue Agents. Proc. of the ACL/EACL 35th
Ann. Meeting of the Assoc. for Computational Linguis-
tics, Madrid, Morgan Kaufmann, San Francisco CA,
pp. 271-280.
N. Ward, A. G. Rivera, K. Ward, and D. G. Novick. 2005.
Root Causes of Lost Time and User Stress in a Simple
Dialog System. Proc. 9th European Conf. on Speech
Communication and Technology (Interspeech 2005),
Lisboa.
N. Ward, and W. Tsukahara. 2003. A Study in Respon-
siveness in Spoken Dialogue. International Journal of
Human-Computer Studies, 59: 603-630.
189
A New Taxonomy for the Quality of Telephone Services
Based on Spoken Dialogue Systems
Sebastian Mo?ller
Institute of Communication Acoustics
Ruhr?University Bochum
D?44780 Bochum, Germany
moeller@ika.ruhr-uni-bochum.de
Abstract
This document proposes a new taxon-
omy for describing the quality of services
which are based on spoken dialogue sys-
tems (SDSs), and operated via a telephone
interface. It is used to classify instru-
mentally or expert?derived dialogue and
system measures, as well as quality fea-
tures perceived by the user of the service.
A comparison is drawn to the quality of
human?to?human telephone services, and
implications for the development of evalu-
ation frameworks such as PARADISE are
discussed.
1 Introduction
Telephone services which rely on spoken dialogue
systems (SDSs) have now been introduced at a large
scale. For the human user, when dialing the num-
ber it is often not completely clear that the agent
on the other side will be a machine, and not a hu-
man operator. Because of this fact, and because the
interaction with the SDS is performed through the
same type of user interface (e.g. the handset tele-
phone), comparisons will automatically be drawn to
the quality of human?human communication over
the same channel, and sometimes with the same pur-
pose. Thus, while acknowledging the differences in
behaviors from both ? human and machine ? sides,
it seems justified to take the human telephone inter-
action (HHI) as one reference for telephone?based
human?machine interaction (HMI).
The quality of interactions with spoken dialogue
systems is difficult to determine. Whereas structured
approaches have been documented on how to design
spoken dialogue systems so that they adequately
meet the requirements of their users (e.g. by Bernsen
et al, 1998), the quality which is perceived when in-
teracting with SDSs is often addressed in an intuitive
way. Hone and Graham (2001) describe efforts to
determine the underlying dimensions in user quality
judgments, by performing a multidimensional anal-
ysis on subjective ratings obtained on a large number
of different scales. The problem obviously turned
out to be multi?dimensional. Nevertheless, many
other researchers still try to estimate ?overall system
quality?, ?usability? or ?user satisfaction? by sim-
ply calculating the arithmetic mean over several user
ratings on topics as different as perceived TTS qual-
ity, perceived system understanding, and expected
future use of the system. The reason is the lack of
an adequate description of quality dimensions, both
with respect to the system design and to the percep-
tion of the user.
In this paper, an attempt is made to close this
gap. A taxonomy is developed which allows qual-
ity dimensions to be classified, and methods for their
measurement to be developed. The starting point for
this taxonomy was a similar one which has fruitfully
been used for the description of human?to?human
services in telecommunication networks (e.g. tra-
ditional telephony, mobile telephony, or voice over
IP), see Mo?ller (2000). Such a taxonomy can be
helpful in three respects: (1) system elements which
are in the hands of developers, and responsible for
specific user perceptions, can be identified, (2) the
     Philadelphia, July 2002, pp. 142-153.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
dimensions underlying the overall impression of the
user can be described, together with adequate (sub-
jective) measurement methods, and (3) prediction
models can be developed to estimate quality ? as it
would be perceived by the user ? from purely instru-
mental measurements. While we are still far from
the last point in HMI, examples will be presented of
the first two issues.
The next section will discuss what is understood
by the term ?quality?, and will present the taxon-
omy for HMI. In Section 3, quality features under-
lying the aspects of the taxonomy are identified, and
dialogue- and system-related measures for each as-
pect are presented in Section 4, based on measures
which are commonly documented in literature. Sec-
tion 5 shows the parallels to the original taxonomy
for HHI. The outlook gives implications for the de-
velopment of evaluation and prediction models, such
as the PARADISE framework.
2 Quality of Service Taxonomy
It is obvious that quality is not an entity which could
be measured in an easy way, e.g. using a techni-
cal instrument. The quality of a service results from
the perceptions of its user, in relation to what they
expect or desire from the service. In the following,
it will thus be made use of the definition of quality
developed by Jekosch (2000):
?Quality is the result of the judgment of
a perceived constitution of an entity with
regard to its desired constitution. [...] The
perceived constitution contains the totality
of the features of an entity. For the per-
ceiving person, it is a characteristic of the
identity of the entity.?
The entity to be judged in our case is the service
the user interacts with (through the telephone net-
work), and which is based on a spoken dialogue sys-
tem. Its quality is a compromise between what s/he
expects or desires, and the characteristics s/he per-
ceives while using the service.
At this point, it is useful to differentiate between
quality elements and quality features, as it was also
proposed by Jekosch. Whereas the former are sys-
tem or service characteristics which are in the hands
of the designer (and thus can be optimized to reach
high quality), the latter are perceptive dimensions
forming the overall picture in the mind of the user.
Generally, no stable relationship which would be
valid for all types of services, users and situations
can be established between the two. Evaluation
frameworks such as PARADISE establish a tem-
porary relationship, and try to reach some cross?
domain validity. Due to the lack of quality elements
which can really be manipulated in some way by the
designer, however, the framework has to start mostly
from dialogue and system measures which cannot be
directly controlled. These measures will be listed in
Section 4.
The quality of a service (QoS) is often addressed
only from the designer side, e.g. in the definition
used by the International Telecommunication Union
for telephone services (ITU?T Rec. E.800, 1994).
It includes service support, operability, security and
serveability. Whereas these issues are necessary for
a successful set?up of the service, they are not di-
rectly perceived by the user. In the following tax-
onomy, the focus is therefore put on the user side.
The overall picture is presented in Figure 1. It il-
lustrates the categories (white boxes) which can be
sub?divided into aspects (gray boxes), and their rela-
tionships (arrows). As the user is the decision point
for each quality aspect, user factors have to be seen
in a distributed way over the whole picture. This
fact has tentatively been illustrated by the gray cans
on the upper side of the taxonomy, but will not be
further addressed in this paper. The remaining cate-
gories are discussed in the following.
Walker et al (1997) identified three factors which
carry an influence on the performance of SDSs, and
which therefore are thought to contribute to its qual-
ity perceived by the user: agent factors (mainly re-
lated to the dialogue and the system itself), task fac-
tors (related to how the SDS captures the task it has
been developed for) and environmental factors (e.g.
factors related to the acoustic environment and the
transmission channel). Because the taxonomy refers
to the service as a whole, a fourth point is added
here, namely contextual factors such as costs, type
of access, or the availability. All four types of factors
subsume quality elements which can be expected to
carry an influence on the quality perceived by the
user. The corresponding quality features are sum-
marized into aspects and categories in the following
quality
of
service
environmental
factors
agent
factors
task
factors
contextual
factors
speechi/o
quality
dialogue
cooperativity
dialogue
symmetry
communication
efficiency comfort
task
efficiency
usability serviceefficiency
economical
benefit
user
satisfaction
utility
acceptability
attitude experi-enceemotions
flexibility motivation,goals
task/domain
knowledge
transm.channel
backgr.noise
roomacoustics
taskcoverage
domaincov .
taskflexibility
taskdif ficulty
costs
availability
openinghours
access
intelligibility
naturalness
listening-effort
systemunderst.
informativeness
truth&evidence
relevance
manner
backgr.know .
meta-comm.handl.
initiative
interactioncontrol
partnerasymmetry
personality
cognitivedemand
tasksuccess
taskease
easeofuse
enjoyability
serviceadequacy
addedvalue
futureuse
user
factors
linguistic
backgr.
speed/p ace
dialogueconciseness
dialoguesmoothness
valuability
systemknowledge
dialoguestrategy
dialogueflexibility
Figure 1: QoS schematic for task?oriented HCI.
lower part of the picture.
The agent factors carry an influence on three qual-
ity categories. On the speech level, input and output
quality will have a major influence. Quality features
for speech output have been largely investigated in
the literature, and include e.g. intelligibility, natu-
ralness, or listening?effort. They will depend on the
whole system set?up, and on the situation and task
the user is confronted with. Quality features related
to the speech input from the user (and thus to the
system?s recognition and understanding capabilities)
are far less obvious. They are, in addition, much
more difficult to investigate, because the user only
receives an indirect feedback on the system?s capa-
bilities, namely from the system reactions which are
influences by the dialogue as a whole. Both speech
input and output are highly influenced by the envi-
ronmental factors.
On the language and dialogue level, dialogue co-
operativity has been identified as a key requirement
for high?quality services (Bernsen et al, 1998). The
classification of cooperativity into aspects which
was proposed by Bernsen et al, and which is re-
lated to Grice?s maxims (Grice, 1975) of cooperative
behavior in HHI, is mainly adopted here, with one
exception: we regard the partner asymmetry aspect
under a separate category called dialogue symme-
try, together with the aspects initiative and interac-
tion control. Dialogue cooperativity will thus cover
the aspects informativeness, truth and evidence, rel-
evance, manner, the user?s background knowledge,
and meta?communication handling strategies.
Adopting the notion of efficiency used by ETSI
and ISO (ETSI Technical Report ETR 095, 1993),
efficiency designates the effort and resources ex-
panded in relation to the accuracy and complete-
ness with which users can reach specified goals. It
is proposed to differentiate three categories of effi-
ciency. Communication efficiency relates to the ef-
ficiency of the dialogic interaction, and includes ?
besides the aspects speed and conciseness ? also
the smoothness of the dialogue (which is sometimes
called ?dialogue quality?). Note that this is a signif-
icant difference to many other notions of efficiency,
which only address the efforts and resources, but not
the accuracy and completeness of the goals to be
reached. Task efficiency is related to the success of
the system in accomplishing the task; it covers task
success as well as task ease. Service efficiency is
the adequacy of the service as a whole for the pur-
pose defined by the user. It also includes the ?added
value? which is contributed to the service, e.g. in
comparison to other means of information (compa-
rable interfaces or human operators).
In addition to efficiency aspects, other aspects ex-
ist which relate to the agent itself, as well as its
perception by the user in the dialogic interaction.
We subsume these aspects under the category ?com-
fort?, although other terms might exist which bet-
ter describe the according perceptions of the user.
Comfort covers the agent?s ?social personality? (per-
ceived friendliness, politeness, etc.), as well as the
cognitive demand required from the user.
Depending on the area of interest, several notions
of usability are common. Here, we define usabil-
ity as the suitability of a system or service to fulfill
the user?s requirements. It considers mainly the ease
of using the system and may result in user satisfac-
tion. It does, however, not cover service efficiency or
economical benefit, which carry an influence on the
utility (usability in relation to the financial costs and
to other contextual factors) of the service. Walker
et al (1998) also state that ?user satisfaction ratings
[...] have frequently been used in the literature as an
external indicator of the usability of an agent.? As
Kamm and Walker (1997), we assume that user sat-
isfaction is predictive of other system designer ob-
jectives, e.g. the willingness to use or pay for a ser-
vice. Acceptability, which is commonly defined on
this more or less ?economic? level, can therefore be
seen in a relationship to usability and utility. It is
a multidimensional property of a service, describing
how readily a customer will use the service. The ac-
ceptability of a service (AoS) can be represented as
the ratio of potential users to the quantity of the tar-
get user group, see definitions on AoS adopted by
EURESCOM (EURESCOM Project P.807 Deliver-
able 1, 1998).
From the schematic, it can be seen that a large
number of aspects contribute to what can be called
communication efficiency, usability or user satisfac-
tion. Several interrelations (and a certain degree of
inevitable overlap) exist between the categories and
aspects, which are marked by arrows. The interrela-
tions will become more apparent by taking a closer
look to the underlying quality features which can be
associated with each aspect. They will be presented
in the following section.
3 Classification of Quality Features
In Tables 1 and 2, an overview is given of the qual-
ity features underlying each aspect of the QoS tax-
onomy. For the aspects related to dialogue coop-
erativity, these aspects partly stem from the design
guideline definitions given by Bernsen et al (1998).
For the rest, quality features which have been used
in experimental investigations on different types of
dialogue systems have been classified. They do not
solely refer to telephone?based services, but will be
valid for a broader class of systems and services.
By definition, quality features are percepts of the
users. They can consequently only be measured
by asking users in realistic scenarios, in a subjec-
tive way. Several studies with this aim are reported
in the literature. The author analyzed 12 such in-
vestigations and classified the questions which were
asked to the users (as far as they have been reported)
according to the quality features. For each aspect
given in Tables 1 and 2, at least two questions could
be identified which addressed this aspect. This clas-
sification cannot be reproduced here for space rea-
sons. Additional features of the questionnaires di-
rectly address user satisfaction (e.g. perceived sat-
isfaction, degree of enjoyment, user happiness, sys-
tem likability, degree of frustration or irritation) and
acceptability (perceived acceptability, willingness to
use the system in the future).
From the classification, it seems that the taxon-
omy adequately covers what researchers intuitively
would include in questionnaires investigating usabil-
ity, user satisfaction and acceptability.
4 Classification of Dialogue and System
Measures
Experiments with human subjects are still the only
way to investigate quality percepts. They are, how-
ever, time?consuming and expensive to carry out.
For the developers of SDSs, it is therefore interesting
to identify quality elements which are in their hands,
and which can be used for enhancing the quality for
the user. Unfortunately, only few such elements are
known, and their influence on service quality is only
partly understood. Word accuracy or word error rate,
which are common measures to describe the perfor-
mance of speech recognizers, can be taken as an ex-
ample. Although they can be measured partly in-
strumentally (provided that an agreed?upon corpus
with reference transcriptions exists), and the system
designer can tune the system to increase the word
accuracy, it cannot be determined beforehand how
this will affect system usability or user satisfaction.
For filling this gap, dialogue? and system?related
measures have been developed. They can be de-
termined during the users? experimental interaction
with the system or from log?files, either instrumen-
tally (e.g. dialogue duration) or by an expert eval-
uator (e.g. contextual appropriateness). Although
they provide useful information on the perceived
quality of the service, there is no general relation-
ship between one or several such measures, and spe-
cific quality features. The PARADISE framework
(Walker et al, 1997) produces such a relationship
for a specific scenario, using multivariate linear re-
gression. Some generalizablility can be reached, but
the exact form of the relationship and its constitut-
ing input parameters have to be established for each
system anew.
A generalization across systems and services
might be easier if a categorization of dialogue and
system measures can be reached. Tables 3 and 4 in
the Appendix report on the classification of 37 dif-
ferent measures defined in literature into the QoS
taxonomy. No measures have been found so far
which directly relate to speech output quality, agent
personality, service efficiency, usability, or user sat-
isfaction. With the exception of the first aspect, it
may however be assumed that they will be addressed
by a combination of the measures related to the un-
derlying aspects.
5 Comparison to Human-Human Services
It has been stated earlier that the QoS taxonomy
for telephone?based spoken dialogue services has
been derived from an earlier schematic address-
ing human?to?human telephone services (Mo?ller,
2000). This schematic is depicted in Figure 2, with
slight modifications on the labels of single cate-
gories from the original version.
In the HHI case, the focus is placed on the cate-
gories of speech communication. This category (re-
Table 1: Dialogue?related quality features.
Aspect Quality Features
Dialogue Informativeness ? Accuracy / Specificity of Information
Cooperativity ? Completeness of Information
? Clarity of Information
? Conciseness of Information
? System Feedback Adequacy
Truth and ? Credibility of Information
Evidence ? Consistency of Information
? Reliability of Information
? Perceived System Reasoning
Relevance ? System Feedback Adequacy
? Perceived System Understanding
? Perceived System Reasoning
? Naturalness of Interaction
Manner ? Clarity / Non?Ambiguity of Expression
? Consistency of Expression
? Conciseness of Expression
? Transparency of Interaction
? Order of Interaction
Background ? Congruence with User?s Task/Domain Knowl.
Knowledge ? Congruence with User Experience
? Suitability of User Adaptation
? Inference Adequacy
? Interaction Guidance
Meta?Comm. ? Repair Handling Adequacy
Handling ? Clarification Handling Adequacy
? Help Capability
? Repetition Capability
Dialogue Initiative ? Flexibility of Interaction
Symmetry ? Interaction Guidance
? Naturalness of Interaction
Interaction ? Perceived Control Capability
Control ? Barge?In Capability
? Cancel Capability
Partner ? Transparency of Interaction
Asymmetry ? Transparency of Task / Domain Coverage
? Interaction Guidance
? Naturalness of Interaction
? Cognitive Demand Required from the User
? Respect of Natural Information Packages
Speech I/O Speech Output ? Intelligibility
Quality Quality ? Naturalness of Speech
? Listening?Effort Required from the User
Speech Input ? Perceived System Understanding
Quality ? Perceived System Reasoning
Table 2: Communication?, task? and service?related quality features.
Aspect Quality Features
Communic. Speed ? Perceived Interaction Pace
Efficiency ? Perceived Response Time
Conciseness ? Perceived Interaction Length
? Perceived Interaction Duration
Smoothness ? System Feedback Adequacy
? Perceived System Understanding
? Perceived System Reasoning
? Repair Handling Adequacy
? Clarification Handling Adequacy
? Naturalness of Interaction
? Interaction Guidance
? Transparency of Interaction
? Congruence with User Experience
Comfort Agent ? Politeness
Personality ? Friendliness
? Naturalness of Behavior
Cognitive ? Ease of Communication
Demand ? Concentration Required from the User
? Stress / Fluster
Task Task Success ? Adequacy of Task / Domain Coverage
Efficiency ? Validity of Task Results
? Precision of Task Results
? Reliability of Task Results
Task Ease ? Perceived Helpfulness
? Task Guidance
? Transparency of Task / Domain Coverage
Service Service ? Access Adequacy
Efficiency Adequacy ? Availability
? Modality Adequacy
? Task Adequacy
? Perceived Service Functionality
? Perceived Usefulness
Added Value ? Service Improvement
? Comparable Interface
Usability Ease of Use ? Service Operability
? Service Understandability
? Service Learnability
quality
of
service
speechcommunication
factors
service
factors
contextual
factors
voicetransmission
quality
easeof
communication
conversation
effectiveness
communication
efficiency
service
efficiency
usability
economical
benefit
user
satisfaction
utility
acceptability
loudnessratings
roomnoise
circuitnoise
listenersidetone
impulsivenoise
listenerecho
frequencydistortion
codecs
transmissionerrors
interruptions
fading
investmentcost s
operationcost s
accountconditions
user
factors
attitude emotions experi-ence
motivation,
goals
typeofterminal
ergonomics
design
availability
set-uptime
responsetime
reliability
compatibility
talkerecho
talkersidetone
puredelay
Figure 2: QoS schematic for human?to?human telephone services.
placing environmental and agent factors of the HMI
case) is divided into a one?way voice transmission
category, a conversational category (conversation ef-
fectiveness), and a user?related category (ease of
communication; comparable to the category ?com-
fort? in the HMI case). The task and service cate-
gories of the interaction with the SDS are replaced
by the service categories of the HHI schematic. The
rest of the schematic is congruent in both cases, al-
though the single aspects which are covered by each
category obviously differ.
The taxonomy of Figure 2 has fruitfully been used
to classify three types of entities:
  quality elements which are used for the set?up
and planning of telephone networks (some of
these elements are given in the gray boxes of
Figure 2)
  assessment methods commonly used for mea-
suring quality features in telecommunications
  quality prediction models which estimate sin-
gle quality features from the results of instru-
mental measurements
Although we seem to be far from reaching a compa-
rable level in the assessment and prediction of HMI
quality issues, it is hoped that the taxonomy of Fig-
ure 1 can be equally useful with respect to telephone
services based on SDSs.
6 Discussion and Conclusions
The new taxonomy was shown to be useful in clas-
sifying quality features (dimensions of human qual-
ity perception) as well as instrumentally or expert?
derived measures which are related to service qual-
ity, usability, and acceptability. Nonetheless, in both
cases it has not been validated against experimen-
tal (empirical) data. Thus, one cannot guarantee that
the space of quality dimensions is captured in an ac-
curate and complete way.
There are a number of facts reported in liter-
ature, which make us confident that the taxon-
omy nevertheless captures general assumptions and
trends. First of all, in his review of both subjective
evaluations as well as dialogue- or system-related
measures, the author didn?t encounter items which
would not be covered by the schematic. This litera-
ture review is still going on, and it is hoped that more
detailed data can be presented in the near future.
As stated above, the separation of environmental,
agent and task factors was motivated by Walker et al
(1997). The same categories appear in the character-
ization of spoken dialogue systems given by Fraser
(1997) (plus an additional user factor, which obvi-
ously is nested in the quality aspects due to the fact
that it is the user who decides on quality). The
context factor is also recognized by Dybkj?r and
Bernsen (2000). Dialogue cooperativity is a cat-
egory which is based on a relatively sophisticated
theoretical as well as empirical background. It has
proven useful especially in the system design and
set?up phase, and first results in evaluation have
also been reported (Bernsen et al, 1998). The di-
alogue symmetry category captures the remaining
partner asymmetry aspect, and has been designed
separately to additionally cover initiative and inter-
action control aspects. To the authors knowledge,
no similar category has been reported. The relation-
ship between the different efficiency measures and
usability, user satisfaction and utility was already
discussed in Section 2.
In the PARADISE framework, user satisfaction is
composed of maximal task success and minimal di-
alogue costs (Walker et al, 1997), ? thus a type
of efficiency in the way it was defined here. This
concept is still congruent with the proposed taxon-
omy. On the other hand, the separation into ?effi-
ciency measures? and ?quality measures? (same fig-
ure) does not seem to be fine?graded enough. It is
proposed that the taxonomy could be used to clas-
sify different measures beforehand. Based on the
categories, a multi?level prediction model could be
envisaged, first summarizing similar measures (be-
longing to the same category) into intermediate in-
dices, and then combining the contributions of dif-
ferent indices into an estimation of user satisfaction.
The reference for user satisfaction, however, cannot
be a simple arithmetic mean of the subjective ratings
in different categories. Appropriate questionnaires
still have to be developed, and they will take profit
of multidimensional analyses as reported by Hone
and Graham (2001).
References
Niels Ole Bernsen, Hans Dybkj?r, and Laila Dybkj?r.
1998. Designing Interactive Speech Systems: From
First Ideas to User Testing. Springer, D?Berlin.
Morena Danieli and Elisabetta Gerbino. 1995. Metrics
for evaluating dialogue strategies in a spoken language
system. In: Empirical Methods in Discourse Inter-
pretation and Generation. Papers from the 1995 AAAI
Symposium, USA?Stanford CA, pages 34?39, AAAI
Press, USA?Menlo Park CA.
Laila Dybkj?r and Niels Ole Bernsen. 2000. Usability
issues in spoken dialogue systems. Natural Language
Engineering, 6(3-4):243?271.
ETSI Technical Report ETR 095, 1993. Human Factors
(HF); Guide for Usability Evaluations of Telecommu-
nication Systems and Services. European Telecommu-
nications Standards Institute, F?Sophia Antipolis.
EURESCOM Project P.807 Deliverable 1, 1998. Jupiter
II - Usability, Performability and Interoperability Tri-
als in Europe. European Institute for Research
and Strategic Studies in Telecommunications, D?
Heidelberg.
Norman Fraser. 1997. Assessment of Interactive Sys-
tems. In: Handbook on Standards and Resources for
Spoken Language Systems (D. Gibbon, R. Moore and
R. Winski, eds.), pages 564?615, Mouton de Gruyter,
D?Berlin.
H. Paul Grice, 1975. Logic and Conversation, pages 41?
58. Syntax and Semantics, Vol. 3: Speech Acts (P.
Cole and J. L. Morgan, eds.). Academic Press, USA?
New York (NY).
Kate S. Hone and Robert Graham. 2001. Subjective As-
sessment of Speech?System Interface Usability. Proc.
7th Europ. Conf. on Speech Communication and Tech-
nology (EUROSPEECH 2001 ? Scandinavia), pages
2083?2086, DK?Aalborg.
ITU?T Rec. E.800, 1994. Terms and Definitions Related
to Quality of Service and Network Performance In-
cluding Dependability. International Telecommunica-
tion Union, CH?Geneva, August.
Ute Jekosch. 2000. Sprache ho?ren und beurteilen:
Ein Ansatz zur Grundlegung der Sprachqualita?tsbe-
urteilung. Habilitation thesis (unpublished), Univer-
sita?t/Gesamthochschule Essen, D?Essen.
Candance A. Kamm and Marilyn A. Walker. 1997.
Design and Evaluation of Spoken Dialogue Systems.
Proc. 1997 IEEE Workshop on Automatic Speech
Recognition and Understanding, USA?Santa Barbara
(CA), pages 14?17.
Candance Kamm, Shrikanth Narayanan, Dawn Dutton,
and Russell Ritenour. 1997. Evaluating Spoken
Dialogue Systems for Telecommunication Services.
Proc. 5th Europ. Conf. on Speech Communication and
Technology (EUROSPEECH?97), 4:2203?2206, GR?
Rhodes.
Diane J. Litman, Shimei Pan, and Marilyn A. Walker.
1998. Evaluating Response Strategies in a Web?
Based Spoken Dialogue Agent. Proc. of the 36th
Ann. Meeting of the Assoc. for Computational Linguis-
tics and 17th Int. Conf. on Computational Linguistics
(COLING-ACL 98), CAN-Montreal.
Sebastian Mo?ller. 2000. Assessment and Prediction of
Speech Quality in Telecommunications. Kluwer Aca-
demic Publ., USA?Boston.
Joseph Polifroni, Lynette Hirschman, Stephanie Sen-
eff, and Victor Zue. 1992. Experiments in Eval-
uating Interactive Spoken Language Systems. In:
Proc. DARPA Speech and Natural Language Work-
shop, pages 28?33.
Patti J. Price, Lynette Hirschman, Elizabeth Shriberg, and
Elizabeth Wade. 1992. Subject?Based Evaluation
Measures for Interactive Spoken Language Systems.
In: Proc. DARPA Speech and Natural Language Work-
shop, pages 34?39.
Andrew Simpson and Norman M. Fraser. 1993. Black
Box and Glass Box Evaluation of the SUNDIAL Sys-
tem. Proc. 3rd Europ. Conf. on Speech Communi-
cation and Technology (EUROSPEECH?93), 2:1423?
1426, D?Berlin.
Helmer Strik, Catia Cucchiarini, and Judith M. Kessens.
2001. Comparing the Performance of Two CSRs:
How to Determine the Significance Level of the Dif-
ferences. Proc. 7th Europ. Conf. on Speech Communi-
cation and Technology (EUROSPEECH 2001 ? Scan-
dinavia), pages 2091?2094, DK?Aalborg.
Marilyn A. Walker, Diane J. Litman, Candance A.
Kamm, and Alicia Abella. 1997. PARADISE: A
Framework for Evaluating Spoken Dialogue Agents.
Proc. of the ACL/EACL 35th Ann. Meeting of the As-
soc. for Computational Linguistics, pages 271?280.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,
and Alicia Abella. 1998. Evaluating Spoken Dialogue
Agents with PARADISE: Two Case Studies. Com-
puter Speech and Language, 12(3).
A Classification of Dialogue and System
Measures
Table 3: Classification of measures (1). #: average number of ... per dialogue. For references, see cap-
tion of Table 4.
Aspect Dialogue / System Measure
Dialogue ? CA: contextual appropriateness (SF93, F97)
Cooperativity
Informativeness ? # user questions (P92)
? # help requests from the user (W98)
Truth and ? # questions correctly/incorrectly/partially/failed to
Evidence be answered (P92)
? DARPA score, DARPA weighted error (P92)
Relevance ? # barge-in attempts from the user (W98)
Manner ? # system turns (W98)
? no. of words per system turn
Background ? # help requests (W98)
Knowledge ? # cancel attempts from the user (W98)
? # barge-in attempts from the user (W98)
? # time-out prompts (W98)
Meta?Comm. ? # system error messages (Pr92)
Handling ? # help requests (W98)
? # cancel attempts from the user (W98)
? CR: correction rate (SCR) (F97, SF93)
? IR: implicit recovery (DG95)
Dialogue Initiative ? # user questions (P92)
Symmetry ? # system questions
? CR: correction rate (SCR, UCR) (F97, SF93)
Interaction ? # barge-in attempts from the user (W98)
Control ? # help requests (W98)
? # cancel attempts from the user (W98)
? CR: correction rate (UCR) (F97, SF93)
? # time-out prompts (W98)
Partner ? # barge-in attempts from the user (W98)
Asymmetry ? # time-out prompts (W98)
Speech I/O Speech Output ?
Quality Quality
Speech Input ? word accuracy, word error rate (SF93)
Quality ? sentence accuracy, sentence error rate (SF93)
? number or errors per sentence (S01)
? word error per sentence (S01)
?

,
	

,
	
,

 (K97)
? UER: understanding error rate
? # ASR rejections (W98)
? IC: information content (SF93)
? # system error messages (Pr92)
Table 4: Classification of measures (2). #: average number of ... per dialogue. References: DG95: Danieli
and Gerbino (1995); F97: Fraser (1997); K97: Kamm et al (1997); P92: Polifroni et al (1992); Pr92: Price
et al (1992); SF93: Simpson and Fraser (1993); S01: Strik et al (2001); W98: Walker et al (1998).
Aspect Dialogue / System Measure
Communic. Speed ? TD: turn duration (STD, UTD) (F97)
Efficiency ? SRD: system response delay (Pr92)
? URD: user response delay (Pr92)
? # timeout prompts (W98)
? # barge-in attempts from the user (W98)
Conciseness ? DD: dialogue duration (F97, P92)
(Litman et al, 1998: ? # turns (# system turns, # user turns) (W98)
dialogue efficiency)
Smoothness ? # system error messages (Pr92)
(Litman et al, 1998: ? # cancel attempts from the user (W98)
dialogue quality) ? # help requests (W98)
? # ASR rejections (W98)
? # barge-in attempts from the user (W98)
? # timeout prompts (W98)
Comfort Agent ?
Personality
Cognitive ? # timeout prompts (W98)
Demand ? URD: user response delay (Pr92)
Task Task Success ? TS: task success (DG95, F97, SF93)
Efficiency ?  : kappa coefficient (W98)
? task solution (P92)
? solution correctness (P92)
? solution quality
Task Ease ? # help requests (W98)
Service Service ?
Efficiency Adequacy
Added Value ?
Usability Ease of Use ?
User
Satisfaction ?
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 5?6,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Position Paper: Towards Standardized Metrics and Tools 
for Spoken and Multimodal Dialog System Evaluation 
 
 
Sebastian M?ller, Klaus-Peter Engelbrecht, 
Florian Kretzschmar, Stefan Schmidt, Benjamin Weiss 
Quality and Usability Lab, Telekom Innovation Laboratories, TU Berlin 
Ernst-Reuter-Platz 7 
10587 Berlin, Germany 
sebastian.moeller@telekom.de 
 
  
 
Abstract 
We argue that standardized metrics and auto-
matic evaluation tools are necessary for 
speeding up knowledge generation and devel-
opment processes for dialog systems. 
1 Introduction 
The Spoken Dialogue Challenge launched by 
CMU (Black et al, 2011) provides a common plat-
form for dialog researchers in order to test the per-
formance of their systems and components against 
the state-of-the-art. Still, evaluations are individual 
undertakings in most areas, as common metrics 
and procedures which would be applicable for a 
range of systems are sparse. In the following, it is 
argued that significant progress can be made if 
three prerequisites are available: 
? Common metrics for quantifying user and sys-
tem interaction behavior and perceived quality 
? Reliable models for predicting user judgments 
on the basis of automatically-extracted or an-
notated interaction metrics 
? Methods for realistically simulating user be-
havior in response to dialog systems 
The state-of-the-art and necessary research in these 
three areas is outlined in the following paragraphs. 
The Spoken Dialogue Challenge can contribute to 
validating such metrics and models. 
2 Common Metrics  
Whereas early assessment and evaluation cycles 
were based on ad-hoc selected metrics, approaches 
have been made to come up with a standard set of 
metrics for quantifying interactions between users 
and systems which would make evaluation exer-
cises comparable. The International Telecommuni-
cation Union (ITU-T) has standardized two sets of 
metrics: ITU-T Suppl. 24 to P-Series (2005) for 
spoken dialog systems, and ITU-T Suppl. 25 to P-
Series Rec. (2011) for multimodal dialog systems. 
These metrics describe system performance (e.g. in 
terms of error rates) and user/system interaction 
behavior (e.g. in terms of meta-communication 
acts, durations) in a quantitative way, and can thus 
serve as an input to the models discussed below. 
Input is welcome to stabilize these metrics, so that 
they are of more use to researchers and system de-
velopers. The proper conjunction between such 
metrics and standardized annotation schemes (e.g., 
Bunt et al, 2010) will strengthen the establishment 
and spreading of a specific set of metrics. 
When it comes to user-perceived quality, Hone 
and Graham (2000) have made a first attempt to 
come up with a validated questionnaire (SASSI), 
which, however, lacks a scale to assess speech out-
put quality. The approach has been put forward in 
ITU-T Rec. P.851 (2003) by including speech out-
put and dialog managing capabilities. A framework 
structure was preferred over a fixed (and validated) 
questionnaire, in order to more flexibly address the 
needs of researchers and developers. This approach 
still needs to be extended towards multimodal sys-
tems, where modality appropriateness, preference 
and perceived performance have to be considered. 
ITU-T welcomes contributions on this topic. 
5
For practical usage, it is desirable to have evalu-
ation methods which provide diagnostic value to 
the system developer, so that the sources of misbe-
havior can be identified. The diagnosis can be 
based on perceptual dimensions (effectiveness, 
efficiency, mental effort, etc.) or on technical char-
acteristics (error rates, vocabulary coverage, etc.) 
or both. Approaches in this direction are welcome 
and would significantly increase the usefulness of 
evaluation exercises for the system developers. 
3 User-perceived Quality Prediction  
The first approach to predict user judgments on the 
basis of interaction metrics is the well-known 
PARADISE model (Walker et al, 1997). The main 
challenge to date is the low generalizability of such 
models. The reason is that many of the underlying 
input parameters are interdependent, and that a 
simple linear combination does not account for 
more complex relationships (e.g. there might be an 
optimum length for a dialog, which cannot be easi-
ly described by a purely linear model). 
However, other algorithms such as non-linear 
regression, classification trees or Markov models, 
have not shown a significantly improved perfor-
mance (M?ller et al, 2008; Engelbrecht, 2011). 
The latter are however adequate to describe the 
evolution of user opinion during the dialog, and 
thus might have principled advantages over models 
which use aggregated interaction performance met-
rics as an input. 
4 User Behavior Simulation 
During system development, it would be useful to 
anticipate how users would interact with a dialog 
system. Reflected to the system developer, such 
anticipations help to identify usability problems 
already before real users interact with the system. 
Whereas user behavior simulation has frequently 
been used for training statistical dialog managers, 
only few approaches are documented which apply 
them to system evaluation. Early approaches main-
ly selected possible utterances from a set of col-
lected data. The MeMo workbench (Engelbrecht, 
2011) tried to combine statistical selection of prob-
able interaction paths with the knowledge of usa-
bility experts about what typically influences user 
behavior. Such knowledge can also be generated 
by a conversational analysis and categorization 
(Schmidt et al, 2010). 
A different approach has been followed in the 
SpeechEval project (M?ller et al, 2009) where 
statistical dialog managers have been trained on a 
large diverse dataset to generate utterances on a 
conceptual level. The system is then amended with 
ASR and TTS to allow for a speech-based black-
box interaction with telephone-based dialog sys-
tems. Combined with diagnostic quality prediction 
models, such tools can support system developers 
to evaluate different dialog strategies early in the 
design cycle and at low costs, and thus avoid dis-
satisfied users. The approach still has to be extend-
ed towards multimodal dialog systems. 
References  
Alan W Black et al Spoken Dialog Challenge 2010: 
Comparison of Live and Control Test Results, Proc. 
SIGDIAL2011, Portland, OR. 
H. Bunt, et al: Towards an ISO standard for dialogue 
act annotation. Proc. LREC 2010, 19-21. 
K.-P. Engelbrecht. 2011. Estimating Spoken Dialog 
System Quality with User Models, Doctoral Disserta-
tion, TU Berlin, to appear with Springer, Berlin. 
K.S. Hone, R. Graham. 2000. Towards a Tool for Sub-
jective Assessment of Speech System Interfaces 
(SASSI), Natural Language Eng., 6(3-4):287-303. 
ITU-T Rec. P.851. 2003. Subjective Quality Evaluation 
of Telephone Services Based on Spoken Dialogue 
Systems, Int. Telecomm. Union, Gerneva. 
ITU-T Suppl. 24 to P-Series Rec. 2005. Parameters 
Describing the Interaction with Spoken Dialogue 
Systems, Int. Telecomm. Union, Geneva. 
ITU-T Suppl. 25 to P-Series Rec. 2011. Parameters 
Describing the Interaction with Multimodal Dialogue 
Systems, Int. Telecomm. Union, Geneva. 
S. M?ller, K.-P. Engelbrecht, R. Schleicher. 2008. Pre-
dicting the Quality and Usability of Spoken Dialogue 
Services, Speech Communication 50:730-744. 
S. M?ller, R. Schleicher, D. Butenkov, K.-P. Engel-
brecht, F. G?dde, T. Scheffler, R. Roller, N. 
Reithinger. 2009. Usability Engineering for Spoken 
Dialogue Systems Via Statistical User Models, in: 
First IWSDS 2009, Kloster Irsee. 
M.A. Walker, D.J. Litman, C.A. Kamm, A. Abella. 
1997. PARADISE: A Framework for Evaluating 
Spoken Dialogue Agents, Proc. ACL/EACL 35th 
Meeting, Madrid, 271-280. 
S. Schmidt, J. Stubbe, M. T?ppel, S. M?ller. 2010. Au-
tomatic Usability Evaluation for Spoken Dialog Sys-
tems Based on Rules Identified by a Sociotechnical 
Approach, in: Proc. PQS 2010, Bautzen. 
6
