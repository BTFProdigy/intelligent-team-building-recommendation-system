Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1136?1145,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Gender Inference of Twitter Users in Non-English Contexts
Morgane Ciot
School of Computer Science
McGill University
Montreal, Quebec, Canada
morgane.ciot@mail.mcgill.ca
Morgan Sonderegger
Department of Linguistics
McGill University
Montreal, Quebec, Canada
morgan.sonderegger@mcgill.ca
Derek Ruths
School of Computer Science
McGill University
Montreal, Quebec, Canada
derek.ruths@mcgill.ca
Abstract
While much work has considered the problem
of latent attribute inference for users of social
media such as Twitter, little has been done on
non-English-based content and users. Here,
we conduct the first assessment of latent at-
tribute inference in languages beyond English,
focusing on gender inference. We find that
the gender inference problem in quite diverse
languages can be addressed using existing ma-
chinery. Further, accuracy gains can be made
by taking language-specific features into ac-
count. We identify languages with complex
orthography, such as Japanese, as difficult for
existing methods, suggesting a valuable direc-
tion for future research.
1 Introduction
A 2012 study reported that US-based Twitter users
now account for only 28% of all active accounts on
the platform (Semiocast, 2012). Brazil, Japan, India,
and Indonesia all rank in the top 10, each with over
5% of all users. These and other findings confirm
that Twitter enjoys widespread international popu-
larity and usage. This is also reflected in the multi-
national community of researchers who study hu-
man behavior on Twitter and related platforms, e.g.
(Sakaki et al, 2010; Tumasjan et al, 2010; Kim and
Park, 2012).
It is remarkable, then, that advances in la-
tent attribute inference on social media have been
largely confined to English content, e.g. (Liu and
Ruths, 2013; Zamal et al, 2012; Pennacchiotti and
Popescu, 2011; Conover et al, 2011a). This bias
may be partially explained in the context of the re-
search being conducted largely by anglophone re-
searchers. Nonetheless, it has created a notable
silence in the literature concerning the large-scale
analysis of languages, cultures, and people on social
media who do not employ English.
In this paper, we examine the problem of latent at-
tribute inference outside the English-language con-
text. To our knowledge, this is the first such study
ever conducted. Here we specifically focus on gen-
der inference, as it has been the basis for significant
work in recent years (Liu et al, 2012; Zamal et al,
2012; Pennacchiotti and Popescu, 2011; Rao et al,
2010; Burger et al, 2011). Our work makes two
contributions. First, we quantify the extent to which
established gender inference methods can be used
with non-English Twitter content. Second, we ex-
plore the capacity for unique features of other lan-
guages (besides English) to improve inference ac-
curacy. This second aspect, in particular, acknowl-
edges the fact that latent attribute inference may be
easier in some languages due not to conventions in
word usage, but to syntactic structure.
In order to assess the extent to which existing gen-
der inference machinery works for users who use
languages other than English, we assembled Twit-
ter datasets for languages that are both prevalent on
Twitter and representative of diverse language fam-
ilies: Japanese, Indonesian, Turkish, and French.
Each dataset consisted of approximately 1000 users
who tweeted primarily in a given language. We used
Amazon Mechanical Turk to manually label each
user with their gender, using a language-agnostic la-
beling strategy (Liu and Ruths, 2013). For classi-
fication, we employed a performant support vector
machine-based (SVM) technique that has been used
in a range of studies, e.g. (Rao et al, 2010; Burger
et al, 2011; Zamal et al, 2012).
1136
We found that, without any modification to the
types of features given to the SVM, the classifier ac-
curacy was comparable on English, French, and In-
donesian. Turkish actually performed much better,
achieving 87% on average. Gender in Japanese, in
contrast, could not be reliably inferred with any rea-
sonable accuracy (61% on average) despite numer-
ous attempts to preprocess the tweets and tune the
classifier to accommodate the language?s complex
orthography. This indicates that existing approaches
may not generalize well to language systems with
thousands of distinct unigrams (as opposed to tens
or hundreds in the other languages considered).1
To evaluate the extent to which language-specific
features might be used to boost the accuracy of
the SVM classifier further, we focused on French.
French is a valuable case study because, unlike En-
glish, it has a number of syntax-based mechanisms
that can encode the gender of the speaker. The
most common instantiation of gender marking is the
modification of adjective and some past participle
endings to match the gender of the subject in con-
structions beginning with ?je suis? (trans. ?I am?)
constructions. A classifier based on this insight
achieved average accuracy of 90% on the vast ma-
jority of French users, surpassing the accuracy of
standard techniques on English or French.
Overall, our results show that, with little modifi-
cation, existing gender inference machinery can per-
form comparably to English on several other lan-
guages. There are clear areas for substantial im-
provement: incorporating language-specific features
and, in the case of Japanese, finding better ways
of accommodating the complex orthography. These
findings identify promising directions for future re-
search and will, hopefully, call attention to an im-
portant area in the latent attribute inference domain
in need of further work.
2 Background
Non-English Twitter data mining and studies.
Existing work on non-English Twitter content can
been divided into two groups: surveys of the use of
several languages on the platform and studies of a
social phenomenon in a non-English body of tweets.
1In this work, unigram, bigram, and k-gram refer to one,
two, and k-character sequences in a language?s written form.
To our knowledge, only a handful of the former va-
riety exist. One recent paper characterized the re-
lationship between language and geography (Mo-
canu et al, 2012). Another measured how high-level
tweet features (i.e., link, mention, and hashtag fre-
quencies) vary across languages (Weerkamp et al,
2011). These papers show that tweet structure and
content can differ widely across languages.
More work has been done in the latter category:
analysis of social phenomena in a non-English con-
text. A well-known study evaluated usage of Twit-
ter in the aftermath of the 2010 earthquake in Japan
(Sakaki et al, 2010). Another Japanese-oriented
study evaluated the impact of television on tweeted
content (Akioka et al, 2010). Within this cate-
gory, another recurring research topic is the anal-
ysis of political discussion and elections. Outside
of English-based analysis, some attention has been
given to European and East Asian elections, e.g.
(Tumasjan et al, 2010; Giglietto, 2012; Kim and
Park, 2012). However, few of these studies have
considered measures beyond simple hashtag fre-
quencies, relative mention counts among politicians,
and retweet counts. The only study using more
complex features for computational text analysis in-
volved sentiment analysis of a set of German tweets
(Tumasjan et al, 2010). However, the tweets in
this study (conducted at a German university) were
translated into English prior to analysis, a step which
underscores the significant bias towards English in
the literature on analyzing microtext, and the tools
available to researchers in this domain.
Gender inference methods. Gender inference is a
field of research situated with the broader area of la-
tent attribute inference. The majority of recent work
in this area has focused on Twitter users (Rao et al,
2010; Pennacchiotti and Popescu, 2011; Conover et
al., 2011b; Burger et al, 2011; Rao and Yarowsky,
2010; Liu and Ruths, 2013; Liu et al, 2012; Zamal
et al, 2012). Classifiers have been built, predomi-
nantly, using support vector machines, e.g. (Rao et
al., 2010; Pennacchiotti and Popescu, 2011; Burger
et al, 2011; Zamal et al, 2012), though boosted de-
cision trees and latent dirichlet alocation systems
have also been evaluated, e.g. (Pennacchiotti and
Popescu, 2011; Conover et al, 2011b). With one
exception, gender inference accuracy has been re-
1137
ported between 80% and 85%. The one study which
reported 90% accuracy involved the use of a dataset
which has been shown to be quite different from typ-
ical anglophone Twitter users (Burger et al, 2011).
This same study did involve non-English Twitter
users, but did not analyze the performance of the
classifier on different languages (e.g. break down
performance by language, examine to what extent
its results were due to better performance on some
languages), or indeed discuss fully which languages
were present in their sample. Thus, little can be in-
ferred from Burger et al?s study about the relative
performance of attribute inference methods on dif-
ferent languages, which is the focus of our paper.
Language families. Human languages can be
classified into different language families, defined
as a set of languages which are all descended from a
single, ancient parent language. Languages which
are genetically related (in the same family), how-
ever distantly, tend to share many more character-
istics than languages from different families.
Each language considered in this paper belongs
to a different language family: French to Indo-
European, Turkish to Altaic, Japanese to Japonic,
and Indonesian to Austronesian. Thus, these lan-
guages are completely genetically unrelated, by def-
inition. Further, they are both geographically and
culturally dispersed. While they all have some loan-
words from English, these constitute a tiny fraction
of each language?s vocabulary. This selection of lan-
guages allows us to conduct the most far-reaching
survey of non-English latent attribute inference per-
formance to date.
A variety of features make each language se-
lected interesting within the gender inference con-
text. French is noteworthy for its grammatical gen-
der. All nouns, including people, are grammati-
cally ?male? or ?female.? English, in contrast, has
separate pronouns for people of different genders
(e.g., ?he?, ?she?), but does not have grammati-
cal gender. (Besides a handful of exceptions like
?waiter?/?waitress?, there are no words besides pro-
nouns which have different ?masculine? and ?femi-
nine? forms.) Indonesian, Turkish, and Japanese are
all so-called genderless languages. Like many lan-
guages of the world, they do not have distinct male
and female pronouns (like English and French), or
grammatical gender (like French).
3 General Gender Inference
In order to evaluate the extent to which existing gen-
der inference machinery can be used on users whose
tweets are in languages other than English, we de-
veloped gender-labeled datasets of Twitter users for
each language and then evaluated the performance
of a classifier on each.
3.1 Data
The core data for this project consisted of four
datasets of content from Twitter users who tweeted
predominantly in one of four languages?French,
Indonesian, Turkish, and Japanese?collected using
the methods described below.
Data collection. In order to identify users for can-
didate inclusion in a particular language?s (hereafter
the target language) dataset, we walked the stream-
ing output of the Twitter firehose and evaluated the
language of each tweet using the language mod-
els provided by the Natural Language Toolkit (Bird,
2006). Users associated with tweets written in the
target language were added to a list. 5000 such users
were identified for each language. The latest 1000
tweets for each user were downloaded. This com-
prised the base for the target language dataset.
Assigning gender labels. In prior work, e.g. (Rao
et al, 2010; Pennacchiotti and Popescu, 2011; Za-
mal et al, 2012), the dominant way of obtain-
ing datasets consisting of Twitter users with high-
confidence gender-labels is to use gender-name as-
sociations. The use of name-gender associations are
problematic when non-English content is considered
because databases of anglophone name-gender asso-
ciations are no longer useful (Mislove et al, 2011).
We instead used Amazon Mechanical Turk workers
to identify the gender of the person shown in the pro-
file picture associated with a user?s account (Liu and
Ruths, 2013). In our datasets, each user?s profile pic-
ture was coded by 5 separate workers. Users with
non-photographic or celebrity-based profile pictures
was discarded, as well as any users with profile pic-
tures where the gender could not be confidently as-
sessed (less than 4 out of 5 votes for one gender).
Table 1 shows the final composition of each
dataset. In Japanese and Indonesian, we observed
1138
Table 1: The composition of the different language
datasets used in this study.
Language # Males # Females Total Size
French 437 506 943
Indonesian 977 2260 3237
Turkish 1672 1937 3609
Japanese 309 520 829
a notable difference in the number of males and fe-
males in the dataset. Measures were taken to ensure
that classifier results were not biased by these differ-
ences within the datasets.
3.2 Methods
The majority of prior work in gender inference (and
latent inference in general) has used support vector
machines (SVMs). We followed prior work in this
regard, particularly since our intent here is to eval-
uate the relevance of existing gender inference ma-
chinery on other languages. For the present study,
we adopted an SVM-based classifier, described in
(Zamal et al, 2012), that incorporated nearly all fea-
tures used in prior work and showed comparable
(and sometimes better) accuracy than other methods.
Parameter values and kernel choices for the SVM
are discussed in the source paper.
Feature set. SVM classifiers require that
each object to be classified be represented by
a fixed-length feature vector. The features we
employed were: k-top words, k-top digrams
and trigrams, k-top hashtags, k-top mentions,
tweet/retweet/hashtag/link/mention frequencies,
and out/in-neighborhood size. Note that ?k-top X
features? (e.g., k-top hashtags) refers to the k most
discriminating items of that type for each label (i.e.,
Male/Female). Thus, k-top words is actually 2k
features: the k words most associated with males
and the k words most associated with females.
This list of features is the same set of features used
in (Zamal et al, 2012), except that k-top stems and
k-top co-stems were both dropped in our version.
Both of these feature types are specific to English.
Of course, word stems do exist in other languages,
however we found that stemmers (the algorithms
that identify and extract the appropriate stem from
a word) were not available across the whole bank
of languages. Therefore, we omitted these stem and
co-stem features. We also added features for the us-
age frequencies of Eastern-style and Western-style
emoticons but saw no discernible change in accu-
racy; thus, these features are not discussed further.
It is important to note that all features included
in our classifier are language-agnostic. An n-gram
is simply an n-character sequence drawn from the
alphabet and additional symbols (numbers, punctu-
ation, etc.) present in tweets written in the target
language. Words are sequences of characters that
are bounded by whitespace or punctuation. Hash-
tags are words proceeded by a pound (?#?) character,
mentions by an ?@? symbol. A system that properly
supports unicode strings can implement all of these
notions without knowing anything about the target
language it is operating on.
Tokenization of Japanese. While all the defini-
tions provided above for the SVM features are op-
erational, there is a glaring disconnect between the
whitespace-border definition of a word and written
conventions in Japanese. Specifically, in Japanese
words are generally not separated by whitespace.
We used a tokenizer to insert whitespace into
Japanese text to break up words. Tokenization
was done using Kuromoji, the software Japanese
morphological analyzer used and supported by the
Apache software Foundation (Atilika, 2012). No-
tably, this tool tokenizes the mixed character sets
that are often used in informal Japanese writing.
As tokenization does involves some language-
specific processing, its use here somewhat under-
mines the objective set out for this project. Thus, we
report the accuracy achieved for both untokenized
and tokenized Japanese tweets. Curiously, tokeniza-
tion was found to not make a difference in overall
average accuracy.
3.3 Results
For each dataset, 5-fold cross validation was used
to assess the classifier?s performance. The value of
k = 20 was used for all k-top features, though the
results reported are robust to changes of this value
within reason (between 10 and 30). If the num-
bers of male and female users were unbalanced in
a dataset, the larger set was subsampled randomly
to obtain a set of users the same size as the smaller
labeled set. During the training process, the actual
1139
Table 2: The accuracy of the SVM-based classifier on
each of the language datasets. In the case of Japanese,
the performance is given for both the tokenized and un-
tokenized versions of the dataset. (Note that tokenization
did not affect overall accuracy.)
Language Male Female Overall
French 0.79 0.73 0.76
Indonesian 0.87 0.80 0.83
Turkish 0.89 0.85 0.87
Japanese (t) 0.50 0.76 0.63
Japanese (u) 0.58 0.68 0.63
values of the features were extracted from the train-
ing users (e.g., the k-top differentiating words for
males and females were identified). In this way,
the gender model implemented by the SVM was
language-specific, in the sense that a particular lan-
guage?s gender model contained a different set of
features. We call our method language-agnostic on
the grounds that, given a labeled set of users and
tweets drawn from a particular language, a model
can be built without any knowledge of the structure
or content of the language itself.
Tables in Supplementary Material show the fea-
tures for the classifier built over each language?s en-
tire dataset. Note that to conduct the cross-fold eval-
uation, new models (and hence different features)
were recomputed for each fold. As a result, the fea-
tures reported are slightly different from those that
might have appeared in the models for a given fold.
Manual inspection, however, revealed that differ-
ences were slight. The features reported in the Sup-
plementary Material can be safely considered a con-
sensus among the models for the individual folds.
The accuracy of the classifier for each language
is shown in Table 2. Overall, the classifier demon-
strated good performance on all languages ex-
cept for Japanese. Below, we consider the re-
sults for each of the four languages in turn. In
each case we discuss language-specific trends in
which words were most informative for inferring
user gender, and thus help explain the classifier?s
performance. Throughout, we omit discussion of
non-alphanumeric ?words? (such as punctuation or
emoticons), and call the k-top discriminating words
for male and female users the k-top male words and
k-top female words.
French. The k-top words for men and women are
of very different grammatical types. Most male
words are prepositions or articles (16/25; e.g. de
?of?, un ?a/one?); a few others are basic grammati-
cal words (ne ?[part of] not?, et ?and?), or pronouns
or verb forms referring to a single person or object
(he/she/it), as well as one noun (France). In con-
trast, many female words (11/25) are pronouns or
basic verb forms referring to the speaker or a single
addressee (je ?I?, mon/mes/ma ?my?, tu ?you?, j?ai
?I have?). Others are pronouns or basic verbs re-
fer to a single person or object (elle, ?she/it?, c?est
?it?s?), as well as a few other frequent words (trop
?too much?, pas ?[part of] not?, oui ?yes?). The most
salient pattern is that use of words (pronouns, basic
verbs) associated with talking about the speaker or
addressee indicates a tweet is more likely to be from
a female user. Heavy use of other common function
words, specifically prepositions and articles, sug-
gests a male user. These patterns reflect known gen-
der differences in word usage by male and female
French speakers (Witsen, 1981).
Indonesian. Indonesian achieved performance
closest to the inference accuracy for English re-
ported in the literature. The k-top lists for men and
women give some justification for why the classifier
performed well. Some differences can be tentatively
linked to general trends in how men and women use
language differently across cultures. 5/25 of men?s
k-top words are nouns which are either related to
soccer (vs ?versus?, chelsea ?[name of UK soccer
team]?, pemain ?player?) or which could be related
to soccer (jakarta, indonesia, malam ?night?); in
contrast, no women?s words are nouns. It seems
plausible that men tweet about soccer significantly
more than women. In such a situation, a reasonable
concern is that our classifier discriminated soccer
from non-soccer enthusiasts rather than males from
females. To address this, we confirmed that these
topic-based words were not required for accurate
classification: a classifier in which soccer words
were explicitly removed performed just as well
(83.8% vs. 83.3%).
More interestingly, many of the k-top words cor-
respond to men and women using different terms
of address and self-reference. Among the k-top
words, 7/25 for men and 4/25 for women are terms
1140
of address or self-reference. The terms men use are
mostly highly informal, including the slang term lu
(you) and the English borrowing bro; the address
terms women use are mostly medium-formality,
such as aku (I) and kamu (you). Thus, women seem
to be using ?more polite? self-reference and address
terms than men on average on Indonesian Twitter,
in line with the more general tendency for women
to use polite forms more frequently than men cross-
culturally (Holmes, 1995).
Turkish. Turkish achieved notably high accuracy:
the highest of all four languages considered. In
fact, to our knowledge, this is the highest accuracy
achieved in the entire Twitter gender inference lit-
erature on a dataset drawn from the Twitter gen-
eral population. The k-top lists of male and female
words again give some justification for the classi-
fier?s performance. Many differences between the
male and female lists can be linked to men and
women talking about different topics, or to differ-
ent people. Several of the male words refer to soc-
cer (gol ?goal?, galatasaray ?popular Istanbul team?,
mac? ?match?, at ?[part of imperative for] score?),
which men plausibly tweet about more. As with In-
donesian, a concern is that topics represent a biased
sample of the population. Thus, we tested a classi-
fier with soccer-specific terms removed, and again
found no difference in accuracy (86% vs. 87%).
Many other k-top words are familiar terms of ad-
dress for men (lan, abi, karde sim, adam, kanka) or
a greeting used mainly between men (eyvallah), sug-
gesting that male users are addressing or discussing
men more often than female users are. In contrast,
9/25 of the k-top female words are pronouns refer-
ring to the speaker, a familiar addressee, or a third
party (he/she/it), while none of the k-top male words
are, suggesting female users are more often talking
directly about themselves or to others. Finally, 2/25
of the k-top male words are profanity (amk, ulan),
while none of the female k-top words are, suggest-
ing male users swear more.
Japanese. Beyond the Japanese classifier?s gener-
ally poor accuracy, it is striking that tokenization
did not improve overall accuracy. This indicates
that once words were properly tokenized, no ad-
ditional gender-distinguishing signal could be ex-
tracted. This may be an indication that word-based
features carry little information in languages with
complex orthography, such as Japanese (with many
thousands of unigrams).
Despite the classifier?s poor performance, the k-
top discriminating words for male and female users
differ in interesting ways. Some differences can be
understood as resulting from known general trends
in how Japanese men and women?s use language.
Japanese speakers have a choice of many first-
person singular pronouns (equivalent to ?I?), which
signal different levels of politeness and of male ver-
sus female speech. The pronoun boku (?) is asso-
ciated with informal male speech; accordingly, it is
among the k-top male words. Japanese also uses an
extensive system of verb forms corresponding to dif-
ferent levels of politeness, and honorifics (affixes for
names used when referring to others). Women tend
to use polite verb forms and honorifics more fre-
quently than men in Japanese speech (Peng, 1981).
In agreement with this pattern, several polite verb
forms (-masu, -mashi) and a polite honorific (o-) are
among the k-top female words, as is a diminutive
honorific often used to refer to women (-chan).
4 Language-specific Features and
Inference
While the classifier performed well across a diverse
set of languages, recall that all features used by the
SVM were language-agnostic. A natural question
concerns the extent to which language-specific fea-
tures relevant to the attribute of interest (e.g., gen-
der) might improve the classifier?s performance.
We examine this question within the context of
French. Where gender inference is concerned,
French is quite interesting because information
about the gender of nouns (including the speaker) is
often obligatorily marked in the syntax: many words
have different ?masculine? and ?feminine? forms for
referring to male and female nouns, including the
speaker. Thus, it is in principle often possible to in-
fer the gender of the speaker by which form they use,
although it is not clear a priori that this method will
work for Twitter data.
4.1 Method
French grammar dictates that which forms of words
are used often reflects the gender of the speaker.
1141
Adjectives and past participles all have masculine
and feminine forms, which are often spelled dif-
ferently, and in addition often pronounced differ-
ently.Adjectives must agree in gender with the noun
they refer to. For example, ?I am happy? would
be je suis heureuse for a female speaker and je suis
heureux for a male speaker (literally ?I-am-happy?);
heureuse and heureux are the feminine and mascu-
line singular forms of the adjective, and are pro-
nounced differently. Past participles of verbs also
agree with the gender of the subject or object of the
verb, for certain verbs and constructions. For ex-
ample, ?I went? would be je suis alle?e for a female
speaker and je suis alle? for a male speaker (here suis
is used to form the simple past of the verb aller,
?to go?); alle? and alle?e are the masculine and fem-
inine forms of the past participle of aller, and are
pronounced the same.
Note that the phrase je suis (?I am?) occurs in both
the adjectival and verbal constructions referring to
the speaker; however, the function of suis differs be-
tween the two. suis is the first-person singular form
of the verb e?tre (?to be?), and functions as a cop-
ula when followed by an adjective (?I am happy?)
but as an auxiliary verb to mark the past tense, when
followed by the past participle of certain verbs (?I
went?). For our purposes, what is important is that,
in both cases, a following adjective or past participle
will take on the gender of the speaker.
When this construction occurs in a tweet, it is
likely that je is referring to the author of the tweet,
and the rules of French grammar dictate that the
gender of the associated adjective or past partici-
ple should reflect the gender of the tweet?s author.
We implemented a classifier that used this logic to
classify the gender of francophone Twitter users. It
is worth emphasizing that the existence of adjec-
tives and participles which reflect the speaker?s gen-
der does not automatically make gender identifica-
tion in French tweets a trivial task. First, given the
prevalence of non-standard spelling and grammar
on Twitter and other online platforms, French users
may sometimes not use the ?correct? gender marked
form reflecting their actual gender?especially given
that the male and female forms for a given adjective
or participle are often pronounced the same. Sec-
ond, even if gender-marked constructions are used
correctly, they may not occur sufficiently often in
Table 3: The set of patterns that were considered to be
suis-constructions when encountered in a tweet.
jn suis pas, jm suis, jmsuis, jnmsuis pas, jnsuis pas,
je ne suis pas, je suis pas, jsuis, jensuis pas, jemsuis,
jnesuis pas, jmesuis, je me suis, je ne me suis pas
tweets to be a reliably used for speaker gender iden-
tification. Both of these concerns are borne out in
our French dataset, as described further below; the
question addressed in the experiment is how useful
the signal provided by gender-marked forms is, de-
spite these two sources of noise.
Unlike the probabilistic SVM classifier, the suis-
construction classifier can be made entirely deter-
ministic. For a given user, the set of tweets con-
taining a suis-construction are identified, Tsuis(u).
Of these, we can identify the number of those tweets
that involve an adjective or past participle with a fe-
male ending TFsuis(u) ? Tsuis(u). Labeling a user
involves selecting a threshold based on TFsuis(u) and
Tsuis(u) below which a user receives one label and
above which the user receives the other label.
Detecting suis-constructions. As expected, cur-
sory inspection of tweets revealed that Twitter
users often employed shorthand forms of the suis-
construction. We accounted for this by conduct-
ing a manual survey of the shorthand forms of
the suis-construction. A catalog of regular expres-
sions was drawn up that matched the different suis-
construction forms we identified, shown in Table 3.
Recognizing the gender of the adjective or past
participle involved in a suis-construction required
a second processing stage. The Lexique lexical
database was used to tag the word trailing the suis-
construction (New and Landing, 2012). If the tag
was not an adjective or verb, the construction was
discarded as it would not contain a gender indica-
tion. If the word was recognized as an adjective or
verb, Lexique would also return the gender, which
would be returned as the gender indication for that
particular suis-construction.
Threshold selection. We evaluated a number of
policies for assigning the user?s gender based on
the relative values of TFsuis(u) and Tsuis(u). In the
end, however, the best performing threshold was
TFsuis(u) > 1: simply labeling as female any user
1142
Table 4: The component-wise and overall accuracy of the
combined suis-construction and SVM classifier.
Component # Male Female Overall
users Acc Acc Acc
suis-const. 723 0.91 0.90 0.90
SVM 220 0.70 0.54 0.62
Overall 943 0.86 0.82 0.83
who employed the female construction even once.
This threshold makes sense given the plausible intu-
ition that females will (almost always) be the only
users to employ a female suis-construction; how-
ever, it is quite sensitive to uses of female suis-
constructions by males.
Mixed classifier. Since not all users had tweets
which contained suis-constructions, we combined
the SVM-based classifier used previously with the
suis-construction-based classifier. The SVM com-
ponent was applied to any users who lacked suis-
constructions entirely in their tweet history. Any
user who used even one suis-construction would be
labeled according to the TFsuis(u) > 1 threshold.
4.2 Results
We ran our classifier on the French dataset, obtain-
ing the results shown in Table 4.
Coverage of the suis-construction. In spite of
our concerns over the occurrence frequency and de-
tectability of the suis-construction in tweets, our
results show that suis-constructions were found in
tweets belonging to nearly 75% of all users in the
dataset. This suggests that the suis-construction
classifier has quite broad coverage of the popula-
tion. Of course, given the essential role of the verb
?e?tre? in French (like the role of ?to be? in En-
glish), its frequent use is expected. Nonetheless,
the flexible use of grammar and spelling in Twitter
and other online contexts raised a genuine concern
that occurrences of the suis-construction might not
be detected. In fact, when we looked through the
tweets of users who were flagged as not having use-
ful suis-constructions in their tweets, we discovered
that many actually did. The issue was that they em-
ployed highly irregular spellings that our implemen-
tation was not able to pick up. Thus, with additional
refinement, it may be possible to improve the suis-
construction coverage further, well beyond 80%.
Performance of the suis-construction classifier.
On the set of users for which the suis-construction
was detected, the classifier did very well, achiev-
ing an average accuracy of 90%. Recall that the
threshold used to generate the results in Table 4 was
TFsuis(u) > 1. We tested other (larger) thresholds
and found that the performance of the method dra-
matically and monotonically decreased. This was
largely due to female users being misclassified as
males, indicating that females do not exclusively
use female suis-constructions (this was confirmed
via manual inspection of a number of female tweet
histories). This is different from males, most of
whom are quite strict about using only male suis-
constructions. Since forming the female form of
an adjective or participle typically requires adding
an additional character (or more) to the base of the
word, this may reflect a tendency towards dropping
gender modifiers in favor of typing less.
Performance of the SVM classifier. While the
suis-construction classifier performed well, the
SVM component did not do nearly as well on the
Twitter users that could not be labeled using the suis-
construction, achieving an average performance of
62%. At this level of accuracy, the classifier is per-
forming barely better than a random classifier, which
would have achieved around 50% accuracy on the
label-balanced testing data. This result stands in
opposition to our earlier finding that French users
could be labeled with 75% accuracy. This disparity
suggests that the non-suis-construction users com-
prise a particularly difficult-to-classify group.
The suis-construction as a filter. The finding that
the SVM classifier performed poorly in the com-
bined classification setting suggests that the suis-
construction classifier is acting as a very effective fil-
ter for users that are hard for it to classify. While we
might have preferred better classification accuracy
all around, this result is still interesting and useful.
Such filters can decrease classification error by sim-
ply flagging those users who cannot be easily clas-
sified, leaving them to be handled more carefully by
more powerful classifiers or human coding. This is
precisely the function that the suis-construction clas-
sifier appears to play (in addition to classifying the
1143
other users).
This result suggests a question for future work:
whether it is possible to build classifiers that accu-
rately label the sets of users that are discarded by
the suis-construction classifier.
Performance of the combined classifier. Despite
the relatively poor performance of the SVM com-
ponent, the accuracy of the combined classifier im-
proved on the original SVM-only classifier by 8%,
which is a substantial increase in accuracy. With
some additional focus on classifying the difficult
users who could not be labeled by suis-construction
usage, we feel that this accuracy can be increased
upwards of 90%.
5 Discussion
In this project, we have extended, for the first time,
the latent attribute inference problem to users who
tweet primarily in languages other than English. Our
study offers several notable insights.
Existing approaches generalize. While accuracy
levels certainly vary across languages, overall an ex-
isting SVM-based classifier, when trained on users
from a given language, can classify the gender of
other users from that same language with accuracy
comparable to performance reported for English.
We suspect that this result will generalize to the in-
ference of other demographic characteristics (e.g.,
age and political orientation), though this must be
explored in future work.
Complex orthography creates unique issues.
Japanese stands out as being utterly unclassifiable
using existing SVM-based approaches and feature
sets. Even efforts to bridge some of the orthographic
disconnects between the Japanese language and the
assumptions made by the SVM failed to improve
performance. This stands out as a clear direction for
future work, particularly since apparent issues with
the large number of unigrams used by Japanese will
create issues for handling (Mandarin) Chinese, the
world?s most-spoken language.
Language-specific features boost performance.
While unsurprising that customizing a classifier to
the peculiarities of a given language boosts perfor-
mance, our use of the suis-construction in French
highlights how particular linguistic features may be
uniquely well suited to the inference of particular
attributes. The results obtained for French stand in
contrast to various, relatively unsuccessful attempts
to boost gender inference by incorporating syntac-
tic features of English into the classifier (e.g., us-
ing stems and co-stems). It seems that some lan-
guages have features better suited for certain classi-
fication tasks. Identifying and leveraging such fea-
tures will be an interesting and fruitful direction for
future work.
Classifiers as a linguist?s tool. In each language,
a number of the k-top words align with or sug-
gest gender-specific conventions in that particular
language. That a language-agnostic classifier pro-
vided such insights highlights its potential for ex-
ploring language-specific word usage patterns and
nuances. For example, sociolinguistics (a subfield of
linguistics) has long studied the different ways men
and women use language, especially in spontaneous
speech (Eckert and McConnell-Ginet, 2003); recent
work has begun to examine how language is used
differently by men and women online as well (Bam-
man et al, 2012). Such studies could be radically
scaled up in terms of the number of languages con-
sidered using a language-agnostic gender classifier.
6 Conclusion
Though there has been relatively little investigation
into latent attribute inference outside of English-
language content, we consider it both a fruitful
and important area for future research. Here, we
have evaluated the capacity for existing inference
methods to be used outside their intended English-
language context. Furthermore, we have shown how
language-specific features might be incorporated in
order to boost classifier accuracy further. The posi-
tive results suggest that latent attribute inference in
the non-English context as a research direction wor-
thy of further attention.
7 Acknowledgements
The authors gratefully acknowledge three anony-
mous reviewers whose feedback improved the clar-
ity and correctness of the manuscript. The study
was supported by grants from the Social Sciences
1144
and Humanities and Natural Sciences and Engi-
neering Research Councils of Canada (SSHRC In-
sight Grant #435-2012-1802 and NSERC Discovery
Grant #125517855) and the Public Safety Canada
Kanishka Program.
References
S. Akioka, N. Kato, Y. Muraoka, and H. Yamana. 2010.
Cross-media impact on Twitter in Japan. In Proceed-
ings of the International Workshop on Search andMin-
ing User-generated Contents.
Atilika. 2012. Kuromoji morphological analyzer.
http://www.atilika.org.
D. Bamman, J. Eisenstein, and T. Schnoebelen. 2012.
Gender in Twitter: Styles, stances, and social net-
works. arXiv preprint arXiv:1210.4567.
S Bird. 2006. Nltk: the natural language toolkit. In Pro-
ceedings of the COLING/ACL Interactive Presentation
Sessions.
J.D. Burger, J. Henderson, and G. Zarrella. 2011. Dis-
criminating gender on Twitter. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
M. Conover, B. Gonc?alves, J. Ratkiewicz, A. Flammini,
and F. Menczer. 2011a. Predicting the politial align-
ment of Twitter users. In Proceedings of the Interna-
tional Conference on Social Computing.
M.D. Conover, J. Ratkiewicz, M. Francisco,
B. Gonc?alves, F Menczer, and A Flammini. 2011b.
Political polarization on Twitter. In Proceedings of
the International Conference on Weblogs and Social
Media.
P. Eckert and S. McConnell-Ginet. 2003. Language and
gender. Cambridge University Press, Cambridge.
F. Giglietto. 2012. If likes were votes: An empirical
study of the 2011 Italian administrative elections. In
Proceedings of the International Conference on We-
blogs and Social Media.
J. Holmes. 1995. Women, men and politeness. Long-
man, London.
M. Kim and H.W. Park. 2012. e-measuring Twitter-
based political participation and deliberation in the
South Korean context by using social network and
Triple Helix indicators. Scientometrics, 90(1):121?
140.
W. Liu and D. Ruths. 2013. What?s in a name? Using
first names as features for gender inference in Twit-
ter. In Analyzing Microtext: 2013 AAAI Spring Sym-
posium.
W. Liu, F.A. Zamal, and D. Ruths. 2012. Using so-
cial media to infer gender composition from commuter
populations. In Proceedings of the When the City
Meets the Citizen Worksop.
A. Mislove, S. Lehmann, Y.Y. Ahn, J.P. Onnela, and J.N.
Rosenquist. 2011. Understanding the demographics
of Twitter users. In Proceedings of the International
Conference on Weblogs and Social Media.
D. Mocanu, A. Baronchelli, B. Gonc?alves, N. Perra, and
A. Vespignani. 2012. The Twitter of Babel: Map-
ping world languages through microblogging plat-
forms. ArXiv e-prints, December.
B. New and C. Landing. 2012. Lexique 3.
http://www.lexique.org/telLexique.php.
F.C.C. Peng, editor. 1981. Male/female differences in
Japanese. The East-West Sign Language Association,
Tokyo.
M. Pennacchiotti and A.M. Popescu. 2011. A machine
learning approach to Twitter user classification. In
Proceedings of the International Conference on We-
blogs and Social Media.
D. Rao and D. Yarowsky. 2010. Detecting latent user
properties in social media. In Proceedings of the NIPS
workshop on Machine Learning for Social Networks.
D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta. 2010.
Classifying latent user attributes in Twitter. In Pro-
ceedings of the International Workshop on Search and
Mining User-generated Contents.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes Twitter users: Real-time event detection
by social sensors. In Proceedings of the International
World Wide Web Conference.
Semiocast. 2012. Brazil becomes the 2nd country on
Twitter, Japan 3rd, Netherlands most active country.
http://semiocast.com/publications/
2012 01 31 Brazil becomes 2nd country on
Twitter superseds Japan.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with Twitter: What
140 characters reveal about political sentiment. In
Proceedings of the International Conference on We-
blogs and Social Media.
W. Weerkamp, S. Carter, and M. Tsagkias. 2011. How
people use Twitter in different languages. In Proceed-
ings of the Web Science Conference.
R. Schenk-Van Witsen. 1981. Les diffe?rences sex-
uelles dans le franc?ais parle?: une e?tude-pilote des
diffe?rences lexicales entre hommes et femmes. Lan-
gage et socie?te?, 17(1):59?78.
F.A. Zamal, W. Liu, and D. Ruths. 2012. Homophily and
latent attribute inference: Inferring latent attributes of
Twitter users from neighbors. In Proceedings of the
International Conference on Weblogs and Social Me-
dia.
1145
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 51?61,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Twitter Users #CodeSwitch Hashtags! #MoltoImportante #wow #???
David Jurgens, Stefan Dimitrov, Derek Ruths
School of Computer Science
McGill University
Montreal, Canada
jurgens@cs.mcgill.ca, stefan.dimitrov@mail.mcgill.ca,
druths@networkdynamics.org
Abstract
When code switching, individuals incor-
porate elements of multiple languages into
the same utterance. While code switching
has been studied extensively in formal and
spoken contexts, its behavior and preva-
lence remains unexamined in many newer
forms of electronic communication. The
present study examines code switching in
Twitter, focusing on instances where an
author writes a post in one language and
then includes a hashtag in a second lan-
guage. In the first experiment, we per-
form a large scale analysis on the lan-
guages used in millions of posts to show
that authors readily incorporate hashtags
from other languages, and in a manual
analysis of a subset the hashtags, reveal
prolific code switching, with code switch-
ing occurring for some hashtags in over
twenty languages. In the second experi-
ment, French and English posts from three
bilingual cities are analyzed for their code
switching frequency and its content.
1 Introduction
Online platforms enable individuals from a wide
variety of linguistic backgrounds to communi-
cate. When individuals share multiple languages
in common, their communication will occasion-
ally include linguistic elements from multiple lan-
guages (Nilep, 2006), a practice commonly re-
ferred to as code switching. Typically, during code
switching, the text or speech in a language retains
its syntactic and morphological constraints for that
language, rather than having text from both lan-
guages conform to one of the language?s grammat-
ical rules. This requirement enables code switch-
ing to be separated from borrowing, where foreign
words are integrated into a native language?s lexi-
con and morphology (Gumperz, 1982; Poplack et
al., 1988; Sankoff et al., 1990).
While work on code switching began with con-
versational analyses, recent work has examined
the phenomena in electronic communication, find-
ing similar evidence of code switching (Climent
et al., 2003; Lee, 2007; Paolillo, 2011). How-
ever, these investigations into code switching have
largely examined interpersonal communication or
settings where the number of participants is lim-
ited. In contrast, social media platforms such as
Twitter offer individuals the ability to write a text
that is decoupled from direct conversation but may
be read widely.
Twitter enables users to post messages with spe-
cial markers known as hashtags, which can serve
as a side channel to comment on the post itself
(Davidov et al., 2010). As a result, multilingual
authors have embraced using hashtags from lan-
guages other than the language of their post. Con-
sider the following real examples:
? Eating an apple for lunch while everyone
around me eats cheeseburgers and fries.
#yoquiero
? Jetzt gibt?s was vern?unftiges zum es-
sen! #salad #turkey #lunch #healthy
#healthylifestyle #loveit
? Hasta ma?nana a todo mundo. Que tengan
linda noche. #MarketerosNocturnos #Mar-
ketingDigital #BlackVirs #SocialMedia
? 1% ????????????? ??????????????????-
???D+ ???? C ??? B+????A ????????????-
???????????? #???? #??????? #fail
Here, the first author posted in English with a
Spanish hashtag reflecting the author?s envious
disposition. In the second, the author comments
in German on sensible food, using multiple En-
glish hashtags to describe the meal and their atti-
tude. In the third and fourth, the authors comment
51
on sleep and school, respectively, and then each
use hashtags with similar meanings in both their
native language and English.
Hashtags provide authors with a communica-
tion medium that also has broader social utility
by embedding their post within global discussion
of other posts using the same hashtag (Letierce et
al., 2010) or by becoming a part of a virtual com-
munity (Gupta et al., 2010). These social motiva-
tions resemble those seen for why individuals may
code switch, such as to assimilate into a group or
make discussions easier (Urciuoli, 1995). Twit-
ter and other hashtag-supporting platforms such as
Instagram and Facebook offer a unique setting for
code switching hashtags for two reasons: (1) po-
tential readers are disconnected from the author,
who may not know of their language fluency, and
(2) text translation is built into the platform, which
enables readers to translate a post into their na-
tive language. As such, authors may be motivated
to include a hashtag of another language to in-
crease their potential audience size or to appear as
a member of a multilingual virtual community.
Despite the prevalence of non-English tweets,
which are approaching 50% of the total volume
(Liu et al., 2014), no study has examined the
prevalence of hashtag code switching. We pro-
pose an initial study of hashtag code switching in
Twitter focusing on three central questions: (1)
for which language pairs do authors write in the
first language and then incorporate a hashtag of
the second language, (2) when tweets include a
hashtag of a different language, which instances
signal code switching behavior, and (3) the degree
to which bilingual populations code switch hash-
tags. Here, we adopt a general definition of code
switching as instances where an individual estab-
lishes a linguistic context in one language and
then includes elements (such as words) from one
or more other languages different from the first.
Two experiments are performed to answer these
questions. In the first, we test general methods to
identify which languages adopt the same hashtags
and whether those shared hashtags are examples of
code switching. In the second, we focus on three
bilingual cities to examine hashtag code switching
behavior in French and English speakers.
Our study provides three main contributions.
First, we demonstrate that hashtag code switching
is widespread in Twitter. Second, we show that
Twitter as a platform includes multiple phenom-
ena that can be falsely interpreted as code switch-
ing and therefore must be accounted for in future
analyses. Third, in a study of French and English
tweets from three cities, we find that an increased
rate of bilinguality decreases the frequency of in-
cluding hashtags from another language but in-
creases the overall rate of code switching when
such hashtags are present. Furthermore, all data
for the experiments is made publicly available.
2 Related Work
Research on code switching is long standing, with
many theories proposed for the motivations be-
hind code switching and how the two languages
interact linguistically (Poplack and Sankoff, 1984;
Myers-Scotton, 1997; Auer, 1998). Most related
to the present work are those studies examining
code switching in online communications.
Climent et al. (2003) examined the use of Span-
ish and Catalan in newsgroups, finding it occurs
2.2% and 4.4% of the Catalan and Spanish con-
texts, respectively. Lee (2007) analyzed a cor-
pus of Cantonese and English emails and ICQ
instant messages and surveyed Hong Kong users
of each form of communication. She found that
the users preferred mixed-language communica-
tion, with no user indicating that they communi-
cated in only Cantonese. Furthermore, the shorter,
more informal ICQ messages were more likely to
be code switched (99.4%) than emails (41.3%).
Paolillo (2011) measured code switching
amongs English, Hindi, and Punjabi in both
IRC and Usenet forum posts, finding similar
to Climent et al. (2003) that the shorter, more
conversational IRC posts had higher rates of
code switching. Paolillo (2011) also note that
code switching rates differed between Hindi and
Punjabi speakers.
The present work differs significantly from
these three studies in two aspects. First, we as-
sess code switching across all language commu-
nities on Twitter, rather than examining individual
groups of bilingual speakers. Second, we focus
our analysis only on the code switching of a post?s
hashtag due to its unique role in microtext (Gupta
et al., 2010), which has yet to be examined in this
context.
3 Hashtag Use in Twitter
Hashtags provide general functionality on Twit-
ter and prior works have proposed that they serve
52
Name Description Examples
ANNOTATION Serves as an annotation about the author?s feelings or comments
on the content of a tweet.
#happy #fail #cute #joking
#YoloSwaggins
COMMUNITY A topical entity that links the tweet with an external community,
which is commonly topical but also includes ?team-like? groups
#music #friends #BecauseItIs-
TheCup #TeamEdward
NAMED
ENTITY
Refers to a specific entity that has a universally recognized
name.
#Glee #TeenChoiceAwards
#WorldCup2014
PLATFORM Refer to some feature or behavior specific to the Twitter plat-
form.
#followback #lasttweet #oomf
APPLICATION Generated by a third-party application, which automatically in-
cludes its hashtag in the message.
#AndroidGames #NowPlaying
#iPhone #Android
VOTING Created as a result of certain real-world phenomena asking in-
dividuals to tweet with specific hashtags as a way of voting.
#MtvHottest #iHeartAwards
ADVERTISING Promoting an item, good, or service, which can be sought out
by interested parties.
#forsale #porn
SPAM Used by adversarial parties to appear on trending lists and to
make spam accounts appear real.
#NanaLoveLingga #681team
#LORDJASONJEROME
Table 1: A taxonomy of hashtag according to their intended use.
a dual role as (1) bookmarking content with the
tag?s particular expression and (2) functioning as a
method for ad hoc community formation and dis-
cussion around a tag?s topic (Gupta et al., 2010;
Davidov et al., 2010; Yang et al., 2012). However,
the diverse user base of the Twitter platform has
given rise to additional roles for hashtags beyond
these two. For example, many popular hashtags
focus on promoting users to follow each other,
1
such as #followback and #openfollow. Similarly,
contests are run on Twitter, which have individu-
als vote by posting using a specific hashtag, e.g.,
#MtvHottest.
Given hashtags? flexible roles, some may be
used in multiple languages without being exam-
ples of code switching, such as the contest-based
or follower-promotion hashtags noted above.
Therefore, we first propose a taxonomy for clas-
sifying all types of hashtags according to their pri-
mary observed use in order to disentangle poten-
tial code switching behavior from Twitter-specific
behavior. To construct the taxonomy, two an-
notators independently reviewed several thousand
hashtags of different frequency to assess the dif-
ferences in how the tag was used in practice. Each
annotator then proposed their own taxonomy. The
final taxonomy was produced from a discussion of
differences, with both annotators initially propos-
ing highly similar taxonomies.
2
1
In Twitter, following denotes creating a directional social
relation from one account to another.
2
We note that a small number of hashtags did not fit this
taxonomy due to their idiosyncratic use. These hashtags were
typically single-letter hashtags used when spelling out words,
e.g., ?tonight is going to be #f #u #n,? or when the author has
mistakenly used punctuation, which is not included in Twit-
Table 1 shows the proposed taxonomy, contain-
ing eight broad types of hashtags. The first two
types of hashtags correspond to the main hash-
tag roles proposed in Yang et al. (2012). The
NAMED ENTITY tags also serve as method for
individuals to link their content with a specific
audience like the COMMUNITY type; however,
NAMED ENTITY tags were treated as a separate
group for the purposes of this study because the
entities typically have a common name which is
used in all languages and therefore would not be
translated; in contrast, COMMUNITY hashtags re-
fer to more general topics such as #soccer, which
may be translated, e.g., #futbol. Hashtags of the
five remaining types would likely not be observed
in instances of code switching, with such hash-
tags often being used for purposes other than inter-
personal communication.
4 Experiment 1: Popular Hashtags
Persistently popular hashtags reflect established
norms of communication on Twitter. We hypoth-
esize that these hashtags may be adopted by the
speakers of multiple languages for joining a global
discussion. Therefore, the first experiment ex-
amines the most-used hashtags over a five month
period to measure two aspects: (1) which lan-
guages adopt the hashtags of other languages and
(2) which hashtags used in multiple languages are
evidence of code switching.
ter?s definition of a hashtag, e.g., ?#I?mAwesome,? which has
the hashtag #I rather than the full expression.
53
4.1 Experimental Setup
Data Hashtag frequencies were calculated from
981M tweets spanning March 2014 to July 2014.
Frequencies were calculated over this five month
period in order to focus on widely-used hashtags,
rather than bursty hashtags that are popular only
for a short time, such as those studied in Huang
et al. (2010) and Lin et al. (2013). For each hash-
tag, up to 10K non-retweet posts containing that
hashtag were retained, randomly sampling from
the time period studied when more than 10K were
observed. To enable a more reliable estimate of
the language distribution, we restrict our analysis
to only those hashtags with more than 1000 posts,
for a total number of 19.4M posts for 4624 hash-
tags, with an average of 4204 posts per hashtag.
Language Identification The languages of
tweets were identified using a two-step procedure.
First, message content was filtered to remove con-
tent such as usernames, URLs, emoji, and hash-
tags. Tweets with fewer than three remaining to-
kens were excluded (e.g., a message with only
hashtags). Second, the remaining content was
processed using langid.py (Lui and Baldwin,
2012), a state of the art language identification
program that supports the diversity of languages
found on Twitter.
Determining the language of a hashtag in a gen-
eral setting for all languages is difficult due to the
presence of acronyms, abbreviations, and slang.
Therefore, we adopt a heuristic where a hashtag?s
language is set as the language used by the major-
ity of its tweets. To quantify the accuracy of this
heuristic, two annotators inspected the tweets of
200 hashtags to identify the language of the hash-
tag and for the majority of the tweets. This anal-
ysis showed that the heuristic correctly identifies
the hashtag?s language in 96.5% of the instances.
4.2 Hashtag Sharing by Languages
The adoption of a hashtag by a second language
was measured by calculating the frequency with
which tweets using a hashtag with language l
1
were labeled with language l
2
. The noisy nature
of microtext is known to make language identifi-
cation difficult (Bergsma et al., 2012; Goldszmidt
et al., 2013) and can create spurious instances
of second-language hashtag adoption. Therefore,
we impose a minimum frequency of hashtag use
where l
2
is only said to use a hashtag of l
1
if at
least 20 tweets using that hashtag were labeled
Hashtag # Langs. Primary
Lang.
Type
#lastfm 39 en APPLICATION
#WaliSupitKEPO 32 id SPAM
#RenggiTampan-
DanKece
32 id SPAM
#NP 32 en APPLICATION
#Np 32 en APPLICATION
#MTVHottest 31 en VOTING
#SidikLoveTini 30 id SPAM
#np 30 en APPLICATION
#GER 29 en NAMED ENTITY
#User Indonesia 29 id APPLICATION
#Soccer 29 en COMMUNITY
#RobotKepo 29 id APPLICATION
#KeePO 27 id APPLICATION
#NowPlaying 28 en APPLICATION
#Hot 28 en ADVERTISEMENT
Table 3: The hashtags associated with the most
number of languages having at least 20 tweets us-
ing that hashtag
with l
2
. To quantify the accuracy of our hashtag
adoption measure, two annotators inspected the
second-language tweets of 200 hashtags, sampled
from the data and representing 40 language pair
combinations; this analysis showed that with the
filtering the assertion that at least one author from
language l
1
used a hashtag of language l
2
was cor-
rect in 67% of the instances.
Table 2 shows the frequency with which au-
thors using the 15 most-commonly observed lan-
guages (shown as columns using their ISO 639-1
language codes) adopt a hashtag from another of
the most-common languages (shown as rows), re-
vealing widespread sharing of hashtags between
languages. English hashtags are the most fre-
quently used in other languages, likely due to it be-
ing the most common language in Twitter. How-
ever, other languages? hashtags are also adopted,
with Spanish, Japanese, and Indonesian being the
most common after English.
Despite the strong evidence of using of a sin-
gle hashtag in multiple languages, the results in
Table 2 should not be interpreted as evidence of
code switching. Table 3 shows the 15 hashtags
used in the most number of languages. The ma-
jority of these hashtags are generated by either
(1) Twitter-based applications that automatically
write a tweet in a user?s native language and then
append a fixed English-language hashtag or (2)
spam-like accounts that use the same hashtag and
include random text snippets in various languages,
neither of which signal code switching behavior.
Furthermore, given the noise introduced by lan-
guage misidentification and spam behavior on the
54
Language of tweet
de ru ko pt en it fr zh es ar th ja id nl tr
de 2 1 4 15 6 9 4 9 1 4 6 1
ru 3 3 3 25 7 5 8 7 2 1 7 7 7 1
ko 4 2 13 3 6 5 10 3 10 11 4 2
pt 14 3 64 45 40 13 63 2 4 3 15 10
en 1705 532 155 1235 1735 2183 1171 2482 362 176 742 1097 1101 342
it 5 2 1 10 29 15 4 22 5 3 6 3 1
fr 38 2 3 36 87 49 28 67 8 1 12 19 29 6
zh 3 4 2 2 12 1 2 4 1 11 1 1
es 67 17 3 321 435 264 206 105 29 5 32 66 66 31
ar 6 2 38 4 9 6 7 8 5 1 2
th 3 7 1 24 5 4 8 8 2 6 4 1
ja 17 18 11 11 123 17 24 132 45 2 2 14 12 4
id 84 2 6 25 131 88 58 14 92 6 5 11 52 17
nl 13 1 3 17 6 11 2 9 1 1
tr 17 1 3 28 9 7 7 13 3 1 22 9
Table 2: The frequency with which a hashtag is used by multiple languages. Columns denote the lan-
guage in which the tweet is written; rows denote the hashtag?s language; and cell values report the
number of hashtags where the column?s language has used the hashtag in at least 20 tweets. Diagonal
same-language values are omitted for clarity.
Twitter platform, we view the initial results in Ta-
ble 2 an overestimate of hashtag adoption by lan-
guages other than the hashtag?s source language.
A further inspection of language classification er-
rors revealed four common factors: (1) the lack of
accents on characters,
3
(2) the use of short words,
which appeared ambiguous to langid.py, (3)
the use of non-Latin characters for emoticons or
visual affect, and (4) proper names originating
from a language different from the tweet?s. Never-
theless, the observed trends do provide some guid-
ance as to which language pairs might share hash-
tags and also may code switch.
Among the hashtags in Table 3, two are legit-
imately used by authors in multiple languages:
#soccer and #GER, the latter corresponding to the
German soccer team. Both hashtags were popular
due to the World Cup, which occurred during the
time period studied. For both, authors included
these hashtags while taking part in a global con-
versation about the games and event. The hashtag
#soccer is a clear case of code switching, where
individuals are communicating their interests in
multiple languages, even when equivalent hash-
tags in the tweet?s language are actively being
used. Indeed, over half of the languages using
#football had at least one tweet containing both
#football and #futbol. The example of #GER high-
lights a boundary case of code switching. Here,
GER is an abbreviation for the country?s name,
making it a highly-recognized marker, rather than
3
In particular, the lack of character accents caused signif-
icant difficulties in distinguish between Spanish and Catalan.
an example of a language change that results in
code switching; however, the country has differ-
ent names depending on the language used (e.g.,
Deutschland), which does point to an active choice
on an author?s part when selecting a particular
name and its abbreviation.
4.3 Analysis by Hashtag Type
In a second analysis, we focus specifically on
hashtags classified as COMMUNITY and ANNO-
TATION, which are more associated with inten-
tional communication actions and therefore more
likely to be used in instances of code switching.
Performing such an analysis at scale would re-
quire automated methods for classifying hashtags
by their use, which is beyond the scope of this ini-
tial investigation. Therefore, we performed a man-
ual analysis of the 100 most-common, 100 least-
common, and 100 median-frequency hashtags in
our dataset to assess the distribution of hashtag
types and cases of code switching among the
COMMUNITY and ANNOTATION hashtags. Two
annotators labeled each hashtag, achieving 64.6%
agreement on the type annotations; disagreements
were largely due to mistaken assignments rather
that disputed classifications.
4
An adjudication step
resolved all disagreements. Additionally, eleven
hashtags were excluded from analysis due being
made of common words (e.g., #go, #be) which had
4
In particular, mistakes were more common when analyz-
ing hashtags used in languages outside the annotators? flu-
ency, which required a more careful assessment of why the
hashtag was being used.
55
 0
 10
 20
 30
 40
 50
advertisement
annotation
application
community
named entity
platform
spam voting
Fre
que
ncy
Lowest FrequencyMedian FrequencyHighest Frequency
Figure 1: Type distributions of the sets of 100
highest, median, and lowest frequency hashtags
used in our dataset
no meaningful interpretation for their use. Fol-
lowing, we describe the results of the analysis and
then highlight several types of hashtags.
Figure 1 shows the distribution of hashtag types
observed in the three samples. SPAM and AP-
PLICATION hashtags were most common among
highest frequency hashtags, whereas the low-
est frequency tags in the dataset were also ei-
ther SPAM or VOTING. Surprisingly, the me-
dian frequency hashtags had the majority of the
discussion-related hashtag types
Within the ANNOTATION and COMMUNITY
types, we selected thirteen hashtags each to man-
ually evaluate if code switching behavior was ob-
served. For each hashtag, two annotators reviewed
all associated tweets that were identified as using
a different language than that of the hashtag. An-
notators were instructed to consider the tweet an
instance of code switching only in cases where
(1) there was sufficient text to determine the mes-
sage?s actual language and (2) the message was an
act of communication (in contrast to spam-like or
nonsensical messages).
Code switching behavior was observed for
eleven of the ANNOTATION hashtags and twelve
of the COMMUNITY hashtags. Table 4 shows
those code switched hashtags and the languages
in which they were seen, highlighting the varying
frequency with which hashtags were used in multi-
ple languages. For example, the primarily Arabic
hashtag #Hadith was used in English and Dutch
tweets; similarity, all three Spanish hashtags were
used in English tweets.
Many hashtags are used primarily with lan-
guages that are associated with countries known
Hashtag Lang. Lang. of Code Switched Tweet
#Noticias es en
#Facts en id th fr es ru
#simple en id es fr ms tr tl sw zh ja ko
#bitch en ar cs de es fr id it ja ms nl pt ru
sv tl tr zh
#delicious en ca de es fr id it ja ko ms nl ru th
tr zh
#Design en ar de es fr ja kr pt th tl zh
#Felicidad es ca en
#SWAG en de es fr id it pl pt ru
#fresh en es fr id it ms nl sv
#BoludecesNO es en
#truth en ar bs bu es fr hi id ja it ms pa pt
ru tl zh
#Hadith ar nl en
#Quran ar fa ms id sw az it de en
#hadith ar fr en
#tech en de es nl ar el fr ro id it ja ms no
pl pt ru sq sv zh
#RemajaIndonesia jv ms
#class en ar tr es bg de fr pt he hr id it ja
lt lv ms nl ru sw tl uk zh
#animals en ar ca de es fr pt it ms ja mk pl pt
ro ru tl tr ur vi
#cine es ca de en fr ja pt ro ru
#sunday en es ar tr fr ca de el gl hu id it ja
ms ko pt nl nn no pl ro ru sl sv
th tl zh
#Energy en ru es de fr it pt tr
#change en ar nl es cs de el eu fr pt id it ja
ko jv lv ms nb no pl ro uk ru sv
ta th tl tr ur zh
#magic en nl fr ar ru ca cs de el it es hu id
ja jv ko lv ru ms nn pl pt ro sq
sv sw sl tl tr zh
Table 4: Code switched hashtags and the lan-
guages of the tweets in which they were seen
(ANNOTATION types top, COMMUNITY types bot-
tom).
to have bilingual speakers fluent in English. How-
ever, several hashtags were used in a variety of
diverse languages. For example, #truth was used
with languages such as Arabic, Bosnian, Bulgar-
ian Hindi, and Punjabi. The most widely code
switched hashtag was #magic. In English, the
hashtag is commonly used with content on magic
tricks; however, in other languages, the hashtag
often connotes surprise. For example, the Lat-
vian tweet ?Es izmekl?eju visu plauktu, nekur nav.
Mamma piejiet ne sekunde nepag?aja, kad vin?a
atrada. #magic? comments on having an item on
the shelf disappear when looking for it, only for it
to reappear like magic.
During annotation, we observed that authors
were highly productive in their code switching, us-
ing these hashtags to generate the types of emo-
tional and sarcastic messages typically seen in
same-language messages. For example, in the
Swedish tweet ?Bussen luktar spya och ?ol. #fresh?
the author is sarcastically commenting on a bus
56
that smells of vomit and beer.
4.4 Discussion
The process of annotating code switching for
hashtags revealed four notable trends in author be-
havior that occurred with multiple hashtags. First,
authors fluent in non-Latin writing systems will
often use Latin-transliterated hashtags, which are
then adopted by authors of Latin-based systems.
For example, the hashtag #aikatsu describes a col-
lectible card game and anime and is heavily used
by both Japanese and English authors. Similar-
ity, the transliterated hashtags #Hadith and #Quran
are commonly associated with Arabic-language
tweets, which rarely include an Arabic-script ver-
sion of those hashtags even when the tweets in-
clude other hashtags in Arabic.
Second, when two or more languages share the
same written form of a word (i.e., homographs),
the resulting hashtags become conflated and ap-
pear as false examples of code switching. For ex-
ample, #Real was widely used in both English and
Spanish, but with two meanings: the English us-
ages denoting something existent (i.e., not fake)
and the Spanish usages referring to Real Madrid
FC, a soccer club. The hashtag #cine also posed
a challenge due to abbreviation. While many
Spanish-language tweets include #cine (cinema),
tweets in other languages include #cinema and its
abbreviated form #cine, which matches the Span-
ish term, creating false evidence of code switch-
ing.
Third, multilingual individuals may adopt a
common hashtag for reasons other than code
switching, which we highlight with two examples.
The hashtag #1DWelcomeToBrazil is used in a
large number of English and Portuguese tweets.
This hashtag is associated with the travel arrival
of the English-speaking band One Direction to
Brazil. Similarly, the #100happydays hashtag was
spawned from a movement where individuals de-
scribe positive aspects of their day. These global
phenomena increases the difficulty of automati-
cally identifying code switching instances.
Fourth, spam accounts will occasionally latch
onto a hashtag and use it in a variety of languages.
For example, the popular hashtag #1000ADAY is
used to attract new followers, which resulted in
adult content services also using the hashtag to
post spam advertisements. Surprisingly, nearly
a third of tweets for this hashtag are in Russian
and feature fully-grammatical text that appears to
be randomly sampled from other sources, such as
lists of proverbs. After examining multiple ac-
counts, we speculate that these messages are actu-
ally bot accounts who need to generate sufficient
number of messages to avoid Twitter?s spam fil-
ters. Work on detecting fake accounts has largely
been done in English (Benevenuto et al., 2010;
Grier et al., 2010; Ghosh et al., 2012) and so may
benefit from detecting this cross-lingual hashtag
use in accounts.
5 Experiment 2: Bilingual Cities
The second experiment measures the prevalence
of hashtag code switching in tweets from three
cities with different populations of English and
French speakers: Montreal, Canada, Quebec City,
Canada and Paris, France. All three cities are
known to contain bilingual speaker as well, who
have been shown to actively code switch (Heller,
1992). To test for differences in the code switch-
ing behavior of populations, each city is analyzed
according to the degree to which Anglophone
and Francophone speakers incorporate hashtags
of other languages into their tweets and whether
translations of the code switched hashtags are used
in the original language.
5.1 Experimental Setup
Data Tweets were gathered for each city by us-
ing the method of Jurgens (2013) to identify Twit-
ter users with a home location within each city?s
greater metropolitan area. Tweets were then ex-
tracted for these users over a three year sample of
10% of Twitter. This process yielded 4.4M tweets
for Montreal, 203K for Quebec City, and 58.1M
for Paris. For efficiency, we restricted the Paris
dataset to 5M tweets, randomly sampled across the
time period.
Language Identification The language of a
tweet was identified using a similar process as in
Experiment 1. Because this setting restricts the
analysis to only English and French, a different
method was used to determine the language of a
hashtag. Given a tweet in language l
1
, the text of
a hashtag is tested to see if it wholly occurs within
the dictionary for l
1
; if not, a greedy tokenization
algorithm is run to attempt to split a hashtag into
constituent words that are in the dictionary of l
1
. If
either the dictionary-lookup and tokenization steps
57
French hashtags on English tweets English hashtags on French tweets
Quebec City Montreal Paris Quebec City Montreal Paris
imfc imfc comprendraquipourra lasttweet gohabsgo bbl
rilive charte sachezle bbl fail teaminsomniaque
relev seriea nian mtvhottest ind teamportugal
ceta bel hollande gohabsgo mtvhottest ps
preorderproblemonitunes brasil2014 federer not not findugame
derpatrash touspourgagner tropa fail soccer adp
villequebec 2ne1 guillaumeradio 100factsaboutme wow lasttweet
tufnations ma vousetespaspret herbyvip podcast follow
ta lavoixtva bel foodies ukraine teamom
rougeetor passionforezria retouraupensionnat electionsqc2014 int thebest
Table 5: The ten most frequent hashtags occurring in French and English tweets
 0
 2
 4
 6
 8
 10
 12
 14
      Montreal       Quebec City Paris
Perc
ent
age
 of 
twe
ets
Englist tweet with French hastagFrench tweet with English hastag
Figure 2: Percentages of tweets with any hashtag
that include a hashtag from the other language
succeed, the hashtag is said to be in l
1
. Other-
wise, the tests are repeated with the second lan-
guage l
2
. If the hashtag cannot be recognized in
l
1
or l
2
, it is assumed to be in the language of its
tweet. The aspell dictionaries were used to rec-
ognize words. Furthermore, after analyzing the er-
rors made due to missing words, dictionaries were
augmented to include common social media terms
in each language (e.g., ?selfie?). A manual anal-
ysis of 100 hashtags each for French and English
showed that this language assignment method was
correct for 91% of the instances.
5.2 Results
Francophone authors were much more likely to
use English hashtags than Anglophone authors
were for French hashtags. For tweets in each lo-
cale and language, Figure 2 shows the percentage
containing a hashtag in the other language relative
to the total number in that city using a hashtag in
either language. Notably, Paris has a higher rate of
using English hashtags than both Canadian cities.
We speculate that this difference is due to the high
rate of bilingualism in Montreal and Quebec City;
because authors are fully fluent in both languages,
should Francophone authors need to express them-
selves with an English hashtag, they may write the
entire tweet in English, rather than code switch-
ing. In contrast, Parisian authors are less likely to
be fully fluent in English (though functional) and
therefore express themselves primarily in French
with English hashtags as desired. An analogous
trend may be seen for French hashtags in the En-
glish tweets from Montreal, which has a higher
population of primarily Anglophone speakers who
might be less willing to communicate entirely in
French but will still use French hashtags to con-
nect their content with the dominant language used
in the city.
For each language and city, Table 5 shows
the ten most popular hashtags incorporated into
tweets of the other language. Examining the most
popular English tags in French tweets shows a
clear distinction in the two populations; French
Parisian tweets include more universal English
hashtags or those generated by applications, which
are not generally instances of code switching. In
contrast, the Canadian cities include more AN-
NOTATION type hashtags, including the sarcasm-
marking #not, which are more indicative of code
switching behavior.
An established linguistic convention within a
population can also motivate authors to prefer
one language?s expression over another (Myers-
Scotton, 1997). To test whether a high-frequency
concept was equally expressed in French and En-
glish or whether one language?s expression was
preferred, we created pairs of equivalent English
and French hashtags expressing the same con-
cept (e.g., #happy/#heureux) by translating the
50 most-popular English hashtags used in French
tweets. Then, the tweets for each city were an-
alyzed to identify which languages were used in
expressing each concept as a hashtag. The results
in Figure 3 reveal that for nearly half of the hash-
58
 0
 10
 20
 30
 40
 50
Montreal
Quebec City
Paris
Ha
sht
ag 
cou
nt
English onlyBoth Languages French Only
Figure 3: For 50 most-common concepts ex-
pressed in equivalent French and English transla-
tions, the frequency with which the hashtags for a
concept were seen in each language.
tags, equivalent French language versions are in
use; however, examining the relative frequencies
shows that in all cases, the English version is still
preferred, despite the presence of a large Franco-
phone population. For hashtags that were only
seen in English, many were of the COMMUNITY
type, e.g., #50factsaboutme, which may not have
an equivalent French-language version. However,
we observed that when both an English hashtag
and its French translation were attested, the use
of the English hashtag in French was most often
an instance of code switching. Hence, testing for
the presence hashtag translation pairs may serve as
a helpful heuristic for identifying hashtags whose
use signals code switching behavior.
6 Discussion
Typically, code switching is distinguished from
the related phenomena of borrowing by testing
whether the word is being fluently mixed into
the utterance instead of simply functioning as a
loan word (Poplack, 2001). Hashtags present a
unique challenge for distinguishing between the
two phenomena due their brief content and un-
structured usage: a hashtag may occur anywhere
in a tweet and its general content lacks grammat-
ical constraints. Examining the hashtags seen in
our study, we find evidence spanning both types
of uses. Common hashtags such as #win or #fail
are widely recognized outside of English and their
uses could easily be interpreted instances of bor-
rowing. However, the complexity of other hash-
tags gives the appearance that their uses go be-
yond that of borrowing, e.g., #goingbacktoschool
in ?Nadie dijo que ser??a f?acil, pero c?omo cuesta
estudiar despu?es de 4 a?nos de no tener nada
acad?emico cerquita #goingbacktoschool? where
the author is commenting on the difficulty of re-
turning for a degree. Still other posts include
multiple single-token hashtags from a second lan-
guage, e.g., the earlier example of ?Jetzt gibt?s was
vern?unftiges zum essen! #salad #turkey #lunch
#healthy #healthylifestyle #loveit.? Although indi-
vidually these hashtags may be widely recognized
and operate as interlingual markers, their com-
bined presence suggests an intentional language
shift on the part of the author that could be inter-
preted as code switching. Together, the examples
point to hashtag use by multiple languages as a
complex phenomena where shared hashtag enti-
ties exist on a graded scale from simple borrow-
ing to fully signaling code switching. Our study
is intended as a starting point for analyzing this
practice and all our data is made available to sup-
port future discussions on the roles these hashtags
play and how they facilitate communication both
within and across language communities.
7 Conclusion
The present work has provided an initial study of
code switching in Twitter focusing on instances
where an author produces a message in one lan-
guage and then includes a hashtag from a sec-
ond language. Our work provides three main con-
tributions. First, using state-of-the-art language
identification techniques, we show that hashtags
are widely shared across languages, though the
challenges of correctly classifying the language
of tweets limits our ability to quantify the exact
scale. Second, in a manual analysis of ANNOTA-
TION and COMMUNITY hashtags, we show that
authors readily code switch with these types of
hashtags, using them just as they would in single
language tweets (e.g., indicating sarcasm). Third,
in a case study of French and English tweets from
three Francophone cities with bilingual speakers,
we find that the cities with more bilingual speakers
tended to have fewer occurrences English hashtags
in French tweets, which we speculate is due to au-
thors being more likely to write such tweets en-
tirely in English, rather than code switch; however,
when English hashtags were observed in French
tweets from these more bilingual cities, they were
much more likely to be used in instances of code
switching. Data for all of the experiments is
59
available at http://www.networkdynamics.org/
datasets/.
Our work raises several avenues for future
work. First, we plan to examine how to improve
language identification in microtext in order to
gain a more accurate estimation of hashtag sharing
and code switching rates for languages. Second,
the Twitter platform enables measuring additional
factors that may influence an individual?s rate of
code switching; specifically, we plan to investigate
(1) a user?s historical tweets to estimate the degree
of bilinguality and (2) the impact of a user?s social
network with respect to homophily and language
use.
References
Peter Auer. 1998. Code-switching in conversation:
Language, interaction and identity. Routledge.
Fabr?cio Benevenuto, Gabriel Magno, Tiago Ro-
drigues, and Virg?lio Almeida. 2010. Detect-
ing spammers on twitter. In Collaboration, elec-
tronic messaging, anti-abuse and spam conference
(CEAS), volume 6, page 12.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74. Associ-
ation for Computational Linguistics.
S. Climent, J. Mor?e, A. Oliver, M. Salvatierra,
I. S`anchez, M. Taul?e, and L. Vallmanya. 2003.
Bilingual newsgroups in catalonia: A challenge for
machine translation. Journal of Computer-Mediated
Communication, 9(1).
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 107?116. Associa-
tion for Computational Linguistics.
Saptarshi Ghosh, Bimal Viswanath, Farshad Kooti,
Naveen Kumar Sharma, Gautam Korlam, Fabri-
cio Benevenuto, Niloy Ganguly, and Krishna Phani
Gummadi. 2012. Understanding and combating
link farming in the twitter social network. In Pro-
ceedings of the 21st international conference on
World Wide Web (WWW), pages 61?70. ACM.
Moises Goldszmidt, Marc Najork, and Stelios Papari-
zos. 2013. Boot-strapping language identifiers
for short colloquial postings. In Proceedings of
the European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in
Databases (ECMLPKDD 2013). Springer Verlag,
September.
Chris Grier, Kurt Thomas, Vern Paxson, and Michael
Zhang. 2010. @ spam: the underground on 140
characters or less. In Proceedings of the 17th ACM
conference on Computer and communications secu-
rity (CCS), pages 27?37. ACM.
John Joseph Gumperz. 1982. Discourse strategies.
Cambridge University Press.
Manish Gupta, Rui Li, Zhijun Yin, and Jiawei Han.
2010. Survey on social tagging techniques. ACM
SIGKDD Explorations Newsletter, 12(1):58?72.
Monica Heller. 1992. The politics of codeswitch-
ing and language choice. Journal of Multilingual
& Multicultural Development, 13(1-2):123?142.
Jeff Huang, Katherine M Thornton, and Efthimis N
Efthimiadis. 2010. Conversational tagging in twit-
ter. In Proceedings of the 21st ACM conference on
Hypertext and hypermedia, pages 173?178. ACM.
David Jurgens. 2013. That?s what friends are for:
Inferring location in online social media platforms
based on social relationships. In Proceedings of the
7th International Conference on Weblogs and Social
Media (ICWSM). AAAI.
Carmen K. M. Lee. 2007. Linguistic features of email
and icq instant messaging in hong kong. In Brenda
Danet and Susan C. Herring, editors, The Multilin-
gual Internet: Language, Culture, and Communica-
tion Online. Oxford University Press.
Julie Letierce, Alexandre Passant, John Breslin, and
Stefan Decker. 2010. Understanding how twitter
is used to spread scientific messages. In WebSci10:
Extending the Frontiers of Society On-Line.
Yu-Ru Lin, Drew Margolin, Brian Keegan, Andrea
Baronchelli, and David Lazer. 2013. # bigbirds
never die: Understanding social dynamics of emer-
gent hashtags. In Seventh International Conference
on Weblogs and Social Media (ICWSM). AAAI.
Yabing Liu, Chloe Kliman-Silver, and Alan Mislove.
2014. The tweets they are a-changin?: Evolution
of twitter users and behavior. In Proceedings of the
8th International Conference on Weblogs and Social
Media (ICWSM). AAAI.
Marco Lui and Timothy Baldwin. 2012. langid. py:
An off-the-shelf language identification tool. In
Proceedings of the ACL 2012 System Demonstra-
tions, pages 25?30. Association for Computational
Linguistics.
Carol Myers-Scotton. 1997. Duelling Languages:
Grammatical Structure in Codeswitching. Claren-
don Press.
Chad Nilep. 2006. Code switching in sociocultural lin-
guistics. Colorado Research in Linguistics, 19(1):1?
22.
60
John C. Paolillo. 2011. Conversational codeswitch-
ing on usenet and internet relay chat. Lan-
guage@Internet, 8.
Shana Poplack and David Sankoff. 1984. Borrowing:
the synchrony of integration. Linguistics, 22(1):99?
136.
Shana Poplack, David Sankoff, and Christopher Miller.
1988. The social correlates and linguistic processes
of lexical borrowing and assimilation. Linguistics,
26(1):47?104.
Shana Poplack. 2001. Code-switching (linguistic). In
International Encyclopedia of the Social and Behav-
ioral Sciences, pages 2062?2065. Elsevier Science
Ltd., 2nd edition.
David Sankoff, Shana Poplack, and Swathi Vanniara-
jan. 1990. The case of the nonce loan in tamil. Lan-
guage variation and change, 2(01):71?101.
Bonnie Urciuoli. 1995. Language and borders. An-
nual Review of Anthropology, 24:pp. 525?546.
Lei Yang, Tao Sun, Ming Zhang, and Qiaozhu Mei.
2012. We know what@ you# tag: does the dual
role affect hashtag adoption? In Proceedings of the
21st international conference on World Wide Web
(WWW), pages 261?270. ACM.
61
