Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322?327,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Morphology Rivals Supervised Morphology for Arabic MT
David Stallard Jacob Devlin
Michael Kayser
BBN Technologies
{stallard,jdevlin,rzbib}@bbn.com
Yoong Keok Lee Regina Barzilay
CSAIL
Massachusetts Institute of Technology
{yklee,regina}@csail.mit.edu
Abstract
If unsupervised morphological analyzers
could approach the effectiveness of super-
vised ones, they would be a very attractive
choice for improving MT performance on
low-resource inflected languages. In this
paper, we compare performance gains for
state-of-the-art supervised vs. unsupervised
morphological analyzers, using a state-of-the-
art Arabic-to-English MT system. We apply
maximum marginal decoding to the unsu-
pervised analyzer, and show that this yields
the best published segmentation accuracy
for Arabic, while also making segmentation
output more stable. Our approach gives
an 18% relative BLEU gain for Levantine
dialectal Arabic. Furthermore, it gives higher
gains for Modern Standard Arabic (MSA), as
measured on NIST MT-08, than does MADA
(Habash and Rambow, 2005), a leading
supervised MSA segmenter.
1 Introduction
If unsupervised morphological segmenters could ap-
proach the effectiveness of supervised ones, they
would be a very attractive choice for improving ma-
chine translation (MT) performance in low-resource
inflected languages. An example of particular cur-
rent interest is Arabic, whose various colloquial di-
alects are sufficiently different from Modern Stan-
dard Arabic (MSA) in lexicon, orthography, and
morphology, as to be low-resource languages them-
selves. An additional advantage of Arabic for study
is the availability of high-quality supervised seg-
menters for MSA, such as MADA (Habash and
Rambow, 2005), for performance comparison. The
MT gain for supervised MSA segmenters on dialect
establishes a lower bound, which the unsupervised
segmenter must exceed if it is to be useful for dialect.
And comparing the gain for supervised and unsuper-
vised segmenters on MSA tells us how useful the
unsupervised segmenter is, relative to the ideal case
in which a supervised segmenter is available.
In this paper, we show that an unsupervised seg-
menter can in fact rival or surpass supervised MSA
segmenters on MSA itself, while at the same time
providing superior performance on dialect. Specifi-
cally, we compare the state-of-the-art morphological
analyzer of Lee et al (2011) with two leading super-
vised analyzers for MSA, MADA and Sakhr1, each
serving as an alternative preprocessor for a state-of-
the-art statistical MT system (Shen et al, 2008). We
measure MSA performance on NIST MT-08 (NIST,
2010), and dialect performance on a Levantine di-
alect web corpus (Zbib et al, 2012b).
To improve performance, we apply maximum
marginal decoding (Johnson and Goldwater, 2009)
(MM) to combine multiple runs of the Lee seg-
menter, and show that this dramatically reduces the
variance and noise in the segmenter output, while
yielding an improved segmentation accuracy that
exceeds the best published scores for unsupervised
segmentation on Arabic Treebank (Naradowsky and
Toutanova, 2011). We also show that it yields MT-
08 BLEU scores that are higher than those obtained
with MADA, a leading supervised MSA segmenter.
For Levantine, the segmenter increases BLEU score
by 18% over the unsegmented baseline.
1http://www.sakhr.com/Default.aspx
322
2 Related Work
Machine translation systems that process highly in-
flected languages often incorporate morphological
analysis. Some of these approaches rely on mor-
phological analysis for pre- and post-processing,
while others modify the core of a translation system
to incorporate morphological information (Habash,
2008; Luong et al, 2010; Nakov and Ng, 2011). For
instance, factored translation Models (Koehn and
Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis
and Koehn, 2008) parametrize translation probabili-
ties as factors encoding morphological features.
The approach we have taken in this paper is
an instance of a segmented MT model, which di-
vides the input into morphemes and uses the de-
rived morphemes as a unit of translation (Sadat and
Habash, 2006; Badr et al, 2008; Clifton and Sarkar,
2011). This is a mainstream architecture that has
been shown to be effective when translating from a
morphologically rich language.
A number of recent approaches have explored
the use of unsupervised morphological analyzers
for MT (Virpioja et al, 2007; Creutz and Lagus,
2007; Clifton and Sarkar, 2011; Mermer and Ak?n,
2010; Mermer and Saraclar, 2011). Virpioja et al
(2007) apply the unsupervised morphological seg-
menter Morfessor (Creutz and Lagus, 2007), and
apply an existing MT system at the level of mor-
phemes. The system does not outperform the word
baseline partially due to the insufficient accuracy of
the automatic morphological analyzer.
The work of Mermer and Ak?n (2010) and Mer-
mer and Saraclar (2011) attempts to integrate mor-
phology and MT more closely than we do, by in-
corporating bilingual alignment probabilities into a
Gibbs-sampled version of Morfessor for Turkish-to-
English MT. However, the bilingual strategy shows
no gain over the monolingual version, and nei-
ther version is competitive for MT with a super-
vised Turkish morphological segmenter (Oflazer,
1993). By contrast, the unsupervised analyzer we
report on here yields MSA-to-English MT perfor-
mance that equals or exceed the performance ob-
tained with a leading supervised MSA segmenter,
MADA (Habash and Rambow, 2005).
3 Review of Lee Unsupervised Segmenter
The segmenter of Lee et al (2011) is a probabilis-
tic model operating at word-type level. It is di-
vided into four sub-model levels. Model 1 prefers
small affix lexicons, and assumes that morphemes
are drawn independently. Model 2 generates a la-
tent POS tag for each word type, conditioning the
word?s affixes on the tag, thereby encouraging com-
patible affixes to be generated together. Model 3
incorporates token-level contextual information, by
generating word tokens with a type-level Hidden
Markov Model (HMM). Finally, Model 4 models
morphosyntactic agreement with a transition proba-
bility distribution, encouraging adjacent tokens with
the same endings to also have the same final suffix.
4 Applying Maximum Marginal Decoding
to Reduce Variance and Noise
Maximum marginal decoding (Johnson and Gold-
water, 2009) (MM) is a technique which assigns
to each latent variable the value with the high-
est marginal probability, thereby maximizing the
expected number of correct assignments (Rabiner,
1989). Johnson and Goldwater (2009) extend MM
to Gibbs sampling by drawing a set of N indepen-
dent Gibbs samples, and selecting for each word the
most frequent segmentation found in them. They
found that MM improved segmentation accuracy
over the mean, consistent with its maximization cri-
terion. However, for our setting, we find that MM
provides several other crucial advantages as well.
First, MM dramatically reduces the output vari-
ance of Gibbs sampling (GS). Table 1 documents the
severity of this variance for the MT-08 lexicon, as
measured by the average exact-match accuracy and
segmentation F-measure between different runs. It
shows that on average, 13% of the word tokens, and
25% of the word types, are segmented differently
from run to run, which obviously makes the input to
MT highly unstable. By contrast the ?MM? column
of Table 1 shows that two different runs of MM, each
derived by combining separate sets of 25 GS runs,
agree on the segmentations of over 95% of the word
token ? a dramatic improvement in stability.
Second, MM reduces noise from the spurious af-
fixes that the unsupervised segmenter induces for
large lexicons. As Table 2 shows, the segmenter
323
Decoding Level Rec Prec F1 Acc
Gibbs Type 82.9 83.2 83.1 74.5
Token 87.5 89.1 88.3 86.7
MM Type 95.9 95.8 95.9 93.9
Token 97.3 94.0 95.6 95.1
Table 1: Comparison of agreement in outputs between
25 runs of Gibbs sampling vs. 2 runs of MM on the
full MT-08 data set. We give the average segmentation
recall, precision, F1-measure, and exact-match accuracy
between outputs, at word-type and word-token levels.
ATB MT-08
GS GS MM Morf
Unique prefixes 17 130 93 287
Unique suffixes 41 261 216 241
Top-95 prefixes 7 7 6 6
Top-95 suffixes 14 26 19 19
Table 2: Affix statistics of unsupervised segmenters. For
the ATB lexicon, we show statistics for the Lee seg-
menter with regular Gibbs sampling (GS). For the MT-
08 lexicon, we also show the output of the Lee segmenter
with maximum marginal decoding (MM). In addition, we
show statistics for Morfessor.
induces 130 prefixes and 261 suffixes for MT-08
(statistics for Morfessor are similar). This phe-
nomenon is fundamental to Bayesian nonparamet-
ric models, which expand indefinitely to fit the data
they are given (Wasserman, 2006). But MM helps
to alleviate it, reducing unique prefixes and suffixes
for MT-08 by 28% and 21%, respectively. It also re-
duces the number of unique prefixes/suffixes which
account for 95% of the prefix/suffix tokens (Top-95).
Finally, we find that in our setting, MM increases
accuracy not just over the mean, but over even the
best-scoring of the runs. As shown in Table 3, MM
increases segmentation F-measure from 86.2% to
88.2%. This exceeds the best published results on
ATB (Naradowsky and Toutanova, 2011).
These results suggest that MM may be worth con-
sidering for other GS applications, not only for the
accuracy improvements pointed out by Johnson and
Goldwater (2009), but also for its potential to pro-
vide more stable and less noisy results.
Model Mean Min Max MM
M1 80.1 79.0 81.5 81.8
M2 81.4 80.2 83.0 82.0
M3 81.4 80.1 82.8 83.2
M4 86.2 85.4 87.2 88.2
Table 3: Segmentation F-scores on ATB dataset for Lee
segmenter, shown for each Model level M1?M4 on the
Arabic segmentation dataset used by (Poon et al, 2009):
We give the mean, minimum, and maximum F-scores for
25 independent runs of Gibbs sampling, together with the
F-score from running MM over that same set of runs.
5 MT Evaluation
5.1 Experimental Design
MT System. Our experiments were performed
using a state-of-the-art, hierarchical string-to-
dependency-tree MT system, described in Shen et
al. (2008).
Morphological Analyzers. We compare the Lee
segmenter with the supervised MSA segmenter
MADA, using its ?D3? scheme. We also compare
with Sakhr, an intensively-engineered, supervised
MSA segmenter which applies multiple NLP tech-
nologies to the segmentation problem, and which
has given the best results for our MT system in pre-
vious work (Zbib et al, 2012a). We also compare
with Morfessor.
MT experiments. We apply the appropriate seg-
menter to split words into morphemes, which we
then treat as words for alignment and decoding. Fol-
lowing Lee et al (2011), we segment the test and
training sets jointly, estimating separate translation
models for each segmenter/dataset combination.
Training and Test Corpora. Our ?Full MSA? cor-
pus is the NIST MT-08 Constrained Data Track Ara-
bic training corpus (35M total, 336K unique words);
our ?Small MSA? corpus is a 1.3M-word subset.
Both are tested on the MT-08 evaluation set. For
dialect, we use a Levantine dialectal Arabic cor-
pus collected from the web with 1.5M total, 160K
unique words and 18K words held-out for test (Zbib
et al, 2012b)
PerformanceMetrics. We evaluate MTwith BLEU
score. To calculate statistical significance, we use
the boot-strap resampling method of Koehn (2004).
324
5.2 Results and Discussion
Table 4 summarizes the BLEU scores obtained from
using various segmenters, for three training/test sets:
Full MSA, Small MSA, and Levantine dialect.
As expected, Sakhr gives the best results for
MSA. Morfessor underperforms the other seg-
menters, perhaps because of its lower accuracy on
Arabic, as reported by Poon et al (2009). The
Lee segmenter gives the best results for Levantine,
inducing valid Levantine affixes (e.g ?hAl+? for
MSA?s ?h*A-Al+?, English ?this-the?) and yielding
an 18% relative gain over the unsegmented baseline.
What is more surprising is that the Lee segmenter
compares favorably with the supervised MSA seg-
menters on MSA itself. In particular, the Lee seg-
menter with MM yields higher BLEU scores than
does MADA, a leading supervised segmenter, while
preserving almost the same performance as GS on
dialect. On Small MSA, it recoups 93% of even
Sakhr?s gain.
By contrast, the Lee segmenter recoups only 79%
of Sakhr?s gain on Full MSA. This might result from
the phenomenon alluded to in Section 4, where addi-
tional data sometimes degrades performance for un-
supervised analyzers. However, the Lee segmenter?s
gain on Levantine (18%) is higher than its gain on
Small MSA (13%), even though Levantine has more
data (1.5M vs. 1.3M words). This might be be-
cause dialect, being less standardized, has more or-
thographic and morphological variability, which un-
supervised segmentation helps to resolve.
These experiments also show that while Model 4
gives the best F-score, Model 3 gives the best MT
scores. Comparison of Model 3 and 4 segmentations
shows that Model 4 induces a much larger num-
ber of inflectional suffixes, especially the feminine
singular suffix ?-p?, which accounts for a plurality
(16%) of the differences by token. While such suf-
fixes improve F-measure on the segmentation refer-
ences, they do not correspond to any English lexical
unit, and thus do not help alignment.
An interesting question is how much performance
might be gained from a supervised segmenter that
was as intensively engineered for dialect as Sakhr
was for MSA. Assuming a gain ratio of 0.93, similar
to Small MSA, the estimated BLEU score would be
20.38, for a relative gain of just 5% over the unsuper-
System Small Full Lev
MSA MSA Dial
Unsegmented 38.69 43.45 17.10
Sakhr 43.99 46.51 19.60
MADA 43.23 45.64 19.29
Morfessor 42.07 44.71 18.38
Lee GS
M1 43.12 44.80 19.70
M2 43.16 45.45 20.15+
M3 43.07 44.82 19.97
M4 42.93 45.06 19.55
Lee MM
M1 43.53 45.14 19.75
M2 43.45 45.29 19.75
M3 43.64+ 45.84 20.09
M4 43.56 45.16 19.93
Table 4: BLEU scores for all experiments. Full MSA is
the the full MT-08 corpus, Small MSA is a 1.3M-word
subset, Lev Dial our Levantine dataset. For each of these,
the highest Lee segmenter score is in bold, with ?+? if
statistically significant vs. MADA at the 95% confidence
level or higher. The highest overall score is in bold italic.
vised segmenter. Given the large engineering effort
that would be required to achieve this gain, the un-
supervised segmenter may be a more cost-effective
choice for dialectal Arabic.
6 Conclusion
We compare unsupervised vs. supervised morpho-
logical segmentation for Arabic-to-English machine
translation. We add maximum marginal decoding
to the unsupervised segmenter, and show that it
surpasses the state-of-the-art segmentation perfor-
mance, purges the segmenter of noise and variabil-
ity, yields BLEU scores on MSA competitive with
those from supervised segmenters, and gives an 18%
relative BLEU gain on Levantine dialectal Arabic.
Acknowledgements
This material is based upon work supported by
DARPA under Contract Nos. HR0011-12-C00014
and HR0011-12-C00015, and by ONR MURI Con-
tract No. W911NF-10-1-0533. Any opinions, find-
ings and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the views of the US government.
We thank Rabih Zbib for his help with interpreting
Levantine Arabic segmentation output.
325
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL-08: HLT.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In Proceedings of ACL-08: HLT, Short
Papers.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4:3:1?
3:34, February.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in Arabic-English statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Short Papers.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparametric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of EMNLP-CoNLL, pages
868?876.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2011. Modeling syntactic context improves
morphological segmentation. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Cos?kun Mermer and Ahmet Afs??n Ak?n. 2010. Unsuper-
vised search for the optimal segmentation for statisti-
cal machine translation. In Proceedings of the ACL
2010 Student Research Workshop, pages 31?36, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Cos?kun Mermer and Murat Saraclar. 2011. Unsuper-
vised Turkish morphological segmentation for statis-
tical machine translation. In Workshop on Machine
Translation and Morphologically-rich languages, Jan-
uary.
Preslav Nakov and Hwee Tou Ng. 2011. Trans-
lating from morphologically complex languages: A
paraphrase-based approach. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
NIST. 2010. NIST 2008 Open Machine Translation
(Open MT) Evaluation. http://www.ldc.
upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2010T21/.
Kemal Oflazer. 1993. Two-level description of Turkish
morphology. In Proceedings of the Sixth Conference
of the European Chapter of the Association for Com-
putational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages 257?
286.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statisti-
cal machine translation based on morphs induced in an
unsupervised manner. In Proceedings of the Machine
Translation Summit XI.
LarryWasserman. 2006. All of Nonparametric Statistics.
Springer.
326
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of EACL.
Rabih Zbib, Michael Kayser, Spyros Matsoukas, John
Makhoul, Hazem Nader, Hamdy Soliman, and Rami
Safadi. 2012a. Methods for integrating rule-based and
statistical systems for Arabic to English machine trans-
lation. Machine Translation, 26(1-2):67?83.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012b. Machine translation of Arabic dialects. In
NAACL 2012: Proceedings of the 2012 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Quebec, Canada, June. Association for
Computational Linguistics.
327
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130?135,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Faster Phrase-Based Decoding by Refining Feature State
Kenneth Heafield Michael Kayser Christopher D. Manning
Computer Science Department Stanford University, Stanford, CA, 94305
{heafield,mkayser,manning}@stanford.edu
Abstract
We contribute a faster decoding algo-
rithm for phrase-based machine transla-
tion. Translation hypotheses keep track
of state, such as context for the language
model and coverage of words in the source
sentence. Most features depend upon only
part of the state, but traditional algorithms,
including cube pruning, handle state atom-
ically. For example, cube pruning will re-
peatedly query the language model with
hypotheses that differ only in source cov-
erage, despite the fact that source cover-
age is irrelevant to the language model.
Our key contribution avoids this behav-
ior by placing hypotheses into equivalence
classes, masking the parts of state that
matter least to the score. Moreover, we ex-
ploit shared words in hypotheses to itera-
tively refine language model scores rather
than handling language model state atom-
ically. Since our algorithm and cube prun-
ing are both approximate, improvement
can be used to increase speed or accuracy.
When tuned to attain the same accuracy,
our algorithm is 4.0?7.7 times as fast as
the Moses decoder with cube pruning.
1 Introduction
Translation speed is critical to making suggestions
as translators type, mining for parallel data by
translating the web, and running on mobile de-
vices without Internet connectivity. We contribute
a fast decoding algorithm for phrase-based ma-
chine translation along with an implementation in
a new open-source (LGPL) decoder available at
http://kheafield.com/code/.
Phrase-based decoders (Koehn et al, 2007; Cer
et al, 2010; Wuebker et al, 2012) keep track
of several types of state with translation hypothe-
ses: coverage of the source sentence thus far, con-
text for the language model, the last position for
the distortion model, and anything else features
need. Existing decoders handle state atomically:
hypotheses that have exactly the same state can be
recombined and efficiently handled via dynamic
programming, but there is no special handling for
partial agreement. Therefore, features are repeat-
edly consulted regarding hypotheses that differ
only in ways irrelevant to their score, such as cov-
erage of the source sentence. Our decoder bun-
dles hypotheses into equivalence classes so that
features can focus on the relevant parts of state.
We pay particular attention to the language
model because it is responsible for much of the hy-
pothesis state. As the decoder builds translations
from left to right (Koehn, 2004), it records the last
N ? 1 words of each hypothesis so that they can
be used as context to score the first N ? 1 words
of a phrase, where N is the order of the language
model. Traditional decoders (Huang and Chiang,
2007) try thousands of combinations of hypothe-
ses and phrases, hoping to find ones that the lan-
guage model likes. Our algorithm instead discov-
ers good combinations in a coarse-to-fine manner.
The algorithm exploits the fact that hypotheses of-
ten share the same suffix and phrases often share
the same prefix. These shared suffixes and prefixes
allow the algorithm to coarsely reason over many
combinations at once.
Our primary contribution is a new search algo-
rithm that exploits the above observations, namely
that state can be divided into pieces relevant to
each feature and that language model state can be
further subdivided. The primary claim is that our
algorithm is faster and more accurate than the pop-
ular cube pruning algorithm.
2 Related Work
Our previous work (Heafield et al, 2013) devel-
oped language model state refinement for bottom-
130
up decoding in syntatic machine translation. In
bottom-up decoding, hypotheses can be extended
to the left or right, so hypotheses keep track of
both their prefix and suffix. The present phrase-
based setting is simpler because sentences are
constructed from left to right, so prefix infor-
mation is unnecessary. However, phrase-based
translation implements reordering by allowing hy-
potheses that translate discontiguous words in the
source sentence. There are exponentially many
ways to cover the source sentence and hypotheses
carry this information as additional state. A main
contribution in this paper is efficiently ignoring
coverage when evaluating the language model. In
contrast, syntactic machine translation hypotheses
correspond to contiguous spans in the source sen-
tence, so in prior work we simply ran the search
algorithm in every span.
Another improvement upon Heafield et al
(2013) is that we previously made no effort to
exploit common words that appear in translation
rules, which are analogous to phrases. In this
work, we explicitly group target phrases by com-
mon prefixes, doing so directly in the phrase table.
Coarse-to-fine approaches (Petrov et al, 2008;
Zhang and Gildea, 2008) invoke the decoder
multiple times with increasingly detailed models,
pruning after each pass. The key difference in our
work is that, rather than refining models in lock
step, we effectively refine the language model on
demand for hypotheses that score well. More-
over, their work was performed in syntactic ma-
chine translation while we address issues specific
to phrase-based translation.
Our baseline is cube pruning (Chiang, 2007;
Huang and Chiang, 2007), which is both a way
to organize search and an algorithm to search
through cross products of sets. We adopt the same
search organization (Section 3.1) but change how
cross products are searched.
Chang and Collins (2011) developed an exact
decoding algorithm based on Lagrangian relax-
ation. However, it has not been shown to tractably
scale to 5-gram language models used by many
modern translation systems.
3 Decoding
We begin by summarizing the high-level organiza-
tion of phrase-based cube pruning (Koehn, 2004;
Koehn et al, 2007; Huang and Chiang, 2007).
Sections 3.2 and later show our contribution.
0 word 1 word
the
cat
.
2 words
cat
the cat
cat the
. the
3 words
cat .
the cat .
cat the .
. the cat
Figure 1: Stacks to translate the French ?le chat .?
into English. Filled circles indicate that the source
word has been translated. A phrase translates ?le
chat? as simply ?cat?, emphasizing that stacks are
organized by the number of source words rather
than the number of target words.
3.1 Search Organization
Phrase-based decoders construct hypotheses from
left to right by appending phrases in the target lan-
guage. The decoder organizes this search process
using stacks (Figure 1). Stacks contain hypothe-
ses that have translated the same number of source
words. The zeroth stack contains one hypothe-
sis with nothing translated. Subsequent stacks are
built by extending hypotheses in preceding stacks.
For example, the second stack contains hypothe-
ses that translated two source words either sepa-
rately or as a phrasal unit. Returning to Figure 1,
the decoder can apply a phrase pair to translate ?le
chat? as ?cat? or it can derive ?the cat? by translat-
ing one word at a time; both appear in the second
stack because they translate two source words. To
generalize, the decoder populates the ith stack by
pairing hypotheses in the i ? jth stack with tar-
get phrases that translate source phrases of length
j. Hypotheses remember which source word they
translated, as indicated by the filled circles.
The reordering limit prevents hypotheses from
jumping around the source sentence too much and
dramatically reduces the search space. Formally,
the decoder cannot propose translations that would
require jumping back more than R words in the
source sentence, including multiple small jumps.
In practice, stacks are limited to k hypothe-
ses, where k is set by the user. Small k is faster
but may prune good hypotheses, while large k is
slower but more thorough, thereby comprising a
time-accuracy trade-off. The central question in
this paper is how to select these k hypotheses.
Populating a stack boils down to two steps.
First, the decoder matches hypotheses with source
phrases subject to three constraints: the total
source length matches the stack being populated,
none of the source words has already been trans-
131
country
a
nations
few
countries
Figure 2: Hypothesis suffixes arranged into a trie.
The leaves indicate source coverage and any other
hypothesis state.
lated, and the reordering limit. Second, the de-
coder searches through these matches to select
k high-scoring hypotheses for placement in the
stack. We improve this second step.
The decoder provides our algorithm with pairs
consisting of a hypothesis and a compatible source
phrase. Each source phrase translates to multiple
target phrases. The task is to grow these hypothe-
ses by appending a target phrase, yielding new hy-
potheses. These new hypotheses will be placed
into a stack of size k, so we are interested in se-
lecting k new hypotheses that score highly.
Beam search (Lowerre, 1976; Koehn, 2004)
tries every hypothesis with every compatible tar-
get phrase then selects the top k new hypotheses
by score. This is wasteful because most hypothe-
ses are discarded. Instead, we follow cube pruning
(Chiang, 2007) in using a priority queue to gen-
erate k hypotheses. A key difference is that we
generate these hypotheses iteratively.
3.2 Tries
For each source phrase, we collect the set of com-
patible hypotheses. We then place these hypothe-
ses in a trie that emphasizes the suffix words be-
cause these matter most when appending a target
phrase. Figure 2 shows an example. While it suf-
fices to build this trie on the last N ? 1 words
that matter to the language model, Li and Khu-
danpur (2008) have identified cases where fewer
words are necessary because the language model
will back off. The leaves of the trie are complete
hypotheses and reveal information irrelevant to the
language model, such as coverage of the source
sentence and the state of other features.
Each source phrase translates to a set of tar-
get phrases. Because these phrases will be ap-
pended to a hypothesis, the first few words mat-
ter the most to the language model. We therefore

which
have diplomatic
are
that have
diplomatic
Figure 3: Target phrases arranged into a trie. Set
in italic, leaves reveal parts of the phrase that are
irrelevant to the language model.
arrange the target phrases into a prefix trie. An
example is shown in Figure 3. Similar to the hy-
pothesis trie, the depth may be shorter than N ? 1
in cases where the language model will provably
back off (Li and Khudanpur, 2008). The trie can
also be short because the target phrase has fewer
than N ? 1 words. We currently store this trie
data structure directly in the phrase table, though
it could also be computed on demand to save mem-
ory. Empirically, our phrase table uses less RAM
than Moses?s memory-based phrase table.
As an optimization, a trie reveals multiple
words when there would otherwise be no branch-
ing. This allows the search algorithm to make de-
cisions only when needed.
Following Heafield et al (2013), leaves in the
trie take the score of the underlying hypothesis or
target phrase. Non-leaf nodes take the maximum
score of their descendants. Children of a node are
sorted by score.
3.3 Boundary Pairs
The idea is that the decoder reasons over pairs of
nodes in the hypothesis and phrase tries before de-
vling into detail. In this way, it can determine what
the language model likes and, conversely, quickly
discard combinations that the model does not like.
A boundary pair consists of a node in the hy-
pothesis trie and a node in the target phrase trie.
For example, the decoder starts at the root of each
trie with the boundary pair (, ). The score of a
boundary pair is the sum of the scores of the un-
derlying trie nodes. However, once some words
have been revealed, the decoder calls the language
model to compute a score adjustment. For exam-
ple, the boundary pair (country, that) has score ad-
justment
log
p(that | country)
p(that)
times the weight of the language model. This
has the effect of cancelling out the estimate made
132
when the phrase was scored in isolation, replacing
it with a more accurate estimate based on avail-
able context. These score adjustments are efficient
to compute because the decoder retained a pointer
to ?that? in the language model?s data structure
(Heafield et al, 2011).
3.4 Splitting
Refinement is the notion that the boundary pair
(, ) divides into several boundary pairs that re-
veal specific words from hypotheses or target
phrases. The most straightforward way to do this
is simply to split into all children of a trie node.
Continuing the example from Figure 2, we could
split (, ) into three boundary pairs: (country, ),
(nations, ), and (countries, ). However, it is
somewhat inefficient to separately consider the
low-scoring child (countries, ). Instead, we con-
tinue to split off the best child (country, ) and
leave a note that the zeroth child has been split off,
denoted ([1
+
], ). The index increases each time
a child is split off.
The the boundary pair ([1
+
], ) no longer
counts (country, ) as a child, so its score is lower.
Splitting alternates sides. For example,
(country, ) splits into (country, that) and
(country, [1
+
]). If one side has completely
revealed words that matter to the language model,
then splitting continues with the other side.
This procedure ensures that the language model
score is completely resolved before considering
irrelevant differences, such as coverage of the
source sentence.
3.5 Priority Queue
Search proceeds in a best-first fashion controlled
by a priority queue. For each source phrase,
we convert the compatible hypotheses into a trie.
The target phrases were already converted into a
trie when the phrase table was loaded. We then
push the root (, ) boundary pair into the prior-
ity queue. We do this for all source phrases under
consideration, putting their root boundary pairs
into the same priority queue. The algorithm then
loops by popping the top boundary pair. It the top
boundary pair uniquely describes a hypothesis and
target phrase, then remaining features are evalu-
ated and the new hypothesis is output to the de-
coder?s stack. Otherwise, the algorithm splits the
boundary pair and pushes both split versions. Iter-
ation continues until k new hypotheses have been
found.
3.6 Overall Algorithm
We build hypotheses from left-to-right and man-
age stacks just like cube pruning. The only dif-
ference is how the k elements of these stacks are
selected.
When the decoder matches a hypothesis with a
compatible source phrase, we immediately evalu-
ate the distortion feature and update future costs,
both of which are independent of the target phrase.
Our future costs are exactly the same as those used
in Moses (Koehn et al, 2007): the highest-scoring
way to cover the rest of the source sentence. This
includes the language model score within target
phrases but ignores the change in language model
score that would occur were these phrases to be
appended together. The hypotheses compatible
with each source phrase are arranged into a trie.
Finally, the priority queue algorithm from the pre-
ceding section searches for options that the lan-
guage model likes.
4 Experiments
The primary claim is that our algorithm performs
better than cube pruning in terms of the trade-off
between time and accuracy. We compare our new
decoder implementation with Moses (Koehn et al,
2007) by translating 1677 sentences from Chinese
to English. These sentences are a deduplicated
subset of the NIST Open MT 2012 test set and
were drawn from Chinese online text sources, such
as discussion forums. We trained our phrase table
using a bitext of 10.8 million sentence pairs, which
after tokenization amounts to approximately 290
million words on the English side. The bitext con-
tains data from several sources, including news ar-
ticles, UN proceedings, Hong Kong government
documents, online forum data, and specialized
sources such as an idiom translation table. We also
trained our language model on the English half of
this bitext using unpruned interpolated modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998).
The system has standard phrase table, length,
distortion, and language model features. We
plan to implement lexicalized reordering in future
work; without this, the test system is 0.53 BLEU
(Papineni et al, 2002) point behind a state-of-the-
art system. We set the reordering limit to R = 15.
The phrase table was pre-pruned by applying the
same heuristic as Moses: select the top 20 target
phrases by score, including the language model.
133
-29.5
-29.0
-28.5
-28.0
-27.5
0 1 2 3 4
A
v
e
r
a
g
e
m
o
d
e
l
s
c
o
r
e
CPU seconds/sentence
This Work
Moses
13
14
15
0 1 2 3 4
U
n
c
a
s
e
d
B
L
E
U
CPU seconds/sentence
This Work
Moses
Figure 4: Performance of our decoder and Moses for various stack sizes k.
Moses (Koehn et al, 2007) revision d6df825
was compiled with all optimizations recom-
mended in the documentation. We use the in-
memory phrase table for speed. Tests were run
on otherwise-idle identical machines with 32 GB
RAM; the processes did not come close to running
out of memory. The language model was com-
piled into KenLM probing format (Heafield, 2011)
and placed in RAM while text phrase tables were
forced into the disk cache before each run. Timing
is based on CPU usage (user plus system) minus
loading time, as measured by running on empty
input; our decoder is also faster at loading. All re-
sults are single-threaded. Model score is compa-
rable across decoders and averaged over all 1677
sentences; higher is better. The relationship be-
tween model score and uncased BLEU (Papineni
et al, 2002) is noisy, so peak BLEU is not attained
by the highest search accuracy.
Figure 4 shows the results for pop limits k rang-
ing from 5 to 10000 while Table 1 shows select
results. For Moses, we also set the stack size to
k to disable a second pruning pass, as is common.
Because Moses is slower, we also ran our decoder
with higher beam sizes to fill in the graph. Our
decoder is more accurate, but mostly faster. We
can interpret accuracy improvments as speed im-
provements by asking how much time is required
to attain the same accuracy as the baseline. By
this metric, our decoder is 4.0 to 7.7 times as fast
as Moses, depending on k.
Model CPU BLEU
Stack Moses This Moses This Moses This
10 -29.96 -29.70 0.019 0.004 12.92 13.46
100 -28.68 -28.54 0.057 0.016 14.19 14.40
1000 -27.87 -27.80 0.463 0.116 14.91 14.95
10000 -27.46 -27.39 4.773 1.256 15.32 15.28
Table 1: Results for select stack sizes k.
5 Conclusion
We have contributed a new phrase-based search al-
gorithm based on the principle that the language
model cares the most about boundary words. This
leads to two contributions: hiding irrelevant state
from features and an incremental refinement algo-
rithm to find high-scoring combinations. This al-
gorithm is implemented in a new fast phrase-based
decoder, which we release as open-source under
the LGPL at kheafield.com/code/.
Acknowledgements
This work was supported by the Defense Ad-
vanced Research Projects Agency (DARPA)
Broad Operational Language Translation (BOLT)
program through IBM. This work used Stam-
pede provided by the Texas Advanced Comput-
ing Center (TACC) at The University of Texas at
Austin under XSEDE allocation TG-CCR140009.
XSEDE is supported by NSF grant number OCI-
1053575. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA or the US government.
134
References
Daniel Cer, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2010. Phrasal: A statis-
tical machine translation toolkit for exploring new
model features. In Proceedings of the NAACL HLT
2010 Demonstration Session, pages 9?12, Los An-
geles, California, June. Association for Computa-
tional Linguistics.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, Scotland, UK, July.
Association for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, August.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201?228,
June.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tet-
suo Kiso, and Marcello Federico. 2011. Left lan-
guage model state for syntactic machine translation.
In Proceedings of the International Workshop on
Spoken Language Translation, San Francisco, CA,
USA, December.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words
to speed k-best extraction from hypergraphs. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, Georgia, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, Prague, Czech Repub-
lic, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181?184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic, June.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Machine translation: From real
users to research, pages 115?124. Springer, Septem-
ber.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings of the Second ACL Workshop on Syn-
tax and Structure in Statistical Translation (SSST-2),
pages 10?18, Columbus, Ohio, June.
Bruce T. Lowerre. 1976. The Harpy speech recogni-
tion system. Ph.D. thesis, Carnegie Mellon Univer-
sity, Pittsburgh, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 108?116, Honolulu, HI,
USA, October.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In Proceedings of COLING 2012:
Demonstration Papers, pages 483?492, Mumbai, In-
dia, December.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of ACL-08: HLT, pages 209?
217, Columbus, Ohio.
135
