R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 1 ? 9, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A New Method for Sentiment Classification  
in Text Retrieval 
Yi Hu1, Jianyong Duan1, Xiaoming Chen1,2, Bingzhen Pei1,2, and Ruzhan Lu1 
1 Department of Computer Science and Engineering, 
Shanghai Jiao Tong University, Shanghai, China, 200030 
2 School of Computer Science and Engineering, 
Guizhou University, Guiyang, China, 550025 
{huyi, duan_jy, chen-xm, peibz, rz-lu}@cs.sjtu.edu.cn 
Abstract. Traditional text categorization is usually a topic-based task, but a 
subtle demand on information retrieval is to distinguish between positive and 
negative view on text topic. In this paper, a new method is explored to solve 
this problem. Firstly, a batch of Concerned Concepts in the researched domain 
is predefined. Secondly, the special knowledge representing the positive or 
negative context of these concepts within sentences is built up. At last, an 
evaluating function based on the knowledge is defined for sentiment classifica-
tion of free text. We introduce some linguistic knowledge in these procedures to 
make our method effective. As a result, the new method proves better compared 
with SVM when experimenting on Chinese texts about a certain topic. 
1   Introduction 
Classical technology in text categorization pays much attention to determining 
whether a text is related to a given topic [1], such as sports and finance. However, as 
research goes on, a subtle problem focuses on how to classify the semantic orientation 
of the text. For instance, texts can be for or against ?racism?, and not all the texts are 
bad. There exist two possible semantic orientations: positive and negative (the neutral 
view is not considered in this paper). Labeling texts by their semantic orientation 
would provide readers succinct summaries and be great useful in intelligent retrieval 
of information system. 
Traditional text categorization algorithms, including Na?ve Bayes, ANN, SVM, etc, 
depend on a feature vector representing a text. They usually utilize words or n-grams 
as features and construct the weightiness according to their presence/absence or fre-
quencies. It is a convenient way to formalize the text for calculation. On the other 
hand, employing one vector may be unsuitable for sentiment classification. See the 
following simple sentence in English: 
? Seen from the history, the great segregation is a pioneering work. 
Here, ?segregation? is very helpful to determine that the text is about the topic of 
racism, but the terms ?great? and ?pioneering work? may just be the important hints 
for semantic orientation (support the racism). These two terms probably contribute 
2 Y. Hu et al 
less to sentiment classification if they are dispersed into the text vector because the 
relations between them and ?segregation? are lost. Intuitively, these terms can provide 
more contribution if they are considered as a whole within the sentence. We explore a 
new idea for sentiment classification by focusing on sentences rather than entire text. 
?Segregation? is called as Concerned Concept in our work. These Concerned 
Concepts are always the sensitive nouns or noun phrases in the researched domain 
such as ?race riot?, ?color line? and ?government?. If the sentiment classifying 
knowledge about how to comment on these concepts can be acquired, it will be 
helpful for sentiment classification when meeting these concepts in free texts again. 
In other words, the task of sentiment classification of entire text has changed into 
recognizing the semantic orientation of the context of all Concerned Concepts.  
We attempt to build up this kind of knowledge to describe different sentiment 
context by integrating extended part of speech (EPOS), modified triggered bi-grams 
and position information within sentences. At last, we experiment on Chinese texts 
about ?racism? and draw some conclusions. 
2    Previous Work 
A lot of past work has been done about text categorization besides topic-based clas-
sification. Biber [2] concentrated on sorting texts in terms of their source or source 
style with stylistic variation such as author, publisher, and native-language  
background. 
Some other related work focused on classifying the semantic orientation of indi-
vidual words or phrases by employing linguistic heuristics [3][4]. Hatzivassiloglou 
et alworked on predicting the semantic orientation of adjectives rather than phrases 
containing adjectives and they noted that there are linguistic constraints on these 
orientations of adjectives in conjunctions.   
Past work on sentiment-based categorization of entire texts often involved using 
cognitive linguistics [5][11] or manually constructing discriminated lexicons 
[7][12]. All these work enlightened us on the research on Concerned Concepts in 
given domain.  
Turney?s work [9] applied an unsupervised learning algorithm based on the mu-
tual information between phrases and the both words ?excellent? and ?poor?. The 
mutual information was computed using statistics gathered by a search engine and 
simple to be dealt with, which encourage further work with sentiment classification.  
Pang et al[10] utilized several prior-knowledge-free supervised machine learning 
methods in the sentiment classification task in the domain of movie review, and 
they also analyzed the problem to understand better how difficult it is. They ex-
perimented with three standard algorithms: Na?ve Bayes, Maximum Entropy and 
Support Vector Machines, then compared the results. Their work showed that, gen-
erally, these algorithms were not able to achieve accuracies on the sentiment  
classification problem comparable to those reported for standard topic-based  
categorization. 
 A New Method for Sentiment Classification in Text Retrieval 3 
3   Our Work 
3.1   Basic Idea 
As mentioned above, terms in a text vector are usually separated from the Concerned 
Concepts (CC for short), which means no relations between these terms and CCs. To 
avoid the coarse granularity of text vector to sentiment classification, the context of 
each CC is researched on. We attempt to determine the semantic orientation of a free 
text by evaluating context of CCs contained in sentences. Our work is based on the 
two following hypothesizes: 
? H1.  A sentence holds its own sentiment context and it is the processing 
unit for sentiment classification. 
? H2.  A sentence with obvious semantic orientation contains at least one 
Concerned Concept. 
H1 allows us to research the classification task within sentences and H2 means that a 
sentence with the value of being learnt or evaluated should contain at least one de-
scribed CC. A sentence can be formed as: 
                         
( 1) 1 1 ( 1)... ...m m i n nword word word CC word word word? ? ? ? ?
 .                  (1) 
CCi (given as an example in this paper) is a noun or noun phrase occupying the po-
sition 0 in sentence that is automatically tagged with extended part of speech (EPOS 
for short)(see section 3.2). A word and its tagged EPOS combine to make a 2-tuple, 
and all these 2-tuples on both sides of CCi can form a sequence as follows: 
??
???
???
???
?
?????
???
???
???
?
?????
???
?
??
???
?
?
?
?
?
??
??
?
?
n
n
n
ni
m
m
m
m
epos
word
epos
word
epos
wordCC
epos
word
epos
word
epos
word
)1(
)1(
1
1
1
1
)1(
)1(
.        (2) 
All the words and corresponding EPOSes are divided into two parts: m 2-tuples on 
the left side of CCi (from ?m to -1) and n 2-tuples on the right (from 1 to n). These 2-
tuples construct the context of the Concerned Concept CCi.  
The sentiment classifying knowledge (see sections 3.3 and 3.4) is the contribution 
of all the 2-tuples to sentiment classification. That is to say, if a 2-tuple often co-
occurs with CCi in training corpus with positive view, it contributes more to positive 
orientation than negative one. On the other hand, if the 2-tuple often co-occurs with 
CCi in training corpus with negative view, it contributes more to negative orientation. 
This kind of knowledge can be acquired by statistic technology from corpus.  
When judging a free text, the context of CCi met in a sentence is respectively com-
pared with the positive and negative sentiment classifying knowledge of the same CCi 
trained from corpus. Thus, an evaluating function E (see section 3.5) is defined to 
evaluate the semantic orientation of the free text. 
3.2   Extended Part of Speech 
Usual part of speech (POS) carries less sentiment information, so it cannot distinguish 
the semantic orientation between positive and negative. For example, ?hearty? and 
?felonious? are both tagged as ?adjective?, but for the sentiment classification, only 
4 Y. Hu et al 
the tag ?adjective? cannot classify their sentiment. This means different adjective has 
different effect on sentiment classification. So we try to extend words? POS (EPOS) 
according to its semantic orientation. 
Generally speaking, empty words only have structural function without sentiment 
meaning. Therefore, we just consider substantives in context, which mainly include 
nouns/noun phrases, verbs, adjectives and adverbs. We give a subtler manner to de-
fine EPOS of substantives. Their EPOSes are classified to be positive orientation 
(PosO) or negative orientation (NegO). Thus, ?hearty? is labeled with ?pos-adj?, 
which means PosO of adjective; ?felonious? is labeled with ?neg-adje?, which means 
NegO of adjective. Similarly, nouns, verbs and adverbs tagged with their EPOS con-
struct a new word list. In our work, 12,743  Chinese entries in machine readable dic-
tionary are extended by the following principles: 
? To nouns, their PosO or NegO is labeled according to their semantic ori-
entation to the entities or events they denote (pos-n or neg-n).  
? To adjectives, their common syntax structure is {Adj.+Noun*}. If adjec-
tives are favor of or oppose to their headwords (Noun*), they will be de-
fined as PosO or NegO (pos-adj or neg-adj). 
? To adverbs, their common syntax structure is {Adv.+Verb*/Adj*.}, and 
Verb*/Adj*. is headword. Their PosO or NegO are analyzed in the same 
way of adjective (pos-adv or neg-adv).  
? To transitive verb, their common syntax structure is {TVerb+Object*}, 
and Object* is headword. Their PosO or NegO are analyzed in the same 
way of adjective (pos-tv or neg-tv). 
? To intransitive verb, their common syntax structure is {Sub-
ject*+InTVerb}, and Subject* is headword. Their PosO or NegO are ana-
lyzed in the same way of adjective (pos-iv or neg-iv). 
3.3   Sentiment Classifying Knowledge Framework 
Sentiment classifying knowledge is defined as the importance of all 2-tuples <word, 
epos> that compose the context of CCi (given as an example) to sentiment classifica-
tion and every Concerned Concept like CCi has its own positive and negative senti-
ment classifying knowledge that
 
can be formalized as a 3-tuple K: 
: ( , , )pos negK CC S S=  .                                              (3) 
To CCi, its Sipos has concrete form that is described as a set of 5-tuples: 
{ }: ( , , , , , )pos left rightiS word epos wordval eposval? ? ? ? ? ?? ?= < >  .              (4) 
Where Sipos represents the positive sentiment classifying knowledge of CCi, and it is a 
data set about all 2-tuples <word, epos> appearing in the sentences containing CCi in 
training texts with positive view. In contrast, Sineg is acquired from the training texts 
with negative view. In other words, Sipos and Sineg respectively reserve the features for 
positive and negative classification to CCi in corpus. 
In terms of Sipos, the importance of ,word epos? ?< > is divided into wordval?  and 
eposval?  (see section 4.1) which is estimated by modified triggered bi-grams to fit the 
 A New Method for Sentiment Classification in Text Retrieval 5 
long distance dependence. If ,word epos? ?< > appears on the left side of CCi, the 
?side? adjusting factor is lefti? ; if it appears on the right, the ?side? adjusting factor is 
right
i? . We also define another factor ?  (see section 4.3) that denotes dynamic ?posi-
tional? adjusting information during processing a sentence in free text. 
3.4   Contribution of <word, epos>  
If a <word, epos> often co-occurs with CCi in sentences in training corpus with posi-
tive view, which may means it contribute more to positive orientation than negative 
one, and if it often co-occurs with CCi in negative corpus, it may contribute more to 
negative orientation.  
We modify the classical bi-grams language model to introduce long distance trig-
gered mechanism of ,iCC word epos?< > . Generally to describe, the contribution c of 
each 2-tuple in a positive or negative context (denoted by Pos_Neg) is calculated by 
(5). This is an analyzing measure of using multi-feature resources.  
( )( , | , _ ) : exp Pr( , | , _ ) , 0i ic word epos CC Pos Neg word epos CC Pos Neg?? ? ?< > = < > >  .   (5) 
The value represents the contribution of <word, epos> to sentiment classification in 
the sentence containing CCi. Obviously, when ? and ?  are fixed, the bigger 
Pr(<word, epos>|CCi, Pos_Neg>) is, the bigger contribution c of the 2-tuple <word, 
epos> to the semantic orientation Pos_Neg (one of {positive, negative} view) is. 
It has been mentioned that ? and ? are adjusting factor to the sentiment contribu-
tion of pair <word, epos>. ?  rectifies the effect of the 2-tuple according to its ap-
pearance on which side of CCi, and ?  rectifies the effect of the 2-tuple according to 
its distance from CCi. They embody the effect of ?side? and ?position?. Thus, it can 
be inferred that even the same <word, epos> will contribute differently because of its 
side and position. 
3.5   Evaluation Function E 
We propose a function E (equation (6)) to evaluate a free text by comparing the con-
text of every appearing CC with the two sorts of sentiment context of the same CC 
trained from corpus respectively.  
                              ( )' '
1
(1/ ) ( , ) ( , )
N
pos neg
i i i i
i
E N Sim S S Sim S S
=
= ?? .                           (6) 
N is the number of total Concerned Concepts in the free text, and i denotes certain 
CCi. E is the semantic orientation of the whole text. Obviously, if 0?E , the text is to 
be regarded as positive, otherwise, negative. 
To clearly explain the function E, we just give the similarity between the context 
of CCi (Si?) in free text and the positive sentiment context of the same CCi trained 
from corpus. The function Sim is defined as follows: 
6 Y. Hu et al 
'
11
11
( , ) exp Pr( , | , )
exp Pr( , | , )
m m
pos left left
i i i
n n
right right
i
Sim S S word epos CC positive
word epos CC positive
? ? ? ?
??
? ? ? ?
??
? ?
? ?
? ?
=?=?
==
? ? ? ?
= < >? ? ? ?? ?? ?
? ? ? ?
+ < >? ? ? ?? ?? ?
??
??
.           (7) 
11
exp Pr( , | , )
m m
left left
iword epos CC positive? ? ? ?
??
? ?
? ?
=?=?
? ? ? ?
< >? ? ? ?? ?? ? ??
 is the positive orientation of the left 
context of CCi, and 
11
exp Pr( , | , )
n n
right right
iword epos CC positive? ? ? ?
??
? ?
==
? ? ? ?
< >? ? ? ?? ?? ? ??
 is the right one. 
Equation (7) means that the sentiment contribution c of each <word, epos> calculated 
by (5) in the context of CCi within a sentence in free text, which is Si?, construct the 
overall semantic orientation of the sentence together. On the other hand, '( , )negi iSim S S  
can be thought about in the same way. 
4   Parameter Estimation 
4.1  Estimating Wordval and Eposval 
In terms of CCi, its sentiment classifying knowledge is depicted by (3) and (4), and 
the parameters wordval and eposval need to be leant from corpus. Every calculation 
of Pr(<word, epos>|CCi, Pos_Neg) is divided into two parts like (8) according to 
statistic theory: 
Pr( , | , _ ) Pr( | , _ ) Pr( | , _ , )i i iword epos CC Pos Neg epos CC Pos Neg word CC Pos Neg epos? ? ? ? ?< > = ? .(8) 
eposval := Pr( | , _ )iepos CC Pos Neg?  and wordval := Pr( | , _ , )iword CC Pos Neg epos? ? .  
The ?eposval? is the probability of epos? appearing on both sides of the CCi and is 
estimated by Maximum Likelihood Estimation (MLE). Thus, 
#( , ) 1
Pr( | , _ )
#( , )
i
i
i
epos
epos CC
epos CC Pos Neg
epos CC EPOS?
? +
=
+?
.                 (9) 
The numerator in (9) is the co-occurring frequency between epos? and CCi within 
sentence in training texts with Pos_Neg (certain one of {positive, negative}) view and 
the denominator is the frequency of co-occurrence between all EPOSes appearing in 
CCi ?s context with Pos_Neg view.  
The ?wordval?is the conditional probability of ?word  given CCi and epos?  which 
can also be estimated by MLE: 
#( , , ) 1
Pr( , _ , )
#( , , ) 1
i
i
i
word word
word epos CC
word CC Pos Neg epos
word epos CC
? ?
? ?
?
+
=
+? ? .        (10) 
 A New Method for Sentiment Classification in Text Retrieval 7 
The numerator in (10) is the frequency of co-occurrence between < ?word , epos? > 
and CCi , and the denominator is the frequency of co-occurrence between all possible 
words  corresponding to epos?  appearing in CCi ?s context with Pos_Neg view. 
For smoothing, we adopt add?one method in (9) and (10). 
4.2   Estimating ?  
The ??  is the adjusting factor representing the different effect of the ,word epos? ?< >  
to CCi in texts with Pos_Neg view according to the side it appears, which means dif-
ferent side has different contribution.
 
So, it includes left??  and right?? : 
i
i
# of ,  appearing on the left side of CC
# of ,  appearing on both sides of CC
left word epos  
word epos
? ?
?
? ?
?
< >
=
< >
,          (11) 
           
i
i
# of ,  appearing on the right side of CC
# of ,  appearing on both sides of CC
right word epos
word epos
? ?
?
? ?
?
< >
=
< >
.      (12) 
4.3   Calculating ?  
?  is positional adjusting factor, which means different position to some CC will be 
assigned different weight. This is based on the linguistic hypothesis that the further a 
word get away from a researched word, the looser their relation is. That is to say, ?  
ought to satisfy an inverse proportion relationship with position.  
Unlike wordval, eposval and ? which are all private knowledge to some CC, ?  is 
a dynamic positional factor which is independent of semantic orientation of training 
texts and it is only depend on the position from CC. To the example CCi, ?  of 
,word epos? ?< > occupying the 
th?  position on its left side is left?? , which can be de-
fined as:             
| | 1 1 1(1 2) (2 (1 2) )left m??? ? ? ?= ?   1 ~ m? = ? ? .                      (13) 
?  of ,word epos? ?< > occupying  the th? position on the right side of CCi is right?? , 
which can be defined as: 
1 1 1(1 2) (2 (1 2) )right n??? ? ? ?= ?    1 ~ n? = .                          (14) 
5   Test and Conclusions 
Our research topic is about ?Racism? in Chinese texts. The training corpus is built up 
from Chinese web pages and emails. As mentioned above, all these extracted texts in 
corpus have obvious semantic orientations to racism: be favor of or oppose to. There are 
1137 texts with positive view and 1085 texts with negative view. All the Chinese texts 
are segmented and tagged with defined EPOS in advance. They are also marked posi-
8 Y. Hu et al 
tive/negative for supervised learning. The two sorts of texts with different view are 
respectively divided into 10 folds. 9 of them are trained and the left one is used for test. 
For the special domain, there is no relative result that can be consulted. So, we com-
pare the new method with a traditional classification algorithm, i.e. the popular SVM 
that uses bi-grams as features. Our experiment includes two parts: a part experiments on 
the relatively ?long? texts that contain more than 15 sentences and the other part ex-
periments on the ?short? texts that contain less than 15 sentences. We choose ?15? as 
the threshold to distinguish long or short texts because it is the mathematic expectation 
of ?length? variable of text in our testing corpus. The recall, precision and F1-score are 
listed in the following Experiment Result Table. 
Table. Experiment Result 
 
Texts with Positive View 
(more than 15 sentences) 
Texts with Negative View 
(more than 15 sentences) 
 SVM Our Method SVM Our Method 
Recall(%) 80.6 73.2 68.4 76.1 
Precision(%) 74.1 75.3 75.6 73.8 
F1-score(%) 77.2 74.2 71.82 74.9 
 
Texts with Positive View 
(less than 15 sentences) 
Texts with Negative View 
(less than 15 sentences) 
 SVM Our Method SVM Our Method 
Recall(%) 62.1 63.0 62.1 69.5 
Precision(%) 65.1 70.1 59.0 62.3 
F1-score(%) 63.6 66.4 60.5 65.7 
The experiment shows that our method is useful for sentiment classifica-
tion?especially for short texts. Seen from the table, when evaluating texts that have 
more than 15 sentences, for enough features, SVM has better result, while ours is aver-
agely close to it. However, when evaluating the texts containing less than 15 sentences, 
our method is obviously superior to SVM in either positive or negative view. That 
means our method has more potential value to sentiment classification of short texts, 
such as emails, short news, etc.  
The better result owes to the fine description within sentences and introducing lin-
guistic knowledge to sentiment classification (such as EPOS, ?  and ? ), which proved 
the two hypothesizes may be reasonable. We use modified triggered bi-grams to de-
scribe the importance among features ({<word, epos>}) and Concerned Concepts, then 
construct sentiment classifying knowledge rather than depend on statistic algorithm 
only.  
To sum up, we draw the following conclusions from our work: 
? Introducing more linguistic knowledge is helpful for improving statistic 
sentiment classification. 
 A New Method for Sentiment Classification in Text Retrieval 9 
? Sentiment classification is a hard task, and it needs subtly describing capa-
bility of language model. Maybe the intensional logic of words will be help-
ful in this field in future. 
? Chinese is a language of concept combination and the usage of words is 
more flexible than Indo-European language, which makes it more difficult 
to acquire statistic information than English [10]. 
? We assume an independent condition among sentences yet. We should in-
troduce a suitable mathematic model to group the close sentences. 
Our experiment also shows that the algorithm will become weak when no CC ap-
pears in sentences, but this method is still deserved to explore further. In future, we 
will integrate more linguistic knowledge and expand our method to a suitable sen-
tence group to improve its performance. Constructing a larger sentiment area may 
balance the capability of our method between long and short text sentiment  
classification. 
Acknowledgement. This work is supported by NSFC Major Research Program 
60496326: Basic Theory and Core Techniques of Non Canonical Knowledge and also 
supported by National 863 Project (No. 2001AA114210-11). 
References 
1. Hearst, M.A.: Direction-based text interpretation as an information access refinement. In 
P. Jacobs (Ed.), Text-Based Intelligent Systems: Current Research and Practice in Infor-
mation Extraction and Retrieval. Mahwah, NJ: Lawrence Erlbaum Associates (1992) 
2. Douglas Biber: Variation across Speech and Writing. Cambridge University Press (1988) 
3. Vasileios Hatzivassiloglou and Kathleen McKeown: Predicting the semantic orientation of 
adjectives. In Proc. of the 35th ACL/8th EACL (1997) 174-181 
4. Peter D. Turney and Michael L. Littman: Unsupervised learning of semantic orientation 
from a hundred-billion-word corpus. Technical Report EGB-1094, National Research 
Council Canada (2002) 
5. Marti Hearst: Direction-based text interpretation as an information access refinement. In 
Paul Jacobs, editor, Text-Based Intelligent Systems. Lawrence Erlbaum Associates (1992) 
6. Bo Pang and Lillian Lee: A Sentimental Education: Sentiment Analysis Using Subjectivity 
Summarization Based on Minimum Cuts. Proceedings of the 42nd ACL (2004) 271--278 
7. Sanjiv Das and Mike Chen: Yahoo! for Amazon: Extracting market sentiment from stock 
message boards. In Proc. of the 8th Asia Pacific Finance Association Annual Conference 
(2001) 
8. Vasileios Hatzivassiloglou, Janyce Wiebe: Effects of Adjective Orientation and Gradabil-
ity on Sentence Subjectivity. COLING (2000) 299-305 
9. Peter Turney: Thumbs up or thumbs down? Semantic orientation applied to unsupervised 
classication of reviews. In Proc. of the ACL (2002) 
10. Bo Pang, Lillian Lee and Shivakumar Vaithyanathan: Thumbs up? Sentiment Classifica-
tion using Machine Learning Techniques. In Proc. Conf. on EMNLP (2002) 
11. Warren Sack: On the computation of point of view. In Proc. of the Twelfth AAAI, page 
1488. Student abstract (1994) 
12. Richard M. Tong: An operational system for detecting and tracking opinions in on-line 
discussion. Workshop note, SIGIR Workshop on Operational Text Classification (2001) 
 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 176?182,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Bio-inspired Approach for Multi-Word Expression Extraction
Jianyong Duan, Ruzhan Lu
Weilin Wu, Yi Hu
Department of Computer Science
Shanghai Jiao Tong University
Shanghai, 200240, P.R. China
duanjy@hotmail.com
{lu-rz,wl-wu,huyi}@cs.sjtu.edu.cn
Yan Tian
School of Foreign Languages
Department of Computer Science
Shanghai Jiao Tong University
Shanghai, 200240, P.R. China
tianyan@sjtu.edu.cn
Abstract
This paper proposes a new approach for
Multi-word Expression (MWE)extraction
on the motivation of gene sequence align-
ment because textual sequence is simi-
lar to gene sequence in pattern analy-
sis. Theory of Longest Common Subse-
quence (LCS) originates from computer
science and has been established as affine
gap model in Bioinformatics. We per-
form this developed LCS technique com-
bined with linguistic criteria in MWE ex-
traction. In comparison with traditional
n-gram method, which is the major tech-
nique for MWE extraction, LCS approach
is applied with great efficiency and per-
formance guarantee. Experimental results
show that LCS-based approach achieves
better results than n-gram.
1 Introduction
Language is under continuous development. Peo-
ple enlarge vocabulary and let words carry more
meanings. Meanwhile the language also devel-
ops larger lexical units to carry specific meanings;
specifically MWE?s, which include compounds,
phrases, technical terms, idioms and collocations,
etc. The MWE has relatively fixed pattern because
every MWE denotes a whole concept. In compu-
tational view, the MWE repeats itself constantly in
corpus(Taneli,2003).
The extraction of MWE plays an important role
in several areas, such as machine translation (Pas-
cale,1997), information extraction (Kalliopi,2000)
etc. On the other hand, there is also a need
for MWE extraction in a much more widespread
scenario namely that of human translation and
technical writing. Many efforts have been de-
voted to the study of MWE extraction (Beat-
rice,2003; Ivan,2002; Jordi,2001). These statis-
tical methods detect MWE by frequency of can-
didate patterns. Linguistic information as a filter-
ing strategy is also performed to improve precision
by ranking their candidates (Violeta,2003; Ste-
fan,2004; Arantza,2002). Some measures based
on advance statistical methods are also used,
such as mutual expectation with single statis-
tic model (Paul,2005),C-value/NC-value method
(Katerina,2000),etc.
Frequent information is the original data for
further MWE extraction. Most approaches adopt
n-gram technique(Daniel,1977; Satanjeev,2003;
Makoto,1994). n-gram concerns about one se-
quence for each time. Every sequence can be
cut into some segments with varied lengths be-
cause any length of segment has the possibility to
become candidate MWE. The larger the context
window is, the more difficulty its parameters ac-
quire. Thus data sparseness problem deteriorates.
Another problem arises from the flexible MWE
which can be separated by an arbitrary number of
blanks, for instance, ?make. . . . . . decision?. These
models cannot effectively distinguish all kinds of
variations in flexible MWE.
On the consideration of relations between tex-
tual sequence and gene sequence, we propose a
new bio-inspired approach for MWE identifica-
tion. Both statistical and linguistic information are
incorporated into this model.
2 Multi-word Expression
Multi-word Expression( in general, term) as the
linguistic representation of concepts, also has
some special statistical features. The component
words of terms co-occur in the same context fre-
176
quently. MWE extraction can be viewed as a prob-
lem of pattern extraction. It has two major phases.
The first phase is to search the candidateMWEs by
their frequent occurrence in the corpus. The sec-
ond phase is to filter true MWEs from noise candi-
dates. Filtering process involves linguistic knowl-
edge and some intelligent observations.
MWE can be classified into strict patterns and
flexible patterns by structures of their component
words(Joaquim,1999). For example, a textual se-
quence s = w1w2 ? ? ?wi ? ? ?wn, may contain two
kinds of patterns:
Strict pattern: pi = wiwi+1wi+2
Flexible pattern: pj = wiunionsqwi+2unionsqwi+4, pk =
wi unionsq unionsqwi+3wi+4
where unionsq denotes the variational or active ele-
ment in pattern. The flexible pattern extraction is
always a bottleneck for MWE extraction for lack
of good knowledge of global solution.
3 Algorithms for MWE Extraction
3.1 Pure Mathematical Method
Although sequence alignment algorithm has been
well-developed in bioinformatics (Michael,2003),
(Knut,2000), (Hans,1999), it was rarely reported
in MWE extraction. In fact, it also applies to
MWE extraction especially for complex struc-
tures.
Algorithm.1.
1. Input:tokenlized textual sequences Q =
{s1, s2, ? ? ? , sn}
2. Initionalization : pool, ? = {?k},?
3. Computation:
I. Pairwise sequence alignment
for all si, sj ? Q, si 6= sj
Similarity(si, sj)
Align(si, sj)
path(li,lj)?? {li, lj , ck}
pool ? pool ? {(li, ck), (lj , ck)}
? ? ? ? ck
II. Creation of consistent set
for all ck ? ?, (li, ck) ? pool
?k ? ?k + {li}
pool ? pool ? (li, ck)
III. Multiple sequence alignment
for all ?k
star align(?k) ? MWU ? ?
? ?MWU
4. Output: ?
Our approach is directly inspired by gene se-
quence alignment as algorithm. 1. showed. The
textual sequence should be preprocessed before in-
put. For example, plurals recognition is a rela-
tively simple task for computers which just need
to check if the word accord with the general rule
including rule (+s) and some alternative rules (-y +
ies), etc. A set of tense forms, such as past, present
and future forms, are also transformed into origi-
nal forms. These tokenlized sequences will im-
prove extraction quality.
Pairwise sequence alignment is a crucial step.
Our algorithm uses local alignment for textual se-
quences. The similarity score between s[1 . . . i]
and t[1 . . . i] can be computed by three arrays
G[i, j], E[i, j] ,F[i, j] and zero, where entry ?(x, y)
means word x matches with word y; V[i, j] de-
notes the best score of entry ?(x, y); G[i, j] de-
notes s[i] matched with t[j]:?(s[i], t[j]); E[i, j]
denotes a blank of string s matched with t[j] :
?(unionsq, t[j]); F [i, j] denotes s[i] matched with a
blank of string t : ?(s[i],unionsq).
Initialization:
V [0, 0] = 0; V [i, 0] = E[i, 0] = 0; 1 ? i ?
m. V [0, j] = F [0, j] = 0; 1 ? j ? n.
A dynamic programming solution:
V [i, j] = max{G[i, j], E[i, j], G[i, j], 0};
G[i, j] = ?(i, j) + max
?
?
?
?
?
?
?
?
?
G[i? 1, j ? 1]
E[i? 1, j ? 1]
F [i? 1, j ? 1]
0
E[i, j] = max
?
?
?
?
?
?
?
?
?
?(h + g) + G[i, j ? 1]
? g + E[i, j ? 1]
?(h + g) + F [i, j ? 1]
0
F [i, j] = max
?
?
?
?
?
?
?
?
?
?(h + g) + G[i? 1, j]
?(h + g) + E[i? 1, j]
? g + F [i? 1, j]
0
Here we explain the meaning of these arrays:
I. G[i, j] includes the entry ?(i, j), it denotes
the sum score is the last row plus the max-
imal score between prefix s[1 . . . i ? 1] and
t[1 . . . j ? 1].
177
II. Otherwise the related prefixes s[1 . . . i] and
t[1 . . . j ? 1] are needed1. They are used to
check the first blank or additional blank in or-
der to give appropriate penalty.
a. ForG[i, j?1] and F [i, j?1], they don?t
end with a blank in string s. The blank
s[i] is the first blank. Its score isG[i, j?
1] (or F [i, j ? 1]) minus (h + g).
b. For E[i, j ? 1],The blank is the addi-
tional blank which should be only sub-
tracted g.
In the maximum entry, it records the best score
of optimum local alignment. This entry can be
viewed as the started point of alignment. Then
we backtrack entries by checking arrays which are
generated from dynamic programming algorithm.
When the score decrease to zero, alignment exten-
sion terminates. Finally, the similarity and align-
ment results are easily acquired.
Lots of aligned segments are extracted from
pairwise alignment. Those segments with com-
mon component words (ck) will be collected into
the same set. It is called as consistent set for
further multiple sequence alignment. These con-
sistent sets collect similar sequences with score
greater than certain threshold.
We perform star-alignment in multiple se-
quence alignment. The center sequence in the con-
sistent set which has the highest score in com-
parison with others, is picked out from this set.
Then all the other sequences gather to the cen-
ter sequence with the technique of ?once a blank,
always a blank?. These aligned sequences form
common regions with n-column or a column. Ev-
ery column contains one or more words. Calcula-
tion of dot-matrices is a widespread tool for com-
mon region analysis. Dot-plot agreement is de-
veloped to identify common patterns and reliably
aligned regions in a set of related sequences. If
several plots calculate consistently in a sequence
set, it displays the similarity among them. It in-
creases credibility of extracted pattern in this con-
sistent set. Finally MWE with detailed pattern
emerges from this aligned sequence set.
1Analysis approaches for F [i, j] and E[i, j] are the same,
here only E[i, j] is given its detailed explanation.
3.2 Linguistic Knowledge Combination
3.2.1 Heuristic Knowledge
Original candidate set is noise. Many meaning-
less patterns are extracted from corpus. Some lin-
guistic rules (Argamon,1999) are introduced into
our model. It is observed that candidate pattern
should contain content words. Some patterns are
only organized by pure function words, such as the
most frequent patterns ?the to?, ?of the?. These
patterns should be moved out from the candidate
set. Filter table with certain words is also per-
formed. For example, some words, like ?then?,
cannot occur in the beginning position of MWE.
These filters will reduce the number of noise pat-
terns in great extent.
3.2.2 Embedded Base Phrase detection
Short textual sequence is apt to produce frag-
ments of MWE because local alignment ends pat-
tern extension when similarity score reduces to
zero. The matched component words increase
similarity score while unmatched words decrease
it. The similarity scores of candidates in textual
sequences are lower for lack of matched compo-
nent words. Without accumulation of higher sim-
ilarity score, pattern extension terminates quickly.
Pattern extension becomes especially sensitive to
unmatched words. Some isolated fragments are
generated in this circumstance. One solution is to
give higher scores for matched component words.
It strengthens pattern extension ability at the ex-
pense of introducing noise.
We propose Embedded base phrase(EBP) de-
tection as algorithm.2. It improves pattern ex-
traction by giving lower penalty for longer base
phrase. EBP is the base phrase in a gap (Changn-
ing,2000). It does not contain other phrase recur-
sively. Good quality of MWE should avoid irrela-
tive unit in its gap. The penalty function discerns
the true EBP and irrelative unit in a gap only by
length information. Longer gap means more irrel-
ative unit. It builds a rough penalty model for lack
of semantic information. We improve this model
by POS information. POS tagged textual sequence
is convenient to grammatical analysis. True EBP2
gives comparatively lower penalty.
Algorithm.2.
1. Input: LCS of sl, sk
2The performance of our EBP tagger is 95% accuracy for
base noun phrase and 90% accuracy for general use.
178
2. Check breakpoint in LCS
i. Anchor neighbored common words and
denote gaps
for all ws = wp, wt = wq
if ws ? ls, wt ? lt, ls 6= lt
denote gst, gpq
ii. Detect EBP in gaps
gst
EBP?? g?st, gpq
EBP?? g?pq
iii. Compute new similariy matrix in gaps
similarity(g?st, g?pq)
3. Link broken segment
if path(g?st, g?pq)
lst = ls + lt, lpq = lp + lq
For textual sequence: w1w2 ? ? ?wn, and its
corresponding POS tagged sequence: t1t2 ? ? ? tn,
we suppose [wi ? ? ?wj ] is a gap from wi to wj
in sequence ? ? ? wi?1 [wi ? ? ?wj ]wj ? ? ? . The
corresponding tag sequence is [ti ? ? ? tj ] . We
only focus on EBP analysis in a gap instead of
global sequence. Context Free Grammar (CFG)
is employed in EBP detection. CFG rules follow
this form:
(1)EBP ? adj. + noun
(2)EBP ? noun + ?of? + noun
(3)EBP ? adv. + adj.
(4)EBP ? art. + adj. + noun
? ? ?
The sequences inside breakpoint of LCS are an-
alyzed by EBP detection. True base phrase will
be given lower penalty. When the gap penalty for
breakpoint is lower than threshold, the broken seg-
ment reunites. Based on experience knowledge,
when the length of a gap is less than four words,
EBP detection using CFG can gain good results.
Lower penalty for true EBP will help MWE to
emerge from noise pattern easily.
4 Experiments
4.1 Resources
A large amount of free texts are collected in order
to meet the need of MWE extraction. These texts
are downloaded from internet with various aspects
including art, entertainment, military, business,
etc. Our corpus size is 200, 000 sentences. The
average sentence length is 15 words in corpus.
In addition, result evaluation is a hard job. Its
difficulty comes from two aspects. Firstly, MWE
identification for test corpus is a kind of labor-
intensive business. The judgment of MWEs re-
quires great efforts of domain expert. It is hard and
boring to make a standard test corpus for MWE
identification use. It is a bottleneck for large scales
use. Secondly it relates to human cognition in psy-
chological world. It is proved by experience that
various opinions cannot simply be judged true or
false. As a compromise way, gold standard set
can be established by some accepted resources, for
example, WordNet, as an online lexical reference
system, including many compounds and phrases.
Some terms extracted from dictionaries are also
employed in our experiments. There are nearly
70,000 MWEs in our list.
4.2 Results and Discussion
4.2.1 Close Test
We created a closed test set of 8,000 sen-
tences. MWEs in corpus are extracted by man-
ual work. Every measure in both n-gram and LCS
approaches complies with the same threshold, for
example threshold for frequency is five times.Two
conclusions are drawn from Tab.1.
Firstly, LCS has higher recall than n-gram but
lower precision on the contrary. In close test set,
LCS recall is higher than n-gram. LCS unifies all
the cases of flexible patterns by GAM. However
n-gram only considers limited flexible patterns be-
cause of model limitation. LCS nearly includes
all the n-gram results. Higher recall decreases its
precision to a certain extent because some flexible
patterns are noisier than strict patterns. Flexible
patterns tend to be more irrelevant than strict pat-
terns. The GAM just provides a wiser choice for
all flexible patterns by its gap penalty function. N-
gram gives up analysis on many flexible patterns
without further ado. N-gram ensures its precision
by taking risk of MWE loss .
Secondly, advanced evaluation criterion can
place more MWEs in the front rank of candi-
date list. Evaluation metrics for extracted pat-
terns play an important role in MWE extraction.
Many criteria, which are reported with better per-
formances, are tested. MWE identification is sim-
ilar to IR task. These measures have their own
advantages to move interested patterns forward
in the candidate list. For example, Frequency
data contains much noise. True mutual infor-
179
Table 1: Close Test for N-gram and LCS Approaches
Measure N-Gram LCS
Precision Recall F-Measure Precision Recall F-Measure
(%) (%) (%) (%) (%) (%)
Frequency 35.2 38.0 36.0 32.1 48.2 38.4
TMI 44.7 56.2 49.1 43.2 62.1 51.4
ME 51.6 52.6 51.2 44.7 65.2 52.0
Rankratio 62.1 61.5 61.1 57.0 83.1 68.5
mation (TMI) concerns mutual information with
logarithm(Manning,1999). Mutual expectation
(ME) takes into account the relative probability of
each word compared to the phrase(Joaquim,1999).
Rankratio performs the best on both n-gram and
LCS approaches because it provides all the con-
texts which associated with each word in the cor-
pus and ranks them(Paul,2005). With the help of
advanced statistic measures, the scores of MWEs
are high enough to be detected from noisy pat-
terns.
4.2.2 Open Test
In open test, we just show the extracted MWE
numbers in different given corpus sizes. Two phe-
nomena are observed in Fig.1.








FRUSXVVL]H
0:8
QX
PEH
U
      An Enhanced Model  
for Chinese Word Segmentation and Part-of-speech Tagging 
Jiang Feng 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030  
f_jiang@sjtu.edu.cn 
 
Liu Hui 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030 
lh_Charles@sjtu.edu.cn 
 
Chen Yuquan 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030 
yqchen@mail.sjtu.edu.cn 
Lu Ruzhan 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030 
rzlu@mail.sjtu.edu.cn 
 
Abstract 
This paper will present an enhanced 
probabilistic model for Chinese word 
segmentation and part-of-speech (POS) 
tagging. The model introduces the information 
of Chinese word length as one of its features 
to reach a more accurate result. And in 
addition, the model also achieves the 
integration of segmentation and POS tagging. 
After presenting the model, this paper will 
give a brief discussion on how to solve the 
problems in statistics and how to further 
integrate Chinese Named Entity Recognition 
into the model. Finally, some figures of 
experiments and comparisons will be reported, 
which shows that the accuracy of word 
segmentation is 97.09%, and the accuracy of 
POS tagging is 98.77%. 
1 Introduction 
Generally, Chinese Lexical Analysis consists of 
two phases; one is word segmentation and the 
other is part-of-speech(POS) tagging. Rule -based 
approach and statistic -based approach are two 
dominant ways in natural language processing, as 
well as Chinese Lexical Analysis. This paper will 
only focus on the later one. Hence, our model is 
called a probabilistic model.  
Scanning through the researches in this field 
before, we have just found two points at which the 
performance of a Chinese word segmentation and 
POS tagging system could get better. One is the on 
the system architecture, and the other is from the 
Machine Learning theory. 
First, the traditional way of Chinese Lexical 
Analysis simply regards the word segmentation 
and POS tagging as two separated phases. Each 
one of them has its own algorithms and models.  
Dividing the whole process into two independent 
parts can lower the complexity of the design of 
system, but decrease the performance as well, 
because the two are fully integrated when a human 
processing a sentence. Fortunately, many 
researchers have already noticed it, and recent 
projects pay more attention on the integration of 
word segmentation and POS tagging, such as [Gao 
Shan, Zhang Yan. 2001]?s pseudo trigram 
integrated model, [Fu Guohong et al 2001]?s 
analyzer which incorporates backward Dynamic 
Programming and A* algorithm, [Sun Maosong, et 
al. 2003]?s ?Divide and Conquer integration?, 
[Zhang Huaping, et al 2003]?s hierarchical hidden 
Markov model and so on. The experiments given 
by these papers also showed a great potential of the 
integrated models. 
Besides the system architecture, another point 
should be noticed. A probabilistic model of word 
segmentation and POS tagging can be regarded as 
an instance of Machine Learning. In Machine 
Learning, the feature extraction is the most 
important aspect, and far more important than a 
learning algorithm. In the models nowadays, it 
seems that the features for Chinese Lexical 
Analysis are a little too simple. Most of them take 
tag sequences, or word frequencies as the 
distinguishing features and ignore the other useful 
information that are provided by Chinese itself. 
In this paper, we will present an enhanced, not 
too complex, model for word segmentation and 
POS tagging, which will not only inherit the merit 
of an integrated model, but also take a new feature 
(word length) into account.  
The second part of this paper will describe the 
model, including the input, output, and some 
assumptions. The third part will give some brief 
discussion about the model on some issues like 
data sparseness and Named Entity Recognition. In 
the final part, the results of our experiments will be 
reported. 
2 The Model 
The first step to establish the model is to make a 
formal description for its input and output. Here, a 
Chinese word segmentation and POS tagging 
system is viewed as with input, 
nCCC ,...,, 21  
where Ci is the i'th Chinese character of the input 
sentence, and with output pairs, ( nm ? ) 
???
?
???
?
???
?
???
?
???
?
???
?
m
m
T
L
T
L
T
L
,...,
2
2
1
1  
where Li is the word length of the i?th word in 
the segmented word sequence, Ti is the word tag, 
and each (Li, Ti) pair is corresponding to a 
segmented and tagged word, and ?
=
=
m
i
ni
1
. 
 
It is easily seen that the distinction between this 
model and other models is that this one introduces 
word length. In fact, word length really works, and 
affects the performance of the system in a great 
deal, of which our later experiments will approve.  
The motivation to introduce word length into 
our model is initially from the classical Chinese 
poems. When we read these poems, we may 
spontaneously obey some laws in where to have a 
pause. For example, in most cases, a 7-character-
lined Jueju(A kind of poem format) is read as 
**/**/***. And the pauses in a sentence are much 
related to the length of words or chunks. Even in 
modern Chinese, word length also plays a part. 
Sometimes we prefer to use disyllabic words rather 
than single one, though both are correct in 
grammar. For example, in our daily lives, we 
always say ? /n /v /n? or ? /n 
/v /n?, but seldom hear ? /n /v /n?, 
where ? ?, ? ? and ? ? have the same 
meaning. So, it is reasonable to assume that the 
occurrence of the word length will obey some 
unwritten laws when human writes or speaks. 
Introducing the word length into the word 
segmentation and POS tagging model may be in 
accord with the needs for processing Chinese. 
Another main characteristic of the model is that 
it is an integrated model, because there is only one 
hop through the input sentence to the output word-
tag sequence.  
 
The following text will introduce how the model 
works. We will also inherit n-gram assumption in 
our model. 
Our destination is to find a sequence of (Li, Ti) 
pairs that maximizes the probability,  
)|),(( CTLP  
i.e. 
)|),((maxarg),(
),(
CTLPTL
TL
R =  ? ? ? ? . 2.1 
(For conveniece, we will use A
r
 to represent a 
squence of A1, A2, A3...) 
And,  
)(
),(*)),(|(
)|),((
CP
TLPTLCP
CTLP = .......2.2 
For )(CP  is a constant given a C , we just need 
to consider )),(|( TLCP and ),( TLP . 
First consider )),(|( TLCP . Suppose W  is the 
vertex of words that ),( TL represents(i.e. the 
segmented word sequence), and the dependency 
assumption is like the following Bayers Network: 
 
Figure 2.1: Dependency assumption among  
length-tag pair, word and character 
 
So, we have, 
)),(|(*)|()),(|( TLWPWCPTLCP = ?2.3 
Because W  is the segmentation of C , 
)|( WCP  is always 1, and by another assumption 
that the occurrence of every word is independent to 
each other, then 
?
=
=
m
i
iii TLWPTLWP
1
)),(|()),(|( ???2. 4 
where )),(|( iii TLWP  means the conditional 
probability of Wi under Li and Ti. For example, 
P(? ?|2, v) is the conditional probability of 
? ? under a 2-charactered verb which may be 
computed as (the number of ? ? appearing as a 
verb) / (the number of all 2-charactered verbs). 
With 2.3 and 2.4, )),(|( TLCP  is ready. 
 
Then consider ),( TLP , which is easy to 
retrieve when we apply n-gram assumption. 
Suppose n is 2, which means that (Li, Ti) only 
depends on (Li-1, Ti-1). 
?
=
--=
m
i
iiii TLTLPTLP
1
11 )),(|),((),( ? ? ..2.5 
Here )),(|),(( 11 -- iiii TLTLP means the 
probability of a Tag Ti with Length Li appearing 
next to Tag Ti-1 with Length Li-1, which may be 
computed as (the number of (Li-1, Ti-1)(Li, Ti) 
appearing in corpus) / (the number of (Li-1,  Ti-1) 
appearing in corpus). So, ),( TLP  is also ready. 
 
Combining formula 2.1, 2.2, 2.3, 2.4 and 2.5, we 
have, 
?
= -
-
???
?
???
?
???
?
???
?
???
?
???
?=
m
i i
i
i
i
i
i
i
TL
R T
L
T
L
P
T
L
WPTL
1 1
1
),(
)|(*)|(maxarg),(
............................................2.6 
 
Now, the enhanced model is complete with 2.6. 
When establishing the model, we have made 
several assumptions.  
1. the dependency assumption between tag-length 
pairs, words and characters like the Bayers 
network of figure 2.1 
2. Word and word are independent. 
3. n-gram assumption on (T,L) pairs. 
The validation of these assumptions is still 
somewhat in doubt, but the computational 
complexity of the model is decreased. 
All the resources required to achieve this model 
are also listed, i.e., a word list with 
probability )|( ???
?
???
?
i
i
i T
L
WP , and an n-gram transition 
network with probability ),...,|(
1
1
1
1
???
?
???
?
???
?
???
?
???
?
???
?
-
-
+-
+-
i
i
ni
ni
i
i
T
L
T
L
T
L
P . 
The algorithm to implement this model is also 
rather simple, and using Dynamic Programming, 
we could finish the algorithm in O(cn), where n is 
the length of input sentence, and c is a constant 
related to the maximum ambiguity in a position. 
 
3 Discussion 
Though the model itself is not difficult to 
implement as we have presented in last section, 
there are still some problems that we will be 
probably encountered with in practice. The first 
one is the data sparseness when we do the statistics. 
Another is how to further integrate Chinese Named 
Entity Recognition into the new, word-length-
introduced model. 
 
3.1 Data Sparseness 
The Data Sparseness happens when we are 
calculating ),...,|(
1
1
1
1
???
?
???
?
???
?
???
?
???
?
???
?
-
-
+-
+-
i
i
ni
ni
i
i
T
L
T
L
T
L
P . After the 
word length is introduced, the need for larger 
corpus is greatly increased. Suppose we are using a 
tri-gram assumption on length-tag pairs, the 
number of tags is 28 as that of our system, and the 
max word length is 6, then the number of patterns 
we should count is, 
 28 * 6 * 28 * 6 * 28 * 6 = 4,741,632.  
To retrieve a reasonable statistical result, the 
scale of the corpus should at least be several times 
larger than that value. It is common that we don?t 
have such a large corpus, and meet the problem so 
called Data Sparseness.  
One way to deal with the problem is to find a 
good smoothing, and another is to make further 
independent assumption between word length and 
word tag. The word length sequence and word tag 
sequence can be considered independent. That 
means, 
),...,|(*),...,|(
),...,|(
1111
1
1
1
1
-+--+-
-
-
+-
+-
=
???
?
???
?
???
?
???
?
???
?
???
?
iniiinii
i
i
ni
ni
i
i
TTTPLLLP
T
L
T
L
T
L
P  
.........................................3.1 
Now, the patterns to count are just as many as 
those of a traditional n-gram assumption that only 
assumes the dependency among tags. 
 
3.2 Named Entity Recognition Integration 
Named Entity Recognition is one of the most 
important parts of word segmentation and POS 
tagging systems, for the words in word list are 
limited while the language seems infinite. There 
are always new words appearing in human 
language, among which human names, place 
names and organization names are most common 
and most valuble  to recognize. The performance of 
Named Entity Recognition will have a deep impact 
on the performance of a whole word segmentation 
and POS tagging system. The research on Named 
Entity Recognition has appeared for many years. 
No matter whether the current performance of 
Named Entity Recognition is ideal or not, we will 
not discuss it here, and instead, we will just show 
how to integrate the existing Name Entity 
Recognition methods into the new model.  
During the integration, more attention should be 
paid to the structural and probabilistic consistency. 
For structural consistency, the original system 
structure does not need modifying when a new 
method of Named Entity Recognition is applied. 
For probabilistic  consistency, the probabilit ies 
outputted by the Named Entity Recognition should 
be compatible with the probabilit ies of the words 
in the original word list.  
Here, we will take the Human Name 
Recognition as an example to show how to do the 
integration. 
[Zheng Jiahen, et al 2000] has presented a 
probabilistic  method for Chinese Human Name 
Recognition, which is easy to understand and 
suitable to be borrowed as a demonstration. 
That paper defined the probability for a Chinese 
Human Name as: 
)(*)()|( kEiFiknsP = ............................3.2 
)(*)(*)()|( kEjMiFijknpP = .............3.3 
Where each one of ?i?, ?j?, ?k? represents a 
single Chinese characters, ?ik?, ?ijk? are the strings 
which may be a human name, ?ns? means a single 
name when ?j? is empty, ?np? means plural name 
when ?j? is not empty, F(i) is the probability of ?i? 
being a family name, M(j) means the probability of 
?j? being the middle character of a human name, 
E(k) means the probability of ?k? being the tailing 
character of a human name, P(ns | ik ) is the 
probability of ?ik? being a single name, and P(np | 
ijk ) is the probability of ?ijk? being a plural name. 
F(i), M(j), and E(k) are easily retrieved from 
corpus, so P(ns | ik ) and P(np | ijk) can be known. 
However, P(ns | ik ) and P(np | ijk) do not satisfy 
the requirements of the word length introduced 
model. The model needs probabilit ies like 
)),(|( tlwP , where w is a word, t is a word tag, and 
l is the word length. Therefore, P(ns | ik) needs to 
be modified into P(ik | nh, 2), for ik is always a 2-
charactered word, and likewise, P(np | ijk) needs to 
be modified into P(ijk | nh, 3), where ?nh? is the 
word tag for human name in our system. 
P(ns | ik ) is equivalent to P(nh, 2 | ik ) and P(np | 
ijk ) is equivalent to P(nh, 3 | ijk). P(ns | ik) can be 
converted into P(ik | nh, 2) through following way, 
)2,(
)()()|2,(
)2,(
)()|2,(
)2,|(
nhP
kPiPiknhP
nhP
ikPiknhP
nhikP
==
 
.........................................3.4 
where ?i?, ?k? have the same meaning with those 
in 3.2 and 3.3. and nh is the tag for human name. 
In this formula, ?i? and ?k? are assumed to be 
independent. P(nh, 2), P(i), P(k) are easy to 
retrieve, which represent the probability of a 2-
charactered human name, the probability of 
character ?i? and the probability of character ?k?. 
P(nh, 2 | ik) is computed from 3.2. Thus, the 
conversion of P(nk  | nh, 2) to P(nh, 2 | ik ) is done. 
In the same way, P(np | ijk) can be converted 
into P(ijk | nh, 3) by: 
)3,(
)()()()|3,(
)3,(
)()|3,(
)3,|(
nhP
kPjPiPiknhP
nhP
ijkPijknhP
nhijkP
==
 
...........................................3.5 
Finally, the Human Name Recognition Module  
is integrated into the whole system. The input 
string C1, C2, ?, Cn first goes through the Human 
Name Recognition module, and the module 
outputs a temporary word list, which consists of a 
column of words that are probably human names 
and a column of probabilities corresponding to the 
words, which can be computed by 3.4 and 3.5. The 
whole system then merges the temporary word list 
and the original word list into a new word list, and 
applies the new word list in segmenting and 
tagging C1, C2, ?, Cn. 
 
4 Conclusion & Experiments 
This paper has presented an enhanced 
probabilistic model of Chinese Lexical Analysis, 
which introduces word length as one of the 
features and achieves the integration of word 
segmentation, Named Entity Recognition and POS 
tagging.  
At last, we will briefly give the results of our 
experiments. In the previous experiments, we have 
compared many simple probabilistic models for 
Chinese word segmentation and POS tagging, and 
found that the system using maximum word 
frequency as segmentation strategy and forward 
tri-gram Markov model as POS tagging model 
(MWF + FTMM) reaches the best performance. 
Our comparisons will be done between the 
MWF+FTMM and the enhance model with tri-
gram assumption. The training corpus is 40MB 
annotated Chinese text from People?s Daily. The 
testing data is about 1MB in size and is from 
People?s Daily, too.  
 
 MWF+FTMM New Model 
WSA 95.24% 97.09% 
PTA 97.12% 98.77% 
Total 92.50% 95.90% 
Table 4.1: The accuracy by word,  
with named entity not considered 
 
 MWF+FTMM New Model 
WSA 93.86% 95.68% 
PTA 93.89% 95.72% 
Total 88.13% 91.59% 
Table 4.2: The accuracy by word, 
with named entity considered 
 
 MWF+FTMM New Model 
WSA 69.46% 82.63% 
PTA 72.58% 80.33% 
Total 50.42% 66.38% 
Table 4.3: The accuracy by sentence, 
with named entity not considered 
 
 MWF+FTMM New Model 
WSA 63.86% 74.78% 
PTA 61.40% 67.41% 
Total 39.21% 50.42% 
Table 4.4: The accuracy by sentence, 
with named entity considered 
 
NOTES: 
MWF: Maximum Word Frequency, a very simple 
strategy in word segmentation disambiguation, 
which chooses the word sequence with max 
probability as its result.  
FTMM: Forward Tri-gram Markov Model, a 
popular model in POS tagging. 
MWF+FTMM: A strategy, which chooses the 
output that makes a balance between the MWF 
and FTMM as its result. 
WSA (by word): Word Segmentation Accuracy, 
measured by recall, i.e. the number of correct 
segments divided by the number of segments 
in corpus.  
(In a problem like word segmentation, the 
result of precision measurement is commonly 
around that of recall measurement.) 
PTA (by word): POS Tagging Accuracy based on 
correct segmentation, the number of words that 
are correctly segmented and tagged divided by 
the number of words that are correctly 
segmented. 
Total (by word): total accuracy of the system, 
measured by recall, i.e. the number of words 
that are correctly segmented and tagged 
divided by the number of words in corpus, or 
simply WSA * PTA. 
WSA (by sentence): the number of correctly 
segmented sentences divided by the number of 
sentences in corpus. A correctly segmented 
sentence is a sentence whose words are all 
correctly segmented.  
PTA (by sentence): the number of correctly tagged 
sentences divided by the number of correctly 
segmented sentences in corpus. A correctly 
tagged sentence is a sentence whose words are 
all correctly segmented and tagged. 
Total (by sentence): WSA * PTA. 
Named entity considered or not: When named 
entity is not considered, all the unknown words 
in corpus are deleted before evaluation. 
Otherwise, nothing is done on the corpus. 
 
According to the results above (Table 4.1, Table 
4.2, Table 4.3, Table 4.4), the new enhanced model 
does better than the MWF + FTMM in every field. 
Introducing the word length into a Chinese word 
segmentation and POS tagging system seems 
effective.  
This paper just focuses on the pure probabilistic 
model for word segmetation and POS tagging. It 
can be predicted that, with more disambiguation 
strategies, such as some rule based approaches, 
being implemented into the new model to achieve 
a multi-engine system, the performance will be 
further improved. 
 
5 Acknowledgements 
Thank Fang Hua and Kong Xianglong for their 
previous work, who have just graduated. 
References  
Sun Maosong, Xu Dongliang, Benjamin K Tsou. 
2003. Integrated Chinese word segmentation and 
part-of-speech tagging based on the divide-and-
conquer strategy. International Conference on 
Natural Language Processing and Knowledge 
Engineering Proceedings, Beijing. 
Zhang Huaping, Liu Qun, et al 2003. Chinese 
lexical analysis using hierarchical hidden 
Markov model. 2nd SIGHAN workshop 
affiliated with 41th ACL, Sapporo Japan 
Fu Guohong, Wang Ping, Wang Xiaolong. 2001. 
Research on the approach of integrating chinese 
wordd segmentation with part-of-speech tagging. 
Application Research of Computer. (In Chinese) 
Gao Shan, Zhang Yan. 2001. The Research on 
Integrated Chinese Word Segmentation and 
Labeling based on trigram statistical model. 
Natural Language Understanding & Machine 
Translation (JSCL-2001), Taiyuan. (In Chinese) 
Zheng Jiahen, Li Xin, et al 2000. The Research of 
Chinese names recognition method based on 
corpus. Journal of Chinese Information 
Processing. (In Chinese) 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 199?207,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Weakly Supervised Learning Approach 
for Spoken Language Understanding 
Wei-Lin Wu, Ru-Zhan Lu, Jian-Yong Duan,  
Hui Liu, Feng Gao, Yu-Quan Chen 
Department of Computer Science and Engineering 
Shanghai Jiao Tong University 
Shanghai, 200030, P. R. China 
{wu-wl,lu-rz,duan-jy,liuhui,gaofeng,chen-yq} 
@cs.sjtu.edu.cn 
 
 
Abstract 
In this paper, we present a weakly super-
vised learning approach for spoken lan-
guage understanding in domain-specific 
dialogue systems. We model the task of 
spoken language understanding as a suc-
cessive classification problem. The first 
classifier (topic classifier) is used to iden-
tify the topic of an input utterance. With 
the restriction of the recognized target 
topic, the second classifier (semantic 
classifier) is trained to extract the corre-
sponding slot-value pairs. It is mainly 
data-driven and requires only minimally 
annotated corpus for training whilst re-
taining the understanding robustness and 
deepness for spoken language. Most im-
portantly, it allows the employment of 
weakly supervised strategies for training 
the two classifiers. We first apply the 
training strategy of combining active 
learning and self-training (Tur et al, 
2005) for topic classifier. Also, we pro-
pose a practical method for bootstrapping 
the topic-dependent semantic classifiers 
from a small amount of labeled sentences. 
Experiments have been conducted in the 
context of Chinese public transportation 
information inquiry domain. The experi-
mental results demonstrate the effective-
ness of our proposed SLU framework 
and show the possibility to reduce human 
labeling efforts significantly. 
1 Introduction 
Spoken Language Understanding (SLU) is one of 
the key components in spoken dialogue systems.  
Its task is to identify the user?s goal and extract 
from the input utterance the information needed 
to complete the query. Traditionally, there are 
mainly two mainstreams in the SLU researches: 
knowledge-based approaches, which are based 
on robust parsing or template matching tech-
niques (Sneff, 1992; Dowding et al, 1993; Ward 
and Issar, 1994); and data-driven approaches, 
which are generally based on stochastic models 
(Pieraccini and Levin, 1993; Miller et al, 1995). 
Both approaches have their drawbacks, however. 
The former approach is cost-expensive to de-
velop since its grammar development is time-
consuming, laboursome and requires linguistic 
skills. It is also strictly domain-dependent and 
hence difficult to be adapted to new domains. On 
the other hand, although addressing such draw-
backs associated with knowledge-based ap-
proaches, the latter approach often suffers the 
data sparseness problem and hence needs a fully 
annotated corpus in order to reliably estimate an 
accurate model. More recently, some new varia-
tion methods are proposed through certain trade-
offs, such as the semi-automatically grammar 
learning approach (Wang and Acero, 2001) and 
Hidden Vector State (HVS) model (He and 
Young, 2005). The two methods require only 
minimally annotated data (only the semantic 
frames are annotated). 
This paper proposes a novel weakly super-
vised spoken language understanding approach. 
Our SLU framework mainly includes two suc-
cessive classifiers: topic classifier and semantic 
classifier. The main advantage of the proposed 
approach is that it is mainly data-driven and re-
quires only minimally annotated corpus for train-
ing whilst retaining the understanding robustness 
and deepness for spoken language. In particular, 
the two classifiers are trained using weakly su-
pervised strategies: the former one is trained 
through the combination of active learning and 
self-training (Tur et al, 2005), and the latter one 
199
is trained using a practical bootstrapping tech-
nique. 
2 The System Architecture 
The semantic representation of an application 
domain is usually defined in terms of the 
semantic frame, which contains a frame type 
representing the topic of the input sentence, and 
some slots representing the constraints the query 
goal has to satisfy. Then, the goal of the SLU 
system is to translate an input utterance into a 
semantic frame. Besides the two key components, 
i.e., topic classifier and semantic classifier, our 
system also contains a preprocessor and a slot-
value merger. Figure 1 illustrates the overall 
system architecture. It also describes the whole 
SLU procedure using an example sentence. 
 
Preprocessor
Please tell me how can I
go from the people's
square to the bund by bus
Topic
classification
Semantic
classification
Slot-value merger
Please tell me how can
I go from [location]1 to
[location]2 by [bus]
Please tell me how can
I go from [location]1 to
[location]2 by [bus]
FRAME:  ShowRoute
FRAME:  ShowRoute
[location]1:  ShowRoute.[route].[origin]
[location]2:  ShowRoute.[route].[destination]
[bus]: ShowRoute.[route].[transport_type]
FRAME: ShowRoute
SLOTS: [route].[origin] = the people's square
[route].[destination] = the bund
              [route].[transport_type] = bus
Inconsistent
slot-values
  
Figure 1: The System architecture1 
2.1 The Preprocessor 
Usually, the preprocessor is to look for the sub-
strings in a sentence that correspond to a seman-
tic class or matching a regular expression and to 
replace them with the class label, e.g., ?Huashan 
Road? and ?1954? are replaced with two class 
labels [road_name] and [number] respectively. In 
our system, the preprocessor can recognize more 
complex word sequences, e.g., ?1954 Huashan 
Road? can be recognized as [address] through 
matching a rule like ?[address] ? [number] 
[road_name]?. The preprocessor is implemented 
with a local chart parser, which is a variation of 
the robust parser introduced in (Wang, 1999). 
The robust local parser can skip noise words in 
the sentence, which ensures that the system has 
the low level robustness. For example, ?1954 of 
the Huashan Road)? can also be recognized as 
                                                 
1 Because the length is limited, in this paper we only illus-
trate all the example sentences in English, which are Chi-
nese sentences, in fact. 
[address] by skipping the words ?of the?. How-
ever, the robust local parser possibly skips the 
words in the sentence by mistake and produces 
an incorrect class label. To avoid this side-effect, 
this local parser exploits an embedded decision 
tree for pruning, of which the details can be seen 
in (Wu et al, 2005). According to our experience, 
it is fairly easy for a general developer with good 
understanding of the application to author the 
small grammar used by the local chart parser and 
annotate the training cases for the embedded de-
cision tree. The work can be finished in several 
hours. 
2.2 Topic Classification 
Given the representation of semantic frame, topic 
classification can be regarded as identifying the 
frame type. It is suited to be dealt using pattern 
recognition techniques. The application of statis-
tical pattern techniques to topic classification can 
improve the robustness of the whole understand-
ing system. Also, in our system, topic classifica-
tion can greatly reduce the search space and 
hence improve the performance of subsequent 
semantic classification. For example, the total 
number of slots into which the concept [location] 
can be filled in all topics is 33 and the corre-
sponding maximum number of slots in a single 
topic is decreased to 10. 
Many statistical pattern recognition techniques 
have been applied to similar tasks, such as Na?ve 
Bayes, N-Gram and Support Vector Machines 
(SVMs) (Wang et al, 2002). According to the 
literature (Wang et al, 2002) and our experi-
ments, the SVMs showed the best performance 
among many other statistical classifiers. Also, it 
has been showed that active learning can be ef-
fectively applied to the SVMs (Schohn and Cohn, 
2000; Tong and Koller, 2000). Therefore, we 
choose the SVMs as the topic classifier. We re-
sorted to the LIBSVM toolkit (Chang and Lin, 
2001) to construct the SVMs for our experiments. 
Following the practice in (Wang et al, 2002), the 
SVMs use a binary valued features vector. If the 
simplest feature (Chinese character) is used, each 
query is converted into a feature vector 
1 | |, , chch ch ch=< >JJK
JJK ?  ( | |ch
JJK  is the total number of 
Chinese characters occur in the corpus) with bi-
nary valued elements: 1 if a given Chinese char-
acter is in this input sentence or 0 otherwise. Due 
to the existence of the preprocessor, we can also 
include semantic class labels (e.g., [location]) as 
features for topic classification. Intuitively, the 
class label features are more informative than the 
200
Chinese character features. At the same time, 
including class labels as features can also relieve 
the data sparseness problem. 
2.3 Topic-dependent Semantic Classifica-
tion 
The job of semantic classification is to assign the 
concepts with the most likely slots. It can also be 
modeled as a classification problem since the 
number of possible slot names for each concept 
is limited. Let?s consider the example sentence in 
Figure 1. After the preprocessing and topic clas-
sification, we get the preprocessed result ?Please 
tell me how can I go from [location]1 to [loca-
tion]2 by [bus]?? and the topic ShowRoute. We 
have to work out which slots are to be filled with 
the values such as [location]2. The first clue is 
the surrounding literal context. Intuitively, we 
can infer that it is a [destination] since a [destina-
tion] indicator ?to? is before it. If [location]1 has 
already been recognized as a [origin], it is an-
other clue to imply that  [location]2  is a [destina 
tion]. Since initially the slot context is not avail-
able, the slot context is only employed for the 
semantic re-classification, which will be de-
scribed in latter section. 
To learn the topic-dependent semantic classi-
fiers, the training sentences need to be annotated 
against the semantic frame. Our annotating sce-
nario is relatively simple and can be performed 
by general developers. For example, for the sen-
tence ?Please tell me how can I go from the peo-
ple?s square to the bund by bus??, the annotated 
results are like the following: 
 
 
 
 
 
 
The corresponding slot names can be automati-
cally extracted from the domain model. A do-
main model is usually a hierarchical structure of 
the relevant concepts in the application domain. 
For every occurrence of a concept in the domain 
model graph, we list all the concept names along 
the path from the root to its occurrence position 
and regard their concatenation as a slot name. 
Thus, the slot name is not flat since it inherits the 
hierarchy from the domain model. 
With provision of the annotated data, we can 
collect all the literal and slot context features re-
lated to each concept. The examples of features 
for the concept [location] are illustrated as fol-
lows:  
(1) to within the ?3 windows 
(2) from _ to  
(3) ShowRoute.[route].[origin] within the 2?  
windows 
The former two are literal context features. Fea-
ture (1) is a context-word that tends to indicate 
ShowRoute.[route].[destination]. Feature (2) is a 
collocation that checks for the pattern ?from? 
and ?to? immediately before and after the con-
cept [location] respectively, and tends to indicate 
ShowRoute.[route].[origin]. The third one is a 
slot context feature, which tends to imply the 
target concept [location] is of type Show-
Route.[route].[destination]. In nature, these fea-
tures are equivalent to the rules in the semantic 
grammar used by the robust rule-based parser. 
For example, the feature (2) has the same func-
tion as the semantic rule ?[origin] ? from [loca-
tion] to?. The advantage of our approach is that 
we can automatically learn the semantic ?rules? 
from the training data rather than manually au-
thoring them. Also, the learned ?rules? are intrin-
sically robust since they may involves gaps, for 
example, feature (1) allows skipping some noise 
words between ?to? and [location]. 
The next problem is how to apply these fea-
tures when predicting a new case since the active 
features for a new case may make opposite pre-
dictions. One simple and effective strategy is 
employed by the decision list (Rivest, 1987), i.e., 
always applying the strongest features. In a deci-
sion list, all the features are sorted in order of 
descending confidence. When a new target con-
cept is classified, the classifier runs down the list 
and compares the features against the contexts of 
the target concept. The first matched feature is 
applied to make a predication. Obviously, how to 
measure the confidence of features is a very im-
portant issue for the decision list. We use the 
metric described in (Yarowsky, 1994; Golding, 
1995). Provided that 1( | ) 0P s f >  for all i : 
( ) max ( | )iiconfidence f P s f=                 (1) 
This value measures the extent to which the con-
text is unambiguously correlated with one par-
ticular slot is . 
2.4 Slot-value merging and semantic re-
classification 
The slot-value merger is to combine the slots 
assigned to the concepts in an input sentence. 
Another simultaneous task of the slot-value 
merger is to check the consistency among the 
identified slot-values. Since the topic-dependent 
classifiers corresponding to different concepts 
FRAME: ShowRoute 
Slots:   [route].[origin].[location].( the people?s square)
[route].[destination].[location].(the bund) 
[route].[transport_type].[by_bus].(bus) 
201
are training and running independently, it possi-
bly results in inconsistent predictions.  Consider-
ing the preprocessed word sequence ?Please tell 
me how can I go from [location]1 to [location]2 
by [bus]? , they are semantically clashed if [loca-
tion]1 and [location]2 are both classified as 
ShowRoute.[route].[origin]. To relieve this prob-
lem, we can use the semantic classifier based on 
the slot context feature. We apply the context 
features like, for example, ?Show-
Route.[route].[origin] within the k?  windows?, 
which tends to imply Show-
Route.[route].[destination]. The literal contexts 
reflect the local lexical semantic dependency. 
The slot contexts, however, are good at capturing 
the long distance dependency. Therefore, when 
the slot-value merger finds that two or more slot-
value pairs clash, it first anchors the one with the 
highest confidence. Then, it extracts the slot con-
texts for the other concepts and passes them to 
the semantic classification module for re-
classification. If the re- classification results still 
clash, the dialog system can involve the user in 
an interactive dialog for clarity. 
The idea of semantic classification and re-
classification can be understood as follows: it 
first finds the concept or slot islands (like partial 
parsing) and then links them together. This 
mechanism is well-suited for SLU since the spo-
ken utterance usually consists of several phrases 
and noises (restart, repeats and filled pauses, etc) 
are most often between them (Ward and Issar, 
1994). Especially, this phenomena and the out-
of-order structures are very frequent in the spo-
ken Chinese utterances. 
3 Weakly Supervised Training of the 
Topic Classifier and Topic-dependent 
Semantic Classifiers 
As stated before, to train the classifiers for topic 
identification and slot-filling, we need to label 
each sentence in the training set against the se-
mantic frame. Although this annotating scenario 
is relatively minimal, the labeling process is still 
time-consuming and costly. Meanwhile unla-
beled sentences are relatively easy to collect. 
Therefore, to reduce the cost of labeling training 
utterances, we employ weakly supervised tech-
niques for training the topic and semantic classi-
fiers. 
The weakly supervised training of the two 
classifiers is successive. Assume that a small 
amount of seed sentences are manually labeled 
against the semantic frame. We first exploit the 
labeled frame types (e.g. ShowRoute) of the 
seed sentences to train a topic classifier through 
the combination of active learning and self-
training. The resulting topic classifier is used to 
label the remaining training sentences with the 
corresponding topic, which are not selected by 
active learning. Then, we use all the sentences 
annotated against the semantic frame (including 
the seed sentences and sentences labeled by ac-
tive learning) and the remaining training 
sentences labeled the topic to train the semantic 
classifiers using a practical bootstrapping tech-
nique. 
3.1 Combining Active Learning and Self-
training for Topic Classification 
We employ the strategy of combining active 
learning and self-training for training the topic 
classifier, which was firstly proposed in (Tur et 
al., 2005) and applied to a similar task.  
One way to reduce the number of labeling ex-
amples is active learning, which have been ap-
plied in many domains (McCallum and Nigam, 
1998; Tang et al, 2002; Tur et al, 2005). Usu-
ally, the classifier is trained by randomly sam-
pling the training examples. However, in active 
learning, the classifier is trained by selectively 
sampling the training examples (Cohn et al, 
1994). The basic idea is that the most informa-
tive ones are selected from the unlabeled exam-
ples for a human to label. That is to say, this 
strategy tries to always select the examples, 
which will have the largest improvement on per-
formance, and hence minimizes the human label-
ing effort whilst keeping performance (Tur et al, 
2005). According to the strategy of determining 
the informative level of an example, the active 
learning approaches can be divided into two 
categories: uncertainty-based and committee-
based. Here, we employ the uncertainty-based 
strategy for selective sampling. It is assumed that 
a small amount of labeled examples is initially 
available, which is used to train a basic classifier. 
Then the classifier is applied to the unannotated 
examples. Typically the most unconfident exam-
ples are selected for a human to label and then 
added to the training set. The classifier is re-
trained and the procedure is repeated until the 
system performance converges. 
Another alternative for reducing human label-
ing effort is self-training. In self-training, an ini-
tial classifier is built using a small amount of 
annotated examples. The classifier is then used to 
label the unannotated training examples. The 
examples with classification confidence scores 
202
over a certain threshold, together with their pre-
dicted labels, are added to the training set to re-
train the classifier. This procedure repeated until 
the system performance converges. 
These two strategies are complementary and 
hence can be combined. The combination strat-
egy is quite straightforward for pool-based train-
ing. At each iteration, the current classifier is 
applied to the examples in the current pool. The 
most unconfident examples in the pool are se-
lected by active learning and labeled by a human. 
The remaining examples in the pool are auto-
matically labeled by the current classifier. Then, 
these two parts of labeled examples are both 
added into the training set and used for retraining 
the classifier. Since the LIBSVM toolkit pro-
vides the class probability, we directly use the 
class probability as the confidence score. Our 
dynamic pool-based (the pool size is n ) algo-
rithm of combining active learning and self-
training for training the topic classifier is as fol-
lows: 
1. Given a small amount of human-labeled 
training set 
tS  ( n  sentences) and a larger 
amount of unlabeled set uS , build the initial 
classifier using tS . 
2. While labelers/ sentences are available 
(a) Get n  unlabeled sentences from uS  
(b) Apply the current classifier to n  unla-
beled sentences 
(c) Select m  examples which are most in-
formative to the current classifier and 
manually label the selected m  exam-
ples 
(d) Add the m  human-labeled examples 
and the remaining n m?  machine-
labeled examples to the training set 
tS  
(e) Train a new classifier on all labeled ex-
amples 
3.2 Bootstrapping the Topic-dependent 
Semantic Classifiers 
Bootstrapping refers to a problem of inducing a 
classifier given a small set of labeled data and a 
large set of unlabeled data (Abney, 2002). It has 
been applied to problems such as word-sense 
disambiguation (Yarowsky, 1995), web-page 
classification (Blum and Mitchell, 1998), named-
entity recognition (Collins and Singer, 1999) and 
automatic construction of semantic lexicon 
(Thelen and Riloff, 2003). The key to the boot-
strapping methods is to exploit the redundancy in 
the unlabeled data (Collins and Singer, 1999). 
Thus, many language processing problems can 
be dealt using the bootstrapping methods since 
language is highly redundant (Yarowsky, 1995). 
The semantic classification problem here also 
exhibits the redundancy. In the example ?Please 
tell me how can I go from [location]1 to [loca-
tion]2 by [bus]??, there are multiple literal con-
text features which all indicate that [location]1 is 
of type ShowRoute.[route].[origin], such as:  
(1) from within the ?1 windows; 
(2) from _ to ; 
(3) to within the +1 windows. 
If the [location]2 has already be recognized as 
ShowRoute.[route].[destination], thus the slot 
context feature ?ShowRoute.[route].[origin] 
within the 2?  windows? is also a strong evi-
dence that [location]1 is of type Show-
Route.[route].[origin]. That is to say, the literal 
context and slot context features above effec-
tively overdetermine the slot of a concept in the 
input sentence. Especially, the literal and slot 
context features can be seen as two natural 
?views? of an example from the respective of 
?Co-Training? (Blum and Mitchell, 1998). Our 
bootstrapping algorithm exploits the property of 
redundancy to incrementally identify the features 
for assigning slots of a concept, given a few an-
notated seed sentences. 
The bootstrapping algorithm is performed on 
each topic iT  ( 1 i n? ? , n  is the number of 
topic) as follows:  
1. For each concept jC  in iT  (1 j m? ? , m is 
the number of concepts appears in the sen-
tences of topic iT ): 
(1.1) Build the two initial classifiers based on 
literal and slot context features respec-
tively using a small amount of labeled 
seed sentences. 
(1.2) Apply the current classifier based on the 
literal context feature to the remaining 
unlabeled concepts in the training sen-
tences belong to topic iT . Keep those 
classified slots with confidence score 
above a certain threshold (In this paper, 
the threshold is fixed on 0.5). 
2. Check the consistency of the classified slots 
in each sentence. If some slots in a sentence 
clashed, take the one with the highest confi-
dence score among them and leave the others 
unlabeled.  
3. For each concept jC in iT , apply the current 
classifier based on the slot context to the re-
sidual unlabeled concepts. Keep those classi-
203
fied slots with confidence score above a cer-
tain threshold. Repeat Step 3. 
4. Augment the new classified cases into the 
training set and retrain the two classifiers 
based on literal and slot context features re-
spectively.  
5. If new slots are classified from the training 
data, return to step 2. Otherwise, repeat 2-5 
to label training data and keep all new classi-
fied slots regardless of the confidence score. 
Train the two final semantic classifiers based 
on the literal and context features respec-
tively using the new labeled training data. 
4 Experiments and Results 
4.1 Data Collection and Experimental Set-
ting 
Our experiments were carried out in the context 
of Chinese public transportation information in-
quiry domain. We collected two kinds of corpus 
for our domain using different ways. Firstly, a 
natural language corpus was collected through a 
specific website which simulated a dialog system. 
The user can conduct some mixed-initiative con-
versational dialogues with it by typing Chinese 
queries. Then we collected 2,286 natural lan-
guage utterances through this way. It was divided 
into two parts: the training set contained 1,800 
sentences (TR), and the test set contained 486 
sentences (TS1). Also, a spoken language corpus 
was collected through the deployment of a pre-
liminary version of telephone-based dialog sys-
tem, of which the speech recognizer is based on 
the speaker-independent Chinese dictation sys-
tem of IBM ViaVoice Telephony and the SLU 
component is a robust rule-based parser. The 
spoken utterances corpus contained 363 spoken 
utterances. Then we obtained two test set from 
this corpus: one consisted of the recognized text 
(TS2); the other consisted of the corresponding 
transcription (TS3). The Chinese character error 
rate and concept error rate of TS2 are 35.6% and 
41.1% respectively. We defined ten types of 
topic for our domain: ListStop, ShowFare, 
ShowRoute, ShowRouteTime, etc. The first 
corpus covers all the ten topic types and the sec-
ond corpus only covers four topic types. The to-
tal number of Chinese characters appear in the 
data set is 923. All the sentences were annotated 
against the semantic frame. In our experiments, 
the topic classifier and semantic classifiers were 
trained on the natural language training set (TR) 
and tested on three test sets (TS1, TS2 and TS3). 
The performance of topic classification and 
semantic classification are measured in terms of 
topic error rate and slot error rate respectively. 
Topic performance is measured by comparing 
the topic of a sentence predicated by the topic 
classifier with the reference topic. The slot error 
rate is measured by counting the insertion, dele-
tion and substitution errors between the slots 
generated by our system and these in the refer-
ence annotation. 
4.2 Supervised Training Experiments 
Firstly, in order to validate the effectiveness of 
our proposed SLU system using successive 
learners, we compared our system with a rule-
based robust semantic parser. The parsing algo-
rithm of this parser is same as the local chart 
parser used by the preprocessor. The handcrafted 
grammar for this semantic parser took a linguis-
tic expert one month to develop, which consists 
of 798 rules (except the lexical rules for named 
entities such as [loc_name]). In our SLU system, 
we first use the SVMs to identify the topic and 
then apply the semantic classifier (decision list) 
related to the identified topic to assign the slots 
to the concepts. The SVMs used the augmented 
binary features (923 Chinese characters and 20 
semantic class labels). A general developer inde-
pendently annotated the TR set against the se-
mantic frame, which took only four days. 
Through feature extraction from the TR set and 
feature pruning, we obtained 2,259 literal context 
features and 369 slot context features for 20 
kinds of concepts in our domain. Table 1 Shows 
that our SLU method has better performance 
than the rule-based robust parser in both topic 
classification and slot identification. Due to the 
high concept error rate of recognized utterances, 
the performance of semantic classification on the 
TS2 is relatively poor. However, if considering 
only the correctly identified concepts on TS2, the 
slot error rate is 9.2%. Note that, since the TS2 
(recognized speech) covers only four types of 
topic but TS1 (typed utterance) covers ten topics, 
the topic error on the TS2 (recognized speech) is 
lower than that on TS1. 
Table 1 also compares our system with the 
two-stage classification with the reversed order. 
Another alternative for our system is to reverse 
the two main processing stages, i.e., finding the 
roles for the concepts prior to identifying the 
topic. For instance, in the example sentence in 
Fig.1, the concept (e.g., [location]) in the pre-
processed sequence is first recognized as slots 
(e.g., [route].[origin]) before topic classification. 
204
Therefore, the slots like [route].[origin] can be 
included as features for topic classification, 
which is deeper than the concepts like [location] 
and potential to achieve improvement on per-
formance of topic classification. This strategy 
was adopted in some previous works (He and 
Young, 2003; Wutiwiwatchai and Furui, 2003). 
However, the results indicate that, at least in our 
two-stage classification formwork, the strategy 
of identifying the topic before assigning the slots 
to the concepts is more optimal. According to 
our error analysis, the unsatisfied performance of 
the reversed two-stage classification system can 
be explained as follows:  (1) Since the semantic 
classification is performed on all topics, the 
search space is much bigger and the ambiguities 
increase. This deteriorates the performance of 
semantic classification. (2) In the case that the 
slots and Chinese characters are included as fea-
tures, the topic classifier relies heavily on the slot 
features. Then, the errors of semantic classifica-
tion have serious negative effect on the topic 
classification. 
 
Table 1: Performance comparsion of the rule-
based robust semantic parser, the reversed two-
stage classification system and our SLU systems 
(TER: Topic Error Rate; SER: Slot Error Rate; 
DL: Decision List) 
TS1 TS2 TS3 
 TER 
(%)   
SER 
( %) 
TER 
(%)   
SER  
( %) 
TER
(%) 
SER  
( %)
Rule-based se-
mantic parser 6.8  11.6 4.1  47.9 3.0 5.4
Reversed two-
stage classifica-
tion system 
4.9 11.1 3.6 47.4 2.5 4.9
Our system 2.9   8.4 2.2   45.6 1.4  4.6
 
4.3 Weakly Supervised Training 
Experiments 
4.3.1 Active Learning and Self-training Ex-
periments for Topic Classification 
In order to evaluate the performance of active 
learning and self-training, we compared three 
sampling strategies: random sampling, active 
learning only, active learning and self-training. 
At each iteration of pool-based active learning 
and self-training, we get 200 sentences (i.e., the 
pool size is set as 200) and select 50 most uncon-
fident sentences from them for manually labeling 
and exploit the remaining sentences using self-
training. All the experiments were repeated ten 
times with different randomly selected seed sen-
tences and the results were averaged. Figure 1 
plots the learning curves of three strategies 
trained on TR and tested on the TS1 set. It is evi-
dent that active learning significantly reduces the 
need for labeled data. For instance, it requires 
1600 examples if they are randomly chosen to 
achieve a topic error rate of 3.2% on TS1, but 
only 600 actively selected examples, a saving of 
62.5%. The strategy of combing active learning 
and self-training can further improve the per-
formance of topic classification compared with 
active learning only with the same amount of 
labeled data. 
2.00%
3.00%
4.00%
5.00%
6.00%
7.00%
8.00%
9.00%
10.00%
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of labeled sentences
To
pi
c 
er
ro
r 
ra
te
Random
Active Learning
Active Learning &
Self-traing
Figure 2: Learning curves using different sam-
pling strategies. 
 
We also evaluated the performance of topic 
classification using active learning and self-
training with the pool size of 200 on the three 
test sets. Table 2 shows that active learning and 
self-training with the pool size of 200 achieves 
almost the same performance on three test sets as 
random sampling, but requires only 33.3% data. 
 
Table 2: The topic error rate using active learn-
ing and self-training with pool size of 200 on the 
three test sets (AL: Active Learning) 
 TS1 (%) 
TS2 
(%) 
TS3 
(%) 
Labeled 
Sent.(#) 
Random 2.9 2.2 1.4 1,800 
AL 3.2 2.5 1.7 600 
AL & self-training 2.9 2.5 1.4 600 
 
4.3.2 Bootstrapping Experiments for Se-
mantic Classification 
As stated before, the bootstrapping procedure 
begins with a small amount of sentences anno-
tated against the semantic frame, which is the 
initial seed sentence or annotated by active learn-
ing, and the remaining training sentences, the 
topics of which are machine-labeled by the re-
sulting topic classifier. For example, in the 
205
weakly supervised training scenario with the 
pool size of 200, the active learning and self-
training procedure ran 8 iterations. At each itera-
tion, 50 sentences were selected by active learn-
ing. So the total number of labeled sentences is 
600. We compared our bootstrapping methods 
with supervised training for semantic classifica-
tion. We tried two bootstrapping methods: using 
only the literal context features (Bootstrapping 1) 
and using the literal and slot context features 
(Bootstrapping 2). If the step 4 of the bootstrap-
ping algorithm in Section 3.2 is canceled, the 
new bootstrapping variation corresponds to 
Bootstrapping 2. Also, we repeated the experi-
ments ten times with different labeled sentences 
and the results were averaged. Figure 3 plots the 
learning curves of bootstrapping and supervised 
training with different number of labeled sen-
tences on the TS1 set. The results indicate that 
bootstrapping methods can effectively make use 
of the unlabeled data to improve the semantic 
classification performance. In particular, the 
learning curve of bootstrapping 1 achieves more 
significant improvement than the curve of boot-
strapping 2. It can be explained as follows: in-
cluding the slot context features further increases 
the redundancy of data and hence corrects the 
initial misclassified cases by the semantic classi-
fier using only literal context features or provides 
new cases. 
6.00%
8.00%
10.00%
12.00%
14.00%
16.00%
18.00%
20.00%
0 100 200 300 400 500 600 700
Number of labeled sentences
Sl
ot
 e
rr
or
 r
at
e
Supervised
training
Bootstrapping 1
Bootstrapping 2
Figure 3: Learning curves of bootstrapping meth-
ods for semantic classification on TS1. 
 
Finally, we compared two SLU systems 
through weakly supervised and supervised 
training respectively. The supervised one was 
trained using all the annotated sentences in TR 
(1800 sentences). In the weakly supervised 
training scenario (the pool size is still 200), The 
topic classifier and semantic classifiers were both 
trained using only 600 labeled sentences. Table 3 
shows that the weakly supervised scenario 
achieves comparable performance to the super-
vised one, but requires only 33.3% labeled data. 
 
Table 3: Performance comparison of two SLU 
systems through weakly supervised and super-
vised training on the three test sets (TER: Topic 
Error Rate; SER: Slot Error Rate) 
TS1 TS2 TS3 
 TER 
(%)  
SER 
(%)
TER  
(%)   
SER 
(%) 
TER 
(%)  
SER
(%)
Supervised 2.9  8.4 2.2   45.6 1.4  4.6
Weakly  
Supervised 2.9 9.7 2.5 44.8 1.4 5.7
 
5 Conclusion and Future work 
We have presented a new SLU framework using 
two successive classifiers. The proposed frame-
work exhibits the advantages as follows. 
z It has good robustness on processing spoken 
language: (1) The preprocessor provides the 
low level robustness. (2) It inherits the ro-
bustness of topic classification using statis-
tical pattern recognition techniques. It can 
also make use of topic classification to 
guide slot filling. (3) The strategy of first 
finding the concepts or slot islands and then 
linking them is suited for processing spoken 
language. 
z It also keeps the understanding deepness: (1) 
The class of semantic classification is the 
slot name, which inherits the hierarchy from 
the domain model. (2) The semantic re-
classification mechanism ensures the consis-
tency among the identified slot-value pairs. 
z It is mainly data-driven and requires only 
minimally annotated corpus for training. 
Most importantly, our proposed SLU 
framework allows the employment of 
weakly supervised strategies for training the 
two classifiers, which can reduce the cost of 
annotating labeled sentences. 
The future work includes further evaluation of 
our approach in other application domains and 
languages. We also plan to integrate this under-
standing system into a whole dialog system. 
Then, high level knowledges, such as the dialog 
context, can also be included as the features of 
topic and semantic classifiers. Moreover, cur-
rently, the topics are manually defined through 
examination of the example sentences by human. 
Then, it is worthwhile to investigate how to ap-
propriately define topics and the probability of 
206
exploiting the sentence clustering techniques to 
facilitate the topic (frame) designment. 
6 Acknowledgements 
The authors would like to thank three anony-
mous reviewers for their careful reading and 
helpful suggestions. This work is supported by 
National Natural Science Foundation of China 
(NSFC, No. 60496326) and 863 project of China 
(No. 2001AA114210-11). 
References 
S. Abney. 2002. Bootstrapping. In Proc. of ACL, pp. 
360-367, Philadelphia, PA. 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In Proc. of 
COLT, Madison, WI. 
C. Chang and C. Lin. 2001. LIBSVM: a library for 
support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
D. Cohn, L. Atlas and R. Ladner. 1994. Improving 
generalization with active learning. Machine 
Learning 15, pp.201-221. 
M. Collins and Y. Singer.1999. Unsupervised models 
for named entity classification. In Proc. of EMNLP. 
J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. 
Cherny, R. Moore, and D. Moran. 1993. GEMINI: 
A natural language system for spoken-language 
understanding. In Proc. of  ACL, Columbus, Ohio, 
pp. 54-61.  
R. Golding. 1995. A Bayesian Hybrid Method for 
Context-sensitive Spelling Correction. In Proc. 3rd 
Workshop on Very Large Corpora, Boston, MA. 
Y. He and S.J. Young. 2003. A Data-Driven Spoken 
Language Understanding System. IEEE Workshop 
on Automatic Speech Recognition and Understand-
ing, US Virgin Islands. 
Y. He and S. Young. 2005. Semantic Processing us-
ing the Hidden Vector State Model. Computer 
Speech and Language 19(1): 85-106. 
A. McCallum and K. Nigam.1998. Employing EM 
and pool-based active learning for text classifica-
tion. In Proc. of  ICML. 
S. Miller, R. Bobrow, R. Ingria, and R. Schwartz. 
1994. Hidden Understanding Models of Natural 
Language. In Proc. of ACL, pp. 25-32. 
R. Pieraccini and E. Levin. 1993. A learning ap-
proach to natural language understanding. NATO-
ASI, New Advances & Trends in Speech Recogni-
tion and Coding, Springer-Verlag, Bubion, Spain. 
R. L. Rivest. 1987. Learning decision lists. Machine 
Learning, 2(3):229--246, 1987. 
S. Seneff. 1992. TINA: A natural language system for 
spoken language applications. Computational Lin-
guistics, vol. 18, no. 1., pp. 61-86. 
G. Schohn and D. Cohn. 2000. Less Is More: Active 
Learning with Support Vector Machines. In Proc. 
of ICML, pp. 839-846. 
M. Tang, X. Luo, S. Roukos.2002. Active learning for 
statistical natural language parsing. In Proc. of 
ACL, Philadelphia, Pennsylvania. 
M. Thelen and E. Riloff. 2002. A Bootstrapping 
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts. In Proc. of EMNLP?02. 
S. Tong and D. Koller. 2000. Support Vector Machine 
Active Learning with Applications to Text Classifi-
cation. In Proc. of ICML, pp. 999-1006. 
G. Tur, D. Hakkani-T?r, Robert E. Schapire. Combin-
ing Active and Semi-Supervised Learning for Spo-
ken Language Understanding. Speech Communi-
cation, Vol. 45, No. 2, pp. 171-186, 2005. 
Y. Wang. 1999. A Robust Parser for Spoken Lan-
guage Understanding. In Proc. of EUROSPEECH. 
Budapest, Hungary. 
Y. Wang and A Acero. 2001.Grammar learning for 
spoken language understanding. In Proc. of ASRU 
Workshop, Madonna di Campiglio, Italy. 
Y. Wang, A. Acero, C. Chelba, B. Frey and L. Wong. 
2002. Combination of Statistical and Rule-based 
Approaches for Spoken Language Understanding, 
In ICSLP. Denver, Colorado. 
W. Ward and S. Issar. 1994. Recent Improvements in 
the CMU Spoken Language Understanding System. 
In Proc. of ARPA Workshop on HLT, March, 1994. 
W Wu, J Duan, R Lu, F Gao. 2005. Embedded ma-
chine learning systems for Robust Spoken Lan-
guage Parsing. In Proc. of IEEE NLP-KE, Wuhan, 
China. 
C. Wutiwiwatchai and S. Furui. 2003. Combination of 
Finite State Automata and Neural Network for 
Spoken Language Understanding. In Proc. of EU-
ROSPEECH2003, Geneva, Switzerland. 
D. Yarowsky. 1994. Decision Lists for Lexical Ambi-
guity Resolution: Application to Accent Restora-
tion in Spanish and French. In Proc. of ACL 1994, 
pp. 88-95. 
207
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 73?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic Labeling of Compound Nominalization in Chinese
Jinglei Zhao, Hui Liu & Ruzhan Lu
Department of Computer Science
Shanghai Jiao Tong University
800 Dongchuan Road Shanghai, China
{zjl,lh charles,rzlu}@sjtu.edu.cn
Abstract
This paper discusses the semantic interpre-
tation of compound nominalizations in Chi-
nese. We propose four coarse-grained se-
mantic roles of the noun modifier and use a
Maximum Entropy Model to label such re-
lations in a compound nominalization. The
feature functions used for the model are
web-based statistics acquired via role related
paraphrase patterns, which are formed by a
set of word instances of prepositions, sup-
port verbs, feature nouns and aspect mark-
ers. By applying a sub-linear transformation
and discretization of the raw statistics, a rate
of approximately 77% is obtained for classi-
fication of the four semantic relations.
1 Introduction
A nominal compound (NC) is the concatenation of
any two or more nominal concepts which functions
as a third nominal concept (Finin, 1980). (Leonard,
1984) observed that the amount of NCs had been in-
creasing explosively in English in recent years. NCs
such as satellite navigation system are abundant in
news and technical texts. In other languages such as
Chinese, NCs have been more productive since ear-
lier days as evidenced by the fact that many simple
words in Chinese are actually a result of compound-
ing of morphemes.
Many aspects in Natural Language Processing
(NLP), such as machine translation, information re-
trieval, question answering, etc. call for the auto-
matic interpretation of NCs, that is, making explicit
the underlying semantic relationships between the
constituent concepts. For example, the semantic re-
lations involved in satellite communication system
can be expressed by the conceptual graph (Sowa,
1984) in Figure 1, in which, for instance, the se-
mantic relation between satellite and communica-
tion is MANNER. Due to the productivity of NCs
and the lack of syntactic clues to guide the interpre-
tation process, the automatic interpretation of NCs
has been proven to be a very difficult problem in
NLP.
In this paper, we deal with the semantic interpre-
tation of NCs in Chinese. Especially, we will fo-
cus on a subset of NCs in which the head word is a
verb nominalization. Nominalization is a common
phenomenon across languages in which a predica-
tive expression is transformed to refer to an event
or a property. For example, the English verb com-
municate has the related nominalized form commu-
nication. Different from English, Chinese has little
morphology. Verb nominalization in Chinese has the
same form as the verb predicate.
Nominalizations retain the argument structure of
the corresponding predicates. The semantic relation
between a noun modifier and a verb nominalization
head can be characterized by the semantic role the
modifier can take respecting to the corresponding
verb predicate. Our method uses a Maximum En-
tropy model to label coarse-grained semantic roles
in Chinese compound nominalizations. Unlike most
approaches in compound interpretation and seman-
tic role labeling, we don?t exploit features from
any parsed texts or lexical knowledge sources. In-
stead, features are acquired using web-based statis-
73
[satellite]m(MANNER)m[communication]m(TELIC)m[system] 
Figure 1: The conceptual graph for satellite communication system
tics (PMI-IR) produced from paraphrase patterns of
the compound Nominalization.
The remainder of the paper is organized as fol-
lows: Section 2 describes related works. Section
3 describes the semantic relations for our labeling
task. Section 4 introduces the paraphrase patterns
used. Section 5 gives a detailed description of our
algorithm. Section 6 presents the experimental re-
sult. Finally, in Section 7, we give the conclusions
and discuss future work.
2 Related Works
2.1 Nominal Compound Interpretation
The methods used in the semantic interpretation of
NCs fall into two main categories: rule-based ones
and statistic-based ones. The rule-based approaches
such as (Finin, 1980; Mcdonald, 1982; Leonard,
1984; Vanderwende, 1995) think that the interpreta-
tion of NCs depends heavily on the constituent con-
cepts and model the semantic interpretation as a slot-
filling process. Various rules are employed by such
approaches to determine, for example, whether the
modifier can fill in one slot of the head.
The statistic-based approaches view the seman-
tic interpretation as a multi-class classification prob-
lem. (Rosario and Hearst, 2001; Moldovan et al,
2004; Kim and Baldwin, 2005) use supervised meth-
ods and explore classification features from a simple
structured type hierarchy. (Kim and Baldwin, 2006)
use a set of seed verbs to characterize the semantic
relation between the constituent nouns and explores
a parsed corpus to classify NCs. (Turney, 2005) uses
latent relational analysis to classify NCs. The simi-
larity between two NCs is characterized by the sim-
ilarity between their related pattern set.
(Lauer, 1995) is the first to use paraphrase based
unsupervised statistical models to classify semantic
relations of NCs. (Lapata, 2000; Grover et al, 2005;
Nicholson, 2005) use paraphrase statistics computed
from parsed texts to interpret compound nominaliza-
tion, but the relations used are purely syntactic. La-
pata(2000) only classifies syntactic relations of sub-
ject and object. Grover(2005) and Nicholson (2005)
classify relations of subject, object and prepositional
object.
2.2 Semantic Role Labeling of Nominalization
Most previous work on semantic role labeling of
nominalizations are conducted in the situation where
a verb nominalization is the head of a general noun
phrase. (Dahl et al, 1987; Hull and Gomez, 1996)
use hand-coded slot-filling rules to determine the se-
mantic roles of the arguments of a nominalization.
In such approaches, first, parsers are used to identify
syntactic clues such as prepositional types. Then,
rules are applied to label semantic roles according
to clues and constraints of different roles.
Supervised machine learning methods become
prevalent in recent years in semantic role labeling
of verb nominalizations as part of the resurgence
of research in shallow semantic analysis. (Pradhan
et al, 2004) use a SVM classifier for the semantic
role labeling of nominalizations in English and Chi-
nese based on the FrameNet database and the Chi-
nese PropBank respectively. (Xue, 2006) uses the
Chinese Nombank to label nominalizations in Chi-
nese. Compared to English, the main difficulty of
using supervised method for Chinese, as noted by
Xue (2006), is that the precision of current parsers
of Chinese is very low due to the lack of morphol-
ogy, difficulty in segmentation and lack of sufficient
training materials in Chinese.
2.3 Web as a large Corpus
Data sparseness is the most notorious hinder for ap-
plying statistical methods in natural language pro-
cessing. However, the World Wide Web can be seen
as a large corpus. (Grefenstette and Nioche, 2000;
Jones and Ghani, 2000) use the web to generate cor-
pora for languages for which electronic resources
are scarce. (Zhu and Rosenfeld, 2001) use Web-
based n-gram counts for language modeling. (Keller
and Lapata, 2003) show that Web page counts and
n-gram frequency counts are highly correlated in a
log scale.
74
3 Semantic Relations
Although verb nominalization is commonly con-
sidered to have arguments as the verb predicate,
Xue(2006) finds that there tend to be fewer argu-
ments and fewer types of adjuncts in verb nomi-
nalizations compared to verb predicates in Chinese.
We argue that this phenomenon is more obvious in
compound nominalization. By analyzing a set of
compound nominalizations of length two from a bal-
anced corpus(Jin et al, 2003), we find the semantic
relations between a noun modifier and a verb nomi-
nalization head can be characterized by four coarse-
grained semantic roles: Proto-Agent (PA), Proto-
Patient (PP), Range (RA) and Manner (MA). This
is illustrated by Table1.
Relations Examples
PA ???? (Blood Circulation)
ja[? (Bird Migration)
PP ??+n (Enterprise Management)
???a (Animal Categorization)
MA -1; (Laser Storage)
?(?& (Satellite Communication)
RA ??? (Global Positioning)
?u (Long-time Development)
Table 1: Semantic Relations between Noun Modifier
and Verb Nominalization Head.
Due to the linking between semantic roles and
syntactic roles (Dowty, 1991), the relations above
overlap with syntactic roles, for example, Proto-
Agent with Subject and Proto-Patient with Object,
but they are not the same, as illustrated by the
example ???a(Animal Categorization). Al-
though the predicate ?a(categorize) in Chinese is
an intransitive verb, the semantic relation between
??(animal) and ?a(categorization) is Proto-
Patient.
4 Paraphrase Patterns
4.1 Motivations
Syntactic patterns provide clues for semantic rela-
tions (Hearst, 1992). For example, Hearst(1992)
uses the pattern ?NP such as List? to indicate that
nouns in List are hyponyms of NP. To classify the
four semantic relations listed in section 3, we pro-
pose some domain independent surface paraphrase
patterns to characterize each semantic relation. The
patterns we adopted mainly exploit a set of word in-
stances of prepositions, support verbs, feature nouns
and aspect markers.
Prepositions are strong indicators of semantic
roles in Chinese. For example, in sentence 1), the
preposition r(ba) indicates that the noun ?(door)
and ?n(Zhangsan) is the Proto-Patient and Proto-
Agent of verb?(lock) respectively.
1) a. ?nr???
b. Zhangsan ba door locked.
c. Zhangsan locked the door.
The prepositions we use to characterize each rela-
tion are listed in table 2.
Relations Prepositional Indicators
PP 