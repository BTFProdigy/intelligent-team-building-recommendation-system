Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 136?143,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Not a simple yes or no: Uncertainty in indirect answers
Marie-Catherine de Marneffe, Scott Grimm and Christopher Potts
Linguistics Department
Stanford University
Stanford, CA 94305
{mcdm,sgrimm,cgpotts}@stanford.edu
Abstract
There is a long history of using logic to
model the interpretation of indirect speech
acts. Classical logical inference, how-
ever, is unable to deal with the combina-
tions of disparate, conflicting, uncertain
evidence that shape such speech acts in
discourse. We propose to address this by
combining logical inference with proba-
bilistic methods. We focus on responses
to polar questions with the following prop-
erty: they are neither yes nor no, but
they convey information that can be used
to infer such an answer with some de-
gree of confidence, though often not with
enough confidence to count as resolving.
We present a novel corpus study and asso-
ciated typology that aims to situate these
responses in the broader class of indirect
question?answer pairs (IQAPs). We then
model the different types of IQAPs using
Markov logic networks, which combine
first-order logic with probabilities, empha-
sizing the ways in which this approach al-
lows us to model inferential uncertainty
about both the context of utterance and in-
tended meanings.
1 Introduction
Clark (1979), Perrault and Allen (1980), and Allen
and Perrault (1980) study indirect speech acts,
identifying a wide range of factors that govern how
speakers convey their intended messages and how
hearers seek to uncover those messages. Prior dis-
course conditions, the relationship between the lit-
eral meaning and the common ground, and spe-
cific lexical, constructional, and intonational cues
all play a role. Green and Carberry (1992, 1994)
provide an extensive computational model that in-
terprets and generates indirect answers to polar
questions. Their model focuses on inferring cat-
egorical answers, making use of discourse plans
and coherence relations.
This paper extends such work by recasting the
problem in terms of probabilistic modeling. We
focus on the interpretation of indirect answers
where the respondent does not answer with yes or
no, but rather gives information that can be used
by the hearer to infer such an answer only with
some degree of certainty, as in (1).
(1) A: Is Sue at work?
B: She is sick with the flu.
In this case, whether one can move from the re-
sponse to a yes or no is uncertain. Based on typical
assumptions about work and illness, A might take
B?s response as indicating that Sue is at home, but
B?s response could be taken differently depending
on Sue?s character ? B could be reproaching Sue
for her workaholic tendencies, which risk infect-
ing the office, or B could be admiring Sue?s stead-
fast character. What A actually concludes about
B?s indirect reply will be based on some combi-
nation of this disparate, partially conflicting, un-
certain evidence. The plan and logical inference
model of Green and Carberry falters in the face of
such collections of uncertain evidence. However,
natural dialogues are often interpreted in the midst
of uncertain and conflicting signals. We therefore
propose to enrich a logical inference model with
probabilistic methods to deal with such cases.
This study addresses the phenomenon of indi-
rect question?answer pairs (IQAP), such as in (1),
from both empirical and engineering perspectives.
136
First, we undertake a corpus study of polar ques-
tions in dialogue to gather naturally occurring in-
stances and to determine how pervasive indirect
answers that indicate uncertainty are in a natu-
ral setting (section 2). From this empirical base,
we provide a classification of IQAPs which makes
a new distinction between fully- and partially-
resolving answers (section 3). We then show how
inference in Markov logic networks can success-
fully model the reasoning involved in both types
of IQAPs (section 4).
2 Corpus study
Previous corpus studies looked at how pervasive
indirect answers to yes/no questions are in dia-
logue. Stenstro?m (1984) analyzed 25 face-to-face
and telephone conversations and found that 13%
of answers to polar questions do not contain an
explicit yes or no term. In a task dialogue, Hockey
et al (1997) found 38% of the responses were
IQAPs. (This higher percentage might reflect the
genre difference in the corpora used: task dialogue
vs. casual conversations.) These studies, how-
ever, were not concerned with how confidently one
could infer a yes or no from the response given.
We therefore conducted a corpus study to ana-
lyze the types of indirect answers. We used the
Switchboard Dialog Act Corpus (Jurafsky et al,
1997) which has been annotated for approximately
60 basic dialog acts, clustered into 42 tags. We
are concerned only with direct yes/no questions,
and not with indirect ones such as ?May I remind
you to take out the garbage?? (Clark, 1979; Per-
rault and Allen, 1980). From 200 5-minute con-
versations, we extracted yes/no questions (tagged
?qy?) and their answers, but discarded tag ques-
tions as well as disjunctive questions, such as in
(2), since these do not necessarily call for a yes
or no response. We also did not take into account
questions that were lost in the dialogue, nor ques-
tions that did not really require an answer (3). This
yielded a total of 623 yes/no questions.
(2) [sw 0018 4082]
A: Do you, by mistakes, do you mean just
like honest mistakes
A: or do you think they are deliberate sorts
of things?
B: Uh, I think both.
(3) [sw 0070 3435]
A: How do you feel about your game?
A: I guess that?s a good question?
B: Uh, well, I mean I?m not a serious
golfer at all.
To identify indirect answers, we looked at the
answer tags. The distribution of answers is given
in Table 1. We collapsed the tags into 6 categories.
Category I contains direct yes/no answers as well
as ?agree? answers (e.g., That?s exactly it.). Cate-
gory II includes statement?opinion and statement?
non-opinion: e.g., I think it?s great, Me I?m in the
legal department, respectively. Affirmative non-
yes answers and negative non-no answers form
category III. Other answers such as I don?t know
are in category IV. In category V, we put utterances
that avoid answering the question: by holding (I?m
drawing a blank), by returning the question ? wh-
question or rhetorical question (Who would steal
a newspaper?) ? or by using a backchannel in
question form (Is that right?). Finally, category
VI contains dispreferred answers (Schegloff et al,
1977; Pomerantz, 1984).
We hypothesized that the phenomenon we are
studying would appear in categories II, III and VI.
However, some of the ?na/ng? answers are dis-
guised yes/no answers, such as Right, I think so,
or Not really, and as such do not interest us. In the
case of ?sv/sd? and ?nd? answers, many answers
include reformulation, question avoidance (see 4),
or a change of framing (5). All these cases are not
really at issue for the question we are addressing.
(4) [sw 0177 2759]
A: Have you ever been drug tested?
B: Um, that?s a good question.
(5) [sw 0046 4316]
A: Is he the guy wants to, like, deregulate
heroin, or something?
B: Well, what he wants to do is take all the
money that, uh, he gets for drug
enforcement and use it for, uh, drug
education.
A: Uh-huh.
B: And basically, just, just attack the
problem at the demand side.
137
Definition Tag Total
I yes/no answers ny/nn/aa 341
II statements sv/sd 143
III affirmative/negative non-yes/no answers na/ng 91
IV other answers no 21
V avoid answering ?h/qw/qh/bh 18
VI dispreferred answers nd 9
Total 623
Table 1: Distribution of answer tags to yes/no questions.
(6) [sw 0046 4316]
A: That was also civil?
B: The other case was just traffic, and you
know, it was seat belt law.
We examined by hand all yes/no questions for
IQAPs and found 88 examples (such as (6), and
(7)?(11)), which constitutes thus 14% of the total
answers to direct yes/no questions, a figure simi-
lar to those of Stenstro?m (1984). The next section
introduces our classification of answers.
3 Typology of indirect answers
We can adduce the general space of IQAPs from
the data assembled in section 2 (see also Bolinger,
1978; Clark, 1979). One point of departure is that,
in cooperative dialogues, a response to a ques-
tion counts as an answer only when some relation
holds between the content of the response and the
semantic desiderata of the question. This is suc-
cinctly formulated in the relation IQAP proposed
by Asher and Lascarides (2003), p. 403:
IQAP(?,?) holds only if there is a true
direct answer p to the question J?K, and
the questioner can infer p from J?K in
the utterance context.
The apparent emphasis on truth can be set aside
for present purposes; Asher and Lascarides?s no-
tions of truth are heavily relativized to the current
discourse conditions. This principle hints at two
dimensions of IQAPs which must be considered,
and upon which we can establish a classification:
(i) the type of answer which the proffered response
provides, and (ii) the basis on which the inferences
are performed. The typology established here ad-
heres to this, distinguishing between fully- and
partially-resolving answers as well as between the
types of knowledge used in the inference (logical,
linguistic, common ground/world).
3.1 Fully-resolving responses
An indirect answer can fully resolve a question
by conveying information that stands in an inclu-
sion relation to the direct answer: if q ? p (or
?p), then updating with the response q also re-
solves the question with p (or ?p), assuming the
questioner knows that the inclusion relation holds
between q and p. The inclusion relation can be
based on logical relations, as in (7), where the re-
sponse is an ?over-answer?, i.e., a response where
more information is given than is strictly neces-
sary to resolve the question. Hearers supply more
information than strictly asked for when they rec-
ognize that the speaker?s intentions are more gen-
eral than the question posed might suggest. In (7),
the most plausible intention behind the query is
to know more about B?s family. The hearer can
also identify the speaker?s plan and any necessary
information for its completion, which he then pro-
vides (Allen and Perrault, 1980).
(7) [sw 0001 4325]
A: Do you have kids?
B: I have three.
While logical relations between the content of
the question and the response suffice to treat exam-
ples such as (7), other over-answers often require
substantial amounts of linguistic and/or world-
knowledge to allow the inference to go through,
as in (8) and (9).
(8) [sw 0069 3144]
A: Was that good?
B: Hysterical. We laughed so hard.
(9) [sw 0057 3506]
A: Is it in Dallas?
B: Uh, it?s in Lewisville.
138
In the case of (8), a system must recognize
that hysterical is semantically stronger than good.
Similarly, to recognize the implicit no of (9), a sys-
tem must recognize that Lewisville is a distinct
location from Dallas, rather than, say, contained
in Dallas, and it must include more general con-
straints as well (e.g., an entity cannot be in two
physical locations at once). Once the necessary
knowledge is in place, however, the inferences are
properly licensed.
3.2 Partially-resolving responses
A second class of IQAPs, where the content of
the answer itself does not fully resolve the ques-
tion, known as partially-resolved questions (Groe-
nendijk and Stokhof, 1984; Zeevat, 1994; Roberts,
1996; van Rooy, 2003), is less straightforward.
One instance is shown in (10), where the gradable
adjective little is the source of difficulty.
(10) [sw 0160 3467]
A: Are they [your kids] little?
B: I have a seven-year-old and a
ten-year-old.
A: Yeah, they?re pretty young.
The response, while an answer, does not, in and
of itself, resolve whether the children should be
considered little. The predicate little is a grad-
able adjective, which inherently possesses a de-
gree of vagueness: such adjectives contextually
vary in truth conditions and admit borderline cases
(Kennedy, 2007). In the case of little, while some
children are clearly little, e.g., ages 2?3, and some
clearly are not, e.g., ages 14?15, there is another
class in between for which it is difficult to as-
sess whether little can be truthfully ascribed to
them. Due to the slippery nature of these predi-
cates, there is no hard-and-fast way to resolve such
questions in all cases. In (10), it is the questioner
who resolves the question by accepting the infor-
mation proffered in the response as sufficient to
count as little.
The dialogue in (11) shows a second example of
an answer which is not fully-resolving, and inten-
tionally so.
(11) [sw 0103 4074]
A: Did he raise him [the cat] or
something1?
1The disjunct or something may indicate that A is open
B: We bought the cat for him and so he?s
been the one that you know spent the
most time with him.
Speaker B quibbles with whether the relation
his son has to the cat is one of raising, instead cit-
ing two attributes that go along with, but do not
determine, raising. Raising an animal is a com-
posite relation, which typically includes the rela-
tions owning and spending time with. However,
satisfying these two sub-relations does not strictly
entail satisfying the raising relation as well. It
is not obvious whether a system would be mis-
taken in attributing a fully positive response to the
question, although it is certainly a partially posi-
tive response. Similarly, it seems that attributing
a negative response would be misguided, though
the answer is partly negative. The rest of the dia-
logue does not determine whetherA considers this
equivalent to raising, and the dialogue proceeds
happily without this resolution.
The preceding examples have primarily hinged
upon conventionalized linguistic knowledge, viz.
what it means to raise X or for X to be little. A
further class of partially-resolving answers relies
on knowledge present in the common ground. Our
initial example (1) illustrates a situation where dif-
ferent resolutions of the question were possible de-
pending on the respondent?s intentions: no if sym-
pathetic, yes if reproachful or admiring.
The relationship between the response and
question is not secured by any objective world
facts or conventionalized meaning, but rather
is variable ? contingent on specialized world
knowledge concerning the dialogue participants
and their beliefs. Resolving such IQAPs positively
or negatively is achieved only at the cost of a de-
gree of uncertainty: for resolution occurs against
the backdrop of a set of defeasible assumptions.
3.3 IQAP classification
Table 2 is a cross-classification of the examples
discussed by whether the responses are fully- or
partially-resolving answers and by the types of
knowledge used in the inference (logical, linguis-
tic, world). It gives, for each category, the counts
of examples we found in the corpus. The partially-
resolved class contains more than a third of the an-
swers.
to hearing about alternatives to raise. We abstract away from
this issue for present purposes and treat the more general case
by assuming A?s contribution is simply equivalent to ?Did he
raise him??
139
Logic Linguistic World Total
Fully-Resolved 27 (Ex. 7) 18 (Ex. 8) 11 (Ex. 9) 56
Partially-Resolved ? 20 (Ex. 10;11) 12 (Ex. 1) 32
Table 2: Classification of IQAPs by knowledge type and resolvedness: counts and examples.
The examples given in (7)?(9) are fully resolv-
able via inferences grounded in logical relations,
linguistic convention or objective facts: the an-
swer provides enough information to fully resolve
the question, and the modeling challenge is secur-
ing and making available the correct information.
The partially-resolved pairs are, however, qualita-
tively different. They involve a degree of uncer-
tainty that classical inference models do not ac-
commodate in a natural way.
4 Towards modeling IQAP resolution
To model the reasoning involved in all types of
IQAPs, we can use a relational representation, but
we need to be able to deal with uncertainty, as
highlighted in section 3. Markov logic networks
(MLNs; Richardson and Domingos, 2006) exactly
suit these needs: they allow rich inferential reason-
ing on relations by combining the power of first-
order logic and probabilities to cope with uncer-
tainty. A logical knowledge-base is a set of hard
constraints on the set of possible worlds (set of
constants and grounded predicates). In Markov
logic, the constraints are ?soft?: when a world vi-
olates a relation, it becomes less probable, but not
impossible. A Markov logic network encodes a
set of weighted first-order logic constraints, such
that a higher weight implies a stronger constraint.
Given constants in the world, the MLN creates a
network of grounded predicates which applies the
constraints to these constants. The network con-
tains one feature fj for each possible grounding of
each constraint, with a value of 1 if the grounded
constraint is true, and 0 otherwise. The probability
of a world x is thus defined in terms of the con-
straints j satisfied by that world and the weights w
associated with each constraint (Z being the parti-
tion function):
P (X = x) =
1
Z
?
j
wjfj(x)
In practice, we use the Alchemy implemen-
tation of Markov logic networks (Kok et al,
2009). Weights on the relations can be hand-set
or learned. Currently, we use weights set by hand,
which suffices to demonstrate that an MLN han-
dles the pragmatic reasoning we want to model,
but ultimately we would like to learn the weights.
In this section, we show by means of a few
examples how MLNs give a simple and elegant
way of modeling the reasoning involved in both
partially- and fully-resolved IQAPs.
4.1 Fully-resolved IQAPs
While the use of MLNs is motivated by partially-
resolved IQAPs, to develop the intuitions behind
MLNs, we show how they model fully-resolved
cases, such as in (9). We define two distinct places,
Dallas and Lewisville, a relation linking a per-
son to a place, and the fact that person K is in
Lewisville. We also add the general constraint that
an individual can be in only one place at a time,
to which we assign a very high weight. Markov
logic allows for infinite weights, which Alchemy
denotes by a closing period. We also assume that
there is another person L, whose location is un-
known.
Constants and facts:
Place = {Dallas, Lewisville}
Person = {K,L}
BeIn(Person,Place)
BeIn(K,Lewisville)
Constraints:
// ?If you are in one place, you are not in another.?
(BeIn(x,y) ? (y != z))? !BeIn(x,z).
Figure 1 represents the grounded Markov network
obtained by applying the constraint to the con-
stants K, L, Dallas and Lewisville. The graph
contains a node for each predicate grounding, and
an arc between each pair of nodes that appear to-
gether in some grounding of the constraint. Given
that input, the MLN samples over possible worlds,
and infers probabilities for the predicate BeIn,
based on the constraints satisfied by each world
and their weights. The MLN returns a very low
probability for K being in Dallas, meaning that the
answer to the question Is it in Dallas? is no:
BeIn(K,Dallas): 4.9995e-05
140
BeIn(K, Dallas) BeIn(K, Lewisville)
BeIn(L, Lewisville)BeIn(L, Dallas)
Figure 1: Grounded Markov network obtained by applying the constraints to the constants K, L, Dallas
and Lewisville.
Since no information about L?s location has been
given, the probabilities of L being in Dallas or
Lewisville will be equal and low (0.3), which is
exactly what one would hope for. The probabili-
ties returned for each location will depend on the
number of locations specified in the input.
4.2 Partially-resolved IQAPs
To model partially-resolved IQAPs appropriately,
we need probabilities, since such IQAPs feature
reasoning patterns that involve uncertainty. We
now show how we can handle three examples of
partially-resolved IQAPs.
Gradable adjectives. Example (10) is a bor-
derline case of gradable adjectives: the question
bears on the predicate be little for two children of
ages 7 and 10. We first define the constants and
facts about the world, which take into account the
relations under consideration, ?BeLittle(X)? and
?Age(X, i)?, and specify which individuals we are
talking about, K and L, as well as their ages.
Constants and facts:
age = {0 . . . 120}
Person = {K, L}
Age(Person,age)
BeLittle(Person)
Age(K,7)
Age(L,10)
The relation between age and being little involves
some uncertainty, which we can model using a lo-
gistic curve. We assume that a 12-year-old child
lies in the vague region for determining ?little-
ness? and therefore 12 will be used as the center
of the logistic curve.
Constraints:
// ?If you are under 12, you are little.?
1.0 (Age(x,y) ? y < 12)? BeLittle(x)
// ?If you are above 12, you are not little.?
1.0 (Age(x,y) ? y > 12)? !BeLittle(x)
// The constraint below links two instances of Be-
Little.
(Age(x,u)?Age(y,v)? v>u?BeLittle(y))?Be-
Little(x).
Asking the network about K being little and L
being little, we obtain the following results, which
lead us to conclude that K and L are indeed little
with a reasonably high degree of confidence, and
that the indirect answer to the question is heavily
biased towards yes.
BeLittle(K): 0.92
BeLittle(L): 0.68
If we now change the facts, and say that K and L
are respectively 12 and 16 years old (instead of 7
and 10), we see an appropriate change in the prob-
abilities:
BeLittle(K): 0.58
BeLittle(L): 0.16
L, the 16-year-old, is certainly not to be consid-
ered ?little? anymore, whereas the situation is less
clear-cut for K, the 12-year-old (who lies in the
vague region of ?littleness? that we assumed).
Ideally, we would have information about the
speaker?s beliefs, which we could use to update
the constraints? weights. Absent such information,
we could use general knowledge from the Web to
learn appropriate weights. In this specific case, we
could find age ranges appearing with ?little kids?
in data, and fit the logistic curve to these.
This probabilistic model adapts well to cases
where categorical beliefs fit uneasily: for border-
line cases of vague predicates (whose interpreta-
tion varies by participant), there is no determinis-
tic yes or no answer.
141
Composite relations. In example (11), we want
to know whether the speaker?s son raised the cat
inasmuch as he owned and spent time with him.
We noted that raise is a composite relation, which
entails simpler relations, in this case spend time
with and own, although satisfying any one of the
simpler relations does not suffice to guarantee the
truth of raise itself. We model the constants, facts,
and constraints as follows:
Constants and Facts:
Person = {K}
Animal = {Cat}
Raise(Person,Animal)
SpendTime(Person,Animal)
Own(Person,Animal)
SpendTime(K,Cat)
Own(K,Cat)
Constraints:
// ?If you spend time with an animal, you help
raise it.?
1.0 SpendTime(x,y)? Raise(x,y)
// ?If you own an animal, you help raise it.?
1.0 Own(x,y)? Raise(x,y)
The weights on the relations reflect how central we
judge them to be in defining raise. For simplicity,
here we let the weights be identical. Clearly, the
greater number of relevant relations a pair of en-
tities fulfills, the greater the probability that the
composite relation holds of them. Considering
two scenarios helps illustrate this. First, suppose,
as in the example, that both relations hold. We will
then have a good indication that by owning and
spending time with the cat, the son helped raise
him:
Raise(K,Cat): 0.88
Second, suppose that the example is different in
that only one of the relations holds, for instance,
that the son only spent time with the cat, but did
not own it, and accordingly the facts in the net-
work do not contain Own(K,Cat). The probability
that the son raised the cat decreases:
Raise(K,Cat): 0.78
Again this can easily be adapted depending on the
centrality of the simpler relations to the composite
relation, as well as on the world-knowledge con-
cerning the (un)certainty of the constraints.
Speaker beliefs and common ground knowl-
edge. The constructed question?answer pair
given in (1), concerning whether Sue is at work,
demonstrated that how an indirect answer is mod-
eled depends on different and uncertain evidence.
The following constraints are intended to capture
some background assumptions about how we re-
gard working, being sick, and the connections be-
tween those properties:
// ?If you are sick, you are not coming to work.?
Sick(x)? !AtWork(x)
// ?If you are hardworking, you are at work.?
HardWorking(x)? AtWork(x)
// ?If you are malicious and sick, you come to
work.?
(Malicious(x) ? Sick(x))? AtWork(x)
// ?If you are at work and sick, you are malicious
or thoughtless.?
(AtWork(x) ? Sick(x)) ? (Malicious(x) ?
Thoughtless(x))
These constraints provide different answers about
Sue being at work depending on how they are
weighted, even while the facts remain the same
in each instance. If the first constraint is heavily
weighted, we get a high probability for Sue not
being at work, whereas if we evenly weight all the
constraints, Sue?s quality of being a hard-worker
dramatically raises the probability that she is at
work. Thus, MLNs permit modeling inferences
that hinge upon highly variable common ground
and speaker beliefs.
Besides offering an accurate treatment of fully-
resolved inferences, MLNs have the ability to deal
with degrees of certitude. This power is required
if one wants an adequate model of the reasoning
involved in partially-resolved inferences. Indeed,
for the successful modeling of such inferences, it
is essential to have a mechanism for adding facts
about the world that are accepted to various de-
grees, rather than categorically, as well as for up-
dating these facts with speakers? beliefs if such in-
formation is available.
5 Conclusions
We have provided an empirical analysis and ini-
tial treatment of indirect answers to polar ques-
tions. The empirical analysis led to a catego-
rization of IQAPs according to whether their an-
swers are fully- or partially-resolving and accord-
ing to the types of knowledge used in resolving
142
the question by inference (logical, linguistic, com-
mon ground/world). The partially-resolving indi-
rect answers injected a degree of uncertainty into
the resolution of the predicate at issue in the ques-
tion. Such examples highlight the limits of tradi-
tional logical inference and call for probabilistic
methods. We therefore modeled these exchanges
with Markov logic networks, which combine the
power of first-order logic and probabilities. As
a result, we were able to provide a robust model
of question?answer resolution in dialogue, one
which can assimilate information which is not cat-
egorical, but rather known only to a degree of cer-
titude.
Acknowledgements
We thank Christopher Davis, Dan Jurafsky, and
Christopher D. Manning for their insightful com-
ments on earlier drafts of this paper. We also thank
Karen Shiells for her help with the data collection
and Markov logic.
References
James F. Allen and C. Raymond Perrault. 1980. Ana-
lyzing intention in utterances. Artificial Intelligence,
15:143?178.
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press, Cam-
bridge.
Dwight Bolinger. 1978. Yes?no questions are not al-
ternative questions. In Henry Hiz, editor, Questions,
pages 87?105. D. Reidel Publishing Company, Dor-
drecht, Holland.
Herbert H. Clark. 1979. Responding to indirect speech
acts. Cognitive Psychology, 11:430?477.
Nancy Green and Sandra Carberry. 1992. Conver-
sational implicatures in indirect replies. In Pro-
ceedings of the 30th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 64?
71, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Nancy Green and Sandra Carberry. 1994. A hybrid
reasoning model for indirect answers. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 58?65, Las
Cruces, New Mexico, USA, June. Association for
Computational Linguistics.
Jeroen Groenendijk and Martin Stokhof. 1984. Studies
in the Semantics of Questions and the Pragmatics of
Answers. Ph.D. thesis, University of Amsterdam.
Beth Ann Hockey, Deborah Rossen-Knill, Beverly
Spejewski, Matthew Stone, and Stephen Isard.
1997. Can you predict answers to Y/N questions?
Yes, No and Stuff. In Proceedings of Eurospeech
1997.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual, draft
13. Technical Report 97-02, University of Colorado,
Boulder Institute of Cognitive Science.
Christopher Kennedy. 2007. Vagueness and grammar:
The semantics of relative and absolute gradable ad-
jectives. Linguistics and Philosophy, 30(1):1?45.
Stanley Kok, Marc Sumner, Matthew Richardson,
Parag Singla, Hoifung Poon, Daniel Lowd, Jue
Wang, and Pedro Domingos. 2009. The Alchemy
system for statistical relational AI. Technical report,
Department of Computer Science and Engineering,
University of Washington, Seattle, WA.
C. Raymond Perrault and James F. Allen. 1980. A
plan-based analysis of indirect speech acts. Amer-
ican Journal of Computational Linguistics, 6(3-
4):167?182.
Anita M. Pomerantz. 1984. Agreeing and dis-
agreeing with assessment: Some features of pre-
ferred/dispreferred turn shapes. In J. M. Atkinson
and J. Heritage, editors, Structure of Social Action:
Studies in Conversation Analysis. Cambridge Uni-
versity Press.
Matt Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62(1-2):107?
136.
Craige Roberts. 1996. Information structure: To-
wards an integrated formal theory of pragmatics. In
Jae Hak Yoon and Andreas Kathol, editors, OSU
Working Papers in Linguistics, volume 49: Papers
in Semantics, pages 91?136. The Ohio State Uni-
versity Department of Linguistics, Columbus, OH.
Revised 1998.
Robert van Rooy. 2003. Questioning to resolve
decision problems. Linguistics and Philosophy,
26(6):727?763.
Emanuel A. Schegloff, Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. Lan-
guage, 53:361?382.
Anna-Brita Stenstro?m. 1984. Questions and re-
sponses in English conversation. In Claes Schaar
and Jan Svartvik, editors, Lund Studies in English
68, Malmo? Sweden. CWK Gleerup.
Henk Zeevat. 1994. Questions and exhaustivity in up-
date semantics. In Harry Bunt, Reinhard Muskens,
and Gerrit Rentier, editors, Proceedings of the In-
ternational Workshop on Computational Semantics,
pages 211?221. ITK, Tilburg.
143
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng and Christopher Potts
Stanford University, Stanford, CA 94305, USA
richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu
{jeaneis,manning,cgpotts}@stanford.edu
Abstract
Semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. Further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. To remedy this, we introduce a
Sentiment Treebank. It includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-
ality. To address them, we introduce the
Recursive Neural Tensor Network. When
trained on the new treebank, this model out-
performs all previous methods on several met-
rics. It pushes the state of the art in single
sentence positive/negative classification from
80% up to 85.4%. The accuracy of predicting
fine-grained sentiment labels for all phrases
reaches 80.7%, an improvement of 9.7% over
bag of features baselines. Lastly, it is the only
model that can accurately capture the effects
of negation and its scope at various tree levels
for both positive and negative phrases.
1 Introduction
Semantic vector spaces for single words have been
widely used as features (Turney and Pantel, 2010).
Because they cannot capture the meaning of longer
phrases properly, compositionality in semantic vec-
tor spaces has recently received a lot of attention
(Mitchell and Lapata, 2010; Socher et al, 2010;
Zanzotto et al, 2010; Yessenalina and Cardie, 2011;
Socher et al, 2012; Grefenstette et al, 2013). How-
ever, progress is held back by the current lack of
large and labeled compositionality resources and
?
0
0This 0film
?
?
?
0does 0n?t
0
+care +0about ++
+
+
+cleverness 0,
0wit
0or
+
0
0any 00other +kind
+
0of ++intelligent + +humor
0.
Figure 1: Example of the Recursive Neural Tensor Net-
work accurately predicting 5 sentiment classes, very neg-
ative to very positive (? ?, ?, 0, +, + +), at every node of a
parse tree and capturing the negation and its scope in this
sentence.
models to accurately capture the underlying phe-
nomena presented in such data. To address this need,
we introduce the Stanford Sentiment Treebank and
a powerful Recursive Neural Tensor Network that
can accurately predict the compositional semantic
effects present in this new corpus.
The Stanford Sentiment Treebank is the first cor-
pus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser (Klein and Manning, 2003) and includes a
total of 215,154 unique phrases from those parse
trees, each annotated by 3 human judges. This new
dataset alows us to analyze the intricacies of senti-
ment and to capture complex linguistic phenomena.
Fig. 1 shows one of the many examples with clear
compositional structure. The granularity and size of
1631
this dataset will enable the community to train com-
positional models that are based on supervised and
structured machine learning techniques. While there
are several datasets with document and chunk labels
available, there is a need to better capture sentiment
from short comments, such as Twitter data, which
provide less overall signal per document.
In order to capture the compositional effects with
higher accuracy, we propose a new model called the
Recursive Neural Tensor Network (RNTN). Recur-
sive Neural Tensor Networks take as input phrases
of any length. They represent a phrase through word
vectors and a parse tree and then compute vectors for
higher nodes in the tree using the same tensor-based
composition function. We compare to several super-
vised, compositional models such as standard recur-
sive neural networks (RNN) (Socher et al, 2011b),
matrix-vector RNNs (Socher et al, 2012), and base-
lines such as neural networks that ignore word order,
Naive Bayes (NB), bi-gram NB and SVM. All mod-
els get a significant boost when trained with the new
dataset but the RNTN obtains the highest perfor-
mance with 80.7% accuracy when predicting fine-
grained sentiment for all nodes. Lastly, we use a test
set of positive and negative sentences and their re-
spective negations to show that, unlike bag of words
models, the RNTN accurately captures the sentiment
change and scope of negation. RNTNs also learn
that sentiment of phrases following the contrastive
conjunction ?but? dominates.
The complete training and testing code, a live
demo and the Stanford Sentiment Treebank dataset
are available at http://nlp.stanford.edu/
sentiment.
2 Related Work
This work is connected to five different areas of NLP
research, each with their own large amount of related
work to which we cannot do full justice given space
constraints.
Semantic Vector Spaces. The dominant ap-
proach in semantic vector spaces uses distributional
similarities of single words. Often, co-occurrence
statistics of a word and its context are used to de-
scribe each word (Turney and Pantel, 2010; Baroni
and Lenci, 2010), such as tf-idf. Variants of this idea
use more complex frequencies such as how often a
word appears in a certain syntactic context (Pado
and Lapata, 2007; Erk and Pado?, 2008). However,
distributional vectors often do not properly capture
the differences in antonyms since those often have
similar contexts. One possibility to remedy this is to
use neural word vectors (Bengio et al, 2003). These
vectors can be trained in an unsupervised fashion
to capture distributional similarities (Collobert and
Weston, 2008; Huang et al, 2012) but then also be
fine-tuned and trained to specific tasks such as sen-
timent detection (Socher et al, 2011b). The models
in this paper can use purely supervised word repre-
sentations learned entirely on the new corpus.
Compositionality in Vector Spaces. Most of
the compositionality algorithms and related datasets
capture two word compositions. Mitchell and La-
pata (2010) use e.g. two-word phrases and analyze
similarities computed by vector addition, multiplica-
tion and others. Some related models such as holo-
graphic reduced representations (Plate, 1995), quan-
tum logic (Widdows, 2008), discrete-continuous
models (Clark and Pulman, 2007) and the recent
compositional matrix space model (Rudolph and
Giesbrecht, 2010) have not been experimentally val-
idated on larger corpora. Yessenalina and Cardie
(2011) compute matrix representations for longer
phrases and define composition as matrix multipli-
cation, and also evaluate on sentiment. Grefen-
stette and Sadrzadeh (2011) analyze subject-verb-
object triplets and find a matrix-based categorical
model to correlate well with human judgments. We
compare to the recent line of work on supervised
compositional models. In particular we will de-
scribe and experimentally compare our new RNTN
model to recursive neural networks (RNN) (Socher
et al, 2011b) and matrix-vector RNNs (Socher et
al., 2012) both of which have been applied to bag of
words sentiment corpora.
Logical Form. A related field that tackles com-
positionality from a very different angle is that of
trying to map sentences to logical form (Zettlemoyer
and Collins, 2005). While these models are highly
interesting and work well in closed domains and
on discrete sets, they could only capture sentiment
distributions using separate mechanisms beyond the
currently used logical forms.
Deep Learning. Apart from the above mentioned
1632
work on RNNs, several compositionality ideas re-
lated to neural networks have been discussed by Bot-
tou (2011) and Hinton (1990) and first models such
as Recursive Auto-associative memories been exper-
imented with by Pollack (1990). The idea to relate
inputs through three way interactions, parameterized
by a tensor have been proposed for relation classifi-
cation (Sutskever et al, 2009; Jenatton et al, 2012),
extending Restricted Boltzmann machines (Ranzato
and Hinton, 2010) and as a special layer for speech
recognition (Yu et al, 2012).
Sentiment Analysis. Apart from the above-
mentioned work, most approaches in sentiment anal-
ysis use bag of words representations (Pang and Lee,
2008). Snyder and Barzilay (2007) analyzed larger
reviews in more detail by analyzing the sentiment
of multiple aspects of restaurants, such as food or
atmosphere. Several works have explored sentiment
compositionality through careful engineering of fea-
tures or polarity shifting rules on syntactic structures
(Polanyi and Zaenen, 2006; Moilanen and Pulman,
2007; Rentoumi et al, 2010; Nakagawa et al, 2010).
3 Stanford Sentiment Treebank
Bag of words classifiers can work well in longer
documents by relying on a few words with strong
sentiment like ?awesome? or ?exhilarating.? How-
ever, sentiment accuracies even for binary posi-
tive/negative classification for single sentences has
not exceeded 80% for several years. For the more
difficult multiclass case including a neutral class,
accuracy is often below 60% for short messages
on Twitter (Wang et al, 2012). From a linguistic
or cognitive standpoint, ignoring word order in the
treatment of a semantic task is not plausible, and, as
we will show, it cannot accurately classify hard ex-
amples of negation. Correctly predicting these hard
cases is necessary to further improve performance.
In this section we will introduce and provide some
analyses for the new Sentiment Treebank which in-
cludes labels for every syntactically plausible phrase
in thousands of sentences, allowing us to train and
evaluate compositional models.
We consider the corpus of movie review excerpts
from the rottentomatoes.com website orig-
inally collected and published by Pang and Lee
(2005). The original dataset includes 10,662 sen-
nerdy ?folks
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
phenomenal ?fantasy ?best ?sellers
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
 ?
 ?
Figure 3: The labeling interface. Random phrases were
shown and annotators had a slider for selecting the senti-
ment and its degree.
tences, half of which were considered positive and
the other half negative. Each label is extracted from
a longer movie review and reflects the writer?s over-
all intention for this review. The normalized, lower-
cased text is first used to recover, from the origi-
nal website, the text with capitalization. Remaining
HTML tags and sentences that are not in English
are deleted. The Stanford Parser (Klein and Man-
ning, 2003) is used to parses all 10,662 sentences.
In approximately 1,100 cases it splits the snippet
into multiple sentences. We then used Amazon Me-
chanical Turk to label the resulting 215,154 phrases.
Fig. 3 shows the interface annotators saw. The slider
has 25 different values and is initially set to neutral.
The phrases in each hit are randomly sampled from
the set of all phrases in order to prevent labels being
influenced by what follows. For more details on the
dataset collection, see supplementary material.
Fig. 2 shows the normalized label distributions at
each n-gram length. Starting at length 20, the ma-
jority are full sentences. One of the findings from
labeling sentences based on reader?s perception is
that many of them could be considered neutral. We
also notice that stronger sentiment often builds up
in longer phrases and the majority of the shorter
phrases are neutral. Another observation is that most
annotators moved the slider to one of the five po-
sitions: negative, somewhat negative, neutral, posi-
tive or somewhat positive. The extreme values were
rarely used and the slider was not often left in be-
tween the ticks. Hence, even a 5-class classification
into these categories captures the main variability
of the labels. We will name this fine-grained senti-
ment classification and our main experiment will be
to recover these five labels for phrases of all lengths.
1633
5 10 15 20 25 30 35 40 45
N-Gram Length
0%
20%
40%
60%
80%
100%
%
 o
f S
en
tim
en
t V
al
ue
s
Neutral
SomeZhat 3ositiYe
3ositiYe
Ver\ 3ositiYe
SomeZhat NegatiYe
NegatiYe
Ver\ NegatiYe
(a)
(a)
(b)
(b)
(c)
(c)
(d)
(d)
Distributions of sentiment values for (a) unigrams, 
(b) 10-grams, (c) 20-grams, and (d) full sentences.
Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral;
longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence
the two strongest labels and intermediate tick positions are merged into 5 classes.
4 Recursive Neural Models
The models in this section compute compositional
vector representations for phrases of variable length
and syntactic type. These representations will then
be used as features to classify each phrase. Fig. 4
displays this approach. When an n-gram is given to
the compositional models, it is parsed into a binary
tree and each leaf node, corresponding to a word,
is represented as a vector. Recursive neural mod-
els will then compute parent vectors in a bottom
up fashion using different types of compositional-
ity functions g. The parent vectors are again given
as features to a classifier. For ease of exposition,
we will use the tri-gram in this figure to explain all
models.
We first describe the operations that the below re-
cursive neural models have in common: word vector
representations and classification. This is followed
by descriptions of two previous RNN models and
our RNTN.
Each word is represented as a d-dimensional vec-
tor. We initialize all word vectors by randomly
sampling each value from a uniform distribution:
U(?r, r), where r = 0.0001. All the word vec-
tors are stacked in the word embedding matrix L ?
Rd?|V |, where |V | is the size of the vocabulary. Ini-
tially the word vectors will be random but the L ma-
trix is seen as a parameter that is trained jointly with
the compositionality models.
We can use the word vectors immediately as
parameters to optimize and as feature inputs to
a softmax classifier. For classification into five
classes, we compute the posterior probability over
    not      very       good ...
        a          b             c 
p1 =g(b,c)
p2 = g(a,p1)
0 0 +
+ +
-
Figure 4: Approach of Recursive Neural Network mod-
els for sentiment: Compute parent vectors in a bottom up
fashion using a compositionality function g and use node
vectors as features for a classifier at that node. This func-
tion varies for the different models.
labels given the word vector via:
ya = softmax(Wsa), (1)
where Ws ? R5?d is the sentiment classification
matrix. For the given tri-gram, this is repeated for
vectors b and c. The main task of and difference
between the models will be to compute the hidden
vectors pi ? Rd in a bottom up fashion.
4.1 RNN: Recursive Neural Network
The simplest member of this family of neural net-
work models is the standard recursive neural net-
work (Goller and Ku?chler, 1996; Socher et al,
2011a). First, it is determined which parent already
has all its children computed. In the above tree ex-
ample, p1 has its two children?s vectors since both
are words. RNNs use the following equations to
compute the parent vectors:
1634
p1 = f
(
W
[
b
c
])
, p2 = f
(
W
[
a
p1
])
,
where f = tanh is a standard element-wise nonlin-
earity, W ? Rd?2d is the main parameter to learn
and we omit the bias for simplicity. The bias can be
added as an extra column to W if an additional 1 is
added to the concatenation of the input vectors. The
parent vectors must be of the same dimensionality to
be recursively compatible and be used as input to the
next composition. Each parent vector pi, is given to
the same softmax classifier of Eq. 1 to compute its
label probabilities.
This model uses the same compositionality func-
tion as the recursive autoencoder (Socher et al,
2011b) and recursive auto-associate memories (Pol-
lack, 1990). The only difference to the former model
is that we fix the tree structures and ignore the re-
construction loss. In initial experiments, we found
that with the additional amount of training data, the
reconstruction loss at each node is not necessary to
obtain high performance.
4.2 MV-RNN: Matrix-Vector RNN
The MV-RNN is linguistically motivated in that
most of the parameters are associated with words
and each composition function that computes vec-
tors for longer phrases depends on the actual words
being combined. The main idea of the MV-RNN
(Socher et al, 2012) is to represent every word and
longer phrase in a parse tree as both a vector and
a matrix. When two constituents are combined the
matrix of one is multiplied with the vector of the
other and vice versa. Hence, the compositional func-
tion is parameterized by the words that participate in
it.
Each word?s matrix is initialized as a d?d identity
matrix, plus a small amount of Gaussian noise. Sim-
ilar to the random word vectors, the parameters of
these matrices will be trained to minimize the clas-
sification error at each node. For this model, each n-
gram is represented as a list of (vector,matrix) pairs,
together with the parse tree. For the tree with (vec-
tor,matrix) nodes:
(p2,P2)
(a,A) (p1,P1)
(b,B) (c,C)
the MV-RNN computes the first parent vector and its
matrix via two equations:
p1 = f
(
W
[
Cb
Bc
])
, P1 = f
(
WM
[
B
C
])
,
where WM ? Rd?2d and the result is again a d ? d
matrix. Similarly, the second parent node is com-
puted using the previously computed (vector,matrix)
pair (p1, P1) as well as (a,A). The vectors are used
for classifying each phrase using the same softmax
classifier as in Eq. 1.
4.3 RNTN:Recursive Neural Tensor Network
One problem with the MV-RNN is that the number
of parameters becomes very large and depends on
the size of the vocabulary. It would be cognitively
more plausible if there was a single powerful com-
position function with a fixed number of parameters.
The standard RNN is a good candidate for such a
function. However, in the standard RNN, the input
vectors only implicitly interact through the nonlin-
earity (squashing) function. A more direct, possibly
multiplicative, interaction would allow the model to
have greater interactions between the input vectors.
Motivated by these ideas we ask the question: Can
a single, more powerful composition function per-
form better and compose aggregate meaning from
smaller constituents more accurately than many in-
put specific ones? In order to answer this question,
we propose a new model called the Recursive Neu-
ral Tensor Network (RNTN). The main idea is to use
the same, tensor-based composition function for all
nodes.
Fig. 5 shows a single tensor layer. We define the
output of a tensor product h ? Rd via the follow-
ing vectorized notation and the equivalent but more
detailed notation for each slice V [i] ? Rd?d:
h =
[
b
c
]T
V [1:d]
[
b
c
]
;hi =
[
b
c
]T
V [i]
[
b
c
]
.
where V [1:d] ? R2d?2d?d is the tensor that defines
multiple bilinear forms.
1635
            Slices of       Standard   
                Tensor Layer          Layer
p = f             V[1:2]        +   W
Neural Tensor Layer
b
c
b
c
b
c
T
p = f                             +          
Figure 5: A single layer of the Recursive Neural Ten-
sor Network. Each dashed box represents one of d-many
slices and can capture a type of influence a child can have
on its parent.
The RNTN uses this definition for computing p1:
p1 = f
([
b
c
]T
V [1:d]
[
b
c
]
+W
[
b
c
])
,
where W is as defined in the previous models. The
next parent vector p2 in the tri-gram will be com-
puted with the same weights:
p2 = f
([
a
p1
]T
V [1:d]
[
a
p1
]
+W
[
a
p1
])
.
The main advantage over the previous RNN
model, which is a special case of the RNTN when
V is set to 0, is that the tensor can directly relate in-
put vectors. Intuitively, we can interpret each slice
of the tensor as capturing a specific type of compo-
sition.
An alternative to RNTNs would be to make the
compositional function more powerful by adding a
second neural network layer. However, initial exper-
iments showed that it is hard to optimize this model
and vector interactions are still more implicit than in
the RNTN.
4.4 Tensor Backprop through Structure
We describe in this section how to train the RNTN
model. As mentioned above, each node has a
softmax classifier trained on its vector representa-
tion to predict a given ground truth or target vector
t. We assume the target distribution vector at each
node has a 0-1 encoding. If there are C classes, then
it has length C and a 1 at the correct label. All other
entries are 0.
We want to maximize the probability of the cor-
rect prediction, or minimize the cross-entropy error
between the predicted distribution yi ? RC?1 at
node i and the target distribution ti ? RC?1 at that
node. This is equivalent (up to a constant) to mini-
mizing the KL-divergence between the two distribu-
tions. The error as a function of the RNTN parame-
ters ? = (V,W,Ws, L) for a sentence is:
E(?) =
?
i
?
j
tij log y
i
j + ????
2 (2)
The derivative for the weights of the softmax clas-
sifier are standard and simply sum up from each
node?s error. We define xi to be the vector at node
i (in the example trigram, the xi ? Rd?1?s are
(a, b, c, p1, p2)). We skip the standard derivative for
Ws. Each node backpropagates its error through to
the recursively used weights V,W . Let ?i,s ? Rd?1
be the softmax error vector at node i:
?i,s =
(
W Ts (y
i ? ti)
)
? f ?(xi),
where ? is the Hadamard product between the two
vectors and f ? is the element-wise derivative of f
which in the standard case of using f = tanh can
be computed using only f(xi).
The remaining derivatives can only be computed
in a top-down fashion from the top node through the
tree and into the leaf nodes. The full derivative for
V and W is the sum of the derivatives at each of
the nodes. We define the complete incoming error
messages for a node i as ?i,com. The top node, in
our case p2, only received errors from the top node?s
softmax. Hence, ?p2,com = ?p2,s which we can
use to obtain the standard backprop derivative for
W (Goller and Ku?chler, 1996; Socher et al, 2010).
For the derivative of each slice k = 1, . . . , d, we get:
?Ep2
?V [k]
= ?p2,comk
[
a
p1
] [
a
p1
]T
,
where ?p2,comk is just the k?th element of this vector.
Now, we can compute the error message for the two
1636
children of p2:
?p2,down =
(
W T ?p2,com + S
)
? f ?
([
a
p1
])
,
where we define
S =
d?
k=1
?p2,comk
(
V [k] +
(
V [k]
)T
)[
a
p1
]
The children of p2, will then each take half of this
vector and add their own softmax error message for
the complete ?. In particular, we have
?p1,com = ?p1,s + ?p2,down[d+ 1 : 2d],
where ?p2,down[d + 1 : 2d] indicates that p1 is the
right child of p2 and hence takes the 2nd half of the
error, for the final word vector derivative for a, it
will be ?p2,down[1 : d].
The full derivative for slice V [k] for this trigram
tree then is the sum at each node:
?E
?V [k]
=
Ep2
?V [k]
+ ?p1,comk
[
b
c
] [
b
c
]T
,
and similarly for W . For this nonconvex optimiza-
tion we use AdaGrad (Duchi et al, 2011) which con-
verges in less than 3 hours to a local optimum.
5 Experiments
We include two types of analyses. The first type in-
cludes several large quantitative evaluations on the
test set. The second type focuses on two linguistic
phenomena that are important in sentiment.
For all models, we use the dev set and cross-
validate over regularization of the weights, word
vector size as well as learning rate and minibatch
size for AdaGrad. Optimal performance for all mod-
els was achieved at word vector sizes between 25
and 35 dimensions and batch sizes between 20 and
30. Performance decreased at larger or smaller vec-
tor and batch sizes. This indicates that the RNTN
does not outperform the standard RNN due to sim-
ply having more parameters. The MV-RNN has or-
ders of magnitudes more parameters than any other
model due to the word matrices. The RNTN would
usually achieve its best performance on the dev set
after training for 3 - 5 hours. Initial experiments
Model
Fine-grained Positive/Negative
All Root All Root
NB 67.2 41.0 82.6 81.8
SVM 64.3 40.7 84.6 79.4
BiNB 71.0 41.9 82.7 83.1
VecAvg 73.3 32.7 85.1 80.1
RNN 79.0 43.2 86.1 82.4
MV-RNN 78.7 44.4 86.8 82.9
RNTN 80.7 45.7 87.6 85.4
Table 1: Accuracy for fine grained (5-class) and binary
predictions at the sentence level (root) and for all nodes.
showed that the recursive models worked signifi-
cantly worse (over 5% drop in accuracy) when no
nonlinearity was used. We use f = tanh in all ex-
periments.
We compare to commonly used methods that use
bag of words features with Naive Bayes and SVMs,
as well as Naive Bayes with bag of bigram features.
We abbreviate these with NB, SVM and biNB. We
also compare to a model that averages neural word
vectors and ignores word order (VecAvg).
The sentences in the treebank were split into a
train (8544), dev (1101) and test splits (2210) and
these splits are made available with the data release.
We also analyze performance on only positive and
negative sentences, ignoring the neutral class. This
filters about 20% of the data with the three sets hav-
ing 6920/872/1821 sentences.
5.1 Fine-grained Sentiment For All Phrases
The main novel experiment and evaluation metric
analyze the accuracy of fine-grained sentiment clas-
sification for all phrases. Fig. 2 showed that a fine
grained classification into 5 classes is a reasonable
approximation to capture most of the data variation.
Fig. 6 shows the result on this new corpus. The
RNTN gets the highest performance, followed by
the MV-RNN and RNN. The recursive models work
very well on shorter phrases, where negation and
composition are important, while bag of features
baselines perform well only with longer sentences.
The RNTN accuracy upper bounds other models at
most n-gram lengths.
Table 1 (left) shows the overall accuracy numbers
for fine grained prediction at all phrase lengths and
full sentences.
1637
    
1*UDP/HQJWK





$
F
F
X
U
D
F
\
    
1*UDP/HQJWK

Did It Happen? The Pragmatic Complexity of
Veridicality Assessment
Marie-Catherine de Marneffe?
Stanford University
Christopher D. Manning??
Stanford University
Christopher Potts?
Stanford University
Natural language understanding depends heavily on assessing veridicality?whether events
mentioned in a text are viewed as happening or not?but little consideration is given to this prop-
erty in current relation and event extraction systems. Furthermore, the work that has been done
has generally assumed that veridicality can be captured by lexical semantic properties whereas
we show that context and world knowledge play a significant role in shaping veridicality. We
extend the FactBank corpus, which contains semantically driven veridicality annotations, with
pragmatically informed ones. Our annotations are more complex than the lexical assumption
predicts but systematic enough to be included in computational work on textual understanding.
They also indicate that veridicality judgments are not always categorical, and should therefore
be modeled as distributions. We build a classifier to automatically assign event veridicality
distributions based on our new annotations. The classifier relies not only on lexical features
like hedges or negations, but also on structural features and approximations of world knowledge,
thereby providing a nuanced picture of the diverse factors that shape veridicality.
?All I know is what I read in the papers?
?Will Rogers
1. Introduction
A reader?s or listener?s understanding of an utterance depends heavily on assessing the
extent to which the speaker (author) intends to convey that the events described did
(or did not) happen. An unadorned declarative like The cancer has spread conveys firm
speaker commitment, whereas qualified variants such as There are strong indicators that
the cancer has spread or The cancer might have spread imbue the claim with uncertainty. We
? Linguistics Department, Margaret Jacks Hall Building 460, Stanford CA 94305, USA.
E-mail: mcdm@stanford.edu.
?? Linguistics Department & Computer Science Department, Gates Building 1A, 353 Serra Mall, Stanford
CA 94305, USA. E-mail: manning@stanford.edu.
? Linguistics Department, Margaret Jacks Hall Building 460, Stanford CA 94305, USA.
E-mail: cgpotts@stanford.edu.
Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication:
30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
call this event veridicality, building on logical, linguistic, and computational insights
about the relationship between language and reader commitment (Montague 1969;
Barwise 1981; Giannakidou 1994, 1995, 1999, 2001; Zwarts 1995; Asher and Lascarides
2003; Karttunen and Zaenen 2005; Rubin, Liddy, and Kando 2005; Rubin 2007; Saur??
2008). The central goal of this article is to begin to identify the linguistic and contextual
factors that shape readers? veridicality judgments.1
There is a long tradition of tracing veridicality to fixed properties of lexical items
(Kiparsky and Kiparsky 1970; Karttunen 1973). On this view, a lexical item L is veridical
if the meaning of L applied to argument p entails the truth of p. For example, because
both true and false things can be believed, one should not infer directly from A believes
that S that S is true, making believe non-veridical. Conversely, realize appears to be veridi-
cal, because realizing S entails the truth of S. The prototypical anti-veridical operator is
negation, because not S entails the falsity of S, but anti-veridicality is a characteristic
of a wide range of words and constructions (e.g., have yet to, fail, without). These basic
veridicality judgments can be further subdivided using modal or probabilistic notions.
For example, althoughmay is non-veridical by the basic classifications, wemight classify
may S as possiblewith regard to S.2
Lexical theories of this sort provide a basis for characterizing readers? veridicality
judgments, but they do not tell the whole story, because they neglect the pragmatic
enrichment that is pervasive in human communication. In the lexical view, say can only
be classified as non-veridical (both true and false things can be said), and yet, if aNew York
Times article contained the sentence United Widget said that its chairman resigned, readers
would reliably infer that United Widget?s chairman resigned?the sentence is, in this
context, veridical (at least to some degree) with respect to the event described by the
embedded clause, with United Widget said functioning to mark the source of evidence
(Simons 2007). Cognitive authority, as termed in information science (Rieh 2010), plays a
crucial role in how people judge the veridicality of events. Here, the provenance of the
document (theNew York Times) and the source (United Widget) combine to reliably lead
a reader to infer that the author intended to convey that the event really happened.
Conversely, allege is lexically non-veridical, and yet this only begins to address the
complex interplay of world knowledge and lexical meaning that will shape people?s
inferences about the sentence FBI agents alleged in court documents today that Zazi had ad-
mitted receiving weapons and explosives training from al Qaeda operatives in Pakistan last year.
We conclude from examples like this that veridicality judgments have an important
pragmatic component, and, in turn, that veridicality should be assessed using infor-
mation from the entire sentence as well as from the context. Lexical theories have a
significant role to play here, but we expect their classifications to be buffeted by other
communicative pressures. For example, the lexical theory can tell us that, as a narrowly
semantic fact, X alleges S is non-veridical with regard to S. Where X is a trustworthy
source for S-type information, however, we might fairly confidently conclude that S is
true. Where X is known to spread disinformation, we might tentatively conclude that
S is false. These pragmatic enrichments move us from uncertainty to some degree of
1 Our use of the term ?veridicality? most closely matches that of Giannakidou (1999), where it is defined
so as to be (i) relativized to particular agents or perspectives, (ii) gradable, and (iii) general enough to
cover not only facts but also the commitments that arise from using certain referential expressions and
aspectual morphology. The more familiar term ?factuality? seems at odds with all three of these criteria,
so we avoid it.
2 Lexical notions of veridicality must be relativized to specific argument positions, with the other
arguments existentially closed for the purposes of checking entailment. For example, believe is
non-veridical on its inner (sentential) argument because ??x : x believes p? does not entail p.
302
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
certainty. Such enrichments can be central to understanding how a listener (or a reader)
understands a speaker?s (or author?s) message.
Embracing pragmatic enrichment means embracing uncertainty. Although listeners
can feel reasonably confident about the core lexical semantics of the words of their lan-
guage, there is no such firm foundation when it comes to this kind of pragmatic enrich-
ment. The newspaper says, United Widget said that its profits were up in the fourth quarter,
but just how trustworthy is United Widget on such matters? Speakers are likely to vary
in what they intend in such cases, and listeners are thus forced to operate under uncer-
tainty when making the requisite inferences. Lexical theories allow us to abstract away
from these challenges, but a pragmatically informed approach must embrace them.
The FactBank corpus is a leading resource for research on veridicality (Saur?? and
Pustejovsky 2009). Its annotations are ?textual-based?: They seek to capture the ways
in which lexical meanings and local semantic interactions determine veridicality judg-
ments. In order to better understand the role of pragmatic enrichment, we had a large
group of linguistically naive annotators annotate a portion of the FactBank corpus,
given very loose guidelines. Whereas the FactBank annotators were explicitly told to
avoid bringing world knowledge to bear on the task, our annotators were encouraged
to choose labels that reflected their own natural reading of the texts. Each sentence was
annotated by 10 annotators, which increases our confidence in them and also highlights
the sort of vagueness and ambiguity that can affect veridicality. These new annotations
help confirm our hypothesis that veridicality judgments are shaped by a variety of other
linguistic and contextual factors beyond lexical meanings.
The nature of such cues is central to linguistic pragmatics and fundamental to
a range of natural language processing (NLP) tasks, including information extrac-
tion, opinion detection, and textual entailment. Veridicality is prominent in BioNLP,
where identifying negations (Chapman et al 2001; Elkin et al 2005; Huang and Lowe
2007; Pyysalo et al 2007; Morante and Daelemans 2009) and hedges or ?speculations?
(Szarvas et al 2008; Kim et al 2009) is crucial to proper textual understanding. Recently,
more attention has been devoted to veridicality within NLP, with the 2010 workshop
on negation and speculation in natural language processing (Morante and Sporleder
2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al
2010), where the goal was to distinguish uncertain events from the rest. The centrality
of veridicality assessment to tasks like event and relation extraction is arguably still
not fully appreciated, however. At present the vast majority of information extraction
systems work at roughly the clause level and regard any relation they find as true. But
relations in actual text may not be facts for all sorts of reasons, such as being embedded
under an attitude verb like doubt, being the antecedent of a conditional, or being part of
the report by an untrustworthy source. To avoid wrong extractions in these cases, it is
essential for NLP systems to assess the veridicality of extracted facts.
In the present article, we argue for three main claims about veridicality. First and
foremost, we aim to show that pragmatically informed veridicality judgments are sys-
tematic enough to be included in computational work on textual understanding. Sec-
ond, we seek to justify FactBank?s seven-point categorization over simpler alternatives
(e.g., certain vs. uncertain, as in the CoNLL task). Finally, the inherent uncertainty of
pragmatic inference suggests to us that veridicality judgments are not always categori-
cal, and thus are better modeled as probability distributions over veridicality categories.
To substantiate these claims, we analyze in detail the annotations we collected, and
we report on experiments that treat veridicality as a distribution-prediction task. Our
feature set includes not only lexical items like hedges, modals, and negations, but
also complex structural features and approximations of world knowledge. Though the
303
Computational Linguistics Volume 38, Number 2
resulting classifier has limited ability to assess veridicality in complex real world
contexts, it still does a quite good job of capturing human pragmatic judgments of
veridicality. We argue that the model yields insights into the complex pragmatic factors
that shape readers? veridicality judgments.
2. Corpus Annotation
FactBank?s annotations are intended to isolate semantic effects from pragmatic ones in
the area of veridicality assessment. Our overarching goal is to examine how pragmatic
enrichment affects this picture. Thus, we use the FactBank sentences in our own inves-
tigation, to facilitate comparisons between the two kinds of information and to create a
supplement of FactBank itself. This section introduces FactBank in more detail and then
thoroughly reviews our own annotation project and its results.
2.1 FactBank Corpus
FactBank provides veridicality annotations on events relative to each participant in-
volved in the discourse. It consists of 208 documents from newswire and broadcast
news reports in which 9,472 event descriptions (verbs, nouns, and adjectives) were
manually identified. There is no fundamental difference in the way verbs, nouns, and
adjectives are annotated. Events are single words. The data come from TimeBank 1.2
and a fragment of AQUAINT TimeML (Pustejovsky et al 2006). The documents in the
AQUAINT TimeML subset come from two topics: ?NATO, Poland, Czech Republic,
Hungary? and ?the Slepian abortion murder.?
The tags annotate ?event, participant? pairs in sentences. The participant can be
anyone mentioned in the sentence as well as its author. In Example (1), the target event
identified by the word means is assigned a value with respect to both the source some
experts and the author of the sentence.
Example 1
Some experts now predict Anheuser?s entry into the fray means near-term earnings
trouble for all the industry players.
Veridicality(means, experts) = PR+
Veridicality(means, author) = Uu
The tags are summarized in Table 1. Each tag consists of a veridicality value (certain
[CT], probable [PR], possible [PS], underspecified [U]) and a polarity value (positive
[+], negative [?], unknown [u]). CT+ corresponds to the standard notion of veridicality,
CT? to anti-veridicality, and Uu to non-veridicality. The PR and PS categories add a
modal or probabilistic element to the scale, to capture finer-grained intuitions.
Examples (2) and (3) illustrate the annotations for a noun and an adjective.
Example 2
But an all-out bidding war between the world?s top auto giants for Britain?s leading
luxury-car maker seems unlikely.
Veridicality(war, author) = PR?
304
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Table 1
FactBank annotation scheme. CT = certain; PR = probable; PS = possible; U = underspecified;
+ = positive; ? = negative; u = unknown.
Value Definition Count
CT+ According to the source, it is certainly the case that X 7,749 (57.6%)
PR+ According to the source, it is probably the case that X 363 (2.7%)
PS+ According to the source, it is possibly the case that X 226 (1.7%)
CT? According to the source, it is certainly not the case that X 433 (3.2%)
PR? According to the source it is probably not the case that X 56 (0.4%)
PS? According to the source it is possibly not the case that X 14 (0.1%)
CTu The source knows whether it is the case that X or that not X 12 (0.1%)
Uu The source does not know what the factual status of the event is, 4,607 (34.2%)
or does not commit to it
13,460
Example 3
Recently, analysts have said Sun also is vulnerable to competition from International
Business Machines Corp., which plans to introduce a group of workstations early next
year, and Next Inc.
Veridicality(vulnerable, analysts) = CT+
Veridicality(vulnerable, author) = Uu
The last column of Table 1 reports the value counts in the corpus. The data are
heavily skewed, with 62% of the events falling to the positive side and 57.6% in the CT+
category alone. The inter-annotator agreement for assigning veridicality tags was high
(? = 0.81, a conservative figure given the partial ordering in the tags). A training/test
split is defined in FactBank: The documents from TimeBank 1.2 are used as the training
data and the ones from the subset of the AQUAINT TimeML corpus as the testbed.
As we noted earlier, FactBank annotations are supposed to be as purely semantic
as possible; the goal of the project was to ?focus on identifying what are the judgments
that the relevant participants make about the factuality nature of events, independently
from their intentions and beliefs, and exclusively based on the linguistic expressions
employed in the text to express such judgments,? disregarding ?external factors such as
source reliability or reader bias? (Saur?? 2008, page 5). The annotation manual contains
an extensive set of discriminatory tests (Saur?? 2008, pages 230?235) that are informed
by lexical theories of veridicality. The resulting annotations are ?textual-based, that
is, reflecting only what is expressed in the text and avoiding any judgment based on
individual knowledge? (Saur?? and Pustejovsky 2009, page 253). In addition, discourse
structure is not taken into account: ?we decided to constrain our annotation to informa-
tion only present at the sentence level? (Saur?? and Pustejovsky 2009, page 253).
2.2 Annotations from the Reader?s Perspective
In the terminology of Levinson (1995, 2000), FactBank seeks to capture aspects of sen-
tence meaning, whereas we aim to capture aspects of utterance meaning, which brings us
305
Computational Linguistics Volume 38, Number 2
closer to characterizing the amount and kind of information that a reader can reliably
extract from an utterance. We thus extend the FactBank annotations by bringing world
knowledge into the picture. Whereas the FactBank annotators were explicitly told to
avoid any reader bias, to disregard the credibility of the source, and to focus only on the
linguistic terms used in the text to express veridicality, we are interested in capturing
how people judge the veridicality of events when reader bias, credibility of the source,
and what we know about the world is allowed to play a role.
To do this, we took a subset of the FactBank sentences annotated at the author
level and recruited annotators for them using Mechanical Turk. We restricted the task to
annotators located in the United States. Our subset consists of 642 sentences (466 verbs,
155 nouns, 21 adjectives); we use all the PR+, PS+, PR?, PS? items from the FactBank
training set plus some randomly chosen Uu, CT+, and CT? items. (We did not take
any CTu sentences into account, as there are not enough of them to support experi-
mentation.) The annotators were asked to decide whether they thought the boldfaced
event described in the sentence did (or will) happen. Thus the judgments are from the
reader?s perspective, and not from the author?s or participants? perspective, as in the original
FactBank annotations. We used Saur???s seven-point annotation scheme (removing CTu).
To ensure that the workers understood the task, we first gave them four mandatory
training items?simple non-corpus examples designed to help them conceptualize the
annotation categories properly. The sentences were presented in blocks of 26 items,
three of which were ?tests? very similar to the training items, included to ensure that
the workers were careful. We discarded data from two Turkers because they did not
correctly tag the three test sentences.3
Like Saur??, we did not take the discourse structure into account: Turkers saw
only disconnected sentences and judged the event sentence by sentence. Subsequent
mentions of the same event in a discourse can, however, lead a listener to revise a
veridicality judgment already posed for that event. For instance in Example (4) from
Saur?? (2008, page 56), a reader?s veridicality judgment about the tipped off event will
probably change when reading the second sentence.
Example 4
Yesterday, the police denied that drug dealers were tipped off before the operation.
However, it emerged last night that a reporter from London Weekend Television
unwittingly tipped off residents about the raid when he phoned contacts on the estate
to ask if there had been a raid?before it had actually happened.
Here, though, we concentrate on the sentence level, and leave the issue of discourse
structure for future work. In other words, we capture the reader?s judgment about the
veridicality of an event after each sentence, independent on whether the judgment will
be revised when later information is read. This is partly to facilitate comparisons with
FactBank and partly because we are presently unsure how to computationally model
the effects of context in this area.
Figure 1 shows how the items were displayed to the Turkers. We rephrased the
event under consideration (the bold sentence in Figure 1), because it is not always
straightforward to identify the intended rephrasing. Following Saur??, we refer to this
rephrasing process as normalization. The normalization strips out any polarity and
3 The data are available at http://christopherpotts.net/ling/data/factbank/.
306
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Figure 1
Design of the Mechanical Turk experiment.
modality markers to focus only on the core event talked about. For example, in Police
gave no details, we needed to make sure that workers evaluated the positive form
(?Police gave details?), rather than the negative one (?Police gave no details?). Similarly,
in Hudson?s Bay Co. announced terms of a previously proposed rights issue that is expected to
raise about 396 million Canadian dollars (US$337 million) net of expenses, the normalization
will remove the modality marker is expected (?the proposed rights issue will raise
about 396 million Canadian dollars net of expenses?). We followed Saur???s extensive
guidelines for this rephrasing process (Saur?? 2008, pages 218?222).
We collected 10 annotations for each of the 642 events. A total of 177 Turkers
participated in the annotations. Most Turkers did just one batch of 23 non-test examples;
themean number of annotations per Turker was 44, and they each annotated between 23
and 552 sentences. Table 2 reports Fleiss kappa scores (Fleiss 1971) using the full seven-
category scheme. These scores are conservative because they do not take into account
the fact that the scale is partially ordered, with CT+, PR+, and PS+ forming a ?posi-
tive? category, CT?, PR?, and PS? forming a ?negative? category, and Uu remaining
alone. The overall Fleiss kappa for this three-category version is much higher (0.66),
reflecting the fact that many of the disagreements were about degree of confidence (e.g.,
CT+ vs. PR+) rather than the basic veridicality judgment of ?positive?, ?negative?, or
?unknown?. At least 6 out of 10 Turkers agreed on the same tag for 500 of the
Table 2
Fleiss kappa scores with associated p-values.
? p value
CT+ 0.63 < 0.001
CT? 0.80 < 0.001
PR+ 0.41 < 0.001
PR? 0.34 < 0.001
PS+ 0.40 < 0.001
PS? 0.12 < 0.001
Uu 0.25 < 0.001
Overall 0.53 < 0.001
307
Computational Linguistics Volume 38, Number 2
642 sentences (78%). For 53% of the examples, at least 8 Turkers agreed with each other,
and total agreement is obtained for 26% of the data (165 sentences).
2.3 An Alternative Scale
One of our goals is to assess whether FactBank?s seven-category scheme is the right one
for the task. To this end, we also evaluated whether a five-tag version would increase
agreement and perhaps provide a better match with readers? intuitions. Logically, PR?
is equivalent to PS+, and PS? to PR+, so it seemed natural to try to collapse them
into a two-way division between ?probable? and ?possible?. We thus ran the MTurk
experiment again with the five-point scheme in Table 3.
The five-point scheme led to lower agreement between Turkers. Globally, the PR?
items were generally mapped to ?no?, and PS? to either ?no? or ?unknown?. Some
Turkers chose the expected mappings (PS? to ?probable? and PR? to ?possible?), but
only very rarely. This is explicable in terms of the pragmatics of veridicality judgments.
Though PR? may be logically equivalent to PS+, and PS? to PR+, there are important
pragmatic differences between giving a positive judgment and giving a negative one.
For example, in Example (5), speaker B will not infer that he can possibly get a further
discount, even if ?Probably not? is consistent with ?Possibly?. Conversely, had the
answer been ?Possibly?, A would have remained hopeful.
Example 5
A: Is it possible to get further discount on the rate?
B: Probably not.
In sum, there seems to be a very intuitive notion of veridicality along the partially
ordered scale proposed by Saur??. In their work on assessing the degree of event certainty
to which an author commits, Rubin, Liddy, and Kando (2005) used the following five-
point scale: absolute, high, moderate, low, and uncertain. They did not obtain very high
inter-annotator agreement (? = 0.41). Saur?? hypothesized that their low agreement is
due to a fuzzy approach and the lack of precise guidelines. Rubin, Liddy, and Kando
had no clear identification of certainty markers, and no explicit test for distinguishing
different degrees of certainty (unlike Saur??). In our experiment, however, the guidelines
were similarly loose: Turkers were instructed only to ?read 30 sentences and decide
whether the events described in these sentences did (or will) happen.? They were not
asked to limit their attention to just the information in the sentence, and they were not
Table 3
An alternative five-tag annotation scheme.
Category Original
yes CT+
probable PR+/PS?
possible PS+/PR?
no CT?
unknown Uu
308
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
given any mappings between linguistic markers and veridicality values. Nonetheless,
Turkers reached good agreement levels in assessing event veridicality on a seven-point
scale. We conclude from this that Saur???s scale comes closer than its competitors to
capturing reader intuitions about veridicality. The good agreement mirrors the general
high inter-annotator agreement levels that have been found for the Recognizing Textual
Entailment task (Manning 2006), perhaps reflecting the fact that judging inference and
veridicality in context is a natural, everyday human task.
Diab et al (2009) annotated a 10,000-word corpus for what they call ?committed
beliefs?: whether the author of the sentence indicates with linguistic means that he
believes or disbelieves that the event described by the sentence is a fact. Thus, in essence,
the annotations assess the degree of event certainty to which an author commits, as in
Rubin?s work. They use a three-point scale: committed belief, non-committed belief, and
not applicable. An example of committed belief is GM has laid off workers. Affirmative
sentences in the future are also considered as committed belief (e.g., GM will lay off
workers). Sentences with modals and events embedded under reported verbs are an-
notated as non-committed belief. The third category, not applicable, consists of sentences
expressing desire (Some wish GM would lay of workers), questions (Many wonder if GM will
lay off workers), and requirements (Lay off workers!). The corpus covers different genres
(newswire, e-mail, blog, dialogue). The inter-annotator agreement was high (95%).
Prabhakaran, Rambow, and Diab (2010) used the corpus to automatically tag committed
beliefs according to that three-point scale. This, too, is an important resource, but it is
difficult to compare it with our own task, for two reasons. First, the annotators sought to
prevent world knowledge from influencing their annotations, which is concerned only
with linguistic markers. Second, the category non-committed belief conflates the possible,
probable, and unknown categories of our corpus (Saur???s). Though some work in the
biomedical domain (i.e., Hobby et al 2000) suggests that the distinction between possi-
ble and probable is hard to make, we did not want to avoid it, because people routinely
make such fine-grained modal distinctions when assessing claims. What?s more, the
approach we develop allows us to quantify the degree to which such judgments are
in fact variable and uncertain.
3. Lessons from the New Annotations
This section presents two kinds of high-level analysis of our annotations. We first com-
pare them with FactBank annotations for veridicality according to the author, identify-
ing places where the annotations point to sharp divergences between sentence meaning
and utterance meaning. We then study the full distribution of annotations we received
(10 per sentence), using them to highlight the uncertainty of veridicality judgments.
Both of these discussions deeply inform the modeling work of Section 4.
3.1 The Impact of Pragmatic Enrichment
Although the MTurk annotations largely agree with those of FactBank, there are sys-
tematic differences between the two that are indicative of the ways in which pragmatic
enrichment plays a role in assessing veridicality. The goal of this section is to uncover
those differences. To sharpen the picture, we limit attention to the sentences for which
there is a majority-vote category, that is, at least 6 out of 10 Turkers agreed on the
annotation. This threshold was met for 500 of the 642 examples.
309
Computational Linguistics Volume 38, Number 2
Table 4
Inter-annotator agreement comparing FactBank annotations with MTurk annotations. The data
are limited to the 500 examples in which at least 6 of the 10 Turkers agreed on the label, which is
then taken to be the true MTurk label. The very poor value for PS? derives from the fact, in this
subset, that label was chosen only once in FactBank and not at all by our annotators.
? p value
CT+ 0.37 < 0.001
PR+ 0.79 < 0.001
PS+ 0.86 < 0.001
CT? 0.91 < 0.001
PR? 0.77 < 0.001
PS? ?0.001 = 0.982
Uu 0.06 = 0.203
Overall 0.60 < 0.001
Table 4 uses kappa scores to measure the agreement between FactBank and our
annotations on this 500-sentence subset of the data. We treat FactBank as one annotator
and our collective Turkers as a second annotator, with the majority label the correct one
for that annotator. What we see is modest to very high agreement for all the categories
except Uu. The agreement level is also relatively low for CT+. The corresponding
confusion matrix in Table 5 helps explicate these numbers. The Uu category is used
much more often in FactBank than by Turkers, and the dominant alternative choice for
the Turkers was CT+. Thus, the low score for Uu also effectively drops the score for
CT+. The question is why this contrast exists. In other words, why do Turkers choose
CT+ where FactBank says Uu?
The divergence can be traced to the way in which lexicalist theories handle events
embedded under attitude predicates like say, report, and indicate: any such embedded
event is tagged Uu in FactBank. In our annotations, readers are not viewing the veridi-
cality of reported events as unknown. Instead they are sensitive to a wide range of
syntactic and contextual features, including markers in the embedded clause, expec-
tations about the subject as a source for the information conveyed by the embedded
clause, and lexical competition between the author?s choice of attitude predicate and
its alternatives. For example, even though the events in Example (6) are all embedded
Table 5
Confusion matrix comparing the FactBank annotations (rows) with our annotations (columns).
MTurk
CT+ PR+ PS+ CT? PR? PS? Uu Total
F
a
ct
B
a
n
k
CT+ 54 2 0 0 0 0 0 56
PR+ 4 63 2 0 0 0 0 69
PS+ 1 1 55 0 0 0 2 59
CT? 5 0 0 146 0 0 2 153
PR? 0 0 0 0 5 0 1 6
PS? 0 0 0 0 0 0 1 1
Uu 94 18 9 12 2 0 21 156
Total 158 84 66 158 7 0 27 500
310
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
under an attitude predicate (say), the events in Examples (6a) and (6b) are assessed as
certain (CT+), whereas the words highly confident in Example (6c) trigger PR+, and may
in Example (6d) leads to PS+.
Example 6
(a) Magna International Inc.?s chief financial officer, James McAlpine,
resigned and its chairman, Frank Stronach, is stepping in to help
turn the automotive-parts manufacturer around, the company said.
Normalization: James McAlpine resigned
Annotations: CT+: 10
(b) In the air, U.S. Air Force fliers say they have engaged in ?a little cat and
mouse? with Iraqi warplanes.
Normalization: U.S. Air Force fliers have engaged in ?a little cat and
mouse? with Iraqi warplanes
Annotations: CT+: 9, PS+: 1
(c) Merieux officials said last week that they are ?highly confident? the offer
will be approved.
Normalization: the offer will be approved
Annotations: PR+: 10
(d) U.S. commanders said 5,500 Iraqi prisoners were taken in the first hours of
the ground war, though some military officials later said the total may
have climbed above 8,000.
Normalization: the total Iraqi prisoners climbed above 8,000
Annotations: PS+: 7, PR+: 3
In Example (6a), the company said is a parenthetical modifying the main clause
(Ross 1973). Asher (2000), Rooryck (2001), and Simons (2007) argue that such construc-
tions often mark the evidential source for the main-clause information, rather than
embedding it semantically. In terms of our annotations, this predicts that CT+ or CT?
judgments will be common for such constructions, because they become more like two-
part meanings: the main-clause and the evidential commentary.
To test this hypothesis, we took from the Turkers? annotations the subset of sen-
tences tagged Uu in FactBank where the event is directly embedded under an attitude
verb or introduced by a parenthetical. We also removed examples where a modal aux-
iliary modified the event, because those are a prominent source of non-CT annotations
independently of attitude predication. This yielded a total of 78 sentences. Of these,
33 are parenthetical, and 31 (94%) of those are tagged CT+ or CT?. Of the remaining
311
Computational Linguistics Volume 38, Number 2
45 non-parenthetical examples, 42 (93%) of those are tagged CT+ or CT?. Thus, both
parenthetical and non-parenthetical verbs are about equally likely to lead to a CT tag.4
This finding is consistent with the evidential analysis of such parentheticals,
but it suggests that standard embedding can function pragmatically as an evidential
as well. This result is expected under the analysis of Simons (2007) (see also Frazier
and Clifton 2005; Clifton and Frazier 2010). It is also anticipated by Karttunen (1973),
who focuses on the question of whether attitude verbs are plugs for presuppositions,
that is, whether presuppositions introduced in their complements are interpreted as
semantically embedded. He reviews evidence suggesting that these verbs can be veridi-
cal with respect to such content, but he tentatively concludes that these are purely
pragmatic effects, writing, ?we do not seem to have any alternative except to classify
all propositional attitude verbs as plugs, although I am still not convinced that this is
the right approach? (page 190). The evidence of the present article leads us to agree with
Karttunen about this basic lexicalist classification, with the major caveat that the utter-
ance meanings involved are considerably more complex. (For additional discussion of
this point, see Zaenen, Karttunen, and Crouch [2005] and Manning [2006].)
There are also similarities between the FactBank annotations and our own in the
case of Uu. As in FactBank, antecedents of conditionals (7), generic sentences (8), and
clear cases of uncertainty with respect to the future (9) were tagged Uu by a majority of
Turkers.
Example 7
(a) If the heavy outflows continue, fund managers will face increasing
pressure to sell off some of their junk to pay departing investors in the
weeks ahead.
Normalization: the heavy outflows will continue
Annotations: Uu: 7, PS+: 2, CT+: 1
(b) A unit of DPC Acquisition Partners said it would seek to liquidate the
computer-printer maker ?as soon as possible,? even if a merger isn?t
consummated.
Normalization: a merger will be consummated
Annotations: Uu: 8, PS+: 2
Example 8
When prices are tumbling, they must bewilling to buy shares from sellers when no one
else will.
Normalization: they are willing to buy shares
Annotations: Uu: 7, PR+: 2, PS+: 1
4 We do not regard this as evidence that there is no difference between parenthetical and non-parenthetical
uses when it comes to veridicality, but rather only that the categorical examples do not reveal one.
Indeed, if we consider the full distribution of annotations, then a linear model with Parenthetical and
Verb predicting the number of CT tags reveals Parenthetical to be positively correlated with CT+
(coefficient estimate = 1.36, p = 0.028).
312
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Example 9
(a) The program also calls for coordination of economic reforms and joint
improvement of social programs in the two countries.
Normalization: there will be coordination of economic reforms and
joint improvement of social programs in the two countries
Annotations: Uu: 7, PR+: 2, PS+: 1
(b) But weak car sales raise questions about future demand from the
auto sector.
Normalization: there will be demand from the auto sector
Annotations: Uu: 6, PS+: 2, CT+: 1, PR?: 1
Another difference between FactBank and the Turkers is the more nuanced cate-
gories for PS and PR events. In FactBank, markers of possibility or probability, such
as could or likely, uniquely determine the corresponding tag (Saur?? 2008, page 233). In
contrast, the Turkers allow the bias created by these lexical items to be swayed by other
factors. For example, the auxiliary could can trigger a possible or an unknown event (10).
In FactBank, all such sentences are marked PS+.
Example 10
(a) They aren?t being allowed to leave and could become hostages.
Normalization: they will become hostages
Annotations: PS+: 10
(b) Iraq could start hostilitieswith Israel either through a direct attack
or by attacking Jordan.
Normalization: there will be hostilities
Annotations: Uu: 6, PS+: 3, PR+: 1
Similarly, expected and appeared are often markers of PR events. Whereas it is
uniquely so in FactBank, however, our annotations showmuch shifting to PS. Examples
(11) and (12) highlight the contrast: It seems likely that the annotators simply have
different overall expectations about the forecasting described in each example, a high-
level pragmatic influence that does not attach to any particular lexical item.
Example 11
(a) Big personal computer makers are developing 486-based machines, which
are expected to reach the market early next year.
Normalization: 486-based machines will reach the market early
next year
Annotations: PR+: 10
313
Computational Linguistics Volume 38, Number 2
(b) Beneath the tepid news-release jargon lies a powerful threat from the
brewing giant, which last year accounted for about 41% of all U.S. beer
sales and is expected to see that grow to 42.5% in the current year.
Normalization: there will be growth to 42.5% in the current year
Annotations: PS+: 6, PR+: 3, CT+: 1
Example 12
(a) Despite the lack of any obvious successors, the Iraqi leader?s internal
power base appeared to be narrowing even before the war began.
Normalization: the Iraqi leader?s internal power base was narrowing
even before the war began
Annotations: PR+: 7, CT+: 1, PS+: 1, PS?: 1
(b) Saddam appeared to accept a border demarcation treaty he had rejected in
peace talks following the August 1988 cease-fire of the eight-year war with
Iran.
Normalization: Saddam accepted a border demarcation treaty
Annotations: PS+: 6, PR+: 2, CT+: 2
Another difference is that nouns appearing in a negative context were tagged as
CT+ by the Turkers but as CT? or PR? in FactBank.
Example 13
(a) However, its equity in the net income of National Steel declined to
$6.3 million from $10.9 million as a result of softer demand and lost
orders following prolonged labor talks and a threatened strike.
Normalization: there were orders
Annotations: CT+: 6, PR+: 1, PR?: 1, PS+: 1, Uu: 1
This seems to trace to uncertainty about what the annotation should be when
the event involves a change of state (from orders existing to not existing). Saur?? and
Pustejovsky (2009, page 260) note that noun events were a frequent source of disagree-
ment between the two annotators because the annotation guidelines did not address at
all how to deal with them.
3.2 The Uncertainty of Pragmatic Enrichment
For the purpose of comparing our annotations with those of FactBank, it is useful to sin-
gle out the Turkers? majority-choice category, as we did here. We have 10 annotations for
each event, however, which invites exploration of the full distribution of annotations,
314
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
to see if the areas of stability and variation can teach us something about the nature
of speakers? veridicality judgments. In this section, we undertake such an exploration,
arguing that the patterns reveal veridicality judgments to be importantly probabilistic,
as one would expect from a truly pragmatic phenomenon.
Figure 2 provides a high-level summary of the reaction distributions that our sen-
tences received. The labels on the y-axis characterize types of distribution. For example,
5/5 groups the sentences for which the annotators were evenly split between two
categories (e.g., a sentence for which 5 Turkers assigned PR+ and 5 assigned PS+, or
a sentence for which 5 Turkers chose PR+ and 5 chose Uu). The largest grouping, 10,
pools the examples on which all the annotators were in agreement.
We can safely assume that some of the variation seen in Figure 2 is due to the
noisiness of the crowd-sourced annotation process. Some annotators might have been
inattentive or confused, or simply lacked the expertise to make these judgments (Snow
et al 2008). For example, the well-represented 1/9 and 1/1/8 groups probably rep-
resent examples for which veridicality assessment is straightforward but one or two
of the annotators did not do a good job. If all the distributions were this skewed,
we might feel secure in treating veridicality as categorical. There are many examples
for which it seems implausible to say that the variation is due to noise, however.
For example, 5/5 groups include sentences like Examples (14) and (15), for which the
judgments depend heavily on one?s prior assumptions about the entities and concepts
involved.
Figure 2
Reaction distributions by type.
315
Computational Linguistics Volume 38, Number 2
Example 14
In a statement, the White House said it would do ?whatever is necessary? to ensure
compliance with the sanctions.
Normalization: there will be compliance with the sanctions
Annotations: Uu: 5, PR+: 5
Example 15
Diplomacy appears to be making headway in resolving the United Nations? standoff
with Iraq.
Normalization: diplomacy is resolving the United Nations? standoff with Iraq
Annotations: PR+: 5, PS+: 5
The 4/6, 1/4/5, 4/4, and 3/3 groups contain many similarly difficult cases. Com-
bining all of the rows where two categories received at least 3 votes, we get 162 exam-
ples, which is 25% of the total data set. Thus, a non-negligible subset of our sentences
seem to involve examples where readers? responses are divided, suggesting that there
is no unique correct label for them.
Finally, it seems likely that the long tail of very high-entropy distributions at the top
of the graph in Figure 2 is owed in large part to the fact that veridicality judgments
are often not reachable with confidence, because the utterance is inherently under-
specified or because additional contextual information is needed in order to be sure.
This, too, suggests to us that it would be foolhardy to assign a unique veridicality
label to every example. Of course, situating the sentences in context would reduce
some of this uncertainty, but no amount of background information could eliminate it
entirely.
When we look more closely at these distributions, we find additional evidence for
the idea that veridicality is graded and variable. One of the most striking patterns
concerns the question of whether the annotators enriched an example at all, in the
following sense. Consider an event that is semantically non-veridical. This could be
simply because it is embedded under a non-factive attitude predicate (say, allege), or
an evidential marker (according to sources, it seems). The semantic strategy for such
cases is to pick Uu. Depending on the amount and nature of the contextual infor-
mation brought to bear on the assessment, however, one might enrich this into one
of the positive or negative categories. A cautious positive enrichment would be PS+,
for example.
In light of this, it seems promising to look at the subset of 4/6 and 5/5 examples
in which one of the chosen categories is Uu, to see what the other choices are like. On
the enrichment hypothesis, the other choices should be uniformly positive or negative
(up to some noise). Figure 3 summarizes the sentences in our corpus that result in this
kind of split. The y-axis represents the percentage of non-Uu tags, with the positive
values (CT+, PR+, PS+) extending upwards and the negative ones extending down-
wards. For sentences 1?5 and 18?47 (74% of the total), all of the non-Uu tags were
uniform in their basic polarity. What?s more, the distributions within the positive and
negative portions are highly systematic. In the positive realm, the dominant choice is
PS+, the most tentative positive enrichment, followed by PR+, and then CT+. (In the
negative realm, CT? is the most represented, but we are unsure whether this supports
any definitive conclusions, given the small number of examples.) Our generalization
316
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Figure 3
The subset of 4/6 and 5/5 distributions in which one of the dominant categories was Uu. The
bars represent the distribution of non-Uu tags for each of these sentences. The top portion
depicts the positive tags, and the bottom portion depicts the negative tags.
about these patterns is that enrichment from a semantic Uu baseline is systematic and
common, though with interesting variation both in whether it occurs and, if it does,
how much.
The full distributions are also informative when it comes to understanding the
range of effects that specific lexical items can have on veridicality assessment. To illus-
trate, we focus on the modal auxiliary verbs can, could, may, might, must, will, would.5 In
keepingwith lexicalist theories, when they are clausemate to an event, that event is often
tagged with one of the PR and PS tags. The relationship is a loose one, however; the
modal seems to steer people into these weaker categories but does not determine their
final judgment. We illustrate in Example (16) with examples involving may in positive
contexts.
Example 16
(a) Last Friday?s announcement was the first official word that the project was
in trouble and that the company?s plans for a surge in market share may
have been overly optimistic.
Normalization: the company?s plans have been overly optimistic
Annotations: PS+: 5, PR+: 5
(b) In a letter, prosecutors told Mr. Antar?s lawyers that because of the recent
Supreme Court rulings, they could expect that any fees collected from Mr.
Antar may be seized.
Normalization: fees collected from Mr. Antar will be seized
Annotations: PS+: 4, PR+: 6
5 Other modals, such as should, ought, and have to, are either not well represented in our data or
simply absent.
317
Computational Linguistics Volume 38, Number 2
(c) The prospectus didn?t include many details about the studio and theme
park, although conceptual drawings, released this month, show that it
may feature several ?themed? areas similar to those found at parks built
by Walt Disney Co.
Normalization: the park features several ?themed? areas similar to
those found at parks built by Walt Disney Co.
Annotations: PS+: 4, PR+: 4, CT+:1, Uu:1
Figure 4 summarizes the data for the set of modals on which we could focus. Here,
we restrict attention to event descriptions that are clausemate to a modal, effectively
taking each modal to be annotated with the distribution of annotations for its clause-
mate event. We also look only at the positive tags, because the negative ones were too
infrequent to provide reliable estimates.
Two types of modals have been recognized in the literature, weak and strongmodals
(Wierzbicka 1987; S?bo 2001; von Fintel and Iatridou 2008; Finlay 2009). Each type has
different distribution profiles. As expected, the weak possibility modals can, could, may,
and might correlate strongly with PS. The other categories are also well-represented
for these modals, however, indicating that the contribution of these markers is heavily
influenced by other factors. The strong (or necessity) modals must, will, and would are
much more evenly distributed across the categories.
The mixed picture for modal auxiliaries seems to be typical of modal markers more
generally. We do not have enough data to present a quantitative picture for items like
potentially, apparently, and partly, but the following sentences suggest that they are every
bit as nuanced in their contributions to veridicality.
Figure 4
The contribution of modal auxiliaries to veridicality judgments.
318
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Example 17
(a) Anheuser-Busch Cos. said it plans to aggressively discount its major
beer brands, setting the stage for a potentially bruising price war as the
maturing industry?s growth continues to slow.
Normalization: there will be a bruising price war
Annotations: PS+: 5, PR+: 5
(b) The portfolio unit of the French bank group Credit Lyonnais told stock
market regulators that it bought 43,000 shares of Cie. de Navigation
Mixte, apparently to help fend off an unwelcome takeover bid for the
company.
Normalization: the 43,000 shares of Cie. de Navigation Mixte will
fend off an unwelcome takeover bid for the company
Annotations: PS+: 4, PR+: 4, CT:1, Uu:1
(c) Nonetheless, concern about the chip may have been responsible
for a decline of 87.5 cents in Intel?s stock to $32 a share yesterday in
over-the-counter trading, on volume of 3,609,800 shares, and partly
responsible for a drop in Compaq?s stock in New York Stock Exchange
composite trading on Wednesday
Normalization: concern about the chip is responsible for a drop in
Compaq?s stock
Annotations: PS+: 4, PR+: 4, CT+:1, PR?:1
The discussion in this section suggests to us that work on veridicality should
embrace variation and uncertainty as part of the characterization of veridicality, rather
than trying to approximate the problem as one of basic categorization. We now turn
to experiments with a system for veridicality assessment that acknowledges the multi-
valued nature of veridicality.
4. A System for Veridicality Assessment
In this section, we describe amaximum entropy classifier (Berger, Della Pietra, andDella
Pietra 1996) that we built to automatically assign veridicality. For classification tasks, the
dominant tradition within computational linguistics has been to adjudicate differing
human judgments and to assign a single class for each item in the training data. In
Section 3.2, however, we reviewed the evidence in our annotations that veridicality is
not necessarily categorical, by virtue of the uncertainty involved in making pragmatic
judgments of this sort. In order to alignwith our theoretical conception of the problem as
probabilistic, we treat each annotator judgment as a training item. Thus, each sentence
appears 10 times in our training data.
319
Computational Linguistics Volume 38, Number 2
A maximum entropy model computes the probability of each class c given the data
d as follows:
p(c|d, ?) =
exp
?
i ?ifi(c, d)
?
c? exp
?
i ?ifi(c
?, d)
where, for us, the features fi are indicator functions of a property ? of the data d
and a particular class c: fi(c, d) ? ?(d) ? c = ck. The weights ?i of the features are the
parameters of the model chosen to maximize the conditional likelihood of the training
data according to the model. The maximum entropy model thus gives us a distribu-
tion over the veridicality classes, which will be our output. To assess how good the
output of the model is, we will give the log-likelihood of some data according to the
model. For comparison, we will also give the log-likelihood for the exact distribution
from the Turkers (which thus gives an upper-bound) as well as a log-likelihood for
a baseline model which uses only the overall distribution of classes in the training
data.
A maximum entropy classifier is an instance of a generalized linear model with a
logit link function. It is almost exactly equivalent to the standard multi-class (also called
polytomous or multinomial) logistic regression model from statistics, and readers more
familiar with this presentation can think of it as such. In all our experiments, we use the
Stanford Classifier (Manning and Klein 2003) with a Gaussian prior (also known as L2
regularization) set to N (0, 1).6
4.1 Features
The features were selected through 10-fold cross-validation on the training set.
Predicate classes. Saur?? (2008) defines classes of predicates (nouns and verbs) that project
the same veridicality value onto the events they introduce. The classes also define
the grammatical relations that need to hold between the predicate and the event it
introduces, because grammatical contexts matter for veridicality. Different veridicality
values will indeed be assigned to X in He doesn?t know that X and in He doesn?t know if X.
The classes have names like ANNOUNCE, CONFIRM, CONJECTURE, and SAY. Like Saur??,
we used dependency graphs produced by the Stanford parser (Klein andManning 2003;
de Marneffe, MacCartney, and Manning 2006) to follow the path from the target event
to the root of the sentence. If a predicate in the path was contained in one of the classes
and the grammatical relation matched, we added both the lemma of the predicate as a
feature and a feature marking the predicate class.
6 The maximum entropy formulation differs from the standard multi-class logistic regression model by
having a parameter value for each class giving logit terms for how a feature?s value affects the outcome
probability relative to a zero feature, whereas in the standard multi-class logistic regression model
there are no parameters for one distinguished reference class, and the parameters for other classes say
how the value of a feature affects the outcome probability differentially from the reference class. Without
regularization, the maximum entropy formulation is overparameterized, and the parameters are
unidentifiable; in a regularized setting, however, this is no longer a problem and the maximum entropy
formulation then has the advantage that all classes are treated symmetrically, with a simpler symmetric
form of model regularization.
320
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
World knowledge. For each verb found in the path and contained in the predicate classes,
we also added the lemma of its subject, and whether or not the verb was negated.
Our rationale for including the subject is that, as we saw in Section 3, readers? inter-
pretations differ for sentences such as The FBI said it received . . . and Bush said he re-
ceived . . . , presumably because of world knowledge they bring to bear on the judgment.
To approximate such world knowledge, we also obtained subject?verb bigram and sub-
ject counts from theNew York Times portion of GigaWord and then included log(subject?
verb-counts/subject-counts) as a feature. The intuition here is that some embedded
clauses carry the main point of the sentence (Frazier and Clifton 2005; Simons 2007;
Clifton and Frazier 2010), with the overall frequency of the elements introducing the
embedded clause contributing to readers? veridicality assessments.
General features.We used the lemma of the event, the lemma of the root of the sentence,
the incoming grammatical relation to the event, and a general class feature.
Modality features. We used Saur???s list of modal words as features. We distinguished
between modality markers found as direct governors or children of the event under
consideration, andmodal words found elsewhere in the context of the sentence. Figure 4
provides some indication of how these will relate to our annotations.
Negation. A negation feature captures the presence of linguistic markers of negative
contexts. Events are considered negated if they have a negation dependency in the
graph or an explicit linguistic marker of negation as dependent (e.g., simple negation
(not), downward-monotone quantifiers (no, any), or restricting prepositions). Events are
also considered negated if embedded in a negative context (e.g., fail, cancel).
Conditional. Antecedents of conditionals and words clearly marking uncertainty are
reliable indicators of the Uu category. We therefore checked for events in an if -clause
or embedded under markers such as call for.
Quotation. Another reliable indicator of the Uu category is quotation. We generated a
quotation feature if the sentence opened and ended with quotation marks, or if the root
subject was we.
In summary, our feature set alows combinations of evidence from various sources
to determine veridicality. To be sure, lexical features are important, but they must be
allowed to interact with pragmatic ones. In addition, the model does not presume
that individual lexical items will contribute in only one way to veridicality judgments.
Rather, their contributions are affected by the rest of the feature set.
4.2 Test Data
As test set, we used 130 sentences from the test items in FactBank. We took all the
sentences with events annotated PR+ and PS+ at the author level (there are very few),
and we randomly chose sentences for the other values (CT+, CT?, and Uu, because the
FactBank test set does not contain any PR? and PS? items). Three colleagues provided
the normalizations of the sentences following Saur???s guidelines, and the data were then
321
Computational Linguistics Volume 38, Number 2
annotated usingMechanical Turk, as described in Section 2. For 112 of the 130 sentences,
at least six Turkers agreed on the same value.
5. Results
Table 6 gives log-likelihood values of the classifier for the training and test sets, along
with the upper and lower bounds. The upper bound is the log-likelihood of the model
that uses the exact distribution from the Turkers. The lower bound is the log-likelihood
of a model that uses only the overall rate of each class in our annotations for the training
data.
Kullback-Leibler (KL) divergence provides a related way to assess the effectiveness
of the classifier. The KL divergence between two distributions is an asymmetric measure
of the difference between them. We use Example (6d) to illustrate. For that sentence, the
classifier assigns a probability of 0.64 to PS+ and 0.28 to PR+, with very low probabilities
for the remaining categories. It thus closely models the gold distribution (PS+: 7/10,
PR+: 3/10). The KL divergence is correspondingly low: 0.13. The KL divergence for a
classifier that assigned 0.94 probability to the most frequent category (i.e., CT+) and 0.01
to the remaining categories would be much higher: 5.76.
The mean KL divergence of our model is 0.95 (SD 1.13) for the training data and
0.81 (SD 0.91) for the test data. The mean KL divergence for the baseline model is 1.58
(SD 0.57) for the training data and 1.55 (SD 0.47) for the test data. To assess whether our
classifier is a statistically significant improvement over the baseline, we use a paired
two-sided t-test over the KL divergence values for the two models. The t-test requires
that both vectors of values in the comparison have normal distributions. This is not
true of the raw KL values, which have approximately gamma distributions, but it is
basically true of the log of the KL values: For the model?s KL divergences, the normality
assumption is very good, whereas for the baseline model there is some positive skew.
Nonetheless, the t-test provides a fair way to contextualize and compare the KL values
of the two models. By this test, our model improves significantly over the lower bound
(two-sided t = ?11.1983, df = 129, p-value < 2.2e?16).
We can also compute precision and recall for the subsets of the data where there is
a majority vote, that is, where six out of ten annotators agreed on the same label. This
allows us to give results per veridicality tag. We take as the true veridicality value the
one on which the annotators agreed. The value assigned by the classifier is the one with
the highest probability. Table 7 reports precision, recall, and F1 scores on the training
and test sets, along with the number of instances in each category. None of the items in
our test data were tagged with PR? or PS? and these categories were very infrequent
in the training data, so we leave them out. The table also gives baseline results: We
Table 6
Log-likelihood values for the training and test data.
Train Test
lower-bound ?10813.97 ?1987.86
classifier ?8021.85 ?1324.41
upper-bound ?3776.30 ?590.75
322
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Table 7
Precision, recall, and F1 on the subsets of the training data (10-fold cross-validation) and test
data where there is majority vote, as well as F1 for the baseline.
Train Test
# P R F1 Baseline F1 # P R F1 Baseline F1
CT+ 158 74.3 84.2 78.9 32.6 61 86.9 86.9 86.9 31.8
CT? 158 89.4 91.1 90.2 34.1 31 96.6 90.3 93.3 29.4
PR+ 84 74.4 69.1 71.6 19.8 7 50.0 57.1 53.3 6.9
PS+ 66 75.4 69.7 72.4 16.7 7 62.5 71.4 66.7 0.0
Uu 27 57.1 44.4 50.0 10.7 6 50.0 50.0 50.0 0.0
Macro-avg 74.1 71.7 72.6 22.8 69.2 71.1 70.0 13.6
Micro-avg 78.6 78.6 78.6 27.0 83.0 83.0 83.0 22.3
used a weighted random guesser, as for the lower-bound given in Table 6. Our results
significantly exceed the baseline (McNemar?s test, p < 0.001).7
The classifier weights give insights about the interpretation of lexical markers. Some
markers behave as linguistic theories predict. For example, believe is often a marker of
probability whereas could and may are more likely to indicate possibility. But as seen in
Examples (10) and (16), world knowledge and other linguistic factors shape the veridi-
cality of these items. The greatest departure from theoretical predictions occurs with
the SAY category, which is logically non-veridical but correlates highly with certainty
(CT+) in our corpus.8 Conversely, the class KNOW, which includes know, acknowledge,
and learn, is traditionally analyzed as veridical (CT+), but in our data is sometimes a
marker of possibility, as we discuss in the Conclusion. Our model thus shows that to
account for how readers interpret sentences, the space of veridicality should be cut up
differently than the lexicalist theories propose.
6. Error Analysis
We focus on two kinds of errors. First, where there is a majority label (a label six or
more of the annotators agreed on) in the annotations, we can compare that label with
the one assigned the highest probability according to our model. Second, we can study
cases where the the annotation distribution diverges considerably from our model?s
distribution (i.e., cases with a very high KL divergence).
For the majority-label cases, errors of polarity are extremely rare; the classifier
wrongly assesses the polarity of only four events, shown in Example (18). Most of
the errors are thus in the degree of confidence (e.g., CT+ vs. PR+). The graphs next
7 McNemar?s test assesses whether the proportions of right and wrong predictions for two systems are
significantly different. We calculated the test exactly via the binomial distribution. We chose a random
guess baseline rather than a choose-most-frequent-class baseline hoping to illustrate a sensible baseline F1
performance for each class, but actually the baseline F1 remains zero for two classes on the test set (there
are few items in each class and the random guesser never guessed correctly). In sum, the performance
differences are significant for both the training and test sets compared to either of these baselines.
8 This might be due to the nature of our corpus, namely, newswire, where in the vast majority of the cases,
reports are considered true. The situation could be totally different in another genre (such as blogs, for
instance).
323
Computational Linguistics Volume 38, Number 2
to the examples compare the gold annotation from the Turkers (the black bars) with
the distribution proposed by the classifier (the gray bars). The KL divergence value is
included to help convey how such values relate to these distributions.
Example 18
(a) Addressing a NATO flag-lowering ceremony at the Dutch embassy, Orban
said the occasion indicated the end of the embassy?smission of liaison
between Hungary and NATO.
Normalization: there is an embassy?s
mission of liaison between Hungary and
NATO
Annotations: CT+:7, CT?: 3
(b) But never before has NATO reached out to its former Eastern-bloc
enemies.
Normalization: NATO has reached out to its
former Eastern-bloc enemies in the past
Annotations: CT?: 9, Uu: 1
(c) Horsley was not a defendant in the suit, in which the Portland, Ore., jury
ruled that such sites constitute threats to abortion providers.
Normalization: Horsley was a defendant in
the suit
Annotations: CT?: 10
(d) A total of $650,000, meanwhile, is being offered for information leading to
the arrest of Kopp, who is charged with gunning down Dr. Barnett Slepian
last fall in his home in Buffalo.
Normalization: Kopp has been arrested
Annotations: CT?: 8, Uu: 2
When the system missed CT? events, it failed to find an explicit negative marker,
as in Example (18b), where (due to a parse error) never is treated as a dependent of the
324
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
verb have and not of the reaching out event. Similarly, the system could not capture in-
stances in which the negation was merely implicit, as in Example (18d), where the non-
veridicality of the arresting event requires deeper interpretation that our feature-set can
manage.
In Example (19), we give examples of CT+ events that are incorrectly tagged PR+,
PS+, or Uu by the system because of the presence of a weak modal auxiliary or a verb
that lowers certainty, such as believe. As we saw in Section 3.2, these markers correlate
strongly with the PS categories.
Example 19
(a) The NATO summit, she said, would produce an initiative that ?responds
to the grave threat posed by weapons of mass destruction and their means
of delivery.?
Normalization: there will be an initiative
Annotations: CT+: 7, PR+: 3
(b) Kopp, meanwhile, may have approached the border with Mexico, but it is
unknown whether he crossed into that country, said Freeh.
Normalization: it is unknown whether
Kopp crossed into Mexico
Annotations: CT+: 10
(c) They believe Kopp was driven to Mexico by a female friend after the
shooting, and have a trail of her credit card receipts leading to Mexico, the
federal officials have said.
Normalization: there was a shooting
Annotations: CT+: 10
In the case of PR+ and PS+ events, all the erroneous values assigned by the system
are CT+. Some explicit modality markers were not seen in the training data, such as
potential in Example (20a), and thus the classifier assigned them no weight. In other
cases, such as Example (20b), the system did not capture the modality implicit in the
conditional.
325
Computational Linguistics Volume 38, Number 2
Example 20
(a) Albright also used her speech to articulate a forward-looking vision for
NATO, and to defend NATO?s potential involvement in Kosovo.
Normalization: NATO will be involved in
Kosovo
Annotations: PS+: 6, PR+: 2, CT+: 1, Uu: 1
(b) ?And we must be resolute in spelling out the consequences of
intransigence,? she added, referring to the threat of NATO air strikes
against Milosevic if he does not agree to the deployment.
Normalization: there will be NATO air
strikes
Annotations: PS+: 8, PR+: 1, Uu: 1
(c) But the decision by District Attorney Frank C. Clark to begin presenting
evidence to a state grand jury suggests that he has amassed enough
material to support a criminal indictment for homicide.
Normalization: District Attorney Frank C.
Clark has amassed material to support a
criminal indictment for homicide
Annotations: PR+: 6, CT+: 3, PS+: 1
(d) The first round of DNA tests on the hair at the FBI Laboratory here
established a high probability it came from the same person as a hair
found in a New Jersey home where James C. Kopp, a 44-year-old
anti-abortion protester, lived last year, the official said.
Normalization: the hair came from the same
person
Annotations: PR+: 10
326
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
The only Uu events that the system correctly retrieved were antecedents of a con-
ditional. For the other Uu events in Example (21), the system assigned CT+ or PR+.
The majority of Uu events proved to be very difficult to detect automatically since
complex pragmatic factors are at work, many of them only very indirectly reflected in
the texts.
Example 21
(a) Kopp?s stepmother, who married Kopp?s father when Kopp was in his
30s, said Thursday from her home in Irving, Texas: ?I would like to
see him come forward and clear his name if he?s not guilty, and if he?s
guilty, to contact a priest and make his amends with society, face what
he did.?
Normalization: Kopp did something
Annotations: Uu: 7, PS+: 2, CT+: 1
(b) Indeed, one particularly virulent anti-abortion Web site lists the names
of doctors it says perform abortions, or ?crimes against humanity,?
with a code indicating whether they are ?working,? ?wounded? or
a ?fatality.?
Normalization: doctors are working
Annotations: Uu:7, CT+: 3
It is also instructive to look at the examples for which there is a large KL divergence
between our model?s predicted distribution and the annotation distribution. Very often,
this is simply the result of a divergence between the predicted and actual majority
label, as discussed earlier. Instances like Example (22) aremore interesting in this regard,
however: These are cases where there was no majority label, as in Example (22a), or
where the model guessed the correct majority label but failed to capture other aspects
of the distribution, as in Examples (22b) and (22c).
Example 22
(a) On Tuesday, the National Abortion and Reproductive Rights Action
League plans to hold a news conference to screen a television
advertisement made last week, before Slepian died, featuring Emily
327
Computational Linguistics Volume 38, Number 2
Lyons, a nurse who was badly wounded earlier this year in the
bombing of an abortion clinic in Alabama.
Normalization: there will be a news
conference to screen a television
advertisement
Annotations: CT+: 5, PR+: 5
(b) Vacco?s campaign manager, Matt Behrmann, said in a statement that
Spitzer had ?sunk to a new and despicable low by attempting to
capitalize on the murder of a physician in order to garner votes.?
Normalization: Spitzer had attempted to
capitalize on the murder of a physician in
order to garner votes
Annotations: CT+: 5, PR+: 1, PS+: 3, Uu: 1
(c) Since there is no federal homicide statute as such, the federal officials said
Kopp could be charged under the recent Freedom of Access to Clinic
Entrances Act, which provides for a sentence of up to life imprisonment
for someone convicted of physical assaults or threats against abortion
providers.
Normalization: Kopp will be charged under
the recent Freedom of Access to Clinic
Entrances Act
Annotations: PS+: 8, Uu: 2
In Example (22a), the classifier is confused by an ambiguity: it treats hold as a
kind of negation, which leads the system to assign a 0.78 probability to CT?. In Ex-
ample (22b), there are no features indicating possibility, but a number of SAY-related
features are present, which leads to a very strong bias for CT+ (0.86) and a corre-
sponding failure to model the rest of the distribution properly. In Example (23c), the
classifier correctly assigns most probability to PS+, but the rest of the probability mass
is distributed between CT+ and PR+. This is another manifestation of the problem,
noted earlier, that we have very few strong indicators of Uu. The exception to that is
conditional antecedents. As a result, we do well with cases like Example (23a), where
the event is in a conditional; the classifier assigns 70% of the probability to Uu and 0.15
to PS+.
328
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Example 23
(a) On Monday, Spitzer called for Vacco to revive that unit immediately,
vowing that he would do so on his first day in office if elected.
Normalization: Spitzer will be elected
Annotations: Uu: 7, PS+: 3
Overall the system assigns incorrect veridicality distributions in part because it
misses explicit linguistic markers of veridicality, but also because contextual and prag-
matic factors cannot be captured. This is instructive, though, and serves to further
support our central thesis that veridicality judgments are not purely lexical, but rather
involve complex pragmatic reasoning.
7. Conclusion
Our central goal for this article was to explore veridicality judgments at the level of
utterance meaning. To do this, we extended FactBank (Saur?? and Pustejovsky 2009)
with veridicality annotations that are informed by context and world knowledge (Sec-
tion 2). Although the two sets of annotations are similar in many ways, their differences
highlight areas in which pragmatic factors play a leading role in shaping readers?
judgments (Section 3.1). In addition, because each one of our sentences was judged by 10
annotators, we actually have annotation distributions for our sentences, which allows us
to identify areas of uncertainty in veridicality assessment (Section 3.2). This uncertainty
is so pervasive that the problem itself seems better modeled as one of predicting a
distribution over veridicality categories, rather than trying to predict a single label. The
predictive model we developed (Section 4) is true to this intuition, because it trains on
and predicts distributions. All the features of the model, even the basic lexical ones,
show the influence of interacting pragmatic factors (Section 5). Although automatically
assigning veridicality judgments that correspond to readers? intuitions when pragmatic
factors are allowed to play a role is challenging, our classifier shows that it can be done
effectively using a relatively simple feature set, and we expect performance to improve
as we find ways to model richer contextual features.
These findings resonate with the notion of entailment used in the Recognizing
Textual Entailment challenges (Dagan, Glickman, and Magnini 2006), where the goal
is to determine, for each pair of sentences ?T,H?, whether T (the text) justifies H (the
hypothesis). The original task definition draws on ?common-sense? understanding of
language (Chapman 2005), and focuses on how people interpret utterances naturalisti-
cally. Thus, these entailments are not calculated over just the information contained in
the sentence pairs, as amore classical logical approachwould have it, but rather over the
full utterance meaning. As a result, they are imbuedwith all the uncertainty of utterance
meanings (Zaenen, Karttunen, and Crouch 2005; Crouch, Karttunen, and Zaenen 2006;
Manning 2006). This is strongly reminiscent of our distinction between semantic and
pragmatic veridicality. For example, as a purely semantic fact, might(S) is non-veridical
329
Computational Linguistics Volume 38, Number 2
with regard to S. Depending on the nature of S, however, the nature of the source, the
context, and countless other factors, one might nonetheless infer S. This is one of the
central lessons of our new annotations.
In an important sense, we have been conservative in bringing semantics and prag-
matics together, because we do not challenge the basic veridicality categorizations that
come from linguistic and logical work on this topic. Rather, we just showed that those
semantic judgments are often enriched pragmatically?for example, from uncertainty to
one of the positive or negative categories, or from PS to PR or even CT. The interaction
between lexical markers and pragmatic context is also crucial in the case of absolutism:
Too many lexical markers conveying certainty might in some cases undermine the
speaker?s credibility (e.g., in a car salesman pitch) and actually incite mistrust in
the hearer/reader. In such instances, lexical markers only are not good indicators of the
veridicality of the event, but the pragmatic context of the utterance needs to be taken
into account to fully appreciate the interpretation people assign to it. There is, however,
evidence suggesting that we should be even more radically pragmatic (Searle 1978;
Travis 1996), by dropping the notion that lexical items can be reliably classified once
and for all. For example, lexical theories generally agree that know is veridical with
respect to its sentential complement, and the vast majority of its uses seem to support
that claim. There are exceptions, though, as in Example (24) (see also Beaver 2010;
Hazlett 2010):
Example 24
(a) For the first time in history, the U.S. has gone to war with an Arab and
Muslim nation, and we know a peaceful solution was in reach.
(b) Let me tell you something, when it comes to finishing the fight, Rocky
and I have a lot in common. I never quit, I never give up, and I know
that we?re going tomake it together.
? Hillary Clinton, 1 September 2008.
(c) ?That woman who knew I had dyslexia ? I never interviewed her.?
? George W. Bush (New York Times, 16 September 2000. Quoted by
Miller [2001].)
All of these examples seem to use know to report emphatically held belief, a much
weaker sense than a factive lexical semantics would predict. Example (24c) is the
most striking of the group, because it seems to be pragmatically non-veridical: The
continuation is Bush?s evidence that the referent of that woman could not possibly be
in a position to determine whether he is dyslexic. Such examples further emphasize
the importance of a pragmatically informed perspective on veridicality in natural
language.
One key component of veridicality judgments that we left out in this study is the
text provenance. Our data did not allow us to examine its impact because we did not
have enough variation in the provenance. All FactBank sentences are from newspaper
and newswire text such as the Wall Street Journal, the Associated Press, and the New
York Times. The trustworthiness of the document provenance can affect veridicality
judgments, however: People might have different reactions reading a sentence in the
New York Times versus in a random blog on theWeb.We plan to examine and incorporate
the role of text provenance in future work.
330
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
Acknowledgments
We thank our three anonymous referees for
their insights and advice. Thanks also to
Anastasia Giannakidou, Jesse Harris, Ed
Hovy, Lauri Karttunen, James Pustejovsky,
and Annie Zaenen for valuable comments
at various stages of the project. A special
thanks to Eric Acton, Alex Djalali, and Scott
Grimm for their help with the test data. This
research was supported in part by ONR
grant no. N00014-10-1-0109, ARO grant
no. W911NF-07-1-0216, and by a Stanford
Interdisciplinary Graduate Fellowship to
the first author.
References
Asher, Nicholas. 2000. Truth conditional
discourse semantics for parentheticals.
Journal of Semantics, 17(1):31?50.
Asher, Nicholas and Alex Lascarides.
2003. Logics of Conversation. Cambridge
University Press, Cambridge.
Barwise, Jon. 1981. Scenes and other
situations. The Journal of Philosophy,
78(7):369?397.
Beaver, David. 2010. Have you noticed that
your belly button lint colour is related to
the colour of your clothing? In Rainer
Ba?uerle, Uwe Reyle, and Thomas Ede
Zimmermann, editors, Presuppositions
and Discourse: Essays Offered to Hans
Kamp. Elsevier, Philadelphia, PA,
pages 65?99.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?71.
Chapman, Siobhan. 2005. Paul Grice:
Philosopher and Linguist. Palgrave
Macmillan, Houndmills, Basingstoke,
Hampshire.
Chapman, Wendy W., Will Bridewell,
Paul Hanbury, Gregory F. Cooper, and
Bruce G. Buchanan. 2001. A simple
algorithm for identifying negated
findings and diseases in discharge
summaries. Journal of Biomedical
Informatics, 34(5):301?310.
Clifton, Charles Jr. and Lyn Frazier. 2010.
Imperfect ellipsis: Antecedents beyond
syntax? Syntax, 13(4):279?297.
Crouch, Richard, Lauri Karttunen, and
Annie Zaenen. 2006. Circumscribing
is not excluding: A reply to Manning.
Ms., Palo Alto Research Center,
Palo Alto, CA.
Dagan, Ido, Oren Glickman, and
Bernardo Magnini. 2006. The PASCAL
recognising textual entailment challenge.
In J. Quinonero-Candela, I. Dagan,
B. Magnini, and F. d?Alche?-Buc, editors,
Machine Learning Challenges, Lecture
Notes in Computer Science, volume 3944.
Springer-Verlag, New York, pages 177?190.
de Marneffe, Marie-Catherine, Bill
MacCartney, and Christopher D. Manning.
2006. Generating typed dependency
parses from phrase structure parses.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation, Genoa, Italy, pages 449?454.
Diab, Mona, Lori Levin, Teruko Mitamura,
Owen Rambow, Vinodkumar
Prabhakaran, and Weiwei Guo. 2009.
Committed belief annotation and tagging.
In Proceedings of the Third Linguistic
Annotation Workshop, Singapore,
pages 68?73.
Elkin, Peter L., Steven H. Brown, Brent A.
Bauer, Casey S. Husser, William Carruth,
Larry R. Bergstrom, and Dietlind L.
Wahner-Roedler. 2005. A controlled trial
of automated classification of negation
from clinical notes. BMC Medical
Informatics and Decision Making, 5(13).
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 shared task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning: Shared Task,
Uppsala, Sweden, pages 1?12.
Finlay, Stephen. 2009. Oughts and ends.
Philosophical Studies, 143(3):315?340.
Fleiss, Joseph I. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Frazier, Lyn and Charles Clifton, Jr. 2005. The
syntax-discourse divide: Processing
ellipsis. Syntax, 8(1):121?174.
Giannakidou, Anastasia. 1994. The semantic
licensing of NPIs and the Modern Greek
subjunctive. In Language and Cognition 4,
Yearbook of the Research Group for Theoretical
and Experimental Linguistics. University of
Groningen, The Netherlands, pages 55?68.
Giannakidou, Anastasia. 1995. Weak and
strong licensing: Evidence from Greek.
In Artemis Alexiadou, Geoffrey Horrocks,
andMelita Stavrou, editors, Studies in Greek
Syntax. Kluwer, Dordrecht, pages 113?133.
Giannakidou, Anastasia. 1999. Affective
dependencies. Linguistics and Philosophy,
22(4):367?421.
331
Computational Linguistics Volume 38, Number 2
Giannakidou, Anastasia. 2001. The meaning
of free choice. Linguistics and Philosophy,
24(6):659?735.
Hazlett, Allan. 2010. The myth of factive
verbs. Philosophy and Phenomenological
Research, 80(3):497?522.
Hobby, Jonathan L., Brian D. M. Tom,
C. Todd, Philip W. P. Bearcroft, and
Adrian K. Dixon. 2000. Communication
of doubt and certainty in radiological
reports. The British Journal of Radiology,
73(873):999?1001.
Huang, Yang and Henry J. Lowe. 2007.
A novel hybrid approach to automated
negation detection in clinical radiology
reports. Journal of the American Medical
Informatics Association, 14(3):304?311.
Karttunen, Lauri. 1973. Presuppositions and
compound sentences. Linguistic Inquiry,
4(2):169?193.
Karttunen, Lauri and Annie Zaenen.
2005. Veridicity. In Graham Katz, James
Pustejovsky, and Frank Schilder, editors,
Annotating, Extracting and Reasoning about
Time and Events, number 05151 in Dagstuhl
Seminar Proceedings, Dagstuhl, Germany.
Internationales Begegnungs- und
Forschungszentrum fu?r Informatik
(IBFI), Schloss Dagstuhl, Germany.
Kim, Jin-Dong, Tomoko Ohta, Sampo
Pyysalo, Yoshinobu Kano, and Jun?ichi
Tsujii. 2009. Overview of BioNLP?09
shared task on event extraction. In
Proceedings of the Workshop on BioNLP:
Shared Task, Boulder, Colorado, pages 1?9.
Kiparsky, Paul and Carol Kiparsky. 1970.
Facts. In M. Bierwisch and K. E. Heidolph,
editors, Progress in Linguistics. Mouton.
The Hague, Paris, pages 143?173.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In Proceedings of the 41st Meeting of the
Association of Computational Linguistics,
Sapporo, Japan, pages 423?430.
Levinson, Stephen C. 1995. Three levels of
meaning: Essays in honor of Sir John
Lyons. In Frank R. Palmer, editor, Grammar
and Meaning. Cambridge University Press,
Cambridge, pages 90?115.
Levinson, Stephen C. 2000. Presumptive
Meanings: The Theory of Generalized
Conversational Implicature. MIT Press,
Cambridge, MA.
Manning, Christopher D. 2006. Local textual
inference: it?s hard to circumscribe, but
you know it when you see it?and NLP
needs it. Ms., Stanford University.
Manning, Christopher D. and Dan Klein.
2003. Optimization, maxent models, and
conditional estimation without magic. In
Tutorial at HLT-NAACL 2003 and ACL 2003.
Available at http://nlp.stanford.edu/
software/classifier.shtml.
Miller, Mark Crispin. 2001. The Bush
Dyslexicon. W. W. Norton and Company,
New York, NY.
Montague, Richard. 1969. On the nature of
certain philosophical entities. The Monist,
volume 2, pages 159?194.
Morante, Roser and Walter Daelemans. 2009.
A metalearning approach to processing
the scope of negation. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning, Boulder,
Colorado, pages 21?29.
Morante, Roser and Caroline Sporleder,
editors. 2010. Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, Uppsala, Sweden.
Prabhakaran, Vinodkumar, Owen Rambow,
and Mona Diab. 2010. Automatic
committed belief tagging. In Proceedings
of COLING 2010: Poster Volume, Beijing,
China, pages 1014?1022.
Pustejovsky, James, Marc Verhagen, Roser
Saur??, Jessica Littman, Robert Gaizauskas,
Graham Katz, Inderjeet Mani, Robert
Knippen, and Andrea Setzer. 2006.
Timebank 1.2. Linguistic Data
Consortium, Philadelphia, PA.
Pyysalo, Sampo, Filip Ginter, Juho
Heimonen, Jari Bjo?rne, Jorma Boberg,
Jouni Ja?rvinen, and Tapio Salakoski.
2007. BioInfer: A corpus for information
extraction in the biomedical domain.
BMC Bioinformatics, 8(50). doi:10.1186/
1471-2105-8-50.
Rieh, Soo Young. 2010. Credibility and
cognitive authority of information.
In M. Bates and M. N. Maack, editors,
Encyclopedia of Library and Information
Sciences, 3rd ed., Taylor and Francis
Group, LLC, New York, pages 1337?1344.
Rooryck, Johan. 2001. Evidentiality, Part I.
Glot International, 5(4):3?11.
Ross, John Robert. 1973. Slifting. In Maurice
Gross, Morris Halle, and Marcel-Paul
Schu?tzenberger, editors, The Formal
Analysis of Natural Languages. Mouton de
Gruyter, The Hague, pages 133?169.
Rubin, Victoria L. 2007. Stating with
certainty or stating with doubt: Intercoder
reliability results for manual annotation
of epistemically modalized statements.
In Proceedings of the NAACL-HLT 2007,
Rochester, NY, pages 141?144.
Rubin, Victoria L., Elizabeth D. Liddy,
and Noriko Kando. 2005. Certainty
332
de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment
identification in texts: Categorization
model and manual tagging results.
In J. G. Shanahan, Y. Qu, and J. Wiebe,
editors, Computing Attitude and Affect
in Text: Theory and Applications (The
Information Retrieval Series), Springer-
Verlag, New York, pages 61?76.
S?bo, Kjell Johan. 2001. Necessary
conditions in a natural language. In
Caroline Fe?ry and Wolfgang Sternefeld,
editors, Audiatur Vox Sapientia. A Festschrift
for Arnim von Stechow. Akademie Verlag,
Berlin, pages 427?449.
Saur??, Roser. 2008. A Factuality Profiler
for Eventualities in Text. Ph.D. thesis,
Computer Science Department,
Brandeis University.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources
and Evaluation, 43(3):227?268.
Searle, John R. 1978. Literal meaning.
Erkenntnis, 13:207?224.
Simons, Mandy. 2007. Observations on
embedding verbs, evidentiality, and
presupposition. Lingua, 117(6):1034?1056.
Snow, Rion, Brendan O?Connor, Daniel
Jurafsky, and Andrew Y. Ng. 2008. Cheap
and fast?but is it good? Evaluating
non-expert annotations for natural
language tasks. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu,
Hawaii, pages 254?263.
Szarvas, Gyo?rgy, Veronika Vincze, Richa?rd
Farkas, and Ja?nos Csirik. 2008. The
BioScope corpus: annotation for negation,
uncertainty and their scope in biomedical
texts. In BioNLP 2008: Current Trends in
Biomedical Natural Language Processing,
Columbus, OH, pages 38?45.
Travis, Charles. 1996. Meaning?s role in
truth.Mind, 105(419):451?466.
von Fintel, Kai and Sabine Iatridou.
2008. How to say ought in foreign: The
composition of weak necessity modals.
In Jacqueline Gue?ron and Jacqueline
Lecarme, editors, Studies in Natural
Language and Linguistic Theory, volume 75.
Springer, Berlin, pages 115?141.
Wierzbicka, Anna. 1987. The semantics of
modality. Folia Linguistica, 21(1):25?43.
Zaenen, Annie, Lauri Karttunen, and Richard
Crouch. 2005. Local textual inference: Can
it be defined or circumscribed? In ACL
Workshop on Empirical Modelling of Semantic
Equivalence and Entailment, pages 31?36,
Ann Arbor, MI.
Zwarts, Frans. 1995. Nonveridical contexts.
Linguistic Analysis, 25(3?4):286?312.
333
Proceedings of NAACL-HLT 2013, pages 627?633,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
The Life and Death of Discourse Entities: Identifying Singleton Mentions
Marta Recasens
Linguistics Department
Stanford University
Stanford, CA 94305
recasens@google.com
Marie-Catherine de Marneffe
Linguistics Department
The Ohio State University
Columbus, OH 43210
mcdm@ling.osu.edu
Christopher Potts
Linguistics Department
Stanford University
Stanford, CA 94305
cgpotts@stanford.edu
Abstract
A discourse typically involves numerous en-
tities, but few are mentioned more than once.
Distinguishing discourse entities that die out
after just one mention (singletons) from those
that lead longer lives (coreferent) would ben-
efit NLP applications such as coreference res-
olution, protagonist identification, topic mod-
eling, and discourse coherence. We build a lo-
gistic regression model for predicting the sin-
gleton/coreferent distinction, drawing on lin-
guistic insights about how discourse entity
lifespans are affected by syntactic and seman-
tic features. The model is effective in its own
right (78% accuracy), and incorporating it into
a state-of-the-art coreference resolution sys-
tem yields a significant improvement.
1 Introduction
Not all discourse entities are created equal. Some
lead long lives and appear in a variety of discourse
contexts (coreferent), whereas others never escape
their birthplaces, dying out after just one mention
(singletons). The ability to make this distinction
based on properties of the NPs used to identify these
referents (mentions) would benefit not only corefer-
ence resolution, but also topic analysis, textual en-
tailment, and discourse coherence.
The existing literature provides numerous gen-
eralizations relevant to answering the question of
whether a given discourse entity will be singleton
or coreferent. These involve the internal syntax and
morphology of the target NP (Prince, 1981a; Prince,
1981b; Wang et al, 2006), the grammatical function
and discourse role of that NP (Chafe, 1976; Hobbs,
1979; Walker et al, 1997; Beaver, 2004), and the in-
teraction of all of those features with semantic oper-
ators like negation, modals, and attitude predicates
(Karttunen, 1973; Karttunen, 1976; Kamp, 1981;
Heim, 1982; Heim, 1992; Roberts, 1990; Groe-
nendijk and Stokhof, 1991; Bittner, 2001).
The first step in our analysis is to bring these
insights together into a single logistic regression
model ? the lifespan model ? and assess their
predictive power on real data. We show that the
features generally behave as the existing literature
leads us to expect, and that the model itself is highly
effective at predicting whether a given mention is
singleton or coreferent. We then provide an initial
assessment of the engineering value of making the
singleton/coreferent distinction by incorporating our
lifespan model into the Stanford coreference resolu-
tion system (Lee et al, 2011). This addition results
in a significant improvement on the CoNLL-2012
Shared Task data, across the MUC, B3, CEAF, and
CoNLL scoring algorithms.
2 Data
All the data used throughout the paper come from
the CoNLL-2012 Shared Task (Pradhan et al,
2012), which included the 1.6M English words from
OntoNotes v5.0 (Hovy et al, 2006) that have been
annotated with different layers of annotation (coref-
erence, parse trees, etc.). We used the training, de-
velopment (dev), and test splits as defined in the
shared task (Table 1). Since the OntoNotes corefer-
ence annotations do not contain singleton mentions,
we automatically marked as singletons all the NPs
627
MENTIONS
Dataset Docs Tokens Coreferent Singletons
Training 2,802 1.3M 152,828 192,248
Dev 343 160K 18,815 24,170
Test 348 170K 19,392 24,921
Table 1: CoNLL-2012 Shared Task data statistics. We
added singletons (NPs not annotated as coreferent).
not annotated as coreferent. Thus, our singletons in-
clude non-referential NPs but not verbal mentions.
3 Predicting lifespans
Our lifespan model makes a binary distinction be-
tween discourse referents that are not part of a coref-
erence chain (singletons) and items that are part of
one (coreferent). The distribution of lifespans in our
data (Figure 1) suggests that this is a natural divi-
sion. The propensity of singletons also highlights
the relevance of detecting singletons for a coref-
erence system. We fit a binary logistic regression
model in R (R Core Team, 2012) on the training
data, coding singletons as ?0? and coreferent men-
tions as ?1?. Throughout the following tables of co-
efficient estimates, positive values favor coreferents
and negative ones favor singletons. We turn now to
describing and motivating the features of this model.
Singleton 2 3 4 5 6-10 11-15 16-20 >20
0
5K
15
K
25
K
Figure 1: Distribution of lifespans in the dev set. Single-
tons account for 56% of the data.
Internal morphosyntax of the mention Table 2
summarizes the features from our model that con-
cern the internal morphology and syntactic structure
of the mention. Many are common in coreference
systems (Recasens and Hovy, 2009), but our model
highlights their influence on lifespans. The picture
is expected on the taxonomy of given and new de-
fined by Prince (1981b) and assumed throughout dy-
namic semantics (Kamp, 1981; Heim, 1982): pro-
nouns depend on anaphoric connections to previous
mentions for disambiguation and thus are very likely
to be coreferent. This is corroborated by the pos-
itive coefficient estimate for ?Type = pronoun? in
Table 2. Few quantified phrases easily participate
in discourse anaphora (Partee, 1987; Wang et al,
2006), accounting for the association between quan-
tifiers and singletons (negative coefficient estimate
for ?Quantifier = quantified? in Table 2). The one
surprise is the negative coefficient for indefinites. In
theories stretching back to Karttunen (1976), indef-
inites function primarily to establish new discourse
entities, and should be able to participate in coref-
erence chains, but here the association with such
chains is negative. However, interactions explain
this fact (see Table 4 and our discussion of it).
The person, number, and animacy values suggest
that singular animates are excellent coreferent NPs,
a previous finding of Centering Theory (Grosz et al,
1995; Walker et al, 1998) and of cross-linguistic
work on obviative case-marking (Aissen, 1997).
Our model also includes named-entity features for
all of the eighteen OntoNotes entity-types (omitted
from Table 2 for space and clarity reasons). As a
rule, they behave like ?Type = proper noun? in asso-
ciating with coreferents. The exceptions are ORDI-
NAL, PERCENT, and QUANTITY, which seem intu-
itively unlikely to participate in coreference chains.
Estimate P-value
Type = pronoun 1.21 < 0.001
Type = proper noun 1.88 < 0.001
Animacy = inanimate ?1.36 < 0.001
Animacy = unknown ?0.38 < 0.001
Person = 1 1.05 < 0.001
Person = 2 0.13 < 0.001
Person = 3 1.62 < 0.001
Number = singular 0.61 < 0.001
Number = unknown 0.17 < 0.001
Quantifier = indefinite ?1.49 < 0.001
Quantifier = quantified ?1.23 < 0.001
Number of modifiers ?0.39 < 0.001
Table 2: Internal morphosyntactic features.
Grammatical role of the mention Synthesizing
much work in Centering Theory and information
structuring, we conclude that coreferent mentions
are likely to appear as core verbal arguments and
will favor sentence-initial (topic-tracking) positions
(Ward and Birner, 2004). The coefficient estimates
628
Estimate P-value
Sentence Position = end ?0.22 < 0.001
Sentence Position = first 0.04 0.07
Sentence Position = last ?0.31 < 0.001
Sentence Position = middle ?0.11 < 0.001
Relation = noun argument 0.56 < 0.001
Relation = other ?0.67 < 0.001
Relation = root ?0.61 < 0.001
Relation = subject 0.65 < 0.001
Relation = verb argument 0.32 < 0.001
In coordination ?0.48 < 0.001
Table 3: Grammatical role features.
in Table 3 corroborate these conclusions. To de-
fine the ?Relation? and ?In coordination? features, we
used the Stanford dependencies (de Marneffe et al,
2006) on the gold constituents.
Semantic environment of the mention Table 4
highlights the complex interactions between dis-
course anaphora and semantic operators. These
interactions have been a focus of logical seman-
tics since Karttunen (1976), whose guiding obser-
vation is semantic: an indefinite interpreted inside
the scope of a negation, modal, or attitude predicate
is generally unavailable for anaphoric reference out-
side of the scope of that operator, as in Kim didn?t
understand [an exam question]i. #Iti was too hard.
Of course, such discourses cohere if the indefinite
is interpreted as taking wide scope (?there is a ques-
tion Kim didn?t understand?). Such readings are of-
ten disfavored, but they become more salient when
modifiers like certain are included (Schwarzschild,
2002) or when the determiner is sensitive to the po-
larity or intensionality of its environment (Baker,
1970; Ladusaw, 1980; van der Wouden, 1997; Is-
rael, 1996; Israel, 2001; Giannakidou, 1999). Sub-
sequent research identified many other factors that
further extend or restrict the anaphoric potential of
an indefinite (Roberts, 1996).
We do not have direct access to semantic scope,
but we expect syntactic scope to correlate strongly
with semantic scope, so we used dependency rep-
resentations to define features capturing syntactic
scope for negation, modal auxiliaries, and a broad
range of attitude predicates. These features tend to
bias in favor of singletons because they so radically
restrict the possibilities for intersentential anaphora.
Interacting these features with those for the inter-
nal syntax of mentions is also informative. Since
proper names and pronouns are not scope-taking,
they are largely unaffected by the environment fea-
tures, whereas indefinites emerge as even more re-
stricted, just as Karttunen and others would predict.
Attitude predicates seem initially anomalous,
though. They share the relevant semantic proper-
ties with negation and modals, and yet they seem
to facilitate coreference. Here, the findings of de
Marneffe et al (2012) seem informative. Those au-
thors find that, in texts of the sort we are studying,
attitude predicates are used predominantly to mark
the source of information that is effectively asserted
despite being embedded (Rooryck, 2001; Simons,
2007). That is, though X said p does not semanti-
cally entail p, it is often interpreted as a commitment
to p, which correspondingly elevates mentions in p
to main-clause status (Harris and Potts, 2009).
Estimate P-value
Presence of negation ?0.18 < 0.001
Presence of modality ?0.22 < 0.001
Under an attitude verb 0.03 0.01
AttitudeVerb * (Type = pronoun) 0.29 < 0.001
AttitudeVerb * (Type = proper noun) 0.14 < 0.001
Modal * (Type = pronoun) 0.12 0.04
Modal * (Type = proper noun) 0.35 < 0.001
Negation * (Type = pronoun) 1.07 < 0.001
Negation * (Type = proper noun) 0.30 < 0.001
Negation * (Quantifier = indefinite) ?0.37 < 0.001
Negation * (Quantifier = quantified) ?0.36 0.23
Negation * (Number of modifiers) 0.11 < 0.001
Table 4: Semantic environment features and interactions.
Results The model successfully learns to tease
singletons and coreferent mentions apart. Table 5
summarizes its performance on the dev set. The
STANDARD model uses 0.5 as the decision bound-
ary, with 78% accuracy. The CONFIDENT model
predicts singleton if Pr < .2 and coreferent if Pr > .8,
which increases precision (P) at a cost to recall (R).
STANDARD CONFIDENT
Prediction R P F1 R P F1
Singleton 82.3 79.2 80.7 50.5 89.6 64.6
Coreferent 72.2 76.1 74.1 41.3 86.8 55.9
Table 5: Recall, precision, and F1 for the lifespan model.
629
MUC B3 CEAF-?3 CEAF-?4 CoNLL
System R P F1 R P F1 R / P / F1 R P F1 F1
Baseline 66.64* 64.72 65.67 68.05* 71.58 69.77* 58.31 45.49 47.55* 46.50 60.65
w/ Lifespan 66.08 67.33* 66.70* 66.40 73.14* 69.61 58.83* 47.77* 46.38 47.07* 61.13*
Table 6: Performance on the test set according to the official CoNLL-2012 scorer. Scores are on automatically pre-
dicted mentions. Stars indicate a statistically significant difference (paired Mann-Whitney U-test, p < 0.05).
B3 CEAF-?3 CoNLL
System R P F1 R P F1 F1
Baseline 58.53* 71.58 64.40 63.71* 58.31 60.89 58.86
w/ Lifespan 58.14 73.14* 64.78* 63.38 58.83* 61.02 59.52*
Table 7: B3, CEAF-?3 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that follows
Cai and Strube (2010). Scores are on automatically predicted mentions.
4 Application to coreference resolution
To assess the usefulness of the lifespan model in an
NLP application, we incorporate it into the Stanford
coreference resolution system (Lee et al, 2011),
which we take as our baseline. This was the highest-
scoring system in the CoNLL-2011 Shared Task,
and was also part of the highest-scoring system in
the CoNLL-2012 Shared Task (Fernandes et al,
2012). It is a rule-based system that includes a to-
tal of ten rules (or ?sieves?) for entity coreference,
such as exact string match and pronominal resolu-
tion. The sieves are applied from highest to lowest
precision, each rule adding coreference links.
Incorporating the lifespan model The lifespan
model can improve coreference resolution in two
different ways: (i) mentions classified as singletons
should not be considered as either antecedents or
coreferent, and (ii) mentions classified as coreferent
should be linked with another mention(s). By suc-
cessfully predicting singletons (i), we can enhance
the system?s precision; by successfully predicting
coreferent mentions (ii), we can improve the sys-
tem?s recall. Here we focus on (i) and use the lifes-
pan model for detecting singletons. This decision
is motivated by two factors. First, given the large
number of singletons (Figure 1), we are more likely
to see a gain in performance from discarding sin-
gletons. Second, the multi-sieve nature of the Stan-
ford coreference system does not make it straightfor-
ward to decide which antecedent a mention should
be linked to even if we know that it is coreferent.
We leave the incorporation of coreferent predictions
for future work.
To integrate the singleton model into the Stanford
coreference system, we let a sieve consider whether
a pair of mentions is coreferent only if neither of
the two mentions are classified as singletons by our
CONFIDENT model. Experiments on the dev set
showed that the model often made wrong predic-
tions for NEs. We do not trust the model for NE
mentions. Performance on coreference (on the dev
set) was higher with the CONFIDENT model than
with the STANDARD model.
Results and discussion To evaluate the corefer-
ence system with and without the lifespan model, we
used the English dev and test sets from the CoNLL-
2012 Shared Task, presented in Section 2. Although
the CoNLL shared task evaluated systems on only
multi-mention (i.e., non-singleton) entities, by stop-
ping singletons from being linked to multi-mention
entities, we expected the lifespan model to increase
the system?s precision. Our evaluation uses five
of the measures given by the CoNLL-2012 scorer:
MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,
1998), CEAF-?3 and CEAF-?4 (Luo, 2005), and the
CoNLL official score (Denis and Baldridge, 2009).
We do not include BLANC (Recasens and Hovy,
2011) because it assumes gold mentions and so is
not suited for the scenario considered in this paper,
which uses automatically predicted mentions.
Table 6 summarizes the test set performance. All
the scores are on automatically predicted mentions.
We use gold POS, parse trees, and NEs. The base-
630
line is the Stanford system, and ?w/ Lifespan? is the
same system extended with our lifespan model to
discard singletons, as explained above.
As expected, the lifespan model increases preci-
sion but decreases recall. Overall, however, we ob-
tain a significant improvement of 0.5?1 points in the
F1 score of MUC, CEAF-?3, CEAF-?4 and CoNLL.
The drop in B3 traces to a bug in the CoNLL scorer?s
implementation of Cai and Strube (2010)?s algo-
rithm for aligning gold and automatically predicted
mentions, which affects the computation of B3 and
CEAF-?3.1 Table 7 presents the results after mod-
ifying the CoNLL-2012 scorer to compute B3 and
CEAF-?3 according to Cai and Strube (2010).2 We
do see an improvement in the precision and F1
scores of B3, and the overall CoNLL score remains
significant. The CEAF-?3 F1 score is no longer sig-
nificant, but is still in the expected direction.
5 Conclusion
We built a model to predict the lifespan of discourse
referents, teasing apart singletons from coreferent
mentions. The model validates existing linguistic
insights and performs well in its own right. This
alone has ramifications for tracking topics, identify-
ing protagonists, and modeling coreference and dis-
course coherence. We applied the lifespan model to
coreference resolution, showing how to incorporate
it effectively into a state-of-the-art rule-based coref-
erence system. We expect similar improvements
with machine-learning-based coreference systems,
where incorporating all the power of the lifespan
model would be easier.
Our lifespan model has been integrated into the
latest version of the Stanford coreference resolution
system.3
1At present, if the system links two mentions that do not
exist in the gold standard, the scorer adds two singletons to the
gold standard. This results in a higher B3 F1 score (when it
should be lower) because recall increases instead of staying the
same (precision goes up).
2In the modified scorer, twinless predicted mentions are
added to the gold standard to compute precision but not to com-
pute recall.
3http://nlp.stanford.edu/software/
dcoref.shtml
Acknowledgments
We thank Emili Sapena for modifying the CoNLL-
2012 scorer to follow Cai and Strube (2010).
This research was supported in part by ONR
grant No. N00014-10-1-0109 and ARO grant
No. W911NF-07-1-0216. The first author was sup-
ported by a Beatriu de Pino?s postdoctoral schol-
arship (2010 BP-A 00149) from Generalitat de
Catalunya.
References
Judith Aissen. 1997. On the syntax of obviation. Lan-
guage, 73(4):705?750.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
C. L. Baker. 1970. Double negatives. Linguistic Inquiry,
1(2):169?186.
David Beaver. 2004. The optimization of discourse
anaphora. Linguistics and Philosophy, 27(1):3?56.
Maria Bittner. 2001. Surface composition as bridging.
Journal of Semantics, 18(2):127?177.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of SIGDIAL 2010, pages 28?36.
Wallace L. Chafe. 1976. Givenness, Contrastiveness,
Definiteness, Subjects, Topics, and Point of View. In
Charles N. Li, editor, Subject and Topic, pages 25?55.
Academic Press, New York.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC 2006.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2012. Did it happen?
The pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301?333.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Pro-
ceedings of CoNLL-2012: Shared Task, pages 41?48.
Anastasia Giannakidou. 1999. Affective dependencies.
Linguistics and Philosophy, 22(4):367?421.
Jeroen Groenendijk and Martin Stokhof. 1991. Dynamic
predicate logic. Linguistics and Philosophy, 14(1):39?
100.
631
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jesse A. Harris and Christopher Potts. 2009.
Perspective-shifting with appositives and expressives.
Linguistics and Philosophy, 32(6):523?552.
Irene Heim. 1982. The Semantics of Definite and Indefi-
nite Noun Phrases. Ph.D. thesis, UMass Amherst.
Irene Heim. 1992. Presupposition projection and the
semantics of attitude verbs. Journal of Semantics,
9(2):183?221.
Jerry R. Hobbs. 1979. Coherence and coreference. Cog-
nitive Science, 3(1):67?90.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT-
NAACL 2006, pages 57?60.
Michael Israel. 1996. Polarity sensitivity as lexical se-
mantics. Linguistics and Philosophy, 19(6):619?666.
Michael Israel. 2001. Minimizers, maximizers, and the
rhetoric of scalar reasoning. Journal of Semantics,
18(4):297?331.
Hans Kamp. 1981. A theory of truth and discourse
representation. In Jeroen Groenendijk, Theo M. V.
Janssen, and Martin Stockhof, editors, Formal Meth-
ods in the Study of Language, pages 277?322. Mathe-
matical Centre, Amsterdam.
Lauri Karttunen. 1973. Presuppositions and compound
sentences. Linguistic Inquiry, 4(2):169?193.
Lauri Karttunen. 1976. Discourse referents. In James D.
McCawley, editor, Syntax and Semantics, volume 7:
Notes from the Linguistic Underground, pages 363?
385. Academic Press, New York.
William A. Ladusaw. 1980. On the notion ?affective?
in the analysis of negative polarity items. Journal of
Linguistic Research, 1(1):1?16.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of CoNLL-2011: Shared Task, pages 28?34.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25?32.
Barbara H. Partee. 1987. Noun phrase interpretation
and type-shifting principles. In Jeroen Groenendijk,
Dick de Jong, and Martin Stokhof, editors, Studies in
Discourse Representation Theory and the Theory of
Generalized Quantifiers, pages 115?143. Foris Publi-
cations, Dordrecht.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of EMNLP
and CoNLL-2012: Shared Task, pages 1?40.
Ellen Prince. 1981a. On the inferencing of indefi-
nite ?this? NPs. In Bonnie Lynn Webber, Ivan Sag,
and Aravind Joshi, editors, Elements of Discourse Un-
derstanding, pages 231?250. Cambridge University
Press, Cambridge.
Ellen F. Prince. 1981b. Toward a taxonomy of given?
new information. In Peter Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press, New York.
R Core Team, 2012. R: A Language and Environment
for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria.
Marta Recasens and Eduard Hovy. 2009. A deeper
look into features for coreference resolution. In Sobha
Lalitha Devi, Anto?nio Branco, and Ruslan Mitkov, ed-
itors, Anaphora Processing and Applications, volume
5847 of Lecture Notes in Computer Science, pages 29?
42. Springer.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Craige Roberts. 1990. Modal Subordination, Anaphora,
and Distributivity. Garland, New York.
Craige Roberts. 1996. Anaphora in intensional contexts.
In Shalom Lappin, editor, The Handbook of Contem-
porary Semantic Theory, pages 215?246. Blackwell
Publishers, Oxford.
Johan Rooryck. 2001. Evidentiality, Part II. Glot Inter-
national, 5(5):161?168.
Roger Schwarzschild. 2002. Singleton indefinites. Jour-
nal of Semantics, 19(3):289?314.
Mandy Simons. 2007. Observations on embedding
verbs, evidentiality, and presupposition. Lingua,
117(6):1034?1056.
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity and Multiple Negation. Routledge,
London and New York.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince,
editors. 1997. Centering in Discourse. Oxford Uni-
versity Press.
Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince.
1998. Centering in naturally-occurring discourse: An
overview. In Marilyn A. Walker, Aravind K. Joshi,
and Ellen F. Prince, editors, Centering Theory in Dis-
course, pages 1?28, Oxford. Clarendon Press.
Linton Wang, Eric McCready, and Nicholas Asher. 2006.
Information dependency in quantificational subordina-
tion. In Klaus von Heusinger and Ken Turner, editors,
632
Where Semantics Meets Pragmatics, pages 267?304.
Elsevier Science, Amsterdam.
Gregory Ward and Betty Birner. 2004. Information
structure and non-canonical syntax. In Laurence R.
Horn and Gregory Ward, editors, The Handbook of
Pragmatics, pages 153?174. Blackwell, Oxford.
633
Proceedings of NAACL-HLT 2013, pages 1072?1081,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Emergence of Gricean Maxims from Multi-Agent Decision Theory
Adam Vogel, Max Bodoia, Christopher Potts, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{acvogel,mbodoia,cgpotts,jurafsky}@stanford.edu
Abstract
Grice characterized communication in terms
of the cooperative principle, which enjoins
speakers to make only contributions that serve
the evolving conversational goals. We show
that the cooperative principle and the associ-
ated maxims of relevance, quality, and quan-
tity emerge from multi-agent decision theory.
We utilize the Decentralized Partially Observ-
able Markov Decision Process (Dec-POMDP)
model of multi-agent decision making which
relies only on basic definitions of rationality
and the ability of agents to reason about each
other?s beliefs in maximizing joint utility. Our
model uses cognitively-inspired heuristics to
simplify the otherwise intractable task of rea-
soning jointly about actions, the environment,
and the nested beliefs of other actors. Our
experiments on a cooperative language task
show that reasoning about others? belief states,
and the resulting emergent Gricean commu-
nicative behavior, leads to significantly im-
proved task performance.
1 Introduction
Grice (1975) famously characterized communica-
tion among rational agents in terms of an overarch-
ing cooperative principle and a set of more specific
maxims, which enjoin speakers to make contribu-
tions that are truthful, informative, relevant, clear,
and concise. Since then, there have been many at-
tempts to derive the maxims (or perhaps just their ef-
fects) from more basic cognitive principles concern-
ing how people make decisions, formulate plans,
and collaborate to achieve goals. This research
traces to early work by Lewis (1969) on signaling
systems. It has recently been the subject of ex-
tensive theoretical discussion (Clark, 1996; Merin,
1997; Blutner, 1998; Parikh, 2001; Beaver, 2002;
van Rooy, 2003; Benz et al, 2005; Franke, 2009)
and has been tested experimentally using one-step
games in which the speaker produces a message and
the hearer ventures a guess as to its intended refer-
ent (Rosenberg and Cohen, 1964; Dale and Reiter,
1995; Golland et al, 2010; Stiller et al, 2011; Frank
and Goodman, 2012; Krahmer and van Deemter,
2012; Degen and Franke, 2012; Rohde et al, 2012).
To date, however, these theoretical models and ex-
periments have not been extended to multi-step in-
teractions extending over time and involving both
language and action together, which leaves this work
relatively disconnected from research on planning
and goal-orientation in artificial agents (Perrault
and Allen, 1980; Allen, 1991; Grosz and Sidner,
1986; Bratman, 1987; Hobbs et al, 1993; Allen
et al, 2007; DeVault et al, 2005; Stone et al,
2007; DeVault, 2008). We attribute this in large
part to the complexity of Gricean reasoning itself,
which requires agents to model each other?s belief
states. Tracking these as they evolve over time in re-
sponse to experiences is extremely demanding. Our
approach complements slot-filling dialog systems,
where the focus is on managing speech recogni-
tion uncertainty (Young et al, 2010; Thomson and
Young, 2010).
However, recent years have seen significant ad-
vances in multi-agent decision-theoretic models and
their efficient implementation. With the current pa-
per, we seek to show that the Decentralized Par-
1072
tially Observable Markov Decision Process (Dec-
POMDP) provides a robust, flexible foundation for
implementing agents that communicate in a Gricean
manner. Dec-POMDPs are multi-agent, partially-
observable models in which agents maintain be-
lief distributions over the underlying, hidden world
state, including the beliefs of the other players, and
speech actions change those beliefs. In this setting,
informative, relevant communication emerges as the
best way to maximize joint utility.
The complexity of pragmatic reasoning is still
forbidding, though. Correspondingly, optimal de-
cision making in Dec-POMDPs is NEXP complete
(Bernstein et al, 2002). To manage this issue, we
introduce several cognitively-plausible approxima-
tions which allow us to simplify the Dec-POMDP to
a single-agent POMDP, for which relatively efficient
solvers exist (Spaan and Vlassis, 2005). We demon-
strate our algorithms on a variation of the Cards task,
a partially-observable collaborative search problem
(Potts, 2012). Spatial language comprises the bulk
of communication in the Cards task, and we dis-
cuss a model of spatial semantics in Section 3. Us-
ing this task and a model of the meaning of spatial
language, we next discuss two agents that play the
game: ListenerBot (Section 4) makes decisions us-
ing a single-agent POMDP that does not take into
account the beliefs or actions of its partner, whereas
DialogBot (Section 5) maintains a model of its part-
ner?s beliefs. As a result of the cooperative structure
of the underlying model and the effects of commu-
nication within it, DialogBot?s contributions are rel-
evant, truthful, and informative, which leads to sig-
nificantly improved task performance.
2 The Cards Task and Corpus
The Cards corpus consists of 1,266 transcripts1 from
an online, two-person collaborative game in which
two players explore a maze-like environment, com-
municating with each other via a text chat window
(Figure 1). A deck of playing cards has been dis-
tributed randomly around the environment, and the
players? task is to find six consecutive cards of the
same suit. Our implemented agents solve a sim-
plified version of this task in which the two agents
1Released by Potts (2012) at http://cardscorpus.
christopherpotts.net
Figure 1: The Cards corpus gameboard. Player 1?s
location is marked ?P1?. The nearby yellow boxes
mark card locations. The dialogue history and chat
window are at the top. This board, the one we use
throughout, consists of 231 open grid squares.
must both end up co-located with a single card, the
Ace of Spades (AS). This is much simpler than the
six-card version from the human?human corpus, but
it involves the same kind of collaborative goal and
forces our agents to deal with the same kind of par-
tial knowledge about the world as the humans did.
Each agent knows its own location, but not his part-
ner?s, and a player can see the AS only when co-
located with it. The agents use (simplified) English
to communicate with each other.
3 Spatial Semantics
Much of the communication in the Cards task in-
volves referring to spatial locations on the board.
Accordingly, we focus on spatial language for our
artificial agents. In this section, we present a model
of spatial semantics, which we create by leveraging
the human?human Cards transcripts. We discuss the
spatial semantic representation, how we classify the
semantics of new locative expressions, and our use
of spatial semantics to form a high-level state space
for decision making.
3.1 Semantic Representation
Potts (2012) released annotations, derived from the
Cards corpus, which reduce 599 of the players?
statements about their locations to formulae of the
form ? (?1 ? ?? ? ? ?k), where ? is a domain and
?1, . . . ,?k are semantic literals. For example, the ut-
terance ?(I?m) at the top right of the board? is anno-
tated as BOARD(top? right), and ?(I?m) in bottom
1073
of the C room? is annotated as C room(bottom). Ta-
ble 1 lists the full set of semantic primitives that ap-
pear as domain expressions and literals.
Because the Cards transcripts are so highly struc-
tured, we can interpret these expressions in terms
of the Cards world itself. For a given formula
? = ? (?1 ? ?? ? ? ?k), we compute the number of
times that a player identified its location with (an
utterance translated as) ? while standing on grid
square (x,y). These counts are smoothed using
a simple 2D-smoothing scheme, detailed in (Potts,
2012), and normalized in the usual manner to form a
distribution over board squares Pr((x,y)|?). These
grounded interpretations are the basis for commu-
nication between the artificial agents we define in
Section 4.
BOARD, SQUARE, right, middle, top, left, bot-
tom, corner, approx, precise, entrance, C room,
hall, room, sideways C, loop, reverse C,
U room, T room, deadend, wall, sideways F
Table 1: The spatial semantic primitives.
3.2 Semantics Classifier
Using the corpus examples of utterances paired with
their spatial semantic representations, we learn a set
of classifiers to predict a spatial utterance?s semantic
representation. We train a binary classifier for each
semantic primitive ?i using a log-linear model with
simple bag of words features. The words are not
normalized or stemmed and we use whitespace tok-
enization. We additionally train a multi-class clas-
sifier for all possible domains ? . At test time, we
use the domain classifier and each primitive binary
classifier to produce a semantic representation.
3.3 Semantic State Space
The decision making algorithms that we discuss in
Section 4 are highly sensitive to the size of the state
space. The full representation of the game board
consists of 231 squares. Representing the location
of both players and the location of the card requires
3233 = 12,326,391 states, well beyond the capabil-
ities of current decision-making algorithms.
To ameliorate this difficulty, we cluster squares
together using the spatial referring expression cor-
Figure 2: Semantic state space clusters with k = 16.
pus. This approach follows from research that
shows that humans? mental spatial representations
are influenced by their language (Hayward and Tarr,
1995). Our intuition is that human players do not
consider all possible locations of the card and play-
ers, but instead lump them into semantically coher-
ent states, such as ?the card is in the top right cor-
ner.? Following this intuition, we cluster states to-
gether which have similar referring expressions, al-
lowing our agents to use language as a cognitive
technology and not just a tool for communication.
For each board square (x,y) we form a vector
?(x,y) with ?i(x,y) = Pr((x,y)|?i), where ?i is the
ith distinct semantic representation in the corpus.
This forms a 136-dimensional vector for each board
square. We then use k-means clustering with a Eu-
clidean distance metric in this semantic space to
cluster states which are referred to similarly.
Figure 2 shows a clustering for k = 16 which we
utilize for the remainder of the paper. Denoting the
board regions by {1, . . . ,Nregions}, we compute the
probability of an expression ? referring to a region
r by averaging over the squares in the region:
Pr(r|?i) ? ?
(x,y)? region r
Pr((x,y)|?i)
|{(x,y)|(x,y) ? region r}|
4 ListenerBot
We first introduce ListenerBot, an agent that does
not take into account the actions or beliefs of its
partner. ListenerBot decides what actions to take
using a Partially Observable Markov Decision Pro-
cess (POMDP). This allows ListenerBot to track its
beliefs about the location of the card and to incor-
porate linguistic advice. However, ListenerBot does
not produce utterances.
1074
A POMDP is defined by a tuple
(S,A,T,O,?,R,b0,?). We explicate each com-
ponent with examples from our task. Figure 3(a)
provides the POMDP influence diagram.
States S is the finite state space of the world. The
state space S of ListenerBot consists of the location
of the player p and the location of the card c. As
discussed above in Section 3.3, we cluster squares of
the board into Nregions semantically coherent regions,
denoted by {1, . . . ,Nregions}. The state space over
these regions is defined as
S := {(p,c)|p,c ? {1, . . . ,Nregions}}
Two regions r1 and r2 are called adjacent, written
adj(r1,r2), if any of their constituent squares touch.
Actions A is the set of actions available to the
agent. ListenerBot can only take physical actions
and has no communicative ability. Physical actions
in our region-based state space are composed of two
types: traveling to a region and searching a region.
? travel(r): travel to region r
? search: player exhaustively searches the cur-
rent region
Transition Distributions The transition distribu-
tion T (s?|a,s) models the dynamics of the world.
This represents the ramifications of physical actions
such as moving around the map. For a state s =
(p,c) and action a = travel(r), the player moves to
region r if it is adjacent to p, and otherwise stays in
the same place:
T ((p?,c?)|travel(r),(p,c))=
?
???????
???????
1 adj(r, p)? p? = r
?c = c?
1 ?adj(r, p)? p = p?
?c = c?
0 otherwise
Search actions are only concerned with observations
and do not change the state of the world:2
T ((p?,c?)|search,(p,c)) = 1
[
p? = p? c? = c
]
The travel and search high-level actions are trans-
lated into low-level (up, down, left, right) actions
using a simple A? path planner.
Observations Agents receive observations from
a set O according to an observation distribution
2
1[Q] is the indicator function, which is 1 if proposition Q
is true and 0 otherwise.
?(o|s?,a). Observations include properties of the
physical world, such as the location of the card, and
also natural language utterances, which serve to in-
directly change agents? beliefs about the world and
the beliefs of their interlocutors.
Search actions generate two possible observa-
tions: ohere and o?here, which denote the presence
or absence of the card from the current region.
?(ohere|(p
?,c?),search) = 1
[
p? = c?
]
?(o?here|(p
?,c?),search) = 1
[
p? 6= c?
]
Travel actions do not generate meaningful observa-
tions:
?(o?here|(p
?,c?), travel) = 1
Linguistic Advice We model linguistic advice as
another form of observation. Agents receive mes-
sages from a finite set ?, and each message ? ? ?
has a semantics, or distribution over the state space
Pr(s|?). In the Cards task, we use the semantic dis-
tributions defined in Section 3. To combine the se-
mantics of language with the standard POMDP ob-
servation model, we apply Bayes? rule:
Pr(? |s) = Pr(s|?)Pr(?)
?? ? Pr(s|? ?)Pr(? ?)
(1)
The prior, Pr(?), can be derived from corpus data.
By treating language as just another form of ob-
servation, we are able to leverage existing POMDP
solution algorithms. This approach contrasts with
previous work on communication in Dec-POMDPs,
where agents directly share their perceptual obser-
vations (Pynadath and Tambe, 2002; Spaan et al,
2008), an assumption which does not fit natural lan-
guage.
Reward The reward function R(s,a) : S? R rep-
resents the goals of the agent, who chooses actions
to maximize reward. The goal of the Cards task is
for both players to be on top of the card, so any ac-
tion that leads to this state receives a high reward R+.
All other actions receive a small negative reward R?,
which gives agents an incentive to finish the task as
quickly as possible.
R((p,c),a) =
{
R+ p = c
R? p 6= c
Lastly, ? ? [0,1) is the discount factor, specifying
the trade-off between immediate and future rewards.
1075
s s?
o o?a
R
(a) ListenerBot POMDP
s s?
o1 o?1
o2 o?2
a1
a2
R
(b) Full Dec-POMDP
s s?
o o?a
R
s? s??
(c) DialogBot POMDP
Figure 3: The decision diagram for the ListenerBot POMDP, the full Dec-POMDP, and the DialogBot ap-
proximation POMDP. The ListenerBot (a) only considers his own location p and the card location c. In the
full Dec-POMDP (b), both agents receive individual observations and choose actions independently. Opti-
mal decision making requires tracking all possible histories of beliefs of the other agent. In diagram (c), Di-
alogBot approximates the full Dec-POMDP as single-agent POMDP. At each time step, DialogBot marginal-
izes out the possible observations o? that ListenerBot received, yielding an expected belief state b?.
Initial Belief State The initial belief state, b0 ?
?(S), is a distribution over the state space S. Lis-
tenerBot begins each game with a known initial lo-
cation p0 but a uniform distribution over the location
of the card c:
b0(p,c) ?
{
1
Nregions
p = p0
0 otherwise
Belief Update and Decision Making The key de-
cision making problem in POMDPs is the construc-
tion of a policy pi : ?(S)? A, a function from beliefs
to actions which dictates how the agent acts. Deci-
sion making in POMDPs proceeds as follows. The
world starts in a hidden state s0 ? b0. The agent
executes action a0 = pi(b0). The underlying hid-
den world state transitions to s1 ? T (s?|a0,s0), the
world generates observation o0 ? ?(o|s1,a0), and
the agent receives reward R(s0,a0). Using the obser-
vation o0, the agent constructs a new belief b1 ??(S)
using Bayes? rule:
bat ,ott+1 (s
?) = Pr(s?|at ,ot ,bt)
=
Pr(ot |at ,s?,bt)Pr(s?|at ,bt)
Pr(ot |bt ,at)
=
?(ot |s?,at)?s?S T (s
?|at ,s)bt(s)
?s???(ot |s??,at)?s?S T (s??|at ,s)bt(s)
This process is referred to as belief update and is
analogous to the forward algorithm in HMMs. To in-
corporate communication into the standard POMDP
model, we consider observations (o,?) ? O ? ?
which are a combination of a perceptual observation
o and a received message ? . The semantics of the
message ? is included in the belief update equation
using Pr(s|?), derived in Equation 1:
bat ,ot ,?tt+1 (s
?) =
?(o|s?,a) Pr(s
?|?)Pr(?)
?? ??? Pr(s
?|? ?)Pr(? ?) ?s?S T (s
?|a,s)bt(s)
?s???S?(o|s??,a)
Pr(s??|?)Pr(?)
?? ??? Pr(s
??|? ?)Pr(? ?) ?s?S T (s
??|a,s)bt(s)
Using this new belief state b1, the agent selects an
action a1 = pi(b1), and the process continues.
An initial belief state b0 and a policy pi to-
gether define a Markov chain over pairs of states
and actions. For a given policy pi , we define a
value function V pi : ?(S)? R which represents the
expected discounted reward with respect to that
Markov chain:
V pi(b0) =
?
?
t=0
? t E[R(bt ,at)|b0,pi]
The goal of the agent is find a policy pi? which max-
imizes the value of the initial belief state:
pi? = argmax
pi
V pi(b0)
Exact computation of pi? is PSPACE-complete (Pa-
padimitriou and Tsitsiklis, 1987), making approx-
imation algorithms necessary for all but the sim-
plest problems. We use Perseus (Spaan and Vlassis,
2005), an anytime approximate point-based value it-
1076
eration algorithm.
5 DialogBot
We now introduce DialogBot, a Cards agent which
is capable of producing linguistic advice. To decide
when and how to speak, DialogBot maintains a dis-
tribution over its partner?s beliefs and reasons about
the effects his utterances will have on those beliefs.
To handle these complexities, DialogBot models
the world as a Decentralized Partially Observable
Markov Decision Process (Dec-POMDP) (Bernstein
et al, 2002). See Figure 3(b) for the influence dia-
gram. The definition of Dec-POMDPs mirrors that
of the POMDP, with the following changes.
There is a finite set I of agents, which we re-
strict to two. Each agent takes an action ai at
each time step, forming a joint action ~a = (a1,a2).
Each agent receives its own observation oi accord-
ing to ?(o1,o2|a1,a2,s?). The transition distribu-
tions T (s?|a1,a2,s) and the reward R(s,a1,a2) both
depend on both agents? actions.
Optimal decision making in Dec-POMDPs re-
quires maintaining a probability distribution over
all possible sequences of actions and observations
(a?1, o?1, . . . , a?t , o?t) that the other player might have
received. As t increases, we have an exponential in-
crease in the belief states an agent must consider.
Confirming this informal intuition, decision mak-
ing in Dec-POMDPs is NEXP-complete, a complex-
ity class above P-SPACE (Bernstein et al, 2002).
This computational complexity limits the applica-
tion of Dec-POMDPs to very small problems. To
address this difficulty we make several simplifying
assumptions, allowing us to construct a single-agent
POMDP which approximates the full Dec-POMDP.
Firstly, we assume that other agents do not take
into account our own beliefs, i.e., the other agent
acts like a ListenerBot. This bypasses the infinitely
nested belief problem by assuming that other agents
track one less level of nested beliefs, a common
approach (Goodman and Stuhlmu?ller, 2012; Gmy-
trasiewicz and Doshi, 2005).
Secondly, instead of tracking the full tree of pos-
sible observation histories, we maintain a point es-
timate b? of the other agent?s beliefs, which we
term the expected belief state. Rather than track-
ing each possible observation/action history of the
other agent, at each time step we marginalize out
the observations they could have received. Figure 4
compares this approach with exact belief update.
Thirdly, we assume that the other agent acts ac-
cording to a variant of the QMDP approximation
(Littman et al, 1995). Under this approximation, the
other agent solves a fully-observable MDP version
of the ListenerBot POMDP, yielding an MDP pol-
icy p?i : S? A. This critically allows us to approxi-
mate the other agent?s belief update using a specially
formed POMDP, which we detail next.
State Space To construct the approximate single-
agent POMDP from the full Dec-POMDP problem,
we formulate the state space as S? S. (See Figure
3(c) for the influence diagram.) We write a state
(s, s?) ? S? S, where s is DialogBot?s beliefs about
the true state of the world, and s? is DialogBot?s esti-
mate of the other agent?s beliefs.
Transition Distribution The main difficulty
in constructing the approximate single-agent
POMDP is specifying the transition distribu-
tion T ((s?, s??)|a,(s, s?)). To address this, we
break this distribution into two components:
T ((s?, s??)|a,(s, s?)) = T? (s??|s?,a,(s, s?))T (s?|a,s, s?).
The first term dictates how DialogBot updates its
beliefs about the other agent?s beliefs:
T? (s??|s?,a,(s, s?)) = Pr(s??|s?,a,(s, s?))
=?
o??O
Pr(s??|a, o?, s?,s)Pr(o?|s?,a, p?i(s?))
=?
o??O
(
?(o?|s??,a, p?i(s?))T (s??|a, p?i(s?), s?)
?s????(o?|s???,a, p?i(s?))T (s???|a, p?i(s?), s?)
??(o?|s?,a, p?i(s?))
)
We sum over all observations o? the other agent could
have received, updating our probability of s?? as Lis-
tenerBot would have, multiplied by the probability
that ListenerBot would have received that observa-
tion, ?(o?|s?, p?i(s?)). The QMDP approximation al-
lows us to simulate ListenerBot?s belief update in
T? (s??|s?,a,(s, s?)). Exact belief update would require
access to b?: by using p?i(s?) we can estimate the action
that ListenerBot would have taken.
In cases where s? contradicts s such that for all o? ei-
ther ?(o?|s?, p?i(s?)) = 0 or ?(o?|s??, p?i(s?)) = 0, we redis-
tribute the belief mass uniformly: T? (s??|s?,a,(s, s?))?
1077
b?t
b?o1t+1
o1
b?o2t+1
o2
b?o1,o1t+2
o1
b?o1,o2t+2
o2
b?o2,o1t+2
o1
b?o2,o2t+2
o2
(a) Exact multi-agent belief tracking
b?t
o1
o2
o1
o2
b?t+1
o1
o2
o1
o2
b?t+2
(b) Approximate multi-agent belief tracking
Figure 4: Exact multi-agent belief tracking compared with our approximate approach. Each node represents
a belief state. In exact tracking (a), the agent tracks every possible history of observations that its partner
could have received, which grows exponentially in time. In approximate update (b), the agent considers each
possible observation and then averages the resulting belief states, weighted by the probability the other agent
received that observation, resulting in a single summary belief state b?t+1. Under the QMDP approximation,
the agent considers what action the other agent would have taken if it completely believed the world was in
a certain state. Thus, there are four belief states resulting from b?t , as opposed to two in the exact case.
1 ?s?? 6= s?. This approach to managing contradiction
is analogous to logical belief revision (Alchourrono?n
et al, 1985; Ga?rdenfors, 1988; Ferme? and Hansson,
2011).
Speech Actions Speech actions are modeled by
how they change the beliefs of the other agent.
The effects of a speech actions are modeled in
T? (s??|s?,a,(s, s?)), our model of how ListenerBot?s be-
liefs change. For a speech action a = say(?) with
? ? ?,
T? (s??|s?,a,(s, s?)) =
?
o??O
(
?(o?|s??,a, p?i(s?))Pr(? |s??)T (s??|a, p?i(s?), s?)
?s????(o?|s???,a, p?i(s?))Pr(? |s???)T (s???|a, p?i(s?), s?)
??(o?|s?,a, p?i(s?))
)
DialogBot is equipped with the five most
frequent speech actions: BOARD(middle),
BOARD(top), BOARD(bottom), BOARD(left),
and BOARD(right). It produces concrete utterances
by selecting a sentence from the training corpus
with the desired semantics.
Reward DialogBot receives a large reward when
both it and its partner are located on the card, and a
negative cost when moving or speaking:
R((p,c, p?, c?),a) =
{
R+ p = c? p? = c
R? p 6= c? p? 6= c
DialogBot?s reward is not dependent on the beliefs
of the other player, only the true underlying state of
the world.
6 Experimental Results
We now experimentally evaluate our semantic clas-
sifiers and the agents? task performance.
6.1 Spatial Semantics Classifiers
We report the performance of our spatial seman-
tics classifiers, although their accuracy is not the fo-
cus of this paper. We use 10-fold cross validation
on a corpus of 577 annotated utterances. We used
simple bag-of-words features, so overfitting the data
with cross validation is not a pressing concern. Of
the 577 utterances, our classifiers perfectly labeled
325 (56.3% accuracy). The classifiers correctly pre-
dicted the domain ? of 515 (89.3%) utterances. The
1078
precision of our binary semantic primitive classifiers
was 9691126 = .861 and recall
969
1242 = .780, yielding F1
measure .818.
6.2 Cards Task Evaluation
We evaluated our ListenerBot and DialogBot agents
in the Cards task. Using 500 randomly generated
initial player and card locations, we tested each
combination of ListenerBot and DialogBot partners.
Agents succeeded at a given initial position if they
both reached the card within 50 moves. Table 2
shows how many trials each dyad won and how
many high-level actions they took to do so.
Agents % Success Moves
LB & LB 84.4% 19.8
LB & DB 87.2% 17.5
DB & DB 90.6% 16.6
Table 2: The evaluation for each combination of
agents. LB = ListenerBot; DB = DialogBot.
Collaborating DialogBots performed the best,
completing more trials and using fewer moves than
the ListenerBots. The DialogBots initially explore
the space in a similar manner to the ListenerBots,
but then share card location information. This leads
to shorter interactions, as once the DialogBot finds
the card, the other player can find it more quickly.
In the combination of ListenerBot and DialogBot,
we see about half of the improvement over two Lis-
tenerBots. Roughly 50% of the time, the Listener-
Bot finds the card first, which doesn?t help the Di-
alogBot find the card any faster.
7 Emergent Pragmatics
Grice?s original model of pragmatics (Grice, 1975)
involves the cooperative principle and four maxims:
quality (?say only what you know to be true?), rela-
tion (?be relevant?), quantity (?be as informative as
is required; do not say more than is required?), and
manner (roughly, be clear and concise).
In most interactions, DialogBot searches for the
card and then reports its location to the other agent.
These reports obey quality in that they are made only
when based on actual observations. The behavior
is not hard-coded, but rather emerges, because only
accurate information serves the agents? goals. In
contrast, sub-optimal policies generated early in the
POMDP solving process sometimes lie about card
locations. Since this behavior confuses the other
agent and thus has a lower utility, it gets replaced
by truthful communication as the policies improve.
We also capture the effects of relation and the first
clause of quantity, because the nature of the reward
function and the nested belief structures ensure that
DialogBot offers only relevant, informative informa-
tion. For instance, when DialogBot finds the card in
the lower left corner, it alternates saying ?left? and
?bottom?, effectively overcoming its limited gener-
ation capabilities. Again, early sub-optimal policies
sometimes do not report the location of the card at
all, thereby failing to fulfill these maxims.
We expect these models to produce behavior con-
sistent with manner and the second clause of quan-
tity, but evaluating this claim will require a richer ex-
perimental paradigm. For example, if DialogBot had
a larger and more structured vocabulary, it would
have to choose between levels of specificity as well
as more or less economical forms.
8 Conclusion
We have shown that cooperative pragmatic behavior
can arise from multi-agent decision-theoretic mod-
els in which the agents share a joint utility func-
tion and reason about each other?s belief states.
Decision-making in these models is intractable,
which has been a major obstacle to achieving exper-
imental results in this area. We introduced a series
of approximations to manage this intractability: (i)
combining low-level states into semantically coher-
ent high-level ones; (ii) tracking only an averaged
summary of the other agent?s potential beliefs; (iii)
limiting belief state nesting to one level, and (iv)
simplifying each agent?s model of the other?s be-
liefs so as to reduce uncertainty. These approxima-
tions bring the problems under sufficient control that
they can be solved with current POMDP approxi-
mation algorithms. Our experimental results high-
light the rich pragmatic behavior this gives rise to
and quantify the communicative value of such be-
havior. While there remain insights from earlier the-
oretical proposals and logic-based methods that we
have not fully captured, our current results support
1079
the notion that probabilistic decision-making meth-
ods can yield robust, widely applicable models that
address the real-world difficulties of partial observ-
ability and uncertainty.
Acknowledgments
This research was supported in part by ONR
grants N00014-10-1-0109 and N00014-13-1-0287
and ARO grant W911NF-07-1-0216.
References
Carlos E. Alchourrono?n, Peter Ga?rdenfors, and David
Makinson. 1985. On the logic of theory change: Par-
tial meets contradiction and revision functions. Jour-
nal of Symbolic Logic, 50(2):510?530.
James F. Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. PLOW: A collaborative
task learning agent. In Proceedings of the Twenty-
Second AAAI Conference on Artificial Intelligence,
pages 1514?1519. AAAI Press, Vancouver, British
Columbia, Canada.
James F. Allen. 1991. Reasoning About Plans. Morgan
Kaufmann, San Francisco.
David Beaver. 2002. Pragmatics, and that?s an order. In
David Barker-Plummer, David Beaver, Johan van Ben-
them, and Patrick Scotto di Luzio, editors, Logic, Lan-
guage, and Visual Information, pages 192?215. CSLI,
Stanford, CA.
Anton Benz, Gerhard Ja?ger, and Robert van Rooij, edi-
tors. 2005. Game Theory and Pragmatics. Palgrave
McMillan, Basingstoke, Hampshire.
Daniel S. Bernstein, Robert Givan, Neil Immerman, and
Shlomo Zilberstein. 2002. The complexity of decen-
tralized control of Markov decision processes. Mathe-
matics of Operations Research, 27(4):819?840.
Reinhard Blutner. 1998. Lexical pragmatics. Journal of
Semantics, 15(2):115?162.
Michael Bratman. 1987. Intentions, Plans, and Practical
Reason. Harvard University Press.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Judith Degen and Michael Franke. 2012. Optimal rea-
soning about referential expressions. In Proceedings
of SemDIAL 2012, Paris, September.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Proceed-
ings of the ACL Interactive Poster and Demonstration
Sessions, pages 1?4, Ann Arbor, MI, June. Association
for Computational Linguistics.
David DeVault. 2008. Contribution Tracking: Partici-
pating in Task-Oriented Dialogue under Uncertainty.
Ph.D. thesis, Rutgers University, New Brunswick, NJ.
Eduardo Ferme? and Sven Ove Hansson. 2011. AGM 25
years: Twenty-five years of research in belief change.
Journal of Philosophical Logic, 40(2):295?331.
Michael C. Frank and Noah D. Goodman. 2012. Predict-
ing pragmatic reasoning in language games. Science,
336(6084):998.
Michael Franke. 2009. Signal to Act: Game Theory
in Pragmatics. ILLC Dissertation Series. Institute for
Logic, Language and Computation, University of Am-
sterdam.
Peter Ga?rdenfors. 1988. Knowledge in Flux: Modeling
the Dynamics of Epistemic States. MIT Press.
Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A
framework for sequential planning in multi-agent set-
tings. Journal of Artificial Intelligence Research,
24:24?49.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 410?419, Cambridge, MA, October. ACL.
Noah D. Goodman and Andreas Stuhlmu?ller. 2012.
Knowledge and implicature: Modeling language un-
derstanding as social cognition. In Proceedings of the
Thirty-Fourth Annual Conference of the Cognitive Sci-
ence Society.
H. Paul Grice. 1975. Logic and conversation. In Peter
Cole and Jerry Morgan, editors, Syntax and Semantics,
volume 3: Speech Acts, pages 43?58. Academic Press,
New York.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175?204, July.
William G. Hayward and Michael J. Tarr. 1995. Spa-
tial language and spatial representation. Cognition,
55:39?84.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38(1):173?218.
David Lewis. 1969. Convention. Harvard University
Press, Cambridge, MA. Reprinted 2002 by Blackwell.
1080
Michael L. Littman, Anthony R. Cassandra, and
Leslie Pack Kaelbling. 1995. Learning policies for
partially observable environments: Scaling up. In Ar-
mand Prieditis and Stuart J. Russell, editors, ICML,
pages 362?370. Morgan Kaufmann.
Arthur Merin. 1997. If all our arguments had to be con-
clusive, there would be few of them. Arbeitspapiere
SFB 340 101, University of Stuttgart, Stuttgart.
Christos Papadimitriou and John N. Tsitsiklis. 1987. The
complexity of markov decision processes. Math. Oper.
Res., 12(3):441?450, August.
Prashant Parikh. 2001. The Use of Language. CSLI,
Stanford, CA.
C. Raymond Perrault and James F. Allen. 1980. A plan-
based analysis of indirect speech acts. American Jour-
nal of Computational Linguistics, 6(3?4):167?182.
Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Nathan Arnett and Ryan
Bennett, editors, Proceedings of the 30th West Coast
Conference on Formal Linguistics, Somerville, MA.
Cascadilla Press.
David V. Pynadath and Milind Tambe. 2002. The com-
municative multiagent team decision problem: Ana-
lyzing teamwork theories and models. Journal of Ar-
tificial Intelligence Research, 16:2002.
Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard
Ja?ger, and Stefan Kaufmann. 2012. Communicat-
ing with cost-based implicature: A game-theoretic ap-
proach to ambiguity. In The 16th Workshop on the Se-
mantics and Pragmatics of Dialogue, Paris, Septem-
ber.
Robert van Rooy. 2003. Questioning to resolve decision
problems. Linguistics and Philosophy, 26(6):727?
763.
Seymour Rosenberg and Bertram D. Cohen. 1964.
Speakers? and listeners? processes in a word commu-
nication task. Science, 145:1201?1203.
Matthijs T. J. Spaan and Nikos Vlassis. 2005. Perseus:
Randomized point-based value iteration for POMDPs.
Journal of Artificial Intelligence Research, 24(1):195?
220, August.
Matthijs T. J. Spaan, Frans A. Oliehoek, and Nikos Vlas-
sis. 2008. Multiagent planning under uncertainty with
stochastic communication delays. In In Proc. of the
18th Int. Conf. on Automated Planning and Schedul-
ing, pages 338?345.
Alex Stiller, Noah D. Goodman, and Michael C. Frank.
2011. Ad-hoc scalar implicature in adults and chil-
dren. In Proceedings of the 33rd Annual Meeting of
the Cognitive Science Society, Boston, July.
Matthew Stone, Richmond Thomason, and David De-
Vault. 2007. Enlightened update: A computational
architecture for presupposition and other pragmatic
phenomena. To appear in Donna K. Byron; Craige
Roberts; and Scott Schwenter, Presupposition Accom-
modation.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A pomdp framework for spoken
dialogue systems. Comput. Speech Lang., 24(4):562?
588, October.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The hidden information state model: A
practical framework for pomdp-based spoken dialogue
management. Comput. Speech Lang., 24(2):150?174,
April.
1081
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 167?176,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
?Was it good? It was provocative.?
Learning the meaning of scalar adjectives
Marie-Catherine de Marneffe, Christopher D. Manning and Christopher Potts
Linguistics Department
Stanford University
Stanford, CA 94305
{mcdm,manning,cgpotts}@stanford.edu
Abstract
Texts and dialogues often express infor-
mation indirectly. For instance, speak-
ers? answers to yes/no questions do not
always straightforwardly convey a ?yes?
or ?no? answer. The intended reply is
clear in some cases (Was it good? It was
great!) but uncertain in others (Was it
acceptable? It was unprecedented.). In
this paper, we present methods for inter-
preting the answers to questions like these
which involve scalar modifiers. We show
how to ground scalar modifier meaning
based on data collected from the Web. We
learn scales between modifiers and infer
the extent to which a given answer conveys
?yes? or ?no?. To evaluate the methods,
we collected examples of question?answer
pairs involving scalar modifiers from CNN
transcripts and the Dialog Act corpus and
use response distributions from Mechani-
cal Turk workers to assess the degree to
which each answer conveys ?yes? or ?no?.
Our experimental results closely match the
Turkers? response data, demonstrating that
meanings can be learned from Web data
and that such meanings can drive prag-
matic inference.
1 Introduction
An important challenge for natural language pro-
cessing is how to learn not only basic linguistic
meanings but also how those meanings are system-
atically enriched when expressed in context. For
instance, answers to polar (yes/no) questions do
not always explicitly contain a ?yes? or ?no?, but
rather give information that the hearer can use to
infer such an answer in a context with some degree
of certainty. Hockey et al (1997) find that 27% of
answers to polar questions do not contain a direct
?yes? or ?no? word, 44% of which they regard as
failing to convey a clear ?yes? or ?no? response. In
some cases, interpreting the answer is straightfor-
ward (Was it bad? It was terrible.), but in others,
what to infer from the answer is unclear (Was it
good? It was provocative.). It is even common
for the speaker to explicitly convey his own uncer-
tainty with such answers.
In this paper, we focus on the interpretation
of answers to a particular class of polar ques-
tions: ones in which the main predication in-
volves a gradable modifier (e.g., highly unusual,
not good, little) and the answer either involves an-
other gradable modifier or a numerical expression
(e.g., seven years old, twenty acres of land). Inter-
preting such question?answer pairs requires deal-
ing with modifier meanings, specifically, learning
context-dependent scales of expressions (Horn,
1972; Fauconnier, 1975) that determine how, and
to what extent, the answer as a whole resolves the
issue raised by the question.
We propose two methods for learning the
knowledge necessary for interpreting indirect an-
swers to questions involving gradable adjectives,
depending on the type of predications in the ques-
tion and the answer. The first technique deals
with pairs of modifiers: we hypothesized that on-
line, informal review corpora in which people?s
comments have associated ratings would provide
a general-purpose database for mining scales be-
tween modifiers. We thus use a large collection of
online reviews to learn orderings between adjec-
tives based on contextual entailment (good < ex-
cellent), and employ this scalar relationship to in-
fer a yes/no answer (subject to negation and other
qualifiers). The second strategy targets numeri-
cal answers. Since it is unclear what kind of cor-
pora would contain the relevant information, we
turn to the Web in general: we use distributional
information retrieved via Web searches to assess
whether the numerical measure counts as a posi-
167
tive or negative instance of the adjective in ques-
tion. Both techniques exploit the same approach:
using texts (the Web) to learn meanings that can
drive pragmatic inference in dialogue. This paper
demonstrates to some extent that meaning can be
grounded from text in this way.
2 Related work
Indirect speech acts are studied by Clark (1979),
Perrault and Allen (1980), Allen and Perrault
(1980) and Asher and Lascarides (2003), who
identify a wide range of factors that govern how
speakers convey their intended messages and how
hearers seek to uncover those messages from
uncertain and conflicting signals. In the com-
putational literature, Green and Carberry (1994,
1999) provide an extensive model that interprets
and generates indirect answers to polar questions.
They propose a logical inference model which
makes use of discourse plans and coherence rela-
tions to infer categorical answers. However, to ad-
equately interpret indirect answers, the uncertainty
inherent in some answers needs to be captured (de
Marneffe et al, 2009). While a straightforward
?yes? or ?no? response is clear in some indirect an-
swers, such as in (1), the intended answer is less
certain in other cases (2):1
(1) A: Do you think that?s a good idea, that we
just begin to ignore these numbers?
B: I think it?s an excellent idea.
(2) A: Is he qualified?
B: I think he?s young.
In (2), it might be that the answerer does not
know about qualifications or does not want to talk
about these directly, and therefore shifts the topic
slightly. As proposed by Zeevat (1994) in his work
on partial answers, the speaker?s indirect answer
might indicate that he is deliberately leaving the
original question only partially addressed, while
giving a fully resolving answer to another one.
The hearer must then interpret the answer to work
out the other question. In (2) in context, we get a
sense that the speaker would resolve the issue to
?no?, but that he is definitely not committed to that
in any strong sense. Uncertainty can thus reside
both on the speaker and the hearer sides, and the
four following scenarios are attested in conversa-
tion:
1Here and throughout, the examples come from the corpus
described in section 3.
a. The speaker is certain of ?yes? or ?no? and
conveys that directly and successfully to the
hearer.
b. The speaker is certain of ?yes? or ?no? but
conveys this only partially to the hearer.
c. The speaker is uncertain of ?yes? or ?no? and
conveys this uncertainty to the hearer.
d. The speaker is uncertain of ?yes? or ?no?,
but the hearer infers one of those with con-
fidence.
The uncertainty is especially pressing for pred-
ications built around scalar modifiers, which are
inherently vague and highly context-dependent
(Kamp and Partee, 1995; Kennedy and McNally,
2005; Kennedy, 2007). For example, even if we
fix the basic sense for little to mean ?young for a
human?, there is a substantial amount of gray area
between the clear instances (babies) and the clear
non-instances (adults). This is the source of un-
certainty in (3), in which B?s children fall into the
gray area.
(3) A: Are your kids little?
B: I have a seven year-old and a ten
year-old.
3 Corpus description
Since indirect answers are likely to arise in in-
terviews, to gather instances of question?answer
pairs involving gradable modifiers (which will
serve to evaluate the learning techniques), we use
online CNN interview transcripts from five dif-
ferent shows aired between 2000 and 2008 (An-
derson Cooper, Larry King Live, Late Edition,
Lou Dobbs Tonight, The Situation Room). We
also searched the Switchboard Dialog Act corpus
(Jurafsky et al, 1997). We used regular expres-
sions and manual filtering to find examples of two-
utterance dialogues in which the question and the
reply contain some kind of gradable modifier.
3.1 Types of question?answer pairs
In total, we ended up with 224 question?answer
pairs involving gradable adjectives. However
our collection contains different types of answers,
which naturally fall into two categories: (I) in
205 dialogues, both the question and the answer
contain a gradable modifier; (II) in 19 dialogues,
the reply contains a numerical measure (as in (3)
above and (4)).
168
Modification in answer Example Count
I Other adjective (1), (2) 125
Adverb - same adjective (5) 55
Negation - same adjective (6), (7) 21
Omitted adjective (8) 4
II Numerical measure (3), (4) 19
Table 1: Types of question?answer pairs, and
counts in the corpus.
I Modification in answer Mean SD
Other adjective 1.1 0.6
Adverb - same adjective 0.8 0.6
Negation - same adjective 1.0 0.5
Omitted adjective 1.1 0.2
II Numerical measure 1.5 0.2
Table 2: Mean entropy values and standard devi-
ation obtained in the Mechanical Turk experiment
for each question?answer pair category.
(4) A: Have you been living there very long?
B: I?m in here right now about twelve and
a half years.
Category I, which consists of pairs of modifiers,
can be further divided. In most dialogues, the an-
swer contains another adjective than the one used
in the question, such as in (1). In others, the an-
swer contains the same adjective as in the ques-
tion, but modified by an adverb (e.g., very, basi-
cally, quite) as in (5) or a negation as in (6).
(5) A: That seems to be the biggest sign of
progress there. Is that accurate?
B: That?s absolutely accurate.
(6) A: Are you bitter?
B: I?m not bitter because I?m a soldier.
The negation can be present in the main clause
when the adjectival predication is embedded, as in
example (7).
(7) A: [. . . ] Is that fair?
B: I don?t think that?s a fair statement.
In a few cases, when the question contains an ad-
jective modifying a noun, the adjective is omitted
in the answer:
(8) A: Is that a huge gap in the system?
B: It is a gap.
Table 1 gives the distribution of the types ap-
pearing in the corpus.
3.2 Answer assignment
To assess the degree to which each answer con-
veys ?yes? or ?no? in context, we use response dis-
tributions from Mechanical Turk workers. Given a
written dialogue between speakers A and B, Turk-
ers were asked to judge what B?s answer conveys:
?definite yes?, ?probable yes?, ?uncertain?, ?proba-
ble no?, ?definite no?. Within each of the two ?yes?
and ?no? pairs, there is a scalar relationship, but
the pairs themselves are not in a scalar relationship
with each other, and ?uncertain? is arguably a sep-
arate judgment. Figure 1 shows the exact formu-
lation used in the experiment. For each dialogue,
we got answers from 30 Turkers, and we took the
dominant response as the correct one though we
make extensive use of the full response distribu-
tions in evaluating our approach.2 We also com-
puted entropy values for the distribution of an-
swers for each item. Overall, the agreement was
good: 21 items have total agreement (entropy of
0.0 ? 11 in the ?adjective? category, 9 in the
?adverb-adjective? category and 1 in the ?nega-
tion? category), and 80 items are such that a single
response got chosen 20 or more times (entropy <
0.9). The dialogues in (1) and (9) are examples of
total agreement. In contrast, (10) has response en-
tropy of 1.1, and item (11) has the highest entropy
of 2.2.
(9) A: Advertisements can be good or bad.
Was it a good ad?
B: It was a great ad.
(10) A: Am I clear?
B: I wish you were a little more forthright.
(11) A: 91 percent of the American people still
express confidence in the long-term
prospect of the U.S. economy; only 8
percent are not confident. Are they
overly optimistic, in your professional
assessment?
2120 Turkers were involved (the median number of items
done was 28 and the mean 56.5). The Fleiss? Kappa score for
the five response categories is 0.46, though these categories
are partially ordered. For the three-category response system
used in section 5, which arguably has no scalar ordering, the
Fleiss? Kappa is 0.63. Despite variant individual judgments,
aggregate annotations done with Mechanical Turk have been
shown to be reliable (Snow et al, 2008; Sheng et al, 2008;
Munro et al, 2010). Here, the relatively low Kappa scores
also reflect the uncertainty inherent in many of our examples,
uncertainty that we seek to characterize and come to grips
with computationally.
169
Indirect Answers to Yes/No Questions
In the following dialogue, speaker A asks a simple Yes/No
question, but speaker B answers with something more in-
direct and complicated.
dialogue here
Which of the following best captures what speaker B
meant here:
? B definitely meant to convey ?Yes?.
? B probably meant to convey ?Yes?.
? B definitely meant to convey ?No?.
? B probably meant to convey ?No?.
? (I really can?t tell whether B meant to convey ?Yes?
or ?No?.)
Figure 1: Design of the Mechanical Turk experi-
ment.
B: I think it shows how wise the American
people are.
Table 2 shows the mean entropy values for the
different categories identified in the corpus. Inter-
estingly, the pairs involving an adverbial modifi-
cation in the answer all received a positive answer
(?yes? or ?probable yes?) as dominant response.
All 19 dialogues involving a numerical measure
had either ?probable yes? or ?uncertain? as domi-
nant response. There is thus a significant bias for
positive answers: 70% of the category I items and
74% of the category II items have a positive an-
swer as dominant response. Examining a subset
of the Dialog Act corpus, we found that 38% of
the yes/no questions receive a direct positive an-
swers, whereas 21% have a direct negative answer.
This bias probably stems from the fact that people
are more likely to use an overt denial expression
where they need to disagree, whether or not they
are responding indirectly.
4 Methods
In this section, we present the methods we propose
for grounding the meanings of scalar modifiers.
4.1 Learning modifier scales and inferring
yes/no answers
The first technique targets items such as the ones
in category I of our corpus. Our central hypothesis
is that, in polar question dialogues, the semantic
relationship between the main predication PQ in
the question and the main predication PA in the an-
swer is the primary factor in determining whether,
and to what extent, ?yes? or ?no? was intended. If
PA is at least as strong as PQ, the intended answer
is ?yes?; if PA is weaker than PQ, the intended an-
swer is ?no?; and, where no reliable entailment re-
lationship exists between PA and PQ, the result is
uncertainty.
For example, good is weaker (lower on the rel-
evant scale) than excellent, and thus speakers in-
fer that the reply in example (1) above is meant to
convey ?yes?. In contrast, if we reverse the order
of the modifiers ? roughly, Is it a great idea?;
It?s a good idea ? then speakers infer that the
answer conveys ?no?. Had B replied with It?s a
complicated idea in (1), then uncertainty would
likely have resulted, since good and complicated
are not in a reliable scalar relationship. Negation
reverses scales (Horn, 1972; Levinson, 2000), so it
flips ?yes? and ?no? in these cases, leaving ?uncer-
tain? unchanged. When both the question and the
answer contain a modifier (such as in (9?11)), the
yes/no response should correlate with the extent to
which the pair of modifiers can be put into a scale
based on contextual entailment.
To ground such scales from text, we collected a
large corpus of online reviews from IMDB. Each
of the reviews in this collection has an associated
star rating: one star (most negative) to ten stars
(most positive). Table 3 summarizes the distribu-
tion of reviews as well as the number of words and
vocabulary across the ten rating categories.
As is evident from table 3, there is a signif-
icant bias for ten-star reviews. This is a com-
mon feature of such corpora of informal, user-
provided reviews (Chevalier and Mayzlin, 2006;
Hu et al, 2006; Pang and Lee, 2008). However,
since we do not want to incorporate the linguis-
tically uninteresting fact that people tend to write
a lot of ten-star reviews, we assume uniform pri-
ors for the rating categories. Let count(w, r) be
the number of tokens of word w in reviews in rat-
ing category r, and let count(r) be the total word
count for all words in rating category r. The prob-
ability of w given a rating category r is simply
Pr(w|r) = count(w, r)/ count(r). Then under the
assumption of uniform priors, we get Pr(r|w) =
Pr(w|r)/
?
r??R Pr(w|r?).
In reasoning about our dialogues, we rescale
the rating categories by subtracting 5.5 from each,
to center them at 0. This yields the scale R =
170
Rating Reviews Words Vocabulary Average words per review
1 124,587 25,389,211 192,348 203.79
2 51,390 11,750,820 133,283 228.66
3 58,051 13,990,519 148,530 241.00
4 59,781 14,958,477 156,564 250.22
5 80,487 20,382,805 188,461 253.24
6 106,145 27,408,662 225,165 258.22
7 157,005 40,176,069 282,530 255.89
8 195,378 48,706,843 313,046 249.30
9 170,531 40,264,174 273,266 236.11
10 358,441 73,929,298 381,508 206.25
Total 1,361,796 316,956,878 1,160,072 206.25
Table 3: Numbers of reviews, words and vocabulary size per rating category in the IMDB review corpus,
as well as the average number of words per review.
enjoyable
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 0.74
best
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 1.08
great
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 1.1
superb
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 2.18
disappointing
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -1.1
bad
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -1.47
awful
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -2.5
worst
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -2.56
Pr
(R
ati
ng
|W
ord
)
Rating (centered at 0)
Figure 2: The distribution of some scalar modifiers across the ten rating categories. The vertical lines
mark the expected ratings, defined as a weighted sum of the probability values (black dots).
??4.5,?3.5,?2.5,?1.5,?0.5, 0.5, 1.5, 2.5, 3.5, 4.5?.
Our rationale for this is that modifiers at the neg-
ative end of the scale (bad, awful, terrible) are
not linguistically comparable to those at the
positive end of the scale (good, excellent, superb).
Each group forms its own qualitatively different
scale (Kennedy and McNally, 2005). Rescaling
allows us to make a basic positive vs. negative
distinction. Once we have done that, an increase
in absolute value is an increase in strength. In
our experiments, we use expected rating values
to characterize the polarity and strength of mod-
ifiers. The expected rating value for a word w
is ER(w) =
?
r?R r Pr(r|w). Figure 2 plots these
values for a number of scalar terms, both positive
and negative, across the rescaled ratings, with
the vertical lines marking their ER values. The
weak scalar modifiers all the way on the left are
most common near the middle of the scale, with
a slight positive bias in the top row and a slight
negative bias in the bottom row. As we move
from left to right, the bias for one end of the scale
grows more extreme, until the words in question
are almost never used outside of the most extreme
rating category. The resulting scales correspond
well with linguistic intuitions and thus provide
an initial indication that the rating categories
are a reliable guide to strength and polarity for
scalar modifiers. We put this information to use
in our dialogue corpus via the decision procedure
171
Let D be a dialogue consisting of (i) a polar question
whose main predication is based on scalar predicate PQ
and (ii) an indirect answer whose main predication is
based on scalar predicate PA. Then:
1. if PA or PQ is missing from our data, infer ?Uncer-
tain?;
2. else if ER(PQ) and ER(PA) have different signs, in-
fer ?No?;
3. else if abs(ER(PQ)) 6 abs(ER(PA)), infer ?Yes?;
4. else infer ?No?.
5. In the presence of negation, map ?Yes? to ?No?, ?No?
to ?Yes?, and ?Uncertain? to ?Uncertain?.
Figure 3: Decision procedure for using the word
frequencies across rating categories in the review
corpus to decide what a given answer conveys.
described in figure 3.
4.2 Interpreting numerical answers
The second technique aims at determining
whether a numerical answer counts as a positive
or negative instance of the adjective in the ques-
tion (category II in our corpus).
Adjectives that can receive a conventional unit
of measure, such as little or long, inherently pos-
sess a degree of vagueness (Kamp and Partee,
1995; Kennedy, 2007): while in the extreme cases,
judgments are strong (e.g., a six foot tall woman
can clearly be called ?a tall woman? whereas a
five foot tall woman cannot), there are borderline
cases for which it is difficult to say whether the
adjectival predication can truthfully be ascribed
to them. A logistic regression model can capture
these facts. To build this model, we gather distri-
butional information from the Web.
For instance, in the case of (3), we can retrieve
from the Web positive and negative examples of
age in relation to the adjective and the modified en-
tity ?little kids?. The question contains the adjec-
tive and the modified entity. The reply contains the
unit of measure (here ?year-old?) and the numer-
ical answer. Specifically we query the Web using
Yahoo! BOSS (Academic) for ?little kids? year-
old (positive instances) as well as for ?not little
kids? year-old (negative instances). Yahoo! BOSS
is an open search services platform that provides a
query API for Yahoo! Web search. We then ex-
tract ages from the positive and negative snippets
obtained, and we fit a logistic regression to these
data. To remove noise, we discard low counts
(positive and negative instances for a given unit
< 5). Also, for some adjectives, such as little or
young, there is an inherent ambiguity between ab-
solute and relative uses. Ideally, a word sense dis-
ambiguation system would be used to filter these
cases. For now, we extract the largest contiguous
range for which the data counts are over the noise
threshold.3 When not enough data is retrieved for
the negative examples, we expand the query by
moving the negation outside the search phrase. We
also replace the negation and the adjective by the
antonyms given in WordNet (using the first sense).
The logistic regression thus has only one fac-
tor ? the unit of measure (age in the case of lit-
tle kids). For a given answer, the model assigns a
probability indicating the extent to which the ad-
jectival property applies to that answer. If the fac-
tor is a significant predictor, we can use the prob-
abilities from the model to decide whether the an-
swer qualifies as a positive or negative instance of
the adjective in the question, and thus interpret the
indirect response as a ?yes? or a ?no?. The prob-
abilistic nature of this technique adheres perfectly
to the fact that indirect answers are intimately tied
up with uncertainty.
5 Evaluation and results
Our primary goal is to evaluate how well we can
learn the relevant scalar and entailment relation-
ships from the Web. In the evaluation, we thus ap-
plied our techniques to a manually coded corpus
version. For the adjectival scales, we annotated
each example for its main predication (modifier, or
adverb?modifier bigram), including whether that
predication was negated. For the numerical cases,
we manually constructed the initial queries: we
identified the adjective and the modified entity in
the question, and the unit of measure in the answer.
However, we believe that identifying the requisite
predications and recognizing the presence of nega-
tion or embedding could be done automatically us-
ing dependency graphs.4
3Otherwise, our model is ruined by references to ?young
80-year olds?, using the relative sense of young, which are
moderately frequent on the Web.
4As a test, we transformed our corpus into the Stanford
dependency representation (de Marneffe et al, 2006), using
the Stanford parser (Klein and Manning, 2003) and were able
to automatically retrieve all negated modifier predications,
except one (We had a view of it, not a particularly good one),
172
Modification in answer Precision Recall
I Other adjective 60 60
Adverb - same adjective 95 95
Negation - same adjective 100 100
Omitted adjective 100 100
II Numerical 89 40
Total 75 71
Table 4: Summary of precision and recall (%) by
type.
Response Precision Recall F1
I Yes 87 76 81
No 57 71 63
II Yes 100 36 53
Uncertain 67 40 50
Table 5: Precision, recall, and F1 (%) per response
category. In the case of the scalar modifiers exper-
iment, there were just two examples whose dom-
inant response from the Turkers was ?Uncertain?,
so we have left that category out of the results. In
the case of the numerical experiment, there were
not any ?No? answers.
To evaluate the techniques, we pool the Me-
chanical Turk ?definite yes? and ?probable yes?
categories into a single category ?Yes?, and we
do the same for ?definite no? and ?probable no?.
Together with ?uncertain?, this makes for three-
response categories. We count an inference as
successful if it matches the dominant Turker re-
sponse category. To use the three-response scheme
in the numerical experiment, we simply catego-
rize the probabilities as follows: 0?0.33 = ?No?,
0.33?0.66 = ?Uncertain?, 0.66?1.00 = ?Yes?.
Table 4 gives a breakdown of our system?s per-
formance on the various category subtypes. The
overall accuracy level is 71% (159 out of 224 in-
ferences correct). Table 5 summarizes the results
per response category, for the examples in which
both the question and answer contain a gradable
modifier (category I), and for the numerical cases
(category II).
6 Analysis and discussion
Performance is extremely good on the ?Adverb ?
same adjective? and ?Negation ? same adjective?
cases because the ?Yes? answer is fairly direct for
them (though adverbs like basically introduce an
interesting level of uncertainty). The results are
because of a parse error which led to wrong dependencies.
Response Precision Recall F1
WordNet-based Yes 82 83 82.5
(items I) No 60 56 58
Table 6: Precision, recall, and F1 (%) per response
category for the WordNet-based approach.
somewhat mixed for the ?Other adjective? cate-
gory.
Inferring the relation between scalar adjectives
has some connection with work in sentiment de-
tection. Even though most of the research in that
domain focuses on the orientation of one term us-
ing seed sets, techniques which provide the ori-
entation strength could be used to infer a scalar
relation between adjectives. For instance, Blair-
Goldensohn et al (2008) use WordNet to develop
sentiment lexicons in which each word has a posi-
tive or negative value associated with it, represent-
ing its strength. The algorithm begins with seed
sets of positive, negative, and neutral terms, and
then uses the synonym and antonym structure of
WordNet to expand those initial sets and refine
the relative strength values. Using our own seed
sets, we built a lexicon using Blair-Goldensohn
et al (2008)?s method and applied it as in figure
3 (changing the ER values to sentiment scores).
Both approaches achieve similar results: for the
?Other adjective? category, the WordNet-based
approach yields 56% accuracy, which is not signif-
icantly different from our performance (60%); for
the other types in category I, there is no difference
in results between the two methods. Table 6 sum-
marizes the results per response category for the
WordNet-based approach (which can thus be com-
pared to the category I results in table 5). However
in contrast to the WordNet-based approach, we re-
quire no hand-built resources: the synonym and
antonym structures, as well as the strength values,
are learned from Web data alone. In addition, the
WordNet-based approach must be supplemented
with a separate method for the numerical cases.
In the ?Other adjective? category, 31 items
involve oppositional terms: canonical antonyms
(e.g., right/wrong, good/bad) as well as terms
that are ?statistically oppositional? (e.g., ready/
premature, true/preposterous, confident/nervous).
?Statistically oppositional? terms are not opposi-
tional by definition, but as a matter of contingent
fact. Our technique accurately deals with most
173
0 10 20 30 40 50 60
0.0
0.2
0.4
0.6
0.8
little kids
Age
Pro
ba
bili
ty 
of 
be
ing
 "li
ttle
"
0 10 20 30 40 50 60
0.2
0.4
0.6
0.8
young kids
Age
Pro
ba
bili
ty 
of 
be
ing
 "y
ou
ng
"
0 20 40 60 80 100 120
0.3
0.4
0.5
0.6
0.7
0.8
warm weather
Degree
Pro
ba
bili
ty 
of 
be
ing
 "w
arm
"
Figure 4: Probabilities of being appropriately described as ?little?, ?young? or ?warm?, fitted on data
retrieved when querying the Web for ?little kids?, ?young kids? and ?warm weather?.
of the canonical antonyms, and also finds some
contingent oppositions (qualified/young, wise/
neurotic) that are lacking in antonymy resources or
automatically generated antonymy lists (Moham-
mad et al, 2008). Out of these 31 items, our tech-
nique correctly marks 18, whereas Mohammad et
al.?s list of antonyms only contains 5 and Blair-
Goldensohn et al (2008)?s technique finds 11. Our
technique is solely based on unigrams, and could
be improved by adding context: making use of de-
pendency information, as well as moving beyond
unigrams.
In the numerical cases, precision is high but re-
call is low. For roughly half of the items, not
enough negative instances can be gathered from
the Web and the model lacks predictive power (as
for items (4) or (12)).
(12) A: Do you happen to be working for a
large firm?
B: It?s about three hundred and fifty
people.
Looking at the negative hits for item (12), one
sees that few give an indication about the num-
ber of people in the firm, but rather qualifications
about colleagues or employees (great people, peo-
ple?s productivity), or the hits are less relevant:
?Most of the people I talked to were actually pretty
optimistic. They were rosy on the job market
and many had jobs, although most were not large
firm jobs?. The lack of data comes from the fact
that the queries are very specific, since the adjec-
tive depends on the product (e.g., ?expensive ex-
ercise bike?, ?deep pond?). However when we
do get a predictive model, the probabilities corre-
Entropy of response distribution
Pro
bab
ility 
of c
orre
ct in
fere
nce
 by 
our 
sys
tem
0.0 0.5 1.0 1.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 5: Correlation between agreement among
Turkers and whether the system gets the correct
answer. For each dialogue, we plot a circle at
Turker response entropy and either 1 = correct
inference or 0 = incorrect inference, except the
points are jittered a little vertically to show where
the mass of data lies. As the entropy rises (i.e., as
agreement levels fall), the system?s inferences be-
come less accurate. The fitted logistic regression
model (black line) has a statistically significant co-
efficient for response entropy (p < 0.001).
174
late almost perfectly with the Turkers? responses.
This happens for 8 items: ?expensive to call (50
cents a minute)?, ?little kids (7 and 10 year-old)?,
?long growing season (3 months)?, ?lot of land
(80 acres)?, ?warm weather (80 degrees)?, ?young
kids (5 and 2 year-old)?, ?young person (31 year-
old)? and ?large house (2450 square feet)?. In
the latter case only, the system output (uncer-
tain) doesn?t correlate with the Turkers? judgment
(where the dominant answer is ?probable yes? with
15 responses, and 11 answers are ?uncertain?).
The logistic curves in figure 4 capture nicely the
intuitions that people have about the relation be-
tween age and ?little kids? or ?young kids?, as
well as between Fahrenheit degrees and ?warm
weather?. For ?little kids?, the probabilities of be-
ing little or not are clear-cut for ages below 7 and
above 15, but there is a region of vagueness in be-
tween. In the case of ?young kids?, the probabil-
ities drop less quickly with age increasing (an 18
year-old can indeed still be qualified as a ?young
kid?). In sum, when the data is available, this
method delivers models which fit humans? intu-
itions about the relation between numerical mea-
sure and adjective, and can handle pragmatic in-
ference.
If we restrict attention to the 66 examples on
which the Turkers completely agreed about which
of these three categories was intended (again pool-
ing ?probable? and ?definite?), then the percent-
age of correct inferences rises to 89% (59 cor-
rect inferences). Figure 5 plots the relation-
ship between the response entropy and the accu-
racy of our decision procedure, along with a fit-
ted logistic regression model using response en-
tropy to predict whether our system?s inference
was correct. The handful of empirical points in
the lower left of the figure show cases of high
agreement between Turkers but incorrect infer-
ence from the system. The few points in the up-
per right indicate low agreement between Turk-
ers and correct inference from the system. Three
of the high-agreement/incorrect-inference cases
involve the adjectives right?correct. For low-
agreement/correct-inference, the disparity could
trace to context dependency: the ordering is clear
in the context of product reviews, but unclear in
a television interview. The analysis suggests that
overall agreement is positively correlated with our
system?s chances of making a correct inference:
our system?s accuracy drops as human agreement
levels drop.
7 Conclusion
We set out to find techniques for grounding ba-
sic meanings from text and enriching those mean-
ings based on information from the immediate lin-
guistic context. We focus on gradable modifiers,
seeking to learn scalar relationships between their
meanings and to obtain an empirically grounded,
probabilistic understanding of the clear and fuzzy
cases that they often give rise to (Kamp and Partee,
1995). We show that it is possible to learn the req-
uisite scales between modifiers using review cor-
pora, and to use that knowledge to drive inference
in indirect responses. When the relation in ques-
tion is not too specific, we show that it is also pos-
sible to learn the strength of the relation between
an adjective and a numerical measure.
Acknowledgments
This paper is based on work funded in part by
ONR award N00014-10-1-0109 and ARO MURI
award 548106, as well as by the Air Force Re-
search Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the view of the Air Force Re-
search Laboratory (AFRL), ARO or ONR.
References
James F. Allen and C. Raymond Perrault. 1980. Ana-
lyzing intention in utterances. Artificial Intelligence,
15:143?178.
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press, Cam-
bridge.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A. Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era (NLPIX).
Judith A. Chevalier and Dina Mayzlin. 2006. The
effect of word of mouth on sales: Online book re-
views. Journal of Marketing Research, 43(3):345?
354.
Herbert H. Clark. 1979. Responding to indirect speech
acts. Cognitive Psychology, 11:430?477.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
175
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC-2006).
Marie-Catherine de Marneffe, Scott Grimm, and
Christopher Potts. 2009. Not a simple ?yes? or
?no?: Uncertainty in indirect answers. In Proceed-
ings of the 10th Annual SIGDIAL Meeting on Dis-
course and Dialogue.
Gilles Fauconnier. 1975. Pragmatic scales and logical
structure. Linguistic Inquiry, 6(3):353?375.
Nancy Green and Sandra Carberry. 1994. A hybrid
reasoning model for indirect answers. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 58?65.
Nancy Green and Sandra Carberry. 1999. Interpret-
ing and generating indirect answers. Computational
Linguistics, 25(3):389?435.
Beth Ann Hockey, Deborah Rossen-Knill, Beverly
Spejewski, Matthew Stone, and Stephen Isard.
1997. Can you predict answers to Y/N questions?
Yes, No and Stuff. In Proceedings of Eurospeech
1997, pages 2267?2270.
Laurence R Horn. 1972. On the Semantic Properties of
Logical Operators in English. Ph.D. thesis, UCLA,
Los Angeles.
Nan Hu, Paul A. Pavlou, and Jennifer Zhang. 2006.
Can online reviews reveal a product?s true quality?:
Empirical findings and analytical modeling of online
word-of-mouth communication. In Proceedings of
Electronic Commerce (EC), pages 324?330.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual, draft
13. Technical Report 97-02, University of Colorado,
Boulder Institute of Cognitive Science.
Hans Kamp and Barbara H. Partee. 1995. Prototype
theory and compositionality. Cognition, 57(2):129?
191.
Christopher Kennedy and Louise McNally. 2005.
Scale structure and the semantic typology of grad-
able predicates. Language, 81(2):345?381.
Christopher Kennedy. 2007. Vagueness and grammar:
The semantics of relative and absolute gradable ad-
jectives. Linguistics and Philosophy, 30(1):1?45.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association of Computational
Linguistics.
Stephen C. Levinson. 2000. Presumptive Meanings:
The Theory of Generalized Conversational Implica-
ture. MIT Press, Cambridge, MA.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-2008).
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: The new gen-
eration of linguistic data. In NAACL 2010 Workshop
on Creating Speech and Language Data With Ama-
zon?s Mechanical Turk.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1):1?135.
C. Raymond Perrault and James F. Allen. 1980. A
plan-based analysis of indirect speech acts. Amer-
ican Journal of Computational Linguistics, 6(3-
4):167?182.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of KDD-2008.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-2008).
Henk Zeevat. 1994. Questions and exhaustivity in up-
date semantics. In Harry Bunt, Reinhard Muskens,
and Gerrit Rentier, editors, Proceedings of the In-
ternational Workshop on Computational Semantics,
pages 211?221.
176
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142?150,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Word Vectors for Sentiment Analysis
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
Andrew Y. Ng, and Christopher Potts
Stanford University
Stanford, CA 94305
[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu
Abstract
Unsupervised vector-based approaches to se-
mantics can model rich lexical meanings, but
they largely fail to capture sentiment informa-
tion that is central to many word meanings and
important for a wide range of NLP tasks. We
present a model that uses a mix of unsuper-
vised and supervised techniques to learn word
vectors capturing semantic term?document in-
formation as well as rich sentiment content.
The proposed model can leverage both con-
tinuous and multi-dimensional sentiment in-
formation as well as non-sentiment annota-
tions. We instantiate the model to utilize the
document-level sentiment polarity annotations
present in many online documents (e.g. star
ratings). We evaluate the model using small,
widely used sentiment and subjectivity cor-
pora and find it out-performs several previ-
ously introduced methods for sentiment clas-
sification. We also introduce a large dataset
of movie reviews to serve as a more robust
benchmark for work in this area.
1 Introduction
Word representations are a critical component of
many natural language processing systems. It is
common to represent words as indices in a vocab-
ulary, but this fails to capture the rich relational
structure of the lexicon. Vector-based models do
much better in this regard. They encode continu-
ous similarities between words as distance or angle
between word vectors in a high-dimensional space.
The general approach has proven useful in tasks
such as word sense disambiguation, named entity
recognition, part of speech tagging, and document
retrieval (Turney and Pantel, 2010; Collobert and
Weston, 2008; Turian et al, 2010).
In this paper, we present a model to capture both
semantic and sentiment similarities among words.
The semantic component of our model learns word
vectors via an unsupervised probabilistic model of
documents. However, in keeping with linguistic and
cognitive research arguing that expressive content
and descriptive semantic content are distinct (Ka-
plan, 1999; Jay, 2000; Potts, 2007), we find that
this basic model misses crucial sentiment informa-
tion. For example, while it learns that wonderful
and amazing are semantically close, it doesn?t cap-
ture the fact that these are both very strong positive
sentiment words, at the opposite end of the spectrum
from terrible and awful.
Thus, we extend the model with a supervised
sentiment component that is capable of embracing
many social and attitudinal aspects of meaning (Wil-
son et al, 2004; Alm et al, 2005; Andreevskaia
and Bergler, 2006; Pang and Lee, 2005; Goldberg
and Zhu, 2006; Snyder and Barzilay, 2007). This
component of the model uses the vector represen-
tation of words to predict the sentiment annotations
on contexts in which the words appear. This causes
words expressing similar sentiment to have similar
vector representations. The full objective function
of the model thus learns semantic vectors that are
imbued with nuanced sentiment information. In our
experiments, we show how the model can leverage
document-level sentiment annotations of a sort that
are abundant online in the form of consumer reviews
for movies, products, etc. The technique is suffi-
142
ciently general to work also with continuous and
multi-dimensional notions of sentiment as well as
non-sentiment annotations (e.g., political affiliation,
speaker commitment).
After presenting the model in detail, we pro-
vide illustrative examples of the vectors it learns,
and then we systematically evaluate the approach
on document-level and sentence-level classification
tasks. Our experiments involve the small, widely
used sentiment and subjectivity corpora of Pang and
Lee (2004), which permits us to make comparisons
with a number of related approaches and published
results. We also show that this dataset contains many
correlations between examples in the training and
testing sets. This leads us to evaluate on, and make
publicly available, a large dataset of informal movie
reviews from the Internet Movie Database (IMDB).
2 Related work
The model we present in the next section draws in-
spiration from prior work on both probabilistic topic
modeling and vector-spaced models for word mean-
ings.
Latent Dirichlet Allocation (LDA; (Blei et al,
2003)) is a probabilistic document model that as-
sumes each document is a mixture of latent top-
ics. For each latent topic T , the model learns a
conditional distribution p(w|T ) for the probability
that word w occurs in T . One can obtain a k-
dimensional vector representation of words by first
training a k-topic model and then filling the matrix
with the p(w|T ) values (normalized to unit length).
The result is a word?topic matrix in which the rows
are taken to represent word meanings. However,
because the emphasis in LDA is on modeling top-
ics, not word meanings, there is no guarantee that
the row (word) vectors are sensible as points in a
k-dimensional space. Indeed, we show in section
4 that using LDA in this way does not deliver ro-
bust word vectors. The semantic component of our
model shares its probabilistic foundation with LDA,
but is factored in a manner designed to discover
word vectors rather than latent topics. Some recent
work introduces extensions of LDA to capture sen-
timent in addition to topical information (Li et al,
2010; Lin and He, 2009; Boyd-Graber and Resnik,
2010). Like LDA, these methods focus on model-
ing sentiment-imbued topics rather than embedding
words in a vector space.
Vector space models (VSMs) seek to model words
directly (Turney and Pantel, 2010). Latent Seman-
tic Analysis (LSA), perhaps the best known VSM,
explicitly learns semantic word vectors by apply-
ing singular value decomposition (SVD) to factor a
term?document co-occurrence matrix. It is typical
to weight and normalize the matrix values prior to
SVD. To obtain a k-dimensional representation for a
given word, only the entries corresponding to the k
largest singular values are taken from the word?s ba-
sis in the factored matrix. Such matrix factorization-
based approaches are extremely successful in prac-
tice, but they force the researcher to make a number
of design choices (weighting, normalization, dimen-
sionality reduction algorithm) with little theoretical
guidance to suggest which to prefer.
Using term frequency (tf) and inverse document
frequency (idf) weighting to transform the values
in a VSM often increases the performance of re-
trieval and categorization systems. Delta idf weight-
ing (Martineau and Finin, 2009) is a supervised vari-
ant of idf weighting in which the idf calculation is
done for each document class and then one value
is subtracted from the other. Martineau and Finin
present evidence that this weighting helps with sen-
timent classification, and Paltoglou and Thelwall
(2010) systematically explore a number of weight-
ing schemes in the context of sentiment analysis.
The success of delta idf weighting in previous work
suggests that incorporating sentiment information
into VSM values via supervised methods is help-
ful for sentiment analysis. We adopt this insight,
but we are able to incorporate it directly into our
model?s objective function. (Section 4 compares
our approach with a representative sample of such
weighting schemes.)
3 Our Model
To capture semantic similarities among words, we
derive a probabilistic model of documents which
learns word representations. This component does
not require labeled data, and shares its foundation
with probabilistic topic models such as LDA. The
sentiment component of our model uses sentiment
annotations to constrain words expressing similar
143
sentiment to have similar representations. We can
efficiently learn parameters for the joint objective
function using alternating maximization.
3.1 Capturing Semantic Similarities
We build a probabilistic model of a document us-
ing a continuous mixture distribution over words in-
dexed by a multi-dimensional random variable ?.
We assume words in a document are conditionally
independent given the mixture variable ?. We assign
a probability to a document d using a joint distribu-
tion over the document and ?. The model assumes
each word wi ? d is conditionally independent of
the other words given ?. The probability of a docu-
ment is thus
p(d) =
?
p(d, ?)d? =
?
p(?)
N
?
i=1
p(wi|?)d?. (1)
Where N is the number of words in d and wi is
the ith word in d. We use a Gaussian prior on ?.
We define the conditional distribution p(wi|?) us-
ing a log-linear model with parameters R and b.
The energy function uses a word representation ma-
trix R ? R(? x |V |) where each word w (represented
as a one-on vector) in the vocabulary V has a ?-
dimensional vector representation ?w = Rw corre-
sponding to that word?s column in R. The random
variable ? is also a ?-dimensional vector, ? ? R?
which weights each of the ? dimensions of words?
representation vectors. We additionally introduce a
bias bw for each word to capture differences in over-
all word frequencies. The energy assigned to a word
w given these model parameters is
E(w; ?, ?w, bw) = ??T?w ? bw. (2)
To obtain the distribution p(w|?) we use a softmax,
p(w|?;R, b) = exp(?E(w; ?, ?w, bw))?
w??V exp(?E(w?; ?, ?w? , bw?))
(3)
= exp(?
T?w + bw)
?
w??V exp(?T?w? + bw?)
. (4)
The number of terms in the denominator?s sum-
mation grows linearly in |V |, making exact com-
putation of the distribution possible. For a given
?, a word w?s occurrence probability is related to
how closely its representation vector ?w matches the
scaling direction of ?. This idea is similar to the
word vector inner product used in the log-bilinear
language model of Mnih and Hinton (2007).
Equation 1 resembles the probabilistic model of
LDA (Blei et al, 2003), which models documents
as mixtures of latent topics. One could view the en-
tries of a word vector ? as that word?s association
strength with respect to each latent topic dimension.
The random variable ? then defines a weighting over
topics. However, our model does not attempt to
model individual topics, but instead directly models
word probabilities conditioned on the topic mixture
variable ?. Because of the log-linear formulation of
the conditional distribution, ? is a vector in R? and
not restricted to the unit simplex as it is in LDA.
We now derive maximum likelihood learning for
this model when given a set of unlabeled documents
D. In maximum likelihood learning we maximize
the probability of the observed data given the model
parameters. We assume documents dk ? D are i.i.d.
samples. Thus the learning problem becomes
max
R,b
p(D;R, b) =
?
dk?D
?
p(?)
Nk
?
i=1
p(wi|?;R, b)d?.
(5)
Using maximum a posteriori (MAP) estimates for ?,
we approximate this learning problem as
max
R,b
?
dk?D
p(??k)
Nk
?
i=1
p(wi|??k;R, b), (6)
where ??k denotes the MAP estimate of ? for dk.
We introduce a Frobenious norm regularization term
for the word representation matrix R. The word bi-
ases b are not regularized reflecting the fact that we
want the biases to capture whatever overall word fre-
quency statistics are present in the data. By taking
the logarithm and simplifying we obtain the final ob-
jective,
?||R||2F +
?
dk?D
?||??k||22 +
Nk
?
i=1
log p(wi|??k;R, b),
(7)
which is maximized with respect to R and b. The
hyper-parameters in the model are the regularization
144
weights (? and ?), and the word vector dimension-
ality ?.
3.2 Capturing Word Sentiment
The model presented so far does not explicitly cap-
ture sentiment information. Applying this algorithm
to documents will produce representations where
words that occur together in documents have sim-
ilar representations. However, this unsupervised
approach has no explicit way of capturing which
words are predictive of sentiment as opposed to
content-related. Much previous work in natural lan-
guage processing achieves better representations by
learning from multiple tasks (Collobert and Weston,
2008; Finkel and Manning, 2009). Following this
theme we introduce a second task to utilize labeled
documents to improve our model?s word representa-
tions.
Sentiment is a complex, multi-dimensional con-
cept. Depending on which aspects of sentiment we
wish to capture, we can give some body of text a
sentiment label s which can be categorical, continu-
ous, or multi-dimensional. To leverage such labels,
we introduce an objective that the word vectors of
our model should predict the sentiment label using
some appropriate predictor,
s? = f(?w). (8)
Using an appropriate predictor function f(x) we
map a word vector ?w to a predicted sentiment label
s?. We can then improve our word vector ?w to better
predict the sentiment labels of contexts in which that
word occurs.
For simplicity we consider the case where the sen-
timent label s is a scalar continuous value repre-
senting sentiment polarity of a document. This cap-
tures the case of many online reviews where doc-
uments are associated with a label on a star rating
scale. We linearly map such star values to the inter-
val s ? [0, 1] and treat them as a probability of pos-
itive sentiment polarity. Using this formulation, we
employ a logistic regression as our predictor f(x).
We use w?s vector representation ?w and regression
weights ? to express this as
p(s = 1|w;R,?) = ?(?T?w + bc), (9)
where ?(x) is the logistic function and ? ? R? is the
logistic regression weight vector. We additionally
introduce a scalar bias bc for the classifier.
The logistic regression weights ? and bc define
a linear hyperplane in the word vector space where
a word vector?s positive sentiment probability de-
pends on where it lies with respect to this hyper-
plane. Learning over a collection of documents re-
sults in words residing different distances from this
hyperplane based on the average polarity of docu-
ments in which the words occur.
Given a set of labeled documents D where sk is
the sentiment label for document dk, we wish to
maximize the probability of document labels given
the documents. We assume documents in the collec-
tion and words within a document are i.i.d. samples.
By maximizing the log-objective we obtain,
max
R,?,bc
|D|
?
k=1
Nk
?
i=1
log p(sk|wi;R,?, bc). (10)
The conditional probability p(sk|wi;R,?, bc) is
easily obtained from equation 9.
3.3 Learning
The full learning objective maximizes a sum of the
two objectives presented. This produces a final ob-
jective function of,
?||R||2F +
|D|
?
k=1
?||??k||22 +
Nk
?
i=1
log p(wi|??k;R, b)
+
|D|
?
k=1
1
|Sk|
Nk
?
i=1
log p(sk|wi;R,?, bc). (11)
|Sk| denotes the number of documents in the dataset
with the same rounded value of sk (i.e. sk < 0.5
and sk ? 0.5). We introduce the weighting 1|Sk| to
combat the well-known imbalance in ratings present
in review collections. This weighting prevents the
overall distribution of document ratings from affect-
ing the estimate of document ratings in which a par-
ticular word occurs. The hyper-parameters of the
model are the regularization weights (? and ?), and
the word vector dimensionality ?.
Maximizing the objective function with respect to
R, b, ?, and bc is a non-convex problem. We use
alternating maximization, which first optimizes the
145
word representations (R, b, ?, and bc) while leav-
ing the MAP estimates (??) fixed. Then we find the
new MAP estimate for each document while leav-
ing the word representations fixed, and continue this
process until convergence. The optimization algo-
rithm quickly finds a global solution for each ??k be-
cause we have a low-dimensional, convex problem
in each ??k. Because the MAP estimation problems
for different documents are independent, we can
solve them on separate machines in parallel. This
facilitates scaling the model to document collections
with hundreds of thousands of documents.
4 Experiments
We evaluate our model with document-level and
sentence-level categorization tasks in the domain of
online movie reviews. For document categoriza-
tion, we compare our method to previously pub-
lished results on a standard dataset, and introduce
a new dataset for the task. In both tasks we com-
pare our model?s word representations with several
bag of words weighting methods, and alternative ap-
proaches to word vector induction.
4.1 Word Representation Learning
We induce word representations with our model us-
ing 25,000 movie reviews from IMDB. Because
some movies receive substantially more reviews
than others, we limited ourselves to including at
most 30 reviews from any movie in the collection.
We build a fixed dictionary of the 5,000 most fre-
quent tokens, but ignore the 50 most frequent terms
from the original full vocabulary. Traditional stop
word removal was not used because certain stop
words (e.g. negating words) are indicative of senti-
ment. Stemming was not applied because the model
learns similar representations for words of the same
stem when the data suggests it. Additionally, be-
cause certain non-word tokens (e.g. ?!? and ?:-)? )
are indicative of sentiment, we allow them in our vo-
cabulary. Ratings on IMDB are given as star values
(? {1, 2, ..., 10}), which we linearly map to [0, 1] to
use as document labels when training our model.
The semantic component of our model does not
require document labels. We train a variant of our
model which uses 50,000 unlabeled reviews in addi-
tion to the labeled set of 25,000 reviews. The unla-
beled set of reviews contains neutral reviews as well
as those which are polarized as found in the labeled
set. Training the model with additional unlabeled
data captures a common scenario where the amount
of labeled data is small relative to the amount of un-
labeled data available. For all word vector models,
we use 50-dimensional vectors.
As a qualitative assessment of word represen-
tations, we visualize the words most similar to a
query word using vector similarity of the learned
representations. Given a query word w and an-
other word w? we obtain their vector representations
?w and ?w? , and evaluate their cosine similarity as
S(?w, ?w?) = ?
T
w?w?
||?w||?||?w? ||
. By assessing the simi-
larity of w with all other words w?, we can find the
words deemed most similar by the model.
Table 1 shows the most similar words to given
query words using our model?s word representations
as well as those of LSA. All of these vectors cap-
ture broad semantic similarities. However, both ver-
sions of our model seem to do better than LSA in
avoiding accidental distributional similarities (e.g.,
screwball and grant as similar to romantic) A com-
parison of the two versions of our model also begins
to highlight the importance of adding sentiment in-
formation. In general, words indicative of sentiment
tend to have high similarity with words of the same
sentiment polarity, so even the purely unsupervised
model?s results look promising. However, they also
show more genre and content effects. For exam-
ple, the sentiment enriched vectors for ghastly are
truly semantic alternatives to that word, whereas the
vectors without sentiment also contain some content
words that tend to have ghastly predicated of them.
Of course, this is only an impressionistic analysis of
a few cases, but it is helpful in understanding why
the sentiment-enriched model proves superior at the
sentiment classification results we report next.
4.2 Other Word Representations
For comparison, we implemented several alternative
vector space models that are conceptually similar to
our own, as discussed in section 2:
Latent Semantic Analysis (LSA; Deerwester et
al., 1990) We apply truncated SVD to a tf.idf
weighted, cosine normalized count matrix, which
is a standard weighting and smoothing scheme for
146
Our model Our model
Sentiment + Semantic Semantic only LSA
melancholy
bittersweet thoughtful poetic
heartbreaking warmth lyrical
happiness layer poetry
tenderness gentle profound
compassionate loneliness vivid
ghastly
embarrassingly predators hideous
trite hideous inept
laughably tube severely
atrocious baffled grotesque
appalling smack unsuspecting
lackluster
lame passable uninspired
laughable unconvincing flat
unimaginative amateurish bland
uninspired cliche?d forgettable
awful insipid mediocre
romantic
romance romance romance
love charming screwball
sweet delightful grant
beautiful sweet comedies
relationship chemistry comedy
Table 1: Similarity of learned word vectors. Each target word is given with its five most similar words using cosine
similarity of the vectors determined by each model. The full version of our model (left) captures both lexical similarity
as well as similarity of sentiment strength and orientation. Our unsupervised semantic component (center) and LSA
(right) capture semantic relations.
VSM induction (Turney and Pantel, 2010).
Latent Dirichlet Allocation (LDA; Blei et
al., 2003) We use the method described in sec-
tion 2 for inducing word representations from the
topic matrix. To train the 50-topic LDA model we
use code released by Blei et al (2003). We use the
same 5,000 term vocabulary for LDA as is used for
training word vector models. We leave the LDA
hyperparameters at their default values, though
some work suggests optimizing over priors for LDA
is important (Wallach et al, 2009).
Weighting Variants We evaluate both binary (b)
term frequency weighting with smoothed delta idf
(?t?) and no idf (n) because these variants worked
well in previous experiments in sentiment (Mar-
tineau and Finin, 2009; Pang et al, 2002). In all
cases, we use cosine normalization (c). Paltoglou
and Thelwall (2010) perform an extensive analysis
of such weighting variants for sentiment tasks.
4.3 Document Polarity Classification
Our first evaluation task is document-level senti-
ment polarity classification. A classifier must pre-
dict whether a given review is positive or negative
given the review text.
Given a document?s bag of words vector v, we
obtain features from our model using a matrix-
vector product Rv, where v can have arbitrary tf.idf
weighting. We do not cosine normalize v, instead
applying cosine normalization to the final feature
vector Rv. This procedure is also used to obtain
features from the LDA and LSA word vectors. In
preliminary experiments, we found ?bnn? weighting
to work best for v when generating document fea-
tures via the product Rv. In all experiments, we
use this weighting to get multi-word representations
147
Features PL04 Our Dataset Subjectivity
Bag of Words (bnc) 85.45 87.80 87.77
Bag of Words (b?t?c) 85.80 88.23 85.65
LDA 66.70 67.42 66.65
LSA 84.55 83.96 82.82
Our Semantic Only 87.10 87.30 86.65
Our Full 84.65 87.44 86.19
Our Full, Additional Unlabeled 87.05 87.99 87.22
Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58
Our Full + Bag of Words (bnc) 87.85 88.33 88.45
Our Full, Add?l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13
Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00
Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A
tf.?idf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A
Appraisal Taxonomy (Whitelaw et al, 2005) 90.20 N/A N/A
Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews
often used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB,
and the sentence subjectivity dataset alo released by (Pang and Lee, 2004). All tasks are balanced two-class problems.
from word vectors.
4.3.1 Pang and Lee Movie Review Dataset
The polarity dataset version 2.0 introduced by Pang
and Lee (2004) 1 consists of 2,000 movie reviews,
where each is associated with a binary sentiment po-
larity label. We report 10-fold cross validation re-
sults using the authors? published folds to make our
results comparable with others in the literature. We
use a linear support vector machine (SVM) classifier
trained with LIBLINEAR (Fan et al, 2008), and set
the SVM regularization parameter to the same value
used by Pang and Lee (2004).
Table 2 shows the classification performance of
our method, other VSMs we implemented, and pre-
viously reported results from the literature. Bag of
words vectors are denoted by their weighting nota-
tion. Features from word vector learner are denoted
by the learner name. As a control, we trained ver-
sions of our model with only the unsupervised se-
mantic component, and the full model (semantic and
sentiment). We also include results for a version of
our full model trained with 50,000 additional unla-
beled examples. Finally, to test whether our mod-
els? representations complement a standard bag of
words, we evaluate performance of the two feature
representations concatenated.
1http://www.cs.cornell.edu/people/pabo/movie-review-data
Our method?s features clearly outperform those of
other VSMs, and perform best when combined with
the original bag of words representation. The vari-
ant of our model trained with additional unlabeled
data performed best, suggesting the model can effec-
tively utilize large amounts of unlabeled data along
with labeled examples. Our method performs com-
petitively with previously reported results in spite of
our restriction to a vocabulary of only 5,000 words.
We extracted the movie title associated with each
review and found that 1,299 of the 2,000 reviews in
the dataset have at least one other review of the same
movie in the dataset. Of 406 movies with multiple
reviews, 249 have the same polarity label for all of
their reviews. Overall, these facts suggest that, rela-
tive to the size of the dataset, there are highly corre-
lated examples with correlated labels. This is a nat-
ural and expected property of this kind of document
collection, but it can have a substantial impact on
performance in datasets of this scale. In the random
folds distributed by the authors, approximately 50%
of reviews in each validation fold?s test set have a
review of the same movie with the same label in the
training set. Because the dataset is small, a learner
may perform well by memorizing the association be-
tween label and words unique to a particular movie
(e.g., character names or plot terms).
We introduce a substantially larger dataset, which
148
uses disjoint sets of movies for training and testing.
These steps minimize the ability of a learner to rely
on idiosyncratic word?class associations, thereby
focusing attention on genuine sentiment features.
4.3.2 IMDB Review Dataset
We constructed a collection of 50,000 reviews from
IMDB, allowing no more than 30 reviews per movie.
The constructed dataset contains an even number of
positive and negative reviews, so randomly guessing
yields 50% accuracy. Following previous work on
polarity classification, we consider only highly po-
larized reviews. A negative review has a score ? 4
out of 10, and a positive review has a score ? 7
out of 10. Neutral reviews are not included in the
dataset. In the interest of providing a benchmark for
future work in this area, we release this dataset to
the public.2
We evenly divided the dataset into training and
test sets. The training set is the same 25,000 la-
beled reviews used to induce word vectors with our
model. We evaluate classifier performance after
cross-validating classifier parameters on the training
set, again using a linear SVM in all cases. Table 2
shows classification performance on our subset of
IMDB reviews. Our model showed superior per-
formance to other approaches, and performed best
when concatenated with bag of words representa-
tion. Again the variant of our model which utilized
extra unlabeled data during training performed best.
Differences in accuracy are small, but, because
our test set contains 25,000 examples, the variance
of the performance estimate is quite low. For ex-
ample, an accuracy increase of 0.1% corresponds to
correctly classifying an additional 25 reviews.
4.4 Subjectivity Detection
As a second evaluation task, we performed sentence-
level subjectivity classification. In this task, a clas-
sifier is trained to decide whether a given sentence is
subjective, expressing the writer?s opinions, or ob-
jective, expressing purely facts. We used the dataset
of Pang and Lee (2004), which contains subjective
sentences from movie review summaries and objec-
tive sentences from movie plot summaries. This task
2Dataset and further details are available online at:
http://www.andrew-maas.net/data/sentiment
is substantially different from the review classifica-
tion task because it uses sentences as opposed to en-
tire documents and the target concept is subjectivity
instead of opinion polarity. We randomly split the
10,000 examples into 10 folds and report 10-fold
cross validation accuracy using the SVM training
protocol of Pang and Lee (2004).
Table 2 shows classification accuracies from the
sentence subjectivity experiment. Our model again
provided superior features when compared against
other VSMs. Improvement over the bag-of-words
baseline is obtained by concatenating the two feature
vectors.
5 Discussion
We presented a vector space model that learns word
representations captuing semantic and sentiment in-
formation. The model?s probabilistic foundation
gives a theoretically justified technique for word
vector induction as an alternative to the overwhelm-
ing number of matrix factorization-based techniques
commonly used. Our model is parametrized as a
log-bilinear model following recent success in us-
ing similar techniques for language models (Bengio
et al, 2003; Collobert and Weston, 2008; Mnih and
Hinton, 2007), and it is related to probabilistic latent
topic models (Blei et al, 2003; Steyvers and Grif-
fiths, 2006). We parametrize the topical component
of our model in a manner that aims to capture word
representations instead of latent topics. In our ex-
periments, our method performed better than LDA,
which models latent topics directly.
We extended the unsupervised model to incor-
porate sentiment information and showed how this
extended model can leverage the abundance of
sentiment-labeled texts available online to yield
word representations that capture both sentiment
and semantic relations. We demonstrated the util-
ity of such representations on two tasks of senti-
ment classification, using existing datasets as well
as a larger one that we release for future research.
These tasks involve relatively simple sentiment in-
formation, but the model is highly flexible in this
regard; it can be used to characterize a wide variety
of annotations, and thus is broadly applicable in the
growing areas of sentiment analysis and retrieval.
149
Acknowledgments
This work is supported by the DARPA Deep Learn-
ing program under contract number FA8650-10-C-
7020, an NSF Graduate Fellowship awarded to AM,
and ONR grant No. N00014-10-1-0109 to CP.
References
C. O. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: machine learning for text-based emotion predic-
tion. In Proceedings of HLT/EMNLP, pages 579?586.
A. Andreevskaia and S. Bergler. 2006. Mining Word-
Net for fuzzy sentiment: sentiment tag extraction from
WordNet glosses. In Proceedings of the European
ACL, pages 209?216.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
a neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155, August.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning Re-
search, 3:993?1022, May.
J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment
analysis across languages: multilingual supervised la-
tent Dirichlet alocation. In Proceedings of EMNLP,
pages 45?55.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing. In Proceedings of the
ICML, pages 160?167.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41:391?407, September.
R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and
C. J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. The Journal of Machine Learning
Research, 9:1871?1874, August.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In Proceedings of NAACL,
pages 326?334.
A. B. Goldberg and J. Zhu. 2006. Seeing stars when
there aren?t many stars: graph-based semi-supervised
learning for sentiment categorization. In TextGraphs:
HLT/NAACL Workshop on Graph-based Algorithms
for Natural Language Processing, pages 45?52.
T. Jay. 2000. Why We Curse: A Neuro-Psycho-
Social Theory of Speech. John Benjamins, Philadel-
phia/Amsterdam.
D. Kaplan. 1999. What is meaning? Explorations in the
theory of Meaning as Use. Brief version ? draft 1.
Ms., UCLA.
A. Kennedy and D. Inkpen. 2006. Sentiment clas-
sification of movie reviews using contextual valence
shifters. Computational Intelligence, 22:110?125,
May.
F. Li, M. Huang, and X. Zhu. 2010. Sentiment analysis
with global topics and local dependency. In Proceed-
ings of AAAI, pages 1371?1376.
C. Lin and Y. He. 2009. Joint sentiment/topic model for
sentiment analysis. In Proceeding of the 18th ACM
Conference on Information and Knowledge Manage-
ment, pages 375?384.
J. Martineau and T. Finin. 2009. Delta tfidf: an improved
feature space for sentiment analysis. In Proceedings
of the 3rd AAAI International Conference on Weblogs
and Social Media, pages 258?261.
A. Mnih and G. E. Hinton. 2007. Three new graphical
models for statistical language modelling. In Proceed-
ings of the ICML, pages 641?648.
G. Paltoglou and M. Thelwall. 2010. A study of informa-
tion retrieval weighting schemes for sentiment analy-
sis. In Proceedings of the ACL, pages 1386?1395.
B. Pang and L. Lee. 2004. A sentimental education:
sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
pages 271?278.
B. Pang and L. Lee. 2005. Seeing stars: exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of ACL, pages 115?
124.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learning
techniques. In Proceedings of EMNLP, pages 79?86.
C. Potts. 2007. The expressive dimension. Theoretical
Linguistics, 33:165?197.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings of
NAACL, pages 300?307.
M. Steyvers and T. L. Griffiths. 2006. Probabilistic topic
models. In T. Landauer, D McNamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road
to Meaning.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proceedings of the ACL, page
384394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
H. Wallach, D. Mimno, and A. McCallum. 2009. Re-
thinking LDA: why priors matter. In Proceedings of
NIPS, pages 1973?1981.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proceedings
of CIKM, pages 625?631.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of AAAI, pages 761?769.
150
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 250?259,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A computational approach to politeness with application to social factors
Cristian Danescu-Niculescu-Mizil??, Moritz Sudhof?, Dan Jurafsky?,
Jure Leskovec?, and Christopher Potts?
?Computer Science Department, ?Linguistics Department
??Stanford University, ?Max Planck Institute SWS
cristiand|jure@cs.stanford.edu, sudhof|jurafsky|cgpotts@stanford.edu
Abstract
We propose a computational framework
for identifying linguistic aspects of polite-
ness. Our starting point is a new corpus
of requests annotated for politeness, which
we use to evaluate aspects of politeness
theory and to uncover new interactions
between politeness markers and context.
These findings guide our construction of
a classifier with domain-independent lexi-
cal and syntactic features operationalizing
key components of politeness theory, such
as indirection, deference, impersonaliza-
tion and modality. Our classifier achieves
close to human performance and is effec-
tive across domains. We use our frame-
work to study the relationship between po-
liteness and social power, showing that po-
lite Wikipedia editors are more likely to
achieve high status through elections, but,
once elevated, they become less polite. We
see a similar negative correlation between
politeness and power on Stack Exchange,
where users at the top of the reputation
scale are less polite than those at the bot-
tom. Finally, we apply our classifier to
a preliminary analysis of politeness vari-
ation by gender and community.
1 Introduction
Politeness is a central force in communication, ar-
guably as basic as the pressure to be truthful, in-
formative, relevant, and clear (Grice, 1975; Leech,
1983; Brown and Levinson, 1978). Natural lan-
guages provide numerous and diverse means for
encoding politeness and, in conversation, we con-
stantly make choices about where and how to use
these devices. Kaplan (1999) observes that ?peo-
ple desire to be paid respect? and identifies hon-
orifics and other politeness markers, like please,
as ?the coin of that payment?. In turn, polite-
ness markers are intimately related to the power
dynamics of social interactions and are often a
decisive factor in whether those interactions go
well or poorly (Gyasi Obeng, 1997; Chilton, 1990;
Andersson and Pearson, 1999; Rogers and Lee-
Wong, 2003; Holmes and Stubbe, 2005).
The present paper develops a computational
framework for identifying and characterizing po-
liteness marking in requests. We focus on re-
quests because they involve the speaker imposing
on the addressee, making them ideal for exploring
the social value of politeness strategies (Clark and
Schunk, 1980; Francik and Clark, 1985). Requests
also stimulate extensive use of what Brown and
Levinson (1987) call negative politeness: speaker
strategies for minimizing (or appearing to mini-
mize) the imposition on the addressee, for exam-
ple, by being indirect (Would you mind) or apolo-
gizing for the imposition (I?m terribly sorry, but)
(Lakoff, 1973; Lakoff, 1977; Brown and Levin-
son, 1978).
Our investigation is guided by a new corpus
of requests annotated for politeness. The data
come from two large online communities in which
members frequently make requests of other mem-
bers: Wikipedia, where the requests involve edit-
ing and other administrative functions, and Stack
Exchange, where the requests center around a di-
verse range of topics (e.g., programming, garden-
ing, cycling). The corpus confirms the broad out-
lines of linguistic theories of politeness pioneered
by Brown and Levinson (1987), but it also reveals
new interactions between politeness markings and
the morphosyntactic context. For example, the po-
liteness of please depends on its syntactic position
and the politeness markers it co-occurs with.
Using this corpus, we construct a polite-
ness classifier with a wide range of domain-
independent lexical, sentiment, and dependency
features operationalizing key components of po-
250
liteness theory, including not only the negative
politeness markers mentioned above but also el-
ements of positive politeness (gratitude, positive
and optimistic sentiment, solidarity, and inclusive-
ness). The classifier achieves near human-level ac-
curacy across domains, which highlights the con-
sistent nature of politeness strategies and paves the
way to using the classifier to study new data.
Politeness theory predicts a negative correlation
between politeness and the power of the requester,
where power is broadly construed to include so-
cial status, authority, and autonomy (Brown and
Levinson, 1987). The greater the speaker?s power
relative to her addressee, the less polite her re-
quests are expected to be: there is no need for her
to incur the expense of paying respect, and failing
to make such payments can invoke, and hence re-
inforce, her power. We support this prediction by
applying our politeness framework to Wikipedia
and Stack Exchange, both of which provide in-
dependent measures of social status. We show
that polite Wikipedia editors are more likely to
achieve high status through elections; however,
once elected, they become less polite. Similarly,
on Stack Exchange, we find that users at the top of
the reputation scale are less polite than those at the
bottom.
Finally, we briefly address the question of how
politeness norms vary across communities and so-
cial groups. Our findings confirm established re-
sults about the relationship between politeness and
gender, and they identify substantial variation in
politeness across different programming language
subcommunities on Stack Exchange.
2 Politeness data
Requests involve an imposition on the addressee,
making them a natural domain for studying the
inter-connections between linguistic aspects of po-
liteness and social variables.
Requests in online communities We base our
analysis on two online communities where re-
quests have an important role: the Wikipedia
community of editors and the Stack Exchange
question-answer community.1 On Wikipedia, to
coordinate on the creation and maintenance of
the collaborative encyclopedia, editors can in-
teract with each other on user talk-pages;2 re-
1http://stackexchange.com/about
2http://en.wikipedia.org/wiki/
Wikipedia:User_pages
quests posted on a user talk-page, although pub-
lic, are generally directed to the owner of the talk-
page. On Stack Exchange, users often comment
on existing posts requesting further information or
proposing edits; these requests are generally di-
rected to the authors of the original posts.
Both communities are not only rich in user-
to-user requests, but these requests are also part
of consequential conversations, not empty social
banter; they solicit specific information or con-
crete actions, and they expect a response.
Politeness annotation Computational studies of
politeness, or indeed any aspect of linguistic prag-
matics, demand richly labeled data. We there-
fore label a large portion of our request data
(over 10,000 utterances) using Amazon Mechan-
ical Turk (AMT), creating the largest corpus with
politeness annotations (see Table 1 for details).3
We choose to annotate requests containing ex-
actly two sentences, where the second sentence
is the actual request (and ends with a question
mark). This provides enough context to the an-
notators while also controlling for length effects.
Each annotator was instructed to read a batch of
13 requests and consider them as originating from
a co-worker by email. For each request, the anno-
tator had to indicate how polite she perceived the
request to be by using a slider with values rang-
ing from ?very impolite? to ?very polite?.4 Each
request was labeled by five different annotators.
We vetted annotators by restricting their resi-
dence to be in the U.S. and by conducting a lin-
guistic background questionnaire. We also gave
them a paraphrasing task shown to be effective
for verifying and eliciting linguistic attentiveness
(Munro et al, 2010), and we monitored the an-
notation job and manually filtered out annotators
who submitted uniform or seemingly random an-
notations.
Because politeness is highly subjective and an-
notators may have inconsistent scales, we ap-
plied the standard z-score normalization to each
worker?s scores. Finally, we define the politeness
score (henceforth politeness) of a request as the
average of the five scores assigned by the annota-
tors. The distribution of resulting request scores
(shown in Figure 1) has an average of 0 and stan-
3Publicly available at http://www.mpi-sws.org/
?cristian/Politeness.html4We used non-categorical ratings for finer granularity and
to help account for annotators? different perception scales.
251
domain #requests #annotated #annotators
Wiki 35,661 4,353 219
SE 373,519 6,604 212
Table 1: Summary of the request data and its po-
liteness annotations.
Figure 1: Distribution of politeness scores. Posi-
tive scores indicate requests perceived as polite.
dard deviation of 0.7 for both domains; positive
values correspond to polite requests (i.e., requests
with normalized annotations towards the ?very po-
lite? extreme) and negative values to impolite re-
quests. A summary of all our request data is shown
in Table 1.
Inter-annotator agreement To evaluate the re-
liability of the annotations we measure the inter-
annotator agreement by computing, for each batch
of 13 documents that were annotated by the same
set of 5 users, the mean pairwise correlation of the
respective scores. For reference, we compute the
same quantities after randomizing the scores by
sampling from the observed distribution of polite-
ness scores. As shown in Figure 2, the labels are
coherent and significantly different from the ran-
domized procedure (p < 0.0001 according to a
Wilcoxon signed rank test).5
Binary perception Although we did not im-
pose a discrete categorization of politeness, we
acknowledge an implicit binary perception of the
phenomenon: whenever an annotator moved a
slider in one direction or the other, she made a
binary politeness judgment. However, the bound-
5The commonly used Cohen/Fleiss Kappa agreement
measures are not suitable for this type of annotation, in which
labels are continuous rather than categorical.
Figure 2: Inter-annotator pairwise correlation,
compared to the same measure after randomizing
the scores.
Quartile: 1st 2nd 3rd 4th
Wiki 62% 8% 3% 51%
SE 37% 4% 6% 46%
Table 2: The percentage of requests for which all
five annotators agree on binary politeness. The
4th quartile contains the requests with the top 25%
politeness scores in the data. (For reference, ran-
domized scoring yields agreement percentages of
<20% for all quartiles.)
ary between somewhat polite and somewhat im-
polite requests can be blurry. To test this intuition,
we break the set of annotated requests into four
groups, each corresponding to a politeness score
quartile. For each quartile, we compute the per-
centage of requests for which all five annotators
made the same binary politeness judgment. As
shown in Table 2, full agreement is much more
common in the 1st (bottom) and 4th (top) quar-
tiles than in the middle quartiles. This suggests
that the politeness scores assigned to requests that
are only somewhat polite or somewhat impolite
are less reliable and less tied to an intuitive notion
of binary politeness. This discrepancy motivates
our choice of classes in the prediction experiments
(Section 4) and our use of the top politeness quar-
tile (the 25% most polite requests) as a reference
in our subsequent discussion.
3 Politeness strategies
As we mentioned earlier, requests impose on the
addressee, potentially placing her in social peril if
she is unwilling or unable to comply. Requests
therefore naturally give rise to the negative po-
252
liteness strategies of Brown and Levinson (1987),
which are attempts to mitigate these social threats.
These strategies are prominent in Table 3, which
describes the core politeness markers we analyzed
in our corpus of Wikipedia requests. We do not
include the Stack Exchange data in this analysis,
reserving it as a ?test community? for our predic-
tion task (Section 4).
Requests exhibiting politeness markers are au-
tomatically extracted using regular expression
matching on the dependency parse obtained by the
Stanford Dependency Parser (de Marneffe et al,
2006), together with specialized lexicons. For ex-
ample, for the hedges marker (Table 3, line 19),
we match all requests containing a nominal subject
dependency edge pointing out from a hedge verb
from the hedge list created by Hyland (2005). For
each politeness strategy, Table 3 shows the aver-
age politeness score of the respective requests (as
described in Section 2; positive numbers indicate
polite requests), and their top politeness quartile
membership (i.e., what percentage fall within the
top quartile of politeness scores). As discussed at
the end of Section 2, the top politeness quartile
gives a more robust and more intuitive measure of
politeness. For reference, a random sample of re-
quests will have a 0 politeness score and a 25% top
quartile membership; in both cases, larger num-
bers indicate higher politeness.
Gratitude and deference (lines 1?2) are ways
for the speaker to incur a social cost, helping to
balance out the burden the request places on the
addressee. Adopting Kaplan (1999)?s metaphor,
these are the coin of the realm when it comes to
paying the addressee respect. Thus, they are indi-
cators of positive politeness.
Terms from the sentiment lexicon (Liu et al,
2005) are also tools for positive politeness, either
by emphasizing a positive relationship with the ad-
dressee (line 4), or being impolite by using nega-
tive sentiment that damages this positive relation-
ship (line 5). Greetings (line 3) are another way to
build a positive relationship with the addressee.
The remainder of the cues in Table 3 are neg-
ative politeness strategies, serving the purpose of
minimizing, at least in appearance, the imposition
on the addressee. Apologizing (line 6) deflects the
social threat of the request by attuning to the impo-
sition itself. Being indirect (line 9) is another way
to minimize social threat. This strategy allows the
speaker to avoid words and phrases convention-
ally associated with requests. First-person plural
forms like we and our (line 15) are also ways of
being indirect, as they create the sense that the
burden of the request is shared between speaker
and addressee (We really should . . . ). Though in-
directness is not invariably interpreted as polite-
ness marking (Blum-Kulka, 2003), it is nonethe-
less a reliable marker of it, as our scores indicate.
What?s more, direct variants (imperatives, state-
ments about the addressee?s obligations) are less
polite (lines 10?11).
Indirect strategies also combine with hedges
(line 19) conveying that the addressee is unlikely
to accept the burden (Would you by any chance
. . . ?, Would it be at all possible . . . ?). These too
serve to provide the addressee with a face-saving
way to deny the request. We even see subtle effects
of modality at work here: the irrealis, counterfac-
tual forms would and could are more polite than
their ability (dispositional) or future-oriented vari-
ants can and will; compare lines 12 and 13. This
parallels the contrast between factuality markers
(impolite; line 20) and hedging (polite; line 19).
Many of these features are correlated with each
other, in keeping with the insight of Brown and
Levinson (1987) that politeness markers are of-
ten combined to create a cumulative effect of in-
creased politeness. Our corpora also highlight in-
teractions that are unexpected (or at least unac-
counted for) on existing theories of politeness. For
example, sentence-medial please is polite (line 7),
presumably because of its freedom to combine
with other negative politeness strategies (Could
you please . . . ). In contrast, sentence-initial please
is impolite (line 8), because it typically signals a
more direct strategy (Please do this), which can
make the politeness marker itself seem insincere.
We see similar interactions between pronominal
forms and syntactic structure: sentence-initial you
is impolite (You need to . . . ), whereas sentence-
medial you is often part of the indirect strategies
we discussed above (Would/Could you . . . ).
4 Predicting politeness
We now show how our linguistic analysis can be
used in a machine learning model for automati-
cally classifying requests according to politeness.
A classifier can help verify the predictive power,
robustness, and domain-independent generality of
the linguistic strategies of Section 3. Also, by pro-
viding automatic politeness judgments for large
253
Strategy Politeness In top quartile Example
1. Gratitude 0.87*** 78%*** I really appreciate that you?ve done them.
2. Deference 0.78*** 70%*** Nice work so far on your rewrite.
3. Greeting 0.43*** 45%*** Hey, I just tried to . . .
4. Positive lexicon 0.12*** 32%*** Wow! / This is a great way to deal. . .
5. Negative lexicon -0.13*** 22%** If you?re going to accuse me . . .
6. Apologizing 0.36*** 53%*** Sorry to bother you . . .
7. Please 0.49*** 57%*** Could you please say more. . .
8. Please start ?0.30* 22% Please do not remove warnings . . .
9. Indirect (btw) 0.63*** 58%** By the way, where did you find . . .
10. Direct question ?0.27*** 15%*** What is your native language?
11. Direct start ?0.43*** 9%*** So can you retrieve it or not?
12. Counterfactual modal 0.47*** 52%*** Could/Would you . . .
13. Indicative modal 0.09 27% Can/Will you . . .
14. 1st person start 0.12*** 29%** I have just put the article . . .
15. 1st person pl. 0.08* 27% Could we find a less complex name . . .
16. 1st person 0.08*** 28%*** It is my view that ...
17. 2nd person 0.05*** 30%*** But what?s the good source you have in mind?
18. 2nd person start ?0.30*** 17%** You?ve reverted yourself . . .
19. Hedges 0.14*** 28% I suggest we start with . . .
20. Factuality ?0.38*** 13%*** In fact you did link, . . .
Table 3: Positive (1-5) and negative (6?20) politeness strategies and their relation to human perception of
politeness. For each strategy we show the average (human annotated) politeness scores for the requests
exhibiting that strategy (compare with 0 for a random sample of requests; a positive number indicates
the strategy is perceived as being polite), as well as the percentage of requests exhibiting the respective
strategy that fall in the top quartile of politeness scores (compare with 25% for a random sample of
requests). Throughout the paper: for politeness scores, statistical significance is calculated by comparing
the set of requests exhibiting the strategy with the rest using a Mann-Whitney-Wilcoxon U test; for top
quartile membership a binomial test is used.
amounts of new data on a scale unfeasible for hu-
man annotation, it can also enable a detailed anal-
ysis of the relation between politeness and social
factors (Section 5).
Task setup To evaluate the robustness and
domain-independence of the analysis from Sec-
tion 3, we run our prediction experiments on two
very different domains. We treat Wikipedia as a
?development domain? since we used it for de-
veloping and identifying features and for training
our models. Stack Exchange is our ?test domain?
since it was not used for identifying features. We
take the model (features and weights) trained on
Wikipedia and use them to classify requests from
Stack Exchange.
We consider two classes of requests: polite
and impolite, defined as the top and, respectively,
bottom quartile of requests when sorted by their
politeness score (based on the binary notion of
politeness discussed in Section 2). The classes
are therefore balanced, with each class consisting
of 1,089 requests for the Wikipedia domain and
1,651 requests for the Stack Exchange domain.
We compare two classifiers ? a bag of words
classifier (BOW) and a linguistically informed
classifier (Ling.) ? and use human labelers as a
reference point. The BOW classifier is an SVM
using a unigram feature representation.6 We con-
sider this to be a strong baseline for this new
6Unigrams appearing less than 10 times are excluded.
254
classification task, especially considering the large
amount of training data available. The linguisti-
cally informed classifier (Ling.) is an SVM using
the linguistic features listed in Table 3 in addition
to the unigram features. Finally, to obtain a ref-
erence point for the prediction task we also collect
three new politeness annotations for each of the re-
quests in our dataset using the same methodology
described in Section 2. We then calculate human
performance on the task (Human) as the percent-
age of requests for which the average score from
the additional annotations matches the binary po-
liteness class of the original annotations (e.g., a
positive score corresponds to the polite class).
Classification results We evaluate the classi-
fiers both in an in-domain setting, with a standard
leave-one-out cross validation procedure, and in a
cross-domain setting, where we train on one do-
main and test on the other (Table 4). For both our
development and our test domains, and in both the
in-domain and cross-domain settings, the linguis-
tically informed features give 3-4% absolute im-
provement over the bag of words model. While
the in-domain results are within 3% of human per-
formance, the greater room for improvement in the
cross-domain setting motivates further research on
linguistic cues of politeness.
The experiments in this section confirm that
our theory-inspired features are indeed effective in
practice, and generalize well to new domains. In
the next section we exploit this insight to automat-
ically annotate a much larger set of requests (about
400,000) with politeness labels, enabling us to re-
late politeness to several social variables and out-
comes. For new requests, we use class probabil-
ity estimates obtained by fitting a logistic regres-
sion model to the output of the SVM (Witten and
Frank, 2005) as predicted politeness scores (with
values between 0 and 1; henceforth politeness, by
abuse of language).
5 Relation to social factors
We now apply our framework to studying the rela-
tionship between politeness and social variables,
focussing on social power dynamics. Encour-
aged by the close-to-human performance of our
in-domain classifiers, we use them to assign po-
liteness labels to our full dataset and then compare
these labels to independent measures of power and
status in our data. The results closely match those
obtained with human-labeled data alone, thereby
In-domain Cross-domain
Train Wiki SE Wiki SE
Test Wiki SE SE Wiki
BOW 79.84% 74.47% 64.23% 72.17%
Ling. 83.79% 78.19% 67.53% 75.43%
Human 86.72% 80.89% 80.89% 86.72%
Table 4: Accuracies of our two classifiers for
Wikipedia (Wiki) and Stack Exchange (SE), for
in-domain and cross-domain settings. Human per-
formance is included as a reference point. The ran-
dom baseline performance is 50%.
supporting the use of computational methods to
pursue questions about social variables.
5.1 Relation to social outcome
Earlier, we characterized politeness markings as
currency used to pay respect. Such language is
therefore costly in a social sense, and, relatedly,
tends to incur costs in terms of communicative ef-
ficiency (Van Rooy, 2003). Are these costs worth
paying? We now address this question by studying
politeness in the context of the electoral system of
the Wikipedia community of editors.
Among Wikipedia editors, status is a salient so-
cial variable (Anderson et al, 2012). Administra-
tors (admins) are editors who have been granted
certain rights, including the ability to block other
editors and to protect or delete articles.7 Ad-
mins have a higher status than common editors
(non-admins), and this distinction seems to be
widely acknowledged by the community (Burke
and Kraut, 2008b; Leskovec et al, 2010; Danescu-
Niculescu-Mizil et al, 2012). Aspiring editors
become admins through public elections,8 so we
know when the status change from non-admin to
admins occurred and can study users? language
use in relation to that time.
To see whether politeness correlates with even-
tual high status, we compare, in Table 5, the po-
liteness levels of requests made by users who will
eventually succeed in becoming administrators
(Eventual status: Admins) with requests made by
users who are not admins (Non-admins).9 We ob-
serve that admins-to-be are significantly more po-
7http://en.wikipedia.org/wiki/
Wikipedia:Administrators
8http://en.wikipedia.org/wiki/
Wikipedia:Requests_for_adminship
9We consider only requests made up to one month before
the election, to avoid confusion with pre-election behavior.
255
Eventual status Politeness Top quart.
Admins 0.46** 30%***
Non-admins 0.39*** 25%
Failed 0.37** 22%
Table 5: Politeness and status. Editors who
will eventually become admins are more polite
than non-admins (p<0.001 according to a Mann-
Whitney-Wilcoxon U test) and than editors who
will eventually fail to become admins (p<0.001).
Out of their requests, 30% are rated in the top po-
liteness quartile (significantly more than the 25%
of a random sample; p<0.001 according to a bi-
nomial test). This analysis was conducted on 31k
requests (1.4k for Admins, 28.9k for Non-admins,
652 for Failed).
lite than non-admins. One might wonder whether
this merely reflects the fact that not all users aspire
to become admins, and those that do are more po-
lite. To address this, we also consider users who
ran for adminship but did not earn community ap-
proval (Eventual status: Failed). These users are
also significantly less polite than their successful
counterparts, indicating that politeness indeed cor-
relates with a positive social outcome here.
5.2 Politeness and power
We expect a rise in status to correlate with a de-
cline in politeness (as predicted by politeness the-
ory, and discussed in Section 1). The previous sec-
tion does not test this hypothesis, since all editors
compared in Table 5 had the same (non-admin)
status when writing the requests. However, our
data does provide three ways of testing this hy-
pothesis.
First, after the adminship elections, successful
editors get a boost in power by receiving admin
privileges. Figure 3 shows that this boost is mir-
rored by a significant decrease in politeness (blue,
diamond markers). Losing an election has the op-
posite effect on politeness (red, circle markers),
perhaps as a consequence of reinforced low status.
Second, Stack Exchange allows us to test more
situational power effects.10 On the site, users re-
quest, from the community, information they are
lacking. This informational asymmetry between
the question-asker and his audience puts him at
10We restrict all experiments in this section to the largest
subcommunity of Stack Exchange, namely Stack Overflow.
Before election Election After election
0.41
0.37
0.39
0.46
Pred
icte
d po
liten
ess 
sco
res
Successful candidatesFailed candidates
Figure 3: Successful and failed candidates be-
fore and after elections. Editors that will even-
tually succeed (diamond marker) are significantly
more polite than those that will fail (circle mark-
ers). Following the elections, successful editors
become less polite while unsuccessful editors be-
come more polite.
a social disadvantage. We therefore expect the
question-asker to be more polite than the people
who respond. Table 6 shows that this expectation
is born out: comments posted to a thread by the
original question-asker are more polite than those
posted by other users.
Role Politeness Top quart.
Question-asker 0.65*** 32%***
Answer-givers 0.52*** 20%***
Table 6: Politeness and dependence. Requests
made in comments posted by the question-asker
are significantly more polite than the other re-
quests. Analysis conducted on 181k requests
(106k for question-askers, 75k for answer-givers).
Third, Stack Exchange allows us to examine
power in the form of authority, through the com-
munity?s reputation system. Again, we see a neg-
ative correlation between politeness and power,
even after controlling for the role of the user mak-
ing the requests (i.e., Question-asker or Answer-
giver). Table 7 summarizes the results.11
Human validation The above analyses are
based on predicted politeness from our classifier.
This allows us to use the entire request data cor-
11Since our data does not contain time stamps for reputa-
tion scores, we only consider requests that were issued in the
six months prior to the available snapshot.
256
Reputation level Politeness Top quart.
Low reputation 0.68*** 27%***
Middle reputation 0.66*** 25%
High reputation 0.64*** 23%***
Table 7: Politeness and Stack Exchange reputation
(texts by question-askers only). High-reputation
users are less polite. Analysis conducted on 25k
requests (4.5k low, 12.5k middle, 8.4k high).
pus to test our hypotheses and to apply precise
controls to our experiments (such as restricting
our analysis to question-askers in the reputation
experiment). In order to validate this methodol-
ogy, we turned again to human annotation: we
collected additional politeness annotation for the
types of requests involved in the newly designed
experiments. When we re-ran our experiments on
human-labeled data alone we obtained the same
qualitative results, with statistical significance al-
ways lower than 0.01.12
Prediction-based interactions The human val-
idation of classifier-based results suggests that
our prediction framework can be used to explore
differences in politeness levels across factors of
interest, such as communities, geographical re-
gions and gender, even where gathering suffi-
cient human-annotated data is infeasible. We
mention just a few such preliminary results here:
(i) Wikipedians from the U.S. Midwest are most
polite (when compared to other census-defined
regions), (ii) female Wikipedians are generally
more polite (consistent with prior studies in which
women are more polite in a variety of domains;
(Herring, 1994)), and (iii) programming language
communities on Stack Exchange vary significantly
by politeness (Table 8; full disclosure: our analy-
ses were conducted in Python).
6 Related work
Politeness has been a central concern of modern
pragmatic theory since its inception (Grice, 1975;
Lakoff, 1973; Lakoff, 1977; Leech, 1983; Brown
and Levinson, 1978), because it is a source of
pragmatic enrichment, social meaning, and cul-
tural variation (Harada, 1976; Matsumoto, 1988;
12However, due to the limited size of the human-labeled
data, we could not control for the role of the user in the Stack
Exchange reputation experiment.
PL name Politeness Top quartile
Python 0.47*** 23%
Perl 0.49 24%
PHP 0.51 24%
Javascript 0.53** 26%**
Ruby 0.59*** 28%*
Table 8: Politeness of requests from different lan-
guage communities on Stack Exchange.
Ide, 1989; Blum-Kulka and Kasper, 1990; Blum-
Kulka, 2003; Watts, 2003; Byon, 2006). The start-
ing point for most research is the theory of Brown
and Levinson (1987). Aspects of this theory
have been explored from game-theoretic perspec-
tives (Van Rooy, 2003) and implemented in lan-
guage generation systems for interactive narratives
(Walker et al, 1997), cooking instructions, (Gupta
et al, 2007), translation (Faruqui and Pado, 2012),
spoken dialog (Wang et al, 2012), and subjectivity
analysis (Abdul-Mageed and Diab, 2012), among
others.
In recent years, politeness has been studied in
online settings. Researchers have identified vari-
ation in politeness marking across different con-
texts and media types (Herring, 1994; Brennan
and Ohaeri, 1999; Duthler, 2006) and between
different social groups (Burke and Kraut, 2008a).
The present paper pursues similar goals using or-
ders of magnitude more data, which facilitates a
fuller survey of different politeness strategies.
Politeness marking is one aspect of the broader
issue of how language relates to power and status,
which has been studied in the context of workplace
discourse (Bramsen et al, ; Diehl et al, 2007;
Peterson et al, 2011; Prabhakaran et al, 2012;
Gilbert, 2012; McCallum et al, 2007) and so-
cial networking (Scholand et al, 2010). However,
this research focusses on domain-specific textual
cues, whereas the present work seeks to lever-
age domain-independent politeness cues, build-
ing on the literature on how politeness affects
worksplace social dynamics and power structures
(Gyasi Obeng, 1997; Chilton, 1990; Andersson
and Pearson, 1999; Rogers and Lee-Wong, 2003;
Holmes and Stubbe, 2005). Burke and Kraut
(2008b) study the question of how and why spe-
cific individuals rise to administrative positions
on Wikipedia, and Danescu-Niculescu-Mizil et al
(2012) show that power differences on Wikipedia
257
are revealed through aspects of linguistic accom-
modation. The present paper complements this
work by revealing the role of politeness in social
outcomes and power relations.
7 Conclusion
We construct and release a large collection of
politeness-annotated requests and use it to evalu-
ate key aspects of politeness theory. We build a
politeness classifier that achieves near-human per-
formance and use it to explore the relation between
politeness and social factors such as power, status,
gender, and community membership. We hope the
publicly available collection of annotated requests
enables further study of politeness and its relation
to social factors, as this paper has only begun to
explore this area.
Acknowledgments
We thank Jean Wu for running the AMT an-
notation task, and all the participating turkers.
We thank Diana Minculescu and the anonymous
reviewers for their helpful comments. This
work was supported in part by NSF IIS-1016909,
CNS-1010921, IIS-1149837, IIS-1159679, ARO
MURI, DARPA SMISC, Okawa Foundation, Do-
como, Boeing, Allyes, Volkswagen, Intel, Alfred
P. Sloan Fellowship, the Microsoft Faculty Fel-
lowship, the Gordon and Dailey Pattee Faculty
Fellowship, and the Center for Advanced Study in
the Behavioral Sciences at Stanford.
References
Muhammad Abdul-Mageed and Mona Diab. 2012.
AWATIF: A multi-genre corpus for Modern Stan-
dard Arabic subjectivity and sentiment analysis. In
Proceedings of LREC, pages 3907?3914.
Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg,
and Jure Leskovec. 2012. Effects of user similarity
in social media. In Proceedings of WSDM, pages
703?712.
Lynne M. Andersson and Christine M. Pearson. 1999.
Tit for tat? the spiraling effect of incivility in the
workplace. The Academy of Management Review,
24(3):452?471.
Shoshana Blum-Kulka and Gabriele Kasper. 1990.
Special issue on politeness. Journal of Pragmatics,
144(2).
Shoshana Blum-Kulka. 2003. Indirectness and po-
liteness in requests: Same or different? Journal of
Pragmatics, 11(2):131?146.
Philip Bramsen, Martha Escobar-Molana, Ami Patel,
and Rafael Alonso. Extracting social power rela-
tionships from natural language. In Proceedings of
ACL, pages 773?782.
Susan E Brennan and Justina O Ohaeri. 1999. Why
do electronic conversations seem less polite? the
costs and benefits of hedging. SIGSOFT Softw. Eng.
Notes, 24(2):227?235.
Penelope Brown and Stephen C. Levinson. 1978.
Universals in language use: Politeness phenomena.
In Esther N. Goody, editor, Questions and Polite-
ness: Strategies in Social Interaction, pages 56?311,
Cambridge. Cambridge University Press.
Penelope Brown and Stephen C Levinson. 1987. Po-
liteness: some universals in language usage. Cam-
bridge University Press.
Moira Burke and Robert Kraut. 2008a. Mind your
Ps and Qs: the impact of politeness and rudeness
in online communities. In Proceedings of CSCW,
pages 281?284.
Moira Burke and Robert Kraut. 2008b. Taking up the
mop: identifying future wikipedia administrators. In
CHI ?08 extended abstracts on Human factors in
computing systems, pages 3441?3446.
Andrew Sangpil Byon. 2006. The role of linguistic in-
directness and honorifics in achieving linguistic po-
liteness in Korean requests. Journal of Politeness
Research, 2(2):247?276.
Paul Chilton. 1990. Politeness, politics, and diplo-
macy. Discourse and Society, 1(2):201?224.
Herbert H. Clark and Dale H. Schunk. 1980. Polite
responses to polite requests. Cognition, 8(1):111?
143.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: Language effects and power differences in
social interaction. In Proceedings of WWW, pages
699?708.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454.
Christopher P. Diehl, Galileo Namata, and Lise Getoor.
2007. Relationship identification for social network
discovery. In Proceedings of the AAAI Workshop on
Enhanced Messaging, pages 546?552.
Kirk W Duthler. 2006. The Politeness of Requests
Made Via Email and Voicemail: Support for the Hy-
perpersonal Model. Journal of Computer-Mediated
Communication, 11(2):500?521.
Manaal Faruqui and Sebastian Pado. 2012. Towards a
model of formal and informal address in english. In
Proceedings of EACL, pages 623?633.
258
Elen P. Francik and Herbert H. Clark. 1985. How to
make requests that overcome obstacles to compli-
ance. Journal of Memory and Language, 24:560?
568.
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of CSCW, pages 1037?1046.
H. Paul Grice. 1975. Logic and conversation. In Pe-
ter Cole and Jerry Morgan, editors, Syntax and Se-
mantics, volume 3: Speech Acts, pages 43?58. Aca-
demic Press, New York.
S Gupta, M Walker, and D Romano. 2007. How rude
are you?: Evaluating politeness and affect in inter-
action. Affective Computing and Intelligent Interac-
tion, pages 203?217.
Samuel Gyasi Obeng. 1997. Language and politics:
Indirectness in political discourse. Discourse and
Society, 8(1):49?83.
S. I. Harada. 1976. Honorifics. In Masayoshi
Shibatani, editor, Syntax and Semantics, volume
5: Japanese Generative Grammar, pages 499?561.
Academic Press, New York.
Susan Herring. 1994. Politeness in computer cul-
ture: Why women thank and men flame. In Cul-
tural performances: Proceedings of the third Berke-
ley women and language conference, volume 278,
page 94.
Janet Holmes and Maria Stubbe. 2005. Power and Po-
liteness in the Workplace: A Sociolinguistic Analysis
of Talk at Work. Longman, London.
Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum, London and New York.
Sachiko Ide. 1989. Formal forms and discernment:
Two neglected aspects of universals of linguistic po-
liteness. Multilingua, 8(2?3):223?248.
David Kaplan. 1999. What is meaning? Explorations
in the theory of Meaning as Use. Brief version ?
draft 1. Ms., UCLA.
Robin Lakoff. 1973. The logic of politeness; or, mid-
ing your P?s and Q?s. In Proceedings of the 9th
Meeting of the Chicago Linguistic Society, pages
292?305.
Robin Lakoff. 1977. What you can do with words:
Politeness, pragmatics and performatives. In Pro-
ceedings of the Texas Conference on Performatives,
Presuppositions and Implicatures, pages 79?106.
Geoffrey N. Leech. 1983. Principles of Pragmatics.
Longman, London and New York.
Jure Leskovec, Daniel Huttenlocher, and Jon Klein-
berg. 2010. Governance in Social Media: A case
study of the Wikipedia promotion process. In Pro-
ceedings of ICWSM, pages 98?105.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: analyzing and comparing opin-
ions on the Web. In Proceedings of WWW, pages
342?351.
Yoshiko Matsumoto. 1988. Reexamination of the uni-
versality of face: Politeness phenomena in Japanese.
Journal of Pragmatics, 12(4):403?426.
Andrew McCallum, Xuerui Wang, and Andr?es
Corrada-Emmanuel. 2007. Topic and role discovery
in social networks with experiments on Enron and
academic email. Journal of Artificial Intelligence
Research, 30(1):249?272.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: the new gen-
eration of linguistic data. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 122?130.
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study on
the enron corpus. In Proceedings of the ACL Work-
shop on Language in Social Media, pages 86?95.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Proceedings of NAACL-HLT,
pages 518?522.
Priscilla S. Rogers and Song Mei Lee-Wong. 2003.
Reconceptualizing politeness to accommodate dy-
namic tensions in subordinate-to-superior reporting.
Journal of Business and Technical Communication,
17(4):379?412.
Andrew J. Scholand, Yla R. Tausczik, and James W.
Pennebaker. 2010. Social language network analy-
sis. In Proceedings of CSCW, pages 23?26.
Robert Van Rooy. 2003. Being polite is a handicap:
Towards a game theoretical analysis of polite lin-
guistic behavior. In Proceedings of TARK, pages
45?58.
Marilyn A Walker, Janet E Cahn, and Stephen J Whit-
taker. 1997. Improvising linguistic style: social and
affective bases for agent personality. In Proceedings
of AGENTS, pages 96?105.
William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan W. Black, and Justine Cassell. 2012.
?love ya, jerkface?: Using sparse log-linear mod-
els to build positive and impolite relationships with
teens. In Proceedings of SIGDIAL, pages 20?29.
Richard J. Watts. 2003. Politeness. Cambridge Uni-
versity Press, Cambridge.
Ian H Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
259
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 74?80,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs
Adam Vogel, Christopher Potts, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{acvogel,cgpotts,jurafsky}@stanford.edu
Abstract
Conversational implicatures involve rea-
soning about multiply nested belief struc-
tures. This complexity poses significant
challenges for computational models of
conversation and cognition. We show that
agents in the multi-agent Decentralized-
POMDP reach implicature-rich interpreta-
tions simply as a by-product of the way
they reason about each other to maxi-
mize joint utility. Our simulations involve
a reference game of the sort studied in
psychology and linguistics as well as a
dynamic, interactional scenario involving
implemented artificial agents.
1 Introduction
Gricean conversational implicatures (Grice, 1975)
are inferences that listeners make in order to
reconcile the speaker?s linguistic behavior with
the assumption that the speaker is cooperative.
As Grice conceived of them, implicatures cru-
cially involve reasoning about multiply-nested be-
lief structures: roughly, for p to count as an impli-
cature, the speaker must believe that the listener
will infer that the speaker believes p. This com-
plexity makes implicatures an important testing
ground for models of conversation and cognition.
Implicatures have received considerable atten-
tion in the context of simple reference games in
which the listener uses the speaker?s utterance
to try to identify the speaker?s intended referent
(Rosenberg and Cohen, 1964; Clark and Wilkes-
Gibbs, 1986; Dale and Reiter, 1995; DeVault and
Stone, 2007; Krahmer and van Deemter, 2012).
Many implicature patterns can be embedded in
these games using specific combinations of poten-
tial referents and message sets. The paradigm has
proven fruitful not only for evaluating computa-
tional models (Golland et al, 2010; Degen and
Franke, 2012; Frank and Goodman, 2012; Rohde
et al, 2012; Bergen et al, 2012) but also for study-
ing children?s pragmatic abilities without implic-
itly assuming they have mastered challenging lin-
guistic structures (Stiller et al, 2011).
In this paper, we extend these results beyond
simple reference games to full decision-problems
in which the agents reason about language and ac-
tion together over time. To do this, we use the De-
centralized Partially Observable Markov Decision
Process (Dec-POMDP) to implement agents that
are capable of manipulating the multiply-nested
belief structures required for implicature calcula-
tion. Optimal decision making in Dec-POMDPs
is NEXP complete, so we employ the single-agent
POMDP approximation of Vogel et al (2013).
We show that agents in the Dec-POMDP reach
implicature-rich interpretations simply as a by-
product of the way they reason about each other
to maximize joint utility. Our simulations involve
a reference game and a dynamic, interactional sce-
nario involving implemented artificial agents.
2 Decision-Theoretic Communication
The Decentralized Partially Observable Markov
Decision Process (Dec-POMDP) (Bernstein et
al., 2002) is a multi-agent generalization of the
POMDP, where agents act to maximize a shared
utility function. Formally, a Dec-POMDP con-
sists of a tuple (S,A,O,R, T,?, b0, ?). S is a
finite set of states, A is the set of actions, O is
the set of observations, and T (s?|a1, a2, s) is the
transition distribution which determines what ef-
fect the joint action (a1, a2) has on the state of the
world. The true state s ? S is not observable to
the agents, who must utilize observations o ? O,
which are emitted after each action according to
the observation distribution ?(o1, o2|s?, a). The
reward functionR(s, a1, a2) represents the goal of
the agents, who act to maximize expected reward.
Lastly, b0 ? ?(S) is the initial belief state and
74
? ? [0, 1) is the discount factor.
The true state of the world s ? S is not ob-
servable to either agent. In single-agent POMDPs,
agents maintain a belief state b(s) ? ?(S), which
is a distribution over states. Agents acting in Dec-
POMDPs must take into account not only their
beliefs about the state of the world, but also the
beliefs of their partners, leading to nested belief
states. In the model presented here, our agent
models the other agent?s beliefs about the state of
the world, and assumes that the other agent does
not take into account our own beliefs, a common
approach (Gmytrasiewicz and Doshi, 2005).
Agents make decisions according to
a policy pii : ?(S) ? A which max-
imizes the discounted expected reward??
t=0 ?tE[R(st, at1, at2)|b0, pi1, pi2]. Using
the assumption that the other agent tracks one less
level of belief, we can solve for the other agent?s
policy p?i, which allows us to estimate his actions
and beliefs over time. To construct policies,
we use Perseus (Spaan and Vlassis, 2005), a
point-based value iteration algorithm.
Even tracking just one level of nested beliefs
quickly leads to a combinatorial explosion in the
number of belief states the other agent might have.
This causes decision making in Dec-POMDPs to
be NEXP complete, limiting their application to
problems with only a handful of states (Bernstein
et al, 2002). To ameliorate this difficulty, we
use the method of Vogel et al (2013), which cre-
ates a single-agent approximation to the full Dec-
POMDP. To form this single-agent POMDP, we
augment the state space to be S ? S, where the
second set of state variables allows us to model
the other agent?s beliefs. We maintain a point
estimate b? of the other agent?s beliefs, which
is formed by summing out observations O that
the other player might have received. To ac-
complish this, we factor the transition distribu-
tion into two terms: T ((s?, s??)|a, p?i(s?), (s, s?)) =
T? (s??|s?, a, p?i(s?), (s, s?))T (s?|a, p?i(s?), (s, s?)). This
observation marginalization can be folded into the
transition distribution T? (s??|s?, a, p?i(s?), (s, s?)):
T? (s??| s?, a, p?i(s?), (s, s?)) = Pr(s??|s?, a, p?i(s?), (s, s?))
=
?
o??O
( ?(o?|s??, a, p?i(s?))T (s??|a, p?i(s?), s?)?
s??? ?(o?|s???, a, p?i(s?))T (s???|a, p?i(s?), s?)
? ?(o?|s?, a, p?i(s?))
)
(1)
Communication is treated as another type of ob-
servation, with messages coming from a finite set
M . Each message m ? M has the semantics
Pr(s|m), which represents the probability that the
world is in state s ? S given that m is true. Mes-
sages m received from a partner are combined
with perceptual observations o ? O, to form a
joint observation (m, o).
A literal listener, denoted L, interprets mes-
sages according to this semantics, without taking
into account the beliefs of the speaker. L assumes
that the perceptual observations and messages are
conditionally independent given the state of the
world. Using Bayes? rule, the literal listener?s joint
observation/message distribution is
Pr((o,m)|s, s?, a) = ?(o|s?, a) Pr(m|s)
= ?(o|s?, a) Pr(s|m) Pr(m)?
m??M Pr(s|m?) Pr(m?)
(2)
The Pr(m) prior over messages can be estimated
from corpus data, but we use a uniform prior for
simplicity.
A literal speaker, denoted S, produces mes-
sages according to the most descriptive term:
piS(s) = arg max
m?M
p(s|m). (3)
The literal speaker does not model the beliefs of
the listener.
To interpret implicatures, a level-one lis-
tener, denoted L(S), models the beliefs a literal
speaker must have had to produce an utterance:
Pr(m|s) = 1[p?iS(s) = m], where p?iS is the level-
one listener?s estimate of the speaker?s policy. In
this setting, we denote the level-one listener?s es-
timate of the speaker?s belief as s?, yielding the be-
lief update equation
Pr((o,m)|(s, s?), (s?, s??), a, p?iS(s?)) =
?(o|s?, a)1[p?iS(s?) = m] (4)
The literal semantics of messages is not explicitly
included in the level-one listener?s belief update.
Instead, when he solves for the literal speaker?s
policy p?iS , the meaning of a message is the set of
beliefs that would lead the literal speaker to pro-
duce the utterance.
A level-one speaker, S(L), produces utterances
to influence a literal listener, and a level-two lis-
tener, L(S(L)), uses two levels of belief nesting to
interpret utterances as the beliefs that a level-one
speaker might have to produce that utterance. At
each level of nesting, we apply the marginalized
75
r1 0 0 1
r2 0 1 1
r3 1 1 0
hat
glasses
mustache
r1 r2 r3
(a) Scenario.
Message r1 r2 r3
moustache 12 12 0
glasses 0 12 12
hat 0 0 1
(b) Literal interpretations.
Message r1 r2 r3
moustache 1 0 0
glasses 0 1 0
hat 0 0 1
(c) Implicature-rich interpretations.
Figure 1: A simple reference game. The matrices
give distributions Pr(t = ri|utterance)
belief-state approach of (Vogel et al, 2013), aug-
menting the state space with another copy of the
underlying world state space, where the new copy
represents the next level of belief. For instance, the
L(S(L)) agent will make decisions in the S?S?S
space. For an L(S(L)) state (s, s?, s?), s is the true
state of the world, s? is the speaker?s belief of the
state of the world, and s? is the speaker?s belief of
the listener?s beliefs. In the next two sections we
show how a level-one and level-two listener infer
implicatures.
3 Reference Game Implicatures
Fig. 1a is the scenario for a reference game of the
sort pioneered by Rosenberg and Cohen (1964)
and Dale and Reiter (1995). The potential refer-
ents are r1, r2, and r3. Speakers use a restricted
vocabulary consisting of three messages: ?mous-
tache?, ?glasses?, and ?hat?. The speaker is as-
signed a referent ri (hidden from the listener) and
produces a message on that basis. The speaker and
listener share the goal of having the listener iden-
tify the speaker?s intended referent ri.
Fig. 1b depicts the literal interpretations for
this game. It looks like the listener?s chances
of success are low. Only ?hat? refers unambigu-
ously. However, the language and scenario fa-
cilitate scalar implicature (Horn, 1972; Harnish,
1979; Gazdar, 1979). Briefly, the scalar implica-
ture pattern is that a speaker who is knowledgeable
about the relevant domain will choose a commu-
nicatively weak utterance U over a communica-
tively stronger utterance U ? iff U ? is false (assum-
ing U and U ? are relevant). The required sense of
communicative strength encompasses logical en-
tailments as well as more particularized pragmatic
partial orders (Hirschberg, 1985).
In our scenario, ?hat? is stronger than ?glasses?:
the referents wearing a hat are a proper subset
of those wearing glasses. Thus, given the play-
ers? goal, if the speaker says ?glasses?, the lis-
tener should draw the scalar implicature that ?hat?
is false. Thus, ?glasses? comes to unambiguously
refer to r2 (Fig. 1c, line 2). Similarly, though
?moustache? and ?glasses? do not literally stand in
the specific?general relationship needed for scalar
implicature, they do with ?glasses? pragmatically
associated with r2 (Fig. 1c, line 1).
Our implementation of these games as Dec-
POMDPs mirrors their intuitive description and
their treatment in iterated best response models
(Ja?ger, 2007; Ja?ger, 2012; Franke, 2009; Frank
and Goodman, 2012). The state space S encodes
the attributes of the referents (e.g., hat(r2) = T,
glasses(r1) = F) and includes a target variable t
identifying the speaker?s referent (hidden from the
listener). The speaker has three speech actions,
identified with the three messages. The listener
has four actions: ?listen? plus a ?choose? action ci
for each referent ri. The set of observations O is
just the set of messages (construed as utterances).
The agents receive a positive reward iff the listener
action ci corresponds to the speaker?s target t. Be-
cause this is a one-step reference game, the transi-
tion distribution T is the identity distribution.
The literal listener L interprets utterances as
a truth-conditional speaker would produce them
(Fig. 1b). The level-one speaker S(L) augments
the state space with a variable ?listener target? and
models L?s beliefs b? using the approximate meth-
ods of Sec. 2. Crucially, the optimal speaker pol-
icy piS(L) is such that piS(L)(t=r3) = ?hat? and
piS(L)(t=r1) = ?moustache?. The level-two lis-
tener L(S(L)) models S(L) via an estimate of the
?listener target? variable. For each speech action
m, L(S(L)) considers all values of t and the likeli-
76
hood that S(L) would have produced m:
Pr(t=ri|m) ? 1[p?iS(L)(t=ri) = m]
Since S(L) uses ?hat? to describe r3 and
?moustache? to describe r1, L(S(L)) correctly in-
fers that ?glasses? refers to r2, completing Fig. 1c?s
full implicature-rich pattern of mutual exclusivity
(Clark, 1987; Frank et al, 2009).
This basic pattern is robustly attested empiri-
cally in human data. The experimental data are,
of course, invariably less crisp than our idealized
model predicts, but many important sources of
variation could be brought into our model, with
the addition of strong salience priors (Frank and
Goodman, 2012; Stiller et al, 2011), assumptions
about bounded rationality (Camerer et al, 2004;
Franke, 2009), and a ?soft-max? view of the lis-
tener (Frank et al, 2009).
4 Cards World Implicatures
The Cards corpus1 contains 1266 metadata-rich
transcripts from a two-player chat-based game.
The world is a simple maze in which a deck of
cards has been distributed. The players? goal is to
find specific subsets of the cards, subject to a vari-
ety of constraints on what they can see and do. The
Dec-POMDP-based agents of Vogel et al (2013)
play a simplified version in which the goal is to be
co-located with a single card. Vogel et al show
that their agents? linguistic behavior is broadly
Gricean. However, their agents? language is too
simple to reveal implicatures. The present section
remedies this shortcoming. Implicature-rich inter-
pretations are an immediate consequence.
We implement the simplified Cards tasks as fol-
lows. The state space S is composed of the loca-
tion of each player and the location of the card.
The transition distribution T (s?|s, a1, a2) encodes
the outcome of movement actions. Agents receive
one of two sensor observations, indicating whether
the card is at their current location. The players are
rewarded when they are both located on the card.
Each player begins knowing his own location, but
not the location of the other player nor of the card.
The players have four movement actions (?up?,
?down?, ?left?, ?right?) and nine speech actions in-
terpreted as identifying card locations. Fig. 2 de-
picts these utterances as a partial order determined
by entailment. These general-to-specific relation-
1http://cardscorpus.christopherpotts.net
top right top left bottom right bottom left
top right left bottom middle
,,
,,
\\
\\
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 122?130,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Crowdsourcing and language studies: the new generation of linguistic data
Robert Munroa Steven Bethardb Victor Kupermana Vicky Tzuyin Laic
Robin Melnicka Christopher Pottsa Tyler Schnoebelena Harry Tilya
aDepartment of Linguistics, Stanford University
bDepartment of Computer Science, Stanford University
cDepartment of Linguistics, University of Colorado
{rmunro,bethard,vickup,rmelnick,cgpotts,tylers,hjt}
@stanford.edu
vicky.lai@colorado.edu
Abstract
We present a compendium of recent and cur-
rent projects that utilize crowdsourcing tech-
nologies for language studies, finding that the
quality is comparable to controlled labora-
tory experiments, and in some cases superior.
While crowdsourcing has primarily been used
for annotation in recent language studies, the
results here demonstrate that far richer data
may be generated in a range of linguistic dis-
ciplines from semantics to psycholinguistics.
For these, we report a number of successful
methods for evaluating data quality in the ab-
sence of a ?correct? response for any given
data point.
1 Introduction
Crowdsourcing?s greatest contribution to language
studies might be the ability to generate new kinds
of data, especially within experimental paradigms.
The speed and cost benefits for annotation are cer-
tainly impressive (Snow et al, 2008; Callison-
Burch, 2009; Hsueh et al, 2009) but we hope to
show that some of the greatest gains are in the very
nature of the phenomena that we can now study.
For psycholinguistic experiments in particular, we
are not so much utilizing ?artificial artificial? intelli-
gence as the plain intelligence and linguistic intu-
itions of each crowdsourced worker ? the ?voices
in the crowd?, so to speak. In many experiments
we are studying gradient phenomena where there
are no right answers. Even when there is binary
response we are often interested in the distribution
of responses over many speakers rather than spe-
cific data points. This differentiates experimentation
from more common means of determining the qual-
ity of crowdsourced results as there is no gold stan-
dard against which to evaluate the quality or ?cor-
rectness? of each individual response.
The purpose of this paper is therefore two-fold.
We summarize seven current projects that are utiliz-
ing crowdsourcing technologies, all of them some-
what novel to the NLP community but with potential
for future research in computational linguistics. For
each, we also discuss methods for evaluating quality,
finding the crowdsourced results to often be indistin-
guishable from controlled laboratory experiments.
In Section 2 we present the results from seman-
tic transparency experiments showing near-perfect
interworker reliability and a strong correlation be-
tween crowdsourced data and lab results. Ex-
tending to audio data, we show in Section 3
that crowdsourced subjects were statistically in-
distinguishable from a lab control group in seg-
mentation tasks. Section 4 shows that labora-
tory results from simple Cloze tasks can be repro-
duced with crowdsourcing. In Section 5 we offer
strong evidence that crowdsourcing can also repli-
cate limited-population, controlled-condition lab re-
sults for grammaticality judgments. In Section 6 we
use crowdsourcing to support corpus studies with a
precision not possible with even very large corpora.
Moving to the brain itself, Section 7 demonstrates
that ERP brainwave analysis can be enhanced by
crowdsourced analysis of experimental stimuli. Fi-
nally, in Section 8 we outline simple heuristics for
ensuring that microtasking workers are applying the
linguistic attentiveness required to undertake more
complex tasks.
122
2 Transparency of phrasal verbs
Phrasal verbs are those verbs that spread their mean-
ing out across both a verb and a particle, as in ?lift
up?. Semantic transparency is a measure of how
strongly the phrasal verb entails the component verb.
For example, to what extent does ?lifting up? entail
?lifting?? We can see the variation between phrasal
verbs when we compare the transparency of ?lift up?
to the opacity of ?give up?.
We conducted five experiments around seman-
tic transparency, with results showing that crowd-
sourced results correlate well with each other and
against lab data (? up to 0.9). Interrater reliability is
also very high: ? = 0.823, which Landis and Koch
(1977) would call ?almost perfect agreement.?
The crowdsourced results reported here represent
judgments by 215 people. Two experiments were
performed using Stanford University undergradu-
ates. The first involved a questionnaire asking par-
ticipants to rate the semantic transparency of 96
phrasal verbs. The second experiment consisted of
a paper questionnaire with the phrasal verbs in con-
text. That is, the first group of ?StudentLong? par-
ticipants rated the similarity of ?cool? to ?cool down?
on a scale 1-7:
cool cool down
The ?StudentContext? participants performed the
same basic task but saw each verb/phrasal verb pair
with an example of the phrasal verb in context.
With Mechanical Turk, we had three conditions:
TurkLong: A replication of the first questionnaire
and its 96 questions.
TurkShort: The 96-questions were randomized into
batches of 6. Thus, some participants ended up giv-
ing responses to all phrasal verbs, while others only
gave 6, 12, 18, etc responses.
TurkContext: A variation of the ?StudentContext?
task ? participants were given examples of the
phrasal verbs, though as with ?TurkShort?, they were
only asked to rate 6 phrasal verbs at a time.
What we find is a split into relatively high and low
correlations, as Figure 1 shows. All Mechanical
Turk tests correlate very well with one another (all
? > 0.7), although the tasks and raters are differ-
ent. The correlation between the student participants
who were given sentence contexts and the workers
TurkLong
2 3 4 5 6 2 3 4 5 6
2.0
3.5
5.0
2
3
4
5
6
r = 0.92
p = 0
rs = 0.92
p = 0
TurkShort
r = 0.74
p = 0
rs = 0.73
p = 0
r = 0.77
p = 0
rs = 0.75
p = 0
TurkContext
3
4
5
6
2
3
4
5
6 r = 0.68
p = 0
rs = 0.67
p = 0
r = 0.7
p = 0
rs = 0.67
p = 0
r = 0.9
p = 0
rs = 0.9
p = 0
StudentContext
2.0 3.5 5.0
r = 0.46
p = 0
rs = 0.46
p = 0
r = 0.48
p = 0
rs = 0.48
p = 0
3 4 5 6
r = 0.46
p = 0
rs = 0.45
p = 0
r = 0.41
p = 0
rs = 0.44
p = 0
2.5 3.5 4.5 5.5
2.5
3.5
4.5
5.5StudentLong
Figure 1: Panels at the diagonal report histograms of dis-
tributions of ratings across populations of participants;
panels above the diagonal plot the locally weighted scat-
terplot smoothing Lowess functions for a pair of corre-
lated variables; panels below the diagonal report correla-
tion coefficients (the r value is Pearson?s r, the rs value
is Spearman?s ?) and respective ? values.
who saw context is especially high (0.9). All corre-
lations with StudentLong are relatively low, but this
is actually true for StudentLong vs. StudentContext,
too (? = 0.44), even though both groups are Stan-
ford undergraduates.
Intra-class correlation coefficients (ICC) measure
the agreement among participants, and these are
high for all groups except StudentLong. Just among
StudentLong participants, the ICC consistency is
only 0.0934 and their ICC agreement is 0.0854.
Once we drop StudentLong, we see that all of the
remaining tests have high consistency (average of
0.78 for ICC consistency, 0.74 for ICC agreement).
For example, if we combine TurkContext and Stu-
dentContext, ICC consistency is 0.899 and ICC
agreement of 0.900. Cohen?s kappa measurement
also measures how well raters agree, weeding out
chance agreements. Again, StudentLong is an out-
lier. Together, TurkContext / StudentContext gets a
weighted kappa score of 0.823 ? the overall average
(excepting StudentLong) is ? = 0.700.
More details about the results in this section can
be found in Schnoebelen and Kuperman (submit-
ted).
123
3 Segmentation of an audio speech stream
The ability of browsers to present multimedia re-
sources makes it feasible to use crowdsourcing tech-
niques to generate data using spoken as well as writ-
ten stimuli. In this section we report an MTurk repli-
cation of a classic psycholinguistic result that relies
on audio presentation of speech. We developed a
web-based interface that allows us to collect data
in a statistical word segmentation paradigm. The
core is a Flash applet developed using Adobe Flex
which presents audio stimuli and collects participant
responses (Frank et al, submitted).
Human children possess a remarkable ability to
learn the words and structures of languages they are
exposed to without explicit instruction. One partic-
ularly remarkable aspect is that unlike many written
languages, spoken language lacks spaces between
words: from spoken input, children learn not only
the mapping between meanings and words but also
what the words themselves are, with no direct infor-
mation about where one ends and the next begins.
Research in statistical word segmentation has shown
that both infants and adults use statistical properties
of speech in an unknown language to infer a proba-
ble vocabulary. In one classic study, Saffran, New-
port & Aslin (1996) showed that after a few minutes
of exposure to a language made by randomly con-
catenating copies of invented words, adult partici-
pants could discriminate those words from syllable
sequences that also occurred in the input but crossed
a word boundary. We replicated this study showing
that cheap and readily accessible data from crowd-
sourced workers compares well to data from partic-
ipants recorded in person in the lab.
Participants heard 75 sentences from one of 16 ar-
tificially constructed languages. Each language con-
tained 2 two-syllable, 2 three-syllable, and 2 four
syllable words, with syllables drawn from a possi-
ble set of 18. Each sentence consisted of four words
sampled without replacement from this set and con-
catenated. Sentences were rendered as audio by
the MBROLA synthesizer (Dutoit et al, 1996) at a
constant pitch of 100Hz with 25ms consonants and
225ms vowels. Between each sentence, participants
were required to click a ?next? button to continue,
preventing workers from leaving their computer dur-
ing this training phase. To ensure workers could ac-
Figure 2: Per-subject correct responses for lab and MTurk
participants. Bars show group means, and the dashed line
indicates the chance baseline.
tually hear the stimuli, they were first asked to enter
an English word presented auditorily.
Workers then completed ten test trials in which
they heard one word from the language and one non-
word made by concatenating all but the first syllable
of one word with the first syllable of another. If the
words ?bapu? and ?gudi? had been presented adja-
cently, the string ?pugu? would have been heard, de-
spite not being a word of the language. Both were
also displayed orthographically, and the worker was
instructed to click on the one which had appeared in
the previously heard language.
The language materials described above were
taken from a Saffran et al (1996) replication re-
ported as Experiment 2 in Frank, Goldwater, Grif-
fiths & Tenenbaum (under review). We compared
the results from lab participants reported in that ar-
ticle to data from MTurk workers using the applet
described above. Each response was marked ?cor-
rect? if the participant chose the word rather than the
nonword. 12 lab subjects achieved 71% correct re-
sponses, while 24 MTurk workers were only slightly
lower at 66%. The MTurk results proved signif-
icantly different from a ?random clicking? base-
line of 50% (t(23) = 5.92, p = 4.95 ? 10?06)
but not significantly different from the lab subjects
(Welch two-sample t-test for unequal sample sizes,
t(21.21) = ?.92, p = .37). Per-subject means for
the lab and MTurk data are plotted in Figure 2.
124
4 Contextual predictability
As psycholinguists build models of sentence pro-
cessing (e.g., from eye tracking studies), they need
to understand the effect of the available sentence
context. One way to gauge this is the Cloze task pro-
posed in Taylor (1953): participants are presented
with a sentence fragment and asked to provide the
upcoming word. Researchers do this for every word
in every stimulus and use the percentage of ?correct?
guesses as input into their statistical and computa-
tional models.
Rather than running such norming studies on un-
dergraduates in lab settings (as is typical), our results
suggest that psycholinguists will be able to crowd-
source these tasks, saving time and money without
sacrificing reliability (Schnoebelen and Kuperman,
submitted).
Our results are taken from 488 Americans, rang-
ing from age 16-80 (mean: 34.49, median: 32,
mode: 27) with about 25% each from the East and
Midwest, 31% from the South, the rest from the
West and Alaska. They represent a range of educa-
tion levels, though the majority had been to college:
about 33.8% had bachelor?s degrees, another 28.1%
had some college but without a degree.
By contrast, the lab data was gathered from 20
participants, all undergraduates at the University of
Massachusetts at Amherst in the mid-1990?s (Re-
ichle et al, 1998). Both populations provided judg-
ments on 488 words in 48 sentences. In general,
crowdsourcing gave more diverse responses, as we
would expect from a more diverse population.
The correlation between lab and crowdsourced
data by Spearman?s rank correlation is 0.823 (? <
0.0001), but we can be even more conservative by
eliminating the 124 words that had predictability
scores of 0 across both groups. By and large, the
lab participants and the workers are consistent in
which words they fail to predict. Even when we
eliminate these shared zeros, the correlation is still
high between the two data sets: weighted ? = 0.759
(? < 0.0001).
5 Judgment studies of fine-grained
probabilistic grammatical knowledge
Moving to syntax, we demonstrate here that gram-
maticality judgments from lab studies can also be
Figure 3: Mean ?that?-inclusion ratings plotted against
corresponding corpus-model predictions. The solid line
would represent perfect alignment between judgments
and corpus model. Non-parametric Lowess smoothers il-
lustrate the significant correlation between lab and crowd
population results.
reproduced through crowdsourcing.
Corpus studies of spontaneous speech suggest
that grammaticality is gradient (Wasow, 2008), and
models of English complement clause (CC) and rel-
ative clause (RC) ?that?-optionality have as their
most significant factor the predictability of embed-
ding, given verb (CC) and head noun (RC) lemma
(Jaeger, 2006; Jaeger, in press). Establishing that
these highly gradient factors are similarly involved
in judgments could provide evidence that such fine-
grained probabilistic knowledge is part of linguistic
competence.
We undertook six such judgment experiments:
two baseline studies with lab populations then four
additional crowdsourced trials via MTurk.
Experiment 1, a lab trial (26 participants, 30
items), began with the models of RC-reduction de-
veloped in Jaeger (2006). Corpus tokens were
binned by relative model-predicted probability of
?that?-omission. Six tokens were extracted at ran-
dom from each of five bins (0??<20% likelihood of
?that?-inclusion; 20??<40%; and so on). In a gra-
dient scoring paradigm with 100 points distributed
between available options (Bresnan, 2007) partici-
125
pants rated how likely each choice ? with or without
?that? ? was as the continuation of a segment of dis-
course. As hypothesized, mean participant ratings
significantly correlate with corpus model predictions
(r = 0.614, ? = 0.0003).
Experiment 2 (29 participants) replicated Exper-
iment 1 to address concerns that subjects might be
?over-thinking? the process. We used a timed forced-
choice paradigm where participants had from 5 to 24
seconds (varied as a linear function of token length)
to choose between the reduced/unreduced RC stim-
uli. These results correlate even more closely with
predictions (r = 0.838, ? < 0.0001).
Experiments 3 and 4 replicated 1 and 2 on MTurk
(1200 tasks each). Results were filtered by volun-
teered demographics to select the same subject pro-
file as the lab experiments. Response-time outliers
were also excluded to avoid fast-click-through and
distracted-worker data. Combined, these steps elim-
inated 384 (32.0%) and 378 (31.5%) tasks, respec-
tively, with 89 and 66 unique participants remaining.
While crowdsourced measures might be expected to
yield lower correlations due to such unbalanced data
sets, the results remain significant in both trials (r =
0.562, ? = 0.0009; r = 0.364, ? = 0.0285), offer-
ing strong evidence that crowdsourcing can replicate
limited-population, controlled-condition lab results,
and of the robustness of the alignment between pro-
duction and judgment models. Figure 3 compares
lab and crowd population results in the 100-point
task (Experiments 1 and 3).
Experiments 5 and 6 (1600 hits each) employed
the same paradigms via MTurk to investigate ?that?-
mentioning in CCs, where predictability of embed-
ding is an even stronger factor in the corpus model.
Filtering reduced the data by 590 (36.9%) and 863
(53.9%) hits. As with the first four experiments,
each of these trials produced significant correlations
(r = 0.433, ? = 0.0107; r = 0.500, ? = 0.0034; re-
spectively). Finally, mixed-effect binary logistic re-
gression models ? with verb lemma and test subject
ID as random effects ? were fitted to these judgment
data. As in the corpus-derived models, predictability
of embedding remains the most significant factor in
all experimental models.
The results across both lab and crowdsourced
studies suggest that speakers consider the same fac-
tors in judgment as in production, offering evidence
Figure 4: Odds ratio of a Nominal Agent being embed-
ded within a Sentential Agent or non-Agent, relative to
random chance. (? < 0.001 for all)
that competence grammar includes access to prob-
ability distributions. Meanwhile, the strong cor-
relations across populations offer encouraging evi-
dence in support of using the latter in psycholinguis-
tic judgment research.
6 Confirming corpus trends
Crowdsourcing can also be used to establish the va-
lidity of corpus trends found in otherwise skewed
data. The experiments in this section were mo-
tivated by the NomBank corpus of nominal pred-
icate/arguments (Meyers et al, 2004) where we
found that an Agent semantic role was much more
likely to be embedded within a sentential Agent. For
example, (1) is more likely than (2) to receive the
Agent interpretation for the ?the police?, but both
have same potential range of meanings:
(1) ?The investigation of the police took 3 weeks to
complete?
(2) ?It took 3 weeks to complete the investigation of
the police?
While the trend is significant (? < 0.001), the
corpus is not representative speech.
First, there are no minimal pairs of sentences in
NomBank like (1) and (2) that have the same poten-
tial range of meanings. Second, the s-genitive (?the
police?s investigation?) is inherently more Agen-
tive than the of-genitive (?the investigation of the
police?) and it is also more compact. Sentential
subjects tend to be lighter than objects, and more
likely to realize Agents, so the resulting correlation
could be indirect. Finally, if we sampled only the
predicates/arguments in NomBank that are frequent
in different sentential positions, we are limited to:
126
?earning, product, profit, trading, loss, share, rate,
sale, price?. This purely financial terminology is not
representative of a typical acquisition environment ?
no child should be exposed to only such language ?
so it is difficult to draw broad conclusions about the
cognitive viability of this correlation, even within
English. It is because of factors like these that cor-
pus linguistics has been somewhat of a ?poor cousin?
to theoretical linguistics.
Therefore, two sets of experiments were under-
taken to confirm that the trend is not epiphenomenal,
one testing comprehension and one testing produc-
tion.
The first tested thousands of workers? interpre-
tations of sentences like those in (1) and (2), over
a number of predicate/argument pairs (?shooting of
the hunters?, ?destruction of the army? etc). Work-
ers were asked their interpretation of the most likely
meaning. For example, does (1) mean: ?a: the po-
lice were doing the investigation? or ?b: the po-
lice are being investigated?. To control for errors
or click-throughs, two plainly incorrect options were
included. We estimate the erroneous response rate at
about 0.4% ? less than many lab studies.
For the second set of experiments, workers were
asked to reword an unambiguous sentence using a
given phrase. For example, rewording the following
using ?the investigation of the police?:
(3) ?Following the shooting of a commuter in Oak-
land last week, a reporter has uncovered new evi-
dence while investigating the police involved.?
We then (manually) recorded whether the required
phrase was in a sentential Agent or non-Agent posi-
tion.
Figure 4 gives the results from the corpus analy-
sis and both experiments. The results clearly show
a significant trend for all, and that the NomBank
trend falls between the comprehension and produc-
tion tasks, which would be expected for this highly
edited register. It therefore supports the validity of
the corpus results.
The phenomena likely exists to aid comprehen-
sion, as the cognitive realization of just one role
needs to be activated at a given moment. Despite
the near-ubiquity of ?Agent? in studies of semantic
roles, we do not yet have a clear theory of this lin-
guistic entity, or even firm evidence of its existence
Figure 5: Distribution of metaphorical frequencies.
(Parikh, 2010). This study therefore goes some way
towards illuminating this. More broadly, the experi-
ments in this section support the wider use of crowd-
sourcing as a tool for language cognition research in
conjunction with more traditional corpus studies.
7 Post-hoc metaphorical frequency
analysis of electrophysiological responses
Beyond reproducing laboratory and corpus studies,
crowdsourcing also offers the opportunity to newly
analyze data drawn from many other experimental
stimuli. In this section, we demonstrate that crowd-
sourced workers can help us better understand ERP
brainwave data by looking at how frequently words
are used metaphorically.
Recent work in event related potentials (ERP) has
suggested that even conventional metaphors, such as
?All my ideas were attacked? require additional pro-
cessing effort in the brain as compared to literal sen-
tences like ?All the soldiers were attacked? (Lai et
al., 2009). This study in particular observed an N400
effect where negative waves 400 milliseconds after
the presentation of the target words (e.g. attacked)
were larger when the word was used metaphorically
than when used literally.
The proposed explanation for this effect is that
metaphors really do demand more from the brain
than literal sentences. However, N400 effects are
also observed when subjects encounter something
that is semantically inappropriate or unexpected.
While the Lai experiment controlled for overall
word frequency, it might be possible to explain away
these N400 effects if it turned out that in the real
127
world the target words were almost always used
literally, so that seeing them used metaphorically
would be semantically incongruous.
To test this alternative hypothesis, we gathered
sense frequency distributions for each of the target
words ? the hypothesis predicts that these should
be skewed towards literal senses. For each of the
104 target words, we selected 50 random sentences
from the American National Corpus (ANC), fill-
ing in with British National Corpus sentences when
there were too few in the ANC. We gave the sen-
tences to crowdsourced workers and asked them to
label each target word as being used literally or
metaphorically. Each task contained one sentence
for each of the 104 target words, with the order of
words and the literal/metaphorical buttons random-
ized. Each sentence was annotated 5 times.
To encourage native speakers of English, we had
the MTurk service require that our workers be within
the United States, and posted the text ?Please ac-
cept this HIT only if you are a native speaker of En-
glish? in bold at the top of each HIT. We also used
Javascript to force workers to spend at least 2 sec-
onds on each sentence and we rejected results from
workers that had chance level (50%) agreement with
the other workers.
Though our tasks produced words annotated with
literal and metaphorical tags, we were less inter-
ested in the individual annotations (though agree-
ment was decent at 73%) and more interested in the
overall pattern for each target word. Some words,
like fruit, were almost always used literally (92%),
while other words, like hurdle were almost always
used metaphorically (91%) .
Overall, the target words had a mean metaphor-
ical frequency of 53%, indicating that their literal
and metaphorical senses were used in nearly equal
proportions. Figure 5 shows that the metaphorical
frequencies follow roughly a bell-curved distribu-
tion1, which is especially interesting given that the
target words were hand-selected for the Lai experi-
ment and not drawn randomly from a corpus. We did
not observe any skew towards literal senses as the
alternative hypothesis would have predicted. This
suggests that the findings of Lai, Curran, and Menn
1A Shapiro-Wilk test fails to reject the null hypothesis of a
normal distribution (p=0.09).
Item type correct incorrect
?easy? 60 2
?promise? 59 3
stacked genitive 55 7
Table 1: Response data for three control items, with the
goal of identifying workers who lack the requisite atten-
tiveness. All show high attentiveness. The difference be-
tween the ?easy? and ?stacked genitive? is trending but not
significant (? = 0.0835), indicating that any of these may
be used.
(2009) cannot be dismissed based on a sense fre-
quency argument.
We also took advantage of the collected sense fre-
quency distributions to re-analyze data from the Lai
experiment. We split the target words into a high bin
(average 72% metaphorical) and a low bin (average
33% metaphorical), matching the number of items
and average word log-frequency per bin. Looking at
the average ERPs (brain waves) over time for each
bin revealed that when subjects were reading novel
metaphors, there was a significant difference (p =
.01) at about 200ms (P200) between the ERPs for
the highly literal words and the ERPs for the highly
metaphorical words. Thus, not only does metaphori-
cal frequency influence figurative language process-
ing, but it does so much earlier than semantic effects
are usually observed (e.g. N400 effects at 400ms)2.
8 Screening for linguistic attentiveness
For annotation tasks, crowdsourcing is most suc-
cessful when the tasks are designed to be as simple
as possible, but in experimental work we don?t al-
ways want to target the shallowest knowledge of the
workers, so here we seek to discover just how atten-
tive the workers really are.
When running psycholinguistics experiments in
the lab, the experimenters generally have the chance
to interact with participants. It is not uncommon
for prospective subjects to be visibly exhausted, dis-
tracted, or inebriated, or not fluent in the given lan-
guage to a requisite level of competence. When
these participants turn up as outliers in the experi-
mental data, it is easy enough to see why ? they
fell asleep, couldn?t understand the instructions, etc.
2These results are consistent with recent findings that irony
frequency may also produce P200 effects (Regel et al, 2010).
128
With crowdsourcing we lose the chance to have
these brief but valuable encounters, and so anoma-
lous response data are harder to interpret.
We present two simple experiments for measuring
linguistic attentiveness, which can be used as one
component of a language study or to broadly evalu-
ate the linguistic competency of the workers. Taking
well-known constructions from the literature, we se-
lected constructions that: (a) exist in most (perhaps
all) dialects of English; (b) involve high frequency
lexical items; and (c) tend to be acquired relatively
late by first-language learners.
We have found two constructions from Carol
Chomsky?s (1969) work on first-language acquisi-
tion to be particularly useful:
(4) John is easy to see.
(5) John is eager to see.
Example (4) is accurately paraphrased as ?It is easy
to see John?, where John is the object of ?see?,
whereas (5) is accurately paraphrased as ?John is ea-
ger for John to see?, where John is the subject of
?see?. A similar shift happens with ?promise?:
(6) Bozo told Donald to sing.
(7) Bozo promised Donald to sing.
We presented workers with a multiple-choice ques-
tion that contained both subject and object para-
phrases as options.
In similar experiments, we adapted examples
from Roeper (2007), who looked at stacked prenom-
inal possessive constructions:
(8) John?s sister?s friend?s car.
These are cross-linguistically rare and challenging
even for native speakers. As above, the workers
were asked to choose between paraphrases.
Workers who provide accurate judgments are
likely to have a level of English competence and de-
votion to the task that suffices for many language
experiments. The results from one short audio study
are given in Table 1. They indicate a high degree of
attentiveness; as a group, our subjects performed at
the near-perfect levels we expect for fluent adults.
We predict that adding tasks like these to experi-
ments will not only screen for attentiveness, but also
prompt for greater attention from an otherwise dis-
tracted worker, improving results at both ends.
9 Conclusions
While crowdsourcing was first used by linguists for
annotation, we hope that the results here demon-
strate the potential for far richer studies. In a
range of linguistic disciplines from semantics to
psycholinguistics it enables systematic, large-scale
judgment studies that are more affordable and con-
venient than expensive, time-consuming lab-based
studies. With crowdsourcing technologies, linguists
have a reliable new tool for experimentally investi-
gating language processing and linguistic theory.
Here, we have reproduced many ?classic? large-
scale lab studies with a relative ease. We can en-
vision many more ways that crowdsourcing might
come to shape new methodologies for language
studies. The affordability and agility brings experi-
mental linguistics closer to corpus linguistics, allow-
ing the quick generation of targeted corpora. Multi-
ple iterations that were previously possible only over
many years and several grants (and therefore never
attempted) are now possible in a matter of days. This
could launch whole new multi-tiered experimental
designs, or at the very least allow ?rapid prototyp-
ing? of experiments for later lab-based verification.
Crowdsourcing also brings psycholinguistics
much closer to computational linguistics. The
two fields have always shared empirical data-driven
methodologies and computer-aided methods. We
now share a work-space too. Historically, NLP has
necessarily drawn corpora from the parts of linguis-
tic theory that have stayed still long enough to sup-
port time-consuming annotation projects. The re-
sults here have implications for such tasks, includ-
ing parsing, word-sense disambiguation and seman-
tic role labeling, but the most static parts of a field
are rarely the most exciting. We therefore predict
that crowdsourcing will also lead to an expanded,
more dynamic NLP repertoire.
Finally, for the past half-century theoretical lin-
guistics has relied heavily on ?introspective? corpus
generation, as the rare edge cases often tell us the
most about the boundaries of a given language. Now
that we can quickly and confidently generate empir-
ical results to evaluate hypotheses drawn from intu-
itions about the most infrequent linguistic phenom-
ena, the need for this particular fallback has dimin-
ished ? the stimuli are abundant.
129
Acknowledgements
We owe thanks to many people, especially within
the Department of Linguistics at Stanford, which has
quickly become a hive of activitiy for crowdsourced
linguistic research. In particular, we thank Tom Wa-
sow for his guidance in Section 5, Chris Manning for
his guidance in Section 6, and Florian T. Jaeger for
providing the corpus-derived base models in Section
5 (Jaeger, 2006). We also thank Michael C. Frank
for providing the design, materials, and lab data used
to evaluate the methods in Section 3. Several of the
projects reported here were supported by Stanford
Graduate Fellowships.
References
Joan Bresnan. 2007. Is syntactic knowledge probabilis-
tic? Experiments with the English dative alternation.
In Sam Featherston and Wolfgang Sternefeld, editors,
Roots: Linguistics in search of its evidential base,
pages 75?96. Mouton de Gruyter, Berlin.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 286?295.
Carol Chomsky. 1969. The Acquisition of Syntax in Chil-
dren from 5 to 10. MIT Press, Cambridge, MA.
Thierry Dutoit, Vincent Pagel, Nicolas Pierret, Franois
Bataille, and Olivier van der Vrecken. 1996. The
MBROLA project: Towards a set of high quality
speech synthesizers free of use for non commercial
purposes. In Fourth International Conference on Spo-
ken Language Processing, pages 75?96.
Michael Frank, Harry Tily, Inbal Arnon, and Sharon
Goldwater. submitted. Beyond transitional probabili-
ties: Human learners impose a parsimony bias in sta-
tistical word segmentation.
Michael Frank, Sharon Goldwater, Thomas Griffiths, and
Joshua Tenenbaum. under review. Modeling human
performance in statistical word segmentation.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35.
Florian Jaeger. 2006. Redundancy and syntactic reduc-
tion in spontaneous speech. Ph.D. thesis, Stanford
University, Stanford, CA.
Florian Jaeger. in press. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology.
Vicky Tzuyin Lai, Tim Curran, and Lise Menn. 2009.
Comprehending conventional and novel metaphors:
An ERP study. Brain Research, 1284:145?155, Au-
gust.
Richard Landis and Gary Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1).
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, , and Ralph
Grishman. 2004. Annotating noun argument structure
for NomBank. In Proceedings of LREC-2004.
Prashant Parikh. 2010. Language and Equilibrium. MIT
Press, Cambridge, MA.
Stefanie Regel, Seana Coulson, and Thomas C. Gunter.
2010. The communicative style of a speaker can af-
fect language comprehension? ERP evidence from the
comprehension of irony. Brain Research, 1311:121?
135.
Erik D. Reichle, Alexander Pollatsek, Donald L. Fisher,
and Keith Rayner. 1998. Toward a model of eye
movement control in reading. Psychological Review,
105:125?157.
Tom Roeper. 2007. The Prism of Grammar: How Child
Language Illuminates Humanism. MIT Press, Cam-
bridge, MA.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-
port. 1996. Word segmentation: The role of distribu-
tional cues. Journal of memory and language, 35:606?
621.
Tyler Schnoebelen and Victor Kuperman. submitted. Us-
ing Amazon Mechanical Turk for linguistic research:
Fast, cheap, easy, and reliable.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew T. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263.
Wilson Taylor. 1953. Cloze procedure: A new tool for
measuring readability. Journalism Quarterly, 30:415?
433.
Tom Wasow. 2008. Gradient data and gradient gram-
mars. In Proceedings of the 43rd Annual Meeting of
the Chicago Linguistics Society, pages 255?271.
130
