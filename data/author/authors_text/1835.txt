Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21?29,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Semi-supervised Semantic Role Labeling
Using the Latent Words Language Model
Koen Deschacht
Department of computer science
K.U.Leuven, Belgium
koen.deshahts.kuleuven.be
Marie-Francine Moens
Department of computer science
K.U.Leuven, Belgium
sien.moenss.kuleuven.be
Abstract
Semantic Role Labeling (SRL) has proved
to be a valuable tool for performing auto-
matic analysis of natural language texts.
Currently however, most systems rely on
a large training set, which is manually an-
notated, an effort that needs to be repeated
whenever different languages or a differ-
ent set of semantic roles is used in a cer-
tain application. A possible solution for
this problem is semi-supervised learning,
where a small set of training examples
is automatically expanded using unlabeled
texts. We present the Latent Words Lan-
guage Model, which is a language model
that learns word similarities from unla-
beled texts. We use these similarities for
different semi-supervised SRL methods as
additional features or to automatically ex-
pand a small training set. We evaluate the
methods on the PropBank dataset and find
that for small training sizes our best per-
forming system achieves an error reduc-
tion of 33.27% F1-measure compared to
a state-of-the-art supervised baseline.
1 Introduction
Automatic analysis of natural language is still a
very hard task to perform for a computer. Al-
though some successful applications have been de-
veloped (see for instance (Chinchor, 1998)), im-
plementing an automatic text analysis system is
still a labour and time intensive task. Many ap-
plications would benefit from an intermediate rep-
resentation of texts, where an automatic analysis
is already performed which is sufficiently general
to be useful in a wide range of applications.
Syntactic analysis of texts (such as Part-Of-
Speech tagging and syntactic parsing) is an ex-
ample of such a generic analysis, and has proved
useful in applications ranging from machine trans-
lation (Marcu et al, 2006) to text mining in the
bio-medical domain (Cohen and Hersh, 2005). A
syntactic parse is however a representation that is
very closely tied with the surface-form of natural
language, in contrast to Semantic Role Labeling
(SRL) which adds a layer of predicate-argument
information that generalizes across different syn-
tactic alternations (Palmer et al, 2005). SRL has
received a lot of attention in the research commu-
nity, and many systems have been developed (see
section 2). Most of these systems rely on a large
dataset for training that is manually annotated. In
this paper we investigate whether we can develop a
system that achieves state-of-the-art semantic role
labeling without relying on a large number of la-
beled examples. We aim to do so by employing the
Latent Words Language Model that learns latent
words from a large unlabeled corpus. Latent words
are words that (unlike observed words) did not oc-
cur at a particular position in a text, but given se-
mantic and syntactic constraints from the context
could have occurred at that particular position.
In section 2 we revise existing work on SRL and
on semi-supervised learning. Section 3 outlines
our supervised classifier for SRL and section 4 dis-
cusses the Latent Words Language Model. In sec-
tion 5 we will combine the two models for semi-
supervised role labeling. We will test the model
on the standard PropBank dataset and compare it
with state-of-the-art semi-supervised SRL systems
in section 6 and finally in section 7 we draw con-
clusions and outline future work.
2 Related work
Gildea and Jurafsky (2002) were the first to de-
scribe a statistical system trained on the data from
the FrameNet project to automatically assign se-
mantic roles. This approach was soon followed
by other researchers (Surdeanu et al, 2003; Prad-
han et al, 2004; Xue and Palmer, 2004), focus-
21
ing on improved sets of features, improved ma-
chine learning methods or both, and SRL became
a shared task at the CoNLL 2004, 2005 and 2008
conferences1 . The best system (Johansson and
Nugues, 2008) in CoNLL 2008 achieved an F1-
measure of 81.65% on the workshop?s evaluation
corpus.
Semi-supervised learning has been suggested
by many researchers as a solution to the annota-
tion bottleneck (see (Chapelle et al, 2006; Zhu,
2005) for an overview), and has been applied suc-
cessfully on a number of natural language pro-
cessing tasks. Mann and McCallum (2007) ap-
ply Expectation Regularization to Named Entity
Recognition and Part-Of-Speech tagging, achiev-
ing improved performance when compared to su-
pervised methods, especially on small numbers of
training data. Koo et al (2008) present an algo-
rithm for dependency parsing that uses clusters of
semantically related words, which were learned
in an unsupervised manner. There has been lit-
tle research on semi-supervised learning for SRL.
We refer to He and Gildea (2006) who tested ac-
tive learning and co-training methods, but found
little or no gain from semi-supervised learning,
and to Swier and Stevenson (2004), who achieved
good results using semi-supervised methods, but
tested their methods on a small number of Verb-
Net roles, which have not been used by other SRL
systems. To the best of our knowledge no sys-
tem was able to reproduce the successful results
of (Swier and Stevenson, 2004) on the PropBank
roleset. Our approach most closely resembles the
work of F?rstenau and Lapata (2009) who auto-
matically expand a small training set using an au-
tomatic dependency alignment of unlabeled sen-
tences. This method was tested on the FrameNet
corpus and improved results when compared to a
fully-supervised classifier. We will discuss their
method in detail in section 5.
3 Semantic role labeling
Fillmore (1968) introduced semantic structures
called semantic frames, describing abstract ac-
tions or common situations (frames) with common
roles and themes (semantic roles). Inspired by this
idea different resources were constructed, includ-
ing FrameNet (Baker et al, 1998) and PropBank
(Palmer et al, 2005). An alternative approach to
semantic role labeling is the framework developed
1See http://www.cnts.ua.ac.be/conll/ for an overview.
by Halliday (1994) and implemented by Mehay
et al (2005). PropBank has thus far received the
most attention of the research community, and is
used in our work.
3.1 PropBank
The goal of the PropBank project is to add seman-
tic information to the syntactic nodes in the En-
glish Penn Treebank. The main motivation for this
annotation is the preservation of semantic roles
across different syntactic realizations. Take for in-
stance the sentences
1. The window broke.
2. John broke the window.
In both sentences the constituent ?the window? is
broken, although it occurs at different syntactic
positions. The PropBank project defines for a
large collection of verbs (excluding auxiliary
verbs such as ?will?, ?can?, ...) a set of senses,
that reflect the different meanings and syntactic
alternations of this verb. Every sense has a
number of expected roles, numbered from Arg0
to Arg5. A small number of arguments are shared
among all senses of all verbs, such as temporals
(Arg-TMP), locatives (Arg-LOC) and directionals
(Arg-DIR). Additional to the frame definitions,
PropBank has annotated a large training corpus
containing approximately 113.000 annotated
verbs. An example of an annotated sentence is
[John Arg0][broke BREAK.01] [the window Arg1].
Here BREAK.01 is the first sense of the ?break?
verb. Note that (1) although roles are defined for
every frame separately, in reality roles with iden-
tical names are identical or very similar for all
frames, a fact that is exploited to train accurate role
classifiers and (2) semantic role labeling systems
typically assume that a frame is fully expressed in
a single sentence and thus do not try to instanti-
ate roles across sentence boundaries. Although the
original PropBank corpus assigned semantic roles
to syntactic phrases (such as noun phrases), we use
the CoNLL dataset, where the PropBank corpus
was converted to a dependency representation, as-
signing semantic roles to single (head) words.
3.2 Features
In this section we discuss the features used in the
semantic role labeling system. All features but the
22
Split path feature are taken from existing seman-
tic role labeling systems, see for example (Gildea
and Jurafsky, 2002; Lim et al, 2004; Thompson
et al, 2006). The number in brackets denotes the
number of unique features for that type.
Word We split every sentence in (unigram) word
tokens, including punctuation. (37079)
Stem We reduce the word tokens to their stem,
e.g. ?walks? -> ?walk?. (28690)
POS The part-of-speech tag for every word, e.g.
?NNP? (for a singular proper noun). (77)
Neighbor POS?s The concatenated part-of-
speech tags of the word before and the word
just after the current word, e.g. ?RBS_JJR?.
(1787)
Path This important feature describes the path
through the dependency tree from the current
word to the position of the predicate, e.g.
?coord?obj?adv?root?dep?nmod?pmod?,
where ??? indicates going up a constituent
and ??? going down one constituent.
(829642)
Split Path Because of the nature of the path fea-
ture, an explosion of unique features is found
in a given data set. We reduce this by split-
ting the path in different parts and using every
part as a distinct feature. We split, for exam-
ple, the previous path in 6 different features:
?coord?, ??obj?, ??adv?, ??root?, ??dep?,
??nmod?, ??pmod?. Note that the split path
feature includes the POS feature, since the
first component of the path is the POS tag for
the current word. This feature has not been
used previously for semantic role detection.
(155)
For every word wi in the training and test set we
construct the feature vector f(wi), where at every
position in this vector 1 indicates the presence for
the corresponding feature and 0 the absence of that
feature.
3.3 Discriminative model
Discriminative models have been found to outper-
form generative models for many different tasks
including SRL (Lim et al, 2004). For this reason
we also employ discriminative models here. The
structure of the model was inspired by a similar
Figure 1: Discriminative model for SRL. Grey
circles represent observed variables, white circles
hidden variables and arrows directed dependen-
cies. s ranges over all sentences in the corpus and
j over the n words in the sentence.
(although generative) model in (Thompson et al,
2006) where it was used for semantic frame clas-
sification. The model (fig. 1) assumes that the role
label ri j for the word wi is conditioned on the fea-
tures fi and on the role label ri?1 j of the previous
word and that the predicate label p j for word w j is
conditioned on the role labels R j and on the fea-
tures f j. This model can be seen as an extension
of the standard Maximum Entropy Markov Model
(MEMM, see (Ratnaparkhi, 1996)) with an extra
dependency on the predicate label, we will hence-
forth refer to this model as MEMM+pred.
To estimate the parameters of the MEMM+pred
model we turn to the successful Maximum En-
tropy (Berger et al, 1996) parameter estimation
method. The Maximum Entropy principle states
that the best model given the training data is the
model such that the conditional distribution de-
fined by the model has maximum entropy subject
to the constraints represented by the training ex-
amples. There is no closed form solution to find
this maximum and we thus turn to an iterative
method. In this work we use Generalized Itera-
tive Scaling2, but other methods such as (quasi-)
Newton optimization could also have been used.
4 Latent Words Language Model
4.1 Rationale
As discussed in sections 1 and 3 most SRL sys-
tems are trained today on a large set of manually
annotated examples. PropBank for example con-
tains approximately 50000 sentences. This man-
ual annotation is both time and labour-intensive,
and needs to be repeated for new languages or
2We use the maxent package available on
http://maxent.sourceforge.net/
23
for new domains requiring a different set of roles.
One approach that can help to solve this problem
is semi-supervised learning, where a small set of
annotated examples is used together with a large
set of unlabeled examples when training a SRL
model.
Manual inspection of the results of the super-
vised model discussed in the previous section
showed that the main source of errors was in-
correct labeling of a word because the word to-
ken did not occur, or occurred only a small num-
ber of times in the training set. We hypothesize
that knowledge of semantic similar words could
overcome this problem by associating words that
occurred infrequently in the training set to sim-
ilar words that occurred more frequently. Fur-
thermore, we would like to learn these similar-
ities automatically, to be independent of knowl-
edge sources that might not be available for all
languages or domains.
The Distributional Hypothesis, supported by
theoretical linguists such as Harris (1954), states
that words that occur in the same contexts tend
to have similar meanings. This suggests that one
can learn the similarity between two words auto-
matically by comparing their relative contexts in
a large unlabeled corpus, which was confirmed by
different researchers (e.g. (Lin, 1998; McDonald
and Ramscar, 2001; Grefenstette, 1994)). Differ-
ent methods for computing word similarities have
been proposed, differing between methods to rep-
resent the context (using dependency relationship
or a window of words) and between methods that,
given a set of contexts, compute the similarity be-
tween different words (ranging from cosine simi-
larity to more complex metrics such as the Jaccard
index). We refer to (Lin, 1998) for a comparison
of the different similarity metrics.
In the next section we propose a novel method
to learn word similarities, the Latent Words Lan-
guage Model (LWLM) (Deschacht and Moens,
2009). This model learns similar words and learns
the a distribution over the contexts in which cer-
tain types of words occur typically.
4.2 Definition
The LWLM introduces for a text T = w1...wN of
length N for every observed word wi at position i
a hidden variable hi. The model is a generative
model for natural language, in which the latent
variable hi is generated by its context C(hi) and the
observed word wi is generated by the latent vari-
able hi. In the current model we assume that the
context is C(hi) = hi?1i?2h
i+2
i+1 where h
i?1
i?2 = hi?2hi?1
is the two previous words and hi+2i+1 = hi+1hi+2 is
the two next words. The observed wi has a value
from the vocabulary V , while the hidden variable
hi is unknown, and is modeled as a probability
distribution over all words of V . We will see in
the next section how this distribution is estimated
from a large unlabeled training corpus. The aim
of this model is to estimate, at every position i,
a distribution for hi, assigning high probabilities
to words that are similar to wi, given the context
of this word C(hi), and low probabilities to words
that are not similar to wi in this context.
A possible interpretation of this model states
that every hidden variable hi models the ?mean-
ing? for a particular word in a particular context.
In this probabilistic model, when generating a sen-
tence, we generate the meaning of a word (which
is an unobserved representation) with a certain
probability, and then we generate a certain obser-
vation by writing down one of the possible words
that express this meaning.
Creating a representation that models the mean-
ing of a word is an interesting (and controversial)
topic in its own right, but in this work we make
the assumption that the meaning of a particular
word can be modeled using other words. Model-
ing the meaning of a word with other words is not
an unreasonable one, since it is already employed
in practice by humans (e.g. by using dictionar-
ies and thesauri) and machines (e.g. relying on a
lexical resource such as WordNet) in word sense
disambiguation tasks.
4.3 Parameter estimation
As we will further see the LWLM model has three
probability distributions: P(wi|hi), the probability
of the observed word w j given the latent variable
h j, P(hi|hi?1i?2), the probability of the hidden word
h j given the previous variables h j?2 and h j?1, and
P(hi|hi+2i+1), the probability of the hidden word h j
given the next variables h j+1 and h j+2. These dis-
tributions need to be learned from a training text
Ttrain =< w0...wz > of length Z.
4.3.1 The Baum-Welch algorithm
The attentive reader will have noticed the sim-
ilarity between the proposed model and a stan-
dard second-order Hidden Markov Model (HMM)
where the hidden state is dependent on the two
24
previous states. However, we are not able to use
the standard Baum-Welch (or forward-backward)
algorithm, because the hidden variable hi is mod-
eled as a probability distribution over all words
in the vocabulary V . The Baum-Welch algorithm
would result in an execution time of O(|V |3NG)
where |V | is the size of the vocabulary, N is the
length of the training text and G is the number of
iterations needed to converge. Since in our dataset
the vocabulary size is more than 30K words (see
section 3.2), using this algorithm is not possible.
Instead we use techniques of approximate infer-
ence, i.e. Gibbs sampling.
4.3.2 Initialization
Gibbs sampling starts from a random initializa-
tion for the hidden variables and then improves
the estimates in subsequent iterations. In prelimi-
nary experiments it was found that a pure random
initialization results in a very long burn-in-period
and a poor performance of the final model. For
this reason we initially set the distributions for the
hidden words equal to the distribution of words as
given by a standard language model3.
4.3.3 Gibbs sampling
We store the initial estimate of the hidden vari-
ables in M0train =< h0...hZ >, where hi generates
wi at every position i. Gibbs sampling is a Markov
Chain Monte Carlo method that updates the esti-
mates of the hidden variables in a number of it-
erations. M?train denotes the estimate of the hid-
den variables in iteration ? . In every iteration a
new estimate M?+1train is generated from the previ-
ous estimate M?train by selecting a random posi-
tion j and updating the value of the hidden vari-
able at that position. The probability distributions
P?(w j|h j), P?(h j|h j?1j?2) and P?(h j|h
j+2
j+1) are con-
structed by collecting the counts from all positions
i 6= j. The hidden variable h j is dependent on h j?2,
h j?1, h j+1, h j+2 and w j and we can compute the
distribution of possible values for the variable h j
as
P?(h j|w j,h j?10 ,h
Z
j+1) =
P?(w j|h j)P?(h j|h j?1j?2h
j+2
j+1)
?hi P?(wi|hi)P?(h j|h j?1j?2h j+2j+1)
We set P(h j|h j?1j?2h
j+2
j+1) = P(h j|h
j?1
j?2) ?P(h j|h
j+2
j+1)
which can be easily computed given the above dis-
3We used the interpolated Kneser-Ney model as described
in (Goodman, 2001).
tributions. We select a new value for the hidden
variable according to P?(h j|w j,h j?10 ,hZj+1) and
place it at position j in M?+1train. The current esti-
mate for all other unobserved words remains the
same. After performing this iteration a large num-
ber of times (|V | ?10 in this experiment), the dis-
tribution approaches the true maximum likelihood
distribution. Gibbs sampling however samples this
distribution, and thus will never reach it exactly. A
number of iterations (|V | ?100) is then performed
in which Gibbs sampling oscillates around the cor-
rect distribution. We collect independent samples
of this distribution every |V | ?10 iterations, which
are then used to construct the final model.
4.4 Evaluation of the Language Model
A first evaluation of the quality of the automat-
ically learned latent words is by translation of
this model into a sequential language model and
by measuring its perplexity on previously unseen
texts. In (Deschacht and Moens, 2009) we per-
form a number of experiments, comparing differ-
ent corpora (news texts from Reuters and from
Associated Press, and articles from Wikipedia)
and n-gram sizes (3-gram and 4-gram). We also
compared the proposed model with two state-of-
the-art language models, Interpolated Kneser-Ney
smoothing and fullibmpredict (Goodman, 2001),
and found that LWLM outperformed both models
on all corpora, with a perplexity reduction ranging
between 12.40% and 5.87%. These results show
that the estimated distributions over latent words
are of a high quality and lead us to believe they
could be used to improve automatic text analysis,
like SRL.
5 Role labeling using latent words
The previous section discussed how the LWLM
learns similar words and how these similarities im-
proved the perplexity on an unseen text of the lan-
guage model derived from this model. In this sec-
tion we will see how we integrate the latent words
model in two novel semi-supervised SRL models
and compare these with two state-of-the-art semi-
supervised models for SRL and dependency pars-
ing.
Latent words as additional features
In a first approach we estimate the distribution of
latent words for every word for both the training
and test set. We then use the latent words at every
25
position as additional probabilistic features for the
discriminative model. More specifically, we ap-
pend |V | extra values to the feature vector f(w j),
containing the probability distribution over the |V |
possible words for the hidden variable hi4. We call
this the LWFeatures method.
This method has the advantage that it is simple
to implement and that many existing SRL systems
can be easily extended by adding additional fea-
tures. We also expect that this method can be em-
ployed almost effortless in other information ex-
traction tasks, such as Named Entity Recognition
or Part-Of-Speech labeling.
We compare this approach to the semi-
supervised method in Koo et al (2008) who em-
ploy clusters of related words constructed by the
Brown clustering algorithm (Brown et al, 1992)
for syntactic processing of texts. Interestingly,
this clustering algorithm has a similar objective as
LWLM since it tries to optimize a class-based lan-
guage model in terms of perplexity on an unseen
test text. We employ a slightly different clustering
method here, the fullibmpredict method discussed
in (Goodman, 2001). This method was shown
to outperform the class based model proposed in
(Brown et al, 1992) and can thus be expected to
discover better clusters of words. We append the
feature vector f(w j) with c extra values (where c is
the number of clusters), respectively set to 1 if the
word wi belongs to the corresponding cluster or to
0 otherwise. We call this method the ClusterFea-
tures method.
Automatic expansion of the training set using
predicate argument alignment
We compare our approach with a method proposed
by F?rstenau and Lapata (2009). This approach is
more tailored to the specific case of SRL and is
summarized here.
Given a set of labeled seed verbs with annotated
semantic roles, for every annotated verb a number
of occurrences of this verb is found in unlabeled
texts where the context is similar to the context of
the annotated example. The context is defined here
as all words in the sentence that are direct depen-
dents of this verb, given the syntactic dependency
tree. The similarity between two occurrences of a
particular verb is measured by finding all different
alignments ? : M? ? {1...n} (M? ? {1, ...,m})
4Probabilities smaller than 1e10?4 were set to 0 for effi-
ciency reasons.
between the m dependents of the first occurrence
and the n dependents of the second occurrence.
Every alignment ? is assigned a score given by
?
i?M?
(
A ? syn(gi,g?(i))+ sem(wi,w?(i))?B
)
where syn(gi,g?(i)) denotes the syntactic simi-
larity between grammatical role5 gi of word wi
and grammatical role g?(i) of word w?(i), and
sem(wi,w?(i)) measures the semantic similarity
between words wi and w?(i). A is a constant
weighting the importance of the syntactic simi-
larity compared to semantic similarity, and B can
be interpreted as the lowest similarity value for
which an alignment between two arguments is
possible. The syntactic similarity syn(gi,g?(i)) is
defined as 1 if the dependency relations are iden-
tical, 0 < a < 1 if the relations are of the same
type but of a different subtype6 and 0 otherwise.
The semantic similarity sem(wi,w?(i)) is automat-
ically estimated as the cosine similarity between
the contexts of wi and w?(i) in a large text cor-
pus. For details we refer to (F?rstenau and Lapata,
2009).
For every verb in the annotated training set we
find the k occurrences of that verb in the unlabeled
texts where the contexts are most similar given the
best alignment. We then expand the training set
with these examples, automatically generating an
annotation using the discovered alignments. The
variable k controls the trade-off between anno-
tation confidence and expansion size. The final
model is then learned by running the supervised
training method on the expanded training set. We
call this method AutomaticExpansionCOS7 . The
values for k, a, A and B are optimized automati-
cally in every experiment on a held-out set (dis-
joint from both training and test set).
We adapt this approach by employing a different
method for measuring semantic similarity. Given
two words wi and w?(i) we estimate the distri-
bution of latent words, respectively L(hi) and
5Note that this is a syntactic role, not a semantic role as
the ones discussed in this article.
6Subtypes are fine-grained distinctions made by the parser
such as the underlying grammatical roles in passive construc-
tions.
7The only major differences with (F?rstenau and Lap-
ata, 2009) are the dependency parser which was used (the
MALT parser (Nivre et al, 2006) instead of the RASP parser
(Briscoe et al, 2006)) and the corpus employed to learn se-
mantic similarities (the Reuters corpus instead of the British
National Corpus). We expect that these differences will only
influence the results minimally.
26
5% 20% 50% 100%
Supervised 40.49% 67.23% 74.93% 78.65%
LWFeatures 60.29% 72.88% 76.42% 80.98%
ClusterFeatures 59.51% 66.70% 70.15% 72.62%
AutomaticExpansionCOS 47.05% 53.72% 64.51% 70.52%
AutomaticExpansionLW 45.40% 53.82% 65.39% 72.66%
Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing
the supervised method (Supervised) with the semi-supervised methods LWFeatures, ClusterFeatures,
AutomaticExpansionCOS and AutomaticExpansionLW. See section 5 for details on the different methods.
Best results are in bold.
L(h?(i)). We then compute the semantic similarity
measure as the Jensen-Shannon (Lin, 1997) diver-
gence
JS(L(hi)||L(h?(i))) =
1
2
[
D(L(hi)||avg)+D
(
L(h?(i))||avg
)]
where avg = (L(hi) + L(h?(i)))/2 is the average
between the two distributions and D(L(hi)||avg)
is the Kullback?Leiber divergence (Cover and
Thomas, 2006).
Although this change might appear only a slight
deviation from the original model discussed in
(F?rstenau and Lapata, 2009) it is potentially an
important one, since an accurate semantic similar-
ity measure will greatly influence the accuracy of
the alignments, and thus of the accuracy of the au-
tomatic expansion. We call this method Automat-
icExpansionLW.
6 Experiments
We perform a number of experiments where we
compare the fully supervised model with the semi-
supervised models proposed in the previous sec-
tion. We first train the LWLM model on an unla-
beled 5 million word Reuters corpus8.
We perform different experiments for the super-
vised and the four different semi-supervised meth-
ods (see previous section). Table 1 shows the re-
sults of the different methods on the test set of the
CoNLL 2008 shared task. We experimented with
different sizes for the training set, ranging from
5% to 100%. When using a subset of the full train-
ing set, we run 10 different experiments with ran-
dom subsets and average the results.
We see that the LWFeatures method performs
better than the other methods across all train-
ing sizes. Furthermore, these improvements are
8See http://www.daviddlewis.com/resources
larger for smaller training sets, showing that the
approach can be applied successfully in a setting
where only a small number of training examples
is available.
When comparing the LWFeatures method with
the ClusterFeatures method we see that, although
the ClusterFeatures method has a similar perfor-
mance for small training sizes, this performance
drops for larger training sizes. A possible expla-
nation for this result is the use of the clusters em-
ployed in the ClusterFeatures method. By defini-
tion the clusters merge many words into one clus-
ter, which might lead to good generalization (more
important for small training sizes) but can poten-
tially hurt precision (more important for larger
training sizes).
A third observation that can be made from table
1 is that, although both automatic expansion meth-
ods (AutomaticExpansionCOS and AutomaticEx-
pansionCOS) outperform the supervised method
for the smallest training size, for other sizes of the
training set they perform relatively poorly. An in-
formal inspection showed that for some examples
in the training set, little or no correct similar occur-
rences were found in the unlabeled text. The algo-
rithm described in section 5 adds the most similar
k occurrences to the training set for every anno-
tated example, also for these examples where lit-
tle or no similar occurrences were found. Often
the automatic alignment fails to generate correct
labels for these occurrences and introduces errors
in the training set. In the future we would like to
perform experiments that determine dynamically
(for instance based on the similarity measure be-
tween occurrences) for every annotated example
how many training examples to add.
27
7 Conclusions and future work
We have presented the Latent Words Language
Model and showed how it learns, from unla-
beled texts, latent words that capture the mean-
ing of a certain word, depending on the con-
text. We then experimented with different meth-
ods to incorporate the latent words for Semantic
Role Labeling, and tested different methods on the
PropBank dataset. Our best performing method
showed a significant improvement over the su-
pervised model and over methods previously pro-
posed in the literature. On the full training set
the best method performed 2.33% better than the
fully supervised model, which is a 10.91% error
reduction. Using only 5% of the training data the
best semi-supervised model still achieved 60.29%,
compared to 40.49% by the supervised model,
which is an error reduction of 33.27%. These re-
sults demonstrate that the latent words learned by
the LWLM help for this complex information ex-
traction task. Furthermore we have shown that the
latent words are simple to incorporate in an ex-
isting classifier by adding additional features. We
would like to perform experiments on employing
this model in other information extraction tasks,
such as Word Sense Disambiguation or Named
Entity Recognition. The current model uses the
context in a very straightforward way, i.e. the two
words left and right of the current word, but in
the future we would like to explore more advanced
methods to improve the similarity estimates. Lin
(1998) for example discusses a method where a
syntactic parse of the text is performed and the
context of a word is modeled using dependency
triples.
The other semi-supervised methods proposed
here were less successful, although all improved
on the supervised model for small training sizes.
In the future we would like to improve the de-
scribed automatic expansion methods, since we
feel that their full potential has not yet been
reached. More specifically we plan to experiment
with more advanced methods to decide whether
some automatically generated examples should be
added to the training set.
Acknowledgments
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978) and the IWT-SBO project AMASS++
(IWT-SBO-060051). We thank the anonymous re-
viewers for their helpful comments and Dennis N.
Mehay for his help on clarifying the linguistic mo-
tivation of our models.
References
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, volume 98.
Montreal, Canada.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational linguistics,
22(1):39?71.
T. Briscoe, J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings of
the Interactive Demo Session of COLING/ACL, vol-
ume 6.
P.F. Brown, R.L. Mercer, V.J. Della Pietra, and J.C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
O. Chapelle, B. Sch?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
N.A. Chinchor. 1998. Overview of MUC-7/MET-2. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7), volume 1.
A.M. Cohen and W.R. Hersh. 2005. A survey of cur-
rent work in biomedical text mining. Briefings in
Bioinformatics, 6(1):57?71.
T.M. Cover and J.A. Thomas. 2006. Elements of In-
formation Theory. Wiley-Interscience.
Koen Deschacht and Marie-Francine Moens. 2009.
The Latent Words Language Model. In Proceed-
ings of the 18th Annual Belgian-Dutch Conference
on Machine Learning.
C. J. Fillmore. 1968. The case for case. In E. Bach and
R. Harms, editors, Universals in Linguistic Theory.
Rinehart & Winston.
Hagen F?rstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 220?228, Athens, Greece.
Association for Computational Linguistics.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling, extended version. Technical re-
port, Microsoft Research.
28
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Springer.
M.A.K. Halliday. 1994. An Introduction to Functional
Grammar (second edition). Edward Arnold, Lon-
don.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
S. He and D. Gildea. 2006. Self-training and Co-
training for Semantic Role Labeling: Primary Re-
port. Technical report. TR 891.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis with
propbank and nombank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 183?187,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 595?603.
J.-H. Lim, Y.-S. Hwang, S.-Y. Park, and H.-C. Rim.
2004. Semantic role labeling using maximum en-
tropy model. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learning,
pages 122?125, Boston, Massachusetts, USA. ACL.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, volume 35,
pages 64?71. ACL.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational Linguistics,
pages 768?774. Association for Computational Lin-
guistics Morristown, NJ, USA.
G.S. Mann and A. McCallum. 2007. Simple, ro-
bust, scalable semi-supervised learning via expecta-
tion regularization. In Proceedings of the 24th In-
ternational Conference on Machine Learning, pages
593?600. ACM Press New York, USA.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntact-
ified target language phrases. In Proceedings of the
Conference on Empirical Methods for Natural Lan-
guage Processing, pages 44?52.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of the 23rd Annual Conference of the Cognitive Sci-
ence Society, pages 611?616.
Dennis Mehay, Rik De Busser, and Marie-Francine
Moens. 2005. Labeling generic semantic roles. In
Proceedings of the Sixth International Workshop on
Computational Semantics.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A
datadriven parser-generator for dependency parsing.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation, pages
2216?2219.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proceedings of the Hu-
man Language Technology Conference/North Amer-
ican chapter of the Association of Computational
Linguistics, Boston, MA.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142. Association for Com-
putational Linguistics.
M. Surdeanu, S. Harabagiu, J. Williams, and
P. Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 8?15.
R.S. Swier and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 95?102.
C. Thompson, R. Levy, and C. Manning. 2006. A gen-
erative model for FrameNet semantic role labeling .
In Proceedings of the 14th European Conference on
Machine Learning, Cavtat-Dubrovnik, Croatia.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, volume 4.
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences,
University of Wisconsin-Madison.
29
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1000?1007,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Text Analysis for Automatic Image Annotation
Koen Deschacht and Marie-Francine Moens
Interdisciplinary Centre for Law & IT
Department of Computer Science
Katholieke Universiteit Leuven
Tiensestraat 41, 3000 Leuven, Belgium
{koen.deschacht,marie-france.moens}@law.kuleuven.ac.be
Abstract
We present a novel approach to automati-
cally annotate images using associated text.
We detect and classify all entities (persons
and objects) in the text after which we de-
termine the salience (the importance of an
entity in a text) and visualness (the extent to
which an entity can be perceived visually)
of these entities. We combine these mea-
sures to compute the probability that an en-
tity is present in the image. The suitability
of our approach was successfully tested on
100 image-text pairs of Yahoo! News.
1 Introduction
Our society deals with a growing bulk of un-
structured information such as text, images and
video, a situation witnessed in many domains (news,
biomedical information, intelligence information,
business documents, etc.). This growth comes along
with the demand for more effective tools to search
and summarize this information. Moreover, there is
the need to mine information from texts and images
when they contribute to decision making by gov-
ernments, businesses and other institutions. The
capability to accurately recognize content in these
sources would largely contribute to improved index-
ing, classification, filtering, mining and interroga-
tion.
Algorithms and techniques for the disclosure of
information from the different media have been de-
veloped for every medium independently during the
last decennium, but only recently the interplay be-
tween these different media has become a topic of
interest. One of the possible applications is to help
analysis in one medium by employing information
from another medium. In this paper we study text
that is associated with an image, such as for instance
image captions, video transcripts or surrounding text
in a web page. We develop techniques that extract
information from these texts to help with the diffi-
cult task of accurate object recognition in images.
Although images and associated texts never contain
precisely the same information, in many situations
the associated text offers valuable information that
helps to interpret the image.
The central objective of the CLASS project1 is to
develop advanced learning methods that allow ima-
ges, video and associated text to be automatically
analyzed and structured. In this paper we test the
feasibility of automatically annotating images by us-
ing textual information in near-parallel image-text
pairs, in which most of the content of the image
corresponds to content of the text and vice versa.
We will focus on entities such as persons and ob-
jects. We will hereby take into account the text?s dis-
course structure and semantics, which allow a more
fine-grained identification of what content might be
present in the image, and will enrich our model with
world knowledge that is not present in the text.
We will first discuss the corpus on which we ap-
ply and test our techniques in section 2, after which
we outline what techniques we have developed: we
start with a baseline system to annotate images with
person names (section 3) and improve this by com-
puting the importance of the persons in the text (sec-
tion 4). We will then extend the model to include all
1http://class.inrialpes.fr/
1000
Hiram Myers, of Edmond, Okla., walks across the
fence, attempting to deliver what he called a ?people?s
indictment? of Halliburton CEO David Lesar, outside the
site of the annual Halliburton shareholders meeting in
Duncan, Okla., leading to his arrest, Wednesday, May 17,
2006.
Figure 1: Image-text pair with entity ?Hiram Myers?
appearing both in the text and in the image.
types of objects (section 5) and improve it by defin-
ing and computing the visualness measure (section
6). Finally we will combine these different tech-
niques in one probabilistic model in section 7.
2 The parallel corpus
We have created a parallel corpus consisting of 1700
image-text pairs, retrieved from the Yahoo! News
website2. Every image has an accompanying text
which describes the content of the image. This text
will in general discuss one or more persons in the
image, possibly one or more other objects, the loca-
tion and the event for which the picture was taken.
An example of an image-text pair is given in fig. 1.
Not all persons or objects who are pictured in the
images are necessarily described in the texts. The
inverse is also true, i.e. content mentioned in the
text may not be present in the image.
We have randomly selected 100 text-pairs from
the corpus, and one annotator has labeled every
image-text pair with the entities (i.e. persons and
2http://news.yahoo.com/
other objects) that appear both in the image and in
the text. For example, the image-text pair shown in
fig. 1 is annotated with one entity, ?Hiram Myers?,
since this is the only entity that appears both in the
text and in the image. On average these texts contain
15.04 entities, of which 2.58 appear in the image.
To build the appearance model of the text, we
have combined different tools. We will evaluate
every tool separately on 100 image-text pairs. This
way we have a detailed view on the nature of the
errors in the final model.
3 Automatically annotating person names
Given a text that is associated with an image, we
want to compute a probabilistic appearance model,
i.e. a collection of entities that are visible in the
image. We will start with a model that holds the
names of the persons that appear in the image, such
as was done by (Satoh et al, 1999; Berg et al, 2004),
and extend this model in section 5 to include all
other objects.
3.1 Named Entity Recognition
A logical first step to detect person names is Named
Entity Recognition (NER). We use the OpenNLP
package3, which detects noun phrase chunks in the
sentences that represent persons, locations, organi-
zations and dates. To improve the recognition of
person names, we use a dictionary of names, which
we have extracted from the Wikipedia4 website. We
have manually evaluated performance of NER on
our test corpus and found that performance was sa-
tisfying: we obtained a precision of 93.37% and a re-
call of 97.69%. Precision is the percentage of iden-
tified person names by the system that corresponds
to correct person names, and recall is the percentage
of person names in the text that have been correctly
identified by the system.
The texts contain a small number of noun phrase
coreferents that are in the form of pronouns, we have
resolved these using the LingPipe5 package.
3.2 Baseline system
We want to annotate an image using the associated
text. We try to find the names of persons which are
3http://opennlp.sourceforge.net/
4http://en.wikipedia.org/
5http://www.alias-i.com/lingpipe/
1001
both described in the text and visible in the image,
and we want to do so by relying only on an analysis
of the text. In some cases, such as the following
example, the text states explicitly whether a person
is (not) visible in the image:
President Bush [...] with Danish Prime
Minister Anders Fogh Rasmussen, not
pictured, at Camp David [...].
Developing a system that could extract this informa-
tion is not trivial, and even if we could do so, only a
very small percentage of the texts in our corpus con-
tain this kind of information. In the next section we
will look into a method that is applicable to a wide
range of (descriptive) texts and that does not rely on
specific information within the text.
To evaluate the performance of this system, we
will compare it with a simple baseline system. The
baseline system assumes that all persons in the text
are visible in the image, which results in a precision
of 71.27% and a recall of 95.56%. The (low) preci-
sion can be explained by the fact that the texts often
discuss people which are not present in the image.
4 Detection of the salience of a person
Not all persons discussed in a text are equally im-
portant. We would like to discover what persons
are in the focus of a text and what persons are only
mentioned briefly, because we presume that more
important persons in the text have a larger proba-
bility of appearing in the image than less important
persons. Because of the short lengths of the docu-
ments in our corpus, an analysis of lexical cohesion
between terms in the text will not be sufficient for
distinguishing between important and less important
entities. We define a measure, salience, which is a
number between 0 and 1 that represents the impor-
tance of an entity in a text. We present here a method
for computing this score based on an in depth ana-
lysis of the discourse of the text and of the syntactic
structure of the individual sentences.
4.1 Discourse segmentation
The discourse segmentation module, which we de-
veloped in earlier research, hierarchically and se-
quentially segments the discourse in different topics
and subtopics resulting in a table of contents of a
text (Moens, 2006). The table shows the main en-
tities and the related subtopic entities in a tree-like
structure that also indicates the segments (by means
of character pointers) to which an entity applies. The
algorithm detects patterns of thematic progression in
texts and can thus recognize the main topic of a sen-
tence (i.e., about whom or what the sentence speaks)
and the hierarchical and sequential relationships be-
tween individual topics. A mixture model, taking
into account different discourse features, is trained
with the Expectation Maximization algorithm on an
annotated DUC-2003 corpus. We use the resulting
discourse segmentation to define the salience of in-
dividual entities that are recognized as topics of a
sentence. We compute for each noun entity er in the
discourse its salience (Sal1) in the discourse tree,
which is proportional with the depth of the entity in
the discourse tree -hereby assuming that deeper in
this tree more detailed topics of a text are described-
and normalize this value to be between zero and one.
When an entity occurs in different subtrees, its max-
imum score is chosen.
4.2 Refinement with sentence parse
information
Because not all entities of the text are captured in the
discourse tree, we implement an additional refine-
ment of the computation of the salience of an entity
which is inspired by (Moens et al, 2006). The seg-
mentation module already determines the main topic
of a sentence. Since the syntactic structure is often
indicative of the information distribution in a sen-
tence, we can determine the relative importance of
the other entities in a sentence by relying on the re-
lationships between entities as signaled by the parse
tree. When determining the salience of an entity, we
take into account the level of the entity mention in
the parse tree (Sal2), and the number of children for
the entity in this structure (Sal3), where the normal-
ized score is respectively inversely proportional with
the depth of the parse tree where the entity occurs,
and proportional with the number of children.
We combine the three salience values (Sal1,
Sal2 and Sal3) by using a linear weighting. We
have experimentally determined reasonable coeffi-
cients for these three values, which are respectively
0.8, 0.1 and 0.1. Eventually, we could learn these
coefficients from a training corpus (e.g., with the
1002
Precision Recall F-measure
NER 71.27% 95.56% 81.65%
NER+DYN 97.66% 92.59% 95.06%
Table 1: Comparison of methods to predict what per-
sons described in the text will appear in the image,
using Named Entity Recognition (NER), and the
salience measure with dynamic cut-off (DYN).
Expectation Maximization algorithm).
We do not separately evaluate our technology
for salience detection as this technology was
already extensively evaluated in the past (Moens,
2006).
4.3 Evaluating the improved system
The salience measure defines a ranking of all the
persons in a text. We will use this ranking to improve
our baseline system. We assume that it is possible
to automatically determine the number of faces that
are recognized in the image, which gives us an indi-
cation of a suitable cut-off value. This approach is
reasonable since face detection (determine whether a
face is present in the image) is significant easier than
face recognition (determine which person is present
in the image). In the improved model we assume
that persons which are ranked higher than, or equal
to, the cut-off value appear in the image. For ex-
ample, if 4 faces appear in the image, we assume
that only the 4 persons of which the names in the
text have been assigned the highest salience appear
in the image. We see from table 1 that the precision
(97.66%) has improved drastically, while the recall
remained high (92.59%). This confirms the hypoth-
esis that determining the focus of a text helps in de-
termining the persons that appear in the image.
5 Automatically annotating persons and
objects
After having developed a reasonable successful sys-
tem to detect what persons will appear in the image,
we turn to a more difficult case : Detecting persons
and all other objects that are described in the text.
5.1 Entity detection
We will first detect what words in the text refer to an
entity. For this, we perform part-of-speech tagging
(i.e., detecting the syntactic word class such as noun,
verb, etc.). We take that every noun in the text rep-
resents an entity. We have used LTPOS (Mikheev,
1997), which performed the task almost errorless
(precision of 98.144% and recall of 97.36% on the
nouns in the test corpus). Person names which were
segmented using the NER package are also marked
as entities.
5.2 Baseline system
We want to detect the objects and the names of per-
sons which are both visible in the image and de-
scribed in the text. We start with a simple baseline
system, in which we assume that every entity in the
text appears in the image. As can be expected, this
results in a high recall (91.08%), and a very low pre-
cision (15.62%). We see that the problem here is
far more difficult compared to detecting only per-
son names. This can be explained by the fact that
many entities (such as for example August, idea and
history) will never (or only indirectly) appear in an
image. In the next section we will try to determine
what types of entities are more likely to appear in
the image.
6 Detection of the visualness of an entity
The assumption that every entity in the text appears
in the image is rather crude. We will enrich our
model with external world knowledge to find enti-
ties which are not likely to appear in an image. We
define a measure called visualness, which is defined
as the extent to which an entity can be perceived vi-
sually.
6.1 Entity classification
After we have performed entity detection, we want
to classify every entity according to a certain seman-
tic database. We use the WordNet (Fellbaum, 1998)
database, which organizes English nouns, verbs, ad-
jectives and adverbs in synsets. A synset is a col-
lection of words that have a close meaning and that
represent an underlying concept. An example of
such a synset is ?person, individual, someone, some-
body, mortal, soul?. All these words refer to a hu-
1003
man being. In order to correctly assign a noun in
a text to its synset, i.e., to disambiguate the sense
of this word, we use an efficient Word Sense Dis-
ambiguation (WSD) system that was developed by
the authors and which is described in (Deschacht
and Moens, 2006). Proper names are labeled by
the Named Entity Recognizer, which recognizes per-
sons, locations and organizations. These labels in
turn allow us to assign the corresponding WordNet
synset.
The combination of the WSD system and the
NER package achieved a 75.97% accuracy in classi-
fying the entities. Apart from errors that resulted
from erroneous entity detection (32.32%), errors
were mainly due to the WSD system (60.56%) and
in a smaller amount to the NER package (8.12%).
6.2 WordNet similarity
We determine the visualness for every synset us-
ing a method that was inspired by Kamps and Marx
(2002). Kamps and Marx use a distance measure
defined on the adjectives of the WordNet database
together with two seed adjectives to determine the
emotive or affective meaning of any given adjective.
They compute the relative distance of the adjective
to the seed synsets ?good? and ?bad? and use this
distance to define a measure of affective meaning.
We take a similar approach to determine the visu-
alness of a given synset. We first define a similarity
measure between synsets in the WordNet database.
Then we select a set of seed synsets, i.e. synsets
with a predefined visualness, and use the similarity
of a given synset to the seed synsets to determine the
visualness.
6.3 Distance measure
The WordNet database defines different relations be-
tween its synsets. An important relation for nouns is
the hypernym/hyponym relation. A noun X is a hy-
pernym of a noun Y if Y is a subtype or instance of
X. For example, ?bird? is a hypernym of ?penguin?
(and ?penguin? is a hyponym of ?bird?). A synset
in WordNet can have one or more hypernyms. This
relation organizes the synsets in a hierarchical tree
(Hayes, 1999).
The similarity measure defined by Lin (1998) uses
the hypernym/hyponym relation to compute a se-
mantic similarity between two WordNet synsets S1
and S2. First it finds the most specific (lowest in the
tree) synset Sp that is a parent of both S1 and S2.
Then it computes the similarity of S1 and S2 as
sim(S1, S2) =
2logP (Sp)
logP (S1) + logP (S2)
Here the probability P (Si) is the probability of
labeling any word in a text with synset Si or with
one of the descendants of Si in the WordNet hier-
archy. We estimate these probabilities by counting
the number of occurrences of a synset in the Sem-
cor corpus (Fellbaum, 1998; Landes et al, 1998),
where all noun chunks are labeled with their Word-
Net synset. The probability P (Si) is computed as
P (Si) =
C(Si)
?N
n=1 C(Sn)
+ ?Kk=1 P (Sk)
where C(Si) is the number of occurrences of Si,
N is the total number of synsets in WordNet and
K is the number of children of Si. The Word-
Net::Similarity package (Pedersen et al, 2004) im-
plements this distance measure and was used by the
authors.
6.4 Seed synsets
We have manually selected 25 seed synsets in Word-
Net, where we tried to cover the wide range of topics
we were likely to encounter in the test corpus. We
have set the visualness of these seed synsets to either
1 (visual) or 0 (not visual). We determine the visu-
alness of all other synsets using these seed synsets.
A synset that is close to a visual seed synset gets a
high visualness and vice versa. We choose a linear
weighting:
vis(s) =
?
i
vis(si)
sim(s, si)
C(s)
where vis(s) returns a number between 0 and 1 de-
noting the visualness of a synset s, si are the seed
synsets, sim(s, t) returns a number between 0 and 1
denoting the similarity between synsets s and t and
C(s) is constant given a synset s:
C(s) =
?
i
sim(s, si)
1004
6.5 Evaluation of the visualness computation
To determine the visualness, we first assign the cor-
rect WordNet synset to every entity, after which we
compute a visualness score for these synsets. Since
these scores are floating point numbers, they are
hard to evaluate manually. During evaluation, we
make the simplifying assumption that all entities
with a visualness below a certain threshold are not
visual, and all entities above this threshold are vi-
sual. We choose this threshold to be 0.5. This re-
sults in an accuracy of 79.56%. Errors are mainly
caused by erroneous entity detection and classifica-
tion (63.10%) but also because of an incorrect as-
signment of the visualness (36.90%) by the method
described above.
7 Creating an appearance model using
salience and visualness
In the previous section we have created a method to
calculate a visualness score for every entity, because
we stated that removing the entities which can never
be perceived visually will improve the performance
of our baseline system. An experiment proves that
this is exactly the case. If we assume that only the
entities that have a visualness above a 0.5 thresh-
old are visible and will appear in the image, we get
a precision of 48.81% and a recall of 87.98%. We
see from table 2 that this is already a significant im-
provement over the baseline system.
In section 4 we have seen that the salience mea-
sure helps in determining what persons are visible in
the image. We have used the fact that face detection
in images is relatively easily and can thus supply a
cut-off value for the ranked person names. In the
present state-of-the-art, we are not able to exploit a
similar fact when detecting all types of entities. We
will thus use the salience measure in a different way.
We compute the salience of every entity, and we
assume that only the entities with a salience score
above a threshold of 0.5 will appear in the image.
We see that this method drastically improves preci-
sion to 66.03%, but also lowers recall until 54.26%.
We now create a last model where we combine
both the visualness and the salience measures. We
want to calculate the probability of the occurrence of
an entity eim in the image, given a text t, P (eim|t).
We assume that this probability is proportional with
Precision Recall F-measure
Ent 15.62% 91.08% 26.66%
Ent+Vis 48.81% 87.98% 62.78%
Ent+Sal 66.03% 54.26% 59.56%
Ent+Vis+Sal 70.56% 67.82% 69.39%
Table 2: Comparison of methods to predict the en-
tities that appear in the image, using entity detec-
tion (Ent), and the visualness (Vis) and salience (Sal)
measures.
the degree of visualness and salience of eim in t. In
our framework, P (eim|t) is computed as the product
of the salience of the entity eim and its visualness
score, as we assume both scores to be independent.
Again, for evaluation sake, we choose a threshold
of 0.4 to transform this continuous ranking into a
binary classification. This results in a precision of
70.56% and a recall of 67.82%. This model is the
best of the 4 models for entity annotation which have
been evaluated.
8 Related Research
Using text that accompanies the image for annotat-
ing images and for training image recognition is not
new. The earliest work (only on person names) is
by Satoh (1999) and this research can be considered
as the closest to our work. The authors make a dis-
tinction between proper names, common nouns and
other words, and detect entities based on a thesaurus
list of persons, social groups and other words, thus
exploiting already simple semantics. Also a rudi-
mentary approach to discourse analysis is followed
by taking into account the position of words in a
text. The results were not satisfactory: 752 words
were extracted from video as candidates for being in
the accompanying images, but only 94 were correct
where 658 were false positives. Mori et al (2000)
learn textual descriptions of images from surround-
ing texts. These authors filter nouns and adjectives
from the surrounding texts when they occur above
a certain frequency and obtain a maximum hit rate
of top 3 words that is situated between 30% and
40%. Other approaches consider both the textual
and image features when building a content model
of the image. For instance, some content is selected
from the text (such as person names) and from the
1005
image (such as faces) and both contribute in describ-
ing the content of a document. This approach was
followed by Barnard (2003).
Westerveld (2000) combines image features and
words from collateral text into one semantic space.
This author uses Latent Semantic Indexing for rep-
resenting the image/text pair content. Ayache et al
(2005) classify video data into different topical con-
cepts. The results of these approaches are often dis-
appointing. The methods here represent the text as a
bag of words possibly augmented with a tf (term fre-
quency) x idf (inverse document frequency) weight
of the words (Amir et al, 2005). In exceptional
cases, the hierarchical XML structure of a text doc-
ument (which was manually annotated) is taken into
account (Westerveld et al, 2005). The most inter-
esting work here to mention is the work of Berg
et al (2004) who also process the nearly parallel
image-text pairs found in the Yahoo! news corpus.
They link faces in the image with names in the text
(recognized with named entity recognition), but do
not consider other objects. They consider pairs of
person names (text) and faces (image) and use clus-
tering with the Expectation Maximization algorithm
to find all faces belonging to a certain person. In
their model they consider the probability that an en-
tity is pictured given the textual context (i.e., the
part-of-speech tags immediately prior and after the
name, the location of the name in the text and the
distance to particular symbols such as ?(R)?), which
is learned with a probabilistic classifier in each step
of the EM iteration. They obtained an accuracy of
84% on person face recognition.
In the CLASS project we work together with
groups specialized in image recognition. In future
work we will combine face and object recognition
with text analysis techniques. We expect the recog-
nition and disambiguation of faces to improve if
many image-text pairs that treat the same person are
used. On the other hand our approach is also valu-
able when there are few image-text pairs that picture
a certain person or object. The approach of Berg
et al could be augmented with the typical features
that we use, namely salience and visualness. In De-
schacht et al (2007) we have evaluated the ranking
of persons and objects by the method we have de-
scribed here and we have shown that this ranking
correlates with the importance of persons and ob-
jects in the picture.
None of the above state-of-the-art approaches
consider salience and visualness as discriminating
factors in the entity recognition, although these as-
pects could advance the state-of-the-art.
9 Conclusion
Our society in the 21st century produces gigantic
amounts of data, which are a mixture of different
media. Our repositories contain texts interwoven
with images, audio and video and we need auto-
mated ways to automatically index these data and
to automatically find interrelationships between the
various media contents. This is not an easy task.
However, if we succeed in recognizing and aligning
content in near-parallel image-text pairs, we might
be able to use this acquired knowledge in index-
ing comparable image-text pairs (e.g., in video) by
aligning content in these media.
In the experiment described above, we analyze
the discourse and semantics of texts of near-parallel
image-text pairs in order to compute the probability
that an entity mentioned in the text is also present in
the accompanying image. First, we have developed
an approach for computing the salience of each en-
tity mentioned in the text. Secondly, we have used
the WordNet classification in order to detect the vi-
sualness of an entity, which is translated into a vi-
sualness probability. The combined salience and vi-
sualness provide a score that signals the probability
that the entity is present in the accompanying image.
We extensively evaluated all the different modules
of our system, pinpointing weak points that could be
improved and exposing the potential of our work in
cross-media exploitation of content.
We were able to detect the persons in the text
that are also present in the image with a (evenly
weighted) F-measure of more than 95%, and in addi-
tion were able to detect the entities that are present
in the image with a F-measure of more than 69%.
These results have been obtained by relying only on
an analysis of the text and were substantially better
than the baseline approach. Even if we can not re-
solve all ambiguity, keeping the most confident hy-
potheses generated by our textual hypotheses will
greatly assist in analyzing images.
In the future we hope to extrinsically evaluate
1006
the proposed technologies, e.g., by testing whether
the recognized content in the text, improves image
recognition, retrieval of multimedia sources, mining
of these sources, and cross-media retrieval. In addi-
tion, we will investigate how we can build more re-
fined appearance models that incorporate attributes
and actions of entities.
Acknowledgments
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978). We acknowledge the CLASS consortium
partners for their valuable comments and we are es-
pecially grateful to Yves Gufflet from the INRIA
research team (Grenoble, France) for collecting the
Yahoo! News dataset.
References
Arnon Amir, Janne Argillander, Murray Campbell,
Alexander Haubold, Giridharan Iyengar, Shahram
Ebadollahi, Feng Kang, Milind R. Naphade, Apostol
Natsev, John R. Smith, Jelena Tes?io?, and Timo Volk-
mer. 2005. IBM Research TRECVID-2005 Video
Retrieval System. In Proceedings of TRECVID 2005,
Gaithersburg, MD.
Ste?phane Ayache, Gearges M. Qunot, Jrme Gensel, and
Shin?Ichi Satoh. 2005. CLIPS-LRS-NII Experiments
at TRECVID 2005. In Proceedings of TRECVID
2005, Gaithersburg, MD.
Kobus Barnard, Pinar Duygulu, Nando de Freitas, David
Forsyth, David Blei, and Michael I. Jordan. 2003.
Matching Words and Pictures. Journal of Machine
Learning Research, 3(6):1107?1135.
Tamara L. Berg, Alexander C. Berg, Jaety Edwards, and
D.A. Forsyth. 2004. Who?s in the Picture? In Neural
Information Processing Systems, pages 137?144.
Koen Deschacht and Marie-Francine Moens. 2006. Ef-
ficient Hierarchical Entity Classification Using Con-
ditional Random Fields. In Proceedings of the
2nd Workshop on Ontology Learning and Population,
pages 33?40, Sydney, July.
Koen Deschacht, Marie-Francine Moens, and
W Robeyns. 2007. Cross-media entity recogni-
tion in nearly parallel visual and textual documents.
In Proceedings of the 8th RIAO Conference on Large-
Scale Semantic Access to Content (Text, Image, Video
and Sound). Cmu. (in press).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Brian Hayes. 1999. The Web of Words. American Sci-
entist, 87(2):108?112, March-April.
Jaap Kamps and Maarten Marx. 2002. Words with Atti-
tude. In Proceedings of the 1st International Confer-
ence on Global WordNet, pages 332?341, India.
Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998. Building Semantic Concordances. In Chris-
tiane Fellbaum, editor, WordNet: An Electronic Lex-
ical Database. The MIT Press.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proc. 15th International Conf. on Ma-
chine Learning.
Andrei Mikheev. 1997. Automatic Rule Induction for
Unknown-Word Guessing. Computational Linguis-
tics, 23(3):405?423.
Marie-Francine Moens, Patrick Jeuniaux, Roxana
Angheluta, and Rudradeb Mitra. 2006. Measur-
ing Aboutness of an Entity in a Text. In Proceed-
ings of HLT-NAACL 2006 TextGraphs: Graph-based
Algorithms for Natural Language Processing, East
Stroudsburg. ACL.
Marie-Francine Moens. 2006. Using Patterns of The-
matic Progression for Building a Table of Content of
a Text. Journal of Natural Language Engineering,
12(3):1?28.
Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka.
2000. Automatic Word Assignment to Images Based
on Image Division and Vector Quantization. In RIAO-
2000 Content-Based Multimedia Information Access,
Paris, April 12-14.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In The Proceedings of Fifth An-
nual Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-04),
Boston, May.
Shin?ichi Satoh, Yuichi Nakamura, and Takeo Kanade.
1999. Name-It: Naming and Detecting Faces in News
Videos. IEEE MultiMedia, 6(1):22?35, January-
March.
Thijs Westerveld, Jan C. van Gemert, Roberto Cornac-
chia, Djoerd Hiemstra, and Arjen de Vries. 2005. An
Integrated Approach to Text and Image Retrieval. In
Proceedings of TRECVID 2005, Gaithersburg, MD.
Thijs Westerveld. 2000. Image Retrieval: Content versus
Context. In Content-Based Multimedia Information
Access, RIAO 2000 Conference Proceedings, pages
276?284, April.
1007
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient Hierarchical Entity Classifier Using Conditional Random Fields
Koen Deschacht
Interdisciplinary Centre for Law & IT
Katholieke Universiteit Leuven
Tiensestraat 41, 3000 Leuven, Belgium
koen.deschacht@law.kuleuven.ac.be
Marie-Francine Moens
Interdisciplinary Centre for Law & IT
Katholieke Universiteit Leuven
Tiensestraat 41, 3000 Leuven, Belgium
marie-france.moens@law.kuleuven.be
Abstract
In this paper we develop an automatic
classifier for a very large set of labels, the
WordNet synsets. We employ Conditional
Random Fields (CRFs) because of their
flexibility to include a wide variety of non-
independent features. Training CRFs on a
big number of labels proved a problem be-
cause of the large training cost. By tak-
ing into account the hypernym/hyponym
relation between synsets in WordNet, we
reduced the complexity of training from
O(TM2NG) to O(T (logM)2NG) with
only a limited loss in accuracy.
1 Introduction
The work described in this paper was carried out
during the CLASS project1. The central objec-
tive of this project is to develop advanced learning
methods that allow images, video and associated
text to be analyzed and structured automatically.
One of the goals of the project is the alignment of
visual and textual information. We will, for exam-
ple, learn the correspondence between faces in an
image and persons described in surrounding text.
The role of the authors in the CLASS project is
mainly on information extraction from text.
In the first phase of the project we build a clas-
sifier for automatic identification and categoriza-
tion of entities in texts which we report here. This
classifier extracts entities from text, and assigns a
label to these entities chosen from an inventory
of possible labels. This task is closely related to
both named entity recognition (NER), which tra-
ditionally assigns nouns to a small number of cate-
gories and word sense disambiguation (Agirre and
1http://class.inrialpes.fr/
Rigau, 1996; Yarowsky, 1995), where the sense
for a word is chosen from a much larger inventory
of word senses.
We will employ a probabilistic model that?s
been used successfully in NER (Conditional Ran-
dom Fields) and use this with an extensive inven-
tory of word senses (the WordNet lexical database)
to perform entity detection.
In section 2 we describe WordNet and it?s use
for entity categorization. Section 3 gives an
overview of Conditional Random Fields and sec-
tion 4 explains how the parameters of this model
are estimated during training. We will drastically
reduce the computational complexity of training in
section 5. Section 6 describes the implementation
of this method, section 7 the obtained results and
finally section 8 future work.
2 WordNet
WordNet (Fellbaum et al, 1998) is a lexical
database whose design is inspired by psycholin-
guistic theories of human lexical memory. English
nouns, verbs, adjectives and adverbs are organized
in synsets. A synset is a collection of words that
have a close meaning and that represent an under-
lying concept. An example of such a synset is
?person, individual, someone, somebody, mortal,
soul?. All these words refer to a human being.
WordNet (v2.1) contains 155.327 words, which
are organized in 117.597 synsets. WordNet de-
fines a number of relations between synsets. For
nouns the most important relation is the hyper-
nym/hyponym relation. A noun X is a hypernym
of a noun Y if Y is a subtype or instance of X. For
example, ?bird? is a hypernym of ?penguin? (and
?penguin? is a hyponym of ?bird?). This relation
organizes the synsets in a hierarchical tree (Hayes,
1999), of which a fragment is pictured in fig. 1.
33
Figure 1: Fragment of the hypernym/hyponym
tree
This tree has a depth of 18 levels and maximum
width of 17837 synsets (fig. 2).
We will build a classifier using CRFs that tags
noun phrases in a text with their WordNet synset.
This will enable us to recognize entities, and to
classify the entities in certain groups. Moreover,
it allows learning the context pattern of a certain
meaning of a word. Take for example the sentence
?The ambulance took the remains of the bomber
to the morgue.? Having every noun phrase tagged
with it?s WordNet synset reveals that in this sen-
tence, ?bomber? is ?a person who plants bombs?
(and not ?a military aircraft that drops bombs dur-
ing flight?). Using the hypernym/hyponym rela-
tions from WordNet, we can also easily find out
that ?ambulance? is a kind of ?car?, which in turn
is a kind of ?conveyance, transport? which in turn
is a ?physical object?.
3 Conditional Random Fields
Conditional random fields (CRFs) (Lafferty et al,
2001; Jordan, 1999; Wallach, 2004) is a statistical
method based on undirected graphical models. Let
X be a random variable over data sequences to be
labeled and Y a random variable over correspond-
ing label sequences. All components Yi of Y are
assumed to range over a finite label alphabet K.
In this paper X will range over the sentences of
a text, tagged with POS-labels and Y ranges over
the synsets to be recognized in these sentences.
We define G = (V,E) to be an undirected
graph such that there is a node v ? V correspond-
ing to each of the random variables representing an
element Yv of Y . If each random variable Yv obeys
the Markov property with respect to G (e.g., in a
first order model the transition probability depends
only on the neighboring state), then the model
(Y,X) is a Conditional Random Field. Although
the structure of the graph G may be arbitrary, we
limit the discussion here to graph structures in
Figure 2: Number of synsets per level in WordNet
which the nodes corresponding to elements of Y
form a simple first-order Markov chain.
A CRF defines a conditional probability distri-
bution p(Y |X) of label sequences given input se-
quences. We assume that the random variable se-
quences X and Y have the same length and use
x = (x1, ..., xT ) and y = (y1, ..., yT ) for an input
sequence and label sequence respectively. Instead
of defining a joint distribution over both label and
observation sequences, the model defines a condi-
tional probability over labeled sequences. A novel
observation sequence x is labeled with y, so that
the conditional probability p(y|x) is maximized.
We define a set of K binary-valued features or
feature functions fk(yt?1, yt,x) that each express
some characteristic of the empirical distribution of
the training data that should also hold in the model
distribution. An example of such a feature is
fk(yt?1, yt,x) =
?
?
?
?
?
1 if x has POS ?NN? andyt is concept ?entity?
0 otherwise
(1)
Feature functions can depend on the previous
(yt?1) and the current (yt) state. Considering K
feature functions, the conditional probability dis-
tribution defined by the CRF is
p(y|x) = 1Z(x)exp
{ T
?
t=1
K
?
k=1
?kfk(yt?1, yt,x)
}
(2)
where ?j is a parameter to model the observed
statistics and Z(x) is a normalizing constant com-
puted as
Z(x) =
?
y?Y
exp
{ T
?
t=1
K
?
k=1
?kfk(yt?1, yt,x)
}
This method can be thought of a generalization
of both the Maximum Entropy Markov model
(MEMM) and the Hidden Markov model (HMM).
34
It brings together the best of discriminative mod-
els and generative models: (1) It can accommo-
date many statistically correlated features of the
inputs, contrasting with generative models, which
often require conditional independent assumptions
in order to make the computations tractable and (2)
it has the possibility of context-dependent learning
by trading off decisions at different sequence posi-
tions to obtain a global optimal labeling. Because
CRFs adhere to the maximum entropy principle,
they offer a valid solution when learning from in-
complete information. Given that in information
extraction tasks, we often lack an annotated train-
ing set that covers all possible extraction patterns,
this is a valuable asset.
Lafferty et al (Lafferty et al, 2001) have shown
that CRFs outperform both MEMM and HMM
on synthetic data and on a part-of-speech tagging
task. Furthermore, CRFs have been used success-
fully in information extraction (Peng and McCal-
lum, 2004), named entity recognition (Li and Mc-
Callum, 2003; McCallum and Li, 2003) and sen-
tence parsing (Sha and Pereira, 2003).
4 Parameter estimation
In this section we?ll explain to some detail how to
derive the parameters ? = {?k}, given the train-
ing data. The problem can be considered as a con-
strained optimization problem, where we have to
find a set of parameters which maximizes the log
likelihood of the conditional distribution (McCal-
lum, 2003). We are confronted with the problem
of efficiently calculating the expectation of each
feature function with respect to the CRF model
distribution for every observation sequence x in
the training data. Formally, we are given a set
of training examples D =
{
x(i),y(i)
}N
i=1
where
each x(i) =
{
x(i)1 , x
(i)
2 , ..., x
(i)
T
}
is a sequence
of inputs and y(i) =
{
y(i)1 , y
(i)
2 , ..., y
(i)
T
}
is a se-
quence of the desired labels. We will estimate the
parameters by penalized maximum likelihood, op-
timizing the function:
l(?) =
N
?
i=1
log p(y(i)|x(i)) (3)
After substituting the CRF model (2) in the like-
lihood (3), we get the following expression:
l(?) =
N
?
i=1
T
?
t=1
K
?
k=1
?kfk(y(i)t?1, y
(i)
t ,x(i))
?
N
?
i=1
log Z(x(i))
The function l(?) cannot be maximized in closed
form, so numerical optimization is used. The par-
tial derivates are:
?l(?)
??k =
N
?
i=1
T
?
t=1
fk(y(i)t , y
(i)
t?1,x(i))
?
N
?
i=1
T
?
t=1
?
y,y?
fk(y?, y,x(i)) p(y?, y|x(i))
(4)
Using these derivates, we can iteratively adjust
the parameters ? (with Limited-Memory BFGS
(Byrd et al, 1994)) until l(?) has reached an opti-
mum. During each iteration we have to calculate
p(y?, y|x(i)). This can be done, as for the Hid-
den Markov Model, using the forward-backward
algorithm (Baum and Petrie, 1966; Forney, 1996).
This algorithm has a computational complexity of
O(TM2) (where T is the length of the sequence
and M the number of the labels). We have to exe-
cute the forward-backward algorithm once for ev-
ery training instance during every iteration. The
total cost of training a linear-chained CRFs is thus:
O(TM2NG)
where N is the number of training examples and G
the number of iterations. We?ve experienced that
this complexity is an important delimiting factor
when learning a big collection of labels. Employ-
ing CRFs to learn the 95076 WordNet synsets with
20133 training examples was not feasible on cur-
rent hardware. In the next section we?ll describe
the method we?ve implemented to drastically re-
duce this complexity.
5 Reducing complexity
In this section we?ll see how we create groups of
features for every label that enable an important
reduction in complexity of both labeling and train-
ing. We?ll first discuss how these groups of fea-
tures are created (section 5.1) and then how both
labeling (section 5.2) and training (section 5.3) are
performed using these groups.
35
Figure 3: Fragment of the tree used for labeling
5.1 Hierarchical feature selection
To reduce the complexity of CRFs, we assign a
selection of features to every node in the hierar-
chical tree. As discussed in section 2 WordNet de-
fines a relation between synsets which organises
the synsets in a tree. In its current form this tree
does not meet our needs: we need a tree where
every label used for labeling corresponds to ex-
actly one leaf-node, and no label corresponds to
a non-leaf node. We therefor modify the existing
tree. We create a new top node (?top?) and add the
original tree as defined by WordNet as a subtree to
this top-node. We add leaf-nodes corresponding
to the labels ?NONE?, ?ADJ?, ?ADV?, ?VERB?
to the top-node and for the other labels (the noun
synsets) we add a leaf-node to the node represent-
ing the corresponding synset. For example, we
add a node corresponding to the label ?ENTITY?
to the node ?entity?. Fig. 3 pictures a fraction of
this tree. Nodes corresponding to a label have an
uppercase name, nodes not corresponding to a la-
bel have a lowercase name.
We use v to denote nodes of the tree. We call
the top concept vtop and the concept v+ the parent
of v, which is the parent of v?. We call Av the
collection of ancestors of a concept v, including v
itself.
We will now show how we transform a regular
CRF in a CRF that uses hierarchical feature selec-
tion. We first notice that we can rewrite eq. 2 as
p(y|x) = 1Z(x)
T
?
t=1
G(yt?1, yt,x)
with G(yt?1, yt,x) = exp(
K
?
k=1
?kfk(yt?1, yt,x))
We rewrite this equation because it will enable
us to reduce the complexity of CRFs and it has
the property that p(yt|yt?1,x) ? G(yt?1, yt,x)
which we will use in section 5.3.
We now define a collection of features Fv for
every node v. If v is leaf-node, we define Fv as the
collection of features fk(yt?1, yt,x) for which it is
possible to find a node vt?1 and input x for which
fk(vt?1, v,x) 6= 0. If v is a non-leaf node, we de-
fine Fv as the collection of features fk(yt?1, yt,x)
(1) which are elements of Fv? for every child node
v? of v and (2) for every v?1 and v?2 , children of
v, it is valid that for every previous label vt?1 and
input x fk(vt?1, v?1 ,x) =fk(vt?1, v?2 ,x).
Informally, Fv is the collection of features
which are useful to evaluate for a certain node. For
the leaf-nodes, this is the collection of features that
can possibly return a non-zero value. For non-leaf
nodes, it?s useful to evaluate features belonging to
Fv when they have the same value for all the de-
scendants of that node (which we can put to good
use, see further).
We define F ?v = Fv\Fv+ where v+ is the parent
of label v. For the top node vtop we define F ?vtop =
Fvtop . We also set
G?(yt?1, yt,x) = exp
?
?
?
?
fk?F ?yt
?kfk(yt?1, yt,x)
?
?
?
We?ve now organised the collection of features in
such a way that we can use the hierarchical rela-
tions defined by WordNet when determining the
probability of a certain labeling y. We first see
that
G(yt?1, yt,x) = exp
?
?
?
?
fk?Fyt
?kfk(yt?1, yt,x)
?
?
?
= G(yt?1, y+t , x)G?(yt?1, yt, x)
= ...
=
?
v?Ayt
G?(yt?1, v, x)
we can now determine the probability of a labeling
y, given input x
p(y|x) = 1Z(x)
T
?
t=1
?
v?Ayt
G?(yt?1, v,x) (5)
This formula has exactly the same result as eq. 2.
Because we assigned a collection of features to ev-
ery node, we can discard parts of the search space
when searching for possible labelings, obtaining
an important reduction in complexity. We elab-
orate this idea in the following sections for both
labeling and training.
36
5.2 Labeling
The standard method to label a sentence with
CRFs is by using the Viterbi algorithm (Forney,
1973; Viterbi, 1967) which has a computational
complexity of O(TM2). The basic idea to reduce
this computational complexity is to select the best
labeling in a number of iterations. In the first itera-
tion, we label every word in a sentence with a label
chosen from the top-level labels. After choosing
the best labeling, we refine our choice (choose a
child label of the previous chosen label) in subse-
quent iterations until we arrive at a synset which
has no children. In every iteration we only have
to choose from a very small number of labels, thus
breaking down the problem of selecting the correct
label from a large number of labels in a number of
smaller problems.
Formally, when labeling a sentence we find the
label sequence y such that y has the maximum
probability of all labelings. We will estimate the
best labeling in an iterative way: we start with
the best labeling ytop?1 = {ytop?11 , ..., y
top?1
T }
choosing only from the children ytop?1t of the top
node. The probability of this labeling ytop?1 is
p(ytop?1|x) = 1Z ?(x)
T
?
t=1
G?(yt?1, ytop?1t ,x)
where Z ?(x) is an appropriate normalizing con-
stant. We now select a labeling ytop?2 so that on
every position t node ytop?2t is a child of y
top?1
t .
The probabilty of this labeling is (following eq. 5)
p(ytop?2|x) = 1Z ?(x)
T
?
t=1
?
v?A
ytop?2t
G?(yt?1, v,x)
After selecting a labeling ytop?2 with maximum
probability, we proceed by selecting a labeling
ytop?3 with maximum probability etc.. We pro-
ceed using this method until we reach a labeling
in which every yt is a node which has no children
and return this labeling as the final labeling.
The assumption we make here is that if a node
v is selected at position t of the most probable la-
beling ytop?s the children v? have a larger prob-
ability of being selected at position t in the most
probable labeling ytop?s?1. We reduce the num-
ber of labels we take into consideration by stating
that for every concept v for which v 6= ytop?st , we
set G?(yt?1, v?t ,x) = 0 for every child v? of v.
This reduces the space of possible labelings dras-
tically, reducing the computational complexity of
Figure 4: Nodes that need to be taken into account
during the forward-backward algorithm
the Viterbi algorithm. If q is the average number
of children of a concept, the depth of the tree is
logq(M). On every level we have to execute the
Viterbi algorithm for q labels, thus resulting in a
total complexity of
O(T logq(M)q2) (6)
5.3 Training
We will now discuss how we reduce the compu-
tational complexity of training. As explained in
section 4 we have to estimate the parameters ?k
that optimize the function l(?). We will show here
how we can reduce the computational complex-
ity of the calculation of the partial derivates ?l(?)??k(eq. 4). The predominant factor with regard to
the computational complexity in the evaluation of
this equation is the calculation of p(yt?1, y|x(i)).
Recall we do this with the forward-backward al-
gorithm, which has a computational complexity
of O(TM2). We reduce the number of labels to
improve performance. We will do this by mak-
ing the same assumption as in the previous sec-
tion: for every concept v at level s, for which
v 6= ytop?st , we set G?(yt?1, v?t ,x) = 0 for
every child v? of v. Since (as noted in sect.
5.2) p(vt|yt?1,x) ? G(yt?1, vt,x), this has the
consequence that p(vt|yt?1,x) = 0 and that
p(vt, yt?1|x) = 0. Fig. 4 gives a graphical repre-
sentation of this reduction of the search space. The
correct label here is ?LABEL1? , the grey nodes
have a non-zero p(vt, yt?1|x) and the white nodes
have a zero p(vt, yt?1|x).
In the forward backward algorithm we only
have to account every node v that has a non-zero
p(v, yt?1|x). As can be easily seen from fig. 4,
the number of nodes is qlogqM , where q is the
average number of children of a concept. The to-
tal complexity of running the forward-backward
algorithm is O(T (q logqM)2). Since we have to
run this algorithm once for every gradient compu-
37
Figure 5: Time needed for one training cycle
tation for every training instance we find the total
training cost
O(T (q logqM)2NG) (7)
6 Implementation
To implement the described method we need two
components: an interface to the WordNet database
and an implementation of CRFs using a hierar-
chical model. JWordNet is a Java interface to
WordNet developed by Oliver Steele (which can
be found on http://jwn.sourceforge.
net/). We used this interface to extract the Word-
Net hierarchy.
An implementation of CRFs using the hierar-
chical model was obtained by adapting the Mallet2
package. The Mallet package (McCallum, 2002)
is an integrated collection of Java code useful for
statistical natural language processing, document
classification, clustering, and information extrac-
tion. It also offers an efficient implementation of
CRFs. We?ve adapted this implementation so it
creates hierarchical selections of features which
are then used for training and labeling.
We used the Semcor corpus (Fellbaum et al,
1998; Landes et al, 1998) for training. This cor-
pus, which was created by the Princeton Univer-
sity, is a subset of the English Brown corpus con-
taining almost 700,000 words. Every sentence in
the corpus is noun phrase chunked. The chunks
are tagged by POS and both noun and verb phrases
are tagged with their WordNet sense. Since we do
not want to learn a classification for verb synsets,
we replace the tags of the verbs with one tag
?VERB?.
2http://mallet.cs.umass.edu/
Figure 6: Time needed for labeling
7 Results
The major goal of this paper was to build a clas-
sifier that could learn all the WordNet synsets in a
reasonable amount of time. We will first discuss
the improvement in time needed for training and
labeling and then discuss accuracy.
We want to test the influence of the number of
labels on the time needed for training. Therefor,
we created different training sets, all of which had
the same input (246 sentences tagged with POS la-
bels), but a different number of labels. The first
training set only had 5 labels (?ADJ?, ?ADV?,
?VERB?, ?entity? and ?NONE?). The second had
the same labels except we replaced the label ?en-
tity? with either ?physical entity?, ?abstract entity?
or ?thing?. We continued this procedure, replac-
ing parent nouns labels with their children (i.e.
hyponyms) for subsequent training sets. We then
trained both a CRF using a hierarchical feature se-
lection and a standard CRF on these training sets.
Fig. 5 shows the time needed for one iteration
of training with different numbers of labels. We
can see how the time needed for training slowly
increases for the CRF using hierarchical feature
selection but increases fast when using a standard
CRF. This is conform to eq. 7.
Fig. 6 shows the average time needed for la-
beling a sentence. Here again the time increases
slowly for a CRF using hierarchical feature selec-
tion, but increases fast for a standard CRF, con-
form to eq. 6.
Finally, fig 7 shows the error rate (on the train-
ing data) after each training cycle. We see that a
standard CRF and a CRF using hierarchical fea-
ture selection perform comparable. Note that fig
7 gives the error rate on the training data but this
38
can differ considerable from the error rate on un-
seen data.
After these tests on a small section of the Sem-
cor corpus, we trained a CRF using hierarchi-
cal feature selection on 7/8 of the full corpus.
We trained for 23 iterations, which took approx-
imately 102 hours. Testing the model on the re-
maining 1/8 of the corpus resulted in an accuracy
of 77.82%. As reported in (McCarthy et al, 2004),
a baseline approach that ignors context but simply
assigns the most likely sense to a given word ob-
tains a accuracy of 67%. We did not have the pos-
sibility to compare the accuracy of this model with
a standard CRF, since as already stated, training
such a CRF takes impractically long, but we can
compare our systems with existing WSD-systems.
Mihalcea and Moldovan (Mihalcea and Moldovan,
1999) use the semantic density between words to
determine the word sense. They achieve an ac-
curacy of 86.5% (testing on the first two tagged
files of the Semcor corpus). Wilks and Stevenson
(Wilks and Stevenson, 1998) use a combination
of knowledge sources and achieve an accuracy of
92%3. Note that both these methods use additional
knowledge apart from the WordNet hierarchy.
The sentences in the training and testing sets
were already (perfectly) POS-tagged and noun
chunked, and that in a real-life situation addi-
tional preprocessing by a POS-tagger (such as the
LT-POS-tagger4) and noun chunker (such as de-
scribed in (Ramshaw and Marcus, 1995)) which
will introduce additional errors.
8 Future work
In this section we?ll discuss some of the work we
plan to do in the future. First of all we wish to
evaluate our algorithm on standard test sets, such
as the data of the Senseval conference5 , which
tests performance on word sense disambiguation,
and the data of the CoNLL 2003 shared task6, on
named entity recognition.
An important weakness of our algorithm is the
fact that, to label a sentence, we have to traverse
the hierarchy tree and choose the correct synsets
at every level. An error at a certain level can not
be recovered. Therefor, we would like to perform
3This method was tested on the Semcore corpus, but use
the word senses of the Longman Dictionary of Contemporary
English
4http://www.ltg.ed.ac.uk/software/
5http://www.senseval.org/
6http://www.cnts.ua.ac.be/conll2003/
Figure 7: Error rate during training
some a of beam-search (Bisiani, 1992), keeping
a number of best labelings at every level. We
strongly suspect this will have a positive impact
on the accuracy of our algorithm.
As already mentioned, this work is carried out
during the CLASS project. In the second phase
of this project we will discover classes and at-
tributes of entities in texts. To accomplish this
we will not only need to label nouns with their
synset, but we also need to label verbs, adjec-
tives and adverbs. This can become problem-
atic as WordNet has no hypernym/hyponym rela-
tion (or equivalent) for the synsets of adjectives
and adverbs. WordNet has an equivalent relation
for verbs (hypernym/troponym), but this structures
the verb synsets in a big number of loosely struc-
tured trees, which is less suitable for the described
method. VerbNet (Kipper et al, 2000) seems a
more promising resource to use when classify-
ing verbs, and we will also investigate the use
of other lexical databases, such as ThoughtTrea-
sure (Mueller, 1998), Cyc (Lenat, 1995), Open-
mind Commonsense (Stork, 1999) and FrameNet
(Baker et al, 1998).
Acknowledgments
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978).
References
Eneko Agirre and German Rigau. 1996. Word sense
disambiguation using conceptual density. In Pro-
ceedings of the 16th International Conference on
39
Computational Linguistics (Coling?96), pages 16?
22, Copenhagen, Denmark.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley Framenet project. In Proceedings of the
COLING-ACL.
L. E. Baum and T. Petrie. 1966. Statistical in-
ference for probabilistic functions of finite state
markov chains. Annals of Mathematical Statistics,,
37:1554?1563.
R. Bisiani. 1992. Beam search. In S. C. Shapiro,
editor, Encyclopedia of Artificial Intelligence, New
York. Wiley-Interscience.
Richard H. Byrd, Jorge Nocedal, and Robert B. Schn-
abel. 1994. Representations of quasi-newton matri-
ces and their use in limited memory methods. Math.
Program., 63(2):129?156.
C. Fellbaum, J. Grabowski, and S. Landes. 1998. Per-
formance and confidence in a semantic annotation
task. In C. Fellbaum, editor, WordNet: An Elec-
tronic Lexical Database. The MIT Press.
G. D. Forney. 1973. The viterbi algorithm. In Pro-
ceeding of the IEEE, pages 268 ? 278.
G. D. Forney. 1996. The forward-backward algo-
rithm. In Proceedings of the 34th Allerton Confer-
ence on Communications, Control and Computing,
pages 432?446.
Brian Hayes. 1999. The web of words. American
Scientist, 87(2):108?112, March-April.
Michael I. Jordan, editor. 1999. Learning in Graphical
Models. The MIT Press, Cambridge.
K. Kipper, H.T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. Proceedings
of the Seventh National Conference on Artificial In-
telligence (AAAI-2000).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning.
S. Landes, C. Leacock, and R.I. Tengi. 1998. Build-
ing semantic concordances. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. The MIT
Press.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11):32?38.
Wei Li and Andrew McCallum. 2003. Rapid develop-
ment of hindi named entity recognition using con-
ditional random fields and feature induction. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 2(3):290?294.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced lex-
icons. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of CoNLL-2003, pages 188?191.
Edmonton, Canada.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of the
Nineteenth Conference on Uncertainty in Artificial
Intelligence.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Using automatically acquired predominant
senses for word sense disambiguation. In Proceed-
ings of the ACL SENSEVAL-3 workshop, pages 151?
154, Barcelona, Spain.
R. Mihalcea and D.I. Moldovan. 1999. A method
for word sense disambiguation of unrestricted text.
In Proceedings of the 37th conference on Associa-
tion for Computational Linguistics, pages 152?158.
Association for Computational Linguistics Morris-
town, NJ, USA.
Erik T. Mueller. 1998. Natural language processing
with ThoughtTreasure. Signiform, New York.
F. Peng and A. McCallum. 2004. Accurate infor-
mation extraction from research papers using con-
ditional random fields. In Proceedings of Human
Language Technology Conference and North Amer-
ican Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 329?336.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceed-
ings of the Third ACL Workshop on Very Large Cor-
pora, pages 82?94. Cambridge MA, USA.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of Human
Language Technology, HLT-NAACL.
D. Stork. 1999. The openmind initiative. IEEE Intelli-
gent Systems & their applications, 14(3):19?20.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimal decoding algo-
rithm. IEEE Trans. Informat. Theory, 13:260?269.
Hanna M. Wallach. 2004. Conditional random fields:
An introduction. Technical Report MS-CIS-04-21.,
University of Pennsylvania CIS.
Y. Wilks and M. Stevenson. 1998. Word sense disam-
biguation using optimised combinations of knowl-
edge sources. Proceedings of COLING/ACL, 98.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196.
40
