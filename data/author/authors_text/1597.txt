Cascading Use of Soft and Hard Matching Pattern Rules for Weakly 
Supervised Information Extraction 
Jing Xiao 
School of Computing, 
National University of 
Singapore, 117543 
xiaojing@comp.nus.edu.sg 
Tat-Seng Chua 
School of Computing, 
National University of 
Singapore, 117543 
chuats@comp.nus.edu.sg 
Hang Cui 
School of Computing, 
National University of 
Singapore, 117543 
cuihang@comp.nus.edu.sg 
 
Abstract 
Current rule induction techniques based on hard 
matching (i.e., strict slot-by-slot matching) tend to 
fare poorly in extracting information from natural 
language texts, which often exhibit great 
variations. The reason is that hard matching 
techniques result in relatively high precision but 
low recall. To tackle this problem, we take 
advantage of the newly proposed soft pattern rules 
which offer high recall through the use of 
probabilistic matching. We propose a 
bootstrapping framework in which soft and hard 
matching pattern rules are combined in a cascading 
manner to realize a weakly supervised rule 
induction scheme. The system starts with a small 
set of hand-tagged instances. At each iteration, we 
first generate soft pattern rules and utilize them to 
tag new training instances automatically. We then 
apply hard pattern rule induction on the overall 
tagged data to generate more precise rules, which 
are used to tag the data again. The process can be 
repeated until satisfactory results are obtained. Our 
experimental results show that our bootstrapping 
scheme with two cascaded learners approaches the 
performance of a fully supervised information 
extraction system while using much fewer hand-
tagged instances. 
1 Introduction 
Information Extraction (IE) aims to extract specific 
information items of interest from free or semi-
structured texts, and pattern rule induction is one 
of the most common techniques for IE tasks 
(Muslea, 1999). There has been much work in 
learning extraction pattern rules from tagged data, 
e.g., AutoSlog-TS (Riloff, 1996), WHISK 
(Soderland, 1999) and LP2 (Ciravegna, 2001). In a 
typical IE system, generalized pattern rules are 
usually represented as regular expressions and 
matched against test instances through exact 
matching for each slot, which we call hard 
matching. Utilizing hard matching pattern rules 
could obtain precise results from test instances. 
However, the approach is problematic in dealing 
with natural language text, such as news articles, 
which often exhibits great variations in both lexical 
and syntactic constructions. For instance, in the 
terrorism domain, given a common rule ?<victim> 
be kidnapped by ??, hard matching pattern rules 
cannot pick up the instance ?<victim> , kidnapped 
by ?? due to the mismatch in only one token. 
Such hard matching techniques often result in low 
recall. To achieve flexibility in pattern matching 
for natural language texts, soft matching pattern 
rules have been proposed for question answering 
(Cui, et al, 2004). Soft pattern rules match test 
instances using a probabilistic model to better 
accommodate variations in expressions. However, 
differing from the question answering problem, the 
IE task needs to precisely locate the boundaries of 
the extracted slots. As such, soft pattern rules may 
not meet the precision requirement of the task. 
   In this paper, we aim to minimize the number of 
hand-tagged training instances needed to start the 
learning process by adopting a bootstrapping 
strategy such as that proposed in Riloff and Jones 
(1999). In contrast to the existing work, we 
propose a weakly supervised IE framework which 
takes advantages of both soft and hard matching 
pattern rules in both the training and test phases. 
Starting with only a small set of hand-tagged 
training instances, we first generate a set of soft 
pattern rules and utilize them to tag more training 
instances. Next, we apply a hard matching pattern 
rule induction algorithm, GRID (Xiao, et al, 
2003), over both manually and automatically 
tagged instances to generalize precise hard-
matching rules. These hard pattern rules are 
utilized to tag training instances for soft pattern 
rule generation in the next iteration. The process 
runs iteratively till the termination criteria are met. 
At the end of the training process, we obtain two 
sets of pattern rules, namely the hard and soft 
pattern rules. During the test phase, both sets of 
pattern rules are used in a cascaded way, with hard 
pattern rules followed by soft pattern rules, to 
extract target slots from new documents. We have 
conducted two experiments on both semi-
structured and free texts to demonstrate the 
effectiveness of our method. The experimental 
results show that the bootstrapping scheme with 
two cascaded pattern rule learners could achieve a  
performance close to that obtained by fully 
supervised learning while using only 5~10% of the 
hand-tagged data.  
   The main contribution of our work is in 
incorporating soft matching pattern rules in the 
bootstrapping framework. Rooted in instance-
based learning, soft pattern rules are more 
appropriate in dealing with sparse data (Cui, et al, 
2004), and thus can be learned from a relatively 
small number of training instances to start the 
bootstrapping process. Moreover, in test phase, 
soft pattern rules are expected to cover more 
unseen instances, which are likely to be missed by 
hard-matching rules, with its flexible matching 
mechanism. 
   The rest of the paper is organized as follows. 
Section 2 presents the design of our system. 
Section 3 describes the details of data preparation, 
soft pattern matching, hard pattern rule induction 
and the application of the two pattern rules on new 
test instances. Section 4 presents the experimental 
evaluation. We review other work in Section 5 and 
conclude the paper in Section 6. 
2 System Design 
Figure 1 shows the overall system architecture of 
our IE system. The training phase of the system is 
carried out as follows: 
(a) We take a small set of hand-tagged instances 
(seed instances) provided by the user. 
(b) We generate soft pattern rules using the seed 
instances, and denote the soft pattern rules as SPi. 
(c) We apply the learned soft pattern rules (SPi) to 
automatically tag unannotated data. We employ a 
simple cut-off strategy that keeps only the highly-
ranked instances by the soft pattern rules. 
(d) We generate hard pattern rules using GRID 
over the automatically tagged instances and seed 
instances. The resulting hard pattern rules are 
denoted as HPi.  
(e) If the termination condition is satisfied, the 
process ends with a set of learned soft and hard 
pattern rules. Otherwise, the hard pattern rules HPi 
are used to tag the training data again.  We start a 
new round of training from Step (b) using the 
newly tagged training instances and seed instances. 
   In the test phase, we apply both the hard and soft 
pattern rules to match against test instances. 
Specifically, soft matching pattern rules would 
assign a probabilistic score to an instance that is 
not matched by any of the hard matching pattern 
rules. Only those fields that are matched by the 
hard pattern rules or have high scores in soft 
pattern matching will be extracted. 
 
 
 
  Figure 1: Architecture of our IE system 
 
3 Soft and Hard Pattern Rule Learning 
3.1 Data Preparation 
Before pattern rule learning commences, we pre-
process the training and test instances by using a 
natural language chunker 1  to perform part-of-
speech (PoS) tagging and chunking. We also use a 
rule-based named entity tagger (Chua and Liu, 
2002) to capture semantic entities. Given a tagged 
instance, we consider the left and right k chunks 
around the tagged slot as the context:  
<c-k>?<c-2><c-1>tagged_slot<c+1><c+2>?<c+k>       
Here <ci> {i=-k to +k} represents the contextual 
chunks (or slots) of the tagged slot, where k is the 
number of contextual slots considered. <ci> can be 
of various feature types, namely words, 
punctuations, chunking tags like verb and noun 
phrases, or semantic classes. We perform selective 
substitution to generalize the specific terms in each 
slot so as to make the learned pattern rules general 
enough to be applied to other instances. Table 1 
shows the substitution heuristics employed in our 
system with examples.  
(1) 
   Figure 2 gives five examples of original training 
instances for ?starting time? in the seminar 
announcement domain. We substitute the more 
                                                     
1  We use NLProcessor, a commercial parser from 
Infogistics Ltd. http://www.infogistics.com/. 
general syntactic or semantic classes for the lexical 
tokens according to the heuristics in Table 1. 
  
Tokens Substitution Examples 
9 types of 
named 
entities 
NP_Person, 
NP_Location, 
NP_Organization, 
NP_Date,  
NP_Day, 
NP_Time, 
NP_Percentage, 
NP_Money, 
NP_Number. 
?Friday??NP_Day 
?Feb.27??NP_Date 
Noun 
Phrase NP_HeadNoun 
?the seminar? 
?NP_seminar 
Verb Phrase 
(passive or 
active) 
VPpas_RootVerb, 
VPact_RootVerb 
?will speak? 
?VPact_speak, 
?will be held? 
?VPpas_hold 
Preposition 
Phrase 
PP 
?in civilian clothes? 
? PP 
Adjectival 
and 
adverbial 
modifiers 
To be deleted  
All other 
words and 
punctuations 
No substitution ?Time?, ?at?, ?by?, etc. are unchanged. 
Table 1: Substitution heuristics 
 
 
 
 
 
 
 
3.2 Soft Matching Pattern Rules 
Soft pattern rules have been successfully applied to 
text mining (Nahm and Mooney, 2001) and 
question answering (Cui, et al, 2004). We employ 
a variation of the soft pattern rules generation and 
matching method presented in Cui, et al (2004). 
We expect soft pattern rules to offer higher 
coverage in matching against a variety of instances 
in both the training and test phases. 
   For each type of tagged slot (Slot0) such as stime 
in Figure 2, we accumulate all the tagged instances 
and align them according to the positions of Slot0. 
As a result, we obtain a virtual vector Pa 
representing the contextual soft pattern rule as: 
<Slot-k, ? , Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk: 
Pa>                                                                      (2) 
where Sloti is a vector of tokens occurring in that 
slot with their probabilities of occurrence: 
<(tokeni1, weighti1), (tokeni2, weighti2) ?.(tokenim, 
weightim): Sloti>                                                   (3) 
   Here, tokenij denotes any word, punctuation, 
syntactic or semantic tag contained in Sloti, and 
weightij gives the proportion of occurrences of the 
jth token to the ith slot. Figure 3 shows the 
generated soft pattern rules for the examples given 
in Figure 2. 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1) Training instances: 
Time : <stime> NP_Time </stime> 
VPact_be at <stime> NP_Time </stime>  
NP_Day , NP_Date <stime> NP_Time </stime> - NP_Time 
VPact_be at <stime> NP_Time </stime> , NP_Day , NP_Date 
Time : <stime> NP_Time </stime> - NP_Time 
 
(2) Soft pattern rules based on the instances: 
?? <Slot-2>          <Slot-1>            <Slot0>           <Slot1> ?...
 Time 0.4 
VPact_be 0.4 
, 0.2 
 : 0.4 
at 0.4 
NP_Date  0.2 
 
NP_Time  1 - 0.67 
, 0.33 
 
Figure 3: An excerpt of soft pattern rules 
What results from the generalization process is a 
virtual vector Pa representing the soft pattern rule. 
The soft pattern vector Pa is then used to compute 
the degree of match for the unseen instances. The 
unseen instances are first pre-processed with the 
identical procedures as outlined in Section 3.1. 
Using the same window size k, the token fragment 
S surrounding the potential slot is derived: 
(1) Original instances for slot <stime>: 
Time : <stime> 2:30 PM </stime> 
? will be at <stime> 3 pm </stime> ? 
?Friday, February 17 <stime> 12:00pm </stime> - 1:00pm
    ? will be at <stime> 4pm </stime> , Monday, Feb. 27 ? 
Time: <stime> 12:00 PM </stime> - 1:30 PM 
(2) Substituted instances: 
    Time : <stime> NP_Time </stime> 
    VPact_be at <stime> NP_Time </stime>  
    NP_Day , NP_Date <stime> NP_Time </stime> - NP_Time 
    VPact_be at <stime> NP_Time </stime> , NP_Day , NP_Date 
    Time : <stime> NP_Time </stime> - NP_Time 
<token-k,?, token-2, token-1, Potential_Slot, token1, 
token2, ?, tokenk: S>                                           (4) 
   The degree of match for the unseen instance 
against the soft pattern rules is measured by the 
similarity between the vector S and the virtual soft 
pattern vector Pa. In particular, the match degree is 
the combination of the individual slot content 
similarities and the fidelity degree of slot 
sequences measured by a bi-gram model (Cui, et 
al., 2004).  
Figure 2: Illustration of generalizing instances 
   When applying the soft pattern rules to 
automatically tag training instances, for each 
potential slot, we assign a target tag whose soft 
pattern rule gives the highest score beyond a pre-
defined threshold. 
3.3 Hard Pattern Rule Induction 
We employ a pattern rule induction algorithm 
called GRID (Xiao, et al, 2003) to generalize the 
hard pattern rules over all instances hand-tagged 
by users and automatically annotated by soft 
pattern rules. GRID is a supervised covering 
algorithm. It uses chunks as contextual slots and 
considers a context size of k slots around the 
tagged item as definition in Equation (1).   
   Given the cluster of training instances for a 
specific slot type, GRID aligns all the instances 
according to the central slot (Slot0) as is done in 
soft pattern rules. For each context slot, we store 
all possible representations of slot units as listed in 
Table 1 at the levels of lexical, syntactic and 
semantic simultaneously. Thus, we obtain a global 
context feature representation for the whole 
training corpus as shown in Figure 4. GRID 
records the occurrences of the common slot 
features at a specific position as eij (i = -k, ? ,  -1, 
0, 1, ?, k; jth feature for Sloti).  
 
inst.1: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk 
inst.2: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk 
.         .      ?      .         .          .         .        .      ?    . 
.         .      ?      .         .          .         .        .      ?    . 
.         .      ?      .         .          .         .        .      ?    . 
inst.h: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk 
 
 
 
 
   GRID generates a pattern rule rk(f) by adding slot 
features into the feature set f. The quality of rk(f) is 
determined not only by its coverage in the positive 
training set but also by the number of instances in 
the negative set that it covers which would be 
regarded as errors. We define the remaining 
instances which are not annotated by human and 
soft pattern rules as negative instances.  
   We use a modified Laplacian expected error 
(Soderland, 1999) to define the quality of the rule 
as follows: 
17.0
1
))((
21 +?++
+=
kkk
k
k ppn
n
frLaplacian  
where pk1 denotes the number of instances covered 
by rule rk(f) in the manually annotated set, and pk2 
denotes the number of instances covered by the 
rule rk(f) in the automatically annotated set. nk is 
the number of negative examples or errors covered 
by the rule. We consider all the manually 
annotated instances as correctly tagged and thus 
we put more weight on them than on the 
automatically annotated data set. 
   Instead of generalizing a rule from a specific 
instance as is done in most existing pattern rule 
induction algorithms, GRID examines the global 
feature distribution on the whole set of training 
examples in order to make better decision on rule 
induction. Each time, GRID selects top w features 
(in terms of the eij values) and selects slot feature fij 
with the minimum Laplacian value of the rule 
(rk(f?fij)) according to Equation (5) to induce 
pattern rules (Xiao, et  al., 2003). 
We use GRID to generate rules that cover all 
seed instances and discard some rules generated 
from the automatically tagged instances whose 
Laplacian value is greater than a preset threshold.  
3.4 Cascading Matching of Hard and Soft 
Pattern Rules 
After we have obtained the set of hard pattern rules 
and the set of soft pattern rules through the 
bootstrapping rule induction process, we apply 
both sets of rules in a cascaded way to assign 
appropriate tag to potential slots in new instances. 
The tag assigned to the given test instance t is 
selected by: 
1) tagg   matched by GRID ruleg; 
2) If not matched by any GRID rule, 
tagi  ?>
?
)|Pr(maxarg i
PaPa
Pat
i
   We apply the high-precision hard pattern rules 
generated by GRID first. In this case, we assign 
tagg to the instance if it matches ruleg. In order to 
increase the coverage of the hard pattern rules, we 
allow up to one shift in the context vectors of new 
test instances when matching the instances against 
the hard pattern rules.  
   For the remaining test instances that are not 
matched by any of the hard pattern rules, we score 
them using the soft pattern rules. A test instance is 
assigned tagi if it has the highest conditional 
probability of having t given the soft pattern rule i 
(represented by vector Pai) which is greater than a 
pre-defined threshold ? among all the soft pattern 
rules.  
4 Evaluation 
To verify the generality and effectiveness of our 
bootstrapping framework, we have conducted two 
experiments on free and semi-structured texts. In 
our supervised IE system using GRID (Xiao, et al, 
2003), we had done some trial experiments to 
examine the effect of varying the different context 
length k, and found the IE performance became 
stable when the context length reached 4. As such, 
we set the context length k to 4 for all subsequent 
experiments.  
4.1 Results on free text corpus 
The first evaluation was conducted on the 
terrorism domain using the MUC-4 free text 
corpus (MUC-4, 1992). We employed the same 
evaluation measures as that in (Riloff, 1996; Xiao, 
et al, 2003). The target extracted slots were 
?perpetrator? (Perp.), ?victim? (Vic.) and ?target? 
(Tar.). We varied the number of the human-
annotated instances from the 772 relevant 
Pos. 
e-kj ? e-2j e-1j e0j e1j e2j ? ekj? ? ? ? 
Figure 4: Global distribution of positive instances
(5) 
documents set (the standard training documents for 
MUC-4 plus TST1 and TST2) used in supervised 
IE learning. The manual annotation was guided by 
the associated answer keys given in the MUC-4 
corpus. During testing, we used the 100 texts 
comprising 25 relevant and 25 irrelevant texts from 
the TST3 test set, and 25 relevant and 25 irrelevant 
texts from the TST4 test set.  
   Following the procedure discussed in Section 2, 
we repeated the automated annotation process 
several times (i ?1 in Figure 1). To examine the 
variation of performance along with the changing 
of the number of iterations, we plotted the average 
F1 measures of the three target slots against the 
iteration number (see Figure 5). We also varied the 
number of manually tagged instances that were 
utilized as seed instances for starting the 
bootstrapping process. As can be seen in Figure 5, 
the results improved as the number of iterations 
increased. The system achieved a steady 
performance when the number of iterations 
reached four. Accordingly in the next experiments, 
we considered the system?s performance based on 
four bootstrapping iterations. 
40
45
50
55
60
1 2 3 4 5 6 7 8 9 10
Iteration
A
ve
ra
g
e 
F
1 
m
ea
su
re
5% manually annotated instances
10% manually annotated instances
20% manually annotated instances
 
        Figure 5: Effect of the number of iterations 
   Table 2 shows the performance of the system on 
the test data in terms of F1-measure (with 
recall/precision value in the brackets) using various 
amounts of manually tagged data after four 
iterations. To demonstrate the effectiveness of the 
combination of hard and soft pattern rules, we also 
ran four iterations using only soft pattern rules (SP) 
and another four with only GRID rules.    
   From Table 2, we can draw the following 
conclusions: 
(a) The cascaded learner by combining SP and 
GRID outperforms the learner SP or GRID alone. 
The soft pattern learner (SP) alone cannot achieve 
good precision while the hard pattern learner 
(GRID) alone cannot achieve high recall with a 
small set of hand-annotated instances. 
   
 Perp. Vic. Tar. 
5%(SP) 36 (42/32) 
45 
(49/42) 
42 
(47/38) 
5%(GRID) 34 (35/33) 
44 
(40/49) 
39 
(36/43) 
5%(SP+GRID) 47 (49/45) 
58 
(59/57) 
50 
(50/50) 
10%(SP) 38 (45/33) 
46 
(51/42) 
45 
(49/42) 
10%(GRID) 37 (39/35) 
46 
(41/52) 
44 
(41/47) 
10%(SP+GRID) 50 (53/47) 
61 
(63/59) 
53 
(52/54) 
20%(SP) 40 (46/35) 
48 
(54/43) 
47 
(50/44) 
20%(GRID) 40 (41/39) 
47 
(44/50) 
47 
(45/49) 
20%(SP+GRID) 51 (52/50) 
62 
(63/61) 
54 
(55/53) 
AutoSlog-TS 38 (53/30) 
48 
(62/39) 
47 
(58/39) 
supervised(GRID) 52 
(48/57) 
62 
(58/67) 
56 
(51/62) 
           
 
Results presented in terms of F1(recall/precision). 
               Table 2: Results on free text domain 
(b) Compared with another weakly supervised IE 
system in the same domain, AutoSlog-TS (Riloff, 
1996), our cascaded learner outperforms it with the 
use of only 5% of the manually tagged instances. 
(c) As the percentage of the hand-annotated 
instances increases from 5% to 20%, the 
performance of the cascaded learner (SP+GRID) 
increases steadily, indicating that the bootstrapping 
process is stable and consistent. 
(d) With 20% of hand-tagged training instances, 
the performance of the cascaded learner 
approaches that of the fully supervised IE tagger. 
When more manually tagged instances (>20%) are 
used, the performance of the cascaded learner 
becomes steady. 
(e) Looking at the instances automatically tagged 
by the soft pattern rules, we found that about 75% 
instances are correctly annotated in the first and 
second iteration. The percentage of correctly 
tagged instances by soft pattern rules increases to 
90% when the bootstrapping process runs for four 
times. The percentage increase verifies that our 
automated annotation can provide relatively 
accurate training instances for later rule induction. 
   Nevertheless, our system missed some cases 
which needed deeper NLP analysis. For example, 
given a test sentence ?THEY ARE THE TOP 
MILITARY AND POLITICAL FIGURES IN 
ALFREDO CRISTIANI'S ADMINISTRATION.?, the 
system could not identify ?ALFREDO CRISTIAN?S 
ADMINISTRATION? as the ?perpetrator?. If we 
could associate the previously found ?perpetrator? 
(maybe located far away) to ?they?, then we might 
be able to infer that the ?ALFREDO CRISTIAN?S 
ADMINISTRATION? is the ?perpetrator? too. 
4.2 Results on semi-structured corpus 
The second experiment was conducted on semi-
structured text documents. We used the CMU 
seminar announcements2 for the evaluation. The IE 
task for this domain is to extract the entities of 
?speaker? (SP), ?location? (LOC), ?starting time? 
(ST), and ?ending time? (ET) from a seminar 
announcement. There were 485 seminar 
announcements. In the supervised IE experiments, 
we made five runs and in each run we used one 
half for training and the other half for testing. 
Similarly, to evaluate our weakly supervised 
learning framework, we did five trials as well. In 
each run, we varied the percentage of manually 
annotated instances for training in the supervised 
experiments. Table 3 shows the performance (the 
average F1 measure and recall/precision for five 
runs) of the system with different percentage of 
manually tagged instances used to start the 
training. We also compare the performances 
between the single learners and the cascaded 
learner. All results are based on four bootstrapping 
iterations. 
 SP LOC ST ET 
5%(SP) 70 (74/66) 
65 
(70/61) 
94 
(95/93) 
90 
(93/88) 
5%(GRID) 68 (65/72) 
61 
(59/64) 
93 
(91/94) 
89 
(86/92) 
5%(SP+GRID) 82 (83/81) 
73 
(74/72) 
98 
(98/98) 
94 
(96/92) 
10%(SP) 72 (75/70) 
68 
(72/64) 
96 
(96/95) 
93 
(94/92) 
10%(GRID) 72 (67/77) 
67 
(63/72) 
95 
(94/96) 
93 
(91/96) 
10%(SP+GRID) 84 (84/83) 
75 
(75/74) 
99 
(99/99) 
95 
(97/94) 
20%(SP) 75  (77/74) 
71 
(75/67) 
97 
(97/97) 
95 
(96/95) 
20%(GRID) 75 (69/82) 
71 
(66/77) 
97 
(95/99) 
95 
(94/96) 
20%(SP+GRID) 85 (85/85) 
76 
(76/75) 
99 
(99/99) 
96 
(97/95) 
supervised 
(GRID) 
86 
(84/88) 
76 
(73/80) 
99 
(99/100) 
96 
(95/97)
Results presented in terms of F1(recall/precision).         
         Table 3: Results on semi-structured data 
     
    From Table 3, we make the following 
observations: 
(a) The cascaded learner with two pattern learners 
significantly outperforms the learner SP or GRID 
alone as in the case of free text corpus. With 10% 
of hand-tagged instances, the cascaded learner 
(SP+GRID) approaches the performance of the 
fully supervised IE tagger. Also the performance of 
the cascaded learner increases steadily when the 
number of hand-tagged instances increases from 
5% to 20%. 
(b) With more hand-annotated instances (>20%), 
the performance of the bootstrapping system with 
the cascading use of SP and GRID becomes stable 
and consistent. 
                                                     
2 http://www.isi.edu/info-agents/RISE/repository.html 
(c) Soft pattern rules tag 90% of the instances 
correctly, as we found out in our random checks. 
   The lower performance of our system on the 
?location? slot is mainly due to the use of a general 
named entity recognizer which is good at 
identifying common locations such as cities, 
mountains etc. In seminar announcements, many 
locations are room numbers such as ?WeH 8220?; 
thus, we missed out some seminar venues. 
5 Related Work 
Many hard pattern rule inductive learning systems 
have been developed for information extraction 
from free texts or semi-structured texts. 
Specifically, AutoSlog-TS (Riloff, 1996) generates 
extraction patterns using annotated text and a set of 
heuristic rules and it eliminates the dependency on 
tagged text and only requires the pre-classified 
texts as input. WHISK (Soderland, 1999) induces 
multi-slot rules from a training corpus top-down. It 
is designed to handle text styles ranging from 
highly structured text to free text. WHISK 
performs rule induction starting from a randomly 
selected seed instance. (LP)2 (Ciravegna, 2001) is a 
covering algorithm for adaptive IE systems that 
induces symbolic rules. In (LP)2, training is 
performed in two steps: first, a set of tagging rules 
is learned to identify the boundaries of slots; next, 
additional rules are induced to correct mistakes in 
the first step of tagging. In contrast to their work, 
GRID utilizes global feature distribution to induce 
pattern rules and uses chunk as the context unit. 
Nahm and Mooney (2001) proposed the learning 
of soft matching rules from texts by combining 
rule-based and instance-based learning. Words in 
each slot are generalized by traditional rule 
induction techniques and test instances are 
matched to the rules by their cosine similarities. 
The learning of soft pattern rules in this paper 
augments the soft matching method advocated by 
Nahm and Mooney (2001) by combining lexical 
tokens alongside syntactic and semantic features 
and adopting a probabilistic framework that 
combines slot content and sequential fidelity in 
computing the degree of pattern match. 
   The bootstrapping scheme using the co-training 
(Blum and Mitchell, 1998) technique has been 
widely explored for IE tasks in recent years. 
Collins and Singer (1999) presented several 
techniques using co-training schemes for Named 
Entity (NE) extraction seeded by a small set of 
manually crafted NE rules. Riloff and Jones (1999) 
presented a multi-level bootstrapping algorithm 
that generates both the semantic lexicon and 
extraction patterns simultaneously. Yangarber 
(2003) proposed a counter-training approach to 
provide natural stopping criteria for unsupervised 
learning. 
   Our framework of combining two pattern 
learners is close to Niu, et al (2003) in which two 
successive learners are used to learn named entities 
classifiers starting from a small set of concept-
based seed words. The bootstrapping procedure is 
implemented as training a decision list and an 
HMM classifier sequentially. The HMM classifier 
uses the training corpus automatically, tagged by 
the first learner, i.e., the decision list learner. Our 
work differs from Niu, et al (2003) in two ways. 
First, we repeat the automatic annotation process 
until it satisfies the stopping criteria. Second, we 
apply different patterns (hard and soft pattern 
rules) in both the training and test phases. 
6 Conclusion 
We have presented a novel bootstrapping 
approach for information extraction by the 
cascading use of soft and hard pattern rules. Our 
framework takes advantages of the high-recall of 
soft pattern rules and the high-precision of hard 
pattern rules. We use soft pattern rules to 
automatically annotate more training instances so 
as to provide a more comprehensive basis for hard 
pattern rule induction. The integration of soft 
pattern matching in the extraction phase also 
provides more target entities from test instances 
that would otherwise be missed by hard pattern 
matching. With much less manual input, the 
proposed bootstrapping system approaches the 
performance obtained by fully supervised learning 
on both semi-structured and free texts corpora. 
7 Acknowledgement 
The authors would like to thank Alexia Leong 
for proofreading this paper. The third author is 
supported by Singapore Millennium Foundation 
Scholarship (ref no. 2003-SMS-0230). 
References  
A. Blum and T. Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-training. 
Proceedings of the 11th Annual Conference on 
Computational Learning Theory (COLT-98), 
pages 92-100. 
T.-S. Chua and J. Liu. 2002. Learning Pattern 
Rules for Chinese Named Entity Extraction. 
Proceedings of the 18th National Conference on 
Artificial Intelligence. (AAAI-02), pages 411-
418. 
F. Ciravegna. 2001. Adaptive Information 
Extraction from Text by Rule Induction and 
Generalisation. Proceedings of the 17th 
International Joint Conference on Artificial 
Intelligence (IJCAI-2001),  pages 1251-1256. 
M. Collins and Y. Singer. 1999. Unsupervised 
Models for Named Entity Classification. 
Proceedings of the 1999 Joint SIGDAT 
Conference on EMNLP and VLC. 
H. Cui, M.-Y. Kan and T.-S. Chua. 2004. 
Unsupervised Learning of Soft Patterns for 
Definitional Question Answering. Proceedings 
of 13th World Wide Web Conference. (WWW-04), 
pages 90-99. 
MUC-4, 1992. Proceedings of the Fourth Message 
Understanding Conference. San Mateo, CA: 
Morgan Kaufmann. 1992. 
I. Muslea. 1999. Extraction Patterns for 
Information Extraction Tasks: A Survey. The 
AAAI-99 Workshop on Machine Learning for 
Information Extraction. 
U. Y. Nahm and R. J. Mooney. 2001. Mining Soft 
Matching Rules from Textual Data. Proceedings 
of the 17th International Joint Conference on 
Artificial Intelligence. (IJCAI-01), pages 979-
986. 
C. Niu, W. Li, J. Ding and R. K. Srihari. 2003. A 
Bootstrapping Approach to Named Entity 
Classification Using Successive Learners. 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics. 
(ACL-03), pages 335-342. 
E. Riloff. 1996. Automatically Generating 
Extraction  Patterns from Untagged Text. 
Proceedings of the 13th National Conference on 
Artificial Intelligence (AAAI-96), pages 1044-
1049. 
E. Riloff and R. Jones, 1999,  Learning 
Dictionaries for Information Extraction by 
Multi-Level Bootstrapping, Proceedings of the 
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99), pages 474-479. 
S. Soderland. 1999. Learning Information 
Extraction Rules for Semi-structured and Free 
Text. Machine Learning, vol.34, pages 233-272. 
J. Xiao, T.-S. Chua and J. Liu. 2003. A Global 
Rule Induction Approach to Information 
Extraction. Proceedings of the 15th IEEE 
International Conference on Tools with Artificial 
Intelligence. (ICTAI-03), pages 530-536. 
R. Yangarber. 2003. Counter-Training in 
Discovery of Semantic Patterns. Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-03), pages 343-
350. 
Extending corpus-based identification of light verb constructions
using a supervised learning framework
Yee Fan Tan, Min-Yen Kan and Hang Cui
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{tanyeefa, kanmy, cuihang}@comp.nus.edu.sg
Abstract
Light verb constructions (LVCs), such as
?make a call? in English, can be said
to be complex predicates in which the
verb plays only a functional role. LVCs
pose challenges for natural language un-
derstanding, as their semantics differ from
usual predicate structures. We extend the
existing corpus-based measures for iden-
tifying LVCs between verb-object pairs
in English, by proposing using new fea-
tures that use mutual information and as-
sess other syntactic properties. Our work
also incorporates both existing and new
LVC features into a machine learning ap-
proach. We experimentally show that us-
ing the proposed framework incorporat-
ing all features outperforms previous work
by 17%. As machine learning techniques
model the trends found in training data,
we believe the proposed LVC detection
framework and statistical features is easily
extendable to other languages.
1 Introduction
Many applications in natural language processing
rely on the relationships between words in a docu-
ment. Verbs play a central role in many such tasks;
for example, the assignment of semantic roles
to noun phrases in a sentence heavily depends
on the verb that link the noun phrases together
(as in ?Pierre Vinken/SUBJ, will join/PRED, the
board/OBJ?).
However, verb processing is difficult because of
many phenomena, such as normalization of ac-
tions, verb particle constructions and light verb
constructions. Applications that process verbs
must handle these cases effectively. We focus on
the identification of light verb constructions (also
known as support verb constructions) in English,
as such constructions play a prominent and pro-
ductive role in many other languages (Butt and
Geuder, 2001; Miyamoto, 2000). Although the
exact definition of a LVC varies in the literature,
we use the following operational definition:
A light verb construction (LVC) is a
verb-complement pair in which the verb
has little lexical meaning (is ?light?) and
much of the semantic content of the con-
struction is obtained from the comple-
ment.
Examples of LVCs in English include ?give a
speech?, ?make good (on)? and ?take (NP) into
account?. In the case in which the complement is a
noun, it is often a deverbal noun and, as such, can
usually be paraphrased using the object?s root verb
form without (much) loss in its meaning (e.g., take
a walk ? walk, make a decision ? decide, give a
speech ? speak).
We propose a corpus-based approach to de-
termine whether a verb-object pair is a LVC.
Note that we limit the scope of LVC detection to
LVCs consisting of verbs with noun complements.
Specifically, we extend previous work done by
others by examining how the local context of the
candidate construction and the corpus-wide fre-
quency of related words to the construction play
an influence on the lightness of the verb.
A second contribution is to integrate our new
features with previously reported ones under a ma-
chine learning framework. This framework op-
timizes the weights for these measures automati-
cally against a training corpus in supervised learn-
ing, and attests to the significant modeling im-
49
provements of our features on our corpus. Our
corpus-based evaluation shows that the combina-
tion of previous work and our new features im-
proves LVC detection significantly over previous
work.
An advantage gained by adopting a machine
learning framework is that it can be easily adapted
to other languages that also exhibit light verbs.
While we perform evaluations on English, light
verbs exist in most other languages. In some of
these languages, such as Persian, most actions are
expressed as LVCs rather than single-word verbs
(Butt, 2003). As such, there is currently a un-
met demand for developing an adaptable frame-
work for LVC detection that applies across lan-
guages. We believe the features proposed in this
paper would also be effective in identifying light
verbs in other languages.
We first review previous corpus-based ap-
proaches to LVC detection in Section 2. In Section
3, we show how we extend the use of mutual infor-
mation and employ context modeling as features
for improved LVC detection. We next describe our
corpus processing and how we compiled our gold
standard judgments used for supervised machine
learning. In Section 4, we evaluate several feature
combinations before concluding the paper.
2 Related Work
With the recent availability of large corpora, statis-
tical methods that leverage syntactic features are a
current trend. This is the case for LVC detection
as well.
Grefenstette and Teufel (1995) considered a
similar task of identifying the most probable light
verb for a given deverbal noun. Their approach fo-
cused on the deverbal noun and occurrences of the
noun?s verbal form, arguing that the deverbal noun
retains much of the verbal characteristics in the
LVCs. To distinguish the LVC from other verb-
object pairs, the deverbal noun must share similar
argument/adjunct structures with its verbal coun-
terpart. Verbs that appear often with these char-
acteristic deverbal noun forms are deemed light
verbs. They approximate the identification of ar-
gument/adjunct structures by using the preposition
head of prepositional phrases that occur after the
verb or object of interest.
Let n be a deverbal noun whose most likely
light verb is to be found. Denote its verbal form by
v?, and let P be the set containing the three most
frequently occurring prepositions that occur after
v?. The verb-object pairs that are not followed by
a preposition in P are filtered out. For any verb
v, let g(v, n) be the count of verb-object pairs v-n
that remain after the filtering step above. Grefen-
stette and Teufel proposed that the light verb for n
be returned by the following equation:
GT95(n) = argmax
v
g(v, n) (1)
Interestingly, Grefenstette and Teufel indicated
that their subsequent experiments suggested that
the filtering step may not be necessary.
Whereas the GT95 measure centers on the de-
verbal object, Dras and Johnson (1996) also con-
sider the verb?s corpus frequency. The use of this
complementary information improves LVC iden-
tification, as it models the inherent bias of some
verbs to be used more often as light verbs than oth-
ers. Let f(v, n) be the count of verb-object pairs
occurring in the corpus, such that v is the verb, n
is a deverbal noun. Then, the most probable light
verb for n is given by:
DJ96(n) = argmax
v
f(v, n)
?
n
f(v, n) (2)
Stevenson et al (2004)?s research examines ev-
idence from constructions featuring determiners.
They focused on expressions of the form v-a-n
and v-det-n, where v is a light verb, n is a de-
verbal noun, a is an indefinite determiner (namely,
?a? or ?an?), and det is any determiner other than
the indefinite. Examples of such constructions are
?give a speech? and ?take a walk?. They employ
mutual information which measures the frequency
of co-occurrences of two variables, corrected for
random agreement. Let I(x, y) be the mutual in-
formation between x and y. Then the following
measure can be used:
SFN04(v, n) = 2? I(v, a-n)? I(v, det-n), (3)
where higher values indicate a higher likelihood of
v-a-n being a light verb construction. Also, they
suggested that the determiner ?the? be excluded
from the development data since it frequently oc-
curred in their data.
Recently, Fazly et al (2005) have proposed a
statistical measure for the detection of LVCs. The
probability that a verb-object pair v-n (where v is a
light verb) is a LVC can be expressed as a product
of three probabilities: (1) probability of the object
50
n occurring in the corpus, (2) the probability that n
is part of any LVC given n, and (3) the probability
of v occurring given n and that v-n is a LVC. Each
of these three probabilities can then be estimated
by the frequency of occurrence in the corpus, us-
ing the assumption that all instances of v?-a-n is a
LVC, where v? is any light verb and a is an indefi-
nite determiner.
To summarize, research in LVC detection
started by developing single measures that utilized
simple frequency counts of verbs and their com-
plements. From this starting point, research has
developed in two different directions: using more
informed measures for word association (specifi-
cally, mutual information) and modeling the con-
text of the verb-complement pair.
Both the GT95 and DJ96 measures suffer from
using frequency counts directly. Verbs that are not
light but occur very frequently (such as ?buy? and
?sell? in the Wall Street Journal) will be marked by
these measures. As such, given a deverbal noun,
they sometimes suggest verbs that are not light.
We hypothesize that substituting MI for frequency
count can alleviate this problem.
The SFN04 metric adds in the context provided
by determiners to augment LVC detection. This
measure may work well for LVCs that are marked
by determiners, but excludes a large portion of
LVCs that are composed without determiners. To
design a robust LVC detector requires integrating
such specific contextual evidence with other gen-
eral evidence.
Building on this, Fazly et al (2005) incorpo-
rate an estimation of the probability that a cer-
tain noun is part of a LVC. However, like SFN04,
LVCs without determiners are excluded.
3 Framework and Features
Previous work has shown that different measures
based on corpus statistics can assist in LVC detec-
tion. However, it is not clear to what degree these
different measures overlap and can be used to re-
inforce each other?s results. We solve this problem
by viewing LVC detection as a supervised clas-
sification problem. Such a framework can inte-
grate the various measures and enable us to test
their combinations in a generic manner. Specifi-
cally, each verb-object pair constitutes an individ-
ual classification instance, which possesses a set
of features f
1
, . . . , fn and is assigned a class label
from the binary classification of {LV C,?LV C}.
In such a machine learning framework, each of the
aforementioned metrics are separate features.
In our work, we have examined three different
sets of features for LVC classification: (1) base,
(2) extended and (3) new features. We start by de-
riving three base features from key LVC detection
measures as described by previous work ? GT95,
DJ96 and SFN04. As suggested in the previous
section, we can make alternate formulations of the
past work, such as to discard a pre-filtering step
(i.e. filtering of constructions that do not include
the top three most frequent prepositions). These
measures make up the extended feature set. The
third set of features are new and have not been
used for LVC identification before. These include
features that further model the influence of context
(e.g. prepositions after the object) in LVC detec-
tion.
3.1 Base Features
These features are based on the original previ-
ous work discussed in Section 2, but have been
adapted to give a numeric score. We use the ini-
tials of the original authors without year of publi-
cation to denote our derived base features.
Recall that the aim of the original GT95 and
DJ96 formulae is to rank the possible support
verbs given a deverbal noun. As each of these for-
mulae contain a function which returns a numeric
score inside the argmaxv, we use these functions
as two of our base features:
GT(v, n) = g(v, n) (4)
DJ(v, n) = f(v, n)
?
n
f(v, n) (5)
The SFN04 measure can be used without modifi-
cation as our third base feature, and it will be re-
ferred to as SFN for the remainder of this paper.
3.2 Extended Features
Since Grefenstette and Teufel indicated that the
filtering step might not be necessary, i.e., f(v, n)
may be used instead of g(v, n), we also have the
following extended feature:
FREQ(v, n) = f(v, n) (6)
In addition, we experiment with the reverse pro-
cess for the DJ feature, i.e., to replace f(v, n) in
the function for DJ with g(v, n), yielding the fol-
lowing extended feature:
DJ-FILTER(v, n) = g(v, n)
?
n
g(v, n) (7)
51
In Grefenstette and Teufel?s experiments, they
used the top three prepositions for filtering. We
further experiment with using all possible prepo-
sitions.
3.3 New Features
In our new feature set, we introduce features that
we feel better model the v and n components as
well as their joint occurrences v-n. We also intro-
duce features that model the v-n pair?s context, in
terms of deverbal counts, derived from our under-
standing of LVCs.
Most of these new features we propose are not
good measures for LVC detection by themselves.
However, the additional evidence that they give
can be combined with the base features to create
a better composite classification system.
Mutual information: We observe that a verb v
and a deverbal noun n are more likely to appear
in verb-object pairs if they can form a LVC. To
capture this evidence, we employ mutual informa-
tion to measure the co-occurrences of a verb and
a noun in verb-object pairs. Formally, the mutual
information between a verb v and a deverbal noun
n is defined as
I(v, n) = log
2
P (v, n)
P (v)P (n) , (8)
where P (v, n) denotes the probability of v and n
constructing verb-object pairs. P (v) is the prob-
ability of occurrence of v and P (n) represents
the probability of occurrence of n. Let f(v, n)
be the frequency of occurrence of the verb-object
pair v-n and N be the number of all verb-object
pairs in the corpus. We can estimate the above
probabilities using their maximum likelihood esti-
mates: P (v, n) = f(v,n)N , P (v) =
P
n f(v,n)
N and
P (n) =
P
v f(v,n)
N .
However, I(v, n) only measures the local in-
formation of co-occurrences between v and n. It
does not capture the global frequency of verb-
object pair v-n, which is demonstrated as effective
by Dras and Johnson (1996). As such, we need
to combine the local mutual information with the
global frequency of the verb-object pair. We thus
create the following feature, where the log func-
tion is used to smooth frequencies:
MI-LOGFREQ = I(v, n)? log
2
f(v, n) (9)
Deverbal counts: Suppose a verb-object pair v-
n is a LVC and the object n should be a dever-
bal noun. We denote v? to be the verbalized form
of n. We thus expect that v-n should express the
same semantic meaning as that of v?. However,
verb-object pairs such as ?have time? and ?have
right? in English scored high by the DJ and MI-
LOGFREQ measures, even though the verbalized
form of their objects, i.e., ?time? and ?right?, do
not express the same meaning as the verb-object
pairs do. This is corroborated by Grefenstette and
Teufel claim that if a verb-object pair v-n is a
LVC, then n should share similar properties with
v?. Based on our empirical analysis on the corpus
using a small subset of LVCs, we believe that:
1. The frequencies of n and v? should not differ
very much, and
2. Both frequencies are high given the fact that
LVCs occur frequently in the text.
The first observation is true in our corpus where
light verb and verbalized forms are freely inter-
changable in contexts. Then, let us denote the fre-
quencies of n and v? to be f(n) and f(v?) respec-
tively. We devise a novel feature based on the hy-
potheses:
min(f(n), f(v?))
max(f(n), f(v?)) ?min(f(n), f(v
?
)) (10)
where the two terms correspond to the above two
hypotheses respectively. A higher score from this
metric indicates a higher likelihood of the com-
pound being a LVC.
Light verb classes: Linguistic studies of light
verbs have indicated that verbs of specific seman-
tic character are much more likely to participate in
LVCs (Wang, 2004; Miyamoto, 2000; Butt, 2003;
Bjerre, 1999). Such characteristics have been
shown to be cross-language and include verbs that
indicate (change of) possession (Danish give, to
give, direction (Chinese guan diao to switch off),
aspect and causation, or are thematically incom-
plete (Japanese suru, to do). As such, it makes
sense to have a list of verbs that are often used
lightly. In our work, we have predefined a light
verb list for our English experiment as exactly
the following seven verbs: ?do?, ?get?, ?give?,
?have?, ?make?, ?put? and ?take?, all of which
have been studied as light verbs in the literature.
We thus define a feature that considers the verb in
the verb-object pair: if the verb is in the prede-
fined light verb list, the feature value is the verb
itself; otherwise, the feature value is another de-
fault value.
52
One may ask whether this feature is necessary,
given the various features used to measure the fre-
quency of the verb. As all of the other metrics are
corpus-based, they rely on the corpus to be a repre-
sentative sample of the source language. Since we
extract the verb-object pairs from the Wall Street
Journal section of the Penn Treebank, terms like
?buy?, ?sell?, ?buy share? and ?sell share? occur
frequently in the corpus that verb-object pairs such
as ?buy share? and ?sell share? are ranked high by
most of the measures. However, ?buy? and ?sell?
are not considered as light verbs. In addition,
the various light verbs have different behaviors.
Despite their lightness, different light verbs com-
bined with the same noun complement often gives
different semantics, and hence affect the lightness
of the verb-object pair. For example, one may say
that ?make copy? is lighter than ?put copy?. Incor-
porating this small amount of linguistic knowledge
into our corpus-based framework can enhance per-
formance.
Other features: In addition to the above fea-
tures, we also used the following features: the de-
terminer before the object, the adjective before the
object, the identity of any preposition immediately
following the object, the length of the noun object
(if a phrase) and the number of words between the
verb and its object. These features did not improve
performance significantly, so we have omitted a
detailed description of these features.
4 Evaluation
In this section, we report the details of our exper-
imental settings and results. First, we show how
we constructed our labeled LVC corpus, used as
the gold standard in both training and testing un-
der cross validation. Second, we describe the eval-
uation setup and discuss the experimental results
obtained based on the labeled data.
4.1 Data Preparation
Some of the features rely on a correct sentence
parse. In order to minimize this source of error,
we employ the Wall Street Journal section in the
Penn Treebank, which has been manually parsed
by linguists. We extract verb-object pairs from the
Penn Treebank corpus and lemmatize them using
WordNet?s morphology module. As a filter, we re-
quire that a pair?s object be a deverbal noun to be
considered as a LVC. Specifically, we use Word-
Net to check whether a noun has a verb as one of
its derivationally-related forms. A total of 24,647
candidate verb-object pairs are extracted, of which
15,707 are unique.
As the resulting dataset is too large for complete
manual annotation given our resources, we sam-
ple the verb-object pairs from the extracted set.
As most verb-object pairs are not LVCs, random
sampling would provide very few positive LVC in-
stances, and thus would adversely affect the train-
ing of the classifier due to sparse data. Our aim in
the sampling is to have balanced numbers of po-
tential positive and negative instances. Based on
the 24,647 verb-object pairs, we count the corpus
frequencies of each verb v and each object n, de-
noted as f(v) and f(n). We also calculate the DJ
score of the verb-object pair DJ(v, n) by counting
the pair frequencies. The data set is divided into
5 bins using f(v) on a linear scale, 5 bins using
f(n) on a linear scale and 4 bins using DJ(v, n)
on a logarithmic scale.1 We cross-multiply these
three factors to generate 5 ? 5 ? 4 = 100 bins.
Finally, we uniformly sampled 2,840 verb-object
pairs from all the bins to construct the data set for
labeling.
4.2 Annotation
As noted by many linguistic studies, the verb in
a LVC is often not completely vacuous, as they
can serve to emphasize the proposition?s aspect,
its argument?s semantics (cf., ? roles) (Miyamoto,
2000), or other function (Butt and Geuder, 2001).
As such, previous computational research had pro-
posed that the ?lightness? of a LVC might be best
modeled as a continuum as opposed to a binary
class (Stevenson et al, 2004). We have thus anno-
tated for two levels of lightness in our annotation
of the verb-object pairs. Since the purpose of the
work reported here is to flag all such constructions,
we have simplified our task to a binary decision,
similar to most other previous corpus-based work.
A website was set up for the annotation task,
so that annotators can participate interactively.
For each selected verb-object pair, a question is
constructed by displaying the sentence where the
verb-object pair is extracted, as well as the verb-
object pair itself. The annotator is then asked
whether the presented verb-object pair is a LVC
given the context of the sentence, and he or she
will choose from the following options: (1) Yes,
1Binning is the process of grouping measured data into
data classes or histogram bins.
53
(2) Not sure, (3) No. The following three sen-
tences illustrate the options.
(1) Yes ? A Compaq Computer Corp.
spokeswoman said that the company
hasn?t made a decision yet, although ?it isn?t
under active consideration.?
(2) Not Sure ? Besides money, criminals have
also used computers to steal secrets and in-
telligence, the newspaper said, but it gave no
more details.
(3) No ? But most companies are too afraid to
take that chance.
The three authors, all natural language process-
ing researchers, took part in the annotation task,
and we asked all three of them to annotate on the
same data. In total, we collected annotations for
741 questions. The average correlation coefficient
between the three annotators is r = 0.654, which
indicates fairly strong agreement between the an-
notators. We constructed the gold standard data
by considering the median of the three annotations
for each question. Two gold standard data sets are
created:
? Strict ? In the strict data set, a verb-object
pair is considered to be a LVC if the median
annotation is 1.
? Lenient ? In the lenient data set, a verb-
object pair is considered to be a LVC if the
median annotation is either 1 or 2.
Each of the strict and lenient data sets have 741
verb-object pairs.
4.3 Experiments
We have two aims for the experiments: (1) to com-
pare between the various base features and the ex-
tended features, and (2) to evaluate the effective-
ness of our new features.
Using the Weka data mining toolkit (Witten
and Frank, 2000), we have run a series of ex-
periments with different machine learning algo-
rithms. However, since our focus of the exper-
iments is to determine which features are useful
and not to evaluate the machine learners, we re-
port the results achieved by the best single clas-
sifier without additional tuning, the random for-
est classifier (Breiman, 2001). Stratified ten-fold
cross-validation is performed. The evaluation cri-
teria used is the F
1
-measure on the LV C class,
which is defined as
F
1
=
2PR
P + R, (11)
where P and R are the precision and recall for the
LV C class respectively.
4.3.1 Base and Extended Features
Regarding the first aim, we make the following
comparisons:
? GT (top 3 prepositions) versus GT (all prepo-
sitions) and FREQ
? DJ versus DJ-FILTER (top 3 prepositions and
all prepositions)
Feature Strict Lenient
GT (3 preps) 0.231 0.163
GT (all preps) 0.272 0.219
FREQ 0.289 0.338
DJ 0.491 0.616
DJ-FILTER (3 preps) 0.433 0.494
DJ-FILTER (all preps) 0.429 0.503
SFN 0.000 0.000
Table 1: F
1
-measures of base features and ex-
tended features.
We first present the results for the base features
and the extended features in Table 1. From these
results, we make the following observations:
? Overall, DJ and DJ-FILTER perform better
than GT and FREQ. This is consistent with
the results by Dras and Johnson (1996).
? The results for both GT/FREQ and DJ show
that filtering using preposition does not im-
pact performance significantly. We believe
that the main reason for this is that the fil-
tering process causes information to be lost.
163 of the 741 verb-object pairs in the corpus
do not have a preposition following the object
and hence cannot be properly classified using
the features with filtering.
? The SFN metric does not appear to work with
our corpus. We suspect that it requires a far
larger corpus than our corpus of 24,647 verb-
object pairs to work. Stevenson et al (2004)
54
have used a corpus whose estimated size is at
least 15.7 billion, the number of hits returned
in a Google search for the query ?the? as of
February 2006. The large corpus requirement
is thus a main weakness of the SFN metric.
4.3.2 New Features
We now evaluate the effectiveness of our class
of new features. Here, we do not report results of
classification using only the new features, because
these features alone are not intended to constitute a
stand-alone measure of the lightness. As such, we
evaluate these new features by adding them on top
of the base features. We first construct a full fea-
ture set by utilizing the base features (GT, DJ and
SFN) and all the new features. We chose not to add
the extended features to the full feature set because
these extended features are not independent to the
base features. Next, to show the effectiveness of
each new feature individually, we remove it from
the full feature set and show the performance of
classifier without it.
Feature(s) Strict Lenient
GT (3 preps) 0.231 0.163
DJ 0.491 0.616
SFN 0.000 0.000
GT (3 preps) + DJ + SFN 0.537 0.676
FULL 0.576 0.689
- MI-LOGFREQ 0.545 0.660
- DEVERBAL 0.565 0.676
- LV-CLASS 0.532 0.640
Table 2: F
1
-measures of the various feature com-
binations for our evaluation.
Table 2 shows the resulting F
1
-measures when
using various sets of features in our experiments.2
We make the following observations:
? The combinations of features outperform the
individual features. We observe that using in-
dividual base features alone can achieve the
highest F
1
-measure of 0.491 on the strict data
set and 0.616 on the lenient data set respec-
tively. When applying the combination of
all base features, the F
1
-measures on both
2For the strict data set, the base feature set has a preci-
sion and recall of 0.674 and 0.446 respectively, while the full
feature set has a precision and recall of 0.642 and 0.523 re-
spectively. For the lenient data set, the base feature set has a
precision and recall of 0.778 and 0.598 respectively, while the
full feature set has a precision and recall of 0.768 and 0.624
respectively.
data sets increased to 0.537 and 0.676 respec-
tively.
Previous work has mainly studied individ-
ual statistics in identifying LVCs while ig-
noring the integration of various statistics.
The results demonstrate that integrating dif-
ferent statistics (i.e. features) boosts the per-
formance of LVC identification. More impor-
tantly, we employ an off-the-shelf classifier
without special parameter tuning. This shows
that generic machine learning methods can be
applied to the problem of LVC detection. It
provides a sound way to integrate various fea-
tures to improve the overall performance.
? Our new features boost the overall perfor-
mance. Applying the newly proposed fea-
tures on top of the base feature set, i.e., us-
ing the full feature set, gives F
1
-measures
of 0.576 and 0.689 respectively (shown in
bold) in our experiments. These yield a sig-
nificant increase (p < 0.1) over using the
base features only. Further, when we remove
each of the new features individually from
the full feature set, we see a corresponding
drop in the F
1
-measures, of 0.011 (deverbal
counts) to 0.044 (light verb classes) for the
strict data set, and 0.013 (deverbal counts)
to 0.049 (light verb classes) for the lenient
data set. It shows that these new features
boost the overall performance of the classi-
fier. We think that these new features are
more task-specific and examine intrinsic fea-
tures of LVCs. As such, integrated with the
statistical base features, these features can be
used to identify LVCs more accurately. It is
worth noting that light verb class is a simple
but important feature, providing the highest
F
1
-measure improvement compared to other
new features. This is in accordance with the
observation that different light verbs have dif-
ferent properties (Stevenson et al, 2004).
5 Conclusions
Multiword expressions (MWEs) are a major obsta-
cle that hinder precise natural language processing
(Sag et al, 2002). As part of MWEs, LVCs remain
least explored in the literature of computational
linguistics. Past work addressed the problem of
automatically detecting LVCs by employing single
statistical measures. In this paper, we experiment
55
with identifying LVCs using a machine learning
framework that integrates the use of various statis-
tics. Moreover, we have extended the existing sta-
tistical measures and established new features to
detect LVCs.
Our experimental results show that the inte-
grated use of different features in a machine learn-
ing framework performs much better than using
any of the features individually. In addition, we
experimentally show that our newly-proposed fea-
tures greatly boost the performance of classifiers
that use base statistical features. Thus, our system
achieves state-of-the-art performance over previ-
ous approaches for identifying LVCs. As such, we
suggest that future work on automatic detection of
LVCs employ a machine learning framework that
combines complementary features, and examine
intrinsic features that characterize the local con-
text of LVCs to achieve better performance.
While we have experimentally showed the ef-
fectiveness of the proposed framework incorporat-
ing existing and new features for LVC detection
on an English corpus, we believe that the features
we have introduced are generic and apply to LVC
detection in other languages. The reason is three-
fold:
1. Mutual information is a generic metric for
measuring co-occurrences of light verbs and
their complements. Such co-occurrences are
often an obvious indicator for determining
light verbs because light verbs are often cou-
pled with a limited set of complements. For
instance, in Chinese, directional verbs, such
as xia (descend) and dao (reach), which are
often used lightly, are often co-located with a
certain class of verbs that are related to peo-
ple?s behaviors.
2. For LVCs with noun complements, most of
the semantic meaning of a LVC is expressed
by the object. This also holds for other lan-
guages, such as Chinese. For example, in
Chinese, zuo xuanze (make a choice) and
zuo jueding (make a decision) has the word
zuo (make) acting as a light verb and xuan-
ze (choice) or jueding (decision) acting as a
deverbal noun (Wang, 2004). Therefore, the
feature of deverbal count should also be ap-
plicable for other languages.
3. It has been observed that in many languages,
light verbs tend to be a set of closed class
verbs. This allows us to use a list of pre-
defined verbs that are often used lightly as a
feature which helps distinguish between light
and non-light verbs when used with the same
noun complement. The identity of such verbs
has been shown to be largely independent of
language, and corresponds to verbs that trans-
mit information about possession, direction,
aspect and causation.
References
T. Bjerre. 1999. Event structure and support verb
constructions. In 4th Student Session of European
Summer School on Logic, Language and Informa-
tion 1999. Universiteit Utrecht Press, Aug, 1999.
L. Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32, Oct, 2001.
M. Butt and W. Geuder. 2001. On the (semi)lexical
status of light verbs. In Semi-lexical Categories,
pages 323?370. Mouton de Gruyter.
M. Butt. 2003. The light verb jungle. In Workshop on
Multi-Verb Constructions.
M. Dras and M. Johnson. 1996. Death and light-
ness: Using a demographic model to find support
verbs. In 5th International Conference on the Cog-
nitive Science of Natural Language Processing.
A. Fazly, R. North, and S. Stevenson. 2005. Automat-
ically distinguishing literal and figurative usages of
highly polysemous verbs. In ACL 2005 Workshop
on Deep Lexical Acquisition, pages 38?47.
G. Grefenstette and S. Teufel. 1995. A corpus-based
method for automatic identification of support verbs
for nominalizations. In EACL ?95.
T. Miyamoto. 2000. The Light Verb Construction in
Japanese. The role of the verbal noun. John Ben-
jamins.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A pain
in the neck for NLP. In Lecture Notes in Computer
Science, volume 2276, Jan, 2002.
S. Stevenson, A. Fazly, and R. North. 2004. Statisti-
cal measures of the semi-productivity of light verb
constructions. In 2nd ACL Workshop on Multiword
Expressions: Integrating Processing, pages 1?8.
L. Wang. 2004. A corpus-based study of mandarin
verbs of doing. Concentric: Studies in Linguistics,
30(1):65?85, Jun, 2004.
I. Witten and E. Frank. 2000. Data Mining: Practical
machine learning tools with Java implementations.
Morgan Kaufmann.
56
