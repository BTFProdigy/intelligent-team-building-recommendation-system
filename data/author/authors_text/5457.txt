Dialogue Act Modeling for 
Automatic Tagging and Recognition 
of Conversational Speech 
Andreas Stolcke* 
SRI International 
Noah Coccaro 
University of Colorado at Boulder 
Rebecca Bates 
University of Washington 
Paul Taylor 
University of Edinburgh 
Carol Van Ess-Dykema 
U.S. Department of Defense 
Klaus Ries 
Carnegie Mellon University and 
University of Karlsruhe 
Elizabeth Shriberg 
SRI International 
Daniel Jurafsky 
University of Colorado at Boulder 
Rachel Martin 
Johns Hopkins University 
Marie Meteer 
BBN Technologies 
We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- 
act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREE- 
MENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, colloca- 
tional, and prosodic ues, as well as on the discourse coherence of the dialogue act sequence. 
The dialogue model is based on treating the discourse structure of a conversation as a hidden 
Markov model and the individual dialogue acts as observations emanating from the model states. 
Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The 
statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks 
modeling the idiosyncratic lexical and prosodic manifestations ofeach dialogue act. We develop 
a probabilistic ntegration of speech recognition with dialogue modeling, to improve both speech 
recognition and dialogue act classification accuracy. Models are trained and evaluated using a 
large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous 
human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based 
on errorful, automatically recognized words and prosody, and 71% based on word transcripts, 
compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction 
in word recognition error. 
? Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 
94025, 1-650-859-2544. E-mail: stolcke@speech.sri.com. 
@ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 3 
Table 1 
Fragment of a labeled conversation (from the Switchboard corpus). 
Speaker Dialogue Act Utterance 
A YEs-No-QuESTION So do you go to college right now? 
A ABANDONED Are yo-, 
B YES- ANSWER Yeah, 
B STATEMENT it's my last year \[laughter\]. 
A DECLARATIVE-QUESTION You're a, so you're a senior now. 
B YEs-ANSWER Yeah, 
B STATEMENT I'm working on my projects trying to graduate 
\[laughter\]. 
A APPRECIATION Oh, good for you. 
B BACKCHANNEL Yeah. 
A APPRECIATION That's great, 
A YEs-No-QUESTION um, is, is N C University is that, uh, State, 
B STATEMENT N C State. 
A SIGNAL-NoN-UNDERSTANDING What did you say? 
B STATEMENT N C State. 
1. Introduction 
The ability to model and automatically detect discourse structure is an important 
step toward understanding spontaneous dialogue. While there is hardly consensus 
on exactly how discourse structure should be described, some agreement exists that 
a useful first level of analysis involves the identification of dialogue acts (DAs). A 
DA represents he meaning of an utterance at the level of illocutionary force (Austin 
1962). Thus, a DA is approximately the equivalent of the speech act of Searle (1969), 
the conversational game move of Power (1979), or the adjacency pair part of Schegloff 
(1968) and Saks, Schegloff, and Jefferson (1974). 
Table 1 shows a sample of the kind of discourse structure in which we are inter- 
ested. Each utterance is assigned a unique DA label (shown in column 2), drawn from 
a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that 
classifies utterances according to a combination of pragmatic, semantic, and syntactic 
criteria. The computational community has usually defined these DA categories so as 
to be relevant to a particular application, although efforts are under way to develop 
DA labeling systems that are domain-independent, such as the Discourse Resource 
Initiative's DAMSL architecture (Core and Allen 1997). 
While not constituting dialogue understanding in any deep sense, DA tagging 
seems clearly useful to a range of applications. For example, a meeting summarizer 
needs to keep track of who said what to whom, and a conversational agent needs to 
know whether it was asked a question or ordered to do something. In related work 
DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin 
and Moore 1977; Levin et al 1999), a slightly higher level unit that comprises a small 
number of DAs. Interactional dominance (Linell 1990) might be measured more ac- 
curately using DA distributions than with simpler techniques, and could serve as an 
indicator of the type or genre of discourse at hand. In all these cases, DA labels would 
enrich the available input for higher-level processing of the spoken words. Another im- 
portant role of DA information could be feedback to lower-level processing. For exam- 
ple, a speech recognizer could be constrained by expectations of likely DAs in a given 
context, constraining the potential recognition hypotheses so as to improve accuracy. 
340 
Stolcke et al Dialogue Act Modeling 
Table 2 
The 42 dialogue act labels. DA frequencies are given as percentages of the total 
number of utterances in the overall corpus. 
Tag Example % 
STATEMENT 
BACKCHANNEL/ACKNOWLEDGE 
OPINION 
ABANDONED/UNINTERPRETABLE 
AGREEMENT/ACCEPT 
APPRECIATION 
YEs-No-QUESTION 
NON-VERBAL 
YES ANSWERS 
CONVENTIONAL-CLOSING 
WH-QUESTION 
NO ANSWERS 
RESPONSE ACKNOWLEDGMENT 
HEDGE 
DECLARATIVE YES-No-QuESTION 
OTHER 
BACKCHANNEL-QUESTION 
QUOTATION 
SUMMARIZE/REFORMULATE 
AFFIRMATIVE NON-YES ANSWERS 
ACTION-DIRECTIVE 
COLLABORATIVE COMPLETION 
REPEAT-PHRASE 
OPEN-QUESTION 
RHETORICAL-QUESTIONS 
HOLD BEFORE ANSWER/AGREEMENT 
REJECT 
NEGATIVE NON-NO ANSWERS 
SIGNAL-NON-UNDERSTANDING 
OTHER ANSWERS 
CONVENTIONAL-OPENING 
OR-CLAUSE 
DISPREFERRED ANSWERS 
3RD-PARTY-TALK 
OFFERS, OPTIONS ~ COMMITS 
SELF-TALK 
D OWNPLAYER 
MAYBE/AcCEPT-PART 
TAG-QUESTION 
DECLARATIVE WH-QUESTION 
APOLOGY 
THANKING 
Me, I'm in the legal department. 36% 
Uh-huh. 19% 
I think it's great 13% 
So, -/ 6% 
That's exactly it. 5% 
I can imagine. 2% 
Do you have to have any special training? 2% 
<Laughter>, < Throat_clearing> 2% 
Yes. 1% 
Well, it's been nice talking to you. 1% 
What did you wear to work today? 1% 
No. 1% 
Oh, okay. 1% 
I don't know if I'm making any sense or not. 1% 
So you can afford to get a house? 1% 
Well give me a break, you know. 1% 
Is that right? 1% 
You can't be pregnant and have cats .5% 
Oh, you mean you switched schools for the kids. .5% 
It is. .4% 
Why don't you go first .4% 
Who aren't contributing. .4% 
Oh, fajitas .3% 
How about you ? .3% 
Who would steal a newspaper? .2% 
I'm drawing a blank. .3% 
Well, no .2% 
Uh, not a whole lot. .1% 
Excuse me? .1% 
I don't know .1% 
How are you? .1% 
or is it more of a company? .1% 
Well, not so much that. .1% 
My goodness, Diane, get down from there. .1% 
I'I1 have to check that out .1% 
What's the word I'm looking for .1% 
That's all right. .1% 
Something like that <.1% 
Right? <.1% 
You are what kind of buff? <.1% 
I'm sorry. <.1% 
Hey thanks a lot <.1% 
The goal of this article is twofold: On the one hand, we aim to present a com- 
prehensive f ramework for model ing and automatic lassification of DAs, founded on 
wel l -known statistical methods. In doing so, we will pull together previous approaches 
as well as new ideas. For example, our model draws on the use of DA n-grams and the 
hidden Markov models of conversation present in earlier work, such as Nagata and 
Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However,  
our f ramework generalizes earlier models, giving us a clean probabilistic approach for 
performing DA classification from unreliable words and nonlexical evidence. For the 
341 
Computational Linguistics Volume 26, Number 3 
speech recognition task, our framework provides a mathematically principled way to 
condition the speech recognizer on conversation context through dialogue structure, as 
well as on nonlexical information correlated with DA identity. We will present meth- 
ods in a domain-independent framework that for the most part treats DA labels as an 
arbitrary formal tag set. Throughout the presentation, we will highlight he simplifi- 
cations and assumptions made to achieve tractable models, and point out how they 
might fall short of reality. 
Second, we present results obtained with this approach on a large, widely available 
corpus of spontaneous conversational speech. These results, besides validating the 
methods described, are of interest for several reasons. For example, unlike in most 
previous work on DA labeling, the corpus is not task-oriented in nature, and the 
amount of data used (198,000 utterances) exceeds that in previous tudies by at least 
an order of magnitude (see Table 14). 
To keep the presentation i teresting and concrete, we will alternate between the 
description of general methods and empirical results. Section 2 describes the task 
and our data in detail. Section 3 presents the probabilistic modeling framework; a
central component of this framework, the discourse grammar, is further discussed in 
Section 4. In Section 5 we describe xperiments for DA classification. Section 6 shows 
how DA models can be used to benefit speech recognition. Prior and related work is 
summarized in Section 7. Further issues and open problems are addressed in Section 8, 
followed by concluding remarks in Section 9. 
2. The Dialogue Act Labeling Task 
The domain we chose to model is the Switchboard corpus of human-human con- 
versational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by 
the Linguistic Data Consortium. Each conversation i volved two randomly selected 
strangers who had been charged with talking informally about one of several, self- 
selected general-interest topics. To train our statistical models on this corpus, we com- 
bined an extensive ffort in human hand-coding of DAs for each utterance, with a 
variety of automatic and semiautomatic tools. Our data consisted of a substantial 
portion of the Switchboard waveforms and corresponding transcripts, totaling 1,155 
conversations. 
2.1 Utterance Segmentation 
Before hand-labeling each utterance in the corpus with a DA, we needed to choose an 
utterance segmentation, as the raw Switchboard ata is not segmented in a linguis- 
tically consistent way. To expedite the DA labeling task and remain consistent with 
other Switchboard-based research efforts, we made use of a version of the corpus that 
had been hand-segmented into sentence-level units prior to our own work and in- 
dependently of our DA labeling system (Meteer et al 1995). We refer to the units of 
this segmentation as utterances. The relation between utterances and speaker turns 
is not one-to-one: a single turn can contain multiple utterances, and utterances can 
span more than one turn (e.g., in the case of backchanneling by the other speaker in 
midutterance). Each utterance unit was identified with one DA, and was annotated 
with a single DA label. The DA labeling system had special provisions for rare cases 
where utterances seemed to combine aspects of several DA types. 
Automatic segmentation f spontaneous speech is an open research problem in its 
own right (Mast et al 1996; Stolcke and Shriberg 1996). A rough idea of the difficulty 
of the segmentation problem on this corpus and using the same definition of utterance 
units can be derived from a recent study (Shriberg et al 2000). In an automatic labeling 
342 
Stolcke t al. Dialogue Act Modeling 
of word boundaries as either utterance or nonboundaries u ing a combination oflexical 
and prosodic ues, we obtained 96% accuracy based on correct word transcripts, and 
78% accuracy with automatically recognized words. The fact that the segmentation 
and labeling tasks are interdependent (Warnke et al 1997; Finke et al 1998) further 
complicates the problem. 
Based on these considerations, we decided not to confound the DA classification 
task with the additional problems introduced by automatic segmentation a d assumed 
the utterance-level s gmentations as given. An important consequence of this decision 
is that we can expect utterance l ngth and acoustic properties at utterance boundaries 
to be accurate, both of which turn out to be important features of DAs (Shriberg et al 
1998; see also Section 5.2.1). 
2.2 Tag Set 
We chose to follow a recent standard for shallow discourse structure annotation, the 
Dialog Act Markup in Several Layers (DAMSL) tag set, which was designed by the 
natural language processing community under the auspices of the Discourse Resource 
Initiative (Core and Allen 1997). We began with the DAMSL markup system, but modi- 
fied it in several ways to make it more relevant to our corpus and task. DAMSL aims to 
provide a domain-independent framework for dialogue annotation, as reflected by the 
fact that our tag set can be mapped back to DAMSL categories (Jurafsky, Shriberg, and 
Biasca 1997). However, our labeling effort also showed that content- and task-related 
distinctions will always play an important role in effective DA labeling. 
The Switchboard omain itself is essentially "task-free," thus giving few external 
constraints on the definition of DA categories. Our primary purpose in adapting the 
tag set was to enable computational DA modeling for conversational speech, with 
possible improvements to conversational speech recognition. Because of the lack of a 
specific task, we decided to label categories that seemed inherently interesting linguis- 
tically and that could be identified reliably. Also, the focus on conversational speech 
recognition led to a certain bias toward categories that were lexically or syntactically 
distinct (recognition accuracy is traditionally measured including all lexical elements 
in an utterance). 
While the modeling techniques described in this paper are formally independent of 
the corpus and the choice of tag set, their success on any particular task will of course 
crucially depend on these factors. For different asks, not all the techniques used in 
this study might prove useful and others could be of greater importance. However, 
we believe that this study represents a fairly comprehensive application of technology 
in this area and can serve as a point of departure and reference for other work. 
The resulting SWBD-DAMSL tag set was multidimensional; pproximately 50ba- 
sic tags (e.g., QUESTION, STATEMENT) could each be combined with diacritics indicat- 
ing orthogonal information, for example, about whether or not the dialogue function 
of the utterance was related to Task-Management and Communication-Management. 
Approximately 220 of the many possible unique combinations ofthese codes were used 
by the coders (Jurafsky, Shriberg, and Biasca 1997). To obtain a system with somewhat 
higher interlabeler agreement, as well as enough data per class for statistical mod- 
eling purposes, a less fine-grained tag set was devised. This tag set distinguishes 42 
mutually exclusive utterance types and was used for the experiments reported here. 
Table 2 shows the 42 categories with examples and relative frequencies. 1 While some 
1 For the study focusing on prosodic modeling ofDAs reported elsewhere (Shriberg et al 1998), the tag 
set was further reduced to six categories. 
343 
Computational Linguistics Volume 26, Number 3 
of the original infrequent classes were collapsed, the resulting DA type distribution 
is still highly skewed. This occurs largely because there was no basis for subdividing 
the dominant DA categories according to task-independent a d reliable criteria. 
The tag set incorporates both traditional sociolinguistic and discourse-theoretic 
notions, such as rhetorical relations and adjacency pairs, as well as some more form- 
based labels. Furthermore, the tag set is structured so as to allow labelers to annotate 
a Switchboard conversation from transcripts alone (i.e., without listening) in about 
30 minutes. Without hese constraints he DA labels might have included some finer 
distinctions, but we felt that this drawback was balanced by the ability to cover a large 
amount of data. 2
Labeling was carried out in a three-month period in 1997 by eight linguistics 
graduate students at CU Boulder. Interlabeler agreement for the 42-1abel tag set used 
here was 84%, resulting in a Kappa statistic of 0.80. The Kappa statistic measures 
agreement ormalized for chance (Siegel and Castellan, Jr. 1988). As argued in Carletta 
(1996), Kappa values of 0.8 or higher are desirable for detecting associations between 
several coded variables; we were thus satisfied with the level of agreement achieved. 
(Note that, even though only a single variable, DA type, was coded for the present 
study, our goal is, among other things, to model associations between several instances 
of that variable, e.g., between adjacent DAs.) 
A total of 1,155 Switchboard conversations were labeled, comprising 205,000 ut- 
terances and 1.4 million words. The data was partitioned into a training set of 1,115 
conversations (1.4M words, 198K utterances), used for estimating the various compo- 
nents of our model, and a test set of 19 conversations (29K words, 4K utterances). 
Remaining conversations were set aside for future use (e.g., as a test set uncompro- 
mised of tuning effects). 
2.3 Major Dialogue Act Types 
The more frequent DA types are briefly characterized below. As discussed above, the 
focus of this paper is not on the nature of DAs, but on the computational framework 
for their recognition; full details of the DA tag set and numerous motivating examples 
can be found in a separate report (Jurafsky, Shriberg, and Biasca 1997). 
Statements and Opinions. The most common types of utterances were STATEMENTS 
and OPINIONS. This split distinguishes "descriptive, narrative, or personal" statements 
(STATEMENT) from "other-directed opinion statements" (OPINION). The distinction was 
designed to capture the different kinds of responses we saw to opinions (which are 
often countered or disagreed with via further opinions) and to statements (which more 
often elicit continuers or backchannels): 
Dialogue Act 
STATEMENT 
STATEMENT 
STATEMENT 
OPINION 
OPINION 
Example Utterance 
Well, we have a cat, um, 
He's probably, oh, a good two years old, 
big, old, fat and sassy tabby. 
He's about five months old 
Well, rabbits are darling. 
I think it would be kind of stressful. 
2 The effect of lacking acoustic information on labeling accuracy was assessed by relabeling a subset of 
the data with listening, and was found to be fairly small (Shriberg et al 1998). A conservative estimate 
based on the relabeling study is that, for most DA types, at most 2% of the labels might have changed 
based on listening. The only DA types with higher uncertainty were BACKCHANNELS and 
AGREEMENTS, which are easily confused with each other without acoustic ues; here the rate of change 
was no more than 10%. 
344 
Stolcke t al. Dialogue Act Modeling 
OPINIONS often include such hedges as I think, I believe, it seems, and I mean. We 
combined the STATEMENT and OPINION classes for other studies on dimensions in 
which they did not differ (Shriberg et al 1998). 
Questions. Questions were of several types. The YES-No-QUESTION label includes only 
utterances having both the pragmatic force of a yes-no-question and the syntactic mark- 
ings of a yes-no-question (i.e., subject-inversion r sentence-final t gs). DECLARATIVE- 
QUESTIONS are utterances that function pragmatically as questions but do not have 
"question form." By this we mean that declarative questions normally have no wh- 
word as the argument of the verb (except in "echo-question" format), and have "declar- 
ative" word order in which the subject precedes the verb. See Weber (1993) for a survey 
of declarative questions and their various realizations. 
Dialogue Act Example Utterance 
YEs-No-QUESTION 
YEs-No-QUESTION 
YEs-No-QuESTION 
DECLARATIVE- QUESTION 
WH-QUESTION 
Do you have to have any special training? 
But that doesn't eliminate it, does it? 
Uh, I guess a year ago you're probably 
watching C N N a lot, right? 
So you're taking a government course? 
Well, how old are you? 
Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, 
e.g., indicating that the speaker should go on talking. These are usually referred to in 
the conversation analysis literature as "continuers" and have been studied extensively 
(Jefferson 1984; Schegloff 1982; Yngve 1970). We expect recognition of backchannels to
be useful because of their discourse-structuring role (knowing that the hearer expects 
the speaker to go on talking tells us something about the course of the narrative) 
and because they seem to occur at certain kinds of syntactic boundaries; detecting a 
backchannel may thus help in predicting utterance boundaries and surrounding lexical 
material. 
For an intuition about what backchannels look like, Table 3 shows the most com- 
mon realizations of the approximately 300 types (35,827 tokens) of backchannel in 
our Switchboard subset. The following table shows examples of backchannels in the 
context of a Switchboard conversation: 
Speaker Dialogue Act Utterance 
B STATEMENT 
A BACKCHANNEL 
B STATEMENT 
B STATEMENT 
A BACKCHANNEL 
B STATEMENT 
B STATEMENT 
A APPRECIATION 
but, uh, we're to the point now where our 
financial income is enough that we can consider 
putting some away - 
Uh-huh. / 
- for college, / 
so we are going to be starting a regular payroll 
deduction - 
Urn. / 
- -  in the fall / 
and then the money that I will be making this 
summer we'll be putting away for the college 
fund. 
Urn. Sounds good. 
Turn Exits and Abandoned Utterances. Abandoned utterances are those that the speaker 
breaks off without finishing, and are followed by a restart. Turn exits resemble aban- 
doned utterances in that they are often syntactically broken off, but they are used 
345 
Computational Linguistics Volume 26, Number 3 
Table 3 
Most common realizations of backchannels in Switchboard. 
Frequency Form Frequency Form Frequency Form 
38% uh-huh 2% yes 1% sure 
34% yeah 2% okay 1.% um 
9% right 2% oh yeah 1% huh-uh 
3% oh 1% huh 1% uh 
mainly as a way of passing speakership to the other speaker. Turn exits tend to be 
single words, often so or or. 
Speaker Dialogue Act Utterance 
A STATEMENT we're from, uh, I 'm from Ohio / 
A STATEMENT and my wife's from Florida / 
A TURN-ExIT SO, -/  
B BACKCHANNEL Uh-huh./ 
A HEDGE 
A ABANDONED 
A STATEMENT 
so, I don't know, / 
it's Klipsmack>, - / 
I 'm glad it's not the kind of problem I have to 
come up with an answer to because it's not - 
Answers and Agreements. YES-ANSWERS include yes, yeah, yep, uh-huh, and other varia- 
tions on yes, when they are acting as an answer to a YES-NO-QUESTION or DECLARA- 
TWE-0UESTION. Similarly, we also coded NO-ANSWERS. Detecting ANSWERS can help 
tell us that the previous utterance was a YES-NO-QUESTION. Answers are also seman- 
tically significant since they are likely to contain new information. 
AGREEMENT/ACCEPT, REJECT, and MAYBE/ACCEPT-PART all mark the degree 
to which a speaker accepts ome previous proposal, plan, opinion, or statement. The 
most common of these are the AGREEMENT/AccEPTS. These are very often yes or yeah, 
so they look a lot like ANSWERS. But where ANSWERS follow questions, AGREEMENTS 
often follow opinions or proposals, so distinguishing these can be important for the 
discourse. 
3. Hidden Markov Modeling of Dialogue 
We will now describe the mathematical  nd computational  f ramework used in our 
study. Our goal is to perform DA classification and other tasks using a probabilis- 
tic formulation, giving us a principled approach for combining multiple knowledge 
sources (using the laws of probability), as well as the ability to derive model  parame- 
ters automatical ly from a corpus, using statistical inference techniques. 
Given all available evidence E about a conversation, the goal is to find the DA 
sequence U that has the highest posterior probabil ity P(UIE ) given that evidence. 
Apply ing Bayes' rule we get 
U* = argmaxP(UIE ) 
U 
P(U)P(ElU) = argmax u P(E) 
= argmaxP(U)P(ElU) (1) 
U 
Here P(U) represents the prior probabil ity of a DA sequence, and P(EIU ) is the like- 
346 
Stolcke t al. Dialogue Act Modeling 
Table 4 
Summary of random variables used in dialogue modeling. 
(Speaker labels are introduced in Section 4.) 
Symbol Meaning 
U 
E 
F 
A 
W 
T 
sequence of DA labels 
evidence (complete speech signal) 
prosodic evidence 
acoustic evidence (spectral features used in ASR) 
sequence of words 
speakers labels 
lihood of U given the evidence. The likelihood is usually much more straightforward 
to model than the posterior itself. This has to do with the fact that our models are 
generative or causal in nature, i.e., they describe how the evidence is produced by the 
underlying DA sequence U. 
Estimating P (U) requires building a probabilistic discourse grammar, i.e., a statisti- 
cal model of DA sequences. This can be done using familiar techniques from language 
modeling for speech recognition, although the sequenced objects in this case are DA 
labels rather than words; discourse grammars will be discussed in detail in Section 4. 
3.1 Dialogue Act Likelihoods 
The computation of likelihoods P(EIU ) depends on the types of evidence used. In our 
experiments we used the following sources of evidence, ither alone or in combination: 
Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W 
refers to the true (hand-transcribed) words spoken in a conversation. 
Recognized words: The evidence consists of recognizer acoustics A, and we seek 
to compute P(A I U). As described later, this involves considering multiple 
alternative recognized word sequences. 
Prosodic features- Evidence is given by the acoustic features F capturing various 
aspects of pitch, duration, energy, etc., of the speech signal; the associated 
likelihoods are P(F I U). 
For ease of reference, all random variables used here are summarized in Table 4. 
The same variables are used with subscripts to refer to individual utterances. For 
example, Wi is the word transcription of the ith utterance within a conversation ( ot 
the ith word). 
To make both the modeling and the search for the best DA sequence feasible, we 
further equire that our likelihood models are decomposable byutterance. This means 
that the likelihood given a complete conversation can be factored into likelihoods 
given the individual utterances. We use Ui for the ith DA label in the sequence U, 
i.e., U = (U1 . . . . .  Ui,..., Un), where n is the number of utterances in a conversation. 
In addition, we use Ei for that portion of the evidence that corresponds to the ith 
utterance, .g., the words or the prosody of the ith utterance. Decomposability of the 
likelihood means that 
P(EIU) = P(E11 U1).... .  P(En \[Un) (2) 
Applied separately to the three types of evidence Ai, Wi, and Fi mentioned above, 
it is clear that this assumption is not strictly true. For example, speakers tend to reuse 
347 
Computational Linguistics Volume 26, Number 3 
E1 Ei E.  
T T T 
<start> , U1 , . . .  ~ Ui ) . . . - - - *  Un 
Figure 1 
The discourse HMM as Bayes network. 
<end> 
words found earlier in the conversation (Fowler and Housum 1987) and an answer 
might actually be relevant o the question before it, violating the independence of the 
P(WilUi). Similarly, speakers adjust their pitch or volume over time, e.g., to the con- 
versation partner or because of the structure of the discourse (Menn and Boyce 1982), 
violating the independence of the P(FilUi). As in other areas of statistical modeling, 
we count on the fact that these violations are small compared to the properties actually 
modeled, namely, the dependence of Ei on Ui. 
3.2 Markov  Mode l ing  
Returning to the prior distribution of DA sequences P(U), it is convenient to make 
certain independence assumptions here, too. In particular, we assume that the prior 
distribution of U is Markovian, i.e., that each Ui depends only on a fixed number k of 
preceding DA labels: 
P(U i lU l ,  . . . ,  U i -1 )  ~- P (U i lU i -k  . . . . .  Ui -1 )  (3) 
(k is the order of the Markov process describing U). The n-gram-based discourse gram- 
mars we used have this property. As described later, k = 1 is a very good choice, i.e., 
conditioning on the DA types more than one removed from the current one does not 
improve the quality of the model by much, at least with the amount of data available 
in our experiments. 
The importance of the Markov assumption for the discourse grammar is that 
we can now view the whole system of discourse grammar and local utterance-based 
likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). 
The HMM states correspond to DAs, observations correspond to utterances, transition 
probabilities are given by the discourse grammar (see Section 4), and observation 
probabilities are given by the local likelihoods P(Eil Ui). 
We can represent the dependency structure (as well as the implied conditional 
independences) as a special case of Bayesian belief network (Pearl 1988). Figure 1 
shows the variables in the resulting HMM with directed edges representing conditional 
dependence. To keep things simple, a first-order HMM (bigram discourse grammar) 
is assumed. 
3.3 D ia logue  Act  Decod ing  
The HMM representation allows us to use efficient dynamic programming algorithms 
to compute relevant aspects of the model, such as 
? the most probable DA sequence (the Viterbi algorithm) 
? the posterior probability of various DAs for a given utterance, after 
considering all the evidence (the forward-backward algorithm) 
The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probable 
state sequence. When applied to a discourse model with locally decomposable ike- 
lihoods and Markovian discourse grammar, it will therefore find precisely the DA 
348 
Stolcke et al Dialogue Act Modeling 
sequence with the highest posterior probability: 
U* = argmaxP(UIE ) (4) 
u 
The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is 
fundamentally the same as the standard probabilistic approaches to speech recognition 
(Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the prob- 
ability of getting the entire DA sequence correct, but it does not necessarily find the 
DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To 
minimize the total number of utterance labeling errors, we need to maximize the prob- 
ability of getting each DA label correct individually, i.e., we need to maximize P(UilE) 
for each i = 1 . . . . .  n. We can compute the per-utterance posterior DA probabilities by 
summing: 
P(u\[E) = E P(UIE) (5) 
U: Ui=u 
where the summation is over all sequences U whose ith element matches the label in 
question. The summation is efficiently carried out by the forward-backward algorithm 
for HMMs (Baum et al 1970). 3
For zeroth-order (unigram) discourse grammars, Viterbi decoding and forward- 
backward decoding necessarily ield the same results. However, for higher-order 
discourse grammars we found that forward-backward decoding consistently gives 
slightly (up to 1% absolute) better accuracies, as expected. Therefore, we used this 
method throughout. 
The formulation presented here, as well as all our experiments, uses the entire 
conversation as evidence for DA classification. Obviously, this is possible only during 
off-line processing, when the full conversation is available. Our paradigm thus follows 
historical practice in the Switchboard omain, where the goal is typically the off-line 
processing (e.g., automatic transcription, speaker identification, indexing, archival) of 
entire previously recorded conversations. However, the HMM formulation used here 
also supports computing posterior DA probabilities based on partial evidence, e.g., 
using only the utterances preceding the current one, as would be required for on-line 
processing. 
4. Discourse Grammars 
The statistical discourse grammar models the prior probabilities P(U) of DA sequences. 
In the case of conversations for which the identities of the speakers are known (as 
in Switchboard), the discourse grammar should also model turn-taking behavior. A 
straightforward approach is to model sequences of pairs (Ui, Ti) where Ui is the DA 
label and Ti represents he speaker. We are not trying to model speaker idiosyncrasies, 
so conversants are arbitrarily identified as A or B, and the model is made symmetric 
with respect o the choice of sides (e.g., by replicating the training sequences with 
sides switched). Our discourse grammars thus had a vocabulary of 42 x 2 = 84 labels, 
plus tags for the beginning and end of conversations. For example, the second DA tag 
in Table 1 would be predicted by a trigram discourse grammar using the fact that the 
same speaker previously uttered a YES-NO-QUESTION, which in turn was preceded by 
the start-of-conversation. 
3 We note in passing that he Viterbi and Baum algorithms have quivalent formulations in the Bayes 
network framework (Pearl 1988). The HMM terminology was chosen here mainly for historical reasons. 
349 
Computational Linguistics Volume 26, Number 3 
Table 5 
Perplexities of DAs with and without urn 
information. 
Discourse Grammar P(U) P(U, T) P(UIT )
None 42 84 42 
Unigram 11.0 18.5 9.0 
Bigram 7.9 10.4 5.1 
Trigram 7.5 9.8 4.8 
4.1 N-gram Discourse Mode ls  
A computationally convenient type of discourse grammar is an n-gram model based on 
DA tags, as it allows efficient decoding in the HMM framework. We trained standard 
backoff n-gram models (Katz 1987), using the frequency smoothing approach of Witten 
and Bell (1991). Models of various orders were compared by their perplexities, i.e., 
the average number of choices the model predicts for each tag, conditioned on the 
preceding tags. 
Table 5 shows perplexities for three types of models: P(U), the DAs alone; P(U, T), 
the combined DA/speaker ID sequence; and P(UIT ), the DAs conditioned on known 
speaker IDs (appropriate for the Switchboard task). As expected, we see an improve- 
ment (decreasing perplexities) for increasing n-gram order. However, the incremental 
gain of a trigram is small, and higher-order models did not prove useful. (This ob- 
servation, initially based on perplexity, is confirmed by the DA tagging experiments 
reported in Section 5.) Comparing P(U) and P(U\[T), we see that speaker identity adds 
substantial information, especially for higher-order models. 
The relatively small improvements from higher-order models could be a result of 
lack of training data, or of an inherent independence of DAs from DAs further re- 
moved. The near-optimality of the bigram discourse grammar is plausible given con- 
versation analysis accounts of discourse structure in terms of adjacency pairs (Schegloff 
1968; Sacks, Schegloff, and Jefferson 1974). Inspection of bigram probabilities estimated 
from our data revealed that conventional djacency pairs receive high probabilities, as 
expected. For example, 30% of YES-NO-QUESTIONS are followed by YES-ANSWERS, 
14% by NO-ANSWERS (confirming that the latter are dispreferred). COMMANDS are fol- 
lowed by AGREEMENTS in 23% of the cases, and STATEMENTS elicit BACKCHANNELS 
in 26% of all cases. 
4.2 Other Discourse Mode ls  
We also investigated non-n-gram discourse models, based on various language model- 
ing techniques known from speech recognition. One motivation for alternative models 
is that n-grams enforce a one-dimensional representation  DA sequences, whereas 
we saw above that the event space is really multidimensional (DA label and speaker 
labels). Another motivation is that n-grams fail to model long-distance dependencies, 
such as the fact that speakers may tend to repeat certain DAs or patterns throughout 
the conversation. 
The first alternative approach was a standard cache model (Kuhn and de Mori 
1990), which boosts the probabilities of previously observed unigrams and bigrams, on 
the theory that tokens tend to repeat hemselves over longer distances. However, this 
does not seem to be true for DA sequences in our corpus, as the cache model showed 
no improvement over the standard N-gram. This result is somewhat surprising since 
unigram dialogue grammars are able to detect speaker gender with 63% accuracy (over 
350 
Stolcke t al. Dialogue Act Modeling 
a 50% baseline) on Switchboard (Ries 1999b), indicating that there are global variables 
in the DA distribution that could potentially be exploited by a cache dialogue grammar. 
Clearly, dialogue grammar adaptation needs further esearch. 
Second, we built a discourse grammar that incorporated constraints on DA se- 
quences in a nonhierarchical way, using maximum entropy (ME) estimation (Berger, 
Della Pietra, and Della Pietra 1996). The choice of features was informed by similar 
ones commonly used in statistical language models, as well our general intuitions 
about potentially information-bearing elements in the discourse context. Thus, the 
model was designed so that the current DA label was constrained by features uch as 
unigram statistics, the previous DA and the DA once removed, DAs occurring within a 
window in the past, and whether the previous utterance was by the same speaker. We 
found, however, that an ME model using n-gram constraints performed only slightly 
better than a corresponding backoff n-gram. 
Additional constraints such as DA triggers, distance-1 bigrams, separate ncoding 
of speaker change and bigrams to the last DA on the same/other channel did not 
improve relative to the trigram model. The ME model thus confirms the adequacy of 
the backoff n-gram approach, and leads us to conclude that DA sequences, at least 
in the Switchboard omain, are mostly characterized by local interactions, and thus 
modeled well by low-order n-gram statistics for this task. For more structured tasks this 
situation might be different. However, we have found no further exploitable structure. 
5. Dialogue Act Classification 
We now describe in more detail how the knowledge sources of words and prosody 
are modeled, and what automatic DA labeling results were obtained using each of the 
knowledge sources in turn. Finally, we present results for a combination of all knowl- 
edge sources. DA labeling accuracy results hould be compared to a baseline (chance) 
accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT) in 
our test set. 4 
5.1 Dialogue Act Classification Using Words 
DA classification using words is based on the observation that different DAs use 
distinctive word strings. It is known that certain cue words and phrases (Hirschberg 
and Litman 1993) can serve as explicit indicators of discourse structure. Similarly, 
we find distinctive correlations between certain phrases and DA types. For example, 
92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams "<start> 
do you" occur in YES-NO-QUESTIONS. To leverage this information source, without 
hand-coding knowledge about which words are indicative of which DAs, we will use 
statistical language models that model the full word sequences associated with each 
DA type. 
5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words 
of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) 
in a straightforward way, by building a statistical language model for each of the 42 
DAs. All DAs of a particular type found in the training corpus were pooled, and 
a DA-specific trigram model was estimated using standard techniques (Katz backoff 
\[Katz 1987\] with Witten-Bell discounting \[Witten and Bell 1991\]). 
4 The frequency of STATEMENTS across all labeled ata was slightly different, cf. Table 2. 
351 
Computational Linguistics Volume 26, Number 3 
A1 Ai An 
T T 
wl wi w.  
T T T 
<start> ~ U1 ~ . . . .  ~ Ui ~ . . . .  Un ~ <end> 
Figure 2 
Modified Bayes network including word hypotheses and recognizer acoustics. 
5.1.2 Classification from Recognized Words. For fully automatic DA classification, 
the above approach is only a partial solution, since we are not yet able to recognize 
words in spontaneous speech with perfect accuracy. A standard approach is to use 
the 1-best hypothesis from the speech recognizer in place of the true word transcripts. 
While conceptually simple and convenient, his method will not make optimal use of 
all the information in the recognizer, which in fact maintains multiple hypotheses as 
well as their relative plausibilities. 
A more thorough use of recognized speech can be derived as follows. The classifi- 
cation framework is modified such that the recognizer's acoustic information (spectral 
features) A appear as the evidence. We compute P(A\[U) by decomposing it into an 
acoustic likelihood P(A\]W) and a word-based likelihood P(W\[ U), and summing over 
all word sequences: 
P(AlU) = ~-~ P(AIW, U)P(WIU) 
w 
= ~P(A IW)P(W\ [U  ) 
w 
(6) 
The second line is justified under the assumption that the recognizer acoustics (typ- 
ically, cepstral coefficients) are invariant o DA type once the words are fixed. Note 
that this is another approximation i our modeling. For example, different DAs with 
common words may be realized by different word pronunciations. Figure 2 shows the 
Bayes network resulting from modeling recognizer acoustics through word hypothe- 
ses under this independence assumption; note the added Wi variables (that have to 
be summed over) in comparison to Figure 1. 
The acoustic likelihoods P(A\[W) correspond to the acoustic scores the recognizer 
outputs for every hypothesized word sequence W. The summation over all W must 
be approximated; in our experiments we summed over the (up to) 2,500 best hypothe- 
ses generated by the recognizer for each utterance. Care must be taken to scale the 
recognizer acoustic scores properly, i.e., to exponentiate he recognizer acoustic scores 
by 1/~, where A is the language model weight of the recognizer, s 
5 In a standard recognizer the total og score of a hypothesis Wi is computed as 
logP(AdWi ) + )~ logP(Wi) - I~\]Wi\], 
where \[Wi\] is the number of words in the hypothesis, and both A and/~ are parameters optimized to 
minimize the word error rate. The word insertion penalty/~ represents a correction to the language 
model that allows balancing insertion and deletion errors. The language model weight ,~ compensates 
for acoustic score variances that are effectively too large due to severe independence assumptions in 
the recognizer acoustic model. According to this rationale, it is more appropriate o divide all score 
components by ),. Thus, in all our experiments, we computed a summand in Equation 6whose 
352 
Stolcke t al. Dialogue Act Modeling 
Table 6 
DA classification accuracies (in %) from transcribed and recognized 
words (chance = 35%). 
Discourse Grammar True Recognized Relative Error Increase 
None 54.3 42.8 25.2% 
Unigram 68.2 61.8 20.1% 
Bigram 70.6 64.3 21.4% 
Trigram 71.0 64.8 21.4% 
5.1.3 Results. Table 6 shows DA classification accuracies obtained by combining the 
word- and recognizer-based likelihoods with the n-gram discourse grammars de- 
scribed earlier. The best accuracy obtained from transcribed words, 71%, is encour- 
aging given a comparable human performance of84% (the interlabeler agreement, see 
Section 2.2). We observe about a 21% relative increase in classification error when us- 
ing recognizer words; this is remarkably small considering that the speech recognizer 
used had a word error rate of 41% on the test set. 
We also compared the n-best DA classification approach to the more straightfor- 
ward 1-best approach. In this experiment, only the single best recognizer hypothesis 
is used, effectively treating it as the true word string. The 1-best method increased 
classification error by about 7% relative to the n-best algorithm (61.5% accuracy with 
a bigram discourse grammar). 
5.2 Dialogue Act Classification Using Prosody 
We also investigated prosodic information, i.e., information i dependent of the words 
as well as the standard recognizer acoustics. Prosody is important for DA recogni- 
tion for two reasons. First, as we saw earlier, word-based classification suffers from 
recognition errors. Second, some utterances are inherently ambiguous based on words 
alone. For example, some YES-NO-QUESTiONS have word sequences identical to those 
of STATEMENTS, but can often be distinguished by their final F0 rise. 
A detailed study aimed at automatic prosodic lassification of DAs in the Switch- 
board domain is available in a companion paper (Shriberg et al 1998). Here we investi- 
gate the interaction of prosodic models with the dialogue grammar and the word-based 
DA models discussed above. We also touch briefly on alternative machine learning 
models for prosodic features. 
5.2.1 Prosodic Features. Prosodic DA classification was based on a large set of fea- 
tures computed automatically from the waveform, without reference to word or phone 
information. The features can be broadly grouped as referring to duration (e.g., utter- 
ance duration, with and without pauses), pauses (e.g., total and mean of nonspeech 
regions exceeding 100 ms), pitch (e.g., mean and range of F0 over utterance, slope of 
F0 regression line), energy (e.g., mean and range of RMS energy, same for signal-to- 
logarithm was 
1 logP(Ai\]Wi) + logP(WilUi) - ~lWil. -d 
We found this approach to give better esults than the standard multiplication of logP(W) by ,L Note 
that for selecting the best hypothesis in a recognizer only the relative magnitudes of the score weights 
matter; however, for the summation in Equation 6 the absolute values become important. The 
parameter values for )~ and # were those used by the standard recognizer; they were not specifically 
optimized for the DA classification task. 
353 
Computational Linguistics Volume 26, Number 3 
~ 23.403 
~ an utt < 0 .3" /~U >= 0.3"/17 
Figure 3 
Decision tree for the classification of BACKCHANNELS (B) and AGREEMENTS (A). Each node is 
labeled with the majority class for that node, as well as the posterior probabilities of the two 
classes. The following features are queried in the tree: number of frames in continuous (> 1 s) 
speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration 
excluding pauses > 100 ms (ling_dur_minus_minlOpause), andmean signal-to-noise ratio 
(snr_mean_utt ). 
noise ratio \[SNR\]), speaking rate (based on the "enrate" measure of Morgan, Fosler, 
and Mirghafori \[1997\]), and gender (of both speaker and listener). In the case of ut- 
terance duration, the measure correlates both with length in words and with overall 
speaking rate. The gender feature that classified speakers as either male or female was 
used to test for potential inadequacies in F0 normalizations. Where appropriate, we 
included both raw features and values normalized by utterance and/or  conversation. 
We also included features that are the output of the pitch accent and boundary tone 
event detector of Taylor (2000) (e.g., the number of pitch accents in the utterance). A 
complete description of prosodic features and an analysis of their usage in our models 
can be found in Shriberg et al (1998). 
5.2.2 Prosodic Decision Trees. For our Prosodic classifiers, we used CART-style deci- 
sion trees (Breiman et al 1984). Decision trees allow the combination of discrete and 
continuous features, and can be inspected to help in understanding the role of different 
features and feature combinations. 
To illustrate one area in which prosody could aid our classification task, we applied 
trees to DA classifications known to be ambiguous from words alone. One frequent 
example in our corpus was the distinction between BACKCHANNELS and AGREEMENTS 
(see Table 2), which share terms such as right and yeah. As shown in Figure 3, a prosodic 
tree trained on this task revealed that agreements have consistently longer durations 
and greater energy (as reflected by the SNR measure) than do backchannels. 
354 
Stolcke t al. Dialogue Act Modeling 
Table 7 
DA classification using prosodic 
decision trees (chance = 35%). 
Discourse Grammar Accuracy (%) 
None 38.9 
Unigram 48.3 
Bigram 49.7 
The HMM framework requires that we compute prosodic likelihoods of the form 
P(FilUi) for each utterance Ui and associated prosodic feature values Fi. We have 
the apparent difficulty that decision trees (as well as other classifiers, such as neural 
networks) give estimates for the posterior probabilities, P(Ui\[Fi). The problem can be 
overcome by applying Bayes' rule locally: 
P(Ui) t rue)  
(7) 
Note that P(Fi) does not depend on Ui and can be treated as a constant for the purpose 
of DA classification. A quantity proportional to the required likelihood can therefore 
be obtained either by dividing the posterior tree probability by the prior P(Ui), 6 or by 
training the tree on a uniform prior distribution of DA types. We chose the second 
approach, downsampling our training data to equate DA proportions. This also coun- 
teracts a common problem with tree classifiers trained on very skewed distributions 
of target classes, i.e., that low-frequency classes are not modeled in sufficient detail 
because the majority class dominates the tree-growing objective hznction. 
5.2.3 Results with Dec is ion Trees. As a preliminary experiment to test the integra- 
tion of prosody with other knowledge sources, we trained a single tree to discriminate 
among the five most frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABAN- 
DONED, and AGREEMENT, totaling 79% of the data) and an Other category comprising 
all remaining DA types. The decision tree was trained on a downsampled training 
subset containing equal proportions of these six DA classes. The tree achieved a clas- 
sification accuracy of 45.4% on an independent test set with the same uniform six-class 
distribution. The chance accuracy on this set is 16.6%, so the tree clearly extracts useful 
information from the prosodic features. 
We then used the decision tree posteriors as scaled DA likelihoods in the dialogue 
model HMM, combining it with various n-gram dialogue grammars for testing on our 
full standard test set. For the purpose of model integration, the likelihoods of the Other 
class were assigned to all DA types comprised by that class. As shown in Table 7, the 
tree with dialogue grammar performs ignificantly better than chance on the raw DA 
distribution, although not as well as the word-based methods (cf. Table 6). 
5.2.4 Neural  Network  Classifiers. Although we chose to use decision trees as prosodic 
classifiers for their relative ase of inspection, we might have used any suitable proba- 
bilistic classifier, i.e., any model that estimates the posterior probabilities of DAs given 
the prosodic features. We conducted preliminary experiments o assess how neural 
6 Bourlard and Morgan (1993) use this approach tointegrate neural network phonetic models in a 
speech recognizer. 
355 
Computational Linguistics Volume 26, Number 3 
Table 8 
Performance ofvarious prosodic neural network classifiers on 
an equal-priors, ix-class DA set (chance = 16.6%). 
Network Architecture Accuracy (%) 
Decision tree 45.4 
No hidden layer, linear output function 44.6 
No hidden layer, softmax output function 46.0 
40-unit hidden layer, softmax output function 46.0 
networks compare to decision trees for the type of data studied here. Neural networks 
are worth investigating since they offer potential advantages over decision trees. They 
can learn decision surfaces that lie at an angle to the axes of the input feature space, 
unlike standard CART trees, which always split continuous features on one dimen- 
sion at a time. The response function of neural networks is continuous (smooth) at 
the decision boundaries, allowing them to avoid hard decisions and the complete 
fragmentation f data associated with decision tree questions. 
Most important, however, related work (Ries 1999a) indicated that similarly struc- 
tured networks are superior classifiers if the input features are words and are therefore 
a plug-in replacement for the language model classifiers described in this paper. Neural 
networks are therefore a good candidate for a jointly optimized classifier of prosodic 
and word-level information since one can show that they are a generalization of the 
integration approach used here. 
We tested various neural network models on the same six-class downsampled 
data used for decision tree training, using a variety of network architectures and out- 
put layer functions. The results are summarized in Table 8, along with the baseline 
result obtained with the decision tree model. Based on these experiments, a softmax 
network (Bridle 1990) without hidden units resulted in only a slight improvement 
over the decision tree. A network with hidden units did not afford any additional 
advantage, ven after we optimized the number of hidden units, indicating that com- 
plex combinations of features (as far as the network could learn them) do not predict 
DAs better than linear combinations of input features. While we believe alternative 
classifier architectures should be investigated further as prosodic models, the results 
so far seem to confirm our choice of decision trees as a model class that gives close to 
optimal performance for this task. 
5.2.5 Intonation Event Likel ihoods. An alternative way to compute prosodically based 
DA likelihoods uses pitch accents and boundary phrases (Taylor et al 1997). The ap- 
proach relies on the intuition that different utterance types are characterized by dif- 
ferent intonational "tunes" (Kowtko 1996), and has been successfully applied to the 
classification ofmove types in the DCIEM Map Task corpus (Wright and Taylor 1997). 
The system detects equences ofdistinctive pitch patterns by training one continuous- 
density HMM for each DA type. Unfortunately, the event classification accuracy on 
the Switchboard corpus was considerably poorer than in the Map Task domain, and 
DA recognition results when coupled with a discourse grammar were substantially 
worse than with decision trees. The approach could prove valuable in the future, 
however, if the intonation event detector can be made more robust to corpora like 
OURS. 
356 
Stolcke et al Dialogue Act Modeling 
A1 Ai An 
T 1 1 
Wl Wi W,, 
T T t 
<start> - , 0"1 , . . .---* U/ , ... ~ Un , <end> 
1 1 ,t 
F1 Fi G 
Figure 4 
Bayes network for discourse HMM incorporating both word recognition and prosodic features. 
5.3 Using Multiple Knowledge Sources 
As mentioned earlier, we expect improved performance from combining word and 
prosodic information. Combining these knowledge sources requires estimating a com- 
bined likelihood P(Ai, Fi\[Ui) for each utterance. The simplest approach is to assume 
that the two types of acoustic observations (recognizer acoustics and prosodic features) 
are approximately conditionally independent once Ui is given: 
P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) 
~, P(ai, Wi\[Ui)P(FilUi) (8) 
Since the recognizer acoustics are modeled by way of their dependence on words, it 
is particularly important o avoid using prosodic features that are directly correlated 
with word identities, or features that are also modeled by the discourse grammars, 
such as utterance position relative to turn changes. Figure 4 depicts the Bayes network 
incorporating evidence from both word recognition and prosodic features. 
One important respect in which the independence assumption is violated is in the 
modeling of utterance length. While utterance length itself is not a prosodic feature, 
it is an important feature to condition on when examining prosodic characteristics 
of utterances, and is thus best included in the decision tree. Utterance length is cap- 
tured directly by the tree using various duration measures, while the DA-specific 
LMs encode the average number of words per utterance indirectly through n-gram 
parameters, but still accurately enough to violate independence in a significant way 
(Finke et al 1998). As discussed in Section 8, this problem is best addressed by joint 
lexical-prosodic models. 
We need to allow for the fact that the models combined in Equation 8 give es- 
timates of differing qualities. Therefore, we introduce an exponential weight a on 
P(Fi\[Ui) that controls the contribution of the prosodic likelihood to the overall likeli- 
hood. Finally, a second exponential weight fl on the combined likelihood controls its 
dynamic range relative to the discourse grammar scores, partially compensating for 
any correlation between the two likelihoods. The revised combined likelihood estimate 
thus becomes: 
P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi\[Ui)~}  (9) 
In our experiments, the parameters a and fl were optimized using twofold jackknifing. 
The test data was split roughly in half (without speaker overlap), each half was used 
to separately optimize the parameters, and the best values were then tested on the 
respective other half. The reported results are from the aggregate outcome on the two 
test set halves. 
357 
Computational Linguistics Volume 26, Number 3 
Table 9 
Combined utterance classification accuracies (chance = 
35%). The first two columns correspond to Tables 7 
and 6, respectively. 
Discourse Grammar Accuracy (%) 
Prosody Recognizer Combined 
None 38.9 42.8 56.5 
Unigram 48.3 61.8 62.4 
Bigram 49.7 64.3 65.0 
Table 10 
Accuracy (in %) for individual 
subtasks, using uniform priors 
and combined models for two 
(chance = 50%). 
Classification Task True Words Recognized Words 
Knowledge Source 
QUESTIONS/STATEMENTS 
prosody only 76.0 76.0 
words only 85.9 75.4 
words+prosody 87.6 79.8 
AGREEMENTS / BACKCHANNELS 
prosody only 72.9 72.9 
words only 81.0 78.2 
words+prosody 84.7 81.7 
5.3.1 Results. In this experiment we combined the acoustic n-best likelihoods based 
on recognized words with the Top-5 tree classifier mentioned in Section 5.2.3. Results 
are summarized in Table 9. 
As shown, the combined classifier presents a slight improvement over the rec- 
ognizer-based classifier, The experiment without discourse grammar indicates that 
the combined evidence is considerably stronger than either knowledge source alone, 
yet this improvement seems to be made largely redundant by the use of priors and 
the discourse grammar. For example, by definition DECLARATIVE-QUESTIONS are not 
marked by syntax (e.g., by subject-auxiliary inversion) and are thus confusable with 
STATEMENTS and OPINIONS. While prosody is expected to help disambiguate hese 
cases, the ambiguity can also be removed by examining the context of the utterance, 
e.g., by noticing that the following utterance is a YEs-ANswER or NO-ANSWER. 
5.3.2 Focused Classifications. To gain a better understanding of the potential for 
prosodic DA classification i dependent of the effects of discourse grammar and the 
skewed DA distribution i  Switchboard, we examined several binary DA classification 
tasks. The choice of tasks was motivated by an analysis of confusions committed by a 
purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, 
and BACKCHANNELS for AGREEMENTS (and vice versa). We tested aprosodic lassifier, 
a word-based classifier (with both transcribed and recognized words), and a combined 
classifier on these two tasks, downsampling the DA distribution to equate the class 
sizes in each case. Chance performance in all experiments i  therefore 50%. Results 
are summarized in Table 10. 
358 
Stolcke et al Dialogue Act Modeling 
As shown, the combined classifier was consistently more accurate than the classi- 
fier using words alone. Although the gain in accuracy was not statistically significant 
for the small recognizer test set because of a lack of power, replication for a larger 
hand-transcribed test set showed the gain to be highly significant for both subtasks 
by a Sign test, p < .001 and p < .0001 (one-tailed), respectively. Across these, as well 
as additional subtasks, the relative advantage of adding prosody was larger for recog- 
nized than for true words, suggesting that prosody is particularly helpful when word 
information is not perfect. 
6. Speech Recognition 
We now consider ways to use DA modeling to enhance automatic speech recognition 
(ASR). The intuition behind this approach is that discourse context constrains the 
choice of DAs for a given utterance, and the DA type in turn constrains the choice of 
words. The latter can then be leveraged for more accurate speech recognition. 
6.1 Integrating DA Modeling and ASR 
Constraints on the word sequences hypothesized by a recognizer are expressed prob- 
abilistically in the recognizer language model (LM). It provides the prior distribution 
P(Wi) for finding the a posteriori most probable hypothesized words for an utterance, 
given the acoustic evidence Ai (Bahl, Jelinek, and Mercer 1983): 7
W 7 = argmaxP(WilAi) 
wi 
P(Wi)P(AilWi) = argmax 
wi P(Ai) 
= argmaxP(Wi)P(AilWi) (10) 
wi 
The likelihoods P(AilWi) are estimated by the recognizer's acoustic model. In a stan- 
dard recognizer the language model P(Wi) is the same for all utterances; the idea here 
is to obtain better-quality LMs by conditioning on the DA type Ui, since presumably 
the word distributions differ depending on DA type. 
W7 -- argmaxP(WilAi, Ui) 
wi 
P( WilUi)P(AilWi, Ui) = argmax 
Wi P(AiIUi) 
argmaxP(WilUi)P(AirWi) (11) 
wi 
As before in the DA classification model, we tacitly assume that the words Wi depend 
only on the DA of the current utterance, and also that the acoustics are independent of
the DA type if the words are fixed. The DA-conditioned language models P(Wil Ui) are 
readily trained from DA-specific training data, much as we did for DA classification 
from words. 8 
7 Note the similarity of Equations 10 and 1. They are identical except for the fact that we are now 
operating at the level of an individual utterance, the evidence isgiven by the acoustics, and the targets 
are word hypotheses instead of DA hypotheses. 
8 In Equation 11 and elsewhere in this section we gloss over the issue of proper weighting of model 
probabilities, which is extremely important in practice. The approach explained in detail in footnote 5
applies here as well. 
359 
Computational Linguistics Volume 26, Number 3 
The problem with applying Equation 11, of course, is that the DA type Ui is 
generally not known (except maybe in applications where the user interface can be 
engineered to allow only one kind of DA for a given utterance). Therefore, we need 
to infer the likely DA types for each utterance, using available evidence E from the 
entire conversation. This leads to the following formulation: 
W~ = argmaxP(WilAi, E)
wi 
---- argmax ~-~ P(WilAi, Ui, E)P(UilE) 
Wi Ui 
argmax ~\[\] P( WiiAi, Ui)P( Ui\[E) 
W~ U~ 
(12) 
The last step in Equation 12 is justified because, as shown in Figures 1 and 4, the 
evidence E (acoustics, prosody, words) pertaining to utterances other than i can affect 
the current utterance only through its DA type Ui. 
We call this the mixture-of-posteriors approach, because it amounts to a mixture of 
the posterior distributions obtained from DA-specific speech recognizers (Equation 11), 
using the DA posteriors as weights. This approach is quite expensive, however, as it 
requires multiple full recognizer or rescoring passes of the input, one for each DA 
type. 
A more efficient, though mathematically ess accurate, solution can be obtained 
by combining uesses about the correct DA types directly at the level of the LM. We 
estimate the distribution of likely DA types for a given utterance using the entire 
conversation E as evidence, and then use a sentence-level mixture (Iyer, Ostendorf, 
and Rohlicek 1994) of DA-specific LMs in a single recognizer run. In other words, we 
replace P(WilUi) in Equation 11 with 
~_~ P(WilUi)P(Ui\]E), 
ui 
a weighted mixture of all DA-specific LMs. We call this the mixture-of-LMs ap- 
proach. In practice, we would first estimate DA posteriors for each utterance, us- 
ing the forward-backward algorithm and the models described in Section 5, and then 
rerecognize the conversation or rescore the recognizer output, using the new posterior- 
weighted mixture LM. Fortunately, as shown in the next section, the mixture-of-LMs 
approach seems to give results that are almost identical to (and as good as) the mixture- 
of-posteriors approach. 
6.2 Computational Structure of Mixture Modeling 
It is instructive to compare the expanded scoring formulas for the two DA mixture 
modeling approaches for ASK The mixture-of-posteriors approach yields 
P(WilAi, E) = ~ P(ailui) 
ui 
(13) 
whereas the mixture-of-LMs approach gives 
) P(A,Iw,) 
P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) 
360 
Stolcke t al. Dialogue Act Modeling 
Table 11 
Switchboard word recognition error rates and 
LM perplexities. 
Model WER (%) Perplexity 
Baseline 41.2 76.8 
1-best LM 41.0 69.3 
Mixture-of-posteriors 41.0 n/a 
Mixture-of-LMs 40.9 66.9 
Oracle LM 40.3 66.8 
We see that the second equation reduces to the first under the crude approximation 
P(Ai\] Ui) ~ P(Ai). In practice, the denominators are computed by summing the numer- 
ators over a finite number of word hypotheses Wi, so this difference translates into 
normalizing either after or before summing over DAs. When the normalization takes 
place as the final step it can be omitted for score maximization purposes; this shows 
why the mixture-of-LMs approach is less computationally expensive. 
6.3 Experiments and Results 
We tested both the mixture-of-posteriors and the mixture-of-LMs approaches on our 
Switchboard test set of 19 conversations. Instead of decoding the data from scratch 
using the modified models, we manipulated n-best lists consisting of up to 2,500 best 
hypotheses for each utterance. This approach is also convenient since both approaches 
require access to the full word string for hypothesis scoring; the overall model is no 
longer Markovian, and is therefore inconvenient to use in the first decoding stage, or 
even in lattice rescoring. 
The baseline for our experiments was obtained with a standard backoff trigram 
language model estimated from all available training data. The DA-specific language 
models were trained on word transcripts of all the training utterances of a given type, 
and then smoothed further by interpolating them with the baseline LM. Each DA- 
specific LM used its own interpolation weight, obtained by minimizing the perplexity 
of the interpolated model on held-out DA-specific training data. Note that this smooth- 
ing step is helpful when using the DA-specific LMs for word recognition, but not for 
DA classification, since it renders the DA-specific LMs less discriminative. 9 
Table 11 summarizes both the word error rates achieved with the various models 
and the perplexities of the corresponding LMs used in the rescoring (note that per- 
plexity is not meaningful in the mixture-of-posteriors approach). For comparison, we 
also included two additional models: the 'q-best LM" refers to always using the DA- 
specific LM corresponding to the most probable DA type for each utterance. It is thus 
an approximation to both mixture approaches where only the top DA is considered. 
Second, we included an "oracle LM," i.e., always using the LM that corresponds to 
the hand-labeled DA for each utterance. The purpose of this experiment was to give us 
an upper bound on the effectiveness of the mixture approaches, by assuming perfect 
DA recognition. 
It was somewhat disappointing that the word error rate (WER) improvement in
the oracle experiment was small (2.2% relative), even though statistically highly sig- 
nificant (p < .0001, one-tailed, according to a Sign test on matched utterance pairs). 
9 Indeed, during our DA classification experiments, wehad observed that smoothed DA-specific LMs 
yield lower classification accuracy. 
361 
Computational Linguistics Volume 26, Number 3 
Table 12 
Word error reductions through DA oracle, by DA type. 
Dialogue Act Baseline WER Oracle WER WER Reduction 
NO-ANSWER 29.4 11.8 -17.6 
BACKCHANNEL 25.9 18.6 -7.3 
BACKCHANNEL-QUESTION 15.2 9.1 -6.1 
ABANDONED/UNINTERPRETABLE 48.9 45.2 -3.7 
WH-QUESTION 38.4 34.9 -3.5 
YES-No-QUESTION 55.5 52.3 --3.2 
STATEMENT 42.0 41.5 --0.5 
OPINION 40.8 40.4 --0.4 
Other 8% 
onded/Uninterpretable 3% 
kchannel 3% 
as-No-Question 3% 
Statement 53% 
Dpinion 30% 
Figure 5 
Relative contributions to test set word counts by DA type. 
The WER reduction achieved with the mixture-of-LMs approach did not achieve sta- 
tistical significance (0.25 > p > 0.20). The 1-best DA and the two mixture models 
also did not differ significantly on this test set. In interpreting these results one must 
realize, however, that WER results depend on a complex combination of factors, most 
notably interaction between language models and the acoustic models. Since the ex- 
periments only varied the language models used in rescoring, it is also informative to 
compare the quality of these models as reflected by perplexity. On this measure, we 
see a substantial 13% (relative) reduction, which is achieved by both the oracle and 
the mixture-of-LMs. The perplexity reduction for the 1-best LM is only 9.8%, showing 
the advantage of the mixture approach. 
To better understand the lack of a more substantial reduction in word error, we an- 
alyzed the effect of the DA-conditioned rescoring on the individual DAs, i.e., grouping 
the test utterances by their true DA types. Table 12 shows the WER improvements for 
a few DA types, ordered by the magnitude of improvement achieved. As shown, all 
frequent DA types saw improvement, but the highest wins were observed for typically 
short DAs, such as ANSWERS and BACKCHANNELS. This is to be expected, as such DAs 
tend to be syntactically and lexically highly constrained. Furthermore, the distribution 
of number of words across DA types is very uneven (Figure 5). STATEMENTS and 
OPINIONS, the DA types dominating in both frequency and number of words (83% of 
total), see no more than 0.5% absolute improvement, thus explaining the small overall 
improvement. In hindsight, this is also not surprising, since the bulk of the training 
data for the baseline LM consists of these DAs, allowing only little improvement in
362 
Stolcke et al Dialogue Act Modeling 
the DA-specific LMs. A more detailed analysis of the effect of DA modeling on speech 
recognition errors can be found elsewhere (Van Ess-Dykema nd Ries 1998). 
In summary, our experiments confirmed that DA modeling can improve word 
recognition accuracy quite substantially in principle, at least for certain DA types, 
but that the skewed distribution of DAs (especially in terms of number of words per 
type) limits the usefulness of the approach on the Switchboard corpus. The benefits 
of DA modeling might therefore be more pronounced on corpora with more even 
DA distribution, as is typically the case for task-oriented ialogues. Task-oriented 
dialogues might also feature specific subtypes of general DA categories that might 
be constrained by discourse. Prior research on task-oriented dialogues ummarized in 
the next section, however, has also found only small reductions in WER (on the order 
of 1%). This suggests that even in task-oriented domains more research is needed to 
realize the potential of DA modeling for ASR. 
7. Prior and Related Work 
As indicated in the introduction, our work builds on a number of previous efforts 
in computational discourse modeling and automatic discourse processing, most of 
which occurred over the last half-decade. It is generally not possible to directly com- 
pare quantitative results because of vast differences in methodology, tag set, type and 
amount of training data, and, principally, assumptions made about what information 
is available for "free" (e.g., hand-transcribed versus automatically recognized words, 
or segmented versus unsegmented utterances). Thus, we will focus on the conceptual 
aspects of previous research efforts, and while we do offer a summary of previous 
quantitative results, these should be interpreted as informative datapoints only, and 
not as fair comparisons between algorithms. 
Previous research on DA modeling has generally focused on task-oriented ia- 
logue, with three tasks in particular garnering much of the research effort. The Map 
Task corpus (Anderson et al 1991; Bard et al 1995) consists of conversations between 
two speakers with slightly different maps of an imaginary territory. Their task is to 
help one speaker eproduce a route drawn only on the other speaker's map, all with- 
out being able to see each other's maps. Of the DA modeling algorithms described 
below, Taylor et al (1998) and Wright (1998) were based on Map Task. The VERBMO- 
BIL corpus consists of two-party scheduling dialogues. A number of the DA m6deling 
algorithms described below were developed for VERBMOBIL, including those of Mast 
et al (1996), Warnke et al (1997), Reithinger et al (1996), Reithinger and Klesen (1997), 
and Samuel, Carberry, and Vijay-Shanker (1998). The ATR Conference corpus is a sub- 
set of a larger ATR Dialogue database consisting of simulated dialogues between a
secretary and a questioner at international conferences. Researchers using this corpus 
include Nagata (1992), Nagata and Morimoto (1993, 1994), and Kita et al (1996). Ta- 
ble 13 shows the most commonly used versions of the tag sets from those three tasks. 
As discussed earlier, these domains differ from the Switchboard corpus in being 
task-oriented. Their tag sets are also generally smaller, but some of the same problems 
of balance occur. For example, in the Map Task domain, 33% of the words occur in 1 
of the 12 DAs 0NSTRUCT). Table 14 shows the approximate size of the corpora, the tag 
set, and tag estimation accuracy rates for various recent models of DA prediction. The 
results summarized in the table also illustrate the differences in inherent difficulty of 
the tasks. For example, the task of Warnke et al (1997) was to simultaneously segment 
and tag DAs, whereas the other results rely on a prior manual segmentation. Similarly, 
the task in Wright (1998) and in our study was to determine DA types from speech 
input, whereas work by others is based on hand-transcribed textual input. 
363 
Computational Linguistics Volume 26, Number 3 
Table 13 
Dialogue act tag sets used in three other extensively studied corpora. 
VERBMOBIL. These 18 high-level DAs used in VERBMOBIL-1 are 
abstracted over a total of 43 more specific DAs; most experiments on 
VERBMOBIL DAs use the set of 18 rather than 43. Examples are from 
Jekat et al (1995). 
Tag Example 
THANK 
GREET 
INTRODUCE 
BYE 
REQUEST~COMMENT 
SUGGEST 
REJECT 
ACCEPT 
REQUEST-SUGGEST 
INIT 
GIVE_REASON 
FEEDBACK 
DELIBERATE 
CONFIRM 
CLARIFY 
DIGRESS 
MOTIVATE 
GARBAGE 
Thanks 
Hello Dan 
It's me again 
Alright bye 
How does that look? 
from thirteenth through seventeenth June 
No Friday I'm booked all day 
Saturday sounds fine, 
What is a good day of the week for you? 
I wanted to make an appointment with you 
Because I have meetings all afternoon 
Okay 
Let me check my calendar here 
Okay, that would be wonderful 
Okay, do you mean Tuesday the 23rd? 
\[we could meet for lunch\] and eat lots of ice cream 
We should go to visit our subsidiary in Munich 
Oops, I- 
Maptask. The 12 DAs or "move types" used in Map Task. Examples are 
from Taylor et al (1998). 
Tag Example 
INSTRUCT 
EXPLAIN 
ALIGN 
CHECK 
QUERY-YN 
QUERY-W 
ACKNOWLEDGE 
CLARIFY 
REPLY-Y 
REPLY-N 
REPLY-W 
READY 
Go round, ehm horizontally underneath diamond mine 
I don't have a ravine 
Okay? 
So going down to Indian Country? 
Have you got the graveyard written down ? 
In where? 
Okay 
{you want to go. . .  diagonally} Diagonally down 
I do. 
No, I don't 
{And across to?} The pyramid. 
Okay 
ATR. The 9 DAs ("illocutionary force types") used in the ATR Dialogue 
database task; some later models used an extended set of 15 DAs. 
Examples are from the English translations given by Nagata (1992). 
Tag Example 
PHATIC 
EXPRESSIVE 
RESPONSE 
PROMISE 
REQUEST 
INFORM 
QUESTIONIP 
QUESTIONREF 
QUESTIONCONF 
Hello 
Thank you 
That's right 
I will send you a registration form 
Please go to Kitaooji station by subway 
We are not giving any discount his time 
Do you have the announcement of the conference ? 
What should I do? 
You have already transferred the registration fee, right ? 
364 
Stolcke et al D ia logue Act  Mode l ing  
,.0 
C 
~'~ o 
% > 
.~  to 
.~~ ~ 
g,..l 
m m ~ 
c~8 ,~.~ ~ 
c ~ Z 
~m 
.~  K 
, -~  
NS~ 
~ ~.~ 
?? l l  ~ ~  
v 
r--~ ~ , O', ?~ 
365 
Computational Linguistics Volume 26, Number 3 
The use of n-grams to model the probabilities of DA sequences, or to predict 
upcoming DAs on-line, has been proposed by many authors. It seems to have been 
first employed by Nagata (1992), and in follow-up papers by Nagata and Morimoto 
(1993, 1994) on the ATR Dialogue database. The model predicted upcoming DAs by 
using bigrams and trigrams conditioned on preceding DAs, trained on a corpus of 
2,722 DAs. Many others subsequently relied on and enhanced this n-grams-of-DAs 
approach, often by applying standard techniques from statistical language modeling. 
Reithinger et al (1996), for example, used deleted interpolation tosmooth the dialogue 
n-grams. Chu-Carroll (1998) uses knowledge of subdialogue structure to selectively 
skip previous DAs in choosing conditioning for DA prediction. 
Nagata and Morimoto (1993, 1994) may also have been the first to use word n- 
grams as a miniature grammar for DAs, to be used in improving speech recognition. 
The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnke 
et al (1997), Reithinger and Klesen (1997), and Taylor et al (1998) all use variants of 
backoff, interpolated, orclass n-gram language models to estimate DA likelihoods. Any 
kind of sufficiently powerful, trainable language model could perform this function, of 
course, and indeed Alexandersson and Reithinger (1997) propose using automatically 
learned stochastic context-free grammars. Jurafsky, Shriberg, Fox, and Curl (1998) show 
that the grammar of some DAs, such as appreciations, can be captured by finite-state 
automata over part-of-speech tags. 
N-gram models are likelihood models for DAs, i.e., they compute the conditional 
probabilities of the word sequence given the DA type. Word-based posterior probability 
estimators are also possible, although less common. Mast et al (1996) propose the use 
of semantic lassification trees, a kind of decision tree conditioned on word patterns 
as features. Finally, Ries (1999a) shows that neural networks using only unigram fea- 
tures can be superior to higher-order n-gram DA models. Warnke et al (1999) and 
Ohler, Harbeck, and Niemann (1999) use related discriminative training algorithms 
for language models. 
Woszczyna nd Waibel (1994) and Suhm and Waibel (1994), followed by Chu- 
Carroll (1998), seem to have been the first to note that such a combination of word 
and dialogue n-grams could be viewed as a dialogue HMM with word strings as 
the observations. (Indeed, with the exception of Samuel, Carberry, and Vijay-Shanker 
(1998), all models listed in Table 14 rely on some version of this HMM metaphor.) 
Some researchers explicitly used HMM induction techniques to infer dialogue gram- 
mars. Woszczyna nd Waibel (1994), for example, trained an ergodic HMM using 
expectation-maximization o model speech act sequencing. Kita et al (1996) made 
one of the few attempts at unsupervised iscovery of dialogue structure, where a 
finite-state grammar induction algorithm is used to find the topology of the dialogue 
grammar. 
Computational pproaches to prosodic modeling of DAs have aimed to auto- 
matically extract various prosodic parameters--such as duration, pitch, and energy 
patterns--from the speech signal (Yoshimura et al \[1996\]; Taylor et al \[1997\]; Kompe 
\[1997\], among others). Some approaches model F0 patterns with techniques such as 
vector quantization and Gaussian classifiers to help disambiguate utterance types. An 
extensive comparison of the prosodic DA modeling literature with our work can be 
found in Shriberg et al (1998). 
DA modeling has mostly been geared toward automatic DA classification, and 
much less work has been done on applying DA models to automatic speech recog- 
nition. Nagata and Morimoto (1994) suggest conditioning word language models on 
DAs to lower perplexity. Suhm and Waibel (1994) and Eckert, Gallwitz, and Niemann 
(1996) each condition a recognizer LM on left-to-right DA predictions and are able to 
366 
Stolcke et al Dialogue Act Modeling 
show reductions in word error rate of 1% on task-oriented corpora. Most similar to 
our own work, but still in a task-oriented omain, the work by Taylor et al (1998) 
combines DA likelihoods from prosodic models with those from 1-best recognition 
output o condition the recognizer LM, again achieving an absolute reduction in word 
error rate of 1%, as disappointing as the 0.3% improvement in our experiments. 
Related computational tasks beyond DA classification and speech recognition have 
received even less attention to date. We already mentioned Warnke et al (1997) and 
Finke et al (1998), who both showed that utterance segmentation a d classification can 
be integrated into a single search process. Fukada et al (1998) investigate augmenting 
DA tagging with more detailed semantic "concept" tags, as a preliminary step toward 
an interlingua-based dialogue translation system. Levin et al (1999) couple DA clas- 
sification with dialogue game classification; dialogue games are units above the DA 
level, i.e., short DA sequences such as question-answer pairs. 
All the work mentioned so far uses statistical models of various kinds. As we have 
shown here, such models offer some fundamental dvantages, uch as modularity and 
composability (e.g., of discourse grammars with DA models) and the ability to deal 
with noisy input (e.g., from a speech recognizer) in a principled way. However, many 
other classifier architectures are applicable to the tasks discussed, in particular to DA 
classification. A nonprobabilistic approach for DA labeling proposed by Samuel, Car- 
berry, and Vijay-Shanker (1998) is transformation-based l arning (Brill 1993). Finally 
it should be noted that there are other tasks with a mathematical structure similar to 
that of DA tagging, such as shallow parsing for natural anguage processing (Munk 
1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which 
further techniques could be borrowed. 
How does the approach presented here differ from these various earlier models, 
particularly those based on HMMs? Apart from corpus and tag set differences, our 
approach differs primarily in that it generalizes the simple HMM approach to cope 
with new kinds of problems, based on the Bayes network representations depicted in 
Figures 2 and 4. For the DA classification task, our framework allows us to do classifi- 
cation given unreliable words (by marginalizing over the possible word strings corre- 
sponding to the acoustic input) and given nonlexical (e.g., prosodic) evidence. For the 
speech recognition task, the generalized model gives a clean probabilistic framework 
for conditioning word probabilities on the conversation context via the underlying DA 
structure. Unlike previous models that did not address peech recognition or relied 
only on an intuitive 1-best approximation, our model allows computation of the opti- 
mum word sequence by effectively summing over all possible DA sequences as well 
as all recognition hypotheses throughout the conversation, using evidence from both 
past and future. 
8. D iscuss ion and Issues for Future Research 
Our approach to dialogue modeling has two major components: tatistical dialogue 
grammars modeling the sequencing of DAs, and DA likelihood models expressing 
the local cues (both lexical and prosodic) for DAs. We made a number of significant 
simplifications to arrive at a computationally and statistically tractable formulation. 
In this formulation, DAs serve as the hinges that join the various model components, 
but also decouple these components through statistical independence assumptions. 
Conditional on the DAs, the observations across utterances are assumed to be inde- 
pendent, and evidence of different kinds from the same utterance (e.g., lexical and 
prosodic) is assumed to be independent. Finally, DA types themselves are assumed 
to be independent beyond a short span (corresponding to the order of the dialogue 
367 
Computational Linguistics Volume 26, Number 3 
n-gram). Further research within this framework can be characterized by which of 
these simplifications are addressed. 
Dialogue grammars for conversational speech need to be made more aware of the 
temporal properties of utterances. For example, we are currently not modeling the fact 
that utterances by the conversants may actually overlap (e.g., backchannels interrupt- 
ing an ongoing utterance). In addition, we should model more of the nonlocal aspects 
of discourse structure, despite our negative results so far. For example, a context-free 
discourse grammar could potentially account for the nested structures proposed in 
Grosz and Sidner (1986). 1? 
The standard n-gram models for DA discrimination with lexical cues are probably 
suboptimal for this task, simply because they are trained in the maximum likelihood 
framework, without explicitly optimizing discrimination between DA types. This may 
be overcome by using discriminative training procedures (Warnke et al 1999; Ohler, 
Harbeck, and Niemann 1999). Training neural networks directly with posterior prob- 
ability (Ries 1999a) seems to be a more principled approach and it also offers much 
easier integration with other knowledge sources. Prosodic features, for example, can 
simply be added to the lexical features, allowing the model to capture dependencies 
and redundancies across knowledge sources. Keyword-based techniques from the field 
of message classification should also be applicable here (Rose, Chang, and Lippmann 
1991). Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodic 
cues into a single model, e.g., one that predicts the next DA based on DA history and 
all the local evidence. 
The study of automatically extracted prosodic features for DA modeling is likewise 
only in its infancy. Our preliminary experiments with neural networks have shown that 
small gains are obtainable with improved statistical modeling techniques. However, 
we believe that more progress can be made by improving the underlying features 
themselves, in terms of both better understanding of how speakers use them, and 
ways to reliably extract hem from data. 
Regarding the data itself, we saw that the distribution of DAs in our corpus limits 
the benefit of DA modeling for lower-level processing, in particular speech recognition. 
The reason for the skewed distribution was in the nature of the task (or lack thereof) in 
Switchboard. It remains to be seen if more fine-grained DA distinctions can be made 
reliably in this corpus. However, it should be noted that the DA definitions are really 
arbitrary as far as tasks other than DA labeling are concerned. This suggests using 
unsupervised, self-organizing learning schemes that choose their own DA definitions 
in the process of optimizing the primary task, whatever it may be. Hand-labeled DA 
categories may still serve an important role in initializing such an algorithm. 
We believe that dialogue-related tasks have much to benefit from corpus-driven, 
automatic learning techniques. To enable such research, we need fairly large, stan- 
dardized corpora that allow comparisons over time and across approaches. Despite 
its shortcomings, the Switchboard omain could serve this purpose. 
9. Conclusions 
We have developed an integrated probabilistic approach to dialogue act modeling for 
conversational speech, and tested it on a large speech corpus. The approach combines 
models for lexical and prosodic realizations of DAs, as well as a statistical discourse 
10 The inadequacy of n-gram models for nested discourse structures i  pointed out by Chu-Carroll (1998), 
although the suggested solution is a modified n-gram approach. 
368 
Stolcke et al Dialogue Act Modeling 
grammar. All components of the model are automatically trained, and are thus appli- 
cable to other domains for which labeled data is available. Classification accuracies 
achieved so far are highly encouraging, relative to the inherent difficulty of the task as 
measured by human labeler performance. We investigated several modeling alterna- 
tives for the components of the model (backoff n-grams and maximum entropy models 
for discourse grammars, decision trees and neural networks for prosodic lassification) 
and found performance largely independent of these choices. Finally, we developed a
principled way of incorporating DA modeling into the probability model of a contin- 
uous speech recognizer, by constraining word hypotheses using the discourse context. 
However, the approach gives only a small reduction in word error on our corpus, 
which can be attributed to a preponderance of a single dialogue act type (statements). 
Note 
The research described here is based on a 
project at the 1997 Workshop on Innovative 
Techniques in LVCSR at the Center for Speech 
and Language Processing at Johns Hopkins 
University (Jurafsky et al 1997; Jurafsky et 
al. 1998). The DA-labeled Switchboard tran- 
scripts as well as other project-related publi- 
cations are available at http://www.colorado. 
edu/ling/jurafsky/ws97/. 
Acknowledgments 
We thank the funders, researchers, and 
support staff of the 1997 Johns Hopkins 
Summer Workshop, especially Bill Byrne, 
Fred Jelinek, Harriet Nock, Joe Picone, 
Kimberly Shiring, and Chuck Wooters. 
Additional support came from the NSF via 
grants IRI-9619921 and IRI-9314967, and 
from the UK Engineering and Physical 
Science Research Council (grant 
GR/J55106). Thanks to Mitch Weintraub, to 
Susann LuperFoy, Nigel Ward, James Allen, 
Julia Hirschberg, and Marilyn Walker for 
advice on the design of the SWBD-DAMSL 
tag set, to the discourse labelers at CU 
Boulder (Debra Biasca, Marion Bond, Traci 
Curl, Anu Erringer, Michelle Gregory, Lori 
Heintzelman, Taimi Metzler, and Amma 
Oduro) and the intonation labelers at the 
University of Edinburgh (Helen Wright, 
Kurt Dusterhoff, Rob Clark, Cassie Mayo, 
and Matthew Bull). We also thank Andy 
Kehler and the anonymous reviewers for 
valuable comments on a draft of this paper. 
References 
Alexandersson, Jan and Norbert Reithinger. 
1997. Learning dialogue structures from a 
corpus. In G. Kokkinakis, N. Fakotakis, 
and E. Dermatas, editors, Proceedings ofthe 
5th European Conference on Speech 
Communication a d Technology, volume 4, 
pages 2,231-2,234. Rhodes, Greece, 
September. 
Anderson, Anne H., Miles Bader, Ellen G. 
Bard, Elizabeth H. Boyle, Gwyneth M. 
Doherty, Simon C. Garrod, Stephen D. 
Isard, Jacqueline C. Kowtko, Jan M. 
McAllister, Jim Miller, Catherine F. Sotillo, 
Henry S. Thompson, and Regina Weinert. 
1991. The HCRC Map Task corpus. 
Language and Speech, 34(4):351-366. 
Austin, J. L. 1962. How to do Things with 
Words. Clarendon Press, Oxford. 
Bahl, Lalit R., Frederick Jelinek, and 
Robert L. Mercer. 1983. A maximum 
likelihood approach to continuous speech 
recognition. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 
5(2):179-190, March. 
Bard, Ellen G., Catherine Sotillo, Anne H. 
Anderson, and M. M. Taylor. 1995. The 
DCIEM Map Task corpus: Spontaneous 
dialogues under sleep deprivation and 
drug treatment. In Isabel Trancoso and 
Roger Moore, editors, Proceedings ofthe 
ESCA-NATO Tutorial and Workshop on 
Speech under Stress, pages 25-28, Lisbon, 
September. 
Baum, Leonard E., Ted Petrie, George 
Soules, and Norman Weiss. 1970. A 
maximization technique occurring in the 
statistical analysis of probabilistic 
functions in Markov chains. The Annals of 
Mathematical Statistics, 41(1):164-171. 
Berger, Adam L., Stephen A. Della Pietra, 
and Vincent J. Della Pietra. 1996. A 
maximum entropy approach to natural 
language processing. Computational 
Linguistics, 22(1):39-71. 
Bourlard, Herv6 and Nelson Morgan. 1993. 
Connectionist Speech Recognition. A Hybrid 
Approach. Kluwer Academic Publishers, 
Boston, MA. 
Breiman, L., J. H. Friedman, R. A. Olshen, 
and C. J. Stone. 1984. Classification and 
Regression Trees. Wadsworth and Brooks, 
Pacific Grove, CA. 
369 
Computational Linguistics Volume 26, Number 3 
Bridle, J. S. 1990. Probabilistic interpretation 
of feedforward classification etwork 
outputs, with relationships to statistical 
pattern recognition. In F. Fogleman Soulie 
and J. Herault, editors, Neurocomputing: 
Algorithms, Architectures and Applications. 
Springer, Berlin, pages 227-236. 
Brill, Eric. 1993. Automatic grammar 
induction and parsing free text: A 
transformation-based approach. In 
Proceedings ofthe ARPA Workshop on Human 
Language Technology, Plainsboro, NJ, 
March. 
Carletta, Jean. 1996. Assessing agreement on 
classification tasks: The Kappa statistic. 
Computational Linguistics, 22(2):249-254. 
Carlson, Lari. 1983. Dialogue Games: An 
Approach to Discourse Analysis. D. Reidel. 
Chu-Carroll, Jennifer. 1998. A statistical 
model for discourse act recognition in 
dialogue interactions. In Jennifer 
Chu-Carroll and Nancy Green, editors, 
Applying Machine Learning to Discourse 
Processing. Papers from the 1998 AAAI 
Spring Symposium. Technical Report 
SS-98-01, pages 12-17. AAAI Press, Menlo 
Park, CA. 
Church, Kenneth Ward. 1988. A stochastic 
parts program and noun phrase parser 
for unrestricted text. In Second Conference 
on Applied Natural Language Processing, 
pages 136-143, Austin, TX. 
Core, Mark and James Allen. 1997. Coding 
dialogs with the DAMSL annotation 
scheme. In Working Notes of the AAAI Fall 
Symposium on Communicative Action in 
Humans and Machines, pages 28-35, 
Cambridge, MA, November. 
Dermatas, Evangelos and George 
Kokkinakis. 1995. Automatic stochastic 
tagging of natural anguage texts. 
Computational Linguistics, 21(2):137-163. 
Eckert, Wieland, Florian Gallwitz, and 
Heinrich Niemann. 1996. Combining 
stochastic and linguistic language models 
for recognition of spontaneous speech. In 
Proceedings ofthe IEEE Conference on 
Acoustics, Speech, and Signal Processing, 
volume 1, pages 423-426, Atlanta, GA, 
May. 
Finke, Michael, Maria Lapata, Alon Lavie, 
Lori Levin, Laura Mayfield Tomokiyo, 
Thomas Polzin, Klaus Ries, Alex Waibel, 
and Klaus Zechner. 1998. Clarity: 
Inferring discourse structure from speech. 
In Jennifer Chu-Carroll and Nancy Green, 
editors, Applying Machine Learning to 
Discourse Processing. Papers from the 1998 
AAAI Spring Symposium. Technical Report 
SS-98-01, pages 25-32. AAAI Press, Menlo 
Park, CA. 
Fowler, Carol A. and Jonathan Housum. 
1987. Talkers' signaling of "new" and 
"old" words in speech and listeners' 
perception and use of the distinction. 
Journal of Memory and Language, 26:489-504. 
Fukada, Toshiaki, Detlef Koll, Alex Waibel, 
and Kouichi Tanigaki. 1998. Probabilistic 
dialogue act extraction for concept based 
multilingual translation systems. In 
Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 6, pages 2,771-2,774, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Godfrey, J. J., E. C. Holliman, and 
J. McDaniel. 1992. SWITCHBOARD: 
Telephone speech corpus for research and 
development. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 517-520, San 
Francisco, CA, March. 
Grosz, Barbara J. and Candace L. Sidner. 
1986. Attention, intention, and the 
structure of discourse. Computational 
Linguistics, 12(3):175-204. 
Hirschberg, Julia B. and Diane J. Litman. 
1993. Empirical studies on the 
disambiguation of cue phrases. 
Computational Linguistics, 19(3):501-530. 
Iyer, Rukmini, Mari Ostendorf, and J. Robin 
Rohlicek. 1994. Language modeling with 
sentence-level mixtures. In Proceedings of
the ARPA Workshop on Human Language 
Technology, pages 82-86, Plainsboro, NJ, 
March. 
Jefferson, Gail. 1984. Notes on a systematic 
deployment of the acknowledgement 
tokens 'yeah' and 'mm hm'. Papers in 
Linguistics, 17:197-216. 
Jekat, Susanne, Alexandra Klein, Elisabeth 
Maier, Ilona Maleck, Marion Mast, and 
Joachim Quantz. 1995. Dialogue acts in 
VERBMOBIL. Verbmobil-Report 65, 
Universit~it Hamburg, DFKI GmbH, 
Universit~it Erlangen, and TU Berlin, 
April. 
Jurafsky, Dan, Rebecca Bates, Noah Coccaro, 
Rachel Martin, Marie Meteer, Klaus Ries, 
Elizabeth Shriberg, Andreas Stolcke, Paul 
Taylor, and Carol Van Ess-Dykema. 1997. 
Automatic detection of discourse 
structure for speech recognition and 
understanding. In Proceedings ofthe IEEE 
Workshop on Speech Recognition and 
Understanding, pages 88-95, Santa 
Barbara, CA, December. 
Jurafsky, Daniel, Rebecca Bates, Noah 
Coccaro, Rachel Martin, Marie Meteer, 
Klaus Ries, Elizabeth Shriberg, Andreas 
Stolcke, Paul Taylor, and Carol Van 
370 
Stolcke et al Dialogue Act Modeling 
Ess-Dykema. 1998. Switchboard iscourse 
language modeling project final report. 
Research Note 30, Center for Language 
and Speech Processing, Johns Hopkins 
University, Baltimore, MD, January. 
Jurafsky, Daniel, Elizabeth Shriberg, and 
Debra Biasca. 1997. Switchboard-DAMSL 
Labeling Project Coder's Manual. 
Technical Report 97-02, University of 
Colorado, Institute of Cognitive Science, 
Boulder, CO. http://www.colorado.edu/ 
ling/jurafsky/manual.augustl.html. 
Jurafsky, Daniel, Elizabeth E. Shriberg, 
Barbara Fox, and Traci Curl. 1998. Lexical, 
prosodic, and syntactic ues for dialog 
acts. In Proceedings ofACL/COLING-98 
Workshop on Discourse Relations and 
Discourse Markers, pages 114-120. 
Association for Computational 
Linguistics. 
Katz, Slava M. 1987. Estimation of 
probabilities from sparse data for the 
language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech, and Signal Processing, 35(3):400-401, 
March. 
Kita, Kenji, Yoshikazu Fukui, Masaaki 
Nagata, and Tsuyoshi Morimoto. 1996. 
Automatic acquisition of probabilistic 
dialogue models. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 1, 
pages 196-199, Philadelphia, PA, October. 
Kompe, Ralf. 1997. Prosody in speech 
understanding systems. Springer, Berlin. 
Kowtko, Jacqueline C. 1996. The Function of 
Intonation in Task Oriented Dialogue. Ph.D. 
thesis, University of Edinburgh, 
Edinburgh. 
Kuhn, Roland and Renato de Mori. 1990. A 
cache-base natural anguage model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
12(6):570-583, June. 
Levin, Joan A. and Johanna A. Moore. 1977. 
Dialogue games: Metacommunication 
structures for natural anguage 
interaction. Cognitive Science, 1(4):395-420. 
Levin, Lori, Klaus Ries, Ann Thym~-Gobbel, 
and Alon Lavie. 1999. Tagging of speech 
acts and dialogue games in Spanish 
CallHome. In Towards Standards and Tools 
for Discourse Tagging (Proceedings ofthe ACL 
Workshop at ACL'99), pages 42-47, College 
Park, MD, June. 
Linell, Per. 1990. The power of dialogue 
dynamics. In Ivana Markov~ and Klaus 
Foppa, editors, The Dynamics of Dialogue. 
Harvester, Wheatsheaf, New York, 
London, pages 147-177. 
Mast, M., R. Kompe, S. Harbeck, 
A. Kiel~ling, H. Niemann, E. NOth, E. G. 
Schukat-Talamazzini, and V. Warnke. 
1996. Dialog act classification with the 
help of prosody. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 3, 
pages 1,732-1,735, Philadelphia, PA, 
October. 
Menn, Lise and Suzanne E. Boyce. 1982. 
Fundamental frequency and discourse 
structure. Language and Speech, 25:341-383. 
Meteer, Marie, Ann Taylor, Robert 
MacIntyre, and Rukmini Iyer. 1995. 
Dysfluency annotation stylebook for the 
Switchboard corpus. Distributed by LDC, 
ftp://ftp.cis.upenn.edu/pub/treebank/ 
swbd/doc/DFL-book.ps, February. 
Revised June 1995 by Ann Taylor. 
Morgan, Nelson, Eric Fosler, and Nikki 
Mirghafori. 1997. Speech recognition 
using on-line estimation of speaking rate. 
In G. Kokkinakis, N. Fakotakis, and E. 
Dermatas, editors, Proceedings ofthe 5th 
European Conference on Speech 
Communication a d Technology, volume 4, 
pages 2,079-2,082, Rhodes, Greece, 
September. 
Munk, Marcus. 1999. Shallow Statistical 
Parsing for Machine Translation. Diploma 
thesis, Carnegie Mellon University. 
Nagata, Masaaki. 1992. Using pragmatics to 
rule out recognition errors in cooperative 
task-oriented dialogues. In John J. Ohala, 
Terrance M. Nearey, Bruce L. Derwing, 
Megan M. Hodge, and Grace E. Wiebe, 
editors, Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 1, pages 647-650, Banff, Canada, 
October. 
Nagata, Masaaki and Tsuyoshi Morimoto. 
1993. An experimental statistical dialogue 
model to predict he speech act type of 
the next utterance. In Katsuhiko Shirai, 
Tetsunori Kobayashi, and Yasunari 
Harada, editors, Proceedings ofthe 
International Symposium on Spoken Dialogue, 
pages 83-86, Tokyo, November. 
Nagata, Masaaki and Tsuyoshi Morimoto. 
1994. First steps toward statistical 
modeling of dialogue to predict he 
speech act type of the next utterance. 
Speech Communication, 15:193-203. 
Ohler, Uwe, Stefan Harbeck, and Heinrich 
Niemann. 1999. Discriminative training of 
language model classifiers. In Proceedings 
of the 6th European Conference on Speech 
Communication a d Technology, volume 4, 
pages 1607-1610, Budapest, September. 
371 
Computational Linguistics Volume 26, Number 3 
Pearl, Judea. 1988. Probabilistic Reasoning in 
Intelligent Systems: Networks of Plausible 
Inference. Morgan Kaufmann, San Mateo, 
CA. 
Power, Richard J. D. 1979. The organization 
of purposeful dialogues. Linguistics, 
17:107-152. 
Rabiner, L. R. and B. H. Juang. 1986. An 
introduction to hidden Markov models. 
IEEE ASSP Magazine, 3(1):4-16, January. 
Reithinger, Norbert, Ralf Engel, Michael 
Kipp, and Martin Klesen. 1996. Predicting 
dialogue acts for a speech-to-speech 
translation system. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 2, 
pages 654-657, Philadelphia, PA, October. 
Reithinger, Norbert and Martin Klesen. 
1997. Dialogue act classification using 
language models. In G. Kokkinakis, N. 
Fakotakis, and E. Dermatas, editors, 
Proceedings ofthe 5th European Conference on 
Speech Communication a d Technology, 
volume 4, pages 2,235-2,238, Rhodes, 
Greece, September. 
Ries, Klaus. 1999a. HMM and neural 
network based speech act classification. In
Proceedings ofthe IEEE Conference on 
Acoustics, Speech, and Signal Processing, 
volume 1, pages 497-500, Phoenix, AZ, 
March. 
Ries, Klaus. 1999b. Towards the detection 
and description of textual meaning 
indicators in spontaneous conversations. 
In Proceedings ofthe 6th European Conference 
on Speech Communication a d Technology, 
volume 3, pages 1,415--1,418, Budapest, 
September. 
Rose, R. C., E. I. Chang, and R. P. 
Lippmann. 1991. Techniques for 
information retrieval from voice 
messages. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 317-320, 
Toronto, May. 
Sacks, H., E. A. Schegloff, and G. Jefferson. 
1974. A simplest semantics for the 
organization of turn-taking in 
conversation. Language, 50(4):696-735. 
Samuel, Ken, Sandra Carberry, and 
K. Vijay-Shanker. 1998. Dialogue act 
tagging with transformation-based 
learning. In Proceedings ofthe 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference 
on Computational Linguistics, volume 2, 
pages 1,150-1,156, Montreal. 
Schegloff, Emanuel A. 1968. Sequencing in
conversational openings. American 
Anthropologist, 70:1,075-1,095. 
Schegloff, Emanuel A. 1982. Discourse as an 
interactional chievement: Some uses of 
'uh huh' and other things that come 
between sentences. In Deborah Tannen, 
editor, Analyzing Discourse: Text and Talk. 
Georgetown University Press, 
Washington, D.C., pages 71-93. 
Searle, J. R. 1969. Speech Acts. Cambridge 
University Press, London-New York. 
Shriberg, Elizabeth, Rebecca Bates, Andreas 
Stolcke, Paul Taylor, Daniel Jurafsky, 
Klaus Ries, Noah Coccaro, Rachel Martin, 
Marie Meteer, and Carol Van 
Ess-Dykema. 1998. Can prosody aid the 
automatic lassification of dialog acts in 
conversational speech? Language and 
Speech, 41(3-4):439--487. 
Shriberg, Elizabeth, Andreas Stolcke, Dilek 
Hakkani-Ti~r, and GOkhan Tiir. 2000. 
Prosody-based automatic segmentation f 
speech into sentences and topics. Speech 
Communication, 32(1-2). Special Issue on 
Accessing Information in Spoken Audio. 
To appear. 
Siegel, Sidney and N. John Castellan, Jr. 
1988. Nonparametric Statistics for the 
Behavioral Sciences. Second edition. 
McGraw-Hill, New York. 
Stolcke, Andreas and Elizabeth Shriberg. 
1996. Automatic linguistic segmentation 
of conversational speech. In H. Timothy 
Bunnell and William Idsardi, editors, 
Proceedings ofthe International Conference on 
Spoken Language Processing, volume 2, 
pages 1,005-1,008, Philadelphia, PA, 
October. 
Suhm, B. and A. Waibel. 1994. Toward better 
language models for spontaneous speech. 
In Proceedings ofthe International Conference 
on Spoken Language Processing, volume 2, 
pages 831-834, Yokohama, September. 
Taylor, Paul A. 2000. Analysis and synthesis 
of intonation using the tilt model. Journal 
of the Acoustical Society of America, 
107(3):1,697-1,714. 
Taylor, Paul A., Simon King, Stephen Isard, 
and Helen Wright. 1998. Intonation and 
dialog context as constraints for speech 
recognition. Language and Speech, 
41(3-4):489-508. 
Taylor, Paul A., Simon King, Stephen Isard, 
Helen Wright, and Jacqueline Kowtko. 
1997. Using intonation to constrain 
language models in speech recognition. In 
G. Kokkinakis, N. Fakotakis, and E. 
Dermatas, editors, Proceedings ofthe 5th 
European Conference on Speech 
Communication a d Technology, volume 5, 
pages 2,763-2,766, Rhodes, Greece, 
September. 
372 
Stolcke et al Dialogue Act Modeling 
Van Ess-Dykema, Carol and Klaus Ries. 
1998. Linguistically engineered tools for 
speech recognition error analysis. In 
Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 5, pages 2,091-2,094, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Viterbi, A. 1967. Error bounds for 
convolutional codes and an 
asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, 13:260-269. 
Warnke, Volker, Stefan Harbeck, Elmar 
N0th, Heinrich Niemann, and Michael 
Levit. 1999. Discriminative estimation of 
interpolation parameters for language 
model classifiers. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 525-528, 
Phoenix, AZ, March. 
Warnke, Volker, R. Kompe, Heinrich 
Niemann, and Elmar NOth. 1997. 
Integrated ialog act segmentation a d 
classification using prosodic features and 
language models. In G. Kokkinakis, N. 
Fakotakis, and E. Dermatas, editors, 
Proceedings ofthe 5th European Conference on 
Speech Communication a d Technology, 
volume 1, pages 207-210, Rhodes, Greece, 
September. 
Weber, Elizabeth G. 1993. Varieties of 
Questions in English Conversation. John 
Benjamins, Amsterdam. 
Witten, Ian H. and Timothy C. Bell. 1991. 
The zero-frequency problem: Estimating 
the probabilities of novel events in 
adaptive text compression. IEEE 
Transations on Information Theory, 
37(4):1,085-1,094, July. 
Woszczyna, M. and A. Waibel. 1994. 
Inferring linguistic structure in spoken 
language. In Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 2, pages 847-850, Yokohama, 
September. 
Wright, Helen. 1998. Automatic utterance 
type detection using suprasegmental 
features. In Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 4, pages 1,403-1,406, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Wright, Helen and Paul A. Taylor. 1997. 
Modelling intonational structure using 
hidden Markov models. In Intonation: 
Theory, Models and Applications. Proceedings 
of an ESCA Workshop, pages 333-336, 
Athens, September. 
Yngve, Victor H. 1970. On getting a word in 
edgewise. In Papers from the Sixth Regional 
Meeting of the Chicago Linguistic Society, 
pages 567-577, Chicago, April. University 
of Chicago. 
Yoshimura, Takashi, Satoru Hayamizu, 
Hiroshi Ohmura, and Kazuyo Tanaka. 
1996. Pitch pattern clustering of user 
utterances in human-machine dialogue. In 
H. Timothy Bunnell and William Idsardi, 
editors, Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 2, pages 837-840, Philadelphia, 
PA, October. 
373 

Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 122?129,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
What can NLP tell us about BioNLP?
Attapol Thamrongrattanarit, Michael Shafir, Michael Crivaro, Bensiin Borukhov, Marie Meteer
Department of Computer Science
Brandeis University
Waltham, MA 02453, USA
{tet, mshafir, mcrivaro, bborukhov, mmeteer}@brandeis.edu
Abstract
The goal of this work is to apply NLP tech-
niques to the field of BioNLP in order to gain
a better insight into the field and show connec-
tions and trends that might not otherwise be
apparent. The data we analyzed was the pro-
ceedings from last decade of BioNLP work-
shops. Our findings reveal the prominent re-
search problems and techniques in the field,
their progression over time, the approaches
that researchers are using to solve those prob-
lems, insightful ways to categorize works in
the field, and the prominent researchers and
groups whose works are influencing the field.
1 Introduction
Thanks to improving technology and the discovery
of stronger statistical methods, natural language pro-
cessing techniques have more power than ever to
give us insights into real datasets too large for hu-
mans to efficiently process. In the field of BioNLP,
we see that natural language processing has a wide
range of applications within the medical domain
from analysis of clinical data to literature. With
the increasing amount of publications in this grow-
ing field, building a classification structure is help-
ful both for categorizing papers in a sensible way
and for recognizing the trends that brought the field
to where it is today. Understanding the current na-
ture of the field can show us where the most effort
is needed, while taking a look at where the field has
been can highlight successes and even unanswered
questions.
As the use of NLP in the medical domain has ex-
panded in recent years so has the amount of freely-
available online research. With this wealth of infor-
mation comes a problem, however, as it is not truly
feasible for humans to read through all the research
out there and classify it in a way that will capture the
less-obvious trends and the finer relationships be-
tween seemingly-disconnected works. Instead, we
propose that statistical methods can help us discover
both the most reasonable way to partition the field
and also see how the research has changed over the
past decade. The longer term goal for the work is to
contribute to a ?map? of the field that can be a com-
munity resource, such as www.medlingmap.org, de-
scribed in Meteer, et al (2012).
Schuemie et al (2009) used clustering techniques
to analyze the domain of Medical Informatics. They
processed a large number of Medline abstracts to
find a subset of the journals classified as ?Medical
Informatics? whose content was sufficiently related
to constitute a basis for the field. Using hierarchi-
cal clustering, they determined that such a group of
journals exists and, as we might expect, the rest of
the journals were largely disconnected. They also
used this cluster of journals as the basis for a topic
modeling task. Analyzing the articles from their new
basis of journals, they found three very strong, topic-
based clusters, each comprised of three sub-clusters.
Overall, Schuemie et al (2009) demonstrated how it
is possible to gain a great deal of insight into the na-
ture of a field by using statistical methods over that
field?s literature. More recently, Gupta and Manning
(2011) used automatic methods to tag documents for
?focus,? ?technique,? and ?domain? by examining
122
over 15,000 ACL abstracts. This level of categoriza-
tion is useful because it expands beyond the simple
notion of the ?topic? to implicitly show if a work,
for example, is about an application of named-entity
recognition or if it simply uses NER to achieve a
greater task. The techniques demonstrated by Gupta
and Manning could be very enlightening if applied
to the BioNLP proceedings, though in this paper we
refrain from drawing conclusions about individual
papers. Instead, we will relate them through the top-
ics extracted from the full-text proceedings.
For our task, we look to the ACL and NAACL-
associated workshops on NLP applications in the
medical domain. Entering its 11th year, the BioNLP
workshop (under a variety of names) has given
us ten rich and varied proceedings in addition to
a pair of more focused shared tasks. All in all,
the workshops have produced over 270 unique pa-
pers. Our data of 270 documents was small relative
to (Schuemie et al, 2009) 6.3 million documents;
therefore, we chose to expand our analysis to the
full text of the documents instead of just the ab-
stracts. Additionally, using the full papers allowed
us to capture information about document content
that abstracts alone could not provide.
2 Methods and Results
2.1 Pipeline Architecture
We implemented a document processing pipeline
that would allow our approaches to be generaliz-
able, easily reproducible, and extendable. Each
of our analytic processes was integrated into this
pipeline and parameterized to allow us proper flex-
ibility for empirical experimentation. The pipeline
works by managing the interaction between a con-
figurable set of data layers and a configurable set
of processing stages over those layers. It supports
saving and loading its internal state between stages.
In addition, layers and stages follow specific tem-
plates that reduce the amount of code to write and
maintain. The ordering and activation of each stage
is also parameterized. This pipeline allowed us to
quickly and efficiently experiment with various ap-
proaches and combine them. The sample imple-
mentation of this pipeline is available publicly at
github.com/attapol/mapping bionlp.
Topic proportion
syntax
CRF
biological tasks
bacteria task
hedging
graph
coreference resolution
word/phrase methods
semantic knowledge
clinical data
q&a
entity relations
information retrieval
WSD
lexical categories
clinical coding
parsing
corpus annotation
UMLS
document structure
name normalization
protein interaction
NER
event extraction
classification
event triggers
modeling/training
research
result/analysis
result presentation
0.00 0.05 0.10 0.15
Figure 1: Average topic proportion across all the docu-
ments output by the LDA model
2.2 Preprocessing
The papers from the BioNLP workshop are all avail-
able freely from the ACL Anthology Archive 1. We
first extracted the text from the PDF files using
pdf2text unix tool and then tagged them all for title,
authors, places of origin, abstract, content, and ref-
erences. In all cases, the abstract, content, and refer-
ences were separated automatically using a script,
and the places had to be hand-annotated. Papers
from 2004 onward (starting with the first BioLINK
workshop) have complete BibTeX entries that al-
lowed us to automatically extract the titles and au-
thors, but for 2002 and 2003 this work had to be
done manually. Since we wanted to perform our
analysis solely on the prose of the papers, and not on
any of the numerical data, we filtered out portions of
the text containing elements such as tables, graphs,
footnotes, and URLs. We also filtered out stopwords
(as defined by the NLTK package (Bird and Loper,
2004) for Python).
1aclweb.org
123
2.3 Topic Modeling
Using the Mallet toolkit (McCallum, 2002), we were
able to generate topics from our cleaned data using
the Latent Dirichlet Allocation (LDA) model. This
approach allows us to represent each document as a
vector of topic proportions instead of a bag of words,
which prevents the problem of sparsity. When we
set the number of topics to 30, the system output a
set of distinct topics that seem to describe a range
of tasks and methods within the domain of BioNLP.
The topics generated by the LDA model reflect areas
of study that are being pursued, techniques that are
being applied, and resources that are being consulted
in the field. A list of the generated topics along with
the associated keywords is shown in Table 1 and the
distributions of the topics across the entire document
set is displayed in Figure 1.
Additionally, we found that the topics generated
by LDA were more informative about the full con-
tent of a work than those generated by TF-IDF as
TF-IDF would often give too much weight to spe-
cific examples over general concepts. For exam-
ple, TF-IDF tended to select specific names of re-
sources and ontologies rather than general terms.
For example, it selected ?Frame-net? instead ?ontol-
ogy? and ?RadLex? instead of ?lexicon?. We con-
cluded that, while interesting, TF-IDF results were
not strongly suited for capturing an overall glimpse
of the field. However, we think that TF-IDF can be
much more useful in its more traditional capacity of
finding document-specific keywords; we aim to use
these indices to partially automate keyword genera-
tion for MedlingMap (Meteer et al, 2012), which is
our accompanying project.
2.4 Topic Correlation
While looking at the topic proportions for each of
our LDA topics overall can help us paint a broad
picture of the field, it can also help to look at the
relationship between these topics as they occur in
the documents. Some topics appear highly ranked
in nearly all papers, such as the topic that is char-
acterized by terms such as ?system? and ?results?,
and the topic that includes ?precision? and ?recall?
because they reflect the performance evaluation con-
vention in the field. However, most topics are only
dominant in a small subset of the papers. Some
Topic
Cou
nt
0
2
4
6
8
10
12
14
parsing graph lexical categories semantic knowledge
Figure 2: The bar plot shows the frequency of the co-
occurrences between the event extraction topic and some
of the method-related topics.
topics refer to tasks (e.g. named-entity recognition,
hedging) and others refer to techniques (e.g. CRFs,
parsing). We can look at how often pairs of task-
related topic and method-related topic co-occur to
see if researchers in the community are using certain
techniques in conjunction with solving certain prob-
lems. We first turned a topic proportion vector into a
binary vector where each element indicates which
topic is discussed more extensively than average.
Then, we counted the co-occurrences of tasks and
methods of interest. To demonstrate this, we com-
puted the number of papers that substantially discuss
event extraction in conjunction with parsing, graph,
lexical categories, or semantic knowledge (Figure
2). This topic comparison method provides a means
of visualizing how researchers in the field are ap-
proaching BioNLP problems. It reveals that parsing
and graph-based methods are commonly used in bio-
logical event extraction, while lexical categories and
semantic knowledge are not as central to many of the
approaches to this task. Moving forward, tracking
how these correlations change over time will pro-
vide an insightful reflection of the field?s progress
on the task in a more meaningful way than evalu-
ation scores alone. While a deeper analysis of all
of such trends is beyond the scope of this paper, it
certainly warrants further investigation.
124
Table 1: The resulting topics and their associated keywords generated by LDA model with 30 topics
Topic Name Keywords
Event Extraction event, task, extraction, types, data, annotation
Coreference Resolution anaphora, resolution, referring, links, antecedent
Graph graph, relationships, nodes, edges, path, constraint, semanics
Clinical Coding medical, data, codes, patients, notes, reports
Hedging negation, scope, cues, speculative, hedge, lexical
Clinical Data condition, historical, clinical, temporal, reports, context
Bacteria Task bacteria, names, location, organisms, taxonomic, host, roles, type
Entity Relations relations, entities, feature, static, renaming, annotated, pairs
Document Structure Analysis rst, classification, abstracts, identification, data, terms
Q&A question, answer, structure, passage, evidence, purpose
Event Triggers triggers, dependency, binding, type, training, token, detection
Semantic Knowledge semantic, frame, structures, argument, patterns, domain, types
Protein Interaction protein, patterns, interaction, extraction, biological
Parsing dependency, parser, tree, syntactic, structures, grammar, link
Name Normalization gene, names, dictionary, normalization, protein, database, synonyms
Named Entity Recognition entity, named, word, recognition, features, class, protein
Information Retrieval search, queries, interface, text, retrieval, document
Corpus Annotation corpus, annotation, guidelines, agreement, papers
Lexical Categories semantic, categories, resources, simstring, lexical, gazetteer, features
Research text, figure, knowledge, domain, research, complex, processing
CRF crf, skip, chain, linear, dependency, words, edges, sentence
Result Discussion system, based, results, set, table, test, shown, approach
Biological Tasks species, disease, mutation, mentions, features, entities, acronym
UMLS terms, semantic, phrases, umls, concepts, ontology, corpus
Word/Phrase Methods words, measures, morphological, tag, token, chunking, form
WSD disambiguation, sense, word, semantic, wsd, ambiguous
Result Analysis found, number, precision, recall, cases, high, related, results
Classification features, training, data, classification, set, learning, svm
Modeling/Training training, data, model, tagger, performance, corpus, annotated
Syntax attachment, pps, np, fragments, pp, noun, vp, nos, pattern
2.5 Trends within the subdisciplines in
Biomedical NLP Literature
Our analysis of temporal trends builds on the idea
proposed by (Hall et al, 2008) in their analysis of
the changing trends in the field of computational lin-
guistics over time. In their approach, they attempted,
among other things, to analyze which topics were up
and coming in the field and which were becoming
less popular. Given their sound results, we decided
to perform the same kind of trend analysis over the
BioNLP topics. For many of our 30 topics, there
was little change in the topic frequency over time.
Considering the relative youth of the BioNLP field,
this result is not entirely surprising. We did, how-
ever, find a few topics that have undergone notable
changes in these past ten years, as observable in Fig-
ure 3. In particular, we found that two topics have
seen surges of activity in recent years, whereas there
were three topics that started out strong in the early
years but that have since petered off. The two top-
ics that have gained popularity in the past few years
both involve biomedical events. Specifically, one
such topic is primarily about event extraction tasks,
and the other is about event triggers and the more
fine-grained roles one needs to tag to categorize such
events. The popularity of these two tasks is hardly
surprising, given that they were the focus of the 2009
and 2011 shared tasks which were about working
with events in both general and detailed ways. We
do notice, however, that the growing trends continue
in 2010 as well, when there was no shared task, and
so we can see that events are of great interest in
the field at present even without the added incen-
tive of the shared tasks. It is reasonable to suggest
that the 2009 BioNLP Shared Task in event extrac-
tion generated interest in the topic that continued
through 2010 and 2011. Two more topics originally
saw their popularity rise in the early years, but have
125
Year
To
pic
 pr
op
ort
ion
0.00
0.05
0.10
0.15
Event extraction
O O
O O O O O
O
O
O
2002 2006 2010
Named?entity recognition
O
O
O
O
O
O O
O O O
2002 2006 2010
UMLS
O
O
O
O O O
O O O O
2002 2006 2010
Event triggers
O O O O O O O
O
O
O
2002 2006 2010
Protein interaction
O
O
O
O
O
O
O O
O
O
2002 2006 2010
Figure 3: Topic proportions for some topics have gone through dramatic changes, which reflect how research interest
and methodology evolve over time.
since seen it fade. Each of these is a specific task:
named-entity recognition, which dropped off after
2004, and protein interaction, which saw a sharp de-
cline after 2005. Although a detailed causal analysis
is beyond the scope of this paper, we might wonder
what accounts for these drops in topic proportion.
The explanation that seems most likely is that great
strides were made in these areas early on, but we
have since reached a plateau in advancements. As
such, the research has moved elsewhere. The only
topic to see a steady decrease from the start was the
topic associated with the Unified Medical Language
System. In general, we can view a trend associated
with a resource differently from one associated with
a task. Above, when discussing tasks, we saw where
the research currently has been heading and where it
has been. With a resource, we could consider an up-
ward trend to represent either an increased number
of applications to a task or perhaps an expansion of
the resource itself. In the case of UMLS, the down-
ward trend likely suggests that the field has moved
away from this particular resource, either because it
does not apply as well to newer tasks or because it
has been replaced with something more powerful.
2.6 Cluster Analysis
Our next step with the LDA-generated topics was
to run a k-means clustering algorithm. We used the
same topic proportion vector and a Euclidean met-
ric to create the feature space for clustering. We
used the standard k-means function in the statisti-
cal language R (R Development Core Team, 2010).
The assumption of the LDA model biases each topic
proportion vector to be sparse (Blei et al, 2003), and
this turns out to be true in our data set. Therefore, we
chose the number of clusters to match the number of
topics so that the document space can be partitioned
proportionally to its dimensionality. This clustering
provides us with a useful schema for document clas-
sification within the domain of BioNLP. We can use
the clusters as a guide for how to organize the cur-
rent papers, and we can also view the clusters as a
guide for how to select relevant research to build fu-
ture work on. Clusters bring together related papers
from different research groups and multiple work-
shops, such as those shown in Table 2. In all of these
examples, the selection of these sets of papers sim-
ply based on keyword search would be very difficult,
since many of the key terms are going to be present
in a much larger set of documents.
2.7 Author Relation Analysis
As an additional task, we investigated the connec-
tions between authors in the BioNLP proceedings.
Eggers et al (2005) used a graph to visualize who
was being cited by whom in ISI publications. There,
the hope was to identify which authors worked
within the same subdisciplines by examining clus-
ters within the citation graph. By examining who
cited whom in the BioNLP publications, we hoped
instead to uncover the authors of the most influen-
tial papers, both within our own clusters and outside
the scope of the BioNLP workshops. In our model,
which can be viewed in Figure 4, we constructed a
126
List of papers assigned to the cluster where the most discussed topic is parsing (44.74% on average)
A Comparative Study of Syntactic Parsers for Event Extraction
Analysis of Link Grammar on Biomedical Dependency Corpus Targeted at Protein-Protein Interactions
On the unification of syntactic annotations under the Stanford dependency scheme
A Transformational-based Learner for Dependency Grammars in Discharge Summaries
A Study on Dependency Tree Kernels for Automatic Extraction of Protein-Protein Interaction
List of papers assigned to the cluster where the most discussed topic is clinical data (48.74% on average)
Applying the TARSQI Toolkit to Augment Text Mining of EHRs
Temporal Annotation of Clinical Text
Extracting Distinctive Features of Swine (H1N1) Flu through Data Mining Clinical Documents
ConText: An Algorithm for Identifying Contextual Features from Clinical Text
Distinguishing Historical from Current Problems in Clinical Reports ? Which Textual Features Help?
Table 2: Two sample clusters from running k-means clustering algorithm on the corpus
Figure 4: Citation relation graph. Each node represents an author whose papers are either published in the BioNLP
proceedings or are cited by one of the papers in the proceedings. Each edge represents a citation activity.
directed graph of author citations from the BioNLP
workshops and shared tasks. We disregarded the au-
thor ordering within each paper and gave the same
weights for all authors whose names appear on the
paper. In this graph, a node points to another node if
that author cited the other author at least three times.
Additionally, a white node signifies an author who
published in the BioNLP workshop between 2008
and 2011, whereas a grey node is someone who did
not, but was cited in papers during that time span. As
can be seen in Figure 4 above, which is itself only
a piece of the complete graph, this graph is rather
large and complex, showing us a large degree of in-
terconnectedness and interdependence in the field.
Simply from the density of the lines, we can find
some of the most influential figures, such as Jun?ichi
Tsujii, shown in Region 3 and Yoshimasa Tsuruoka,
shown in Region 2. Unsurprisingly, Tsujii?s node is
bustling with activity, as a very large number of au-
thors cite works with Tsujii as an author, and his own
prolific authorship (or co-authorship) naturally has
him citing a variety of authors. The white nodes near
his own show the authors who published BioNLP
papers and primarily referenced his works, whereas
the grey nodes near his show people who didn?t pub-
lish, but who Tsujii cited in the proceedings multiple
127
times. Thus, proximity can also be very telling in a
graph like this. Since nodes with a heavier reliance
on one another tend to end up closer to one another,
we can also observe something of a ?citation hierar-
chy? in sections of the graph. Region 2 is a prime
example of this notion. We observe Ananiadou at
the bottom with a large number of incoming edges.
Above her node, we see Korhonen, who cites Ana-
niadou but is also cited by a number of other authors
herself. Finally, above Korhonen there are a series
of single nodes who cite her (and Ananiadou) but are
without incoming edges of their own. We can think
of this as something of a ?local hierarchy?, consist-
ing of authors who are closely connected, with the
more heavily-cited (and heavily-citing) easy to pick
out.
3 Next Steps
The work described here provides a snapshot into
the field. Underlying the work is a toolset able to
reproduce the results on new sets of data to continue
tracking the trends, topics, and collaborations. How-
ever, to be really useful to the research community,
the results need to be captured in a way that can fa-
cilitate searches in this domain and support ongoing
research. In order to do this, we are in the process of
incorporating the results presented here in a content
management system, MedLingMap (Meteer et al,
2012), which supports faceted indexing. Research
in search interface design has shown that techniques
which can create hierarchical faceted metadata stuc-
tures of a domain significantly increase the ability of
users to efficiently access documents in the collec-
tion (Stoica et al, 2005). The techniques described
here can be fed into MedLingMap to create much
of the metadata required to efficiently navigate the
space.
4 Conclusion
In this report, we have outlined a variety of meth-
ods that can be used to gain a better understand-
ing of BioNLP as a field. Our use of topic model-
ing demonstrates that the field already has several
well-defined tasks, techniques, and resources, and
we showed that we can use these topics to gain in-
sight into the major research areas in the field and
how those efforts areas are progressing. We put forth
that this analysis could be powerful in recogniz-
ing when a problem has been effectively ?solved?,
when a technique falls out of favor, and when a re-
source grows outdated. At the same time, we can
see rising trends, such as how the 2009 shared task
spurred an obvious 2010 interest in event extraction,
and the correlations in the field between certain ap-
proaches and certain tasks. Through clustering, we
were able to show that these topics also can help us
separate the documents from the field into distinc-
tive groups with a common theme, which can aid in
building a database for current documents and clas-
sifying future ones. Finally, we ended with an anal-
ysis of author relations based on citation frequency
and demonstrated how such a structure can be useful
in identifying influential figures through their works.
As a further benefit of this work, we propose to
use it to create a more lasting resource for the com-
munity that makes these results available to support
search and and navigation in the bio-medical NLP
field.
References
Andrew McCallum. 2002. MALLET: A Machine Learn-
ing for Language Toolkit. http://mallet.cs.umass.edu.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993-1022
David Hall, Dan Jurafsky, and Christopher D. Manning.
2008. Studying the history of ideas using topic mod-
els. In EMNLP.
MJ Schuemie, JL Talmon, PW Moorman, and JA Kors
2009. Mapping the domain of medical informatics.
Methods Inf Med 48:76-83.
Marie Meteer, Bensiin Borukhov, Michael Crivaro,
Michael Shafir, and Attapol Thamrongrattanarit 2012.
MedLingMap: Growing a resource for the Bio-
Medical NLP field.
R Development Core Team 2010. R: A language and
environment for statistical computing. http://www.R-
project.org.
S. Eggers, Z. Huang, H. Chen, L. Yan, C. Larson, A.
Rashid, M. Chau, and C. Lin. 2005 Mapping Medical
Informatics Research Medical Informatics: Knowl-
edge Management and Data Mining in BioMedicine.
Springer Science+Business Media, Inc.
S Gupta, and C. Manning 2011. Analyzing the dynam-
ics of research by extracting key aspects of scientific
papers. Proceedings of IJCNLP.
128
Steven Bird, Edward Loper, and Ewan Klein. 2009. Nat-
ural Language Processing with Python. OReilly Me-
dia Inc.
Emilia Stoica, Marti A. Hearst, and Megan Richardson.
2007. Automating creation of hierarchical faceted
metadata structure. Human Language Technologies:
The Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT 2007).
129
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 140?145,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
MedLingMap:  A growing resource mapping the Bio-Medical NLP field 
Marie Meteer, Bensiin Borukhov, Michael Crivaro,  Michael Shafir, Attapol Thamrongrattanarit  {mmeteer, bborukhov, mcrivaro, mshafir, tet}@brandeis.edu Department of Computer Science  Brandeis University  Waltham, MA 02453, USA  Abstract 
The application of natural language process-ing (NLP) in the biology and medical domain crosses many fields from Healthcare Informa-tion to Bioinformatics to NLP itself.  In order to make sense of how these fields relate and intersect, we have created ?MedLingMap? (www.medlingmap.org) which is a compila-tion of references with a multi-faceted index.  The initial focus has been creating the infra-structure and populating it with references an-notated with facets such as topic, resources used (ontologies, tools, corpora), and organi-zations.  Simultaneously we are applying NLP techniques to the text to find clusters, key terms and other relationships.  The goal for this paper is to introduce MedLingMap to the community and show how it can be a power-ful tool for research and exploration in the field. 1 Introduction In any field, understanding the scope of the field as well as finding materials relevant to a particular project paradoxically gets more difficult as the field gets larger.  This is even more difficult in a field such as Bio-Medical NLP, since it is at the crossroads of multiple disciplines. The drawbacks of keyword search, even using a specific engine such as Google Scholar, are well documented (Stoica et. al 2007) and recent trends in content aggregation and content curation have emerged to attempt to address the problem.  Uses of curation range from those in library science to ensure mate-rial remain accessible as format and electronic readers change and to make that information more findable (e.g. Peer and Green 2012) to those in marketing to increase revenue by providing more relevant content (Beaulaurier 2012).   
However, these approaches still have chal-lenges. Automatic aggregation over a large body of content still provides too many results without ad-ditional filtering mechanism.  Content curation, which filters content by value and annotates it to ensure higher quality returns, is expensive since  annotating large collections of content with the metadata needed to support faceted search and navigation is a huge challenge.   The goal of the work described in this paper is to provide a framework for creating a useful re-source tool bounded by the interests of a specific community which can take advantage of automated clustering and keyword extraction techniques and the use of community based annotation through crowd sourcing and social tatting to provide valu-able curation. What is an impossible task for a sin-gle team because doable when we successfully harness and empower the community. The MedLingMap site is available at www.medlingmap.org. Currently MedLingMap has over 300 references many of which are anno-tated according to a set of ?contextual? facets (de-scribed below). We first provide some use cases for the system and then go into more detail on the content, infrastructure and origins of the system.   We welcome members of the field to join MedLingMap as a curator to help extend the re-source.  Just email info@medlingmap.org to get an account.  Please include your affiliation. 2 Use Cases MedLingMap was started as a class project in a Brandeis graduate course on NLP in the Medical Domain to provide a means of finding and organiz-ing the publications in the field and as a data source for exploring trends in topics and relation-
140
ships among researchers.  While there are many use cases for such a resource, three stand out. The first is simply the ability to find material that meets very specific criteria. for example, to find papers using ?MetaMap? for named entity extraction over clinical data. MedLingMap?s grow-ing collection of references and the necessary meta-data to make it useful is well suited to this task.  The second is to support the exploration of an area.  If I?m interested in clinical coding, I can se-lect that subject area and am presented with a number of papers.  I notice that Phil Resnik is on a number of papers and may want to follow up on his work.  I also see many of the papers are tagged with AHIMA, including an entire proceedings that is worth exploring.  I select a paper and see the 
abstract mentions a particular challenge that is also worth following up on.  We are in the process of developing a personal ?workspace? that will let researchers record searches, annotate findings, and keep a queue of the ?next directions? that might be worth following up. The third use case gets back to one of the origi-nal premises of the work, which is that a ?map? of a field goes beyond a collection of materials, it also provides context and can be used to see ?hot spots? and trends.  In order to provide this information and visualization, we have developed a set of tools applying a variety of NLP techniques, such as clus-tering, topic identification and tf-idf to the content of the papers.  This work is described in more de-tail in (Thamrongrattanarit, et al 2012).   
 
  Figure 1:  MedLingMap site:  www.medlingmap.org 3 Content and Context The core content in MedLingMap are the refer-ences themselves.  The underlying representation is based on bibtex and references can be added by either pasting in a single bibtex item or uploading 
an entire file.  A reference can be added through a form interface as well.  We have added BioNLP and related workshops dating back to 2002 as well as many other docu-ments.  In addition to the references, there are en-tries for a variety of organizations and resources 
141
with a short description and links for each.  These elements are entered by hand.  The assumption is that there are a limited number of them and edito-rial control is more important than speed of entry. 3.1 Examples of the interface The MedLingMap interface is shown in Figure 1.  All references, resources and organizations are linked through a set of ?taxonomies? (described below), which have been developed bottom up based on the material tagged to date.  Selecting any item from the taxonomies will select content anno-tated by that tag.  So selecting a ?Technical area? from the box on the right brings in all the papers annotated by that topic.  A similar box of ?re-sources? allows the user to select all papers that have been annotated as using a particular resource. For example, in Figure 1 the user has selected MedLEE from the ?Resources? taxonomy and is shown the information on MedLee as well as refer-ences that have been annotated as discussing MedLEE.  In addition to the basic bibliographic information, the user can export the reference in bibtex or xml or jump directly to it Google scholar, which can provide multiple ways of accessing the resource. Alternative views show all of the refer-ences by year, author or title.  
 Figure 2:  Information on a particular reference By selecting a reference in MedLingMap, addi-tional information is available, as shown in Figure 
2. By selecting any of the key terms from the tax-onomy at the top of the ?view?, the user can go to more papers tagged with that term.  By selecting any of the authors, the user is shown other papers by that author.  Those with a ?curator? account (described below) can select ?edit? and make changes or provide additional tags. In addition, there is a standard search mecha-nism, as shown in Figure 3.  We are in the process of implementing true faceted search, similar to ?advanced search? for recipes, where you can se-lect one or more item from each taxonomy to con-strain the search. 
 Figure 3:  Open search 3.2 Faceted indexing Indexing content along multiple dimensions or ?facets? is not new to search (Alan 1995) and sig-nificant work has gone into creating effective inter-faces for faceted search (Hearst 2006).  When searching for research materials, the context the work was done can be a significant contributor to being able to find related materials.  ?Necessity is the mother of invention? implies that if you want to find similar solutions, look for similar needs.   To try to capture this kind of information, MedLingMap has facets organized into taxono-mies:  ? Technical area or topic of the work (shown in the screen shot above) ? Resources used: 
142
? Data: Corpora such as Genia, CRAFT, i2B2, BioInfer ? Lexical Resources, which are organized into dictionaries, and ontologies and  include UMLS, PubMEd, MedLine, MeSH, and Medical Wordnet ? Tools, such as parsers, taggers, annotation toolkits and more complete systems, such as MedLee, GATE, and MIST ? Shared tasks, such as the BioNLP 2009 and 2011 shared tasks, BioCreative, and i2b2 ? Institution the work was done in or is associated with in some way (e.g. funding, providing re-sources, etc) As the project continues, these facets will grow and new ones will be added.  Additional facets un-der consideration include the program (e.g. across multiple institutions, generally associated with a single funding source), target data (e.g. medical literature or clinical records).   4 Origins of MedLingMap As mentioned above, MedLingMap was started as a class project in a graduate course NLP in the Medical Domain and the creation of the taxono-mies and population of the material was done as part of the class. However, the underlying architec-ture itself is based on a system that has been under development for speech recognition for the past two years (www.stcspeechmap.org) by author Marie Meteer as part of the Speech Technology Consortium?s effort to improve prior art research in non-patent literature.  The driving principle is that the ?art? in any field (the papers, documentation, product descrip-tions, etc) can only be understood in terms of the context in which they were produced, contexts which show relations between them that is usually not available in the individual documents.  For ex-ample, much of the early work in speech recogni-tion addressed the challenges of multimodal interfaces well before we had sophisticated mobile devices.  Solutions are being reinvented and pat-ents applied for that would not considered novel if the original research were more readily available.  Similar issues arise in multidisciplinary fields such as Bio-Medical NLP where different groups come 
together who do not have the same historical con-text and may not know about previous research.  5 Infrastructure  MedLingMap and SpeechMap are built on Dru-pal1 an open architecture Content Management System (CMS), which underlies many web sites ranging from www.whitehouse.gov to BestBuy.   Using Drupal ensures that MedLingMap can be a living, growing resource. Drupal provides the following functionality: ? A database to store, retrieve, and maintain large documents sets and web pages, provid-ing multiple views into the contents. ? Specific content types for resources, organi-zations, authors, and references, all linked though a set of taxonomies. ? The capability to load in references in bibtex format either in a group or individually and annotate them with terms from the taxono-mies. ? Maintenance facilities, such as suggesting when multiple authors may be the same per-son and merging them. ? User profiles with different permission lev-els to accommodate viewers, contributors, social tagging, and private workspaces with the appropriate levels of security. ? The ability to integrate powerful search components, such as SOLR2, as well as spe-cific modules, such as the Bibliography module which provides automatic links to Google Scholar to retrieve those documents. ? Web-based to allow easy outside access and be more compatible with other systems.  ? Extensibility both for more content, more content types, and more functionality.  For example while there is a module that pro-duces a warning if a possible duplicate ref-erence, we are still looking for one that would search out potential duplicates and propose merges.  If none exists, such a mod-ule can be written and easily integrated.                                                 1 http://drupal.org/ 2 SOLR is an open-source search server based on the Lucene Java search library. http://lucene.apache.org/solr/ 
143
6 Value for Stakeholders The value of MedLingMap varies with the audi-ence.  We first talk about the value to the current community and contrast MedLingMap to similar resources already available.  We then look at stakeholders outside or entering the community and the value MedLingMap brings to them. 6.1 BioMedical NLP community For members of the community, a central reposi-tory for papers in the field is a ?nice to have?.  There is information that is surfaced by seeing the organization of the information and links to re-sources in one place, but if you have been attend-ing conferences and workshops regularly, this is not new information.  You know the players and already follow the work you are interested in.   In addition, similar information is available elsewhere, though in a more distributed form.  ACL has made all of the proceedings to confer-ences and workshops available3. Similarly ACM and IEEE Xplore provide access to all of the pa-pers they control.  The significant difference is that in these collections even the advance search is re-lying on standard bibliographic elements, such as author and title, and keyword search and there is no segmentation of the material by field, which introduces significant ambiguity as the same term can mean different things in different fields.  Simi-larly PubMed and GoPubMed offer documents and advanced search on a huge body of literature, but focused on biology and medicine, not the applica-tion of NLP techniques to those fields.  MedLing-Map is designed to be focused on a smaller community with more like interests.  It is also important to note that MedLingMap is providing links to papers, not the actual papers, which are controlled by the publishers.  While many papers are readily available using the links provided or can be found through the Google Scholar link for each reference, if you need a sub-scription to see the entire paper such as for IEEE, you still need to go through your standard method to get those papers. LREC?s Resource Map is more similar in that it provides more in depth information that the aggre-gations described above, however the focus is on                                                 3 http://aclweb.org/anthology-new/ 
mapping the resources themselves, not necessarily all of the publications that have taken advantage of those resources, though some of that information may be available by following the links.  LREC is also using a crowd sourcing method for growing the resource by asking those who submit papers also submit the information about the resources they used.  This is an interesting model in that it assures that those contributing have a stake in the result since they are members of the community by virtue of submitting a paper. Organizations such as BioNLP.org and Sig-BioMed are also important resource aggregators for the community.  Neither are focused on publi-cations and we hope that MedLingMap willl be-come one more resource they would point to. 6.2 From the outside For students or those who come to the field from a neighboring field, the aggregation of the material in MedLingMap can save considerable time and provide overview or ?map? of the field.  Queries that are ambiguous in Google Scholar are more precise when the domain is limited.  <example> This increase in the ability of newcomers in the field to find what they are looking for actual turns into benefits for those in the field in two ways:  First, one?s own papers become more findable, increasing citations and potential collaborations.  Second, for those who teach, MedLingMap pro-vides a great environment for the students to do targeted research.  Letting them loose in a con-strained search environment increases the likeli-hood they will find a rich body of material to learn from and build on without having to always hand select the papers. 7 Growing the resource The real challenge for a community resource such as MedLingMap is how to grow it to be compre-hensive and keep it up to date, specifically how to: ? grow the number of references and resources ? increase high quality annotations that go be-yond what can be extracted automatically. ? provide visualizations that bring to light the connections in the material. ? maintain the quality of the data, for example by fining and merging duplicate entries and 
144
ensuring information about resources and organizations is up to data. The two choices for growing are automatic tech-niques and human annotation.  We discuss the former in a related paper (Thamrongrattanarit 2012).  Here we describe how manual annotation can be feasible. 7.1 Distributed Power The key to high quality documents and tagging is community involvement.  There are two comple-mentary approaches that are key to the MedLing-Map project:  crowd sourcing and social tagging.  Crowd sourcing involves the community in finding relevant resources, particularly those that are fairly obscure and predate the internet.  The second is social tagging which lets individuals check on their own materials or materials in areas related to their own work and adding or adjusting the tags to make the content more searchable. The key to making these tactics work is setting up the right support in the underlying system.  For-tunately, the MedLingMap infrastructure allows for easy signup for those volunteering to contrib-ute.  These technique have been used successfully in patent prior art search by Article One, Inc.4 which puts out a call to researchers to find art on a particular patent.  If the client selects that art to support their case, the contributor is paid.  The pat-ent office itself attempted something similar in the Peer to Patent program5, which depended on peo-ple?s desire to improve the quality of patents by letting them contribute art.  It was moderately suc-cessful, but without the kind of specific reward the Article One provides, they did not get nearly as much material as they would have liked. MedLingMap, SpeechMap and other efforts of its kind have the same problem:  no one has enough time.  So how do we address it?  How do we create a convincing value proposition?  Here are a couple suggestions: Teaching:  MedLingMap is a great teaching tool.  Not only can students use it to do research on the material that?s in it, we as educators can enlist them to both tag material and go out on the web to find additional material to tag and add.  In just one semester we have made considerable progress.  If                                                 4 http://www.articleonepartners.com/ 5 http://peertopatent.org/ 
everyone teaching a similar course enlisted their students, the students would gain and the resource would grow. Research support:  With the implementation of the personal workspace described above, the sys-tem will provide a unique service not available from other aggregators or content owners. Funded project: Being able to hire student an-notators would accelerate the process.  For the SpeechMap project we have a proposal into the US Patent Office for funding.  We are open to sugges-tions about funding sources for MedLingMap. Conclusion With MedLingMap?s infrastructure in place and enough content to provide an exemplar of how it can grow, the challenge now is engaging the com-munity in what we see as an exiting experiment in harnessing the resources of the internet through crowd sourcing and social tagging to create a liv-ing resource that will benefit both the current and future members of the field.  MedLingMap also provides a resource for exploring automated ways of annotating and organizing research materials.  We also hope that this can be a map itself, to build similar ?maps? in other subfields. References Allen, RB. 1995. Retrieval from Facet Spaces, Elec-tronic Publishing Chichester, Vol. 8(2 & 3), 247?257.  Beaulaurier, Joe. 2012. Content Curating for Fun and Profit, http://whatcommarketing.com/content-curating-for-fun-and-profit/.  Hearst, Marti. 2006. Design Recommendations for Hi-erarchical Faceted Search Interfaces. ACM SIGIR Workshop on Faceted Search, August, 2006  Peer, L. Green, A. 2012. Building an Open Data Reposi-tory for a Specialized Research Community: Process, Challenges and Lessons, International Journal of Digital Curation, Vol 7, No 1.  E. Stoica, M.A. Hearst, and M. Richardson. 2007. Au- tomating Creation of Hierarchical Faceted Metadata Structures. In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2007), pages 244?251.  Thamrongrattanarit, A., Shafir, M., Crivaro, M., Boruk-hov, B. , Meteer, M. What can NLP tell us about Bio NLP? BioNLP Workshop, Montreal, CA, 2012  
145
