Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 571?578,
Sydney, July 2006. c?2006 Association for Computational Linguistics
  ARE: Instance Splitting Strategies for Dependency Relation-based 
Information Extraction 
Mstislav Maslennikov Hai-Kiat Goh Tat-Seng Chua 
Department of Computer Science 
School of Computing 
National University of Singapore 
{maslenni, gohhaiki, chuats}@ comp.nus.edu.sg 
 
Abstract 
Information Extraction (IE) is a fundamen-
tal technology for NLP. Previous methods 
for IE were relying on co-occurrence rela-
tions, soft patterns and properties of the 
target (for example, syntactic role), which 
result in problems of handling paraphrasing 
and alignment of instances. Our system 
ARE (Anchor and Relation) is based on the 
dependency relation model and tackles 
these problems by unifying entities accord-
ing to their dependency relations, which we 
found to provide more invariant relations 
between entities in many cases. In order to 
exploit the complexity and characteristics 
of relation paths, we further classify the re-
lation paths into the categories of ?easy?, 
?average? and ?hard?, and utilize different 
extraction strategies based on the character-
istics of those categories. Our extraction 
method leads to improvement in perform-
ance by 3% and 6% for MUC4 and MUC6 
respectively as compared to the state-of-art 
IE systems. 
1 Introduction 
Information Extraction (IE) is one of the funda-
mental problems of natural language processing. 
Progress in IE is important to enhance results in 
such tasks as Question Answering, Information 
Retrieval and Text Summarization. Multiple efforts 
in MUC series allowed IE systems to achieve near-
human performance in such domains as biological 
(Humphreys et al, 2000), terrorism (Kaufmann, 
1992; Kaufmann, 1993) and management succes-
sion (Kaufmann, 1995). 
The IE task is formulated for MUC series as 
filling of several predefined slots in a template. The 
terrorism template consists of slots Perpetrator, 
Victim and Target; the slots in the management 
succession template are Org, PersonIn, PersonOut 
and Post. We decided to choose both terrorism and 
management succession domains, from MUC4 and 
MUC6 respectively, in order to demonstrate that 
our idea is applicable to multiple domains. 
Paraphrasing of instances is one of the crucial 
problems in IE. This problem leads to data sparse-
ness in situations when information is expressed in 
different ways. As an example, consider the ex-
cerpts ?Terrorists attacked victims? and ?Victims 
were attacked by unidentified terrorists?. These 
instances have very similar semantic meaning. 
However, context-based approaches such as 
Autoslog-TS by Riloff (1996) and Yangarber et al 
(2002) may face difficulties in handling these in-
stances effectively because the context of entity 
?victims? is located on the left context in the first 
instance and on the right context in the second. For 
these cases, we found that we are able to verify the 
context by performing dependency relation parsing 
(Lin, 1997), which outputs the word ?victims? as an 
object in both instances, with ?attacked? as a verb 
and ?terrorists? as a subject. After grouping of same 
syntactic roles in the above examples, we are able 
to unify these instances.  
Another problem in IE systems is word align-
ment. Insertion or deletion of tokens prevents in-
stances from being generalized effectively during 
learning. Therefore, the instances ?Victims were 
attacked by terrorists? and ?Victims were recently 
attacked by terrorists? are difficult to unify. The 
common approach adopted in GRID by Xiao et al 
(2003) is to apply more stable chunks such as noun 
phrases and verb phrases. Another recent approach 
by Cui et al (2005) utilizes soft patterns for prob-
abilistic matching of tokens. However, a longer 
insertion leads to a more complicated structure, as 
in the instance ?Victims, living near the shop, went 
out for a walk and were attacked by terrorists?. 
Since there may be many inserted words, both ap-
proaches may also be inefficient for this case. Simi-
lar to the paraphrasing problem, the word align-
ment problem may be handled with dependency 
relations in many cases. We found that the relation 
subject-verb-object for words ?victims?, ?attacked? 
and ?terrorists? remains invariant for the above two 
instances. 
Before IE can be performed, we need to iden-
tify sentences containing possible slots. This is 
571
done through the identification of cue phrases 
which we call anchors or anchor cues. However, 
natural texts tend to have diverse terminologies, 
which require semantic features for generalization. 
These features include semantic classes, Named 
Entities (NE) and support from ontology (for ex-
ample, synsets in Wordnet). If such features are 
predefined, then changes in terminology (for in-
stance, addition of new terrorism organization) will 
lead to a loss in recall. To avoid this, we exploit 
automatic mining techniques for anchor cues. Ex-
amples of anchors are the words ?terrorists? or 
?guerrilla? that signify a possible candidate for the 
Perpetrator slot. 
From the reviewed works, we observe that the 
inefficient use of relations causes problems of 
paraphrasing and alignment and the related data 
sparseness problem in current IE systems. As a re-
sult, training and testing instances in the systems 
often lack generality. This paper aims to tackle 
these problems with the help of dependency rela-
tion-based model for IE. Although dependency re-
lations provide invariant structures for many in-
stances as illustrated above, they tend to be effi-
cient only for short sentences and make errors on 
long distance relations. To tackle this problem, we 
classify relations into ?simple?, ?average? and 
?hard? categories, depending on the complexity of 
the dependency relation paths. We then employ 
different strategies to perform IE in each category. 
The main contributions of our work are as fol-
lows. First, we propose a dependency relation 
based model for IE. Second, we perform classifica-
tion of instances into several categories based on 
the complexity of dependency relation structures, 
and employ the action promotion strategy to tackle 
the problem of long distance relations. 
The remaining parts of the paper are organized 
as follows. Section 2 discusses related work and 
Section 3 introduces our approach for constructing 
ARE. Section 4 introduces our method for splitting 
instances into categories. Section 5 describes our 
experimental setups and results and, finally, Sec-
tion 6 concludes the paper. 
2 Related work 
There are several research directions in Information 
Extraction. We highlight a few directions in IE 
such as case frame based modeling in PALKA by 
Kim and Moldovan (1995) and CRYSTAL by So-
derland et al (1995); rule-based learning in 
Autoslog-TS by Riloff et al (1996); and classifica-
tion-based learning by Chieu et al (2002). Al-
though systems representing these directions have 
very different learning models, paraphrasing and 
alignment problems still have no reliable solution.  
Case frame based IE systems incorporate do-
main-dependent knowledge in the processing and 
learning of semantic constraints. However, concept 
hierarchy used in case frames is typically encoded 
manually and requires additional human labor for 
porting across domains. Moreover, the systems 
tend to rely on heuristics in order to match case 
frames. PALKA by Kim and Moldovan (1995) per-
forms keyword-based matching of concepts, while 
CRYSTAL by Soderland et al (1995) relied on 
additional domain-specific annotation and associ-
ated lexicon for matching. 
Rule-based IE models allow differentiation of 
rules according to their performance. Autoslog-TS 
by Riloff (1996) learns the context rules for extrac-
tion and ranks them according to their performance 
on the training corpus. Although this approach is 
suitable for automatic training, Xiao et al (2004) 
stated that hard matching techniques tend to have 
low recall due to data sparseness problem. To over-
come this problem, (LP)2 by Ciravegna (2002) util-
izes rules with high precision in order to improve 
the precision of rules with average recall. However, 
(LP)2 is developed for semi-structured textual do-
main, where we can find consistent lexical patterns 
at surface text level. This is not the same for free-
text, in which different order of words or an extra 
clause in a sentence may cause paraphrasing and 
alignment problems respectively, such as the ex-
ample excerpts ?terrorists attacked peasants? and 
?peasants were attacked 2 months ago by terrorists?.  
The classification-based approaches such as by 
Chieu and Ng (2002) tend to outperform rule-based 
approaches. However, Ciravegna (2001) argued 
that it is difficult to examine the result obtained by 
classifiers. Thus, interpretability of the learned 
knowledge is a serious bottleneck of the classifica-
tion approach. Additionally, Zhou and Su (2002) 
trained classifiers for Named Entity extraction and 
reported that performance degrades rapidly if the 
training corpus size is below 100KB. It implies that 
human experts have to spend long hours to annotate  
a sufficiently large amount of training corpus. 
Several recent researches focused on the ex-
traction of relationships using classifiers. Roth and 
Yih (2002) learned the entities and relations to-
gether. The joint learning improves the perform-
ance of NE recognition in cases such as ?X killed 
Y?. It also prevents the propagation of mistakes in 
NE extraction to the extraction of relations. How-
ever, long distance relations between entities are 
likely to cause mistakes in relation extraction. A 
possible approach for modeling relations of differ-
ent complexity is the use of dependency-based ker-
nel trees in support vector machines by Culotta and 
Sorensen (2004). The authors reported that non-
relation instances are very heterogeneous, and 
572
hence they suggested the additional step of extract-
ing candidate relations before classification. 
3 Our approach 
Differing from previous systems, the language 
model in ARE is based on dependency relations 
obtained from Minipar by Lin (1997). In the first 
stage, ARE tries to identify possible candidates for 
filling slots in a sentence. For example, words such 
as ?terrorist? or ?guerrilla? can fill the slot for Per-
petrator in the terrorism domain. We refer to these 
candidates as anchors or anchor cues. In the sec-
ond stage, ARE defines the dependency relations 
that connect anchor cues. We exploit dependency 
relations to provide more invariant structures for 
similar sentences with different syntactic structures. 
After extracting the possible relations between an-
chor cues, we form several possible parsing paths 
and rank them.  Based on the ranking, we choose 
the optimal filling of slots.  
Ranking strategy may be unnecessary in cases 
when entities are represented in the SVO form. 
Ranking strategy may also fail in situations of long 
distance relations. To handle such problems, we 
categorize the sentences into 3 categories of: sim-
ple, average and hard, depending on the complexity 
of the dependency relations. We then apply differ-
ent strategies to tackle sentences in each category 
effectively. The following subsections discuss de-
tails of our approach. 
 
Features Perpetrator_Cue 
(A) 
Action_Cue 
(D) 
Victim_Cue 
(A) 
Target_Cue 
(A) 
Lexical  
(Head 
noun) 
terrorists,  
individuals,  
soldiers 
attacked, 
murder,  
massacre 
mayor, 
general, 
priests 
bridge,  
house,  
ministry 
Part-of-
Speech 
Noun Verb Noun Noun 
Named 
Entities 
Soldiers  
(PERSON) 
- Jesuit priests 
(PERSON) 
WTC  
(OBJECT) 
Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71 
Concept 
Class 
ID 2, 3 ID 9  ID 22, 43 ID 61, 48 
Co-
referenced 
entity 
He -> terrorist, 
soldier 
- They -> 
peasants 
- 
Table 1. Linguistic features for anchor extraction 
Every token in ARE may be represented at a 
different level of representations, including: Lexi-
cal, Part-of-Speech, Named Entities, Synonyms and 
Concept classes. The synonym set and concept 
classes are mainly obtained from Wordnet. We use 
NLProcessor from Infogistics Ltd for the extraction 
of part-of-speech, noun phrases and verb phrases 
(we refer to them as phrases). Named Entities are 
extracted with the program used in Yang et al 
(2003). Additionally, we employed the co-
reference module for the extraction of meaningful 
pronouns. It is used for linking entities across 
clauses or sentences, for example in ?John works in 
XYZ Corp. He was appointed as a vice-president a 
month ago? and could achieve an accuracy of 62%. 
After preprocessing and feature extraction, we ob-
tain the linguistic features in Table 1. 
3.1 Mining of anchor cues 
In order to extract possible anchors and relations 
from every sentence, we need to select features to 
support the generalization of words. This generali-
zation may be different for different classes of 
words. For example, person names may be general-
ized as a Named Entity PERSON, whereas for 
?murder? and ?assassinate?, the optimal generaliza-
tion would be the concept class ?kill? in the Word-
Net hypernym tree. To support several generaliza-
tions, we need to store multiple representations of 
every word or token. 
Mining of anchor cues or anchors is crucial in 
order to unify meaningful entities in a sentence, for 
example words ?terrorists?, ?individuals? and ?sol-
diers? from Table 1. In the terrorism domain, we 
consider 4 types of anchor cues: Perpetrator, Action, 
Victim, and Target of destruction. For management 
succession domain, we have 6 types: Post, Person 
In, Person Out, Action and Organization. Each set 
of anchor cues may be seen as a pre-defined se-
mantic type where the tokens are mined automati-
cally. The anchor cues are further classified into 
two categories: general type A and action type D. 
Action type anchor cues are those with verbs or 
verb phrases describing a particular action or 
movement. General type encompasses any prede-
fined type that does not fall under the action type 
cues.  
In the first stage, we need to extract anchor 
cues for every type. Let P be an input phrase, and 
Aj be the anchor of type j that we want to match. 
The similarity score of P for Aj in sentence S is 
given by: 
 
Phrase_Scores(P,Aj)=?1* S_lexicalS(P,Aj+?2* S_POSS(P,Aj) 
                         +?3* S_NES(P,Aj) +?4 * S_SynS(P,Aj)  
                         +?5* S_Concept-ClassS(P,Aj)   (1) 
 
where S_XXXS(P,Aj) is a score function for the type 
Aj and ?i is the importance weight for Aj. In order to 
extract the score function, we use entities from 
slots in the training instances. Each S_XXXS(P,Aj) is 
calculated as a ratio of occurrence in positive slots 
versus all the slots: 
 
  )2(
)(#
)(#
),(_
j
j
jS Atypetheofslotsall
AtypetheofslotspositiveinP
APXXXS =  
 
We classify the phrase P as belonging to an anchor 
cue A of type j if Phrase_ScoreS(P,Aj) ? ?, where 
? is an empirically determined threshold. The 
weights ( )51 ,..., ??? = are learned automatically 
using Expectation Maximization by Dempster et al 
(1977). Using anchors from training instances as 
ground truth, we iteratively input different sets of 
weights into EM to maximize the overall score. 
573
Consider the excerpts ?Terrorists attacked 
victims?, ?Peasants were murdered by unidentified 
individuals? and ?Soldiers participated in massacre 
of Jesuit priests?. Let Wi denotes the position of 
token i in the instances. After mining of anchors, 
we are able to extract meaningful anchor cues in 
these sentences as shown in Table 2: 
 
W-3 W-2 W-1 W0 W1 W2 W3
 Perp_Cue Action_Cue Victim_Cue    
   Victim_Cue were Action_Cue by
In Action_Cue Of Victim_Cue    
Table 2. Instances with anchor cues 
3.2 Relationship extraction and ranking 
In the next stage, we need to 
find meaningful relations to 
unify instances using the anchor 
cues. This unification is done 
using dependency trees of sen-
tences. The dependency 
relations for the first sentence 
are given in Figure 1.  
 
 From the dependency tree, we need to identify 
the SVO relations between anchor cues. In cases 
when there are multiple relations linking many po-
tential subjects, verbs or objects, we need to select 
the best relations under the circumstances. Our 
scheme for relation ranking is as follows.  
First, we rank each single relation individually 
based on the probability that it appears in the re-
spective context template slot in the training data. 
We use the following formula to capture the quality 
of a relation Rel which gives higher weight to more 
frequently occurring relations:  
 
)3(
||}|{||
||},|{||
),,( 21 ?
?
?
=?=
S iii
S iii
SRR
elRRRRR
AAleRQuality
where S is a set of sentences containing relation 
Rel, anchors A1 and A2; R denotes relation path con-
necting A1 and A2 in a sentence Si; ||X|| denotes size 
of the set X. 
 Second, we need to take into account the entity 
height in the dependency tree. We calculate height 
as a distance to the root node. Our intuition is that 
the nodes on the higher level of dependency tree 
are more important, because they may be linked to 
more nodes or entities. The following example in 
Figure 2 illustrates it.  
 
 
Figure 2. Example of entity in a dependency tree 
Here, the node ?terrorists? is the most representative 
in the whole tree, and thus relations nearer to ?ter-
rorists? should have higher weight. Therefore, we 
give a slightly higher weight to the links that are 
closer to the root node as follows: 
 
   Heights(Rel) = log2(Const ? Distance(Root, Rel))         (4) 
 
where Const is set to be larger than the depth of 
nodes in the tree.  
Third, we need to calculate the score of rela-
tion path Ri->j between each pair of anchors Ai and 
Aj, where Ai and Aj belong to different anchor cue 
types. The path score of Ri->j depends on both qual-
ity and height of participating relations:  
 
Scores(Ai, Aj)=?Ri?R {Heights(Ri)*Quality(Ri)}/Lengthij   (5) 
 
where Lengthij is the length of path Ri->j. Division 
on Lengthij allows normalizing Score against the 
length of Ri->j. The formula (5) tends to give higher 
scores to shorter paths. Therefore, the path ending 
with ?terrorist? will be preferred in the previous 
example to the equivalent path ending with 
?MRTA?. 
 Finally, we need to find optimal filling of a 
template T. Let C = {C1, .. , CK} be the set of slot 
types in T and A = {A1, .., AL} be the set of ex-
tracted anchors. First, we regroup anchors A ac-
cording to their respective types. Let 
},...,{ )()(1
)( k
L
kk
k
AAA =  be the projection of A onto 
the type Ck, ?k?N, k ? K. Let F = A(1) ? A(2) ?..? 
A(K) be the set of possible template fillings. The 
elements of F are denoted as F1, ..,FM, where every 
Fi ? F is represented as Fi = {Ai(1),..,Ai(K)}. Our aim 
is to evaluate F and find the optimal filling F0 ? F. 
For this purpose, we use the previously calculated 
scores of relation paths between every two anchors 
Ai and Aj.  
 Based on the previously defined ScoreS(Ai, Aj), 
it is possible to rank all the fillings in F. For each 
filling Fi?F we calculate the aggregate score for all 
the involved anchor pairs: 
)7(
),(
)(_ ,1
M
AAcoreS
FScoreelationR
jiSKji
iS
? ??=
where K is number of slot types and M denotes the 
number of relation paths between anchors in Fi.  
 After calculating Relation_ScoreS(Fi), it is used 
for ranking all possible template fillings. The next 
step is to join entity and relation scores. We defined 
the entity score of Fi as an average of the scores of 
participating anchors:   
 
)8(/)(_)(_
1
)(? ??= Kk kiSiS KAScorePhraseFScoreEntity
We combine entity and relation scores of Fi into the 
overall formula for ranking. 
 
 RankS(Fi)=?*Entity_ScoreS(Fi)+(1-?)*Relation_ScoreS(Fi )      (9) 
 
The application of Subject-Verb-Object (SVO) 
relations facilitates the grouping of subjects, 
Figure 1.  
Dependency tree 
574
verbs and objects together. For the 3 instances in 
Table 2 containing the anchor cues, the unified 
SVO relations are given in Table 3. 
 
W-2 W-1 W0 Instance is 
Perp_Cue attacked Victim_Cue + 
Perp_Cue murdered Victim_Cue + 
Perp_Cue participated ? - 
Table 3.  Unification based on SVO relations 
The first 2 instances are unified correctly. The 
only exception is the slot in the third case, which 
is missing because the target is not an object of 
?participated?. 
4 Category Splitting 
Through our experiments, we found that the com-
bination of relations and anchors are essential for 
improving IE performance. However, relations 
alone are not applicable across all situations be-
cause of long distance relations and possible de-
pendency relation parsing errors, especially for 
long sentences. Since the relations in long sen-
tences are often complicated, parsing errors are 
very difficult to avoid. Furthermore, application of 
dependency relations on long sentences may lead to 
incorrect extractions and decrease the performance.  
Through the analysis of instances, we noticed 
that dependency trees have different complexity for 
different sentences. Therefore, we decided to clas-
sify sentences into 3 categories based on the com-
plexity of dependency relations between the action 
cues (V) and the likely subject (S) and object cues 
(O). Category 1 is when the potential SVO?s are 
connected directly to each other (simple category); 
Category 2 is when S or O is one link away from V 
in terms of nouns or verbs (average category); and 
Category 3 is when the path distances between po-
tential S, V, and Os are more than 2 links away 
(hard category).  
 
 
 
 
  
Figure 3. Simple category   Figure 4. Average category  
Figure 3 and Figure 4 illustrate the dependency 
parse trees for the simple and average categories 
respectively derived from the sentences: ?50 peas-
ants of have been kidnapped by terrorists? and ?a 
colonel was involved in the massacre of the Jesu-
its?. These trees represent 2 common structures in 
the MUC4 domain. By taking advantage of this 
commonality, we can further improve the perform-
ance of extraction. We notice that in the simple 
category, the perpetrator cue (?terrorists?) is always 
a subject, action cue (?kidnapped?) a verb, and vic-
tim cue (?peasants?) an object. For the average 
category, perpetrator and victim commonly appear 
under 3 relations: subject, object and pcomp-n. The 
most difficult category is the hard category, since 
in this category relations can be distant. We thus 
primarily rely on anchors for extraction and have to 
give less importance to dependency parsing.   
 In order to process the different categories, we 
utilize the specific strategies for each category. As 
an example, the instance ?X murdered Y? requires 
only the analysis of the context verb ?murdered? in 
the simple category. It is different from the in-
stances ?X investigated murder of Y? and ?X con-
ducted murder of Y? in the average category, in 
which transition of word ?investigated? into ?con-
ducted? makes X a perpetrator. We refer to the an-
chor ?murder? in the first and second instances as 
promotable and non-promotable respectively. Ad-
ditionally, we denote that the token ?conducted? is 
the optimal node for promotion of ?murder?, 
whereas the anchor ?investigate? is not. This exam-
ple illustrates the importance of support verb analy-
sis specifically for the average category.  
 
 
 
 
Figure 5. Category processing 
The main steps of our algorithm for performing IE 
in different categories are given in Figure 5. Al-
though some steps are common for every category, 
the processing strategies are different. 
 
Simple category 
For simple category, we reorder tokens according 
to their slot types. Based on this reordering, we fill 
the template. 
 
Algorithm 
 
1) Analyze category  
     If(simple)  
       - Perform token reordering based on SVO relations 
     Else if (average) ProcessAverage 
     Else ProcessHard 
2) Fill template slots 
 
Function ProcessAverage 
1) Find the nearest missing anchor in the previous sentences  
2) Find the optimal linking node for action anchor in every Fi 
3) Find the filling Fi(0) = argmaxi Rank(Fi) 
4) Use Fi for filling the template if Rank0 > ?2, where ?2 is an 
empirical threshold 
 
Function ProcessHard 
1) Perform token reordering based on anchors 
2) Use linguistic+ syntactic + semantic feature of the head 
noun. Eg. Caps, ?subj?, etc 
3) Find the optimal linking node for action anchor in every Fi 
4) Find the filling Fi(0) = argmaxi Rank(Fi) 
5) Use Fi for filling the template if Rank0 > ?3, where ?3 is an 
empirical threshold 
575
Average category 
For average category, our strategy consists of 4 
steps. First, in the case of missing anchor type we 
try to find it in the nearest previous sentence. Con-
sider an example from MUC-6: ?Look at what hap-
pened to John Sculley, Apple Computer's former 
chairman. Earlier this month he abruptly resigned 
as chairman of troubled Spectrum Information 
Technologies.? In this example, a noisy cue ?he? 
needs to be substituted with ?John Sculley?, which 
is a strong anchor cue. Second, we need to find an 
optimal promotion of a support verb. For example, 
in ?X conducted murder of Y?, the verb ?murder? 
should be linked with X and in the excerpt ?X in-
vestigated murder of Y?, it should not be promoted. 
Thus, we need to make 2 steps for promotion: (a) 
calculate importance of every word connecting the 
action cue such as ?murder? and ?distributed? and (b) 
find the optimal promotion for the word ?murder?. 
Third, using the predefined threshold ? we cutoff 
the instances with irrelevant support verbs (e.g., 
?investigated?). Fourth, we reorder the tokens in 
order to group them according to the anchor types. 
The following algorithm in Figure 6 estimates 
the importance of a token W for type D in the sup-
port verb structure. The input of the algorithm con-
sists of sentences S1?SN and two sets of tokens 
Vneg, Vpos co-occurring with anchor cue of type D. 
Vneg and Vpos are automatically tagged as irrelevant 
and relevant respectively based on preliminary 
marked keys in the training instances. The algo-
rithm output represents the importance value be-
tween 0 to 1.  
 
 
Figure 6. Evaluation of word importance 
We use the linguistic features for W and D as given 
in Table 1 to form the instances.  
 
Hard category 
In the hard category, we have to deal with long-
distance relations: at least 2 anchors are more than 
2 links away in the dependency tree. Consequently, 
dependency tree alone is not reliable for connecting 
nodes. To find an optimal connection, we primarily 
rely on comparison between several possible fill-
ings of slots based on previously extracted anchor 
cues. Depending on the results of such comparison, 
we chose the filling that has the highest score. As 
an example, consider the hard category in the ex-
cerpt ?MRTA today distributed leaflets claiming 
responsibility for the murder of former defense 
minister Enrique Lopez Albujar?. The dependency 
tree for this instance is given in Figure 7.  
 
Although words ?MRTA?, ?murder? and ?min-
ister? might be correctly extracted as anchors, the 
challenging problem is to decide whether ?MRTA? 
is a perpetrator. Anchors ?MRTA? and ?minister? 
are connected via the verb ?distributed?. However, 
the word ?murder? belongs to another branch of this 
verb. 
 
 
Figure 7. Hard case 
Processing of such categories is challenging. 
Since relations are not reliable, we first need to rely 
on the anchor extraction stage. Nevertheless, the 
promotion strategy for the anchor cue ?murder? is 
still possible, although the corresponding branch in 
the dependency tree is long. Henceforth, we try to 
replace the verb ?distributed? by promoting the an-
chor ?murder?. To do so, we need to evaluate 
whether the nodes in between may be eliminated. 
For example, such elimination is possible in the 
pairs ?conducted? -> ?murder? and not possible in 
the pair ?investigated? -> ?murder?, since in the ex-
cerpt ?X investigated murder? X is not a perpetra-
tor. If the elimination is possible, we apply the 
promotion algorithm given on Figure 8: 
 
 
Figure 8. Token promotion algorithm 
The algorithm checks path Pj1->j2 that connect an-
chors Ai(j1) and Ai(j2) in the filling Fi; the nodes from 
Pj1->j2 are added to the set Z. Finally, the top node 
of the set Z is chosen as an optimal node for the 
promotion. The example optimal node for promo-
tion of the word ?murder? on Figure 7 is the node 
?distributed?. 
Another important difference between the hard 
and average cases is in the calculation of RankS (Fi) 
in Equation (9). We set ?hard > ?average because long 
distance relations are less reliable in the hard case 
than in the average case. 
CalculateImportance (W, D) 
 
1) Select sentences that contain anchor cue D 
2) Extract linguistic features of Vpos, Vneg and D 
3) Train using SVM on instances (Vpos,D) and  
instances (Vneg,D) 
4) Return Importance(W) using SVM 
FindOptimalPromotion (Fi) 
1) Z = ? 
2) For each Ai(j1), Ai(j2) ? Fi 
   Z = Z ? Pj1->j2 
     End_for 
3) Output Top(Z) 
576
5 Evaluation 
In order to evaluate the efficiency of our method, 
we conduct our experiments in 2 domains: MUC4 
(Kaufmann, 1992) and MUC6 (Kaufmann, 1995). 
The official corpus of MUC4 is released with 
MUC3; it covers terrorism in the Latin America 
region and consists of 1,700 texts. Among them, 
1,300 documents belong to the training corpus. 
Testing was done on 25 relevant and 25 irrelevant 
texts from TST3, plus 25 relevant and 25 irrelevant 
texts from TST4, as is done in Xiao et al (2004). 
MUC6 covers news articles in Management Suc-
cession domain. Its training corpus consists of 1201 
instances, whereas the testing corpus consists of 76 
person-ins, 82 person-outs, 123 positions, and 79 
organizations. These slots we extracted in order to 
fill templates on a sentence-by-sentence basis, as is 
done by Chieu et al (2002) and Soderland (1999). 
Our experiments were designed to test the 
effectiveness of both case splitting and action verb 
promotion. The performance of ARE is compared 
to both the state-of-art systems and our baseline 
approach. We use 2 state-of-art systems for MUC4 
and 1 system for MUC6. Our baseline system, 
Anc+rel, utilizes only anchors and relations 
without category splitting as described in Section 3. 
For our ARE system with case splitting, we present 
the results on Overall corpus, as well as separate 
results on Simple, Average and Hard categories. 
The Overall performance of ARE represents the 
result for all the categories combined together. 
Additionally, we test the impact of the action 
promotion (in the right column) for the average and 
hard categories. 
 
 Without promotion With promotion 
Case (%) P R F1 P R F1 
GRID 58% 56% 57% - - - 
Riloff?05 46% 52% 48% - - - 
Anc+rel (100%) 58% 59% 58% 58% 59% 58% 
Overall (100%) 57% 60% 59% 58% 61% 60% 
Simple (13%) 79% 86% 82% 79% 86% 82% 
Average (22%) 64% 70% 67% 67% 71% 69% 
Hard (65%) 50% 52% 51% 51% 53% 52% 
Table 4. Results on MUC4 with case splitting 
 
The comparative results are presented in Table 
4 and Table 5 for MUC4 and MUC6, respectively. 
First, we review our experimental results on MUC4 
corpus without promotion (left column) before pro-
ceeding to the right column. 
a) From the results on Table 4 we observe that our 
baseline approach Anc+rel outperforms all the 
state-of-art systems. It demonstrates that both an-
chors and relations are useful. Anchors allow us to 
group entities according to their semantic meanings 
and thus to select of the most prominent candidates. 
Relations allow us to capture more invariant repre-
sentation of instances. However, a sentence may 
contain very few high-quality relations. It implies 
that the relations ranking step is fuzzy in nature. In 
addition, we noticed that some anchor cues may be 
missing, whereas the other anchor types may be 
represented by several anchor cues. All these fac-
tors lead only to moderate improvement in per-
formance, especially in comparison with GRID 
system. 
b) Overall, the splitting of instances into categories 
turned out to be useful. Due to the application of 
specific strategies the performance increased by 1% 
over the baseline. However, the large dominance of 
the hard cases (65%) made this improvement mod-
est. 
c) We notice that the amount of variations for con-
necting anchor cues in the Simple category is rela-
tively small. Therefore, the overall performance for 
this case reaches F1=82%. The main errors here 
come from missing anchors resulting partly from 
mistakes in such component as NE detection. 
d) The performance in the Average category is 
F1=67%. It is lower than that for the simple cate-
gory because of higher variability in relations and 
negative influence of support verbs. For example, 
for excerpt such as ?X investigated murder of Y?, 
the processing tends to make mistake without the 
analysis of semantic value of support verb ?investi-
gated?. 
e) Hard category achieves the lowest performance 
of F1=51% among all the categories. Since for this 
category we have to rely mostly on anchors, the 
problem arises if these anchors provide the wrong 
clues. It happens if some of them are missing or are 
wrongly extracted. The other cause of mistakes is 
when ARE finds several anchor cues which belong 
to the same type. 
Additional usage of promotion strategies al-
lowed us to improve the performance further. 
f) Overall, the addition of promotion strategy en-
ables the system to further boost the performance to 
F1=60%. It means that the promotion strategy is 
useful, especially for the average case. The im-
provement in comparison to the state-of-art system 
GRID is about 3%. 
g) It achieved an F1=69%, which is an improve-
ment of 2%, for the Average category. It implies 
that the analysis of support verbs helps in revealing 
the differences between the instances such as ?X 
was involved in kidnapping of Y? and ?X reported 
kidnapping of Y?.  
h) The results in the Hard category improved mod-
erately to F1=52%. The reason for the improvement 
is that more anchor cues are captured after the 
promotion. Still, there are 2 types of common mis-
577
takes: 1) multiple or missing anchor cues of the 
same type and 2) anchors can be spread across sev-
eral sentences or several clauses in the same sen-
tence.  
 
 Without promotion With promotion 
Case (%) P R F1 P R F1 
Chieu et al?02 74% 49% 59% - - - 
Anc+rel (100%) 78% 52% 62% 78% 52% 62% 
Overall (100%) 72% 58% 64% 73% 58% 65% 
Simple (45%) 85% 67% 75% 87% 68% 76% 
Average (27%) 61% 55% 58% 64% 56% 60% 
Hard (28%) 59% 44% 50% 59% 44% 50% 
Table 5. Results on MUC6 with case splitting 
For the MUC6 results given in Table 5, we ob-
serve that the overall improvement in performance 
of ARE system over Chieu et al?02 is 6%. The 
trends of results for MUC6 are similar to that in 
MUC4. However, there are few important differ-
ences. First, 45% of instances in MUC6 fall into 
the Simple category, therefore this category domi-
nates. The reason for this is that the terminologies 
used in Management Succession domain are more 
stable in comparison to the Terrorism domain. Sec-
ond, there are more anchor types for this case and 
therefore the promotion strategy is applicable also 
to the simple case. Third, there is no improvement 
in performance for the Hard category. We believe 
the primary reason for it is that more stable lan-
guage patterns are used in MUC6. Therefore, de-
pendency relations are also more stable in MUC6 
and the promotion strategy is not very important. 
Similar to MUC4, there are problems of missing 
anchors and mistakes in dependency parsing. 
6 Conclusion 
The current state-of-art IE methods tend to use co-
occurrence relations for extraction of entities. Al-
though context may provide a meaningful clue, the 
use of co-occurrence relations alone has serious 
limitations because of alignment and paraphrasing 
problems. In our work, we proposed to utilize de-
pendency relations to tackle these problems. Based 
on the extracted anchor cues and relations between 
them, we split instances into ?simple?, ?average? 
and ?hard? categories. For each category, we ap-
plied specific strategy. This approach allowed us to 
outperform the existing state-of-art approaches by 
3% on Terrorism domain and 6% on Management 
Succession domain. In our future work we plan to 
investigate the role of semantic relations and inte-
grate ontology in the rule generation process. An-
other direction is to explore the use of bootstrap-
ping and transduction approaches that may require 
less training instances. 
 
References 
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Ap-
proach to Information Extraction from Semi-Structured 
and Free Text. In Proc of AAAI-2002, 786-791. 
H. Cui, M.Y. Kan, and Chua T.S. 2005. Generic Soft Pat-
tern Models for Definitional Question Answering. In 
Proc of ACM SIGIR-2005. 
A. Culotta and J. Sorensen J. 2004. Dependency tree kernels 
for relation extraction. In Proc of ACL-2004. 
F. Ciravegna. 2001. Adaptive Information Extraction from 
Text by Rule Induction and Generalization. In Proc of 
IJCAI-2001. 
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum like-
lihood from incomplete data via the EM algorithm. Jour-
nal of the Royal Statistical Society B, 39(1):1?38 
K. Humphreys, G. Demetriou and R. Gaizuskas. 2000. Two 
applications of Information Extraction to Biological Sci-
ence: Enzyme interactions and Protein structures. In 
Proc of the Pacific Symposium on Biocomputing, 502-
513 
M. Kaufmann. 1992. MUC-4. In Proc of  MUC-4. 
M. Kaufmann. 1995. MUC-6. In Proc of MUC-6. 
J. Kim and D. Moldovan. 1995. Acquisition of linguistic 
patterns for knowledge-based information extraction. 
IEEE Transactions on KDE, 7(5): 713-724 
D. Lin. 1997. Using Syntactic Dependency as Local Context 
to Resolve Word Sense Ambiguity. In Proc of ACL-97. 
E. Riloff. 1996. Automatically Generating Extraction Pat-
terns from Untagged Text. In Proc of AAAI-96, 1044-
1049. 
D. Roth and W. Yih. 2002. Probabilistic Reasoning for En-
tity & Relation Recognition. In Proc of COLING-2002. 
S. Soderland, D. Fisher, J. Aseltine and W. Lehnert. 1995. 
Crystal: Inducing a Conceptual Dictionary. In Proc of 
IJCAI-95, 1314-1319. 
S. Soderland. 1999. Learning Information Extraction Rules 
for Semi-Structured and Free Text. Machine Learning 
34:233-272. 
J. Xiao, T.S. Chua and H. Cui. 2004. Cascading Use of Soft 
and Hard Matching Pattern Rules for Weakly Supervised 
Information Extraction. In Proc of COLING-2004. 
H. Yang, H. Cui, M.-Y. Kan, M. Maslennikov, L. Qiu and 
T.-S. Chua. 2003. QUALIFIER in TREC 12 QA Main 
Task. In Proc of TREC-12, 54-65. 
R. Yangarber, W. Lin, R. Grishman. 2002. Unsupervised 
Learning of Generalized Names. In Proc of COLING-
2002. 
G.D. Zhou and J. Su. 2002. Named entity recognition using 
an HMM-based chunk tagger. In Proc of ACL-2002, 
473-480 
578
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 592?599,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Multi-resolution Framework for Information Extraction from Free Text
Mstislav Maslennikov and Tat-Seng Chua 
Department of Computer Science 
National University of Singapore 
{maslenni,chuats}@comp.nus.edu.sg 
Abstract 
Extraction of relations between entities is 
an important part of Information Extraction 
on free text. Previous methods are mostly 
based on statistical correlation and depend-
ency relations between entities. This paper 
re-examines the problem at the multi-
resolution layers of phrase, clause and sen-
tence using dependency and discourse rela-
tions. Our multi-resolution framework 
ARE (Anchor and Relation) uses clausal 
relations in 2 ways: 1) to filter noisy de-
pendency paths; and 2) to increase reliabil-
ity of dependency path extraction. The re-
sulting system outperforms the previous 
approaches by 3%, 7%, 4% on MUC4, 
MUC6 and ACE RDC domains respec-
tively. 
1 Introduction 
Information Extraction (IE) is the task of identify-
ing information in texts and converting it into a 
predefined format. The possible types of informa-
tion include entities, relations or events. In this 
paper, we follow the IE tasks as defined by the 
conferences MUC4, MUC6 and ACE RDC: slot-
based extraction, template filling and relation ex-
traction, respectively. 
Previous approaches to IE relied on co-
occurrence (Xiao et al, 2004) and dependency 
(Zhang et al, 2006) relations between entities. 
These relations enable us to make reliable extrac-
tion of correct entities/relations at the level of a 
single clause. However, Maslennikov et al (2006) 
reported that the increase of relation path length 
will lead to considerable decrease in performance. 
In most cases, this decrease in performance occurs 
because entities may belong to different clauses.  
Since clauses in a sentence are connected by 
clausal relations (Halliday and Hasan, 1976), it is 
thus important to perform discourse analysis of a 
sentence.  
Discourse analysis may contribute to IE in sev-
eral ways. First, Taboada and Mann (2005) re-
ported that discourse analysis helps to decompose 
long sentences into clauses. Therefore, it helps to 
distinguish relevant clauses from non-relevant 
ones. Second, Miltsakaki (2003) stated that entities 
in subordinate clauses are less salient. Third, the 
knowledge of textual structure helps to interpret 
the meaning of entities in a text (Grosz and Sidner 
1986). As an example, consider the sentences 
?ABC Co. appointed a new chairman. Addition-
ally, the current CEO was retired?. The word ?ad-
ditionally? connects the event in the second sen-
tence to the entity ?ABC Co.? in the first sentence. 
Fourth, Moens and De Busser (2002) reported that 
discourse segments tend to be in a fixed order for 
structured texts such as court decisions or news. 
Hence, analysis of discourse order may reduce the 
variability of possible relations between entities. 
To model these factors, we propose a multi-
resolution framework ARE that integrates both 
discourse and dependency relations at 2 levels. 
ARE aims to filter noisy dependency relations 
from training and support their evaluation with 
discourse relations between entities. Additionally, 
we encode semantic roles of entities in order to 
utilize semantic relations. Evaluations on MUC4, 
MUC6 and ACE RDC 2003 corpora demonstrates 
that our approach outperforms the state-of-art sys-
tems mainly due to modeling of discourse rela-
tions. 
The contribution of this paper is in applying dis-
course relations to supplement dependency rela-
tions in a multi-resolution framework for IE. The 
592
framework enables us to connect entities in differ-
ent clauses and thus improve the performance on 
long-distance dependency paths.  
Section 2 describes related work, while Section 
3 presents our proposed framework, including the 
extraction of anchor cues and various types of rela-
tions, integration of extracted relations, and com-
plexity classification. Section 4 describes our ex-
perimental results, with the analysis of results in 
Section 5. Section 6 concludes the paper. 
2 Related work 
Recent work in IE focuses on relation-based, se-
mantic parsing-based and discourse-based ap-
proaches. Several recent research efforts were 
based on modeling relations between entities. Cu-
lotta and Sorensen (2004) extracted relationships 
using dependency-based kernel trees in Support 
Vector Machines (SVM). They achieved an F1-
measure of 63% in relation detection. The authors 
reported that the primary source of mistakes comes 
from the heterogeneous nature of non-relation in-
stances. One possible direction to tackle this prob-
lem is to carry out further relationship classifica-
tion. Maslennikov et al (2006) classified relation 
path between candidate entities into simple, aver-
age and hard cases. This classification is based on 
the length of connecting path in dependency parse 
tree. They reported that dependency relations are 
not reliable for the hard cases, which, in our opin-
ion, need the extraction of discourse relations to 
supplement dependency relation paths. 
Surdeanu et al (2003) applied semantic parsing 
to capture the predicate-argument sentence struc-
ture. They suggested that semantic parsing is use-
ful to capture verb arguments, which may be con-
nected by long-distance dependency paths. How-
ever, current semantic parsers such as the ASSERT 
are not able to recognize support verb construc-
tions such as ?X conducted an attack on Y? under 
the verb frame ?attack? (Pradhan et al 2004). 
Hence, many useful predicate-argument structures 
will be missed. Moreover, semantic parsing be-
longs to the intra-clausal level of sentence analysis, 
which, as in the dependency case, will need the 
support of discourse analysis to bridge inter-clausal 
relations. 
Webber et al (2002) reported that discourse 
structure helps to extract anaphoric relations. How-
ever, their set of grammatical rules is heuristic. Our 
task needs construction of an automated approach 
to be portable across several domains. Cimiano et 
al. (2005) employed a discourse-based analysis for 
IE. However, their approach requires a predefined 
domain-dependent ontology in the format of ex-
tended logical description grammar as described by 
Cimiano and Reely (2003). Moreover, they used 
discourse relations between events, whereas in our 
approach, discourse relations connect entities. 
3 Motivation for using discourse relations  
Our method is based on Rhetorical Structure The-
ory (RST) by Taboada and Mann (2005). RST 
splits the texts into 2 parts: a) nuclei, the most im-
portant parts of texts; and b) satellites, the secon-
dary parts. We can often remove satellites without 
losing the meaning of text. Both nuclei and satel-
lites are connected with discourse relations in a 
hierarchical structure. In our work, we use 16 
classes of discourse relations between clauses: At-
tribution, Background, Cause, Comparison, Condi-
tion, Contrast, Elaboration, Enablement, Evalua-
tion, Explanation, Joint, Manner-Means, Topic-
Comment, Summary, Temporal, Topic-Change. 
The additional 3 relations impose a tree structure: 
textual-organization, span and same-unit. All the 
discourse relation classes are potentially useful, 
since they encode some knowledge about textual 
structure. Therefore, we decide to include all of 
them in the learning process to learn patterns with 
best possible performance. 
We consider two main rationales for utilizing 
discourse relations to IE. First, discourse relations 
help to narrow down the search space to the level 
of a single clause. For example, the sentence 
?[<Soc-A1>Trudeau</>'s <Soc-A2>son</> told 
everyone], [their prime minister was his father], 
[who took him to a secret base in the arctic] [and 
let him peek through a window].? contains 4 
clauses and 7 anchor cues (key phrases) for the 
type Social, which leads to 21 possible variants. 
Splitting this sentence into clauses reduces the 
combinations to 4 possible variants. Additionally, 
this reduction eliminates the long and noisy de-
pendency paths.  
Second, discourse analysis enables us to connect 
entities in different clauses with clausal relations. 
As an example, we consider a sentence ?It?s a dark 
comedy about a boy named <AT-A1>Marshal</> 
played by Amourie Kats who discovers all kinds of 
593
on and scary things going on in <AT-A2>a seem-
ingly quiet little town</>?. In this example, we 
need to extract the relation ?At? between the enti-
ties ?Marshal? and ?a seemingly quiet little town?. 
The discourse structure of this sentence is given in 
.Figure 1   
 
 
Figure 1. Example of discourse parsing 
The discourse path ?Marshal <-elaboration- _ 
<-span- _ -elaboration-> _ -elaboration-> town? 
is relatively short and captures the necessary rela-
tions. At the same time, prediction based on de-
pendency path ?Marshal <?obj- _ <-i- _ <-fc- _ 
<-pnmod- _ <-pred- _ <-i- _ <-null- _ -null-> _ -
rel-> _ -i-> _ -mod-> _ -pcomp-n-> town? is un-
reliable, since the relation path is long. Thus, it is 
important to rely on discourse analysis in this ex-
ample. In addition, we need to evaluate both the 
score and reliability of prediction by relation path 
of each type. 
4 Anchors and Relations 
In this section, we define the key components that 
we use in ARE: anchors, relation types and general 
architecture of our system. Some of these compo-
nents are also presented in detail in our previous 
work (Maslennikov et al, 2006). 
4.1 Anchors 
The first task in IE is to identify candidate phrases 
(which we call anchor or anchor cue) of a pre-
defined type  (anchor  type) to fill a desired slot in 
an  IE  template.  The  example  anchor  for  the  phrase 
 
 ?Marshal? is shown in Figure 2. 
Given a training set of sentences, 
we extract the anchor cues ACj = 
[A1, ?, ANanch] of type Cj using 
the procedures described in 
Maslennikov et al (2006). The 
linguistic features of these an-
chors for the anchor types of Per-  
petrator, Action, Victim and Target for the MUC4 
domain are given in Table 1. 
 
Anchor  
types
Feature 
Perpetrator_Cue 
(A) 
Action_Cue 
(D) 
Victim_Cue 
(A) 
Target_Cue 
(A) 
Lexical  
(Head noun) 
terrorists,  
individuals,  
Soldiers 
attacked, 
murder,  
Massacre 
Mayor, 
general, 
priests 
bridge,  
house,  
Ministry 
Part-of-Speech Noun Verb Noun Noun 
Named Enti-
ties 
Soldiers  
(PERSON) 
- Jesuit priests 
(PERSON) 
WTC  
(OBJECT) 
Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71 
Concept Class ID 2, 3 ID 9  ID 22, 43 ID 61, 48 
Co-referenced 
entity 
He -> terrorist, 
soldier 
- They -> 
peasants 
- 
Clausal type Nucleus 
Satellite 
Nucleus, 
Satellite 
Nucleus, 
Satellite 
Nucleus, 
Satellite 
Argument type Arg0 , Arg1
Root 
 Target, -, 
ArgM-MNR 
Arg0 ,  Arg1 Arg1 , ArgM-
MNR 
Table 1. Linguistic features for anchor extraction 
 
Given an input phrase P from a test sentence, we 
need to classify if the phrase belongs to anchor cue 
type Cj. We calculate the entity score as: 
 
 Entity_Score(P) =?  ? i * Feature_Scorei(P,Cj) (1) 
 
where Feature_Score(P,Cj) is a score function for 
a particular linguistic feature representation of type 
Cj, and ? i is the corresponding weight for that rep-
resentation in the overall entity score.  The weights 
are learned automatically using Expectation Maxi-
mization (Dempster et al, 1977). The Fea-
ture_Scorei(P,Cj) is estimated from the training set 
as the number of slots containing the correct fea-
ture representation type versus all the slots: 
 
 Feature_Scorei(P,Cj) = #(positive slots) / #(all slots) (2) 
 
We classify the phrase P as belonging to an anchor 
type Cj when its Entity_score(P) is above an em-
pirically determined threshold ?. We refer to this 
anchor as Aj. We allow a phrase to belong to mul-
tiple anchor types and hence the anchors alone are 
not enough for filling templates. 
4.2 Relations 
To resolve the correct filling of phrase P of type Ci 
in a desired slot in the template, we need to con-
sider the relations between multiple candidate 
phrases of related slots. To do so, we consider sev-
eral types of relations between anchors: discourse, 
dependency and semantic relations. These relations 
capture the interactions between anchors and are 
therefore useful for tackling the paraphrasing and 
alignment problems (Maslennikov et al, 2006). 
Given 2 anchors Ai and Aj of anchor types Ci and 
Cj, we consider a relation Pathl = [Ai, Rel1,?, 
Reln, Aj] between them, such that there are no an-
chors between Ai and Aj. Additionally, we assume 
that the relations between anchors are represented 
in the form of a tree Tl, where l = {s, c, d} refers to 
Satellite 
who discovers all kinds of on and 
scary things going on in a seem-
ingly quiet little town. 
Nucleus 
It's a dark 
comedy 
about a boy 
Satellite 
named Mar-
shal 
Nucleus 
played by 
Amourie Kats 
Nucleus Satellite
span elaboration 
span elaboration elaboration span 
Figure 2. Exam-
ple of anchor 
Anchor Ai 
 
Marshal 
pos_NNP 
list_personWord 
Cand_AtArg1 
Minipar_obj 
Arg2 
Spade_Satellite 
594
discourse, dependency and semantic relation types 
respectively. We describe the nodes and edges of 
Tl separately for each type, because their represen-
tations are different: 
1) The nodes of discourse tree Tc consist of clauses 
[Clause1, ?, ClauseNcl]; and their relation edges 
are obtained from the Spade system described in 
Soricut and Marcu (2003). This system performs 
RST-based parsing at the sentence level. The re-
ported accuracy of Spade is 49% on the RST-DT 
corpus. To obtain a clausal path, we map each 
anchor Ai to its clause in Spade. If anchors Ai 
and Aj belong to the same clause, we assign 
them the relation same-clause. 
es.  
2) The nodes of dependency tree Td consist of 
words in sentences; and their relation edges are 
obtained from Minipar by Lin (1997). Lin 
(1997) reported a parsing performance of Preci-
sion = 88.5% and Recall = 78.6% on the SU-
SANNE corpus. 
3) The nodes of semantic tree Ts consist of argu-
ments [Arg0, ?, ArgNarg] and targets [Target1, 
?, TargetNtarg]. Both arguments and targets are 
obtained from the ASSERT parser developed by 
Pradhan (2004). The reported performance of 
ASSERT is F1=83.8% on the identification and 
classification task for all arguments, evaluated 
using PropBank and AQUAINT as the training 
and testing corpora, respectively. Since the rela-
tion edges have a form Targetk -> Argl, the rela-
tion path in semantic frame contains only a sin-
gle relation. Therefore, we encode semantic rela-
tions as part of the anchor features.  
In later parts of this paper, we consider only dis-
course and dependency relation paths Pathl, where 
l={c, d}. 
 
 
Figure 3. Architecture of the system 
4.3 Architecture of ARE system 
In order to perform IE, it is important to extract 
candidate entities (anchors) of appropriate anchor 
types, evaluate the relationships between them, 
further evaluate all possible candidate templates, 
and output the final template. For the case of rela-
tion extraction task, the final templates are the 
same as an extracted binary relation. The overall 
architecture of ARE is given in Figure 3. 
The focus of this paper is in applying discourse 
relations for binary relationship evaluation. 
5 Overall approach 
In this section, we describe our relation-based ap-
proach to IE. We start with the evaluation of rela-
tion paths (single relation ranking, relation path 
ranking) to assess the suitability of their anchors as 
entities to template slots. Here we want to evaluate 
given a single relation or relation path, whether the 
two anchors are correct in filling the appropriate 
slots in a template. This is followed by the integra-
tion of relation paths and evaluation of templates. 
5.1 Evaluation of relation path 
In the first stage, we evaluate from training data 
the relevance of relation path Pathl = [Ai, Rel1,?, 
Reln, Aj] between candidate anchors Ai and Aj of 
types Ci and Cj. We divide this task into 2 steps. 
The first step ranks each single relation Relk ? 
Pathl; while the second step combines the evalua-
tions of Relk to rank the whole relation path Pathl.  
Single relation ranking 
Let Seti and Setj be the set of linguistic features of 
anchors Ai and Aj respectively. To evaluate Relk, 
we consider 2 characteristics: (1) the direction of 
relation Relk as encoded in the tree structure; and 
(2) the linguistic features, Seti and Setj, of anchors 
Ai and Aj. We need to construct multiple single 
relation classifiers, one for each anchor pair of 
types Ci and Cj, to evaluate the relevance of Relk 
with respect to these 2 anchor typ
Preprocessing Corpus 
 
(a) Construction of classifiers. The training data 
to each classifier consists of anchor pairs of types 
Ci and Cj extracted from the training corpus. We 
use these anchor pairs to construct each classifier 
in four stages. First, we compose the set of possi-
ble patterns in the form P+ = { Pm = <Si ?Rel-> 
Sj> | Si ? Seti , Sj ? Setj }. The construction of Pm 
Anchor 
evaluation 
Templates 
Anchor NEs 
Template 
evaluation 
Sentences 
Binary relationship 
evaluation 
Candidate 
templates 
595
conforms to the 2 characteristics given above. 
Figure 4 illustrates several discourse and depend-
ency patterns of P+ constructed from a sample sen-
tence.  
 
 
Figure 4.  Examples of discourse and dependency patterns 
Second, we identify the candidate anchor A, 
whose type matches slot C in a template. Third, we 
find the correct patterns for the following 2 cases: 
1) Ai, Aj are of correct anchor types; and 2) Ai is an 
action anchor, while Aj is a correct anchor. Any 
other patterns are considered as incorrect. We note 
that the discourse and dependency paths between 
anchors Ai and Aj are either correct or wrong si-
multaneously. 
  
Fourth, we evaluate the relevance of each pat-
tern Pm ? P+. Given the training set, let PairSetm 
be the set of anchor pairs extracted by Pm; and 
PairSet+(Ci, Cj) be the set of correct anchor pairs 
of types Ci, Cj. We evaluate both precision and 
recall of Pm as
 
   
 ||||
|),(||
)(
m
jim
m PairSet
CCPairsSetPairSet
PrecisionP
|=
+?  
 (3) 
 
   
 ||),(||
|),(||
)(
ji
jim
m CCPairsSet
CCPairsSetPairSet
PecallR +
+ |= ?   (4) 
 
These values are stored and used in the training 
model for use during testing. 
 
(b) Evaluation of relation. Here we want to 
evaluate whether relation InputRel belongs to a 
path between anchors InputAi and InputAj. We 
employ the constructed classifier for the anchor 
types InputCi and InputCj in 2 stages. First, we 
find a subset P(0) = { Pm = <Si ?InputRel-> Sj> ? 
P+  | Si ? InputSeti, Sj ? InputSetj } of applicable 
patterns. Second, we utilize P(0) to find the pattern 
Pm(0) with maximal precision: 
 
 
 
 Precision(Pm
(0)) = argmaxPm?P(0) Precision (Pm) (5) 
 
A problem arises if Pm(0) is evaluated only on a 
small amount of training instances. For example, 
we noticed that patterns that cover 1 or 2 instances 
may lead to Precision=1, whereas on the testing 
corpus their accuracy becomes less than 50%. 
Therefore, it is important to additionally consider 
the recall parameter of Pm(0). 
Relation path ranking 
In this section, we want to evaluate relation path 
connecting template slots Ci and Cj. We do this 
independently for each relation of type discourse 
and dependency. Let Recallk and Precisionk be the 
recall  and precision values of Relk in Path = [Ai, 
Rel1,?, Reln, Aj], both obtained from the previous 
step. First, we calculate the average recall of the 
involved relations: 
 
 W = (1/LengthPath) * ?Relk?Path Recallk (6) 
 
W gives the average recall of the involved rela-
tions and can be used as a measure of reliability of 
the relation Path. Next, we compute a combined 
score of average Precisionk weighted by Recallk:  
 
 Score = 1/(W*LengthPath)*?Relk?Path Recallk*Precisionk (7) 
 
We use all Precisionk values in the path here, be-
cause omitting a single relation may turn a correct 
path into the wrong one, or vice versa. The com-
bined score value is used as a ranking of the rela-
tion path. Experiments show that we need to give 
priority to scores with higher reliability W. Hence 
we use (W, Score) to evaluate each Path.  
5.2 Integration of different relation path 
types 
The purpose of this stage is to integrate the evalua-
tions for different types of relation paths. The input 
to this stage consists of evaluated relation paths 
PathC and PathD for discourse and dependency 
relations respectively. Let (Wl, Scorel) be an 
evaluation for Pathl, l ? [c, d]. We first define an 
integral path PathI between Ai and Aj as: 1) PathI 
is enabled if at least one of Pathl, l ? [c, d], is en-
abled; and 2) PathI is correct if at least one of 
Pathl is correct. To evaluate PathI, we consider the 
average recall Wl of each Pathl, because Wl esti-
elaboration 
obj 
Anchor Aj 
 
town 
pos_NN 
Cand_AtArg2 
Minipar_pcompn
ArgM-Loc 
Spade_Satellite
Anchor Ai 
 
Marshal 
pos_NNP 
list_personWord 
Cand_AtArg1 
Minipar_obj 
Arg2 
Spade Satellite 
pcomp-n
fc 
span 
Discourse path 
Dependency path 
i 
elaboration 
Input sentence 
Marshal? named <At-A1> </> played by Amourie Kats who discovers all kinds 
of on and scary things going on in <At-A2>
Dependency patterns 
 
Minipar_obj <?i- ArgM-Loc 
Minipar_obj <?obj- ArgM-Loc 
Minipar_obj ?pcompn-> Minipar_pcompn
Minipar_obj ?mod-> Minipar_pcompn 
?
a seemingly quiet little town</> ... 
elaboration 
pnmod 
pred i 
null 
null 
rel 
i mod 
Discourse patterns 
 
list_personWord <?elaboration- pos_NN 
list_personWord ?elaboration-> town 
list_personWord <?span- town 
list_personWord <?elaboration- town 
? 
596
mates the reliability of Scorel. We define a 
weighted average for Pathl as: 
 
 WI = WC + WD (8) 
 
 ScoreI = 1/WI * ? l  Wl*Scorel (9) 
 
Next, we want to determine the threshold score 
ScoreIO above which ScoreI is acceptable. This 
score may be found by analyzing the integral paths 
on the training corpus. Let SI = { PathI } be the set 
of integral paths between anchors Ai and Aj on the 
training set. Among the paths in SI, we need to de-
fine a set function SI(X) = { PathI | ScoreI(PathI) 
? X } and find the optimal threshold for X. We find 
the optimal threshold based on F1-measure, be-
cause precision and recall are equally important in 
IE. Let SI(X)+ ? SI(X) and S(X)+ ? S(X) be sets of 
correct path extractions. Let FI(X) be F1-measure 
of SI(X): 
 
    
 ||)(||
||)(||
)(
XS
XS
XP
I
I
I
+
=  (10)  
||)(||
||)(||
)( +
+
=
XS
XS
XR II
 (11) 
 
  
 )()(
)(*)(*2
)(
XRXP
XRXP
XF
II
II
I +=
 (12) 
 
Based on the computed values FI(X) for each X on 
the training data, we determine the optimal thresh-
old as Score  = argmax  F  (X)IO X I , which corre-
sponds to the maximal expected F1-measure of 
anchor pair Ai and Aj.  
5.3 Evaluation of templates 
At this stage, we have a set of accepted integral 
relation paths between any anchor pair Ai and Aj. 
The next task is to merge appropriate set of an-
chors into candidate templates. Here we follow the 
methodology of Maslennikov et al (2006). For 
each sentence, we compose a set of candidate tem-
plates T using the extracted relation paths between 
each Ai and Aj. To evaluate each template Ti?T, 
we combine the integral scores from relation paths 
between its anchors Ai and Aj into the overall Rela-
tion_ScoreT: 
 
  
 
M
AAScore
TScoreelationR Kji
jiI
iT
? ??= ,1 ),()(_  (13) 
 
where K is the number of extracted slots, M is the 
number of extracted relation paths between an-
chors Ai and Aj, and ScoreI(Ai, Aj) is obtained 
from Equation (9). 
Next, we calculate the extracted entity score 
based on the scores of all the anchors in Ti: 
 
 ? ??= Kk kiT KAScoreEntityTScoreEntity 1 /)(_)(_  (14) 
 
where Entity_Score(Ai) is taken from Equation (1).  
Finally, we obtain the combined evaluation for a 
template:  
 
  
 
ScoreT(Ti) = (1- ?) * Entity_ScoreT (Ti) + 
                           ?  * Relation_ScoreT (Ti) (15) 
 
where ? is a predefined constant. 
In order to decide whether the template Ti 
should be accepted or rejected, we need to deter-
mine a threshold ScoreTO from the training data. If 
anchors of a candidate template match slots in a 
correct template, we consider the candidate tem-
plate as correct. Let TrainT = { Ti }  be the set of 
candidate templates extracted from the training 
data, TrainT+ ? TrainT be the subset of correct 
candidate templates, and TotalT+ be the total set of 
correct templates in the training data. Also, let 
TrainT(X) = { Ti | ScoreT(Ti) ? X, Ti ? TrainT } be 
the set of candidate templates with score above X 
and TrainT+(X) ? TrainT(X) be the subset of cor-
rect candidate templates. We define the measures 
of precision, recall and F1 as follows: 
 
   
 ||)(||
||)(||
)(
XTrainT
XTrainT
XPT
+
=  (16) ||||
||)(||
)( +
+
=
TotalT
XTrainT
XRT (17) 
 
   
 )()(
)()(*2
)(
XRXP
XRXP
XF
TT
TT
T +=
 
 
 
(18) 
 
Since the performance in IE is measured in F1-
measure, an appropriate threshold to be used for 
the most prominent candidate templates is: 
 
 ScoreT
O = argmaxX FT (X) (19) 
 
The value ScoreTO is used as a training model. 
During testing, we accept a candidate template In-
putTi if ScoreT(InputTi) > Sco Ore . T
As an additional remark, we note that domains 
MUC4, MUC6 and ACE RDC 2003 are signifi-
cantly different in the evaluation methodology for 
the candidate templates. While the performance of 
the MUC4 domain is measured for each slot indi-
vidually; the MUC6 task measures the perform-
ance on the extracted templates; and the ACE RDC 
2003 task evaluates performance on the matching 
relations. To overcome these differences, we con-
struct candidate templates for all the domains and 
measure the required type of performance for each 
domain. Our candidate templates for the ACE 
RDC 2003 task consist of only 2 slots, which cor-
respond to entities of the correct relations. 
597
6 Experimental results 
We carry out our experiments on 3 domains: 
MUC4 (Terrorism), MUC6 (Management Succes-
sion), and ACE-Relation Detection and Characteri-
zation (2003). The MUC4 corpus contains 1,300 
documents as training set and 200 documents 
(TST3 and TST4) as official testing set. We used a 
modified version of the MUC6 corpus described 
by Soderland (1999). This version includes 599 
documents as training set and 100 documents as 
testing set. Following the methodology of Zhang et 
al. (2006), we use only the English portion of ACE 
RDC 2003 training data. We used 97 documents 
for testing and the remaining 155 documents for 
training. Our task is to extract 5 major relation 
types and 24 subtypes. 
 
Case (%) P R F1
GRID 52% 62% 57% 
Riloff?05 46% 51% 48% 
ARE (2006) 58% 61% 60% 
ARE 65% 61% 63% 
Table 2. Results on MUC4 
To compare the results on the terrorism domain 
in MUC4, we choose the recent state-of-art sys-
tems GRID by Xiao et al (2004), Riloff et al 
(2005) and ARE (2006) by Maslennikov et al 
(2006) which does not utilize discourse and seman-
tic relations. The comparative results are given in 
Table 2. It shows that our enhanced ARE results in 
3% improvement in F1 measure over ARE (2006) 
that does not use clausal relations. The improve-
ment was due to the use of discourse relations on 
long paths, such as ?X distributed leaflets claiming 
responsibility for murder of Y?. At the same time, 
for many instances, it would be useful to store the 
extracted anchors for another round of learning. 
For example, the extracted features of discourse 
pattern ?murder ?same_clause-> HUM_PERSON? 
may boost the score for patterns that correspond to 
relation path ?X <-span- _ -Elaboration->  mur-
der?. In this way, high-precision patterns will sup-
port the refinement of patterns with average recall 
and low precision. This observation is similar to 
that described in Ciravegna?s work on (LP)2 
(Ciravegna 2001). 
 
Case (%) P R F1  
Chieu et al?02 75% 49% 59% 
ARE (2006) 73% 58% 65% 
ARE 73% 70% 72% 
Table 3. Results on MUC6 
Next, we present the performance of our system 
on MUC6 corpus (Management Succession) as 
shown in Table 3. The improvement of 7% in F1 is 
mainly due to the filtering of irrelevant depend-
ency relations. Additionally, we noticed that 22% 
of testing sentences contain 2 answer templates, 
and entities in many of such templates are inter-
twined. One example is the sentence ?Mr. Bronc-
zek who is 39 years old succeeds Kenneth Newell 
55 who was named to the new post of senior vice 
president?, which refers to 2 positions. We there-
fore we need to extract 2 templates ?PersonIn: 
Bronczek, PersonOut: Newell? and ?PersonIn: 
Newell, Post: senior vice president?. The discourse 
analysis is useful to extract the second template, 
while rejecting another long-distance template 
?PersonIn: Bronczek, PersonOut: Newell, Post: 
seniour vice president?. Another remark is that it 
is important to assign 2 anchors of 
?Cand_PersonIn? and ?Cand_PersonOut? for the 
phrase ?Kenneth Newell?.  
The characteristic of the ACE corpus is that it 
contains a large amount of variations, while only 
2% of possible dependency paths are correct. Since 
many of the relations occur only at the level of sin-
gle clause (for example, most instances of relation 
At), the discourse analysis is used to eliminate 
long-distance dependency paths. It allows us to 
significantly decrease the dimensionality of the 
problem. We noticed that 38% of relation paths in 
ACE contain a single relation, 28% contain 2 rela-
tions and 34% contain ? 3 relations. For the case 
of  ? 3 relations, the analysis of dependency paths 
alone is not sufficient to eliminate the unreliable 
paths. Our results for general types and specific 
subtypes are presented in Tables 6 and 7, respec-
tively. 
 
 
Case (%) P R F1  
Zhang et al?06 77% 65% 70% 
ARE 79% 66% 73% 
Table 4. Results on ACE RDC?03, general types 
Based on our results in Table 4, discourse and 
dependency relations support each other in differ-
ent situations. We also notice that multiple in-
stances require modeling of entities in the path. 
Thus, in our future work we need to enrich the 
search space for relation patterns. This observation 
corresponds to that reported in Zhang et al (2006). 
Discourse parsing is very important to reduce 
the amount of variations for specific types on ACE  
598
RDC?03, as there are 48 possible anchor types.  
 
Case (%) P R F1
Zhang et al?06 64% 51% 57% 
ARE 67% 54% 61% 
Table 5. Results on ACE RDC?03, specific types 
The relatively small improvement of results in 
Table 5 may be attributed to the following reasons: 
1) it is important to model the commonality rela-
tions, as was done by Zhou et al (2006); and 2) 
our relation paths do not encode entities. This is 
different from Zhang et al (2006), who were using 
entities in their subtrees. 
Overall, the results indicate that the use of dis-
course relations leads to improvement over the 
state-of-art systems.  
7 Conclusion 
We presented a framework that permits the inte-
gration of discourse relations with dependency re-
lations. Different from previous works, we tried to 
use the information about sentence structure based 
on discourse analysis. Consequently, our system 
improves the performance in comparison with the 
state-of-art IE systems. Another advantage of our 
approach is in using domain-independent parsers 
and features. Therefore, ARE may be easily port-
able into new domains.  
Currently, we explored only 2 types of relation 
paths: dependency and discourse. For future re-
search, we plan to integrate more relations in our 
multi-resolution framework. 
References 
P. Cimiano and U. Reyle. 2003. Ontology-based semantic 
construction, underspecification and disambiguation. In 
Proc of the Prospects and Advances in the Syntax-
Semantic Interface Workshop. 
P. Cimiano, U. Reyle and J. Saric. 2005. Ontology-driven 
discourse analysis for information extraction. Data & 
Knowledge Engineering, 55(1):59-83. 
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Ap-
proach to Information Extraction from Semi-Structured 
and Free Text. In Proc of AAAI-2002. 
F. Ciravegna. 2001. Adaptive Information Extraction from 
Text by Rule Induction and Generalization. In Proc of 
IJCAI-2001. 
A. Culotta and J. Sorensen J. 2004. Dependency tree ker-
nels for relation extraction. In Proc of ACL-2004. 
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum 
likelihood from incomplete data via the EM algorithm. 
Journal of the Royal Statistical Society B, 39(1):1?38. 
B. Grosz and C. Sidner. 1986. Attention, Intentions and  
the Structure of Discourse. Computational Linguistics, 
12(3):175-204. 
M. Halliday and R. Hasan. 1976. Cohesion in English. 
Longman, London. 
D. Lin. 1997. Dependency-based Evaluation of Minipar. In 
Workshop on the Evaluation of Parsing systems. 
M. Maslennikov, H.K. Goh and T.S. Chua. 2006. ARE: 
Instance Splitting Strategies for Dependency Relation-
based Information Extraction. In Proc of ACL-2006. 
E. Miltsakaki. 2003. The Syntax-Discourse Interface: Ef-
fects of the Main-Subordinate Distinction on Attention 
Structure. PhD thesis. 
M.F. Moens and R. De Busser. 2002. First steps in building 
a model for the retrieval of court decisions. International 
Journal of Human-Computer Studies, 57(5):429-446. 
S. Pradhan, W. Ward, K. Hacioglu, J. Martin and D. Juraf-
sky. 2004. Shallow Semantic Parsing using Support 
Vector Machines. In Proc of HLT/NAACL-2004. 
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting Sub-
jectivity Classification to Improve Information Extrac-
tion. In Proc of AAAI-2005. 
S. Soderland. 1999. Learning Information Extraction Rules 
for Semi-Structured and Free Text. Machine Learning, 
34:233-272. 
R. Soricut and D. Marcu. 2003. Sentence Level Discourse 
Parsing using Syntactic and Lexical Information. In 
Proc of HLT/NAACL. 
M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth. 2003. 
Using Predicate Arguments Structures for Information 
Extraction. In Proc of ACL-2003. 
M. Taboada and W. Mann. 2005. Applications of Rhetori-
cal Structure Theory. Discourse studies, 8(4). 
B. Webber, M. Stone, A. Joshi and A. Knott. 2002. 
Anaphora and Discourse Structure. Computational Lin-
guistics, 29(4). 
J. Xiao, T.S. Chua and H. Cui. 2004. Cascading Use of 
Soft and Hard Matching Pattern Rules for Weakly Su-
pervised Information Extraction. In Proc of COLING-
2004. 
M. Zhang, J. Zhang, J. Su and G. Zhou. 2006. A Compos-
ite Kernel to Extract Relations between Entities with 
both Flat and Structured Features. In Proc of ACL-2006. 
G. Zhou, J. Su and M. Zhang. 2006. Modeling Commonal-
ity among Related Classes in Relation Extraction. In 
Proc of ACL-2006. 
599
