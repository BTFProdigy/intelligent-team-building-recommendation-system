Cut and Paste Based Text Summarizat ion 
Hongyan J ing  and  Kath leen  R .  McKeown 
Depar tment  of Computer  Sc ience 
Co lumbia  Un ivers i ty  
New York,  NY  10027, USA 
hj ing,  kathyQcs .co lumbia .edu  
Abst ract  
We present a cut and paste based text summa- 
rizer, which uses operations derived from an anal- 
ysis of human written abstracts. The summarizer 
edits extracted sentences, using reduction to remove 
inessential phrases and combination to merge re- 
suiting phrases together as coherent sentences. Our 
work includes a statistically based sentence decom- 
position program that identifies where the phrases of 
a summary originate in the original document, pro- 
ducing an aligned corpus of summaries and articles 
which we used to develop the summarizer. 
1 I n t roduct ion  
There is a big gap between the summaries produced 
by current automatic summarizers and the abstracts 
written by human professionals. Certainly one fac- 
tor contributing to this gap is that automatic sys- 
tems can not always correctly identify the important 
topics of an article. Another factor, however, which 
has received little attention, is that automatic sum- 
marizers have poor text generation techniques. Most 
automatic summarizers rely on extracting key sen- 
tences or paragraphs from an article to produce a 
summary. Since the extracted sentences are discon- 
nected in the original article, when they are strung 
together, the resulting summary can be inconcise, 
incoherent, and sometimes even misleading. 
We present a cut and paste based text sum- 
marization technique, aimed at reducing the gap 
between automatically generated summaries and 
human-written abstracts. Rather than focusing 
on how to identify key sentences, as do other re- 
searchers, we study how to generate the text of a 
summary once key sentences have been extracted. 
The main idea of cut and paste summarization 
is to reuse the text in an article to generate the 
summary. However, instead of simply extracting 
sentences as current summarizers do, the cut and 
paste system will "smooth" the extracted sentences 
by editing them. Such edits mainly involve cutting 
phrases and pasting them together in novel ways. 
The key features of this work are: 
(1) The identification of cutting and past- 
ing operations. We identified six operations that 
can be used alone or together to transform extracted 
sentences into sentences in human-written abstracts. 
The operations were identified based on manual and 
automatic omparison of human-written abstracts 
and the original articles. Examples include sentence 
reduction, sentence combination, syntactic transfor- 
mation, and lexical paraphrasing. 
(2) Deve lopment  o f  an automatic system to 
per form cut and paste operations. Two opera- 
tions - sentence reduction and sentence combination 
- are most effective in transforming extracted sen- 
tences into summary sentences that are as concise 
and coherent as in human-written abstracts. We 
implemented a sentence reduction module that re- 
moves extraneous phrases from extracted sentences, 
and a sentence combination module that merges the 
extracted sentences or the reduced forms resulting 
from sentence reduction. Our sentence reduction 
model determines what to cut based on multiple 
sources of information, including syntactic knowl- 
edge, context, and statistics learned from corpus 
analysis. It improves the conciseness of extracted 
sentences, making them concise and on target. Our 
sentence combination module implements combina- 
tion rules that were identified by observing examples 
written by human professionals. It improves the co- 
herence of extracted sentences. 
(3) Decomposing human-wrltten summary  
sentences. The cut and paste technique we propose 
here is a new computational model which we based 
on analysis of human-written abstracts. To do this 
analysis, we developed an automatic system that can 
match a phrase in a human-written abstract o the 
corresponding phrase in the article, identifying its 
most likely location. This decomposition program 
allows us to analyze the construction of sentences 
in a human-written abstract. Its results have been 
used to train and test the sentence reduction and 
sentence combination module. 
In Section 2, we discuss the cut and paste tech- 
nique in general, from both a professional nd com- 
putational perspective. We also describe the six cut 
and paste operations. In Section 3, we describe the 
178 
system architecture. The major components of the 
system, including sentence reduction, sentence com- 
bination, decomposition, and sentence selection, are 
described in Section 4. The evaluation results are 
shown in Section 5. Related work is discussed in 
Section 6. Finally, we conclude and discuss future 
work. 
Document sentence: When it arrives some- 
time next year in new TV sets, the V-chip will 
give parents a new and potentially revolution- 
ary device to block out programs they don't 
want their children to see. 
Summary sentence: The V-chip will give par- 
ents a device to block out programs they don't 
want their children to see. 
2 Cut  and paste  in  summar izat ion  
2.1 Re la ted  work  in professional 
summarizing 
Professionals take two opposite positions on whether 
a summary should be produced by cutting and past- 
ing the original text. One school of scholars is 
opposed; "(use) your own words... Do not keep 
too close to the words before you", states an early 
book on abstracting for American high school stu- 
dents (Thurber, 1924). Another study, however, 
shows that professional abstractors actually rely on 
cutting and pasting to produce summaries: "Their 
professional role tells abstractors to avoid inventing 
anything. They follow the author as closely as pos- 
sible and reintegrate the most important points of 
a document in a shorter text" (Endres-Niggemeyer 
et al, 1998). Some studies are somewhere in be- 
tween: "summary language may or may not follow 
that of author's" (Fidel, 1986). Other guidelines or 
books on abstracting (ANSI, 1997; Cremmins, 1982) 
do not discuss the issue. 
Our cut and paste based summarization is a com- 
putational model; we make no claim that humans 
use the same cut and paste operations. 
2.2 Cut and paste operations 
We manually analyzed 30 articles and their corre- 
sponding human-written summaries; the articles and 
their summaries come from different domains ( 15 
general news reports, 5 from the medical domain, 
10 from the legal domain) and the summaries were 
written by professionals from different organizations. 
We found that reusing article text for summarization 
is almost universal in the corpus we studied. We de- 
fined six operations that can be used alone, sequen- 
tially, or simultaneously to transform selected sen- 
tences from an article into the corresponding sum- 
mary sentences in its human-written abstract: 
(1) sentence reduction 
Remove extraneous phrases from a selected sen- 
tence, as in the following example 1: 
1 All the examples in this section were produced by human 
professionals 
The deleted material can be at any granularity: a
word, a phrase, or a clause. Multiple components 
can be removed. 
(2) sentence combination 
Merge material from several sentences. It can be 
used together with sentence reduction, as illustrated 
in the following example, which also uses paraphras- 
ing: 
Text Sentence 1: But it also raises serious 
questions about the privacy of such highly 
personal information wafting about the digital 
world. 
Text Sentence 2: The issue thus fits squarely 
into the broader debate about privacy and se- 
curity on the internet, whether it involves pro- 
tecting credit card number or keeping children 
from offensive information. 
Summary sentence: But it also raises the is- 
sue of privacy of such personal information 
and this issue hits the head on the nail in the 
broader debate about privacy and security on 
the internet. 
(3) syntactic transformation 
In both sentence reduction and combination, syn- 
tactic transformations may be involved. For exam- 
ple, the position of the subject in a sentence may be 
moved from the end to the front. 
(4) lexical paraphrasing 
Replace phrases with their paraphrases. For in- 
stance, the summaries ubstituted point out with 
note, and fits squarely into with a more picturesque 
description hits the head on the nail in the previous 
examples. 
(5) genera l i za t ion  or specification 
Replace phrases or clauses with more general or 
specific descriptions. Examples of generalization 
and specification include: 
Generalization: "a proposed new law that 
would require Web publishers to obtain 
parental consent before collecting personal in- 
formation from children" --+ "legislation to 
protect children's privacy on-line" 
Specification: "the White House's top drug 
official" ~ "Gen. Barry R. McCaffrey, the 
White House's top drug official" 
179 
p . . . . .  
,_e_ yr _', - 
I , Co-reference ~, 
I . . . . . . . . .  I 
,\]WordNet'l 
~ned ie~ -~ 
Input~icle . I ~  
I Sentenc i extracti?nl ) 
extracteikey sentenc~ 
Cut and paste based generation 
\[ Sentence r duction \] 
I Sentence combinatio~ 
Output summary 
Figure 1: System architecture 
(6) reorder ing  
Change the order of extracted sentences. For in- 
stance, place an ending sentence in an article at the 
beginning of an abstract. 
In human-written abstracts, there are, of course, 
sentences that are not based on cut and paste, but 
completely written from scratch. We used our de- 
composition program to automatically analyze 300 
human-written abstracts, and found that 19% of sen- 
tences in the abstracts were written from scratch. 
There are also other cut and paste operations not 
listed here due to their infrequent occurrence. 
3 Sys tem arch i tec ture  
The architecture of our cut and paste based text 
summarization system is shown in Figure 1. Input 
to the system is a single document from any domain. 
In the first stage, extraction, key sentences in the ar- 
ticle are identified, as in most current summarizers. 
In the second stage, cut and paste based generation, a 
sentence reduction module and a sentence combina- 
tion module implement the operations we observed 
in human-written abstracts. 
The cut and paste based component receives as 
input not only the extracted key sentences, but also 
the original article. This component can be ported 
to other single-document summarizers to serve as 
the generation component, since most current sum- 
marizers extract key sentences - exactly what the 
extraction module in our system does. 
Other resources and tools in the summarization 
system include a corpus of articles and their human- 
written abstracts, the automatic decomposition pro- 
gram, a syntactic parser, a co-reference resolution 
system, the WordNet lexical database, and a large- 
scale lexicon we combined from multiple resources. 
The components in dotted lines are existing tools or 
resources; all the others were developed by ourselves. 
4 Ma jor  components  
The main focus of our work is on decomposition of
summaries, sentence reduction, and sentence com- 
bination. We also describe the sentence xtraction 
module, although it is not the main focus of our 
work .  
4.1 Decomposi t ion of human-wr i t ten  
summary  sentences 
The decomposition program, see (Jing and McKe- 
own, 1999) for details, is used to analyze the con- 
struction of sentences in human-written abstracts. 
The results from decomposition are used to build 
the training and testing corpora for sentence reduc- 
tion and sentence combination. 
The decomposition program answers three ques- 
tions about a sentence in a human-written abstract: 
(1) Is the sentence constructed by cutting and past- 
ing phrases from the input article? (2) If so, what 
phrases in the sentence come from the original arti- 
cle? (3) Where in the article do these phrases come 
from? 
We used a Hidden Markov Model (Baum, 1972) 
solution to the decomposition problem. We first 
mathematically formulated the problem, reducing it 
to a problem of finding, for each word in a summary 
180 
Summary sentence: 
(F0:S1 arthur b sackler v ice pres ident  for law and publ ic  po l i cy  of  t ime warner  inc ) 
(FI:S-1 and) (F2:S0 a member  of  the direct market ing  assoc iat ion told ) (F3:$2 the com-  
mun icat ions  subcommit tee  o f  the senate  commerce  commit tee  ) (F4:S-1 that legislation ) 
(F5:Slto protect  ) (F6:$4 chi ldren' s ) (F7:$4 pr ivacy ) (F8:$4 onl ine ) (F9:S0 could dest roy  
the spontaneous  nature that makes  the internet  unique ) 
Source document sentences: 
Sentence 0: a proposed new law that would require web publishers to obtain parental consent before 
collecting personal information from children (F9 could destroy the spontaneous  nature  that  
makes  the internet  un ique ) (F2 a member  of the direct market ing  assoc iat ion told) a 
senate panel thursday 
Sentence 1:(F0 arthur  b sackler v ice pres ident  for law and publ ic  po l icy  of  t ime warner  
inc ) said the association supported efforts (F5 to protect  ) children online but he urged lawmakers 
to find some middle ground that also allows for interactivity on the internet 
Sentence 2: for example a child's e-mail address is necessary in order to respond to inquiries such 
as updates on mark mcguire's and sammy sosa's home run figures this year or updates of an online 
magazine sackler said in testimony to (F3 the communicat ions  subcommit tee  o f  the senate 
commerce  commit tee  ) 
Sentence 4: the subcommittee is considering the (F6 children's ) (F8 online ) (F7 privacy ) 
protection act which was drafted on the recommendation f the federal trade commission 
Figure 2: Sample output of the decomposition program 
sentence, a document position that it most likely 
comes from. The position of a word in a document 
is uniquely identified by the position of the sentence 
where the word appears, and the position of the word 
within the sentence. Based on the observation of cut 
and paste practice by humans, we produced a set of 
general heuristic rules. Sample heuristic rules in- 
clude: two adjacent words in a summary sentence 
are most likely to come from two adjacent words in 
the original document; adjacent words in a summary 
sentence are not very likely to come from sentences 
that are far apart in the original document. We 
use these heuristic rules to create a Hidden Markov 
Model. The Viterbi algorithm (Viterbi, 1967) is used 
to efficiently find the most likely document position 
for each word in the summary sentence. 
Figure 2 shows sample output of the program. 
For the given summary sentence, the program cor- 
rectly identified that the sentence was combined 
from four sentences in the input article. It also di- 
vided the summary sentence into phrases and pin- 
pointed the exact document origin of each phrase. 
A phrase in the summary sentence is annotated as 
(FNUM:SNUM actual-text), where FNUM is the se- 
quential number of the phrase and SNUM is the 
number of the document sentence where the phrase 
comes from. SNUM = -1 means that the compo- 
nent does not come from the original document. The 
phrases in the document sentences are annotated as 
(FNUM actual-text). 
4.2 Sentence  reduct ion  
The task of the sentence reduction module, de- 
scribed in detail in (Jing, 2000), is to remove extra- 
neous phrases from extracted sentences. The goal of 
reduction is to "reduce without major loss"; that is, 
we want to remove as many extraneous phrases as 
possible from an extracted sentence so that it can be 
concise, but without detracting from the main idea 
that the sentence conveys. Ideally, we want to re- 
move a phrase from an extracted sentence only if it 
is irrelavant to the main topic. 
Our reduction module makes decisions based on 
multiple sources of knowledge: 
(1) Grammar  checking.  In this step, we mark 
which components of a sentence or a phrase are 
obligatory to keep it grammatical ly correct. To do 
this, we traverse the sentence parse tree, produced 
by the English Slot Grammar(ESG) parser devel- 
oped at IBM (McCord, 1990), in top-down order 
and mark for each node in the parse tree, which 
of its children are obligatory. The main source of 
knowledge the system relies on in this step is a 
large-scale, reusable lexicon we combined from mul- 
tiple resources (Jing and McKeown, 1998). The lexi- 
con contains ubcategorizations for over 5,000 verbs. 
This information is used to mark the obligatory ar- 
guments of verb phrases. 
(2) Context  in fo rmat ion .  We use an extracted 
sentence's local context in the article to decide which 
components in the sentence are likely to be most 
relevant o the main topic. We link the words in the 
extracted sentence with words in its local context, 
if they are repetitions, morphologically related, or 
linked with each other in WordNet through certain 
type of lexical relation, such as synonymy, antonymy, 
or meronymy. Each word in the extracted sentence 
gets an importance score, based on the number of 
links it has with other words and the types of links. 
Each phrase in the sentence is then assigned a score 
181 
Example 1: 
Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give 
parents a new and potentially revolutionary device to block out programs they don't 
want their children to see. 
Reduction program: The V-chip will give parents a new and potentially revolutionary device to 
block out programs they don't want their children to see. 
Professionals : The V-chip will give parents a device to block out programs they don't want 
their children to see. 
Example 2: 
Original sentence : Sore and Hof fman's  creation would allow broadcasters to insert 
multiple ratings into a show, enabling the V-chip to filter out racy or violent material but leave 
unexceptional portions o.f a show alone. 
Reduction Program: Som and Hoffman's creation would allow broadcasters to insert multiple rat- 
ings into a show. 
Professionals : Som and Hoffman's creation would allow broadcasters to insert multiple rat- 
ings into a show. 
Figure 3: Sample output of the 
by adding up the scores of its children nodes in the 
parse tree. This score indicates how important he 
phrase is to the main topic in discussion. 
(3) Corpus  ev idence .  The program uses a cor- 
pus of input articles and their corresponding reduced 
forms in human-written abstracts to learn which 
components of a sentence or a phrase can be re- 
moved and how likely they are to be removed by 
professionals. This corpus was created using the de- 
composition program. We compute three types of 
probabilities from this corpus: the probability that 
a phrase is removed; the probability that a phrase is 
reduced (i.e., the phrase is not removed as a whole, 
but some components in the phrase are removed); 
and the probability that a phrase is unchanged at 
all (i.e., neither removed nor reduced). These cor- 
pus probabilities help us capture human practice. 
(4) Final  dec is ion.  The final reduction decision 
is based on the results from all the earlier steps. A 
phrase is removed only if it is not grammatically 
obligatory, not the focus of the local context (indi- 
cated by a low context importance score), and has a 
reasonable probability of being removed by humans. 
The phrases we remove from an extracted sentence 
include clauses, prepositional phrases, gerunds, and 
to-infinitives. 
The result of sentence reduction is a shortened 
version of an extracted sentence 2. This shortened 
text can be used directly as a summary, or it can 
be fed to the sentence combination module to be 
merged with other sentences. 
Figure 3 shows two examples produced by the re- 
duction program. The corresponding sentences in 
human-written abstracts are also provided for com- 
parison. 
2It is actually also possible that the reduction program 
decides no phrase in a sentence should be removed, thus the 
result of reduction is the same as the input. 
sentence reduction program 
4.3 Sentence  combinat ion  
To build the combination module, we first manu- 
ally analyzed a corpus of combination examples pro- 
duced by human professionals, automatically cre- 
ated by the decomposition program, and identified 
a list of combination operations. Table 1 shows the 
combination operations. 
To implement a combination operation, we need 
to do two things: decide when to use which com- 
bination operation, and implement he combining 
actions. To decide when to use which operation, we 
analyzed examples by humans and manually wrote 
a set of rules. Two simple rules are shown in Fig- 
ure 4. Sample outputs using these two simple rules 
are shown in Figure 5. We are currently exploring 
using machine learning techniques to learn the com- 
bination rules from our corpus. 
The implementation of the combining actions in- 
volves joining two parse trees, substituting a subtree 
with another, or adding additional nodes. We im- 
plemented these actions using a formalism based on 
Tree Adjoining Grammar  (Joshi, 1987). 
4.4 Ext rac t ion  Modu le  
The extraction module is the front end of the sum- 
marization system and its role is to extract key sen- 
tences. Our method is primarily based on lexical re- 
lations. First, we link words in a sentence with other 
words in the article through repetitions, morpholog- 
ical relations, or one of the lexical relations encoded 
in WordNet, similar to step 2 in sentence reduction. 
An importance score is computed for each word in a 
sentence based on the number of lexical links it has 
with other words, the type of links, and the direc- 
tions of the links. 
After assigning a score to each word in a sentence, 
we then compute a score for a sentence by adding up 
the scores for each word. This score is then normal- 
182 
Categories Combinat ion Operat ions 
Add descriptions or names for people or organizations 
Aggregations 
Substitute incoherent phrases 
Substitute phrases with more general or specific information 
add description (see Figure 5) 
add name 
extract common subjects or objects (see Figure 5) 
change one sentence to a clause 
add connectives (e.g., and or while) 
add punctuations (e.g., ";") 
substitute dangling anaphora 
substitute dangling noun phrases 
substitute adverbs (e.g., here) 
remove connectives 
substitute with more general information 
substitute with more specific information 
Mixed operations combination of any of above operations (see Figure 2) 
Table 1: Combination operations 
Rule 1: 
IF: ((a person or an organization is mentioned the first time) and (the full name or the full descrip- 
tion of the person or the organization exists somewhere in the original article but is missing in the 
summary)) 
THEN" replace the phrase with the full name plus the full description 
Rule 2: 
IF: ((two sentences are close to each other in the original article) and (their subjects refer to the 
same entity) and (at least one of the sentences i the reduced form resulting from sentence reduc- 
tion)) 
THEN:  merge the two sentences by removing the subject in the second sentence, and then com- 
bining it with the first sentence using connective "and". 
Figure 4: Sample sentence combination rules 
ized over the number of words a sentence contains. 
The sentences with high scores are considered im- 
portant. 
The extraction system selects sentences based on 
the importance computed as above, as well as other 
indicators, including sentence positions, cue phrases, 
and tf*idf scores. 
5 Eva luat ion  
Our evaluation includes separate valuations of each 
module and the final evaluations of the overall sys- 
tem. 
We evaluated the decomposition program by two 
experiments, described in (Jing and McKeown, 
1999). In the first experiment, we selected 50 
human-written abstracts, consisting of 305 sentences 
in total. A human subject then read the decomposi- 
tion results of these sentences to judge whether they 
are correct. 93.8% of the sentences were correctly 
decomposed. In the second experiment, we tested 
the system in a summary alignment ask. We ran 
the decomposition program to identify the source 
document sentences that were used to construct he 
sentences in human-written abstracts. Human sub- 
jects were also asked to select the document sen- 
tences that are semantlc-equivalent to the sentences 
in the abstracts. We compared the set of sentences 
identified by the program with the set of sentences 
selected by the majority of human subjects, which is 
used as the gold standard in the computation of pre- 
cision and recall. The program achieved an average 
81.5% precision, 78.5% recall, and 79.1% f-measure 
for 10 documents. The average performance of 14 
human judges is 88.8% precision, 84.4% recall, and 
85.7% f-measure. Recently, we have also tested the 
system on legal documents (the headnotes used by 
Westlaw company), and the program works well on 
those documents too. 
The evaluation of sentence reduction (see (Jing, 
2000) for details) used a corpus of 500 sentences and 
their reduced forms in human-written abstracts. 400 
sentences were used to compute corpus probabili- 
ties and 100 sentences were used for testing. The 
results show that 81.3% of the reduction decisions 
made by the system agreed with those of humans. 
The humans reduced the length of the 500 sentences 
by 44.2% on average, and the system reduced the 
length of the 100 test sentences by 32.7%. 
The evaluation of sentence combination module 
is not as straightforward as that of decomposition 
or reduction since combination happens later in the 
pipeline and it depends on the output from prior 
183 
Example  1: add descr ipt ions or names  for people or organization 
Original document sentences: 
"We're trying to prove that there are big benefits to the patients by involving them more deeply in 
their treatment", said Paul Clayton, Chairman of the Depar tment  deal ing with comput-  
erized medica l  in fo rmat ion  at Columbia. 
"The economic  payoff  f rom breaking into health care records is a lot less than for 
banks", said Clayton at Columbia. 
Combined sentence: 
"The economic payoff rom breaking into health care records is a lot less than for banks", said Paul 
Clayton, Chairman of the Department dealing with computerized medical information at Columbia. 
Professional: (the same) 
Example 2: extract  common sub jects  
Original document sentences: 
The new measure  is an echo of  the original bad idea, blurred just enough to cloud prospects 
both for enforcement and for court review. 
Unlike the 1996 act, this one appl ies only  to commerc ia l  Web sites - thus sidestepping 
1996 objections to the burden such regulations would pose for museums, libraries and freewheeling 
conversation deemed "indecent" by somebody somewhere. 
The new vers ion also replaces the vague " indecency"  standard,  to which the court objected, 
with the better-defined one of material ruled "harmful to minors." 
Combined sentences: 
The new measure is an echo of the original bad idea. 
The new version applies only to commercial web sites and replaces the vague "indecency" standard 
with the better-defined one of material ruled "harmful to minors." 
Professional: 
While the new law replaces the "indecency" standard with "harmful to minors" and now only 
applies to commercial Web sites, the "new measure is an echo of the original bad idea." 
Figure 5: Sample output of the sentence combination program 
modules. To evaluate just the combination compo- 
nent, we assume that the system makes the same 
reduction decision as humans and the co-reference 
system has a perfect performance. This involves 
manual tagging of some examples to prepare for the 
evaluation; this preparation is in progress. The eval- 
uation of sentence combination will focus on the ac- 
cessment of combination rules. 
The overM1 system evMuation includes both in- 
trinsic and extrinsic evaluation. In the intrinsic evM- 
uation, we asked human subjects to compare the 
quality of extraction-based summaries and their re- 
vised versions produced by our sentence reduction 
and combination modules. We selected 20 docu- 
ments; three different automatic summarizers were 
used to generate a summary for each document, pro- 
ducing 60 summaries in total. These summaries 
are all extraction-based. We then ran our sentence 
reduction and sentence combination system to re- 
vise the summaries, producing a revised version for 
each summary. We presented human subjects with 
the full documents, the extraction-based summaries, 
and their revised versions, and asked them to com- 
pare the extraction-based summaries and their re- 
vised versions. The human subjects were asked to 
score the conciseness of the summaries (extraction- 
based or revised) based on a scale from 0 to 10 - 
the higher the score, the more concise a summary is. 
They were also asked to score the coherence of the 
summaries based on a scale from 0 to 10. On aver- 
age, the extraction-based summaries have a score of 
4.2 for conciseness, while the revised summaries have 
a score of 7.9 (an improvement of88%). The average 
improvement for the three systems are 78%, 105%, 
and 88% respectively. The revised summaries are 
on average 41% shorter than the original extraction- 
based summaries. For summary coherence, the aver- 
age score for the extraction-based summaries i 3.9, 
while the average score for the revised summaries i
6.1 (an improvement of 56%). The average improve- 
ment for the three systems are 69%, 57%, and 53% 
respectively. 
We are preparing a task-based evaluation, in 
which we will use the data from the Summariza- 
tion EvMuation Conference (Mani et al, 1998) and 
compare how our revised summaries can influence 
humans' performance in tasks like text categoriza- 
tion and ad-hoc retrieval. 
6 Re la ted  work  
(Mani et al, 1999) addressed the problem of revising 
summaries to improve their quality. They suggested 
three types of operations: elimination, aggregation, 
and smoothing. The goal of the elimination opera- 
tion is similar to that of the sentence reduction op- 
184 
eration in our system. The difference is that while 
elimination always removes parentheticals, sentence- 
initial PPs and certain adverbial phrases for every 
extracted sentence, our sentence reduction module 
aims to make reduction decisions according to each 
case and removes a sentence component only if it 
considers it appropriate to do so. The goal of the 
aggregation operation and the smoothing operation 
is similar to that of the sentence combination op- 
eration in our system. However, the combination 
operations and combination rules that we derived 
from corpus analysis are significantly different from 
those used in the above system, which mostly came 
from operations in traditional natural language gen- 
eration. 
7 Conclusions and future work  
This paper presents a novel architecture for text 
summarization using cut and paste techniques ob- 
served in human-written abstracts. In order to auto- 
matically analyze a large quantity of human-written 
abstracts, we developed a decomposition program. 
The automatic decomposition allows us to build 
large corpora for studying sentence reduction and 
sentence combination, which are two effective op- 
erations in cut and paste. We developed a sentence 
reduction module that makes reduction decisions us- 
ing multiple sources of knowledge. We also investi- 
gated possible sentence combination operations and 
implemented the combination module. A sentence 
extraction module was developed and used as the 
front end of the summarization system. 
We are preparing the task-based evaluation of the 
overall system. We also plan to evaluate the porta- 
bility of the system by testing it on another corpus. 
We will also extend the system to query-based sum- 
marization and investigate whether the system can 
be modified for multiple document summarization. 
Acknowledgment 
We thank IBM for licensing us the ESG parser 
and the MITRE corporation for licensing us the co- 
reference resolution system. This material is based 
upon work supported by the National Science Foun- 
dation under Grant No. IRI 96-19124 and IRI 
96-18797. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the authors and do not necessarily reflect 
the views of the National Science Foundation. 
References 
ANSI. 1997. Guidelines for abstracts. Technical Re- 
port Z39.14-1997, NISO Press, Bethesda, Mary- 
land. 
L. Baum. 1972. An inequality and associated max- 
imization technique in statistical estimation of 
probabilistic functions of a markov process. In- 
equalities, (3):1-8. 
Edward T. Cremmins. 1982. The Art of Abstracting. 
ISI Press, Philadelphia. 
Brigitte Endres-Niggemeyer, Kai Haseloh, Jens 
Mfiller, Simone Peist, Irene Santini de Sigel, 
Alexander Sigel, Elisabeth Wansorra, Jan 
Wheeler, and Brfinja Wollny. 1998. Summarizing 
Information. Springer, Berlin. 
Raya Fidel. 1986. Writing abstracts for free-text 
searching. Journal of Documentation, 42(1):11- 
21, March. 
Hongyan Jing and Kathleen R. McKeown. 1998. 
Combining multiple, large-scale resources in a 
reusable lexicon for natural anguage generation. 
In Proceedings of the 36th Annual Meeting of the 
Association for Computational Linguistics and the 
17th International Conference on Computational 
Linguistics, volume 1, pages 607-613, Universit6 
de Montreal, Quebec, Canada, August. 
Hongyan Jing and Kathleen R. McKeown. 1999. 
The decomposition of human-written summary 
sentences. In Proceedings of the P2nd In- 
ternational ACM SIGIR Conference on Re- 
search and Development in Information Re- 
trieval(SIGIR'99), pages 129-136, University of 
Berkeley, CA, August. 
Hongyan Jing. 2000. Sentence reduction for au- 
tomatic text summarization. In Proceedings of 
ANLP 2000. 
Aravind.K. Joshi. 1987. Introduction to tree- 
adjoining rammars. In A. Manaster-Ramis, ed- 
itor, Mathematics of Language. John Benjamins, 
Amsterdam. 
Inderjeet Mani, David House, Gary Klein, Lynette 
Hirschman, Leo Obrst, Therese Firmin, Michael 
Chrzanowski, and Beth Sundheim. 1998. The 
TIPSTER SUMMAC text summarization eval- 
uation final report. Technical Report MTR 
98W0000138, The MITRE Corporation. 
Inderjeet Mani, Barbara Gates, and Erie Bloedorn. 
1999. Improving summaries by revising them. In 
Proceedings of the 37th Annual Meeting of the As- 
sociation for Computational Linguistics(A CL '99), 
pages 558-565, University of Maryland, Mary- 
land, June. 
Michael MeCord, 1990. English Slot Grammar. 
IBM. 
Samuel Thurber, editor. 1924. Prgcis Writing for 
American Schools. The Atlantic Monthly Press, 
INC., Boston. 
A.J. Viterbi. 1967. Error bounds for convolution 
codes and an asymptotically optimal decoding al- 
gorithm. IEEE Transactions on Information The- 
ory, 13:260-269. 
185 
Exper iments  in Automated  Lex icon  Bu i ld ing  for Text  Search ing  
Barry Schiffman and Kathleen R. McKeown 
Department of Computer Science 
Columbia University 
New York, NY 10027, USA 
{ bschiff,kathy} @)cs.columbia.edu 
Abstract 
This paper describes experiment's in the automat'ic 
construction of lexicons that would be useflfl in 
searching large document collect'ions tot text frag~ 
ments tinct address a specific inibrmation eed, such 
as an answer to a quest'ion. 
1 Introduct ion 
In develot)ing a syst'em to find answers in text to 
user questions, we mmovered a major obstacle: Doe- 
mnent sentences t'hat contained answers dkl not of_ 
ten use the same expressions as the question. While 
an:;wers in documents and questiolts llse terms that' 
are relat'e(l to each other, a system that sear(:hes for 
answers based on the quesl:ion wording will often 
fail. 3.b address t'his probleln, we develol)ed tech- 
niques to al,tomatically build a lexicon of associated 
terms t'hat can be used to hell) lind al)lIrol/riate bext' 
seglllent,s. 
The mismatch })et'ween (tuestion an(l doctlttlent 
wording was I)rought home to us in an analysis of a 
testbed of question/answer l/airs. \~Ze had a collec- 
tion of newswire articles about the Clinton impeach- 
ment t'() use as a small-scale corl)uS fin' development 
of ;_t system. V~Ze asked several )eol)le to 1)ose ques- 
tions about this well-known t'opic, but we (lid not 
make the corpus availal)le to our cont'ril)utors. \~Ze 
wanted to avoid quest'ions that tracked t'he terminol- 
ogy in t'he corlms too (:losely to s innl late quest'ions 
t'o a real-world syst'em. The result was a set of ques- 
tions that  used language that' rarely nmtched t'he 
phrasing in the. corl)us. \,Ve had expected t'hat' we 
would be able to make most of these lexical connec- 
tions with the hel l) of V~rordnet (Miller, 1990). 
For example, consider a simple quest'ion al)out tes- 
timony: "Did Secret Service agents give testimony 
about' Bill Clinton?" There is no reason t'o expect 
that' the answer would appear 1)aldly st'ated as "Se- 
cret Service. agents dkl testi(y ..." What  we need 
to know is what' testimony is about', where it: occurs, 
who gives it. The answer would lie likely to be found 
in a passage ment'ioning juries, or 1)roseeut'ors, like 
these tbund in our Clinton corl)uS: 
Starr immediately brought Secret Service 
employees before tim grand jury for ques- 
tioning. 
Prosecutors repeat'edly asked Secret Ser- 
viee 1)ersonnel to rel)eat' gossil) they may 
have heard. 
Yet, tile V~ordnet synsets fbr "testinlony" offer: 
"evidence, assertion, averment alia asseveration," 
not a very hell)tiff selection here. -Wordnet hyper- 
nyms become general quickly: "declarat'ion," indi- 
cat'ion" and " infor lnat ion" are only one st, eli u 1) in 
t'lle hierarehy. Following these does not lead us into 
a courtroom. 
We asked our cont'ril)ut'ors for a second round of 
questions, but this time made the corpus available 
to them, exl)laining t'hat we wanted to be sure the 
answers were contained in t'he collection of articles. 
'J'he result was a set of questions that' mueh more 
closely matched t'he wording in the corpus. This was~ 
in t'aet, what' the 1999 DARPA question-answering 
(:oml)et'ition did in order t'o ensure that their ques- 
tions couhl be answered (Singhal, 1!199). The sec- 
trod quest ion-answer ing  conference adopted a new 
approach to gathering questions and verifying sepa- 
rately that' they a.re answerable. 
Our intuition is t'hat if we can lind the tyl)ical 
lexical neighborhoods of concept's, we can efficiently 
locate a concept described in a query or a question 
without needing to know the precise way the answer 
is phrased and without relying on a cost'ly, hand- 
built concept' hierarchy. 
The example above illustrat'es the 1)oint. Tes- 
t imony is given 1) 3, wit'nesses, defendant's, eyewit- 
nesses. It is solicited by 1)rosecutors, counsels, 
lawyers. It is heard by judges, juries at trials, hear- 
ings, and recorded in depositions and transcripts. 
What' we wanted was a complete description of t'he 
world of testimony - the who, what, when and 
where of the word. Or, in other words, the "meta- 
aboutness" of terms. 
To this end, we exl)erimented /tSitlg shallow lin- 
guist.k: techniques t'o gat'her and analyze word co- 
occurrence data in various configurat'ions. Unlike 
previous collocation research, we were int'erested 
in an expansive set' of relationships between words 
719 
rather than a specific relationship. More important, 
we felt that the information we needed could be de- 
rived from an analysis that crossed clause and sen- 
tence boundaries. We hyl)othesized that news ar- 
ticles would be coherent so that the sequences of 
sentences and clauses would be linked conceptually. 
We exanfined the nouns in a number of configura- 
tions - paragraphs, entences, clauses and sequences 
of clauses - and obtained tile strongest results from 
configurations that count co-occurrences across the 
surface subjects of sequences of two to six clauses. 
Exl)eriments with multi-clause configurations were 
generally more accurate in a variety of experiments. 
In the next section, we briefly review related re- 
search. In section 3 we describe our experiments. 
In section 4, we discuss the problem of evaluation, 
and look ahead to future directions in the concluding 
sections. 
2 Re la ted  Work 
There has been a large body of work ill the collec- 
tion of co-occurrence data from a broad spectrum of 
perspectives, fi'om information retrieval to the devel- 
opnlent of statistical methods for investigating word 
similarity and classification. Our efforts fall some- 
where in tile middle. 
Compared with document retrieval tasks, we are 
more closely focused on the words themselves and 
on specific concepts than on document "aboutness." 
Jing and Croft (1994) exanfined words and phrases 
in paragraph units, and found that the association 
data improves retrieval performance. Callan (1994) 
compared paragraph units and fixed windows of text 
in examining passage-level retrieval. 
In the question-answering context, Morton (1999) 
collected document co-occurrence statistics to un- 
cover 1)art-whole and synonymy relationships to use 
in a question-answering system. The key differ- 
ence here was that co-occurrence was considered on 
a whole-docmnent basis. Harabagiu and Maiorano 
(1999) argued that indexing in question answering 
should be based on 1)aragraphs. 
One recent al)proach to automatic lexicon build- 
ing has used seed words to lmild up larger sets of 
semmltically similar words in one or nlore categories 
(Riloff and Shepherd, 1997). In addition, Strza- 
lkowski and Wang (1996) used a bootstrapping tech- 
nique to identify types of references, and Riloff and 
Jones (1999) adapted bootstrapping techniques to 
lexicon building targeted to information extraction. 
In the same vein, researchers at Brown Univer- 
sity (Caraballo and Charniak, 1999)~ (Berland and 
Charniak, 1999), (Caraballo, 1999) and (Roark and 
Charniak, 1998) focused on target constructions, in
particular complex noun t)hrases, and searched for 
information ot only on identifying classes of nouns, 
lint also hypernyms, noun specificity and meronymy. 
We have a diflbrent perspective than these lines of 
inquiry. They were specifying various semantic rela- 
tionships and seeking ways to collect similar pairs. 
We. have a less restrictive focus and are relying on 
surface syntactic information about clauses. 
For more than a decade, a variety of statistical 
techniques have been developed and refilled. Tile 
focus of much of this work was to develop the 
methods themselves. Church and Hanks (1989) ex- 
plored tile use of mutual information statistics in 
ranking co-occurrences within five-word windows. 
Smadja (1992) gathered co-occurrences within five- 
word windows to find collocations, particularly in 
specific domains. Hindle (1990) classified nouns 
on the basis of co-occurring patterns of subject- 
verb and verb-object pairs. Hatzivassiloglou and 
MeKeown (1993) clustered adjectives into semantic 
classes, and Pereira et al (1993) clustered nouns on 
their appearance ill verb-object pairs. We are try- 
ing to be less restrictive in learning multiple salient 
relationshil)s between words rather than seeldng a 
particular elationship. 
Ill a way, our idea is the mirror image of Barzilay 
and Elhadad (1997), who used Wordnet to identify 
lexical chains that would coincide with cohesive text 
segments. We assunmd that documents are cohesive 
and that co-occurrence l)atterns call uncover word 
relationships. 
3 Experiments 
Tile focus of onr experiment was on units of text in 
which the constituents must fit together in order for 
the discourse to be coherent. We made the assump- 
tion that the documents in our corpus were coherent 
and reasoned that if we had enough text, covering 
a broad range of topics, we could pick out domain- 
independent associations. For example, testimony 
can be about virtually anything, since anything can 
wind up in a court dispute. But over a large enough 
collection of text, the terms that directly relate to 
tile "who," "what" and "where" of testimony per 
se should appear in segments with testimony more 
frequently than chance. 
These associations do not necessarily appear in a 
dictionary or thesaurus. When huntans explain all 
unfamiliar word, they often use scenarios and analo- 
gies. 
We divided the experiments in two groups: one 
group that looks at co-occurrences within a single 
unit, and another that looks at a sequence of units. 
In the first group of experinmnts, we considered 
paragraphs, sentences and clauses, each with and 
without prepositional phrases. 
? Single paragraphs with/without PP 
? Single sentences with/without PP 
? Single clauses with/without PP 
720 
\]in the second group, we considered two clauses 
and sequences of subject 110un phrases from two to 
six chmses. Ill this group, we had: 
,, Two clauses with/without Pl) 
,, A sequence of subject NPs fl'onl 2 clauses 
A sequence of subject NPs Dora 3 clauses 
,, A sequence of subject NPs from 4 clauses 
? A sequence of subject NPs fi'om 5 clauses 
,, A sequence of subject NPs from 6 clauses 
The intuition for the second groul) is that a topic 
flows from one granmm.tical unit to another so that 
the salient nouns, l)articularly the surface subjects, 
in successive clauses should reveal the associations 
we are seeldng. 
'\[lo illustrate the method, consider the three-clause 
configuration: Say that ~vordi apl)ears in clausc,~. 
We maintain a table of all word pairs and increment 
the entries for O,,o,'(h , ',,,o,'d~ ), where , ,0 ,% is a sub- 
ject noun in cla'usc,~, clauscn+~, or ell'use,+2. No 
effort was made to resolve pronomial references, and 
these were skipped. 
We used nollnS Olfly' because l)reliminary tests 
showed that pairings between ouns seemed to stand 
out. V~Te included tokens that were tagged as 1)roper 
nall leS when they also have have con ln lon  n lean ings .  
For example, consider the Linguistic Data Consor- 
l;ium at the University of Pennsylvania. Data, Con- 
sortium and University wouM be on tile list used to 
build the table of nmtchul)s with other nouns, \])lit 
l)emlsylvania would not. V~To also collected noun 
modifiers as well as head nouns as they can carry 
more information than the surface heads, such as 
"business group", '".science class" or "crinm scene." 
The corpus consisted of all tile general-interest ar- 
ticles from the New York Tinms newswire in 1996 
in the North American News Corlms , and (lid not 
include either st)orts or l)usiness news. We tirst re- 
moved dul)licate articles. The data fl'om 1996 was 
too slmrse for the sequence-of-subjects ontigura- 
lions. '\]'o l)alance the expcrinmnts better, we added 
another year's worth of newswire articles, from 1995, 
tbr the sequence-of subject configurations sothat we 
had more than one million matchups for each con- 
figuration (Table 1). 
The I)roeess is flflly automatic, requiring no su- 
1)ervision or training examples. The corpus was 
tagged with a decision-tree tagger (Schmid, 1994) 
and parsed with a finite-state parser (Abney, 1996) 
using a specially written context-fi'ee-grannnar that 
focused on locating clause boundaries. The gram- 
mar also identified extended noun l)hrases in tile sub- 
ject position, verb l)hrases and other noun l)hrases 
and prepositional 1)hrases. The nouns in the tagged, 
parsed corl)uS were reduced to their syntactic roots 
(removing l)lurals from nouns) with a lookup table 
created t'rom Wordnet (Miller, 1990) and CELEX 
(1995). We. performed this last step mainly to ad- 
dress the sparse data problem. There were a sub- 
stantial nunfl)er of paMngs that occurred only once. 
We elinfinated from considerat;ion all such single- 
tons, although it did not al)peal to have much etfect 
on the overall outcome. 
Confi.q Matchups 
Para +pp 6.5 million 
Sent 1.7 million 
Sent +pp 4 million 
1 Clause 1.1 million 
1 Clause +pp 2.8 million 
2 Clause 1.9 million 
2 Clause +I)P 5 nfillion 
Subj 2 Clause 1.1 million* 
Subj 3 Clause 1.6 million* 
Subj 4 Clause 2.1 million* 
Subj 5 Clause 2 .6m~ 
Subj 6 Clause 3.1 million* 
'lhble 1: Nmnl)er of matchut)s ibund; tile "*" de- 
notes the inclusion of 1995 data 
There were about 1.2 million paragraphs, 2.2 mil- 
lion sentences and 3.4 million clauses in the selected 
portions of the 1996 COl'pus. The total number of 
words was 57 million. Table 2 shows the nmnl)er of 
distinct nouns. 
I I All Extracted 
No l)ps 74,500 
W/pps 91,700 
Subjs 51,000 
Counts > 1. 
44,400 
53,900 
30,800 
Td)le 2: Distinct Nouns, 1996 Data 
To score the nmtchups in our initial exlmriments , 
we used the Dice Coeliicient, which l)roduces values 
i'ronl 0 to 1, to measure the association between pairs 
of words and then produced an ordered association 
list fl'om the co-occurrence table, ranked according 
to the scores of the entries. 
2 ? f, '~q(wo,.,h n ,oo ,%)  
score,, = frcq(wordi) + frcq(wordj) 
One 1)roblem was immediately al)parent: The 
quality of tile association lists wxried greatly. Tile 
scoring was doing an acceptable job in ranking the 
words within each list, but tile scores varied greatly 
from one list to another. Our initial strategy was 
to choose a cutoff, which we set at 21 tbr each list, 
and we tried several alternatives to weed out weak 
associations. 
721 
In one method, we filtered the association lists 
by cross-referencing, removing from the association 
list for wordi any wordj that failed to reciprocate 
and to give a high rank to wordi on its association 
list. Another similar approach was to try to con> 
bine evidence fl'om different experiments by taking 
the results fl'om two configurations into considera- 
tion. A third strategy was to calculate the mutual 
information between the target word and the other 
words on its association list. 
scorc,,i = p(xy) * log \p(z)p(y) ( (xy) ) 
Using the mutual information computation pro- 
vided an way of using a single measure that was able 
to compare matchups across lists. We set a threshold 
of lxl.0 -6 for all matchups. Thus these association 
lists vary in length, depending on the distributions 
for the words, allowing them to grow up to 40, while 
some ended up with only one or two words. 
4 Evaluation 
The evaluation of a system like ours is problematic. 
The judgments we made to determine correctness 
were not only highly subjective but time-consunfing. 
We had 12 large lexicons fl'om the different config- 
urations. We had chosen a random sample of 10 
percent of the 2,700 words that occurred at least 
100 times in tile corpus, and manually constructed 
an answer key, which ended up with ahnost 30,000 
entries. 
From the resulting 270 words, we discarded 15 of 
those that coincided with common names of peo- 
ple, such as "carter," which could refer to the for- 
mer American president, Chris Carter (creator of 
tile television show "X-Files"), among others. We 
thought it better to delay making decisions on how 
to handle such cases, especially since it would require 
distinguishing one Carter fl'om another. Such words 
presented several difficulties. Unless the individuals 
involved were well-known, it was often impossible to 
distinguish whether the system was making errors 
or whether the resulting descriptive terms were in- 
tbrmative. 
Tables 3 and 4 show an example from the answer 
key tbr the word "faculty." 
The overall results from the first stage of the pro- 
cess, before the cross-referencing filter are shown in 
Table 5, ranging from 73% to 80% correct. The con- 
figurations that included prepositional phrases and 
those that used sequences of subject noun phrases 
outperformed the configurations that relied on suh- 
jects and objects in a single grammatical unit. These 
differences were statistically significant, with p < 
0.01 in all eases. 
The overall results after cross-referencing, in Ta- 
ble 6, showed improvements of 5 to 10 percentage 
enrollment hiring adnfinistrator 
journalism alumnus student 
school union math 
engineering curriculum trustee 
group seminar thesis 
tenure stair department 
mathematician educator member 
ivy arts college 
chancellor report senate 
activism university el,airman 
professor teaching law 
regent doctorate mtministration 
academic committee semester 
board camI)us undergraduate 
salary council research 
president adviser mathematics 
course advisor sociology 
dean study science 
teacher cannon provost 
vote 
Table 3: Answer Key for Faculty: OK 
load tratllcway unrest 
architecture diversity hurdle 
shield minority revision 
disburse percent woman 
clement 
Table 4: Answer Key ff)r Faculty: Wrong 
points, while the effect of the number of matchups 
was diminished. Here, the subject-sequence onfig- 
urations showed a distinct advantage. While more 
noise might be expected when a large segment of text; 
is considered, these results support the notion that 
the nnderlying coherence of a discourse can be recov- 
ered with the prol)er selection of linguistic features. 
The improvements in each configuration over the 
corresponding configuration in the first stage were 
all statistically significant, with p < 0.01. Likewise, 
the edge the sequence-of subjects configurations had 
over tile other configurations, was also statistically 
significant. 
The results fl'om combining the evidence from dif- 
ferent configurations, in Table 7, showed a much 
higher accnrae> but a sharp drop in the total nnm- 
ber of associated words found. The most fl'uitful 
pairs of experiments were those that combined dis- 
tinct approaches, for example, tile five-subject con- 
figuration with either fifll paragraphs or with sen- 
tences with prepositional phrases. It will remain 
unclear until we conduct a task-based evaluation 
whether the smaller number of associations will be 
harnfful. 
The final experiment, computing the mutual in- 
formation statistic tbr the matchul)s of a key word 
with co-occurring words was perhaps the most ill- 
teresting because it gave us the ability to apply a 
722 
(Jontig OK Wrong l)ct OK 
Para +l/ l)  3832 1054 78 
,qent 3773 1270 75 
Sent +Pl) 3973 1070 79 
\] Clause 3652 1371 73 
\] Clauses q-l)l) 3935 1108 78 
"! Clauses 3695 1328 74 
"! Clauses -t-l)l) 3983 1018 80 
Subj 2 CI 3877 1139 77 
Sul)j 3 CI 3899 1117 78 
Subj 4 CI 3!)(/5 :1082 78 
Sul)j 5 C1 390d 1076 78 
Sul)j 6 CI 3909 1066 7!) 
Table 5: Results 13efore Cross I loferencing 
Contig ()K Wrong Pet ()K 
Para q-Pl) 3651} 73/1 83 
Sent 3328 742 82 
Sent -bpp 3751 8:18 82 
:1 Clause 3067 748 80 
1 Clauses +1)I / 3659 826 82 
2 Cbmses 3048 55d 85 
2 Clauses +pp 3232 60d 8d 
Subj 2 CI 2910 450 87 
t-;ul~j 3 CI 3020 4d() 87 
Subj 4 CI 3050 d28 88 
l~tll).j 5 (J\] ;1:12t3 dd2 88 
Subj 6 C1 3237 dd9 88 
' lhble 6: l{esults After Cross Referencing 
single threshold across different key words, saving 
the effort of performing the cross-retbrencing calm> 
lations and providing a deeper assorl:ment in SOllle 
C~lSeS. lilt lnost of the configurations, lltlltllPl illfOr- 
mat.ion gave 118 lllore \Vol'ds, and greater ln'ecision 
at; the sanle time, but nlost of all, gave us a reason- 
able threshold to apply throughout  he exlicrinlent. 
Whi le the accuracies in most of the configurations 
were close to one another,  those that  used only sin- 
g\]e units tended to be weaker than the mult i -c lause 
units. Note that  the paragraI)h contiguration was 
tested with far more data  than any of the others. 
Our system maD~s no eth)rt to aeCOllnt for lexi- 
cal aml)iguil;y. The uses we intend for our lexicon 
should provide some insulat ion from the ett'ects of 
polysemy, since searches will be conducted on a nun> 
l)er of terms, which should converge to one meaning. 
It is clear that  in lists for key words with mult i -  
ple senses, the donfinant sense where there is one, 
al)pears much lnore frequently, such as "faculty ," 
where the meaning of "teacher" is more t:'re(tuent 
than the meaning of "al)ility." F igure \] shows the 
top 21 words in the sequence-otCsix subjects,  beibre 
the cross-referencing ii lter was applied. Twenty of 
the 21. entries were scored aeceptal)le. 
After the cross-referencing is applied, doctorate,, 
education and revision were elinfinated. 
Contig OK \?rong Pcl; OK 
l~ara 2003 183 92 
Sent 1962 222 90 
Sent-t- 2033 213 91 
1 Clause 1791 218 89 
1 C lause+ 2004 198 91 
2 Clause 2028 277 88 
2 C lause+ 2:129 24,1 90 
Tal)le 7: Results of coml)ining evidence; all configu- 
rat ions were combined with the sequence of six sub- 
jects 
Conlig OK "Wrong Pet OK 
Para +pp 4923 807 86 
Sent 5193 990 84 
Sent +Pl) 4876 775 86 
1 Claus(; 52!)!/ 1233 81 
1 Clauses-t-l)l) 5047 878 85 
2 Clauses 5025 928 84 
2 Clauses -I-Pl) d668 728 87 
Subj 2 C1 5229 939 85 
Subj 3 C1 5187 860 85 
Subj ~1 C1 5119 808 86 
Subj 5 C1 500"{ 76d 87 
Subj 6 CI 4!)80 736 87 
Table 8: l lesults with mutual  information 
The results from the single clause configuration 
(Figure 2) were almost as strong, with three erroFs, 
and a fair amount of overlap between the two. 
The word "admiral" was more difficult %r the ex- 
\])erilllellt ilSilig the l)ice coefficient. The. list shows 
some of l.he confusion arising from our strate.~y Oll 
prot)er nouns. Admiral  would be expected to oc- 
cur with many proper ll~tnles, i l lcluding some that  
axe st)elled liD; common 11o1111.q, bi l l  the list h)r the 
single clause q pp conf iguratkm presented a lmzzling 
list (F igure 3). 
The sparseness of the data  is also al)lmrent, but it 
was the dog reDxenees that  al)peared quite strange 
at a ghulce: Inspection of the. articles showed that  
they callle froln all a.rticle on the pets of famous 
people. Note that the dogs did not al)l)ear in top 
ranks of the sequence of subjects  configuration in 
the Dice exper iment (Figure 4), nor were they in the 
results t'rom the experiments with cross-referencing, 
combining evidence and mutua l  information. 
After cross-reR;rencing, the much-shorter list for 
the Sub j-6 configuration had "aviator",  "break-up",  
' ;commander",  "decoration",  "equal-ot)portunity",  
"tleet", "merino", "navf ' ,  "pearl",  "promotion", 
"rear" ~ alia "short". 
' l 'he combined-evidence list contained only eight 
words: "navy", "short", "aviator",  "merino", "dis- 
honor",  "decoration",  "sul)" and "break-ul)". 
Using the mutual  intbr lnat ion scoring, the list 
in the Subj-6 configuration tbr admiral  had only 
723 
faculty trustee(51) 0.053; carat)us(d1) (/.045; 
college(ll3) 0.034; member(369) 0.028; profes- 
sor(102) 0.028; university(203) 0.027; student(206) 
0.025; regent(19) 0.025; tenure(15) 0.025; ctmncel- 
lor(28) 0.023; administrator(34) 0.023; provost(12) 
0.023; dean(27) 0.021; ahmmus(13) 0.021; math(12) 
0.017; revision(8) 0.013; salary(13) 0.013; so- 
ciology(7) 0.013; educator(l l) 0.012; doctorate(6) 
0.011.; teaching(9) 0.011; 
Figure 1: Tile top-ranked matchups for "fac- 
ulty" from the Subj-6-Clause configuration be- 
fore cross-referencing. The nmnbers in paren- 
theses are the number of matchups and the real 
umnbers following are the scores. Errors are in 
bold 
faculty trustee(31) 0.033; meml)er(266) 0.025; ad- 
nfinistrator(31) 0.023; college(42) 0.012; dean(15) 
0.012; tenure(8) 0.011; ivy(6) 0.011; staff(a3) 0.01; 
semester(6) 0.01; regent(7) 0.01; salary(12) 0.01; 
math(7) 0.008; professor(a1) 0.008; load(6) 0.007; 
curricuhun(5) 0.006; revision(4) 0.006; minor- 
ity( l l )  0.006; 
Figure 2: The top-ranked matchups for "fac- 
ulty" under the single clause confignration. Er- 
rors are in bold. 
nine words: "navy", "general", "commander", 
"vice", "promotion", "officer", "fleet", "military" 
and "smith." 
Finally, the even-sparser mutual information list 
for the paragraph configuration lists only "navy" 
and "suicide." 
5 Conc lus ion  
Our results are encouraging. We were able to deci- 
pher a broad type of word association, and showed 
that our method of searching sequences of subjects 
outperformed the snore traditional approaches in 
finding collocations. We believe we can use tiffs tech- 
nique to build a large-scale l xicon to help in diffi- 
cult information retrieval and information extraction 
tasks like question answering. 
The most interesting aspect of" this work lies in 
the system's ability to look across several clauses 
and strengthen tile connections between associated 
words. We are able to deal with input that con- 
tains numerous errors from the tagging and shallow 
parsing processes. Local context has been studied 
extensively in recent years with sophisticated statis- 
tical tools and the availability of enormous amounts 
of text in digital form. Perhaps we can expand this 
perspective to look at a window of perhaps everal 
sentences by extracting the correct linguistic units in 
order to explore a large range of language processing 
problems. 
admiral- navy(all) 0.027; ayMon(d) 0.024; cheat- 
ing(5) 0.02; gallantry(3) 0.016; chow(4) 0.015; ser- 
vice,nan(d) 0.013; short(3) 0.013; wardroom(2) 
0.012; american(2) 0.012; enos(2) 0.012; self- 
assessment(2) 0.(/11; merino(2) 0.011; ocelot(2) 
0.011; wolfhound(2)0.011; igloo(2)0.011; pa- 
prika(2) 0.011; spaniel(2) 0.01; medal(8) 0.01; 
awe(a) 0.01; pedigree(2) 0.009; te,'rier(2) 0.009; 
Figure 3: Top-ranked matchups for "adnfiral" 
under the clause +pp configuration. 
admiral - navy(88) 0.071; short(7) 0.03; promo- 
tion(ll) 0.027; hal)l)iness(8) 0.026; fleet(ll) 0.024; 
aviator(5) 0.022; mnbition(8) 0.019; merino(3) 
0.019; dishonor(3)0.018; rear(4)0.018; deco- 
ration(4) 0.015; sub(a) 0.013; airman(3) 0.013; 
graveses(2) 0.012; submariner(2) 0.012; equal- 
opportunity(2) 0.012; break-up(2) 0.012; comman- 
der(18) 0.012; pearl(7) 0.012; l)rophccy(d) 0.01.2; 
torturer(2) 0.012; 
Figure 4: The list for admiral fi'om the Sub j-6 
contiguration. 
6 Future  Work  
? We will have the scoring key itself evaluated by 
people who are not involved in tile research. 
? ~re are planning to conduct ask-based evalua- 
tion in question answering. 
? We are considering deploying a named entity 
module to provide sonic classification of which 
proper nouns should be counted and which 
should not. 
? We 1)lan to experiment with ways to incorpo- 
rate using examining verbs and making use of 
surface objects in the configurations with se- 
quences of clauses, as well as strengthen the fi- 
nite state grammar. 
? We will explore using tile system to extract bi- 
ographic information. 
Acknowledgments  
This material is based upon work supported by tile 
National Science Foundation under grants Nos. IIS- 
96-19124 and IRI-96-18797, and work jointly sup- 
ported by the National Science Foundation and the 
National Library of Medicine under grant No. IIS- 
98-17434. Any opinions, findings, and conclusions 
or recmmnendations expressed in this material are 
those of tile authors and do not necessarily reflect 
the views of the National Science Foundation. 
724 
References  
Steven Abney. 1996. Partial parsing via finite-state 
cascades. In Proceedin9s of th, e ESSLLI '95 Robust 
Parsin9 Workshop. 
Regina Barzilay and Michael Elhadad. 1997. Using 
lexical chains tbr text smmnarization. In Pwcced- 
ings of the Ntelligent Scalable Text b'ummariza- 
tion Workshop. ACL. 
Matthew Berland and Eugene Charniak. 1999. 
Finding parts in very large corpora. 'l.bchnical Re- 
port TR CS99-02, Brown University. 
James P. Callan. 1994. Passage-level vidence in 
document retrieval. In Proceedin9s of the Seven- 
teenth Annual Intcunational A CM SIGIR Confer- 
ence, Dublin, Ireland. ACM. 
Sharon Caraballo and Eugene Charniak. 1999. De- 
termining the speciticity of nouns from text. In 
P~vceedinfls of Co~@rcnce on E,mpi~eal Methods 
in Nat'u'ral Langua9e Processing. 
Sharon Caraballo. 1999. Automatic acquisition of 
a hylmrnym-labeled noun hierarchy from text. In 
Pwceedings of th, e 37th Annual Meeting of the As- 
sociation for Comp'utational Linguistics, June. 
CELEX, 19!)5. Tit(; CELEX lezical database 
Dutch,, English, Ge.rntan. Centr for Lexical hffor- 
mation, Max Planck Institute for Psycholinguis- 
ties, Nijmegen. 
Kenneth W. Church and Patrk:k Itanks. 1989. Word 
association orms, mutual infornmtion and lexi- 
cography. In Proceedings of th.e 27th. nteetin9 of 
the ACL. 
S&nda ~/\[. Ilara,bagiu anti S|;even J. Maiorano. 1999. 
Finding answers in large collectkms of texts: Para- 
graph indexiltg -t- adductive inference. In Q'aes- 
tion Answering Systema'. AAAI, November. 
Vasileios Hatziw~ssiloglou and Kathleen R. McKe- 
own. 1993. 'lbwards the automatic identification 
of adjectival scales: Clustering adjectives accord- 
ing to meaning. In P~vceedin9 s of th, c 31st Annual 
Meeting of th, e A CL. 
Donald Hindle. 1990. Noun classitication ti'om 
predicate-argument structures. In PTvceedin9s of 
the 28th Annual Meeting of the A CL. 
Yufcng Jing and W. Bruce Croft. 1994. An associa- 
tion thesaurus for information retrieval, tech. rep. 
no 94-17. 2bchnical report, Amherst: University 
of Massachusetts, Center for Intelligent hfforma- 
tion Retrieval. 
G. Millet'. 1990. Wordnet: An on-line lexical 
database. International . ournal of Lezicoqraphy. 
Thomas S. Morton. 1999. Using coreibrence tor 
question answering. In P~vccedings of thc Work- 
shop on Coreference and Its Applications, l)ages 
85-89, College Park, Maryland, June. Associa- 
tion for Computational Linguisties, Association 
for Computation Linguistics. 
Fernando Pereira, Naffali Tishby, and Lillian Lee. 
1993. Distributional clustering of english words. 
In Pwcecdings of the 31st Annual Meeting of the 
ACL. 
Ellen Rilotf and Pmsie Jones. 1999. Learning die- 
tionarics for intbrmation extraction by multi- 
level bootstral)ping. In Proceedings of the Six- 
teenth Na, tional Co~@rencc on Artificial Intelli- 
gence. AAAI. 
Ellen I{ilotf and Jessica Shepherd. 1997. A corpus- 
based approach for building semantic lexicons. In 
Proceedings of the Second Conference on Empir~i - 
cal Meth, ods in Natural Langua9 c Processing. 
Brian lloark and Eugene Charniak. 1998. Noun- 
phrasae co-occurrence statistics for semi- 
automatic semantk: lexicon construction. In 
P~vcccdings of thc 36th Annual Meetin9 of the 
Association for Computational Linguistics and 
the 17th htternational Conference on Computa- 
tion Linguistics. 
Hehnut Schmid. 1994. Probabilistic part-of speech 
tagging using decision trees. In Proceedings of the 
International Cor@rence on New Methods in Lan- 
9ua.qe Proecssin9. 
Amit Singhal. 1999. Question and answer track 
home page. WWW. 
Frank Smadja. 1992. Retrieving collocations fi'om 
text: Xtract. Comp'll, tational Linguistics, Special 
Issue. 
Tomek Strzalkowski and Jin V~rang. 1996. A self- 
learning universal concept spotter. In lhvceedinfls 
of th, e International 6'm@renee on Computational 
Linfluisties (Colin 9 199@. 
725 
Generating Overview Summaries of Ongoing Email Thread Discussions 
Stephen Wan 
Department of Computing 
Macquarie University 
Sydney NSW 2109 
 swan@ics.mq.edu.au 
Kathy McKeown 
Columbia University 
Department of Computer Science 
1214 Amsterdam Avenue 
NY - 10027-7003, USA 
kathy@cs.columbia.edu 
 
Abstract 
The tedious task of responding to a backlog of 
email is one which is familiar to many researchers.  
As a subset of email management, we address the 
problem of constructing a summary of email 
discussions.  Specifically, we examine ongoing 
discussions which will ultimately culminate in a 
consensus in a decision-making process.  Our 
summary provides a snapshot of the current state-
of-affairs of the discussion and facilitates a speedy 
response from the user, who might be the 
bottleneck in some matter being resolved.  We 
present a method which uses the structure of the 
thread dialogue and word vector techniques to 
determine which sentence in the thread should be 
extracted as the main issue.   Our solution 
successfully identifies the sentence containing the 
issue of the thread being discussed, potentially 
more informative than subject line.   
1 Introduction 
Imagine the chore of sifting through your 
overflowing email inbox after an extended period 
away from the office.  The discovery that some of 
these emails form part of a larger decision-making 
discussion only heightens the sense of urgency and 
stress.  Such a discussion may require an urgent 
response and a user?s lack of contribution may be a 
bottleneck in some matter being resolved.  Such a 
scenario is seems quite familiar and intuitively, one 
would expect that better solutions to presenting the 
contents of the email inbox might be useful in 
facilitating a timely reply to a missed email 
discussion. 
One such solution might be a summary of that 
very email discussion.  However, it would be much 
more useful if the summary did not just tell the 
user what the thread is about.  Such information 
might be easily obtained from the subject line, or if 
not, a conventional off-the-shelf summarizer might 
provide the gist of the thread quite easily.     
However, in contrast to a conventional sentence 
extraction summary in Figure 1, the ideal summary 
ought to provide sufficient information about the 
current state-of-affairs of the discussion, in order to 
minimize any further delay in the matter being 
resolved.    Specifically, this might include a 
description of the matter being discussed and the 
responses received so far.  An example of such a 
summary is presented in Figure 2.  In this example, 
it is not sufficient to know that a plaque is being 
designed.  Crucially, the wording of the plaque is 
under discussion and requires feedback from the 
thread participants.  It is not difficult to appreciate 
the usefulness of such a summary to avoid writing 
responses to older, and hence irrelevant, emails.  
Accordingly, we envisage that the resulting 
summary to be not just indicative of the thread 
content but informative Borko (1975).   
 
1. Here's the plaque info. 
2. http://www.affordableawards.com/plaques/o
rdecon.htm  
3. I like the plaque, and aside for exchanging 
Dana's name for "Sally Slater" and ACM for 
"Ladies Auxiliary", the wording is nice. 
4. We just need to contact the plaque folks and 
ask what format they need for the logo. 
 
Figure 1. Example summary from a conventional 
sentence extraction summarizer 
Issue: Let me know if you agree or disagree 
w/choice of plaque and (especially) wording. 
 
Response 1: I like the plaque, and aside for 
exchanging Dana's name for "Sally Slater" 
and ACM for "Ladies Auxiliary", the 
wording is nice.  
Response 2: I prefer Christy's wording to the 
plaque original.  
Figure 2. Example summary from our system 
We present a novel approach which identifies 
the main issue within the email and finds the 
responses to that issue within subsequent emails. 
Our approach uses a combination of traditional 
vector space techniques and Singular Value 
Decomposition (SVD).  We rely on the premise 
that the participants of the discussion have 
implicitly determined which sentence from the 
initiating email of the thread is most important and 
that we can see evidence of this inherent in the 
content of their respective replies.   
In the remainder of the paper, we provide 
background on email usage and our observations 
of discussion thread structure in Section 2 to 
support our basic premise.  Section 3 provides a 
description of related work in the area.   To date, 
use of dialogue structure has mostly been limited 
to finding question-answer pairs in order to extract 
the pairing as a whole for the sake of coherence.  
We present a more formal description of the 
problem we are addressing and our algorithms for 
issue in Section 4.  Section 5 outlines our handling 
of response extraction.  In Section 6, we present a 
preliminary evaluation we have conducted along 
with the results.  Finally we end with concluding 
remarks in Section 7.
2 Background: Email Threads 
2.1 Email Discussions supporting a Decision-
Making Process 
The focus of this paper is on email discussions 
supporting a group decision-making process.  In 
contrast to studies on individual email usage (for 
an overview see: Ducheneaut and Bellotti, 2001), 
this research area has been less explored.  
Occasionally, such discussions end with an online 
vote.   However, Ducheneaut and Belotti do note 
that voting is relatively infrequent and our own 
experience with our email corpora tends to support 
this.   
In general, we expect that these threads contain 
supporting discussions, and the actual decision 
might occur outside of the email medium, for 
example in a board meeting.  What we hope to 
observe is that, for some issue discussed, candidate 
solutions and responses highlighting the pros and 
cons of a solution are introduced via email.  
Decision-making discussion threads occur 
frequently enough in environments which depend 
on professional usage of email.  In the corpus we 
examined, 40% of the threads were decision-
making discussions.   
2.2 Constraints on and Choice of a Corpus of 
Email Discussions 
To collect a corpus of these threads, we placed a 
few constraints on the mailing list archives we 
found online.   
To begin with, we focused on threads from 
mailing lists that were set up to support 
organization activities as these often involve 
decision-making processes.  As we are also 
interested in examining the role of dialogue, we 
required access to the email thread structure from 
which we can infer a basic dialogue structure.      
We chose to use the archives of the Columbia 
University ACM Student Chapter Committee as 
this group has organized several events and used 
email as their primary mode of communication 
outside of meetings.  For practical reasons, it was 
relatively straightforward to obtain the necessary 
permissions to use the data, something that might 
be more difficult for other archives. Possible 
alternative corpora might be the mailing lists of 
organizing committees, for example that of a 
conference organizing committee or a steering 
group.  Project-based mailing lists might also be 
potentially used, especially if the group 
participants have sufficient shared background to 
engage in discussions.   
2.3 Observations on Thread Structure  
The Columbia University ACM Student Chapter 
Committee was made up of about 10 people. Upon 
initial examination of the data, we found that we 
could classify the threads of email according to its 
purpose.  The set of group tasks facilitated by the 
email correspondence were: decision-making, 
information provision, requests for action and 
social conversation. 
However, it is natural for the group to engage in 
multiple tasks.  Thus, we use the term ?task shift? 
to refer to adjacent segments of the thread 
(comprised of emails) which reflect distinct group 
goals.  In the corpus we use, we observe that these 
tasks usually occur sequentially.  In some cases, a 
single email proposes more than one issue for 
discussion, and subsequent responses address each 
of these in turn. 
Intuitively, it makes sense to create a summary 
for a single task.  Accordingly, we have designed 
our algorithm to accept only dialogue structures 
addressing a single group task.  If discussions 
invoke short clarification questions, these should 
not be treated differently if the task remains the 
same.  One supporting reason for this is the 
syntactic variation with which participants express 
disagreement.  We have observed that 
disagreement is often expressed as a clarification 
question, or as a question which offers an 
alternative suggestion. 
3 Related Work 
To date, email thread summarization has not 
been explored in any great depth within the Natural 
Language Processing (NLP) research community.   
Research on thread summarization has included 
some work on using dialogue structure for email 
summarization.  Nenkova et al (2003) advocate 
the use of overview sentences similar to ours.  
They extract sentences based on the presence of 
subject line key words.  However, should the 
subject line not reflect the content of the thread, 
our method has the potential to extract the true 
discussion issue since it based on the responses of 
other participants.  
Lam et al (2002) use the context of the 
preceding thread to provide background 
information for email summaries.  However, they 
note that even after appropriate preprocessing of 
email text, simply concatenating preceding context 
can lead to long summaries.  In contrast, instead of 
extracting email texts verbatim, we extract single 
sentences from particular emails in the thread. As a 
result, our summaries tend to be much shorter.     
Murakoshi et al (1999) describe an approach 
which extracts question-answer pairs from an 
email thread.  Extraction is based on the use of 
pattern-based information extraction methods.  The 
summary thus provides the question-answer pair 
intact, thereby improving the coherence.  Question-
answer summaries would presumably be suited to 
discussions which support an information 
provision task, a complementary task to the one we 
examine. 
Rambow et al (2004) apply sentence extraction 
techniques to the thread to construct a generic 
summary.  Though not specifically using dialogue 
structure, one feature used marks if a sentence is a 
question or not. 
Work has also been done on more accurately 
constructing the dialogue structure.  Newman and 
Blitzer (2003) focus on clustering related 
newsgroup messages into dialogue segments.  The 
segments are then linked using email header 
information to form a hierarchical structure.  Their 
summary is simply the first sentence from each 
segment.  We envisage dialogue structure 
summaries showing an overview of topics would 
be combined with approaches such as ours which 
provide summaries of segments.   
We also note the existing work that explores the 
summarization of speech transcripts.  Speech is a 
very different mode of communication.  An 
overview of the differences between asynchronous 
and synchronous modes of communication is 
provided by Clark (1991) and Simpson-Young et 
al. (2000).  Alexandersson et al (2000) note that in 
speech there is a tendency not to repeat shared 
conversation context.  They use the preceding 
dialogue structure, modeled using Dialogue 
Representation Theory, to provide additional 
ellipsed information.  It is unclear how such an 
approach might apply to an email corpus which has 
the potential to cover a broader set of domains.   
More recently, Zechner and Lavie (2001) 
identify question-answer dialogue segments in 
order to extract the pair as a whole.     
Hillard et al (2003) have also produced a system 
which generates summaries of speech discussions 
supporting a decision-making process.  Their work 
differs from ours in that they focus on categorizing 
the polarity of responses in order to summarize 
consensus. 
4 Issue Detection 
To make the problem more manageable we 
make the following assumptions about the types of 
threads that our algorithm will handle.  To begin 
with, we assume that the threads have been 
correctly constructed and classified as discussions 
supporting decision-making.  Needless to say, the 
first assumption is a little unrealistic given that 
thread construction is a difficult problem.  For 
example, it is not uncommon to receive emails 
with recycled subject lines simply because replying 
to an email is often more convenient than typing in 
an address.   
The other assumptions we make have to do with 
the dialogue structure of the threads.  The first is 
that the issue being discussed (usually a statement 
describing the matter to be decided) is to be found 
in the first email.  The second is that the email 
thread doesn?t shift task, nor does it contain 
multiple issues.   
The first assumption is based on what we have 
observed to be normal behavior.  Exceptions to this 
rule are broken threads and cases where the 
participants have responded to a forwarded email.  
In the first case, this can be seen as an error in 
thread construction and identification.  In such 
cases however, even in such a thread, the first 
email usually contains a reference to the issue at 
hand, although it may be an impoverished 
paraphrase.  Our algorithm extracts these 
paraphrases in lieu of the original wording.  Cases 
where participants have responded to a forwarded 
email are not common.  For such threads, we 
attempt to extract the sentence participants respond 
to.  However, again, this may not be the best 
formulation of the issue. 
Secondly, we assume that a text segmentation 
algorithm (for examples see Hearst?s ?Text-Tiling? 
algorithm 1997, Choi et al 2000) has already 
segmented the threads according to shifts in task.  
Operationally, our detection of shifts in task would 
then be based on corresponding changes in 
vocabulary used. 
4.1 The Algorithm 
Our summarization approach is to extract a set of 
sentences consisting of one issue, and the 
corresponding responses ? one per participant.  
Our sentence extraction mechanisms borrow from 
information retrieval methods which represent text 
as weighted term frequency vectors (for an 
overview see: Salton and McGill, 1983).   
In Figure 3, we present the general framework of 
the algorithm.  In this framework we divide the 
thread into two parts, the initiating email and the 
replies.  We create a comparison vector that 
represents what the replies are about.  We can 
construct variations of this framework by changing 
the way we build our comparison vector.  The aim 
is to compare each sentence to the comparison 
vector for the replies.  Thus, we build separate 
vector representations, called candidate vectors, for 
each sentence in the first email.  Using the cosine 
similarity metric to compare candidate vectors with 
the comparison vector, we rank the sentences of 
the first email.  Conceptually, the highest ranked 
sentence will be the one that is closest in content to 
the replies and this is extracted as the issue of the 
discussion. 
 
1. Separate thread into issue_email  and replies 
2. Create ?comparison vector? V representing replies 
3. For each sentence s in issue_email 
3.1 Construct vector representation S for sentence s 
3.2   Compare V and S using cosine similarity 
4. Rank sentences according to their cosine similarity 
scores 
5. Extract top ranking sentence 
Figure 3. Framework for extracting discussion 
Issues. 
We now discuss the four methods for building 
the comparison vector.  These are:  
1. The Centroid method 
2. The SVD Centroid method.   
3. The SVD Key Sentence method  
4. Combinations of methods: Oracles 
4.1.1 The Centroid Method 
In the Centroid method, we first build a term by 
sentence (t ? s) matrix, A, from the reply emails.  
In this matrix, rows represent sentences and 
columns represent unique words found in the 
thread.  Thus, the cells of a row store the term 
frequencies of words in a particular sentence.  
From this matrix, we form a centroid to represent 
the content of the replies.  This is a matter of 
summing each row vector and normalizing by the 
number of rows.  This centroid is then what we use 
as our comparison vector. 
4.1.2 The SVD Centroid Method 
Our interpretation of the SVD results is based on 
that of Gong and Liu (1999) and Hoffman (1999).  
Gong and Liu use SVD for text segmentation and 
summarization purposes.  Hoffman describes the 
results of SVD within a probabilistic framework.  
For a more complete summary of our interpretation 
of the SVD analysis see Wan et al (2003).     
To begin with, we construct the matrix A as in 
the Centroid Method.  The matrix A provides a 
representation of each sentence in w 
dimensionality, where w is the size of the 
vocabulary of the thread.  The SVD analysis1 is the 
product of three matrices U, S and V transpose.  In 
the following equation, dimensionality is indicated 
by the subscripts.  
 
SVD(At ? s) = Ut ? r Sr ? r(Vs ? r) tr 
 
Conceptually, the analysis essentially maps the 
sentences into a smaller dimensionality r, which 
we interpret as the main ?concepts? that are 
discussed in the sentences.  These dimensions, or 
concepts, are automatically identified by the SVD 
analysis on the basis of similarities of co-
occurrences.  The rows of V matrix represent the 
sentences of the first email, and each row vector 
describes how a given sentence relates to the 
discovered concepts.  Importantly, the number of 
discovered concepts is less than or equal to the 
vocabulary of the thread in question.  If it is less 
than the vocabulary size, then the SVD analysis 
has been able to combine several related terms into 
a single concept.  Conceptually, this corresponds to 
finding word associations between synonyms 
though in general, this association may not 
conserve part-of-speech.  In contrast to the values 
of the A matrix which are always positive (since 
they are based on frequencies), the values of each 
cell in the V matrix can be negative.  This 
represents the degree to which the sentence relates 
to a particular concept.  We build a centroid from 
the V matrix to form our comparison vector. 
4.1.3 The SVD Key Sentence Method 
The SVD Key Sentence Method is similar to the 
preceding method.  We build the matrix A, apply 
the SVD analysis and obtain the matrix V.   Instead 
of constructing a vector which represents all of the 
replies, we choose one sentence from the replies 
that is most representative of the thread content.  
This is done by selecting the most important 
concept and finding the sentence that contains the 
most words related to it.  The SVD analysis by 
default sorts the concepts according to degree to 
which sentences are associated with it.   By this 
definition, the most important sentence is 
                                                     
1
 We use the SVD function in the JAMA Java Matrix 
Package (http://math.nist.gov/javanumerics/jama/) to 
compute the analysis. 
represented by the values in the first column of the 
matrix V.  We then take the maximum of this 
column vector and note its row index, r, which 
denotes a sentence.   We use the rth  row vector of 
the V matrix as the comparison vector. 
In both the SVD Centroid method and the SVD 
Key Sentence method, the comparison vector has a 
different dimensionality than the candidate vectors.  
To perform the comparison, we must map the 
candidate vectors into this new dimensionality.  
This is done by pre-multiplying each candidate 
vector with the result of the matrix multiplication:  
Utranspose ? S.  Both of the matrices involved are 
obtained from the SVD analysis. 
4.1.4 Combinations of methods: Oracles 
Since we have three alternatives for constructing 
the comparison vector we consider the possibility 
of combining the approaches.  In Wan et al (2003) 
we showed that using a combination of traditional 
TF?IDF approaches and SVD approaches was 
useful given that SVD provided additional 
information about word associations.  Similarly, 
our two SVD methods provide complementary 
information.  The vector computed by the SVD 
centroid method provides information about the 
replies and accounts for word associations such as 
synonyms.  However, like the centroid method, 
this vector will include all topics discussed in the 
replies, even small digressions.  In contrast, the 
SVD Key sentence is potentially better at ignoring 
these digressions by focusing on a single concept.  
We present three heuristic oracles which 
essentially re-rank the candidate issue sentences 
identified by each of the three methods.  Re-
ranking is based on a voting mechanism.  The rules 
for three oracles are presented in Figures 4 and 5.   
 
1. If a majority exists return it 
2. If tie then: 
retrieve the lowest index number i,  
where i    1 
3. If all methods return different answers, then 
choose Centroid Method?s answer 
Figure 4.  Oracle 1 heuristic rules 
The oracle in Figure 4 attempts to choose the 
best sentence, retrieving a single sentence.  Rule 2 
attempts to encode the intuition that the issue 
sentence is likely to occur early in the email, 
however, not usually at the top of the email.  
Finally, we use the Centroid Method as a default 
because it is less prone to errors arising from low 
vocabulary sizes found in shorter threads.  For 
such threads, we found that SVD approaches tend 
not to perform so well.   
The second oracle again relies on a majority 
vote.  However, it relaxes the constraint of just 
returning a single sentence if the majority is the 
first sentence of the email.  Since we tend not to 
find issue sentences in at the very top of emails, we 
return all possible issue sentences in rule 1. 
 
1. If a majority exists then return it;  
UNLESS i = 1 in which case, return all 
choices  
2. If tie then retrieve the lowest index number i,  
where i    1 
3. If all methods return different answers, then 
choose Centroid Method?s answer 
Figure 5.  Oracle 2 heuristic rules 
Finally, as a baseline, the third oracle returns all 
the possible issue sentences identified by all of the 
contributing methods.  
5 Extracting the Responses to the Issue 
To extract the responses to the issue, we simply 
take the first sentence of the replies of each 
responding participant.  We make sure to only 
extract one response per participant.   
An alternative solution analogous to that of issue 
detection was also considered.  In this solution, we 
applied the issue detection algorithm to the reply 
email in question.  However, it turns out that most 
of the tagged responses occurred at the start of 
each reply email and a more complex approach 
was unnecessary and potentially introduced more 
errors. 
6 Evaluation of Issue Detection Algorithms 
6.1 The Test Data 
The test data used was a portion of the Columbia 
ACM Student Chapter corpus.  This corpus 
included a total of 300 threads which were 
constructed using message-ID information found 
in the header.  On average, there were 190 words 
per thread and 6.9 sentences in the first email.  
Threads longer than two emails2 were 
categorized manually.  We identified discussions 
that supported a decision-making process.  For 
these, we manually annotated the issue of the 
thread and the responses to the issue.  Although we 
do not currently use this information, we also 
classified the responses as being either in 
agreement or disagreement.  According to the 
assumptions listed in Section 4, we discarded those 
threads in which the issue was not found in the first 
email.  In total, we identified 37 discussion 
                                                     
2
 Longer threads offered a great chance of identifying 
a discussion. 
threads, each of which forms a test case.  A manual 
annotation of the discussion issues was done by 
following the instruction: Select the sentence from 
the first email that subsequent emails are 
responding to.?  These annotated issue sentences 
formed our gold standard. 
Our approach was designed to operate on the 
new textual contributions of each participant.  
Thus, the emails underwent a limited 
preprocessing stage.  Email headers, automatically 
embedded ?reply context? text and static signatures 
were ignored.   
6.2 Evaluation Framework and Results 
The evaluation was designed to test if our 
methods which use dialogue structure improve 
sentence extraction results.  We used the recall-
precision metric to compare the results of a system 
with the manually annotated gold standard.  In 
total, we tested 6 variations of our issue detection 
algorithms.  These included the Centroid method, 
the SVD Centroid method and the SVD Key 
Sentence method and the 3 oracles. 
For each test case, the approach being tested was 
used to extract one or more sentences 
corresponding to the issue of the discussion, which 
was then compared to the gold standard.  The 
baseline used was the first n sentences of the first 
email as a summary, where n ranged from 1 to 3 
sentences.   
The recall-precision results of the evaluation are 
presented in Table 1.  On average, the chance of 
correctly choosing the correct sentence randomly 
in a test set was 21.9%.   
We used an ANOVA to test whether there was 
an overall effect between the various methods for 
recall and precision.   We rejected the null 
hypothesis, that is, the choice of method does 
affect recall and precision (?=0.05, dfnumerator= 8, 
dfdenoinator= 324). 
To determine if our techniques were statistically 
significant compared to the baselines, we ran pair-
wise two-tailed student t-tests to compare the three 
methods and the first oracle to the n=1 baseline 
since these all returned a single sentence.  The 
results are presented in Table 2.   Similarly, Table 
3 shows the t-test comparisons for the oracle and 
oracle baseline against the n=3 baseline.   
Except for the SVD Key Sentence method, all 
the methods were significantly better than the n=1 
baseline.  However, a useful recall score was only 
obtained using the oracle methods.  When 
comparing the oracle methods which returned 
more than one sentence against the n=3 baseline, 
we found no significant difference in recall.  
However, when comparing precision performance 
we found that the difference between the precision 
of Centroid method and the three oracles were 
significantly different compared to the baseline. 
 
Method Ave.Recall % Ave. Prec. & 
Centroid 62.2 62.2 
SVD Centroid 48.6 48.6 
SVD Key Sent 37.8 37.8 
Oracle 1 62.2 62.2 
Oracle 2 70.3 62.7 
Oracle Baseline 83.8 45.1 
Baseline n=1 24.3 24.3 
Baseline n=2 48.6 24.3 
Baseline n=3 64.0 21.6 
Table 1. Average recall and precision values for 
each method.   
Method Prob(Recall)  Prob(Prec.) 
Centroid 0.0016 0.0016 
SVD Centroid 0.0187 0.0187 
SVD Key Sent 0.1601 0.1601 
Oracle 1 0.0004 0.0004 
Table 2.  Pair-wise t-test scores comparing each 
method to the n=1 baseline (df = 36).  The values 
show the probability of the obtained t value. 
Method Prob(Recall)  Prob(Prec.) 
Oracle 2 0.5998 0.0001 
Oracle Baseline 0.1686 0.0108 
Table 3. Pair-wise t-test scores comparing each 
method to the n=3 baseline (df = 36).  The values 
show the probability of the obtained t value. 
The recall and precision statistics for the 
Centroid method was the most impressive of the 
three methods proposed, far outperforming the 
baseline.  The results of comparisons involving the 
oracles, which combine the three methods, showed 
improved performance, suggesting that such 
techniques might potentially be useful in an email 
thread summary.  Whilst there was little difference 
between the recall values of the three oracles and 
the baselines, the benefit of using a more involved 
approach such as ours is demonstrated clearly by 
the gain in precision performance which will 
impact the usefulness of such a summary.   It is 
also interesting to note that the performance of the 
oracles was achieved by simply using simple rules 
without any corpus training. 
7 Conclusion and Future Work 
The methods described in this paper would form 
part of a larger email thread summarizer able to 
identify task boundaries and then initiate the 
appropriate summarization strategy for that task.  
We have addressed the sub-problem of 
summarizing the decision-making processes which 
have been supported by discussions over email. 
Despite the preliminary nature of our investigation, 
our findings are encouraging and lend support to 
the view that a combination of simple word vector 
approaches with singular value decomposition 
approaches do well at extracting discussion issues.  
Such methods, even with only a simple notion 
dialogue structure achieve a useful level of recall 
and precision.   We would like to conduct extrinsic 
experiments to test our assumptions about the 
usefulness of these summaries. Further 
investigations will also focus on examine issues of 
scalability, with regard to group size, and domain 
independence.   We would also like to investigate 
how issue detection might be integrated with a 
more complete solution to email thread 
summarization. 
8 Acknowledgements 
The research described in this paper was 
partially supported by a grant provided through 
NSF's Knowledge Discovery and Dissemination 
Program. We would like to thank the NLP groups 
of Columbia University, Macquarie University and 
CSIRO for feedback received on this work.   
References  
J. Alexandersson, P. Poller, M. Kipp, and R. Engel. 
2000. Multilingual Summary Generation in a 
Speech-To-Speech Translation System for 
Multilingual Dialogues. In Proc. of INLG-2000, 
Mitzpe Ramon, Israel.
B. Simpson-Young, N. Ozkan, C.Paris, C. Chung, 
J. Brook, K. Yap. 2000 Video Messaging: 
Addressing the Characteristics of the Medium. In 
the Proceedings of the Euromedia 2000 
Conference. Antwerp, May 2000. 
Borko, H., and Bernier, C. 1975 Abstracting 
Concepts and Methods. New York: Academic 
Press. 
F.Y.Y. Choi. 2000 Advances in domain 
independent linear text segmentation. In the 
Proc. of the North American Chapter of the 
Association. for Comp. Linguistics, pp. 26-33. 
Clark, H.H. and S.E Brennan. 1991. Grounding in 
Communication? In Readings in Groupware and 
Computer-Supported Collaborative Work, R.M. 
Baeker, ed. Morgan Kaufmann, California, 222-
223. 
Nicolas Ducheneaut, Victoria Bellotti 2001. E-mail 
as habitat: an exploration of embedded personal 
information management, interactions. in 
Communications of the ACM v.8 n.5, p.30-38. 
Dustin Hillard, Mari Ostendorf, and Elizabeth 
Shriberg 2003. Detection Of Agreement vs. 
Disagreement In Meetings: Training With 
Unlabeled Data. in the Proc. HLT-NAACL 
Conference, Edmonton, Canada, May 2003. 
Gong Y., and Liu, X. 2001. Generic Text 
Summarization Using Relevance Measure and 
Latent Semantic Analysis. In the Proceedings 
SIGIR 2001: pages 19-25. 
M. Hearst. 1994. Multi-paragraph segmentation of 
expository text. In the Proc. of the 2nd Annual 
Meeting of the Association for Computational 
Linguistics, Las Cruces, NM. 
Hiroyuki Murakoshi, Akira Shimazu, and Koichiro 
Ochimizu. 1999. Construction of Deliberation 
Structure in Email Communication. In 
Proceedings of the Pacific Association for 
Computational Linguistics (PACLING'99), pages 
16--28, Aug. 
T. Hofmann. 1999. Probabilistic latent semantic 
analysis, in the Proceedings of the Fifteenth 
Conference on Uncertainty in Artificial 
Intelligence, Morgan Kaufmann Publishers, San 
Francisco, CA, pp. 289-296. 
Lam, Derek and Rohall, Steven L. and Schmandt, 
Chris and Stern, Mia K. 2002. Exploiting E-mail 
Structure to Improve Summarization.  Technical 
Paper at IBM Watson Research Center #20-02 
Ani Nenkova and Amit Bagga. 2003. Facilitating 
email thread access by extractive summary 
generation. In Proceedings of RANLP, Bulgaria. 
Newman and Blitzer, Paula Newman and John 
Blitzer. 2002. Summarizing Archived 
Discussions: a Beginning. In the Proceeding of 
Intelligent User Interfaces. 
G. Salton and M. J. McGill. 1983. Introduction to 
modern information retrieval, McGraw-Hill, 
New York. 
Owen Rambow, Lokesh Shrestha, John Chen and 
Christy Laurdisen. 2004. Summarizing Email 
Threads. In the Proc. of HLT-NAACL 2004: 
Short Papers. 
Stephen Wan, Mark Dras, C?cile Paris, Robert 
Dale. 2003. Using Thematic Information in 
Statistical Headline Generation. In the 
Proceedings of the Workshop on Multilingual 
Summarization and Question Answering at ACL 
2003, July 11, Sapporo, Japan 
K. Zechner and A. Lavie. 2001 Increasing the 
coherence of spoken dialogue summaries by 
cross-speaker information linking. In 
Proceedings of the NAACL-01 Workshop on 
Automatic Summarization, Pittsburgh, PA, June, 
2001. 
Detection of Question-Answer Pairs in Email Conversations
Lokesh Shrestha and Kathleen McKeown
Columbia University
Computer Science Deparment
New York, NY 10027,
USA,
lokesh@cs.columbia.edu, kathy@cs.columbia.edu
Abstract
While sentence extraction as an approach to
summarization has been shown to work in docu-
ments of certain genres, because of the conver-
sational nature of email communication where
utterances are made in relation to one made
previously, sentence extraction may not capture
the necessary segments of dialogue that would
make a summary coherent. In this paper, we
present our work on the detection of question-
answer pairs in an email conversation for the
task of email summarization. We show that var-
ious features based on the structure of email-
threads can be used to improve upon lexical
similarity of discourse segments for question-
answer pairing.
1 Introduction
In this paper, we discuss work on the detection of
question and answer pairs in email threads, i.e., co-
herent exchanges of email messages among several
participants. Email is a written medium of asyn-
chronous multi-party communication. This means
that, as in face-to-face spoken dialog, the email
thread as a whole is a collaborative effort with inter-
action among the discourse participants. However,
unlike spoken dialog, the discourse participants are
not physically co-present, so that the written word is
the only channel of communication. Furthermore,
replies do not happen immediately, so that respon-
ders need to take special precautions to identify rel-
evant elements of the discourse context (for exam-
ple, by quoting previous messages). Thus, email is
a distinct linguistic genre that poses its own chal-
lenges to summarization.
With the increasing popularity of email as a
means of communication, an increasing number of
meetings are scheduled, events planned, issues re-
solved, and questions answered through emails. As
the number of emails in one?s mailbox increases, in-
Regarding ?acm home/bjarney?, on Apr 9,
2001, Muriel Danslop wrote:
Two things: Can someone be responsible for
the press releases for Stroustrup?
Responding to this on Apr 10, 2001, Theresa
Feng wrote:
I think Phil, who is probably a better writer
than most of us, is writing up something for
dang and Dave to send out to various ACM
chapters. Phil, we can just use that as our
?press release?, right?
In another subthread, on Apr 12, 2001, Kevin
Danquoit wrote:
Are you sending out upcoming events for this
week?
Figure 1: Sample summary obtained with sentence
extraction
formation from past conversations becomes increas-
ingly inaccessible and difficult to manage. For ex-
ample, a number of emails can be used in schedul-
ing a meeting, and a search for information on the
meeting may retrieve all the intermediate emails,
thus hindering one?s access to the required informa-
tion. Access to required information could be dra-
matically improved by querying summaries of email
conversations
While summarization of email conversations
seems a natural way to improve upon current
methods of email management, research on email
summarization is in early stages. Consider an
example summary of a thread of email conver-
sation produced by a sentence extraction based
email thread summarization system developed at
Columbia (Rambow et al, 2004) shown in Figure 1.
While this summary does include an answer to the
first question, it does not include answers to the
two questions posed subsequently even though the
answers are present in the thread. This example
demonstrates one of the inadequacies of sentence
extraction based summarization modules: namely,
the absence of discourse segments that would have
made the summaries more readable and complete.
A summarization module that includes answers to
questions posed in extractive summaries, then, be-
comes very useful.
Further, questions are a natural means of resolv-
ing any issue. This is especially so of email conver-
sations through which most of our issues, whether
professional or personal, get resolved. And, the
asynchronous nature of email conversation makes it
possible for users to pursue several questions in par-
allel. In fact, in our corpus of email exchanges, we
found that about 20% of all email threads focus pri-
marily on a question-answer exchange, whether one
question is posed and multiple people respond or
whether multiple questions are posed and multiple
responses given. For these type of email exchanges,
a summary that can highlight the main question(s)
asked and the response(s) given would be useful.
Being able to distinguish questions pertaining to dif-
ferent issues in an email thread and being able to
associate the answers with their questions is a ne-
cessity in order to generate this type of summary.
In this paper, we present our work on the detec-
tion of question and answer pairs in email conver-
sations. The question-answer detection system we
present will ultimately serve as one component of a
full email summarization system, providing a por-
tion of summary content. We developed one ap-
proach for the detection of questions in email mes-
sages, and a separate approach to detect the corre-
sponding answers. These are described in turn be-
low.
2 Previous and Related Work
(Muresan et al, 2001) describe work on summariz-
ing individual email messages using machine learn-
ing approaches to learn rules for salient noun phrase
extraction. In contrast, our work aims at summariz-
ing whole threads and at capturing the interactive
nature of email.
(Nenkova and Bagga, 2003) present work on gen-
erating extractive summaries of threads in archived
discussions. A sentence from the root message and
from each response to the root is extracted using
ad-hoc algorithms crafted by hand. This approach
works best when the subject of the root email best
describes the ?issue? of the thread, and when the
root email does not discuss more than one issue. In
our work, we do not make any assumptions about
the nature of the email, and try to learn strategies to
link question and answer segments for summariza-
tion.
(Newman and Blitzer, 2003) also address the
problem of summarizing archived discussion lists.
They cluster messages into topic groups, and then
extract summaries for each cluster. The summary of
a cluster is extracted using a scoring metric based on
sentence position, lexical similarity of a sentence to
cluster centroid, and a feature based on quotation,
among others. Because the summaries are extrac-
tive in nature, this approach still suffers from the
possibility of incomplete summaries.
(Lam et al, 2002) present work on email summa-
rization by exploiting the thread structure of email
conversation and common features such as named
entities and dates. They summarize the message
only, though the content of the message to be sum-
marized is ?expanded? using the content from its an-
cestor messages. The expanded message is passed
to a document summarizer which is used as a black
box to generate summaries. Our work, in contrast,
aims at summarizing the whole thread, and we are
precisely interested in changing the summarization
algorithm itself, not in using a black box summa-
rizer.
In addition, there has been some work on summa-
rizing meetings. As discussed in Section 1, email is
different in important respects from multi-party di-
alog. However, some important aspects are related.
(Zechner and Lavie, 2001), for example, presents
a spoken dialogue summarization system that takes
into consideration local cross-speaker coherence by
linking question answer pairs, and uses this infor-
mation to generate extract based summaries with
complete question-answer regions. While we have
used a similar question detection approach, our ap-
proach to answer detection is different. We get back
to this in Section 4.
(Rambow et al, 2004) show that sentence ex-
traction techniques can work for summarizing email
threads, but profit from email-specific features. In
addition, they show that the presentation of the sum-
mary should take into account the dialogic structure
of email communication. However, since their ap-
proach does not try to detect question and answer
pairs, the extractive summaries suffer from the pos-
sibility of incomplete summaries.
3 Automatic Question Detection
While the detection of questions in email messages
is not as difficult a problem as in speech conversa-
tions where features such as the question mark char-
acter are absent, relying on the use of question mark
character for identifying questions in email mes-
sages is not adequate. The need for special attention
in detecting questions in email messages arises due
to three reasons. First, the use of informal language
means users might use the question mark character
in cases other than questions (for example, to de-
note uncertainty) and may overlook using a question
mark after a question. Second, a question may be
stated in a declarative form, as in, ?I was wondering
if you are free at 5pm today.? Third, not every ques-
tion, whether in an interrogative form or in a declar-
ative form, is meant to be answered. For example,
rhetorical questions are used for purposes other than
to obtain the information the question asked, and are
not required to be associated with answer segments.
We used supervised rule induction for the de-
tection of interrogative questions. Training exam-
ples were extracted from the transcribed SWITCH-
BOARD corpus annotated with DAMSL tags.1
This particular corpus was chosen not only be-
cause an adequate number of training examples
could be extracted from the manual annotations,
but also because of the use of informal language
in speech that is also characteristic of email con-
versation. Utterances with DAMSL tags ?sv?
(speech act ?statement-opinion?) and ?sd? (speech
act ?statement-non-opinion?) were used to extract
5,000 negative examples. Similarly, utterances
with tags ?qy? (?yes-no-question?), ?qw? (?Wh-
question?), and ?qh? (?rhetorical-question?) were
used to extract 5,000 positive examples. Each utter-
ance was then represented by a feature vector which
included the following features:
? POS tags for the first five terms (shorter utter-
ances were padded with dummies)
? POS tags for the last five terms (shorter utter-
ances were padded with dummies)
? length of the utterance
1From the Johns Hopkins University LVCSR
Summer Workshop 1997, available from
http://www.colorado.edu/ling/jurafsky/ws97/
Scheme Ripper Ripper+
Recall 0.56 0.72
Precision 0.96 0.96
F1-score 0.70 0.82
Table 1: Test results for detection of questions in
interrogative form
? POS-bigrams from a list of 100 most discrimi-
nating POS-bigrams list.
The list of most discriminating POS-bigrams was
obtained from the training data by following the
same procedure that (Zechner and Lavie, 2001)
used.
We then used Ripper (Cohen, 1996) to learn
rules for question detection. Like many learning
programs, Ripper takes as input the classes to be
learned, a set of feature names and possible values,
and training data specifying the class and feature
values for each training example. In our case, the
training examples are the speech acts extracted from
the SWITCHBOARD corpus as described above.
Ripper outputs a classification model for predicting
the class (i.e., whether a speech act is a question
or not) of future examples; the model is expressed
as an ordered set of if-then rules. For testing, we
manually extracted 300 questions in interrogative
form and 300 statements in declarative form from
the ACM corpus.2 We show our test results with re-
call, precision and F1-score3 in Table 1 on the first
column.
While the test results show that the precision was
very good, the recall score could be further im-
proved. Upon further investigation on why the re-
call was so low, we found that unlike the positive
examples we used in our training data, most of the
questions in the test data that were missed by the
rules learned by Ripper started with a declarative
phrase. For example, both ?I know its on 108th, but
after that not sure, how exactly do we get there??,
and ?By the way, are we shutting down clic?? be-
gin with declarative phrases and were missed by the
Ripper learned rules. Following this observation,
2More information on the ACM corpus will be provided in
Section 4.1. At the time of the development of the question
detection module the annotations were not available to us, so
we had to manually extract the required test speech acts.
3F1-score = 2PRP+R , where P=Precision and R=Recall
we manually updated our question detection mod-
ule to break a speech act that was not initially pre-
dicted as question into phrases separated by comma
characters. Then we applied the rules on the first
phrase of the speech act and if that failed on the last
phrase. For example, the rules would fail on the
phrase ?I know its on 108th?, but would be able to
classify the phrase ?how exactly do we get there?
as a question. In doing this we were able to increase
the recall score to 0.72, leading to a F1-score of 0.82
as shown in Table 1 in the second column.
4 Automatic Answer Detection
While the automatic detection of questions in email
messages is relatively easier than the detection of
the same in speech conversations, the asynchronous
nature of email conversations makes detection and
pairing of question and answer pairs a more diffi-
cult task. Whereas in speech a set of heuristics can
be used to identify answers as shown by (Zechner
and Lavie, 2001), such heuristics cannot be readily
applied to the task of question and answer pairing in
email conversations. First, more than one topic can
be discussed in parallel in an email thread, which
implies that questions relating to more than a single
topic can be pursued in parallel. Second, even when
an email thread starts with the discussion of a single
topic, the thread may eventually be used to initiate a
different topic just because the previous topic?s list
of recipients closely matched those required for the
newly initiated topic. Third, because of the use of
?Reply? and ?ReplyAll? functions in email clients,
a user may be responding to an issue posed earlier
in the thread while using one of the email messages
subsequent to the message posing the issue to ?reply
back? to that issue. So, while it may seem from the
structure of the email thread that a person is reply-
ing back to a certain email, the person may actually
be replying back to an email earlier in the thread.
This implies that when several persons answer a
question, there may be answers which appear sev-
eral emails after the email posing the question. Fi-
nally, the fact that the question and its correspond-
ing answers may have few words in common fur-
ther complicates answer detection. This is possible
when a person uses the context of the email conver-
sation to ask questions and make answers, and the
semantics of such questions and answers have to be
interpreted with respect to the context they appear
in. Such context is readily available for a reader
through the use of quoted material from past email
messages. All of these make the task of detecting
and linking question and answer pairs in email con-
versations a complicated task. However, this task
is not as complicated a task as automatic question
answering where the search space for candidate an-
swers is much wider and more sophisticated mea-
sures than those based on lexical similarity have to
be employed.
Our approach to automatic answer detection in
email conversations is based on the observation that
while a number of issues may be pursued in paral-
lel, users tend to use separate paragraphs to address
separate issues in the same email message. While
a more complicated approach to segmentation of
email messages could be possible, we have used this
basic observation to delineate discourse segments in
email messages. Further, because discourse seg-
ments contain more lexical context than their in-
dividual sentences, our approach detects associa-
tions between pairs of discourse segments rather
than pairs of sentences.
We now present our machine learning approach
to automatic detection of question and answer pairs.
4.1 Corpus
Our corpus consists of approximately 300 threads of
about 1000 individual email messages sent during
one academic year among the members of the board
of the student organization of the ACM at Columbia
University. The emails dealt mainly with planning
events of various types, though other issues were
also addressed. On average, each thread contained
3.25 email messages, with all threads containing at
least two messages, and the longest thread contain-
ing 18 messages. Threads were constructed from
the individual email messages using the ?In-Reply-
To? header information to link parent and child
email messages.
Two annotators (DB and GR) each were asked
to highlight and link question and answer pairs in
the corpus. Our work presented here is based on
the work these annotators had completed at the time
of this writing. GR has completed work on 200
threads of which there are 81 QA threads (threads
with question and answer pairs), 98 question seg-
ments, and 142 question and answer pairs. DB has
completed work on 138 threads of which there are
62 QA threads, 72 question segments, and 92 ques-
tion and answer pairs. We consider a segment to
be a question segment if a sentence in that segment
has been highlighted as a question. Similarly, we
consider a segment to be an answer segment if a
sentence in that segment has been paired with a
question to form a question and answer pair. The
kappa statistic (Carletta, 1996) for identifying ques-
tion segments is 0.68, and for linking question and
answer segments given a question segment is 0.81.
4.2 Features
For each question segment in an email message,
we make a list of candidate answer segments.
This is basically just a list of original (content
that is not quoted from past emails)4 segments in
all the messages in the thread subsequent to the
message of the question segment. Let the thread in
consideration be called t, the container message of
the question segment be called mq, the container
message of the candidate answer segment be called
ma, the question segment be called q, and the
candidate answer segment be called a. For each
question and candidate answer pair, we compute
the following sets of features:
4.2.1 Some Standard Features
(a) number of non stop words in segment q and seg-
ment a;
(b) cosine similarity5 and euclidean distance6 be-
tween segment q and a;
4.2.2 Features derived from the structure of
the thread t
(c) the number of intermediate messages between
mq and ma in t;
(d) the ratio of the number of messages in t sent ear-
4While people do use quoted material to respond to specific
segments of past emails, a phenomenon more common is dis-
cussion lists, because the occurrence of such is rare in the ACM
corpus we decided not to use them as a feature.
5
cosine sim(x, y) =
?N
i=1(cxi ? cyi)??N
j=1 c2xj ?
?N
j=1 c2xj
where cxi is the count of word i in segment x, and cyi is thecount of word i in segment y.
6
euclidean dis(x, y) =
????
N?
i=1
(c2xi ? c2yi)
where cxi is the count of word i in segment x, and cyi is thecount of word i in segment y.
lier than mq and all the messages in t, and similarly
for ma;
(e) whether a is the first segment in the list of candi-
date answer segments of q (this is true if a segment
is the first segment in the first message sent in reply
to mq);
4.2.3 Features based on the other candidate
answer segments of q
(f) number of candidate answer segments of q and
the number of candidate answer segments of q after
a (a segment x is considered to be after another seg-
ment y if x is from a message sent later than that of
y, or if x appears after y in the same message);
(g) the ratio of the number of candidate answer seg-
ments before a and the number of all candidate an-
swer segments (a segment x is considered to be be-
fore another segment y if x is from a message sent
earlier than that of y, or if x appears before y in the
same message); and
(h) whether q is the most similar segment of a
among all segments from ancestor messages of ma
based on cosine similarity (the list of ancestor mes-
sages of a message is computed by recursively fol-
lowing the ?In-Reply-To? header information that
points to the parent message of a message).
While the contribution of a single feature to the
classification task may not be intuitively apparent,
we hope that a combination of a subset of these
features, in some way, would help us in detecting
question-answer pairs. For example, when the num-
ber of candidate answer segments for a question
segment is less than or equal to two, feature (e) may
be the best contributor to the classification task. But,
when the number of candidate answer segments is
high, a collection of some features may be the best
contributor.
We categorized each feature vector for the pair q
and a as a positive example if a has been marked as
an answer and linked with q.
4.3 Training Data
We computed four sets of training data. Two for
each of the annotators separately, which we call DB
and GR according to the label of their respective
annotator. One taking the union of the annotators,
which we call Union, and another taking the inter-
section, which we call Inter. For the first two sets,
we collected the threads that had at least one ques-
tion and answer pair marked by the respective anno-
tator. For each question that has an answer marked
Data Set DB GR Union Inter
datapoints 259 355 430 181
positives 89 139 168 59
questions 72 98 118 52
threads 62 81 97 46
Table 2: Summary of training data: number of in-
stances
Data Set Precision Recall F1-score
DB 0.6 0.27 0.372
GR 0.629 0.439 0.517
Union 0.61 0.429 0.503
Inter 0.571 0.407 0.475
Table 3: Baseline results
(some of the highlighted questions do not have cor-
responding answers in the thread), we computed a
list of feature vectors as described above with all of
its candidate answer segments. For the union set,
we collected all the threads that had question and
answer pairs marked by either annotator, and com-
puted the feature vectors for each such question seg-
ment. A feature vector was categorized positive if
any of the two annotators have marked the respec-
tive candidate answer segment as an answer. For
the intersection set, we collected all the threads that
had question and answer pairs marked by both an-
notators. Here, a feature vector was labelled posi-
tive only if both the annotators marked the respec-
tive candidate answer segment as an answer. Ta-
ble 2 summarizes the information on the four sets
of training data.
4.4 Experiments and Results
This section describes experiments using Ripper to
automatically induce question and candidate answer
pair classifiers, using the features described in Se-
cion 4.2. We obtained the results presented here us-
ing five-fold cross-validation.
Table 3 shows the precision, recall and F1-score
for the four datasets using the cosine similarity fea-
ture only. We use these results as the baseline
against which we compare the results for the full
set of features shown in Table 4. While precision
using the full feature set is comparable to that of
the baseline measure, we get a significant improve-
ment on recall with the full feature set. The base-
Data Set Precision Recall F1-score
DB 0.69 0.652 0.671
GR 0.68 0.612 0.644
Union 0.698 0.619 0.656
Inter 0.6 0.508 0.55
Table 4: Summary of results
line measure predicts that the candidate answer seg-
ment whose similarity with the question segment is
above a certain threshold will be an actual answer
segment. Our results suggest that lexical similarity
cannot alone capture the rules associated with ques-
tion and answer pairing, and that the use of various
features based on the structure of the thread of email
conversations can be used to improve upon lexical
similarity of discourse segments. Further, while the
results do not seem to suggest a clear preference for
the data set DB over the data set GR (this could be
explained by their high kappa score of 0.81), taking
the union of the two datasets does seem to be bet-
ter than taking the intersection of the two datasets.
This could be because the intersection greatly re-
duces the number of positive data points from what
is available in the union, and hence makes the learn-
ing of rules more difficult with Inter.
Finally, on observing that some questions had at
most 2 candidate answers, and others had quite a
few, we investigated what happens when we divide
the data set Union into two data sets, one for ques-
tion segments with 2 or less candidate answer seg-
ments which we call the data set Union a, and the
other with the rest of the data set which we call the
data set Union b. Union a has, on average, 1.5 can-
didate answer segments, while Union b has 5.7. We
show the results of this experiment with the full fea-
ture set in Table 5. Our results show that it is much
easier to learn rules for questions with the data set
Union a, which we show in the first row, than other-
wise. We compare our results for the baseline mea-
sure of predicting the majority class, which we show
in the second row, to demonstrate that the results
obtained with the dataset Union a were not due to
majority class prediction. While the results for the
other subset, Union b, which we show in the third
row, compare well with the results for Union, when
the results for the data sets Union a and Union b
are combined, which we show in the fourth row,
we achieve better results than without the splitting,
Data Set Precision Recall F1-score positives datapoints
Union a 0.879 0.921 0.899 63 79
Baseline for Union a 0.797 1.0 0.887 63 79
Union b 0.631 0.619 0.625 105 351
Combined 0.728 0.732 0.730 168 430
Union 0.698 0.619 0.656 168 430
Table 5: Summary of results with the split data set compared with a baseline and the unsplit data set
shown in the last row.
5 Conclusion and Future Work
We have presented an approach to detect question-
answer pairs with good results. Our approach is
the first step towards a system which can highlight
question-answer pairs in a generated summary. Our
approach works well with interrogative questions,
but we have not addressed the automatic detection
of questions in the declarative form and rhetorical
questions. People often pose their requests in a
declarative form in order to be polite among other
reasons. Such requests could be detected with their
use of certain key phrases some of which include
?Please let me know...?, ?I was wondering if...?, and
?If you could....that would be great.?. And, because
rhetorical questions are used for purposes other than
to obtain the information the question asked, such
questions do not require to be paired with answers.
The automatic detection of these question types are
still under investigation.
Further, while the approach to the detection of
question-answer pairs in threads of email conver-
sation we have presented here is quite effective, as
shown by our results, the use of such pairs of dis-
course segments for use in summarization of email
conversations is an area of open research to us.
As we discussed in Section 1, generation of sum-
maries for email conversations that are devoted to
question-answer exchanges and that integrate iden-
tified question-answer pairs as part of a full sum-
mary is also needed.
6 Acknowledgements
We are grateful to Owen Rambow for his helpful ad-
vice. We also thank Andrew Rosenberg for his dis-
cussion on kappa statistic as it relates to the ACM
corpus. This work was supported by the National
Science Foundation under the KDD program. Any
opinions, findings, and conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
References
Jean Carletta. 1996. Assessing agreement on clas-
sification tasks: The kappa statistic. Computa-
tional Linguistics, 22(2):249?254.
William Cohen. 1996. Learning trees and rules
with set-valued features. In Fourteenth Confer-
ence of the American Association of Articial In-
telligence. AAAI.
Derek Lam, Steven L. Rohall, Chris Schmandt, and
Mia K. Stern. 2002. Exploiting e-mail structure
to improve summarization. In ACM 2002 Confer-
ence on Computer Supported Cooperative Work
(CSCW2002), Interactive Posters, New Orleans,
LA.
Smaranda Muresan, Evelyne Tzoukermann, and Ju-
dith Klavans. 2001. Combining Linguistic and
Machine Learning Techniques for Email Sum-
marization. In Proceedings of the CoNLL 2001
Workshop at the ACL/EACL 2001 Conference.
Ani Nenkova and Amit Bagga. 2003. Facilitating
email thread access by extractive summary gen-
eration. In Proceedings of RANLP, Bulgaria.
Paula Newman and John Blitzer. 2003. Summariz-
ing archived discussions: a beginning. In Pro-
ceedings of Intelligent User Interfaces.
Owen Rambow, Lokesh Shrestha, John Chen, and
Christy Lauridsen. 2004. Summarizaing email
threads. In Proceedings of HLT-NAACL 2004:
Short Papers.
Klaus Zechner and Alon Lavie. 2001. Increasing
the coherence of spoken dialogue summaries by
cross-speaker information linking. In Proceed-
ings of the NAACL-01 Workshop on Automatic
Summarization, Pittsburgh, PA.
Syntactic Simplication for Improving Content Selection in Multi-Document
Summarization
Advaith Siddharthan, Ani Nenkova and Kathleen McKeown
Columbia University
Computer Science Department
 	
 

@ Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 873?880
Manchester, August 2008
A Framework for Identifying Textual Redundancy
Kapil Thadani and Kathleen McKeown
Department of Computer Science,
Columbia University,
New York, NY USA
{kapil,kathy}@cs.columbia.edu
Abstract
The task of identifying redundant infor-
mation in documents that are generated
from multiple sources provides a signifi-
cant challenge for summarization and QA
systems. Traditional clustering techniques
detect redundancy at the sentential level
and do not guarantee the preservation of
all information within the document. We
discuss an algorithm that generates a novel
graph-based representation for a document
and then utilizes a set cover approximation
algorithm to remove redundant text from it.
Our experiments show that this approach
offers a significant performance advantage
over clustering when evaluated over an an-
notated dataset.
1 Introduction
This paper approaches the problem of identifying
and reducing redundant information in documents
that are generated from multiple sources. This task
is closely related to many well-studied problems
in the field of natural language processing such as
summarization and paraphrase recognition. Sys-
tems that utilize data from multiple sources, such
as question-answering and extractive summariza-
tion systems that operate on news data, usually in-
clude a component to remove redundant informa-
tion from appearing in their generated output.
However, practical attempts at reducing redun-
dancy in the output of these types of systems usu-
ally involve clustering the sentences of the gener-
ated output, picking a representative sentence from
each cluster and discarding the rest. Although
this strategy would remove some redundant in-
formation, clustering approaches tuned for coarse
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
matches could also remove non-redundant infor-
mation whereas clustering approaches tuned for
near-exact matches could end up removing very
little repeated information. This is simply a con-
sequence of the fact that information can, and usu-
ally does, exist at the sub-sentential level and that
clusters of sentences don?t necessarily correspond
to clusters of information.
In this paper, we discuss a framework for build-
ing a novel graph-based representation to detect re-
dundancy within documents. We identify redun-
dancy at the sub-sentential level through pairwise
alignment between the sentences of a document
and use this to build a bipartite graph which en-
ables us to keep track of redundant information
across all sentences. Common information be-
tween pairs of sentences, detected with the align-
ment algorithm, can be extrapolated to document-
wide units of information using the graph struc-
ture. Individual sentences that are encompassed
by the information in the rest of the document can
then be identified and removed efficiently by us-
ing a well-known greedy algorithm adapted for this
representation.
2 Related Work
The challenge of minimizing redundant informa-
tion is commonly faced by IR engines and extrac-
tive summarization systems when generating their
responses. A well-known diversity-based rerank-
ing technique for these types of systems is MMR
(Carbonell and Goldstein, 1998), which attempts
to reduce redundancy by preferring sentences that
differ from the sentences already selected for the
summary. However, this approach does not at-
tempt to identify sub-sentential redundancy.
Alternative approaches to identifying redun-
dancy use clustering at the sentence level (Lin and
Hovy, 2001) to remove sentences that are largely
repetitive; however, as noted earlier, this is not
well-suited to the redundancy task. The use of sen-
873
tence simplification in conjunction with clustering
(Siddharthan et al, 2004) could help alleviate this
problem by effectively clustering smaller units, but
this issue cannot be avoided unless sentences are
simplified to atomic elements of information.
Other research has introduced the notion of
identifying concepts in the input text (Filatova and
Hatzivassiloglou, 2004), using a set cover algo-
rithm to attempt to include as many concepts as
possible. However, this approach uses tf-idf to
approximate concepts and thus doesn?t explicitly
identify redundant text. Our work draws on this
approach but extends it to identify all detectable
redundancies within a document set.
Another approach does identify small sub-
sentential units of information within text called
?Basic Elements? and uses these for evaluating
summarizations (Hovy et al, 2006). Our approach,
in contrast, does not make assumptions about the
size or structure of redundant information since
this is uncovered through alignments.
We thus require the use of an alignment algo-
rithm to extract the common information between
two pieces of text. This is related to the well-
studied problem of identifying paraphrases (Barzi-
lay and Lee, 2003; Pang et al, 2003) and the more
general variant of recognizing textual entailment,
which explores whether information expressed in
a hypothesis can be inferred from a given premise.
Entailment problems have also been approached
with a wide variety of techniques, one of which
is dependency tree alignment (Marsi et al, 2006),
which we utilize as well to align segments of text
while respecting syntax. However, our definition
of redundancy does not extend to include unidi-
rectional entailment, and the alignment process is
simply required to identify equivalent information.
3 Levels of Information
In describing the redundancy task, we deal with
multiple levels of semantic abstraction from the
basic lexical form. This section describes the ter-
minology used in this paper and the graph-based
representation that is central to our approach.
3.1 Terminology
The following terms are used throughout this paper
to refer to different aspects of a document.
Snippet: This is any span of text in the doc-
ument and is a lexical realization of information.
While a snippet generally refers to a single sen-
tence within a document, it can apply to multiple
sentences or phrases within sentences. Since re-
dundancy will be reduced by removing whole snip-
pets, a snippet can be defined as the smallest unit
of text that can be dropped from a document for
the purpose of reducing redundancy.
To illustrate the levels of information that we
consider, consider the following set of short sen-
tences as snippets. Although this is a synthe-
sized example to simplify presentation, sentences
with this type of overlapping information occur
frequently in the question-answering scenario over
news in which our approach has been used.
x
1
: Whittington is an attorney.
x
2
: Cheney shot Whittington, a lawyer.
x
3
: Whittington, an attorney, was shot in Texas.
x
4
: Whittington was shot by Cheney while hunting quail.
x
5
: This happened during a quail hunt in Texas.
We can see that all the information in x
1
is con-
tained in both x
2
and x
3
. While no other snip-
pet is completely subsumed by any single snippet,
they can be made redundant given combinations of
other snippets; for example, x
4
is redundant given
x
2
, x
3
and x
5
. In order to identify these combina-
tions, we need to identify the elements of informa-
tion within each snippet.
Concept: This refers to a basic unit of informa-
tion within a document. Concepts may be facts,
opinions or details. These are not necessarily se-
memes, which are atomic units of meaning, but
simply units of information that are seen as atomic
within the document. We further restrict the defi-
nition of a concept to a unit of information seen in
more than one snippet, since we are only interested
in concepts which help in identifying redundancy.
Formally, a document can be defined as a set of
S snippets X = {x
1
, . . . ,x
S
}, which is a literal
representation of the document. However, it can
also be defined in terms of its information content.
We use Z = {z
1
, . . . , z
C
} to represent the set of C
concepts that cover all information appearing more
than once in the document. In the example above,
we can identify five non-overlapping concepts:
z
A
: Whittington was shot
z
B
: Whittington is an attorney
z
C
: The shooting occurred in Texas
z
D
: It happened during a hunt for quail
z
E
: Cheney was the shooter
We use subscripts for snippet indices and super-
scripts for concept indices throughout this paper.
874
Nugget: This is a textual representation of a
concept in a snippet and therefore expresses some
information which is also expressed elsewhere in
the document. Different nuggets for a given con-
cept may have unique lexico-syntactic realizations,
as long as they all embody the same semantics.
With regard to the notation used above, nuggets
can be represented by an S ? C matrix Y where
each y
c
s
denotes the fragment of text (if any) from
the snippet x
s
that represents concept z
c
.
Since a concept itself has no unique textual re-
alization, it can be simply represented by the com-
bination of all its nuggets. For instance, in the ex-
ample shown above, concept z
D
is seen in both x
4
and x
5
in the form of two nuggets y
D
4
(?... while
hunting quail?) and y
D
5
(?... during a quail hunt?),
which are paraphrases. The degree to which we
can consider this and other types of lexical or syn-
tactic differences between nuggets that have the
same semantic identity depends on the alignment
algorithm used.
Intersection: This is the common information
between two snippets that can be obtained through
their alignment. For example, the intersection
from the alignment between x
2
and x
4
consists
of two fragments of text that express that Cheney
shot Whittington (an active-voiced fragment from
x
2
and a passive-voiced fragment from x
4
).
In general, aligning x
i
and x
j
produces an in-
tersection v
i,j
which is simply a pair of aligned
text fragments covering the set of concepts that x
i
and x
j
have in common. However, these undi-
vided segments of text may actually contain mul-
tiple nuggets from a document-wide perspective.
We assume that intersections can be decomposed
into smaller intersections through further align-
ments with snippets or other intersections; this pro-
cess is explained in Subsection 4.3.
3.2 Concept graph representation
Figure 1 illustrates the example introduced in Sub-
section 3.1 as a network with intersections repre-
sented as edges between snippets. This is the type
of graph that would be built using pairwise align-
ments between all snippets. Note that although
some intersections such as v
1,2
(between x
1
and
x
2
) and v
3,5
express concepts directly, other inter-
sections such as v
2,3
and v
2,4
are undivided com-
binations of concepts. Since we cannot directly
identify concepts and their nuggets, this graph is
not immediately useful for reducing redundancy.
x
1
B
x
2
ABE
x
3
ABC
x
4
ADE
x
5
CD
B
B
AB
AE
A
C
D
Figure 1: Graph representing pairwise alignments
between the example snippets from Section 3. For
clarity, alphabetic labels like A represent concepts
z
A
. Node labels show concepts within snippets and
edge labels indicate concepts seen in intersections.
x
1
B
x
2
ABE
x
3
ABC
x
4
ADE
x
5
CD
z
A
z
B
z
C
z
D
z
E
y
B
1
y
A
2
y
B
2
y
E
2
y
A
3
y
B
3
y
C
3
y
A
4
y
D
4
y
E
4
y
C
5
y
D
5
Figure 2: Structure of the equivalent concept graph
for the example document illustrated in Figure 1.
Circular nodes x
s
represent snippets, large squares
z
c
represent concepts and small squares y
c
s
depict
nuggets for each concept within a snippet.
Now, since the matrix Y describes the interac-
tion of concepts with snippets, it can be viewed as
an incidence matrix that defines a bipartite graph
between snippets and concepts with nuggets rep-
resenting the edges. In this concept graph repre-
sentation, each snippet can connect to any number
of other snippets via a shared concept. Since con-
cepts serve to connect multiple snippets together,
the concept graph can also be seen as a hypergraph,
which is a generalization of a graph in which each
edge may connect together multiple vertices.
875
Figure 2 illustrates the structure of the equiva-
lent concept graph for the previous example. This
is simply the bipartite graph with the two types of
nodes, namely snippets and concepts, represented
using different symbols. For clarity, nuggets are
also depicted as nodes in the graph, thereby reduc-
ing edges to simple links indicating membership.
This representation identifies the redundancy be-
tween snippets in terms of non-overlapping con-
cepts and is therefore more useful than the graph
from Figure 1 for reducing redundancy.
4 Constructing the Concept Graph
We now describe how a concept graph can be con-
structed from a document by using dependency
tree alignment and leveraging the existing struc-
ture of the graph during construction.
4.1 Alignment of snippets
In order to obtain the concept graph representation,
the common information between each pair of
snippets in the document must first be discovered
by aligning all pairs of snippets with each other.
We make use of dependency parsing and alignment
of dependency parse trees to obtain intersections
between each pair of snippets, where each inter-
section may be a discontiguous span of text corre-
sponding to an aligned subtree within each snip-
pet. In our experiments, dependency parsing is
accomplished with Minipar (Lin, 1998) and align-
ment is done using a bottom-up tree alignment al-
gorithm (Barzilay and McKeown, 2005) modified
to account for the shallow semantic role labels pro-
duced by the parser. The alignment implementa-
tion is not the focus of this work, however, and the
framework described here could by applied using
any alignment technique between segments of text
in potentially any language.
As seen in Figure 1, the intersections that can be
extracted solely by pairwise comparisons are not
unique and may contain multiple concepts. A truly
information-preserving approach requires the ex-
plicit identification of concepts as in the concept
graph from Figure 2, but efficiently converting the
former into the latter poses a non-trivial challenge.
4.2 Extraction of irreducible concepts
Our approach attempts to obtain a set of irre-
ducible concepts such that each concept in this set
cannot wholly or partially contain any other con-
cept in the set (thereby conforming to the defini-
tion of a concept in Subsection 3.1).
We attempt to build the concept graph and main-
tain irreducible concepts alongside each of the
S(S ? 1)/2 pairwise alignment steps. Every in-
tersection found by aligning a pair of snippets is
assumed to represent some concept that these snip-
pets share; it is then compared with existing con-
cepts and is decomposed into smaller intersections
if it overlaps partially with any one of them. This
implies a worst-case of C comparisons at each
pairwise alignment step (2C if both fragments of
an intersection are compared separately). How-
ever, this can be made more efficient by exploiting
the structure of the graph. A new intersection only
has to be compared with concepts which might be
affected by it and only affects the other snippets
containing these concepts. We can show that this
leads to an algorithm that requires fewer than C
comparisons, and additionally, that these compar-
isons can be performed efficiently.
Consider the definition of alignment along the
lines of a mathematical relation. We require snip-
pet algnment to be an equivalence relation and it
therefore must have the following properties.
Symmetry: If an intersection v
i,j
contains a
concept z
?
, then v
j,i
will also contain z
?
. This
property allows only S(S ? 1)/2 alignments to
suffice instead of the full S(S ? 1). Therefore,
without loss of generality, we can specify that all
alignments between x
i
and x
j
should have i < j.
Transitivity: If intersections v
i,j
and v
j,k
both
contain some concept z
?
, then v
i,k
will also con-
tain z
?
. This property leads to an interesting con-
sequence. Assuming we perform alignments in
order (initially aligning x
1
and x
2
and iterating
for j within each i), we observe that x
i
has been
aligned with snippets {x
1
, . . . ,x
j?1
} and, for any
i > 1, snippets {x
1
, . . . ,x
i?1
} were aligned with
all snippets {x
1
, . . . ,x
S
}. Since i < j, this im-
plies that x
i
was directly aligned with snippets
{x
1
, . . . ,x
i?1
} which in turn were each aligned
with all S snippets. Therefore, due to the prop-
erty of transitivity, all concepts contained in a
new intersection v
i,j
that also exist in the partly-
constructed graph would already be directly asso-
ciated with x
i
. Note that this does not hold for
x
j
as well, since x
j
has not been aligned with
{x
i+1
, . . . ,x
j?1
}; therefore, it may not have en-
countered all relevant concepts.
This implies that for any i and j, all concepts
that might be affected by a new intersection v
i,j
876
have already been uncovered in x
i
and thus v
i,j
only needs to be compared to these concepts.
4.3 Comparisons after alignment
For every new intersection v
i,j
produced by an
alignment between x
i
and x
j
, the algorithm com-
pares it (specifically, the fragment from x
i
) with
each existing nugget y
k
i
for each concept z
k
al-
ready seen in x
i
. Checking for the following cases
ensures that the graph structure contains only irre-
ducible concepts for all the alignments seen:
1. If v
i,j
doesn?t overlap with any current
nugget from x
i
, it becomes a new concept that
links to x
i
and x
j
. In our example, the first
intersection v
1,2
contains ?Whittington ... an
attorney? from x
1
and ?... Whittington, a
lawyer? from x
2
; this becomes a new concept
z
B
since x
1
has no other nuggets.
2. If v
i,j
overlaps completely with a nugget y
k
i
,
then x
j
must also be linked to concept z
k
. For
example, x
1
?s fragment in the second inter-
section v
1,3
is also ?Whittington ... an attor-
ney?, so x
3
must also link to z
B
.
3. If v
i,j
subsumes y
k
i
, it is split up and the non-
overlapping portion is rechecked against ex-
isting nuggets recursively. For example, x
2
?s
fragment in the intersection v
2,3
is ?... shot
Whittington, a lawyer?, part of which over-
laps with y
B
2
, so this intersection is divided
up and the part representing ?... shot Whit-
tington ...? becomes a new concept z
A
.
4. If, on the other hand, y
k
i
subsumes v
i,j
, the
concept z
k
is itself split up along with all
nuggets that it links to, utilizing the present
structure of the graph.
When comparing intersections, we can restrict the
decomposition of nuggets to prevent the creation
of overly-granular concepts. For instance, we
can filter out intersections containing only isolated
named-entities or syntactic artifacts like determin-
ers since they contain no information by them-
selves. We can also prevent verbs and their ar-
guments from being split apart using information
from a snippet?s dependency parse, if available.
4.4 Efficiency of the algorithm
Instead of C additional comparisons in the worst
case after each pairwise snippet algnment, we
need no more comparisons in the worst case than
the maximum number of concepts that can exist in
a single snippet. Since this value grows no faster
than C as S increases, this is a significant improve-
ment. Other factors, such as the overhead required
to split up concepts, remain unchanged.
Furthermore, since all the additional compar-
isons are carried out between nuggets of the same
snippet, we don?t need to perform any further
alignment among nuggets or concepts. Alignments
are expensive; each is O(n
1
n
2
) where n
1
and n
2
are the number of words in the two segments of
text being aligned (if dependency tree alignment is
used) along with an overhead for checking word
similarity. However, since we now only need to
compare text from the same snippet, the com-
parison can be performed in linear time by sim-
ply comparing spans of word indices, thereby also
eliminating the overhead for comparing words.
5 Decreasing redundancy
The concept graph can now be applied to the task
of reducing redundancy in the document by drop-
ping snippets which contain no information that is
not already present in the rest of the document.
5.1 Reduction to set cover
Every snippet x
s
in a document can be represented
as a set of concepts {z
c
: y
c
s
? Y}. Since concepts
are defined as information that is seen in more than
one snippet as per the definition in Subsection 3.1,
representing snippets as sets of concepts will over-
look any unique information present in a snippet.
Without loss of generality, we can add any such
unique information in the form of an artificial con-
cept for each snippet to Z so that snippets can be
completely represented as sets of concepts from Z.
Note that the union of snippets
?
S
s=1
x
s
equals Z.
Reducing redundancy in the document while
preserving all information requires us to identify
the most snippets whose entire informational con-
tent is covered by the rest of the snippets in the
document, thereby targeting them for removal.
Since we express informational content in con-
cepts, this problem reduces to the task of finding
the smallest group of snippets that together cover
all the concepts in the document, i.e. we need to
find the smallest subset X
?
? X such that, if X
?
contains R snippets x
?
r
, the union of these snippets
?
R
r=1
x
?
r
also equals Z. Therefore, every concept
in a snippet from X?X
?
also exists in at least one
snippet from X
?
and no concept from Z is lost.
This formulation of the problem is the classic
877
set cover problem, which seeks to find the smallest
possible group of subsets of a universe that cov-
ers all the other subsets. A more general variant
of this problem is weighted set cover in which the
subsets have weights to be maximized or costs to
be minimized. While this problem is known to
be NP-hard, there exists a straightforward local
maximization approach (Hochbaum, 1997) which
runs in polynomial time and is proven to give so-
lutions within a known bound of the optimal solu-
tion. This greedy approximation algorithm can be
adapted to our representation.
5.2 Selecting non-redundant snippets
The algorithm selects a snippet x
r
to the subset X
?
such that information content of X ? x
r
is max-
imized. In general, this implies that the snippet
with the highest degree over uncovered concepts
must be selected at each iteration. Other measures
such as snippet length, fluency, or rank in an or-
dered list can be included in a weight measure in
order to break ties and introduce a preference for
shorter, more fluent, or higher-ranked snippets.
Consider the example from Section 3. The can-
didates for selection are x
2
, x
3
and x
4
since they
contain the most uncovered concepts. If x
2
is se-
lected, its concepts z
A
, z
B
and z
E
are covered.
At this stage, x
5
contains two uncovered concepts
while x
3
and x
4
contain just one each. Thus, x
5
is selected next and its concepts z
C
and z
D
are
covered. Since no uncovered concepts remain, all
snippets which haven?t been selected are redun-
dant. This solution, which is shown in Figure 3,
selects the following text to cover all the snippets:
x
2
: Cheney shot Whittington, a lawyer.
x
5
: This happened during a quail hunt in Texas.
Other solutions are also possible depending on
the factors involved in choosing the snippet to be
selected at each iteration. For example, the algo-
rithm might choose to select x
3
first instead of x
2
,
thereby yielding the following solution:
x
3
: Whittington, an attorney, was shot in Texas.
x
4
: Whittington was shot by Cheney while hunting quail.
6 Experiments
To evaluate the effectiveness of this framework
empirically, we ran experiments over documents
containing annotations corresponding to concepts
within the document. We also defined a metric
x
1
B
x
2
ABE
x
3
ABC
x
4
ADE
x
5
CD
z
A
z
B
z
C
z
D
z
E
y
B
1
y
A
2
y
B
2
y
E
2
y
A
3
y
B
3
y
C
3
y
A
4
y
D
4
y
E
4
y
C
5
y
D
5
Figure 3: Pruned version of the concept graph ex-
ample shown in Figure 2, illustrating the outcome
of removing redundant snippets.
for comparing any concept graph over a document
to a gold-standard concept graph. This was used
to compare the concept graphs created by our ap-
proach to perturbed versions of the gold-standard
graphs and graphs created by clustering.
6.1 Dataset
Due to the scarcity of available annotated datasets
suitable for evaluating redundancy, we utilized the
pyramid dataset from DUC 2005 (Nenkova et al,
2007) which was created from 20 articles for the
purpose of summarization evaluation. Each pyra-
mid document is a hierarchical representation of 7
summaries of the orginal news article. These sum-
maries have been annotated to identify the indi-
vidual semantic content units or SCUs where each
SCU represents a certain fact, observation or piece
of information in the summary. A sentence frag-
ment representing an occurrence of an SCU in a
summary is a contributor to the SCU.
The pyramid construction for a group of sum-
maries of the same article mirrors the concept
graph representation described in Subsection 3.2.
SCUs with more than two contributors are simi-
lar in definition to concepts while their contribu-
tors fill the role of nuggets. Using this analogy,
each dataset consists of a combination of the seven
summaries in a single pyramid document; the 20
pyramid documents therefore yield 20 datasets.
6.2 Evaluation metrics
The evaluation task requires us to compare the con-
cept graph generated by our algorithm to the ideal
878
x1
x
2
x
3
x
4
x
1
x
2
x
3
x
4
Concepts
SCUs
(merge)
(split)
L
alg
L
pyr
Figure 4: The bipartite graph on the left shows
snippets x
s
linked to concepts produced automati-
cally; the one on the right shows the same snippets
linked to SCUs from annotated data. Dashed lines
indicate mappings between concepts and SCUs.
concept graph extracted from the pyramid docu-
ment annotations. Standard metrics do not ap-
ply easily to the problem of comparing bipartite
graphs, so we define a novel metric modeled on
the well-known IR measures of precision, recall
and F-measure. Figure 4 illustrates the elements
involved in the evaluation task.
We define the metrics of precision, recall and F-
measure over the links between snippets and con-
cepts. Assuming we have a mapping between gen-
erated concepts and gold-standard SCUs, we can
judge whether each link is correct. Let each single
link between a snippet and a concept have an asso-
ciated weight of 1 by default and let L indicate a
set of such links. We use L
alg
and L
pyr
to distin-
guish between the sets of links generated by the al-
gorithm and retrieved from the annotations respec-
tively. Precision and recall are defined as follows
while F-measure retains its traditional definition as
their harmonic mean.
Precision =
Sum of weights in L
alg
? L
pyr
Sum of weights in L
alg
Recall =
Sum of weights in L
alg
? L
pyr
Sum of weights in L
pyr
To determine a mapping between concepts and
SCUs, we identify every concept and SCU pair,
say z
c
and z
s
, which has one or more snippets in
common and, for each snippet x
i
that they have
in common, we find the longest common subse-
quence between their nuggets y
c
i
and y
s
i
to obtain
the following score which ranges from 0 to 1.
LCS score =
length(LCS)
min (length(y
c
i
), length(y
s
i
))
Measure Random Clustering Concepts
Precision 0.0510 0.2961 0.4496
Recall 0.0515 0.1162 0.3266
F
1
score 0.0512 0.1669 0.3783
Table 1: Summary of the evaluation metrics aver-
aged over all 20 pyramid documents when m=0.5
This score is compared with a user-defined map-
ping threshold m to determine if the concept and
SCU are sufficiently similar. In order to avoid bi-
asing the metric by permitting multiple mappings
per concept, we adjust for merges or 1 : N map-
pings by cloning the concept and creating N 1 : 1
mappings in its place. We then adjust for splits or
N : 1 mappings by dividing the weight of each of
the links connected to a participating concept by
N . Due to this normalization, the metrics are ob-
served to be stable over variations in m.
6.3 Baselines
We compare the performance of the algorithm
against two baselines. The first approach involves
a random concept assignment scheme to build arti-
ficial concept graphs using the distributional prop-
erties of the gold-standard concept graphs. The
number of concepts C and the number of snippets
that each concept links to is determined by sam-
pling from distributions over these properties de-
rived from the statistics of the actual SCU graph
for that document. For evaluation, these artificial
concepts are randomly mapped to SCUs using m
to control the likelihood of mapping. The best
scores from 100 evaluation runs were considered.
The second baseline used for comparison is a
clustering algorithm, since clustering is the most
common approach to dealing with redundancy. For
this purpose, we use a recursive spectral partition-
ing algorithm, a variant of spectral clustering (Shi
and Malik, 2000) which obtains an average V-
measure (Rosenberg and Hirschberg, 2007) of 0.93
when clustering just pyramid contributors labeled
by their SCUs. The algorithm requires a parame-
ter that controls the homogeneity of each cluster;
we run it over the entire range of settings of this
parameter. We consider the clustering that maxi-
mizes F-measure in order to avoid any uncertainty
regarding optimal parameter selection and to im-
plicitly compare our algorithm against an entire hi-
erarchy of possible clusterings.
879
6.4 Results
Table 1 shows the F
1
scores over evaluation runs
using the random concept assignment, clustering
and concept graph techniques. These results are
obtained at a mapping threshold of m = 0.5,
which implies that we consider a mapping between
a concept and an SCU if their nuggets over com-
mon sentences share more than 50% of their words
on average. The results do not vary significantly at
different settings of m.
We observe that the concepts extracted by our
graph-based approach perform significantly better
than the best-performing clustering configuration.
Despite a fairly limited alignment approach that
doesn?t use synonyms or semantic analysis, the
concept graph outperforms the baselines by nearly
an order of magnitude on each document. This
validates our initial hypothesis that clustering ap-
proaches are not suitable for tackling the redun-
dancy problem at the sub-sentential level.
7 Conclusions and Future Work
We have described a graph-based algorithm for
identifying redundancy at the sub-snippet level and
shown that it outperforms clustering methods that
are traditionally applied to the redundancy task.
Though the algorithm identifies redundancy at
the sub-snippet level, redundancy can be decreased
by dropping only entirely redundant snippets. We
hope to be able to overcome this limitation by
extending this information-preserving approach to
the synthesis of new non-redundant snippets which
minimize redundant content in the document.
In addition, this work currently assumes that re-
dundancy is bidirectional; however, we intend to
also address the case of unidirectional redundancy
by considering entailment recognition approaches.
Acknowledgements
We are grateful to Andrew Rosenberg, David El-
son, Mayank Lahiri and the anonymous review-
ers for their useful feedback. This material is
based upon work supported by the Defense Ad-
vanced Research Projects Agency under Contract
No. HR0011-06-C-0023.
References
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL, pages 16?23.
Barzilay, Regina and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?328.
Carbonell, Jaime G. and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reorder-
ing documents and producing summaries. In Pro-
ceedings of ACM-SIGIR, pages 335?336.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of COL-
ING, page 397.
Hochbaum, Dorit S. 1997. Approximating covering
and packing problems: set cover, vertex cover, in-
dependent set, and related problems. In Approxi-
mation algorithms for NP-hard problems, pages 94?
143. PWS Publishing Co., Boston, MA, USA.
Hovy, Eduard, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with basic elements. In Proceedings of LREC.
Lin, Chin-Yew and Eduard Hovy. 2001. From single
to multi-document summarization: a prototype sys-
tem and its evaluation. In Proceedings of ACL, pages
457?464.
Lin, Dekang. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems, LREC.
Marsi, Erwin, Emiel Krahmer, Wauter Bosma, and Ma-
riet Theune. 2006. Normalized alignment of depen-
dency trees for detecting textual entailment. In Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge, pages 56?61.
Nenkova, Ani, Rebecca Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorporat-
ing human content selection variation in summariza-
tion evaluation. ACM Transactions on Speech and
Language Processing, 4(2):4.
Pang, Bo, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: ex-
tracting paraphrases and generating new sentences.
In Proceedings of HLT-NAACL, pages 102?109.
Rosenberg, Andrew and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP,
pages 410?420.
Shi, Jianbo and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
Siddharthan, Advaith, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document summa-
rization. In Proceedings of COLING, page 896.
880
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 24?32,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Contextual Phrase-Level Polarity Analysis using Lexical Affect Scoring
and Syntactic N-grams
Apoorv Agarwal
Department of Computer Science
Columbia University
New York, USA
aa2644@columbia.edu
Fadi Biadsy
Department of Computer Science
Columbia University
New York, USA
fadi@cs.columbia.edu
Kathleen R. Mckeown
Department of Computer Science
Columbia University
New York, USA
kathy@cs.columbia.edu
Abstract
We present a classifier to predict con-
textual polarity of subjective phrases in
a sentence. Our approach features lexi-
cal scoring derived from the Dictionary of
Affect in Language (DAL) and extended
through WordNet, allowing us to automat-
ically score the vast majority of words in
our input avoiding the need for manual la-
beling. We augment lexical scoring with
n-gram analysis to capture the effect of
context. We combine DAL scores with
syntactic constituents and then extract n-
grams of constituents from all sentences.
We also use the polarity of all syntactic
constituents within the sentence as fea-
tures. Our results show significant im-
provement over a majority class baseline
as well as a more difficult baseline consist-
ing of lexical n-grams.
1 Introduction
Sentiment analysis is a much-researched area that
deals with identification of positive, negative and
neutral opinions in text. The task has evolved from
document level analysis to sentence and phrasal
level analysis. Whereas the former is suitable for
classifying news (e.g., editorials vs. reports) into
positive and negative, the latter is essential for
question-answering and recommendation systems.
A recommendation system, for example, must be
able to recommend restaurants (or movies, books,
etc.) based on a variety of features such as food,
service or ambience. Any single review sentence
may contain both positive and negative opinions,
evaluating different features of a restaurant. Con-
sider the following sentence (1) where the writer
expresses opposing sentiments towards food and
service of a restaurant. In tasks such as this, there-
fore, it is important that sentiment analysis be done
at the phrase level.
(1) The Taj has great food but I found their ser-
vice to be lacking.
Subjective phrases in a sentence are carriers of
sentiments in which an experiencer expresses an
attitude, often towards a target. These subjective
phrases may express neutral or polar attitudes de-
pending on the context of the sentence in which
they appear. Context is mainly determined by con-
tent and structure of the sentence. For example, in
the following sentence (2), the underlined subjec-
tive phrase seems to be negative, but in the larger
context of the sentence, it is positive.1
(2) The robber entered the store but his efforts
were crushed when the police arrived on time.
Our task is to predict contextual polarity of sub-
jective phrases in a sentence. A traditional ap-
proach to this problem is to use a prior polarity
lexicon of words to first set priors on target phrases
and then make use of the syntactic and semantic
information in and around the sentence to make
the final prediction. As in earlier approaches, we
also use a lexicon to set priors, but we explore
new uses of a Dictionary of Affect in Language
(DAL) (Whissel, 1989) extended using WordNet
(Fellbaum, 1998). We augment this approach with
n-gram analysis to capture the effect of context.
We present a system for classification of neutral
versus positive versus negative and positive versus
negative polarity (as is also done by (Wilson et al,
2005)). Our approach is novel in the use of fol-
lowing features:
? Lexical scores derived from DAL and ex-
tended through WordNet: The Dictionary
of Affect has been widely used to aid in in-
terpretation of emotion in speech (Hirschberg
1We assign polarity to phrases based on Wiebe (Wiebe et
al., 2005); the polarity of all examples shown here is drawn
from annnotations in the MPQA corpus. Clearly the assign-
ment of polarity chosen in this corpus depends on general
cultural norms.
24
et al, 2005). It contains numeric scores as-
signed along axes of pleasantness, activeness
and concreteness. We introduce a method for
setting numerical priors on words using these
three axes, which we refer to as a ?scoring
scheme? throughout the paper. This scheme
has high coverage of the phrases for classi-
fication and requires no manual intervention
when tagging words with prior polarities.
? N-gram Analysis: exploiting automatically
derived polarity of syntactic constituents
We compute polarity for each syntactic con-
stituent in the input phrase using lexical af-
fect scores for its words and extract n-grams
over these constituents. N-grams of syntactic
constituents tagged with polarity provide pat-
terns that improve prediction of polarity for
the subjective phrase.
? Polarity of Surrounding Constituents: We
use the computed polarity of syntactic con-
stituents surrounding the phrase we want to
classify. These features help to capture the
effect of context on the polarity of the sub-
jective phrase.
We show that classification of subjective
phrases using our approach yields better accuracy
than two baselines, a majority class baseline and a
more difficult baseline of lexical n-gram features.
We also provide an analysis of how the differ-
ent component DAL scores contribute to our re-
sults through the introduction of a ?norm? that
combines the component scores, separating polar
words that are less subjective (e.g., Christmas ,
murder) from neutral words that are more subjec-
tive (e.g., most, lack).
Section 2 presents an overview of previous
work, focusing on phrasal level sentiment analy-
sis. Section 3 describes the corpus and the gold
standard we used for our experiments. In sec-
tion 4, we give a brief description of DAL, dis-
cussing its utility and previous uses for emotion
and for sentiment analysis. Section 5 presents, in
detail, our polarity classification framework. Here
we describe our scoring scheme and the features
we extract from sentences for classification tasks.
Experimental set-up and results are presented in
Section 6. We conclude with Section 7 where we
also look at future directions for this research.
2 Literature Survey
The task of sentiment analysis has evolved from
document level analysis (e.g., (Turney., 2002);
(Pang and Lee, 2004)) to sentence level analy-
sis (e.g., (Hu and Liu., 2004); (Kim and Hovy.,
2004); (Yu and Hatzivassiloglou, 2003)). These
researchers first set priors on words using a prior
polarity lexicon. When classifying sentiment at
the sentence level, other types of clues are also
used, including averaging of word polarities or
models for learning sentence sentiment.
Research on contextual phrasal level sentiment
analysis was pioneered by Nasukawa and Yi
(2003), who used manually developed patterns to
identify sentiment. Their approach had high preci-
sion, but low recall. Wilson et al, (2005) also ex-
plore contextual phrasal level sentiment analysis,
using a machine learning approach that is closer to
the one we present. Both of these researchers also
follow the traditional approach and first set priors
on words using a prior polarity lexicon. Wilson
et al (2005) use a lexicon of over 8000 subjec-
tivity clues, gathered from three sources ((Riloff
and Wiebe, 2003); (Hatzivassiloglou and McKe-
own, 1997) and The General Inquirer2). Words
that were not tagged as positive or negative were
manually labeled. Yi et al (2003) acquired words
from GI, DAL and WordNet. From DAL, only
words whose pleasantness score is one standard
deviation away from the mean were used. Na-
sukawa as well as other researchers (Kamps and
Marx, 2002)) also manually tag words with prior
polarities. All of these researchers use categorical
tags for prior lexical polarity; in contrast, we use
quantitative scores, making it possible to use them
in computation of scores for the full phrase.
While Wilson et al (2005) aim at phrasal level
analysis, their system actually only gives ?each
clue instance its own label? [p. 350]. Their gold
standard is also at the clue level and assigns a
value based on the clue?s appearance in different
expressions (e.g., if a clue appears in a mixture of
negative and neutral expressions, its class is neg-
ative). They note that they do not determine sub-
jective expression boundaries and for this reason,
they classify at the word level. This approach is
quite different from ours, as we compute the po-
larity of the full phrase. The average length of
the subjective phrases in the corpus was 2.7 words,
with a standard deviation of 2.3. Like Wilson et al
2http://www.wjh.harvard.edu/ inquirer
25
(2005) we do not attempt to determine the bound-
ary of subjective expressions; we use the labeled
boundaries in the corpus.
3 Corpus
We used the Multi-Perspective Question-
Answering (MPQA version 1.2) Opinion corpus
(Wiebe et al, 2005) for our experiments. We
extracted a total of 17,243 subjective phrases
annotated for contextual polarity from the corpus
of 535 documents (11,114 sentences). These
subjective phrases are either ?direct subjective?
or ?expressive subjective?. ?Direct subjective?
expressions are explicit mentions of a private state
(Quirk et al, 1985) and are much easier to clas-
sify. ?Expressive subjective? phrases are indirect
or implicit mentions of private states and therefore
are harder to classify. Approximately one third of
the phrases we extracted were direct subjective
with non-neutral expressive intensity whereas the
rest of the phrases were expressive subjective. In
terms of polarity, there were 2779 positive, 6471
negative and 7993 neutral expressions. Our Gold
Standard is the manual annotation tag given to
phrases in the corpus.
4 DAL
DAL is an English language dictionary built to
measure emotional meaning of texts. The samples
employed to build the dictionary were gathered
from different sources such as interviews, adoles-
cents? descriptions of their emotions and univer-
sity students? essays. Thus, the 8742 word dictio-
nary is broad and avoids bias from any one par-
ticular source. Each word is given three kinds of
scores (pleasantness ? also called evaluation, ee,
activeness, aa and imagery, ii) on a scale of 1 (low)
to 3 (high). Pleasantness is a measure of polarity.
For example, in Table 1, affection is given a pleas-
antness score of 2.77 which is closer to 3.0 and
is thus a highly positive word. Likewise, active-
ness is a measure of the activation or arousal level
of a word, which is apparent from the activeness
scores of slug and energetic in the table. The third
score, imagery, is a measure of the ease with which
a word forms a mental picture. For example, af-
fect cannot be imagined easily and therefore has a
score closer to 1, as opposed to flower which is a
very concrete and therefore has an imagery score
of 3.
A notable feature of the dictionary is that it has
different scores for various inflectional forms of a
word ( affect and affection) and thus, morphologi-
cal parsing, and the possibility of resulting errors,
is avoided. Moreover, Cowie et al, (2001) showed
that the three scores are uncorrelated; this implies
that each of the three scores provide complemen-
tary information.
Word ee aa ii
Affect 1.75 1.85 1.60
Affection 2.77 2.25 2.00
Slug 1.00 1.18 2.40
Energetic 2.25 3.00 3.00
Flower 2.75 1.07 3.00
Table 1: DAL scores for words
The dictionary has previously been used for de-
tecting deceptive speech (Hirschberg et al, 2005)
and recognizing emotion in speech (Athanaselis et
al., 2006).
5 The Polarity Classification Framework
In this section, we present our polarity classifi-
cation framework. The system takes a sentence
marked with a subjective phrase and identifies the
most likely contextual polarity of this phrase. We
use a logistic regression classifier, implemented
in Weka, to perform two types of classification:
Three way (positive, negative, vs. neutral) and
binary (positive vs. negative). The features we
use for classification can be broadly divided into
three categories: I. Prior polarity features com-
puted from DAL and augmented using WordNet
(Section 5.1). II. lexical features including POS
and word n-gram features (Section 5.3), and III.
the combination of DAL scores and syntactic fea-
tures to allow both n-gram analysis and polarity
features of neighbors (Section 5.4).
5.1 Scoring based on DAL and WordNet
DAL is used to assign three prior polarity scores
to each word in a sentence. If a word is found in
DAL, scores of pleasantness (ee), activeness (aa),
and imagery (ii) are assigned to it. Otherwise, a
list of the word?s synonyms and antonyms is cre-
ated using WordNet. This list is sequentially tra-
versed until a match is found in DAL or the list
ends, in which case no scores are assigned. For
example, astounded, a word absent in DAL, was
scored by using its synonym amazed. Similarly,
in-humane was scored using the reverse polarity of
26
its antonym humane, present in DAL. These scores
are Z-Normalized using the mean and standard de-
viation measures given in the dictionary?s manual
(Whissel, 1989). It should be noted that in our cur-
rent implementation all function words are given
zero scores since they typically do not demonstrate
any polarity. The next step is to boost these nor-
malized scores depending on how far they lie from
the mean. The reason for doing this is to be able
to differentiate between phrases like ?fairly decent
advice? and ?excellent advice?. Without boosting,
the pleasantness scores of both phrases are almost
the same. To boost the score, we multiply it by
the number of standard deviations it lies from the
mean.
After the assignment of scores to individual
words, we handle local negations in a sentence by
using a simple finite state machine with two states:
RETAIN and INVERT. In the INVERT state, the
sign of the pleasantness score of the current word
is inverted, while in the RETAIN state the sign of
the score stays the same. Initially, the first word in
a given sentence is fed to the RETAIN state. When
a negation (e.g., not, no, never, cannot, didn?t)
is encountered, the state changes to the INVERT
state. While in the INVERT state, if ?but? is en-
countered, it switches back to the RETAIN state.
In this machine we also take care of ?not only?
which serves as an intensifier rather than nega-
tion (Wilson et al, 2005). To handle phrases like
?no better than evil? and ?could not be clearer?,
we also switch states from INVERT to RETAIN
when a comparative degree adjective is found after
?not?. For example, the words in phrase in Table
(2) are given positive pleasantness scores labeled
with positive prior polarity.
Phrase has no greater desire
POS VBZ DT JJR NN
(ee) 0 0 3.37 0.68
State RETAIN INVERT RETAIN RETAIN
Table 2: Example of scoring scheme using DAL
We observed that roughly 74% of the content
words in the corpus were directly found in DAL.
Synonyms of around 22% of the words in the cor-
pus were found to exist in DAL. Antonyms of
only 1% of the words in the corpus were found in
DAL. Our system failed to find prior semantic ori-
entations of roughly 3% of the total words in the
corpus. These were rarely occurring words like
apartheid, apocalyptic and ulterior. We assigned
zero scores for these words.
In our system, we assign three DAL scores, us-
ing the above scheme, for the subjective phrase
in a given sentence. The features are (1) ?ee, the
mean of the pleasantness scores of the words in the
phrase, (2) ?aa, the mean of the activeness scores
of the words in the phrase, and similarly (3) ?ii,
the mean of the imagery scores.
5.2 Norm
We gave each phrase another score, which we call
the norm, that is a combination of the three scores
from DAL. Cowie et al (2001) suggest a mecha-
nism of mapping emotional states to a 2-D contin-
uous space using an Activation-Evaluation space
(AE) representation. This representation makes
use of the pleasantness and activeness scores from
DAL and divides the space into four quadrants:
?delightful?, ?angry?, ?serene?, and ?depressed?.
Whissel (2008), observes that tragedies, which
are easily imaginable in general, have higher im-
agery scores than comedies. Drawing on these ap-
proaches and our intuition that neutral expressions
tend to be more subjective, we define the norm in
the following equation (1).
norm =
?
ee2 + aa2
ii
(1)
Words of interest to us may fall into the follow-
ing four broad categories:
1. High AE score and high imagery: These
are words that are highly polar and less sub-
jective (e.g., angel and lively).
2. Low AE score and low imagery: These are
highly subjective neutral words (e.g., gener-
ally and ordinary).
3. High AE score and low imagery: These are
words that are both highly polar and subjec-
tive (e.g., succeed and good).
4. Low AE score and high imagery: These are
words that are neutral and easily imaginable
(e.g., car and door).
It is important to differentiate between these
categories of words, because highly subjective
words may change orientation depending on con-
text; less subjective words tend to retain their prior
orientation. For instance, in the example sentence
from Wilson et al(2005)., the underlined phrase
27
seems negative, but in the context it is positive.
Since a subjective word like succeed depends on
?what? one succeeds in, it may change its polar-
ity accordingly. In contrast, less subjective words,
like angel, do not depend on the context in which
they are used; they evoke the same connotation as
their prior polarity.
(3) They haven?t succeeded and will never succeed
in breaking the will of this valiant people.
As another example, AE space scores of good-
ies and good turn out to be the same. What differ-
entiates one from the another is the imagery score,
which is higher for the former. Therefore, value of
the norm is lower for goodies than for good. Un-
surprisingly, this feature always appears in the top
10 features when the classification task contains
neutral expressions as one of the classes.
5.3 Lexical Features
We extract two types of lexical features, part of
speech (POS) tags and n-gram word features. We
count the number of occurrences of each POS in
the subjective phrase and represent each POS as
an integer in our feature vector.3 For each subjec-
tive phrase, we also extract a subset of unigram,
bigrams, and trigrams of words (selected automat-
ically, see Section 6). We represent each n-gram
feature as a binary feature. These types of features
were used to approximate standard n-gram lan-
guage modeling (LM). In fact, we did experiment
with a standard trigram LM, but found that it did
not improve performance. In particular, we trained
two LMs, one on the polar subjective phrases and
another on the neutral subjective phrases. Given a
sentence, we computed two perplexities of the two
LMs on the subjective phrase in the sentence and
added them as features in our feature vectors. This
procedure provided us with significant improve-
ment over a chance baseline but did not outper-
form our current system. We speculate that this
was caused by the split of training data into two
parts, one for training the LMs and another for
training the classifier. The resulting small quantity
of training data may be the reason for bad perfor-
mance. Therefore, we decided to back off to only
binary n-gram features as part of our feature vec-
tor.
3We use the Stanford Tagger to assign parts of speech tags
to sentences. (Toutanova and Manning, 2000)
5.4 Syntactic Features
In this section, we show how we can combine the
DAL scores with syntactic constituents. This pro-
cess involves two steps. First, we chunk each
sentence to its syntactic constituents (NP, VP,
PP, JJP, and Other) using a CRF Chunker.4 If
the marked-up subjective phrase does not contain
complete chunks (i.e., it partially overlaps with
other chunks), we expand the subjective phrase to
include the chunks that it overlaps with. We term
this expanded phrase as the target phrase, see Fig-
ure 1.
Second, each chunk in a sentence is then as-
signed a 2-D AE space score as defined by Cowie
et al, (2001) by adding the individual AE space
scores of all the words in the chunk and then nor-
malizing it by the number of words. At this point,
we are only concerned with the polarity of the
chunk (i.e., whether it is positive or negative or
neutral) and imagery will not help in this task; the
AE space score is determined from pleasantness
and activeness alone. A threshold, determined
empirically by analyzing the distributions of posi-
tive (pos), negative (neg) and neutral (neu) expres-
sions, is used to define ranges for these classes of
expressions. This enables us to assign each chunk
a prior semantic polarity. Having the semantic ori-
entation (positive, negative, neutral) and phrasal
tags, the sentence is then converted to a sequence
of encodings [Phrasal ? Tag]polarity. We mark
each phrase that we want to classify as a ?target? to
differentiate it from the other chunks and attach its
encoding. As mentioned, if the target phrase par-
tially overlaps with chunks, it is simply expanded
to subsume the chunks. This encoding is illus-
trated in Figure 1.
After these two steps, we extract a set of fea-
tures that are used in classifying the target phrase.
These include n-grams of chunks from the all
sentences, minimum and maximum pleasantness
scores from the chunks in the target phrase itself,
and the syntactic categories that occur in the con-
text of the target phrase. In the remainder of this
section, we describe how these features are ex-
tracted.
We extract unigrams, bigrams and trigrams of
chunks from all the sentences. For example, we
may extract a bigram from Figure 1 of [V P ]neu
followed by [PP ]targetneg . Similar to the lexical
4Xuan-Hieu Phan, ?CRFChunker: CRF English Phrase
Chunker?, http://crfchunker.sourceforge.net/, 2006.
28
?? ??????? ?? ?? ?? ?? ??
?????????
????????????????????? ?????????????????????
? ? ? ? ? ?
Figure 1: Converting a sentence with a subjective phrase to a sequence of chunks with their types and polarities
n-grams, for the sentence containing the target
phrase, we add binary values in our feature vec-
tor such that the value is 1 if the sentence contains
that chunk n-gram.
We also include two features related to the tar-
get phrase. The target phrase often consists of
many chunks. To detect if a chunk of the target
phrase is highly polar, minimum and maximum
pleasantness scores over all the chunks in the tar-
get phrase are noted.
In addition, we add features which attempt to
capture contextual information using the prior se-
mantic polarity assigned to each chunk both within
the target phrase itself and within the context of the
target phrase. In cases where the target phrase is
in the beginning of the sentence or at the end, we
simply assign zero scores. Then we compute the
frequency of each syntactic type (i.e., NP, VP, PP,
JJP) and polarity (i.e., positive, negative, neutral)
to the left of the target, to the right of the target
and for the target. This additional set of contextual
features yields 36 features in total: three polari-
ties: {positive, negative, neutral} * three contexts:
{left, target, right} * four chunk syntactic types:
{NP, VP, PP, JJP}.
The full set of features captures different types
of information. N-grams look for certain patterns
that may be specific to either polar or neutral senti-
ments. Minimum and maximum scores capture in-
formation about the target phrase standalone. The
last set of features incorporate information about
the neighbors of the target phrase. We performed
feature selection on this full set of n-gram related
features and thus, a small subset of these n-gram
related features, selected automatically (see sec-
tion 6) were used in the experiments.
6 Experiments and Results
Subjective phrases from the MPQA corpus were
used in 10-fold cross-validation experiments. The
MPQA corpus includes gold standard tags for each
Feature Types Accuracy Pos.* Neg.* Neu.*
Chance baseline 33.33% - - -
N-gram baseline 59.05% 0.602 0.578 0.592
DAL scores only 59.66% 0.635 0.635 0.539
+ POS 60.55% 0.621 0.542 0.655
+ Chunks 64.72% 0.681 0.665 0.596
+ N-gram (all) 67.51% 0.703 0.688 0.632
All (unbalanced) 70.76% 0.582 0.716 0.739
Table 3: Results of 3 way classification (Positive, Negative,
and Neutral). In the unbalanced case, majority class baseline
is 46.3% (*F-Measure).
Feature Types Accuracy Pos.* Neg.*
Chance baseline 50% - -
N-gram baseline 73.21% 0.736 0.728
DAL scores only 77.02% 0.763 0.728
+ POS 79.02% 0.788 0.792
+ Chunks 80.72% 0.807 0.807
+ N-gram (all) 82.32% 0.802 0.823
All (unbalanced) 84.08% 0.716 0.889
Table 4: Positive vs. Negative classification results. Baseline
is the majority class. In the unbalanced case, majority class
baseline is 69.74%. (* F-Measure)
phrase. A logistic classifier was used for two po-
larity classification tasks, positive versus negative
versus neutral and positive versus negative. We
report accuracy, and F-measure for both balanced
and unbalanced data.
6.1 Positive versus Negative versus Neutral
Table 3 shows results for a 3-way classifier. For
the balanced data-set, each class has 2799 in-
stances and hence the chance baseline is 33%. For
the unbalanced data-set, there are 2799 instances
of positive, 6471 instances of negative and 7993
instances of neutral phrases and thus the baseline
is about 46%. Results show that the accuracy in-
creases as more features are added. It may be
seen from the table that prior polarity scores do
not do well alone, but when used in conjunction
with other features they play an important role
in achieving an accuracy much higher than both
baselines (chance and lexical n-grams). To re-
29
Figure 2: (a) An example sentence with three annotated subjective phrases in the same sentence. (b) Part of the sentence with
the target phrase (B) and their chunks with prior polarities.
confirm if prior polarity scores add value, we ex-
perimented by using all features except the prior
polarity scores and noticed a drop in accuracy by
about 4%. This was found to be true for the
other classification task as well. The table shows
that parts of speech and lexical n-grams are good
features. A significant improvement in accuracy
(over 4%, p-value = 4.2e-15) is observed when
chunk features (i.e., n-grams of constituents and
polarity of neighboring constituents) are used in
conjunction with prior polarity scores and part of
speech features.5 This improvement may be ex-
plained by the following observation. The bi-
gram ?[Other]targetneu [NP ]neu? was selected as a
top feature by the Chi-square feature selector. So
were unigrams, [Other]targetneu and [Other]
target
neg .
We thus learned n-gram patterns that are char-
acteristic of neutral expressions (the just men-
tioned bigram and the first of the unigrams) as
well as a pattern found mostly in negative ex-
pressions (the latter unigram). It was surpris-
ing to find another top chunk feature, the bigram
?[Other]targetneu [NP ]neg? (i.e., a neutral chunk of
syntactic type ?Other? preceding a negative noun
phrase), present in neutral expressions six times
more than in polar expressions. An instance where
these chunk features could have been responsi-
ble for the correct prediction of a target phrase is
shown in Figure 2. Figure 2(a) shows an exam-
ple sentence from the MPQA corpus, which has
three annotated subjective phrases. The manually
labeled polarity of phrases (A) and (C) is negative
and that of (B) is neutral. Figure 2(b) shows the
5We use the binomial test procedure to test statistical sig-
nificance throughout the paper.
relevant chunk bigram which is used to predict the
contextual polarity of the target phrase (B).
It was interesting to see that the top 10 features
consisted of all categories (i.e., prior DAL scores,
lexical n-grams and POS, and syntactic) of fea-
tures. In this and the other experiment, pleasant-
ness, activation and the norm were among the top
5 features. We ran a significance test to show the
importance of the norm feature in our classifica-
tion task and observed that it exerted a significant
increase in accuracy (2.26%, p-value = 1.45e-5).
6.2 Positive versus Negative
Table 4 shows results for positive versus negative
classification. We show results for both balanced
and unbalanced data-sets. For balanced, there are
2779 instances of each class. For the unbalanced
data-set, there are 2779 instances of positive and
6471 instances of neutral, thus our chance base-
line is around 70%. As in the earlier classification,
accuracy and F-measure increase as we add fea-
tures. While the increase of adding the chunk fea-
tures, for example, is not as great as in the previous
classification, it is nonetheless significant (p-value
= 0.0018) in this classification task. The smaller
increase lends support to our hypothesis that po-
lar expressions tend to be less subjective and thus
are less likely to be affected by contextual polar-
ity. Another thing that supports our hypothesis that
neutral expressions are more subjective is the fact
that the rank of imagery (ii), dropped significantly
in this classification task as compared to the previ-
ous classification task. This implies that imagery
has a much lesser role to play when we are dealing
with non-neutral expressions.
30
7 Conclusion and Future Work
We present new features (DAL scores, norm
scores computed using DAL, n-gram over chunks
with polarity) for phrasal level sentiment analysis.
They work well and help in achieving high accu-
racy in a three-way classification of positive, neg-
ative and neutral expressions. We do not require
any manual intervention during feature selection,
and thus our system is fully automated. We also
introduced a 3-D representation that maps differ-
ent classes to spatial coordinates.
It may seem to be a limitation of our system that
it requires accurate expression boundaries. How-
ever, this is not true for the following two reasons:
first, Wiebe et al, (2005) declare that while mark-
ing the span of subjective expressions and hand
annotating the MPQA corpus, the annotators were
not trained to mark accurate expression bound-
aries. The only constraint was that the subjective
expression should be within the mark-ups for all
annotators. Second, we expanded the marked sub-
jective phrase to subsume neighboring phrases at
the time of chunking.
A limitation of our scoring scheme is that it
does not handle polysemy, since words in DAL
are not provided with their parts of speech. Statis-
tics show, however, that most words occurred with
primarily one part of speech only. For example,
?will? occurred as modal 1272 times in the corpus,
whereas it appeared 34 times as a noun. The case
is similar for ?like? and ?just?, which mostly occur
as a preposition and an adverb, respectively. Also,
in our state machine, we haven?t accounted for the
impact of connectives such as ?but? or ?although?;
we propose drawing on work in argumentative ori-
entation to do so ((Anscombre and Ducrot, 1983);
(Elhadad and McKeown, 1990)).
For future work, it would be interesting to do
subjectivity and intensity classification using the
same scheme and features. Particularly, for the
task of subjectivity analysis, we speculate that the
imagery score might be useful for tagging chunks
with ?subjective? and ?objective? instead of posi-
tive, negative, and neutral.
Acknowledgments
This work was supported by the National Science
Foundation under the KDD program. Any opin-
ions, ndings, and conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reect the views of the National
Science Foundation. score.
We would like to thank Julia Hirschberg for use-
ful discussion. We would also like to acknowledge
Narayanan Venkiteswaran for implementing parts
of the system and Amal El Masri, Ashleigh White
and Oliver Elliot for their useful comments.
References
J.C. Anscombre and O. Ducrot. 1983. Philosophie et
langage. l?argumentation clans la langue. Bruxelles:
Pierre Mardaga.
T. Athanaselis, S. Bakamidis, , and L. Dologlou. 2006.
Automatic recognition of emotionally coloured
speech. In Proceedings of World Academy of Sci-
ence, Engineering and Technology, volume 12, ISSN
1307-6884.
R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Vot-
sis, S. Kollias, and W. Fellenz et al 2001. Emo-
tion recognition in human-computer interaction. In
IEEE Signal Processing Magazine, 1, 32-80.
M. Elhadad and K. R. McKeown. 1990. Generating
connectives. In Proceedings of the 13th conference
on Computational linguistics, pages 97?101, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. In MIT press.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of ACL.
J. Hirschberg, S. Benus, J.M. Brenier, F. Enos, and
S. Friedman. 2005. Distinguishing deceptive from
non-deceptive speech. In Proceedings of Inter-
speech, 1833-1836.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of KDD.
J. Kamps and M. Marx. 2002. Words with attitude. In
1st International WordNet Conference.
S. M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In In Coling.
T. Nasukawa and J. Yi. 2003. Sentiment analysis:
Capturing favorability using natural language pro-
cessing. In Proceedings of K-CAP.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of ACL.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A comprehensive grammar of the english lan-
guage. Longman, New York.
31
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
EMNLP.
K. Toutanova and C. D. Manning. 2000. Enriching
the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pp. 63-70.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of ACL.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors,
Emotion: theory research and experience, volume 4,
Acad. Press., London.
C. M. Whissell. 2008. A psychological investiga-
tion of the use of shakespeare=s emotional language:
The case of his roman tragedies. In Edwin Mellen
Press., Lewiston, NY.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
In Language Resources and Evaluation, volume 39,
issue 2-3, pp. 165-210.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recog-
nizing contextual polarity in phrase level sentiment
analysis. In Proceedings of ACL.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In Proceedings of IEEE ICDM.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP.
32
Sentence Ordering in Multidocument Summarization
Regina Barzilay
Computer Science
Department
1214 Amsterdam Ave
New York, 10027, NY, USA
regina@cs.columbia.edu
Noemie Elhadad
Computer Science
Department
1214 Amsterdam Ave
New York, 10027, NY, USA
noemie@cs.columbia.edu
Kathleen R. McKeown
Computer Science
Department
1214 Amsterdam Ave
New York, 10027, NY, USA
kathy@cs.columbia.edu
ABSTRACT
The problem of organizing information for multidocument
summarization so that the generated summary is coherent
has received relatively little attention. In this paper, we
describe two naive ordering techniques and show that they
do not perform well. We present an integrated strategy for
ordering information, combining constraints from chronolog-
ical order of events and cohesion. This strategy was derived
from empirical observations based on experiments asking hu-
mans to order information. Evaluation of our augmented
algorithm shows a signicant improvement of the ordering
over the two naive techniques we used as baseline.
1. INTRODUCTION
Multidocument summarization poses a number of new
challenges over single document summarization. Researchers
have already investigated issues such as identifying repeti-
tions or contradictions across input documents and deter-
mining which information is salient enough to include in the
summary [1, 3, 6, 11, 15, 19]. One issue that has received
little attention is how to organize the selected information
so that the output summary is coherent. Once all the rel-
evant pieces of information have been selected across the
input documents, the summarizer has to decide in which
order to present them so that the whole text makes sense.
In single document summarization, one possible ordering of
the extracted information is provided by the input docu-
ment itself. However, [10] observed that, in single document
summaries written by professional summarizers, extracted
sentences do not retain their precedence orders in the sum-
mary. Moreover, in the case of multiple input documents,
this does not provide a useful solution: information may
be drawn from dierent documents and therefore, no one
document can provide an ordering. Furthermore, the order
between two pieces of information can change signicantly
from one document to another.
We investigate constraints on ordering in the context of
multidocument summarization. We rst describe two naive
ordering algorithms, used in several systems and show that
they do not yield satisfactory results. The rst, Majority
Ordering, is critically linked to the level of similarity of the
information ordering across the input texts. But many times
input texts have dierent structure, and therefore, this al-
gorithm is not acceptable. The second, Chronological Or-
dering, can produce good results when the information is
event-based and can, therefore, be ordered based on tempo-
ral occurence. However, texts do not always refer to events.
We have conducted experiments to identify additional con-
straints using a manually built collection of multiple order-
ings of texts. These experiments show that cohesion as an
important constraint. While it is recognized in the gener-
ation community that cohesion is a necessary feature for a
generated text, we provide an operational way to automati-
cally ensure cohesion when ordering sentences in an output
summary. We augment the Chronological Ordering algo-
rithm with a cohesion constraint, and compare it to the
naive algorithms.
Our framework is the MultiGen system [15], a domain in-
dependent multidocument summarizer which has been trained
and tested on news articles. In the following sections, we
rst give an overview of MultiGen. We then describe the
two naive ordering algorithms and evaluate them. We follow
this with a study of multiple orderings produced by humans.
This allows us to determine how to improve the Chronologi-
cal Ordering algorithm using cohesion as an additional con-
straint. The last section describes the augmented algorithm
along with its evaluation.
2. MULTIGEN OVERVIEW
MultiGen operates on a set of news articles describing
the same event. It creates a summary which synthesizes
common information across documents. In the case of mul-
tidocument summarization of articles about the same event,
source articles can contain both repetitions and contradic-
tions. Extracting all the similar sentences would produce a
verbose and repetitive summary, while extracting only some
of the similar sentences would produce a summary biased
towards some sources. MultiGen uses a comparison of ex-
tracted similar sentences to select the appropriate phrases
to include in the summary and reformulates them as a new
text.
MultiGen consists of an analysis and a generation compo-
nent. The analysis component [7] identies units of text
which convey similar information across the input docu-
ments using statistical techniques and shallow text analy-
sis. Once similar text units are identied, we cluster them
into themes. Themes are sets of sentences from dierent
documents that contain repeated information and do not
necessarily contain sentences from all the documents. For
each theme, the generation component [1] identies phrases
which are in the intersection of the theme sentences, and
selects them as part of the summary. The intersection sen-
tences are then ordered to produce a coherent text.
3. NAIVE ORDERING ALGORITHMS ARE
NOT SUFFICIENT
When producing a summary, any multidocument summa-
rization system has to choose in which order to present the
output sentences. In this section, we describe two algorithms
for ordering sentences suitable for domain independent mul-
tidocument summarization. The rst algorithm, Majority
Ordering (MO), relies only on the original orders of sen-
tences in the input documents. It is the rst solution one can
think of when addressing the ordering problem. The second
one, Chronological Ordering (CO) uses time related features
to order sentences. We analyze this strategy because it was
originally implemented in MultiGen and followed by other
summarization systems [18]. In the MultiGen framework,
ordering sentences is equivalent to ordering themes and we
describe the algorithms in terms of themes, but the con-
cepts can be adapted to other summarization systems such
as [3]. Our evaluation shows that these methods alone do
not provide an adequate strategy for ordering.
3.1 Majority Ordering
3.1.1 The Algorithm
Typically, in single document summarization, the order
of sentences in the output summary is determined by their
order in the input text. This strategy can be adapted to
multidocument summarization. Consider two themes, Th
1
and Th
2
; if sentences from Th
1
preceed sentences from Th
2
in all input texts, then presenting Th
1
before Th
2
is an ac-
ceptable order. But, when the order between sentences from
Th
1
and Th
2
varies from one text to another, this strategy
is not valid anymore. One way to dene the order between
Th
1
and Th
2
is to adopt the order occuring in the majority
of the texts where Th
1
and Th
2
occur. This strategy denes
a pairwise order between themes. However, this pairwise re-
lation is not transitive; for example, given the themes Th
1
and Th
2
occuring in a text, Th
2
and Th
3
occuring in another
text, and Th
3
and Th
1
occuring in a third text, there is a
conict between the orders (Th
1
; Th
2
; Th
3
) and (Th
3
; Th
1
).
Since transitivity is a necessary condition for a relation to be
called an order, this relation does not form a global order.
We, therefore, have to expand this pairwise relation to
a global order. In other words, we have to nd a linear
order between themes which maximizes the agreement be-
tween the orderings imposed by the input texts. For each
pair of themes, Th
i
and Th
j
, we keep two counts, C
i;j
and
C
j;i
| C
i;j
is the number of input texts in which sentences
from Th
i
occur before sentences from Th
j
and C
j;i
is the
same for the opposite order. The weight of a linear order
(Th
i
1
; : : : ; Th
i
k
) is dened as the sum of the counts for every
pair C
i
l
;i
m
, such that i
l
 i
m
and l; m 2 f1 : : : kg. Stating
this problem in terms of a directed graph where nodes are
themes, and a vertex from Th
i
to Th
j
has for weight C
i;j
,
we are looking for a path with maximal weight which tra-
verses each node exactly once. Unfortunately this problem
is NP-complete; this can be shown by reducing the travel-
ing salesman problem to this problem. Despite this fact, we
still can apply this ordering, because typically the length of
the output summary is limited to a small number of sen-
tences. For longer summaries, the approximation algorithm
described in [4] can be applied. Figures 1 and 2 show ex-
amples of produced summaries.
The main problem with this strategy is that it can pro-
duce several orderings with the same weight. This happens
when there is a tie between two opposite orderings. In this
situation, this strategy does not provide enough constraints
to determine one optimal ordering; one order is chosen ran-
domly among the orders with maximal weight.
The man accused of rebombing two Manhattan subways
in 1994 was convicted Thursday after the jury rejected the
notion that the drug Prozac led him to commit the crimes.
He was found guilty of two counts of attempted murder,
14 counts of rst-degree assault and two counts of criminal
possession of a weapon.
In December 1994, Leary ignited rebombs on two Manhat-
tan subway trains. The second blast injured 50 people { 16
seriously, including Leary.
Leary wanted to extort money from the Transit Authority.
The defense argued that Leary was not responsible for his
actions because of "toxic psychosis" caused by the Prozac.
Figure 1: A summary produced using the Majority Or-
dering algorithm, graded as Good.
A man armed with a handgun has surrendered to Spanish
authorities, peacefully ending a hijacking of a Moroccan jet.
O?cials in Spain say a person commandeered the plane.
After the plane was directed to Spain, the hijacker said he
wanted to be taken to Germany.
After several hours of negotiations, authorities convinced
the person to surrender early today.
Police said the man had a pistol, but a Moroccan security
source in Rabat said the gun was likely a \toy".
There were no reported injuries.
O?cials in Spain say the Boeing 737 left Casablanca, Mo-
rocco, Wednesday night with 83 passengers and a nine- per-
son crew headed for Tunis, Tunisia.
Spanish authorities directed the plane to an isolated section
of El Prat Airport and o?cials began negotiations.
Figure 2: A summary produced using the Majority Or-
dering algorithm, graded as Poor.
3.1.2 Evaluation
We asked three human judges to evaluate the order of
information in 20 summaries produced using the MO algo-
rithm into three categories| Poor, Fair and Good. We de-
ne a Poor summary, in an operational way, as a text whose
readability would be signicantly improved by reordering its
sentences. A Fair summary is a text which makes sense but
reordering of some sentences can yield a better readability.
Finally, a summary which cannot be further improved by
any sentence reordering is considered a Good summary.
The judges were asked to grade the summaries taking only
into account the order in which the information is presented.
To help them focus on this aspect of the texts, we resolved
dangling references beforehand. Figure 8 shows the grades
assigned to the summaries using majority to combine the
judges grades. In our experiments, judges had strong agree-
ment; they never gave three dierent grades to a summary.
TheMO algorithm produces a small number of Good sum-
maries, but most of the summaries were graded as Fair. For
instance, the summary graded Good shown in Figure 1 or-
ders the information in a natural way; the text starts with
a sentence summary of the event, then the outcome of the
trial is given, a reminder of the facts that caused the trial
and a possible explanation of the facts. Looking at the Good
summaries produced by MO, we found that it performs well
when the input articles follow the same order when present-
ing the information. In other words, the algorithm produces
a good ordering if the input articles orderings have high
agreement.
On the other hand, when analyzing Poor summaries, as in
Figure 2, we observe that the input texts have very dierent
orderings. By trying to maximize the agreement of the input
texts orderings, MO produces a new ordering that doesn't
occur in any input text. The ordering is, therefore, not guar-
anteed anymore to be acceptable. An example of a new pro-
duced ordering is given in Figure 2. The summary would be
more readable if several sentences were moved around (the
last sentence would be better placed before the fourth sen-
tence because they both talk about the Spanish authorities
handling the hijacking).
This algorithm can be used to order sentences accurately
if we are certain that the input texts follow similar orga-
nizations. This assumption may hold in limited domains.
However, in our case, the input texts we are processing do
not have such regularities. MO's performance critically de-
pends on the quality of the input texts, therefore, we should
design an ordering strategy which better ts our input data.
From here on, we will focus only on the Chronological Or-
dering algorithm and ways to improve it.
3.2 Chronological Ordering
3.2.1 The Algorithm
Multidocument summarization of news typically deals with
articles published on dierent dates, and articles themselves
cover events occurring over a wide range in time. Using
chronological order in the summary to describe the main
events helps the user understand what has happened. It
seems like a natural and appropriate strategy. As mentioned
earlier, in our framework, we are ordering themes; in this
strategy, we therefore need to assign a date to themes. To
identify the date an event occured requires a detailed in-
terpretation of temporal references in articles. While there
have been recent developments in disambiguating temporal
expressions and event ordering [12], correlating events with
the date on which they occurred is a hard task. In our case,
we approximate the theme time by its rst publication date;
that is, the rst time the theme has been reported in our
set of input articles. It is an acceptable approximation for
news events; the rst publication date of an event usually
corresponds to its occurrence in real life. For instance, in a
terrorist attack story, the theme conveying the attack itself
will have a date previous to the date of the theme describing
a trial following the attack.
Articles released by news agencies are marked with a pub-
lication date, consisting of a date and a time with three elds
(hour, minutes and seconds). Articles from the same news
agency are, then, guaranteed to have dierent publication
dates. This also holds for articles coming from dierent
news agencies. We never encountered two articles with the
same publication date during the development of MultiGen.
Thus, the publication date serves as a unique identier over
articles. As a result, when two themes have the same pub-
lication date, it means that they both are reported for the
rst time in the same article.
Our Chronological Ordering (CO) algorithm takes as in-
put a set of themes and orders them chronologically when-
ever possible. Each theme is assigned a date corresponding
to its rst publication. This establishes a partial order over
the themes. When two themes have the same date (that is,
they are reported for the rst time in the same article) we
sort them according to their order of presentation in this ar-
ticle. We have now a complete order over the input themes.
To implement this algorithm in MultiGen, we select for
each theme the sentence that has the earliest publication
date. We call it the time stamp sentence and assign its
publication date as the time stamp of the theme. Figures 3
and 4 show examples of produced summaries using CO.
One of four people accused along with former Pakistani
Prime Minister Nawaz Sharif has agreed to testify against
him in a case involving possible hijacking and kidnapping
charges, a prosecutor said Wednesday.
Raja Quereshi, the attorney general, said that the former
Civil Aviation Authority chairman has already given a state-
ment to police.
Sharif's lawyer dismissed the news when speaking to re-
porters after Sharif made an appearance before a judicial
magistrate to hear witnesses give statements against him.
Sharif has said he is innocent.
The allegations stem from an alleged attempt to divert
a plane bringing army chief General Pervez Musharraf to
Karachi from Sri Lanka on October 12.
Figure 3: A summary produced using the Chronological
Ordering algorithm graded as Good.
Thousands of people have attended a ceremony in Nairobi
commemorating the rst anniversary of the deadly bombings
attacks against U.S. Embassies in Kenya and Tanzania.
Saudi dissidentOsama bin Laden, accused of masterminding
the attacks, and nine others are still at large.
President Clinton said, "The intended victims of this vicious
crime stood for everything that is right about our country
and the world".
U.S. federal prosecutors have charged 17 people in the
bombings.
Albright said that the mourning continues.
Kenyans are observing a national day of mourning in honor
of the 215 people who died there.
Figure 4: A summary produced using the Chronological
Ordering algorithm graded as Poor.
3.2.2 Evaluation
Following the same methodology we used for the MO al-
gorithm evaluation, we asked three human judges to grade
20 summaries generated by the system using the CO algo-
rithm applied to the same collection of input texts. The
results are shown in Figure 8.
Our rst suspicion was that our approximation deviates
too much from the real chronological order of events, and,
therefore, lowers the quality of sentence ordering. To ver-
ify this hypothesis, we identied sentences that broke the
original chronological order and restored the ordering man-
ually. Interestingly, the displaced sentences were mainly
background information. The evaluation of the modied
summaries shows a slight but not visible improvement.
When comparing Good (Figure 3) and Poor (Figure 4)
summaries, we notice two phenomena: rst, many of the
badly placed sentences cannot be ordered based on their
temporal occurence. For instance, in Figure 4, the sentence
quoting Clinton is not one event in the sequence of events
being described, but rather a reaction to the main events.
This is also true for the sentence reporting Albright's reac-
tion. Assigning a date to a reaction, or more generally to
any sentence conveying background information, and plac-
ing it into the chronological stream of the main events does
not produce a logical ordering. The ordering of these themes
is therefore not covered by the CO algorithm.
The second phenomenon we observed is that Poor sum-
maries typically contain abrupt switches of topics and gen-
eral incoherences. For instance, in Figure 4, quotes from US
o?cials (third and fth sentences) are split and sentences
about the mourning (rst and sixth sentences) appear too
far apart in the summary. Grouping them together would
increase the readability of the summary. At this point, we
need to nd additional constraints to improve the ordering.
4. IMPROVING THE ORDERING:
EXPERIMENTS AND ANALYSIS
In the previous section, we showed that using naive or-
dering algorithms does not produce satisfactory orderings.
In this section, we investigate through experiments with hu-
mans, how to identify patterns of orderings that can improve
the algorithm.
Sentences in a text can be ordered in a number of ways,
and the text as a whole will still convey the same meaning.
But undoubtedly, some orders are denitely unacceptable
because they break conventions of information presentation.
One way to identify these conventions is to nd common-
alities between dierent acceptable orderings of the same
information. Extracting regularities in several acceptable
orderings can help us specify the main ordering constraints
for a given input type. Since a collection of multiple sum-
maries over the same set of articles doesn't exist, we created
our own collection of multiple orderings produced by dif-
ferent humans. Using this collection, we studied common
behaviors and mapped them to strategies for ordering.
Our collection of multiple orderings is available at
http://www.cs.columbia.edu/~noemie/ordering/. It was
built in the following way. We collected ten sets of articles.
Each set consisted of two to three news articles reporting the
same event. For each set, we manually selected the inter-
section sentences, simulating MultiGen
1
. On average, each
set contained 8.8 intersection sentences. The sentences were
cleaned of explicit references (for instance, occurrences of
\the President" were resolved to \President Clinton") and
connectives, so that participants wouldn't use them as clues
for ordering. Ten subjects participated in the experiment
and they each built one ordering per set of intersection sen-
tences. Each subject was asked to order the intersection
1
We performed a manual simulation to ensure that ideal
data was provided to the subjects of the experiments
sentences of a set so that they form a readable text. Over-
all, we obtained 100 orderings, ten alternative orderings per
set. Figure 5 shows the ten alternative orderings collected
for one set.
We rst observe that a surprising majority of orderings
are dierent. Out of the ten sets, only two sets had some
identical orderings (in one set, one pair of orderings were
identical while in the other set, two pairs of orderings were
identical). In other words, there are many acceptable order-
ings given one set of sentences. This conrms the intuition
that we do not need to look for a single ideal global ordering
but rather construct an acceptable one.
We also notice that, within the multiple orderings of a
set, some sentences always appear together. They do not
appear in the same order from one ordering to another, but
they share an adjacency relation. From now on, we refer to
them as blocks. For each set, we identify blocks by cluster-
ing sentences. We use as a distance metric between two sen-
tences the average number of sentences that separate them
over all orderings. In Figure 5, for instance, the distance
between the sentences D and G is 2. The blocks identied
by clustering are: sentences B, D, G and I; sentences A and
J; sentences C and F; and sentences E and H.
Participant 1 D B G I H F C J A E
Participant 2 D G B I C F A J E H
Participant 3 D B I G F J A E H C
Participant 4 D C F G I B J A H E
Participant 5 D G B I H F J A C E
Participant 6 D G I B F C E H J A
Participant 7 D B G I F C H E J A
Participant 8 D B C F G I E H A J
Participant 9 D G I B E H F A J C
Participant 10 D B G I C F A J E H
Figure 5: Multiple orderings for one set in our collec-
tion.
We observed that all the blocks in the experiment cor-
respond to clusters of topically related sentences. These
blocks form units of text dealing with the same subject, and
exhibit cohesive properties. For ordering, we can use this to
opportunistically group sentences together that all refer to
the same topic.
Collecting a set of multiple orderings is an expensive task;
it is di?cult and time consuming for a human to order sen-
tences from scratch. Furthermore, to discover signicant
commonalities across orderings, many multiple orderings of
the same set are necessary. We plan to extend our collection
and we are condent that it will provide more insights on
ordering. Still, the existing collection enables us to identify
cohesion as an important factor for ordering. We describe
next how we integrate the cohesion constraint in the CO
algorithm.
5. THE AUGMENTED ALGORITHM
In the output of the CO algorithm, disuencies arise when
topics are distributed over the whole text, violating cohesion
properties [13]. A typical scenario is illustrated in Figure 6.
The inputs are texts T
1
, T
2
, T
3
(in order of publication).
A
1
, A
2
and A
3
belong to the same theme whose intersection
sentence is A and similarly for B and C. The themes A and
B are topically related, but C is not related. Summary S
1
,
based only on chronological clues, contains two topical shifts;
from A to C and back from C to B. A better summary would
be S
2
which keeps A and B together.
AA C A
B
1 2 3
3
C1
...
B2
2A
C3 B
C
...
...
T T T S1 2 3 1
A
C
B
S 2
Figure 6: Input texts T
1
T
2
T
3
are summarized by the
Chronological Ordering (S
1
) or by the Augmented algo-
rithm (S
2
).
5.1 The Algorithm
Our goal is to remove disuencies from the summary by
grouping together topically related themes. This can be
achieved by integrating cohesion as an additional constraint
to the CO algorithm. The main technical di?culty in in-
corporating cohesion in our ordering algorithm is to iden-
tify and to group topically related themes across multiple
documents. In other words, given two themes, we need to
determine if they belong to the same cohesion block. For a
single document, segmentation [8] could be used to identify
blocks, but we cannot use such a technique to identify co-
hesion between sentences across multiple documents. The
main reason is that segmentation algorithms exploit the lin-
ear structure of an input text; in our case, we want to group
together sentences belonging to dierent texts.
Our solution consists of the following steps. In a prepro-
cessing stage, we segment each input text, so that given two
sentences within the same text, we can determine if they
are topically related. Assume the themes A and B, where
A contains sentences (A
1
: : :A
n
), and B contains sentences
(B
1
: : :B
m
). Recall that a theme is a set of sentences con-
veying similar information drawn from dierent input texts.
We denote #AB to be the number of pairs of sentences
(A
i
;B
j
) which appear in the same text, and #AB
+
to be
the number of sentence pairs which appear in the same text
and are in the same segment.
In a rst stage, for each pair of themes A and B, we com-
pute the ratio #AB
+
=#AB to measure the relatedness of
two themes. This measure takes into account both positive
and negative evidence. If most of the sentences in A and
B that appear together in the same texts are also in the
same segments, it means that A and B are highly topically
related. In this case, the ratio is close to 1. On the other
hand, if among the texts containing sentences from A and
B, only a few pairs are in the same segments, then A and B
are not topically related. Accordingly the ratio is close to 0.
A and B are considered related if this ratio is higher than
a predetermined threshold. In our experiments, we set it to
0.6.
This strategy denes pairwise relations between themes.
A transitive closure of this relation builds groups of related
themes and as a result ensures that themes that do not ap-
pear together in any article but are both related to a third
theme will still be linked. This creates an even higher degree
of relatedness among themes. Because we use a threshold
to establish pairwise relations, the transitive closure does
not produce elongated chains that could link together unre-
lated themes. We are now able to identify topically related
themes. At the end of the rst stage, they are grouped into
blocks.
In a second stage, we assign a time stamp to each block of
related themes, as the earliest time stamp of the themes it
contains. We adapt the CO algorithm described in 3.2.1 to
work at the level of the blocks. The blocks and the themes
correspond to, respectively, themes and sentences in the CO
algorithm. By analogy, we can easily show that the adapted
algorithm produces a complete order of the blocks. This
yields a macro-ordering of the summary. We still need to
order the themes inside each block.
In the last stage of the augmented algorithm, for each
block, we order the themes it contains by applying the CO
algorithm to them. Figure 7 shows an example of a summary
produced by the augmented algorithm.
This algorithm ensures that cohesively related themes will
not be spread over the text, and decreases the number of
abrupt switches of topics. Figure 7 shows how the Aug-
mented algorithm improves the sentence order compared
with the order in the summary produced by the CO al-
gorithm in Figure 4; sentences quoting US o?cials are now
grouped together and so are descriptions of the mourning.
Thousands of people have attended a ceremony in Nairobi
commemorating the rst anniversary of the deadly bomb-
ings attacks against U.S. Embassies in Kenya and Tanzania.
Kenyans are observing a national day of mourning in honor
of the 215 people who died there.
Saudi dissidentOsama bin Laden, accused of masterminding
the attacks, and nine others are still at large. U.S. federal
prosecutors have charged 17 people in the bombings.
President Clinton said, "The intended victims of this vicious
crime stood for everything that is right about our country
and the world". Albright said that the mourning continues.
Figure 7: A Summary produced using the Aug-
mented algorithm. Related sentences are grouped
into paragraphs.
5.2 Evaluation
Following the same methodology used to evaluate the MO
and the CO algorithms, we asked the judges to grade 20
summaries produced by the Augmented algorithm. Results
are shown in Figure 8.
The manual eort needed to compare and judge system
output is extensive; consider that each human judge had to
read three summaries for each input set as well as skim the
input texts to verify that no misleading order was introduced
in the summaries. Consequently, the evaluation that we
performed to date is limited. Still, this evaluation shows a
signicant improvement in the quality of the orderings from
the CO algorithm to the augmented algorithm. To assess the
signicance of the improvement, we used the Fisher exact
test, conating Poor and Fair summaries into one category.
This test is adapted to our case because of the reduced size
of our test set. We obtained a p value of 0.014 [20].
6. RELATED WORK
Finding an acceptable ordering has not been studied be-
fore in summarization. In single document summarization,
Poor Fair Good
Majority Ordering 2 12 6
Chronological Ordering 7 7 6
Augmented Ordering 2 7 11
Figure 8: Evaluation of the the Majority Ordering, the
Chronological Ordering and the Augmented Ordering.
summary sentences are typically arranged in the same order
that they were found in the full document (although [10]
reports that human summarizers do sometimes change the
original order). In multidocument summarization, the sum-
mary consists of fragments of text or sentences that were
selected from dierent texts. Thus, there is no complete
ordering of summary sentences that can be found in the
original documents.
The ordering task has been extensively investigated in the
generation community [14, 17, 9, 2, 16]. One approach is
top-down, using schemas [14] or plans [5] to determine the
organizational structure of the text. This appproach postu-
lates a rhetorical structure which can be used to select in-
formation from an underlying knowledge base. Because the
domain is limited, an encoding can be developed of the kinds
of propositional content that match rhetorical elements of
the schema or plan, thereby allowing content to be selected
and ordered. Rhetorical Structure Theory (RST) allows for
more exibility in ordering content. The relations occur be-
tween pairs of propositions. Constraints based on intention
(e.g., [17]), plan-like conventions [9], or stylistic constraints
[2] are used as preconditions on the plan operators contain-
ing RST relations to determine when a relation is used and
how it is ordered with respect to other relations.
MultiGen generates summaries of news on any topic. In
an unconstrained domain like this, it would be impossible
to enumerate the semantics for all possible types of sen-
tences which could match the elements of a schema, a plan
or rhetorical relations. Furthermore, it would be di?cult to
specify a generic rhetorical plan for a summary of news. In-
stead, content determination in MultiGen is opportunistic,
depending on the kinds of similarities that happen to exist
between a set of news documents. Similarly, we describe
here an ordering scheme that is opportunistic and bottom-
up, depending on the coherence and temporal connections
that happen to exist between selected text. Our approach
is similar to the use of basic blocks [16] where a bottom-up
technique is used to group together stretches of text in a
long, generated document by nding propositions that are
related by a common focus. Since this approach was devel-
oped for a generation system, it nds related propositions by
comparisons of proposition arguments at the semantic level.
In our case, we are dealing with a surface representation, so
we nd alternative methods for grouping text fragments.
7. CONCLUSION AND FUTURE WORK
In this paper we investigated information ordering con-
straints in multidocument summarization. We analyzed two
naive ordering algorithms, the Majority Ordering (MO) and
the Chronological Ordering (CO). We show that the MO al-
gorithm performs well only when all input texts follow sim-
ilar presentation of the information. The CO algorithm can
provide an acceptable solution for many cases, but is not
su?cient when summaries contain information that is not
event based. We report on the experiments we conducted
to identify other constraints contributing to ordering. We
show that cohesion is an important factor, and describe an
operational way to incorporate it in the CO algorithm. This
results in a denite improvement of the overall quality of au-
tomatically generated summaries.
In future work, we rst plan to extend our collection of
multiple orderings, so that we can extract more regulari-
ties and understand better how human order information to
produce a readable and uent text. Even though we did
not encounter any misleading inferences introduced by re-
ordering MultiGen output, we plan to do an extended study
of the side eects caused by reorderings. We also plan to
investigate whether the MO algorithm can be improved by
applying it on cohesive blocks of themes, rather than themes.
8. ACKNOWLEDGMENT
This work was partially supported by DARPA grant N66001-
00-1-8919, a Louis Morin scholarship and a Viros scholar-
ship. We thank Eli Barzilay for providing help with the
experiments interface, Michael Elhadad for the useful dis-
cussions and comments, and all the voluntary participants
in the experiments.
9. REFERENCES
[1] R. Barzilay, K. McKeown, and M. Elhadad.
Information fusion in the context of multi-document
summarization. In Proc. of the 37th Annual Meeting
of the Assoc. of Computational Linguistics, 1999.
[2] N. Bouayad-Agha, R. Power, and D. Scott. Can text
structure be incompatible with rhetorical structure?
In Proceedings of the First International Conference
on Natural Language Generation (INLG'2000),
Mitzpe Ramon, Israel, 2000.
[3] J. Carbonell and J. Goldstein. The use of mmr,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
1998.
[4] T. Cormen, C. Leiserson, and R. Rivest. Introduction
to Algorithms. The MIT Press, 1990.
[5] R. Dale. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. MIT Press, Cambridge, MA, 1992.
[6] N. Elhadad and K. McKeown. Generating patient
specic summaries of medical articles. Submitted,
2001.
[7] V. Hatzivassiloglou, J. Klavans, and E. Eskin.
Detecting text similarity over short passages:
Exploring linguistic feature combinations via machine
learning. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, 1999.
[8] M. Hearst. Multi-paragraph segmentation of
expository text. In Proceedings of the 32th Annual
Meeting of the Association for Computational
Linguistics, 1994.
[9] E. Hovy. Automated discourse generation using
discourse structure relations. Articial Intelligence, 63,
1993. Special Issue on NLP.
[10] H. Jing. Summary generation through intelligent
cutting and pasting of the input document. Technical
report, Columbia University, 1998.
[11] I. Mani and E. Bloedorn. Multi-document
summarization by graph search and matching. In
Proceedings of the Fifteenth National Conference on
Articial Intelligence, 1997.
[12] I. Mani and G. Wilson. Robust temporal processing of
news. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, 2000.
[13] K. McCoy and J. Cheng. Focus of attention:
Constraining what can be said next. In C. Paris,
W. Swartout, and W. Mann, editors, Natural
Language Generation in Articial Intelligence and
Computational Linguistics. Kluwer Academic
Publishers, 1991.
[14] K. McKeown. Text Generation: Using Discourse
Strategies and Focus Constraints to Generate Natural
Language Text. Cambridge University Press, England,
1985.
[15] K. McKeown, J. Klavans, V. Hatzivassiloglou,
R. Barzilay, and E. Eskin. Towards multidocument
summarization by reformulatin: Progress and
prospects. In Proceedings of the Seventeenth National
Conference on Articial Intelligence, 1999.
[16] D. Mooney, S. Carberry, and K. McCoy. The
generation of high-level structure for extended
explanations. In Proceedings of the International
Conference on Computational Linguistics
(COLING{90), pages 276{281, Helsinki, 1990.
[17] J. Moore and C. Paris. Planning text for advisory
dialogues: Capturing intentional and rhetorical
information. Journal of Computational Linguistics,
19(4), 1993.
[18] D. Radev, H. Jing, and M. Budzikowska.
Centroid-based summarization of multiple documents:
sentence extraction, utility-based evaluation, and user
studies. In Proceedings of the ANLP/NAACL 2000
Workshop on Automatic Summarization, 2000.
[19] D. Radev and K. McKeown. Generating natural
language summaries from multiple on-line sources.
Computational Linguistics, 24(3):469{500, September
1998.
[20] S. Siegal and N. J. Castellan. Non-Parametric
statistics for the behavioural sciences. McGraw Hill,
1988.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 33?40, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving Multilingual Summarization: Using Redundancy in the Input to
Correct MT errors
Advaith Siddharthan and Kathleen McKeown
Columbia University Computer Science Department
1214 Amsterdam Avenue, New York, NY 10027, USA.
 
as372,kathy  @cs.columbia.edu
Abstract
In this paper, we use the information re-
dundancy in multilingual input to correct
errors in machine translation and thus im-
prove the quality of multilingual sum-
maries. We consider the case of multi-
document summarization, where the input
documents are in Arabic, and the output
summary is in English. Typically, infor-
mation that makes it to a summary appears
in many different lexical-syntactic forms
in the input documents. Further, the use of
multiple machine translation systems pro-
vides yet more redundancy, yielding dif-
ferent ways to realize that information in
English. We demonstrate how errors in the
machine translations of the input Arabic
documents can be corrected by identify-
ing and generating from such redundancy,
focusing on noun phrases.
1 Introduction
Multilingual summarization is a relatively nascent
research area which has, to date, been addressed
through adaptation of existing extractive English
document summarizers. Some systems (e.g. SUM-
MARIST (Hovy and Lin, 1999)) extract sentences
from documents in a variety of languages, and trans-
late the resulting summary. Other systems (e.g.
Newsblaster (Blair-Goldensohn et al, 2004)) per-
form translation before sentence extraction. Read-
ability is a major issue for these extractive systems.
The output of machine translation software is usu-
ally errorful, especially so for language pairs such
as Chinese or Arabic and English. The ungrammati-
cality and inappropriate word choices resulting from
the use of MT systems leads to machine summaries
that are difficult to read.
Multi-document summarization, however, has in-
formation available that was not available during the
translation process and which can be used to im-
prove summary quality. A multi-document summa-
rizer is given a set of documents on the same event
or topic. This set provides redundancy; for example,
each document may refer to the same entity, some-
times in different ways. It is possible that by ex-
amining many translations of references to the same
entity, a system can gather enough accurate informa-
tion to improve the translated reference in the sum-
mary. Further, as a summary is short and serves as
a surrogate for a large set of documents, it is worth
investing more resources in its translation; readable
summaries can help end users decide which docu-
ments they want to spend time deciphering.
Current extractive approaches to summarization
are limited in the extent to which they address qual-
ity issues when the input is noisy. Some new sys-
tems attempt substituting sentences or clauses in
the summary with similar text from extraneous but
topic related English documents (Blair-Goldensohn
et al, 2004). This improves readability, but can only
be used in limited circumstances, in order to avoid
substituting an English sentence that is not faith-
ful to the original. Evans and McKeown (2005)
consider the task of summarizing a mixed data set
that contains both English and Arabic news reports.
Their approach is to separately summarize informa-
tion that is contained in only English reports, only
Arabic reports, and in both. While the only-English
and in-both information can be summarized by se-
lecting text from English reports, the summaries of
only-Arabic suffer from the same readability issues.
In this paper, we use principles from information
33
theory (Shannon, 1948) to address the issue of read-
ability in multilingual summarization. We take as
input, multiple machine translations into English of
a cluster of news reports in Arabic. This input is
characterized by high levels of linguistic noise and
by high levels of information redundancy (multiple
documents on the same or related topics and mul-
tiple translations into English). Our aim is to use
automatically acquired knowledge about the English
language in conjunction with the information redun-
dancy to perform error correction on the MT. The
main benefit of our approach is to make machine
summaries of errorful input easier to read and com-
prehend for end-users.
We focus on noun phrases in this paper. The
amount of error correction possible depends on the
amount of redundancy in the input and the depth of
knowledge about English that we can utilize. We
begin by tackling the problem of generating refer-
ences to people in English summaries of Arabic texts
(  2). This special case involves large amounts of re-
dundancy and allows for relatively deep English lan-
guage modeling, resulting in good error correction.
We extend our approach to arbitrary NPs in  3.
The evaluation emphasis in multi-document sum-
marization has been on evaluating content (not read-
ability), using manual (Nenkova and Passonneau,
2004) as well as automatic (Lin and Hovy, 2003)
methods. We evaluate readability of the generated
noun phrases by computing precision, recall and f-
measure of the generated version compared to mul-
tiple human models of the same reference, comput-
ing these metrics on n-grams. Our results show that
our system performs significantly better on precision
over two baselines (most frequent initial reference
and randomly chosen initial reference). Precision is
the most important of these measures as it is impor-
tant to have a correct reference, even if we don?t re-
tain all of the words used in the human models.
2 References to people
2.1 Data
We used data from the DUC 2004 Multilingual
summarization task. The Document Understanding
Conference (http://duc.nist.gov) has been run annu-
ally since 2001 and is the biggest summarization
evaluation effort, with participants from all over the
world. In 2004, for the first time, there was a multi-
lingual multi-document summarization task. There
were 25 sets to be summarized. For each set con-
sisting of 10 Arabic news reports, the participants
were provided with 2 different machine translations
into English (using translation software from ISI
and IBM). The data provided under DUC includes
4 human summaries for each set for evaluation pur-
poses; the human summarizers were provided a hu-
man translation into English of each of the Arabic
New reports, and did not have to read the MT output
that the machine summarizers took as input.
2.2 Task definition
An analysis of premodification in initial references
to people in DUC human summaries for the mono-
lingual task from 2001?2004 showed that 71% of
premodifying words were either title or role words
(eg. Prime Minister, Physicist or Dr.) or temporal
role modifying adjectives such as former or desig-
nate. Country, state, location or organization names
constituted 22% of premodifying words. All other
kinds of premodifying words, such as moderate or
loyal constitute only 7%. Thus, assuming the same
pattern in human summaries for the multilingual
task (cf. section 2.6 on evaluation), our task for each
person referred to in a document set is to:
1. Collect all references to the person in both translations of
each document in the set.
2. Identify the correct roles (including temporal modifica-
tion) and affiliations for that person, filtering any noise.
3. Generate a reference using the above attributes and the
person?s name.
2.3 Automatic semantic tagging
As the task definition above suggests, our approach
is to identify particular semantic attributes for a per-
son, and generate a reference formally from this se-
mantic input. Our analysis of human summaries tells
us that the semantic attributes we need to identify
are role, organization, country, state,
location and temporal modifier. In addi-
tion, we also need to identify the person name.
We used BBN?s IDENTIFINDER (Bikel et al, 1999)
to mark up person names, organizations and lo-
cations. We marked up countries and (American)
states using a list obtained from the CIA factsheet1 .
1http://www.cia.gov/cia/publications/factbook provides a
list of countries and states, abbreviations and adjectival forms,
for example United Kingdom/U.K./British/Briton and Califor-
nia/Ca./Californian.
34
To mark up roles, we used a list derived from Word-
Net (Miller et al, 1993) hyponyms of the person
synset. Our list has 2371 entries including multi-
word expressions such as chancellor of the exche-
quer, brother in law, senior vice president etc. The
list is quite comprehensive and includes roles from
the fields of sports, politics, religion, military, busi-
ness and many others. We also used WordNet to ob-
tain a list of 58 temporal adjectives. WordNet classi-
fies these as pre- (eg. occasional, former, incoming
etc.) or post-nominal (eg. elect, designate, emeritus
etc.). This information is used during generation.
Further, we identified elementary noun phrases us-
ing the LT TTT noun chunker (Grover et al, 2000),
and combined NP of NP sequences into one com-
plex noun phrase. An example of the output of our
semantic tagging module on a portion of machine
translated text follows:
...  NP   ROLE  representative  ROLE  of
 COUNTRY  Iraq  COUNTRY  of the  ORG 
United Nations  ORG  PERSON  Nizar Hamdoon
 PERSON  NP  that  NP  thousands of people
 NP  killed or wounded in  NP  the  TIME  next
 TIME  few days four of the aerial bombardment of
 COUNTRY  Iraq  COUNTRY 	 NP  ...
Our principle data structure for this experiment is
the attribute value matrix (AVM). For example, we
create the following AVM for the reference to Nizar
Hamdoon in the tagged example above:


 ffProceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 241?248, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Automatically Learning Cognitive Status for Multi-Document
Summarization of Newswire
Ani Nenkova and Advaith Siddharthan and Kathleen McKeown
Department of Computer Science
Columbia University
 
ani,advaith,kathy  @cs.columbia.edu
Abstract
Machine summaries can be improved by
using knowledge about the cognitive sta-
tus of news article referents. In this paper,
we present an approach to automatically
acquiring distinctions in cognitive status
using machine learning over the forms of
referring expressions appearing in the in-
put. We focus on modeling references to
people, both because news often revolve
around people and because existing natu-
ral language tools for named entity iden-
tification are reliable. We examine two
specific distinctions?whether a person in
the news can be assumed to be known to a
target audience (hearer-old vs hearer-new)
and whether a person is a major charac-
ter in the news story. We report on ma-
chine learning experiments that show that
these distinctions can be learned with high
accuracy, and validate our approach using
human subjects.
1 Introduction
Multi-document summarization has been an active
area of research over the past decade (Mani and
Maybury, 1999) and yet, barring a few exceptions
(Daume? III et al, 2002; Radev and McKeown,
1998), most systems still use shallow features to pro-
duce an extractive summary, an age-old technique
(Luhn, 1958) that has well-known problems. Ex-
tractive summaries contain phrases that the reader
cannot understand out of context (Paice, 1990) and
irrelevant phrases that happen to occur in a relevant
sentence (Knight and Marcu, 2000; Barzilay, 2003).
Referring expressions in extractive summaries illus-
trate this problem, as sentences compiled from dif-
ferent documents might contain too little, too much
or repeated information about the referent.
Whether a referring expression is appropriate de-
pends on the location of the referent in the hearer?s
mental model of the discourse?the referent?s cog-
nitive status (Gundel et al, 1993). If, for example,
the referent is unknown to the reader at the point of
mention in the discourse, the reference should in-
clude a description, while if the referent was known
to the reader, no descriptive details are necessary.
Determining a referent?s cognitive status, how-
ever, implies the need to model the intended audi-
ence of the summary. Can such a cognitive status
model be inferred automatically for a general read-
ership? In this paper, we address this question by
performing a study with human subjects to confirm
that reasonable agreement on the distinctions can be
achieved between different humans (cf.  5). We
present an automatic approach for inferring what the
typical reader is likely to know about people in the
news. Our approach uses machine learning, exploit-
ing features based on the form of references to peo-
ple in the input news articles (cf.  4). Learning
cognitive status of referents is necessary if we want
to ultimately generate new, more appropriate refer-
ences for news summaries.
1.1 Cognitive status
In human communication, the wording used by
speakers to refer to a discourse entity depends on
their communicative goal and their beliefs about
what listeners already know. The speaker?s goals
and beliefs about the listener?s knowledge are both a
part of a cognitive/mental model of the discourse.
241
Cognitive status distinctions depend on two pa-
rameters related to the referent?a) whether it al-
ready exists in the hearer?s model of the discourse,
and b) its degree of salience. The influence of these
distinctions on the form of referring expressions has
been investigated in the past. For example, center-
ing theory (Grosz et al, 1995) deals predominantly
with local salience (local attentional status), and the
givenness hierarchy (information status) of Prince
(1992) focuses on how a referent got in the discourse
model (e.g. through a direct mention in the current
discourse, through previous knowledge, or through
inference), leading to distinctions such as discourse-
old, discourse-new, hearer-old, hearer-new, infer-
able and containing inferable. Gundel et al (1993)
attempt to merge salience and givenness in a single
hierarchy consisting of six distinctions in cognitive
status (in focus, activated, familiar, uniquely identi-
fiable, referential, type-identifiable).
Among the distinctions that have an impact on the
form of references in a summary are the familiarity
of the referent:
D. Discourse-old vs discourse-new
H. Hearer-old vs hearer-new
and its global salience1:
M. Major vs minor
In general, initial (discourse-new) references to en-
tities are longer and more descriptive, while sub-
sequent (discourse-old) references are shorter and
have a purely referential function. Nenkova and
McKeown (2003) have studied this distinction for
references to people in summaries and how it can be
used to automatically rewrite summaries to achieve
better fluency and readability.
The other two cognitive status distinctions,
whether an entity is central to the summary or not
(major or minor) and whether the hearer can be as-
sumed to be already familiar with the entity (hearer-
old vs hearer-new status), have not been previously
studied in the context of summarization. There is
a tradeoff, particularly important for a short sum-
mary, between what the speaker wants to convey
1The notion of global salience is very important to summa-
rization, both during content selection and during generation on
initial references to entities. On the other hand, in focus or local
attentional state are relevant to anaphoric usage during subse-
quent mentions.
and how much the listener needs to know. The
hearer-old/new distinction can be used to determine
whether a description for a character is required
from the listener?s perspective. The major/minor
distinction plays a role in defining the communica-
tive goal, such as what the summary should be about
and which characters are important enough to refer
to by name.
1.2 Hearer-Old vs Hearer-New
Hearer-new entities in a summary should be de-
scribed in necessary detail, while hearer-old enti-
ties do not require an introductory description. This
distinction can have a significant impact on over-
all length and intelligibility of the produced sum-
maries. Usually, summaries are very short, 100 or
200 words, for input articles totaling 5,000 words
or more. Several people might be involved in a
story, which means that if all participants are fully
described, little space will be devoted to actual
news. In addition, introducing already familiar en-
tities might distract the reader from the main story
(Grice, 1975). It is thus a good strategy to refer
to an entity that can be assumed hearer-old by just
a title + last name, e.g. President Bush, or by full
name only, with no accompanying description, e.g.
Michael Jackson.
1.3 Major vs Minor
Another distinction that human summarizers make
is whether a character in a story is a major or a
minor one and this distinction can be conveyed by
using different forms of referring expressions. It is
common to see in human summaries references such
as the dissident?s father. Usually, discourse-initial
references solely by common noun, without the in-
clusion of the person?s name, are employed when
the person is not the main focus of a story (San-
ford et al, 1988). By detecting the cognitive sta-
tus of a character, we can decide whether to name
the character in the summary. Furthermore, many
summarization systems use the presence of named
entities as a feature for computing the importance
of a sentence (Saggion and Gaizaukas, 2004; Guo
et al, 2003). The ability to identify the major story
characters and use only them for sentence weighting
can benefit such systems since only 5% of all peo-
ple mentioned in the input are also mentioned in the
summaries.
242
2 Why care about people in the news?
News reports (and consequently, news summaries)
tend to have frequent references to people (in DUC
data - see  3 for description - from 2003 and 2004,
there were on average 3.85 references to people per
100-word human summary); hence it is important
for news summarization systems to have a way of
modeling the cognitive status of such referents and
a theory for referring to people.
It is also important to note that there are differ-
ences in references to people between news reports
and human summaries of news. Journalistic con-
ventions for many mainstream newspapers dictate
that initial mentions to people include a minimum
description such as their role or title and affilia-
tion. However, in human summaries, where there
are greater space constraints, the nature of initial ref-
erences changes. Siddharthan et al (2004) observed
that in DUC?04 and DUC?03 data2, news reports
contain on average one appositive phrase or relative
clause every 3.9 sentences, while the human sum-
maries contain only one per 8.9 sentences on aver-
age. In addition to this, we observe from the same
data that the average length of a first reference to a
named entity is 4.5 words in the news reports and
only 3.6 words in human summaries. These statis-
tics imply that human summarizers do compress ref-
erences, and thus can save space in the summary for
presenting information about the events. Cognitive
status models can inform a system when such refer-
ence compression is appropriate.
3 Data preparation: the DUC corpus
The data we used to train classifiers for these two
distinctions is the Document Understanding Confer-
ence collection (2001?2004) of 170 pairs of doc-
ument input sets and the corresponding human-
written multi-document summaries (2 or 4 per set).
Our aim is to identify every person mentioned in
the 10 news reports and the associated human sum-
maries for each set, and assign labels for their cog-
nitive status (hearer old/new and major/minor). To
do this, we first preprocess the data (  3.1) and then
perform the labeling (  3.2).
2The data provided under DUC for these years includes sets
of about 10 news reports, 4 human summaries for each set, and
the summaries by participating machine summarizers.
3.1 Automatic preprocessing
All documents and summaries were tagged with
BBN?s IDENTIFINDER (Bikel et al, 1999) for
named entities, and with a part-of-speech tagger and
simplex noun-phrase chunker (Grover et al, 2000).
In addition, for each named entity, relative clauses,
appositional phrases and copula constructs, as well
as pronominal co-reference were also automatically
annotated (Siddharthan, 2003). We thus obtained
coreference information (cf. Figure 1) for each per-
son in each set, across documents and summaries.
Andrei Sakharov
Doc 1:
[IR] laureate Andrei D. Sakharov [CO] Sakharov
[CO] Sakharov [CO] Sakharov [CO] Sakharov [PR]
his [CO] Sakharov [PR] his [CO] Sakharov [RC] who
acted as an unofficial Kremlin envoy to the troubled
Transcaucasian region last month [PR] he [PR] He
[CO] Sakharov
Doc 1:
[IR] Andrei Sakharov [AP] , 68 , a Nobel Peace Prize
winner and a human rights activist , [CO] Sakharov
[IS] a physicist [PR] his [CO] Sakharov
Figure 1: Example information collected for Andrei
Sakharov from two news report. ?IR? stands for ?ini-
tial reference?, ?CO? for noun co-reference, ?PR? for
pronoun reference, ?AP? for apposition, ?RC? for rel-
ative clause and ?IS? for copula constructs.
The tools that we used were originally devel-
oped for processing single documents and we had
to adapt them for use in a multi-document setting.
The goal was to find, for each person mentioned
in an input set, the list of all references to the per-
son in both input documents and human summaries.
For this purpose, all input documents were concate-
nated and processed with IDENTIFINDER. This was
then automatically post-processed to mark-up core-
ferring names and to assign a unique canonical name
(unique id) for each name coreference chain. For the
coreference, a simple rule of matching the last name
was used, and the canonical name was the ?First-
Name LastName? string where the two parts of the
name could be identified 3. Concatenating all docu-
ments assures that the same canonical name will be
assigned to all named references to the same person.
3Occasionally, two or more different people with the same
last name are discussed in the same set and this algorithm would
lead to errors in such cases. We did keep a list of first names
associated with the entity, so a more refined matching model
could be developed, but this was not the focus of this work.
243
The tools for pronoun coreference and clause and
apposition identification and attachment were run
separately on each document. Then the last name of
each of the canonical names derived from the IDEN-
TIFINDER output was matched with the initial ref-
erence in the generic coreference list for the doc-
ument with the last name. The tools that we used
have been evaluated separately when used in nor-
mal single document setting. In our cross-document
matching processes, we could incur more errors, for
example when the general coreference chain is not
accurate. On average, out of 27 unique people per
cluster identified by IDENTIFINDER, 4 people and
the information about them are lost in the matching
step for a variety of reasons such as errors in the
clause identifier, or the coreference.
3.2 Data labeling
Entities were automatically labeled as hearer-old or
new by analyzing the syntactic form that human
summarizers used for initial references to them. The
labeling rests on the assumption that the people who
produced the summaries used their own model of the
reader when choosing appropriate references for the
summary. The following instructions had been given
to the human summarizers, who were not profes-
sional journalists: ?To write this summary, assume
you have been given a set of stories on a news topic
and that your job is to summarize them for the gen-
eral news sections of the Washington Post. Your au-
dience is the educated adult American reader with
varied interests and background in current and re-
cent events.? Thus, the human summarizers were
given the freedom to use their assumptions about
what entities would be generally hearer-old and they
could refer to these entities using short forms such as
(1) title or role+ last name or (2) full name only with
no pre- or post-modification. Entities that the major-
ity of human summarizers for the set referred to us-
ing form (1) or (2) were labeled as hearer-old. From
the people mentioned in human summaries, we ob-
tained 118 examples of hearer-old and 140 examples
of hearer-new persons - 258 examples in total - for
supervised machine learning.
In order to label an entity as major or minor, we
again used the human summaries?entities that were
mentioned by name in at least one summary were la-
beled major, while those not mentioned by name in
any summary were labeled minor. The underlying
assumption is that people who are not mentioned in
any human summary, or are mentioned without be-
ing named, are not important. There were 258 major
characters who made it to a human summary and
3926 minor ones that only appeared in the news re-
ports. Such distribution between the two classes is
intuitively plausible, since many people in news ar-
ticles express opinions, make statements or are in
some other way indirectly related to the story, while
there are only a few main characters.
4 Machine learning experiments
The distinction between hearer-old and hearer-new
entities depends on the readers. In other words, we
are attempting to automatically infer which charac-
ters would be hearer-old for the intended readership
of the original reports, which is also expected to be
the intended readership of the summaries. For our
experiments, we used the WEKA (Witten and Frank,
2005) machine learning toolkit and obtained the best
results for hearer-old/new using a support vector ma-
chine (SMO algorithm) and for major/minor, a tree-
based classifier (J48). We used WEKA?s default set-
tings for both algorithms.
We now discuss what features we used for our
two classification tasks (cf. list of features in table
1). Our hypothesis is that features capturing the fre-
quency and syntactic and lexical forms of references
are sufficient to infer the desired cognitive model.
Intuitively, pronominalization indicates that an
entity was particularly salient at a specific point of
the discourse, as has been widely discussed in at-
tentional status and centering literature (Grosz and
Sidner, 1986; Gordon et al, 1993). Modified noun
phrases (with apposition, relative clauses or premod-
ification) can also signal different status.
In addition to the syntactic form features, we used
two months worth of news articles collected over the
web (and independent of the DUC collection we use
in our experiments here) to collect unigram and bi-
gram lexical models of first mentions of people. The
names themselves were removed from the first men-
tion noun phrase and the counts were collected over
the premodifiers only. One of the lexical features
we used is whether a person?s description contains
any of the 20 most frequent description words from
our web corpus. We reasoned that these frequent de-
244
0,1: Number of references to the person, including pro-
nouns (total and normalized by feature 16)
2,3: Number of times apposition was used to describe
the person(total and normalized by feature 16)
4,5: Number of times a relative clause was used to de-
scribe the person (total and normalized by 16)
6: Number of times the entity was referred to by
name after the first reference
7,8: Number of copula constructions involving the per-
son (total and normalized by feature 16)
9,10: Number of apposition, relative clause or copula
descriptions (total and normalized by feature 16)
11,12,13: Probability of an initial reference according to the
bigram model (av.,max and min of all initial refer-
ences)
14: Number of top 20 high frequency description
words (from references to people in large news
corpus) present in initial references
15: Proportion of first references containing full name 16: Total number of documents containing the person
17,18: Number of appositives or relative clause attaching
to initial references (total and normalized by fea-
ture 16)
Table 1: List of Features provided to WEKA.
scriptors may signal importance; the full list is:
president, former, spokesman, sen, dr, chief, coach,
attorney, minister, director, gov, rep, leader, secre-
tary, rev, judge, US, general, manager, chairman.
Another lexical feature was the overall likelihood
of a person?s description using the bigram model
from our web corpus. This indicates whether a per-
son has a role or affiliation that is frequently men-
tioned. We performed 20-fold cross validation for
both classification tasks. The results are shown in
Table 2 (accuracy) and Table 3 (precision/recall).
4.1 Major vs Minor results
For major/minor classification, the majority class
prediction has 94% accuracy, but is not a useful
baseline as it predicts that no person should be men-
tioned by name and all are minor characters. J48
correctly predicts 114 major characters out of 258
in the 170 document sets. As recall appeared low,
we further analyzed the 148 persons from DUC?03
and DUC?04 sets, for which DUC provides four hu-
man summaries. Table 4 presents the distribution of
recall taking into account how many humans men-
tioned the person by name in their summary (origi-
nally, entities were labeled as main if any summary
had a reference to them, cf.  3.2). It can be seen that
recall is high (0.84) when all four humans consider
a character to be major, and falls to 0.2 when only
one out of four humans does. These observations re-
flect the well-known fact that humans differ in their
choices for content selection, and indicate that in the
automatic learning is more successful when there is
more human agreement.
In our data there were 258 people mentioned by
name in at least one human summary. In addition,
there were 103 people who were mentioned in at
least one human summary using only a common
noun reference (these were identified by hand, as
common noun coreference cannot be performed re-
liably enough by automatic means), indicating that
29% of people mentioned in human summaries are
not actually named. Examples of such references
include an off duty black policeman, a Nigerian
born Roman catholic priest, Kuwait?s US ambas-
sador. For the purpose of generating references in
a summary, it is important to evaluate how many of
these people are correctly classified as minor char-
acters. We removed these people from the training
data and kept them as a test set. WEKA achieved
a testing accuracy of 74% on these 103 test exam-
ples. But as discussed before, different human sum-
marizers sometimes made different decisions on the
form of reference to use. Out of the 103 referent
for which a non-named reference was used by a
summarizer, there were 40 where other summariz-
ers used named reference. Only 22 of these 40 were
labeled as minor characters in our automatic proce-
dure. Out of the 63 people who were not named in
any summary, but mentioned in at least one by com-
mon noun reference, WEKA correctly predicted 58
(92%) as minor characters. As before, we observe
that when human summarizers generate references
of the same form (reflecting consensus on convey-
ing the perceived importance of the character), the
machine predictions are accurate.
We performed feature selection to identify which
are the most important features for the classification
task. For the major/minor classification, the impor-
tant features used by the classifier were the number
of documents the person was mentioned in (feature
16), number of mentions within the document set
(features 1,6), number of relative clauses (feature
245
4,5) and copula (feature 8) constructs, total number
of apposition, relative clauses and copula (feature
9), number of high frequency premodifiers (feature
14) and the maximum bigram probability (feature
12). It was interesting that presence of apposition
did not select for either major or minor class. It is
not surprising that the frequency of mention within
and across documents were significant features?a
frequently mentioned entity will naturally be consid-
ered important for the news report. Interestingly, the
syntactic form of the references was also a signifi-
cant indicator, suggesting that the centrality of the
character was signaled by the journalists by using
specific syntactic constructs in the references.
Major/Minor Hearer New/Old
WEKA 0.96 (J48) 0.76 (SMO)
Majority class prediction 0.94 0.54
Table 2: Cross validation testing accuracy results.
Class Precision Recall F-measure
SMO hearer-new 0.84 0.68 0.75
hearer-old 0.69 0.85 0.76
J48 major-character 0.85 0.44 0.58
minor-character 0.96 0.99 0.98
Table 3: Cross validation testing P/R/F results.
Number of summaries Number of Number and %
containing the person examples recalled by J48
1 out of 4 59 15 (20%)
2 out of 4 35 20 (57%)
3 out of 4 29 23 (79%)
4 out of 4 25 21 (84%)
Table 4: J48 Recall results and human agreement.
4.2 Hearer Old vs New Results
The majority class prediction for the hearer-old/new
classification task is that no one is known to the
reader and it leads to overall classification accu-
racy of 54%. Using this prediction in a summarizer
would result in excessive detail in referring expres-
sions and a consequent reduction in space available
to summarize the news events. The SMO prediction
outperformed the baseline accuracy by 22% and is
more meaningful for real tasks.
For the hearer-old/new classification, the feature
selection step chose the following features: the num-
ber of appositions (features 2,3) and relative clauses
(feature 5), number of mentions within the docu-
ment set (features 0,1), total number of apposition,
relative clauses and copula (feature 10), number of
high frequency premodifiers (feature 14) and the
minimum bigram probability (feature 13). As in the
minor-major classification, the syntactic choices for
reference realization were useful features.
We conducted an additional experiment to see
how the hearer old/new status impacts the use of ap-
position or relative clauses for elaboration in refer-
ences produced in human summaries. It has been
observed (Siddharthan et al, 2004) that on average
these constructs occur 2.3 times less frequently in
human summaries than in machine summaries. As
we show, the use of postmodification to elaborate re-
lates to the hearer-old/new distinction.
To determine when an appositive or relative
clause can be used to modify a reference, we con-
sidered the 151 examples out of 258 where there was
at least one relative clause or apposition describing
the person in the input. We labeled an example as
positive if at least one human summary contained
an apposition or relative clause for that person and
negative otherwise. There were 66 positive and 85
negative examples. This data was interesting be-
cause while for the majority of examples (56%) all
the human summarizers agreed not to use postmod-
ification, there were very few examples (under 5%)
where all the humans agreed to postmodify. Thus it
appears that for around half the cases, it should be
obvious that no postmodification is required, but for
the other half, human decisions go either way.
Notably, none of the hearer-old persons (using test
predictions of SMO) were postmodified. Our cogni-
tive status predictions cleanly partition the examples
into those where postmodification is not required,
and those where it might be. Since no intuitive rule
handled the remaining examples, we added the test-
ing predictions of hearer-old/new and major/minor
as features to the list in Table 1, and tried to learn
this task using the tree-based learner J48. We report
a testing accuracy of 71.5% (majority class baseline
is 56%). There were only three useful features?
the predicted hearer-new/old status, the number of
high frequency premodifiers for that person in the
input (feature 14 in table 1) and the average number
of postmodified initial references in the input docu-
ments (feature 17).
5 Validating the results on current news
We tested the classifiers on data different from that
provided by DUC, and also tested human consen-
246
sus on the hearer-new/old distinction. For these pur-
poses, we downloaded 45 clusters from one day?s
output from Newsblaster4. We then automatically
compiled the list of people mentioned in the ma-
chine summaries for these clusters. There were 107
unique people that appeared in the machine sum-
maries, out of 1075 people in the input clusters.
5.1 Human agreement on hearer-old/new
A question arises when attempting to infer hearer-
new/old status: Is it meaningful to generalize this
across readers, seeing how dependent it is on the
world knowledge of individual readers?
To address this question, we gave 4 Ameri-
can graduate students a list of the names of peo-
ple in the DUC human summaries (cf.  3), and
asked them to write down for each person, their
country/state/organization affiliation and their role
(writer/president/attorney-general etc.). We consid-
ered a person hearer-old to a subject if they correctly
identified both role and affiliation for that person.
For the 258 people in the DUC summaries, the four
subjects demonstrated 87% agreement ( 	
 5.
Similarly, they were asked to perform the same
task for the Newsblaster data, which dealt with con-
temporary news6, in contrast with the DUC data
that contained news from the the late 80s and early
90s. On this data, the human agreement was 91%
( 	
 ). This is a high enough agreement to
suggest that the classification of national and inter-
national figures as hearer old/new across the edu-
cated adult American reader with varied interests
and background in current and recent events is a
well defined task. This is not necessarily true for
the full range of cognitive status distinctions; for
example Poesio and Vieira (1998) report lower hu-
man agreement on more fine-grained classifications
of definite descriptions.
5.2 Results on the Newsblaster data
We measured how well the models trained on DUC
data perform with current news labeled using human
4http://newsblaster.cs.columbia.edu
5  (kappa) is a measure of inter-annotator agreement over
and above what might be expected by pure chance (See Carletta
(1996) for discussion of its use in NLP).  if there is perfect
agreement between annotators and ffProceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 716?723, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Context and Learning in Novelty Detection
Barry Schiffman and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, N.Y.
{bschiff,kathy}@cs.columbia.edu
Abstract
We demonstrate the value of using con-
text in a new-information detection sys-
tem that achieved the highest precision
scores at the Text Retrieval Conference?s
Novelty Track in 2004. In order to de-
termine whether information within a sen-
tence has been seen in material read pre-
viously, our system integrates information
about the context of the sentence with
novel words and named entities within the
sentence, and uses a specialized learning
algorithm to tune the system parameters.
1 Introduction
New-information detection addresses two important
problems in a society awash in more digital infor-
mation than people can exploit. A novelty detection
system could help people who are tracking an event
in the news, where numerous sources present simi-
lar material. It could also provide a way to organize
summaries by focusing on the most recent informa-
tion, much like an automated bulletin service.
We envision that many types of users would find
such a system valuable. Certainly analysts, busi-
ness people, and anyone interested in current events,
would benefit from being able to track news stories
automatically, without repetition. Different news or-
ganizations report on the same event, often working
hard to make their reports look different from one
another, whether or not they have new material to
report. Our system would help readers to zero in
on new information. In addition, a focus on new
information provides a way of organizing a general
summary.
Our approach is unique in representing and main-
taining the focus in discourse. The idea stems from
the fact that novelty often comes in bursts, which
is not surprising since the articles are composed of
some number of smaller, coherent segments. Each
segment is started by some kind of introductory pas-
sage, and that is where we expect to find the novel
words. Novel words are identified by comparing
the current sentence?s words against a table of all
words seen in the inputs to that point. They let us
know whether the entire segment is likely to con-
tain more novel material. Subsequent passages are
likely to continue the novel discussion whether or
not they contain novel words. They may contain
pronomial references or other anaphoric references
to the novel entity. Our long-term goal is to inte-
grate the approach described in this paper into our
larger new-information detector, a system that per-
forms a more complicated syntactic analysis of the
input texts and employs machine learning to classify
passages as new or old.
Meanwhile, we tested our focus-based approach
at the Novelty Track at the Text Retrieval Confer-
ence (TREC) in 2004. The Novelty Tracks in 2003
and in 2004 were divided into four tasks; Task 1
and Task 3 incorporate retrieval, requiring submis-
sions to locate the relevant sentences before filter-
ing them for novelty. Tasks 2 and 4 are novelty de-
tection alone, using the relevant sentences selected
by humans as input. Since our interest is in nov-
716
elty detection, we chose to concentrate on Task 21
Our TREC submission was also designed to test a
specialized learning mechanism we implemented to
target either high precision or high recall.
In all, the problem of novelty detection is decep-
tively difficult. We were struck by the difficulty that
all groups in the Novelty Track in 2002 and 2003
had in obtaining high precision scores. Submissions
that classify a very large proportion of the input
sentences as novel reached the highest F-measure
scores by getting high recall scores, but failed to
achieve any substantial compression of material for
users. Given that our goal is to generate an up-
date summary, we focused on improving precision
and increasing compression, removing as many false
positives as possible.
The next section discusses the Novelty Track and
the approaches others have tried; Section 3 details
our system, and Section 4 presents the experiments.
2 Novelty Track
Much of the work in new-information detection has
been done for the TREC Novelty Track. The task
is related to first story detection, which is defined
on whole documents rather than on passages within
documents. In Task 1 of the Novelty Track, a system
is given about 25 documents on a topic and asked to
find all sentences relevant to the topic. In Task 2,
the inputs are the set of relevant sentences, so that
the program does not see the entire documents. The
program must scan the sentences in order and output
all that contain new information, that is information
not seen in the previous input sentences.
2.1 Related Work
At the recent TREC, Dublin City University did well
by comparing the words in a sentence against the
accumulated words in all previous sentences (Blott
et al, 2004). Their runs varied the way in which
the words were weighted with frequency and inverse
document frequency. Like our system, theirs follows
from the intuition that words that are new to a dis-
cussion are evidence of novelty. But our system dis-
1Task 4 was similar to Task 2, in that both have the human
annotations as input. For Task 2, participants only get the anno-
tations, but in Task 4, they also receive the novel sentences from
the first five documents as input. We felt that we would learn as
much from the one task as from both.
tinguishes between several kinds of words, includ-
ing common nouns, named persons, named organi-
zation, etc. Our system also incorporates a mecha-
nism for looking at the context of the sentence.
Both the Dublin system and ours are preceded by
the University of Iowa?s approach at TREC 2003. It
based novelty decisions on a straightforward count
of new named entities and noun phrases in a sen-
tence (Eichmann et al, 2003). In 2004, the Iowa sys-
tem (Eichmann et al, 2004) tried several embellish-
ments, one using synonyms in addition to the words
for novelty comparisons, and one using word-sense
disambiguation. These two runs were above average
in F-measure and about average in precision.
The University of Massachusetts system (Abdul-
Jaleel et al, 2004) mixed a vector-space model with
cosine similarity and a count of previously unseen
named entities. Their system resembled one of two
baseline methods that we submitted without our fo-
cus feature. Their submission used a similarity
threshold that was tuned experimentally, while ours
was learned automatically. In earlier work with the
TREC 2002 data, UMass (Allan et al, 2003) com-
pared a number of sentence-based models ranging
in complexity from a count of new words and cosine
distance, to a variety of sophisticated models based
on KL divergence with different smoothing strate-
gies and a ?core mixture model? that considered the
distribution of the words in the sentence with the
distributions in a topic model and a general English
model.
A number of groups have experimented with
matrix-based methods. In 2003, a group from the
University of Maryland and the Center for Com-
puting Sciences (Conroy et al, 2003) used three
techniques that used QR decomposition and sin-
gular value decomposition. The University of
Maryland, Baltimore County, worked with cluster-
ing algorithms and singular value decomposition
in sentence-sentence similarity matrices (Kallurkar
et al, 2003). In 2004, Conroy (Conroy, 2004)
tested Maximal Marginal Relevance (Goldstein et
al., 2000) as well as QR decomposition.
The information retrieval group at Tsinghua Uni-
versity used a pooling technique, grouping similar
sentences into clusters in order to capture sentences
that partially match two or more other sentences(Ru
et al, 2004). They said they had found difficulties
717
with sentence-by-sentence comparisons.
2.2 Precision
At all three Novelty Track evaluations, from 2002 to
2004, it is clear that high precision is much harder
to obtain than high recall. Trivial baselines ? such
as accept all sentences as novel ? have proved to be
difficult to beat by very much. This one-line algo-
rithm automatically obtains 100% recall and preci-
sion equal to the proportion of novel sentences in
the input. In 2003, when 66% of the relevant sen-
tences were novel, the mean precision score was
0.6352 and the median was 0.7. In 2004, 41% of the
relevant sentences were novel, and the average pre-
cision dropped to 0.46. The median precision was
also 0.46. Meanwhile, average recall scores across
all submissions actually rose to 0.861 in 2004, com-
pared with 0.795 in 2003. In terms of a real world
system, this means that as the number of target sen-
tences shrank, the number of sentences in the aver-
age program output rose. Likewise, a trivial system
could guarantee no errors by returning nothing, but
this would have no value.
2.3 Sentences
Normally, in Information Retrieval tasks, stricter
thresholds result in higher precision, and looser
thresholds, higher recall. In that way, a system can
target its results to a user?s needs. But in new-
information detection, this rule of thumb fails at
some point as thresholds become stricter. Recall
does fall, but precision does not rise. In other words,
there seems to be a ceiling for precision.
Several participants noted that their simpler
strategies produced the best results. For example,
in 2003, the Chinese Academy of Sciences (Sun et
al., 2003), noted that word overlap was surprisingly
strong as a similarity measure. As we have seen
above, the Iowa approach of counting nouns was in-
corporated by a few others for 2004, including us.
This strategy compares words in a sentence against
all previous seen words and thus, avoids comput-
ing pairwise similarity between all sentences. Al-
2One group appeared to have submitted a large number of ir-
relevant sentences in its submission, since it obtained relatively
high recall scores, but very low precision scores, causing the
average to drop below 0.66. The average precision of all other
groups is about 0.7.
most all participants performed such pairwise com-
parisons of systems.
A sentence-by-sentence comparison is clearly not
the optimal operation for establishing novelty. Sen-
tences with a large amount of overlap can express
very different thoughts. In the extreme, a single
word change can reverse the meaning of two sen-
tences: accept and reject. This phenomenon led the
Tsinghua University group to remark, ?many sen-
tences with an overlap of nearly 1 are real novel
ones.? (Ru et al, 2004).
On the other hand, it?s not hard to find cases where
realizations of equivalent statements take many dif-
ferent surface forms ? with different choices of
words and different syntactic structures. The data in
the Novelty Task is drawn from three news services
and clustered into fairly cohesive sets. The news
writers consciously try to avoid echoing each other,
and over time, echoing themselves. Sentences such
as these have low word overlap, but are not novel.
For this reason, we turned to a strategy of classifying
each sentence Si against the cumulative background
of all the words in all preceding sentences S1...i?1.
3 System
The system described in this paper was built with the
Novelty Track in mind. The goal was to look at ways
to consider longer spans of text than a sentence, and
to avoid sentence by sentence comparisons.
In the Novelty track, the relevant sentences are
presented in natural order, i.e. by the date of the
document they came from, and then by their loca-
tion in the document. The key characteristics of our
program include:
? For each relevant sentence, our program cal-
culates a sum of novel terms, which are terms
that have not been previously seen. The terms
are weighted according to their category, like
person, location, common noun or verb. The
weights are learned automatically.
? For the entire set, the program maintains a fo-
cus variable, which indicates whether the pre-
vious sentence is novel or old. Thresholds de-
termine whether to continue or shift the focus.
These are also learned automatically.
718
All input documents are fed in parallel into a
named-entity recognizer, which marks persons, or-
ganizations, locations, part-of-speech tags for com-
mon nouns, and into a finite-state parser, which is
used only to identify sentences beginning with sub-
ject pronouns. The output from the two preprocess-
ing modules are merged and sent to the classifier.
The classifier reads a configuration file that con-
tains a set of weights that were learned on the 2003
Novelty Track data to apply to different classes of
words that have not been previously seen.
For each sentence, the system adds up the amount
of novelty from the weighted terms in a sentence and
compares that to a learned threshold; it classifies the
sentence as novel if it exceeds the threshold. It also
stores the classification in a focus variable. If the
novelty threshold is not met, the system performs a
series of tests described below, and possibly classi-
fies some sentences with few content words as novel,
depending on the status of the focus variable. Our
algorithm enumerates all cases of changes in focus,
and tests these in the order that allows the system
to make the decision it can be most confident about
first. Thus, when we find a named entity new to the
discussion, we can be pretty sure that we have found
a novel sentence. We can classify that sentence as
new without regard to what preceded it. But, when
we find a sentence devoid of high-content words,
like ?She said the idea sounded good,? the system
uses the classification of the previous sentence. If
the antecedents to she or idea are novel, then this
sentence must also be novel. The series of learned
thresholds are imposed in a cascade to maximize the
number of correct decisions over the training cases,
in hopes the values will also cover unseen cases.
Thus, the classifier puts each sentence through the
tests below, using the learned thresholds and weights
described in Section 3.1. If any test succeeds, the
system goes on to the next sentence.
1. If there is a sufficient concentration of novel
words, classify the sentence as novel A suffi-
cient concentration occurs when the sum of the
weights of the novel content words (including
named entities) exceeds a threshold, Tnovel. If
the previous focus was old, this indicates the
focus has shifted to a novel segment.
2. If there is a lack of novel words, classify the
sentence as old This is computed by compar-
ing the sum of the weights of the already-seen
content words to a separate threshold, Told. If
the previous focus was novel, this means the
focus has shifted to an old segment.
3. For any remaining sentences, the classification
is based on context:
(a) If the sentence does not have a sufficient
number of content words, use the classifi-
cation in the focus variable. This adds the
sums of both new and old content words
and compares that to a threshold, Tkeep.
(b) If the first noun phrase is a third person
personal pronoun, use the classification in
the focus variable. Pronouns are known
to signal that the same focus continues
(Grosz and Sidner, 1986).
(c) If the sentence has not met any of the
above tests but has a minimum number of
content words, shift the focus. If all tests
above fail and there are a minimum num-
ber of content words, with a sum of Tshift
shift the focus.
4. Default This rarely occurs but the default is to
continue the focus, whether novel or old.
We examined the 2003 Novelty Track data and
found that more than half the novel sentences ap-
pear in sequences of consecutive sentences (See Ta-
ble 1). This circumstance creates an opportunity to
make principled classifications on some sentences
that have few, if any, clearly novel words, but con-
tinue a new segment. The use of a focus variable
handles these cases.
3.1 Learning
In all, the system uses 11 real-valued parameters,
weights and thresholds, and we wanted to learn op-
timal values for these. In particular, we wanted to be
able to target either high recall or high precision, As
we noted above, precision was much more difficult,
and for a summarization task, much more important.
To learn the optimal values for the parameters, we
opted to use an ad hoc algorithm. The main advan-
tage in doing so was when considering instance i,
the program can reference the classifications made
719
Length of Run Count
1 1338
2 421
3 132
4 72
5 43
6 22
7 11
8 2
9 3
10 3
11 2
12 2
15 2
17 1
Table 1: Novelty often comes in bursts. This table
shows that 1,338 of the novel sentences in the 2003
evaluation were singletons, and not a part of a run of
novel sentences. Meanwhile, 1,526 of the sentences
were part of runs of 2, 3 or 4 sentences.
for instance i ? 1, i ? 2, and possibly all the way
back to instance 1, because the classification for in-
stance i partly depends on the classification of pre-
vious instances. Not only do many standard super-
vised learning methods assume conditional indepen-
dence, but they also do not provide access to the on-
line classifications during learning. We constructed
a randomized hill-climbing. The learner is struc-
tured like a neural net, but the weight adjustments
are chosen at random as they are in genetic algo-
rithms (See Figure 1). The evaluation, or fitness
function, is the Novelty Track score itself, and the
training data was the 2003 Novelty Track data.
Changes to the hypothesis are selected at random
and evaluated. If the change does not hurt results, it
is accepted. Otherwise the program backtracks and
chooses another weight to update. We required the
new configuration to produce a score greater than or
equal to the previous one before we accepted it. The
choice of which weight to update is made at ran-
dom, in an effort to avoid local minima in the search
space, but with an important restriction: the previous
n choices are kept in a history list, which is checked
to avoid re-use. This list is updated at each iteration.
The configurations usually converge well within 100
iterations.
1. Initialize weights, history
Weights take random values
2. Run the system using current weight set
3. If current score >= previous best
Update previous best
4. Otherwise
Undo move
5. Update history
6. Choose next weight to change
7. Go to step 2
Figure 1: The learning algorithm uses a randomized
hill climbing approach with backtracking
3.2 Bias Adjustment
In training on the 2003 data, the biggest problem
was to find a way to deal with the large percentage
of novel sentences. About 65% of the instances are
positive, so that a random system achieves a rela-
tively high F-measure by increasing the number of
sentences it calls novel ? until recall reaches 1.0.
Another strategy would be to choose only the sen-
tences in the first document, achieving a high pre-
cision ? more than 90% of the relevant sentences in
the first document for each topic were called novel.
In the Novelty Track the F-measure was set to
give equal weight to precision and recall, but we
wanted to be able to coax the learner to give greater
weight to either precision or by adjusting the F-
measure computation:
F = 1?
prec +
(1??)
recall
? is a number between 0 and 1. The closer it gets
to 1, the more the formula favors precision.
We chose whether to emphasize precision or re-
call by altering the value of ?. At the most extreme,
we set ? at 0.9 for the largest emphasis on precision.
When emphasizing recall, we left ? at 0.5.
The design was motivated by the need to explore
the problem more fully and inform the algorithm for
deciding novelty as much as to find optimal param-
eters for the values. Thus, we wanted to be able
to record all the steps the learner made through the
search space, and to save the intermediate states. At
times, the learner would settle into a configuration
720
that produced a trivial solution, and we could choose
one of the intermediate configurations that produced
a more reasonable score.
3.3 Vector-Space Module
In addition to the system which integrates novel
word features with focus tracking, we also imple-
mented a vector-space approach as a baseline ? the
Cosine run. We tested the vector-space system alone
to contrast it with the focus system, but we also
tested a version which integrated the vector-space
system with the focus system.
Our vector-space module assigns all non-stop-
words a value of 1, and uses the cosine distance met-
ric to compute similarity.
Cos(u, v) = u ? v?u? ? ?v?
and
Novel(si)
?
?
?
?
?
True if Cos(si, sj) < T,
for j = 1 . . . i ? 1
False otherwise
As each sentence is scanned, its similarity is com-
puted with all previous sentences and the maximum
similarity is compared to a threshold T . If that max-
imum exceeds T , it is considered novel. We chose
the value of T after trials on the 2003 Novelty Track
data. It was set at 0.385, resulting in a balanced sys-
tem that matched the results of one of the strongest
performers at the TREC evaluations that year.
On the 2003 data, when we set T at .9, we found
that we had a precision of .71 and a recall of 0.98,
indicating that about 6% of the sentences were quite
similar to some preceding sentence (See Figure 2).
After that, each point of precision was very costly in
terms of recall. Our experience was mirrored by the
participants at TREC 2003 and again at TREC 2004.
We considered this vector-space model to be our
baseline. We also tried it in combination with the
Recall run explained above. Because both the Re-
call and Cosine runs produced a relatively large out-
put and because they used different methods, we
thought the intersection would result in higher pre-
cision, though with some loss of recall.
In practice, the range of recall was much greater
than precision. Judging from the experiences of the
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
Precision
Precision, recall tradeoff
Cos <= .10
Cos <= .20
Cos <= .30
Cos <= .40
Cos > .40
(Novelty = 1 - Cos)
Figure 2: The precision and recall scores of a
vector-space model with cosine similarity at differ-
ent thresholds, on the TREC 2003 data. Making the
test for novelty stricter fails to improve precision but
has a drastic effect on recall.
participants at TREC and our own exploratory ex-
periments, it was difficult to push precision above
0.80 with the TREC 2003 data, and above 0.50 with
the TREC 2004 data.
4 Experiments
4.1 Results from TREC 2004
Our results are encouraging, especially since the
configurations that were oriented toward higher pre-
cision, indeed, achieved the best precision scores
in the evaluation, with our best precision run about
20% higher in precision than the best of all the runs
by other groups (See Figure 3.) Meanwhile, our
recall-oriented run was one of eight runs that were in
a virtual tie for achieving the top f-measure. These
eight runs were within 0.01 of one another in the
measure.
Our five submitted runs were:
Prec1 aimed at moderately high precision, with rea-
sonable recall.
Prec2 aimed at high precision, with little attention
to recall.
Recall weighted precision and recall equally.
Cosine a baseline of a standard vector-space model
with a cosine similarity metric.
721
Figure 3: The graph shows all 54 submission in
Task 2 for the Novelty Track, with our five submis-
sions labeled. Our precision-oriented runs were well
ahead of all others in precision, while our recall-
oriented run was in a large group that reached about
0.5 precision with relatively high recall.
Combo a composite submission using the intersec-
tion of Recall and Cosine.
Table 2 shows the numbers of our performance of
our five submissions. Prec1 had an F-score close
to the average of 0.577 for all systems, while Prec2
was 50% ahead of random selection in accuracy.
Both our Combo system and our baseline Cosine
were above average in F-measure. Our emphasis on
precision is justified in a number of ways, although
the official yardstick was the F-measure.
An analysis of the system?s behavior under the
different parameters showed that the precision-
oriented runs, in particular Prec1, valued verbs and
common nouns more than named entities in decid-
ing novelty. The precision-oriented runs also bene-
fited more from the focus variable, with their scores
about 5% higher in terms of F-measure than they
were without it. The pronoun test, however, was
rarely used, firing less than 1% of the time.
We note that we are developing novelty detection
for summarization, where compression of the report
is valuable. Table 2 shows the lengths of our re-
turns. It is impossible to compare these precisely
with other systems, because the averages given by
NIST are averages of the scores for each of the 50
sets, and we do not have the breakdown of the num-
bers by set for any submissions but our own. How-
ever, we can estimate the size of the other output by
considering average precision and recall as if they
were computed over the total number of sentences in
all 50 sets. This computation shows an average out-
put for all participants of about 6,500 sentences and
a median of 6,981 ? out of a total of 8,343 sentences.
However, this total includes some amount of header
material, not only the headline, but the document ID
and other identifiers, the date and some shorthand
messages from the wire services to its clients. In
addition, a number of the sets had near perfect du-
plicate articles. This is in sharp contrast with typi-
cal summaries. At the 2004 Document Understand-
ing Conference, the typical input cluster contained
more than 4,000 words, and the task required that
this be reduced to 100 words. We contend there is
little value in a system that does no more than weed
out very few sentences, even though they might have
achieved high F-measures.
Second, our experience, and the results of other
groups, shows that high precision is harder than high
recall. In all three years of the Novelty Track, pre-
cision scores tended to hover in a narrow band just
above what one would get by mechanically labeling
all sentences as novel.
5 Conclusion
The success of our use of context in the TREC
Novelty Track led us to incorporate the idea into a
larger system. This system identifies clauses within
sentences that express new information and tries to
identify semantic equivalents. It is being developed
as part of a multi-document summarizer that pro-
duces topical updates for users.
In addition, the work here suggests three direc-
tions for future work:
? Adapt the features used here to some of the
newer probabilistic formalisms, like condi-
tional random fields.
? Try full segmentation of the input documents
rather than treat the sentences as a sequence.
? Try to identify all nominal references to canon-
ical forms.
With this experimental system, we obtained the
the top precision scores in the Novelty Track, and
722
Run-Id Precision Recall F-meas Output length
Prec1 0.57 0.58 0.562 3276
Prec2 0.61 0.45 0.506 2372
Recall 0.51 0.82 0.611 5603
Cosine 0.49 0.81 0.599 5537
Combo 0.53 0.73 0.598 4578
Choose All 0.41 1.000 0.581 8343
Average All Runs 0.46 0.86 0.577 6500
Table 2: Comparison of results of our five runs, compared to a random selection of sentences, and the overall
average F-scores by all 55 submissions.
we obtained the program settings to do this auto-
matically. High precision is very difficult to obtain,
and every point in precision costs too much in recall.
Further exploration is needed to determine whether
linguistic knowledge will help, and whether state-
of-the-art tools are powerful enough to improve per-
formance.
Beyond new-information detection, the idea of
tracking context with a surface means like the focus
variable is worth exploring in other tasks, including
summarization and question-answering.
References
Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fer-
nando Diaz, Leah Larkey, Xiaoyan Li, Donald Met-
zler, Mark D. Smucker, Trevor Strohman, Howard Tur-
tle, and Courtney Wade. 2004. Umass at trec 2004:
Notebook. In The Thirteenth Text Retrieval Confer-
ence (TREC 2004) Notebook.
James Allan, Courtney Wade, and Alvaro Bolivar. 2003.
Retrieval and novelty detection at the sentence level.
In Proceedings of the ACM SIGIR conference on re-
search and development in information retrieval.
Stephen Blott, Oisin Boydell, Fabrice Camous, Paul Fer-
guson, Georgina Gaughan, Cathal Gurrin, Noel Mur-
phy, Noel O?Connor, Alan F. Smeaton, Barry Smyth,
and Peter Wilkins. 2004. Experiments in terabyte
searching, genomic retrieval and novelty detection for
trec-2004. In The Thirteenth Text Retrieval Confer-
ence (TREC 2004) Notebook.
John M Conroy, Daniel M. Dunlavy, and Dianne P.
O?Leary. 2003. From trec to duc to trec again. In
TREC Notebook Proceedings.
John M. Conroy. 2004. A hidden markov model for
trec?s novelty task. In The Thirteenth Text Retrieval
Conference (TREC 2004) Notebook.
David Eichmann, Padmini Srinivasan, Marc Light,
Hudong Wang, Xin Ying Qiu, Robert J. Arens, and
Aditya Sehgal. 2003. Experiments in novelty, genes
and questions at the university of iowa. In TREC Note-
book Proceedings.
David Eichmann, Yi Zhang, Shannon Bradshaw,
Xin Ying Qiu, Li Zhou, Padmini Srinivasan,
Aditya Kumar Sehgal, and Hudon Wong. 2004. Nov-
elty, question answering and genomics: The univer-
sity of iowa response. In The Thirteenth Text Retrieval
Conference (TREC 2004) Notebook.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. In Proceedings of ANLP/NAACL-
2000 Workshop on Automatic Summarization.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention, and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Srikanth Kallurkar, Yongmei Shi, R. Scott Cost, Charles
Nicholas, Akshay Java, Christopher James, Sowjanya
Rajavaram, Vishal Shanbhag, Sachin Bhatkar, and
Drew Ogle. 2003. Umbc at trec 12. In TREC Note-
book Proceedings.
R. Ohgaya, A. Shimmura, and T. Takagi. 2003. Meiji
university web and novelty track experiments at trec
2003. In TREC Notebook Proceedings.
Liyun Ru, Le Zhao, Min Zhang, and Shaoping Ma. 2004.
Improved feature selection and redundancy computing
? thuir at trec 2004 novelty track. In The Thirteenth
Text Retrieval Conference (TREC 2004) Notebook.
Ian Soboroff. 2004. Draft overview of the trec 2004 nov-
elty track. In The Thirteenth Text Retrieval Conference
(TREC 2004) Notebook.
Jian Sun, Wenfeng pan, Huaping Zhang, Zhe Yang, Bin
Wang, Gang Zhang, and Xueqi Cheng. 2003. Trec-
2003 novelty and web track at ict. In TREC Notebook
Proceedings.
723
Learning Methods to Combine Linguistic 
Indicators: Improving Aspectual 
Classification and Revealing Linguistic 
Insights 
Eric V. Siegel* 
Columbia University 
Kathleen R. McKeown* 
Columbia University 
Aspectual classification maps verbs to a small set of primitive categories in order to reason about 
time. This classification isnecessary for interpreting temporal modifiers and assessing temporal 
relationships, and is therefore a required component for many natural anguage applications. 
A verb's aspectual category can be predicted by co-occurrence fr quencies between the verb 
and certain linguistic modifiers. These frequency measures, called linguistic indicators, are chosen 
by linguistic insights. However, linguistic indicators used in isolation are predictively incomplete, 
and are therefore insufficient when used individually. 
In this article, we compare three supervised machine learning methods for combining multiple 
linguistic indicators for aspectual c assification: decision trees, genetic programming, and logistic 
regression. A set of 14 indicators are combined for classification according to two aspectual distinc- 
tions. This approach improves the classification performance for both distinctions, as evaluated 
over unrestricted sets of verbs occurring across two corpora. This demonstrates the effectiveness 
of the linguistic indicators and provides amuch-needed full-scale method for automatic aspectual 
classification. Moreover, the models resulting from learning reveal several linguistic insights that 
are relevant o aspectual classification. We also compare supervised learning methods with an 
unsupervised method for this task. 
1. Introduction 
Aspectual classification maps clauses (e.g., simple sentences) to a small set of cate- 
gories in order to reason about ime. For example, events, such as, You called your father, 
are distinguished from states, such as, You resemble your father. The ability to distinguish 
stative clauses from event clauses is a fundamental component of natural language 
understanding. These two high-level categories correspond to fundamental distinc- 
tions in many domains, including the distinctions between diagnosis and procedure 
in the medical domain, and between analysis and activity in the financial domain. 
Stativity is the first high-level distinction made when defining the aspectual class 
of a clause. Events are further distinguished according to completedness ( ometimes 
called telicity), which determines whether an event reaches a culmination or comple- 
tion point at which a new state is introduced. For example, I made afire is culminated, 
since a new state is introduced--something is made, whereas I gazed at the sunset is 
nonculminated. 
* Computer Science Dept., 1214 Amsterdam Ave., New York NY 10027. E-mail: evs@cs.columbia.edu 
t Computer Science Dept., 1214 Amsterdam Ave., New York, NY 10027. E-mail: kathy@cs.columbia.edu 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 4 
Aspectual classification is necessary for interpreting temporal modifiers and as- 
sessing temporal entailments (Moens and Steedman 1988; Dorr 1992; Klavans 1994) 
and is therefore a required component for applications that perform certain natural 
language interpretation, generation, summarization, information retrieval, and ma- 
chine translation tasks. Each of these applications requires the ability to reason about 
time. 
A verb's aspectual category can be predicted by co-occurrence fr quencies between 
the verb and linguistic phenomena such as the progressive tense and certain temporal 
modifiers (Klavans and Chodorow 1992). These frequency measures are called linguis- 
tic indicators. The choice of indicators is guided by linguistic insights that describe 
how the aspectual category of a clause is constrained by the presence of these mod- 
ifiers. For example, an event can be placed in the progressive, as in, She was jogging, 
but many stative clauses cannot, e.g., *She was resembling her father (Dowty 1979). One 
advantage of linguistic indicators is that they can be measured automatically. 
However, individual linguistic indicators are predictively incomplete, and are 
therefore insufficient when used in isolation. As demonstrated mpirically in this arti- 
cle, individual linguistic indicators uffer from limited classification performance due 
to several linguistic and pragmatic factors. For example, some indicators were not mo- 
tivated by specific linguistic insights. However, linguistic indicators have the potential 
to interact and supplement one another, so it would be beneficial to combine them 
systematically. 
In this article, we compare three supervised machine learning methods for com- 
bining multiple linguistic indicators for aspectual classification: decision trees, genetic 
programming, and logistic regression. A set of 14 indicators are combined, first for 
classification according to stativity, and then for classification according to complet- 
edness. This approach realizes the potential of linguistic indicators, improving classi- 
fication performance over a baseline method for both tasks with minimal overfitting, 
as evaluated over an unrestricted set of verbs occurring in two corpora. This serves 
to demonstrate he effectiveness of these linguistic indicators and thus provides a 
much-needed full-scale, expandable method for automatic aspectual classification. 
The results of learning are linguistically viable in two respects. First, learning au- 
tomatically produces models that are specialized for different aspectual distinctions; 
the same set of 14 indicators are combined in different ways according to which clas- 
sification problem is targeted. Second, inspecting the models resulting from learning 
revealed linguistic insights that are relevant to aspectual classification. 
We also evaluate an unsupervised method for this task. This method uses co- 
occurrence statistics to group verbs according to meaning. Although this method 
groups verbs generically and is not designed to distinguish according to aspectual 
class in particular, we show that the results do distinguish verbs according to stativity. 
The next two sections of this article describe aspectual classification and linguistic 
indicators. Section 4 describes the three supervised learning methods employed to 
combine linguistic indicators for aspectual classification. Section 5 gives results in 
terms of classification performance and resulting linguistic insights, comparing these 
results, across classification tasks, to baseline methods. Section 6 describes experiments 
with an unsupervised approach. Finally, Sections 7, 8, and 9 survey related work, 
describe future work, and present conclusions. 
2. Aspect in Natural Language 
Because, in general, the sequential order of clauses is not enough to determine the 
underlying chronological order, aspectual classification is required for interpreting 
596 
Siegel and McKeown Improving Aspectual Classification 
Table 1 
Aspectual classes. This table is adapted from Moens and Steedman (1988, p. 17). 
Culminated 
Nonculminated 
EVENTS 
punctual extended 
CULMINATION CULMINATED 
PROCESS 
recognize build a house 
POINT PROCESS 
hiccup run, swim, walk 
STATES 
understand 
even the simplest narratives in natural language. For example, consider: 
(1) Sue mentioned Miami (event). Jim cringed (event). 
In this case, the first sentence describes an event that takes place before the event 
described by the second sentence. However, in 
(2) Sue mentioned Miami (event). Jim already knew (state). 
the second sentence describes a state, which begins before the event described by the 
first sentence. 
Aspectual classification is also a necessary prerequisite for interpreting certain 
adverbial adjuncts, as well as identifying temporal constraints between sentences in 
a discourse (Moens and Steedman 1988; Dorr 1992; Klavans 1994). In addition, it 
is crucial for lexical choice and tense selection in machine translation (Moens and 
Steedman 1988; Klavans and Chodorow 1992; Klavans 1994; Dorr 1992). 
Table 1 sun~narizes the three aspectual distinctions, which compose five aspec- 
tual categories. In addition to the two distinctions described in the previous ection, 
atomicity distinguishes punctual events (e.g., She noticed the picture on the wall) from 
extended events, which have a time duration (e.g., She ran to the store). Therefore, four 
classes of events are derived: culmination, culminated process, process, and point. 
These aspectual distinctions are motivated by a series of syntactic and entailment 
constraints described in the first three subsections below. Further cognitive and philo- 
sophical rationales behind these semantic distinctions are surveyed by Siegel (1998b). 
First we describe aspectual constraints that linguistically motivate the design of sev- 
eral of the linguistic indicators. Next we describe an array of semantic entailments and 
temporal constraints that can be put to use by an understanding system once input 
clauses have been aspectually classified. Then we describe how aspect influences the 
interpretation of temporal connectives and modifiers. Aspectual transformations are 
described, and we introduce the concept of a clause's fundamental aspectual category. 
Finally, we describe several natural anguage applications that require an aspectual 
classification component. 
2.1 Aspectual Markers and Constraints 
Certain features of a clause, such as the presence of adjuncts and tense, are constrained 
by and contribute to the aspectual class of the clause (Vendler 1967; Dowty 1979; 
Pustejovsky 1991; Passonneau 1988; Klavans 1994; Resnik 1996; Olsen and Resnik 
1997). Table 2 illustrates an array of linguistic constraints, as more comprehensively 
597 
Computational Linguistics Volume 26, Number 4 
Table 2 
Several aspectual markers and associated constraints on aspectual c ass. 
If a clause can occur: then it must be: 
with a temporal adverb (e.g., then) 
in progressive 
as a complement offorce~persuade 
after "What happened was..." 
with a duration in-PP (e.g., in an hour) 
in the perfect ense 
Event 
Extended Event 
Event 
Event 
Culminated Event 
Culminated Event or State 
summarized by Klavans (1994) and Siegel (1998b). Each entry in this table describes 
an aspectual marker and the constraints on the aspectual category of any clause that 
appears with that marker. For example, a clause must be extended to appear in the 
progressive tense, e.g., 
(3) He was prospering in India extended event), 
which contrasts with, 
(4) *You were noticing that I can hardly be blamed ... (atomic event)) 
As a second example, since the perfect ense requires that the clause it occurs in 
must entail a consequent s ate, an event must be culminated to appear in the perfect 
tense. For example, 
(5) Thrasymachus has made an attempt to get the argument into his own 
hands (culminated event), 
contrasts with, 
(6) *He has cowered own in a paralysis of fear (nonculminated vent). 
2.2 Aspectua l  Enta i lments  
Table 3 lists several aspectual entailments. A more comprehensive list can be found 
in Klavans (1994) or Siegel (1998b). Each entry in this table describes a linguistic 
phenomenon, a resulting entailment, and the constraints on aspectual class that apply 
if the resulting entailment holds. For example, the simple present reading of an event, 
e.g., He jogs, denotes the habi tua l  reading, i.e., every day, whereas the simple present 
reading of a state, e.g., He appears healthy, entails at the moment. 
These entailments serve two purposes: They further validate the three aspectual 
distinctions, and they illustrate an array of inferences that can be made by an under- 
standing system. However, these inferences can only be made after identifying the 
aspectual category of input clauses. 
1 These example sentences are modifications of samples from the corpus of novels described below. 
598 
Siegel and McKeown Improving Aspectual Classification 
Table 3 
Several aspectual entailments. 
If a clause occurring: necessarily entails: then it must be: 
in past progressive t nse 
as argument ofstopped 
in simple present tense 
past tense reading 
past tense reading 
the habitual reading 
Nonculminated Event 
Nonculminated Event or State 
Event 
2.3 Interpret ing Tempora l  Connect ives  and Modi f iers  
Several researchers have developed models that incorporate aspectual class to assess 
temporal constraints between connected clauses (Hwang and Schubert 1991; Schubert 
and Hwang 1990; Dorr 1992; Passonneau 1988; Moens and Steedman 1988; Hitzeman, 
Moens, and Grover 1994). For example, stativity must be identified to detect emporal 
constraints between clauses connected with when. For example, in interpreting, 
(7) She had good strength when objectively tested. 2 
the have state began before or at the beginning of the test event, and ended after or at 
the end of the test event: 
have 
I ? I 
t2z2_  
However, in interpreting, 
(8) Phototherapy was d iscont inued when the bilirubin came down to 13. 
the discontinue vent began at the end of the come event: 
come 
I I 
discont inue 
I I 
Such models also predict temporal relations between clauses combined with other 
connectives such as before, after, and until. 
Certain temporal modifiers are disambiguated with aspectual class. For example, 
for an hour can denote the duration of a nonculminated vent, as in, Igazed at the sunset 
for an hour. In this case, an hour is the duration of the gazing event. However, when 
applied to a culminated event, it denotes the duration of the resulting state, as in, I 
left the room for an hour. In this case, an hour is not the duration of the leaving event, 
but, rather, the duration of what resulted from leaving, i.e., being gone. 
2 These xamples ofwhen come from the corpus of medical discharge summaries described below. 
599 
Computational Linguistics Volume 26, Number 4 
2.4 Aspectual Transformations and Coercion 
Several aspectual markers uch as those shown in Table 2 transform the aspectual c ass 
of the clause they modify. For example, a durationfor-PP, e.g., for 10 minutes, denotes 
the time duration of a (nonculminated) process, resulting in a culminated process, e.g., 
(9) I stared at it (process). 
(10) I stared at it for 10 minutes (culminated process). 
Some aspectual auxiliaries also perform an aspectual transformation of the clause 
they modify, e.g., 
(11) I finished staring at it (culminated process). 
Aspectual coercion, a second type of aspectual transformation, can take place 
when a clause is modified by an aspectual marker that violates an aspectual constraint 
(Moens and Steedman 1988; Pustejovsky 1991). In this case, an alternative interpre- 
tation of the clause is inferred which satisfies the aspectual constraint. For example, 
the progressive marker is constrained to appear with an extended event. Therefore, if
it appears with an atomic event, e.g., 
(12) He hiccupped (point), 
the event is transformed to an extended event, e.g., 
(13) He was hiccupping (process). 
in this case with the iterated reading of the clause (Moens and Steedman 1988). 
2.5 The First Problem: Fundamental Aspect 
We define fundamental aspectual class as the aspectual class of a clause before any 
aspectual transformations or coercions. That is, the fundamental spectual category 
is the category the clause would have if it were stripped of any and all aspectual 
markers that induce an aspectual transformation, as well as all components of the 
clause's pragmatic context that induce a transformation. Fundamental spectual c ass is 
therefore a function of the main verb and a select group of complements, asillustrated 
in the previous two subsections. It is the task of detecting fundamental spect hat we 
address in this article. As established by some previous work in linguistics, adjuncts 
are to be handled separately from other clausal constituents in assessing aspectual 
class (Pustejovsky 1995). 
An understanding system can recognize the aspectual transformations that have 
affected a clause only after establishing the clause's fundamental spectual category. 
Linguistic models motivate the division between a module that first detects fundamen- 
tal aspect and a second that detects aspectual transformations (Hwang and Schubert 
1991; Schubert and Hwang 1990; Dorr 1992; Passonneau 1988; Moens and Steedman 
1988; Hitzeman, Moens, and Grover 1994). In principle, it is possible for this second 
module to detect aspectual transformations that apply to any input clause, indepen- 
dent of the manner in which the core constituents interact to produce its fundamental 
aspectual class. 
600 
Siegel and McKeown Improving Aspectual Classification 
2.6 Applications of Aspectual Classification 
Aspectual classification is a required component of applications that perform nat- 
ural language interpretation, atural language generation, summarization, informa- 
tion retrieval, and machine translation tasks (Moens and Steedman 1988; Klavans and 
Chodorow 1992; Klavans 1994; Dorr 1992; Wiebe et al 1997). These applications require 
the ability to reason about time, i.e., temporal reasoning. 
Assessing temporal relationships i a prerequisite for inferring sequences of med- 
ical procedures in medical domains. Many applications that process medical reports 
require aspectual classification because a patient's medical progress and history are 
established as a series of states and events that are temporally related. One task is 
to automatically complete a database ntry for the patient by processing a medical 
discharge summary detailing a patient's visit to the hospital. For example, consider 
the temporal relationship between the clauses connected with when in, 
(14) The small bowel became completely free when dissection was 
continued. 3 
In this case, the become culmination takes place at the onset of the continue process. 
However, in 
(15) The small bowel became completely free when dissection was 
performed. 
the become culmination takes place at the completion of the perform culminated process. 
Aspect is also crucial for tense selection in machine translation between certain 
pairs of languages because some languages have explicit perfective markers and oth- 
ers do not. The perfective marker is used in many languages, such as Bulgarian and 
Russian, to indicate completedness. Therefore, a system translating from a language 
without explicit perfective markers, such as English, to one with explicit perfective 
markers must first detect he aspectual category of an input phrase in order to deter- 
mine the form of the output (Stys 1991; Dorr 1992). Aspect is also required for lexical 
selection in machine translation since, for example, some languages, e.g., German and 
French, have different words for the two uses of for discussed previously in Section 2.3. 
Applications that incorporate aspect rely on the ability to first automatically iden- 
tify the aspectual category of a clause. For example, Passonneau (1998) describes an 
algorithm that depends on what is called lexical aspect, the aspectual information 
stored in the lexicon for each verb, and Dorr (1992) augments Jackendoff's lexical en- 
tries with aspectual information. Combining linguistic indicators with machine learn- 
ing automatically produces domain-specialized aspectual lexicons. 
3. Linguistic Indicators 
Aspectually categorizing verbs is the first step towards aspectually classifying clauses, 
since many clauses in certain domains can be categorized based on their main verb 
only (Siegel 1997, 1998b, 1999). However, the most frequent category of a verb is 
often domain dependent, so it is necessary to perform a specialized analysis for each 
domain. 
3 These xample sentences are modifications of samples from the corpus of medical discharge summaries 
described below. 
601 
Computational Linguistics Volume 26, Number 4 
Table 4 
Fourteen linguistic indicators evaluated for aspectual c assification. 
Linguistic Indicator Example Clause 
frequency 
not or never 
temporal adverb 
no subject 
past/pres participle 
duration in-PP 
perfect 
present ense 
progressive 
manner adverb 
evaluation adverb 
past tense 
duration for-PP 
continuous adverb 
(not applicable) 
She can not explain why. 
I saw to it then. 
He was admitted to the hospital. 
... blood pressure going up. 
She built it in an hour. 
They have landed. 
I am happy. 
I am behaving myself. 
She studied iligently. 
They performed horribly. 
I was happy. 
I sang for ten minutes. 
She will live indefinitely. 
Naturally occurring text contains vast amounts of information pertaining to as- 
pectual classification encoded by aspectual markers that have associated aspectual 
constraints. However, the best way to make use of these markers is not obvious. In 
general, while the presence of a marker in a particular clause indicates a constraint on 
the aspectual class of the clause, the absence thereof does not place any constraint. 
Therefore, as with most statistical methods for natural anguage, the linguistic 
constraints associated with markers are best exploited by a system that measures co- 
occurrence frequencies. In particular, we measure the frequencies ofaspectual markers 
across verbs. This way, the aspectual tendencies ofverbs are measured. These tenden- 
cies are likely to correlate with aspectual class (Klavans and Chodorow 1992). For 
example, a verb that appears more frequently in the progressive is more likely to 
describe an event. The co-occurrence frequency of a linguistic marker is a linguistic 
indicator. 
The first column of Table 4 lists the 14 linguistic indicators evaluated to classify 
verbs. Each indicator has a unique value for each verb. The first indicator, frequency, 
is simply the frequency with which each verb occurs over the entire corpus. The 
remaining 13 indicators measure how frequently each verb occurs in a clause with 
a linguistic marker listed in Table 4. For example, the next three indicators listed 
measure the frequency with which verbs (1) are modified by not or never, (2) are 
modified by a temporal adverb such as then or frequently, and (3) have no deep subject 
(e.g., passivized phrases uch as, She was admitted to the hospital). 
Nine of these indicators measure the frequencies of aspectual markers, each of 
which have linguistic onstraints: perfect, progressive, duration in-PP, durationfor-PP, 
no subject, and four adverb groups. The remaining five indicators were discovered ur- 
ing the course of this research. Further details regarding the measurement of hese indi- 
cators, and the linguistic onstraints hat motivate them, can be found in Siegel (1998b). 
Linguistic indicators are measured over corpora automatically. To do this, the 
automatic dentification of individual constituents within a clause is required to detect 
the presence of aspectual markers and to identify the main verb of each clause. We 
employ the English Slot Grammar (ESG) parser (McCord 1990), which has previously 
been used on corpora to accumulate aspectual data (Klavans and Chodorow 1992). 
ESG is particularly attractive for this task since its output describes a clause's deep 
roles, detecting, for example, the deep subject and object of a passivized phrase. 
602 
Siegel and McKeown Improving Aspectual Classification 
4. Combining Linguistic Indicators with Machine Learning 
There are several reasons to expect superior classification performance when employ- 
ing multiple linguistic indicators in combination rather than using them individually. 
While individual indicators have predictive value, they are predictively incomplete. 
This incompleteness has been illustrated empirically by showing that some indicators 
help for only a subset of verbs (Siegel 1998b). Such incompleteness i  due in part to 
sparsity and noise of data when computing indicator values over a corpus with lim- 
ited size and some parsing errors. However, this incompleteness i  also a consequence 
of the linguistic haracteristics of various indicators. For example: 
? While the progressive indicator is linguistically linked to extendedness, it 
is only indirectly linked to completedness. It may be useful for 
predicting whether a verb is culminated or nonculminated due to the 
fact that nonextended (i.e., atomic) verbs are more likely to be 
culminated than extended, i.e., points are rare. 
? Many location verbs can appear in the progressive, ven in their stative 
sense, e.g., The book was lying on the shelf. 
? Some aspectual markers uch as the pseudocleft and many manner 
adverbs test for intentional events in particular (not all events in general), 
and therefore are not compatible with all events, e.g., *I died diligently. 
? Aspectual coercion such as iteration can allow a punctual event o 
appear in the progressive, .g. She was sneezing for a week 
(point ~ process ~ culminated process) 4 (Moens and Steedman 1988). 
? The predictive power of some indicators is uncertain, since several 
measure phenomena that are not linguistically constrained by any 
aspectual category, e.g., the present ense, durative for-PPs, frequency 
and not~never indicators. 
Therefore, the predictive power of individual linguistic indicators is incomplete; 
only the subset of verbs that adhere to the respective constraints or trends can be 
correctly classified. Such incomplete indicators may complement one another when 
placed in combination. Our goal is to take full advantage of the complete range of 
indicators by coordinating and combining them. 
Machine learning methods can be employed to automatically generate a model 
that will combine indicator values. Figure 1 shows a system overview for this process 
(with additional details that are addressed below in Section 5.1.1). This diagram out- 
lines a generic system that combines numerical indicators with machine learning for a 
classification problem, in natural language understanding or otherwise. Indicators are 
computed over an automatically parsed corpus. Then, in the Combine Indicators tage, 
supervised training cases are used to automatically generate a model (Classification 
Method) with supervised machine learning. This method (the hypothesis) inputs in- 
dicator values and outputs the aspectual class. The hypothesis then evaluated over 
unseen supervised test cases. 
4 In this example, for a week requires an extended vent, hus the first coercion. However, this phrase also 
makes an event culminated, thus the second transformation. 
603 
Computational Linguistics Volume 26, Number 4 
States vs. Events 
~ , 1 5 9 , 8 9 1  words 
Manual I. 1.863 clauses I Parser I ESG 
Classification I" 
( ,Test_)739 ( Training )739 t l Co.mp,t; i t  
\ \ / 
~)e(ISlOn I ree  
I I 
Figure 1 
System overview ith statistics of the medical discharge summary data for distinguishing 
according to stativity. 
There are five advantages to automating this process with machine learning: 
? The cost of manually generating a model, which is often prohibitive, is 
avoided. 
? Biases introduced by a human engineer are avoided. 
? Automated approaches are extensible to multiple natural anguage 
classification problems, across multiple domains and multiple languages. 
? Once a system has been trained to distinguish verbs by indicator values, 
it can automatically classify all the verbs that appear in a corpus, 
including unseen verbs that were not included in the supervised training 
sample. 
? Resulting models may reveal new linguistic insights. 
The remainder of this section describes the three supervised learning methods 
evaluated for combining linguistic indicators: logistic regression, decision tree induc- 
tion, and a genetic algorithm. At the end of this section, the designs of these three 
methods are compared. In the following section, the three are compared empirically: 
each method is evaluated for classification according to both stativity and completed- 
ness. 
604 
Siegel and McKeown Improving Aspectual Classification 
4.1 Logistic Regression 
As suggested by Klavans and Chodorow (1992), a weighted sum of multiple indica- 
tors that results in one "overall" indicator may provide an increase in classification 
performance. This method follows the intuition that each indicator correlates with the 
probability that a verb belongs in a certain class, but that each indicator has its own 
unique scale, polarity, and predictive significance and so must be weighted accord- 
ingly. 
For example, consider the problem of using in combination (only) two indicators, 
not~never and progressive, to determine the stativity of verbs in a corpus of medical 
reports. The former indicator may show higher values for stative verbs since diag- 
noses (i.e., states) are often ruled out in medical discharge surrunaries, e.g., "The patient 
was not hypertensive," but procedures (i.e., events) that were not done are not usually 
mentioned, e.g., "?An examination was not performed." The progressive indicator is lin- 
guistically predicted to show higher values for event verbs in general, so its polarity 
is the opposite of the not~never indicator. Furthermore, a certain group of stative verbs 
(including, e.g., sit, lay, and rest) can also occur in the progressive, so this indicator 
may be less powerful in its predictiveness. Therefore, the best overall indicator may 
result from adding the value of the not or never indicator, as multiplied by a negative 
weight, to the value of the progressive indicator, as multiplied by a positive weight of 
lesser magnitude. A detailed examination of the weights resulting from learning and 
their linguistic interpretation is described below in Section 5.1.4. 
This set of weights can be determined by a standard gradient descent algorithm 
(see, for example, Mitchell \[1997\]). However, the algorithm employed here is logistic 
regression (Sjantner and Duffy 1989), a popular technique for binary classification. 
This technique determines a set of weights for a linear model, which are applied in 
combination with a certain nonlinear model. In particular, the iterative reweighted 
least squares algorithm (Baker and Nelder 1989) is employed, and the inverse logic 
(nonlinear) function is applied. The Splus statistical package was used for the induction 
process. 
4.2 Decision Tree Induction 
Another method capable of modeling nonlinear elationships between indicators is a 
decision tree. An example tree is shown in Figure 2 (with additional details discussed 
in Section 5.1.4). Each internal node of a decision tree is a choice point, dividing an 
individual indicator into two ranges of possible values by way of a threshold. Each leaf 
node is labeled with a classification (e.g., state or event, in the case of the tree shown). 
In effect, this is simply a set of nested if-then-else statements. Given the set of indicator 
values corresponding to a verb, that verb's class is predicted by traversing the tree 
from the root node to a leaf as follows: at each node, the arc leading downward to 
the left or right is traversed according to the question posed about an indicator value 
at that node. When a leaf node is reached, its label is then taken to be the verb's 
classification. For example, if the frequency is less than 2,013, the arc to the left is 
traversed. Then, if the not~never indicator is greater than or equal to 3.48%, the arc to 
the right is traversed. Finally, if the frequency is greater than or equal to 314, the arc 
to the right is traversed, arriving at a leaf labeled, State. 
This representation e ables complex interactions between indicator values. In par- 
ticular, if one indicator can only help classify a proper subset of verbs, while another 
applies only to a subset hat is distinct but intersects with the first, certain ranges of 
indicator values may delimit verb groups for which the indicators complement one 
another. An example of such delimitation within a learned decision tree is illustrated 
below in Section 5.1.4. 
605 
Computational Linguistics Volume 26, Number 4 
< 2,013 
"Izot" < 
28/29 
temporal adv < 0.68%) ~, j requency < 
< 
4~'7488 42/~-~'61 19/19 
106/132 9/10 
Figure 2 
Top portion of decision tree automatically created to distinguish events from states. Leftward 
arcs are traversed when comparisons test true, rightward arcs when they test false. The values 
under each leaf indicate the number of correctly classified examples in the corresponding 
partition of training cases. The full tree has 59 nodes and achieves 93.9% accuracy over unseen 
test cases. 
Table 5 
Default decision tree induction parameters implemented by Splus. 
Minimum partition size before first split: 
Minimum partition size for additional splits: 
Node selection criterion: 
Node purity threshold: 
5 
10 
deviance = -2 times log-likelihood 
deviance < .01 
The most popular method of decision tree induction, which we employ here, is 
recursive partitioning (Quinlan 1986; Breiman et al 1984). This method "grows" a 
decision tree by expanding it from top to bottom. Initially, the tree has only one node, 
a leaf, corresponding to the entire set of training examples. Then, the following process 
is repeated: At each leaf node of the tree, an indicator and threshold are selected such 
that the examples are best distinguished according to aspectual class. This adds two 
leaves beneath the node, and distributes the examples to the new leaves accordingly. 
Table 5 shows the parameters used to control decision tree growth. The criterion 
optimized for each split is deviance, implemented as minus twice the log likelihood. 
The Splus statistical package was used for the induction process. We also compared 
these results to standard CART decision tree induction (Friedman 1977; Breiman et al 
1984). 
4.3 Genetic Programming 
An alternative method that enables arbitrary mathematical combinations of indicators 
is to generate ftmction trees that combine them. A popular method for generating 
606 
Siegel and McKeown Improving Aspectual Classification 
Table 6 
Applying genetic programming to induce a function tree that combines linguistic indicators. 
Objective: 
Terminal set: 
Function set: 
Training cases: 
Raw fitness: 
Parameters: 
Identification of best of run: 
Function tree to combine linguistic indicators 
14, corresponding tothe set of linguistic indicators. 
ADD, MULTIPLY, and DIVIDE 
739 (stativity) or 307 (completedness) verb instances. 
Accuracy over training cases, when best threshold is selected. 
Number of generates =50,000, population size = 500, 
5-member tournament selection, steady state population 
(Syswerda 1989). 
Highest raw fitness. 
such function trees is a genetic algorithm (GA) (Holland 1975; Goldberg 1989), which 
is modeled after population genetics and natural selection. The use of GAs to generate 
function trees (Cramer 1985; Koza 1992) is often called genetic programming (GP). 
Inspired by Darwinian survival of the fittest, the GA works with a pool of initially 
random hypotheses (in this case, function trees), stochastically performing enetic 
recombination a d mutation to propagate or create better individuals, which replace 
old or less good individuals. Recombination between function trees usually consists 
of selecting a subtree from each individual, and swapping them, thereby creating two 
new individuals (Cramer 1985; Koza 1992). For random mutation, a randomly chosen 
subtree can be replaced by a new randomly created subtree (Koza 1992). Because 
the genetic algorithm is stochastic, each run may produce a different function tree. 
Therefore, performance is evaluated over the models produced by multiple runs. 
The function trees are generated from a set of 17 primitives: the binary functions 
ADD, MULTIPLY, and DIVIDE, and 14 terminals corresponding to the 14 indicators 
listed in Table 4, which are occurrence frequencies. The GA parameters are shown in 
Table 6. 
This representation e ables trategic ombinations of indicator values that are 
mathematical, s opposed to logical, manipulations. For example, two indicators that 
are high in predictiveness can be multiplied together, and perhaps added to an indi- 
cator with complementary but less reliable predictiveness. 
The set of primitives was established empirically; other primitives uch as con- 
ditional functions, subtraction, and random constants failed to improve performance. 
The polarities for several indicators were reversed according to the polarities of the 
weights established by logistic regression for stativity. Runs of the GA maintain a 
population size of 500 and end after 50,000 new individuals have been evaluated. 
A threshold must be selected for both logistic and function tree combinations of
indicators o overall outputs can be discriminated to maximize classification perfor- 
mance. For both methods, the threshold is established over the training set and frozen 
for evaluation over the test set. 
4.4 Selecting and Comparing Learning Methods 
The use of machine learning techniques to combine numerical indicators for classi- 
fication problems in general is a well-established practice and includes work with 
decision trees (Fayyad and Irani 1992), logistic regression (Sjantner and Duffy 1989), 
and GP (Koza 1992; Tackett and Carrel 1994). Applications include doctrment classifi- 
cation (Masand 1994), image classification (Tackett 1993), and stock market prediction 
(Allen and Karjalainen 1995). 
607 
Computational Linguistics Volume 26, Number 4 
When combining linguistic indicators in particular, the choice of hypothesis repre- 
sentation determines the type of linguistic insights that can result. Decision trees can be 
analyzed by examining the subset of verbs that are sorted to a particular node and the 
constraints on indicator values that put them there. A path from the root to any node 
is a rule that puts constraints on indicator values; this rule can be examined to deter- 
mine if it has a linguistic interpretation. The weights produced by logistic regression 
can be examined to see which indicators are most highly weighted for each classifi- 
cation task. In addition to this, surprisingly, we discovered a decision tree-like rule 
encoded by these weights, as described below in Section 5.1.4. On the other hand, a 
function tree, such as that produced by GP, is more difficult to analyze manually, since 
it is a mathematical combination. However, GP's performance was tested due to the 
potential improvement in classification performance of such a flexible representation 
for numerical functions. 
The relative merit of various learning algorithms is often difficult to ascertain, 
even after results have been collected. In general, each learning algorithm relies on an 
inductive bias, that may produce better esults for some data than for others (Mitchell 
1997). When applied to linguistic indicators, there is little knowledge about how indica- 
tors interact, since initial analysis examines individual indicators in isolation; machine 
learning is being used to automatically discover their interaction. Intuition guides the 
choice and design of algorithms, such as the rationale for each of the three techniques 
described above in this section. Moreover, beyond the particular characteristics of any 
given classification task, the particular data sample to which a learning technique is 
applied may have a large effect on performance, for example, due to the distribution 
and size of the training set, differences between the distributions of the training and 
test sets, and even the particular order in which the training cases are listed. 
The three learning methods we examine in detail are diverse in their representa- 
tion abilities, as described in this section, and, arguably, are therefore representative 
of the abilities of learning algorithms in general when applied to the same data. A 
pilot study showed no further improvement in accuracy or in recall trade-off or ei- 
ther classification problem by another four standard learning algorithms: naive Bayes 
(Duda and Hart 1973), Ripper (Cohen 1995), ID3 (Quinlan 1986) and C4.5 (Quin- 
lan 1993). Furthermore, using metalearning to combine multiple learning methods 
hierachically (sometimes called stacked generalization; Chan and Stolfo \[1993\], and 
Wolpert \[1992\]), according to the JAM (Java Agents for Metalearning) model (Stolfo et 
al. 1997), produced equivalent results. However, this may be due to the limited size 
of our supervised ata. 
5. Supervised Learning: Method and Results 
In this section, we evaluate the models produced by the three supervised learning 
methods. These methods are applied to combine the linguistic indicators computed 
over the medical discharge summaries in order to distinguish between states and 
events. Then, the methods are applied to indicators computed over novels in order to 
distinguish between culminated and nonculminated events. At the end of this section, 
these results are compared to one another, and to an informed baseline classification 
method. 
The two data sets are summarized in Table 7. Table 8 illustrates the schema of 
inputs for supervised learning. There are a total of 14 continuous attributes for the 
two binary learning problems. All attributes are proportions except frequency, which 
is a positive integer. This data is available at http://www.cs.columbia.edu/-evs/ 
VerbData/. 
608 
Siegel and McKeown Improving Aspectual Classification 
Table 7 
Two classification problems on different data sets. 
Stativity Completedness 
Corpus 3,224 medical discharge summaries 10 novels 
Corpus size 1,159,891 words 846,913 words 
Parsed clauses 97,973 75,289 
Training clauses 739 (634 events) 307 (196 culminated) 
Testing clauses 739 (619 events) 308 (195 culminated) 
Verbs in test set 222 204 
Clauses excluded be-, have-clauses stative clauses 
Unsupervised results N/A stativity (see Section 6) 
Table 8 
Schema of inputs for supervised learning. Fourteen continuous attributes for two binary 
learning problems: stativity and completedness. 
Linguistic Indicator stativity completedness 
Class yes..no yes..no 
manner adverb 0.00..0.29 0.00..0.23 
duration in-PP 0.00..0.13 0.00..0.12 
continuous adverb 0.00..1.00 0.00..0.33 
temporal adverb 0.00..0.40 0.00..0.97 
not or never 0.47..1.00 0.58..1.00 
duration for-PP 0.00..0.15 0.00..1.00 
perfect 0.00..0.35 N/A 
perfect (not progressive) N/A 0.00..0.50 
past/pres participle 0.00..1.00 0.33..1.00 
evaluation adverb 0.00..0.46 0.00..0.95 
no subject 0.00..1.00 0.00..1.00 
past tense 0.00..1.00 0.00..1.00 
present tense 0.00..1.00 0.00..1.00 
frequency 1..2,131 1..13,882 
not progressive 0.00..1.00 0.00..0.50 
For both classification problems, we show that individual indicators correlate with 
aspectual class, but attain limited classification accuracy when used alone. Supervised 
learning is then used to combine indicators, improving classification performance and 
providing linguistic insights. The results of unsupervised learning are given in Sec- 
tion 6. 
Classification performance is evaluated according to a variety of performance mea- 
sures, since some applications weigh certain classes more heavily than others (Brodley 
1996; Cardie and Howe 1997). An alternative to evaluation based on overall accu- 
racy is to measure the recall values for the dominant and nondominant (i.e., majority 
and minority) categories eparately. A favorable recall trade-off is achieved if the re- 
call of the nondominant category can be improved with no loss in overall accuracy 
when compared against some baseline (Cardie and Howe 1997). Achieving such a 
trade-off is nontrivial; it is not possible, for example, for an uninformed approach that 
assigns everything to the dominant category. A favorable recall trade-off presents an 
advantage for applications that weigh the identification of nondominant instances, e.g., 
nonculminated clauses, more heavily. For example, correctly identifying the use of for 
depends on identifying the aspectual class of the clause it modifies (see Section 2.3). A 
system that surmnarizes the duration of events which incorrectly classifies She ran (for 
609 
Computational Linguistics Volume 26, Number 4 
a minute) as culminated will not detect hat for a minute describes the duration of the 
run event. As another example, it is advantageous for a medical system that retrieves 
patient diagnoses to identify stative clauses, since there is a correspondence b tween 
states and medical diagnoses. 
Classification performance is evaluated over verb instances, that is, clauses in 
which the verb appears as the main verb. 5 Because of this, as discussed further in 
Section 5.4 below, the same verb may appear multiple times in the training and testing 
sets. This measure is beneficial in several ways: 
? Measured classification performance r flects the true distribution of the 
verbs--some are more frequent than others. 
? Ambiguous verbs may appear with multiple aspectual categories, 
reflecting the true distribution of the data. 
? In related work, clausal constituents other than the verb can be 
incorporated tohelp resolve ambiguity and alleviate verb sparsity (Siegel 
1998a, 1998b). 
5.1 States versus Events 
Our experiments distinguishing states and events were performed across a corpus 
of 3,224 medical discharge summaries, with a total of 1,159,891 words. A medical 
discharge summary describes the symptoms, history, diagnosis, treatment, and out- 
come of a patient's visit to the hospital. Each summary consists of unstructured text, 
divided into several sections with titles such as: "History of Present Illness," and 
"Medical Summary." The text under four of these titles was extracted and parsed with 
the English Slot Grammar, resulting in 97,973 clauses that were parsed fully, with no 
self-diagnostic errors (ESG produced error messages on some of this corpus' com- 
plex sentences). Other sections in the summaries were ignored since they report the 
numerical results of certain medical tests, interspersed with incomplete sentences. 
Be and have, the two most popular verbs, covering 31.9% of the clauses in this 
corpus, are handled separately from all other verbs. Clauses with be as their main 
verb, composing 23.9% of the corpus, always denote a state. Therefore, we can focus 
our efforts on the remaining clauses. Clauses with have as their main verb, composing 
8.0% of the corpus, are highly ambiguous, and have been addressed separately by 
considering the direct object of such clauses (Siegel 1998a, 1998b). 
5.1.1 Manual Marking for Supervised Data. As a basis for evaluating our approach, 
1,851 clauses from the parsed corpus were manually marked according to their fun- 
damental stativity. In contrast to the entire parsed corpus (97,973 clauses), each clause 
in this set of supervised ata had to be judged by a linguist. This subset was selected 
uniformly from clauses in the corpus that had main verbs other than be and have. As 
a linguistic test for marking, each clause was tested for readability with What happened 
was . . . .  Manual labeling followed a strict set of linguistically motivated guidelines 
in order to ascertain fundamental spectual class. Modifiers that result in aspectual 
transformations, such as durativefor-PPs, and in exceptions, uch as not, were ignored. 
A comparison between human markers for this test was performed over a different 
corpus, and is reported below in Section 5.2.1. 
5 For evaluation over sets of unique verbs, see Siegel (1998b). 
610 
Siegel and McKeown Improving Aspectual Classification 
Table 9 
Indicators discriminate between states and events. 
Linguistic Indicator Stative Mean Event Mean T-test P-value 
:frequency 932.89 667.57 0.0000 
not or never 4.44% 1.56% 0.0000 
temporal adverb 1.00% 2.70% 0.0000 
no subject 36.05% 57.56% 0.0000 
past/pres participle 20.98% 15.37% 0.0005 
duration in-PP 0.16% 0.60% 0.0018 
perfect 2.27% 3.44% 0.0054 
present tense 11.19% 8.94% 0.0901 
progressive 1.79% 2.69% 0.0903 
manner adverb 0.00% 0.03% 0.1681 
evaluation adverb 0.69% 1.19% 0.1766 
past tense 62.85% 65.69% 0.2314 
duration for-PP 0.59% 0.61% 0.8402 
continuous adverb 0.04% 0.03% 0.8438 
Because of manually identified parsing problems (verb or direct object incorrectly 
identified), 373 clauses were rejected. This left 1,478 clauses, which were divided 
equally into training and testing sets of 739 clauses each. 
Figure 1 (see Section 4) shows the system overview with details regarding the 
medical discharge summary corpus used in this study. As this shows, the values for 
linguistic indicators are computed across the entire parsed corpus. This is a fully 
automatic process. Then, the 739 training examples are used to derive a mechanism, 
e.g., a decision tree, for combining multiple indicators. The combination of indicators 
achieves an increase in classification performance. This increase in performance is then 
validated over the 739 unseen test cases. 
5.1.2 Upper and Lower Bounds in Accuracy. Of clauses with main verbs other than 
be and have, 83.8% are events. Therefore, simply classifying every verb as an event 
achieves an accuracy of 83.8% over the 739 test cases. However, this approach classifies 
all state clauses incorrectly, achieving a stative recall of 0.0%. This method serves as 
a baseline for comparison, since we are attempting to improve over an uninformed 
approach. 6 
One limitation to our approach places an upper bound on classification accuracy. 
Our approach examines only the main verb, since linguistic indicators are computed 
for verbs only. For example, a verb occurring three times as an event and twice as 
a state will be misclassified at least two of the five times. This limits classification 
accuracy to a maximum of 97.4% over the test cases due to the presence of verbs with 
multiple classes. The ramifications of this limitation are explored below in Section 5.4. 
5.1.3 Individual  Indicators. The values of the 14 indicators listed in Table 9 were 
computed, for each verb, across the 97,973 parsed clauses from our corpus of medical 
discharge summaries. As described in Section 3, each indicator has a unique value for 
each verb, which corresponds to the frequency of the aspectual marker with the verb 
(except verb frequency,  which is an absolute measure over the corpus). 
6 Similar baselines for comparison have been used for many classification problems (Duda and Hart 
1973), e.g., part-of-speech tagging (Church 1988; Allen 1995). 
611 
Computational Linguistics Volume 26, Number 4 
The second and third columns of Table 9 show the average value for each indicator 
over stative and event clauses, as measured over the training examples (which exclude 
be and have). These values are computed solely over the 739 training cases in order to 
avoid biasing the classification experiments in the sections below, which are evaluated 
over the unseen test cases. For example, for the not~never indicator, stative clauses 
have an average value of 4.44%, while event clauses have an average value of 1.56%. 
This makes sense, since diagnoses are often ruled out in medical discharge summaries, 
e.g., The patient was not hypertensive, but procedures that were not done are not usually 
mentioned, e.g., ?An examination was not performed. 
The differences in stative and event means are statistically significant (p < .01) 
for the first seven of the 14 indicators listed in Table 9. The fourth column shows 
the results of t-tests that compare indicator values over stative verbs to those over 
event verbs for each indicator. For example, there is less than a 0.05% chance that the 
differences between stative and event means for the first seven indicators listed is due 
to chance. The differences in average value for the bottom seven indicators were not 
confirmed to be significant with this small sample size (739 training examples). 
A positive correlation between indicator value and verb class does not necessarily 
mean an indicator can be used to increase classification accuracy over the baseline 
of 83.8%. This is because of the dominance of events among the testing examples; 
a threshold to distinguish verbs that correctly classifies more than half of each class 
will have an accuracy lower than the baseline if the number of states correctly clas- 
sified is less than the number of events misclassified. To examine this, each indica- 
tor was tested individually for its ability to improve classification accuracy over the 
baseline by establishing the best classification threshold over the training data. The 
performance of each indicator was validated over the testing data using the same 
threshold. 
Only the frequency indicator succeeded in significantly improving classification 
accuracy. Both frequency and occurrences with not or never improved classification 
accuracy on the training data over the baseline obtained by classifying all clauses 
as events. To validate this improved accuracy, the thresholds established over the 
training set were used over the test set, with resulting accuracies of 88.0% and 84.0%, 
respectively. Binomial tests show the first of these, but not the second, to be a significant 
improvement over the baseline of 83.8%. 
This improvement in accuracy was achieved simply by discriminating the pop- 
ular verb show as a state, but classifying all other verbs as events. Although many 
domains may primarily use show as an event, in its appearances in medical discharge 
smnmaries, such as His lumbar puncture showed evidence of white cells, show primarily 
denotes a state. This observation illustrates the importance of empirical techniques for 
lexical knowledge acquisition. 
5.1.4 Indicators Combined with Learning. All three machine learning methods uc- 
cessfully combined indicator values, improving classification accuracy over the base- 
line measure. As shown in Table 10, the decision tree's accuracy was 93.9%, GP's func- 
tion trees had an average accuracy of 91.2% over seven runs, and logistic regression 
achieved an 86.7% accuracy (Baseline 2 is discussed below in Section 5.4). Binomial 
tests showed that both the decision tree and GP achieved a significant improvement 
over the 88.0% accuracy achieved by the frequency indicator alone. These results 
show that machine learning methods can successfully combine multiple numerical 
indicators to improve verb classification accuracy. 
The increase in the number of stative clauses correctly classified, i.e., stative recall, 
illustrates an even greater improvement over the baseline. As shown in Table 10, stative 
612 
Siegel and McKeown Improving Aspectual Classification 
Table 10 
Comparison of three learning methods, optimizing for accuracy, and two performance 
baselines, distinguishing states from events. 
Overall Accuracy States Events 
Recall Precision Recall Precision 
Decision tree 93.9% 74.2% 86.4% 97.7% 95.1% 
GP (7 runs) 91.2% 47.4% 97.3% 99.7% 90.7% 
Logistic 86.7% 34.2% 68.3% 96.9% 88.4% 
Baseline 1 83.8% 0.0% 100.0% 100.0% 83.8% 
Baseline 2 94.5% 69.2% 95.4% 99.4% 94.3% 
Table 11 
Comparing training and test performance on three learning methods, distinguishing states 
from events. 
Training Accuracy Testing Accuracy 
Decision tree 96.3% 93.9% 
GP 93.4% 91.2% 
Logistic 88.8% 86.7% 
Baseline 85.8% 83.8% 
recalls of 74.2%, 47.4%, and 34.2% were achieved by the three learning methods, as 
compared to the 0.0% stative recall achieved by Baseline 1, while only a small loss 
in recall over event clauses was suffered. The baseline does not classify any stative 
clauses correctly because it classifies all clauses as events. This difference in recall is 
more dramatic than the accuracy improvement because of the dominance of event 
clauses in the test set. 
Overfitting was moderate for each of the three supervised learning algorithms. 
As shown in Table 11, each learning method's performance over the training data 
was about two points higher than that over the test data. This corresponds to a two- 
point difference in baseline performance, which is due to a higher proportion of event 
clauses in the training data. 
The thresholds established to discriminate the outputs of GP's function trees gener- 
alize well to unseen data. When inducing these function trees with the GA, the training 
set is used to form the tree, and to select a threshold that best discriminates between 
verbs of the different classes. There is the potential that a threshold determined over 
the training cases will not generalize well when evaluated over the test cases. To test 
this, for each of the seven function trees generated by the GA to distinguish between 
states and events, the best threshold was selected over the test cases. For five of the 
function trees, there was no threshold that increased classification accuracy beyond 
that attained by the threshold established over the training cases. For the other two, 
a threshold was found that allowed for one more of the 739 test cases to be correctly 
classified. 
In the remainder of this section, we compare the resulting models of the three 
supervised learning method and contrast he ways in which they combine indicators. 
Logistic Regression. Logistic regression successfully combined the 14 linguistic indi- 
cators, achieving an accuracy of 86.7%, as shown in Table 10. This is a significant 
improvement over the baseline accuracy of 83.8%, as measured with a binomial test. 
Furthermore, a stative recall of 34.2% was achieved. 
613 
Computational Linguistics Volume 26, Number 4 
Table 12 
Weights produced by logistic regression to distinguish between stative and event verbs. 
Linguistic Indicator Logistic Weight T-test P-value 
manner adverb 11.04744 0.1681 
duration in-PP 0.06209624 0.0018 
continuous adverb - 0.04168417 0.8438 
temporal adverb 0.02127572 0.0000 
not or never 0.01714499 0.0000 
durationfor-PP - 0.01019155 0.8402 
perfect 0.009528091 0.0054 
past/pres participle 0.006981148 0.0005 
evaluation adverb 0.005695407 0.1766 
no subject 0.002742867 0.0000 
past tense 0.002586572 0.2314 
present tense 0.002409898 0.0901 
frequency - 0.001264895 0.0000 
not progressive - 0.0009369231 0.0903 
The particular weighting scheme resulting from logistic regression for this data ef- 
fectively integrates a decision tree type rule, along with the usual weighting of logistic 
regression. This is illustrated in Table 12, which shows the weights automatically de- 
rived by logistic regression for each of the 14 linguistic indicators. The value assigned 
to the manner adverb indicator, 11.04744, far outweighs the other 13 weights. At first 
glance, it may appear that this weighting scheme favors the manner adverb indicator 
over all other indicators. However, as shown in Table 13, manner adverb indicator 
values are 0.0% for all verbs in the training set except he eight indicated, all of which 
denote vents. Therefore, the large weight assigned to the manner adverb indicator is 
only activated for those verbs, which are therefore ach classified as events, regardless 
of the remaining 13 indicator values. For all other verbs, the remaining 13 indicator 
values determine the classification. 
This rule cannot increase accuracy over the baseline without the remaining 13 
indicators, since it does not positively identify any states--it only identifies events, 
which are all correctly classified by the baseline. Therefore, it is only useful because 
the overall model also correctly identifies ome stative clauses. 
Genetic Programming. GP successfully combined the 14 linguistic indicators, achieving 
an average accuracy of 91.2%, as shown in Table 10. This is a significant improvement 
over the baseline accuracy of 83.8%, according to a binomial test. Furthermore, a stative 
recall of 47.4% was achieved. 
GP improved classification performance by emphasizing a different set of indica- 
tors than those emphasized by logistic modeling. Figure 3 shows an example function 
tree automatically generated by the GA, which achieved 92.7% accuracy. Note that this 
classification performance was attained with a subset of only five linguistic indicators: 
duration in-PP, progressive, not or never, past tense, and frequency. Two of these are 
ranked lowest by logistic regression: frequency and progressive. Furthermore, manner 
adverb, ranked highest by logistic regression, is not incorporated in this function tree 
at all. This may be because this indicator only applies to a small number of verbs, as 
shown in Table 13, and because an/f-rule such as that captured by logistic regression is
difficult o encode with a function tree with no conditional functions. Overall, we can 
conclude that multiple proper subsets of linguistic indicators are useful for aspectual 
classification if combined with the correct model. 
614 
Siegel and McKeown Improving Aspectual Classification 
(/ (+ (+ (* (/ DurationInPP (+ Progressive (+ NotNever NotNever))) (+ 
Progressive 75)) (/ (* (+ Progressive PastTense) Progressive) 
NegFrequency)) NotNever) DurationInPP) 
Figure 3 
Example function tree designed by a genetic algorithm to distinguish between stative and 
event verbs, achieving 92.7% accuracy. 
Table 13 
Linguistic rule discovered by logistic regression. 
If frequency with verbs this Frequency in then 
manner adverbs is ... applies to: Training Set classify as: 
> 0.0% adjust 1 Event 
continue 16 
decline 2 
decrease 4 
improve 5 
increase 4 
progress 2 
resolve 7 
0.0% all other verbs 699 depending on other 13 indicators 
Decision Tree Induction. Decision tree induction successfully combined the 14 linguistic 
indicators, achieving the highest accuracy of the three supervised learning algorithms 
tested, 93.9%, as shown in Table 10. This is a significant improvement over the baseline 
accuracy of 83.8%, as measured with a binomial test. Furthermore, a stative recall of 
74.2% was achieved. The top portion of the tree created with recursive partitioning 
is shown in Figure 2 (in Section 4.2, where it is explained). Note that the root node 
simply distinguishes the stative verb show with the frequency indicator, as described 
in Section 5.1.3. 
To achieve this increase in classification performance, the decision tree divided the 
training cases into relatively small partitions of verbs. Table 14 shows the distribution 
of training case verbs examined by the highlighted tree node in Figure 2. As seen 
by tracing the path from the root to the highlighted node, these are the verbs with 
frequency less than 2,013 across the corpus, and modified by not or never at least 3.48% 
of the time. From this subset, the highlighted node distinguishes the three verbs with 
frequency at least 314, shown in capitals in Table 14, as states. This is correct for all 
19 instances of these three verbs, and does not misclassify any event verbs. 
This example illustrates a benefit of distinguishing verbs based on indicator values 
computed over large corpora. Most of the verbs in Table 14 appear in the training 
set a small number of times, so it would be difficult for a classification system to 
generate rules that apply to these individual verbs. Rather, since our system draws 
generalizations over the indicator values of verbs, it identifies tative verbs without 
misclassifying any of the event verbs shown. 
Classification performance is equally competitive without he frequency indicator. 
Since frequency is the only indicator that can increase accuracy by itself, and since it is 
the first discriminator f the decision tree, it may appear that :frequency highly dom- 
inates the set of indicators. This could be problematic, since the relationship between 
verb frequency and verb category may be particularly domain dependent, in which 
case frequency could be less informative when applied to other domains. However, 
when decision tree induction was employed to combine only the 13 indicators other 
615 
Computational Linguistics Volume 26, Number 4 
Table 14 
Verb distribution in the partition of training examples orted to the decision tree node 
highlighted in Figure 2. The three stative verbs shown in capitals are distinguished at this 
node, with no event verbs misclassified. 
Events :  
get 3 talk 1 persue 1 drive 1 
transfuse 2 suggest 1 provide 1 drink 1 
respond 2 subside 1 pass 1 document 1 
lose 2 sleep 1 notice 1 detect 1 
live 2 retract 1 load 1 change 1 
interfere 2 repair 1 limit 1 achieve 1 
breathe 2 relieve 1 lift 1 regain 1 
visualize 1 recognize 1 help 1 
tell 1 radiate 1 explain 1 
States: 
FEEL 12 associate 3 want 1 
know 4 remember 2 support 1 
APPEAR 4 believe 2 indicate 1 
REQUIRE 3 accompany 2 desire 1 
concern 1 
account 1 
Table 15 
Indicators discriminate between culminated and nonculminated vents. 
Linguistic Indicator Culminated Mean Nonculminated Mean T-test P-value 
perfect 7.87% 2.88% 0.0000 
temporal adverb 5.60% 3.41% 0.0000 
manner adverb 0.19% 0.61% 0.0008 
progressive 3.02% 5.03% 0.0031 
past/pres participle 14.03% 17.98% 0.0080 
no subject 30.77% 26.55% 0.0241 
duration in-PP 0.27% 0.06% 0.0626 
present tense 17.18% 14.29% 0.0757 
duration for-PP 0.34% 0.49% 0.1756 
continuous adverb 0.10% 0.49% 0.2563 
:frequency 345.86 286.55 0.5652 
not or never 3.41% 3.15% 0.6164 
evaluation adverb 0.46% 0.39% 0.7063 
past tense 53.62% 54.36% 0.7132 
than frequency, the resulting decision tree achieved 92.4% accuracy and 77.5% stative 
recall. Therefore, our results are not entirely dependent on the f requency indicator. 
5.2 Culminated versus Noncu lminated  Events 
In medical discharge summaries, nonculminated event clauses are rare. Therefore, 
our experiments for classification according to completedness are performed across a 
corpus of 10 novels, comprising 846,913 words. These novels were parsed with the 
English Slot Grammar, resulting in 75,289 clauses that were parsed fully, with no self- 
diagnostic errors. The values of the 14 indicators listed in Table 15 were computed, 
for each verb, across the parsed clauses. Note that in this section, the perfect indicator 
differs in that we ignore occurrences of the perfect in clauses that are also in the 
progressive, since any progressive clause can appear in the perfect, e.g., I have been 
painting. 
616 
Siegel and McKeown Improving Aspectual Classification 
5.2.1 Manual Marking for Supervised Data. To evaluate the performance of our sys- 
tem, we manually marked 884 clauses from the parsed corpus according to their aspec- 
tual class. These 884 were selected evenly across the corpus from parsed clauses that 
do not have be as the main verb, since we are testing a distinction between events. 
Of these, 109 were rejected because of manually identified parsing problems (verb 
or direct object incorrectly identified), and 160 were rejected because they described 
states. This left 615 event clauses over which to evaluate classification performance. 
The division into training and test sets was derived such that the distribution of classes 
was equal between the two sets. This precaution was taken because preliminary ex- 
periments indicated ifficulty in demonstrating a significant increase in classification 
accuracy for completedness. This process resulted in 307 training cases (196 culmi- 
nated) and 308 test cases (195 culminated). Since 63.3% of test cases are culminated 
events, simply classifying every clause as culminated achieves an accuracy of 63.3% 
over the 308 test cases (Baseline 1A). This method serves as a baseline for compari- 
son. 
We used linguistic tests that were selected for this task by Passonneau (1988) from 
the constraints and entailments listed in Tables 2 and 3. First, the clause was tested for 
stativity with What happened was . . . .  Then, as an additional check, we tested with the 
following rule: if a clause can be read in a pseudocleft, i  is an event, e.g., What itsparents 
did was run off, versus *What we did was know what is on the other side. Second, if a clause 
in the past progressive necessarily entails the past tense reading, the clause describes 
a nonculminated event. For example, We were talking just like men (nonculminated) 
entails that We talked just like men, but The woman was building a house (culminated) does 
not necessarily entail that The woman built a house. The guidelines described above in 
Section 5.1 were used in order to test for fundamental spectual class. 
Cross-checking between linguists shows high agreement. In particular, in a pilot 
study manually annotating 89 clauses from the corpus of novels, two linguists agreed 
81 times (i.e., 91%). Informal analysis suggests the remaining disagreement could be 
further divided in half by a few simple refinements of the annotation protocol. Fur- 
thermore, of 57 clauses agreed to be events, 46 were annotated in agreement with 
respect o completedness. 
The verb say, which is a frequent point, i.e., nonculminated and nonextended, 
poses a challenge for manual marking. Points are misclassified by the test for com- 
pletedness described above since they are nonextended and therefore cannot be placed 
in the progressive. Therefore, say, which occurs nine times in the test set, was marked 
incorrectly as culminated. After some initial experimentation, we switched the class 
of each occurrence of say in our supervised ata to nonculminated. This change to say 
made the class distribution slightly uneven between training and test data. 
5.2.2 Individual Indicators. The second and third columns of Table 15 show the av- 
erage value for each indicator over culminated and nonculminated event clauses, as 
measured over the training examples. For example, for the perfect ense indicator, cul- 
minated clauses have an average value of 7.87%, while nonculminated clauses have 
an average value of 2.88%. These values were computed solely over the 307 training 
cases in order to avoid biasing the classification experiments in the sections below, 
which are evaluated over the unseen cases. 
The differences in culminated and nonculminated means are statistically significant 
(p < .05) for the first six of the 14 indicators listed in Table 15. The fourth column shows 
the results of t-tests that compare indicator values over culminated verbs to those over 
nonculminated verbs. For example, there is less than a 0.05% chance that the differences 
between culminated and nonculminated means for the first six indicators listed is due 
617 
Computational Linguistics Volume 26, Number 4 
Table 16 
Comparison of four learning methods, optimized for accuracy, and three performance 
baselines distinguishing culminated from nonculminated vents. 
Overall Accuracy Culminated Nonculminated 
Recall Precision Recall Precision 
CART 74.0% 86.2% 76.0% 53.1% 69.0% 
Logistic 70.5% 83.1% 73.6% 48.7% 62.5% 
Logistic 2 67.2% 81.5% 71.0% 42.5% 57.1% 
GP (4 runs) 68.6% 77.3% 74.2% 53.6% 57.8% 
Decision tree 68.5% 86.2% 70.6% 38.1% 61.4% 
Baseline 1A 63.3% 100.0% 63.3% 0.0% 100.0% 
Baseline 1B 49.0% 46.4% 63.3% 53.6% 36.7% 
Baseline 2 70.8% 94.9% 69.8% 29.2% 76.7% 
to chance. The differences in average value for the bottom eight indicators were not 
confirmed to be significant with this small sample size (307 training examples). 
For completedness, no individual indicator used in isolation was shown to signif- 
icantly improve classification accuracy over the baseline. 
5.2.3 Indicators Combined  wi th  Learning. When distinguishing according to com- 
pletedness, both CART and logistic regression successfully combined indicator values, 
improving classification accuracy over the baseline measure. As shown in Table 16, 
classification accuracies were 74.0% and 70.5%, respectively. A binomial test showed 
each to be a significant improvement over the 63.3% accuracy achieved by Baseline 1A. 
Although the accuracies attained by GP and decision tree induction, 68.6% and 68.5% 
respectively, are also higher than that of Baseline 1A, based on a binomial test this is 
not significant. However, this may be due to our small test sample size. 
The increase in the number of nonculminated clauses correctly classified, i.e., non- 
culminated recall, illustrates a greater improvement over the baseline. As detailed in 
Table 16, nonculminated recalls of 53.1%, 48.7%, 53.6%, and 38.1% were achieved by the 
learning methods, as compared to the 0.0% nonculminated recall achieved by Baseline 
1A. Baseline 1A does not classify any nonculminated clauses correctly because it clas- 
sifies all clauses as events. This difference in recall is more dramatic than the accuracy 
improvement because of the dominance of culminated clauses in the test set. Note that 
it is possible for an uninformed approach to achieve the same nonculminated recall 
as GP, 53.6%, by arbitrarily classifying 53.6% of all clauses as nonculminated, and the 
rest as culminated. However, as shown in Table 16, the average performance of such 
a method (Baseline 1B) loses in comparison to GP, for example, in overall accuracy 
(49.0%) and nonculminated precision (36.7%). 
All three supervised learning methods highly prioritized the perfect indicator. 
The induced decision tree uses the perfect indicator as its first discriminator, logistic 
regression ranked the perfect indicator as fourth out of 14 (see Table 17), and one 
function tree created by GP includes the perfect indicator as one of five indicators used 
together to increase classification performance (see Figure 4). Furthermore, asshown in 
Table 15, the perfect indicator tied with the temporal adverb indicator as most highly 
correlated with completedness, according to t-test results. This is consistent with the 
fact that, as discussed in Section 2.1, the perfect indicator is strongly connected to 
completedness on a linguistic basis. 
GP maintained classification performance while emphasizing a different set of 
indicators than those emphasized by logistic regression. Figure 4 shows an example 
618 
Siegel and McKeown Improving Aspectual Classification 
Table 17 
Weights produced by logistic regression to distinguish between culminated and 
nonculminated verbs. Contrast with Table 12. 
Linguistic Indicator Logistic Weight T-test P-value 
duration in-PP -0.1207664 0.0626 
manner adverb 0.03808262 0.0008 
evaluation adverb 0.03212381 0.7063 
perfect -0.02304221 0.0000 
temporal adverb -0.01643347 0.0000 
not or never -0.01212703 0.6164 
not progressive -0.01059269 0.0031 
no subject -0.006891114 0.0241 
past/pres participle -0.004127672 0.0080 
past tense 0.002484739 0.7132 
present tense 0.00218274 0.0757 
continuous adverb -0.001775534 0.2563 
duration for-PP 0.0001747421 0.1756 
frequency -0.0000916167 0.5652 
(+ (- (- (+ (/ NoSubject Frequency) TemporalAdv) (- 83 Perfect)) (/ (/ 
NoSubject Frequency) Frequency)) (- (+ NotProgressive NotProgressive) (- 
60 Perfect))) 
Figure 4 
Example function tree designed by a genetic algorithm to distinguish between culminated and 
nonculminated verbs, achieving 69.2% accuracy and 62.8% nonculminated recall. 
function tree automatically generated by GP, which achieved 69.2% accuracy. Note 
that, as for stativity, this classification performance was attained with a subset of 
only five linguistic indicators: no subject, frequency, temporal adverb, perfect, and 
not progressive. (However, only two of these appeared in the example function tree 
for stativity shown in Figure 2: frequency and progressive.) Since multiple proper 
subsets of indicators succeed in improving classification accuracy, this shows that 
some indicators are mutually correlated. 
5.3 Comparing Learning Results Across Classification Tasks 
As shown above, learning methods uccessfully produced models that were special- 
ized for the classification task. In particular, the same set of 14 indicators were com- 
bined in different ways, successfully improving classification performance for both 
stativity and completedness, and revealing linguistic insights for each. 
However, it is difficult to determine which learning method is the best for verb 
classification in general, since their relative ranks differ across classification task and 
evaluation criteria. The relative accuracies of the three supervised learning procedures 
rank in opposite orders when comparing the results in classification according to 
stativity (Table 10) to results in classification according to completedness (Table 16). 
Furthermore, when measuring classification performance as the recall of the nondom- 
inant class (stative and nonculminated, respectively), the rankings are also conflicting 
when comparing results for the two classification tasks. The difficulties in drawing 
conclusions about the relative performance of learning techniques are discussed in 
Section 4.4. 
The same two linguistic indicators were ranked in the top two positions for both 
aspectual distinctions by logistic regression. As shown in Tables 17 and 12, which give 
the weights automatically derived by logistic regression for each of the 14 linguistic 
619 
Computational Linguistics Volume 26, Number 4 
indicators, the manner adverb and duration in-PP indicators are in the top two slots for 
both weighting schemes, corresponding to the two aspectual distinctions. This may 
indicate that these two indicators are universally useful for aspectual classification, 
at least when modeling with logistic regression. However, the remaining rankings of 
linguistic indicators differ greatly between the two weighting schemes. 
5.4 Indicators versus Memorizing Verb Aspect 
In this work, clauses are classified by their main verb only. Therefore, disambiguating 
between multiple aspectual senses of the same verb is not possible, since other parts 
of the clause (e.g., verb arguments) are not available as a source of context with which 
to disambiguate. Thus, the improvement in accuracy attained reveals the extent o 
which, across the corpora examined, most verbs are dominated by one sense. 
A competing baseline approach would be to simply memorize the most frequent 
aspectual category of each verb in the training set, and classify verbs in the test set 
accordingly. In this case, test verbs that did not appear in the training set would be 
classified according to majority class. However, classifying verbs and clauses according 
to numerical indicators has several important advantages over this baseline: 
? Handles rare or unlabeled verbs. The results we have shown serve to 
estimate classification performance over unseen verbs that were not 
included in the supervised training sample. Once the system has been 
trained to distinguish by indicator values, it can automatically classify 
any verb that appears in unlabeled corpora, since measuring linguistic 
indicators for a verb is fully automatic. This also applies to verbs that are 
underrepresented in the training set. For example, as discussed in 
Section 5.1.4, one node of the resulting decision tree trained to 
distinguish according to stativity identifies 19 stative test cases without 
misclassifying any of 27 event est cases with verbs that occur only one 
time each in the training set. 
? Success when training doesn't include test verbs. To test this, all test 
verbs were eliminated from the training set, and logistic regression was 
trained over this smaller set to distinguish according to completedness. 
The result is shown in Table 16 (logistic 2). Accuracy remained higher 
than Baseline 1A (Baseline 2 is not applicable), and the recall trade-off is 
felicitous. 
? Improved performance. Memorizing majority aspect does not achieve as 
high an accuracy as the linguistic indicators for completedness, nor does 
it achieve as wide a recall trade-off for both stativity and completedness. 
These results are indicated as the second baselines (Baseline 2) in 
Tables 10 and 16, respectively. 
? Classifiers output scalar values. This allows the trade-off between recall 
and precision to be selected for particular applications by selecting the 
classification threshold. For example, in a separate study, optimizing for 
F-measure resulted in a more dramatic trade-off in recall values as 
compared to those attained when optimizing for accuracy (Siegel 1998b). 
Moreover, such scalar values can provide input to systems that perform 
reasoning on fuzzy or uncertainty knowledge. 
? Expandable framework. One form of expansion is that additional 
indicators can be integrated by measuring the frequencies of additional 
620 
Siegel and McKeown Improving Aspectual Classification 
aspectual markers. Furthermore, indicators measured over multiple 
clausal constituents (e.g., main verb-object pairs) alleviate verb ambiguity 
and sparsity and improve classification performance (Siegel 1998b). 
Manual analysis reveals linguistic insights. As summarized below in 
Section 9, our analysis reveals linguistic insights that can be used to 
inform future work. 
6. Unsupervised Learning 
Unsupervised methods for clustering words have been developed that do not require 
manually marked examples (Hatzivassiloglou and McKeown 1993; Schfitze 1992). 
These methods automatically determine the number of groups and the number of 
verbs in each group. 
This section evaluates an approach to clustering verbs developed and implemented 
by Hatzivassiloglou, based on previous work for semantically clustering adjectives 
(Hatzivassiloglou and McKeown 1993; Hatzivassiloglou 1997). This system automat- 
ically places verbs into semantically related groups based on the distribution of co- 
occurring direct objects. Such a system avoids the need for a set of manually marked 
examples for the training process. Manual marking is time consuming and domain de- 
pendent, requires linguistic expertise, and must be repeated on a corpus representing 
each new domain. 
The clustering approach differs from our approach combining linguistic indicators 
in two significant ways. First, the method semantically groups words in a general 
sense--qt is not designed or intended to group words according to any particular 
semantic distinction such as stativity or completedness. Second, this method measures 
a co-occurrence r lation not embodied by any of the 14 linguistic indicators presented 
in this article: the direct object. Note, however, that there are several advantages to 
linguistic indicators that measure the frequency of linguistic phenomena such as the 
progressive over measuring the frequencies of open-class words (Siegel 1998b). 
The clustering algorithm was evaluated over the corpus of novels, which, as shown 
in Table 7, has 75,289 parsed clauses. Clauses were eliminated from this set if they had 
no direct object, or if the direct object was a clause, a proper noun, or a pronoun, or 
was misspelled. This left 14,038 distinct verb-object pairs of varying frequencies. 
Because the direct object is an open-class category (noun), occurrences of any 
particular verb-object pair are sparse as compared to the markers measured by the 
linguistic indicators. For example, make dinner occurs only once among the parsed 
clauses from the corpus of novels, but make occurs 34 times in the progressive. For 
this reason, the clustering algorithm was evaluated over a set of frequent verbs only: 
56 verbs occurred more than 50 times each in the set of verb-object pairs. Of these, 
the 19 shown in Figure 5 were selected as an evaluation set because of the natural 
semantic groups they fall into. The groupings hown, which do not pay heed to as- 
pectual classification i  particular, were established manually, but are not used by the 
automatic grouping algorithm. 
Figure 6 shows the output of the unsupervised system. Seven groups were created, 
each with two to four verbs. The grouping algorithm used by this system is designed 
for data that is not as sparse with respect o the frequencies of verb-object pairs, 
e.g., data from a larger corpus. Thus, this partition is not representative of the full 
power of the approach, and a larger amount of data could improve it significantly. For 
more detail on the clustering algorithm and further results see Hatzivassiloglou and 
McKeown (1993) and Hatzivassiloglou (1997). 
621 
Computational Linguistics Volume 26, Number 4 
1. sell(27) buy(20) acquire(8) 
2. push(28) pull(45) 
3. raise(68) lower(24) 
4. leave(160) enter(78) 
5. know(164) forget(50) learn(51) 
6. love(18) hate(16) like(ll2) 
7. want(60) need(69) require(57) demand(15) 
Figure 5 
The set of verbs manually selected for evaluating unsupervised clustering, with frequencies 
shown. The grouping shown here was established manually. 
I. *hate *like pull 
2. lower raise 
3. demand *know *love *want 
4. buy sell 
5. enter forget learn 
6. acquire *need *require 
7. leave push 
Figure 6 
Verb groupings created automatically by an unsupervised learning algorithm developed and 
implemented by Hatzivassiloglou and McKeown (1993) and Hatzivassiloglou (1997) applied 
over the corpus of 10 novels. Stative verbs are shown with an asterisk, and event verbs 
without. 
The algorithm clearly discriminated event verbs from stative verbs. 7 In Figure 6, 
stative verbs are shown with an asterisk; event verbs are shown without. Three of the 
groups are dominated by stative verbs, and the other four groups are composed en- 
tirely of event verbs. Each stative verb is found in a group with 70.2% states, averaged 
across the 7 stative verbs, and each event verb is found in a group with 82.6% events, 
averaged across the 12 event verbs. This is an improvement  over an uninformed base- 
line system that randomly creates groups of two or more verbs each, which would 
achieve average precisions of 63.2% and 36.8%, respectively. 
We can draw two important conclusions from this result. First, unsupervised learn- 
ing is a viable approach for classifying verbs according to particular semantic distinc- 
tions such as stativity. Second, co-occurrence distributions between the verb and direct 
7 The algorithm also grouped verbs according to semantic relatedness in general, as can be seen by 
comparing the manual and automatic groupings. Further analysis of such results are given by 
Hatzivassiloglou and McKeown (1993). 
622 
Siegel and McKeown Improving Aspectual Classification 
object inform the aspectual classification of verbs. This is an additional source of in- 
formation beyond the 14 linguistic indicators we combine with supervised learning. 
7. Related Work 
The aspectual classification of a clause has thus far been primarily approached from a 
knowledge-based perspective. For example, Pustejovsky's generative l xicon describes 
semantic interactions between clausal constituents hat effect aspectual class (Puste- 
jovsky 1991). Additionally, Resnik (1996) demonstrates the influence of implicit direct 
objects on aspectual classification. 
The application of automatic orpus-based techniques to aspectual classification 
is in its infancy. Klavans and Chodorow (1992) pioneered the application of statistical 
corpus analysis to aspectual classification by placing verbs on a scale according to the 
frequency with which they occur with certain aspectual markers from Table 2. This 
way, verbs are automatically ranked according to their "degree of stativity." 
Machine learning has become instrumental in the development of robust natural 
language understanding systems in general (Cardie and Mooney 1999). For example, 
decision tree induction has been applied to word sense disambiguation (Black 1988), 
determiner prediction (Knight et al 1995), coordination parsing (Resnik 1993), syn- 
tactic parsing (Magerman 1993), and disambiguating clue phrases (Siegel 1994; Siegel 
and McKeown 1994; Litman 1994). An overview of psycholinguistic ssues behind 
learning for natural language problems in particular is given by Powers and Turk 
(1989). Models resulting from machine induction have beeen manually inspected to 
discover linguistic insights for disambiguating clue words (Siegel and McKeown 1994). 
However, machine learning techniques have not previously been applied to aspectual 
disambiguation. 
Previous efforts have applied machine induction methods to coordinate corpus- 
based linguistic indicators in particular, for example, to classify adjectives according 
to markedness (Hatzivassiloglou and McKeown 1995), to perform accent restoration 
(Yarowsky 1994), for sense disambiguation problems (Luk 1995), and for the automatic 
identification of semantically related groups of words (Pereira, Tishby, and Lee 1993; 
Hatzivassiloglou and McKeown 1993; Schi.itze 1992). 
8. Future Work 
Parallel bilingual corpora are potential sources of supervised examples for training 
and testing aspectual classification systems. For example, since many languages have 
explicit markings corresponding to completedness (as described in Section 2.6), the 
category of a clause can be determined by its translation. 
Additional machine learning methods hould be evaluated for combining linguis- 
tic indicators. For example, neural networks are especially suited for combining nu- 
merical inputs, and Naive Bayes models are especially suited for additive concepts. 
Also, iteratively refining the model (e.g., for logistic regression) may be an important 
way to eliminate indicators that do not help for a particular classification problem and 
to eliminate redundancy between indicators that correlate highly with one another. 
Machine learning techniques may be able to automatically determine how best to 
measure linguistic indicators, if trained over a large supervised sample. For example, 
previous work has measured indicators by applying a symbolic expression induced by 
GP to a subset of clauses in a corpus (Siegel and McKeown 1996). This way, interactions 
between markers in a clause can be automatically measured. In principle, machine 
learning techniques could further generalize these methods by automatically inducing 
623 
Computational Linguistics Volume 26, Number 4 
an algorithm that scans a corpus dynamically, depending on what it sees as it processes 
clauses. This could automatically select relevant markers as well as relevant portions 
of the corpus for a particular input clause. 
9. Conclusions 
While individual inguistic indicators have predictive value, they are predictively in- 
complete. Such incompleteness i  due to sparsity and noise when computing indicator 
values over a corpus of limited size, and is also a consequence of the linguistic behav- 
ior of certain indicators. However, incomplete indicators can complement one another 
when placed in combination. 
Machine learning has served to illustrate the potential of 14 linguistic indicators 
by showing that they perform well in combination for two aspectual classification 
problems. This potential was not clear when evaluating indicators individually. For 
stativity, decision tree induction achieved an accuracy of 93.9%, as compared to the 
uninformed baseline's accuracy of 83.8%. Furthermore, GP and logistic regression also 
achieved improvements over the baseline. For completedness, CART and logistic re- 
gression achieved accuracies of 74.0% and 70.5%, as compared to the uninformed 
baseline's accuracy of 63.3%. These improvements in classification performance are 
more dramatically illustrated by favorable trade-offs between recall scores achieved 
for both classification problems. Such results are profitable for tasks that weigh the 
identification of the less frequent class more heavily. 
This evaluation was performed over unrestricted sets of verbs occurring across 
two corpora. The system can automatically classify all verbs appearing in a corpus, 
including those that have not been manually classified for supervised training data. 
Therefore, we have demonstrated a much-needed full-scale method for aspectual clas- 
sification that is readily expandable. Since minimal overfitting was demonstrated with 
only a small quantity of manually supervised ata required, this approach is easily 
portable to other domains, languages, and semantic distinctions. 
The results of learning are linguistically viable in two respects. First, learning au- 
tomatically produces models that are specialized for different aspectual distinctions; 
the same set of 14 indicators are combined in different ways according to which clas- 
sification problem is targeted. Second, automatic learning often derives linguistically 
informative insights. We have shown several such insights revealed by inspecting the 
models produced by learning, which are summarized here: 
? Examining the logistic regression model for classification according to 
stativity revealed a decision tree type of rule incorporated with the 
normal weighting scheme. 
? Verb frequency distinguishes stative verbs within multiple subsets of 
verbs. When applied to all verbs in a medical corpus, it identifies 
occurrences of show. Furthermore, examining an example node of the 
decision tree that distinguishes according to stativity revealed that verb 
frequency discriminates 19 stative clauses with 100.0% precision from the 
node's partition of 60 training cases. 
? Several proper subsets of the linguistic indicators prove independently 
useful for aspectual classification when combined with an appropriate 
model. This is illustrated by the fact that certain models reveal 
combinations of small sets of indicators that improved classification 
performance. For example, GP results for both classification tasks 
624 
Siegel and McKeown Improving Aspectual Classification 
incorporated a subset of only five indicators each. In particular, manner 
adverb, which ranked highest by logistic regression, is not incorporated 
in the example function tree induced by GP. This may be because this 
indicator only applies to a small number of verbs, as shown in Table 13, 
and because an/f-rule such as that captured by logistic regression is
difficult o encode with a function tree with no conditional primitives. 
Learning methods discovered that some indicators are particularly useful 
for both classification tasks. For example, the same two indicators were 
weighted most heavily by logistic regression for both tasks: duration 
in-PP and manner adverb. 
However, in general, learning methods emphasized different linguistic 
indicators for different classification tasks. For example, decision tree 
induction used frequency as the main discriminator to classify clauses 
according to stativity, while the perfect indicator was the main 
discriminator for classification according to completedness. 
Comparing the ability of learning methods to combine linguistic indicators is dif- 
ficult, since they rank differently depending on the classification task and evaluation 
criteria. For example, the relative accuracies of the three supervised learning proce- 
dures rank in opposite orders when comparing the results for stativity to the results 
for completedness. 
The unsupervised grouping of verbs provides an additional method for aspectual 
classification according to stativity. Co-occurrence distributions between the verb and 
direct object inform the aspectual classification of verbs. This provides information 
beyond the 14 linguistic indicators that can also be derived automatically. However, 
due to the sparsity intrinsic to pairs of open-class categories such as verb-object pairs, 
this approach was only evaluated over a small set of frequent verbs. 
Acknowledgments 
Judith Klavans was very helpful in our 
formulation of the linguistic techniques in
this work, and Alexander Chaffee, Vasileios 
Hatzivassiloglou, and Dekai Wu in the 
results evaluation methods. Vasileios 
Hatzivassiloglou designed and implemented 
the clustering of verbs described in 
Section 6. For comments on earlier drafts of 
this work, we would like to thank those 
people mentioned, as well as David Evans, 
Hongyan Jing, Min-Yen Kan, Dragomir 
Radev, and Barry Schiffman. Finally, we 
would like to thank Andy Singleton for the 
use of his GPQuick software. 
This research was supported in part by 
the Columbia University Center for 
Advanced Technology in High Performance 
Computing and Communications in 
Healthcare (funded by the New York State 
Science and Technology Foundation), the 
Office of Naval Research under contract 
N00014-95-1-0745, and the National Science 
Foundation under contract GER-90- 
24069. 
References 
Allen, Franklin and Risto Karjalainen. 1995. 
Using genetic algorithms to find technical 
trading rules. Technical Report, Rodney L. 
White Center For Financial Research. 
Allen, James. 1995. Natural Language 
Understanding. Benjamin/Cummings, 
Redwood City, CA. 
Baker, R. J. and J. A. Nelder. 1989. The GLIM 
System, Release 3: Generalized Linear 
Interactive Modeling. Numerical 
Algorithms Group, Oxford. 
Black, Ezra. 1988. An experiment in
computational discrimination ofEnglish 
word senses. IBM Journal of Research and 
Development, 2(32). 
Breiman, Leo, Jerome H. Friedman, 
Richard A. Olshen, and Charles J. Stone. 
1984. Classification and Regression Trees. 
Wadsworth, Belmont. 
Brodley, Carla E. 1996. Applying 
classification algorithms in practice. In 
Statistics and Computing. 
Cardie, Claire and Nicholas Howe. 1997. 
Improving minority class prediction using 
625 
Computational Linguistics Volume 26, Number 4 
case-specific feature weights. In D. Fisher, 
editor, Proceedings ofthe Fourteenth 
International Conference on Machine 
Learning. Morgan Kaufmann. 
Cardie, Claire and Raymond J. Mooney. 
1999. Guest editors' introduction: 
Machine learning and natural language. 
Machine Learning, 1-3(34). 
Chan, Philip K. and Sal J. Stolfo. 1993. 
Toward multistrategy parallel and 
distributed learning in sequence analysis. 
In Proceedings ofthe First International 
Conference on Intelligent Systems for 
Molecular Biology. 
Church, Ken. 1988. A stochastic parts 
program and noun phrase parser for 
unrestricted text. In Proceedings ofthe 2nd 
Conference for Applied Natural Language 
Processing, pages 136-143. 
Cohen, William W. 1995. Fast effective rule 
induction. In Proceedings ofthe 12th 
International Conference on Machine 
Learning, pages 115-123. 
Cramer, Nichael L. 1985. A representation 
for the adaptive generation of simple 
sequential programs. In J. Grefenstette, 
editor, Proceedings ofthe \[First\] International 
Conference on Genetic Algorithms. Lawrence 
Erlbaum. 
Dorr, Bonnie J. 1992. A two-level knowledge 
representation formachine translation: 
lexical semantics and tense/aspect. In 
James Pustejovsky and Sabine Bergler, 
editors, Lexical Semantics and Knowledge 
Representation. Springer Verlag, Berlin. 
Dowty, David R. 1979. Word Meaning and 
Montague Grammar. D. Reidel, Dordrecht. 
Duda, Richard O. and Peter E. Hart. 1973. 
Pattern Class~cation and Scene Analysis. 
Wiley, New York. 
Fayyad, Usama M. and Keki B. Irani. 1992. 
On the handling of continuous-valued 
attributes in decision tree generation. 
Machine Learning, 8. 
Friedman, Jerome H. 1977. A recursive 
partitioning decision rule for 
non-parametric classification. IEEE 
Transactions on Computers. 
Goldberg, David. 1989. Genetic Algorithms in 
Search, Optimization, and Machine Learning. 
Addison-Wesley Publishing Company, 
Inc., Reading, MA. 
Hatzivassiloglou, Vasileios. 1997. Automatic 
Acquisition of Lexical Semantic Knowledge 
from Large Corpora: The Identi~cation f
Semantically Related Words, Markedness, 
Polarity, and Antonymy. Ph.D. thesis, 
Columbia University. 
Hatzivassiloglou, Vasileios and Kathleen R. 
McKeown. 1993. Towards the automatic 
identification of adjectival scales: 
clustering adjectives according to 
meaning. In Proceedings ofthe 31st Annual 
Meeting, pages 172-182, Columbus, OH, 
June. Association for Computational 
Linguistics. 
Hatzivassiloglou, Vasileios and Kathleen R. 
McKeown. 1995. A quantitative 
evaluation of linguistic tests for the 
automatic prediction of semantic 
markedness. In Proceedings ofthe 33rd 
Annual Meeting, pages 197-204, Boston, 
MA, June. Association for Computational 
Linguistics. 
Hitzeman, Janet, Marc Moens, and Claire 
Grover. 1994. Algorithms for analysing 
the temporal structure of discourse. 
Technical Report, University of 
Edinburgh. 
Holland, John. 1975. Adaptation in Natural 
and Art~cial Systems. The University of 
Michigan Press, Ann Arbor, MI. 
Hwang, Chung Hee and Lenhart K. 
Schubert. 1991. Interpreting temporal 
adverbials. Technical Report, University 
of Rochester. 
Klavans, Judith L. 1994. Linguistic tests over 
large corpora: aspectual classes in the 
lexicon. Technical Report, Columbia 
University Dept. of Computer Science. 
Klavans, Judith L. and Martin Chodorow. 
1992. Degrees of stativity: the lexical 
representation f verb aspect. In 
Proceedings ofthe 14th International 
Conference on Computation Linguistics. 
Knight, K., I. Chander, M. Haines, 
V. Hatzivassiloglou, E. Hovy, M. Iida, 
S. K. Luk, R. Whitney, and K. Yamada. 
1995. Filling knowledge gaps in a 
broad-coverage mtsystem. In Proceedings 
of the International Joint Conference on 
ArtiJicial Intelligence. 
Koza, John R. 1992. Genetic Programming: On 
the Programming of Computers by Means of 
Natural Selection. MIT Press, Cambridge, 
MA. 
Litman, Diane J. 1994. Classifying cue 
phrases in text and speech using machine 
learning. In Proceedings of the Twelfth 
National Conference on Artificial Intelligence, 
Menlo Park, CA, July. AAAI Press. 
Luk, Alpha K. 1995. Statistical sense 
disambiguation with relatively small 
corpora using dictionary definitions. In 
Proceedings ofthe 33rd Annual Meeting, 
Columbus, OH, June. Association for 
Computational Linguistics. 
Magerman, David H. 1993. Parsing as 
statistical pattern recognition. Technical 
Report, IBM. 
Masand, Brij. 1994. Optimizing confidence 
of text classification by evolution of 
626 
Siegel and McKeown Improving Aspectual Classification 
symbolic expressions. In K. Kinnear, 
editor, Advances in Genetic Programming. 
MIT Press, Cambridge, MA. 
McCord, Michael C. 1990. SLOT 
GRAMMAR: A system for simpler 
construction of practical natural language 
grammars. In R. Studer, editor, 
International Symposium on Natural 
Language and Logic. Springer Verlag. 
Mitchell, Tom M. 1997. Machine Learning. The 
McGraw-Hill Companies, Inc., New York. 
Moens, Marc and Mark Steedman. 1988. 
Temporal ontology and temporal 
reference. Computational Linguistics, 14(2). 
Olsen, Mari B. and Philip Resnik. 1997. 
Implicit object constructions and the 
(in)transitivity continuum. In Proceedings 
of the 33rd Regional Meeting of the Chicago 
Linguistics Society, April. 
Passonneau, Rebecca J. 1988. A 
computational model of the semantics of 
tense and aspect. Computational 
Linguistics, 14(2). 
Pereira, Fernando, Naftali Tishby, and 
Lillian Lee. 1993. Distributional clustering 
of English words. In Proceedings ofthe 31st 
Annual Meeting, pages 183-190, 
Columbus, OH, June. Association for 
Computational Linguistics. 
Powers, David M. W. and Christopher C. R. 
Turk. 1989. Machine Learning of Natural 
Language. Springer-Verlag. 
Pustejovsky, James. 1991. The syntax of 
event structure. Cognition, 41(103):47-92. 
Pustejovsky, James. 1995. The Generative 
Lexicon. MIT Press, Cambridge, MA. 
Quinlan, Jim R. 1986. Induction of decision 
trees. Machine Learning, 1(1):81-106. 
Quinlan, Jim R. 1993. C4.5: Programs for 
Machine Learning. Morgan Kaufmann, San 
Mateo, CA. 
Resnik, Philip. 1993. Semantic lasses and 
syntactic ambiguity. In Proceedings ofthe 
ARPA Workshop on Human Language 
Technology, March. 
Resnik, Philip. 1996. Selectional constraints: 
An information-theoretic model and its 
computational realization. Cognition, (61). 
Schfitze, Hinrich. 1992. Dimensions of 
meaning. In Proceedings ofSupercomputing. 
Schubert, Lenhart K. and Chung Hee 
Hwang. 1990. Picking reference events 
from tense trees: A formal, implementable 
theory of English tense-aspect semantics. 
Technical Report, University of Rochester. 
Siegel, Eric V. 1994. Competitively evolving 
decision trees against fixed training cases 
for natural anguage processing. In 
K. Kinnear, editor, Advances in Genetic 
Programming. MIT Press, Cambridge, MA. 
Siegel, Eric V. 1997. Learning methods for 
combining linguistic indicators to classify 
verbs. In Proceedings ofthe Second 
Conference on Empirical Methods in Natural 
Language Processing, Providence, RI, 
August. 
Siegel, Eric V. 1998a. Disambiguating verbs 
with the WordNet category of the direct 
object. In Proceedings ofthe Usage of 
WordNet in Natural Language Processing 
Systems Workshop, Montreal, Canada. 
Siegel, Eric V. 1998b. Linguistic Indicators for 
Language Understanding: Using Machine 
Learning Methods to Combine Corpus-based 
Indicators for Aspectual Classification of 
Clauses. Ph.D. thesis, Columbia University. 
Siegel, Eric V. 1999. Corpus-based linguistic 
indicators for aspectual classification. In
Proceedings ofthe 37th Annual Meeting. 
Association for Computational 
Linguistics. 
Siegel, Eric V. and Kathleen R. McKeown. 
1994. Emergent linguistic rules from 
inducing decision trees: disambiguating 
discourse clue words. In Proceedings ofthe 
Twelfth National Conference on Artificial 
Intelligence, Menlo Park, CA, July. AAAI 
Press. 
Siegel, Eric V. and Kathleen R. McKeown. 
1996. Gathering statistics to aspectually 
classify sentences with a genetic 
algorithm. In K. Oflazer and H. Somers, 
editors, Proceedings ofthe Second 
International Conference on New Methods in 
Language Processing, Ankara, Turkey, 
September, Bilkent University. 
Sjantner, T. J. and D. E. Duffy. 1989. The 
Statistical Analysis of Discrete Data. 
Springer-Verlag, New York. 
Stolfo, Salvatore, Andreas L. Prodromidis, 
Shelley Tselepis, Wenke Lee, and Wei Fan. 
1997. JAM: Java agents for meta-learning 
over distributed atabases. Technical 
Report, Columbia University. 
Stys, Malgorzata E. 1991. Parallel science 
texts in English and Polish: Problems of 
language and communication. Technical 
Report, Warsaw University. Master's 
thesis. 
Syswerda, Gibert. 1989. Uniform crossover 
in genetic algorithms. In J. D. Schaffer, 
editor, Proceedings ofthe Third International 
Conference on Genetic Algorithms. Morgan 
Kaufmarm. 
Tackett, Walter A. 1993. Genetic 
programming for feature discovery and 
image discrimination. In Proceedings ofthe 
Fifth International Conference on Genetic 
Algorithms, San Mateo, CA. Morgan 
Kaufmann. 
Tackett, Walter A. and Aviram Carmi. 1994. 
The donut problem: Scalability, 
627 
Computational Linguistics Volume 26, Number 4 
generalization a d breeding policies in 
genetic programming. In K. Kinnear, 
editor, Advances in Genetic Programming. 
MIT Press, Cambridge, MA. 
Vendler, Zeno. 1967. Verbs and times. In 
Linguistics in Philosophy. CorneU 
University Press, Ithaca, NY. 
Wiebe, Janyce M., Thomas P. O'Hara, 
Thorsten Ohrstr6m-Sandgren, a d 
Kenneth J. McKeever. 1997. An empirical 
approach to temporal reference resolution. 
In Proceedings ofthe Second Conference on 
Empirical Methods in Natural Language 
Processing, Providence, RI, August. 
Wolpert, David H. 1992. Stacked 
generalization. Neural Networks, 5. 
Yarowsky, David. 1994. Decision lists for 
lexical ambiguity resolution: Application 
to accent restoration i Spanish and 
French. In Proceedings ofthe 32nd Annual 
Meeting, San Francisco, CA, June. Morgan 
Kaufmann. Association for 
Computational Linguistics. 
628 
c? 2002 Association for Computational Linguistics
Introduction to the Special Issue on
Summarization
Dragomir R. Radev? Eduard Hovy?
University of Michigan USC/ISI
Kathleen McKeown?
Columbia University
1. Introduction and Definitions
As the amount of on-line information increases, systems that can automatically sum-
marize one or more documents become increasingly desirable. Recent research has
investigated types of summaries, methods to create them, and methods to evaluate
them. Several evaluation competitions (in the style of the National Institute of Stan-
dards and Technology?s [NIST?s] Text Retrieval Conference [TREC]) have helped de-
termine baseline performance levels and provide a limited set of training material.
Frequent workshops and symposia reflect the ongoing interest of researchers around
the world. The volume of papers edited by Mani and Maybury (1999) and a book
(Mani 2001) provide good introductions to the state of the art in this rapidly evolving
subfield.
A summary can be loosely defined as a text that is produced from one or more
texts, that conveys important information in the original text(s), and that is no longer
than half of the original text(s) and usually significantly less than that. Text here is
used rather loosely and can refer to speech, multimedia documents, hypertext, etc.
The main goal of a summary is to present the main ideas in a document in less
space. If all sentences in a text document were of equal importance, producing a sum-
mary would not be very effective, as any reduction in the size of a document would
carry a proportional decrease in its informativeness. Luckily, information content in a
document appears in bursts, and one can therefore distinguish between more and less
informative segments. Identifying the informative segments at the expense of the rest
is the main challenge in summarization.
Of the many types of summary that have been identified (Borko and Bernier 1975;
Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries provide
an idea of what the text is about without conveying specific content, and informative
ones provide some shortened version of the content. Topic-oriented summaries con-
centrate on the reader?s desired topic(s) of interest, whereas generic summaries reflect
the author?s point of view. Extracts are summaries created by reusing portions (words,
sentences, etc.) of the input text verbatim, while abstracts are created by regenerating
? Assistant Professor, School of Information, Department of Electrical Engineering and Computer Science
and Department of Linguistics, University of Michigan, Ann Arbor. E-mail: radev@umich.edu.
? ISI Fellow and Senior Project Leader, Information Sciences Institute of the University of Southern
California, Marina del Rey, CA. E-mail: hovy@isi.edu.
? Professor, Department of Computer Science, New York University, New York, NY. E-mail:
kathy@cs.columbia.edu.
400
Computational Linguistics Volume 28, Number 4
the extracted content. Extraction is the process of identifying important material in
the text, abstraction the process of reformulating it in novel terms, fusion the process
of combining extracted portions, and compression the process of squeezing out unim-
portant material. The need to maintain some degree of grammaticality and coherence
plays a role in all four processes.
The obvious overlap of text summarization with information extraction, and con-
nections from summarization to both automated question answering and natural lan-
guage generation, suggest that summarization is actually a part of a larger picture.
In fact, whereas early approaches drew more from information retrieval, more re-
cent approaches draw from the natural language field. Natural language generation
techniques have been adapted to work with typed textual phrases, in place of se-
mantics, as input, and this allows researchers to experiment with approaches to ab-
straction. Techniques that have been developed for topic-oriented summaries are now
being pushed further so that they can be applied to the production of long answers
for the question-answering task. However, as the articles in this special issue show,
domain-independent summarization has several specific, difficult aspects that make it
a research topic in its own right.
2. Major Approaches
We provide a sketch of the current state of the art of summarization by describing
the general areas of research, including single-document summarization through ex-
traction, the beginnings of abstractive approaches to single-document summarization,
and a variety of approaches to multidocument summarization.
2.1 Single-Document Summarization through Extraction
Despite the beginnings of research on alternatives to extraction, most work today
still relies on extraction of sentences from the original document to form a summary.
The majority of early extraction research focused on the development of relatively
simple surface-level techniques that tend to signal important passages in the source
text. Although most systems use sentences as units, some work with larger passages,
typically paragraphs. Typically, a set of features is computed for each passage, and
ultimately these features are normalized and summed. The passages with the highest
resulting scores are sorted and returned as the extract.
Early techniques for sentence extraction computed a score for each sentence based
on features such as position in the text (Baxendale 1958; Edmundson 1969), word
and phrase frequency (Luhn 1958), key phrases (e.g., ?it is important to note?) (Ed-
mundson 1969). Recent extraction approaches use more sophisticated techniques for
deciding which sentences to extract; these techniques often rely on machine learning
to identify important features, on natural language analysis to identify key passages,
or on relations between words rather than bags of words.
The application of machine learning to summarization was pioneered by Kupiec,
Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier
to combine features from a corpus of scientific articles and their abstracts. Aone et
al. (1999) and Lin (1999) experimented with other forms of machine learning and its
effectiveness. Machine learning has also been applied to learning individual features;
for example, Lin and Hovy (1997) applied machine learning to the problem of de-
termining how sentence position affects the selection of sentences, and Witbrock and
Mittal (1999) used statistical approaches to choose important words and phrases and
their syntactic context.
401
Radev, Hovy, and McKeown Summarization: Introduction
Approaches involving more sophisticated natural language analysis to identify key
passages rely on analysis either of word relatedness or of discourse structure. Some
research uses the degree of lexical connectedness between potential passages and the
remainder of the text; connectedness may be measured by the number of shared words,
synonyms, or anaphora (e.g., Salton et al 1997; Mani and Bloedorn 1997; Barzilay
and Elhadad 1999). Other research rewards passages that include topic words, that is,
words that have been determined to correlate well with the topic of interest to the user
(for topic-oriented summaries) or with the general theme of the source text (Buckley
and Cardie 1997; Strzalkowski et al 1999; Radev, Jing, and Budzikowska 2000).
Alternatively, a summarizer may reward passages that occupy important positions
in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This
method requires a system to compute discourse structure reliably, which is not possible
in all genres. This technique is the focus of one of the articles in this special issue (Teufel
and Moens 2002), which shows how particular types of rhetorical relations in the genre
of scientific journal articles can be reliably identified through the use of classification.
An open-source summarization environment, MEAD, was recently developed at the
Johns Hopkins summer workshop (Radev et al 2002). MEAD allows researchers to
experiment with different features and methods for combination.
Some recent work (Conroy and O?Leary 2001) has turned to the use of hidden
Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the
probability of inclusion of a sentence in an extract depends on whether the previous
sentence has been included as well.
2.2 Single-Document Summarization through Abstraction
At this early stage in research on summarization, we categorize any approach that
does not use extraction as an abstractive approach. Abstractive approaches have used
information extraction, ontological information, information fusion, and compression.
Information extraction approaches can be characterized as ?top-down,? since they
look for a set of predefined information types to include in the summary (in con-
trast, extractive approaches are more data-driven). For each topic, the user predefines
frames of expected information types, together with recognition criteria. For example,
an earthquake frame may contain slots for location, earthquake magnitude, number of
casualties, etc. The summarization engine must then locate the desired pieces of infor-
mation, fill them in, and generate a summary with the results (DeJong 1978; Rau and
Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in
restricted domains only.
Compressive summarization results from approaching the problem from the point
of view of language generation. Using the smallest units from the original document,
Witbrock and Mittal (1999) extract a set of words from the input document and then
order the words into sentences using a bigram language model. Jing and McKeown
(1999) point out that human summaries are often constructed from the source docu-
ment by a process of cutting and pasting document fragments that are then combined
and regenerated as summary sentences. Hence a summarizer can be developed to
extract sentences, reduce them by dropping unimportant fragments, and then use in-
formation fusion and generation to combine the remaining fragments. In this special
issue, Jing (2002) reports on automated techniques to build a corpus representing the
cut-and-paste process used by humans; such a corpus can then be used to train an
automated summarizer.
Other researchers focus on the reduction process. In an attempt to learn rules for
reduction, Knight and Marcu (2000) use expectation maximization to train a system
to compress the syntactic parse tree of a sentence in order to produce a shorter but
402
Computational Linguistics Volume 28, Number 4
still maximally grammatical version. Ultimately, this approach can likely be used for
shortening two sentences into one, three into two (or one), and so on.
Of course, true abstraction involves taking the process one step further. Abstraction
involves recognizing that a set of extracted passages together constitute something
new, something that is not explicitly mentioned in the source, and then replacing them
in the summary with the (ideally more concise) new concept(s). The requirement that
the new material not be in the text explicitly means that the system must have access
to external information of some kind, such as an ontology or a knowledge base, and be
able to perform combinatory inference (Hahn and Reimer 1997). Since no large-scale
resources of this kind yet exist, abstractive summarization has not progressed beyond
the proof-of-concept stage (although top-down information extraction can be seen as
one variant).
2.3 Multidocument Summarization
Multidocument summarization, the process of producing a single summary of a set
of related source documents, is relatively new. The three major problems introduced
by having to handle multiple input documents are (1) recognizing and coping with
redundancy, (2) identifying important differences among documents, and (3) ensuring
summary coherence, even when material stems from different source documents.
In an early approach to multidocument summarization, information extraction
was used to facilitate the identification of similarities and differences (McKeown and
Radev 1995). As for single-document summarization, this approach produces more of a
briefing than a summary, as it contains only preidentified information types. Identity of
slot values are used to determine when information is reliable enough to include in the
summary. Later work merged information extraction approaches with regeneration of
extracted text to improve summary generation (Radev and McKeown 1998). Important
differences (e.g., updates, trends, direct contradictions) are identified through a set of
discourse rules. Recent work also follows this approach, using enhanced information
extraction and additional forms of contrasts (White and Cardie 2002).
To identify redundancy in text documents, various similarity measures are used.
A common approach is to measure similarity between all pairs of sentences and then
use clustering to identify themes of common information (McKeown et al 1999; Radev,
Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure
the similarity of a candidate passage to that of already-selected passages and retain
it only if it contains enough new (dissimilar) information. A popular such measure is
maximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonell
and Goldstein 1998).
Once similar passages in the input documents have been identified, the infor-
mation they contain must be included in the summary. Rather than simply listing
all similar sentences (a lengthy solution), some approaches will select a representa-
tive passage to convey information in each cluster (Radev, Jing, and Budzikowska
2000), whereas other approaches use information fusion techniques to identify repet-
itive phrases from the clusters and combine the phrases into the summary (Barzilay,
McKeown, and Elhadad 1999). Mani, Gates, and Bloedorn (1999) describe the use of
human-generated compression and reformulation rules.
Ensuring coherence is difficult, because this in principle requires some understand-
ing of the content of each passage and knowledge about the structure of discourse.
In practice, most systems simply follow time order and text order (passages from
the oldest text appear first, sorted in the order in which they appear in the input).
To avoid misleading the reader when juxtaposed passages from different dates all
say ?yesterday,? some systems add explicit time stamps (Lin and Hovy 2002a). Other
403
Radev, Hovy, and McKeown Summarization: Introduction
systems use a combination of temporal and coherence constraints to order sentences
(Barzilay, Elhadad, and McKeown 2001). Recently, Otterbacher, Radev, and Luo (2002)
have focused on discourse-based revisions of multidocument clusters as a means for
improving summary coherence.
Although multidocument summarization is new and the approaches described
here are only the beginning, current research also branches out in other directions. Re-
search is beginning on the generation of updates on new information (Allan, Gupta,
and Khandelwal 2001). Researchers are currently studying the production of longer
answers (i.e., multidocument summaries) from retrieved documents, focusing on such
types as biographies of people, descriptions of multiple events of the same type
(e.g., multiple hurricanes), opinion pieces (e.g., editorials and letters discussing a con-
tentious topic), and causes of events. Another challenging ongoing topic is the gener-
ation of titles for either a single document or set of documents. This challenge will be
explored in an evaluation planned by NIST in 2003.
2.4 Evaluation
Evaluating the quality of a summary has proven to be a difficult problem, principally
because there is no obvious ?ideal? summary. Even for relatively straightforward news
articles, human summarizers tend to agree only approximately 60% of the time, mea-
suring sentence content overlap. The use of multiple models for system evaluation
could help alleviate this problem, but researchers also need to look at other methods
that can yield more acceptable models, perhaps using a task as motivation.
Two broad classes of metrics have been developed: form metrics and content met-
rics. Form metrics focus on grammaticality, overall text coherence, and organization
and are usually measured on a point scale (Brandow, Mitze, and Rau 1995). Content is
more difficult to measure. Typically, system output is compared sentence by sentence
or fragment by fragment to one or more human-made ideal abstracts, and as in in-
formation retrieval, the percentage of extraneous information present in the system?s
summary (precision) and the percentage of important information omitted from the
summary (recall) are recorded. Other commonly used measures include kappa (Car-
letta 1996) and relative utility (Radev, Jing, and Budzikowska 2000), both of which take
into account the performance of a summarizer that randomly picks passages from the
original document to produce an extract. In the Document Understanding Conference
(DUC)-01 and DUC-02 summarization competitions (Harman and Marcu 2001; Hahn
and Harman 2002), NIST used the Summary Evaluation Environment (SEE) interface
(Lin 2001) to record values for precision and recall. These two competitions, run along
the lines of TREC, have served to establish overall baselines for single-document and
multidocument summarization and have provided several hundred human abstracts
as training material. (Another popular source of training material is the Ziff-Davis cor-
pus of computer product announcements.) Despite low interjudge agreement, DUC
has shown that humans are better summary producers than machines and that, for
the news article genre, certain algorithms do in fact do better than the simple baseline
of picking the lead material.
The largest task-oriented evaluation to date, the Summarization Evaluation Con-
ference (SUMMAC) (Mani et al 1998; Firmin and Chrzanowski 1999) included three
tests: the categorization task (how well can humans categorize a summary compared
to its full text?), the ad hoc task (how well can humans determine whether a full text is
relevant to a query just from reading the summary?) and the question task (how well
can humans answer questions about the main thrust of the source text from reading
just the summary?). But the interpretation of the results is not simple; studies (Jing et
al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000)
404
Computational Linguistics Volume 28, Number 4
show how the same summaries receive different scores under different measures or
when compared to different (but presumably equivalent) ideal summaries created by
humans. With regard to interhuman agreement, Jing et al find fairly high consistency
in the news genre only when the summary (extract) length is fixed relatively short.
Marcu (1997a) provides some evidence that other genres will deliver less consistency.
With regard to the lengths of the summaries produced by humans when not con-
strained by a particular compression rate, both Jing and Marcu find great variation.
Nonetheless, it is now generally accepted that for single news articles, systems produce
generic summaries indistinguishable from those of humans.
Automated summary evaluation is a gleam in everyone?s eye. Clearly, when an
ideal extract has been created by human(s), extractive summaries are easy to evalu-
ate. Marcu (1999) and Goldstein et al (1999) independently developed an automated
method to create extracts corresponding to abstracts. But when the number of available
extracts is not sufficient, it is not clear how to overcome the problems of low inter-
human agreement. Simply using a variant of the Bilingual Evaluation Understudy
(BLEU) scoring method (based on a linear combination of matching n-grams between
the system output and the ideal summary) developed for machine translation (Pap-
ineni et al 2001) is promising but not sufficient (Lin and Hovy 2002b).
3. The Articles in this Issue
The articles in this issue move beyond the current state of the art in various ways.
Whereas most research to date has focused on the use of sentence extraction for sum-
marization, we are beginning to see techniques that allow a system to extract, merge,
and edit phrases, as opposed to full sentences, to generate a summary. Whereas many
summarization systems are designed for summarization of news, new algorithms are
summarizing much longer and more complex documents, such as scientific journal
articles, medical journal articles, or patents. Whereas most research to date has fo-
cused on text summarization, we are beginning to see a move toward summarization
of speech, a medium that places additional demands on the summarization process.
Finally, in addition to providing full summarization systems, the articles in this issue
also focus on tools that can aid in the process of developing summarization systems,
on computational efficiency of algorithms, and on techniques needed for preprocessing
speech.
The four articles that focus on summarization of text share a common theme:
Each views the summarization process as consisting of two phases. In the first, mate-
rial within the original document that is important is identified and extracted. In the
second, this extracted material may be modified, merged, and edited using genera-
tion techniques. Two of the articles focus on the extraction stage (Teufel and Moens
2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically
constructing resources that can be used for the second stage.
Teufel and Moens propose significantly different techniques for sentence extraction
than have been used in the past. Noting the difference in both length and structure
between scientific articles and news, they claim that both the context of sentences and
a more focused search for sentences is needed in order to produce a good summary
that is only 2.5% of the original document. Their approach is to provide a summary
that focuses on the new contribution of the paper and its relation to previous work.
They rely on rhetorical relations to provide information about context and to identify
sentences relating to, for example, the aim of the paper, its basis in previous work,
or contrasts with other work. Their approach features the use of corpora annotated
both with rhetorical relations and with relevance; it uses text categorization to extract
405
Radev, Hovy, and McKeown Summarization: Introduction
sentences corresponding to any of seven rhetorical categories. The result is a set of
sentences that situate the article in respect to its original claims and in relation to other
research.
Silber and McCoy focus on computationally efficient algorithms for sentence ex-
traction. They present a linear time algorithm to extract lexical chains from a source
document (the lexical-chain approach was originally developed by Barzilay and El-
hadad [1997] but used an exponential time algorithm). This approach facilitates the
use of lexical chains as an intermediate representation for summarization. Barzilay and
Elhadad present an evaluation of the approach for summarization with both scientific
documents and university textbooks.
Jing advocates the use of a cut-and-paste approach to summarization in which
phrases, rather than sentences, are extracted from the original document. She shows
that such an approach is often used by human abstractors. She then presents an auto-
mated tool that is used to analyze a corpus of paired documents and abstracts written
by humans, in order to identify the phrases within the documents that are used in
the abstracts. She has developed an HMM solution to the matching problem. The
decomposition program is a tool that can produce training and testing corpora for
summarization, and its results have been used for her own summarization program.
Saggion and Lapalme (2002) describe a system, SumUM, that generates indicative-
informative summaries from technical documents. To build their system, Saggion and
Lapalme have studied a corpus of professionally written (short) abstracts. They have
manually aligned the abstracts and the original documents. Given the structured form
of technical papers, most of the information in the abstracts was also found in either the
author abstract (20%) or in the first section of the paper (40%) or the headlines or cap-
tions (23%). Based on their observations, the authors have developed an approach to
summarization, called selective analysis, which mimics the human abstractors? routine.
The four components of selective analysis are indicative selection, informative selection,
indicative generation, and informative generation.
The final article in the issue (Zechner 2002) is distinct from the other articles in
that it addresses problems in summarization of speech. As in text summarization,
Zechner also uses sentence extraction to determine the content of the summary. Given
the informal nature of speech, however, a number of significant steps must be taken
in order to identify useful segments for extraction. Zechner develops techniques for
removing disfluencies from speech, for identifying units for extraction that are in
some sense equivalent to sentences, and for identifying relations such as question-
answer across turns in order to determine when units from two separate turns should
be extracted as a whole. This preprocessing yields a transcript on which standard
techniques for extraction in text (here the use of MMR [Carbonell and Goldstein 1998]
to identify relevant units) can operate successfully.
Though true abstractive summarization remains a researcher?s dream, the success
of extractive summarizers and the rapid development of compressive and similar
techniques testifies to the effectiveness with which the research community can address
new problems and find workable solutions to them.
References
Allan, James, Rahul Gupta, and Vikas
Khandelwal. 2001. Temporal summaries
of news topics. In Proceedings of the 24th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 10?18.
Aone, Chinatsu, Mary Ellen Okurowski,
James Gorlinsky, and Bjornar Larsen.
1999. A trainable summarizer with
knowledge acquired from robust NLP
techniques. In I. Mani and M. T. Maybury,
editors, Advances in Automatic Text
Summarization. MIT Press, Cambridge,
pages 71?80.
Barzilay, Regina and Michael Elhadad. 1997.
406
Computational Linguistics Volume 28, Number 4
Using lexical chains for text
summarization. In Proceedings of the
ACL/EACL?97 Workshop on Intelligent
Scalable Text Summarization, pages 10?17,
Madrid, July.
Barzilay, Regina and Michael Elhadad. 1999.
Using lexical chains for text
summarization. In I. Mani and M. T.
Maybury, editors, Advances in Automatic
Text Summarization. MIT Press,
Cambridge, pages 111?121.
Barzilay, Regina, Noe?mie Elhadad, and
Kathy McKeown. 2001. Sentence ordering
in multidocument summarization. In
Proceedings of the Human Language
Technology Conference.
Barzilay, Regina, Kathleen McKeown, and
Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, College Park,
MD, 20?26 June, pages 550?557.
Baxendale, P. B. 1958. Man-made index for
technical literature?An experiment. IBM
Journal of Research and Development,
2(4):354?361.
Borko, H. and C. Bernier. 1975. Abstracting
Concepts and Methods. Academic Press,
New York.
Brandow, Ron, Karl Mitze, and Lisa F. Rau.
1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing and
Management, 31(5):675?685.
Buckley, Chris and Claire Cardie. 1997.
Using empire and smart for
high-precision IR and summarization. In
Proceedings of the TIPSTER Text Phase III
12-Month Workshop, San Diego, CA,
October.
Carbonell, Jaime, Y. Geng, and Jade
Goldstein. 1997. Automated
query-relevant summarization and
diversity-based reranking. In Proceedings
of the IJCAI-97 Workshop on AI in Digital
Libraries, pages 12?19.
Carbonell, Jaime G. and Jade Goldstein.
1998. The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Alistair Moffat
and Justin Zobel, editors, Proceedings of the
21st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Melbourne,
Australia, pages 335?336.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Conroy, John and Dianne O?Leary. 2001.
Text summarization via hidden Markov
models. In Proceedings of the 24th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 406?407.
Cremmins, Edward T. 1996. The Art of
Abstracting. Information Resources Press,
Arlington, VA, second edition.
DeJong, Gerald Francis. 1978. Fast Skimming
of News Stories: The FRUMP System. Ph.D.
thesis, Yale University, New Haven, CT.
Donaway, R. L., K. W. Drummey, and
L. A. Mather. 2000. A comparison of
rankings produced by summarization
evaluation measures. In Proceedings of the
Workshop on Automatic Summarization,
ANLP-NAACL2000, Association for
Computational Linguistics, 30 April,
pages 69?78.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264?285.
Firmin, T. and M. J. Chrzanowski. 1999. An
evaluation of automatic text
summarization systems. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 325?336.
Goldstein, Jade, Mark Kantrowitz, Vibhu O.
Mittal, and Jaime G. Carbonell. 1999.
Summarizing text documents: Sentence
selection and evaluation metrics. In
Research and Development in Information
Retrieval, pages 121?128, Berkeley, CA.
Hahn, Udo and Donna Harman, editors.
2002. Proceedings of the Document
Understanding Conference (DUC-02).
Philadelphia, July.
Hahn, Udo and Ulrich Reimer. 1997.
Knowledge-based text summarization:
Salience and generalization operators for
knowledge base abstraction. In I. Mani
and M. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 215?232.
Harman, Donna and Daniel Marcu, editors.
2001. Proceedings of the Document
Understanding Conference (DUC-01). New
Orleans, September.
Hovy, E. and C.-Y. Lin. 1999. Automated
text summarization in SUMMARIST. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 81?94.
Jing, Hongyan. 2002. Using hidden Markov
modeling to decompose human-written
summaries. Computational Linguistics,
28(4), 527?543.
Jing, Hongyan and Kathleen McKeown.
1999. The decomposition of
human-written summary sentences. In
407
Radev, Hovy, and McKeown Summarization: Introduction
M. Hearst, F. Gey, and R. Tong, editors,
Proceedings of SIGIR?99: 22nd International
Conference on Research and Development in
Information Retrieval, University of
California, Berkeley, August,
pages 129?136.
Jing, Hongyan, Kathleen McKeown, Regina
Barzilay, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In Intelligent
Text Summarization: Papers from the 1998
AAAI Spring Symposium, Stanford, CA,
23?25 March. Technical Report SS-98-06.
AAAI Press, pages 60?68.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence
(AAAI-2000), pages 703?710.
Kupiec, Julian, Jan O. Pedersen, and
Francine Chen. 1995. A trainable
document summarizer. In Research and
Development in Information Retrieval,
pages 68?73.
Lin, C. and E. Hovy. 1997. Identifying topics
by position. In Fifth Conference on Applied
Natural Language Processing, Association
for Computational Linguistics, 31
March?3 April, pages 283?290.
Lin, Chin-Yew. 1999. Training a selection
function for extraction. In Proceedings of
the Eighteenth Annual International ACM
Conference on Information and Knowledge
Management (CIKM), Kansas City, 6
November. ACM, pages 55?62.
Lin, Chin-Yew. 2001. Summary evaluation
environment.
http://www.isi.edu/cyl/SEE.
Lin, Chin-Yew and Eduard Hovy. 2002a.
From single to multi-document
summarization: A prototype system and
its evaluation. In Proceedings of the 40th
Conference of the Association of
Computational Linguistics, Philadelphia,
July, pages 457?464.
Lin, Chin-Yew and Eduard Hovy. 2002b.
Manual and automatic evaluation of
summaries. In Proceedings of the Document
Understanding Conference (DUC-02)
Workshop on Multi-Document Summarization
Evaluation at the ACL Conference,
Philadelphia, July, pages 45?51.
Luhn, H. P. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
Development, 2(2):159?165.
Mani, Inderjeet. 2001. Automatic
Summarization. John Benjamins,
Amsterdam/Philadelphia.
Mani, Inderjeet and Eric Bloedorn. 1997.
Multi-document summarization by graph
search and matching. In Proceedings of the
Fourteenth National Conference on Artificial
Intelligence (AAAI-97), Providence, RI.
American Association for Artificial
Intelligence, pages 622?628.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL 99), College
Park, MD, June, pages 558?565.
Mani, Inderjeet, David House, G. Klein,
Lynette Hirshman, Leo Obrst, The?re`se
Firmin, Michael Chrzanowski, and Beth
Sundheim. 1998. The TIPSTER SUMMAC
text summarization evaluation. Technical
Report MTR 98W0000138, The Mitre
Corporation, McLean, VA.
Mani, Inderjeet and Mark Maybury, editors.
1999. Advances in Automatic Text
Summarization. MIT Press, Cambridge.
Marcu, Daniel. 1997a. From discourse
structures to text summaries. In
Proceedings of the ACL?97/EACL?97 Workshop
on Intelligent Scalable Text Summarization,
Madrid, July 11, pages 82?88.
Marcu, Daniel. 1997b. The Rhetorical Parsing,
Summarization, and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto, Toronto.
Marcu, Daniel. 1999. The automatic
construction of large-scale corpora for
summarization research. In M. Hearst,
F. Gey, and R. Tong, editors, Proceedings of
SIGIR?99: 22nd International Conference on
Research and Development in Information
Retrieval, University of California,
Berkeley, August, pages 137?144.
Marcu, Daniel and Laurie Gerber. 2001. An
inquiry into the nature of multidocument
abstracts, extracts, and their evaluation. In
Proceedings of the NAACL-2001 Workshop on
Automatic Summarization, Pittsburgh, June.
NAACL, pages 1?8.
McKeown, Kathleen, Judith Klavans,
Vasileios Hatzivassiloglou, Regina
Barzilay, and Eleazar Eskin. 1999.
Towards multidocument summarization
by reformulation: Progress and prospects.
In Proceedings of the 16th National
Conference of the American Association for
Artificial Intelligence (AAAI-1999), 18?22
July, pages 453?460.
McKeown, Kathleen R. and Dragomir R.
Radev. 1995. Generating summaries of
multiple news articles. In Proceedings of the
18th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Seattle, July,
pages 74?82.
Ono, K., K. Sumita, and S. Miike. 1994.
408
Computational Linguistics Volume 28, Number 4
Abstract generation based on rhetorical
structure extraction. In Proceedings of the
International Conference on Computational
Linguistics, Kyoto, Japan, pages 344?348.
Otterbacher, Jahna, Dragomir R. Radev, and
Airong Luo. 2002. Revisions that improve
cohesion in multi-document summaries:
A preliminary study. In ACL Workshop on
Text Summarization, Philadelphia.
Papineni, K., S. Roukos, T. Ward, and W-J.
Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation.
Research Report RC22176, IBM.
Radev, Dragomir, Simone Teufel, Horacio
Saggion, Wai Lam, John Blitzer, Arda
C?elebi, Hong Qi, Elliott Drabek, and
Danyu Liu. 2002. Evaluation of text
summarization in a cross-lingual
information retrieval framework.
Technical Report, Center for Language
and Speech Processing, Johns Hopkins
University, Baltimore, June.
Radev, Dragomir R., Hongyan Jing, and
Malgorzata Budzikowska. 2000.
Centroid-based summarization of
multiple documents: Sentence extraction,
utility-based evaluation, and user studies.
In ANLP/NAACL Workshop on
Summarization, Seattle, April.
Radev, Dragomir R. and Kathleen R.
McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469?500.
Rau, Lisa and Paul Jacobs. 1991. Creating
segmented databases from free text for
text retrieval. In Proceedings of the 14th
Annual International ACM-SIGIR Conference
on Research and Development in Information
Retrieval, New York, pages 337?346.
Saggion, Horacio and Guy Lapalme. 2002.
Generating indicative-informative
summaries with SumUM. Computational
Linguistics, 28(4), 497?526.
Salton, G., A. Singhal, M. Mitra, and
C. Buckley. 1997. Automatic text
structuring and summarization.
Information Processing & Management,
33(2):193?207.
Silber, H. Gregory and Kathleen McCoy.
2002. Efficiently computed lexical chains
as an intermediate representation for
automatic text summarization.
Computational Linguistics, 28(4), 487?496.
Sparck Jones, Karen. 1999. Automatic
summarizing: Factors and directions. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 1?13.
Strzalkowski, Tomek, Gees Stein, J. Wang,
and Bowden Wise. 1999. A robust
practical text summarizer. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 137?154.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4),
409?445.
White, Michael and Claire Cardie. 2002.
Selecting sentences for multidocument
summaries using randomized local
search. In Proceedings of the Workshop on
Automatic Summarization (including DUC
2002), Philadelphia, July. Association for
Computational Linguistics, New
Brunswick, NJ, pages 9?18.
Witbrock, Michael and Vibhu Mittal. 1999.
Ultra-summarization: A statistical
approach to generating highly condensed
non-extractive summaries. In Proceedings
of the 22nd Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Berkeley,
pages 315?316.
Zechner, Klaus. 2002. Automatic
summarization of open-domain
multiparty dialogues in diverse genres.
Computational Linguistics, 28(4), 447?485.
Sentence Fusion for Multidocument
News Summarization
Regina Barzilay?
Massachusetts Institute of Technology
Kathleen R. McKeown?
Columbia University
A system that can produce informative summaries, highlighting common information found in
many online documents, will help Web users to pinpoint information that they need without
extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation
technique for synthesizing common information across documents. Sentence fusion involves
bottom-up local multisequence alignment to identify phrases conveying similar information and
statistical generation to combine common phrases into a sentence. Sentence fusion moves the
summarization field from the use of purely extractive methods to the generation of abstracts that
contain sentences not found in any of the input documents and can synthesize information across
sources.
1. Introduction
Redundancy in large text collections, such as the Web, creates both problems and
opportunities for natural language systems. On the one hand, the presence of numer-
ous sources conveying the same information causes difficulties for end users of search
engines and news providers; they must read the same information over and over again.
On the other hand, redundancy can be exploited to identify important and accurate
information for applications such as summarization and question answering (Mani
and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke,
Cormack, and Lynam 2001; Dumais et al 2002; Chu-Carroll et al 2003). Clearly, it would
be highly desirable to have a mechanism that could identify common information
among multiple related documents and fuse it into a coherent text. In this article, we
present a method for sentence fusion that exploits redundancy to achieve this task in
the context of multidocument summarization.
A straightforward approach for approximating sentence fusion can be found in the
use of sentence extraction for multidocument summarization (Carbonell and Goldstein
1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy
2002). Once a system finds a set of sentences that convey similar information (e.g.,
by clustering), one of these sentences is selected to represent the set. This is a robust
approach that is always guaranteed to output a grammatical sentence. However, ex-
traction is only a coarse approximation of fusion. An extracted sentence may include
not only common information, but additional information specific to the article from
? Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology,
Cambridge, MA 02458. E-mail: regina@csail.mit.edu.
? Department of Computer Science, Columbia University, New York, NY 10027.
E-mail: kathy@cs.columbia.edu.
Submission received: 14 September 2003; revised submission received: 23 February 2005; accepted for
publication: 19 March 2005.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
which it came, leading to source bias and aggravating fluency problems in the extracted
summary. Attempting to solve this problem by including more sentences to restore the
original context might lead to a verbose and repetitive summary.
Instead, we want a fine-grained approach that can identify only those pieces of
sentences that are common. Language generation offers an appealing approach to the
problem, but the use of generation in this context raises significant research challenges.
In particular, generation for sentence fusion must be able to operate in a domain-
independent fashion, scalable to handle a large variety of input documents with various
degrees of overlap. In the past, generation systems were developed for limited domains
and required a rich semantic representation as input. In contrast, for this task we require
text-to-text generation, the ability to produce a new text given a set of related texts as
input. If language generation can be scaled to take fully formed text as input without
semantic interpretation, selecting content and producing well-formed English sentences
as output, then generation has a large potential payoff.
In this article, we present the concept of sentence fusion, a novel text-to-text gen-
eration technique which, given a set of similar sentences, produces a new sentence
containing the information common to most sentences in the set. The research chal-
lenges in developing such an algorithm lie in two areas: identification of the fragments
conveying common information and combination of the fragments into a sentence.
To identify common information, we have developed a method for aligning syntac-
tic trees of input sentences, incorporating paraphrasing information. Our alignment
problem poses unique challenges: We only want to match a subset of the subtrees in
each sentence and are given few constraints on permissible alignments (e.g., arising
from constituent ordering, start or end points). Our algorithm meets these challenges
through bottom-up local multisequence alignment, using words and paraphrases as
anchors. Combination of fragments is addressed through construction of a fusion lattice
encompassing the resulting alignment and linearization of the lattice into a sentence
using a language model. Our approach to sentence fusion thus features the integration
of robust statistical techniques, such as local, multisequence alignment and language
modeling, with linguistic representations automatically derived from input documents.
Sentence fusion is a significant first step toward the generation of abstracts, as
opposed to extracts (Borko and Bernier 1975), for multidocument summarization. Un-
like extraction methods (used by the vast majority of summarization researchers), sen-
tence fusion allows for the true synthesis of information from a set of input documents.
It has been shown that combining information from several sources is a natural strat-
egy for multidocument summarization. Analysis of human-written summaries reveals
that most sentences combine information drawn from multiple documents (Banko and
Vanderwende 2004). Sentence fusion achieves this goal automatically. Our evaluation
shows that our approach is promising, with sentence fusion outperforming sentence
extraction for the task of content selection.
This article focuses on the implementation and evaluation of the sentence fu-
sion method within the multidocument summarization system MultiGen, which daily
summarizes multiple news articles on the same event as part1 of Columbia?s news
browsing system Newsblaster (http://newsblaster.cs.columbia.edu/). In the next sec-
tion, we provide an overview of MultiGen, focusing on components that produce input
or operate over output of sentence fusion. In Section 3, we provide an overview of
1 In addition to MultiGen, Newsblaster utilizes another summarizer, DEMS (Schiffman, Nenkova, and
McKeown 2002), to summarize heterogeneous sets of articles.
298
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
our fusion algorithm and detail on its main steps: identification of common infor-
mation (Section 3.1), fusion lattice computation (Section 3.2), and lattice linearization
(Section 3.3). Evaluation results and their analysis are presented in Section 4. Analy-
sis of the system?s output reveals the capabilities and the weaknesses of our text-
to-text generation method and identifies interesting challenges that will require new
insights. An overview of related work and a discussion of future directions conclude
the article.
2. Framework for Sentence Fusion: MultiGen
Sentence fusion is the central technique used within the MultiGen summarization
system. MultiGen takes as input a cluster of news stories on the same event and
produces a summary which synthesizes common information across input stories. An
example of a MultiGen summary is shown in Figure 1. The input clusters are automati-
cally produced from a large quantity of news articles that are retrieved by Newsblaster
from 30 news sites each day.
In order to understand the role of sentence fusion within summarization, we
overview the MultiGen architecture, providing details on the processes that precede
sentence fusion and thus, the input that the fusion component requires. Fusion itself is
discussed in the subsequent sections of the article.
MultiGen follows a pipeline architecture, shown in Figure 2. The analysis com-
ponent of the system, Simfinder (Hatzivassiloglou, Klavans, and Eskin 1999) clusters
sentences of input documents into themes, groups of sentences that convey similar
information (Section 2.1). Once themes are constructed, the system selects a subset of
the groups to be included in the summary, depending on the desired compression
Figure 1
An example of MultiGen summary as shown in the Columbia Newsblaster Interface. Summary
phrases are followed by parenthetical numbers indicating their source articles. The last sentence
is extracted because it was repeated verbatim in several input articles.
299
Computational Linguistics Volume 31, Number 3
Figure 2
MultiGen architecture.
length (Section 2.2). The selected groups are passed to the ordering component, which
selects a complete order among themes (Section 2.3).
2.1 Theme Construction
The analysis component of MultiGen, Simfinder, identifies themes, groups of sen-
tences from different documents that each say roughly the same thing. Each theme will
ultimately correspond to at most one sentence in the output summary, generated by
the fusion component, and there may be many themes for a set of articles. An example
of a theme is shown in Table 1. As the set of sentences in the table illustrates, sentences
within a theme are not exact repetitions of each other; they usually include phrases
expressing information that is not common to all sentences in the theme. Information
that is common across sentences is shown in the table in boldface; other portions of
the sentence are specific to individual articles. If one of these sentences were used as
is to represent the theme, the summary would contain extraneous information. Also,
errors in clustering might result in the inclusion of some unrelated sentences. Evalua-
tion involving human judges revealed that Simfinder identifies similar sentences with
49.3% precision at 52.9% recall (Hatzivassiloglou, Klavans, and Eskin 1999). We will
discuss later how this error rate influences sentence fusion.
To identify themes, Simfinder extracts linguistically motivated features for each
sentence, including WordNet synsets (Miller et al 1990) and syntactic dependencies,
such as subject?verb and verb?object relations. A log-linear regression model is used
to combine the evidence from the various features into a single similarity value. The
model was trained on a large set of sentences which were manually marked for similar-
ity. The output of the model is a listing of real-valued similarity values on sentence pairs.
These similarity values are fed into a clustering algorithm that partitions the sentences
into closely related groups.
Table 1
Theme with corresponding fusion sentence.
1. IDF Spokeswoman did not confirm this, but said the Palestinians fired an antitank missile at
a bulldozer.
2. The clash erupted when Palestinian militants fired machine guns and antitank missiles at a
bulldozer that was building an embankment in the area to better protect Israeli forces.
3. The army expressed ?regret at the loss of innocent lives? but a senior commander said troops
had shot in self-defense after being fired at while using bulldozers to build a new embankment
at an army base in the area.
Fusion sentence: Palestinians fired an antitank missile at a bulldozer.
300
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
2.2 Theme Selection
To generate a summary of predetermined length, we induce a ranking on the themes
and select the n highest.2 This ranking is based on three features of the theme: size
measured as the number of sentences, similarity of sentences in a theme, and salience
score. The first two of these scores are produced by Simfinder, and the salience score is
computed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) as
described below. Combining different rankings further filters common information in
terms of salience. Since each of these scores has a different range of values, we perform
ranking based on each score separately, then induce total ranking by summing ranks
from individual categories:
Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme)
+ Rank (Sum of lexical chain scores in theme)
Lexical chains?sequences of semantically related words?are tightly connected to
the lexical cohesive structure of the text and have been shown to be useful for determin-
ing which sentences are important for single-document summarization (Barzilay and
Elhadad 1997; Silber and McCoy 2002). In the multidocument scenario, lexical chains
can be adapted for theme ranking based on the salience of theme sentences within their
original documents. Specifically, a theme that has many sentences ranked high by lexical
chains as important for a single-document summary is, in turn, given a higher salience
score for the multidocument summary. In our implementation, a salience score for a
theme is computed as the sum of lexical chain scores of each sentence in a theme.
2.3 Theme Ordering
Once we filter out the themes that have a low rank, the next task is to order the selected
themes into coherent text. Our ordering strategy aims to capture chronological order
of the main events and ensure coherence. To implement this strategy in MultiGen, we
select for each theme the sentence which has the earliest publication time (theme time
stamp). To increase the coherence of the output text, we identify blocks of topically
related themes and then apply chronological ordering on blocks of themes using theme
time stamps (Barzilay, Elhadad, and McKeown 2002). These stages produce a sorted set
of themes which are passed as input to the sentence fusion component, described in the
next section.
3. Sentence Fusion
Given a group of similar sentences?a theme?the problem is to create a concise and
fluent fusion of information, reflecting facts common to all sentences. (An example of a
fusion sentence is shown in Table 1.) To achieve this goal we need to identify phrases
common to most theme sentences, then combine them into a new sentence.
At one extreme, we might consider a shallow approach to the fusion problem,
adapting the ?bag of words? approach. However, sentence intersection in a set-theoretic
sense produces poor results. For example, the intersection of the first two sentences
2 Typically, Simfinder produces at least 20 themes given an average Newsblaster cluster of nine articles.
The length of a generated summary typically does not exceed seven sentences.
301
Computational Linguistics Volume 31, Number 3
from the theme shown in Table 1 is (the, fired, antitank, at, a, bulldozer). Besides its
being ungrammatical, it is impossible to understand what event this intersection de-
scribes. The inadequacy of the bag-of-words method to the fusion task demonstrates the
need for a more linguistically motivated approach. At the other extreme, previous ap-
proaches (Radev and McKeown 1998) have demonstrated that this task is feasible when
a detailed semantic representation of the input sentences is available. However, these
approaches operate in a limited domain (e.g., terrorist events), where information ex-
traction systems can be used to interpret the source text. The task of mapping input text
into a semantic representation in a domain-independent setting extends well beyond
the ability of current analysis methods. These considerations suggest that we need a
new method for the sentence fusion task. Ideally, such a method would not require a
full semantic representation. Rather, it would rely on input texts and shallow linguistic
knowledge (such as parse trees) that can be automatically derived from a corpus to
generate a fusion sentence.
In our approach, sentence fusion is modeled after the typical generation pipeline:
content selection (what to say) and surface realization (how to say it). In contrast to
that involved in traditional generation systems in which a content selection component
chooses content from semantic units, our task is complicated by the lack of semantics in
the textual input. At the same time, we can benefit from the textual information given
in the input sentences for the tasks of syntactic realization, phrasing, and ordering; in
many cases, constraints on text realization are already present in the input.
The algorithm operates in three phases:
 Identification of common information (Section 3.1)
 Fusion lattice computation (Section 3.2)
 Lattice linearization (Section 3.3)
Content selection occurs primarily in the first phase, in which our algorithm uses local
alignment across pairs of parsed sentences, from which we select fragments to be
included in the fusion sentence. Instead of examining all possible ways to combine these
fragments, we select a sentence in the input which contains most of the fragments and
transform its parsed tree into the fusion lattice by eliminating nonessential information
and augmenting it with information from other input sentences. This construction of the
fusion lattice targets content selection, but in the process, alternative verbalizations are
selected, and thus some aspects of realization are also carried out in this phase. Finally,
we generate a sentence from this representation based on a language model derived
from a large body of texts.
3.1 Identification of Common Information
Our task is to identify information shared between sentences. We do this by aligning
constituents in the syntactic parse trees for the input sentences. Our alignment process
differs considerably from alignment for other NL tasks, such as machine translation,
because we cannot expect a complete alignment. Rather, a subset of the subtrees in
one sentence will match different subsets of the subtrees in the others. Furthermore,
order across trees is not preserved, there is no natural starting point for alignment, and
there are no constraints on crosses. For these reasons we have developed a bottom-
up local multisequence alignment algorithm that uses words and phrases as anchors
for matching. This algorithm operates on the dependency trees for pairs of input sen-
302
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
tences. We use a dependency-based representation because it abstracts over features
irrelevant for comparison such as constituent ordering. In the subsections that follow,
we describe first how this representation is computed, then how dependency subtrees
are aligned, and finally how we choose between constituents conveying overlapping
information.
In this section we first describe an algorithm which, given a pair of sentences,
determines which sentence constituents convey information appearing in both
sentences. This algorithm will be applied to pairwise combinations of sentences in the
input set of related sentences.
The intuition behind the algorithm is to compare all constituents of one sentence
to those of another and select the most similar ones. Of course, how this comparison
is performed depends on the particular sentence representation used. A good sentence
representation will emphasize sentence features that are relevant for comparison, such
as dependencies between sentence constituents, while ignoring irrelevant features,
such as constituent ordering. A representation which fits these requirements is a
dependency-based representation (Melcuk 1988). We first detail how this representation
is computed, then describe a method for aligning dependency subtrees.
3.1.1 Sentence Representation. Our sentence representation is based on a dependency
tree, which describes the sentence structure in terms of dependencies between words.
The similarity of the dependency tree to a predicate?argument structure makes it a
natural representation for our comparison.3 This representation can be constructed
from the output of a traditional parser. In fact, we have developed a rule-based
component that transforms the phrase structure output of Collins?s (2003) parser into
a representation in which a node has a direct link to its dependents. We also mark verb?
subject and verb?node dependencies in the tree.
The process of comparing trees can be further facilitated if the dependency tree is
abstracted to a canonical form which eliminates features irrelevant to the comparison.
We hypothesize that the difference in grammatical features such as auxiliaries, number,
and tense has a secondary effect when the meaning of sentences is being compared.
Therefore, we represent in the dependency tree only nonauxiliary words with their
associated grammatical features. For nouns, we record their number, articles, and class
(common or proper). For verbs, we record tense, mood (indicative, conditional, or
infinitive), voice, polarity, aspect (simple or continuous), and taxis (perfect or none).
The eliminated auxiliary words can be re-created using these recorded features. We also
transform all passive-voice sentences to the active voice, changing the order of affected
children.
While the alignment algorithm described in Section 3.1.2 produces one-to-one
mappings, in practice some paraphrases are not decomposable to words, forming
one-to-many or many-to-many paraphrases. Our manual analysis of paraphrased sen-
tences (Barzilay 2003) revealed that such alignments most frequently occur in pairs of
noun phrases (e.g., faculty member and professor) and pairs including verbs with parti-
cles (e.g., stand up, rise). To correctly align such phrases, we flatten subtrees containing
noun phrases and verbs with particles into one node. We subsequently determine
matches between flattened sentences using statistical metrics.
3 Two paraphrasing sentences which differ in word order may have significantly different trees in
phrase-based format. For instance, this phenomenon occurs when an adverbial is moved from a position
in the middle of a sentence to the beginning of a sentence. In contrast, dependency representations of
these sentences are very similar.
303
Computational Linguistics Volume 31, Number 3
Figure 3
Dependency tree of the sentence The IDF spokeswoman did not confirm this, but said the Palestinians
fired an antitank missile at a bulldozer on the site. The features of the node confirm are explicitly
marked.
An example of a sentence and its dependency tree with associated features is
shown in Figure 3. (In figures of dependency trees hereafter, node features are omitted
for clarity.)
3.1.2 Alignment. Our alignment of dependency trees is driven by two sources of in-
formation: the similarity between the structure of the dependency trees and the similar-
ity between lexical items. In determining the structural similarity between two trees, we
take into account the types of edges (which indicate the relationships between nodes).
An edge is labeled by the syntactic function of the two nodes it connects (e.g., subject?
verb). It is unlikely that an edge connecting a subject and verb in one sentence, for
example, corresponds to an edge connecting a verb and an adjective in another sentence.
The word similarity measures take into account more than word identity: They
also identify pairs of paraphrases, using WordNet and a paraphrasing dictionary. We
automatically constructed the paraphrasing dictionary from a large comparable news
corpus using the co-training method described in Barzilay and McKeown (2001). The
dictionary contains pairs of word-level paraphrases as well as phrase-level para-
phrases.4 Several examples of automatically extracted paraphrases are given in Table 2.
During alignment, each pair of nonidentical words that do not comprise a synset in
4 The comparable corpus and the derived dictionary are available at
http://www.cs.cornell.edu/?regina/thesis-data/comp/input/processed.tbz2 and
http://www.cs.cornell.edu/?regina/thesis-data/comp/output/comp2-ALL.txt. For details on the
corpus collection and evaluation of the paraphrase quality, see Barzilay (2003).
304
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Table 2
Lexical paraphrases extracted by the algorithm from the comparable news corpus.
(auto, automobile), (closing, settling), (rejected, does not accept), (military, army), (IWC,
International Whaling Commission), (Japan, country), (researching, examining), (harvesting,
killing), (mission-control office, control centers), (father, pastor), (past 50 years, four decades),
(Wangler, Wanger), (teacher, pastor), (fondling, groping), (Kalkilya, Qalqilya), (accused,
suspected), (language, terms), (head, president), (U.N., United Nations), (Islamabad, Kabul),
(goes, travels), (said, testified), (article, report), (chaos, upheaval), (Gore, Lieberman), (revolt,
uprising), (more restrictive local measures, stronger local regulations) (countries, nations),
(barred, suspended), (alert, warning), (declined, refused), (anthrax, infection), (expelled,
removed), (White House, White House spokesman Ari Fleischer), (gunmen, militants)
WordNet is looked up in the paraphrasing dictionary; in the case of a match, the pair
is considered to be a paraphrase.
We now give an intuitive explanation of how our tree similarity function, denoted
by Sim, is computed. If the optimal alignment of two trees is known, then the value of
the similarity function is the sum of the similarity scores of aligned nodes and aligned
edges. Since the best alignment of given trees is not known a priori, we select the max-
imal score among plausible alignments of the trees. Instead of exhaustively traversing
the space of all possible alignments, we recursively construct the best alignment for
trees of given depths, assuming that we know how to find an optimal alignment for
trees of shorter depths. More specifically, at each point of the traversal we consider two
cases, shown in Figure 4. In the first case, two top nodes are aligned with each other,
and their children are aligned in an optimal way by applying the algorithm to shorter
trees. In the second case, one tree is aligned with one of the children of the top node of
the other tree; again we can apply our algorithm for this computation, since we decrease
the height of one of the trees.
Before giving the precise definition of Sim, we introduce some notation. When T
is a tree with root node v, we let c(T) denote the set containing all children of v.
For a tree T containing a node s, the subtree of T which has s as its root node is denoted
by Ts.
Figure 4
Tree alignment computation. In the first case two tops are aligned, while in the second case the
top of one tree is aligned with a child of another tree.
305
Computational Linguistics Volume 31, Number 3
Given two trees T and T? with root nodes v and v?, respectively, the similar-
ity Sim(T, T?) between the trees is defined to be the maximum of the three expres-
sions NodeCompare(T, T?), maxs?c(T) Sim(Ts, T?), and maxs??c(T? ) Sim(T, T?s? ). The upper
part of Figure 4 depicts the computation of NodeCompare(T, T?), in which two top
nodes are aligned with each other. The remaining expressions, maxs?c(T) Sim(Ts, T?),
and maxs??c(T? ) Sim(T, T?s? ), capture mappings in which the top of one tree is aligned
with one of the children of the top node of the other tree (the bottom of Figure 4).
The maximization in the NodeCompare formula searches for the best possible
alignment for the child nodes of the given pair of nodes and is defined by
NodeCompare(T, T?) =NodeSimilarity(v, v?)
+ max
m?M(c(T),c(T? ))
?
?
?
(s,s? )?m
(EdgeSimilarity((v, s), (v?, s?)) + Sim(Ts, T?s? ))
?
?
where M(A, A?) is the set of all possible matchings between A and A?, and a matching
(between A and A?) is a subset m of A ? A? such that for any two distinct elements
(a, a?), (b, b?) ? m, both a = b and a? = b?. In the base case, when one of the trees has
depth one, NodeCompare(T, T?) is defined to be NodeSimilarity(v, v?).
The similarity score NodeSimilarity(v, v?) of atomic nodes depends on whether the
corresponding words are identical, paraphrases, or unrelated. The similarity scores for
pairs of identical words, pairs of synonyms, pairs of paraphrases, and edges (given in
Table 3) are manually derived using a small development corpus. While learning of
the similarity scores automatically is an appealing alternative, its application in the fu-
sion context is challenging because of the absence of a large training corpus and the lack
of an automatic evaluation function.5 The similarity of nodes containing flattened
subtrees,6 such as noun phrases, is computed as the score of their intersection nor-
malized by the length of the longest phrase. For instance, the similarity score of the
noun phrases antitank missile and machine gun and antitank missile is computed as a ratio
between the score of their intersection antitank missile (2), divided by the length of the
latter phrase (5).
The similarity function Sim is computed using bottom-up dynamic programming,
in which the shortest subtrees are processed first. The alignment algorithm returns
the similarity score of the trees as well as the optimal mapping between the subtrees
of input trees. The pseudocode of this function is presented in the Appendix. In the
resulting tree mapping, the pairs of nodes whose NodeSimilarity positively contributed
to the alignment are considered parallel. Figure 5 shows two dependency trees and their
alignment.
As is evident from the Sim definition, we are considering only one-to-one node
?matchings?: Every node in one tree is mapped to at most one node in another tree. This
restriction is necessary because the problem of optimizing many-to-many alignments
5 Our preliminary experiments with n-gram-based overlap measures, such as BLEU (Papineni et al 2002)
and ROUGE (Lin and Hovy 2003), show that these metrics do not correlate with human judgments on the
fusion task, when tested against two reference outputs. This is to be expected: As lexical variability across
input sentences grows, the number of possible ways to fuse them by machine as well by human also
grows. The accuracy of match between the system output and the reference sentences largely depends on
the features of the input sentences, rather than on the underlying fusion method.
6 Pairs of phrases that form an entry in the paraphrasing dictionary are compared as pairs of atomic entries.
306
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Table 3
Node and edge similarity scores used by the alignment algorithm.
Category Node Similarity Category Node Similarity
Identical words 1 Edges are subject-verb 0.03
Synonyms 1 Edges are verb-object 0.03
Paraphrases 0.5 Edges are same type 0.02
Other ?0.1 Other 0
is NP-hard.7 The subtree flattening performed during the preprocessing stage aims to
minimize the negative effect of the restriction on alignment granularity.
Another important property of our algorithm is that it produces a local alignment.
Local alignment maps local regions with high similarity to each other rather than
creating an overall optimal global alignment of the entire tree. This strategy is more
meaningful when only partial meaning overlap is expected between input sentences,
as in typical sentence fusion input. Only these high-similarity regions, which we call
intersection subtrees, are included in the fusion sentence.
3.2 Fusion Lattice Computation
Fusion lattice computation is concerned with combining intersection subtrees. During
this process, the system will remove phrases from a selected sentence, add phrases
from other sentences, and replace words with the paraphrases that annotate each
node. Among the many possible combinations of subtrees, we are interested only
in those combinations which yield semantically sound sentences and do not distort
the information presented in the input sentences. We cannot explore every possible
combination, since the lack of semantic information in the trees prohibits us from
assessing the quality of the resulting sentences. In fact, our early experimentation
with generation from constituent phrases (e.g., NPs, VPs) demonstrated that it was
difficult to ensure that semantically anomalous or ungrammatical sentences would
not be generated. Instead, we select a combination already present in the input sentences
as a basis and transform it into a fusion sentence by removing extraneous informa-
tion and augmenting the fusion sentence with information from other sentences. The
advantage of this strategy is that, when the initial sentence is semantically correct
and the applied transformations aim to preserve semantic correctness, the resulting
sentence is a semantically correct one. Our generation strategy is reminiscent of Robin
and McKeown?s (1996) earlier work on revision for summarization, although Robin and
McKeown used a three-tiered representation of each sentence, including its semantics
and its deep and surface syntax, all of which were used as triggers for revision.
The three steps of the fusion lattice computation are as follows: selection of the
basis tree, augmentation of the tree with alternative verbalizations, and pruning of
7 The complexity of our algorithm is polynomial in the number of nodes. Let n1 denote the number of
nodes in the first tree, and n2 denote the number of nodes in the second tree. We assume that the
branching factor of a parse tree is bounded above by a constant. The function NodeCompare is evaluated
only once on each node pair. Therefore, it is evaluated n1 ? n2 times totally. Each evaluation is computed
in constant time, assuming that values of the function for node children are known. Since we use
memoization, the total time of the procedure is O(n1 ? n2).
307
Computational Linguistics Volume 31, Number 3
Figure 5
Two dependency trees and their alignment tree. Solid lines represent aligned edges. Dotted and
dashed lines represent unaligned edges of the theme sentences.
the extraneous subtrees. Alignment is essential for all the steps. The selection of the
basis tree is guided by the number of intersection subtrees it includes; in the best case,
it contains all such subtrees. The basis tree is the centroid of the input sentences?
the sentence which is the most similar to the other sentences in the input. Using the
alignment-based similarity score described in Section 3.1.2, we identify the centroid
by computing for each sentence the average similarity score between the sentence and
the rest of the input sentences, then selecting the sentence with the highest score.
Next, we augment the basis tree with information present in the other input
sentences. More specifically, we add alternative verbalizations for the nodes in the basis
tree and the intersection subtrees which are not part of the basis tree. The alternative
verbalizations are readily available from the pairwise alignments of the basis tree with
other trees in the input computed in the previous section. For each node of the basis tree,
we record all verbalizations from the nodes of the other input trees aligned with a given
node. A verbalization can be a single word, or it can be a phrase, if a node represents
a noun compound or a verb with a particle. An example of a fusion lattice, augmented
308
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Figure 6
A basis lattice before and after augmentation. Solid lines represent aligned edges of the basis
tree. Dashed lines represent unaligned edges of the basis tree, and dotted lines represent
insertions from other theme sentences. Added subtrees correspond to sentences from Table 1.
with alternative verbalizations, is given in Figure 6. Even after this augmentation, the fu-
sion lattice may not include all of the intersection subtrees. The main difficulty in subtree
insertion is finding an acceptable placement; this is often determined by syntactic, se-
mantic, and idiosyncratic knowledge. Therefore, we follow a conservative insertion pol-
icy. Among all the possible aligned sentences, we insert only subtrees whose top node
aligns with one of the nodes in a basis tree.8 We further constrain the insertion procedure
by inserting only trees that appear in at least half of the sentences of a theme. These two
8 Our experimental results show that the algorithm inserts a sufficient amount of new subtrees despite this
limitation.
309
Computational Linguistics Volume 31, Number 3
constituent-level restrictions prevent the algorithm from generating overly long, un-
readable sentences.9
Finally, subtrees which are not part of the intersection are pruned off the basis
tree. However, removing all such subtrees may result in an ungrammatical or seman-
tically flawed sentence; for example, we might create a sentence without a subject.
This overpruning may happen if either the input to the fusion algorithm is noisy
or the alignment has failed to recognize similar subtrees. Therefore, we perform
a more conservative pruning, deleting only the self-contained components which
can be removed without leaving ungrammatical sentences. As previously observed
in the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such com-
ponents include a clause in the clause conjunction, relative clauses, and some ele-
ments within a clause (such as adverbs and prepositions). For example, this procedure
transforms the lattice in Figure 6 into the pruned basis lattice shown in Figure 7 by
deleting the clause the clash erupted and the verb phrase to better protect Israeli forces.
These phrases are eliminated because they do not appear in the other sentences of the
theme and at the same time their removal does not interfere with the well-formedness
of the fusion sentence. Once these subtrees are removed, the fusion lattice construc-
tion is completed.
3.3 Generation
The final stage in sentence fusion is linearization of the fusion lattice. Sentence
generation includes selection of a tree traversal order, lexical choice among avail-
able alternatives, and placement of auxiliaries, such as determiners. Our generation
method utilizes information given in the input sentences to restrict the search space
and then chooses among remaining alternatives using a language model derived from
a large text collection. We first motivate the need for reordering and rephrasing, then
discuss our implementation.
For the word-ordering task, we do not have to consider all the possible travers-
als, since the number of valid traversals is limited by ordering constraints encoded
in the fusion lattice. However, the basis lattice does not uniquely determine the
ordering: The placement of trees inserted in the basis lattice from other theme sen-
tences is not restricted by the original basis tree. While the ordering of many sentence
constituents is determined by their syntactic roles, some constituents, such as time,
location and manner circumstantials, are free to move (Elhadad et al 2001). Therefore,
the algorithm still has to select an appropriate order from among different orders of
the inserted trees.
The process so far produces a sentence that can be quite different from the ex-
tracted sentence; although the basis sentences provides guidance for the generation
process, constituents may be removed, added in, or reordered. Wording can also be
modified during this process. Although the selection of words and phrases which
appear in the basis tree is a safe choice, enriching the fusion sentence with alternative
verbalizations has several benefits. In applications such as summarization, in which
the length of the produced sentence is a factor, a shorter alternative is desirable. This
goal can be achieved by selecting the shortest paraphrase among available alternatives.
Alternate verbalizations can also be used to replace anaphoric expressions, for instance,
9 Furthermore, the preference for shorter fusion sentences is further enforced during the linearization stage
because our scoring function monotonically decreases with the length of a sentence.
310
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Figure 7
A pruned basis lattice.
when the basis tree contains a noun phrase with anaphoric expressions (e.g., his visit)
and one of the other verbalizations is anaphora-free. Substitution of the latter for the
anaphoric expression may increase the clarity of the produced sentence, since frequently
the antecedent of the anaphoric expression is not present in a summary. Moreover,
in some cases substitution is mandatory. As a result of subtree insertions and dele-
tions, the words used in the basis tree may not be a good choice after the transfor-
mations, and the best verbalization might be achieved by using a paraphrase of them
from another theme sentence. As an example, consider the case of two paraphras-
ing verbs with different subcategorization frames, such as tell and say. If the phrase
our correspondent is removed from the sentence Sharon told our correspondent that the
elections were delayed . . . , a replacement of the verb told with said yields a more readable
sentence.
The task of auxiliary placement is alleviated by the presence of features stored
in the input nodes. In most cases, aligned words stored in the same node have
the same feature values, which uniquely determine an auxiliary selection and con-
jugation. However, in some cases, aligned words have different grammatical
features, in which case the linearization algorithm needs to select among avail-
able alternatives.
311
Computational Linguistics Volume 31, Number 3
Linearization of the fusion sentence involves the selection of the best phrasing
and placement of auxiliaries as well as the determination of optimal ordering. Since
we do not have sufficient semantic information to perform such selection, our algo-
rithm is driven by corpus-derived knowledge. We generate all possible sentences10
from the valid traversals of the fusion lattice and score their likelihood according to
statistics derived from a corpus. This approach, originally proposed by Knight and
Hatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method used
in statistical generation. We trained a trigram model with Good?Turing smoothing
over 60 megabytes of news articles collected by Newsblaster using the second version
CMU?Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997).
The sentence with the lowest length-normalized entropy (the best score) is selected as
the verbalization of the fusion lattice. Table 4 shows several verbalizations produced
by our algorithm from the central tree in Figure 7. Here, we can see that the lowest-
scoring sentence is both grammatical and concise.
Table 4 also illustrates that entropy-based scoring does not always correlate with
the quality of the generated sentence. For example, the fifth sentence in Table 4?
Palestinians fired antitank missile at a bulldozer to build a new embankment in the area?is
not a well-formed sentence; however, our language model gave it a better score than
its well-formed alternatives, the second and the third sentences (see Section 4 for
further discussion). Despite these shortcomings, we preferred entropy-based scoring
to symbolic linearization. In the next section, we motivate our choice.
3.3.1 Statistical versus Symbolic Linearization. In the previous version of the
system (Barzilay, McKeown, and Elhadad 1999), we performed linearization of a
fusion dependency structure using the language generator FUF/SURGE (Elhadad
and Robin 1996). As a large-scale linearizer used in many traditional semantic-to-text
generation systems, FUF/SURGE could be an appealing solution to the task of surface
realization. Because the input structure and the requirements on the linearizer are
quite different in text-to-text generation, we had to design rules for mapping between
dependency structures produced by the fusion component and FUF/SURGE input. For
instance, FUF/SURGE requires that the input contain a semantic role for prepositional
phrases, such as manner, purpose, or location, which is not present in our dependency
representation; thus we had to augment the dependency representation with this
information. In the case of inaccurate prediction or the lack of relevant semantic
information, the linearizer scrambles the order of sentence constituents, selects wrong
prepositions, or even fails to generate an output. Another feature of the FUF/SURGE
system that negatively influences system performance is its limited ability to reuse
phrases readily available in the input, instead of generating every phrase from scratch.
This makes the generation process more complex and thus prone to error.
While the initial experiments conducted on a set of manually constructed themes
seemed promising, the system performance deteriorated significantly when it was
applied to automatically constructed themes. Our experience led us to believe that
transformation of an arbitrary sentence into a FUF/SURGE input representation is
similar in its complexity to semantic parsing, a challenging problem in its own right.
Rather than refining the mapping mechanism, we modified MultiGen to use a statis-
10 Because of the efficiency constraints imposed by Newsblaster, we sample only a subset of 20,000 paths.
The sample is selected randomly.
312
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Table 4
Alternative linearizations of the fusion lattice with corresponding entropy values.
Sentence Entropy
Palestinians fired an antitank missile at a bulldozer. 4.25
Palestinian militants fired machine guns and antitank missiles at a
bulldozer.
5.86
Palestinian militants fired machine guns and antitank missiles at a
bulldozer that was building an embankment in the area.
6.22
Palestinians fired antitank missiles at while using a bulldozer. 7.04
Palestinians fired antitank missile at a bulldozer to build a new
embankment in the area.
5.46
tical linearization component, which handles uncertainty and noise in the input in a
more robust way.
4. Sentence Fusion Evaluation
In our previous work, we evaluated the overall summarization strategy of MultiGen
in multiple experiments, including comparisons with human-written summaries in
the Document Understanding Conference (DUC)11 evaluation (McKeown et al 2001;
McKeown et al 2002) and quality assessment in the context of a particular informa-
tion access task in the Newsblaster framework (McKeown et al 2002).
In this article, we aim to evaluate the sentence fusion algorithm in isolation from
other system components; we analyze the algorithm performance in terms of content
selection and the grammaticality of the produced sentences. We first present our eval-
uation methodology (Section 4.1), then we describe our data (Section 4.2), the results
(Section 4.3), and our analysis of them (Section 4.4).
4.1 Methods
4.1.1 Construction of a Reference Sentence. We evaluated content selection by com-
paring an automatically generated sentence with a reference sentence. The reference
sentence was produced by a human (hereafter the RFA), who was instructed to gener-
ate a sentence conveying information common to many sentences in a theme. The RFA
was not familiar with the fusion algorithm. The RFA was provided with the list of
theme sentences; the original documents were not included. The instructions given to
the RFA included several examples of themes with fusion sentences generated by the
authors. Even though the RFA was not instructed to use phrases from input sentences,
the sentences presented as examples reused many phrases from the input sentences.
We believe that phrase reuse elucidates the connection between input sentences and
a resulting fusion sentence. Two examples of themes, reference sentences, and system
outputs are shown in Table 5.
4.1.2 Data Selection. We wanted to test the performance of the fusion component on
automatically computed inputs which reflect the accuracy of the existing preprocessing
tools. For this reason, the test data were selected randomly from material collected by
Newsblaster. To remove themes irrelevant for fusion evaluation, we introduced two
11 DUC is a community-based evaluation of summarization systems organized by DARPA.
313
Computational Linguistics Volume 31, Number 3
Table 5
Examples from the test set. Each example contains a theme, a reference sentence generated by
the RFA, and a sentence generated by the system. Subscripts in the system-generated sentence
represent a theme sentence from which a word was extracted.
#1 The forest is about 70 miles west of Portland.
#2 Their bodies were found Saturday in a remote part of Tillamook State Forest, about
40 miles west of Portland.
#3 Elk hunters found their bodies Saturday in the Tillamook State Forest, about
60 miles west of the family?s hometown of Portland.
#4 The area where the bodies were found is in a mountainous forest about 70 miles
west of Portland.
Reference The bodies were found Saturday in the forest area west of Portland.
System The bodies4 were found2 Saturday2 in3 the Tillamook3 State3 Forest3 west2 of2
Portland2.
#1 Four people including an Islamic cleric have been detained in Pakistan after a fatal
attack on a church on Christmas Day.
#2 Police detained six people on Thursday following a grenade attack on a church that
killed three girls and wounded 13 people on Christmas Day.
#3 A grenade attack on a Protestant church in Islamabad killed five people, including
a U.S. Embassy employee and her 17-year-old daughter.
Reference A grenade attack on a church killed several people.
System A3 grenade3 attack3 on3 a Protestant3 church3 in3 Islamabad3 killed3 six2 people2.
additional filters. First, we excluded themes that contained identical or nearly identical
sentences (with cosine similarity higher than 0.8). When processing such sentences,
our algorithm reduces to sentence extraction, which does not allow us to evaluate the
generation abilities of our algorithm. Second, themes for which the RFA was unable to
create a reference sentence were also removed from the test set. As mentioned above,
Simfinder does not always produce accurate themes,12 and therefore, the RFA could
choose not to generate a reference sentence if the theme sentences had too little in
common. An example of a theme for which no sentence was generated is shown in
Table 6. As a result of this filtering, 34% of the sentences were removed.
4.1.3 Baselines. In addition to the system-generated sentence, we also included in
the evaluation a fusion sentence generated by another human (hereafter, RFA2) and
three baselines. (Following the DUC terminology, we refer to the baselines, our system,
and the RFA2 as peers.) The first baseline is the shortest sentence among the theme
sentences, which is obviously grammatical, and it also has a good chance of being rep-
resentative of common topics conveyed in the input. The second baseline is produced
by a simplification of our algorithm, where paraphrase information is omitted during
the alignment process. This baseline is included to capture the contribution of para-
phrase information to the performance of the fusion algorithm. The third baseline
consists of the basis sentence. The comparison with this baseline reveals the contri-
bution of the insertion and deletion stages in the fusion algorithm. The comparison
against an RFA2 sentence provides an upper bound on the performance of the system
and baselines. In addition, this comparison sheds light on the human agreement on
this task.
12 To mitigate the effects of Simfinder noise in MultiGen, we induced a similarity threshold on input
trees?trees which are not similar to the basis tree are not used in the fusion process.
314
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Table 6
An example of noisy Simfinder output.
The shares have fallen 60% this year.
They said Qwest was forcing them to exchange their bonds at a fraction of face value?between
52.5% and 82.5%, depending on the bond?or else fall lower in the pecking order for repayment
in case Qwest went broke.
Qwest had offered to exchange up to $12.9 billion of the old bonds, which carried interest rates
between 5.875% and 7.9%.
The new debt carries rates between 13% and 14%.
Their yield fell to about 15.22% from 15.98%.
4.1.4 Comparison against the Reference Sentence. One judge was given a peer sen-
tence along with the corresponding reference sentence. The judge also had access
to the original theme from which these sentences were generated. The order of the
presentation was randomized across themes and peer systems. Reference and peer
sentences were divided into clauses by the authors. The judges assessed overlap on
the clause level between reference and peer sentences. The wording of the instructions
was inspired by the DUC instructions for clause comparison. For each clause in
the reference sentence, the judge decided whether the meaning of a corresponding
clause was conveyed in a peer sentence. In addition to 0 score for no overlap and 1
for full overlap, this framework allows for partial overlap with a score of 0.5. From the
overlap data, we computed weighted recall and precision based on fractional count
(Hatzivassiloglou and McKeown 1993). Recall is a ratio of weighted clause overlap
between a peer and a reference sentence, and the number of clauses in a reference
sentence. Precision is a ratio of weighted clause overlap between a peer and a reference
sentence, and the number of clauses in a peer sentence.
4.1.5 Grammaticality Assessment. Grammaticality was rated in three categories:
grammatical (3), partially grammatical (2), and not grammatical (1). The judge was in-
structed to rate a sentence in the grammatical category if it contained no grammatical
mistakes. Partially grammatical included sentences that contained at most one mistake
in agreement, articles, and tense realization. The not grammatical category included
sentences that were corrupted by multiple mistakes of the former type, by erroneous
component order or by the omission of important components (e.g., subject).
Punctuation is one issue in assessing grammaticality. Improper placement of
punctuation is a limitation of our implementation of the sentence fusion algorithm
that we are well aware of.13 Therefore, in our grammaticality evaluation (following the
DUC procedure), the judge was asked to ignore punctuation.
4.2 Data
To evaluate our sentence fusion algorithm, we selected 100 themes following the proce-
dure described in the previous section. Each set varied from three to seven sentences,
13 We were unable to develop a set of rules which works in most cases. Punctuation placement is
determined by a variety of features; considering all possible interactions of these features is hard. We
believe that corpus-based algorithms for automatic restoration of punctuation developed for speech
recognition applications (Beeferman, Berger, and Lafferty 1998; Shieber and Tao 2003) could help in our
task, and we plan to experiment with them in the future.
315
Computational Linguistics Volume 31, Number 3
with 4.22 sentences on average. The generated fusion sentences consisted of 1.91 clauses
on average. None of the sentences in the test set were fully extracted; on average, each
sentence fused fragments from 2.14 theme sentences. Out of 100 sentence, 57 sentences
produced by the algorithm combined phrases from several sentences, while the rest
of the sentences comprised subsequences of one of the theme sentences. (Note that
compression is different from sentence extraction.) We included these sentences in the
evaluation, because they reflect both content selection and realization capacities of the
algorithm.
Table 5 shows two sentences from the test corpus, along with input sentences. The
examples are chosen so as to reflect good- and bad-performance cases. Note that the
first example results in inclusion of the essential information (the fact that bodies were
found, along with time and place) and leaves out details (that it was a remote location
or how many miles west it was, a piece of information that is in dispute in any
case). The problematic example incorrectly selects the number of people killed as six,
even though this number is not repeated and different numbers are referred to in the
text. This mistake is caused by a noisy entry in our paraphrasing dictionary which
erroneously identifies ?five? and ?six? as paraphrases of each other.
4.3 Results
Table 7 shows the length ratio, precision, recall, F-measure, and grammaticality score
for each algorithm. The length ratio of a sentence was computed as the ratio of its
output length to the average length of the theme input sentences.
4.4 Discussion
The results in Table 7 demonstrate that sentences manually generated by the second
human participant (RFA2) not only are the shortest, but are also closest to the reference
sentence in terms of selected information. The tight connection14 between sentences
generated by the RFAs establishes a high upper bound for the fusion task. While
neither our system nor the baselines were able to reach this level of performance, the
fusion algorithm clearly outperforms all the baselines in terms of content selection,
at a reasonable level of compression. The performance of baseline 1 and baseline 2
demonstrates that neither the shortest sentence nor the basis sentence is an adequate
substitution for fusion in terms of content selection. The gap in recall between our
system and baseline 3 confirms our hypothesis about the importance of paraphrasing
information for the fusion process. Omission of paraphrases causes an 8% drop in
recall due to the inability to match equivalent phrases with different wording.
Table 7 also reveals a downside of the fusion algorithm: Automatically generated
sentences contain grammatical errors, unlike fully extracted, human-written sentences.
Given the high sensitivity of humans to processing ungrammatical sentences, one
has to consider the benefits of flexible information selection against the decrease in
readability of the generated sentences. Sentence fusion may not be a worthy direction
to pursue if low grammaticality is intrinsic to the algorithm and its correction requires
14 We cannot apply kappa statistics (Siegel and Castellan 1988) for measuring agreement in the content
selection task since the event space is not well-defined. This prevents us from computing the probability
of random agreement.
316
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Table 7
Evaluation results for a human-crafted fusion sentence (RFA2), our system output, the shortest
sentence in the theme (baseline 1), the basis sentence (baseline 2), and a simplified version of our
algorithm without paraphrasing information (baseline 3).
Peer Length Ratio Precision Recall F-measure Grammaticality
RFA2 54% 98% 94% 96% 2.9
Fusion 78% 65% 72% 68% 2.3
Baseline 1 69% 52% 38% 44% 3.0
Baseline 2 111% 41% 67% 51% 3.0
Baseline 3 73% 63% 64% 63% 2.4
knowledge which cannot be automatically acquired. In the remainder of the section, we
show that this is not the case. Our manual analysis of generated sentences revealed
that most of the grammatical mistakes are caused by the linearization component,
or more specifically, by suboptimal scoring of the language model. Language model-
ing is an active area of research, and we believe that advances in this direction will
be able to dramatically boost the linearization capacity of our algorithm.
4.4.1 Error Analysis. In this section, we discuss the results of our manual analysis of
mistakes in content selection and surface realization. Note that in some cases multiple
errors are entwined in one sentence, which makes it hard to distinguish between a
sequence of independent mistakes and a cause-and-effect chain. Therefore, the pre-
sented counts should be viewed as approximations, rather than precise numbers.
We start with the analysis of the test set and continue with the description of some
interesting mistakes that we encountered during system development.
Mistakes in Content Selection. Most of the mistakes in content selection can be attributed
to problems with alignment. In most cases (17), erroneous alignments missed relevant
word mappings as a result of the lack of a corresponding entry in our paraphrasing
resources. At the same time, mapping of unrelated words (as shown in Table 5) was
quite rare (two cases). This performance level is quite predictable given the accuracy
of an automatically constructed dictionary and limited coverage of WordNet. Even
in the presence of accurate lexical information, the algorithm occasionally produced
suboptimal alignments (four cases) because of the simplicity of our weighting scheme,
which supports limited forms of mapping typology and also uses manually assigned
weights.
Another source of errors (two cases) was the algorithm?s inability to handle
many-to-many alignments. Namely, two trees conveying the same meaning may not
be decomposable into the node-level mappings which our algorithm aims to compute.
For example, the mapping between the sentences in Table 8 expressed by the rule
X denied claims by Y ? X said that Y?s claim was untrue cannot be decomposed into
smaller matching units. At least two mistakes resulted from noisy preprocessing
(tokenization and parsing).
In addition to alignment, overcutting during lattice pruning caused the omission of
three clauses that were present in the corresponding reference sentences. The sentence
Conservatives were cheering language is an example of an incomplete sentence derived
from the following input sentence: Conservatives were cheering language in the final version
317
Computational Linguistics Volume 31, Number 3
Table 8
A pair of sentences which cannot be fully decomposed.
Syria denied claims by Israeli Prime Minister Ariel Sharon . . .
The Syrian spokesman said that Sharon?s claim was untrue . . .
that ensures that one-third of all funds for prevention programs be used to promote abstinence.
The omission of a relative clause was possible because some sentences in the input
theme contained the noun language without any relative clauses.
Mistakes in Surface Realization. Grammatical mistakes included incorrect selection of
determiners, erroneous word ordering, omission of essential sentence constituents, and
incorrect realization of negation constructions and tense. These mistakes (42) originated
during linearization of the lattice and were caused either by incompleteness of the
linearizer or by suboptimal scoring of the language model. Mistakes of the first type
are caused by missing rules for generating auxiliaries given node features. An exam-
ple of this phenomenon is the sentence The coalition to have play a central role, which
verbalizes the verb construction will have to play incorrectly. Our linearizer lacks the
completeness of existing application-independent linearizers, such as the unification-
based FUF/SURGE (Elhadad and Robin 1996) and the probabilistic Fergus (Bangalore
and Rambow 2000). Unfortunately, we were unable to reuse any of the existing large-
scale linearizers because of significant structural differences between input expected
by these linearizers and the format of a fusion lattice. We are currently working on
adapting Fergus for the sentence fusion task.
Mistakes related to suboptimal scoring were the most common (33 out of 42);
in these cases, a language model selected ill-formed sentences, assigning a worse
score to a better sentence. The sentence The diplomats were given to leave the coun-
try in 10 days illustrates a suboptimal linearization of the fusion lattice. The correct
linearizations?The diplomats were given 10 days to leave the country and The diplomats
were ordered to leave the country in 10 days?were present in the fusion lattice, but
the language model picked the incorrect verbalization. We found that in 27 cases the
optimal verbalizations (in the authors? view) were ranked below the top-10 sentences
ranked by the language model. We believe that more powerful language models that
incorporate linguistic knowledge (such as syntax-based models) can improve the
quality of generated sentences.
4.4.2 Further Analysis. In addition to analyzing errors found in this particular study,
we also regularly track the quality of generated summaries on Newsblaster?s Web
page. We have noted a number of interesting errors that crop up from time to time
that seem to require information about the full syntactic parse, semantics, or even
discourse. Consider, for example, the last sentence from a summary entitled Estrogen-
Progestin Supplements Now Linked to Dementia, which is shown in Table 9. This sentence
was created by sentence fusion and clearly, there is a problem. Certainly, there was a
study finding the risk of dementia in women who took one type of combined hormone pill, but
it was not the government study which was abruptly halted last summer. In looking
at the two sentences from which this summary sentence was drawn, we can see that
there is a good amount of overlap between the two, but the component does not have
enough information about the referents of the different terms to know that two different
318
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Table 9
An example of wrong reference selection. Subscripts in the generated sentence indicate the
theme sentence from which the words were extracted.
#1 Last summer, a government study was abruptly halted after finding an increased risk
of breast cancer, heart attacks, and strokes in women who took one type of combined
hormone pill.
#2 The most common form of hormone replacement therapy, already linked to breast
cancer, stroke, and heart disease, does not improve mental functioning as some earlier
studies suggested and may increase the risk of dementia, researchers said on Tuesday.
System Last1 summer1 a1 government1 study1 abruptly1 was1 halted1 after1 finding1 the2 risk2
of2 dementia2 in1 women1 who1 took1 one1 type1 of1 combined1 hormone1 pill1.
studies are involved and that fusion should not take place. One topic of our future work
(Section 6) is the problem of reference and summarization.
Another example is shown in Table 10. Here again, the problem is reference. The
first error is in the references to the segments. The two uses of segments in the first
source document sentence do not refer to the same entity and thus, when the modifier
is dropped, we get an anomaly. The second, more unusual problem is in the equation
of Clinton/Dole, Dole/Clinton, and Clinton and Dole.
5. Related Work
5.1 Text-to-Text Generation
Unlike traditional concept-to-text generation approaches, text-to-text generation
methods take text as input and transform it into a new text satisfying some constraints
(e.g., length or level of sophistication). In addition to sentence fusion, compression
algorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates,
and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al 2003)
and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003)
are other instances of such methods.
Compression methods have been developed for single-document summarization,
and they aim to reduce a sentence by eliminating constituents which are not crucial
for understanding the sentence and not salient enough to include in the summary.
These approaches are based on the observation that the ?importance? of a sentence
constituent can often be determined based on shallow features, such as its syntactic
role and the words it contains. For example, in many cases a relative clause that is
Table 10
An example of incorrect reference selection. Subscripts in the generated sentence indicate the
theme sentence from which the words were extracted.
#1 The segments will revive the ?Point-Counterpoint? segments popular until they
stopped airing in 1979, but will instead be called ?Clinton/Dole? one week and
?Dole/Clinton? the next week.
#2 Clinton and Dole have signed up to do the segment for the next 10 weeks, Hewitt said.
#3 The segments will be called ?Clinton Dole? one week and ?Dole Clinton? the next.
System The1 segments1 will1 revive1 the3 segments3 until1 they1 stopped1 airing1 in1 19791
but1 instead1 will1 be1 called1 Clinton2 and2 Dole2.
319
Computational Linguistics Volume 31, Number 3
peripheral to the central point of the document can be removed from a sentence without
significantly distorting its meaning. While earlier approaches for text compression were
based on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999),
more recent approaches use an aligned corpus of documents and their human written
summaries to determine which constituents can be reduced (Knight and Marcu 2002;
Jing and McKeown 2000; Reizler et al 2003). The summary sentences, which have
been manually compressed, are aligned with the original sentences from which they
were drawn.
Knight and Marcu (2000) treat reduction as a translation process using a noisy-
channel model (Brown et al 1993). In this model, a short (compressed) string is treated
as a source, and additions to this string are considered to be noise. The probability of a
source string s is computed by combining a standard probabilistic context-free grammar
score, which is derived from the grammar rules that yielded tree s, and a word-bigram
score, computed over the leaves of the tree. The stochastic channel model creates a large
tree t from a smaller tree s by choosing an extension template for each node based on
the labels of the node and its children. In the decoding stage, the system searches for
the short string s that maximizes P(s|t), which (for fixed t) is equivalent to maximizing
P(s) ? P(t|s).
While this approach exploits only syntactic and lexical information, Jing and
McKeown (2000) also rely on cohesion information, derived from word distribution in
a text: Phrases that are linked to a local context are retained, while phrases that have no
such links are dropped. Another difference between these two methods is the extensive
use of knowledge resources in the latter. For example, a lexicon is used to identify
which components of the sentence are obligatory to keep it grammatically correct. The
corpus in this approach is used to estimate the degree to which a fragment is extraneous
and can be omitted from a summary. A phrase is removed only if it is not grammati-
cally obligatory, is not linked to a local context, and has a reasonable probability of
being removed by humans. In addition to reducing the original sentences, Jing and
McKeown (2000) use a number of manually compiled rules to aggregate reduced
sentences; for example, reduced clauses might be conjoined with and.
Sentence fusion exhibits similarities with compression algorithms in the ways in
which it copes with the lack of semantic data in the generation process, relying on
shallow analysis of the input and statistics derived from a corpus. Clearly, the difference
in the nature of both tasks and in the type of input they expect (single sentence versus
multiple sentences) dictates the use of different methods. Having multiple sentences in
the input poses new challenges?such as a need for sentence comparison?but at the
same time it opens up new possibilities for generation. While the output of existing
compression algorithms is always a substring of the original sentence, sentence fusion
may generate a new sentence which is not a substring of any of the input sentences. This
is achieved by arranging fragments of several input sentences into one sentence.
The only other text-to-text generation approach able to produce new utterances is
that of Pang, Knight, and Marcu (2003). Their method operates over multiple English
translations of the same foreign sentence and is intended to generate novel paraphrases
of the input sentences. Like sentence fusion, their method aligns parse trees of the input
sentences and then uses a language model to linearize the derived lattice. The main
difference between the two methods is in the type of the alignment: Our algorithm
performs local alignment, while the algorithm of Pang, Knight, and Marcu (2003)
performs global alignment. The differences in alignment are caused by differences in
input: Pang, Knight, and Marcu?s method expects semantically equivalent sentences,
while our algorithm operates over sentences with only partial meaning overlap. The
320
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
presence of deletions and insertions in input sentences makes alignment of comparable
trees a new and particularly significant challenge.
5.2 Computation of an Agreement Tree
The alignment method described in Section 3 falls into a class of tree comparison
algorithms extensively studied in theoretical computer science (Sankoff 1975; Finden
and Gordon 1985; Amir and Keselman 1994; Farach, Przytycka, and Thorup 1995)
and widely applied in many areas of computer science, primarily computational bi-
ology (Gusfield 1997). These algorithms aim to find an overlap subtree that captures
structural commonality across a set of related trees. A typical tree similarity measure
considers the proximity, at both the node and the edge levels, between input trees.
In addition, some algorithms constrain the topology of the resulting alignment based
on the domain-specific knowledge. These constraints not only narrow the search space
but also increase the robustness of the algorithm in the presence of a weak similarity
function.
In the NLP context, this class of algorithms has been used previously in example-
based machine translation, in which the goal is to find an optimal alignment between
the source and the target sentences (Meyers, Yangarber, and Grishman 1996). The
algorithm operates over pairs of parallel sentences, where each sentence is represented
by a structure-sharing forest of plausible syntactic trees. The similarity function is
driven by lexical mapping between tree nodes and is derived from a bilingual dictio-
nary. The search procedure is greedy and is subject to a number of constraints needed
for alignment of parallel sentences.
This algorithm has several features in common with our method: It operates
over syntactic dependency representations and employs recursive computation to find
an optimal solution. However, our method is different in two key aspects. First, our
algorithm looks for local regions with high similarity in nonparallel data, rather than for
full alignment, expected in the case of parallel trees. The change in optimization criteria
introduces differences in the similarity measure?specifically, the relaxation of certain
constraints?and the search procedure, which in our work uses dynamic programming.
Second, our method is an instance of a multisequence alignment,15 in contrast to the
pairwise alignment described in Meyers, Yangarber, and Grishman (1996). Combining
evidence from multiple trees is an essential step of our algorithm?pairwise comparison
of nonparallel trees may not provide enough information regarding their underlying
correspondences. In fact, previous applications of multisequence alignment have been
shown to increase the accuracy of the comparison in other NLP tasks (Barzilay and
Lee 2002; Bangalore, Murdock, and Riccardi 2002; Lacatusu, Maiorano, and Harabagiu
2004); unlike our work these approaches operate on strings, not trees, and with the
exception of (Lacatusu, Maiorano, and Harabagiu 2004), they apply alignment to paral-
lel data, not comparable texts.
6. Conclusions and Future Work
In this article, we have presented sentence fusion, a novel method for text-to-text
generation which, given a set of similar sentences, produces a new sentence contain-
ing the information common to most sentences. Unlike traditional generation methods,
15 See Gusfield (1997) and Durbin et al (1998) for an overview of multisequence alignment.
321
Computational Linguistics Volume 31, Number 3
sentence fusion does not require an elaborate semantic representation of the input
but instead relies on the shallow linguistic representation automatically derived from
the input documents and knowledge acquired from a large text corpus. Generation is
performed by reusing and altering phrases from input sentences.
As the evaluation described in Section 4 shows, our method accurately identifies
common information and in most cases generates a well-formed fusion sentence. Our
algorithm outperforms the shortest-sentence baseline in terms of content selection,
without a significant drop in grammaticality. We also show that augmenting the fu-
sion process with paraphrasing knowledge improves the output by both measures.
However, there is still a gap between the performance of our system and human
performance.
An important goal for future work on sentence fusion is to increase the flexibility
of content selection and realization. We believe that the process of aligning theme
sentences can be greatly improved by having the system learn the similarity function,
instead of using manually assigned weights. An interesting question is how such a
similarity function can be induced in an unsupervised fashion. In addition, we can
improve the flexibility of the fusion algorithm by using a more powerful language
model. Recent research (Daume et al 2002) has show that syntax-based language
models are more suitable for language generation tasks; the study of such models is
a promising direction to explore.
An important feature of the sentence fusion algorithm is its ability to generate
multiple verbalizations of a given fusion lattice. In our implementation, this property is
utilized only to produce grammatical texts in the changed syntactic context, but it can
also be used to increase coherence of the text at the discourse level by taking context
into account. In our current system, each sentence is generated in isolation, inde-
pendently from what is said before and what will be said after. Clear evidence of
the limitation of this approach is found in the selection of referring expressions. For
example, all summary sentences may contain the full description of a named entity
(e.g., President of Columbia University Lee Bollinger), while the use of shorter descriptions
such as Bollinger or anaphoric expressions in some summary sentences would in-
crease the summary?s readability (Schiffman, Nenkova, and McKeown 2002; Nenkova
and McKeown 2003). These constraints can be incorporated into the sentence fusion
algorithm, since our alignment-based representation of themes often contains several
alternative descriptions of the same object.
Beyond the problem of referring-expression generation, we found that by selecting
appropriate paraphrases of each summary sentence, we can significantly improve the
coherence of an output summary. An important research direction for future work is
to develop a probabilistic text model that can capture properties of well-formed texts,
just as a language model captures properties of sentence grammaticality. Ideally, such
a model would be able to discriminate between cohesive fluent texts and ill-formed
texts, guiding the selection of sentence paraphrases to achieve an optimal sentence
sequence.
Appendix. Alignment Pseudocode
Function: EdgeSim(edge1, edge2)
Returns: The similarity score of two input edges based on their type
begin
if type of(edge1) = type of(edge2) = ?subject-verb? then
return SUBJECT VERB SCORE ;
322
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
else if type of(edge1) = type of(edge2) = ?object-verb? then
return OBJECT VERB SCORE ;
else
return EDGE DEFAULT ;
end
end
Function: NodeSim(node1, node2)
Returns: The similarity score of two words or flattened noun phrases based on their
semantic relation
begin
if is phrase (node1) or is phrase (node2) then
return IDENTITY SCORE * |intersection(node1,node2 )|max(|node1|,|node2|) ;
else if node1 = node2 then
return IDENTITY SCORE ;
else if is synonym (node1, node2) then
return SYNONYMY SCORE ;
else
return NODE DEFAULT ;
end
end
All the comparison functions employ memoization, implemented by hash table
wrappers.
Function: MapChildren(tree1, tree2) memoized
Returns: Given two dependency trees, MapChildren finds the optimal alignment of tree
children. The function returns the score of the alignment and the mapping
itself.
begin
/*Generate all legitimate mappings between the children on tree1 and tree2 */
all-maps ? GenerateAllPermutations (tree1, tree2) ;
best ? ??1, void? ;
/*Compute the score of each mapping, and select the one with the highest score */
foreach map in all-maps do
res ? 0 ;
foreach ?s1, s2? in map do
res ? res + EdgeSim (edge (tree1.top, s1), edge (tree2.top, s2))
+ Sim (subtree (tree1, s1), (subtree (tree2, s2));
end
if res > best.score then
best.score ? res ;
best.map ? map ;
end
end
return best
end
Function: NodeCompare(tree1, tree2) memoized
Returns: Given two dependency trees, NodeCompare finds their optimal alignment that
323
Computational Linguistics Volume 31, Number 3
maps two top nodes of the tree one to another. The function returns the score
of the alignment and the mapping itself.
begin
node-sim ? NodeSim(tree1.top, tree2.top) ;
/*If one of the trees is of height one, return the NodeSim score between two tops */
if is leaf(tree1) or is leaf(tree2) then
return ?node-sim, ?tree1, tree2?? ;
else
/*Find an optimal alignment of the children nodes */
res ? MapChildren(tree1, tree2) ;
/*The alignment score is computed as a sum of the similarity of top nodes and
the score of the optimal alignment of node. The tree alignment is assembled
by adding a pair of top nodes to the optimal alignment of their children. */
return ?node-sim + res.score, ?tree1.top, tree2.top? ? res.map? ;
end
end
Function: NodeCompare(tree1, tree2) memoized
Returns: Given two dependency trees, Sim finds their optimal alignment. The function
returns the score of the alignment and the mapping itself.
begin
best ? ??1, void? ;
/*find an optimal alignment between one of the children of tree1 and tree2 */
foreach s in tree1.children do
res ? Sim(s, tree2) ;
if res.score > best.score then best ? res ;
end
/*find an optimal alignment between one of the children of tree1 and tree2 */
foreach s in tree2.children do
res ? Sim(tree1, s) ;
if res.score > best.score then best ? res ;
end
/*find an optimal alignment that include the two top nodes */
res ? NodeCompare(tree1, tree2) ;
if res.score > best.score then best ? res ;
return best
end
Acknowledgments
We are grateful to Eli Barzilay, Michael
Collins, Noe`mie Elhadad, Julia Hirschberg,
Mirella Lapata, Lillian Lee, Smaranda
Muresan, and the anonymous reviewers for
helpful comments and conversations.
Portions of this work were completed while
the first author was a graduate student at
Columbia University. This article is based
upon work supported in part by the
National Science Foundation under grant
IIS-0448168, DARPA grant N66001-00-1-8919
and a Louis Morin scholarship. Any
opinions, findings, and conclusions or
recommendations expressed above are
those of the authors and do not necessarily
reflect the views of the National
Science Foundation.
References
Amir, Amihood and Dmitry Keselman. 1994.
Maximum agreement subtree in a set of
evolutionary trees?Metrics and efficient
algorithms. In Proceedings of FOCS,
pages 758?769, Santa Fe, NM.
Bangalore, Srinivas, Vanessa Murdock, and
Giuseppe Riccardi. 2002. Bootstrapping
324
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
bilingual data using consensus translation
for a multilingual instant messaging
system. In International Conference on
Computational Linguistics (COLING 2002),
Tapei, Taiwan.
Bangalore, Srinivas and Owen Rambow.
2000. Exploiting a probabilistic
hierarchical model for generation. In
Proceedings of COLING, pages 42?48,
Saarbruken, Germany.
Banko, Michele and Lucy Vanderwende.
2004. Using n-grams to understand the
nature of summaries. In Proceedings of
HLT-NAACL, pages 1? 4, Boston, MA.
Barzilay, Regina. 2003. Information Fusion for
Multi-document Summarization:
Paraphrasing and Generation. Ph.D. thesis,
Columbia University.
Barzilay, Regina and Michael Elhadad. 1997.
Using lexical chains for text
summarization. In Proceedings of the ACL
Workshop on Intelligent Scalable Text
Summarization, pages 10?17, Madrid.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multi-document news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Lillian Lee. 2002.
Bootstrapping lexical choice via
multiple-sequence alignment. In
Proceedings of the 2002 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 164?171,
Philadelphia, PA.
Barzilay, Regina and Kathleen McKeown.
2001. Extracting paraphrases from a
parallel corpus. In Proceedings of the
ACL/EACL, pages 50?57, Toulouse, France.
Barzilay, Regina, Kathleen McKeown, and
Michael Elhadad. 1999. Information fusion
in the context of multi-document
summarization. In Proceedings of the ACL,
pages 550?557, College Park, MD.
Beeferman, Doug, Adam Berger, and
John Lafferty. 1998. Cyberpunc: A
lightweight punctuation annotation
system for speech. In Proceedings of
the IEEE International Conference on
Acoustics, Speech and Signal Processing,
pages 689?692, Seattle, WA.
Borko, Harold and Charles Bernier. 1975.
Abstracting Concepts and Methods.
Academic Press, New York.
Brown, Peter F., Stephen Della Pietra,
Vincent Della Pietra, and Robert Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Carbonell, Jaime and Jade Goldstein. 1998.
The use of MMR, diversity-based
reranking for reordering documents
and producing summaries. In
Proceedings of SIGIR, pages 335?336,
Melbourne, Australia.
Chandrasekar, Raman, Christine Doran, and
Srinivas Bangalore. 1996. Motivations and
methods for text simplification. In
International Conference on Computational
Linguistics (COLING 1996),
pages 1041?1044, Copenhagen, Denmark.
Chu-Carroll, Jennifer, Krzysztof Czuba, John
Prager, and Abraham Ittycheriah. 2003. In
question answering: Two heads are better
than one. In Proceedings of HLT-NAACL,
pages 24?31, Edmonton, Alberta.
Clarke, Charles, Gordon Cormack, and
Thomas Lynam. 2001. Exploiting
redundancy in question answering. In
Proceedings of SIGIR, pages 358?365, New
Orleans, LA.
Clarkson, Philip and R. Rosenfeld. 1997.
Statistical language modeling using the
CMU-Cambridge toolkit. In Proceedings
ESCA Eurospeech, volume 5,
pages 2707?2710, Rhodes, Greece.
Collins, Michael. 2003. Head-driven
statistical models for natural language
parsing. Computational Linguistics,
29(4):589?637.
Daume, Hal, Kevin Knight, Irene
Langkilde-Geary, Daniel Marcu, and Kenji
Yamada. 2002. The importance of
lexicalized syntax models for natural
language generation tasks. In Proceedings of
INLG, pages 9?16, Arden House,
Harriman, NJ.
Dumais, Susan, Michele Banko, Eric Brill,
Jimmy Lin, and Andrew Ng. 2002. Web
question answering: Is more always
better? In Proceedings of SIGIR,
pages 291?298, Tampere, Finland.
Durbin, Richard, Sean Eddy, Anders Krogh,
and Graeme Mitchison. 1998.
Biological Sequence Analysis. Cambridge
University Press.
Elhadad, Michael, Yael Netzer, Regina
Barzilay, and Kathleen McKeown. 2001.
Ordering circumstantials for
multi-document summarization.
In Proceedings of BISFAI,
Ramat Gan, Israel.
Elhadad, Michael and Jacques Robin. 1996.
An overview of surge: A reusable
comprehensive syntactic realization
component. Technical Report 96?03,
325
Computational Linguistics Volume 31, Number 3
Department of Mathematics and
Computer Science, Ben Gurion University,
Beer Sheva, Israel.
Farach, Martin, Teresa Przytycka, and Mikkel
Thorup. 1995. On the agreement of many
trees. Information Processing Letters,
55(6):297?301.
Finden, C. R. and A. D. Gordon. 1985.
Obtaining common pruned trees. Journal of
Classification, 2:255?276.
Grefenstette, Gregory. 1998. Producing
intelligent telegraphic text reduction to
provide an audio scanning service for the
blind. In Proceedings of the AAAI Spring
Workshop on Intelligent Text Summarization,
pages 111?115, Palo Alto, CA.
Gusfield, Dan. 1997. Algorithms on
strings, trees and sequences. Cambridge
University Press.
Hatzivassiloglou, Vasileios, Judith Klavans,
and Eleazar Eskin. 1999. Detecting text
similarity over short passages: Exploring
linguistic feature combinations via
machine learning. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora, College Park, MD.
Hatzivassiloglou, Vasileios and Kathleen
McKeown. 1993. Towards the automatic
identification of adjectival scales:
Clustering adjectives according to
meaning. In Proceedings of the ACL,
pages 172?182, Columbus, OH.
Jing, Hongyang and Kathleen McKeown.
2000. Cut and paste based summarization.
In Proceedings of the First Conference
of the North American Chapter of the
Association of Computational Linguistics,
pages 178?185, Seattle.
Knight, Kevin and Vasileios
Hatzivassiloglou. 1995. Two-level,
many-path generation. In Proceedings of the
ACL, pages 252?260, Cambridge, MA.
Knight, Kevin and Daniel Marcu. 2002.
Summarization beyond sentence
extraction: A probabilistic approach to
sentence compression. Artificial Intelligence
Journal, 139(1):91?107.
Lacatusu, V. Finley, Steven J. Maiorano, and
Sanda M. Harabagiu. 2004.
Multi-document summarization using
multi-sequence alignment. In International
Conference on Language Resources and
Evaluation, Lisbon, Portugal.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings
of the ACL/COLING, pages 704?710,
Montreal, Quebec.
Lin, Chin-Yew and Eduard H. Hovy. 2002.
From single to multi-document
summarization: A prototype system and
its evaluation. In Proceedings of the ACL,
pages 457?464, Philadelphia, PA.
Lin, Chin-Yew and Eduard H. Hovy. 2003.
Automatic evaluation of summaries using
n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 150?157,
Edmonton, Alberta.
Mani, Inderjeet and Eric Bloedorn. 1997.
Multi-document summarization by graph
search and matching. In Proceedings of the
Fifteenth National Conference on Artificial
Intelligence (AAAI-97), pages 622?628,
Providence, RI. AAAI.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the ACL,
pages 558?565, College Park, MD.
Marcu, Daniel and Laurie Gerber. 2001. An
inquiry into the nature of multidocument
abstracts, extracts, and their evaluation. In
Proceedings of the NAACL Workshop on
Automatic Summarization, pages 2?11,
Pittsburgh, PA.
McKeown, Kathleen R., Regina Barzilay,
David Evans, Vasileios Hatzivassiloglou,
Judith Klavans, Ani Nenkova,
Carl Sable, Barry Schiffman, and
Sergey Sigelman. 2002. Tracking and
summarizing news on a daily
basis with Columbia?s Newsblaster.
In Proceedings of the Human Language
Technology Conference (HLT-02),
pages 280?285, San Diego, CA.
McKeown, Kathleen R., Regina Barzilay,
David Evans, Vasileios Hatzivassiloglou,
Min Yen Kan, Barry Schiffman, and
Simone Teufel. 2001. Columbia
multi-document summarization:
Approach and evaluation. In Proceedings
of the Document Understanding Conference
(DUC01), New Orleans, LA.
Melcuk, Igor. 1988. Dependency Syntax: Theory
and Practice. Albany: State University of
New York Press.
Meyers, Adam, Roman Yangarber, and
Ralph Grishman. 1996. Alignment
of shared forests for bilingual corpora.
In International Conference on Computational
Linguistics (COLING 1996), pages 460?465,
Copenhagen, Denmark.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine J. Miller. 1990. Introduction to
WordNet: An on-line lexical database.
International Journal of Lexicography,
3(4):235?245.
326
Barzilay and McKeown Sentence Fusion for Multidocument News Summarization
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion, the thesaurus, and the structure
of text. Computational Linguistics,
17(1):21?48.
Nenkova, Ani and Kathleen R. McKeown.
2003. References to named entities:
A corpus study. In Proceedings of the
Human Language Technology Conference,
Companion Volume, pages 70?73,
Edmonton, Alberta.
Pang, Bo, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple
translations: Extracting paraphrases
and generating new sentences. In
Proceedings of HLT-NAACL, pages 180?187,
Edmonton, Alberta.
Papineni, Kishore A., Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of
machine translation. In Proceedings of the
ACL, pages 311?318, Philadelphia, PA.
Radev, Dragomir, Hongyan Jing, and
Malgorzata Budzikowska. 2000.
Centroid-based summarization of multiple
documents: Sentence extraction,
utility-based evaluation, and user studies.
In Proceedings of the ANLP/NAACL 2000
Workshop on Automatic Summarization,
pages 165?172, Seattle, WA.
Radev, Dragomir and Kathleen R. McKeown.
1998. Generating natural language
summaries from multiple on-line
sources. Computational Linguistics,
24(3):469?500.
Radev, Dragomir, John Prager, and Valerie
Samn. 2000. Ranking suspected answers to
natural language questions using
predictive annotation. In Proceedings of
Sixth Conference on Applied Natural
Language Processing (ANLP),
pages 150?157, Philadelphia, PA.
Reizler, Stefan, Tracy H. King, Richard
Crouch, and Annie Zaenen. 2003.
Statistical sentence condensation using
ambiguity packing and stochastic
disambiguation methods for
lexical-functional grammar. In Proceedings
of HLT-NAACL, pages 197?204,
Edmonton, Alberta.
Robin, Jacques and Kathleen McKeown.
1996. Empirically designing and
evaluating a new revision-based model for
summary generation. Artificial Intelligence,
85(1?2):135?179.
Sankoff, David. 1975. Minimal mutation trees
of sequences. SIAM Journal of Applied
Mathematics, 28(1):35?42.
Schiffman, Barry, Ani Nenkova, and
Kathleen R. McKeown. 2002. Experiments
in multidocument summarization. In
Proceedings of HLT, pages 52?58,
San Diego, CA.
Shieber, Stuart and Xiapong Tao. 2003.
Comma restoration using constituency
information. In Proceedings of HLT-NAACL,
pages 142?148, Edmonton, Alberta.
Siegel, Sidney and N. John Castellan. 1988.
Nonparametric Statistics for Behavioral
Sciences. McGraw-Hill.
Silber, Gregory and Kathleen McCoy. 2002.
Computed lexical chains as an
intermediate representation for automatic
text summarization. Computational
Linguistics, 28(4):487?496.
327

References to Named Entities: a Corpus Study
Ani Nenkova and Kathleen McKeown
Columbia University
Computer Science Department
New York, NY 10027
 
ani,kathy  @cs.columbia.edu
Abstract
References included in multi-document sum-
maries are often problematic. In this paper, we
present a corpus study performed to derive a
statistical model for the syntactic realization of
referential expressions. The interpretation of
the probabilistic data helps us gain insight on
how extractive summaries can be rewritten in
an efficient manner to produce more fluent and
easy-to-read text.
1 Introduction
Automatically generated summaries, and particularly
multi-document summaries, suffer from lack of coher-
ence One explanation is that the most widespread sum-
marization strategy is still sentence extraction, where sen-
tences are extracted word for word from the original doc-
uments and are strung together to form a summary. Syn-
tactic form and its influence on summary coherence have
not been taken into account in the implementation of a
full-fledged summarizer, except in the preliminary work
of (Schiffman et al, 2002).
Here we conduct a corpus study focusing on identify-
ing the syntactic properties of first and subsequent men-
tions of people in newswire text (e.g., ?Chief Petty Of-
ficer Luis Diaz of the U.S. Coast Guard in Miami? fol-
lowed by ?Diaz?). The resulting statistical model of the
flow of referential expressions suggest a set of rewrite
rules that can transform the summary back to a more co-
herent and readable text.
In the following sections, we first describe the corpus
that we used and then the statistical model that we de-
veloped. It is based on Markov chains and captures how
subsequent mentions are conditioned by earlier mentions.
We close with discussion of our evaluation, which mea-
sures how well the highest probability path in the model
can be used to regenerate the sequence of references.
2 The Corpus
We used a corpus of news stories, containing 651,000
words drawn from six different newswire agencies, in or-
der to study the syntactic form of noun phrases in which
references to people have been realized. We were inter-
ested in the occurrence of features such as type and num-
ber of premodifiers, presence and type of postmodifiers,
and form of name reference for people.
We constructed a large, automatically annotated cor-
pus by merging the output of Charniak?s statistical
parser (Charniak, 2000) with that of the IBM named
entity recognition system Nominator (Wacholder et al,
1997). The corpus contains 6240 references. In this sec-
tion, we describe the features that were annotated.
Given our focus on references to mentions of peo-
ple, there are two distinct types of premodifiers, ?titles?
and ?name-external modifiers?. The titles are capital-
ized noun premodifiers that conventionally are recog-
nized as part of the name, such as ?president? in ?Presi-
dent George W. Bush?. Name-external premodifiers are
modifiers that do not constitute part of the name, such as
?Irish flutist? in ?Irish flutist James Galway?.
The three major categories of postmodification that we
distinguish are apposition, prepositional phrase modifica-
tion and relative clause. All other postmodifications, such
as remarks in parenthesis and verb-initial modifications
are lumped in a category ?others?.
There are three categories of names corresponding
to the general European and American name structure.
They include full name (first+(middle initial)+last), last
name only, and nickname (first or nickname).
In sum, the target NP features that we examined were:
 Is the target named entity the head of the phrase or
not? Is it in a possessive construction or not?
 If it is the head, what kind of pre- and post- modifi-
cation does it have?
 How was the name itself realized in the NP?
In order to identify the appropriate sequences of syn-
tactic forms in coreferring noun phrases, we analyze
the coreference chains for each entity mentioned in the
text. A coreference chain consists of all the mentions
of an entity within a document. In a manually built
corpus, a coreference chain can include pronouns and
common nouns that refer to the person. However, these
forms could not be automatically identified, so corefer-
ence chains in our corpus only include noun phrases that
contain at least one word from the name. There were
3548 coreference chains in the corpus.
3 A Markov Chain Model
The initial examination of the data showed that syntactic
forms in coreference chains can be effectively modeled
by Markov chains.
Let   be random variables taking values in I. We say
that   	
 is a Markov chain with initial distribution

and transition matrix  if

 
 has distribution

 for  , conditional on   ,  ffColumbia?s Newsblaster: New Features and Future Directions
Kathleen McKeown, Regina Barzilay, John Chen, David Elson, David Evans,
Judith Klavans, Ani Nenkova, Barry Schiffman and Sergey Sigelman
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, New York, N.Y. 10027
kathy@cs.columbia.edu
Abstract
Columbia?s Newsblaster tracking and summa-
rization system is a robust system that clus-
ters news into events, categorizes events into
broad topics and summarizes multiple articles
on each event. Here we outline our most cur-
rent work on tracking events over days, produc-
ing summaries that update a user on new infor-
mation about an event, outlining the perspec-
tives of news coming from different countries
and clustering and summarizing non-English
sources.
1 Introduction
Columbia?s Newsblaster1 provide news updates on a
daily basis from news published on the Internet; it crawls
news sites, categorizes stories into six broad areas, groups
news into stories on the same event, and generates a sum-
mary of the multiple articles describing each event. In ad-
dition to demonstrating the robustness of current summa-
rization and tracking technology, Newsblaster also serves
as a research environment in which we explore new di-
rections and problems. Currently, we are exploring the
tasks of multilingual summarization where input sources
are drawn frommultiple languages and a summary is gen-
erated in English on the same event (Figure 1), tracking
events across days and generating summaries that update
the user on what is new, and editing generated summaries
to improve fluency and accuracy. Our focus here is on
editing references to people, improving coherency of the
summary and ensuring that references are accurate. Edit-
ing is particularly important as we add multilingual capa-
bilities, given the errors inherent in machine translation.
1http://newsblaster.cs.columbia.edu
2 Multilingual Tracking and
Summarization
The multilingual version of Columbia Newsblaster is
built upon the English version of Columbia Newsblaster,
sharing the same structure and components. To add mul-
tilingual capability, the system first crawls web sites in
foreign languages, and stores both the language and en-
coding for the files. To extract the article text from the
HTML pages, we use a new article extraction component
using language-independent statistical features computed
over text blocks along with a machine learning compo-
nent to classify text blocks as one of ?Article Text?, ?Ti-
tle?, ?Image?, ?Image Caption?, or ?Other?. The article
extraction component has been trained and tested on En-
glish, Japanese, and Russian data, but is also being suc-
cessfully applied to French, Spanish, German, and Ital-
ian data. We plan to train the article extractor on other
languages (Chinese, Arabic, Korean, Spanish, German,
French, etc.) in the near future.
To cluster multilingual documents with English doc-
uments, we use the existing Newsblaster English doc-
ument clustering module. Non-English documents are
translated for clustering after the article extraction phase.
We use simple and fast document translation techniques
for clustering if available, since we potentially process
thousands of documents for a language for each run. We
have developed simple dictionary lookup techniques for
translation for clustering for Japanese and Russian; for
other languages we use an interface to the Systran trans-
lation system via Babelfish. We plan on adding Arabic
translation to the system in the near future.
Summarization is performed using the same summa-
rization strategies in Newsblaster. We are experimenting
with different methods for improving summary quality
when translation of text is noisy. For example, when an
input cluster contains both English and foreign sources,
we weight the English higher in cases where we deter-
mine it is representative of both the English and foreign
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 15-16
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Multilingual Version
input documents. We are also experimenting with meth-
ods for determining similarity across documents using
different levels of translation.
3 Different Perspectives
When news media report on international issues, they re-
flect the perspectives of their own countries. In the past,
Newsblaster has included all international sources as in-
put to its summaries. Recently, we have added a feature
of ?international perspectives? to the system. In addition
to the universal summary for a particular event, which
includes all international sources, Newsblaster now gen-
erates separate summaries for each country, which may
illustrate unique biases or disagree on facts. The News-
blaster interface allows users to view any pair of sum-
maries side by side to compare different perspectives.
4 Summary Rewrite
Newsblaster also currently includes a module for rewrit-
ing summaries to achieve better readability. References
to people are rewritten so that the first mention includes
the person?s full name and a selected description and later
mentions are restricted to last name only. In addition to
improving readability, the rewritten version of the sum-
mary is usually shorter than the version before rewrite,
since multiple verbose descriptions of the same entity are
discarded. These changes can be seen when comparing
the summary sentence with the original document via a
link from the summary using a proxy.
5 Event Tracking and Updates
Newsblaster currently identifies events within a single
day; a new set of clusters is generated each day. We have
designed a new module for tracking events across days,
allowing the system to relate stories published on one day
to closely related stories on other days. In this way, the
user can more easily track events of interest as they un-
fold. The typical approach for tracking events across days
represents each event as one monolithic set of stories. We
have focused instead on a model where events on one day
can divide into related sub-events on the next day. For ex-
ample, a set of stories about the start of the Iraq war is an
event that can branch into multiple sets of stories, each set
representing a different facet of the war. We are currently
determining an appropriate evaluation of this approach as
well as investigating different possible interfaces.
If a user is tracking events across days, it is more useful
to have a summary that provides updates on what is new
as opposed to a summary of similarities across all days.
We have built a prototype update summarizer that scans
new articles extracted by the system and compares these
new articles with a background cluster on the same event.
The summarizer will provide the user with a summary of
only important new developments. As the tracking mod-
ule locates new articles, it will pass these to the update
summarizer, which will determine what, if anything, has
changed. This summarizer uses more syntactic and se-
mantic information about the articles to determine nov-
elty than is used in our other summarization strategies and
thus, efficiency is a challenge. We will demo these com-
ponents in a separately fromNewsblaster as they have not
yet been integrated in the development version.
Columbia Newsblaster: Multilingual News Summarization on the Web
David Kirk Evans Judith L. Klavans
Department of Computer Science
Columbia University, NY, NY 10027
{devans, klavans, kathy}@cs.columbia.edu
Kathleen R. McKeown
Abstract
We present the new multilingual version of
the Columbia Newsblaster news summariza-
tion system. The system addresses the problem
of user access to browsing news from multiple
languages from multiple sites on the internet.
The system automatically collects, organizes,
and summarizes news in multiple source lan-
guages, allowing the user to browse news top-
ics with English summaries, and compare per-
spectives from different countries on the topics.
1 Introduction
The Columbia Newsblaster1 system has been online and
providing summaries of topically clustered news daily
since late 2001 (McKeown et al, 2002). The goal of
the system is to aid daily news browsing by providing
an automatic, user-friendly access to important news top-
ics, along with summaries and links to the original arti-
cles for further information. The system has six major
phases: crawling, article extraction, clustering, sum-
marization, classification, and web page generation.
The focus of this paper is to present the entire mul-
tilingual Columbia Newsblaster system as a platform
for multilingual multi-document summarization exper-
iments. The phases in the multilingual version of
Columbia Newsblaster have been modified to take lan-
guage and character encoding into account, and a new
phase, translation, has been added. Figure 1 depicts the
multilingual Columbia Newsblaster architecture. We will
describe the system, in particular a method using machine
learning to extract article text from web pages that is ap-
plicable to different languages, and a baseline approach
to multilingual multi-document summarization.
1.1 Related Research
Previous work in multilingual document summarization,
such as the SUMMARIST system (Hovy and Lin, 1999)
1http://newsblaster.cs.columbia.edu/
Figure 1: Architecture of the multilingual Columbia
Newsblaster system.
extracts sentences from documents in a variety of lan-
guages, and translates the resulting summary. This sys-
tem has been applied to Information Retrieval in the
MuST System (Lin, 1999) which uses query translation
to allow a user to search for documents in a variety of lan-
guages, summarize the documents using SUMMARIST,
and translate the summary. The Keizei system (Ogden et
al., 1999) uses query translation to allow users to search
Japanese and Korean documents in English, and displays
query-specific summaries focusing on passages contain-
ing query terms. Our work differs in the document clus-
tering component ? we cluster news to provide emergent
topic structure from the data, instead of using an informa-
tion retrieval model. This is useful in analysis, monitor-
ing, and browsing settings, where a user does not have an
a priori topic in mind. Our summarization strategy also
differs from the approach taken by MuST in that we focus
our effort on the summarization system, but only target a
single language, shifting the majority of the multilingual
knowledge burden to specialized machine translation sys-
tems. The Keizei system has the advantage of being able
to generate query-specific summaries.
Chen and Lin (Chen and Lin, 2000) describe a sys-
tem that combines multiple monolingual news clustering
components, a multilingual news clustering component,
and a news summarization component. Their system
clusters news in each language into topics, then the mul-
tilingual clustering component relates the clusters that
are similar across languages. A summary is generated
by linking sentences that are similar from the two lan-
guages. The system has been implemented for Chinese
and English, and an evaluation over six topics is pre-
sented. Our clustering strategy differs here, as we trans-
late documents before clustering, and cluster documents
from all languages at the same time. This makes it easy
to add support for additional languages by incorporating a
new translation system for the language; no other changes
need to be made. Our summarization model also provides
summaries for documents from each language, allowing
comparisons between them.
2 Extracting article data
2.1 Extracting article text
To move Columbia Newsblaster into a multilingual ca-
pable environment, we must be able to extract the ?ar-
ticle text? from web pages in multiple languages. The
article text is the portion of a web page that contains the
actual news content of the page, as opposed to site navi-
gation links, ads, layout information, etc. Our previous
approach to extracting article text in Columbia News-
blaster used regular expressions that were hand-tailored
to specific web sites. Adapting this approach to new web
sites is difficult, and it is also difficult to adapt to for-
eign languages sites. We solved this problem by incor-
porating a new article extraction module using machine
learning techniques. The new article extraction module
parses HTML into blocks of text based on HTML markup
and computes a set of 34 features based on simple sur-
face characteristics of the text. We use features such as
the percentage of text that is punctuation, the number
of HTML links in the block, the percentage of question
marks, the number of characters in the text block, and so
on. Since the features are relatively language independent
they can be computed for and applied to any language.
Training data for the system is generated using a GUI
that allows a human to annotate text candidates with one
of fives labels: ?ArticleText?, ?Title?, ?Caption?, ?Im-
age?, or ?Other?. The ?ArticleText? label is associated
with the actual text of the article which we wish to ex-
tract. At the same time, we try to determine document
titles, image caption text, and image blocks in the same
framework. ?Other? is a catch-all category for all other
text blocks, such as links to related articles, navigation
links, ads, and so on. The training data is used with the
Language Training set Precision Recall
English 353 89.10% 90.70%
Russian 112 90.59% 95.06%
Russian English Rules 37.66% 73.05%
Japanese 67 89.66% 100.00%
Japanese English Rules 100.00% 20.00%
Table 1: Article extractor performance for detecting arti-
cle text in three languages.
machine learning program Ripper (Cohen, 1996) to in-
duce a hypothesis for categorizing text candidates accord-
ing to the features. This approach has been trained on
web pages from sites in English, Russian, and Japanese
as shown in Table 1, but has been used with sites in En-
glish, Russian, Japanese, Chinese, French, Spanish, Ger-
man, Italian, Portuguese, and Korean.
The English training set was composed of 353 arti-
cles, collected from 19 web sites. Using 10-fold cross-
validation, the induced hypothesis classify into the article
text category with a precision of 89.1% and a recall of
90.7%. Performance over Russian data was similar, with
a precision of 90.59% and recall of 95.06%. We evalu-
ated the English hypothesis against the Russian data to
observe whether the languages behave differently. As ex-
pected, the English hypothesis resulted in poor perfor-
mance over the Russian data, and we saw comparable
results for Japanese. The same English hypothesis per-
forms adequately on other English sites not in the train-
ing set, so the differences between languages seem to be
significant.
2.2 Title and date extraction
The article extraction component also determines a title
for each document, and attempts to locate a publishing
date for the articles. Title identification is important since
in a cluster, sometimes with as many as 60 articles, the
only information the user sees are the titles for the arti-
cles; if our system chooses poor titles, they will have a
difficult time discriminating between the articles. If the
article extraction component finds a title it is used. Un-
fortunately, this process is not always successful, so we
have a variety of fall-back methods, including taking the
title from the HTML TITLE tag, using heuristics to de-
tect the title from the first text block, and using a portion
of the first sentence. These approaches led to many un-
informative titles extracted from the non-English sites,
since they were developed for English news. We imple-
mented a system to identify titles that are clearly non-
descriptive, such as ?Stock Market News?, that would
apply to non-English text as well. We record the titles
seen and rejected over time and use the list to reject ti-
tles with high frequency. A title with high frequency is
assumed to be not descriptive enough to give a clear idea
of the content of an article in a cluster of similar articles.
To correctly extract dates for articles, we use heuristics
to identify sequences of possible dates, weigh them, and
choose the most likely date as the publication date. Reg-
ular expressions for Japanese date extraction were added
to the system.
3 Multilingual Clustering
The document clustering system that we use (Hatzivas-
siloglou et al, 2000) has been trained on, and extensively
tested with English. While it can cluster documents in
other languages, our goal is to generate clusters with doc-
uments from multiple languages, so a baseline approach
is to translate all non-English documents into English,
and then cluster the translated documents. We take this
approach, and further experimented with using simple
and fast techniques for glossing the input articles for clus-
tering. We developed simple dictionary lookup glossing
systems for Japanese and Russian. Our experimentation
showed that full translation using Systran outperformed
our glossing-based techniques, so the glossing techniques
are not used in the current system.
4 Multilingual Summarization Baseline
Our baseline approach to multilingual multi-document
summarization is to apply our English-based summa-
rization system, the Columbia Summarizer (McKeown
et al, 2001), to document clusters containing machine-
translated versions of non-English documents. The
Columbia Summarizer routes to one of two multi-
document summarization systems based on the similar-
ity of the documents in the cluster. If the documents
are highly similar, the Multigen summarization system
(McKeown et al, 1999) is used. Multigen clusters sen-
tences based on similarity, and then parses and fuses in-
formation from similar sentences to form a summary.
The second summarization system used is DEMS, the
Dissimilarity Engine for Multi-document Summarization
(Schiffman et al, 2002), which uses a sentence extraction
approach to summarization. The resulting summary is
then run through a named entity recovery tool (Nenkova
and McKeown, 2003), which repairs named entity refer-
ences in the summary by making the first reference de-
scriptive, and shortening subsequent reference mentions
in the summary. Using an unmodified version of DEMS,
summaries might contain sentences from translated doc-
uments which are not grammatically correct. The DEMS
summarization system was modified to prefer choosing
a sentence from an English article if there are sentences
that express similar content in multiple languages. By
setting different weight penalties we can take the quality
of the translation system for a given language pair into
Figure 2: A screen shot comparing a summary from En-
glish documents to a summary from German documents.
account.
4.1 Similarity-based Summarization
As part of our multilingual summarization work, we
are investigating approaches to summarization that use
sentence-level similarity computation across languages to
cluster sentences by similarity, and then generate a sum-
mary sentence using translated portions of the relevant
sentences. The multilingual version of Columbia News-
blaster provides us with a platform to frame future ex-
periments for this summarization technique. We are in-
vestigating translation at different levels - sentence level,
clause level, and phrase level. Our initial similarity-based
summarization system works at the sentence level. Start-
ing with machine-translated sentences, we compute their
similarity to English sentences that have been simpli-
fied(Siddharthan, 2002). Foreign-language sentences that
have a high enough similarity to English text are replaced
(or augmented with) the similar English sentence.
This first system using full machine translation over
the sentences and English similarity detection will be ex-
tended using simple features for multilingual similarity
detection in SimFinder MultiLingual (SimFinderML), a
multilingual version of SimFinder (Hatzivassiloglou et
al., 2001). We also plan an experiment evaluating the use-
fulness of noun phrase detection and noun phrase variant
detection as a primitive for multilingual similarity detec-
tion, using tools such as Christian Jacquemin?s FASTR
(Jacquemin, 1994; Jacquemin, 1999).
4.2 Summary presentation
Multilingual Newsblaster presents multiple views of a
cluster of documents to the user, broken down by lan-
guage and by country. Summaries are generated for the
entire cluster, as well as sub-sets of the articles based on
the country of origin and language of the original arti-
cles. Users are first presented with a summary of the en-
tire cluster using all documents, and then have the ability
to focus on countries or languages of their choosing. We
also allow the user to view two summaries side-by-side so
they can easily compare differences between summaries
from different countries. For example, figure 4.2 shows a
summary of articles about talks between America, Japan,
and Korea over nuclear arms, comparing the summaries
from articles in English and German.
5 Evaluation
Evaluation of multi-document summarization is a dif-
ficult task; the Document Understanding Conference
(DUC)2 is designed as an evaluation for multi-document
summarization systems. We participated in the DUC
2004 conference submitting the results of the summariza-
tion system used in Newsblaster, as well as an in-progress
system described in Section 4.1 for multilingual cluster
summarization. The results of the DUC evaluation will
provide us with valuable feedback on the multi-document
multi-lingual summarization components in Newsblaster.
6 Conclusions
In this paper we have described a multilingual version
of Columbia Newsblaster, a system that runs daily offer-
ing users an accessible interface to online news brows-
ing. The multilingual version of the system incorporates
two varieties of machine translation, one for clustering,
and one for translation of documents for summarization.
Existing summarization methods have been applied to
translated text, with plans for an evaluation of the current
method, and incorporation of summarization techniques
specific to translated documents. The system presents a
platform for further multilingual summarization experi-
ments and user-oriented studies.
References
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. A multilin-
gual news summarizer. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics,
pages 159?165.
William W. Cohen. 1996. Learning trees and rules with
set-valued features. In AAAI/IAAI, Vol. 1, pages 709?
716.
Vasileois Hatzivassiloglou, Luis Gravano, and Ankineedu
Maganti. 2000. An investigation of linguistic features
and clustering algorithms for topical document clus-
tering. In Proceedings of the 23rd ACM SIGIR Con-
ference on Research and Development in Information
Retrieval.
2http://duc.nist.gov/
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa
Holcombe, Regina Barzilay, Min-Yen Kan, and Kathy
McKeown. 2001. Simfinder: A flexible clustering tool
for summarization. In Proceedings of the North Amer-
ican Association for Computational Linguistics Auto-
matic Summarization Workshop.
E.H. Hovy and Chin-Yew Lin. 1999. Automated text
summarization in summarist. In I. Mani and M. May-
bury, editors, Advances in Automated Text Summariza-
tion, chapter 8. MIT Press.
Christian Jacquemin. 1994. Fastr: a unification-based
front-end to automatic indexing. In In Proceedings,
Intelligent Multimedia Information Retrieval Systems
and Management (RIAO?94), pages p. 34?47.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics (ACL?99), pages 341?348.
Chin-Yew Lin. 1999. Machine translation for informa-
tion access across the language barrier: the must sys-
tem. In Machine Translation Summit VII, September.
Kathleen McKeown, Judith Klavans, Vasileios Hatzivas-
siloglou, Regina Barzilay, and Eleazar Eskin. 1999.
Towards multidocument summarization by reformula-
tion: Progress and prospects. In AAAI, pages 453?460.
Kathleen R. McKeown, Regina Barzilay, David Kirk
Evans, Vasileios Hatzivassiloglou, Min-Yen Kan,
Barry Schiffman, and Simone Teufel. 2001. Columbia
multi-document summarization: Approach and evalu-
ation. In Proceedings of the Document Understanding
Conference.
Kathleen R. McKeown, Regina Barzilay, David Kirk
Evans, Vasileios Hatzivassiloglou, Judith L. Klavans,
Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with columbia?s newsblaster. In Proceed-
ings of the Human Language Technology Conference.
Ani Nenkova and Kathy McKeown. 2003. References to
named entities: A corpus study. In Short Paper Pro-
ceedings of NAACL-HLT.
William Ogden, James Cowie, Mark Davis, Eugene Lu-
dovik, Hugo Molina-Salgado, and Hyopil Shin. 1999.
Getting information from documents you cannot read:
An interactive cross-language text retrieval and sum-
marization system. In SIGIR/DL Workshop on Mul-
tilingual Information Discovery and Access (MIDAS),
August.
Barry Schiffman, Ani Nenkova, and Kathleen McKeown.
2002. Experiments in multidocument summarization.
In Proceedings of the Human Language Technology
Conference, March.
Advaith Siddharthan. 2002. Resolving attachment and
clause boundary ambiguities for simplifying relative
clause constructs. In Proceedings of the Student Work-
shop, 40th Meeting of the Association for Computa-
tional Linguistics (ACL?02), pages 60?65, Philadel-
phia, USA.
Proceedings of NAACL HLT 2007, pages 180?187,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Lexicalized Markov Grammars for Sentence Compression?
Michel Galley and Kathleen R. McKeown
Columbia University
Department of Computer Science
New York, NY 10027, USA
{galley,kathy}@cs.columbia.edu
Abstract
We present a sentence compression system based
on synchronous context-free grammars (SCFG),
following the successful noisy-channel approach
of (Knight and Marcu, 2000). We define a head-
driven Markovization formulation of SCFG dele-
tion rules, which allows us to lexicalize probabili-
ties of constituent deletions. We also use a robust
approach for tree-to-tree alignment between arbi-
trary document-abstract parallel corpora, which lets
us train lexicalized models with much more data
than previous approaches relying exclusively on
scarcely available document-compression corpora.
Finally, we evaluate different Markovized models,
and find that our selected best model is one that ex-
ploits head-modifier bilexicalization to accurately
distinguish adjuncts from complements, and that
produces sentences that were judged more gram-
matical than those generated by previous work.
1 Introduction
Sentence compression addresses the problem of re-
moving words or phrases that are not necessary
in the generated output of, for instance, summa-
rization and question answering systems. Given
the need to ensure grammatical sentences, a num-
ber of researchers have used syntax-directed ap-
proaches that perform transformations on the out-
put of syntactic parsers (Jing, 2000; Dorr et al,
2003). Some of them (Knight and Marcu, 2000;
Turner and Charniak, 2005) take an empirical ap-
proach, relying on formalisms equivalent to proba-
bilistic synchronous context-free grammars (SCFG)
?This material is based on research supported in part
by the U.S. National Science Foundation (NSF) under Grant
No. IIS-05-34871 and the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of the NSF or DARPA.
(Lewis and Stearns, 1968; Aho and Ullman, 1969) to
extract compression rules from aligned Penn Tree-
bank (PTB) trees. While their approach proved suc-
cessful, their reliance on standard maximum like-
lihood estimators for SCFG productions results in
considerable sparseness issues, especially given the
relative flat structure of PTB trees; in practice, many
SCFG productions are seen only once. This problem
is exacerbated for the compression task, which has
only scarce training material available.
In this paper, we present a head-driven
Markovization of SCFG compression rules, an
approach that was successfully used in syntactic
parsing (Collins, 1999; Klein and Manning, 2003)
to alleviate issues intrinsic to relative frequency
estimation of treebank productions. Markovization
for sentence compression provides several benefits,
including the ability to condition deletions on
a flexible amount of syntactic context, to treat
head-modifier dependencies independently, and to
lexicalize SCFG productions.
Another part of our effort focuses on better align-
ment models for extracting SCFG compression rules
from parallel data, and to improve upon (Knight
and Marcu, 2000), who could only exploit 1.75% of
the Ziff-Davis corpus because of stringent assump-
tions about human abstractive behavior. To alleviate
their restrictions, we rely on a robust approach for
aligning trees of arbitrary document-abstract sen-
tence pairs. After accounting for sentence pairs with
both substitutions and deletions, we reached a reten-
tion of more than 25% of the Ziff-Davis data, which
greatly benefited the lexical probabilities incorpo-
rated into our Markovized SCFGs.
Our work provides three main contributions:
180
(1) Our lexicalized head-driven Markovization
yields more robust probability estimates, and our
compressions outperform (Knight and Marcu, 2000)
according to automatic and human evaluation.
(2) We provide a comprehensive analysis of the im-
pact of different Markov orders for sentence com-
pression, similarly to a study done for PCFGs (Klein
and Manning, 2003). (3) We provide a framework
for exploiting document-abstract sentence pairs that
are not purely compressive, and augment the avail-
able training resources for syntax-directed sentence
compression systems.
2 Synchronous Grammars for Sentence
Compression
One successful syntax-driven approach (Knight and
Marcu, 2000, henceforth K&M) relies on syn-
chronous context-free grammars (SCFG) (Lewis
and Stearns, 1968; Aho and Ullman, 1969). SCFGs
can be informally defined as context-free grammars
(CFGs) whose productions have two right-hand side
strings instead of one, namely source and target
right-hand side. In the case of sentence compres-
sion, we restrict the target side to be a sub-sequence
of the source side (possibly identical), and we will
call this restricted grammar a deletion SCFG. For in-
stance, a deletion SCFG rule that removes an adver-
bial phrase (ADVP) between an noun phrase (NP)
and a verb phrase (VP) may be written as follows:
S ? ?NP ADVP VP, NP VP?
In a sentence compression framework similar to
the one presented by K&M, we build SCFGs that
are fully trainable from a corpus of document and
reduced sentences. Such an approach comprises
two subproblems: (1) transform tree pairs into syn-
chronous grammar derivations; (2) based on these
derivations, assign probabilities to deletion SCFG
productions, and more generally, to compressions
produced by such grammars. Since the main point of
our paper lies in the exploration of better probability
estimates through Markovization and lexicalization
of SCFGs, we first address the latter problem, and
discuss the task of building synchronous derivations
only later in Section 4.
2.1 Stochastic Synchronous Grammars
The overall goal of a sentence compression system is
to transform a given input sentence f into a concise
and grammatical sentence c ? C, which is a sub-
sequence of f . Similarly to K&M and many suc-
cessful syntactic parsers (Collins, 1999; Klein and
Manning, 2003), our sentence compression system
is generative, and attempts to find the optimal com-
pression c? by estimating the following function:1
c? = argmax
c?C
{
p(c|f)
}
= argmax
c?C
{
p(f , c)
}
(1)
If ?(f , c) is the set of all tree pairs that yield (f , c)
according to some underlying SCFG, we can esti-
mate the probability of the sentence pair using:
p(f , c) =
?
(pif ,pic)??(f ,c)
P (pif , pic) (2)
We note that, in practice (and as in K&M), Equa-
tion 2 is often approximated by restricting ?(f , c)
to a unique full tree p?if , the best hypothesis of an
off-the-shelf syntactic parser. This implies that each
possible compression c is the target-side yield of at
most one SCFG derivation.
As in standard PCFG history-based models, the
probability of the entire structure (Equation 2) is fac-
tored into probabilities of grammar productions. If
? is a derivation ? = r1 ? ? ? ? ? rj ? ? ? ? rJ , where
rj denotes the SCFG rule lj ? ??jf , ?
j
c?, we get:
p(pif , pic) =
J?
j=1
p(?jf , ?
j
c|l
j) (3)
The question we will now address is how to esti-
mate the probability p(?jf , ?
j
c|lj) of each SCFG pro-
duction.
2.2 Lexicalized Head-Driven Markovization of
Synchronous Grammars
A main issue in our enterprise is to reliably estimate
productions of deletion SCFGs. In a sentence com-
pression framework as the one presented by K&M,
we use aligned trees of the form of the Penn Tree-
bank (PTB) (Marcus et al, 1994) to acquire and
score SCFG productions. However, the use of the
PTB structure faces many challenges also encoun-
tered in probabilistic parsing.
1In their noisy-channel approach, K&M further break down
p(c, f) into p(f |c) ? p(c), which we refrain from doing for rea-
sons that will become obvious later.
181
Firstly, PTB tree structures are relatively flat, par-
ticularly within noun phrases. For instance, adjec-
tive phrases (ADJP)?which are generally good can-
didates for deletions?appear in 90 different NP-
rooted SCFG productions in Ziff-Davis,2 61 of
which appear only once, e.g., NP ? ?DT ADJP JJ
NN NN, DT JJ NN NN?. While it may seem ad-
vantageous to maintain many constituents within the
same domain of locality of an SCFG production, as
we may hope to exploit its large syntactic context to
condition deletions more accurately, the sparsity of
such productions make them poor candidates for rel-
ative frequency estimation, especially in a task with
limited quantities of training material. Indeed, our
base training corpus described in Section 4 contains
only 951 SCFG productions, 593 appearing once.
Secondly, syntactic categories in the PTB are par-
ticularly coarse grained, and lead to many incorrect
context-free assumptions. Some important distinc-
tions, such as between arguments and adjuncts, are
beyond the scope of the PTB annotation, and it is
often difficult to determine out of context whether a
given constituent can safely be deleted from a right-
hand side.
One first type of annotation that can effectively be
added to each syntactic category is its lexical head
and head part-of-speech (POS), following work in
syntactic parsing (Collins, 1999). This type of an-
notation is particular beneficial in the case of, e.g.,
prepositional phrases (PP), which may be either
complement or adjunct. As in the case of Figure 1
(in which adjuncts appear in italic), knowing that the
PP headed by ?from? appears in a VP headed by
?fell? helps us to determine that the PP is a com-
plement to the verb ?fell?, and that it should pre-
sumably not be deleted. Conversely, the PP headed
by ?because? modifying the same verb is an adjunct,
and can safely be deleted if unimportant.3 Also, as
discussed in (Klein and Manning, 2003), POS an-
notation can be useful as a means of backing off
to more frequently occurring head-modifier POS oc-
currences (e.g., VBD-IN) when specific bilexical co-
2Details about the SCFG extraction procedure are given in
Section 4. In short, we refer here to a grammar generated from
823 sentence pairs.
3The PP headed by ?from? is an optional argument, and thus
may still be deleted. Our point is that lexical information in gen-
eral should help give lower scores to deletions of constituents
that are grammatically more prominent.
NN
Earning
NP
RB
also
ADVP
VBD
fell IN
from DT
the
JJ
year-ago
NN
period
NP
PP
IN
because
IN
of VBG
slowing
NN
microchip
NN
demand
NP
PP
VP .
.
S
Figure 1: Penn Treebank tree with adjuncts in italic.
occurrences are sparsely seen (e.g., ?fell?-?from?).
At a lower level, lexicalization is clearly desirable
for pre-terminals. Indeed, current SCFG models
such as K&M have no direct way of preventing
highly improbable single word removals, such as
deletions of adverbs ?never? or ?nowhere?, which
may turn a negative statement into a positive one.4
A second type of annotation that can be added to
syntactic categories is the so-called parent annota-
tion (Johnson, 1998), which was effectively used in
syntactic parsing to break unreasonable context-free
assumptions. For instance, a PP with a VP parent
is marked as PP?VP. It is reasonable to assume that,
e.g., that constituents deep inside a PP have more
chances to be removed than otherwise expected, and
one may seek to increase the amount of vertical
context that is available for conditioning each con-
stituent deletion.
To achieve the above desiderata for better SCFG
probability estimates?i.e., reduce the amount of
sister annotation within each SCFG production, by
conditioning deletions on a context smaller than an
entire right-hand side, and at the same time in-
crease the amount of ancestor and descendent an-
notation through parent (or ancestor) annotation and
lexicalization?we follow the approach of (Collins,
1999; Klein and Manning, 2003), i.e., factor-
ize n-ary grammar productions into products of n
right-hand side probabilities, a technique sometimes
called Markovization.
Markovization is generally head-driven, i.e., re-
flects a decomposition centered around the head of
each CFG production:
l ? ?Lm ? ? ?L1HR1 ? ? ?Rn? (4)
4K&M incorporate lexical probabilities through n-gram
models, but such language models are obviously not good for
preventing such unreasonable deletions.
182
where H is the head, L1, . . . , Lm the left modi-
fiers, R1, . . . , Rn are right modifiers, and ? termi-
nation symbols needed for accurate probability es-
timations (e.g., to capture the fact that certain con-
stituents are more likely than others to be the right-
most constituent); for simplicity, we will ignore ?
in later discussions. For a given SCFG production
l ? ??f , ?c?, we ask, given the source RHS ?f
that is assumed given (e.g., provided by a syntactic
parser), which of its RHS elements are also present
in ?c. That is, we write:
p(?c|?f , l) = (5)
p(kml , ? ? ? , k
1
l , kh, k
1
r , ? ? ? , k
n
r |?f , l)
where kh, kil , k
j
r (?k? for keep) are binary variables
that are true if and only if constituents H,Li, Rj (re-
spectively) of the source RHS ?f are present in the
target side ?c. Note that the conditional probabil-
ity in Equation 5 enables us to estimate Equation 3,
since p(?f , ?c|l) = p(?c|?f , l) ? p(?f |l). We can
rely on a state-of-the-art probabilistic parser to ef-
fectively compute either p(?f |l) or the probability
of the entire tree pif , and need not worry about esti-
mating this term. In the case of sentence compres-
sion from the one-best hypothesis of the parser, we
can ignore p(?f |l) altogether, since pif is the same
for all compressions.
We can rewrite Equation 5 exactly using a head-
driven infinite-horizon Markovization:
p(?c|?f , l) = p(kh|?f , l) (6)
?
?
i=1...m
p(kil |k
1
l , ? ? ? , k
i?1
l , kh, ?f , l)
?
?
i=1...n
p(kir|k
1
r , ? ? ? , k
i?1
r , kh,?, ?f , l)
where ? = (k1l , ? ? ? , k
m
l ) is a term needed by the
chain rule. One key issue is to make linguistically
plausible assumptions to determine which condi-
tioning variables in the terms should be deleted. Fol-
lowing our discussion in the first part of this section,
we may start by making an order-s Markov approx-
imation centered around the head, i.e., we condi-
tion each binary variable (e.g., kir) on a context of
up to s sister constituents between the current con-
stituent and the head (e.g., (Ri?s, . . . , Ri)). In or-
der to incorporate bilexical dependencies between
the head and each modifier, we also condition all
modifier probabilities on head variables H (and kh).
These assumptions are overall quite similar to the
ones made in Markovized parsing models. If we as-
sume that all other conditioning variables in Equa-
tion 6 are irrelevant, we write:
p(?c|?f , l) = ph(kh|H, l) (7)
?
?
i=1...m
pl(k
i
l |L
i?s, ..., Li, ki?sl , ..., k
i?1
l ,H, kh, l)
?
?
i=1...n
pr(k
i
r|R
i?s, ..., Ri, ki?sr , ..., k
i?1
r ,H, kh, l)
Note that it is important to condition deletions on
both constituent histories (Ri?s, . . . , Ri) and non-
deletion histories (ki?sr , . . . , k
i?1
r ); otherwise we
would be unable to perform deletions that must op-
erate jointly, as in production S??ADVP COMMA
NP VP, NP VP? (in which the ADVP should not be
deleted without the comma). Without binary his-
tories, we often observed superfluous punctuation
symbols and dangling coordinate conjunctions ap-
pearing in our outputs.
Finally, we label l with an order-v ancestor anno-
tation, e.g., for the VP in Figure 1, l = ? for v = 0,
l =VP?S for v = 2, and so on. We also replace H
and modifiers Li and Ri by lexicalized entries, e.g.,
H =(VP,VBD,fell) and Ri =(PP,IN,from). Note
that to estimate pl(kil | ? ? ? ), we only lexicalize L
i
andH , and none of the other conditioning modifiers,
since this would, of course, introduce too many con-
ditioning variables (the same goes for pr(kir| ? ? ? )).
The question of how much sister and vertical (s and
v) context is needed for effective sentence compres-
sion, and whether to use lexical or POS annotation,
will be evaluated in detail in Section 5.
3 The Data
To acquire SCFG productions, we used Ziff-Davis,
a corpus of technical articles and human abstractive
summaries. Articles and summaries are paired by
document, so the first step was to perform sentence
alignment. In the particular case of sentence com-
pression, a simple approach is to just consider com-
pression pairs (f,c), where c is a substring of f. K&M
identified only 1,087 such paired sentences in the en-
tire corpus, which represents a recall of 1.75%.
For our empirical evaluations, we split the data as
follows: among the 1,055 sentences that were taken
183
to train systems described in K&M, we selected the
first 32 sentence pairs to be an auxiliary test corpus
(for future work), the next 200 sentences to be our
development corpus, and the remaining 823 to be
our base training corpus (ZD-0), which will be aug-
mented with additional data as explained in the next
section. We feel it is important to use a relatively
large development corpus, since we will provide in
Section 5 detailed analyses of model selection on
the development set (e.g., by evaluating different
Markov structures), and we want these findings to
be as significant as possible. Finally, we used the
same test data as K&M for human evaluation pur-
poses (32 sentence pairs).
4 Tree Alignment and Synchronous Gram-
mar Inference
We now describe methods to train SCFG models
from sentence pairs. Given a tree pair (f , c), whose
respective parses (pif , pic) were generated by the
parser described in (Charniak and Johnson, 2005),
the goal is to transform the tree pair into SCFG
derivations, in order to build relative frequency es-
timates for our Markovized models from observed
SCFG productions. Clearly, the two trees may
sometimes be structurally quite different (e.g., a
given PP may attach to an NP in pif , while attach-
ing to VP in pic), and it is not always possible to
build an SCFG derivation given the constraints in
(pif , pic). The approach taken by K&M is to analyze
both trees and count an SCFG rule whenever two
nodes are ?deemed to correspond?, i.e., roots are the
same, and ?c is a sub-sequence of ?f . This leads
to a quite restricted number of different productions
on our base training set (ZD-0): 823 different pro-
ductions were extracted, 593 of which appear only
once. This first approach has serious limitations;
the assumption that sentence compression appropri-
ately models human abstractive data is particularly
problematic. This considerably limits the amount
of training data that can be exploited in Ziff-Davis
(which contains overall more than 4,000 documents-
abstract pairs), and this makes it very difficult to
train lexicalized models.
An approach to slightly loosen this assumption
is to consider document-abstract sentence pairs in
which the condensed version contains one or more
substitutions or insertions. Consider for example
DT[3]
The[4]
JJ[5]
second[6]
NN[7]
computer[8]
NP[2]
VBD
started RP
up
PRT
VP CC
and VBD[10]
ran[11] IN[13]
without[14] NN[16]
incident[17]
NP[15]
PP[12]
VP[9]
VP .[18]
.[19]
S[1]
DT[3]
The[4]
JJ[5]
second[6]
NN[7]
unit[8]
NP[2]
VBD[10]
ran[11] IN[13]
without[14] NN[16]
incident[17]
NP[15]
PP[12]
VP[9] .[18]
.[19]
S[1]
Figure 2: Full sentence and its revision. While the latter is not a
compression of the former, it could still be used to gather statis-
tics to train a sentence compression system, e.g., to learn the
reduction of a VP coordination.
the tree pair in Figure 2: the two sentences are syn-
tactically very close, but the substitution of ?com-
puter? with ?unit? makes this sentence pair unus-
able in the framework presented in K&M. Arguably,
there should be ways to exploit abstract sentences
that are slightly reworded in addition to being com-
pressed. To use sentence pairs with insertions and
substitutions, we must find a way to align tree pairs
in order to identify SCFG productions. More specif-
ically, we must define a constituent alignment be-
tween the paired abstract and document sentences,
which determine how the two trees are synchronized
in a derivation. Obtaining this alignment is no triv-
ial matter as the number of non-deleting edits in-
creases. To address this, we synchronized tree pairs
by finding the constituent alignment that minimizes
the edit distance between the two trees, i.e., mini-
mize the number of terminals and non-terminals in-
sertions, substitutions and deletions.5 While criteria
5The minimization problem is known to be NP hard, so we
used an approximation algorithm (Zhang and Shasha, 1989) that
184
other than minimum tree edit distance may be effec-
tive, we found?after manual inspections of align-
ments between sentences with less than five non-
deleting edits?that this method generally produces
good alignments. A sample alignment is provided in
Figure 2. Once a constituent alignment is available,
it is then trivial to extract all deletion SCFG rules
available in a tree pair, e.g., NP ? ?DT JJ NN, DT
JJ NN? in the figure.
We also exploited more general tree productions
known as synchronous tree substitution grammar
(STSG) rules, in an approach quite similar to (Turner
and Charniak, 2005). For instance, the STSG rule
rooted at S can be decomposed into two SCFG pro-
ductions if we allow unary rules such as VP?VP to
be freely added to the compressed tree. More specif-
ically, we decompose any STSG rule that has in its
target (compressed) RHS a single context free pro-
duction, and that contains in its source (full) RHS
a single context free production adjoined with any
number of tree adjoining grammar (TAG) auxiliary
trees (Joshi et al, 1975). In the figure, the initial tree
is S ? NP VP, and the adjoined (auxiliary) tree is
VP ? VP CC VP.6 We found this approach quite
helpful, since most useful compressions that mimic
TAG adjoining operations are missed by the extrac-
tion procedure of K&M.
Since we found that exploiting sentence pairs con-
taining insertions had adverse consequences in terms
of compression accuracies, we only report experi-
ments with sentence pairs containing no insertions.
We gathered sentence pairs with up to six substi-
tutions using minimum edit distance matching (we
will refer to these sets as ZD-0 to ZD-6). With a
limit of up to six substitutions (ZD-6), we were able
to train our models on 16,787 sentences, which rep-
resents about 25% of the total number of summary
sentences of the Ziff-Davis corpus.
5 Experiments
All experiments presented in this section are per-
formed on the Ziff-Davis corpus. We note first that
all probability estimates of our Markovized gram-
runs in polynomial time.
6To determine whether a given one-level tree is an auxiliary,
we simply check the following properties: all its leaves but one
(the ?foot node?) must be nodes attached to deleted subtrees
(e.g., VP and CC in the figure), and the foot node (VP[9]) must
have the same syntactic category as the root node.
mars are smoothed. Indeed, incorporating lexical
dependencies within models trained on data sets as
small as 16,000 sentence pairs would be quite fu-
tile without incorporating robust smoothing tech-
niques. Different smoothing techniques were eval-
uated with our models, and we found that interpo-
lated Witten-Bell discounting was the method that
performed best. We used relative frequency es-
timates for each of the models presented in Sec-
tion 2.2 (i.e., ph, pl, pr), and trained pl separately
from pr. We interpolated our most specific models
(lexical heads, POS tags, ancestor and sister annota-
tion) with lower-order models.7
Automatic evaluation on development sets is per-
formed using word-level classification accuracy, i.e.,
the number of words correctly classified as being
either deleted or not deleted, divided by the to-
tal number of words. In our first evaluation, we
experimented with different horizontal and vertical
Markovizations (Table 1). First, it appears that ver-
tical annotation is moderately helpful. It provides
gains in accuracy ranging from .5% to .9% for v = 1
over a simpler models (v = 0), but higher orders
(v > 1) have a tendency to decrease performance.
On the other hand, sister annotation of order 1 is
much more critical, and provides 4.1% improvement
over a simpler model (s = 0, v = 0). Manual exami-
nations of compression outputs confirmed this anal-
ysis: without sister annotation, deletion of punctu-
ation and function words (determiners, coordinate
conjunctions, etc.) is often inaccurate, and compres-
sions clearly lack fluency. This annotation is also
helpful for phrasal deletions; for instance, we found
that PPs are deleted in 31.4% of cases in Ziff-Davis
if they do not immediately follow the head con-
stituent, but this percentage drops to 11.1% for PPs
that immediately follow the head. It seems, how-
ever, that increasing sister annotation beyond s > 1
only provide limited improvements.
In our second evaluation reported in Table 2, we
7We relied on the SRI language modeling (SRILM) toolkit
library for all smoothing experiments. We used the following
order in our deleted interpolation of ph: lexical head, head POS,
ancestor annotation, and head category. For pl and pr , we re-
moved first: lexical head, lexical head of the modifier, head
POS, head POS of the modifier, sister annotation (Li deleted
before kil ), kh, category of the head, category of the modifier.
We experimented with different deletion interpolation order-
ings, and this ordering appears to work quite well in practice,
and was used in all experiments reported in this paper.
185
assessed the usefulness of lexical and POS anno-
tation (setting s and v to 0). In the table, we use
M to denote any of the modifiers Li or Ri, and
c, t, w respectively represent syntactic constituent,
POS, and lexical conditioning. While POS annota-
tion is clearly advantageous compared to using only
syntactic categories, adding lexical variables to the
model also helps. As is shown in the table, it is es-
pecially important to know the lexical head of the
modifier we are attempting to delete. The addition of
wm to conditioning variables provides an improve-
ment of 1.3% (from 66.5% to 67.8%) on our op-
timal Ziff-Davis training corpus (ZD-6). Further-
more, bilexical head-modifier dependencies provide
a relatively small improvement of .5% (from 69.8%
to 70.3%) over the best model that does not incor-
porate the lexical head wh. Note that lexical con-
ditioning also helps in the case where the training
data is relatively small (ZD-0), though differences
are less significant, and bilexical dependencies actu-
ally hurt performance. In subsequent experiments,
we experimented with different Markovizations and
lexical dependency combination, and finally settled
with a model (s = 1 and v = 1) incorporating all
conditioning variables listed in the last line of Ta-
ble 2. This final tuning was combined with human
inspection of generated outputs, since certain modi-
fications that positively impacted output quality sel-
dom changed accuracies.
We finally took the best configuration selected
above, and evaluated our model against the noisy-
channel model of K&M on the 32 test sentences se-
lected by them. We performed both automatic and
human evaluation against the output produced by
Knight and Marcu?s original implementation of their
noisy channel model (Table 3). In the former case,
we also provide Simple String Accuracies (SSA).8
For human evaluation, we hired six native-speaker
judges who scored grammaticality and content (im-
portance) with scores from 1 to 5, using instructions
as described in K&M. Both types of evaluations fa-
vored our Markovized model against the noisy chan-
nel model.
Table 4 shows several outputs of our system
8SSA is defined as: SSA = 1 ? (I + D + S)/R. The
numerator terms are respectively the number of inserts, deletes,
and substitutions, and R is the length of the reference compres-
sion.
Vertical Horizontal Order
Order s = 0 s = 1 s = 2 s = 3
v = 0 63 67.1 67.2 67.2
v = 1 63.9 67.6 67.7 67.7
v = 2 65.7 66.6 66.9 66.9
v = 3 65.2 66.8 67.1 67
Table 1: Markovizations accuracies on Ziff-Davis devel set.
Conditioning Variables ZD-0 ZD-3 ZD-6
M=cm H=ch 62.2 62.4 64.4
M=(cm, tm) H=ch 63.0 63.4 66.5
M=(cm, wm) H=ch 64.2 65.2 66.7
M=(cm, tm, wm) H=ch 63.8 65.8 67.8
M=(cm, tm, wm) H=(ch, th) 66.7 68.6 69.8
M=(cm, tm, wm) H=(ch, wh) 66.9 68.9 70.3
M=(cm, tm, wm) H=(ch, th, wh) 66.3 69.1 69.8
Table 2: Accuracies on Ziff-Davis devel set with different head-
modifier annotations.
Models Acc SSA Grammar Content Len(%)
NoisyC 61.3 14.6 4.37 ? 0.5 3.87 ? 1.2 70.4
Markov 67.9 31.7 4.68 ? 0.4 4.22 ? 0.4 62.7
Human - - 4.95 ? 0.1 4.43 ? 0.3 53.3
Table 3: Accuracies on Ziff-Davis test set.
(Markov) that significantly differed from the output
of the noisy channel model (NoisyC), which con-
firms our finding that Markovized models can pro-
duce quite grammatical output. Our compression for
the first sentence underlines one of the advantages of
constituent-based classifiers, which have the ability
of deleting a very long phrase (here, a PP) at once.
The three next sentences display some advantages
of our approach over the K&M model: here, the lat-
ter model performs deletion with too little lexico-
syntactic information, and accidentally removes cer-
tain modifiers that are sometimes, but not always,
good candidates for deletions (e.g., ADJP in Sen-
tence 2, PP in sentences 3 and 4). On the other hand,
our model keeps these constituent intact. Finally, the
fifth and last example is one of the only three cases
(among the 32 sentences) where our model produced
a sentence we judged clearly ungrammatical. After
inspection, we found that our parser assigned par-
ticularly errorful trees to those inputs, which may
partially explain these ungrammatical outputs.
6 Related Work
A relatively large body of work addressed the prob-
lem of sentence compression. One successful recent
approach (McDonald, 2006) combines a discrimi-
native framework with a set of features that cap-
ture information similar to the K&M model. Mc-
186
Input Many debugging features, including user-defined break points
and variable-watching and message-watching windows, have
been added.
NoisyC Many debugging features, including user-defined points and
variable-watching and message-watching windows, have been
added.
Markov Many debugging features have been added.
Human Many debugging features have been added.
Input The chemical etching process used for glare protection is effec-
tive and will help if your office has the fluorescent-light overkill
that ?s typical in offices.
NoisyC The process used for glare protection is and will help if your
office has the overkill
Markov The chemical etching process used for glare protection is ef-
fective.
Human Glare protection is effective.
Input The utilities will be bundled with Quickdex II in a $90 pack-
age called super quickdex, which is expected to ship in late
summer.
NoisyC The utilities will be bundled
Markov The utilities will be bundled with Quickdex II.
Human The utilities will be bundled with Quickdex II.
Input The discounted package for the SparcServer 470 is priced at
$89,900, down from the regular $107,795.
NoisyC The package for the 470 is priced
Markov The discounted package for the SparcServer 470 is at $89,900.
Human The SparcServer 470 is priced at $89,900, down from the reg-
ular $107,795.
Input Prices range from $5,000 for a microvax 2000 to $179,000 for
the vax 8000 or higher series.
NoisyC Prices range from $5,000 for a 2000 to $179,000 for the vax
8000 or higher series.
Markov Prices range from $5,000 for a microvax for the vax.
Human Prices range from $5,000 to $179,000.
Table 4: Compressions of sample test sentences.
Donald?s features include compression bigrams, as
well as soft syntactic evidence extracted from parse
trees and dependency trees. The strength of McDon-
ald?s approach partially stems from its robustness
against redundant and noisy features, since each fea-
ture is weighted proportionally to its discriminative
power, and his approach is thus hardly penalized
by uninformative features. In contrast, our work
puts much more emphasis on feature analysis than
on efficient optimization, and relies on a statisti-
cal framework (maximum-likelihood estimates) that
strives for careful feature selection and combination.
It also describes and evaluates models incorporating
syntactic evidence that is new to the sentence com-
pression literature, such as head-modifier bilexical
dependencies, and nth-order sister and vertical an-
notation. We think this work leads to a better un-
derstanding of what type of syntactic and lexical ev-
idence makes sentence compression work. Further-
more, our work leaves the door open to uses of our
factored model in a constituent-based or word-based
discriminative framework, in which each elemen-
tary lexico-syntactic structure of this paper can be
discriminatively weighted to directly optimize com-
pression quality. Since McDonald?s approach does
not incorporate SCFG deletion rules, and conditions
deletions on less lexico-syntactic context, we believe
this will lead to levels of performance superior to
both papers.
7 Conclusions
We presented a sentence compression system based
on SCFG deletion rules, for which we defined
a head-driven Markovization formulation. This
Markovization enabled us to incorporate lexical con-
ditioning variables into our models. We empirically
evaluated different Markov structures, and obtained
a best system that generates particularly grammati-
cal sentences according to a human evaluation. Our
sentence compression system is freely available for
research and educational purposes.
Acknowledgments
We would like to thank Owen Rambow, Michael
Collins, Julia Hirschberg, and Daniel Ellis for their
helpful comments and suggestions.
References
A. Aho and J. Ullman. 1969. Syntax directed translations and
the pushdown assembler. 3:37?56.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and maxent discriminative reranking. In Proc. of ACL.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, U. of Pennsylvania.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge: A parse-and-
trim approach to headline generation. In Proc. of DUC.
H. Jing. 2000. Sentence reduction for automatic text summa-
rization. In Proc. of NAACL, pages 310?315.
M. Johnson. 1998. PCFG models of linguistic tree representa-
tions. Computational Linguistics, 24(4):613?632.
A. Joshi, L. Levy, and M. Takahashi. 1975. Tree adjunct gram-
mar. Journal of Computer and System Science, 21(2).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In Proc. of ACL.
K. Knight and D. Marcu. 2000. Statistics-based summarization
? step one: Sentence compression. In Proc. of AAAI.
P. Lewis and R. Stearns. 1968. Syntax-directed transduction.
In Journal of the Association for Computing Machinery, vol-
ume 15, pages 465?488.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994. Build-
ing a large annotated corpus of english: The penn treebank.
Computational Linguistics, 19(2):313?330.
R. McDonald. 2006. Discriminative sentence compression
with soft syntactic constraints. In Proc. of EACL.
J. Turner and E. Charniak. 2005. Supervised and unsupervised
learning for sentence compression. In Proc. of ACL.
K. Zhang and D. Shasha. 1989. Simple fast algorithms for the
editing distance between trees and related problems. SIAM
J. Comput., 18(6):1245?1262.
187
Proceedings of NAACL HLT 2007, pages 428?435,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Building and Rening Rhetorical-Semantic Relation Models
Sasha Blair-Goldensohn and
Google, Inc.
76 Ninth Avenue
New York, NY
sasha@google.com
Kathleen R. McKeown? and Owen C. Rambow?
? Department of Computer Science
? Center for Computational Learning Systems
Columbia University
{kathy,rambow}@cs.columbia.edu
Abstract
We report results of experiments which
build and refine models of rhetorical-
semantic relations such as Cause and Con-
trast. We adopt the approach of Marcu
and Echihabi (2002), using a small set of
patterns to build relation models, and ex-
tend their work by refining the training
and classification process using parame-
ter optimization, topic segmentation and
syntactic parsing. Using human-annotated
and automatically-extracted test sets, we
find that each of these techniques results in
improved relation classification accuracy.
1 Introduction
Relations such as Cause and Contrast, which we call
rhetorical-semantic relations (RSRs), may be sig-
naled in text by cue phrases like because or how-
ever which join clauses or sentences and explicitly
express the relation of constituents which they con-
nect (Example 1). In other cases the relation may be
implicitly expressed (2).1
Example 1 Because of the recent accounting scan-
dals, there have been a spate of executive resigna-
tions.
Example 2 The administration was once again be-
set by scandal. After several key resignations ...
1The authors would like to thank the four anonymous re-
viewers for helpful comments. This work was supported by the
Defense Advanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023. Any opinions, findings and
conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of
DARPA.
The first author performed most of the research reported in
this paper while at Columbia University.
In this paper, we examine the problem of detect-
ing such relations when they are not explicitly sig-
naled. We draw on and extend the work of Marcu
and Echihabi (2002). Our baseline model directly
implements Marcu and Echihabi?s approach, opti-
mizing a set of basic parameters such as smoothing
weights, vocabulary size and stoplisting. We then
focus on improving the quality of the automatically-
mined training examples, using topic segmenta-
tion and syntactic heuristics to filter out training
instances which may be wholly or partially in-
valid. We find that the parameter optimization and
segmentation-based filtering techniques achieve sig-
nificant improvements in classification performance.
2 Related Work
Rhetorical and discourse theory has a long tradition
in computational linguistics (Moore and Wiemer-
Hastings, 2003). While there are a number of differ-
ent relation taxonomies (Hobbs, 1979; McKeown,
1985; Mann and Thompson, 1988; Martin, 1992;
Knott and Sanders, 1998), many researchers have
found that, despite small differences, these theories
have wide agreement in terms of the core phenom-
ena for which they account (Hovy and Maier, 1993;
Moser and Moore, 1996).
Work on automatic detection of rhetorical and dis-
course relations falls into two categories. Marcu
and Echihabi (2002) use a pattern-based approach
in mining instances of RSRs such as Contrast and
Elaboration from large, unannotated corpora. We
discuss this work in detail in Section 3. Other
work uses human-annotated corpora, such as the
RST Bank (Carlson et al, 2001), used by Soricut
and Marcu (2003), the GraphBank (Wolf and Gib-
son, 2005), used by Wellner et al (2006), or ad-
hoc annotations, used by (Girju, 2003; Baldridge
and Lascarides, 2005). In the past year, the ini-
428
tial public release of the Penn Discourse TreeBank
(PDTB) (Prasad et al, 2006) has significantly ex-
panded the discourse-annotated corpora available to
researchers, using a comprehensive scheme for both
implicit and explicit relations.
Some work in RSR detection has enlisted syntac-
tic analysis as a tool. Marcu and Echihabi (2002) fil-
ter training instances based on Part-of-Speech (POS)
tags, and Soricut and Marcu (2003) use syntac-
tic features to identify sentence-internal RST struc-
ture. Lapata and Lascarides (2004) focus their
work syntactically, analyzing temporal links be-
tween main and subordinate clauses. Sporleder and
Lascarides (2005) extend Marcu and Echihabi?s ap-
proach with the addition of a number of features,
including syntactic features based on POS and ar-
gument structure, as well as lexical and other sur-
face features. They report that, when working with
sparse training data, this richer feature set, combined
with a boosting-based algorithm, achieves more ac-
curate classification than Marcu and Echihabi?s sim-
pler, word-pair based approach (we describe the lat-
ter in the next section).
3 The M&E Framework
We model two RSRs, Cause and Contrast, adopt-
ing the definitions of Marcu and Echihabi (2002)
(henceforth M&E) for their Cause-Explanation-
Evidence and Contrast relations, respectively. In
particular, we follow their intuition that in building
an automated model it is best to adopt a higher-level
view of relations (cf. (Hovy and Maier, 1993)),
collapsing the finer-grained distinctions that hold
within and across relation taxonomies.
M&E use a three-stage approach common in cor-
pus linguistics: collect a large set of class instances
(instance mining), analyze them to create a model
of differentiating features (model building), and use
this model as input to a classication step which
determines the most probable class of unknown in-
stances.
The intuition of the M&E model is to apply a set
of RSR-associated cue phrase patterns over a large
text corpus to compile a training set without the cost
of human annotation. For instance, Example 1 will
match the Cause-associated pattern ?Because of W1
, W2 .?, where W1 and W2 stand for non-empty
strings containing word tokens. In the aggregate,
such instances increase the prior belief that, e.g.,
a text span containing the word scandals and one
containing resignations are in a Cause relation. A
critical point is that the cue words themselves (e.g.,
because) are discarded before extracting these word
pairs; otherwise these cue phrases themselves would
likely be the most distinguishing features learned.
More formally, M&E build up their model
through the three stages mentioned above as fol-
lows: In instance mining, for each RSR r they com-
pile an instance set Ir of (W1,W2) spans which
match a set of patterns associated with r. In
model building, features are extracted from these in-
stances; M&E extract a single feature, namely the
frequency of token pairs derived from taking the
cartesian product of W1 = {w1...wn} ? W2 =
{wn+1...wm} = {(w1, wn+1)...(wn, wm)} over
each span pair instance (W1,W2) ? I; these pair
frequencies are tallied for each RSR into a frequency
table Fr. Then in classication, the most likely re-
lation r between two unknown-relation spans W1
and W2 can be determined by a na??ve Bayesian
classifier as argmaxr?R P (r|W1,W2), where the
probability P (r|W1,W2) is simplified by assum-
ing the independence of the individual token pairs
to: ?(wi,wj)?W1,W2 P ((wi, wj)|r). The frequencycounts Fr are used as maximum likelihood estima-
tors of P ((wi, wj)|r).
4 TextRels
TextRels is our implementation of the M&E frame-
work, and serves as our platform for the experiments
which follow.
For instance mining, we use a set of cue phrase
patterns derived from published lists (e.g., (Marcu,
1997; Prasad et al, 2006)) to mine the Gigaword
corpus of 4.7 million newswire documents2 for re-
lation instances. We mine instances of the Cause
and Contrast RSRs discussed earlier, as well as a
NoRel ?relation?. NoRel is proposed by M&E as
a default model of same-topic text across which no
specific RSR holds; instances are extracted by tak-
ing text span pairs which are simply sentences from
the same document separated by at least three inter-
vening sentences. Table 1 lists a sample of our ex-
2distributed by the Linguistic Data Consortium
429
Type Sample Patterns Instances Instances, M&E
Cause BOS Because W1 , W2 EOS
BOS W1 EOS BOS Therefore , W2 EOS.
926,654 889,946
Contrast BOS W1 , but W2 EOS
BOS W1 EOS BOS However , W2 EOS.
3,017,662 3,881,588
NoRel BOS W1 EOS (BOS EOS){3,} BOS W2 EOS 1,887,740 1,000,000
Table 1: RSR types, sample extraction patterns, number of training instances used in TextRels, and number
of training instances used by M&E. BOS and EOS are sentence beginning/end markers.
traction patterns and the total number of training in-
stances per relation; in addition, we hold out 10,000
instances of each type, which we divide evenly into
development and training sets.
For model building, we compile the training in-
stances into token-pair frequencies. We implement
several parameters which control the way these fre-
quencies are computed; we discuss these parameters
and their optimization in the next section.
For classication, we implement three binary
classifiers (for Cause vs Contrast, Cause vs NoRel
and Contrast vs NoRel) using the nai?ve Bayesian
framework of the M&E approach. We implement
several classification parameters, which we discuss
in the next section.
5 Parameter Optimization
Our first set of experiments examine the impact of
various parameter settings in TextRels, using classi-
fication accuracy on a development set as our heuris-
tic. We find that the following parameters have
strong impacts on classification:
? Tokenizing our training instances using stem-
ming slightly improves accuracy and also reduces
model size.
? Laplace smoothing is as accurate as Good-
Turing, but is simpler to implement. Our experi-
ments find peak performance with 0.25 ? value, i.e.
the frequency assumed for unseen pairs.
? Vocabulary size of 6,400 achieves peak perfor-
mance; tokens which are not in the most frequent
6,400 stems (computed over Gigaword) are replaced
by an UNK pseudo-token before F is computed.
? Stoplisting has a negative impact on accuracy;
we find that even the most frequent tokens contribute
useful information to the model; a stoplist size of
zero achieves peak performance.
? Minimum Frequency cutoff is imposed to dis-
card from F token pair counts with a frequency of
< 4; results degrade slightly below this value, and
discarding this long tail of rare pair counts signifi-
cantly shrinks model size.
Classif.
/
Pdtb Auto Auto-
S
M&E
TestSet Opt Seg Opt Seg Opt Seg
Cau/Con 59.1 61.1 69.8 69.7 70.3 70.6 87
Cau/NR 75.2 74.3 72.7 73.5 71.2 72.3 75
Con/NR 67.4 69.7 70.7 71.3 68.2 70.0 64
Table 2: Classifier accuracy across PDTB, Auto
and Auto-S test sets for the parameter-optimized
classifier (?Opt?) and the same classifier trained on
segment-constrained instances (?Seg?). Accuracy
from M&E is reported for reference, but we note that
they use a different test set so the comparison is not
exact. Baseline in all cases is 50%.
To evaluate the performance of our three binary
classifiers using these optimizations, we follow the
protocol of M&E. We present the classifier for, e.g.,
Cause vs NoRel with an equal number of span-pair
instances for each RSR (as in training, any pattern
text has been removed). We then determine the ac-
curacy of the classifier in predicting the actual RSR
of each instance; in all cases we use an equal num-
ber of input pairs for each RSR so random baseline
is 50 %. We carry out this evaluation over two dif-
ferent test sets.
The first set (?PDTB?) is derived from the Penn
Discourse TreeBank (Prasad et al, 2006). We ex-
tract ?Implicit? relations, i.e. text spans from adja-
cent sentences between which annotators have in-
ferred semantics not marked by any surface lexi-
cal item. To extract test instances for our Cause
RSR, we take all PDTB Implicit relations marked
with ?Cause? or ?Consequence? semantics (344 to-
tal instances); for our Contrast RSR, we take in-
stances marked with ?Contrast? semantics (293 to-
430
tal instances).3 PDTB marks the two ?Arguments?
of these relationship instances, i.e. the text spans to
which they apply; these are used as test (W1,W2)
span pairs for classification. We test the perfor-
mance on PDTB data using 280 randomly selected
instances each from the PDTB Cause and Contrast
sets, as well as 280 randomly selected instances
from our test set of automatically extracted NoRel
instances (while there is a NoRel relation included
in PDTB, it is too sparse to use in this testing, with
53 total examples).
The second test set (?Auto?) uses the 5,000 test
instances of each RSR type automatically extracted
in our instance mining process.
Table 2 lists the accuracy for the optimized
(?Opt?) classifier over the Auto and PDTB test sets4.
(The ?Seg? columns and ?Auto-S? test set are ex-
plained in the next section.)
We also list for reference the accuracy reported
by M&E; however, their training and test sets are
not the same so this comparison is inexact, al-
though their test set is extracted automatically in the
same manner as ours. In the Cause versus Contrast
case, their reported performance exceeds ours sig-
nificantly; however, in a subset of their experiments
which test Cause versus Contrast on instances from
the human annotated RSTBank corpus (Carlson et
al., 2001) where no cue phrase is present, they re-
port only 63% accuracy over a 56% baseline (the
baseline is > 50% because the number of input ex-
amples is unbalanced).
Since we also experience a drop in performance
from the automatically derived test set to the human-
annotated test set (the PDTB in our case), we fur-
ther examined this issue. Our goal was to see if the
lower accuracy on the PDTB examples is due to (1)
the inherent difficulty of identifying implicit rela-
tion spans or (2) something else, such as the corpus-
switching effect due to our model being trained and
3Note that we are using the initial PDTB release, in which
only three of 24 data sections have marked Implicit relations, so
that the number of such examples will presumably grow in the
next release.
4We do not provide pre-optimization baseline accuracy be-
cause this would be arbitrarily depend on how sub-optimally we
select values select parameter values. For instance, by using a
Vocabulary Size of 3,200 (rather than 6,400) and a Laplace ?
value of 1, the mean accuracy of the classifiers on the Auto test
set drops from 71.6 to 70.5; using a Stoplist size of 25 (rather
than 0) drops this number to 67.3.
tested on different corpora (Gigaword and PDTB,
respectively). To informally test this, we tested
against explicitly cue-phrase marked examples gath-
ered from PDTB. That is, we used the M&E-style
method for mining instances, but we gathered them
from the PDTB corpus. Interestingly, we found that
(1) appears to be the case: for the Cause vs. Contrast
(68.7%), Cause vs. NoRel (73.0%) and (Contrast vs.
NoRel (71.0%) classifiers, the performance patterns
with the Auto test set rather than the results from the
PDTB Implicit test set. This bolsters the argument
that ?synthetic? implicit relations, i.e. those created
by stripping of originally present cue phrases, can-
not be treated as fully equivalent to ?organic? ones
annotated by a human judge but which are not ex-
plicitly indicated by a cue phrase. Sporleder and
Lascarides (To Appear) recently investigated this is-
sue in greater detail, and indeed found that such syn-
thetic and organic instances appear to have impor-
tant differences.
6 Using Topic Segmentation
In our experiments with topic segmentation, we aug-
mented the instance mining process to take account
of topic segment boundaries. The intuition here is
that all sentence boundaries should not be treated
equally during RSR instance mining. That is, we
would like to make our patterns recognize that some
sentence boundaries indicate merely an orthographic
break without a switch in topic, while others can
separate quite distinct topics. Sometimes the latter
type are marked by paragraph boundaries, but these
are unreliable markers since they may be used quite
differently by different authors.
Instead, we take the approach of adding topic seg-
ment boundary markers to our corpus, which we can
then integrate into our RSR extraction patterns. In
the case of NoRel, our assumption in our original
patterns is that the presence of at least three inter-
vening sentences is a sufficient heuristic for finding
spans which are not joined by one of the other RSRs;
we add the constraint that sentences in a NoRel re-
lation be in distinct topical segments, we can in-
crease model quality. Conversely, for two-sentence
Cause and Contrast instances, we add the constraint
that there must not be an intervening topic segment
boundary between the two sentences.
431
Before applying these segment-augmented pat-
terns, we must add boundary markers to our cor-
pus. While the concept of a topic segment can
be defined at various granularities, we take a goal-
oriented view and aim to identify segments with a
mean length of approximately four sentences, rea-
soning that these will be long enough to exclude
some candidate NoRel instances, yet short enough to
exclude a non-trivial number of Contrasts and Cause
instances. We use an automatic topic segmentation
tool, LCSeg (Galley et al, 2003) setting parame-
ters so that the derived segments are of the approx-
imate desired length. Using these parameters, LC-
Seg produces topic segments with a mean length of
3.51 sentences over Gigaword, as opposed to 1.54
sentences for paragraph boundaries. Using a sim-
ple metric that assumes ?correct? segment bound-
aries always occur at paragraph boundaries, LCSeg
achieves 76% precision.
We rerun the instance mining step of TextRels
over the segmented training corpus, after adding the
segment-based constraints mentioned above to our
pattern set. Although our constraints reduce the
overall number of instances available in the corpus,
we extract for training the same number of instances
per RSR as listed in Table 1 (our non-segment-
constrained training set does not use all instances
in the corpus). Using the optimal parameter set-
tings determined in the previous section, we build
our models and classifiers based on these segment-
constrained instances.
To evaluate the classifiers built on the segment-
constrained instances, we can essentially follow the
same protocol as in our Parameter Optimization ex-
periments. However, we must choose whether to
use a held-out test set taken from the segment-
constrained instances (?Auto-S?) or the same test
set as used to evaluate our parameter optimization,
i.e. the (?Auto?) test set from unsegmented training
data. We decide to test on both. On the one hand,
segmentation is done automatically, so it is realistic
that given a ?real world? document, we can compute
segment boundaries to help our classification judg-
ments. On the other hand, testing on unsegmented
input allows us to compare more directly to the num-
bers from our previous section. Further, for tasks
which would apply RSR models outside of a single-
document context (e.g., for assessing coherence of
a synthesized abstract), a test on unsegmented input
may be more relevant. Table 2 shows the results for
the ?Seg? classifiers on both Auto test sets, as well
as the PDTB test set.
We observe that the performance of the classi-
fiers is indeed impacted by training on the segment-
constrained instances. On the PDTB test data, per-
formance using the segment-trained classifiers im-
proves in two of three cases, with a mean improve-
ment of 1.2%. However, because of the small size
of this set, this margin is not statistically significant.
On the automatically-extracted test data, the
segment-trained classifier is the best performer in
all three cases when using the segmented test data;
while the margin is not statistically significant for a
single classifier, the overall accurate-inaccurate im-
provement is significant (p < .05) using a Chi-
squared test. On the unsegmented test data, the
segment-trained classifiers are best in two of three
cases, but the overall accurate-inaccurate improve-
ment does not achieve statistical significance. We
conclude tentatively that a classifier trained on ex-
amples gleaned with topic-segment-augmented pat-
terns performs more accurately than our baseline
classifier.
7 Using Syntax
Whether or not we use topic segmentation to con-
strain our training instances, our patterns rely on
sentence boundaries and cue phrase anchors to de-
marcate the extents of the text spans which form
our RSR instances. However, an instance which
matches such a pattern often contains some amount
of text which is not relevant to the relation in ques-
tion. Consider:
Example 3 Wall Street investors, citing a drop in
oil prices because weakness in the automotive
sector, sold off shares in GM today.
In this case, a syntactically informed analysis
could be used to extract the constituents in the cause-
effect relationship from within the boldfaced nomi-
nal clause only, i.e. as ?a drop in oil prices? and
?weakness in the automotive sector.? However, the
output of our instance mining process simply splits
the string around the cue phrase ?because of? and
extracts the entire first and second parts of the sen-
tence as the constituents. Of course, this may be for
432
the best; in this case there is an implicit Cause rela-
tionship between the NP headed by drop and the sold
VP which our pattern-based rules inadvertently cap-
ture; our experiments here test whether such noise is
more helpful than hurtful.
Recognizing the potential complexity of using
syntactic phenomena, we reduce the dimensions of
the problem. First, we focus on single-sentence in-
stances; this means we analyze only Cause and Con-
trast patterns, since NoRel uses only multi-sentence
patterns. Second, within the Cause and Contrast in-
stances, we narrow our investigation to the most pro-
ductive pattern of each type (in terms of training in-
stances extracted), given that different syntactic phe-
nomena may be in play for different patterns. The
two patterns we use are ?W1 because W2? for Cause
(accounts for 54% of training instances) and ?W1
, but W2? for Contrast (accounts for 41% of train-
ing instances). Lastly, we limit the size of our train-
ing set because of parsing time demands. We use
the Collins parser (Collins, 1996) to parse 400,000
instances each of Cause and Contrast for our fi-
nal results. Compared with our other models, this
is approximately 43% of our total Cause instances
and 13% of our total Contrast instances. For the
NoRel model, we use a randomly selected subset of
400,000 instances from our training set. For all rela-
tions, we use the non-segment-constrained instance
set as the source of these instances.
7.1 Analyzing and Classifying Syntactic Errors
To analyze the possible syntactic bases for the type
of over-capturing behavior shown in Example 3, we
create a small development set of 100 examples each
from Cause and Contrast training examples which fit
the criteria just mentioned. We then manually iden-
tify and categorize any instances of over-capturing,
labeling the relation-relevant and irrelevant spans.
We find that 75% of Cause and 58% of Contrast
examples contain at least some over-capturing; we
observe several common reasons for over-capturing
that we characterize syntactically. For example, a
matrix clause with a verb of saying should not be
part of the RSR. Using automatic parses of these in-
stances created by we then design syntactic filtering
heuristics based on a manual examination of parse
trees of several examples from our development set.
For Contrast, we find that using the coordinat-
ing conjunction (CC) analysis of but, we can use a
straightforward rule which limits the extent of RSR
spans captured to the conjuncts/children of the CC
node, e.g. by capturing only the boldfaced clauses
in the following example:
Example 4 For the past six months, management
has been revamping positioning and strategy, but
also scaling back operations.
This heuristic successfully cuts out the irrelevant
temporal relative clause, retaining the relevant VPs
which are being contrasted. Note that the heuris-
tic is not perfect; ideally the adverb also would be
filtered here, but this is more difficult to generalize
since contentful adverbials, e.g. strategically should
not be filtered out.
For the because pattern, we capture the right-
hand span as any text in child(ren) nodes of the be-
cause IN node. We extend the left-hand span only
as far as the first phrasal (e.g. VP) or finite clause
(e.g. SBAR) node above the because node. Analyz-
ing Example 3, the heuristic correctly captures the
right-hand span; however, to the left of because, the
heuristic cuts too much, and misses the key noun
drop.
7.2 Error Analysis: Evaluating the Heuristics
The first question we ask is, how well do our
heuristics work in identifying the actual correct
RSR extents? We evaluate this against the Penn
Discourse TreeBank (PDTB), restricting ourselves
to discourse-annotated but and because sentences
which match the RSR patterns which are the sub-
ject of our syntactic filtering. Since the PDTB
is annotated on the same corpus as Penn Tree-
Bank (PTB), we separately evaluate the perfor-
mance of our heuristics using gold-standard PTB
parses (?PDTB-Gold?) versus the trees generated by
Collins? parser (?PDTB-Prs?). We extract our test
data from the PDTB data corresponding to section
23 of PTB, i.e. the standard testing section, so that
the difference between the gold-standard and real
parse trees is meaningful. Section 23 contains 60
annotated instances of but and 52 instances of be-
cause which we can use for this purpose. We define
the measurement of accuracy here in terms of word-
level precision/recall. That is, the set of words fil-
tered by our heuristics are compared to the ?correct?
433
Heuristic PDTB-Prs PDTB-Gold
Contrast 89.6 / 73.0 / 80.5 79.0 / 80.6 / 79.8
Cause 78.5 / 78.8 / 78.6 87.3 / 79.5 / 83.2
Table 3: Precision/Recall/F-measure of syntactic
heuristics under various data sets and settings as de-
scribed in Section 7.2.
words to cut, i.e. those which the annotated RSR ex-
tents exclude. The results of this analysis are shown
in Table 3.
We performed an analysis of our heuristics on
Section 24 of the PDTB. In that section, there are 74
relevant sentences: 20 sentences with because, and
54 sentences with but. Exactly half of all sentences
(37) have no problems in the application of the
heuristics (7 because sentences, 30 but sentences).
Among the remaining sentences, the main source of
problems is that our heuristics do not always remove
matrix clauses with verbs of saying (15 cases total, 8
of which are because sentences). For the but clauses,
our heuristics removed the subject in 12 cases where
the PDTB did not do so. Additionally, the heuristic
for but sentences does not correctly identify the sec-
ond conjunct in five cases (choosing instead a paren-
thetical, for instance).
In looking at our syntactic heuristics for the
Cause relationship, we see that they indeed elimi-
nate the most frequent source of discrepancies with
the PDTB, namely the false inclusion of a matrix
clause of saying, resulting in 15 out of 20 perfect
analyses.
We also evaluate the difference in performance
between the PDTB-Gold and PDTB-Prs perfor-
mance to determine to what extent using a parser
(as opposed to the Gold Standard) degrades the per-
formance of our heuristics. We find that in Sec-
tion 24, 13 out of 74 sentences contain a parsing
error in the relevant aspects, but the effects are typ-
ically small and result from well-known parser is-
sues, mainly attachment errors. As we can see in Ta-
ble 3, the heuristic performance using an automatic
parser degrades only slightly, and as such we can ex-
pect an automatic parser to contribute to improving
RSR classification (as indeed it does).
Pdtb Test Set Auto Test Set
U Syn P U Syn P
Cau/Con 59.6 60.5 54.5 66.3 65.8 60.8
Cau/NR 72.2 74.9 52.6 70.3 70.2 57.3
Con/NR 61.6 60.2 52.2 69.4 69.8 56.8
Table 4: Classifier accuracy for the Unfiltered (U),
Syntactically Filtered (Syn), and POS (P) models
described in Section 7.3, over PDTB and Auto test
sets. Baseline in all cases is 50%.
7.3 Classification Evaluation
We evaluate the impact of our syntactic heuristics on
classification over the Auto and PDTB test sets using
the same instance set of 400,000 training instances
per relation. However, each applies different filters
to the instances I before computing the frequencies
F (all other parameters use the same values; these
are set slightly differently than the optimized val-
ues discussed earlier because of the smaller train-
ing sets). In addition to an Unfiltered baseline, we
evaluate Filtered models obtained with our syntac-
tic heuristics for Cause and Contrast. To provide an
additional point of comparison, we also evaluate the
Part-of-Speech based filtering heuristic described by
Marcu and Echihabi, which retains only nouns and
verbs. Unlike the other filters, the POS-based filter-
ing is applied to the NoRel instances as well as the
Cause and Contrast instances. Table 4 summarizes
the results of the classifying the PDTB and Auto test
sets with these different models.
Before we examine the results, we note that the
syntactic heuristic cuts a large portion of training
data out. In terms of the total sum of frequencies in
Fcause, i.e. the word pairs extracted from all cause
instances, the syntactic filtering cuts out nearly half.
With this in mind, we see that while the syntac-
tic filtering achieves slightly lower mean accuracy as
compared to the Unfiltered baseline on the Auto test
set, the pairs it does keep appear to be used more ef-
ficiently (the differences are significant). Even with
this reduced training set, the syntactic heuristic im-
proves performance in two out of three cases on the
PDTB test set, including a 2.7 percent improvement
for the Cause vs NoRel classifier. However, due to
the small size of the PDTB test set, none of these
differences is statistically significant.
We posit that bias in the Auto set may cause this
434
difference in performance across training sets; spans
in the Auto set are not true arguments of the rela-
tion in the PDTB sense, but nonetheless occur reg-
ularly with the cue phrases used in instance mining
and thus are more likely to be present in the test set.
Lastly, we observe that the POS-based filtering
described by M&E performs uniformly poorly. We
have no explanation for this at present, given that
M&E?s results with this filter appear promising.
8 Conclusion
In this paper, we analyzed the problem of learning a
model of rhetorical-semantic relations. Building on
the work of Marcu and Echihabi, we first optimized
several parameters of their model, which we found
to have significant impact on classification accuracy.
We then focused on the quality of the automatically-
mined training examples, analyzing two techniques
for data filtering. The first technique, based on au-
tomatic topic segmentation, added additional con-
straints on the instance mining patterns; the sec-
ond used syntactic heuristics to cut out irrelevant
portions of extracted training examples. While the
topic-segmentation filtering approach achieves sig-
nificant improvement and the best results overall,
our analysis of the syntactic filtering approach indi-
cates that refined heuristics and a larger set of parsed
data can further improve those results. We would
also like to experiment with combining the two ap-
proaches, i.e. by applying the syntactic heuristics
to an instance set extracted using topic segmenta-
tion constraints. We conclude that our experiments
show that these techniques can successfully refine
RSR models and improve our ability to classify un-
known relations.
References
Jason Baldridge and Alex Lascarides. 2005. Probabilistic head-
driven parsing for discourse structure. In CoNLL 2005.
L. Carlson, D. Marcu, and M.E. Okurowski. 2001. Building a
discourse-tagged corpus in the framework of rhetorical struc-
ture theory. In Eurospeech 2001 Workshops.
M. Collins. 1996. A new statistical parser based on bigram
lexical dependencies. In ACL 1996.
M. Galley, K.R. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conversation.
In ACL 2003.
R. Girju. 2003. Automatic detection of causal relations for
question answering. In ACL 2003 Workshops.
Jerry R. Hobbs. 1979. Coherence and coreference. Cognitive
Science, 3(1):67?90.
E. Hovy and E. Maier. 1993. Parsimonious or profligate: How
many and which discourse structure relations? Unpublished
Manuscript.
A. Knott and T. Sanders. 1998. The classification of coherence
relations and their linguistic markers: An exploration of two
languages. Journal of Pragmatics, 30(2):135?175.
M. Lapata and A. Lascarides. 2004. Inferring sentence-internal
temporal relations. In HLT 2004.
W.C. Mann and S.A. Thompson. 1988. Rhetorical structure
theory: Towards a functional theory of text organization.
Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised approach to
recognizing discourse relations. In ACL 2002.
D. Marcu. 1997. The Rhetorical Parsing, Summarization and
Generation of Natural Language Texts. Ph.D. thesis, Uni-
versity of Toronto, Department of Computer Science.
J. Martin. 1992. English Text: System and Structure. John
Benjamins.
K.R. McKeown. 1985. Text generation: Using discourse
strategies and focus constraints to generate natural lan-
guage text. Cambridge University Press.
J.D. Moore and P. Wiemer-Hastings. 2003. Discourse in
computational linguistics and artificial intelligence. In
M.A. Gernbacher A.G. Graesser and S.R. Goldman, ed-
itors, Handbook of Discourse Processes, pages 439?487.
Lawrence Erlbaum Associates.
M.G. Moser and J.D. Moore. 1996. Toward a synthesis of two
accounts of discourse structure. Computational Linguistics,
22(3):409?420.
R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi, and
B. Webber. 2006. The penn discourse treebank 1.0. anno-
tation manual. Technical Report IRCS-06-01, University of
Pennsylvania.
R. Soricut and D. Marcu. 2003. Sentence level discourse pars-
ing using syntactic and lexical information. In HLT-NAACL
2003.
C. Sporleder and A. Lascarides. 2005. Exploiting linguistic
cues to classify rhetorical relations. In RANLP 2005.
C. Sporleder and A. Lascarides. To Appear. Using automat-
ically labelled examples to classify rhetorical relations: An
assessment. Natural Language Engineering.
B. Wellner, J. Pustejovsky, C. Havasi, R. Sauri, and
A. Rumshisky. 2006. Classification of discourse coherence
relations: An exploratory study using multiple knowledge
sources. In SIGDial 2006.
F. Wolf and E. Gibson. 2005. Representing discourse coher-
ence: A corpus-based analysis. Computational Linguistics,
31(2):249?287.
435
Proceedings of NAACL HLT 2007, pages 532?539,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Question Answering using Integrated Information Retrieval and
Information Extraction
Barry Schiffman and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, NY 10027
bschiff,kathy@cs.columbia.edu
Ralph Grishman
Department of Computer Science
New York University
New York, NY 10003
grishman@cs.nyu.edu
James Allan
University of Massachusetts
Department of Computer Science
Amherst, MA 01003
allan@cs.umass.edu
Abstract
This paper addresses the task of provid-
ing extended responses to questions re-
garding specialized topics. This task is an
amalgam of information retrieval, topical
summarization, and Information Extrac-
tion (IE). We present an approach which
draws on methods from each of these ar-
eas, and compare the effectiveness of this
approach with a query-focused summa-
rization approach. The two systems are
evaluated in the context of the prosecution
queries like those in the DARPA GALE
distillation evaluation.
1 Introduction
As question-answering systems advance from han-
dling factoid questions to more complex requests,
they must be able to determine how much informa-
tion to include while making sure that the informa-
tion selected is indeed relevant. Unlike factoid ques-
tions, there is no clear criterion that defines the kind
of phrase that answers the question; instead, there
may be many phrases that could make up an answer
and it is often unclear in advance, how many. As
system developers, our goal is to yield high recall
without sacrificing precision.
In response to questions about particular events of
interest that can be enumerated in advance, it is pos-
sible to perform a deeper semantic analysis focusing
on the entities, relations, and sub-events of interest.
On the other hand, the deeper analysis may be error-
ful and will also not always provide complete cov-
erage of the information relevant to the query. The
challenge, therefore, is to blend a shallower, robust
approach with the deeper approach in an effective
way.
In this paper, we show how this can be achieved
through a synergistic combination of information re-
trieval and information extraction. We interleave in-
formation retrieval (IR) and response generation, us-
ing IR in high precision mode in the first stage to
return a small number of documents that are highly
likely to be relevant. Information extraction of enti-
ties and events within these documents is then used
to pinpoint highly relevant sentences and associated
words are selected to revise the query for a sec-
ond pass of retrieval, improving recall. As part of
this process, we approximate the relevant context by
measuring the proximity of the target name in the
query and extracted events.
Our approach has been evaluated in the frame-
work of the DARPA GALE1 program. One of the
GALE evaluations involves responding to questions
based on a set of question templates, ranging from
broad questions like ?Provide information on X?,
where X is an organization, to questions focused on
particular classes of events. For the experiments pre-
sented here, we used the GALE program?s prosecu-
tion class of questions. These are given in the fol-
lowing form: ?Describe the prosecution of X for Y,?
where X is a person and Y is a crime or charge. Our
results show that we are able to achieve higher accu-
1Global Autonomous Language Exploitation
532
racy with a system that exploits the justice events
identified by IE than with an approach based on
query-focused summarization alone.
In the following sections, we first describe the
task and then review related work in question-
answering. Section 3 details our procedure for find-
ing answers as well as performing the information
retrieval and information extraction tasks. Section 4
compares the results of the two approaches. Finally,
we present our conclusion and plans for future work.
1.1 The Task
The language of the question immediately raises the
question of what is meant by prosecution. Unlike a
question such as ?When was X born??, which is ex-
pected to be answered by a clear, concrete phrase,
the prosecution question asks for a much greater
range of material. The answer is in no way limited
to the statements and activities of the prosecuting at-
torney, although these would certainly be part of a
comprehensive answer.
In the GALE relevance guidelines2 , the answer
can include many facets of the case:
? Descriptions of the accused?s involvement in
the crime.
? Descriptions of the activities, motivations, and
involvement in the crime.
? Descriptions of the person as long as they are
related to the trial.
? Information about the defense of the suspect.
? Information about the sentencing of the person.
? Information about similar cases involving the
person.
? Information about the arrest of the person and
statements made by him or her.
? Reactions of people involved in the trial, as
well as statements by officials or reactions by
the general public.
2BAE Systems Advanced Information Technologies, ?Rele-
vance Guidelines for Distillation Evaluation for GALE: Global
Autonomous Language Exploitation?, Version 2.2, January 25,
2007
The guidelines also provide a catchall instruction
to ?include reported information believed to be rele-
vant to the case, but deemed inadmissible in a court
of law.?
It is easy to see that the use of a few search terms
alone will be insufficient to locate a comprehensive
answer.
We took a broad view of the question type and
consider that any information about the investiga-
tion, accusation, pursuit, capture, trial and punish-
ment of the individual, whether a person or organi-
zation, would be desireable in the answer.
1.2 Overview
The first step in our procedure sends a query tai-
lored to this question type to the IR system to ob-
tain a small number of high-quality documents with
which we can determine what name variations are
used in the corpus and estimate how many docu-
ments contain references to the individual. In the
future we will expand the type of information we
want to glean from this small set of documents. A
secondary search is issued to find additional docu-
ments that refer to the individual, or individuals.
Once we have the complete document retrieval,
the foundation for finding these types of events
lies in the Proteus information extraction compo-
nent (Grishman et al, 2005). We employ an IE sys-
tem trained for the tasks of the 2005 Automatic Con-
tent Extraction evaluation, which include entity and
event extraction. ACE defines a number of general
event types, including justice events, which cover in-
dictments, accusations, arrests, trials, and sentenc-
ings. The union of all these specific categories gives
us many of the salient events in a criminal justice
case from beginning to end. The program uses the
events, as well as the entities, to help identify the
passages that respond to the question.
The selection of sentences is based on the as-
sumption that the co-occurrence of the target indi-
vidual and a judicial event indicates that the target
is indeed involved in the event, but these two do not
necesssarily occur in the same sentence.
2 Related Work
A large body of work in question-answering has fol-
lowed from the opening of the Text Retrieval Con-
533
ference?s Q&A track in 1999. The task started as a
group of factoid questions and expanded from there
into more sophisticated problems. TREC provides
a unique testbed of question-answer pairs for re-
searchers and this data has been influential in fur-
thering progress.
In TREC 2006, there was a new secondary task
called ?complex, interactive Question Answering,?
(Dang et al, 2006) which is quite close to the GALE
problem, though it incorporated interaction to im-
prove results. Questions are posed in a canonical
form plus a narrative elaborating on the kind of in-
formation requested. An example question (from the
TREC guidelines) asks, ?What evidence is there for
transport of [drugs] from [Bonaire] to the [United
States]?? Our task is most similar to the fully-
automatic baseline runs of the track, which typically
took the form of passage retrieval with query ex-
pansion (Oard et al, 2006) or synonym processing
(Katz et al, 2006), and not the deeper processing
employed in this work.
Within the broader QA task, the other question
type is closest to the requirements in GALE, but it
is too open ended. In TREC, the input for other
questions is the name or description of the target,
and the response is supposed to be all information
that did not fit in the answers to the previous ques-
tions. While a few GALE questions have similar in-
put, most, including the prosecution questions, pro-
vide more detail about the topic in question.
A number of systems have used techniques in-
spired by information extraction. One of the top sys-
tems in the other questions category at the 2004 and
2005 evaluations generated lexical-syntactic pat-
terns and semantic patterns (Schone et al, 2004).
But they build these patterns from the question. In
our task, we took advantage of the structured ques-
tion format to make use of extensive work on the
semantics of selected domains. In this way we
hope to determine whether we can obtain better per-
formance by adding more sophisticated knowledge
about these domains. The Language Computer Cor-
poration (LCC) has long experimented with incorpo-
rating information extraction techniques. Recently,
in its system for the other type questions at TREC
2005, LCC developed search patterns for 33 target
classes (Harabagiu et al, 2005). These patterns were
learned with features from WordNet, stemming and
named entity recognition.
More and more systems are exploiting the size
and redundancy of the Web to help find answers.
Some obtain answers from the Web and then
project the answer back to the test corpus to find
a supporting document (Voorhees and Dang, 2005).
LCC used ?web boosting features? to add to key
words (Harabagiu et al, 2005). Rather than go to
the Web and enhance the question terms, we made
a beginning at examining the corpus for specific bits
of information, in this prototype, to determine alter-
native realizations of names.
3 Implementation
As stated above, the system takes a query in the
XML format required by the GALE program. The
query templates allow users to amplify their requests
by specifying a timeframe for the information and/or
a locale. In addition, there are provisions for en-
tering synonyms or alternate terms for either of the
main arguments, i.e. the accused and the crime, and
for related but less important terms.
Since this system is a prototype written especially
for the GALE evaluation in July 2006, we paid close
attention to the way example questions were given,
as well as to the evaluation corpus, which consisted
of more than 600,000 short news articles. The goal
in GALE was to offer comprehensive results to the
user, providing all snippets, or segments of texts,
that responded to the information request. This re-
quired us to develop a strategy that balanced pre-
cision against recall. A system that reported only
high-confidence answers was in danger of having no
answers or far fewer answers than other systems,
while a system that allowed lower confidence an-
swers risked producing answers with a great deal of
irrelevant material. Another way to look at this bal-
ancing act was that it was necessary for a system to
know when to quit. For this reason, we sought to
obtain a good estimate of the number of documents
we wanted to scan for answers.
Answer selection focused first on the name of the
suspect, which was always given in the query tem-
plate. In many of the training cases, the suspect was
in the news only because of a criminal charge against
him; and in most, the charge specified was the only
accusation reported in the news. Both location and
534
date constraints seemed to be largely superfluous,
and so we ignored these. But we did have a mecha-
nism for obtaining supplementary answers keyed to
the brief description of the crime and other related
words
The first step in the process is to request a seed
collection of 10 documents from the IR system.
This number was established experimentally. The
IR query combines terms tailored to the prosecution
template and the specific template parameters for a
particular question. The 10 documents returned are
then examined to produce a list of name variations
that substantially match the name as rendered in the
query template. The IR system is then asked for the
number of times that the name appears in the cor-
pus. This figure is adjusted by the frequency per
document in the seed collection and a new query is
submitted, set to obtain the N documents in which
we expect to find the target?s name.
3.1 Information Retrieval
The goal of the information retrieval component of
the system was to locate relevant documents that the
summarization system could then use to construct an
answer. All search, whether high-precision or high-
recall, was performed using the Indri retrieval sys-
tem 3 (Strohman et al, 2005).
Indri provides a powerful query language that
is used here to combine numerous aspects of the
query. The Indri query regarding Saddam Hus-
sein?s prosecution for crimes against humanity in-
cludes the following components: source restric-
tions, prosecution-related words, mentions of Sad-
dam Hussein, justice events, dependence model
phrases (Metzler and Croft, 2005) regarding the
crime, and a location constraint.
The first part of the query located references to
prosecutions by looking for the keywords prosecu-
tion, defense, trial, sentence, crime, guilty, or ac-
cuse, all of which were determined on training data
to occur in descriptions of prosecutions. These
words were important to have in documents for them
to be considered relevant, but the individual?s name
and the description of the crime were far more im-
portant (by a factor of almost 19 to 1).
The more heavily weighted part of the query,
3http://lemurproject.org/indri
then, was a ?justice event? marker found using in-
formation extraction (Section 3.2) and the more de-
tailed description of that event based on phrases ex-
tracted from the crime (here crimes against human-
ity). Those phrases give more probability of rele-
vance to documents that use more terms from the
crime. It also included a location constraint (here,
Iraq) that boosted documents referring to that lo-
cation. And it captured user-provided equivalent
words such as Saddam Hussein being a synonym for
former President of Iraq.
The most complex part of the query handled ref-
erences to the individual. The extraction system had
annotated all person names throughout the corpus.
We used the IR system to index all names across
all documents and used Indri to retrieve any name
forms that matched the individual. As a result, we
were able to find references to Saddam, Hussein,
and so on. This task could have also been accom-
plished with cross-document coreference technol-
ogy but our approach appeared to compensate for
incorrectly translated names slightly better than the
coreference system we had available at the time. For
example, Present rust Hussein was one odd form
that was matched by our simple approach.The final query looked like the following:
#filreq( #syn( #1(AFA).source ... #1(XIE).source )
#weight(
0.05 #combine( prosecution defense trial sentence
crime guilty accuse )
0.95 #combine(
#any:justice
#weight(1.0 #combine(humanity against crimes)
1.0 #combine(
#1(against humanity)
#1(crimes against)
#1(crimes against humanity))
1.0 #combine
#uw8(against humanity)
#uw8(crimes humanity)
#uw8(crimes against)
#uw12(crimes against humanity)))
Iraq
#syn( #1(saddam hussein)
#1(former president iraq))
#syn( #equals( entity 126180 ) ...))))
The actual query is much longer because it con-
tains 100 possible entities and numerous sources.
The processing is described in more detail else-
where (Kumaran and Allan, 2007).
3.2 Information Extraction
The Proteus system produces the full range of anno-
tations as specified for the ACE 2005 evaluation, in-
cluding entities, values, time expressions, relations,
535
and events. We focus here on the two annotations,
entities and events, most relevant to our question-
answering task. The general performance on entity
and event detection in news articles is within a few
percentage points of the top-ranking systems from
the evaluation.
The extraction engine identifies seven semantic
classes of entities mentioned in a document, of
which the most frequent are persons, organizations,
and GPE?s (geo-political entities ? roughly, regions
with a government). Each entity will have one or
more mentions in the document; these mentions in-
clude names, nouns and noun phrases, and pro-
nouns. Text processing begins with an HMM-based
named entity tagger, which identifies and classifies
the names in the document. Nominal and pronomi-
nal mentions are identified either with a chunker or
a full Penn-Treebank parser. A rule-based coref-
erence component identifies coreference relations,
forming entities from the mentions. Finally, a se-
mantic classifier assigns a class to each entity based
on the type of the first named mention (if the entity
includes a named mention) or the head of the first
nominal mention (using statistics gathered from the
ACE training corpus).
The ACE annotation guidelines specify 33 differ-
ent event subtypes, organized into 8 major types.
One of the major types is justice events, which in-
clude arrest, charge, trial, appeal, acquit, convict,
sentence, fine, execute, release, pardon, sue, and ex-
tradite subtypes. In parallel to entities, the event
tagger first identifies individual event mentions and
then uses event coreference to form events. For the
ACE evaluation, an annotated corpus of approxi-
mately 300,000 words is used to train the event tag-
ger.
For each event mention in the corpus, we collect
the trigger word (the main word indicating the event)
and a pattern recording the path from the trigger
to each event argument. These paths are recorded
in two forms: as the sequence of heads of maxi-
mal constituents between the trigger and the argu-
ment, and as the sequence of predicate-argument re-
lations connecting the trigger to the argument4 . In
4These predicate argument relations are based on a repre-
sentation called GLARF (Grammatical-Logical Argument Rep-
resentation Framework), which incorporates deep syntactic re-
lations and the argument roles from PropBank and NomBank.
addition, a set of maximum-entropy classifiers are
trained: to distinguish events from non-events, to
classify events by type and subtype, to distinguish
arguments from non-arguments, and to classify ar-
guments by argument role. In tagging new data, we
first match the context of each instance of a trig-
ger word against the collected patterns, thus iden-
tifying some arguments. The argument classifier is
then used to collect additional arguments within the
sentence. Finally, the event classifier (which uses
the proposed arguments as features) is used to re-
ject unlikely events. The patterns provide somewhat
more precise matching, while the argument classi-
fiers improve recall, yielding a tagger with better
performance than either strategy separately.
3.3 Answer Generation
Once the final batch of documents is received,
the answer generator module selects candidate pas-
sages. The names, with alternate renderings, are lo-
cated through the entity mentions by the IE system.
All sentences that contain a justice event and that
fall within a mention of a target by no more than
n sentences, where n is a settable parameter, which
was put at 5 for this evaluation, form the core of the
system?s answer.
The tactic takes the place of topic segmentation,
which we used for other question types in GALE
that did not have the benefit of the sophisticated
event recognition offered by the IE system. Segmen-
tation is used to give users sufficient context in the
answer without needing a means of identifying dif-
ficult definite nominal resolution cases that are not
handled by extraction.
In order to increase recall, in keeping with the
need for a comprehensive answer in the GALE eval-
uation, we added sentences that contain the name of
the target in documents that have justice events and
sentences that contain words describing the crime.
However, we imposed a limitation on the growth of
the answer size. When the target individual is well-
known, he or she will be mentioned in numerous
contexts, reducing the likelihood that this additional
mention will be relevant. Thus, when the size of the
answer grew too rapidly, we stopped including these
additional sentences, and produced sentences only
from the justice events. The threshold for triggering
this shift was 200 sentences.
536
3.4 Summarization
As a state-of-the-art baseline, we used a generic
multidocument summarization system that has been
tested in numerous contexts. It is, indeed, the
backup answer generator for several question types,
including the prosecution questions, in our GALE
system, and has been been tested in the topic-based
tasks of the 2005 and 2006 Document Understand-
ing Conferences.
A topic statement is formed by collapsing the
template arguments into one list, e.g., ?saddam hus-
sein crimes against humanity prosecution?, and the
answer generation module proceeds by using a hy-
brid approach that combines top-down strategies
based on syntactic patterns, alongside a suite of
summarization methods which guide content in a
bottom-up manner that clusters and combines the
candidate sentences (Blair-Goldensohn and McKe-
own, 2006).
4 Evaluation
The results of our evaluation are shown in Table 1.
We increased the number of test questions over the
number used in the official GALE evaluation and we
used only previously unseen questions. Documents
for the baseline system were selected without use of
the event annotations from Proteus.
We paired the 25 questions for judges, so that both
the system?s answer and the baseline answer were
assigned to the same person. We provided explicit
instructions on the handling on implicit references,
allowing the judges to use the context of the ques-
tion and other answer sentences to determine if a
sentence was relevant ? following the practice of the
GALE evaluation.
Our judges were randomly assigned questions
and asked whether the snippets, which in our case
were individual sentences, were relevant or not;
they could respond Relevant, Not Relevant or Don?t
Know. In cases where references were unclear, the
judges were asked to choose Don?t Know and these
were removed from the scoring.5
5In the GALE evaluation, the snippets are broken down by
hand into nuggets ? discrete pieces of information ? and the
answers are scored on that basis. However, we scored our re-
sponses on the basis of snippets (sentences) only, as it is much
more efficient, and therefore more feasible to repeat in the fu-
ture.
Our system using IE event detection and en-
tity tracking outperformed the summarization-based
baseline, with average precision of 68% compared
with 57%. Moreover, the specialized system sus-
tained that level of precision although it returned a
much larger number of snippets, totaling 2,086 over
the 25 questions, compared with 363 for the base-
line system. We computed a relative recall score, us-
ing the union of the sentences found by the systems
and judged relevant as the ground truth. For recall,
the specialized system scored an average 89% ver-
sus 17% for the baseline system. Computing an F-
measure weighting precision and recall equally, the
specialized system outperformed the baseline sys-
tem 75% to 23%. The difference in relative recall
and F-measure are both statisticaly significant under
a two-tailed, paired t-test, with p < 0.001.
5 Conclusion and Future Work
Our results show that the specialized system statis-
tically outperforms the baseline, a well-tested query
focused summarization approach, on precision. The
specialized system produced a much larger answer
on average (Table 1). Moreover, our answer gener-
ator seemed to adapt well to information in the cor-
pus. Of the six cases where it returned fewer than
10 sentences, the baseline found no additional sen-
tences four times (Questions B006, B011, B015 and
B022). We regard this as an important property in
the question-answering task.
A major challenge is to ascertain whether the
mention of the target is indeed involved in the rec-
ognized justice event. Our event recognition system
was developed within the ACE program and only
seeks to assigns roles within the local context of a
single sentence. We currently use a threshold to con-
sider whether an entity mention is reliable, but we
will experiment with ways to measure the likelihood
that a particular sentence is about the prosecution or
some other issue. We are planning to obtain vari-
ous pieces of information from additional secondary
queries to the search engine. Within the GALE pro-
gram, we are limited to the defined corpus, but in the
general case, we could add more varied resources.
In addition, we are working to produce answers
using text generation, to bring more sophisticated
summarization techniques to make a better presen-
537
QID System with IE Baseline System
Precision Recall F-meas Count Precision Recall F-meas Count
B001 0.728 0.905 0.807 92 0.818 0.122 0.212 11
B002 0.713 0.906 0.798 108 0.889 0.188 0.311 18
B003 0.770 0.942 0.848 148 0.875 0.058 0.109 8
B004 0.930 0.879 0.904 86 1.000 0.154 0.267 14
B005 0.706 0.923 0.800 34 0.400 0.231 0.293 15
B006 1.000 1.000 1.000 3 0.000 0.000 0.000 17
B007 0.507 1.000 0.673 73 0.421 0.216 0.286 19
B008 0.791 0.909 0.846 201 0.889 0.091 0.166 18
B009 0.759 0.960 0.848 158 0.941 0.128 0.225 17
B010 1.000 0.828 0.906 24 0.500 0.276 0.356 16
B011 0.500 1.000 0.667 6 0.000 0.000 0.000 18
B012 0.338 0.714 0.459 74 0.765 0.371 0.500 17
B013 0.375 0.900 0.529 120 0.700 0.280 0.400 20
B014 0.571 0.800 0.667 7 0.062 0.200 0.095 16
B015 0.500 1.000 0.667 2 0.000 0.000 0.000 10
B016 1.000 0.500 0.667 5 0.375 0.600 0.462 16
B017 1.000 1.000 1.000 13 0.125 0.077 0.095 7
B018 0.724 0.993 0.837 199 0.875 0.048 0.092 8
B019 0.617 0.954 0.749 201 0.684 0.100 0.174 19
B020 0.923 0.727 0.814 26 0.800 0.364 0.500 15
B021 0.562 0.968 0.711 162 0.818 0.096 0.171 11
B022 0.667 1.000 0.800 6 0.000 0.000 0.000 18
B023 0.684 0.950 0.795 196 0.778 0.050 0.093 9
B024 0.117 0.636 0.197 60 0.714 0.455 0.556 7
B025 0.610 0.943 0.741 82 0.722 0.245 0.366 18
Aver 0.684 0.893 0.749 83 0.566 0.174 0.229 14
Table 1: The table compares results of our answer generator combining the Indri and the Proteus ACE sys-
tem, against the focused-summarization baseline. This experiment is over 25 previously unseen questions.
The differences between the two systems are statistically significant (p < 0.001) for recall and f-measure by
a two-tailed, paired t-test. A big difference between the two systems is that the answer generator produces
a total of 2,086 answer sentences while sustaining an average precision of 0.684. In only three cases, does
the precision fall below 0.5. In contrast, the baseline system produced only 362, one-sixth the number of
answer sentences. While its average precision was not significantly worse than the answer-generator?s, its
precision varied widely, failing to find any correct sentences four times.
538
tation than an unordered list of sentences.
Finally, we will look into applying the techniques
used here on other topics. The first test would rea-
sonably be Conflict events, for which the ACE pro-
gram has training data. But ultimately, we would
like to adapt our system to arbitrary topic areas.
Acknowledgements
This material is based in part upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).
References
Sasha Blair-Goldensohn and Kathleen McKeown. 2006.Integrating rhetorical-semantic relation models forquery-focused summarization. In Proceedings of 6th
Document Understanding Conference (DUC2006).
Hoa Trang Dang, Jimmy Lin, and Diane Kelly. 2006.Overview of the TREC 2006 question answering track.In Proceedings TREC. Forthcoming.
Ralph Grishman, David Westbrook, and Adam Meyers.2005. NYU?s english ACE 2005 system descrip-tion. In ACE 05 Evaluation Workshop. On-line athttp://nlp.cs.nyu.edu/publication.
Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, Andrew Hickl, and Patrick Wang.2005. Employing two question answering systems in
TREC 2005. In Proceedings of the Fourteenth Text
Retrieval Conference.
B. Katz, G. Marton, G. Borchardt, A. Brownell,S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, O. Uzuner, and A. Wilcox.2006. External knowledge sources for question an-swering. In Proceedings of TREC. On-line at
http://www.trec.nist.gov.
Giridhar Kumaran and James Allan. 2007. Informationretrieval techniques for templated queries. In Proceed-
ings of RIAO. Forthcoming.
D. Metzler and W.B. Croft. 2005. A Markov random
field model for term dependencies. In Proceedings of
ACM SIGIR, pages 472?479.
D. Oard, T. Elsayed, J. Wang, Y. Wu, P. Zhang, E. Abels,
J. Lin, and D. Soergel. 2006. Trec 2006 at maryland:Blog, enterprise, legal and QA tracks. In Proceedings
of TREC. On-line at http://www.trec.nist.gov.
Patrick Schone, Gary Ciany, Paul McNamee, James
Mayeld, Tina Bassi, and Anita Kulman. 2004. Ques-tion answering with QACTIS at TREC-2004. In Pro-
ceedings of the Thirteenth Text Retrieval Conference.
T. Strohman, D. Metzler, H. Turtle, and W.B. Croft.
2005. Indri: A language-model based search enginefor complex queries (extended version). Technical Re-port IR-407, CIIR, UMass Amherst.
Ellen M. Voorhees and Hoa Trang Dang. 2005.
Overview of the TREC 2005 question answering track.In Proceedings of the Fourteenth Text Retrieval Con-
ference.
539
Extracting Paraphrases from a Parallel Corpus
Regina Barzilay and Kathleen R. McKeown
Computer Science Department
Columbia University
10027, New York, NY, USA
 
regina,kathy  @cs.columbia.edu
Abstract
While paraphrasing is critical both for
interpretation and generation of natu-
ral language, current systems use man-
ual or semi-automatic methods to col-
lect paraphrases. We present an un-
supervised learning algorithm for iden-
tification of paraphrases from a cor-
pus of multiple English translations of
the same source text. Our approach
yields phrasal and single word lexical
paraphrases as well as syntactic para-
phrases.
1 Introduction
Paraphrases are alternative ways to convey the
same information. A method for the automatic
acquisition of paraphrases has both practical and
linguistic interest. From a practical point of view,
diversity in expression presents a major challenge
for many NLP applications. In multidocument
summarization, identification of paraphrasing is
required to find repetitive information in the in-
put documents. In generation, paraphrasing is
employed to create more varied and fluent text.
Most current applications use manually collected
paraphrases tailored to a specific application, or
utilize existing lexical resources such as Word-
Net (Miller et al, 1990) to identify paraphrases.
However, the process of manually collecting para-
phrases is time consuming, and moreover, the col-
lection is not reusable in other applications. Ex-
isting resources only include lexical paraphrases;
they do not include phrasal or syntactically based
paraphrases.
From a linguistic point of view, questions
concern the operative definition of paraphrases:
what types of lexical relations and syntactic
mechanisms can produce paraphrases? Many
linguists (Halliday, 1985; de Beaugrande and
Dressler, 1981) agree that paraphrases retain ?ap-
proximate conceptual equivalence?, and are not
limited only to synonymy relations. But the ex-
tent of interchangeability between phrases which
form paraphrases is an open question (Dras,
1999). A corpus-based approach can provide in-
sights on this question by revealing paraphrases
that people use.
This paper presents a corpus-based method for
automatic extraction of paraphrases. We use a
large collection of multiple parallel English trans-
lations of novels1. This corpus provides many
instances of paraphrasing, because translations
preserve the meaning of the original source, but
may use different words to convey the mean-
ing. An example of parallel translations is shown
in Figure 1. It contains two pairs of para-
phrases: (?burst into tears?, ?cried?) and (?com-
fort?, ?console?).
Emma burst into tears and he tried to comfort her, say-
ing things to make her smile.
Emma cried, and he tried to console her, adorning his
words with puns.
Figure 1: Two English translations of the French
sentence from Flaubert?s ?Madame Bovary?
Our method for paraphrase extraction builds
upon methodology developed in Machine Trans-
lation (MT). In MT, pairs of translated sentences
from a bilingual corpus are aligned, and occur-
rence patterns of words in two languages in the
text are extracted and matched using correlation
measures. However, our parallel corpus is far
from the clean parallel corpora used in MT. The
1Foreign sources are not used in our experiment.
rendition of a literary text into another language
not only includes the translation, but also restruc-
turing of the translation to fit the appropriate lit-
erary style. This process introduces differences
in the translations which are an intrinsic part of
the creative process. This results in greater dif-
ferences across translations than the differences
in typical MT parallel corpora, such as the Cana-
dian Hansards. We will return to this point later
in Section 3.
Based on the specifics of our corpus, we de-
veloped an unsupervised learning algorithm for
paraphrase extraction. During the preprocessing
stage, the corresponding sentences are aligned.
We base our method for paraphrasing extraction
on the assumption that phrases in aligned sen-
tences which appear in similar contexts are para-
phrases. To automatically infer which contexts
are good predictors of paraphrases, contexts sur-
rounding identical words in aligned sentences are
extracted and filtered according to their predic-
tive power. Then, these contexts are used to ex-
tract new paraphrases. In addition to learning lex-
ical paraphrases, the method also learns syntactic
paraphrases, by generalizing syntactic patterns of
the extracted paraphrases. Extracted paraphrases
are then applied to the corpus, and used to learn
new context rules. This iterative algorithm con-
tinues until no new paraphrases are discovered.
A novel feature of our approach is the ability to
extract multiple kinds of paraphrases:
Identification of lexical paraphrases. In con-
trast to earlier work on similarity, our approach
allows identification of multi-word paraphrases,
in addition to single words, a challenging issue
for corpus-based techniques.
Extraction of morpho-syntactic paraphrasing
rules. Our approach yields a set of paraphras-
ing patterns by extrapolating the syntactic and
morphological structure of extracted paraphrases.
This process relies on morphological information
and a part-of-speech tagging. Many of the rules
identified by the algorithm match those that have
been described as productive paraphrases in the
linguistic literature.
In the following sections, we provide an
overview of existing work on paraphrasing, then
we describe data used in this work, and detail our
paraphrase extraction technique. We present re-
sults of our evaluation, and conclude with a dis-
cussion of our results.
2 Related Work on Paraphrasing
Many NLP applications are required to deal with
the unlimited variety of human language in ex-
pressing the same information. So far, three
major approaches of collecting paraphrases have
emerged: manual collection, utilization of exist-
ing lexical resources and corpus-based extraction
of similar words.
Manual collection of paraphrases is usually
used in generation (Iordanskaja et al, 1991;
Robin, 1994). Paraphrasing is an inevitable part
of any generation task, because a semantic con-
cept can be realized in many different ways.
Knowledge of possible concept verbalizations can
help to generate a text which best fits existing syn-
tactic and pragmatic constraints. Traditionally, al-
ternative verbalizations are derived from a man-
ual corpus analysis, and are, therefore, applica-
tion specific.
The second approach ? utilization of existing
lexical resources, such as WordNet ? overcomes
the scalability problem associated with an appli-
cation specific collection of paraphrases. Lexical
resources are used in statistical generation, sum-
marization and question-answering. The ques-
tion here is what type of WordNet relations can
be considered as paraphrases. In some appli-
cations, only synonyms are considered as para-
phrases (Langkilde and Knight, 1998); in others,
looser definitions are used (Barzilay and Elhadad,
1997). These definitions are valid in the context
of particular applications; however, in general, the
correspondence between paraphrasing and types
of lexical relations is not clear. The same ques-
tion arises with automatically constructed the-
sauri (Pereira et al, 1993; Lin, 1998). While
the extracted pairs are indeed similar, they are not
paraphrases. For example, while ?dog? and ?cat?
are recognized as the most similar concepts by
the method described in (Lin, 1998), it is hard
to imagine a context in which these words would
be interchangeable.
The first attempt to derive paraphrasing rules
from corpora was undertaken by (Jacquemin et
al., 1997), who investigated morphological and
syntactic variants of technical terms. While these
rules achieve high accuracy in identifying term
paraphrases, the techniques used have not been
extended to other types of paraphrasing yet. Sta-
tistical techniques were also successfully used
by (Lapata, 2001) to identify paraphrases of
adjective-noun phrases. In contrast, our method
is not limited to a particular paraphrase type.
3 The Data
The corpus we use for identification of para-
phrases is a collection of multiple English trans-
lations from a foreign source text. Specifically,
we use literary texts written by foreign authors.
Many classical texts have been translated more
than once, and these translations are available
on-line. In our experiments we used 5 books,
among them, Flaubert?s Madame Bovary, Ander-
sen?s Fairy Tales and Verne?s Twenty Thousand
Leagues Under the Sea. Some of the translations
were created during different time periods and in
different countries. In total, our corpus contains
11 translations 2.
At first glance, our corpus seems quite simi-
lar to parallel corpora used by researchers in MT,
such as the Canadian Hansards. The major dis-
tinction lies in the degree of proximity between
the translations. Analyzing multiple translations
of the literary texts, critics (e.g. (Wechsler, 1998))
have observed that translations ?are never iden-
tical?, and each translator creates his own inter-
pretations of the text. Clauses such as ?adorning
his words with puns? and ?saying things to make
her smile? from the sentences in Figure 1 are ex-
amples of distinct translations. Therefore, a com-
plete match between words of related sentences
is impossible. This characteristic of our corpus
is similar to problems with noisy and comparable
corpora (Veronis, 2000), and it prevents us from
using methods developed in the MT community
based on clean parallel corpora, such as (Brown
et al, 1993).
Another distinction between our corpus and
parallel MT corpora is the irregularity of word
matchings: in MT, no words in the source lan-
guage are kept as is in the target language trans-
lation; for example, an English translation of
2Free of copyright restrictions part of
our corpus(9 translations) is available at
http://www.cs.columbia.edu/?regina /par.
a French source does not contain untranslated
French fragments. In contrast, in our corpus
the same word is usually used in both transla-
tions, and only sometimes its paraphrases are
used, which means that word?paraphrase pairs
will have lower co-occurrence rates than word?
translation pairs in MT. For example, consider oc-
currences of the word ?boy? in two translations of
?Madame Bovary? ? E. Marx-Aveling?s transla-
tion and Etext?s translation. The first text contains
55 occurrences of ?boy?, which correspond to 38
occurrences of ?boy? and 17 occurrences of its
paraphrases (?son?, ?young fellow? and ?young-
ster?). This rules out using word translation meth-
ods based only on word co-occurrence counts.
On the other hand, the big advantage of our cor-
pus comes from the fact that parallel translations
share many words, which helps the matching pro-
cess. We describe below a method of paraphrase
extraction, exploiting these features of our corpus.
4 Preprocessing
During the preprocessing stage, we perform sen-
tence alignment. Sentences which are translations
of the same source sentence contain a number of
identical words, which serve as a strong clue to
the matching process. Alignment is performed
using dynamic programming (Gale and Church,
1991) with a weight function based on the num-
ber of common words in a sentence pair. This
simple method achieves good results for our cor-
pus, because 42% of the words in corresponding
sentences are identical words on average. Align-
ment produces 44,562 pairs of sentences with
1,798,526 words. To evaluate the accuracy of
the alignment process, we analyzed 127 sentence
pairs from the algorithm?s output. 120(94.5%)
alignments were identified as correct alignments.
We then use a part-of-speech tagger and chun-
ker (Mikheev, 1997) to identify noun and verb
phrases in the sentences. These phrases become
the atomic units of the algorithm. We also record
for each token its derivational root, using the
CELEX(Baayen et al, 1993) database.
5 Method for Paraphrase Extraction
Given the aforementioned differences between
translations, our method builds on similarity in
the local context, rather than on global alignment.
Consider the two sentences in Figure 2.
And finally, dazzlingly white, it shone high above
them in the empty ? .
It appeared white and dazzling in the empty ? .
Figure 2: Fragments of aligned sentences
Analyzing the contexts surrounding ? ? ?-
marked blanks in both sentences, one expects that
they should have the same meaning, because they
have the same premodifier ?empty? and relate to
the same preposition ?in? (in fact, the first ? ? ?
stands for ?sky?, and the second for ?heavens?).
Generalizing from this example, we hypothesize
that if the contexts surrounding two phrases look
similar enough, then these two phrases are likely
to be paraphrases. The definition of the context
depends on how similar the translations are. Once
we know which contexts are good paraphrase pre-
dictors, we can extract paraphrase patterns from
our corpus.
Examples of such contexts are verb-object re-
lations and noun-modifier relations, which were
traditionally used in word similarity tasks from
non-parallel corpora (Pereira et al, 1993; Hatzi-
vassiloglou and McKeown, 1993). However, in
our case, more indirect relations can also be clues
for paraphrasing, because we know a priori that
input sentences convey the same information. For
example, in sentences from Figure 3, the verbs
?ringing? and ?sounding? do not share identical
subject nouns, but the modifier of both subjects
?Evening? is identical. Can we conclude that
identical modifiers of the subject imply verb sim-
ilarity? To address this question, we need a way
to identify contexts that are good predictors for
paraphrasing in a corpus.
People said ?The Evening Noise is sounding, the sun
is setting.?
?The evening bell is ringing,? people used to say.
Figure 3: Fragments of aligned sentences
To find ?good? contexts, we can analyze all
contexts surrounding identical words in the pairs
of aligned sentences, and use these contexts to
learn new paraphrases. This provides a basis for
a bootstrapping mechanism. Starting with identi-
cal words in aligned sentences as a seed, we can
incrementally learn the ?good? contexts, and in
turn use them to learn new paraphrases. Iden-
tical words play two roles in this process: first,
they are used to learn context rules; second, iden-
tical words are used in application of these rules,
because the rules contain information about the
equality of words in context.
This method of co-training has been previously
applied to a variety of natural language tasks,
such as word sense disambiguation (Yarowsky,
1995), lexicon construction for information ex-
traction (Riloff and Jones, 1999), and named en-
tity classification (Collins and Singer, 1999). In
our case, the co-training process creates a binary
classifier, which predicts whether a given pair of
phrases makes a paraphrase or not.
Our model is based on the DLCoTrain algo-
rithm proposed by (Collins and Singer, 1999),
which applies a co-training procedure to decision
list classifiers for two independent sets of fea-
tures. In our case, one set of features describes the
paraphrase pair itself, and another set of features
corresponds to contexts in which paraphrases oc-
cur. These features and their computation are de-
scribed below.
5.1 Feature Extraction
Our paraphrase features include lexical and syn-
tactic descriptions of the paraphrase pair. The
lexical feature set consists of the sequence of to-
kens for each phrase in the paraphrase pair; the
syntactic feature set consists of a sequence of
part-of-speech tags where equal words and words
with the same root are marked. For example, the
value of the syntactic feature for the pair (?the
vast chimney?, ?the chimney?) is (?DT  JJ NN ?,
?DT  NN ?), where indices indicate word equali-
ties. We believe that this feature can be useful for
two reasons: first, we expect that some syntac-
tic categories can not be paraphrased in another
syntactic category. For example, a determiner is
unlikely to be a paraphrase of a verb. Second,
this description is able to capture regularities in
phrase level paraphrasing. In fact, a similar rep-
resentation was used by (Jacquemin et al, 1997)
to describe term variations.
The contextual feature is a combination of
the left and right syntactic contexts surrounding
actual known paraphrases. There are a num-
ber of context representations that can be con-
sidered as possible candidates: lexical n-grams,
POS-ngrams and parse tree fragments. The nat-
ural choice is a parse tree; however, existing
parsers perform poorly in our domain3. Part-
of-speech tags provide the required level of ab-
straction, and can be accurately computed for our
data. The left (right) context is a sequence of
part-of-speech tags of  words, occurring on the
left (right) of the paraphrase. As in the case
of syntactic paraphrase features, tags of identi-
cal words are marked. For example, when 

, the contextual feature for the paraphrase pair
(?comfort?, ?console?) from Figure 1 sentences
is left  =?VB  TO ?, (?tried to?), left =?VB 
TO ?, (?tried to?), right  =?PRP$ ,	 ?, (?her,?)
right context$ =?PRP$ ,	 ?, (?her,?). In the next
section, we describe how the classifiers for con-
textual and paraphrasing features are co-trained.
5.2 The co-training algorithm
Our co-training algorithm has three stages: ini-
tialization, training of the contextual classifier and
training of the paraphrasing classifiers.
Initialization Words which appear in both sen-
tences of an aligned pair are used to create the ini-
tial ?seed? rules. Using identical words, we cre-
ate a set of positive paraphrasing examples, such
as word  =tried, word =tried. However, train-
ing of the classifier demands negative examples
as well; in our case it requires pairs of words
in aligned sentences which are not paraphrases
of each other. To find negative examples, we
match identical words in the alignment against
all different words in the aligned sentence, as-
suming that identical words can match only each
other, and not any other word in the aligned sen-
tences. For example, ?tried? from the first sen-
tence in Figure 1 does not correspond to any other
word in the second sentence but ?tried?. Based
on this observation, we can derive negative ex-
amples such as word  =tried, word =Emma and
word  =tried, word =console. Given a pair of
identical words from two sentences of length 
and 
 , the algorithm produces one positive ex-
3To the best of our knowledge all existing statistical
parsers are trained on WSJ or similar type of corpora. In the
experiments we conducted, their performance significantly
degraded on our corpus ? literary texts.
ample and 
 negative examples.
Training of the contextual classifier Using
this initial seed, we record contexts around pos-
itive and negative paraphrasing examples. From
all the extracted contexts we must identify the
ones which are strong predictors of their category.
Following (Collins and Singer, 1999), filtering is
based on the strength of the context and its fre-
quency. The strength of positive context  is de-
fined as ffEmpirically Estimating Order Constraints for
Content Planning in Generation
Pablo A. Duboue and Kathleen R. McKeown
Computer Science Department
Columbia University
10027, New York, NY, USA
{pablo,kathy}@cs.columbia.edu
Abstract
In a language generation system, a
content planner embodies one or more
?plans? that are usually hand?crafted,
sometimes through manual analysis of
target text. In this paper, we present a
system that we developed to automati-
cally learn elements of a plan and the
ordering constraints among them. As
training data, we use semantically an-
notated transcripts of domain experts
performing the task our system is de-
signed to mimic. Given the large degree
of variation in the spoken language of
the transcripts, we developed a novel al-
gorithm to find parallels between tran-
scripts based on techniques used in
computational genomics. Our proposed
methodology was evaluated two?fold:
the learning and generalization capabil-
ities were quantitatively evaluated us-
ing cross validation obtaining a level of
accuracy of 89%. A qualitative evalua-
tion is also provided.
1 Introduction
In a language generation system, a content plan-
ner typically uses one or more ?plans? to rep-
resent the content to be included in the out-
put and the ordering between content elements.
Some researchers rely on generic planners (e.g.,
(Dale, 1988)) for this task, while others use plans
based on Rhetorical Structure Theory (RST) (e.g.,
(Bouayad-Aga et al, 2000; Moore and Paris,
1993; Hovy, 1993)) or schemas (e.g., (McKe-
own, 1985; McKeown et al, 1997)). In all cases,
constraints on application of rules (e.g., plan op-
erators), which determine content and order, are
usually hand-crafted, sometimes through manual
analysis of target text.
In this paper, we present a method for learn-
ing the basic patterns contained within a plan and
the ordering among them. As training data, we
use semantically tagged transcripts of domain ex-
perts performing the task our system is designed
to mimic, an oral briefing of patient status af-
ter undergoing coronary bypass surgery. Given
that our target output is spoken language, there is
some level of variability between individual tran-
scripts. It is difficult for a human to see patterns
in the data and thus supervised learning based on
hand-tagged training sets can not be applied. We
need a learning algorithm that can discover order-
ing patterns in apparently unordered input.
We based our unsupervised learning algorithm
on techniques used in computational genomics
(Durbin et al, 1998), where from large amounts
of seemingly unorganized genetic sequences, pat-
terns representing meaningful biological features
are discovered. In our application, a transcript is
the equivalent of a sequence and we are searching
for patterns that occur repeatedly across multiple
sequences. We can think of these patterns as the
basic elements of a plan, representing small clus-
ters of semantic units that are similar in size, for
example, to the nucleus-satellite pairs of RST.1
By learning ordering constraints over these ele-
1Note, however, that we do not learn or represent inten-
tion.
age, gender, pmh, pmh, pmh, pmh, med-preop,
med-preop, med-preop, drip-preop, med-preop,
ekg-preop, echo-preop, hct-preop, procedure,
. . .
Figure 2: The semantic sequence obtained from
the transcript shown in Figure 1.
ments, we produce a plan that can be expressed
as a constraint-satisfaction problem. In this pa-
per, we focus on learning the plan elements and
the ordering constraints between them. Our sys-
tem uses combinatorial pattern matching (Rigout-
sos and Floratos, 1998) combined with clustering
to learn plan elements. Subsequently, it applies
counting procedures to learn ordering constraints
among these elements.
Our system produced a set of 24 schemata
units, that we call ?plan elements?2 , and 29 order-
ing constraints between these basic plan elements,
which we compared to the elements contained in
the orginal hand-crafted plan that was constructed
based on hand-analysis of transcripts, input from
domain experts, and experimental evaluation of
the system (McKeown et al, 2000).
The remainder of this article is organized as
follows: first the data used in our experiments
is presented and its overall structure and acqui-
sition methodology are analyzed. In Section 3
our techniques are described, together with their
grounding in computational genomics. The quan-
titative and qualitative evaluation are discussed
in Section 4. Related work is presented in Sec-
tion 5. Conclusions and future work are discussed
in Section 6.
2 Our data
Our research is part of MAGIC (Dalal et al, 1996;
McKeown et al, 2000), a system that is designed
to produce a briefing of patient status after un-
dergoing a coronary bypass operation. Currently,
when a patient is brought to the intensive care
unit (ICU) after surgery, one of the residents who
was present in the operating room gives a brief-
ing to the ICU nurses and residents. Several of
these briefings were collected and annotated for
the aforementioned evaluation. The resident was
2These units can be loosely related to the concept of mes-
sages in (Reiter and Dale, 2000).
equipped with a wearable tape recorder to tape
the briefings, which were transcribed to provide
the base of our empirical data. The text was sub-
sequently annotated with semantic tags as shown
in Figure 1. The figure shows that each sentence
is split into several semantically tagged chunks.
The tag-set was developed with the assistance of
a domain expert in order to capture the different
information types that are important for commu-
nication and the tagging process was done by two
non-experts, after measuring acceptable agree-
ment levels with the domain expert (see (McK-
eown et al, 2000)). The tag-set totalled over 200
tags. These 200 tags were then mapped to 29 cat-
egories, which was also done by a domain expert.
These categories are the ones used for our current
research.
From these transcripts, we derive the sequences
of semantic tags for each transcript. These se-
quences constitute the input and working material
of our analysis, they are an average length of 33
tags per transcript (min = 13, max = 66, ? =
11.6). A tag-set distribution analysis showed that
some of the categories dominate the tag counts.
Furthermore, some tags occur fairly regularly to-
wards either the beginning (e.g., date-of-birth) or
the end (e.g., urine-output) of the transcript, while
others (e.g., intraop-problems) are spread more or
less evenly throughout.
Getting these transcripts is a highly expensive
task involving the cooperation and time of nurses
and physicians in the busy ICU. Our corpus con-
tains a total number of 24 transcripts. Therefore,
it is important that we develop techniques that can
detect patterns without requiring large amounts of
data.
3 Methods
During the preliminary analysis for this research,
we looked for techniques to deal with analysis of
regularities in sequences of finite items (semantic
tags, in this case). We were interested in devel-
oping techniques that could scale as well as work
with small amounts of highly varied sequences.
Computational biology is another branch of
computer science that has this problem as one
topic of study. We focused on motif detection
techniques as a way to reduce the complexity of
the overall setting of the problem. In biological
He is 58-year-old
age
male
gender
. History is significant for Hodgkin?s disease
pmh
, treated
with . . . to his neck, back and chest. Hyperspadias
pmh
, BPH
pmh
, hiatal hernia
pmh
and
proliferative lymph edema in his right arm
pmh
. No IV?s or blood pressure down in the left
arm. Medications ? Inderal
med-preop
, Lopid
med-preop
, Pepcid
med-preop
, nitroglycerine
drip-preop
and heparin
med-preop
. EKG has PAC?s
ekg-preop
.
His Echo showed AI, MR of 47 cine amps with hypokinetic basal and anterior apical region.
echo-preop
Hematocrit 1.2
hct-preop
, otherwise his labs are unremarkable. Went to OR for what was felt to be
2 vessel CABG off pump both mammaries
procedure
. . . . . .
Figure 1: An annotated transcription of an ICU briefing (after anonymising).
terms, a motif is a small subsequence, highly con-
served through evolution. From the computer sci-
ence standpoint, a motif is a fixed-order pattern,
simply because it is a subsequence. The problem
of detecting such motifs in large databases has
attracted considerable interest in the last decade
(see (Hudak and McClure, 1999) for a recent sur-
vey). Combinatorial pattern discovery, one tech-
nique developed for this problem, promised to
be a good fit for our task because it can be pa-
rameterized to operate successfully without large
amounts of data and it will be able to iden-
tify domain swapped motifs: for example, given
a?b?c in one sequence and c?b?a in another.
This difference is central to our current research,
given that order constraints are our main focus.
TEIRESIAS (Rigoutsos and Floratos, 1998) and
SPLASH (Califano, 1999) are good representa-
tives of this kind of algorithm. We used an adap-
tation of TEIRESIAS.
The algorithm can be sketched as follows: we
apply combinatorial pattern discovery (see Sec-
tion 3.1) to the semantic sequences. The obtained
patterns are refined through clustering (Section
3.2). Counting procedures are then used to es-
timate order constraints between those clusters
(Section 3.3).
3.1 Pattern detection
In this section, we provide a brief explanation of
our pattern discovery methodology. The explana-
tion builds on the definitions below:
?L,W ? pattern. Given that ? represents the se-
mantic tags alphabet, a pattern is a string of
the form ?(?|?)? ?, where ? represents a
don?t care (wildcard) position. The ?L,W ?
parameters are used to further control the
amount and placement of the don?t cares:
every subsequence of length W, at least L
positions must be filled (i.e., they are non-
wildcards characters). This definition entails
that L ? W and also that a ?L,W ? pattern
is also a ?L,W + 1? pattern, etc.
Support. The support of pattern p given a set of
sequences S is the number of sequences that
contain at least one match of p. It indicates
how useful a pattern is in a certain environ-
ment.
Offset list. The offset list records the matching
locations of a pattern p in a list of sequences.
They are sets of ordered pairs, where the first
position records the sequence number and
the second position records the offset in that
sequence where p matches (see Figure 3).
Specificity. We define a partial order relation on
the pattern space as follows: a pattern p is
said to be more specific than a pattern q
if: (1) p is equal to q in the defined posi-
tions of q but has fewer undefined (i.e., wild-
cards) positions; or (2) q is a substring of p.
Specificity provides a notion of complexity
of a pattern (more specific patterns are more
complex). See Figure 4 for an example.
Using the previous definitions, the algorithm re-
duces to the problem of, given a set of sequences,
L, W , a minimum windowsize, and a support
pattern: AB?D
0 1 2 3 4 5 6 7 8 . . . ? offset
seq?: A B C D F A A B F D . . .
seq? : F C A B D D F F . . . . . .
.
.
.
offset list: {(?, 0); (?, 6); (?, 2); . . .}
Figure 3: A pattern, a set of sequences and an
offset list.
ABC??DF
ABCA?DF ABC??DFG
HHHj

less specific than
Figure 4: The specificity relation among patterns.
threshold, finding maximal ?L,W ?-patterns with
at least a support of support threshold. Our im-
plementation can be sketched as follows:
Scanning. For a given window size n, all the pos-
sible subsequences (i.e., n-grams) occurring
in the training set are identified. This process
is repeated for different window sizes.
Generalizing. For each of the identified subse-
quences, patterns are created by replacing
valid positions (i.e., any place but the first
and last positions) with wildcards. Only
?L,W ? patterns with support greater than
support threshold are kept. Figure 5 shows
an example.
Filtering. The above process is repeated increas-
ing the window size until no patterns with
enough support are found. The list of iden-
tified patterns is then filtered according to
specificity: given two patterns in the list, one
of them more specific than the other, if both
have offset lists of equal size, the less spe-
cific one is pruned3 . This gives us the list
of maximal motifs (i.e. patterns) which are
supported by the training data.
3Since they match in exactly the same positions, we
prune the less specific one, as it adds no new information.
A B C D E F ? subsequence
AB?DEF ABCD?F ? patterns. . .
HHHj

Figure 5: The process of generalizing an existing
subsequence.
3.2 Clustering
After the detection of patterns is finished, the
number of patterns is relatively large. Moreover,
as they have fixed length, they tend to be pretty
similar. In fact, many tend to have their support
from the same subsequences in the corpus. We are
interested in syntactic similarity as well as simi-
larity in context.
A convenient solution was to further cluster the
patterns, according to an approximate matching
distance measure between patterns, defined in an
appendix at the end of the paper.
We use agglomerative clustering with the dis-
tance between clusters defined as the maximum
pairwise distance between elements of the two
clusters. Clustering stops when no inter-cluster
distance falls below a user-defined threshold.
Each of the resulting clusters has a single pat-
tern represented by the centroid of the cluster.
This concept is useful for visualization of the
cluster in qualitative evaluation.
3.3 Constraints inference
The last step of our algorithm measures the fre-
quencies of all possible order constraints among
pairs of clusters, retaining those that occur of-
ten enough to be considered important, accord-
ing to some relevancy measure. We also discard
any constraint that it is violated in any training
sequence. We do this in order to obtain clear-cut
constraints. Using the number of times a given
constraint is violated as a quality measure is a
straight-forward extension of our framework. The
algorithm proceeds as follows: we build a table
of counts that is updated every time a pair of pat-
terns belonging to particular clusters are matched.
To obtain clear-cut constraints, we do not count
overlapping occurrences of patterns.
From the table of counts we need some rele-
vancy measure, as the distribution of the tags is
skewed. We use a simple heuristic to estimate
a relevancy measure over the constraints that are
never contradicted. We are trying to obtain an es-
timate of
Pr (A ?precedes B)
from the counts of
c = A ??preceded B
We normalize with these counts (where x ranges
over all the patterns that match before/after A or
B):
c1 = A ??preceded x
and
c2 = x ??preceded B
The obtained estimates, e1 = c/c1 and e2 = c/c2,
will in general yield different numbers. We use
the arithmetic mean between both, e = (e1+e2)2 ,
as the final estimate for each constraint. It turns
out to be a good estimate, that predicts accuracy
of the generated constraints (see Section 4).
4 Results
We use cross validation to quantitatively evaluate
our results and a comparison against the plan of
our existing system for qualitative evaluation.
4.1 Quantitative evaluation
We evaluated two items: how effective the pat-
terns and constraints learned were in an unseen
test set and how accurate the predicted constraints
were. More precisely:
Pattern Confidence. This figure measures the
percentage of identified patterns that were
able to match a sequence in the test set.
Constraint Confidence. An ordering constraint
between two clusters can only be checkable
on a given sequence if at least one pattern
from each cluster is present. We measure
the percentage of the learned constraints that
are indeed checkable over the set of test se-
quences.
Constraint Accuracy. This is, from our perspec-
tive, the most important judgement. It mea-
sures the percentage of checkable ordering
Table 1: Evaluation results.
Test Result
pattern confidence 84.62%
constraint confidence 66.70%
constraint accuracy 89.45%
constraints that are correct, i.e., the order
constraint was maintained in any pair of
matching patterns from both clusters in all
the test-set sequences.
Using 3-fold cross-validation for computing these
metrics, we obtained the results shown in Ta-
ble 1 (averaged over 100 executions of the exper-
iment). The different parameter settings were de-
fined as follows: for the motif detection algorithm
?L,W ? = ?2, 3? and support threshold of 3. The
algorithm will normally find around 100 maximal
motifs. The clustering algorithm used a relative
distance threshold of 3.5 that translates to an ac-
tual treshold of 120 for an average inter-cluster
distance of 174. The number of produced clusters
was in the order of the 25 clusters or so. Finally, a
threshold in relevancy of 0.1 was used in the con-
straint learning procedure. Given the amount of
data available for these experiments all these pa-
rameters were hand-tunned.
4.2 Qualitative evaluation
The system was executed using all the available
information, with the same parametric settings
used in the quantitative evaluation, yielding a set
of 29 constraints, out of 23 generated clusters.
These constraints were analyzed by hand and
compared to the existing content-planner. We
found that most rules that were learned were val-
idated by our existing plan. Moreover, we gained
placement constraints for two pieces of semantic
information that are currently not represented in
the system?s plan. In addition, we found minor
order variation in relative placement of two differ-
ent pairs of semantic tags. This leads us to believe
that the fixed order on these particular tags can
be relaxed to attain greater degrees of variability
in the generated plans. The process of creation
of the existing content-planner was thorough, in-
formed by multiple domain experts over a three
year period. The fact that the obtained constraints
mostly occur in the existing plan is very encour-
aging.
5 Related work
As explained in (Hudak and McClure, 1999), mo-
tif detection is usually targeted with alignment
techniques (as in (Durbin et al, 1998)) or with
combinatorial pattern discovery techniques such
as the ones we used here. Combinatorial pattern
discovery is more appropriate for our task because
it allows for matching across patterns with permu-
tations, for representation of wild cards and for
use on smaller data sets.
Similar techniques are used in NLP. Align-
ments are widely used in MT, for example
(Melamed, 1997), but the crossing problem is a
phenomenon that occurs repeatedly and at many
levels in our task and thus, this is not a suitable
approach for us.
Pattern discovery techniques are often used for
information extraction (e.g., (Riloff, 1993; Fisher
et al, 1995)), but most work uses data that con-
tains patterns labelled with the semantic slot the
pattern fills. Given the difficulty for humans in
finding patterns systematically in our data, we
needed unsupervised techniques such as those de-
veloped in computational genomics.
Other stochastic approaches to NLG normally
focus on the problem of sentence generation,
including syntactic and lexical realization (e.g.,
(Langkilde and Knight, 1998; Bangalore and
Rambow, 2000; Knight and Hatzivassiloglou,
1995)). Concurrent work analyzing constraints on
ordering of sentences in summarization found that
a coherence constraint that ensures that blocks of
sentences on the same topic tend to occur together
(Barzilay et al, 2001). This results in a bottom-
up approach for ordering that opportunistically
groups sentences together based on content fea-
tures. In contrast, our work attempts to automati-
cally learn plans for generation based on semantic
types of the input clause, resulting in a top-down
planner for selecting and ordering content.
6 Conclusions
In this paper we presented a technique for extract-
ing order constraints among plan elements that
performs satisfactorily without the need of large
corpora. Using a conservative set of parameters,
we were able to reconstruct a good portion of a
carefully hand-crafted planner. Moreover, as dis-
cussed in the evaluation, there are several pieces
of information in the transcripts which are not
present in the current system. From our learned
results, we have inferred placement constraints of
the new information in relation to the previous
plan elements without further interviews with ex-
perts.
Furthermore, it seems we have captured order-
sensitive information in the patterns and free-
order information is kept in the don?t care model.
The patterns, and ordering constraints among
them, provide a backbone of relatively fixed struc-
ture, while don?t cares are interspersed among
them. This model, being probabilistic in nature,
means a great deal of variation, but our gener-
ated plans should have variability in the right po-
sitions. This is similar to findings of floating posi-
tioning of information, together with oportunistic
rendering of the data as used in STREAK (Robin
and McKeown, 1996).
6.1 Future work
We are planning to use these techniques to revise
our current content-planner and incorporate infor-
mation that is learned from the transcripts to in-
crease the possible variation in system output.
The final step in producing a full-fledged
content-planner is to add semantic constraints on
the selection of possible orderings. This can be
generated through clustering of semantic input to
the generator.
We also are interested in further evaluating the
technique in an unrestricted domain such as the
Wall Street Journal (WSJ) with shallow seman-
tics such as the WordNet top-category for each
NP-head. This kind of experiment may show
strengths and limitations of the algorithm in large
corpora.
7 Acknowledgments
This research is supported in part by NLM Con-
tract R01 LM06593-01 and the Columbia Uni-
versity Center for Advanced Technology in In-
formation Management (funded by the New York
State Science and Technology Foundation). The
authors would like to thank Regina Barzilay,
intraop-problems intraop-problems
?
?
?
operation 11.11%
drip 33.33%
intraop-problems 33.33%
total-meds-anesthetics 22.22%
?
?
?
drip
intraop-problems
?
?
?
operation 14.29%
drip 14.29%
intraop-problems 42.86%
total-meds-anesthetics 28.58%
?
?
?
drip drip
intraop-problems intraop-problems
?
?
?
operation 20.00%
drip 20.00%
intraop-problems 20.00%
total-meds-anesthetics 40.00%
?
?
?
drip drip
Figure 6: Cluster and patterns example. Each line corresponds to a different pattern. The elements
between braces are don?t care positions (three patterns conform this cluster: intraop-problems intraop-problems ? drip,
intraop-problems ? drip drip and intraop-problems intraop-problems drip drip the don?t care model shown in each brace must sum up to
1 but there is a strong overlap between patterns ?the main reason for clustering)
Noemie Elhadad and Smaranda Muresan for help-
ful suggestions and comments. The aid of two
anonymous reviewers was also highly appreci-
ated.
References
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In COLING, 2000, Saarbrcken, Germany.
Regina Barzilay, Noemie Elhadad, and Kathleen R.
McKeown. 2001. Sentence ordering in multidoc-
ument summarization. In HLT, 2001, San Diego,
CA.
Nadjet Bouayad-Aga, Richard Power, and Donia
Scott. 2000. Can text structure be incompatible
with rhetorical structure? In Proceedings of the
1st International Conference on Natural Language
Generation (INLG-2000), pages 194?200, Mitzpe
Ramon, Israel.
Andrea Califano. 1999. Splash: Structural pattern lo-
calization analysis by sequential histograms. Bioin-
formatics, 12, February.
Mukesh Dalal, Steven Feiner, , Kathleen McKeown,
ShiMei Pan, Michelle Zhou, Tobias Hollerer, James
Shaw, Yong Feng, and Jeanne Fromer. 1996. Nego-
tiation for automated generation of temporal multi-
media presentations. In Proceedings of ACM Mul-
timedia ?96, Philadelphia.
Robert Dale. 1988. Generating referring expressions
in a domain of objects and processes. Ph.D. thesis,
University of Edinburgh.
Richard Durbin, S. Eddy, A. Krogh, and G. Mitchi-
son. 1998. Biological sequence analysis. Cam-
bridge Univeristy Press.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1995. De-
scription of the umass system as used for muc-
6. In Morgan Kaufman, editor, Proceedings of the
Sixth Message Understanding Conference (MUC-
6), pages 127?140, San Francisco.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence. (Special Issue on Natural Language
Processing).
J. Hudak and Marcela McClure. 1999. A comparative
analysis of computational motif?detection methods.
In R.B. Altman, A. K. Dunker, L. Hunter, T. E.
Klein, and K. Lauderdale, editors, Pacific Sympo-
sium on Biocomputing, ?99, pages 138?149, New
Jersey. World Scientific.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL?95).
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Proceedings
of the Ninth International Natural Language Gen-
eration Workshop (INLG?98).
Kathleen McKeown, ShiMei Pan, James Shaw, Jordan
Desmand, and Barry Allen. 1997. Language gen-
eration for multimedia healthcare briefings. In Pro-
ceedings of the 5th Conference on Applied Natural
Language Processing (ANLP?97), Washington, DC,
April.
Kathleen R. McKeown, Desmond Jordan, Steven
Feiner, James Shaw, Elizabeth Chen, Shabina Ah-
mad, Andre Kushniruk, and Vimla Patel. 2000. A
study of communication in the cardiac surgery in-
tensive care unit and its implications for automated
briefing. In AMIA ?2000.
Kathleen R. McKeown. 1985. Text Generation: Us-
ing Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Cambridge Uni-
versity Press.
I. Dan Melamed. 1997. A portable algorithm for
mapping bitext correspondence. In 35th Confer-
ence of the Association for Computational Linguis-
tics (ACL?97), Madrid, Spain.
Johanna D. Moore and Ce?cile L. Paris. 1993. Plan-
ning text for advisory dialogues: Capturing inten-
tional and rhetorical information. Computational
Linguistics, 19(4):651?695.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Isidore Rigoutsos and Aris Floratos. 1998. Combina-
torial pattern discovery in biological sequences: the
teiresias algorithm. Bioinformatics, 14(1):55?67.
Ellen Riloff. 1993. Automatically constructing a dic-
tionary for information extraction. In AAAI Press
/ MIT Press, editor, Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
811?816.
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision?
based model for summary generation. Artificial In-
telligence, 85(1?2):135?179.
Appendix - Definition of the distance mea-
sure used for clustering.
An approximate matching measure is de-
fined for a given extended pattern. The ex-
tended pattern is represented as a sequence of
sets; defined positions have a singleton set,
while wildcard positions contain the non-zero
probability elements in their don?t care model
(e.g. given intraop-problems, intraop-problems, {drip 10%,intubation
90%}, drip we model this as [{intraop-problems}; {intraop-
problems}; {drip, intubation}; {drip}}]).
Consider p to be such a pattern, o an offset and
S a sequence, the approximate matching is de-
fined by
m?(p, o, S) =
?length(p)
i=0 match(p[i], S[i + o])
length(p)
where the match(P, e) function is defined as 0 if
e ? P , 1 otherwise, and where P is the set at
position i in the extended pattern p and e is the
element of the sequence S at position i + o.
Our measure is normalized to [0, 1]. Using
this function, we define the approximate match-
ing distance measure (one way) between a pattern
p1 and a pattern p2 as the sum (averaged over the
length of the offset list of p1) of all the approxi-
mate matching measures of p2 over the offset list
of p1. This is, again, a real number in [0, 1]. To
ensure symmetry, we define the distance between
p1 and p2 as the average between the one way dis-
tance between p1 and p2 and between p2 and p1.
Discourse Segmentation of Multi-Party Conversation
Michel Galley Kathleen McKeown
Columbia University
Computer Science Department
1214 Amsterdam Avenue
New York, NY 10027, USA
{galley,kathy}@cs.columbia.edu
Eric Fosler-Lussier
Columbia University
Electrical Engineering Department
500 West 120th Street
New York, NY 10027, USA
fosler@ieee.org
Hongyan Jing
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, USA
hjing@us.ibm.com
Abstract
We present a domain-independent topic
segmentation algorithm for multi-party
speech. Our feature-based algorithm com-
bines knowledge about content using a
text-based algorithm as a feature and
about form using linguistic and acous-
tic cues about topic shifts extracted from
speech. This segmentation algorithm uses
automatically induced decision rules to
combine the different features. The em-
bedded text-based algorithm builds on lex-
ical cohesion and has performance compa-
rable to state-of-the-art algorithms based
on lexical information. A significant er-
ror reduction is obtained by combining the
two knowledge sources.
1 Introduction
Topic segmentation aims to automatically divide text
documents, audio recordings, or video segments,
into topically related units. While extensive research
has targeted the problem of topic segmentation of
written texts and spoken monologues, few have stud-
ied the problem of segmenting conversations with
many participants (e.g., meetings). In this paper, we
present an algorithm for segmenting meeting tran-
scripts. This study uses recorded meetings of typi-
cally six to eight participants, in which the informal
style includes ungrammatical sentences and overlap-
ping speakers. These meetings generally do not have
pre-set agendas, and the topics discussed in the same
meeting may or may not related.
The meeting segmenter comprises two compo-
nents: one that capitalizes on word distribution to
identify homogeneous units that are topically cohe-
sive, and a second component that analyzes conver-
sational features of meeting transcripts that are in-
dicative of topic shifts, like silences, overlaps, and
speaker changes. We show that integrating features
from both components with a probabilistic classifier
(induced with c4.5rules) is very effective in improv-
ing performance.
In Section 2, we review previous approaches to
the segmentation problem applied to spoken and
written documents. In Section 3, we describe the
corpus of recorded meetings intended to be seg-
mented, and the annotation of its discourse structure.
In Section 4, we present our text-based segmenta-
tion component. This component mainly relies on
lexical cohesion, particularly term repetition, to de-
tect topic boundaries. We evaluated this segmenta-
tion against other lexical cohesion segmentation pro-
grams and show that the performance is state-of-the-
art. In the subsequent section, we describe conver-
sational features, such as silences, speaker change,
and other features like cue phrases. We present a
machine learning approach for integrating these con-
versational features with the text-based segmenta-
tion module. Experimental results show a marked
improvement in meeting segmentation with the in-
corporation of both sets of features. We close with
discussions and conclusions.
2 Related Work
Existing approaches to textual segmentation can be
broadly divided into two categories. On the one
hand, many algorithms exploit the fact that topic
segments tend to be lexically cohesive. Embodi-
ments of this idea include semantic similarity (Mor-
ris and Hirst, 1991; Kozima, 1993), cosine similarity
in word vector space (Hearst, 1994), inter-sentence
similarity matrix (Reynar, 1994; Choi, 2000), en-
tity repetition (Kan et al, 1998), word frequency
models (Reynar, 1999), or adaptive language models
(Beeferman et al, 1999). Other algorithms exploit
a variety of linguistic features that may mark topic
boundaries, such as referential noun phrases (Pas-
sonneau and Litman, 1997).
In work on segmentation of spoken docu-
ments, intonational, prosodic, and acoustic indica-
tors are used to detect topic boundaries (Grosz and
Hirschberg, 1992; Nakatani et al, 1995; Hirschberg
and Nakatani, 1996; Passonneau and Litman, 1997;
Hirschberg and Nakatani, 1998; Beeferman et al,
1999; Tu?r et al, 2001). Such indicators include
long pauses, shifts in speaking rate, great range in
F0 and intensity, and higher maximum accent peak.
These approaches use different learning mecha-
nisms to combine features, including decision trees
(Grosz and Hirschberg, 1992; Passonneau and Lit-
man, 1997; Tu?r et al, 2001) exponential models
(Beeferman et al, 1999) or other probabilistic mod-
els (Hajime et al, 1998; Reynar, 1999).
3 The ICSI Meeting Corpus
We have evaluated our segmenter on the ICSI Meet-
ing corpus (Janin et al, 2003). This corpus is one of
a growing number of corpora with human-to-human
multi-party conversations. In this corpus, record-
ings of meetings ranged primarily over three differ-
ent recurring meeting types, all of which concerned
speech or language research.1 The average duration
is 60 minutes, with an average of 6.5 participants.
They were transcribed, and each conversation turn
was marked with the speaker, start time, end time,
and word content.
From the corpus, we selected 25 meetings to be
segmented, each by at least three subjects. We
opted for a linear representation of discourse, since
finer-grained discourse structures (e.g. (Grosz and
Sidner, 1986)) are generally considered to be diffi-
cult to mark reliably. Subjects were asked to mark
each speaker change (potential boundary) as either
boundary or non-boundary. In the resulting anno-
tation, the agreed segmentation based on majority
1While it would be desirable to have a broader variety of
meetings, we hope that experiments on this corpus will still
carry some generality.
opinion contained 7.5 segments per meeting on av-
erage, while the average number of potential bound-
aries is 770. We used Cochran?s Q (1950) to eval-
uate the agreement among annotators. Cochran?s
test evaluates the null hypothesis that the number
of subjects assigning a boundary at any position is
randomly distributed. The test shows that the inter-
judge reliability is significant to the 0.05 level for 19
of the meetings, which seems to indicate that seg-
ment identification is a feasible task.2
4 Segmentation based on Lexical Cohesion
Previous work on discourse segmentation of written
texts indicates that lexical cohesion is a strong in-
dicator of discourse structure. Lexical cohesion is
a linguistic property that pertains to speech as well,
and is a linguistic phenomenon that can also be ex-
ploited in our case: while our data does not have
the same kind of syntactic and rhetorical structure
as written text, we nonetheless expect that informa-
tion from the written transcription alone should pro-
vide indications about topic boundaries. In this sec-
tion, we describe our work on LCseg, a topic seg-
menter based on lexical cohesion that can handle
both speech and text, but that is especially designed
to generate the lexical cohesion feature used in the
feature-based segmentation described in Section 5.
4.1 Algorithm Description
LCseg computes lexical chains, which are thought
to mirror the discourse structure of the underly-
ing text (Morris and Hirst, 1991). We ignore syn-
onymy and other semantic relations, building a re-
stricted model of lexical chains consisting of sim-
ple term repetitions, hypothesizing that major topic
shifts are likely to occur where strong term repeti-
tions start and end. While other relations between
lexical items also work as cohesive factors (e.g. be-
tween a term and its super-ordinate), the work on
linear topic segmentation reporting the most promis-
ing results account for term repetitions alone (Choi,
2000; Utiyama and Isahara, 2001).
The preprocessing steps of LCseg are common to
many segmentation algorithms. The input document
is first tokenized, non-content words are removed,
2Four other meetings failed short the significance test, while
there was little agreement on the two last ones (p > 0.1).
and remaining words are stemmed using an exten-
sion of Porter?s stemming algorithm (Xu and Croft,
1998) that conflates stems using corpus statistics.
Stemming will allow our algorithm to more accu-
rately relate terms that are semantically close.
The core algorithm of LCseg has two main parts:
a method to identify and weight strong term repeti-
tions using lexical chains, and a method to hypothe-
size topic boundaries given the knowledge of multi-
ple, simultaneous chains of term repetitions.
A term is any stemmed content word within the
text. A lexical chain is constructed to consist of all
repetitions ranging from the first to the last appear-
ance of the term in the text. The chain is divided into
subchains when there is a long hiatus of h consecu-
tive sentences with no occurrence of the term, where
h is determined experimentally. For each hiatus, a
new division is made and thus, we avoid creating
weakly linked chains.
For all chains that have been identified, we use a
weighting scheme that we believe is appropriate to
the task of inducing the topical or sub-topical struc-
ture of text. The weighting scheme depends on two
factors:
Frequency: chains containing more repeated
terms receive a higher score.
Compactness: shorter chains receive a higher
weight than longer ones. If two chains of different
lengths contain the same number of terms, we assign
a higher score to the shortest one. Our assumption
is that the shorter one, being more compact, seems
to be a better indicator of lexical cohesion.3
We apply a variant of a metric commonly used
in information retrieval, TF.IDF (Salton and Buck-
ley, 1988), to score term repetitions. If R1 . . . Rn is
the set of all term repetitions collected in the text,
t1 . . . tn the corresponding terms, L1 . . . Ln their re-
spective lengths,4 and L the length of the text, the
adapted metric is expressed as follows, combining
frequency (freq(ti)) of a term ti and the compact-
ness of its underlying chain:
score(Ri) = freq(ti) ? log( LLi )
3The latter parameter might seem controversial at first, and
one might assume that longer chains should receive a higher
score. However we point out that in a linear model of dis-
course, chains that almost span the entire text are barely indica-
tive of any structure (assuming boundaries are only hypothe-
sized where chains start and end).
4All lengths are expressed in number of sentences.
In the second part of the algorithm, we combine
information from all term repetitions to compute a
lexical cohesion score at each sentence break (or,
in the case of spoken conversations, speaker turn
break). This step of our algorithm is very similar
in spirit to TextTiling (Hearst, 1994). The idea is to
work with two adjacent analysis windows, each of
fixed size k. For each sentence break, we determine
a lexical cohesion function by computing the cosine
similarity at the transition between the two windows.
Instead of using word counts to compute similarity,
we analyze lexical chains that overlap with the two
windows. The similarity between windows (A and
B) is computed with:5
cosine(A,B) =
?
i
wi,A?wi,B??
i
w2i,A
?
i
w2i,B
where
wi,? =
{
score(Ri) if Ri overlaps ? ? {A,B}
0 otherwise
The similarity computed at each sentence break
produces a plot that shows how lexical cohesion
changes over time; an example is shown in Figure 1.
The lexical cohesion function is then smoothed us-
ing a moving average filter, and minima become po-
tential segment boundaries. Then, in a manner quite
similar to (Hearst, 1994), the algorithm determines
for every local minimum mi how sharp of a change
there is in the lexical cohesion function. The algo-
rithm looks on each side of mi for maxima of cohe-
sion, and once it eventually finds one on each side (l
and r), it computes the hypothesized segmentation
probability:
p(mi) = 12 [LCF(l) + LCF(r) ? 2 ? LCF(m)]
where LCF(x) is the value of the lexical cohesion
function at x.
This score is supposed to capture the sharpness of
the change in lexical cohesion, and give probabilities
close to 1 for breaks like sentence 179 in Figure 1.
Finally, the algorithm selects the hypothesized
boundaries with the highest computed probabilities.
If the number of reference boundaries is unknown,
the algorithm has to make a guess. It computes the
5Normalizing anything in these windows has little ef-
fect, since the cosine similarity is scale invariant, that is
cosine(?xa, xb) = cosine(xa, xb) for ? > 0.
20 40 60 80 100 120 140 160 180 200 220 240 260
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1: Application of the LCseg algorithm on the concatenation of 16 WSJ stories. Numbers on the
x-axis represent sentence indices, and y-axis represents the lexical cohesion function. The representative
example presented here is segmented by LCseg with an error of Pk = 15.79, while the average performance
of the algorithm is Pk = 15.31 on the WSJ test corpus (unknown number of segments).
mean and the variance of the hypothesized probabil-
ities of all potential boundaries (local minima). As
we can see in Figure 1, there are many local minima
that do not correspond to actual boundaries. Thus,
we ignore all potential boundaries with a probability
lower than plimit. For the remaining points, we com-
pute the threshold using the average (?) and standard
deviation (?) of the p(mi) values, and each potential
boundary mi above the threshold ??? ?? is hypoth-
esized as a real boundary.
4.2 Evaluation
We evaluate LCseg against two state-of-the-art seg-
mentation algorithms based on lexical cohesion
(Choi, 2000; Utiyama and Isahara, 2001). We use
the error metric Pk proposed by Beeferman et al
(1999) to evaluate segmentation accuracy. It com-
putes the probability that sentences k units (e.g. sen-
tences) apart are incorrectly determined as being ei-
ther in different segments or in the same one. Since
it has been argued in (Pevzner and Hearst, 2002) that
Pk has some weaknesses, we also include results ac-
cording to the WindowDiff (WD) metric (which is
described in the same work).
A test corpus of concatenated6 texts extracted
from the Brown corpus was built by Choi (2000)
to evaluate several domain-independent segmenta-
tion algorithms. We reuse the same test corpus for
our evaluation, in addition to two other test corpora
we constructed to test how segmenters scale across
genres and how they perform with texts with various
6Concatenated documents correspond to reference seg-
ments.
number of segments.7 We designed two test corpora,
each of 500 documents, using concatenated texts
extracted from the TDT and WSJ corpora, ranging
from 4 to 22 in number of segments.
LCseg depends on several parameters. Parameter
tuning was performed on three tuning corpora of one
thousand texts each.8 We performed searches for the
optimal settings of the four tunable parameters in-
troduced above; the best performance was achieved
with h = 11 (hiatus length for dividing a chain into
parts), k = 2 (analysis window size), plimit = 0.1
and ? = 12 (thresholding limits for the hypothesized
boundaries).
As shown in Table 1, our algorithm is signifi-
cantly better than (Choi, 2000) (labeled C99) on
all three test corpora, according to a one-sided t-
test of the null hypothesis of equal mean at the 0.01
level. It is not clear whether our algorithm is better
than (Utiyama and Isahara, 2001) (U00). When the
number of segments is provided to the algorithms,
our algorithm is significantly better than Utiyama?s
on WSJ, better on Brown (but not significant), and
significantly worse on TDT. When the number of
boundaries is unknown, our algorithm is insignifi-
cantly worse on Brown, but significantly better on
WSJ and TDT ? the two corpora designed to have
a varying number of segments per document. In the
case of the Meeting corpus, none of the algorithms
are significantly different than the others, due to the
7All texts in Choi?s test corpus have exactly 10 segments.
We are concerned that the adjustments of any algorithm param-
eters might overfit this predefined number of segments.
8These texts are different from the ones used for evaluation.
Brown corpus
known unknown
Pk WD Pk WD
C99 11.19% 13.86% 12.07% 14.57%
U00 8.77% 9.44% 9.76% 10.32%
LCseg 8.69% 9.42% 10.49% 11.37%
p-val. 0.42 0.48 0.027 0.0037
TDT corpus
C99 9.37% 11.91% 10.18% 12.72%
U00 4.70% 6.29% 8.70% 11.12%
LCseg 6.15% 8.41% 6.95% 9.09%
p-val. 1.1e-05 2.8e-07 4.5e-05 2.8e-05
WSJ corpus
C99 19.61% 26.42% 22.32% 29.81%
U00 15.18% 21.54% 17.71% 24.06%
LCseg 12.21% 18.25% 15.31% 22.14%
p-val. 1.4e-08 1.7e-08 2.6e-04 0.0063
Meeting corpus
C99 33.79% 37.25% 47.42% 58.08%
U00 31.99% 34.49% 37.39% 40.43%
LCseg 26.37% 29.40% 31.91% 35.88%
p-val. 0.026 0.14 0.14 0.23
Table 1: Comparison C99 and U00. The p-values in
the table are the results of significance tests between
U00 and LCseg. Bold-faced values are scores that
are statistically significant.
small test set size.
In conclusion, LCseg has a performance compara-
ble to state-of-the-art text segmentation algorithms,
with the added advantage of computing a segmen-
tation probability at each potential boundary. This
information can be effectively used in the feature-
based segmenter to account for lexical cohesion, as
described in the next section.
5 Feature-based Segmentation
In the previous section, we have concentrated exclu-
sively on the consideration of content (through lexi-
cal cohesion) to determine the structure of texts, ne-
glecting any influence of form. In this section, we
explore formal devices that are indicative of topic
shifts, and explain how we use these cues to build a
segmenter targeting conversational speech.
5.1 Probabilistic Classifiers
Topic segmentation is reduced here to a classifica-
tion problem, where each utterance break Bi is ei-
ther considered a topic boundary or not. We use
statistical modeling techniques to build a classifier
that uses local features (e.g. cue phrases, pauses)
to determine if an utterance break corresponds to
a topic boundary. We chose C4.5 and C4.5rules
(Quinlan, 1993), two programs to induce classifi-
cation rules in the form of decision trees and pro-
duction rules (respectively). C4.5 generates an un-
pruned decision tree, which is then analyzed by
C4.5rules to generate a set of pruned production
rules (it tries to find the most useful subset of them).
The advantage of pruned rules over decision trees is
that they are easier to analyze, and allow combina-
tion of features in the same rule (feature interactions
are explicit).
The greedy nature of decision rule learning algo-
rithms implies that a large set of features can lead
to bad performance and generalization capability. It
is desirable to remove redundant and irrelevant fea-
tures, especially in our case since we have little data
labeled with topic shifts; with a large set of fea-
tures, we would risk overfitting the data. We tried
to restrict ourselves to features whose inclusion is
motivated by previous work (pauses, speech rate)
and added features that are specific to multi-speaker
speech (overlap, changes in speaker activity).
5.2 Features
Cue phrases: previous work on segmentation has
found that discourse particles like now, well pro-
vide valuable information about the structure of texts
(Grosz and Sidner, 1986; Hirschberg and Litman,
1994; Passonneau and Litman, 1997). We analyzed
the correlation between words in the meeting cor-
pus and labeled topic boundaries, and automatically
extracted utterance-initial cue phrases9 that are sta-
tistically correlated with boundaries. For every word
in the meeting corpus, we counted the number of its
occurrences near any topic boundary, and its num-
ber of appearances overall. Then, we performed ?2
significance tests (e.g. figure 2 for okay) under the
null hypothesis that no correlation exists. We se-
lected terms whose ?2 value rejected the hypothesis
under a 0.01-level confidence (the rejection criterion
is ?2 ? 6.635). Finally, induced cue phrases whose
usage has never been described in other work were
removed (marked with ? in Table 3). Indeed, there
is a risk that the automatically derived list of cue
phrases could be too specific to the word usage in
9As in (Litman and Passonneau, 1995), we restrict ourselves
to the first lexical item of any utterance, plus the second one if
the first item is also a cue word.
Near boundary Distant
okay 64 740
Other 657 25896
Table 2: okay (?2 = 89.11, df = 1, p < 0.01).
okay 93.05 but 13.57
shall ? 27.34 so 11.65
anyway 23.95 and 10.99
we?re ? 17.67 should ? 10.21
alright 16.09 good ? 7.70
let?s ? 14.54
Table 3: Automatically selected cue phrases.
these meetings.
Silences: previous work has found that ma-
jor shifts in topic typically show longer silences
(Passonneau and Litman, 1993; Hirschberg and
Nakatani, 1996). We investigated the presence of
silences in meetings and their correlation with topic
boundaries, and found it necessary to make a distinc-
tion between pauses and gaps (Levinson, 1983). A
pause is a silence that is attributable to a given party,
for example in the middle of an adjacency pair, or
when a speaker pauses in the middle of her speech.
Gaps are silences not attributable to any party, and
last until a speaker takes the initiative of continuing
the discussion. As an approximation of this distinc-
tion, we classified a silence that follows a question or
in the middle of somebody?s speech as a pause, and
any other silences as a gap. While the correlation be-
tween long silences and discourse boundaries seem
to be less pervasive in meetings than in other speech
corpora, we have noticed that some topic boundaries
are preceded (within some window) by numerous
gaps. However, we found little correlation between
pauses and topic boundaries.
Overlaps: we also analyzed the distribution of
overlapping speech by counting the average overlap
rate within some window. We noticed that, many
times, the beginning of segments are characterized
by having little overlapping speech.
Speaker change: we sometimes noticed a corre-
lation between topic boundaries and sudden changes
in speaker activity. For example, in Figure 2, it
is clear that the contribution of individual speakers
to the discussion can greatly change from one dis-
course unit to the next. We try to capture significant
changes in speakership by measuring the dissimilar-
ity between two analysis windows. For each poten-
tial boundary, we count for each speaker i the num-
ber of words that are uttered before (Li) and after
(Ri) the potential boundary (we limit our analysis
to a window of fixed size). The two distributions
are normalized to form two probability distributions
l and r, and significant changes of speakership are
detected by computing their Jensen-Shannon diver-
gence:
JS(l, r) = 12 [D(l||avgl,r) + D(r||avgl,r)]
where D(l||r) is the KL-divergence between the
two distributions.
Lexical cohesion: we also incorporated the lexi-
cal cohesion function computed by LCseg as a fea-
ture of the multi-source segmenter in a manner simi-
lar to the knowledge source combination performed
by (Beeferman et al, 1999) and (Tu?r et al, 2001).
Note that we use both the posterior estimate com-
puted by LCseg and the raw lexical cohesion func-
tion as features of the system.
5.3 Features: Selection and Combination
For every potential boundary Bi, the classifier ana-
lyzes features in a window surrounding Bi to decide
whether it is a topic boundary or not. It is generally
unclear what is the optimal window size and how
features should be analyzed. Windows of various
sizes can lead to different levels of prediction, and
in some cases, it might be more appropriate to only
extract features preceding or following Bi.
We avoided making arbitrary choices of parame-
ters; instead, for any feature F and a set F1, . . . , Fn
of possible ways to measure the feature (different
window sizes, different directions), we picked the Fi
that is in isolation the best predictor of topic bound-
aries (among F1, . . . , Fn). Table 4 presents for each
feature the analysis mode that is the most useful on
the training data.
5.4 Evaluation
We performed 25-fold cross-validation for evaluat-
ing the induced probabilistic classifier, computing
the average of Pk and WD on the held-out meet-
ings. Feature selection and decision rule learning
0 10 20 30
Figure 2: speaker activity in a meeting. Each row represent the speech activity of one speaker, utterance of
words being represented as black. Vertical lines represent topic shifts. The x-axis represents time.
Feature Tag Size (sec.) Side
Cue phrases CUE 5 both
Silence (gaps) SIL 30 left
Overlap? OVR 30 right
Speaker activity ACT 5 both
Lexical cohesion LC 30 both
?: the size of the window that was used to compute the
JS-divergence was also determined automatically.
Table 4: Parameters for feature analysis.
is always performed on sets of 24 meetings, while
the held-out data is used for testing. Table 5 gives
some examples of the type of rules that are learned.
The first rule states that if the value for the lexical
cohesion (LC) function is low at the current sen-
tence break, there is at least one CUE phrase, there
is less than three seconds of silence to the left of the
break,10 and a single speaker holds the floor for a
longer period of time than usual to the right of the
break, then we have a topic break. In general, we
found that the derived rules show that lexical cohe-
sion plays a stronger role than most other features
in determining topic breaks. Nonetheless, the quan-
titative results summarized in table 6, which corre-
spond to the average performance on the held-out
sets, show that the integration of conversational fea-
tures with the text-based segmenter outperforms ei-
ther alone.
6 Conclusions
We presented a domain-independent segmentation
algorithm for multi-party conversation that inte-
grates features based on content with features based
on form. The learned combination of features results
in a significant increase in accuracy over previous
10Note that rules are not always meaningful in isolation and
it is likely that a subordinate rule in the tree to this one would do
further tests on silence to determine if a topic boundary exists.
Condition Decision Conf.
LC ? 0.67,CUE ? 1,
OVR ? 1.20,SIL ? 3.42 yes 94.1
LC ? 0.35,SIL > 3.42,
OVR ? 4.55 yes 92.2
CUE ? 1,ACT > 0.1768,
OVR ? 1.20,LC ? 0.67 yes 91.6
. . .
default no
Table 5: A selection of the most useful rules learned
by C4.5rules along with their confidence levels.
Times for OVR and SIL are expressed in seconds.
Pk WD
feature-based 23.00% 25.47%
LCseg 31.91% 35.88%
U00 37.39% 40.43%
p-value 2.14e-04 3.30e-04
Table 6: Performance of the feature-based seg-
menter on the test data.
approaches to segmentation when applied to meet-
ings. Features based on form that are likely to in-
dicate topic shifts are automatically extracted from
speech. Content based features are computed by a
segmentation algorithm that utilizes a metric of lex-
ical cohesion and that performs as well as state-of-
the-art text-based segmentation techniques. It works
both with written and spoken texts. The text-based
segmentation approach alone, when applied to meet-
ings, outperforms all other segmenters, although the
difference is not statistically significant.
In future work, we would like to investigate the
effects of adding prosodic features, such as pitch
ranges, to our segmenter, as well as the effect of
using errorful speech recognition transcripts as op-
posed to manually transcribed utterances.
An implementation of our lexical cohesion seg-
menter is freely available for educational or research
purposes.11
Acknowledgments
We are grateful to Julia Hirschberg, Dan Ellis, Eliz-
abeth Shriberg, and Mari Ostendorf for their helpful
advice. We thank our ICSI project partners for grant-
ing us access to the meeting corpus and for useful
discussions. This work was funded under the NSF
project Mapping Meetings (IIS-012196).
References
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning,
34(1?3):177?210.
F. Choi. 2000. Advances in domain independent linear
text segmentation. In Proc. of NAACL?00.
W. Cochran. 1950. The comparison of percentages in
matched samples. Biometrika, 37:256?266.
B. Grosz and J. Hirschberg. 1992. Some intonational
characteristics of discourse structure. In Proc. of
ICSLP-92, pages 429?432.
B. Grosz and C. Sidner. 1986. Attention, intentions and
the structure of discourse. Computational Linguistics,
12(3).
M. Hajime, H. Takeo, and O. Manabu. 1998. Text seg-
mentation with multiple surface linguistic cues. In
COLING-ACL, pages 881?885.
M. Hearst. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of the ACL.
J. Hirschberg and D. Litman. 1994. Empirical studies
on the disambiguation of cue phrases. Computational
Linguistics, 19(3):501?530.
J. Hirschberg and C. Nakatani. 1996. A prosodic anal-
ysis of discourse segments in direction-giving mono-
logues. In Proc. of the ACL.
J. Hirschberg and C. Nakatani. 1998. Acoustic indicators
of topic segmentation. In Proc. of ICSLP.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. of ICASSP-03, Hong Kong (to appear).
11http://www.cs.columbia.edu/?galley/research.html
M.-Y. Kan, J. Klavans, and K. McKeown. 1998. Linear
segmentation and segment significance. In Proc. 6th
Workshop on Very Large Corpora (WVLC-98).
H. Kozima. 1993. Text segmentation based on similarity
between words. In Proc. of the ACL.
S. Levinson. 1983. Pragmatics. Cambridge University
Press.
D. Litman and R. Passonneau. 1995. Combining multi-
ple knowledge sources for discourse segmentation. In
Proc. of the ACL.
J. Morris and G. Hirst. 1991. Lexcial cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17:21?48.
C. Nakatani, J. Hirschberg, and B. Grosz. 1995. Dis-
course structure in spoken language: Studies on
speech corpora. In AAAI-95 Symposium on Empirical
Methods in Discourse Interpretation.
R. Passonneau and D. Litman. 1993. Intention-based
segmentation: Human reliability and correlation with
linguistic cues. In Proc. of the ACL.
R. Passonneau and D. Litman. 1997. Discourse seg-
mentation by human and automated means. Compu-
tational Linguistics, 23(1):103?139.
L. Pevzner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmenta-
tion. Computational Linguistics, 28 (1):19?36.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Machine Learning. Morgan Kaufmann.
J. Reynar. 1994. An automatic method of finding topic
boundaries. In Proc. of the ACL.
J. Reynar. 1999. Statistical models for topic segmenta-
tion. In Proc. of the ACL.
G. Salton and C. Buckley. 1988. Term weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management, 24(5):513?523.
G. Tu?r, D. Hakkani-Tu?r, A. Stolcke, and E. Shriberg.
2001. Integrating prosodic and lexical cues for auto-
matic topic segmentation. Computational Linguistics,
27(1):31?57.
M. Utiyama and H. Isahara. 2001. A statistical model
for domain-independent text segmentation. In Proc. of
the ACL.
J. Xu and B. Croft. 1998. Corpus-based stemming using
cooccurrence of word variants. ACM Transactions on
Information Systems, 16(1):61?81.
Identifying Agreement and Disagreement in Conversational Speech:
Use of Bayesian Networks to Model Pragmatic Dependencies
Michel Galley   , Kathleen McKeown   , Julia Hirschberg   ,
  Columbia University
Computer Science Department
1214 Amsterdam Avenue
New York, NY 10027, USA

galley,kathy,julia  @cs.columbia.edu
and Elizabeth Shriberg 
 SRI International
Speech Technology and Research Laboratory
333 Ravenswood Avenue
Menlo Park, CA 94025, USA
ees@speech.sri.com
Abstract
We describe a statistical approach for modeling
agreements and disagreements in conversational in-
teraction. Our approach first identifies adjacency
pairs using maximum entropy ranking based on a
set of lexical, durational, and structural features that
look both forward and backward in the discourse.
We then classify utterances as agreement or dis-
agreement using these adjacency pairs and features
that represent various pragmatic influences of pre-
vious agreement or disagreement on the current ut-
terance. Our approach achieves 86.9% accuracy, a
4.9% increase over previous work.
1 Introduction
One of the main features of meetings is the occur-
rence of agreement and disagreement among par-
ticipants. Often meetings include long stretches
of controversial discussion before some consensus
decision is reached. Our ultimate goal is auto-
mated summarization of multi-participant meetings
and we hypothesize that the ability to automatically
identify agreement and disagreement between par-
ticipants will help us in the summarization task.
For example, a summary might resemble minutes of
meetings with major decisions reached (consensus)
along with highlighted points of the pros and cons
for each decision. In this paper, we present a method
to automatically classify utterances as agreement,
disagreement, or neither.
Previous work in automatic identification of
agreement/disagreement (Hillard et al, 2003)
demonstrates that this is a feasible task when var-
ious textual, durational, and acoustic features are
available. We build on their approach and show
that we can get an improvement in accuracy when
contextual information is taken into account. Our
approach first identifies adjacency pairs using maxi-
mum entropy ranking based on a set of lexical, dura-
tional and structural features that look both forward
and backward in the discourse. This allows us to ac-
quire, and subsequently process, knowledge about
who speaks to whom. We hypothesize that prag-
matic features that center around previous agree-
ment between speakers in the dialog will influence
the determination of agreement/disagreement. For
example, if a speaker disagrees with another per-
son once in the conversation, is he more likely to
disagree with him again? We model context using
Bayesian networks that allows capturing of these
pragmatic dependencies. Our accuracy for classify-
ing agreements and disagreements is 86.9%, which
is a 4.9% improvement over (Hillard et al, 2003).
In the following sections, we begin by describ-
ing the annotated corpus that we used for our ex-
periments. We then turn to our work on identify-
ing adjacency pairs. In the section on identification
of agreement/disagreement, we describe the contex-
tual features that we model and the implementation
of the classifier. We close with a discussion of future
work.
2 Corpus
The ICSI Meeting corpus (Janin et al, 2003) is
a collection of 75 meetings collected at the In-
ternational Computer Science Institute (ICSI), one
among the growing number of corpora of human-
to-human multi-party conversations. These are nat-
urally occurring, regular weekly meetings of vari-
ous ICSI research teams. Meetings in general run
just under an hour each; they have an average of 6.5
participants.
These meetings have been labeled with adja-
cency pairs (AP), which provide information about
speaker interaction. They reflect the structure of
conversations as paired utterances such as question-
answer and offer-acceptance, and their labeling is
used in our work to determine who are the ad-
dressees in agreements and disagreements. The an-
notation of the corpus with adjacency pairs is de-
scribed in (Shriberg et al, 2004; Dhillon et al,
2004).
Seven of those meetings were segmented into
spurts, defined as periods of speech that have no
pauses greater than .5 second, and each spurt was
labeled with one of the four categories: agreement,
disagreement, backchannel, and other.1 We used
spurt segmentation as our unit of analysis instead of
sentence segmentation, because our ultimate goal is
to build a system that can be fully automated, and
in that respect, spurt segmentation is easy to ob-
tain. Backchannels (e.g. ?uhhuh? and ?okay?) were
treated as a separate category, since they are gener-
ally used by listeners to indicate they are following
along, while not necessarily indicating agreement.
The proportion of classes is the following: 11.9%
are agreements, 6.8% are disagreements, 23.2% are
backchannels, and 58.1% are others. Inter-labeler
reliability estimated on 500 spurts with 2 labelers
was considered quite acceptable, since the kappa
coefficient was .63 (Cohen, 1960).
3 Adjacency Pairs
3.1 Overview
Adjacency pairs (AP) are considered fundamental
units of conversational organization (Schegloff and
Sacks, 1973). Their identification is central to our
problem, since we need to know the identity of
addressees in agreements and disagreements, and
adjacency pairs provide a means of acquiring this
knowledge. An adjacency pair is said to consist of
two parts (later referred to as A and B) that are or-
dered, adjacent, and produced by different speakers.
The first part makes the second one immediately rel-
evant, as a question does with an answer, or an offer
does with an acceptance. Extensive work in con-
versational analysis uses a less restrictive definition
of adjacency pair that does not impose any actual
adjacency requirement; this requirement is prob-
lematic in many respects (Levinson, 1983). Even
when APs are not directly adjacent, the same con-
straints between pairs and mechanisms for select-
ing the next speaker remain in place (e.g. the case
of embedded question and answer pairs). This re-
laxation on a strict adjacency requirement is partic-
ularly important in interactions of multiple speak-
ers since other speakers have more opportunities to
insert utterances between the two elements of the
AP construction (e.g. interrupted, abandoned or ig-
nored utterances; backchannels; APs with multiple
second elements, e.g. a question followed by an-
swers of multiple speakers).2
Information provided by adjacency pairs can be
used to identify the target of an agreeing or dis-
agreeing utterance. We define the problem of AP
1Part of these annotated meetings were provided by the au-
thors of (Hillard et al, 2003).
2The percentage of APs labeled in our data that have non-
contiguous parts is about 21%.
identification as follows: given the second element
(B) of an adjacency pair, determine who is the
speaker of the first element (A). A quite effective
baseline algorithm is to select as speaker of utter-
ance A the most recent speaker before the occur-
rence of utterance B. This strategy selects the right
speaker in 79.8% of the cases in the 50 meetings that
were annotated with adjacency pairs. The next sub-
section describes the machine learning framework
used to significantly outperform this already quite
effective baseline algorithm.
3.2 Maximum Entropy Ranking
We view the problem as an instance of statisti-
cal ranking, a general machine learning paradigm
used for example in statistical parsing (Collins,
2000) and question answering (Ravichandran et al,
2003).3 The problem is to select, given a set of  
possible candidates 			
 (in our case, po-
tential A speakers), the one candidate  that maxi-
mizes a given conditional probability distribution.
We use maximum entropy modeling (Berger et
al., 1996) to directly model the conditional proba-
bility  Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207?214,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Creation of Domain Templates
Elena Filatova*, Vasileios Hatzivassiloglou? and Kathleen McKeown*
*Department of Computer Science
Columbia University
{filatova,kathy}@cs.columbia.edu
?Department of Computer Science
The University of Texas at Dallas
vh@hlt.utdallas.edu
Abstract
Recently, many Natural Language Processing
(NLP) applications have improved the quality of
their output by using various machine learning tech-
niques to mine Information Extraction (IE) patterns
for capturing information from the input text. Cur-
rently, to mine IE patterns one should know in ad-
vance the type of the information that should be
captured by these patterns. In this work we pro-
pose a novel methodology for corpus analysis based
on cross-examination of several document collec-
tions representing different instances of the same
domain. We show that this methodology can be
used for automatic domain template creation. As the
problem of automatic domain template creation is
rather new, there is no well-defined procedure for
the evaluation of the domain template quality. Thus,
we propose a methodology for identifying what in-
formation should be present in the template. Using
this information we evaluate the automatically cre-
ated domain templates through the text snippets re-
trieved according to the created templates.
1 Introduction
Open-ended question-answering (QA) systems
typically produce a response containing a vari-
ety of specific facts proscribed by the question
type. A biography, for example, might contain the
date of birth, occupation, or nationality of the per-
son in question (Duboue and McKeown, 2003;
Zhou et al, 2004; Weischedel et al, 2004; Fila-
tova and Prager, 2005). A definition may contain
the genus of the term and characteristic attributes
(Blair-Goldensohn et al, 2004). A response to a
question about a terrorist attack might include the
event, victims, perpetrator and date as the tem-
plates designed for the Message Understanding
Conferences (Radev and McKeown, 1998; White
et al, 2001) predicted. Furthermore, the type of in-
formation included varies depending on context. A
biography of an actor would include movie names,
while a biography of an inventor would include the
names of inventions. A description of a terrorist
event in Latin America in the eighties is different
from the description of today?s terrorist events.
How does one determine what facts are im-
portant for different kinds of responses? Often
the types of facts that are important are hand en-
coded ahead of time by a human expert (e.g., as
in the case of MUC templates). In this paper, we
present an approach that allows a system to learn
the types of facts that are appropriate for a par-
ticular response. We focus on acquiring fact-types
for events, automatically producing a template that
can guide the creation of responses to questions
requiring a description of an event. The template
can be tailored to a specific time period or coun-
try simply by changing the document collections
from which learning takes place.
In this work, a domain is a set of events of a par-
ticular type; earthquakes and presidential elections
are two such domains. Domains can be instanti-
ated by several instances of events of that type
(e.g., the earthquake in Japan in October 2004, the
earthquake in Afghanistan in March 2002, etc.).1
The granularity of domains and instances can be
altered by examining data at different levels of de-
tail, and domains can be hierarchically structured.
An ideal template is a set of attribute-value pairs,
with the attributes specifying particular functional
roles important for the domain events.
In this paper we present a method of domain-
independent on-the-fly template creation. Our
method is completely automatic. As input it re-
quires several document collections describing do-
main instances. We cross-examine the input in-
stances, we identify verbs important for the major-
ity of instances and relationships containing these
verbs. We generalize across multiple domain in-
stances to automatically determine which of these
relations should be used in the template. We re-
port on data collection efforts and results from four
domains. We assess how well the automatically
produced templates satisfy users? needs, as man-
ifested by questions collected for these domains.
1Unfortunately, NLP terminology is not standardized
across different tasks. Two NLP tasks most close to our
research are Topic Detection and Tracking (TDT) (Fiscus
et al, 1999) and Information Extraction (IE) (Marsh and
Perzanowski, 1997). In TDT terminology, our domains are
topics and our instances are events. In IE terminology, our
domains are scenarios and our domain templates are scenario
templates.
207
2 Related Work
Our system automatically generates a template
that captures the generally most important infor-
mation for a particular domain and is reusable
across multiple instances of that domain. Decid-
ing what slots to include in the template, and what
restrictions to place on their potential fillers, is
a knowledge representation problem (Hobbs and
Israel, 1994). Templates were used in the main
IE competitions, the Message Understanding Con-
ferences (Hobbs and Israel, 1994; Onyshkevych,
1994; Marsh and Perzanowski, 1997). One of the
recent evaluations, ACE,2 uses pre-defined frames
connecting event types (e.g., arrest, release) to a
set of attributes. The template construction task
was not addressed by the participating systems.
The domain templates were created manually by
experts to capture the structure of the facts sought.
Although templates have been extensively used
in information extraction, there has been little
work on their automatic design. In the Concep-
tual Case Frame Acquisition project (Riloff and
Schmelzenbach, 1998), extraction patterns, a do-
main semantic lexicon, and a list of conceptual
roles and associated semantic categories for the
domain are used to produce multiple-slot case
frames with selectional restrictions. The system
requires two sets of documents: those relevant to
the domain and those irrelevant. Our approach
does not require any domain-specific knowledge
and uses only corpus-based statistics.
The GISTexter summarization sys-
tem (Harabagiu and Maiorano, 2002) used
statistics over an arbitrary document collection
together with semantic relations from WordNet.
The created templates heavily depend on the top-
ical relations encoded in WordNet. The template
models an input collection of documents. If there
is only one domain instance described in the input
than the template is created for this particular
instance rather than for a domain. In our work,
we learn domain templates by cross-examining
several collections of documents on the same
topic, aiming for a general domain template. We
rely on relations cross-mentioned in different
instances of the domain to automatically prioritize
roles and relationships for selection.
Topic Themes (Harabagiu and La?ca?tus?u, 2005)
used for multi-document summarization merge
various arguments corresponding to the same se-
2http://www.nist.gov/speech/tests/ace/index.htm
mantic roles for the semantically identical verb
phrases (e.g., arrests and placed under arrest).
Atomic events also model an input document
collection (Filatova and Hatzivassiloglou, 2003)
and are created according to the statistics col-
lected for co-occurrences of named entity pairs
linked through actions. GISTexter, atomic events,
and Topic Themes were used for modeling a col-
lection of documents rather than a domain.
In other closely related work, Sudo et al (2003)
use frequent dependency subtrees as measured by
TF*IDF to identify named entities and IE patterns
important for a given domain. The goal of their
work is to show how the techniques improve IE
pattern acquisition. To do this, Sudo et al con-
strain the retrieval of relevant documents for a
MUC scenario and then use unsupervised learn-
ing over descriptions within these documents that
match specific types of named entities (e.g., Ar-
resting Agency, Charge), thus enabling learning
of patterns for specific templates (e.g., the Ar-
rest scenario). In contrast, the goal of our work
is to show how similar techniques can be used to
learn what information is important for a given
domain or event and thus, should be included
into the domain template. Our approach allows,
for example, learning that an arrest along with
other events (e.g., attack) is often part of a ter-
rorist event. We do not assume any prior knowl-
edge about domains. We demonstrate that frequent
subtrees can be used not only to extract specific
named entities for a given scenario but also to
learn domain-important relations. These relations
link domain actions and named entities as well as
general nouns and words belonging to other syn-
tactic categories.
Collier (1998) proposed a fully automatic
method for creating templates for information ex-
traction. The method relies on Luhn?s (1957) idea
of locating statistically significant words in a cor-
pus and uses those to locate the sentences in which
they occur. Then it extracts Subject-Verb-Object
patterns in those sentences to identify the most
important interactions in the input data. The sys-
tem was constructed to create MUC templates for
terrorist attacks. Our work also relies on corpus
statistics, but we utilize arbitrary syntactic pat-
terns and explicitly use multiple domain instances.
Keeping domain instances separated, we cross-
examine them and estimate the importance of a
particular information type in the domain.
208
3 Our Approach to Template Creation
After reading about presidential elections in dif-
ferent countries on different years, a reader has a
general picture of this process. Later, when read-
ing about a new presidential election, the reader al-
ready has in her mind a set of questions for which
she expects answers. This process can be called
domain modeling. The more instances of a partic-
ular domain a person has seen, the better under-
standing she has about what type of information
should be expected in an unseen collection of doc-
uments discussing a new instance of this domain.
Thus, we propose to use a set of document col-
lections describing different instances within one
domain to learn the general characteristics of this
domain. These characteristics can be then used to
create a domain template. We test our system on
four domains: airplane crashes, earthquakes, pres-
idential elections, terrorist attacks.
4 Data Description
4.1 Training Data
To create training document collections we used
BBC Advanced Search3 and submitted queries of
the type ?domain title + country?. For example,
??presidential election? USA?.
In addition, we used BBC?s Advanced Search
date filter to constrain the results to different date
periods of interest. For example, we used known
dates of elections and allowed a search for articles
published up to five days before or after each such
date. At the same time for the terrorist attacks or
earthquakes domain the time constraints we sub-
mitted were the day of the event plus ten days.
Thus, we identify several instances for each of
our four domains, obtaining a document collec-
tion for each instance. E.g., for the earthquake do-
main we collected documents on the earthquakes
in Afghanistan (March 25, 2002), India (January
26, 2001), Iran (December 26, 2003), Japan (Oc-
tober 26, 2004), and Peru (June 23, 2001). Using
this procedure we retrieve training document col-
lections for 9 instances of airplane crashes, 5 in-
stances of earthquakes, 13 instances of presiden-
tial elections, and 6 instances of terrorist attacks.
4.2 Test Data
To test our system, we used document clusters
from the Topic Detection and Tracking (TDT) cor-
3http://news.bbc.co.uk/shared/bsp/search2/
advanced/news_ifs.stm
pus (Fiscus et al, 1999). Each TDT topic has a
topic label, such as Accidents or Natural Disas-
ters.4 These categories are broader than our do-
mains. Thus, we manually filtered the TDT topics
relevant to our four training domains (e.g., Acci-
dents matching Airplane Crashes). In this way, we
obtained TDT document clusters for 2 instances
of airplane crashes, 3 instances of earthquakes, 6
instances of presidential elections and 3 instances
of terrorist attacks. The number of the documents
corresponding to the instances varies greatly (from
two documents for one of the earthquakes up to
156 documents for one of the terrorist attacks).
This variation in the number of documents per
topic is typical for the TDT corpus. Many of the
current approaches of domain modeling collapse
together different instances and make the decision
on what information is important for a domain
based on this generalized corpus (Collier, 1998;
Barzilay and Lee, 2003; Sudo et al, 2003). We,
on the other hand, propose to cross-examine these
instances keeping them separated. Our goal is to
eliminate dependence on how well the corpus is
balanced and to avoid the possibility of greater
impact on the domain template of those instances
which have more documents.
5 Creating Templates
In this work we build domain templates around
verbs which are estimated to be important for the
domains. Using verbs as the starting point we
identify semantic dependencies within sentences.
In contrast to deep semantic analysis (Fillmore
and Baker, 2001; Gildea and Jurafsky, 2002; Prad-
han et al, 2004; Harabagiu and La?ca?tus?u, 2005;
Palmer et al, 2005) we rely only on corpus statis-
tics. We extract the most frequent syntactic sub-
trees which connect verbs to the lexemes used in
the same subtrees. These subtrees are used to cre-
ate domain templates.
For each of the four domains described in Sec-
tion 4, we automatically create domain templates
using the following algorithm.
Step 1: Estimate what verbs are important for
the domain under investigation. We initiate our
algorithm by calculating the probabilities for all
the verbs in the document collection for one do-
main ? e.g., the collection containing all the in-
stances in the domain of airplane crashes. We
4In our experiments we analyze TDT topics used in
TDT-2 and TDT-4 evaluations.
209
discard those verbs that are stop words (Salton,
1971). To take into consideration the distribution
of a verb among different instances of the domain,
we normalize this probability by its VIF value
(verb instance frequency), specifying in how many
domain instances this verb appears.
Score(vbi) =
countvbi?
vbj?comb coll countvbj
? VIF(vbi) (1)
VIF(vbi) = # of domain instances containing vbi# of all domain instances (2)
These verbs are estimated to be the most impor-
tant for the combined document collection for all
the domain instances. Thus, we build the domain
template around these verbs. Here are the top ten
verbs for the terrorist attack domain:
killed, told, found, injured, reported,
happened, blamed, arrested, died, linked.
Step 2: Parse those sentences which contain the
top 50 verbs. After we identify the 50 most impor-
tant verbs for the domain under analysis, we parse
all the sentences in the domain document collec-
tion containing these verbs with the Stanford syn-
tactic parser (Klein and Manning, 2002).
Step 3: Identify most frequent subtrees containing
the top 50 verbs. A domain template should con-
tain not only the most important actions for the do-
main, but also the entities that are linked to these
actions or to each other through these actions. The
lexemes referring to such entities can potentially
be used within the domain template slots. Thus,
we analyze those portions of the syntactic trees
which contain the verbs themselves plus other lex-
emes used in the same subtrees as the verbs. To do
this we use FREQuent Tree miner.5 This software
is an implementation of the algorithm presented
by (Abe et al, 2002; Zaki, 2002), which extracts
frequent ordered subtrees from a set of ordered
trees. Following (Sudo et al, 2003) we are inter-
ested only in the lexemes which are near neighbors
of the most frequent verbs. Thus, we look only for
those subtrees which contain the verbs themselves
and from four to ten tree nodes, where a node is
either a syntactic tag or a lexeme with its tag. We
analyze not only NPs which correspond to the sub-
ject or object of the verb, but other syntactic con-
stituents as well. For example, PPs can potentially
link the verb to locations or dates, and we want to
include this information into the template. Table 1
contains a sample of subtrees for the terrorist at-
tack domain mined from the sentences containing
5http://chasen.org/?taku/software/freqt/
nodes subtree
8 (SBAR(S(VP(VBD killed)(NP(QP(IN at))(NNS people)))))
8 (SBAR(S(VP(VBD killed)(NP(QP(JJS least))(NNS people)))))
5 (VP(ADVP)(VBD killed)(NP(NNS people)))
6 (VP(VBD killed)(NP(ADJP(JJ many))(NNS people)))
5 (VP(VP(VBD killed)(NP(NNS people))))
7 (VP(ADVP(NP))(VBD killed)(NP(CD 34)(NNS people)))
6 (VP(ADVP)(VBD killed)(NP(CD 34)(NNS people)))
Table 1: Sample subtrees for the terrorist attack domain.
the verb killed. The first column of Table 1 shows
how many nodes are in the subtree.
Step 4: Substitute named entities with their re-
spective tags. We are interested in analyzing a
whole domain, not just an instance of this do-
main. Thus, we substitute all the named entities
with their respective tags, and all the exact num-
bers with the tag NUMBER. We speculate that sub-
trees similar to those presented in Table 1 can
be extracted from a document collection repre-
senting any instance of a terrorist attack, with the
only difference being the exact number of causal-
ities. Later, however, we analyze the domain in-
stances separately to identity information typi-
cal for the domain. The procedure of substitut-
ing named entities with their respective tags previ-
ously proved to be useful for various tasks (Barzi-
lay and Lee, 2003; Sudo et al, 2003; Filatova and
Prager, 2005). To get named entity tags we used
BBN?s IdentiFinder (Bikel et al, 1999).
Step 5: Merge together the frequent subtrees. Fi-
nally, we merge together those subtrees which
are identical according to the information encoded
within them. This is a key step in our algorithm
which allows us to bring together subtrees from
different instances of the same domain. For exam-
ple, the information rendered by all the subtrees
from the bottom part of Table 1 is identical. Thus,
these subtrees can be merged into one which con-
tains the longest common pattern:
(VBD killed)(NP(NUMBER)(NNS people))
After this merging procedure we keep only those
subtrees for which each of the domain instances
has at least one of the subtrees from the initial set
of subtrees. This subtree should be used in the in-
stance at least twice. At this step, we make sure
that we keep in the template only the information
which is generally important for the domain rather
than only for a fraction of instances in this domain.
We also remove all the syntactic tags as we want
to make this pattern as general for the domain as
possible. A pattern without syntactic dependencies
contains a verb together with a prospective tem-
210
plate slot corresponding to this verb:
killed: (NUMBER) (NNS people)
In the above example, the prospective template
slots appear after the verb killed. In other cases the
domain slots appear in front of the verb. Two ex-
amples of such slots, for the presidential election
and earthquake domains, are shown below:
(PERSON) won
(NN earthquake) struck
The above examples show that it is not enough to
analyze only named entities, general nouns con-
tain important information as well. We term the
structure consisting of a verb together with the as-
sociated slots a slot structure. Here is a part of the
slot structure we get for the verb killed after cross-
examination of the terrorist attack instances:
killed (NUMBER) (NNS people)
(PERSON) killed
(NN suicide) killed
Slot structures are similar to verb frames, which
are manually created for the PropBank annota-
tion (Palmer et al, 2005).6 An example of the
PropBank frame for the verb to kill is:
Roleset kill.01 ?cause to die?:
Arg0:killerArg1:corpseArg2:instrument
The difference between the slot structure extracted
by our algorithm and the PropBank frame slots is
that the frame slots assign a semantic role to each
slot, while our algorithm gives either the type of
the named entity that should fill in this slot or puts
a particular noun into the slot (e.g., ORGANIZA-
TION, earthquake, people). An ideal domain tem-
plate should include semantic information but this
problem is outside of the scope of this paper.
Step 6: Creating domain templates. After we get
all the frequent subtrees containing the top 50 do-
main verbs, we merge all the subtrees correspond-
ing to the same verb and create a slot structure for
every verb as described in Step 5. The union of
such slot structures created for all the important
verbs in the domain is called the domain template.
From the created templates we remove the slots
which are used in all the domains. For example,
(PERSON) told.2
The presented algorithm can be used to create a
template for any domain. It does not require pre-
defined domain or world knowledge. We learn do-
main templates from cross-examining document
collections describing different instances of the
domain of interest.
6http://www.cs.rochester.edu/?gildea/Verbs/
6 Evaluation
The task we deal with is new and there is no well-
defined and standardized evaluation procedure for
it. Sudo et al (2003) evaluated how well their
IE patterns captured named entities of three pre-
defined types. We are interested in evaluating how
well we capture the major actions as well as their
constituent parts.
There is no set of domain templates which are
built according to a unique set of principles against
which we could compare our automatically cre-
ated templates. Thus, we need to create a gold
standard. In Section 6.1, we describe how the gold
standard is created. Then, in Section 6.2, we eval-
uate the quality of the automatically created tem-
plates by extracting clauses corresponding to the
templates and verifying how many answers from
the questions in the gold standard are answered by
the extracted clauses.
6.1 Stage 1. Information Included into
Templates: Interannotator Agreement
To create a gold standard we asked people to create
a list of questions which indicate what is important
for the domain description. Our decision to aim
for the lists of questions and not for the templates
themselves is based on the following considera-
tions: first, not all of our subjects are familiar with
the field of IE and thus, do not necessarily know
what an IE template is; second, our goal for this
evaluation is to estimate interannotator agreement
for capturing the important aspects for the domain
and not how well the subjects agree on the tem-
plate structure.
We asked our subjects to think of their expe-
rience of reading newswire articles about various
domains.7 Based on what they remember from this
experience, we asked them to come up with a list
of questions about a particular domain. We asked
them to come up with at most 20 questions cover-
ing the information they will be looking for given
an unseen news article about a new event in the
domain. We did not give them any input informa-
tion about the domain but allowed them to use any
sources to learn more information about the do-
main.
We had ten subjects, each of which created one
list of questions for one of the four domains under
7We thank Rod Adams, Cosmin-Adrian Bejan, Sasha
Blair-Goldensohn, Cyril Cerovic, David Elson, David Evans,
Ovidiu Fortu, Agustin Gravano, Lokesh Shresta, John Yundt-
Pacheco and Kapil Thadani for the submitted questions.
211
Jaccard metric
Domain subj1 and subj1 and subj2 and
subj2 (and subj3) MUC MUC
Airplane crash 0.54 - -
Earthquake 0.68 - -
Presidential Election 0.32 - -
Terrorist Attack 0.50 0.63 0.59
Table 2: Creating gold standard. Jaccard metric values for in-
terannotator agreement.
analysis. Thus, for the earthquake and terrorist at-
tack domains we got two lists of questions; for the
airplane crash and presidential election domains
we got three lists of questions.
After the questions lists were created we studied
the agreement among annotators on what infor-
mation they consider is important for the domain
and thus, should be included in the template. We
matched the questions created by different anno-
tators for the same domain. For some of the ques-
tions we had to make a judgement call on whether
it is a match or not. For example, the following
question created by one of the annotators for the
earthquake domain was:
Did the earthquake occur in a well-known area
for earthquakes (e.g. along the San Andreas
fault), or in an unexpected location?
We matched this question to the following three
questions created by the other annotator:
What is the geological localization?
Is it near a fault line?
Is it near volcanoes?
Usually, the degree of interannotator agreement
is estimated using Kappa. For this task, though,
Kappa statistics cannot be used as they require
knowledge of the expected or chance agreement,
which is not applicable to this task (Fleiss et al,
1981). To measure interannotator agreement we
use the Jaccard metric, which does not require
knowledge of the expected or chance agreement.
Table 2 shows the values of Jaccard metric for in-
terannotator agreement calculated for all four do-
mains. Jaccard metric values are calculated as
Jaccard(domaind) = |QS
d
i ? QSdj |
|QSdi ? QSdj |
(3)
where QSdi and QSdj are the sets of questions cre-
ated by subjects i and j for domain d. For the air-
plane crash and presidential election domains we
averaged the three pairwise Jaccard metric values.
The scores in Table 2 show that for some do-
mains the agreement is quite high (e.g., earth-
quake), while for other domains (e.g., presiden-
tial election) it is twice as low. This difference
in scores can be explained by the complexity of
the domains and by the differences in understand-
ing of these domains by different subjects. The
scores for the presidential election domain are pre-
dictably low as in different countries the roles of
presidents are very different: in some countries the
president is the head of the government with a lot
of power, while in other countries the president is
merely a ceremonial figure. In some countries the
presidents are elected by general voting while in
other countries, the presidents are elected by par-
liaments. These variations in the domain cause the
subjects to be interested in different issues of the
domain. Another issue that might influence the in-
terannotator agreement is the distribution of the
presidential election process in time. For example,
one of our subjects was clearly interested in the
pre-voting situation, such as debates between the
candidates, while another subject was interested
only in the outcome of the presidential election.
For the terrorist attack domain we also com-
pared the lists of questions we got from our sub-
jects with the terrorist attack template created by
experts for the MUC competition. In this template
we treated every slot as a separate question, ex-
cluding the first two slots which captured informa-
tion about the text from which the template fillers
were extracted and not about the domain. The re-
sults for this comparison are included in Table 2.
Differences in domain complexity were stud-
ied by IE researchers. Bagga (1997) suggests a
classification methodology to predict the syntac-
tic complexity of the domain-related facts. Hut-
tunen et al (2002) analyze how component sub-
events of the domain are linked together and dis-
cuss the factors which contribute to the domain
complexity.
6.2 Stage 2. Quality of the Automatically
Created Templates
In section 6.1 we showed that not all the domains
are equal. For some of the domains it is much eas-
ier to come to a consensus about what slots should
be present in the domain template than for others.
In this section we describe the evaluation of the
four automatically created templates.
Automatically created templates consist of slot
structures and are not easily readable by human
annotators. Thus, instead of direct evaluation of
the template quality, we evaluate the clauses ex-
tracted according to the created templates and
212
check whether these clauses contain the answers
to the questions created by the subjects during the
first stage of the evaluation. We extract the clauses
corresponding to the test instances according to
the following procedure:
1. Identify all the simple clauses in the docu-
ments corresponding to a particular test in-
stance (respective TDT topic). For example,
for the sentence
Her husband, Robert, survived Thursday?s
explosion in a Yemeni harbor that killed at
least six crew members and injured 35.
only one part is output:
that killed at least six crew members and
injured 35
2. For every domain template slot check all the
simple clauses in the instance (TDT topic)
under analysis. Find the shortest clause (or
sequence of clauses) which includes both the
verb and other words extracted for this slot in
their respective order. Add this clause to the
list of extracted clauses unless this clause has
been already added to this list.
3. Keep adding clauses to the list of extracted
clauses till all the template slots are analyzed
or the size of the list exceeds 20 clauses.
The key step in the above algorithm is Step 2. By
choosing the shortest simple clause or sequence
of simple clauses corresponding to a particular
template slot, we reduce the possibility of adding
more information to the output than is necessary
to cover each particular slot.
In Step 3 we keep only the first twenty clauses
so that the length of the output which potentially
contains an answer to the question of interest is not
larger than the number of questions provided by
each subject. The templates are created from the
slot structures extracted for the top 50 verbs. The
higher the estimated score of the verb (Eq. 1) for
the domain the closer to the top of the template the
slot structure corresponding to this verb will be.
We assume that the important information is more
likely to be covered by the slot structures that are
placed near the top of the template.
The evaluation results for the automatically cre-
ated templates are presented in Figure 1. We cal-
culate what average percentage of the questions is
covered by the outputs created according to the
domain templates. For every domain, we present
the percentage of the covered questions separately
for each annotator and for the intersection of ques-
tions (Section 6.1).
0.00%10.00%
20.00%30.00%
40.00%50.00%
60.00%70.00%
80.00%
Attack Earthquake Presidentialelection Plane crash
IntersectSubj1Subj2Subj3
Figure 1: Evaluation results.
For the questions common for all the annota-
tors we capture about 70% of the answers for
three out of four domains. After studying the re-
sults we noticed that for the earthquake domain
some questions did not result in a template slot
and thus, could not be covered by the extracted
clauses. Here are two of such questions:
Is it near a fault line?
Is it near volcanoes?
According to the template creation procedure,
which is centered around verbs, the chances that
extracted clauses would contain answers to these
questions are low. Indeed, only one of the three
sentence sets extracted for the three TDT earth-
quake topics contain an answer to one of these
questions.
Poor results for the presidential election domain
could be predicted from the Jaccard metric value
for interannotator agreement (Table 2). There is
considerable discrepancy in the questions created
by human annotators which can be attributed to the
great variation in the presidential election domain
itself. It must be also noted that most of the ques-
tions created for the presidential election domain
were clearly referring to the democratic election
procedure, while some of the TDT topics catego-
rized as Elections were about either election fraud
or about opposition taking over power without the
formal resignation of the previous president.
Overall, this evaluation shows that using au-
tomatically created domain templates we extract
sentences which contain a substantial part of the
important information expressed in questions for
that domain. For those domains which have small
diversity our coverage can be significantly higher.
7 Conclusions
In this paper, we presented a robust method for
data-driven discovery of the important fact-types
213
for a given domain. In contrast to supervised meth-
ods, the fact-types are not pre-specified. The re-
sulting slot structures can subsequently be used
to guide the generation of responses to questions
about new instances of the same domain. Our ap-
proach features the use of corpus statistics derived
from both lexical and syntactic analysis across
documents. A comparison of our system output
for four domains of interest shows that our ap-
proach can reliably predict the majority of infor-
mation that humans have indicated are of interest.
Our method is flexible: analyzing document col-
lections from different time periods or locations,
we can learn domain descriptions that are tailored
to those time periods and locations.
Acknowledgements. We would like to thank Re-
becca Passonneau and Julia Hirschberg for the
fruitful discussions at the early stages of this work;
Vasilis Vassalos for his suggestions on the eval-
uation instructions; Michel Galley, Agustin Gra-
vano, Panagiotis Ipeirotis and Kapil Thadani for
their enormous help with evaluation.
This material is based upon work supported
in part by the Advanced Research Devel-
opment Agency (ARDA) under Contract No.
NBCHC040040 and in part by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023. Any opinions,
findings and conclusions expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of ARDA and DARPA.
References
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura,
and Setsuo Arikawa. 2002. Optimized substructure dis-
covery for semi-structured data. In Proc. of PKDD.
Amit Bagga. 1997. Analyzing the complexity of a domain
with respect to an Information Extraction task. In Proc.
7th MUC.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of HLT/NAACL.
Daniel Bikel, Richard Schwartz, and Ralph Weischedel.
1999. An algorithm that learns what?s in a name. Ma-
chine Learning Journal Special Issue on Natural Lan-
guage Learning, 34:211?231.
Sasha Blair-Goldensohn, Kathleen McKeown, and An-
drew Hazen Schlaikjer, 2004. Answering Definitional
Questions: A Hybrid Approach. AAAI Press.
Robin Collier. 1998. Automatic Template Creation for Infor-
mation Extraction. Ph.D. thesis, University of Sheffield.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural language
generation. In Proc. of EMNLP.
Elena Filatova and Vasileios Hatzivassiloglou. 2003.
Domain-independent detection, extraction, and labeling of
atomic events. In Proc. of RANLP.
Elena Filatova and John Prager. 2005. Tell me what you do
and I?ll tell you what you are: Learning occupation-related
activities for biographies. In Proc. of EMNLP/HLT.
Charles Fillmore and Collin Baker. 2001. Frame semantics
for text understanding. In Proc. of WordNet and Other
Lexical Resources Workshop, NAACL.
Jon Fiscus, George Doddington, John Garofolo, and Alvin
Martin. 1999. NIST?s 1998 topic detection and tracking
evaluation (TDT2). In Proc. of the 1999 DARPA Broad-
cast News Workshop, pages 19?24.
Joseph Fleiss, Bruce Levin, and Myunghee Cho Paik, 1981.
Statistical Methods for Rates and Proportions. J. Wiley.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sanda Harabagiu and Finley La?ca?tus?u. 2005. Topic themes
for multi-document summarization. In Proc. of SIGIR.
Sanda Harabagiu and Steven Maiorano. 2002. Multi-docu-
ment summarization with GISTexter. In Proc. of LREC.
Jerry Hobbs and David Israel. 1994. Principles of template
design. In Proc. of the HLT Workshop.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Complexity of event structure in IE scenarios. In
Proc. of COLING.
Dan Klein and Christopher Manning. 2002. Fast exact infer-
ence with a factored model for natural language parsing.
In Proc. of NIPS.
Hans Luhn. 1957. A statistical approach to mechanized en-
coding and searching of literary information. IBM Journal
of Research and Development, 1:309?317.
Elaine Marsh and Dennis Perzanowski. 1997. MUC-7 eval-
uation of IE technology: Overview of results. In Proc. of
the 7th MUC.
Boyan Onyshkevych. 1994. Issues and methodology for
template design for information extraction system. In
Proc. of the HLT Workshop.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In Proc. of HLT/NAACL.
Dragomir Radev and Kathleen McKeown. 1998. Gener-
ating natural language summaries from multiple on-line
sources. Computational Linguistics, 24(3):469?500.
Ellen Riloff and Mark Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In Proc. of
the 6th Workshop on Very Large Corpora.
Gerard Salton, 1971. The SMART retrieval system. Prentice-
Hall, NJ.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003.
An improved extraction pattern representation model for
automatic IE pattern acquisition. In Proc. of ACL.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan, 2004. Hy-
brid Approach to Answering Biographical Questions.
AAAI Press.
Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng,
David Pierce, and Kiri Wagstaff. 2001. Multi-document
summarization via information extraction. In Proc. of
HLT.
Mohammed Zaki. 2002. Efficiently mining frequent trees in
a forest. In Proc. of SIGKDD.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004. Multi-
document biography summarization. In Proc. of EMNLP.
214
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 333?336,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Where's the Verb? 
Correcting Machine Translation During Question Answering 
Wei-Yun Ma, Kathleen McKeown 
Department of Computer Science 
Columbia University 
New York, NY 10027, USA 
{ma,kathy}@cs.columbia.edu 
 
 
 
 
 
 
 
 
 
Abstract 
 
When a multi-lingual question-answering (QA) 
system provides an answer that has been 
incorrectly translated, it is very likely to be 
regarded as irrelevant. In this paper, we 
propose a novel method for correcting a 
deletion error that affects overall 
understanding of the sentence. Our post-editing 
technique uses information available at query 
time: examples drawn from related documents 
determined to be relevant to the query. Our 
results show that 4%-7% of MT sentences are 
missing the main verb and on average, 79% of 
the modified sentences are judged to be more 
comprehensible. The QA performance also 
benefits  from the improved MT: 7% of 
irrelevant response sentences become relevant. 
1. Introduction 
We are developing a multi-lingual question-
answering (QA) system that must provide 
relevant English answers for a given query, 
drawing pieces of the answer from translated 
foreign source. Relevance and translation quality 
are usually inseparable: an incorrectly translated 
sentence in the answer is very likely to be 
regarded as irrelevant even when the 
corresponding source language sentence is 
actually relevant. We use a phrase-based 
statistical machine translation system for the MT 
component and thus, for us, MT serves as a 
black box that produces the translated 
documents in our corpus; we cannot change the 
MT system itself. As MT is used in more and 
more multi-lingual applications, this situation 
will become quite common.  
We propose a novel method which uses 
redundant information available at question-
answering time to correct errors. We present a 
post-editing mechanism to both detect and 
correct errors in translated documents 
determined to be relevant for the response. In 
this paper, we focus on cases where the main 
verb of a Chinese sentence has not been 
translated. The main verb usually plays a crucial 
role in conveying the meaning of a sentence. In 
cases where only the main verb is missing, an 
MT score relying on edit distance (e.g., TER or 
Bleu) may be high, but the sentence may 
nonetheless be incomprehensible.  
Handling this problem at query time rather 
than during SMT gives us valuable information 
which was not available during SMT, namely, a 
set of related sentences and their translations 
which may contain the missing verb. By using 
translation examples of verb phrases and 
alignment information in the related documents, 
we are able to find an appropriate English verb 
and embed it in the right position as the main 
verb in order to improve MT quality. 
 A missing main verb can result in an incom-
prehensible sentence as seen here where the 
Chinese verb ???? was not translated at all. 
 
MT:          On December 13 Saddam . 
REF :        On December 13 Saddam was arrested. 
Chinese:   12?13??????? 
 
In other cases, a deleted main verb can result 
in miscommunication; below the Chinese verb 
???? should have been translated as 
?reduced?. An English native speaker could 
easily misunderstand the meaning to be ?People 
love classical music every year.? which happens 
to be the opposite of the original intended 
meaning. 
 
 
MT:          People of classical music loving every year.  
REF :        People?s love for classical music reduced every year. 
Chinese:   ??????????????? 
2. Related Work 
Post-editing has been used in full MT systems 
for tasks such as article selection (a, an, the) for 
333
English noun phrases (Knight and Chander 
1994). Simard et alin 2007 even developed a 
statistical phrase based MT system in a post-
editing task, which takes the output of a rule-
based MT system and produces post-edited 
target-language text.  Zwarts et al (2008) target 
selecting the best of a set of outputs from 
different MT systems through their 
classification-based approach. Others have also 
proposed using the question-answering context 
to detect errors in MT, showing how to correct 
names (Parton et. al 2008, Ji et. al 2008). 
3. System Overview 
The architecture of our QA system is shown in 
Figure 1. Our MT post-editing system (the bold 
block in Figure 1) runs after document retrieval 
has retrieved all potentially relevant documents 
and before the response generator selects 
sentences for the answer. It modifies any MT 
documents retrieved by the embedded 
information retrieval system that are missing a 
main verb. All MT results are provided by a 
phrase-based SMT system.  
   Post-editing includes three steps: detect a 
clause with a missing main verb, determine 
which Chinese verb should have been translated, 
and find an example sentence in the related 
documents with an appropriate sentence which 
can be used to modify the sentence in question.  
To detect clauses, we first tag the corpus using a 
Conditional Random Fields (CRF) POS tagger 
and then use manually designed regular 
expressions to identify main clauses of the 
sentence, subordinate clauses (i.e., clauses which 
are arguments to a verb) and conjunct clauses in 
a sentence with conjunction. We do not handle 
adjunct clauses. Hereafter, we simply refer to all 
of these as ?clause?. If a clause does not have 
any POS tag that can serve as a main verb (VB, 
VBD, VBP, VBZ), it is marked as missing a 
main verb.  
   MT alignment information is used to further 
ensure that these marked clauses are really 
missing main verbs.  We segment and tag the 
Chinese source sentence using the Stanford 
Chinese segmenter and the CRF Chinese POS 
tagger developed by Purdue University. If we 
find a verb phrase in the Chinese source 
sentence that was not aligned with any English 
words in the SMT alignment tables, then we 
label it as a verb translation gap (VTG) and 
confirm that the marking was correct. 
   In the following sections, we describe how we 
determine which Chinese verb should have been 
translated and how that occurs. 
Query in English
Document Retrieval
Detecting Possible Clauses 
with no Main Verb
Finding the Main Verb Position
Obtain Translation of the Main
Verb and embed it to the 
translated sentence
Corpus of translated 
English documents with
Chinese-English word 
alignment
Dynamic Verb 
Phrase Table
Static Verb 
Phrase Table 
Retrieved English docs
Modified English docs
Response Generator
Response in English  
Figure 1. The System Pipeline 
4. Finding the Main Verb Position  
Chinese ordering differs from English mainly 
in clause ordering (Wang et al, 2007) and 
within the noun phrase. But within a clause 
centered by a verb, Chinese mostly uses a SVO 
or SV structure, like English (Yamada and 
Knight 2001), and we can assume the local 
alignment centered by a verb between Chinese 
and English is a linear mapping relation. Under 
this assumption, the translation of ???? in the 
above example should be placed in the position 
between ?Saddam? and ?.?. Thus, once we find a 
VTG, its translation can be inserted into the 
corresponding position of the target sentence 
using the alignment.  
This assumes, however, that there is only one 
VTG found within a clause. In practice, more 
than one VTG may be found in a clause. If we 
choose one of them, we risk making the wrong 
choice. Instead, we insert the translations of both 
VTGs simultaneously. This strategy could result 
in more than one main verb in a clause, but it is 
more helpful than having no verb at all. 
5. Obtaining a VTG Translation 
We translate VTGs by using verb redundancy 
in related documents: if the VTG was translated 
in other places in related documents, the existing 
translations can be reused. Related documents 
are likely to use a good translation for a specific 
VTG as it is used in a similar context. A verb?s 
aspect and tense can be directly determined by 
referencing the corresponding MT examples and 
their contexts. If, unfortunately, a given VTG 
334
did not have any other translation record, then 
the VTG will not be processed. 
To do this, our system first builds verb phrase 
tables from relevant documents and then uses 
the tables to translate the VTG. We use two verb 
phrase tables: one is built from a collection of 
MT documents before any query and is called 
the ?Static Verb Phrase Table?, and the other 
one is dynamically built from the retrieved 
relevant MT documents for each query and is 
called the ?Dynamic Verb Phrase Table?.  
The construction procedure is the same for 
both. Given a set of related MT documents and 
their MT alignments, we collect all Chinese verb 
phrases and their translations along with their 
frequencies and contexts. 
One key issue is to decide appropriate 
contextual features of a verb. A number of 
researchers (Cabezas and Resnik 2005, Carpuat 
and Wu 2007) provide abundant evidence that 
rich context features are useful in MT tasks. 
Carpuat and Wu (2007) tried to integrate a 
Phrase Sense Disambiguation (PSD) model into 
their Chinese-English SMT system and they 
found that the POS tag preceding a given phrase, 
the POS tag following the phrase and bag-of-
words are the three most useful features. 
Following their approach, we use the word 
preceding and the word following a verb as the 
context features. 
The Static and Dynamic Verb Phrase Tables 
provide us with MT examples to translate a 
VTG. The system first references the Dynamic 
Verb Phrase Table as it is more likely to yield a 
good translation. If the record is not found, the 
Static one is referenced. If it is not found in 
either, the given VTG will not be processed. No 
matter which table is referenced, the following 
Naive Bayes equation is applied to obtain the 
translation of a given VTG. 
 
))|(log)|(log)((logmaxarg    
),|(maxarg'
kkk
t
k
t
tfwPtpwPtP
fwpwtPt
k
k
++=
=
 
pw, fw and tk respectively represent the 
preceding source word, the following source 
word and a translation candidate of a VTG. 
6.  Experiments 
Our test data is drawn from Chinese-English MT 
results generated by Aachen?s 2007 RWTH sys-
tem (Mauser et al, 2007), a phrase-based SMT 
system with 38.5% BLEU score on IWSLT 
2007 evaluation data.  
Newswires and blog articles are retrieved for 
five queries which served as our experimental 
test bed. The queries are open-ended and on av-
erage, answers were 30 sentences in length. 
 
Q1: Who/What is involved in Saddam Hussein's trial 
Q2: Produce a biography of Jacques Rene Chirac 
Q3: Describe arrests of person from Salafist Group for 
Preaching and Combat 
Q4: Provide information on Chen Sui Bian 
Q5: What connections are there between World Cup games and 
stock markets? 
 
We used MT documents retrieved by IR for 
each query to build the Dynamic Verb Phrase 
Table. We tested the system on 18,886 MT 
sentences from the retrieved MT documents for 
all of the five queries. Among these MT 
sentences, 1,142 sentences were detected and 
modified (6 % of all retrieved MT sentences). 
6.1 Evaluation Methodology 
For evaluation, we used human judgments of the 
modified and original MT. We did not have 
reference translations for the data used by our 
question-answering system and thus, could not 
use metrics such as TER or Bleu. Moreover, at 
best, TER or Bleu score would increase by a 
small amount and that is only if we select the 
same main verb in the same position as the 
reference. Critically, we also know that a 
missing main verb can cause major problems 
with comprehension. Thus, readers could better 
determine if the modified sentence better 
captured the meaning of the source sentence. We 
also evaluated relevance of a sentence to a query 
before and after modification. 
We recruited 13 Chinese native speakers who 
are also proficient in English to judge MT 
quality. Native English speakers cannot tell 
which translation is better since they do not 
understand the meaning of the original Chinese. 
To judge relevance to the query, we used native 
English speakers. 
Each modified sentence was evaluated by 
three people. They were shown the Chinese 
sentence and two translations, the original MT 
and the modified one. Evaluators did not know 
which MT sentence was modified. They were 
asked to decide which sentence is a better 
translation, after reading the Chinese sentence. 
An evaluator also had the option of answering 
?no difference?.  
6.2 Results and Discussion 
We used majority voting (two out of three) to 
decide the final evaluation of a sentence judged 
by three people. On average, 900 (79%) of the 
335
1142 modified sentences, which comprise 5% of 
all 18,886 retrieved MT sentences, are better 
than the original sentences based on majority 
voting. And for 629 (70%) of these 900 better 
modified sentences all three evaluators agreed 
that the modified sentence is better. 
 Furthermore, we found that for every 
individual query, the evaluators preferred more 
of the modified sentences than the original MT. 
And among these improved sentences, 81% 
sentences reference the Dynamic Verb Phrase 
Table, while only 19% sentences had to draw 
from the Static Verb Phrase Table, thus 
demonstrating that the question answering 
context is quite helpful in improving MT. 
We also evaluated the impact of post-editing 
on the 234 sentences returned by our response 
generator. In our QA task, response sentences 
were judged as ?Relevant(R)?, ?Partially 
Relevant(PR)?, ?Irrelevant(I)? and ?Too little 
information to judge(T)? sentences. With our 
post-editing technique, 7% of 141 I/T responses 
become R/PR responses and none of the R/PR 
responses become I/T responses. This means 
that R/PR response percentage has an increase of 
4%, thus demonstrating that our correction of 
MT truly improves QA performance. An 
example of a change from T to PR is: 
 
 
Question: What connections are there between World Cup games 
and stock markets? 
Original QA answer: But if winning the ball, not necessarily in 
the stock market. 
Modified QA answer: But if winning the ball, not necessarily in 
the stock market increased.  
6.3 Analysis of Different MT Systems 
In order to examine how often missing verbs 
occur in different recent MT systems, in addition 
to using Aachen?s up-to-date system ? ?RWTH-
PBT?of 2008, we also ran the detection process 
for another state-of-the-art MT system ? ?SRI-
HPBT? (Hierarchical Phrase-Based System) of 
2008 provided by SRI, which uses a grammar on 
the target side as well as reordering, and focuses 
on improving grammaticality of the target 
language. Based on a government 2008 MT 
evaluation, the systems achieve 30.3% and 
30.9% BLEU scores respectively. We used the 
same test set, which includes 94 written articles 
(953 sentences). 
Overall, 7% of sentences translated by 
RWTH-PBT are detected with missing verbs 
while 4% of sentences translated by SRI-HPBT 
are detected with missing verb. This shows that 
while MT systems improve every year, missing 
verbs remain a problem.  
7 Conclusions 
In this paper, we have presented a technique for 
detecting and correcting deletion errors in trans-
lated Chinese answers as part of a multi-lingual 
QA system. Our approach uses a regular gram-
mar and alignment information to detect missing 
verbs and draws from examples in documents 
determined to be relevant to the query to insert a 
new verb translation. Our evaluation demon-
strates that MT quality and QA performance are 
both improved. In the future, we plan to extend 
our approach to tackle other MT error types by 
using information available at query time. 
Acknowledgments 
This material is based upon work supported 
by the Defense Advanced Research Projects 
Agency under Contract No. HR0011-06-C-0023 
References 
Clara Cabezas and Philip Resnik. 2005. Using WSD 
Techniques for Lexical Selection in Statistical 
Machine, Translation Technical report CS-TR-
4736  
Marine Carpuat and Dekai Wu. 2007. Context-
Dependent Phrasal Translation Lexicons for 
Statistical Machine Translation, Machine 
Translation Summit XI, Copenhagen 
Heng Ji, Ralph Grishman and Wen Wang. 2008. 
Phonetic Name Matching For Cross-lingual 
Spoken Sentence Retrieval, IEEE-ACL SLT08. 
Goa, India 
K. Knight and I. Chander. 1994. Automated 
Postediting of Documents, AAAI 
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. 2008. Simultaneous 
multilingual search for translingual information 
retrieval,  ACM 17th CIKM 
Arne Mauser, David Vilar, Gregor Leusch, Yuqi 
Zhang, and Hermann Ney. 2007. The RWTH 
Machine Translation System for IWSLT 2007, 
IWSLT 
Michel Simard, Cyril Goutte and Pierre Isabelle. 
2007. Statistical Phrase-based Post-Editing, 
NAACL-HLT 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for 
Statistical Machine Translation, EMNLP-
CoNLL. 
Kenji Yamada , Kevin Knight. 2001. A syntax-based 
statistical translation model, ACL 
S. Zwarts and M. Dras. 2008. Choosing the Right 
Translation: A Syntactically Informed 
Approach, COLING 
336
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 9?12,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
A Tool for Deep Semantic Encoding of Narrative Texts
David K. Elson
Columbia University
New York City
delson@cs.columbia.edu
Kathleen R. McKeown
Columbia University
New York City
kathy@cs.columbia.edu
Abstract
We have developed a novel, publicly avail-
able annotation tool for the semantic en-
coding of texts, especially those in the
narrative domain. Users can create for-
mal propositions to represent spans of text,
as well as temporal relations and other
aspects of narrative. A built-in natural-
language generation component regener-
ates text from the formal structures, which
eases the annotation process. We have
run collection experiments with the tool
and shown that non-experts can easily cre-
ate semantic encodings of short fables.
We present this tool as a stand-alone, re-
usable resource for research in semantics
in which formal encoding of text, espe-
cially in a narrative form, is required.
1 Introduction
Research in language processing has benefited
greatly from the collection of large annotated
corpora such as Penn PropBank (Kingsbury and
Palmer, 2002) and Penn Treebank (Marcus et al,
1993). Such projects typically involve a formal
model (such as a controlled vocabulary of thematic
roles) and a corpus of text that has been anno-
tated against the model. One persistent tradeoff in
building such resources, however, is that a model
with a wider scope is more challenging for anno-
tators. For example, part-of-speech tagging is an
easier task than PropBank annotation. We believe
that careful user interface design can alleviate dif-
ficulties in annotating texts against deep semantic
models. In this demonstration, we present a tool
we have developed, SCHEHERAZADE, for deep
annotation of text.
1
We are using the tool to collect semantic rep-
resentations of narrative text. This domain occurs
1
Available at http://www.cs.columbia.edu/?delson.
frequently, yet is rarely studied in computational
linguistics. Narrative occurs with every other dis-
course type, including dialogue, news, blogs and
multi-party interaction. Given the volume of nar-
rative prose on the Web, a system competent at un-
derstanding narrative structures would be instru-
mental in a range of text processing tasks, such
as summarization or the generation of biographies
for question answering.
In the pursuit of a complete and connected rep-
resentation of the underlying facts of a story, our
annotation process involves the labeling of verb
frames, thematic roles, temporal structure, modal-
ity, causality and other features. This type of anno-
tation allows for machine learning on the thematic
dimension of narrative ? that is, the aspects that
unite a series of related facts into an engaging and
fulfilling experience for a reader. Our methodol-
ogy is novel in its synthesis of several annotation
goals and its focus on content rather than expres-
sion. We aim to separate the narrative?s fabula, the
content dimension of the story, from the rhetori-
cal presentation at the textual surface (sju?zet) (Bal,
1997). To this end, our model incorporates formal
elements found in other discourse-level annotation
projects such as Penn Discourse Treebank (Prasad
et al, 2008) and temporal markup languages such
as TimeML (Mani and Pustejovsky, 2004). We
call the representation a story graph, because these
elements are embodied by nodes and connected by
arcs that represent relationships such as temporal
order and motivation.
More specifically, our annotation process in-
volves the construction of propositions to best ap-
proximate each of the events described in the tex-
tual story. Every element of the representation
is formally defined from controlled vocabularies:
the verb frames, with their thematic roles, are
adapted from VerbNet (Kipper et al, 2006), the
largest verb lexicon available in English. When
the verb frames are filled in to construct action
9
Figure 1: Screenshot from our tool showing the process of creating a formal proposition. On the left, the
user is nesting three action propositions together; on the right, the user selects a particular frame from a
searchable list. The resulting propositions are regenerated in rectangular boxes.
propositions, the arguments are either themselves
propositions or noun synsets from WordNet (the
largest available noun lexicon (Fellbaum, 1998)).
Annotators can also write stative propositions
and modifiers (with adjectives and adverbs culled
from WordNet), and distinguish between goals,
plans, beliefs and other ?hypothetical? modalities.
The representation supports connectives including
causality and motivation between these elements.
Finally, and crucially, each proposition is bound
to a state (time slice) in the story?s main timeline
(a linear sequence of states). Additional timelines
can represent multi-state beliefs, goals or plans. In
the course of authoring actions and statives, an-
notators create a detailed temporal framework to
which they attach their propositions.
2 Description of Tool
The collection process is amenable to community
and non-expert annotation by means of a graphical
encoding tool. We believe this resource can serve
a range of experiments in semantics and human
text comprehension.
As seen in Figure 1, the process of creating a
proposition with our tool involves selecting an ap-
propriate frame and filling the arguments indicated
by the thematic roles of the frame. Annotators are
guided through the process by a natural-language
generation component that is able to realize textual
equivalents of all possible propositions. A search
in the interface for ?flatter,? for example, offers a
list of relevant frames such as<A character> flat-
ters<a character>. Upon selecting this frame, an
annotator is able to supply arguments by choosing
actors from a list of declared characters. ?The fox
flatters the crow,? for one, would be internally rep-
resented with the proposition <flatters>([Fox
1
],
[Crow
1
]) where flatters, Fox and Crow are not
snippets of surface text, but rather selected Word-
Net and VerbNet records. (The subscript indi-
cates that the proposition is invoking a particular
[Fox] instance that was previously declared.) In
this manner an entire story can be encoded.
Figure 2 shows a screenshot from our interface
in which propositions are positioned on a timeline
to indicate temporal relationships. On the right
side of the screen are the original text (used for
reference) and the entire story as regenerated from
10
Figure 2: The main screen of our tool features a graphical timeline, as well as boxes for the reference
text and the story as regenerated by the system from the formal model.
the current state of the formal model. It is also pos-
sible from this screen to invoke modalities such
as goals, plans and beliefs, and to indicate links
between propositions. Annotators are instructed
to construct propositions until the resulting textual
story, as realized by the generation component, is
as close to their own understanding of the story as
permitted by the formal representation.
The tool includes annotation guidelines for con-
structing the best propositions to approximate the
content of the story. Depending on the intended
use of the data, annotators may be instructed to
model just the stated content in the text, or include
the implied content as well. (For example, causal
links between events are often not articulated in a
text.) The resulting story graph is a unified rep-
resentation of the entire fabula, without a story?s
beginning or end. In addition, the tool allows an-
notators to select spans of text and link them to
the corresponding proposition(s). By indicating
which propositions were stated in the original text,
and in what order, the content and presentation di-
mensions of a story are cross-indexed.
3 Evaluation
We have conducted several formative evaluations
and data collection experiments with this inter-
face. In one, four annotators each modeled four of
the fables attributed to Aesop. In another, two an-
notators each modeled twenty fables. We chose to
model stories from the Aesop corpus due to sev-
eral key advantages: the stories are mostly built
from simple declaratives, which are within the ex-
pressive range of our semantic model, yet are rich
in thematic targets for automatic learning (such as
dilemmas where characters must choose from be-
tween competing values).
In the latter collection, both annotators were un-
dergraduates in our engineering school and native
English speakers, with little background in lin-
guistics. For this experiment, we instructed them
to only model stated content (as opposed to includ-
ing inferences), and skip the linking to spans of
source text. On average, they required 35-45 min-
utes to encode a fable, though this decreased with
practice. The 40 encodings include 574 proposi-
tions, excluding those in hypothetical modalities.
The fables average 130 words in length (so the an-
notators created, on average, one proposition for
every nine words).
Both annotators became comfortable with the
tool after a period of training; in surveys that they
completed after each task, they gave Likert-scale
usability scores of 4.25 and 4.30 (averaged over
all 20 tasks, with a score of 5 representing ?easiest
to use?). The most frequently cited deficiencies in
the model were abstract concepts such as fair (in
the sense of a community event), which we plan to
support in a future release.
4 Results and Future Work
The end result from a collection experiment is
a collection of story graphs which are suitable
for machine learning. An example story graph,
based on the state of the tool seen in Figure 2, is
shown in Figure 3. Nodes in the graph represent
states, declared objects and propositions (actions
and statives). Each of the predicates (e.g.,<lion>,
11
????? ?????????????????
???????????
???????????
?????????
????????
?
????????? ?????
??? ?????
????????????
????????????
?????
??????????
?????????
?????
?????
Figure 3: A portion of a story graph representation as created by SCHEHERAZADE.
<watch>, <cunning>) are linked to their corre-
sponding VerbNet and WordNet records.
We are currently experimenting with ap-
proaches for data-driven analysis of narrative con-
tent along the ?thematic? dimension as described
above. In particular, we are interested in the auto-
matic discovery of deep similarities between sto-
ries (such as analogous structures and prototypical
characters). We are also interested in investigat-
ing the selection and ordering of content in the
story?s telling (that is, which elements are stated
and which remain implied), especially as they per-
tain to the reader?s affectual responses. We plan
to make the annotated corpus publicly available in
addition to the tool.
Overall, while more work remains in expanding
the model as well as the graphical interface, we
believe we are providing to the community a valu-
able new tool for eliciting semantic encodings of
narrative texts for machine learning purposes.
5 Script Outline
Our demonstration involves a walk-through of the
SCHEHERAZADE tool. It includes:
1. An outline of the goals of the project and the
innovative aspects of our formal representa-
tion compared to other representations cur-
rently in the field.
2. A tour of the timeline screen (equivalent to
Figure 2) as configured for a particular Aesop
fable.
3. The procedure for reading a text for impor-
tant named entities, and formally declaring
these named entities for the story graph.
4. The process for constructing propositions in
order to encode actions and statives in the
text, as seen in Figure 1.
5. Other features of the software package, such
as the setting of causal links and the ability to
undo/redo.
6. A review of the results of our formative eval-
uations and data collection experiments, in-
cluding surveys of user satisfaction.
References
Mieke Bal. 1997. Narratology: Introduction to the
Theory of Narrative. University of Toronto Press,
Toronto, second edition.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Canary Islands, Spain.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extensive classifications of
english verbs. In Proceedings of the 12th EURALEX
International Congress, Turin, Italy.
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral discourse models for narrative structure. In
Proceedings of the ACL Workshop on Discourse An-
notation, Barcelona, Spain.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
12
Generating Referring Quantified Expressions 
J ames  Shaw and Kath leen  McKeown 
Dept .  of Computer  Sc ience 
Co lumbia  Un ivers i ty  
New York,  NY  10027, USA 
shaw,  kathy*cs ,  co lumbia ,  edu 
, : ,  ~ .*~ . 
Abst ract  
In this paper, we describe how quantifiers can be 
generated in a text generation system. By taking 
advantage of discourse and ontological information, 
quantified expressions can replace entities in a text, 
making the text more fluent and concise. In ad- 
dition to avoiding ambiguities between distributive 
and collective readings in universal quantification 
generation, we will also show how different scope 
orderings between universal and existential quanti- 
tiers will result in different quantified expressions in
our algorithm. 
1 In t roduct ion  
To convey information concisely and fluently, text 
generation systems often perform opportunistic text 
planning (Robin, 1995; Mellish et al, 1998) and em- 
ploy advanced linguistic constructions such as ellip- 
sis (Shaw, 1998). But a system can also take ad- 
vantage of quantification and ontological informa- 
tion to generate concise references to entities at the 
discourse level. For example, a sentence such as 
"'The patient has an infusion line in each arm." is 
a more concise version of "The patient has an in- 
fusion line ir~ his left arm. The patient has an in- 
fusion line in his right arm." Quantification is an 
active research topic in logic, language, and philoso- 
phy(Carpenter, 1997; de Swart. 1998). Since nat- 
ural language understanding systems need to ob- 
tain as few interpretations as possible from text, 
researchers have studied quantifier scope ambigu- 
ity extensively (Woods~ 1978;-Grosz et al, 1987; 
Hobbs and Shieber, 1987: Pereira, 1990; Moran and 
Pereira, 1992: Park, 1995). Research in quantifica- 
tion interpretation first transforms a sentence into 
predicate logic, raises tim quantifiers to the senten- 
tial level, and permutes these quantifiers {o obtain 
as many readings as possible relaled to quantifier 
scoping. Then, invalid readings are eliminated using 
various consl raints. 
Ambiguity in quantified expressions i caused by 
two main culprits. The  first type of ambiguity in- 
volves the distributive reading versus the collective 
reading. In universal quantification, a referring ex- 
100 
pression refers to multiple entities. There is a po- 
tential ambiguity between whether the aggregated 
entities acted individually (distributive) or acted to- 
gether as one (collective). Under the distributive 
reading, the sentence "All the nurses inspected the 
patient." implies that each nurse individually in- 
spected the patient. Under the collective reading, 
the nurses inspected the patient ogether as a group. 
The other ambiguity in quantification i volves mul- 
tiple quantifiers in the same sentence. The sentence 
"A nurse inspected each patient." has two possi- 
ble quantifier scope orderings. In Vpatient3nurse, 
the universal quantifier V has wide scope, outscop- 
ing the existential quantifier 3. This ordering means 
that each patient is inspected by a nurse, who might 
not be the same in each case. In the other scope 
order, 3nurseVpatient, a single, particular nurse in- 
spected every patient. In both types of ambiguities, 
a generation system should make the desired reading 
clear. 
Fortunately, the difficulties of quantifier scope dis- 
ambiguation faced by the understanding conmmnity 
do not apply to text generation. For generation, the 
problem is the reverse: given an unambiguous rep- 
resentation of a set of facts as input, how can it 
generate a quantified sentence that unambiguously 
conveys the intended meaning? In this paper, we 
propose an algorithm which selects an appropriate 
quantified expression to refer .to a set of entities us- 
ing discourse and ontological knowledge. The algo- 
rithm first identifies the entities for quantification i
;the input :propositions. Then an- appropriate con- 
cept in the ontology is selected to refer to these en- 
tities. Using discourse and ontological information, 
the system determines if quantification is appropri- 
ate and if it is, which particular quantifier to use 
to minimize the anabiguity between distributive and 
collective readings. More importantly, when there 
are multiple quantifiers hi the same sentence, the al- 
gorithm generates different expressions for differen~ 
scope orderings. In this work, we focus on generat- 
ing referring quantified expressions for entities which 
have been mentioned before in the discourse or can 
be inferred from an ontology. There are quantified 
expressions that do not refer to particular entities 
in a domain or discourse, such as generics (i.e. "All 
whales are mammals."), or negatives (i.e., "The pa- 
tient has no allergies."). The synthesis of such quan- 
tifiers is currently performed in earlier stages.of the 
((TYPE EVENT) 
(PRED ((PRED receive) (ID idl))) 
(ARGi ((PRED patient) (ID ptl))) 
(ARG2 ((PRED aprotinin) (ID apl))) 
generation process. (MODS ((PRED after) (ID id2) 
. In the next section;we..vdll..~orapaxe ou_r~.approach . . . . .  . : .  . .  tTYRE_TIME).. ............... 
with previous work in the generation of quantified 
expressions. In Section 3, we will describe the appli- 
cation where the need for concise output motivated ) ) )  
our research in quantification. The algorithm for 
generating universal quantifiers is detailed in Sec- 
tion 4, including how the system handles ambiguity 
between distributive and collective readings. Sec- 
tion 5 describes how our algorithm generates en- 
tences with multiple quantifiers. 
2 Related Work 
Because a quantified expression refers to multiple 
entities in a domain, our work can be categorized as 
referring expression generation (Dale, 1992; Reiter 
and Dale, 1992; Horacek, 1997). Previous work in 
this area did not address the generation of quantified 
expressions directly. In this paper, we are interested 
in how to systematically derive quantifiers from in- 
put propositions, discourse history, and ontological 
information. Recent work on the generation ofquan- 
tifiers (Gailly, 1988; Creaney, 1996; Creaney, 1999) 
follows the analysis viewpoint, discussing scope ana- 
biguities extensively. Though our algorithm gener- 
ates different sentences for different scope orderings, 
we do not achieve this through scoping operations as 
they did. Creaney also discussed various imprecise 
quantifiers, such as some, at least ,  and at most. 
In regards to generating eneric quantified expres- 
sions, (Knott et al, 1997) has proposed an algorithm 
for generating defeasible, but informative descrip- 
tions for objects in nmseums. 
Other researchers (van Eijck and Alshawi, 1992; 
Copestake t al., 1999) proposed representations i  a 
machine translation setting which allow underspec- 
ification in regard to quantifier scope. Our work is 
different, in that we perform quantification directly 
on the instance-based representation btained from 
database tuples. Our input .does not have the in - . .  
formation about which entities are quantified as is 
the case in machine translation, where the quanti- 
tiers are already specified in the input from a source 
language. 
3 The  Application Domain 
We implemented our quantification algorithm as 
part of MAGIC (Dalai et al, 1996: McKeown et 
al., 1997). MAGIC automatically generates multi- 
media briefings to describe the post-operative sta- 
tus of a patient after undergoing Coronary Artery 
Bypass Graft, surgery. The system embodies a stan- 
. f . . . . . . .  
(ARG2 ((PRED critical-point) 
(NAME intubation) (IDcl))) 
Figure h The predicate-argument structure of 
"After intubation, a patient received aprotinin." 
dard text generation system architecture with three 
modules (Rambow and Korelsky, 1992): a content 
planner, a sentence planner, and a linguistic realizer. 
Once the bypass urgery is finished, information that 
is automatically collected during surgery such as 
blood pressure, heart rate, and medications given, 
is sent to a domain=specific medical inference mod- 
ule. Based on the medical inferences and schemas 
(McKeown, 1985), the content planner determines 
the information to convey and the order to convey 
it. 
The sentence planner takes a set of propositions 
(or predicate-argument structures) with rhetorical 
relations from the content planner and uses linguistic 
information to make decisions about how to convey 
the propositions fluently. Each proposition is repre- 
sented as a feature structure (Kaplan and Bresnan, 
1982; Kay, 1979) similar to the one shown in Fig- 
ure 1. The sentence planner's responsibilities include 
referring expression generation, clause aggregation, 
and lexical choice (Wanner and How, 1996). Then 
the aggregated predicate-argument structure is sent 
to FUF/SURGE (Elhadad and Robin, 1992), a lin- 
guistic realizer which t.ransforms the lexicalized se- 
inantic specification i to a string. The quantification 
algorithm is implemented in the sentence planner. 
4 Quantification Algorithm 
in this:,work, weprefergenerating expressions with 
universal quantifiers over conjunction because, as- 
suming that the users and the system have tile same 
domain model, the universally quantified expres- 
sions are more concise and they represent the same 
amount of information as the expression with con- 
joined entities. In contrast,, when given a conjunc- 
tion of entities and an expression with a cardinal 
quantifier, the system, by default, would use the 
conjunction if the conjoined entities can be distin- 
guished at the surface level. This is because once 
the system generates a cardinal quantifier when the 
universal quantification does not hold, such as "three 
101 
patients", it is impossible for the hearer to recover 
the identities of these patients based on the con- 
text. The default heuristics to prefer universal quan- 
tifier over conjunction over cardinal quantifier can 
be superseded by directives f romthe  contentplan- 
ner which are application specific. 
The input to our quaatif ica~omalgorit;hm is a set 
of predicate-argument structures after the referr ing 
expression module selected the properties to identify 
the entities (Dale, 1992; Dale and Reiter, 1995), but 
without carrying out the assignment of quantifiers. 
Our quantification algorithm first identifies the set 
of distinct entities which can be quantified in the 
input propositions. A generalization of the entities 
in the ontology is selected to potentially replace the 
references to these entities. If universal quantifica- 
tion is possible, then the replacement is made and 
the system must select which particular quantifier 
to use. In our system, we have six realizations for 
universal quantifiers: each, every,  a l l  1, both, the, 
and any, and two for existential quantifiers: the in- 
definite article, a/an, and cardinal n. 
4.1 Ident i fy  Themat ic  Ro les  wi th  Dist inct  
Ent i t ies  
Our algorithm identifies the roles containing distinct 
entities among the input propositions as candidates 
for universal and existential quantification. Suppose 
the system is given two propositions imilar to the 
one in Figure 1, "After intubation, Alice received 
aprotinin" and "After start of bypass, Alice received 
aprotinin", each with four roles - PRED, ARG1, 
ARG2, and MODS-TIME. By computing similarity 
anaong entities in the same role, the system deter- 
mines that the entities in ARG1, PRED, and ARG2 
are identical in each role, and only the entities in 
MODS-TIME are different. Based on this result, 
the distinct entities in MODS-TIME, "after intuba- 
tion" and "after start of bypass", are candidates for 
quantificat ion. 
4.2 Genera l i zat ion  and Quant i f icat ion 
We used the axioms in Figure 2 to determine if 
the distinct entities can be universally or existen- 
tially quantified. Though the axioms are similar to 
those used in Generalized Quantifier (Barwise and 
Cooper, 1981; Zwarts, 1983; de Swart, 1998). the 
semantics of set X and set D are different. In the 
previous step. the entities in set X have been iden- 
tified. To compute set D in Figure 2. we introduce 
a concept, Class-X. Class-X is a generalization of 
the distinct entities in set X. Quantification can re- 
place the distinct entities in the propositions with 
a reference to their type restricled by a quantifier. 
accessing discourse and ontological information .to 
provide a context. Our ontology is implemented in
l a l i  is rea l ized as "a l i  the" .  
102 
? bo th :  ID  - X \ [  = 0 and I x I  = 2, can have col- 
lective reading 
? every, a l l ,  the: ID-XI = 0 and IX\[ > 2, can 
have collective reading 
? each: I D - X\[ = 0 and IXI _> 2, only distribu- 
-- ? tive reading . . . . . . . .  
? any: \ ]D-  X\] = 0, when under the scope of 
negation 
? a /an :  IDnXl > 0 and Ixl = 1 
? n (cardinal): IOnXl > 0 and \[Xl = n 
Figure 2: Axioms of the quantifiers discussed in this 
paper. 
CLASSIC(Borgida et al, 1989) and is a subset of 
WordNet(Miller et al, 1990) and an online medical 
dictionary (Cimino et al, 1994) designed to support 
multiple applications across the medical institution. 
Given the entities in set X, queries in CLASSIC de- 
termine the class of each instance and its ancestors 
in the ontology. Based on this information, the gen- 
eralization algorithm identifies Class-X by comput- 
ing the most specific class which covers all the enti- 
ties. Earlier work (Passonneau et al, 1996) provided 
a framework for balancing specificity and verbosity 
in selecting appropriate concepts for generalization. 
However, given the precision needed in medical re- 
ports, our generalization procedure selects the most 
specific class. 
Set D represents the set of instances of Class-X in 
a context. Our system currently computes et D for 
three different contexts: 
e discourse: Previous references can provide an 
appropriate context for universal quantification. 
For example, if "Alice" and "Bob" were men- 
tioned in the previous entence, the system can 
refer t.o them as "both patients" in the current 
sentence. 
? domain ontology: The domain ontology pro- 
vides a closed world from which we can obtain 
't-he set D by matching all the instances of a 
concept in the knowledge base, such as "'ev- 
ery patient". In addition, certain concepts in 
the ontology have limited types. For example, 
knowing that cell savers, platelets and packed 
red blood cells are the only possible types of 
blood products in the ontology, the quantified 
expression "every blood product" can be used 
instead of referring to each entity. 
? domain knowledge: The possessor of the dis- 
tinct entities in a role might contain a maximum 
number of instances allowed for Class-X. For ex- 
ample, because a person has only two arms, the tinguishable xpressions at surface level. A more 
entities "the patient's left arm" and "the pa- developed pragmatic module is needed before quan- 
tient's right arm" can be referred to as "each tifiers such as some, raps'e, a t  leas t ,  and few, can 
arm". be systematically generated. Indiscriminate applica- 
tion of imprecise quantification can result in- vague 
The computation of set D can also involve interac- or inappropriate text in our domain, such as "The 
tions with a referring expression m0dule(Dale aad~ ~-:patient~rec~ived.~some 61ood~produetS:"'-v.ia-our~e~P - 
Reiter, 1995). For example, instead of the expres- plication, knowing exactly what blood products are 
sion "Alice and Bob" and "both patients" covered 
by the current algorithm, by interacting with a refer- 
ring expression module, the system might determine 
that "both CABG patients operated on this morn- 
ing by Dr. Rose" is a clearer expression to refer to 
the entities. Though this is desirable, we did not 
incorporate this capability into our system. 
Although the is often used to indicate a generic 
reference (i.e., "The lion is the king of jungle."), in 
English, the can also be used as an unmarked uni- 
versal quantifier when its head noun is plural, such 
as "the patients." Like the quantifier a l l ,  the can 
be both distributive and collective. However, the 
cannot always replace a l l  as a universal quantifier. 
the cannot be used when universal quantification is
based on the domain ontology. For example, it is 
not obvious that the quantified expression in "John 
received the blood products." refers to "each blood 
product" in the ontology. Although unmarked uni- 
versal quantifiers can be used to refer to body parts, 
as in "The lines include an IV in the arms.", the ex- 
pression is ambiguous between the distributive and 
collective readings. Of the three contexts discussed 
above, the system occationally generates the instead 
of every and both in a discourse context, yielding 
more natural output. 
When the computed set D matches et X exactly 
(ID - X I = 0), a quantified expression with either 
each, a l l ,  every, both, the, and any, replaces the 
entities in set X. 
4.3 Select ing a Par t i cu la r  Quant i f ie r  
In general, the universal quantification of a partic- 
ular type of entity, such as "every patient", refers 
to all such entities in a context. As a result, read- 
ers can recover what a universally quantified expres- 
sion refers to. In contrast, readers cannot pinpoint 
which entity has been refei'red to. in an existentially . 
quantified expression, such as "a patient." or "two 
patients". Because a universally quantified expres- 
sion preserves original semantics and is more con- 
cise than listing each entity, it is the focus of our 
quantificalion algorithm. The universal quantifiers 
hlaplemented in our system include the six possible 
realizations of V in English: every, a l l .  each. both. 
the, and any. The only existential quantifiers im- 
plemented in our system are the singular indefinite 
quantifier, a/an. and cardinal quantifiers, n. They 
are used in sentences with multiple quantifiers and 
when the entities being referred to do not have dis- 
used is very important. To avoid generating such 
inappropriate sentences, the system only performs 
generalization on the entities which can be univer- 
sally quantified. If the distinct entities cannot be 
universally quantified, the system will realize these 
entities using coordinated conjunction. 
Once the system decides that a universally quan- 
tified expression can be used to replace the entities 
in set X, it must select which universal quantifier. 
Because our sentence planner opportunistically com- 
bines distinct entries from separate database ntries 
for conciseness, it is not the case that these aggre- 
gated entities acted together (the collective read- 
ing). Given such input, the referring expression for 
aggregated entities should have only the distribu- 
tive reading 2. The universal quantifier, each, al- 
ways imposes a distributive reading when applied. 
In general, each requires a "matching" between the 
domain of the quantifier and the objects referred 
to(McCawley, 1981, pp. 37). In our algorithm, this 
matching process is exactly what happened, thus it 
is the default universal quantifier in our algorithm. 
Of course, indiscriminate use of each can result in 
awkward sounding text. For example, tile sentence 
"Every patient is awake" sounds more natural than 
"Each patient is awake." However, since quantified 
expressions with the universal quantifiers a l l  and 
every 3 can have collective readings (Vendler, 1967; 
McCawley, 1981), our system generates every and 
a l l  under two conditions when the collective read- 
ing is unlikely. First if the proposition is a state, as 
opposed to an event, we assume only the distribu- 
tive reading is possible 4. The quantifier every is 
used in "Ever.q patient tmd.taehycardia.'" because 
the proposition is a state proposition and contains 
the predicate has-attribute, an attributive relation. 
. . . . .  2For our  system to  generate noun-phrases.wivh ,col}eetive 
readings, the quantification process must be performed at the 
content planner level not in the clause aggregation module. 
3every is also distributive, but it stresses completeness or
rather, exhaustiveness(Vendler, 1967). The sentence "John 
took a picture of everyone in the room."  is ambiguous while 
"John took a picture os t each person in the room." is not. 
4There are cases where state propositions do have dis- 
teibuted readings (e.g., "Mountains urround the village." ). 
Sentences with collective readings are bandied earlier in the 
content planner and thus, this type of problem does not occur 
at this point in our system. Though .this observation seems to 
be true in our medical application, when implementing quan- 
tifiers in a new domain, we can limit this assumption to only 
the subset of state relations for which it holds. 
103 
Second, when the concept being universally quan- 
tified is marked as having a distributive reading in 
the lexicon, such as the concept episode, quantifiers 
every will be used instead of each. These quanti- 
tiers make the quantified sentences more natural be- 
cause they do not pick out the redundant distribu- 
tive meaning. . . . . . .  .~ . -: ....... =~:: .... ~" :~; 
The use of prepositions can also affect which quan- 
tifier to use. For example, "A f te r  all the episodes, 
the patient received obutamine" is ambiguous in re- 
gards to whether the dobutamine is given once dur- 
ing the surgery, or given after each episode. In con- 
trast, the sentence " In  all the episodes, the patient 
received dobutamine." does not have this problem. 
The current system looks at the particular preposi- 
tion (i.e., "before", "after", or "in") before selecting 
the appropriate quantifier. 
4.4 Examples of a Single Quantifier 
Given the four propositions, "After intubation, 
Mrs. Doe had tachycardia", "After skin incision, 
Mrs. Doe had tachycardia", "After start of bypass, 
Mrs. Doe had tachycardia',  and "After coming off 
bypass, Mrs. Doe had tachycardia.", the algorithm 
first identifies roles with similar entities, ARG1, 
PRED, ARG2 and removes them from further quan- 
tification processing while the distinct entities in the 
role MODS-TIME, "after intubation", "after skin in- 
cision", "after start of bypass", and "after coming off 
bypass", are further processed for universal quantifi- 
cation. The role MODS-TIME is further separated 
into two smaller roles, one role with the preposi- 
tions and the other role with different critical points. 
Since the prepositions are all the same, universal 
quantification is only applied to the distinct entities 
in set X, in this case, the four critical points. Queries 
to the CLASSIC ontology indicate that the enti- 
ties in set X, "intubation", "skin-incision", "start- 
of-bypass", and "conaing-off-bypass" match all the 
possible types of the concept c r i t i ca l -po in t ,  sat- 
isfying the domain ontology context in Section 4.2. 
Since set D and set X match exactly, generalization 
and universal quantification can be used to replace 
the references to these entities: "After each criti- 
cal point, Mrs. Doe had tachycardia." The system 
currently does not.perfor.m generMization omeJ~tities 
which failed the univeral quantification test.. In such 
cases, a sentence with conjunction will be generated, 
i.e., "After intubation and skin incision, Mrs. Doe 
had tachycardia." 
In addition to every,  the system generates both 
when the number of entities in set X is two. In 
our application, both is used as a universal quanti- 
tier under discourse context: "Alice had q)isodes of 
bradycardia b@)re inductio1~ and start of bypass, h~ 
both episodes, she received Cefazolin and Phen!lle- 
phrine. " 
When a universal quantifier is under the govern- 
104 
ment of negation, each, a l l ,  every  and both are in- 
appropriate, and any should be used instead. Given 
that the patient went on bypass without compli- 
cations, the system should generate "The patient 
went on bypass without any  problem." In contrast ,  
"The patient went on bypass without every  prob- 
/em.V=-~as-~ a,:differeut.-~meani~g; -,Our, :,system-cur=. 
rently uses any as a universal quantifier when the 
universal quantification is under the government of 
negation, such as "The patient denied any drug al- 
lergy.", or "Her hypertension was controlled without 
any medication." Currently, the generation of nega- 
tion sentences about surgery problems and allergies 
are handled in the content planner. They are not 
synthesized from multiple negation sentences: "The 
patient is not allergic to aspirin. The paitent is not 
allergic to penicillin..." 
5 Generation of Multiple Quantifiers 
When there are two distinct roles across the proposi- 
tions, the algorithm tries to use a universal quantifier 
for one role and an existential quantifier for another. 
To generate sentences with 33, both entities being 
referred to must have no proper names; this triggers 
the use of existential quantifiers. We intentionally 
ignore the cases where two universal quantifiers are 
generated in the same sentence. The likelihood for 
input specifying sentences with W to a text genera- 
tion system is slim. 
When generating multiple quantifiers in the same 
sentence, we differentiate between cases where there 
is or isn't a dependency between the two distinct 
roles. Two roles are independent of each other when 
one is not a modifier of the other. For example, 
the roles ARG1 and ARG2 in a proposition are in- 
dependent. In "Each patient is given a high sever- 
ity rating", performing universal quantification on 
the patients (ARG3) is a separate decision from 
the existential quantification of the severity ratings 
(ARG2). Similarly, in "An abnormal lab result was 
seen in each patient with hypertension after bypass". 
the quantification operations on the abnormal ab 
results and the patients can be performed indepen- 
dently. 
.... When there isa dependency 'between theroles be- 
ing quantified, the quantification process of each role 
might interact because modifiers restrict the range 
of the entities being modified. We found that when 
universal quantification occurs in the MODS role, 
the quantification of PRED and MODS can be per- 
formed independently, just as in the cases withou! 
dependency. Given the input propositions "Alice has 
I I<I in Alice's left arm. Alice has IV-2 in Alice's 
right arm. ", the distinct roles are ARG2 "IV-i" and 
"IV-T',  and ARG2-MODS "in Alice's left arm" and 
"in Alice's right arm". The ARG2-MODS is uni- 
versally quantified based on domain knowledge that 
? Roles without dependency, V Role-l,3 Role-2 
Each patient is given a high severity rating. 
? Roles without dependency, 3 Role-l, 'v' Role-2 
An abnormal lab result was seen in each patient 
geon's name is likely to be known, and the in-  
put is likely to be "Dr. Rose operated on Alice", 
"Dr~ Rose operated on Bob", and "Dr. Rose oper- 
ated on Chris". Given these three propositions, the 
entities in ARG1 and PRED are identical, and only 
with hypertension after bypass, the distinct entities in ARG2, "Alice", "Bob" and 
*Roles  with depend~flcy, V PRED,-3 MODS ............. Ghns:,~-~ d.~be:;~qua,atffied.... ~Wilih-,-am:a;ppropriate 
context, the sentence "Dr. Rose operated on each Every patient with a balloon pump had hyper- 
tension. 
? Roles with dependency, 3 PRED, V MODS 
Alice has an IV in each arm,. 
Figure 3: Sentences with two quantifiers 
a patient is a human and a human has a left arm 
and a right arm. In this example, "an IV in each 
arm", the decision to generate universal and exis- 
tential quantified expressions are independent. But 
in "Every patient with a balloon pump had hyperten- 
sion", the existentially quantified expression "with a 
balloon pump" is a restrictive modifier of its head. In 
this case, the set D does not include all the patients, 
but only the patients "with a balloon pump". When 
computing set D for universal quantification, the al- 
gorithm takes this extra restriction into account by 
eliminating all patients without such a restriction. 
Once a role is universally quantified and the other is 
existentially quantified, our algorithm replaces both 
roles with the corresponding quantified expressions. 
Figure 3 shows the sentences with multiple quanti- 
tiers generated by applying our algorithm. 
5.1 Ambiguity Revisited 
In Section 4.3, we described how to minimize the 
ambiguity between distributive and collective read- 
ings when generating universal quantitiers. What 
about the scope ambiguity when there are muhiple 
quantifiers in the same sentence? If we look at the 
roles which are being universally and existentially 
quantified in our examples in Figure 3, it is inter- 
esting to note that the universal quantifiers always 
have wider scope than the existential quantifiers. In 
the first, example, ,the.scope: order is Vpatient~high- 
severity-rating, the second example is Vpatient31ab- 
result, the third is Vpatient3balloon-pump, and the 
fourth is Varm3IV. The scope orderings are all V3. 
\Vhat happens if a sentence contains an existen- 
tial quantifier which has a wider scope than a uni- 
versal quantifier? In "A suryeon operated on each 
patient.", tile normal reading is Vpatienl3surgeon. 
13ut~ if the existentially quantified noun phrase 
"'a surgeon" refers to tile same surgeon, as in 
3surgeonVpatient. tlle system would generate "(A 
particular/The same) surgeon operated on each pa- 
tient." In an applied generation system, the sur- 
patient" will be generated. If the name of the sur- 
geon is not available but the identifiers for the sur- 
geon entities across the propositions are the same, 
the system will generate "The same surgeon oper- 
ated on each patient." As this example indicates, 
when 3 has a wider scope than V, the first step in 
our algorithm (described in Section 4.1), identify- 
ing roles with distinct entities, would eliminate the 
roles with identical entities from further quantifica- 
tion processing. Based on our algorithm, the sen- 
tences with 3V readings are taken care of by the first 
step, identifying roles with distinct entities, while V3 
cases are handled by quantification operations for 
multiple roles, as described in Section 5. 
In Section 4.3, we mentioned that it is important 
to know exactly what blood products are used in 
our application. As a result, the system would not 
generate the sentence "Each patient received a blood 
product." when the input propositions are "Alice re- 
ceived packed red blood cells. Bob received platelets. 
Chris received platelets." Even though tim conjoined 
entities can be generalized to "blood product", this 
quantification operation would violate our precondi- 
tion for using existential quantifiers: the descriptions 
for each of the conjoined entities must be indistin- 
guishable. Here, one is "red blood cells" and tile oth- 
ers are "platelets". Given these three propositions, 
the system would generate "Alice received packed 
red blood cells, and Bob and Chris, platelets." based 
on the algorithm described in (Shaw. 1998). If in 
our domain the input propositions could be "'Al- 
ice received blood-product-1. Bob received blood- 
product-2. Chris received blood-product-2.", where 
each instance of blood-product-n could be realized 
as "blood product", then the system would generate 
"Each patient received a blood product." since the 
description of conj0ined entities are not dist~inguish - 
able at the surface level. 
6 Conc lus ion  
We have described the quantification operators that 
can make the text more concise while preserving the 
original semantics in the input propositions. Though 
we would like to incorporate imprecise quantifiers 
such as few. many, some into our system because 
they have potential to drastically reduce the text. 
further, these quantifiers do not, have the desired 
property ill which the readers can recover the exact. 
entities in the input propositions. The property of 
105 
preserving the original semantics i very important 
since it guarantees that even though the surface x- 
pressions are modified, the information is preserved. 
This property allows the operators to be domain in- 
dependent and reusable in different natural language 
Norman Creaney. 1999. Generating quantified logi: 
cal forms from raw data. In Proe. of the ESSLLI- 
99 Workshop on the Generation of Nominal Ex- 
pressions. 
M. Dalal~ S. Feiner, K. McKeown, D. Jordan, 
generation systems. B. Allen, and Y. alSafadi. 1996. MAGIC: An 
We have described: an. algo_r.itlma :which.sy.stemati .............. e:~cpertimeeataL:aystem..for: genetattiag~ .multimedia 
cally derives quantifiers from input propositions, dis- 
course history and ontological information. We iden- 
tified three types of information from the discourse 
and ontology to determine if a universal quantifier 
can be applied. We also minimnized the ambiguity 
between distributive and collective readings by se- 
lecting an appropriate universal quantifier. Most 
importantly, for multiple quantifiers in the same sen- 
tence, we have shown how our algorithm generates 
different quantifed expressions for different scope or- 
derings. 
7 Acknowledgement  
We would like to thank anonymous reviewers for 
valuable comments. The research is supported in 
part by the National Library of Medicine under 
grant LM06593-02 and the Columbia University 
Center for Advanced Technology in High Perfor- 
mance Computing and Communications in Health- 
care (funded by the New York State Science and 
Technology Foundation). Any opinions, findings, or 
recommendations expressed in this paper are those 
of the authors and do not necessarily reflect the 
views of the above agencies. 
Re ferences  
Jon Barwise and Robin Cooper. 1981. Generalized 
quantifiers and natural anguage. Linguistics and 
Philosophy, 4:159-219. 
Alexander Borgida, Ronald Brachman, Deborah 
McGuinness, and Lori Alperin Resnick. 1989. 
CLASSIC: A structural data model for objects. 
In A CM SIGMOD International Conference on 
Management of Data. 
Bob Carpenter. 1997. Type-LogicaISemanties. MIT 
Press, Cambridge, Massachusetts. 
James J. Cimino, Paul D. Clayton, George Hripc- 
sak, and Stephen B. Johnson,: 1994. Knowledge-. 
based approaches to the maintenance of a large 
controlled medical terminology. The Journal of 
the American Medical lnformatics Association, 
1(1):35-50. 
Ann Copestake, Dan Flickinger, Ivan A. Sag. and 
Carl J. Pollard. 1999. Minimal recursion seman- 
tics: An introduction. Manuscript available via 
ht tp://lingo.stan ford.edu/pubs.ht ml.
Norman Creaney. 1996. An algorithm for generat- 
ing q~rantifiers. In Proc. of the 8th International 
Workshop on Natural Language Generation, Sus- 
sex, UK. 
briefings about post-bypass patient status. In 
Proc. 1996 AMIA Annual Fall Syrup, pages 684- 
688, Washington, DC, October 26-30. 
Robert Dale and Ehud Reiter. 1995. Computational 
interpretations of the gricean maxims in the gener- 
ation of referring expressions. Cognitive Science, 
19:233-263. 
Robert Dale. 1992. Generating Referring Expres- 
sions: Constructing Descriptions in a Domain of 
Objects and Processes. MIT Press, Cambridge, 
MA. 
Henriette de Swart. 1998. Introduction to Natural 
Language Semantics. CSLI Publications. 
Michael Elhadad and Jacques Robin. 1992. Con- 
trolling content realization with functional unifi- 
cation grammars. In Aspects of Automated Nat- 
ural Language Generation, Lecture Notes in Ar- 
tificial Intelligence, 587, pages 89-104. Springer- 
Verlag, Berlin, April. 
Pierre-Joseph Gailly. 1988. Expressing quantifier 
scope in French generation. In Proceedings of the 
12th International Conference on Computational 
Linguistics (COLING-88), volumne 1, pages 182- 
184, Budapest, August 22-27,. 
Barbara J. Grosz, Douglas E. Appelt, Paul A. 
Martin, and Fernando C. N. Pereira. 1987. 
TEAM: An experiment in the design of trans- 
portable natural-language interfaces. Artificial 
Intelligence, 32(2):173-243, May. 
Jerry Hobbs and Stuart Shieber. 1987. An algo- 
rithm for generating quantifier scopings. Compu- 
tational Linguistics, 13(1-2):47-63, January-June. 
Helmut Horacek. 1997. An algorithm for generating 
referential descriptions with flexible interfaces. In 
Proc. of the 35th ACL and 8th EACL, pages 206 
213. 
Ronald M. Kaplan and Joan Bresnan. 1982. 
..... ,-Lexical-functional,granmaar:, A formal system for 
grammatical representation. I  Joan Bresnan, ed- 
itor, The Mental Representation of Grammatical 
Relations, chapter 4. MIT Press. 
Martin Kay. 1979. Functional grammar. In Proceed- 
ings of the 5th Annual Meeting of the Berkeley 
Linguistic Society, pages 142-158, Berkeley, CA, 
February 17-19, 
Alistair Knott, Mick O'Donnell, Jon Oberlander. 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proc. of 
the 6th European Workshop on Natural Language 
Generation, Duisburg, Germany. 
106 
James D. McCawley. 1981. Everything that linguists 
have always wanted to know about logic (but were 
ashamed to ask). University of Chicago Press. 
Kathleen MeKeown, Shimei Pan, James Shaw, 
Desmond Jordan, and Barry Allen. 1997. Lan- 
guage Engine, pages 11-38. MIT Press, Cam- 
bridge, MA. 
Zeno Vendler. 1967. Each and every, any and all. 
In Linguistics in Philosophy, pages 70-96. Cornell 
University Press, Ithaca and London. 
guage generation for multimedia healthcare brief- Leo Wanner and Eduard Hovy. 1996. The Health- 
ings. In Proc. of the Fifth:~'-AGl~: Cort\[,,~xm A:NL P, ~. -~?.:~. Doe~,.sentence, :planner.. qw:Proc~.:.of 4he~.Sth :.fnter- 
pages 277-282. 
Kathleen R. McKeown. 1985. Tezt Generation: Us- 
ing Discourse Strategies and Focus Constraints to 
Generate Natural Language Tezt. Cambridge Uni- 
versity Press, Cambridge. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proc. of the 9th Inter- 
national Workshop on Natural Language Genera- 
tion., pages 28-37. 
George Miller, Richard Beckwith, Christiane Fell- 
baum, Derek Gross, and Katherine Miller. 1990. 
Five papers on WordNet. CSL Report 43, Cogni- 
tive Science Laboratory, Princeton University. 
Douglas B. Moran and Fernando C. N. Pereira. 
1992. Quantifier scoping. In Hiyan Alshawi, ed- 
itor, The Core Language Engine, pages 149-172. 
MIT Press, Cambridge, MA. 
Jong C. Park. 1995. Quantifier scope and con- 
stituency. In Pvoc. of the 33rd ACL, pages 205- 
212. 
Rebecca Passonneau, Karen Kukich, Vasileios Hatzi- 
vassiloglou, Larry Lefkowitz, and Hongyan Jing. 
1996. Generating summaries of work flow di- 
agrams. In Proc. of the International Confer- 
ence on Natural Language Processing and Indus- 
trial Applications, pages 204-210, New Brunswick, 
Canada. University of Moncton. 
Fernando C. N. Pereira. 1990. Categorial semantics 
and scoping. Computational Linguistics, 16( 1): 1- 
10. 
Owen Rainbow and Tanya Korelsky. 1992. Applied 
text generation. In Proceedings of the Third A CL 
Conference on Applied Natural Language Process- 
ing, pages 40-47, Trento, Italy. 
Ehud Reiter and Robert Dale. 1992. A fast algo- 
rithm for the generation of referring expressions. 
In Proceedings of the I4th International Con- 
ference on Computational Linguistics (COLING- 
92), pages 232-238, Nantes, France. 
Jacques Robin. t995. Revision-Based Generation of 
Natural Language Sum maries Providing Historical 
Background. Ph.D. thesis, Columbia University. 
James Shaw. 1998. Segregatory coordination and el- 
lipsis in text generation, tn Proc. of the 17th COL- 
I.'VG and the 36th .4m~ual Meeting of the ACL.. 
pages 1220-1226. 
Jan van Eijck and Hiyan Alshawi. 1992. Logical 
forms. In Hiyan Alshawi, editor, The Core Lan- 
national Workshop on Natural Language Genera- 
tion, pages 1-10, Sussex, UK. 
William A. Woods. 1978. Semantics and quantifi- 
cation in natural language question answering. In 
Advances in Computers, volume 17, pages 1-87. 
Academic Press. 
Frans Zwarts. 1983. Determiners: a relational 
perspective. In A. ter Meulen, editor, Studies 
in model-theoretic semantics, pages 37-62. Dor- 
drecht: Forts. 
107 
Integrating a Large-scale, Reusable Lexicon with a Natural 
Language Generator 
Hongyan 3 ing  
Department of Computer  Science 
Columbia University 
New York, NY 10027, USA 
hjing@cs.columbia.edu 
Yael Dahan Netzer 
Department of Computer  Science 
Ben-Gurion University 
Be'er-Sheva, 84105, Israel 
yaeln@cs.bgu.ac.il 
Michael Elhadad 
Department  of Computer  Science 
Ben-Gurion University 
Be'er-Sheva, 84105, Israel 
elhadad@cs.bgu.ac.i l  
Kathleen R. McKeown 
Department of Computer  Science 
Columbia University 
New York, NY 10027, USA 
kathy@cs.columbia.edu 
Abst rac t  
This paper presents the integration of a large- 
scale, reusable lexicon for generation with the 
FUF/SURGE unification-based syntactic realizer. 
The lexicon was combined from multiple xisting re- 
sources in a semi-automatic process. The integra- 
tion is a multi-step unification process. This inte- 
gration allows the reuse of lexical, syntactic, and 
semantic knowledge ncoded in the lexicon in the 
development of lexical chooser module in a genera- 
tion system. The lexicon also brings other benefits 
to a generation system: for example, the ability to 
generate many lexical and syntactic paraphrases and 
the ability to avoid non-grammatical output. 
1 In t roduct ion  
Natural language generation requires lexical, syn- 
tactic, and semantic knowledge in order to produce 
meaningful and fluent output. Such knowledge is 
often hand-coded anew when a different application 
is developed. We present in this paper the integra- 
tion of a large-scale, reusable lexicon with a natural 
language generator, FUF/SURGE (Elhadad, 1992; 
Robin, 1994); we show that by integrating the lexi- 
con with FUF/SURGE as a tactical component, we 
can reuse the knowledge ncoded in the lexicon and 
automate to some extent he development of the lex- 
ical realization component in a generation applica- 
tion. 
The integration of the lexicon with FUF/SURGE 
also brings other benefits to generation, including 
the possibility to accept a semantic input at the 
level of WordNet synsets, the production of lexical 
and syntactic paraphrases, the prevention of non- 
grammatical output, reuse across applications, and 
wide coverage. 
We present he process of integrating the lexicon 
with FUF/SUR(;E. including how to represenl the 
lexicon in FUF format, how to unify input with the 
lexicon incrementally to generate more sophisticated 
and informative representations, and how to design 
an appropriate semantic input format so that the 
integration of the lexicon and FUF/SURGE can be 
done easily. 
This paper is organized as follows. In Section 2, 
we explain why a reusable lexical chooser for gen- 
eration needs to be developed. In Section 3, we 
present he large-scale, reusable lexicon which we 
combined from multiple resources, and illustrate its 
benefits to generation by examples. In Section 4, we 
describe the process of integrating the lexicon with 
FUF/SURGE, which includes four unification steps, 
with each step adding additional lexical or syntac- 
tic information. Other applications and comparison 
with related work are presented inSection 5. Finally, 
we conclude by discussing future work. 
2 Bu i ld ing  a reusab le  lex ica l  chooser  
for generat ion  
While reusable components have been widely used in 
generation applications, the concept of a "reusable 
lexical chooser" for generation remains novel. 
There are two main reasons why such a lexical 
chooser has not been developed in the past: 
1. In the overall architecture of a generator, the 
lexical chooser is an internal component that 
depends on the semantic representation a d for- 
.:malism and onthe syntactic realizer used by the 
application. 
2. The lexical chooser links conceptual e ements to 
lexical items. Conceptual elements are by defi- 
nition domain and application dependent ( hey 
are the primitive concepts used in an applica- 
tion knowledge base). These primitives are not 
easily ported from application to application. 
209 
The emergence of standard architectures for gen- 
erators (RAGS, (Reiter, 1994))and the possibility 
to use a standard syntactic realizer answer the first 
issue. 
To address the second issue, one must realize that 
if the whole lexical chooser can not be made domain- 
independent, major parts can be made reusable. 
The main argument is that lexical knowledge is mod- 
ular. Therefore, while choice of words is constrained 
by domain-specific conceptual knowledge (what in- 
formation the sentences are to represent) on the one 
hand, it is also affected by several other dimensions: 
* inter-lexical constraints: collocations among 
words 
o pragmatic onstraints: connotations of words 
o stylistic constraints: familiarity of words 
* syntactic constraints: government patterns of 
words, e.g., thematic structure of verbs. 
We show in this paper how the separation of the 
syntactic and conceptual interfaces of lexical item 
definitions allows us to reuse a large amount of lex- 
ical knowledge across appli.cations. 
3 The  lex icon  and  i t s  benef i t s  to  
generat ion  
3.1  A large-scale,  reusab le  lexicon for 
generat ion  
Natural Language generation starts from semantic 
concepts and then finds words to realize such seman- 
tic concepts. Most existing lexical resources, how- 
ever, are indexed by words rather than by semantic 
concepts. Such resources, therefore, can not be used 
for generation directly. Moreover, generation eeds 
different ypes of knowledge, which typically are en- 
coded in different resources. However, the different 
representation formats used by these resources make 
it impossible to use them simultaneously in a single 
system. 
To overcome these limitations, we built a large- 
scale, reusable lexicon for generation by combining 
multiple existing resources. The resources that are 
combined include: 
o Tile WordNet Lexical Database (Miller et al, 
1990). WordNet is the largest lexical database 
to date, consisting of over 120,000 unique words 
(version 1.6). It also encodes many types of 
lexical relations between words, including syn- 
onytny, antonymy, and many more. 
o English Verb Classes and Alternations 
(EVCA) (Levin, 1993). It categorized 3.104 
verbs into classes based on their syntactic 
properties and studied verb alternations. An 
alternation is a variation in the realization of 
verb arguments. For example, the alternation 
"there-insertion" transforms A ship appeared 
~-on..the horizon_to There,appeared a ship..o~....the 
horizon. A total of 80 alternations for 3,104 
verbs were studied. 
The COMLEX syntax dictionary (Grishman et 
al., 1994). COMLEX contains syntactic infor- 
mation for over 38,000 English words. 
The Brown Corpus tagged with WordNet senses 
(Miller et al, 1993). We use this corpus for 
frequency measurement. . 
In combining these resources, we focused on verbs, 
since they play a more important role in deciding 
sentence structures. The combined lexicon includes 
rich lexical and syntactic knowledge for 5,676 verbs. 
It is indexed by WordNet synsets(which are at the 
semantic oncept level) as required by the generation 
task. The knowledge in the lexicon includes: 
Q A complete list of subcategorizations for each 
sense of a verb. 
o A large variety of alternations for each sense of 
a verb. 
o Frequency of lexical items and verb subcatego- 
rizations in the tagged Brown corpus 
Rich lexicat relations between words 
The sample entry for the verb "appear" is shown 
in Figure 1. It shows that the verb appear has eight 
senses (the sense distinctions come from WordNet). 
For each sense, the lexicon lists all the applicable 
subcategorization for that particular sense of the 
verb. The subcategorizations are represented using 
the same format as in COMLEX. For each sense, 
the lexicon also lists applicable alternations, which 
we encoded based on the information in EVCA. In 
addition, for each subcategorization a d alternation, 
the lexicon lists the semantic ategory constraints on 
verb arguments. In the figure, we omitted the fre- 
quency information derived from Brown Corpus and 
lexical relations (the lexical relations are encoded in 
WordNet). 
The construction of the lexicon is semi-automatic. 
First, COMLEX and EVCA were merged, produc- 
ing a list of syntactic subcategorizations and alter- 
nations for each verb. Distinctions in these syntac- 
tic restrictions according to each sense of a verb 
are achieved in the second stage, where WordNet 
is merged with the result of the first step. Finally, 
the corpus information is added, complementing the 
static resources with actual usage counts for each 
syntactic pattern. For a detailed description of the 
combination process, refer to (Jing and Mchieown, 
1998). 
210 
appear: 
sense  1 give an impress ion  
((PP-TO-INF-gS :PVAL ("to") :SO ((sb,  - ) ) )  
(TO-INF-RS :S0 ((sb, --))) 
(NP-PRED-RS :S0 ((sb,  --))) 
(ADJP-PRED-RS :SO ((sb,  - )  (sth, - - ) ) ) ) )  
sense 2 become v is ib le  
((PP-T0-INF-KS :PVAL ("to") 
:S0 ((sb, -) (sth, -))) 
(INTRANS TIIERE-V-SUB J 
. . . . . . . . .  . . _  
: ALT there-insertion 
:S0 ((sb, --) (sth, --)))) 
sense 8 have an outward express ion 
((NP-PRED-RS :SO ((sth, --))) 
(ADJP-PRED-RS :S0 ((sb, --) (sth, --)))) 
Figure I: Lexicon entry for the verb appear 
3.2 The  benefits of  the  lex icon  
There are a number of benefits that this combined 
lexicon can bring to language generation. 
First, the use of synsets as semantic tags can 
help map an application conceptual model to lexi- 
cal items. Whenever application concepts are repre- 
sented at the abstraction level of a WordNet synset, 
they can be directly accepted as input to the lexi- 
con. By this way, the lexicon can actually lead to 
the generation of many lexical paraphrases. For ex- 
ample, (look, seem, appear} is a WordNet synset; it 
includes a list of words that can convey the seman- 
tic concept ' 'g ive  an impression o f '  '. We can 
use synsets to find words that can lexicalize the se- 
mantic concepts in the semantic input. By choosing 
different words in a synset, we can therefore gen- 
erate lexical paraphrases. For instance, using the 
above synset, the system can generate the following 
paraphrases: 
"He seems happy. "
"He looks happy. "
"He appears happy.'" 
Secondly, the subcategorization information i  the 
lexicon prevents generating a non-grammatical out- 
put. As shown in Figure 1, the lexicon lists appli- 
cable subcategorizations for each sense of a verb. It 
will not allow the generation of sentences like 
"*He convinced me in his innocence" 
(wrong preposition) 
"*He convinced to go to the party" 
(missing object) 
"*Th.e bread cuts" 
(missing adverb (e.g., "'easily" )) 
"*The book consists three parts" 
( m issing t)reposit.ion) 
In addition, alternation information can help gen- 
erate .syntactic paraphrases. For instance, using 
the "simple reciprocal intransitive" alternation, the 
system can generate the following syntactic para- 
phrases: ? , 
"Brenda agreed with Molly." 
"Brenda and Molly agreed?" 
"Brenda and Molly agreed with each other." 
Finally, the corpus frequency information can help 
............... _the.lexicat.. -~ice.proeesa~.,When:multiple .words can 
be used to realize a semantic oncept, the system 
can use corpus frequency information in addition 
to other constraints to choose the most appropriate 
word. 
The knowledge ncoded in the lexicon is general, 
thus it can be used in different applications. The 
lexicon has wide coverage: the final lexicon consists 
of 5,676 verbs in total, over 14,100 senses (on average 
2.5 senses/verb), and over 11,000 semantic oncepts 
(synsets). It uses 147 patterns to represent the sub- 
categorizations and includes 80 alternations. 
To exploit the lexicon's many benefits, its format 
must be made compatible with the architecture of a 
generator. We have integrated the lexicon with the 
FUF/SURGE syntactic realizer to form a combined 
lexico-grammar. 
4 Integration Process 
In this section, we first explain how lexical choosers 
are interfaced with FUF/SURGE. We then describe 
step by step how the lexicon is integrated with 
FUF/SURGE and show that this integration pro- 
cess helps to automate the development of a lexical 
realization component. 
4.1 FUF /SURGE and the lexical chooser 
FUF (Elhadad, 1992) uses a functional unification 
formalism for generation. It unifies the input that a 
user provides with a grammar to generate sentences. 
SURGE (Elhadad and Robin, 1996) is a comprehen- 
sive English Grammar written in FUF. Tile role of 
a lexical realization component is to map a semantic 
representation drawn from the application domain 
to an input format acceptable by SURGE, adding 
necessary lexical and syntactic information during 
this process. 
Figure 2 shows a sample semantic input (a), the 
lexicalization module that is used to map this se- 
mantic input to SURGE input (b), and 'thefinal 
SURGE input (c) - -  taken from a real application 
system(Passoneau et al, 1996). The functions of the 
lexicalization module include selecting words that 
can be used to realize the semalltic oncepts in the 
input, adding syntactic features, and mapping tile 
arguments in tile semantic input to the thematic 
roles in SURGE. 
211 
Sentence :  / t  has 24 activities, including 20 tasks and four decisions. 
concept 
args 
total-node-count 
theme concept 
ref 
concept 
rheme args 
pronounPr?cess-fl?wgraph \] 
elaboration 
concept 
theme args 
expansion concept 
args 
cardinality \] 
\[ theme \[1\] \] / 
t value \[21 l -I. s.ubset-node-countJ 
concept flownode \] 
\[1\] = ref full 
concept 
proc 
partic 
cat  
proc 
partic 
\[2\] = 
concept cardinal \] 
cardinal 24 
ref full 
(a) The semantic input (i.e., input of lexicalization module) 
#(under  total-node-count) 
type possessive \] 
possessor cat pronoun / 
i 
cat common 
cardinal \[ value 
definite no 
head 
possessed 
qualifier 
\[,l\] 
lex "activity" \] 
cat clause 
mood present-participle 
type locative 
proc lex "include" 
partic location \[ cat 
k 
(b) Tile lexicalization module 
\] 
clause 
type possessive \] 
possessor cat pronoun / 
I 
cat  COn l l l l on  
cardinal \[ value 24 \] 
definite no 
head lex "activhy" \] 
possessed cat clause 
mood present-participle 
type locative \] 
qualifier proc lex "include" 
(c) Tile SURGE input (ie., output of lexicalization module) 
1 
I 
I 
I 
I 
I 
Figure 2: A samph~ lexicalization component 
212 
The development of the lexicalizer component was 
done by hand in the past. Furthermore, for. each 
new application, a new lexicatizer component had 
to be written despite the fact that some lexical and 
syntactic information is repeatedly used in different 
applications. The integration process we describe, 
however, partially automates this process. 
4.2 The  in tegrat ion  s teps  
The integration of the lexicon with FUF/SURGE 
is done through incremental unification, using four 
unification steps as shown in Figure 3. Each  step 
adds information to the semantic input, and at the 
end of the four unification steps, the semantic input 
has been mapped to the SURGE input format. 
(1) The semantic input 
Different generation systems usually use different 
representation formats for semantic input. Some 
systems use case roles ; some systems use flat 
attribute-value r presentation (Kukich et al, 1994). 
For the integrated lexicon and FUF/SURGE pack- 
age to be easily pluggable in applications, we need to 
define a standard semantic input format. It should 
be designed in such a way that applications can eas- 
ily adapt their particular semantic inputs to this 
standard format. It should also be easily mapped 
to the SURGE input format. 
In this paper, we only consider the issue of seman- 
tic input format for the expression of the predicate- 
argument relation. Two questions need to be an- 
swered in the design of the standard semantic input 
format: one, how to represent semantic oncepts; 
and two, how to represent he predicate-argument 
relation. 
We use WordNet synsets to represent semantic 
concepts. The input can refer to synsets in several 
ways: either using a globally unique synset num- 
ber I or by specifying a word and its sense number 
in WordNet. 
The representation of verb arguments is a more 
complicated issue. Case roles are frequently used in 
generation systems to represent verb arguments in 
semantic inputs. For example, (Dorr et al, 1998) 
used 20 case roles in their lexical conceptual struc- 
ture corresponding to underlying positions in a com- 
positional lexical structure. (Langkilde and Knight. 
1998) use a list of case roles in their interlingua rep- 
resentations. 
We decided to use numbered arguments (similar to 
the DSyntR in MTT (Mel'cuk and Perstov, 1987)) 
instead of case roles. The difference between the two 
1Since there are a huge number of synsets in WordNet, we 
will provide a searchable database of synsets o that users can 
look up a synset and its index number easily. For a part icular 
appl ication, users can adapt  the synsets to their specific do- 
main, such as removing non-relevant synsets, merging synsets. 
and relabel ing the synsets for convenience, as discussed in 
(,ling, 1998). 
is not critical but the numbered argument approach 
? avoids the need? to commit: the: lexicon to a specific 
ontology and seems to be easier to learn 2. 
Figure 4 shows a sample semantic input. For easy 
understanding, we refer to  the semantic concepts 
using their definitions rather than numerical index 
numbers. There are two arguments in the input. 
The intended output sentence for this semantic in- 
put is "A boat appeared on the horizon" or its para- 
phrases. 
(2) Lexical unification 
In this step, we map the semantic oncepts in the " 
semantic input to concrete words. To do this, we use 
the synsets in WordNet. All the words in the same 
synset can be used to convey the same semantic on- 
cept. For the above example, the semantic oncepts 
"become visible" and "a small vessel for travel on 
water" can be realized by the the verb appear and 
the noun boat respectively. This is the step that can 
produce lexical paraphrases. Note that when the 
system chooses a word, it also determines the par- 
ticular sense number of the word, since a word as 
it belongs to a synset has a unique sense number in 
WordNet. 
We represented all the synsets in Wordnet in FUF 
format. Each synset includes its numerical index 
number and the list of word senses included in the 
synsets. This lexical unification, works for both 
nouns and verbs. 
(3) Structural unification 
After the system has chosen a verb (actually a 
particular sense of a verb), it uses that information 
as an index to unify with the subcategorization a d 
alternations the particular verb sense has. This step 
adds additional syntactic information to the origi- 
nal input and has the capacity to produce syntactic 
paraphrases using alternation information. 
(4) Constraints on the number of arguments 
Next, we use the constraints that a subcategoriza- 
tion has on the number of arguments it requires to 
restrict unification with subcategorization patterns. 
\~k~ use 147 possible patterns. For example, the in- 
put in Figure 4 has two arguments. Although IN- 
TRANS (meaning intransitive) is listed as a possi- 
ble subcategorization pattern for "appear" (see sense 
2 in Figure 1), the input will fail to unify with it 
since INTRANS requires a single argument only. 
This prevents the generation of non-grammatic'A 
sentences. This step adds a feature which specifies 
the transitivity of the verb to FUF/SURGE input, 
selecting one from the lexicon when there is more 
than one possibility for the given verb. 
2The difference between numbered arguments and labeled 
roles is s imi lar  to that between amed semantic primit ives and 
synsets in \.VordNet. Verb classes share the same definition 
of which argument is denoted by l, 2 etc. if they share some 
syntact ic properties as far as argument aking properties are  
concerned. 
213 
Semantic input Synsets verbs lexicon si~ucts Input for SURGE 
Figure 3: The integration process 
\[rel-- i--ept --evisible J 1\] 
1 \[ concept  "a  smal l  vesse l  fo r  t rave l  on  water ' '  \] 
args 2 \[ concept ' ' the  l i ne  at  which the sky and Earth appear to  meet' '  \] 
Figure 4: The semantic input using numbered arguments 
(5) Mapping structures to SURGE input 
In the last step, the subcategorization a d alter- 
nations are mapped to SURGE input format. The 
mapping from subcategorizations to SURGE input 
was manually encoded in the lexicon for each one 
of the 147 patterns. This mapping information can 
be reused for all applications, which is more effi- 
cient than composing SURGE input in the lexical- 
ization component of each different application. Fig- 
ure 5 shows how the subcategorization NP-WITH- 
NP (e.g., The clown amused the children with his 
antics) is mapped to the SURGE input format. This 
mapping mainly involves matching the numbered ar- 
guments in the semantic input to appropriate l xical 
roles and syntactic ategories so that FIJF/SURGE 
can generate them in the correct order. 
The final SURGE input for the sentence ",4 boat 
appeared on the horizon" is shown in Figure 6. Us- 
ing the "THERE-INSERTION" alternation that the 
verb "appear" (sense 2) authorizes, the system can 
also generate the syntactic paraphrase "There ap- 
peared a boat on the hor izon".  The SURGE input 
the system generates for "There appeared a boat on 
the horizon" is very different .from that for "A boat 
appeared on the horizon".  
It is possible that for a given application some 
generated paraphrases are not appropriate. In this 
case, users can edit the synsets and the alternations 
to filter out tile paraphrases tile) do not want. 
Tile four unification steps are completely auto- 
matic. Tile system can send feedback upon failure 
struct 
relation 
args 
proc 
lex-roles 
np-with-np 
1 \[21<...> 
2 \[al<...> 
3 \[41<...> 
type lexical 
lex Ill 
t 
1 
2 
subcat 2 
3 
\ [1  \[all 2 \[3\] 
3 \[41 
cat np \] 
121 
\[rat .p \] 
\[al 
cat ip  
prep lex 
np \[41 
"with" \] 1 
Figure 5: Mapping subcategorization "NP-\VITH- 
NP" to SURGE input 
of unification. 
5 Re la ted  Work  
The lexicon, after it is integrated with 
FUF/SURGE, can also be used for other tasks in 
language generation. For example, revision (Robin, 
1994) is a technique for building semantic inputs 
incrementally. The revision process decides whether 
it is appropriate to attach a new constituent to the 
current semantic input, for example, by adding an 
214 
relation 
args 
struct 
argl 
cat 
lexical-roles 
concept 
word 
1 concept 
word 
concept 
2 word 
ppb 
2 ~ given 
c lause c 
d 
c 'become ~is ib le '  ' \] 
\] "appear"a 
'a small  vessel  for travel on water'' \] 
J "boa~"a 
'Cthe l ine  at  which the sky and Earth appear to meet \] 
"hor,izon ''a \] 
"Enriched in first step 
bEnriched in second step 
CEnriched in third step 
dEnriched in fourth step 
Figure 6: SURGE input for "A boat appeared on the horizon" 
object or an adverb. Such decisions are constrained 
by syntactic properties of verbs. The integrated 
lexicon is useful to verify these properties. 
Nitrogen (Langkilde and Knight, 1998), a natural 
language generation system developed at ISI, also 
includes a large-scale l xicon to support the genera- 
tion process. Given that Nitrogen and FUF/SURGE 
use very different methods for generation, the way 
that we integrate the lexicon with the generation sys- 
tem is also very different. Nitrogen combines ym- 
bolic rules with statistics learned from text corpora, 
while FUF/SURGE is based on Functional Unifica- 
tion Grammar. Other related work includes (Stede, 
1998), which suggests a lexicon structure for multi- 
lingual generation in a knowledge-based generation 
system. The main idea is to handle multilingual gen- 
eration in the same way as paraphrasing of the same 
language. Stede's work concerns mostly the lexical 
semantics of the transitivity alternations. 
6 Conc lus ion  
We have presented in this paper the integration of 
a large-scale, reusable lexicon for generation with 
FUF/SURGE, a unification-based natural language 
generator. This integration makes it possible to 
reuse major parts of a lexical chooser, which is tile 
component in a generation system that is responsi- 
ble for mapping semantic inputs to surface genera- 
tor inputs. We show that although the whole lexical " 
chooser can not be made domain-independent, it is 
possible to reuse a large amount of lexical, syntactic, 
and semantic knowledge across applications. 
In addition, tile lexicon other benefits to a genera- 
tion system, inchiding the abilities to generate nlany 
lexical paraphrases automatically, generate syntac -  
tic paraphrases, av(fid n(m-grammatical output, and 
choose the most frequently used word when there is 
more than one candidate words. Since the lexical, 
syntactic, and semantic knowledge ncoded in the  
lexicon is general and the lexicon has a wide cover- 
age, it can be reused for different applications. 
In the future, we plan to validate the paraphrases 
the lexicon can generate by asking human subjects to 
read the generated paraphrases and judge whether 
they are acceptable. We would like to investigate 
ways that can systematically filter out paraphrases 
that are considered unacceptable. We are also inter- 
ested in exploring the usage of this system in multi- 
lingual generation. 
Re ferences  
B. J. Doff, N. Habash, 
A thematic hierarchy 
from lexical-conceptual. 
and D. Traum. 1998. 
for efficient generation 
Technical Report CS- 
TR-3934, Institute for Advanced Computer Stud- 
ies, Department of Computer Science, University 
of Maryland, October. 
M. Elhadad and J. Robin. 1996. An overview of 
SURGE: a re-usable comprehensive syntactic re- 
alization component. In INLG'96, Brighton, UK. 
(demonstration session). 
M. Elhadad. 1992. Using Argumentation to Control 
Lezical Choice: A Functional Unification-Based 
Approach. Ph.D. thesis, Department of Computer 
Science, Columbia University. 
R. Grishman, C. Macleod, and A. Meyers. 1994. 
COMLEX syntax: Building a computational 
lexicon. In Proceedings of COLING'94, Kyoto, 
,Japan. 
H.. l ing and K. McKeown. 1998. Combining mul- 
tiple, large-scale resources in a reusable lexicon 
for natural language generation. In Proceedings 
215 
of the 36th Annual Meeting of the Association for 
Computational Linguistics and the .17th Interna- 
tional Conference on Computational Linguistics, 
volume 1, pages 607-613, Universit(~ de MontrEal, 
Quebec, Canada, August. 
H. Jing. 1998. Applying wordnet o natural an- 
guage generation. In Proceedings of COLING- 
ACL'98 workshop on the Usage of WordNet in 
Natural Language Processing Systems, University 
of Montreal, Montreal, Canada, August. 
K. Kukich, K. McKeown, J. Shaw, J. Robin, N. Mor- 
gan, and J. Phillips. "1994. User-needs analysis 
and design methodology for an automated oc- 
ument generator. In A. Zampolli, N. Calzolari, 
and M. Palmer, editors, Current Issues in Com- 
putational Linguistics: In Honour of Don Walker. 
Kluwer Academic Press, Boston. 
I. Langkilde and K. Knight. 1998. The practical 
value of n-grams in generation. In INLG'98, pages 
248-255, Niagara-on-the-Lake, Canada, August. 
B. Levin. 1993. English Verb Classes and Alterna- 
tions: A Preliminary Investigation. University of 
Chicago Press, Chicago, Illinois. 
I.A. Mel'cuk and N.V. Perstov. 1987. Surface- 
syntax of English, a formal model in the 
Meaning Text Theory. Benjamins, Amster- 
dam/Philadelphia. 
G. Miller, R. Beckwith C. Fellbaum, and D. Gross K. 
Miller. 1990. Introduction to WordNet: An on- 
line lexical database. International Journal of 
Lexicography (special issue), 3 (4) :235-312. 
G.A. Miller, C. Leacock, R. Tengi, and R.T. Bunker. 
1993. A semantic oncordance. Cognitive Science 
Laboratory, Princeton University. 
R. Passoneau, K. Kukich, J. Robin, V. Hatzivas- 
siloglou, L. Lefkowitz, and H. Jing. 1996. Gen- 
erating summaries of workflow diagrams. In Pro- 
ceedings of the International Conference on Nat- 
ural Language Processing and Industrial Appli- 
cations (NLP-IA'96), Moncton, New Brunswick, 
Canada. 
E. Reiter. 1994. Has a consensus nl generation ar- 
chitecture appeared, and is it psyeholinguistically 
plausible? In Proceedings of the Seventh Interna- 
tional Workshop on Natural Language Generation 
(INLGW-1994), pages 163-170, Kennebunkport, 
Maine, USA. available from the cmp-lg archive as 
paper cmp-lg/9411032. 
J. Robin. 1994. Revision-Based Generation of Nat- 
.ural Language Summaries Providing Historical 
Background: Corpus-Based Analysis, Design, Im- 
plementation, and Evaluation. Ph.D. thesis, De- 
partment of Computer Science, Cohnnbia Univer- 
sity. Also Technical Report CU-CS-034-94. 
M. Stede. 1998. A generative l)ersl}ective on vert} al- 
ternations. Computational Lin.quistics. 24(3):4{}1- 
_430-,September" 
216 
Applying Natural Language Generation to Indicative Summarization
Min-Yen Kan and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
 
min,kathy  @cs.columbia.edu
Judith L. Klavans
Columbia University
Center for Research on Information Access
New York, NY, 10027
klavans@cs.columbia.edu
Abstract
The task of creating indicative sum-
maries that help a searcher decide
whether to read a particular document
is a difficult task. This paper exam-
ines the indicative summarization task
from a generation perspective, by first
analyzing its required content via pub-
lished guidelines and corpus analysis.
We show how these summaries can be
factored into a set of document features,
and how an implemented content plan-
ner uses the topicality document fea-
ture to create indicative multidocument
query-based summaries.
1 Introduction
Automatic summarization techniques have
mostly neglected the indicative summary, which
characterizes what the documents are about. This
is in contrast to the informative summary, which
serves as a surrogate for the document. Indicative
multidocument summaries are an important way
of helping a user discriminate between several
documents returned by a search engine.
Traditional summarization systems are primar-
ily based on text extraction techniques. For an in-
dicative summary, which typically describes the
topics and structural features of the summarized
documents, these approaches can produce sum-
maries that are too specific. In this paper, we pro-
pose a natural language generation (NLG) model
for the automatic creation of indicative multidoc-
ument summaries. Our model is based on the val-
ues of high-level document features, such as its
distribution of topics and media types.
Highlighted differences between the documents:
The topics include "definition" and "what are the risks?"
More information on additional topics which are not
(The American Medical Assocation family medical 
Physicians and Surgeons complete home medical guide).
This file (5 minute emergency medicine consult) is 
close in content to the extract.
included in the extract is available in these files
The Merck manual of medical information contains
extensive information on the topic.
guide and The Columbia University College of
We found 4 documents on Angina:
Summary of the Disease: Angina
Get information on: [ variant angina | treatment? | diag ... ]
N
av
ig
at
io
na
l A
id
s
Treatment is designed to prevent or reduce ischemia andExtract:
minimize symptoms.  Angina that cannot be controlled by drugs ...
Ex
tra
ct
ed
 S
um
m
ar
y
G
en
er
at
ed
  S
um
m
ar
y
Figure 1: A CENTRIFUSER summary on the
healthcare topic of ?Angina?. The generated in-
dicative summary in the bottom half categorizes
documents by their difference in topic distribu-
tion.
Specifically, we focus on the problem of con-
tent planning in indicative multidocument sum-
mary generation. We address the problem of
?what to say? in Section 2, by examining what
document features are important for indicative
summaries, starting from a single document con-
text and generalizing to a multidocument, query-
based context. This yields two rules-of-thumb for
guiding content calculation: 1) reporting differ-
ences from the norm and 2) reporting information
relevent to the query.
We have implemented these rules as part of the
content planning module of our CENTRIFUSER
summarization system. The summarizer?s archi-
tecture follows the consensus NLG architecture
(Reiter, 1994), including the stages of content cal-
culation and content planning. We follow the
generation of a sample indicative multidocument
query-based summary, shown in the bottom half
of Figure 1, focusing on these two stages in the
remainder of the paper.
2 Document features as potential
summary content
Information about topics and structure of the doc-
ument may be based on higher-level document
features. Such information typically does not
occur as strings in the document text. Our ap-
proach, therefore, is to identify and extract the
document features that are relevant for indica-
tive summaries. These features form the poten-
tial content for the generated summary and can
be represented at a semantic level in much the
same way as input to a typical language gener-
ator is represented. In this section, we discuss the
analysis we did to identify features of individual
and sets of multiple documents that are relevant
to indicative summaries and show how feature se-
lection is influenced by the user query.
2.1 Features of individual documents
Document features can be divided into two sim-
ple categories: a) those which can be calculated
from the document body (e.g. topical struc-
ture (Hearst, 1993) or readability using Flesch-
Kincaid or SMOG (McLaughlin, 1969) scores),
and b) ?metadata? features that may not be con-
tained in the source article at all (e.g. author
name, media format, or intended audience). To
decide which of these document features are im-
portant for indicative summarization, we exam-
ined the problem from two points of view. From
a top-down perspective, we examined prescriptive
guidelines for summarization and indexing. We
analyzed a corpus of indicative summaries for the
alternative bottom-up perspective.
Prescriptive Guidelines. Book catalogues in-
dex a number of different document features in
order to provide enhanced search access. The
United States MARC format (2000), provides in-
dex codes for document-derived features, such as
for a document?s table of contents. It provides a
larger amount of index codes for metadata docu-
ment features such as fields for unusual format,
size, and special media. ANSI?s standard on de-
scriptions for book jackets (1979) asks that pub-
lishers mention unusual formats, binding styles,
or whether a book targets a specific audience.
Descriptive Analysis. Naturally indicative
summaries can also be found in library catalogs,
since the goal is to help the user find what they
need. We extracted a corpus of single document
summaries of publications in the domain of con-
sumer healthcare, from a local library. The corpus
contained 82 summaries, averaging a short 2.4
sentences per summary. We manually identified
several document features used in the summaries
and characterized their percentage appearance in
the corpus, presented in Table 1.
Document Feature % appearance
in corpus
Document-derived features
Topicality 100%
(e.g. ?Topics include symptoms, ...?)
Content Types 37%
(e.g. ?figures and tables?)
Internal Structure 17%
(e.g. ?is organized into three parts?)
Readability 18%
(e.g. ?in plain English?)
Special Content 7%
(e.g. ?Offers 12 credit hours?)
Conclusions 3%
Metadata features
Title 32%
Revised/Edition 28%
Author/Editor 21%
Purpose 18%
Audience 17%
Background/Lead 11%
Source 8%
(e.g. ?based on a report?)
Media Type 5%
(e.g. ?Spans 2 CDROMs?)
Table 1: Distribution of document features in li-
brary catalog summaries of consumer healthcare
publications.
Our study reports results for a specific domain,
but we feel that some general conclusions can be
drawn. Document-derived features are most im-
portant (i.e., most frequently occuring) in these
single document summaries, with direct assess-
ment of the topics being the most salient. Meta-
data features such as the intended audience, and
the publication information (e.g. edition) infor-
mation are also often provided (91% of sum-
maries have at least one metadata feature when
they are independently distributed).
2.2 Generalizing to multiple documents
We could not find a corpus of indicative multi-
document summaries to analyze, so we only ex-
amine prescriptive guidelines for multidocument
summarization.
The Open Directory Project?s (an open source
Yahoo!-like directory) editor?s guidelines (2000)
states that category pages that list many different
websites should ?make clear what makes a site
different from the rest?. ?the rest? here can mean
several things, such as ?rest of the documents in
the set to be summarized? or ?the rest of the doc-
uments in the collection?. We render this as the
following rule-of-thumb 1:
1. for a multidocument summary, a content
planner should report differences in the doc-
ument that deviate from the norm for the
document?s type.
This suggests that the content planner has an
idea of what values of a document feature are
considered normal. Values that are significantly
different from the norm could be evidence for
a user to select or avoid the document; hence,
they should be reported. For example, consider
the document-derived feature, length: if a doc-
ument in the set to be summarized is of signifi-
cantly short length, this fact should be brought to
the user?s attention.
We determine a document feature?s norm
value(s) based on all similar documents in the cor-
pus collection. For example, if all the documents
in the summary set are shorter than normal, this is
also a fact that may be significant to report to the
user. The norms need to be calculated from only
documents of similar type (i.e. documents of the
same domain and genre) so that we can model dif-
ferent value thresholds for different kinds of doc-
uments. In this way, we can discriminate between
?long? for consumer healthcare articles (over 10
pages) versus ?long? for mystery novels (over 800
pages).
2.3 Generalizing to interactive queries
If we want to augment a search engine?s ranked
list with an indicative multidocument summary,
we must also handle queries. The search engine
ranked list does this often by highlighting query
terms and/or by providing the context around a
query term. Generalizing this behavior to han-
dling multiple documents, we arrive at rule-of-
thumb 2.
2. for a query-based summary, a content plan-
ner should highlight differences that are rel-
evant to the query.
This suggests that the query can be used to
prioritize which differences are salient enough
to report to the user. The query may be rele-
vant only to a portion of a document; differences
outside of that portion are not relevant. This
mostly affects document-derived document fea-
tures, such as topicality. For example, in the con-
sumer healthcare domain, a summary in response
to a query on treatments of a particular disease
may not want to highlight differences in the doc-
uments if they occur in the symptoms section.
3 Introduction to CENTRIFUSER
CENTRIFUSER is the indicative multi-document
summarization system that we have developed
to operate on domain- and genre-specific doc-
uments. We are currently studying consumer
healthcare articles using it. The system produces
a summary of multiple documents based on a
query, producing both an extract of similar sen-
tences (see Hatzivassiliglou et al (2001)) as well
as generating text to represent differences. We fo-
cus here only on the content planning engine for
the indicative, difference reporting portion. Fig-
ure 2 shows the architecture of the system.
We designed CENTRIFUSER?s input based on
the requirements from our analysis; document
features are extracted from the input texts and
serve as the potential content for the generated
summary. CENTRIFUSER uses a plan to select
summary content, which was developed based on
our analysis and the resulting previous rules.
Our current work focuses on the document fea-
ture which most influences summary content and
form, topicality. It is also the most significant and
useful document feature. We have found that dis-
cussion of topics is the most important part of the
indicative summary. Thus, the text plan is built
around the topicality document feature and other
features are embedded as needed. Our discussion
Query as 
Individual
Features
Indicative Summary
(differences; generated)
Document Features
IR Engine
Navigation Aids
Document set
Query
documents in collection
All domain? / genre?
Centrifuser System
Extracted Synopsis
(similarities; extracted)
to be summarized
Composite
Document Document
Features
Figure 2: CENTRIFUSER architecture.
now focuses on how the topicality document fea-
ture is used in the system.
In the next sections we detail the three stages
that CENTRIFUSER follows to generate the sum-
mary: content calculation, planning and realiza-
tion. In the first, potential summary content is
computed by determining input topics present in
the document set. For each topic, the system as-
sesses its relevance to the query and its prototyp-
icality given knowledge about the topics covered
in the domain. More specifically, each document
is converted to a tree of topics and each of the
topics is assigned a topic type according to its re-
lationship to the query and to its normative value.
In the second stage, our content planner uses a
text plan to select information for inclusion in
the summary. In this stage, CENTRIFUSER deter-
mines which of seven document types each doc-
ument belongs to, based on the relevance of its
topics to the query and their prototypicality. The
plan generates a separate description for the doc-
uments in each document type, as in the sample
summary in Figure 1, where three document cat-
egories was instantiated. In the final stage, the
resulting description is lexicalized to produce the
summary.
4 Computing potential content:
topicality as topic trees
In CENTRIFUSER, the topicality document fea-
ture for individual documents is represented by
a tree data structure. Figure 3 gives an example
document topic tree for a single consumer health-
Coronary Artery Disease
Angina
Uns..Sta...Con...Cor...Ex...Rad...Var..
Causes Symptoms Diagnosis Prognosis Treatment
Uns.. Ex...
Document:  Merck.xml
. . .       . . .      . . .
Figure 3: A topic tree for an article about coro-
nary artery disease from The Merck manual of
medical information, constructed automatically
from its section headers.
care article. Each document in the collection is
represented by such a tree, which breaks each
document?s topic into subtopics.
We build these document topic trees automati-
cally for structured documents using a simple ap-
proach that utilizes section headers, which suf-
fices for our current domain and genre. Other
methods such as layout identification (Hu et al,
1999) and text segmentation / rhetorical parsing
(Yaari, 1999; Kan et al, 1998; Marcu, 1997) can
serve as the basis for constructing such trees in
both structured and unstructured documents, re-
spectively.
4.1 Normative topicality as composite topic
trees
As stated in rule 1, the summarizer needs norma-
tive values calculated for each document feature
to properly compute differences between docu-
ments.
The composite topic tree embodies this
paradigm. It is a data structure that compiles
knowledge about all possible topics and their
structure in articles of the same intersection of
domain and genre, (i.e., rule 1?s notion of ?doc-
ument type?). Figure 4 shows a partial view of
such a tree constructed for consumer healthcare
articles.
The composite topic tree carries topic infor-
mation for all articles of a particular domain and
genre combination. It encodes each topic?s rela-
tive typicality, its prototypical position within an
article, as well as variant lexical forms that it may
be expressed as (e.g. alternate headers). For in-
stance, in the composite topic tree in Figure 4, the
topic ?Symptoms? is very typical (.95 out of 1),
*Definition* . . .*Symptoms*
. . .
Variants:
"CHF", "CAD",
"Atherosclerosis",
Typicality: 1.00
"Angina Pectoris",
"Angina",
Ordering: 1 of 1
Level: 1
"Arterio...", ...
   "How did I get *X*?"
"What is *X*?"
Variants: "Definition",
Typicality: .75
Ordering: 1 of 7
Level: 2
Level: 2
Ordering: 2 of 7
Typicality: .22
Variants: "Causes",
   "What causes *X*?",
Composite Topic Tree
Typicality: .95
Level: 2
Ordering: 3 of 7
Variants: "Symptoms",
    "Signs", "Signs and 
    Symptoms", ...
Genre: Patient Information
Domain : Disease
*Causes*
*Disease*
Figure 4: A sample composite topic tree for con-
sumer health information for diseases.
may be expressed as the variant ?Signs? and usu-
ally comes after other its sibling topics (?Defini-
tion? and ?Cause?).
Compiling composite topic trees from sample
documents is a non-trivial task which can be done
automatically given document topic trees. Within
our project, we developed techniques that align
multiple document topic trees using similarity
metrics, and then merge the similar topics (Kan
et al, 2001), resulting in a composite topic tree.
5 Content Planning
NLG systems traditionally have three compo-
nents: content planning, sentence planning and
linguistic realization. We will examine how the
system generates the summary shown earlier in
Figure 1 by stepping through each of these three
steps.
During content planning, the system decides
what information to convey based on the calcu-
lated information from the previous stage. Within
the context of indicative multidocument summa-
rization, it is important to show the differences
between the documents (rule 1) and their relation-
ship to the query (rule 2). One way to do so is to
classify documents according to their topics? pro-
totypicality and relevance to the query. Figure 5
gives the different document categories we use to
capture these notions and the order in which in-
formation about a category should be presented
in a summary.
e e
e
e
e
ee
e
e
GenericIrrelevant
Deep
Start
End
Prototypical
Comprehensive
Specialized
Atypical
Figure 5: Indicative summary content plan, solid
edges indicate moves in the sample summary.
5.1 Document categories
Each of the document categories in the content
plan in Figure 5 describes documents that are sim-
ilar in their distribution of information with re-
spect to the topical norm (rule 1) and to the query
(rule 2). We explain these document categories
found in the text plan below. The examples in the
list below pertain to a general query of ?Angina?
(a heart disorder) in the same domain of consumer
healthcare.
1. Prototypical - contains information that
one would typically expect to find in an on-topic
document of the domain and genre. An exam-
ple would be a reference work, such as The AMA
Guide to Angina.
2. Comprehensive - covers most of the typical
content but may also contain other added topics.
An example could be a chapter of a medical text
on angina.
3. Specialized - are more narrow in scope than
the previous two categories, treating only a few
normal topics relevant to the query. A specialized
example might be a drug therapy guide for angina.
4. Atypical - contains high amounts of rare
topics, such as documents that relate to other gen-
res or domains, or which discuss special topics.
If the topic ?Prognosis? is rare, then a document
about life expectancy of angina patients would be
an example.
5. Deep - are often barely connected with the
query topic but have much underlying informa-
tion about a particular subtopic of the query. An
example is a document on ?Surgical treatments of
Angina?.
6. Irrelevant - contains mostly information not
relevant to the query. The document may be very
broad, covering mostly unrelated materials. A
document about all cardiovascular diseases may
be considered irrelevant.
7. Generic - don?t display tendencies towards
any particular distribution of information.
5.2 Topic types
Each of these document categories is different
because they have an underlying difference in
their distribution of information. CENTRIFUSER
achieves this classification by examining the dis-
tribution of topic types within a document. CEN-
TRIFUSER types each individual topic in the in-
dividual document topic trees as one of four pos-
sibilities: typical, rare, irrelevant and intricate.
Assigning topic types to each topic is done by op-
erationalizing our two content planning rules.
To apply rule 2, we map the text query to the
single most similar topic in each document topic
tree (currently done by string similarity between
the query text and the topic?s possible lexical
forms). This single topic node ? the query node
? establishes a relevant scope of topics. The rele-
vant scope defines three regions in the individual
topic tree, shown in Figure 6: topics that are rel-
evant to the query, ones that are too intricate, and
ones that are irrelevant with respect to the query.
Irrelevant topics are not subordinate to the query
node, representing topics that are too broad or be-
yond the scope of the query. Intricate topics are
too detailed; they are topics beyond  hops down
from the query node.
Each individual document?s ratio of topics in
these three regions thus defines its relationship
to the query: a document with mostly informa-
tion on treatment would have a high ratio of rele-
vant to other topics if given a treatment query; but
the same document given a query on symptoms
would have a much lower ratio.
To apply rule 1, we need to know whether a
particular topic ?deviates from the norm? or not.
We interpret this as whether or not the topic nor-
mally occurs in similar documents ? exactly the
information encoded in the composite topic tree?s
typicality score. As each topic in the document
topic trees is an instance of a node in the compos-
ite topic tree, each topic can inherit its composite
node?s typicality score. We assign nodes in the
relevant region (as defined by rule 2), with labels
based on their typicality. For convenience, we set
Irrelevant
Tree BTree A
Relevant
Relevant
Intricate
	 

  	  	
Statistical Acquisition of Content Selection Rules
for Natural Language Generation
Pablo A. Duboue and Kathleen R. McKeown
Department of Computer Science
Columbia University
 
pablo,kathy  @cs.columbia.edu
Abstract
A Natural Language Generation system
produces text using as input semantic data.
One of its very first tasks is to decide
which pieces of information to convey in
the output. This task, called Content Se-
lection, is quite domain dependent, requir-
ing considerable re-engineering to trans-
port the system from one scenario to an-
other. In this paper, we present a method
to acquire content selection rules automat-
ically from a corpus of text and associated
semantics. Our proposed technique was
evaluated by comparing its output with in-
formation selected by human authors in
unseen texts, where we were able to fil-
ter half the input data set without loss of
recall.
1 Introduction
CONTENT SELECTION is the task of choosing the
right information to communicate in the output of a
Natural Language Generation (NLG) system, given
semantic input and a communicative goal. In gen-
eral, Content Selection is a highly domain dependent
task; new rules must be developed for each new do-
main, and typically this is done manually. Morevoer,
it has been argued (Sripada et al, 2001) that Content
Selection is the most important task from a user?s
standpoint (i.e., users may tolerate errors in wording,
as long as the information being sought is present in
the text).
Designing content selection rules manually is a
tedious task. A realistic knowledge base contains
a large amount of information that could potentially
be included in a text and a designer must examine
a sizable number of texts, produced in different sit-
uations, to determine the specific constraints for the
selection of each piece of information.
Our goal is to develop a system that can auto-
matically acquire constraints for the content selec-
tion task. Our algorithm uses the information we
learned from a corpus of desired outputs for the sys-
tem (i.e., human-produced text) aligned against re-
lated semantic data (i.e., the type of data the sys-
tem will use as input). It produces constraints on
every piece of the input where constraints dictate if
it should appear in the output at all and if so, under
what conditions. This process provides a filter on the
information to be included in a text, identifying all
information that is potentially relevant (previously
termed global focus (McKeown, 1985) or viewpoints
(Acker and Porter, 1994)). The resulting informa-
tion can be later either further filtered, ordered and
augmented by later stages in the generation pipeline
(e.g., see the spreading activation algorithm used in
ILEX (Cox et al, 1999)).
We focus on descriptive texts which realize a sin-
gle, purely informative, communicative goal, as op-
posed to cases where more knowledge about speaker
intentions are needed. In particular, we present ex-
periments on biographical descriptions, where the
planned system will generate short paragraph length
texts summarizing important facts about famous
people. The kind of text that we aim to generate is
shown in Figure 1. The rules that we aim to acquire
will specify the kind of information that is typically
included in any biography. In some cases, whether
Actor, born Thomas Connery on August 25, 1930, in Fountain-
bridge, Edinburgh, Scotland, the son of a truck driver and char-
woman. He has a brother, Neil, born in 1938. Connery dropped
out of school at age fifteen to join the British Navy. Connery is
best known for his portrayal of the suave, sophisticated British
spy, James Bond, in the 1960s. . . .
Figure 1: Sample Target Biography.
the information is included or not may be condi-
tioned on the particular values of known facts (e.g.,
the occupation of the person being described ?we
may need different content selection rules for artists
than politicians). To proceed with the experiments
described here, we acquired a set of semantic infor-
mation and related biographies from the Internet and
used this corpus to learn Content Selection rules.
Our main contribution is to analyze how varia-
tions in the data influence changes in the text. We
perform such analysis by splitting the semantic input
into clusters and then comparing the language mod-
els of the associated clusters induced in the text side
(given the alignment between semantics and text in
the corpus). By doing so, we gain insights on the rel-
ative importance of the different pieces of data and,
thus, find out which data to include in the generated
text.
The rest of this paper is divided as follows: in the
next section, we present the biographical domain we
are working with, together with the corpus we have
gathered to perform the described experiments. Sec-
tion 3 describes our algorithm in detail. The exper-
iments we perform to validate it, together with their
results, are discussed in Section 4. Section 5 sum-
marizes related work in the field. Our final remarks,
together with proposed future work conclude the pa-
per.
2 Domain: Biographical Descriptions
The research described here is done for the auto-
matic construction of the Content Selection mod-
ule of PROGENIE (Duboue and McKeown, 2003a),
a biography generator under construction. Biogra-
phy generation is an exciting field that has attracted
practitioners of NLG in the past (Kim et al, 2002;
Schiffman et al, 2001; Radev and McKeown, 1997;
Teich and Bateman, 1994). It has the advantages
of being a constrained domain amenable to current
generation approaches, while at the same time of-
fering more possibilities than many constrained do-
mains, given the variety of styles that biographies
exhibit, as well as the possibility for ultimately gen-
erating relatively long biographies.
We have gathered a resource of text and asso-
ciated knowledge in the biography domain. More
specifically, our resource is a collection of human-
produced texts together with the knowledge base
a generation system might use as input for gener-
ation. The knowledge base contains many pieces
of information related to the person the biography
talks about (and that the system will use to generate
that type of biography), not all of which necessarily
will appear in the biography. That is, the associated
knowledge base is not the semantics of the target text
but the larger set1 of all things that could possibly be
said about the person in question. The intersection
between the input knowledge base and the semantics
of the target text is what we are interested in captur-
ing by means of our statistical techniques.
To collect the semantic input, we crawled 1,100
HTML pages containing celebrity fact-sheets from
the E! Online website.2 The pages comprised infor-
mation in 14 categories for actors, directors, produc-
ers, screenwriters, etc. We then proceeded to trans-
form the information in the pages to a frame-based
knowledge representation. The final corpus con-
tains 50K frames, with 106K frame-attribute-value
triples, for the 1,100 people mentioned in each fact-
sheet. An example set of frames is shown in Fig-
ure 3.
The text part was mined from two different web-
sites, biography.com, containing typical biogra-
phies, with an average of 450 words each; and
imdb.com, the Internet movie database, 250-word
average length biographies. In each case, we ob-
tained the semantic input from one website and a
separate biography from a second website. We
linked the two resources using techniques from
record linkage in census statistical analysis (Fellegi
and Sunter, 1969). We based our record linkage on
the Last Name, First Name, and Year of Birth at-
tributes.
1The semantics of the text normally contain information not
present in our semantic input, although for the sake of Content
Selection is better to consider it as a ?smaller? set.
2http://www.eonline.com
inputs
texts texts
clusters
semantic
target
(2)
(3)
(4)
(5)
(A) (B) (C)
(1)
matched
baseline content selectionclass?based
rules rulesrules
semantic
MATCHING
counting and rule inductionrule?mixing
thresholding (RIPPER)logic
CLUSTERING STATISTICALSELECTOR
N?GRAM
DISTILLER
EXAMPLE
EXTRACTOR
Figure 2: Our proposed algorithm, see Section 3 for details.
3 Methods
Figure 2 illustrates our two-step approach. In the
first step (shaded region of the figure), we try to
identify and solve the easy cases for Content Selec-
tion. The easy cases in our task are pieces of data
that are copied verbatim from the input to the out-
put. In biography generation, this includes names,
dates of birth and the like. The details of this pro-
cess are discussed in Section 3.1. After these cases
have been addressed, the remaining semantic data is
clustered and the text corresponding to each cluster
post-processed to measure degrees of influence for
different semantic units, presented in Section 3.2.
Further techniques to improve the precision of the
algorithm are discussed in Section 3.3.
Central to our approach is the notion of data
paths in the semantic network (an example is shown
in Figure 3). Given a frame-based representation of
knowledge, we need to identify particular pieces of
knowledge inside the graph. We do so by selecting
a particular frame as the root of the graph (the
person whose biography we are generating, in our
case, doubly circled in the figure) and considering
the paths in the graph as identifiers for the different
pieces of data. We call these data paths. Each
path will identify a class of values, given the
fact that some attributes are list-valued (e.g., the
relative attribute in the figure). We use the notation
 
attribute  attribute  attribute 
to denote data paths.
3.1 Exact Matching
In the first stage (cf. Fig. 2(1)), the objective is to
identify pieces from the input that are copied ver-
batim to the output. These types of verbatim-copied
anchors are easy to identify and they allow us do two
things before further analyzing the input data: re-
move this data from the input as it has already been
selected for inclusion in the text and mark this piece
of text as a part of the input, not as actual text.
The rest of the semantic input is either verbal-
ized (e.g., by means of a verbalization rule of the
form
 
brother age 
	 ?young?) or not
included at all. This situation is much more chal-
lenging and requires the use of our proposed statis-
tical selection technique.
3.2 Statistical Selection
For each class in the semantic input that
was not ruled out in the previous step (e.g.,
 
brother age  ), we proceed to cluster
(cf. Fig. 2(2)) the possible values in the path,
over all people (e.g., ffProceedings of the Fourth International Natural Language Generation Conference, pages 3?5,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Lessons Learned from Large Scale Evaluation of Systems that Produce
Text: Nightmares and Pleasant Surprises
Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, NY 10027
kathy@cs.columbia.edu
Extended Abstract
As the language generation community explores
the possibility of an evaluation program for lan-
guage generation, it behooves us to examine our
experience in evaluation of other systems that pro-
duce text as output. Large scale evaluation of sum-
marization systems and of question answering sys-
tems has been carried out for several years now.
Summarization and question answering systems
produce text output given text as input, while lan-
guage generation produces text from a semantic
representation. Given that the output has the same
properties, we can learn from the mistakes and
the understandings gained in earlier evaluations.
In this invited talk, I will discuss what we have
learned in the large scale summarization evalua-
tions carried out in the Document Understanding
Conferences (DUC) from 2001 to present, and in
the large scale question answering evaluations car-
ried out in TREC (e.g., the definition pilot) as well
as the new large scale evaluations being carried out
in the DARPA GALE (Global Autonomous Lan-
guage Environment) program.
DUC was developed and run by NIST and pro-
vides a forum for regular evaluation of summariza-
tion systems. NIST oversees the gathering of data,
including both input documents and gold standard
summaries, some of which is done by NIST and
some of which is done by LDC. Each year, some
30 to 50 document sets were gathered as test data
and somewhere between two to nine summaries
were written for each of the input sets. NIST has
carried out both manual and automatic evaluation
by comparing system output against the gold stan-
dard summaries written by humans. The results
are made public at the annual conference. In the
most recent years, the number of participants has
grown to 25 or 30 sites from all over the world.
TREC is also run by NIST and provides an
annual opportunity for evaluating the output of
question-answering (QA) systems. Of the various
QA evaluations, the one that is probably most illu-
minating for language generation is the definition
pilot. In this evaluation, systems generated long
answers (e.g., paragraph length or lists of facts) in
response to a request for a definition. In contrast to
DUC, no model answers were developed. Instead,
system output was pooled and human judges de-
termined which facts within the output were nec-
essary (termed ?vital nuggets?) and which were
helpful, but not absolutely necessary (termed ?OK
nuggets?). Systems could then be scored on their
recall of nuggets and precision of their response.
DARPA GALE is a new program funded by
DARPA that is running its own evaluation, carried
out by BAE Systems, an independent contractor.
Evaluation more closely resembles that done in
TREC, but the systems? scores will be compared
against the scores of human distillers who carry
out the same task. Thus, final numbers will report
percent of human performance. In the DARPA
GALE evaluation, which is a future event at the
time of this writing, in addition to measuring prop-
erties such as precision and recall, BAE will also
measure systems? ability to find all occurrences of
the same fact in the input (redundancy).
One consideration for an evaluation program
is the feel of the program. Does the evalua-
tion program motivate researchers or does it cause
headaches? I liken Columbia?s experience in DUC
and currently in GALE to that of Max in Where the
Wild Things Are by Maurice Sendak. We began
with punishment (i.e., if you don?t do well, your
funding will be in jeopardy), encounter monsters
along the way (seemingly arbitrary methods for
3
measuring output quality), finally tame the mon-
sters and sail back peacefully across time. DUC
has reached the peaceful stage, but GALE has not.
The TREC definition pilot had less of a threat of
punishment.
Evaluation in all of these programs began at the
request of the funders, with the goal of comparing
how well different funded systems perform. Im-
provement over the years is also measured in order
to determine if funding is well spent. This kind of
goal creates anxiety in participants and makes it
most important to get the details of the evaluation
right; errors in how evaluation is carried out can
have great consequences. Coming to agreement
on the metrics used, the methodology for measur-
ing output and the tasks on which performance is
measured can be difficult; the environment does
not feel friendly. Even if evaluation within the
language generation community was not initiated
with the same goals, I think it is reasonable to ex-
pect a certain amount of disagreement as the pro-
gram gets off the ground.
However, over time, researchers come to agree-
ment on some portion of the task and these fea-
tures become accepted. At this point in time, it is
possible to see the benefits of the program. Cer-
tainly, within DUC, we are at this stage. DUC has
generated large amounts of data, including both
input document sets and multiple models of good
output for each input set, which has spurred stud-
ies both on evaluation and summarization. Hal-
teren and Teufel, for example, provide a method
for annotation of content units and study consen-
sus across summarizers (van Halteren and Teufel,
2003; Teufel and van Halteren, 2004b). Nenkova
studies significant differences across DUC04 sys-
tems (Nenkova, 2005) as well as the properties of
human and system summaries (Nenkova, 2006).
We can credit DUC with the emergence of au-
tomatic methods for evaluation such as ROUGE
(Lin and Hovy, 2003; Lin, 2004) which allow
quick measurement of systems during develop-
ment and enable evaluation of larger amounts of
data. We have seen the development of man-
ual methods for evaluation developed both within
DUC (Harman and Over, 2004) and without. The
Pyramid method (Nenkova and Passonneau, 2004)
provides a annotation method and metric that ad-
dresses the issues of reliability and stability of
scoring. Thus, research on evaluation of summa-
rization has become a field in its own right result-
ing in greater understanding of the effect of differ-
ent metrics and methodologies.
?From DUC and TREC, we have learned im-
portant characteristics of a large-scale evaluation,
of which the top three might be:
? Output can be measured by comparison
against a human model, but we know that
this comparison will only be valid if multi-
ple models are used. There are multiple good
summaries of the same input and if system
output is compared against just one, the re-
sults will be biased.
? If the task is appealing to a wide audience,
the evaluation will spur research and motivate
researchers to join in. We have seen this with
growth of participation in DUC. One benefit
of summarization and QA is that the task is
domain-independent and thus, no one site has
an advantage over others through experience
with a particular domain.
? Given the different ways in which evaluation
can be carried out and the fact that different
researchers may be biased towards methods
which favor their own approach, it is impor-
tant the evaluation be overseen by a neutral
party which is not deeply involved in research
on the task itself. On the other hand, some
knowledge is necessary if the evaluation is to
be well-designed.
While my talk will focus on large scale evalua-
tion programs that feature quantitative evaluation
through comparison with a gold standard, there
has been work on task-based evaluation of sum-
marization (McKeown et al 2005). Task-based
evaluation is more intensive and to date, has not
been done on a large scale across sites, but shows
potential for indicating the usefulness of summa-
rization systems.
In this brief abstract, I?ve suggested some of the
topics that will be covered in my talk, which will
tour the land of the wild things for evaluation, il-
luminating monsters and highlighting events that
will allow more peaceful sailing. Evaluation can
be a nightmare, but over time and particularly
if carried out away from the influence of fund-
ing pressures, it can nurture a community of re-
searchers with common goals.
4
Acknowledgments
This material is based upon work supported in
part by the ARDA AQUAINT program (Con-
tract No. MDA908-02-C-0008 and Contract No.
NBCHC040040) and the Defense Advanced Re-
search Projects Agency (DARPA) under Con-
tract No. HR0011-06-C-0023 and Contract No.
N66001-00-1-8919. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the DARPA or ARDA.
References
Harman, D. and Over, P. 2004. The effects of human
variation in duc summarization evaluation. In Text
Summarization Branches Out Workshop, ACL 2004.
Lin, C.-Y. 2003. Rouge: a package for automatic eval-
uation of summaries. In Proceedings of the Work-
shop in Text Summarization, ACL?04.
Lin, C.-Y. and Hovy, E. 2003. Automatic evaluation of
summaries using n-gram co-occurance statistics. In
Proceedings of HLT-NAACL 2003.
McKeown, K. and Passonneau, R.J. and Elson, D.K.,
and Nenkova, A., and Hirschberg, J. 2005. A Task-
Based Evaluation of Multi-Document Summariza-
tion. In Proceedings of SIGIR 2005.
Nenkova, A. 2005. Automatic text summarization of
newswire: Lessons learned from the Document Un-
derstanding Conference. In Proceedings of AAAI
2004.
Nenkova, A. 2006. Understanding the process
of multi-document summarization: content selec-
tion, rewrite and evaluation. Ph.D Dissertation,
Columbia University.
Nenkova, A. and Passonneau, R. 2004. Evaluating
content selection in summarization: The pyramid
method. In Proceedings of HLT/NAACL 2004.
Teufel, S. and van Halteren, H. 2004. Evaluating in-
formation content by factoid analysis: human anno-
tation and stability. In EMNLP-04.
van Halteren, H. and Teufel, S. 2003. Examining the
consensus between human summaries: initial exper-
iments with factoid analysis. In HLT-NAACL DUC
Workshop.
5
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, page 3,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Query-focused Summarization Using Text-to-Text Generation:           
When Information Comes from Multilingual Sources 
 
 
Kathleen McKeown 
Department of Computer Science 
Columbia University 
kathy@cs.columbia.edu 
 
 
  
 
Abstract 
The past five years have seen the emergence of robust, scalable natu-
ral language processing systems that can summarize and answer 
questions about online material. One key to the success of such sys-
tems is that they re-use text that appeared in the documents rather 
than generating new sentences from scratch.  Re-using text is abso-
lutely essential for the development of robust systems; full semantic 
interpretation of unrestricted text is beyond the state of the art. Better 
summaries and answers can be produced, however, if systems can 
generate new sentences from the input text, fusing relevant phrases 
and discarding irrelevant ones. When the underlying sources for 
summarization come from multiple languages, the need for text-to-
text generation is even more pronounced. 
 
In this invited talk I present research on query-focused summariza-
tion over a variety of sources, including news, broadcast news, talks 
shows and blogs. Our research combines approaches from summari-
zation and information extraction to answer open-ended questions. 
Because our sources include informal genres as well as formal genres 
and draw from English, Arabic and Chinese, text-to-text generation is 
critical for improving the intelligibility of responses. In our systems, 
we exploit information available at question answering time to edit 
sentences, removing redundant and irrelevant information and cor-
recting errors in translated sentences. 
3
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1146?1154,
Beijing, August 2010
?Got You!?: Automatic Vandalism Detection in Wikipedia                                                                                         
with Web-based Shallow Syntactic-Semantic Modeling 
                                William Yang Wang and Kathleen R. McKeown 
Department of Computer Science  
Columbia University 
yw2347@columbia.edu kathy@cs.columbia.edu 
Abstract 
Discriminating vandalism edits from 
non-vandalism edits in Wikipedia is a 
challenging task, as ill-intentioned edits 
can include a variety of content and be 
expressed in many different forms and 
styles. Previous studies are limited to 
rule-based methods and learning based 
on lexical features, lacking in linguistic 
analysis. In this paper, we propose a 
novel Web-based shallow syntactic-
semantic modeling method, which utiliz-
es Web search results as resource and 
trains topic-specific n-tag and syntactic 
n-gram language models to detect van-
dalism. By combining basic task-specific 
and lexical features, we have achieved 
high F-measures using logistic boosting 
and logistic model trees classifiers, sur-
passing the results reported by major 
Wikipedia vandalism detection systems.  
1 Introduction 
Online open collaboration systems are becoming 
a major means of information sharing on the 
Web. With millions of articles from millions of 
resources edited by millions of people, Wikipe-
dia is a pioneer in the fast growing, online know-
ledge collaboration era. Anyone who has Inter-
net access can visit, edit and delete Wikipedia 
articles without authentication. 
A primary threat to this convenience, however, 
is vandalism, which has become one of Wikipe-
dia?s biggest concerns (Geiger, 2010). To date, 
automatic countermeasures mainly involve rule-
based approaches and these are not very effec-
tive. Therefore, Wikipedia volunteers have to 
spend a large amount of time identifying vanda-
lized articles manually, rather than spending 
time contributing content to the articles. Hence, 
there is a need for more effective approaches to 
automatic vandalism detection. 
In contrast to spam detection tasks, where a 
full spam message, which is typically 4K Bytes 
(Rigoutsos and Huynh, 2004), can be sampled 
and analyzed (Itakura and Clarke, 2009), Wiki-
pedia vandals typically change only a small 
number of words or sentences in the targeted 
article. In our preliminary corpus (Potthast et al, 
2007), we find the average size of 201 vanda-
lized texts to be only 1K Byte. This leaves very 
few clues for vandalism modeling. The question 
we address in this paper is: given such limited 
information, how can we better understand and 
model Wikipedia vandalism? 
Our proposed approach establishes a novel 
classification framework, aiming at capturing 
vandalism through an emphasis on shallow syn-
tactic and semantic modeling. In contrast to pre-
vious work, we recognize the significance of 
natural language modeling techniques for Wiki-
pedia vandalism detection and utilize Web 
search results to construct our shallow syntactic 
and semantic models. We first construct a base-
line model that captures task-specific clues and 
lexical features that have been used in earlier 
work (Potthast et al, 2008; Smets et al, 2008) 
augmenting these with shallow syntactic and 
semantic features.  Our main contributions are: 
? Improvement over previous modeling me-
thods with three novel lexical features 
? Using Web search results as training data 
for syntactic and semantic modeling 
? Building topic-specific n-tag syntax models 
and syntactic n-gram models for shallow 
syntactic and semantic modeling 
1146
2 Related Work 
So far, the primary method for automatic 
vandalism detection in Wikipedia relies on rule-
based bots. In recent years, however, with the 
rise of statistical machine learning, researchers 
have begun to treat Wikipedia vandalism 
detection task as a classification task. To the best 
of our knowledge, we are among the first to 
consider the shallow syntactic and semantic 
modeling using Natural Language Processing 
(NLP) techniques, utilizing the Web as corpus to 
detect vandalism. 
ClueBot (Carter, 2007) is one of the most ac-
tive bots fighting vandalism in Wikipedia. It 
keeps track of the IP of blocked users and uses 
simple regular expressions to keep Wikipedia 
vandalism free. A distinct advantage of rule-
based bots is that they have very high precision. 
However they suffer from fixed-size knowledge 
bases and use only rigid rules. Therefore, their 
average recall is not very high and they can be 
easily fooled by unseen vandalism patterns. Ac-
cording to Smets et al, (2008) and Potthast et al, 
(2008), rule-based bots have a perfect precision 
of 1 and a recall of around 0.3. 
The Wikipedia vandalism detection research 
community began to concentrate on the machine 
learning approaches in the past two years. Smets 
et al (2008) wrapped all the content in diff text 
into a bag of words, disregarding grammar and 
word order. They used Na?ve Bayes as the 
classification algorithm. Compared to rule-based 
methods, they show an average precision of 0.59 
but are able to reach a recall of 0.37. Though 
they are among the first to try machine learning 
approaches, the features in their study are the 
most straightforward set of features. Clearly, 
there is still room for improvement. 
More recently, Itakura and Clarke (2009) have 
proposed a novel method using Dynamic Mar-
kov Compression (DMC). They model their ap-
proach after the successful use of DMC in Web 
and Mail Spam detection (Bratko et al, 2006). 
The reported average precision is 0.75 and ave- 
rage recall is 0.73.  
To the best of our knowledge, Potthast et al, 
(2008) report the best result so far for Wikipedia 
vandalism detection. They craft a feature set that 
consists of interesting task-specific features. For 
example, they monitor the number of previously 
submitted edits from the same author or IP, 
which is a good feature to model author contri-
bution. Their other contributions are the use of a 
logistic regression classifier, as well as the use 
of lexical features. They successfully demon-
strate the use of lexical features like vulgarism 
frequency.  Using all features, they reach an av-
erage precision of 0.83 and recall of 0.77.  
In addition to previous work on vandalism de-
tection, there is also earlier work using the web 
for modeling. Biadsy et al (2008) extract pat-
terns in Wikipedia to generate biographies au-
tomatically. In their experiment, they show that 
when using Wikipedia as the only resource for 
extracting named entities and corresponding col-
locational patterns, although the precision is typ-
ically high, recall can be very low. For that rea-
son, they choose to use Google to retrieve train-
ing data from the Web. In our approach, instead 
of using Wikipedia edits and historical revisions, 
we also select the Web as a resource to train our 
shallow syntactic and semantic models. 
3 Analysis of  Types of Vandalism 
In order to better understand the characteristics 
of vandalism cases in Wikipedia, we manually 
analyzed 201 vandalism edits in the training set 
of our preliminary corpus.  In order to concen-
trate on textual vandalism detection, we did not 
take into account the cases where vandals hack 
the image, audio or other multimedia resources 
contained in the Wikipedia edit. 
We found three main types of vandalism, 
which are shown in Table 1 along with corres-
ponding examples. These examples contain both 
the title of the edit and a snippet of the diff-ed 
content of vandalism, which is the textual differ-
ence between the old revision and the new revi-
sion, derived through the standard diff algorithm 
(Heckel, 1978). 
? Lexically ill-formed 
This is the most common type of vandal-
ism in Wikipedia. Like other online van-
dalism acts, many vandalism cases in 
Wikipedia involve ill-intentioned or ill-
formed words such as vulgarisms, invalid 
letter sequences, punctuation misuse and 
Web slang. An interesting observation is 
that vandals almost never add emoticons 
in Wikipedia. For the first example in  
1147
Table 1: Vandalism Types and Examples 
Table 1, vulgarism and punctuation mi-
suse are observed. 
? Syntactically ill-formed 
Most vandalism cases that are lexically 
ill-intentioned tend to be syntactically ill-
formed as well. It is not easy to capture 
these cases by solely relying on lexical 
knowledge or rule-based dictionaries and 
it is also very expensive to update dictio-
naries and rules manually. Therefore, we 
think that is crucial to incorporate more 
syntactic cues in the feature set in order to 
improve performance. Moreover, there are 
also some cases where an edit could be 
lexically well-intentioned, yet syntactical-
ly ill-formed. The first example of syntac-
tic ill-formed in Table 1 is of this kind. 
     Table 2: Feature Sets and Corresponding          
Features of Our Vandalism Detection System 
? Lexically and syntactically well 
formed, but semantically ill-
intentioned 
This is the trickiest type of vandalism to 
identify. Vandals of this kind might have 
good knowledge of the rule-based vandal-
ism detecting bots. Usually, this type of 
vandalism involves off-topic comments, 
inserted biased opinions, unconfirmed in-
formation and lobbying using very subjec-
tive comments. However, a common cha-
racteristic of all vandalism in this category 
is that it is free of both lexical and syntac-
tic errors. Consider the first example of 
semantic vandalism in Table 1 with edit 
title ?Global Warming?: while the first 
sentence for that edit seems to be fairly 
normal (the author tries to claim another 
explanation of the global warming effect), 
the second sentence makes a sudden tran-
sition from the previous topic to mention 
a basketball star and makes a ridiculous 
conclusion in the last sentence.  
In this work, we realize the importance of in-
corporating NLP techniques to tackle all the 
above types of vandalism, and our focus is on 
the syntactically ill-formed and semantically ill-
intentioned types that could not be detected by 
rule-based systems and straightforward lexical 
features.  
Vandalism 
Types 
Examples 
Lexically 
ill-formed 
Edit Title:  IPod 
shit!!!!!!!!!!!!!!!!!!!!!! 
 
 
Syntactically 
ill-formed 
Edit Title: Rock music 
DOWN WITH SOCIETY 
MADDISON STREET RIOT 
FOREVER. 
Edit Title: Vietnam War 
Crabinarah sucks dont buy it 
 
 
 
 
 
 
Lexically + 
syntactically  
well-formed, 
semantically  
ill-intentioned 
Edit Title: Global Warming 
Another popular theory in-
volving global warming is 
the concept that global 
warming is not caused by 
greenhouse gases. The theory 
is that Carlos Boozer is the 
one preventing the infrared 
heat from escaping the at-
mosphere. Therefore, the 
Golden State Warriors will 
win next season. 
Edit Title: Harry Potter 
Harry Potter is a teenage 
boy who likes to smoke 
crack with his buds. They 
also run an illegal smuggling 
business to their headmaster 
dumbledore. He is dumb! 
 
Feature 
Sets 
Features 
Task-
specific 
Number of Revisions; 
Revisions Size Ratio; 
Lexical Vulgarism; Web Slang;  
Punctuation Misuse; 
Comment Cue Words; 
Syntactic Normalized Topic-specific N-tag 
Log Likelihood and Perplexity  
Semantic Normalized Topic-specific  
Syntactic N-gram Log  
Likelihood and Perplexity 
 
1148
4 Our System 
We propose a shallow syntactic-semantic fo-
cused classification approach for vandalism de-
tection (Table 2). In contrast to previous work, 
our approach concentrates on the aspect of using 
natural language techniques to model vandalism. 
Our shallow syntactic and semantic modeling 
approaches extend the traditional n-gram lan-
guage modeling method with topic-specific n-
tag (Collins et al, 2005) syntax models and top-
ic-specific syntactic n-gram semantic models. 
Moreover, in the Wikipedia vandalism detection 
task, since we do not have a sufficient amount of 
training data to model the topic of each edit, we 
propose the idea of using the Web as corpus by 
retrieving search engine results to learn our top-
ic-specific n-tag syntax and syntactic n-gram 
semantic models. The difference between our 
syntactic and semantic modeling is that n-tag 
syntax models only model the order of sentence 
constituents, disregarding the corresponding 
words. Conversely, for our syntactic n-gram 
models, we do keep track of words together with 
their POS tags and model both the word and 
syntactic compositions as a sequence. The detail 
of our shallow syntactic-semantic modeling me-
thod will be described in subsection 4.4. 
We use our shallow syntactic-semantic model 
to augment our base model, which builds on ear-
ly work. For example, when building one of our 
task-specific features, we extract the name of the 
author of this revision to query Wikipedia about 
the historical behavior of this author. This kind 
of task-specific global feature tends to be very 
informative and thus forms the basis of our sys-
tem. For lexical level features, we count vulgar-
ism frequencies and also introduce three new 
lexical features: Web slang, punctuation misuse 
and comment cue words, all of which will be 
described in detail in 4.2 and 4.3.  
4.1 Problem Representation 
The vandalism detection task can be formu-
lated as the following problem. Let?s assume we 
have a vandalism corpus C, which contains a set 
of Wikipedia edits S. A Wikipedia edit is de-
noted as ei. In our case, we have S = {e1, e2?,en}. 
Each edit e has two consecutive revisions (an old 
revision Rold and a new revision Rnew) that are 
unique in the entire data set. We write that e = 
{Rold, Rnew}. With the use of the standard diff 
algorithm, we can produce a text Rdiff, showing 
the difference between these two revisions, so 
that e = {Rold, Rnew, Rdiff }.  Our task is: given S, 
to extract features from edit e ?S and train a 
logistic boosting classifier. On receiving an edit 
e from the test set, the classifier needs to decide 
whether this e is a vandalism edit or a non-
vandalism edit. e?{1,0}.  
4.2 Basic Task-specific and Lexical Fea-
tures  
Task-specific features are domain-dependent and 
are therefore unique in this Wikipedia vandalism 
detection task. In this work, we pick two task-
specific features and one lexical feature that 
proved effective in previous studies. 
? Number of Revisions 
This is a very simple but effective feature 
that is used by many studies (Wilkinson 
and Huberman, 2007; Adler et al, 2008; 
Stein and Hess, 2007). By extracting the 
author name for the new revision Rnew, we 
can easily query Wikipedia and count how 
many revisions the author has modified in 
the history. 
? Revision Size Ratio 
Revision size ratio measures the size of 
the new revision versus the size of the old 
revision in an edit. This measure is an in-
dication of how much information is 
gained or lost in the new revision Rnew, 
compared to the old revision Rold, and can 
be expressed as: 
   RevRatio(?)  =  
 Count (w)w  ? R  new
  Count (w)w  ? R  old
 
 
where W represents any word token of a 
revision. 
? Vulgarism Frequency 
Revision size ratio measures the size of 
the new revision versus the Vulgarism 
frequency was first introduced by Potthast 
et al (2008). However, note that not all 
vulgarism words should be considered as 
vandalism and sometime even the Wiki-
pedia edit?s title and content themselves 
contain vulgarism words.  
1149
 
For each diff text in an edit e, we count 
the total number of appearances of vulgar-
ism words v where v is in our vulgarism 
dictionary1. 
VulFreq ? =  Count(?)
??Rdiff
 
4.3 Novel Lexical Features 
In addition to previous lexical features, we pro-
pose three novel lexical features in this paper: 
Web slang frequency, punctuation misuse, and 
comment cue words frequency.  
? Web Slang and Punctuation Misuse  
Since Wikipedia is an open Web applica-
tion, vandalism also contains a fair 
amount of Web slang, such as, ?haha?, 
?LOL? and ?OMG?. We use the same me-
thod as above to calculate Web slang fre-
quency, using a Web slang dictionary2. In 
vandalism edits, many vandalism edits al- 
                                                 
1 http://www.noswearing.com/dictionary 
2 http://www.noslang.com/dictionary/full 
so contain punctuation misuse, for exam-
ple, ?!!!? and ?????. However, we have 
not observed a significant amount of emo-
ticons in the vandalism edits. Based on 
this, we only keep track of Web slang fre-
quency and the occurrence of punctuation 
misuse. 
? Comment Cue Words 
Upon committing each new revision in 
Wikipedia, the author is required to enter 
some comments describing the change. 
Well-intentioned Wikipedia contributors 
consistently use these comments to ex-
plain the motivation for their changes. For 
example, common non-vandalism edits 
may contain cue words and phrases like 
?edit revised, page changed, item cleaned 
up, link repaired or delinked?. In contrast, 
vandals almost never take their time to 
add these kinds of comments. We can 
measure this phenomenon by counting the 
frequency of comment cue words.  
1150
4.4 Topic-specific N-tag Syntax Models and 
Syntactic N-grams for Shallow Syntac-
tic and Semantic Modeling 
In Figure 1, we present the overview of our ap-
proach, which uses Web-trained topic-specific 
training for both: (1) n-tag syntax models for 
shallow syntactic modeling and (2) syntactic n-
gram models for shallow semantic modeling.  
For each Wikipedia edit, we consider its title 
as an approximate semantic representation, using 
it as a query to build topic-specific models.  In 
addition, we also use the title information to 
model the syntax of this topic.  
Given Rdiff, we produce the syntactic version 
of the diff-ed text using a probabilistic POS tag-
ger (Toutanova and Manning, 2000; Toutanova 
et al, 2003). The edit title is extracted from the 
corpus (either Rnew or Rold) and is used to query 
multiple Web search engines in order to collect 
the n-tag and n-gram training data from the top-k 
results. Before we start training language models, 
we tag the top-k results using the POS tagger. 
Note that when modeling n-tag syntax models, it 
is necessary to remove all the words. With the 
POS-only sequences, we train topic-specific n-
tag models to describe the syntax of normal text 
on the same topic associated with this edit. With 
the original tagged sequences, we train syntactic 
n-gram models to represent the semantics of the 
normal text of this edit. 
After completing the training stage, we send 
the test segment (i.e. the diff-ed text sequence) to 
both the learned n-tag syntax models and the 
learned syntactic n-gram models. For the n-tag 
syntax model, we submit the POS tag-only ver-
sion of the segment. For the syntactic n-gram 
model, we submit a version of the segment 
where each original word is associated with its 
POS-tag. In both cases we compute the log-
likelihood and the perplexity of the segment.  
Finally, we normalize the log likelihood and 
perplexity scores by dividing them by the length 
of Rdiff, as this length varies substantially from 
one edit to another. 3 We expect an edit that has 
low log likelihood probability and perplexity to 
be vandalism, and it is very likely to be unre-
lated to the syntax and semantic of the normal 
text of this Wikipedia edit. In the end, the nor-
malized log probability and perplexity scores 
will be incorporated into our back-end classifier 
with all task-specific and lexical features. 
Web as Corpus: In this work, we leverage 
Web search results to train the syntax and se-
mantic models. This is based on the assumption 
that the Web itself is a large corpus and Web 
search results can be a good training set to ap-
proximate the semantics and syntax of the query.    
Topic-specific Modeling: We introduce a 
topic-specific modeling method that treats every 
edit in Wikipedia as a unique topic. We think 
that the title of each Wikipedia edit is an approx-
imation of the topic of the edit, so we extract the 
title of each edit and use it as keywords to re-
trieve training data for our shallow syntactic and 
semantic modeling. 
Topic-specific N-tag and Syntactic N-gram: 
In our novel approach, we tag all the top-k query 
results and diff text with a probabilistic POS tag-
ger in both the training and test set of the vandal-
ism corpus. Figure 2(a) is an example of a POS-
tagged sequence in a top-k query result.  
For shallow syntactic modeling, we use an n-
tag modeling method (Collins et al, 2005). Giv-
en a tagged sequence, we remove all the words 
and only keep track of its POS tags: tagi-2 tagi-1 
                                                 
3 Although we have experimented with using the 
length of Rdiff as a potential feature, it does not appear 
to be a good indicator of vandalism. 
(a) 
Rock/NNP and/CC roll/NN -LRB-/-LRB- 
also/RB spelled/VBD Rock/NNP 'n'/CC 
Roll/NNP 
(b) 
NNP CC NN -LRB- RB VBD NNP CC 
NNP 
(c) 
Rock/NNP !/. !/. !/. and/CC roll/VB 
you/PRP !/. !/. !/. 
(d) 
NNP . . . CC VB PRP . . . 
 
Figure 2. Topic-specific N-tag and Syntactic 
N-gram modeling for the edit ?Rock and 
Roll? in Wikipedia (a) The Web-derived 
POS tagged sequence (b) The Web-derived 
POS tag-only sequence (c) A POS tagged 
vandalism diff text Rdiff (d) A POS tag-only 
vandalism Rdiff 
 
1151
tagi. This is similar to n-gram language modeling, 
but instead, we model the syntax using POS tags, 
rather than its words. In this example, we can 
use the system in Figure 2 (b) to train an n-tag 
syntactic model and use the one in Figure 2 (d) 
to test. As we see, for this test segment, it be-
longs to the vandalism class and has very differ-
ent syntax from the n-tag model. Therefore, the 
normalized log likelihood outcome from the n-
tag model is very low. 
In order to model semantics, we use an im-
proved version of the n-gram language modeling 
method. Instead of only counting wordi-2 wordi-1 
wordi, we model composite tag/word feature, e.g. 
tagi-2wordi-2 tagi-1wordi-1 tagiwordi. This syntactic 
n-gram modeling method has been successfully 
applied to the task of automatic speech recogni-
tion (Collins et al, 2005). In the example in Fig-
ure 2, the vandalism diff text will probably score 
low, because although it shares an overlap bi-
gram ?and roll? with the phrase ?rock and roll? 
in training text, once we apply the shallow syn-
tactic n-gram modeling method, the POS tag 
bigram ?and/CC roll/VB? in diff text will be dis-
tinguished from the ?and/CC roll/NN? or 
?and/CC roll/NNP? in the training data. 
5 Experiments 
To evaluate the effectiveness of our approach, 
we first run experiments on a preliminary corpus 
that is also used by previous studies and com-
pare the results. Then, we conduct a second ex-
periment on a larger corpus and analyze in detail 
the features of our system. 
5.1 Experiment Setup 
In our experiments, we use a Wikipedia vandal-
ism detection corpus (Potthast et al, 2007) as a 
preliminary corpus. The preliminary corpus con-
tains 940 human-assessed edits from which 301 
edits are classified as vandalism. We split the 
corpus and keep a held-out 100 edits for each 
class in testing and use the rest for training. In 
the second experiment, we adopt a larger corpus 
(Potthast et al, 2010) that contains 15,000 edits 
with 944 marked as vandalism. The split is 300 
edits for each class in held-out testing and the 
rest used for training. In the description of the 
second corpus, each edit has been reviewed by at 
least 3 and up to 15 annotators. If more than 2/3 
of the annotators agree on a given edit, then the 
edit is tagged as one of our target classes. Only 
11 cases are reported where annotators fail to 
form a majority inter-labeler agreement and in 
those cases, the class is decided by corpus au-
thors arbitrarily.    
In our implementation, the Yahoo! 4  search 
engine and Bing5 search engine are the source 
for collecting top-k results for topic-specific n-
gram training data, because Google has a daily 
query limit. We retrieve top-100 results from 
Yahoo!, and combine them with the top-50 re-
sults from Bing.   
For POS tagging, we use the Stanford POS 
Tagger (Toutanova and Manning, 2000; Touta-
nova et al, 2003) with its attached wsj3t0-18- 
bidirectional model trained from the Wall Street 
Journal corpus. For both shallow syntactic and 
semantic modeling, we train topic-specific tri-
gram language models on each edit using the 
SRILM toolkit (Stolcke, 2002). 
In this classification task, we used two logistic 
classification methods that haven?t been used 
before in vandalism detection. Logistic model 
trees (Landwehr et al, 2005) combine tree in-
duction with linear modeling. The idea is to use 
the logistic regression to select attributes and 
build logistic regression at the leaves by incre-
mentally refining those constructed at higher 
levels in the tree. The second method we used, 
logistic boosting (Friedman et al, 2000), im-
proves logistic regression with boosting. It 
works by applying the classification algorithm to 
reweighted versions of the data and then taking a 
weighted majority vote of the sequence of clas-
sifiers thus produced.    
5.2 Preliminary Experiment 
In the preliminary experiment, we tried logistic 
boosting classifiers and logistic model trees as 
classifiers with 10-fold cross validation. The 
rule-based method, ClueBot, is our baseline.  
We also implemented another baseline system, 
using the bag of words (BoW) and Naive Bayes 
method (Smets et al, 2008) and the same toolkit 
(McCallum, 1996) that Smets et al used. Then, 
we compare our result with Potthast et al (2008), 
who used the same corpus as us. 
 
                                                 
4 http://www.yahoo.com 
5 http://www.bing.com 
1152
 
Table 3: Preliminary Experiment Results; The 
acronyms: BoW: Bag of Words, LMT: Logistic 
Model Trees, LB: Logistic Boosting, Task-
specific + Lexical: features in section 4.1 and 4.2 
 
As we can see in Table 3, the ClueBot has a 
F-score (F1) of 0.43. The BoW + Na?ve Bayes 
approach improved the result and reached an F1 
of 0.75. Compared to these results, the system of 
Potthast et al (2008) is still better and has a F1 
of 0.80. 
For the results of our system, LMT gives us a 
0.89 F1 and LogitBoost (LB) gives a 0.95 F1. A 
significant F1 improvement of 15% was 
achieved in comparison to the previous study 
(Potthast et al, 2008). Another finding is that we 
find our shallow syntactic-semantic modeling 
method improves 2-4% over our task-specific 
and lexical features.  
5.3 Results and Analysis 
In the second experiment, a notable difference 
from the preliminary evaluation is that we have 
an unbalanced data problem. So, we use random 
down-sampling method to resample the majority 
class into balanced classes in the training stage. 
Then, we also use the two classifiers with 10-
fold cross validation. 
The F1 result reported by our BoW + Na?ve 
Bayes baseline is 0.68. Next, we test our task-
specific and lexical features that specified in sec-
tion 4.1 and 4.2. The best result is a F1 of 0.82, 
using logistic boosting. Finally, with our topic-
specific shallow syntactic and semantic model- 
 
Table 4: Second Experiment Results 
 
ing features, we have a precision of 0.86, a recall 
of 0.85 and F1 of 0.85. 
Though we are surprised to see the overall F1 
for the second experiment are not as high as the 
first one, we do see that the topic-specific shal-
low syntactic and semantic modeling methods 
play an important role in improving the result.  
Looking back at the related work we men-
tioned in section 2, though we use newer data 
sets, our overall results still seem to surpass ma-
jor vandalism detection systems. 
6 Conclusion and Future Works 
We have described a practical classification 
framework for detecting Wikipedia vandalism 
using NLP techniques and shown that it outper-
forms rule-based methods and other major ma-
chine learning approaches that are previously 
applied in the task.  
In future work, we would like to investigate 
deeper syntactic and semantic cues to vandalism. 
We hope to improve our models using shallow 
parsing and full parse trees. We may also try 
lexical chaining to model the internal semantic 
links within each edit. 
Acknowledgements 
The authors are grateful to Julia Hirschberg, 
Yves Petinot, Fadi Biadsy, Mukund Jha, Wei-
yun Ma, and the anonymous reviewers for useful 
feedback. We thank Potthast et al for the Wiki-
pedia vandalism detection corpora. 
Systems Recall Precision F1 
ClueBot 0.27 1 0.43 
BoW + 
Na?ve Bayes 
0.75 0.74 0.75 
Potthast 
et. al., 2008 
0.77 0.83 0.80 
Task-specific 
+Lexical 
(LMT) 
0.87 0.87 0.87 
Task-specific 
+Lexical (LB) 
0.92 0.91 0.91 
Our System 
 (LMT) 
0.89 0.89 0.89 
Our System 
(LB) 
0.95 0.95 0.95 
 
Features Recall Precision F1 
BoW +  
Na?ve Bayes 
0.68 0.68 0.68 
Task-specific 
(LMT) 
0.81 0.80 0.80 
Task-specific 
+Lexical(LMT) 
0.81 0.81 0.81 
Our System 
(LMT) 
0.84 0.83 0.83 
Task-specific 
(LB) 
0.81 0.80 0.80 
Task-specific + 
Lexical (LB) 
0.83 0.82 0.82 
Our System 
(LB) 
0.86 0.85 0.85 
 
1153
References 
Adler, B. Thomas, Luca de Alfaro, Ian Pye and 
Vishwanath Raman. 2008. Measuring Author Con-
tributions to the Wikipedia. In Proc. of the ACM 
2008 International Symposium on Wikis. 
Biadsy, Fadi, Julia Hirschberg, and Elena Filatova. 
2008. An Unsupervised Approach to Biography 
Production using Wikipedia. In Proc. of the 46th 
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 807?815. 
Bratko, Andrej,  Gordon V. Cormack, Bogdan Filip-
ic,Thomas R. Lynam and Blaz Zupan. 2006. Spam 
Filtering Using Statistical Data Compression 
Models. Journal of Machine Learning Research, 
pages 7:2673-2698. 
Collins, Michael, Brian Roark and Murat Saraclar. 
2005. Discriminative Syntactic Language Model-
ing for Speech Recognition. In Proc. of the 43rd 
Annual Meeting of the Association for Computa-
tional Linguistics. pages 507?514. 
Friedman, Jerome, Trevor Hastie and Robert Tibshi-
rani. 2000. Additive Logistic Regression: a Statis-
tical View of Boosting. Annals of Statistics 28(2), 
pages 337-407. 
Geiger, R. Stuart. 2010. The Work of Sustaining Or-
der in Wikipedia: The Banning of a Vandal. In 
Proc. of the 2010 ACM Conference on Computer 
Supported Cooperative Work, pages 117-126. 
Heckel, Paul. 1978. A Technique for Isolating Differ-
ences Between Files. Communications of the ACM, 
pages 264?268 
Itakura, Kelly Y. and Charles L. A. Clarke. 2009.  
Using Dynamic Markov Compression to Detect 
Vandalism in the Wikipedia. In Proc. of the 32nd 
International ACM SIGIR Conference on Research 
and Development in Information Retrieval, pages 
822-823. 
Landwehr, Niels, Mark Hall and Eibe Frank. 2005. 
Logistic Model Trees. Machine Learning, 59(1-2), 
pages 161?205. 
McCallum, Andrew. 1996. Bow: a Toolkit for Statis-
tical Language Modeling, Text Retrieval, Classifi-
cation and Clustering.  
Potthast, Martin, Benno Stein, and Robert Gerling. 
2008. Automatic Vandalism Detection in Wikipe-
dia. In Proc. of the 30th European Conference on 
Information Retrieval, Lecture Notes in Computer 
Science, pages 663-668. 
Potthast, Martin and Robert Gerling. 2007. Wikipedia 
Vandalism Corpus WEBIS-VC07-11. Web Tech-
nology & Information Systems Group, Bauhaus 
University Weimar. 
Potthast, Martin, Benno Stein and Teresa Holfeld. 
2010. PAN Wikipedia Vandalism Training Corpus 
PAN-WVC-10. Web Technology & Information 
Systems Group, Bauhaus University Weimar. 
Rigoutsos, Isidore and Tien Huynh. 2004. Chung-
Kwei: a pattern-discovery-based system for the au-
tomatic identification of unsolicited e-mail mes-
sages (SPAM). In Proc. of the First Conference on 
E-mail and Anti-Spam. 
Smets, Koen, Bart Goethals and Brigitte Verdonk. 
2008. Automatic Vandalism Detection in Wikipe-
dia: Towards a Machine Learning Approach In 
Proc. of AAAI '08, Workshop on Wikipedia and 
Artificial Intelligence, pages 43-48. 
Stein, Klaus and Claudia Hess. 2007. Does It Matter 
Who Contributes: a Study on Featured Articles in 
the German Wikipedia. In Proc. of the ACM 18th 
Conference on Hypertext and Hypermedia, pages 
171?174. 
Stolcke, Andreas. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Interna-
tional Conference on Spoken Language 
Processing, volume 2, pages 901?904. 
Toutanova, Kristina and Christopher D. Manning. 
2000. Enriching the Knowledge Sources Used in a 
Maximum Entropy Part-of-Speech Tagger. In Proc. 
of the Joint SIGDAT Conference on Empirical Me-
thods in Natural Language Processing and Very 
Large Corpora, pages 63-70. 
Toutanova, Kristina, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency 
Network. In Proceedings of Human Language 
Technology Conference and the North American 
Chapter of the Association of Computational Lin-
guistics Series, pages 252-259. 
Wilkinson,Dennis and Bernardo Huberman. 2007. 
Cooperation and Quality in Wikipedia. In Proc. of 
the ACM 2007 International Symposium on Wikis, 
pages 157?164. 
1154
Coling 2010: Poster Volume, pages 946?954,
Beijing, August 2010
MT Error Detection for Cross-Lingual Question Answering
Kristen Parton
Columbia University
New York, NY, USA
kristen@cs.columbia.edu
Kathleen McKeown
Columbia University
New York, NY, USA
kathy@cs.columbia.edu
Abstract
We present a novel algorithm for de-
tecting errors in MT, specifically focus-
ing on content words that are deleted
during MT. We evaluate it in the con-
text of cross-lingual question answering
(CLQA), where we try to correct the
detected errors by using a better (but
slower) MT system to retranslate a lim-
ited number of sentences at query time.
Using a query-dependent ranking heuris-
tic enabled the system to direct scarce
MT resources towards retranslating the
sentences that were most likely to ben-
efit CLQA. The error detection algo-
rithm identified spuriously deleted con-
tent words with high precision. How-
ever, retranslation was not an effective
approach for correcting them, which in-
dicates the need for a more targeted ap-
proach to error correction in the future.
1 Introduction
Cross-lingual systems allow users to find infor-
mation in languages they do not know, an in-
creasingly important need in the modern global
economy. In this paper, we focus on the spe-
cial case of cross-lingual tasks with result trans-
lation, where system output must be translated
back into the user?s language. We refer to tasks
such as these as task-embedded machine trans-
lation, since the performance of the system as a
whole depends on both task performance and the
quality of the machine translation (MT).
Consider the case of cross-lingual question
answering (CLQA) with result translation: a user
enters an English question, the corpus is Ara-
bic, and the system must return answers in En-
glish. If the corpus is translated into English be-
fore answer extraction, an MT error may cause
the system to miss a relevant sentence, leading
to decreased recall. Boschee et al (2010) de-
scribe six queries from a formal CLQA evalu-
ation where none of the competing systems re-
turned correct responses, due to poor translation.
In one example, the answer extractor missed a
relevant sentence because the name ?Abu Hamza
al-Muhajir? was translated as ?Zarqawi?s succes-
sor Issa.? However, even if answer extraction is
done in Arabic, errorful translations of the cor-
rect answer can affect precision: if the user can-
not understand the translated English sentence,
the result will be perceived irrelevant. For in-
stance, the user may not realize that the mistrans-
lation ?Alry$Awy? refers to Al-Rishawi.
Our goal was not to improve a specific CLQA
system, but rather to find MT errors that are
likely to impact CLQA and correct them. We in-
troduce an error detection algorithm that focuses
on several common types of MT errors that are
likely to impact translation adequacy:
? content word deletion
? out-of-vocabulary (OOV) words
? named entity missed translations
The algorithm is language-independent and MT-
system-independent, and generalizes prior work
by detecting errors at the word level and detect-
ing errors across multiple parts of speech.
We demonstrate the utility of our algorithm by
applying it to CLQA at query time, and investi-
gate using a higher-quality MT system to correct
the errors. The CLQA system translates the full
corpus, containing 119,879 text documents and
150 hours of speech, offline using a production
MT system, which is able to translate quickly
(5,000 words per minute) at the cost of lower
quality translations. A research MT system has
higher quality but is too slow to be practical for
a large amount of data (at 2 words per minute,
946
it would take 170 days on 50 machines to trans-
late the corpus). At query-time, we can call the
research MT system to retranslate sentences, but
due to time constraints, we can only retranslate k
sentences (we set k=25). In order to choose the
sentences to best improve CLQA performance,
we rank potential sentences using a relevance
model and a model of error importance.
Our results touch on three areas:
? Evaluation of our algorithm for detecting
content word deletion shows that it is ef-
fective, accurately pinpointing errors 89%
of the time (excluding annotator disagree-
ments).
? Evaluation of the impact of re-ranking
shows that it is crucial for directing scarce
MT resources wisely as the higher-ranked
sentences were more relevant.
? Although the research MT system was per-
ceived to be significantly better than the
production system, evaluation shows that
it corrected the detected errors only 39%
of the time. Furthermore, retranslation
seems to have a negligible effect on rele-
vance. These unexpected results indicate
that, while we can identify errors, retrans-
lation is not a good approach for correcting
them. We discuss this finding and its impli-
cations in our conclusion.
2 Task-Embedded MT
A variety of cross-lingual applications use MT
to enable users to find information in other lan-
guages: e.g., CLQA, cross-lingual information
retrieval (CLIR), and cross-lingual image re-
trieval. However, cross-lingual applications such
as these typically do not do result translation
? for instance, an English-French CLIR system
would take an English query and return French
documents, assuming that result translation is a
separate MT problem. Part of the reason for
the separation between cross-lingual tasks and
MT is that evaluating task performance on MT
is often difficult. For example, for a multilin-
gual summarization task combining English and
machine translated English, Daume? and Marcu
(2006) found that doing a pyramid annotation on
MT was difficult due to the poor MT quality.
Assessing cross-lingual task performance
without result translation is problematic, because
in a real-world application, result translation
would affect task performance. For instance, in
English-Arabic CLIR, a poorly translated rele-
vant Arabic document may appear to be irrel-
evant to an English speaker. Decoupling the
cross-lingual application from the MT system
also limits the opportunity for feedback between
the application and the MT system. Ji and Grish-
man (2007) exploited a feedback loop between
Chinese and English named entity (NE) tagging
and Chinese-English NE translation to improve
both NE extraction and NE translation.
In this paper, error detection is done at query
time so that query context can be taken into ac-
count when determining which sentences to re-
translate. We also use the task context to detect
errors in translating NEs present in the query.
3 Related Work
There is extensive prior work in describing MT
errors, but they usually involve post-hoc error
analysis of specific MT systems (e.g., (Kirch-
hoff et al, 2007), (Vilar et al, 2006)) rather than
online error detection. One exception is Herm-
jakob et al (2008), who studied NE translation
errors, and integrated an improved on-the-fly NE
transliterator into an SMT system.
Content word deletion in MT has been stud-
ied from different perspectives. Li et al (2008)
and Menezes and Quirk (2008) explored ways of
modeling (intentional) source-word deletion in
MT and showed that it can improve BLEU score.
Zhang et al (2009) described how errors made
during the word-alignment and phrase-extraction
phases in training phrase-based SMT often lead
to spurious insertions and deletions during trans-
lation decoding. This is a common error ? Vilar
et al (2006) found that 22% of errors produced
by their Chinese-English MT system were due to
missing content words. Parton et al (2009) did
a post-hoc analysis on the cross-lingual 5W task
and found that content word deletion accounted
for 17-22% of the errors on that task.
Some work has been done in addressing MT
errors for different cross-lingual tasks. Ji and
947
1) Source kmA tHdv wzyr AldfAE AlAsrA}yly Ayhwd bArAk Al*y zAr mwqE Altfjyr AlAntHAry fy dymwnp fy
wqt sAbq En Altfjyr AlAntHAry . . .
ProdM?T There also the Israeli Defense Minister Ehud Barak, who visited the site of the suicide bombing in Dimona
earlier, the suicide bombing . . .
Ref. Moreover, Israeli Defense Minister Ehud Barak, who visited the scene of the suicide bombing in Dimona
earlier, spoke about the suicide bombing . . .
2) Source . . . Akd Ely rgbp hrAry AlAstfAdp mn AltjArb AlAyrAnyp fy mwAjhp Alqwy AlmEtdyp.
ProdM?T . . . stressed the desire to test the Iranian Harare in the face of the invading forces.
Ref. . . . stressed Harare?s desire to benefit from the Iranian experience in the face of the forces of aggressors.
Table 1: Two examples of content word deletion during MT.
Grishman (2007) detected NE translation errors
in the context of cross-lingual entity extraction,
and used the task context to improve NE transla-
tion. Ma and McKeown (2009) investigated verb
deletion in Chinese-English MT in the context
of CLQA. They tested two SMT systems, and
found deleted verbs in 4-7% of the translations.
After using post-editing to correct the verb dele-
tion, QA relevance increased for 7% of the sen-
tences, showing that an error that may have little
impact on translation metrics such as BLEU (Pa-
pineni et al, 2002) can have a significant impact
on cross-lingual applications.
Our work generalizes Ma and McKeown
(2009) by detecting content-word deletions and
other MT errors rather than just verb deletions.
We also relax the assumption that translation pre-
serves part of speech (i.e., that verbs must trans-
late into verbs), assuming only that a phrase con-
taining a content word should be translated into
a phrase containing a content word. Instead of
post-editing, we use an improved MT system to
retranslate sentences with detected errors.
Using retranslation to correct errors exploits
the fact that some sentences are harder to trans-
late than others. In a resource-constrained set-
ting, it makes sense to apply a better MT system
only to sentences for which the fast MT system
has lower confidence. We do not know of other
systems that do multi-pass translation, but it is
an interesting area for further work.
4 MT Error Detection
Most MT systems try to balance translation flu-
ency with adequacy, which refers to the amount
of meaning expressed in the original that is also
expressed in the translation. For task-embedded
MT, errors in adequacy are more likely to have
an impact on performance than errors in fluency.
Many MT metrics (such as BLEU) treat all to-
kens equally, so deleting a verb is penalized the
same as deleting a comma. In contrast, we focus
on errors in translating content words, which are
words with open-class parts of speech (POS), as
they are more likely to impact adequacy. First
we describe how MT deletion errors arise and
how we can detect them, and finally we describe
detection of other types of errors.
4.1 Deletion in MT
The simplest case of content word deletion is
a complete deletion by the translation model
? in other words, a token was not translated.
We assume the MT system produces word or
phrase alignments, so this case can be detected
by checking for a null alignment. However, it
is necessary to distinguish correct deletion from
spurious deletion. Some content words do not
need to be translated ? for example the Arabic
copular verb ?kAn? (?to be?) is often correctly
deleted when translating into English.
A more subtle form of content word deletion
occurs when a content word is translated as a
non-content word. This can be detected by com-
paring the parts of speech of aligned words. Con-
sider the production MT System (Prod. MT) ex-
ample in Table 1: the verb ?tHdv?1 (?spoke?) has
been translated as the expletive ?there.?
Finally, another case of content word deletion
occurs when a content word is translated as part
of a larger MT phrase, but the content word is
not translated. In the second example in Table 1,
an Arabic phrase consisting of a noun and prepo-
sition is translated as just the preposition ?to.?
1Arabic examples in this paper are shown in Buckwalter
transliteration (Buckwalter, 2002).
948
The latter two kinds of content word deletion
are considered mistranslations rather than dele-
tions by the translation model, since the deleted
source-language token does produce one or more
target-language tokens. However, from the per-
spective of a cross-lingual application, there was
a deletion, since some content that was present
in the original is not present in the translation.
4.2 Detecting Deleted Content Words
The deletion detection algorithm is motivated
by the assumption that a source-language
phrase containing one or more meaning-bearing
words should produce a phrase with one or
more meaning-bearing words in the translation.
(Phrase refers to an n-gram rather than a syntac-
tic phrase.) Note that this does not assume a one-
to-one correspondence between content words
? for example, translating the phrase ?spoke
loudly? as the single word ?yelled? satisfies the
assumption. This hypothesis favors precision
over recall, since it may miss cases where two
content words are incorrectly translated as a sin-
gle content word (for instance, if ?coffee table?
is translated as ?coffee?).
The algorithm takes as input POS tags in both
languages and word alignments produced by the
MT system during translation. The exact defi-
nition of ?content word? will depend upon the
language and POS tagset. The system iterates
over all content words in the source sentence,
and, for each word, checks whether it is aligned
to one or more content words in the target sen-
tence. If it has no alignment, or is aligned to
only function words, the system reports an error.
This rule-based approach has poor precision be-
cause of content words that are correctly deleted.
For example, in the sentence ?I am going to
watch TV,? ?am? and ?going? are tagged as
verbs, but may be translated as function words.
To address this, frequent content words were
heuristically filtered using source-language IDF
(inverse-document frequency) over the QA cor-
pus. The cut-off was tuned on a development set.
This algorithm is a lightweight, language-
independent and MT-system-independent way
to find errors in MT. The only requirement is
that the MT system produce word or phrase
alignments. This algorithm generalizes Ma and
McKeown (2009) in several ways. First, it
detects any deleted content words, rather than
just verbs. The previous work only addresses
complete deletions, where the deleted token has
a null alignment, whereas this approach finds
cases where content words are mistranslated as
non-content words. Finally, this error detection
algorithm is more fine-grained, since it is at the
word level rather than the phrase level.
4.3 Additional Error Detection Heuristics
For the CLQA task, we extended our MT er-
ror detection algorithm to handle two additional
types of MT errors, OOV words and NE mis-
translations, and to rank the errors. The pro-
duction MT system was explicitly set to not
delete OOV words, so they were easy to detect
as source-language words left in the target lan-
guage. The CLIR system was used to find occur-
rences of query NEs in the corpus, and then word
alignments were used to extract the correspond-
ing translations. If the translations were not a
fuzzy match to the query, then it was flagged as
a possible NE translation error. For instance,
in a query about al-Rishawi, the CLIR would
return Arabic-language matches to the Arabic
word Alry$Awy. If the aligned English trans-
lation was al-Ryshoui instead of al-Rishawi, it
would be flagged as an error.
Even if the retranslation corrects the errors
in MT, if the sentences are not relevant, they
will have no impact on CLQA. To account for
relevance, we implemented a bilingual bag-of-
words matching model, and ranked sentences
with more keyword matches to the query higher.
Sentences with the same estimated relevance
were further sorted by potential impact of the
MT error on the task. Errors affecting NEs (ei-
ther via source-language POS tagging or source-
language NE recognition) were ranked highest,
since our particular CLQA task is focused on
NEs. The final output of the algorithm is a list of
sentences with MT errors, ranked by relevance
to the query and importance of the error.
949
5 Experimental Setup
We begin by describing the MT systems, which
motivate the need for time-constrained MT. Then
we describe the CLQA task and the baseline
CLQA system, and finally how the error detec-
tion algorithm is used by the CLQA system.
5.1 MT Systems
Both the research and production MT systems
used in our evaluation were based on Direct
Translation Model 2 (Ittycheriah and Roukos,
2007), which uses a maximum entropy approach
to extract minimal translation blocks (one-to-
M phrases with optional variable slots) and
train system parameters over a large number of
source- and target-language features. The re-
search system incorporates many additional syn-
tactic features and does a deeper (and slower)
beam search, both of which cause it to be much
slower than the production system. In addition,
the research MT system filters the training data
to match the test data, as is customary in MT
evaluations, whereas the production system must
be able to handle a wide range of input data. Part
of the reason for the slower running time is that
the research system has to retrain; the advan-
tage is that more test-specific training data can
be used to tailor the MT system to the input.
Overall, the research MT system performs 4
BLEU points better than the production MT sys-
tem on a standard MT evaluation test corpus, but
at a great cost: the production MT handles 5,000
words per minute, while the research MT system
handles 2 words per minute. Using 50 machines,
the production MT system could translate the
corpus in under 2 hours, whereas the research
MT system would take 170 days. This vast dif-
ference succinctly captures the motivation be-
hind the time-constrained retranslation step.
5.2 CLQA Task
The CLQA task was designed for the DARPA
GALE (Global Autonomous Language Exploita-
tion) project. The questions found are open-
ended, non-factoid information needs. There are
22 question types, and each type has its own
relevance guidelines. For instance, one type is
?Describe the election campaign of [PERSON],?
and a question could be about Barack Obama.
Queries are in English, the corpus is in Arabic,
and the system must output comprehensible En-
glish sentences that are relevant to the question.
The Arabic corpus was created for the eval-
uation and consists of four genres: formal text
(72,677 documents), informal text (47,202 doc-
uments), formal speech (50 hours), and informal
speech (80 hours). The speech data was story
segmented and run through a speech recogni-
tion system before translation. We used 31 text
queries developed by the Linguistic Data Con-
sortium (LDC), and 39 speech queries developed
by other researchers working on the CLQA task.
5.3 CLQA System
The baseline CLQA system translates the full
corpus offline before running further processing
on the translated sentences (parsing, NE recog-
nition, information extraction, etc.) and index-
ing the corpus. At query-time, CLIR (imple-
mented with Apache Lucene) returns documents
relevant to the query, and the CLQA answer ex-
traction system is run over the translated doc-
uments. The answer extraction system relies
on target-language annotations, but any MT er-
rors will propagate to target-language process-
ing, and therefore affect answer extraction.
5.4 CLQA System with MT Error
Detection
The error detection and retranslation module was
added to the baseline system after CLIR, but be-
fore answer extraction. The inputs to the de-
tection algorithm are the query and a list of
ranked documents returned by CLIR. The detec-
tion algorithm has access to the indexed (bilin-
gual) corpus, source- and target-language anno-
tations (POS tagging and NE recognition), and
MT word alignments. The error detection algo-
rithm has two stages: first it runs over sentences
in documents related to the query, and after it
finds 2k sentences with errors (or exhausts the
document list), it reranks the errors as described
in section 4.3 and retranslates the top k=25 sen-
tences. Then the merged set of original and re-
translated relevant sentences are passed to the
950
answer extraction module.
By doing retranslation before answer extrac-
tion, the algorithm has the potential to improve
precision and recall. An improved translation of
a relevant Arabic sentence is more likely to be
selected by the answer extraction system and in-
crease recall, as in Boschee et al (2010), where
answers were missed due to mistranslation. A
better translation of a relevant sentence is also
more likely to be perceived as relevant, as shown
by Ma and McKeown (2009).
6 Evaluation
Amazon Mechanical Turk (AMT) was used to
conduct a large-scale evaluation of the impact
of error detection and retranslation on relevance.
An intrinsic evaluation of the error detection was
run on a subset of the sentences, since it required
bilingual annotators.
6.1 Task-Based Evaluation
Each sentence was annotated in the production
MT version and the research MT version. The
annotators were first presented with template
relevance guidelines and an example question,
along with 3 ? 4 example sentences and expected
judgments. Then the actual question was pre-
sented to the annotator, along with 5 sentences
(all from a single MT system). For each sen-
tence, the annotators were first asked to judge
perceived adequacy and then relevance.
The perceived adequacy rating was loosely
based upon MT adequacy evaluations ? in other
words, annotators were told to ignore grammati-
cal errors and focus on perceived meaning. How-
ever, since there were no reference translations,
annotators were asked to rate how much of the
sentence they believed they understood by se-
lecting one of (All, More than half, About half,
# detected errors # detected errors
Genre per sentence per 1,000 tokens
Newswire 0.16 56
Broadcast 0.23 105
news
Broadcast 0.14 84
conversation
Table 2: Number of errors detected across differ-
ent genres.
Less than half, and None).
The relevance rating was based on the tem-
plate relevance guidelines, and annotators could
select one of (Relevant, Maybe relevant, Not rel-
evant, Can?t tell due to bad translation and Can?t
tell due to other reason).
6.2 Amazon Mechanical Turk (AMT)
The evaluation was run on AMT, which has been
extensively used in NLP and has been shown to
have high correlation with expert annotators on
many NLP tasks at a lower cost (Snow et al,
2008). It has also been used in MT evaluation
(Callison-Burch, 2009), though that evaluation
used reference translations.
For 70 queries, the top 25 ranked sentences
in both the production and research MT versions
were evaluated. Each sentence was judged for
both relevance and perceived adequacy by 5 an-
notators, for a total of 35,000 individual judg-
ments. As is standard, some of the judgments
were filtered due to noise by using the percent
of time that an annotator disagreed with all other
annotators, and the relative time spent on a given
annotation. The percent of sentences with ma-
jority agreement was 91% for relevance and 72%
for perceived adequacy.
6.3 Intrinsic Evaluation
Annotators were presented with an Arabic sen-
tence with a single token highlighted, and asked
whether the token was a ?content word? or not.
Then annotators were asked to decide which of
two translations (in random order) translated the
highlighted Arabic word best, or whether they
were equal. In total, 150 sentences were judged
by annotators with knowledge of Arabic. For
both questions, kappa agreement was moderate.
7 Results
Table 2 shows how many errors were found
by the error detection algorithm for each genre.
Not surprisingly, more errors are detected in the
speech genres (84 and 105 errors per 1,000 to-
kens) than in formal text (56 errors per 1,000
tokens). We attribute the large difference be-
tween broadcast news and broadcast conversa-
951
Perceiv
ed Adeq
uacy R
es.MT Prod. M
T
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
Rank
Relevan
ce
Prod. M
T
Rank
Figure 1: Average normalized cumulative sen-
tence perceived adequacy and relevance versus
rank of the sentence, by the ranking heuristic.
tion to the large number of short sentences with-
out content words in informal speech (such as
?hello?, ?thank you?, etc.).
7.1 Perceived MT Adequacy
The research MT significantly outperformed the
production MT in perceived adequacy (accord-
ing to ANOVA with p=0.001). Of the production
MT translations, 58% were considered ?more
than half? or ?all? understandable, whereas 69%
of the research MT were. Overall, retranslation
increased perceived adequacy in 17% of the sen-
tences, and decreased it in only 5% of sentences.
7.2 Ranking Algorithm
Figure 1 show the average cumulative sentence
relevance and perceived adequacy, as ranked by
the error detection algorithm. In other words, at
each rank i, the average relevance (or perceived
adequacy) of sentences (1 ? i) was calculated.
On the perceived adequacy chart, the research
MT system consistently outperforms the produc-
tion MT system by a statistically significant mar-
gin. For relevance, the research MT curve is only
marginally higher than the production MT curve.
The shape of the relevance curves shows that
ranking sentences by a simple bilingual bag-of-
words model did affect sentence relevance, since
sentences that are higher ranked have higher cu-
mulative average relevance. By ranking sen-
tences with a basic relevance model, we were
able to focus the scarce MT resources on sen-
Relevance
? Same ? No maj./
Don?t know
MT ? 20 201 9 56 17%
MT same 93 919 72 212 78%
MT ? 2 56 4 28 5%
7% 70% 5% 18%
Table 3: The relationship between changes in
perceived adequacy and changes in relevance.
tences that are most likely to help the CLQA
task. This underscores the importance of using
the task context to guide MT error detection, es-
pecially in the case of time-constrained MT.
7.3 CLQA Relevance
Annotators judged 14.5% of the production MT
sentences relevant. After retranslation, the over-
all number of sentences considered relevant in-
creased to 14.7%. Although the overall numbers
are similar, the relevance of many individual sen-
tences did change. Table 3 shows the results of
comparing annotations on the original MT with
annotations on the retranslated MT. Relevance
was classified as ? or ? by comparing the ma-
jority judgment of the production MT to the re-
search MT. Changes in MT were based on com-
paring the average rating of both versions, with
a tolerance of 1.0.
Of the sentences with better perceived MT,
7% increased in relevance, and 3% decreased in
relevance. When the retranslated sentence was
considered worse, there was a 2% increased in
relevance and a 4% decrease. In other words,
when retranslation had a positive effect, it more
often led to increased relevance. However, the
impact of retranslation was mixed, and none of
the changes was statistically significant.
7.4 Intrinsic Evaluation
While the extrinsic evaluation focused on the im-
pact on CLQA relevance, the goal of the intrinsic
evaluation was to measure the precision of the
error detection algorithm, and whether retransla-
tion addressed the detected errors.
Of the 82% of sentences where both judges
agreed, 89% of the detected errors were con-
sidered content words. All of the OOV tokens
were content words (except for one disagree-
952
ment). Surprisingly, for the errors involving con-
tent words, 60% of the time both systems were
judged the same with regard to the highlighted
error. The research system was better 39% of the
time, and the original was better only 1% of the
time (excluding 26% disagreements).
8 Discussion
The CLQA evaluation was based on three hy-
potheses:
? That we could detect errors in MT with high
precision.
? That retranslating errorful sentences with a
much better MT system would correct the
errors we detected.
? That correcting errors would cause some
sentences to become relevant which were
not previously relevant, as in (Ma and McK-
eown, 2009).
The intrinsic evaluation confirmed that we can
identify content word deletions in MT with high
precision, thus validating the first hypothesis.
However, detecting the errors and retranslat-
ing them did not lead to large improvements in
CLQA relevance ? the impact of increased per-
ceived adequacy on relevance was mixed and not
significant. The intrinsic evaluation explains this
negative result: even though the retranslated sen-
tences were judged significantly better, the re-
translation only corrected the detected error 39%
of the time. In other words, the better research
MT system was making many of the same mis-
takes as the production MT system, despite us-
ing syntactic features and a much deeper search
space during decoding. Since the second hypoth-
esis did not hold, we need to improve our error
correction algorithm before we can tell whether
the third hypothesis holds.
This result directly motivates the need for tar-
geted error correction of MT. Automatic MT
post-editing has been successfully used for se-
lecting determiners (Knight and Chander, 1994),
reinserting deleted verbs (Ma and McKeown,
2009), correcting NE translations (Parton et al,
2008), and lexical substitutions (Elming, 2006).
Since Arabic and English word order differ
significantly, straightforward re-insertion of the
deleted words is not sufficient for error correc-
tion, so we are currently working on more so-
phisticated post-editing techniques.
9 Conclusions
We presented a novel online algorithm for de-
tecting MT errors in the context of a question,
and a heuristic for ranking MT errors by their
potential impact on the CLQA task. The er-
ror detection algorithm focused on content word
deletion, which has previously been shown to be
a significant problem in SMT. The algorithm is
generally applicable to any MT system that pro-
duces word or phrase alignments for its output
and any language pair that can be POS-tagged,
and it is more fine-grained and covers more types
of errors than previous work. It was able to de-
tect errors in Arabic-English MT across multiple
text and speech genres, and the intrinsic evalu-
ation showed that the large majority of tokens
flagged as errors were indeed content words.
The large-scale CLQA evaluation confirmed
that the slower research MT system was signif-
icantly better than the production MT system.
Relevance judgments showed that the ranking
component was crucial for directing scarce MT
resources wisely, as the higher-ranked sentences
were most likely to be relevant to the query, and
therefore most likely to benefit the CLQA sys-
tem by being retranslated.
Although we correctly identified MT errors,
retranslating the sentences with the errors had a
negligible effect on CLQA relevance. This un-
expected result may be explained by the fact that
only 39% of the errors were actually corrected
by the research MT system, so re-translation was
not a good approach for error correction. We
are currently working on correcting content word
deletion in MT via post-editing.
Acknowledgments The authors are grateful to
Radu Florian, Salim Roukos, Vittorio Castelli,
Dan Bikel and the whole GALE IBM team for
providing the experimental testbed, including the
CLQA and MT systems. This research was par-
tially supported by DARPA grant HR0011-08-C-
0110.
953
References
Boschee, Elizabeth, Marjorie Freedman, Roger
Bock, John Graettinger, and Ralph Weischedel.
2010. Error analysis and future directions for dis-
tillation. In GALE book (in preparation).
Buckwalter, Tim. 2002. Buckwalter arabic mor-
phological analyzer. Linguistic Data Consortium.
(LDC2002L49).
Callison-Burch, Chris. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using ama-
zon?s mechanical turk. In EMNLP ?09, pages 286?
295, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Daume?, III, Hal and Daniel Marcu. 2006. Bayesian
query-focused summarization. In ACL, pages
305?312, Morristown, NJ, USA. Association for
Computational Linguistics.
Elming, Jakob. 2006. Transformation-based correc-
tions of rule-based mt. In EAMT-2006: 11th An-
nual Conference of the European Association for
Machine Translation, pages 219?226.
Hermjakob, Ulf, Kevin Knight, and Hal Daume? III.
2008. Name translation in statistical machine
translation - learning when to transliterate. In
Proceedings of ACL-08: HLT, pages 389?397,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Ittycheriah, Abraham and Salim Roukos. 2007. Di-
rect translation model 2. In Sidner, Candace L.,
Tanja Schultz, Matthew Stone, and ChengXiang
Zhai, editors, HLT-NAACL, pages 57?64. The As-
sociation for Computational Linguistics.
Ji, Heng and Ralph Grishman. 2007. Collaborative
entity extraction and translation. In International
Conference on Recent Advances in Natural Lan-
guage Processing.
Kirchhoff, Katrin, Owen Rambow, Nizar Habash,
and Mona. Diab. 2007. Semi-automatic error
analysis for large-scale statistical machine trans-
lation systems. In Proceedings of the Machine
Translation Summit IX (MT-Summit IX).
Knight, Kevin and Ishwar Chander. 1994. Auto-
mated postediting of documents. In AAAI, pages
779?784.
Li, Chi-Ho, Dongdong Zhang, Mu Li, Ming Zhou,
and Hailei Zhang. 2008. An empirical study
in source word deletion for phrase-based statisti-
cal machine translation. In StatMT ?08: Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 1?8, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Ma, Wei-Yun and Kathleen McKeown. 2009.
Where?s the verb?: correcting machine transla-
tion during question answering. In ACL-IJCNLP
?09: Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 333?336, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Menezes, Arul and Chris Quirk. 2008. Syntactic
models for structural word insertion and deletion.
In EMNLP ?08, pages 735?744, Morristown, NJ,
USA. Association for Computational Linguistics.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In ACL, pages
311?318.
Parton, Kristen, Kathleen R. McKeown, James Al-
lan, and Enrique Henestroza. 2008. Simultane-
ous multilingual search for translingual informa-
tion retrieval. In CIKM 08, pages 719?728, New
York, NY, USA. ACM.
Parton, Kristen, Kathleen R. McKeown, Bob Coyne,
Mona T. Diab, Ralph Grishman, Dilek Hakkani-
Tu?r, Mary Harper, Heng Ji, Wei Yun Ma, Adam
Meyers, Sara Stolbach, Ang Sun, Gokhan Tur, Wei
Xu, and Sibel Yaman. 2009. Who, what, when,
where, why?: comparing multiple approaches to
the cross-lingual 5w task. In ACL-IJCNLP ?09,
pages 423?431, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Snow, Rion, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for nat-
ural language tasks. In EMNLP ?08, pages 254?
263, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Vilar, David, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of machine trans-
lation output. In International Conference on Lan-
guage Resources and Evaluation, pages 697?702,
Genoa, Italy, May.
Zhang, Yuqi, Evgeny Matusov, and Hermann Ney.
2009. Are unaligned words important for machine
translation ? In Conference of the European Asso-
ciation for Machine Translation, pages 226?233,
Barcelona, March.
954
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 317?320,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Time-Efficient Creation of an Accurate Sentence Fusion Corpus
Kathleen McKeown, Sara Rosenthal, Kapil Thadani and Coleman Moore
Columbia University
New York, NY 10027, USA
{kathy,sara,kapil}@cs.columbia.edu, cjm2140@columbia.edu
Abstract
Sentence fusion enables summarization and
question-answering systems to produce out-
put by combining fully formed phrases from
different sentences. Yet there is little data
that can be used to develop and evaluate fu-
sion techniques. In this paper, we present a
methodology for collecting fusions of simi-
lar sentence pairs using Amazon?s Mechani-
cal Turk, selecting the input pairs in a semi-
automated fashion. We evaluate the results
using a novel technique for automatically se-
lecting a representative sentence from multi-
ple responses. Our approach allows for rapid
construction of a high accuracy fusion corpus.
1 Introduction
Summarization and question-answering systems
must transform input text to produce useful output
text, condensing an input document or document set
in the case of summarization and selecting text that
meets the question constraints in the case of question
answering. While many systems use sentence ex-
traction to facilitate the task, this approach risks in-
cluding additional, irrelevant or non-salient informa-
tion in the output, and the original sentence wording
may be inappropriate for the new context in which
it appears. Instead, recent research has investigated
methods for generating new sentences using a tech-
nique called sentence fusion (Barzilay and McKe-
own, 2005; Marsi and Krahmer, 2005; Filippova and
Strube, 2008) where output sentences are generated
by fusing together portions of related sentences.
While algorithms for automated fusion have been
developed, there is no corpus of human-generated
fused sentences available to train and evaluate such
systems. The creation of such a dataset could pro-
vide insight into the kinds of fusions that people
produce. Furthermore, since research in the related
task of sentence compression has benefited from
the availability of training data (Jing, 2000; Knight
and Marcu, 2002; McDonald, 2006; Cohn and La-
pata, 2008), we expect that the creation of this cor-
pus might encourage the development of supervised
learning techniques for automated sentence fusion.
In this work, we present a methodology for cre-
ating such a corpus using Amazon?s Mechanical
Turk1, a widely used online marketplace for crowd-
sourced task completion. Our goal is the generation
of accurate fusions between pairs of sentences that
have some information in common. To ensure that
the task is performed consistently, we abide by the
distinction proposed by Marsi and Krahmer (2005)
between intersection fusion and union fusion. In-
tersection fusion results in a sentence that contains
only the information that the sentences had in com-
mon and is usually shorter than either of the original
sentences. Union fusion, on the other hand, results
in a sentence that contains all information content
from the original two sentences. An example of in-
tersection and union fusion is shown in Figure 1.
We solicit multiple annotations for both union and
intersection tasks separately and leverage the differ-
ent responses to automatically choose a representa-
tive response. Analysis of the responses shows that
our approach yields 95% accuracy on the task of
union fusion. This is a promising first step and indi-
cates that our methodology can be applied towards
efficiently building a highly accurate corpus for sen-
tence fusion.
1https://www.mturk.com
317
1. Palin actually turned against the bridge project only after it
became a national symbol of wasteful spending.
2. Ms. Palin supported the bridge project while running for
governor, and abandoned it after it became a national scandal.
Intersection: Palin turned against the bridge project after it
became a national scandal.
Union: Ms. Palin supported the bridge project while running
for governor, but turned against it when it became a national
scandal and a symbol of wasteful spending.
Figure 1: Examples of intersection and union
2 Related Work
The combination of fragments of sentences on a
common topic has been studied in the domain of sin-
gle document summarization (Jing, 2000; Daume? III
and Marcu, 2002; Xie et al, 2008). In contrast to
these approaches, sentence fusion was introduced to
combine fragments of sentences with common infor-
mation for multi-document summarization (Barzilay
and McKeown, 2005). Automated fusion of sen-
tence pairs has since received attention as an inde-
pendent task (Marsi and Krahmer, 2005; Filippova
and Strube, 2008). Although generic fusion of sen-
tence pairs based on importance does not yield high
agreement when performed by humans (Daume? III
and Marcu, 2004), fusion in the context of a query
has been shown to produce better agreement (Krah-
mer et al, 2008). We examine similar fusion an-
notation tasks in this paper, but we asked workers
to provide two specific types of fusion, intersection
and union, thus avoiding the less specific definition
based on importance. Furthermore, as our goal is
the generation of corpora, our target for evaluation
is accuracy rather than agreement.
This work studies an approach to the automatic
construction of large fusion corpora using workers
through Amazon?s Mechanical Turk service. Previ-
ous studies using this online task marketplace have
shown that the collective judgments of many work-
ers are comparable to those of trained annotators
on labeling tasks (Snow et al, 2008) although these
judgments can be obtained at a fraction of the cost
and effort. However, our task presents an additional
challenge: building a corpus for sentence fusion re-
quires workers to enter free text rather than simply
choose between predefined options; the results are
prone to variation and this makes comparing and ag-
gregating multiple responses problematic.
A. After a decade on the job, Gordon had become an experi-
enced cop.
B. Gordon has a lot of experience in the police force.
Figure 2: An example of sentences that were judged to be
too similar for inclusion in the dataset
3 Collection Methodology
Data collection involved the identification of the
types of sentence pairs that would make suitable
candidates for fusion, the development of a sys-
tem to automatically identify good pairs and manual
filtering of the sentence pairs to remove erroneous
choices. The selected sentence pairs were then pre-
sented to workers on Mechanical Turk in an inter-
face that required them to manually type in a fused
sentence (intersection or union) for each case.
Not all pairs of related sentences are useful for the
fusion task. When sentences are too similar, the re-
sult of fusion is simply one of the input sentences.
For example (Fig. 2), if sentence A contains all the
information in sentence B but not vice versa, then
B is also their intersection while A is their union
and no sentence generation is required. On the other
hand, if the two sentences are too dissimilar, then
no intersection is possible and the union is just the
conjunction of the sentences.
We experimented with different similarity metrics
aimed at identifying pairs of sentences that were in-
appropriate for fusion. The sentences in this study
were drawn from clusters of news articles on the
same event from the Newsblaster summarization
system (McKeown et al, 2002). While these clus-
ters are likely to contain similar sentences, they will
contain many more dissimilar than similar pairs and
thus a metric that emphasizes precision over recall
is important. We computed pairwise similarity be-
tween sentences within each cluster using three stan-
dard metrics: word overlap, n-gram overlap and co-
sine similarity. Bigram overlap yielded the best pre-
cision in our experiments. We empirically arrived at
a lower threshold of .35 to remove dissimilar sen-
tences and an upper threshold of .65 to avoid near-
identical sentences, yielding a false-positive rate of
44.4%. The remaining inappropriate pairs were then
manually filtered. This semi-automated procedure
enabled fast selection of suitable sentence pairs: one
person was able to select 30 pairs an hour yielding
the 300 pairs for the full experiment in ten hours.
318
Responses Intersection Union
All (1500) 0.49 0.88
Representatives (300) 0.54 0.95
Table 1: Union and intersection accuracy
3.1 Using Amazon?s Mechanical Turk
Based on a pilot study with 20 sentence pairs, we
designed an interface for the full study. For inter-
section tasks, the interface posed the question ?How
would you combine the following two sentences into
a single sentence conveying only the information
they have in common??. For union tasks, the ques-
tion was ?How would you combine the following two
sentences into a single sentence that contains ALL of
the information in each??.
We used all 300 pairs of similar sentences for
both union and intersection and chose to collect five
worker responses per pair, given the diversity of
responses that we found in the pilot study. This
yielded a total of 3000 fused sentences with 1500
intersections and 1500 unions.
3.2 Representative Responses
Using multiple workers provides little benefit unless
we are able to harness the collective judgments of
their responses. To this end, we experiment with
a simple technique to select one representative re-
sponse from all responses for a case, hypothesizing
that such a response would have a lower error rate.
We test the hypothesis by comparing the accuracy of
representative responses with the average accuracy
over all responses.
Our strategy for selecting representatives draws
on the common assumption used in human com-
putation that human agreement in independently-
generated labels implies accuracy (von Ahn and
Dabbish, 2004). We approximate agreement be-
tween responses using a simple and transparent
measure for overlap: cosine similarity over stems
weighted by tf-idf where idf values are learned over
the Gigawords corpus2. After comparing all re-
sponses in a pairwise fashion, we need to choose a
representative response. As using the centroid di-
rectly might not be robust to the presence of er-
roneous responses, we first select the pair of re-
sponses with the greatest overlap as candidates and
2LDC Catalog No. LDC2003T05
Errors Intersection Union
Missing clause 2 7
Union/Intersection 46 6
S1/S2 21 8
Additional clause 10 1
Lexical 3 1
Table 2: Errors seen in 30 random cases (150 responses)
then choose the candidate which has the greatest to-
tal overlap with all other responses.
4 Results and Error Analysis
For evaluating accuracy, fused sentences were man-
ually compared to the original sentence pairs. Due to
the time-consuming nature of the evaluation, 50% of
the 300 cases were randomly selected for analysis.
10% were initially analyzed by two of the authors; if
a disagreement occurred, the authors discussed their
differences and came to a unified decision. The re-
maining 40% were then analyzed by one author. In
addition to this high-level analysis, we further ana-
lyzed 10% of the cases to identify the the types of
errors made in fusion as well as the techniques used
and the effect of task difficulty on performance.
The accuracy for intersection and union tasks is
shown in Table 1. For both tasks, accuracy of the se-
lected representatives significantly exceeded the av-
erage response accuracy. In our error analysis, we
found that workers often answered the intersection
task by providing a union, possibly due to a misin-
terpretation of the question. This caused intersection
accuracy to be significantly worse than union. We
analyzed the impact of this error by computing ac-
curacy on the first 30 cases (10%) without this error
and the accuracy for intersection increased 22%.
Error types were categorized as ?missing clause?,
?using union for intersection and vice versa?,
?choosing an input sentence (S1/S2)?, ?additional
clause? and ?lexical error?. Table 2 shows the num-
ber of occurrences of each in 10% of the cases.
We binned the sentence pairs according to
the difficulty of the fusion task for each pair
(easy/medium/hard) and found that performance
was not dependent on difficulty level; accuracy was
relatively similar across bins. We also observed that
workers typically performed fusion by selecting one
sentence as a base and removing clauses or merging
in additional clauses from the other sentence.
319
Figure 3: Number of cases in which x/5 workers pro-
vided accurate responses for fusion
In order to determine the benefit of using many
workers, we studied the number of workers who an-
swered correctly for each case. Figure 3 reveals that
2/5 or more workers (summing across columns) re-
sponded accurately in 99% of union cases and 82%
of intersection cases. The intersection results are
skewed due to the question misinterpretation issue
which, though it was the most common error, was
made by 3/5 workers only 17% of the time. Thus, in
the majority of the cases, accurate fusions can still
be found using the representative method.
5 Conclusion
We presented a methodology to build a fusion cor-
pus which uses semi-automated techniques to select
similar sentence pairs for annotation on Mechanical
Turk3. Additionally, we showed how multiple re-
sponses for each fusion task can be leveraged by au-
tomatically selecting a representative response. Our
approach yielded 95% accuracy for union tasks, and
while intersection fusion accuracy was much lower,
our analysis showed that workers sometimes pro-
vided unions instead of intersections and we sus-
pect that an improved formulation of the question
could lead to better results. Construction of the fu-
sion dataset was relatively fast; it required only ten
hours of labor on the part of a trained undergraduate
and seven days of active time on Mechanical Turk.
Acknowledgements
This material is based on research supported in part
by the U.S. National Science Foundation (NSF) un-
der IIS-05-34871 Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF.
3The corpus described in this work is available at
http://www.cs.columbia.edu/?kathy/fusioncorpus
References
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, pages 137?144.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
ACL, pages 449?456.
Hal Daume? III and Daniel Marcu. 2004. Generic sen-
tence fusion is an ill-defined summarization task. In
Proceedings of the ACL Text Summarization Branches
Out Workshop, pages 96?103.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177?185.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of Applied Natu-
ral Language Processing, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Proceedings of ACL, pages 193?196.
Erwin Marsi and Emiel Krahmer. 2005. Explorations in
sentence fusion. In Proceedings of the European Work-
shopon Natural Language Generation, pages 109?117.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280?285.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254?263.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human Factors in Computing
Systems, pages 319?326.
Zhuli Xie, Barbara Di Eugenio, and Peter C. Nel-
son. 2008. From extracting to abstracting: Gener-
ating quasi-abstractive summaries. In Proceedings of
LREC, May.
320
Proceedings of NAACL-HLT 2013, pages 433?438,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using a Supertagged Dependency Language Model to Select 
a Good Translation in System Combination 
 
Wei-Yun Ma Kathleen McKeown 
Department of Computer Science Department of Computer Science 
Columbia University Columbia University 
New York, NY 10027, USA New York, NY 10027, USA 
ma@cs.columbia.edu kathy@cs.columbia.edu 
 
 
 
 
 
 
Abstract 
We present a novel, structured language 
model - Supertagged Dependency Language 
Model to model the syntactic dependencies 
between words. The goal is to identify 
ungrammatical hypotheses from a set of 
candidate translations in a MT system 
combination framework and help select the 
best translation candidates using a variety of 
sentence-level features. We use a two-step 
mechanism based on constituent parsing and 
elementary tree extraction to obtain supertags 
and their dependency relations. Our 
experiments show that the structured language 
model provides significant improvement in 
the framework of sentence-level system 
combination. 
1 Introduction 
In recent years, there has been a burgeoning 
interest in incorporating syntactic structure into 
Statistical machine translation (SMT) models (e..g, 
Galley et al, 2006; DeNeefe and Knight 2009; 
Quirk et al, 2005). In addition to modeling 
syntactic structure in the decoding process, a 
methodology for candidate translation selection 
has also emerged. This methodology first generates 
multiple candidate translations followed by 
rescoring using global sentence-level syntactic 
features to select the final translation. The 
advantage of this methodology is that it allows for 
easy integration of complex syntactic features that 
would be too expensive to use during the decoding 
process. The methodology is usually applied in two 
scenarios: one is as part of an n-best reranking 
(Och et al, 2004; Hasan et al, 2006), where n-best 
candidate translations are generated through a 
decoding process. The other is translation selection 
or reranking (Hildebrand and Vogel 2008; 
Callison-Burch et al, 2012), where candidate 
translations are generated by different decoding 
processes or different decoders.  
This paper belongs to the latter; the goal is to 
identify ungrammatical hypotheses from given 
candidate translations using grammatical 
knowledge in the target language that expresses 
syntactic dependencies between words. To achieve 
that, we propose a novel Structured Language 
Model (SLM) - Supertagged Dependency 
Language Model (SDLM) to model the syntactic 
dependencies between words. Supertag (Bangalore 
and Joshi, 1999) is an elementary syntactic 
structure based on Lexicalized Tree Adjoining 
Grammar (LTAG). Traditional supertagged n-gram 
LM predicts the next supertag based on the 
immediate words to the left with supertags, so it 
can not explicitly model long-distance dependency 
relations. In contrast, SDLM predicts the next 
supertag using the words with supertags on which 
it syntactically depend, and these words could be 
anywhere and arbitrarily far apart in a sentence. A 
candidate translation?s grammatical degree or 
?fluency? can be measured by simply calculating 
the SDLM likelihood of the supertagged 
dependency structure that spans the entire sentence. 
To obtain the supertagged dependency structure, 
the most intuitive way is through a LTAG parser 
(Schabes et al, 1988). However, this could be very 
433
slow as it has time complexity of O(n6).  Instead 
we propose an alternative mechanism in this paper: 
first we use a constituent parser1 of O(n3) ~ O(n5) 
to obtain the parse of a sentence, and then we 
extract elementary trees with dependencies from 
the parse in linear time.  Aside from the 
consideration of time complexity, another 
motivation of this two-step mechanism is that 
compared with LTAG parsing, the mechanism is 
more flexible for defining syntactic structures of 
elementary trees for our needs. Because those 
structures are defined only within the elementary 
tree extractor, we can easily adjust the definition of 
those structures within the extractor and avoid 
redesigning or retraining our constituent parser. 
We experiment with sentence-level translation 
combination of five different translation systems; 
the goal is for the system to select the best 
translation for each input source sentence among 
the translations provided by the five systems. The 
results show a significant improvement of 1.45 
Bleu score over the best single MT system and 
0.72 Bleu score over a baseline sentence-level 
combination system of using consensus and n-
gram LM. 
2 Related Work  
Och et al, (2004) investigated various syntactic 
feature functions to rerank the n-best candidate 
translations. Most features are syntactically 
motivated and based on alignment information 
between the source sentence and the target 
translation. The results are rather disappointing. 
Only the non-syntactic IBM model 1 yielded 
significant improvement. All other tree-based 
feature functions had only a very small effect on 
the performance. 
In contrast to (Och et al, 2004)?s bilingual 
syntax features, Hasan et al, (2006) focused on 
monolingual syntax features in n-best reranking. 
They also investigated the effect of directly using 
the log-likelihood of the output of a HMM-based 
supertagger, and found it did not improve 
performance significantly. It is worth noticing that 
this log-likelihood is based on supertagged n-gram 
                                                          
1  Stanford parser (http://nlp.stanford.edu/software/lex-
parser.shtml). We use its PCFG version of O(n3) for SDLM 
training of part of Gigaword in addition to Treebank and use 
its factor version of O(n5) to calculate the SDLM likelihood of 
translations. 
LM, which is one type of class-based n-gram LM, 
so it does not model explicit syntactic 
dependencies between words in contrast to the 
work we describe in this paper. Hardmeier et al, 
(2012) use tree kernels over constituency and 
dependency parse trees for either the input or 
output sentences to identify constructions that are 
difficult to translate in the source language, and 
doubtful syntactic structures in the output language. 
The tree fragments extracted by their tree kernels 
are similar to our elementary trees but they only 
regard them as the individual inputs of support 
vector machine regression while binary relations of 
our elementary trees are considered in a 
formulation of a structural language model. 
Outside the field of candidate translation 
selection, Hassan et al, (2007) proposed a phrase-
based SMT model that integrates supertags into the 
target side of the translation model and the target 
n-gram LM. Two kinds of supertags are employed: 
those from LTAG and Combinatory Categorial 
Grannar (CCG), and both yield similar 
improvements. They found that using both or 
either of the supertag-based translation model and 
supertagged LM can achieve significant 
improvement. Again, the supertagged LM is a 
class-based n-gram LM and does not model 
explicit syntactic dependencies during decoding. 
In the field of MT system combination, word-
level confusion network decoding is one of the 
most successful approaches (Matusov et al, 2006; 
Rosti et al, 2007; He et al 2008; Karakos et al 
2008; Sim et al 2007; Xu et al 2011). It is capable 
of generating brand new translations but it is 
difficult to consider more complex syntax such as 
dependency LM during decoding since it adds one 
word at a time while a dependency based LM must 
parse a complete sentence. Typically, a confusion 
network approach selects one translation as the 
best and uses this as the backbone for the 
confusion network. The work we present here 
could provide a more sophisticated mechanism for 
selecting the backbone. Alternatively, one can 
enhance confusion network models by 
collaborating with a sentence-level combination 
model which uses complex syntax to re-rank n-best 
outputs of a confusion network model. This kind of 
collaboration is one of our future works. 
 
 
434
3 LTAG and Supertag 
LTAG (Joshi et al, 1975; Schabes et al, 1988) is a 
formal tree rewriting formalism, which consists of 
a set of elementary trees, corresponding to minimal 
linguistic structures that localize dependencies, 
including long-distance dependencies, such as 
predicate-argument structure. Each elementary tree 
is associated with at least one lexical item on its 
frontier. The lexical item associated with an 
elementary tree is called the anchor in that tree; an 
elementary tree thus serves as a description of 
syntactic constraints of the anchor. The elementary 
syntactic structures of elementary trees are called 
supertags (Bangalore and Joshi, 1999), in order to 
distinguish them from the standard part-of-speech 
tags. Some examples are provided in figure 1 (b).
Elementary trees are divided into initial and 
auxiliary trees. Initial trees are those for which all 
non-terminal nodes on the frontier are substitutable. 
Auxiliary trees are defined as initial trees, except 
that exactly one frontier, non-terminal node must 
be a foot node, with the same label as the root node. 
Two operations - substitution and adjunction - are 
provided in LTAG to combine elementary trees 
into a derived tree. 
4 SDLM 
Our goal is to use SDLM to calculate the 
grammaticality of translated sentences. We do this 
by calculating the likelihood of the supertagged 
dependency structure that spans the entire sentence 
using SDLM. To obtain the supertagged 
dependency linkage, the most intuitive way is 
through a LTAG parser (Schabes et al, 1988). 
However, this could be very slow as it has time 
complexity of O(n6). Another possibility is to 
follow the procedure in (Joshi and Srinivas 1994, 
Bangalore and Joshi, 1999): use a HMM-based 
supertagger to assign words with supertags, 
followed by derivation of a shallow parse in linear 
time based on only the supertags to obtain the 
dependencies. But since this approach uses only 
the local context, in (Joshi and Srinivas 1994), they 
also proposed another greedy algorithm based on 
supertagged dependency probabilities to gradually 
select the path with the maximum path probability 
to extend to the remaining directions in the 
dependency list.  
In contrast to the LTAG parsing and 
supertagging-based approaches, we propose an 
alternative mechanism: first we use a state-of-the-
art constituent parser to obtain the parse of a 
sentence, and then we extract elementary trees with 
dependencies from the parse to assign each word 
with an elementary tree. The second step is similar 
to the approach used in extracting elementary trees 
from the TreeBank (Xia, 1999; Chen and Vijay-
Shanker, 2000).  
4.1 Elementary Tree Extraction 
We use an elementary tree extractor, a 
modification of (Chen and Vijay-Shanker, 2000), 
to serve our purpose. Heuristic rules were used to 
distinguish arguments from adjuncts, and the 
extraction process can be regarded as a process that 
gradually decomposes a constituent parse to 
multiple elementary trees and records substitutions 
and adjunctions. From elementary trees, we can 
obtain supertags by only considering syntactic 
structure and ignoring anchor words. Take the 
sentence ? ?The hungry boys ate dinner? as an 
example; the constituent parse and extracted 
supertags are shown in Figure 1. 
In Figure 1 (b), dotted lines represent the 
operations of substitution and adjunction. Note that 
each word in a translated sentence would be 
assigned exactly one elementary syntactic structure 
which is associated with a unique supertag id for 
the whole corpus. Different anchor words could 
own the same elementary syntactic structure and 
would be assigned the same supertag id, such as 
? 1?  ? for ?boys? and ?dinner?. For our corpus, 
around 1700 different elementary syntactic 
structures (1700 supertag ids) are extracted. 
 
 
 
 
Figure 1. (a) Parse of ?The hungry boys ate dinner?          
                                          
435
NP
the
NP*
boys
S
NP1? VP
ate
NP2?
NP
dinner
DT
anchor
word
NP
hungry
NP*JJ
anchor
word VB
anchor
word
NN
anchor
word
NP
NN
anchor
word
anchor
word:
elementary
syntactic
structure
(supertag):
supertag id: 1? 1?2?1? 2?
 
 
Figure 1. (b) Extracted elementary trees 
4.2 Model  
Bangalore and Joshi (1999) gave a concise 
description for dependencies between supertags: 
?A supertag is dependent on another supertag if the 
former substitutes or adjoins into the latter?. 
Following this description, for the example in 
Figure 1 (b), supertags of ?the? and ?hungry? are 
dependent on the supertag of ?boys?, and supertags 
of ?boys? and ?dinner? are dependent on the 
supertag of ?ate?. These dependencies between 
supertags also provide the dependencies between 
anchor words.  
Since the syntactic constraints for each word in 
its context are decided and described through its 
supertag, the likelihood of SDLM for a sentence 
could also be regarded as the degree of violations 
of the syntactic constraints on all words in the 
sentence. Consider a sentence S = w1 w2 ?wn with 
corresponding supertags T = t1 t2 ?tn. We use di=j 
to represent the dependency relations for words or 
supertags. For example, d3 = 5 means that w3 
depends on w5 or t3 depends on t5. We propose five 
different bigram SDLM as follows and evaluate 
their effects in section 5. 
 
 
 
 
 
 
 
 
               
 
 
SDLM model (2) is the approximation form of 
model (1); model (3) and (4) are individual terms 
of model (2); model (5) models word dependencies 
based on elementary tree dependencies. The 
estimation of the probabilities is done using 
maximum likelihood estimations with Laplace 
smoothing.  Take Figure 1 (b) as an example; if 
using model (1), the SDLM likelihood of ?The 
hungry boys ate dinner? is 
 
)|2,(*)2,|1,(
*)2,|1,(*)1,|2,(*)1,|1,(
rootatePatedinnerP
ateboysPboyshungryPboystheP
???
??????
 
In our experiment on sentence-level translation 
combination, we use a log-linear model to integrate 
all features including SDLM models. The 
corresponding weights are trained discriminatively 
for Bleu score using Minimum Error Rate Training 
(MERT). 
5 Experiment  
Our experiments are conducted and reported on the 
Chinese-English dataset from NIST 2008 
(LDC2010T01). It consists of four human 
reference translations and corresponding machine 
translations for the NIST Open MT08 test set, 
which consists of newswire and web data. The test 
set contains 105 documents with 1312 sentences 
and output from 23 machine translation systems. 
Each system provides the top one translation 
hypothesis for every sentence. We further divide 
the NIST Open MT08 test set into the tuning set 
and test set for our experiment of sentence-level 
translation combination. We divided the 1312 
sentences into tuning data of 524 sentences and the 
test set of 788 sentences. Out of 23 MT systems, 
we manually select the top five MT systems as our 
MT systems for our combination experiment. 
In terms of SDLM training, since the size of 
TreeBank-extracted elementary trees is much 
smaller compared to most practical n-gram LMs 
trained from the Gigaword corpus, we also extract 
elementary trees from automatically-generated 
parses of part of the Gigaword corpus (around one-
year newswire of ?afp_eng? in Gigaword 4) in 
addition to TreeBank-extracted elementary trees. 
5.1 Feature Functions 
For the baseline combination system, we use the 
following feature functions in the log-linear model 
to calculate the score of a system translation. 
 
z Sentence consensus based on Translation Edit 
Ratio (TER) 
z Gigaword-trained 3-gram LM and word 
penalty 
 
|? i model(5) SDLM                                                    )(
model(4) SDLM                                                       )|(
model(3) SDLM                                                       )|(
model(2) SDLM       )|()|()|(
model(1) SDLM                                             )|(
?
?
??
?
?
i
d
i
ii
i
di
ii
i
di
i
ddii
i
ddii
i
i
iii
ii
wwP
twP
ttP
twPttPtwtwP
twtwP
436
For testing SDLM, in additional to all features 
that the baseline combination system uses, we add 
single or multiple SDLM models in the log-linear 
model, and each SDLM model has its own weight. 
5.2 Result 
From table 1, we can see that the combination of 
SDLM model 3, 4 and 5 yields the best 
performance, which is better than the best MT 
system by Bleu of 1.45, TER of 0.67 and 
METEOR of 1.25, and also better than the baseline 
combination system by Bleu of 0.72, TER of 0.25 
and METEOR of 0.44. Compared with SDLM 
model 5, which represents a type of word 
dependency LM without labels, the results show 
that adding appropriate syntactic ?labels? (here, 
they are ?supertags?) on word dependencies brings 
benefits. 
 
 
Table 1. Result of Sentence-level Translation Combination 
6 Conclusion  
In this paper we presented Supertagged 
Dependency Language Model for explicitly 
modeling syntactic dependencies of the words of 
translated sentences. Our goal is to select the most 
grammatical translation from candidate translations.  
To obtain the supertagged dependency structure of 
a translation candidate, a two-step mechanism 
based on constituent parsing and elementary tree 
extraction is also proposed. SDLM shows its 
effectiveness in the scenario of translation 
selection.  
There are several avenues for future work: we 
have focused on bigram dependencies in our 
models; extension to more than two dependent 
elementary trees is straightforward. It would also 
be worth investigating the performance of using 
our sentence-level model to re-rank n-best outputs 
of a confusion network model. And in terms of 
applications, SDLM can be directly applied to 
many other NLP tasks, such as speech recognition 
and natural language generation. 
Acknowledgments  
We would like to thank Owen Rambow for 
providing the elementary tree extractor and also 
thank the anonymous reviewers for their helpful 
comments. This work is supported by the National 
Science Foundation via Grant No. 0910778 
entitled ?Richer Representations for Machine 
Translation?. All views expressed in this paper are 
those of the authors and do not necessarily 
represent the view of the National Science 
Foundation. 
References  
Srinivas Bangalore and Aravind K. Joshi. 1999. 
Supertagging: An approach to almost parsing. 
Computational Linguistics, 25(2):237?265.  
John Chen and K. Vijay-Shanker. 2000. Automated 
extraction of TAGs from the Penn treebank. In 
Proceedings of the Sixth International Workshop on 
Parsing Technologies 
Chris Callison-Burch, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut and Lucia Specia. 2012. 
Findings of the 2012 Workshop on Statistical 
Machine Translation. In Proceedings of WMT12. 
Steve DeNeefe and Kevin Knight. 2009 Synchronous 
Tree Adjoining Machine Translation. In Proceedings 
of EMNLP 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang, Ignacio Thayer. 
2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proceedings of 
the Annual Meeting of the Association for 
Computational Linguistics 
Christian Hardmeier, Joakim Nivre and J?rg Tiedemann. 
2012. Tree Kernels for Machine Translation Quality 
Estimation. In Proceedings of WMT12 
S. Hasan, O. Bender, and H. Ney. 2006. Reranking 
translation hypotheses using structural properties. In 
Proceedings of the EACL'06 Workshop on Learning 
Structured Information in Natural Language 
Applications  
Hany Hassan , Khalil Sima'an and Andy Way. 2007. 
Supertagged Phrase-Based Statistical Machine 
Translation. In Proceedings of the Annual Meeting of 
the Association for Computational Linguistics 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, 
and Robert Moore. 2008. Indirect-hmm-based 
hypothesis alignment for computing outputs from 
machine translation systems. In Proceedings of 
EMNLP 
 Bleu TER METEOR 
Best MT system 30.16 55.45 54.43 
baseline 30.89 55.03 55.24 
baseline+ model 1 31.29 54.99 55.63 
baseline+ model 2 31.25 55.23 55.37 
baseline+ model 3 31.25 55.06 55.40 
baseline+ model 4 31.44 54.70 55.54 
baseline+ model 5 31.39 55.15 55.68 
baseline+ model 3+ 
model 4+ model 5 31.61 54.78 55.68 
437
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
Proceedings of the Eighth Conference of the 
Association for Machine Translation in the Americas 
Aravind K. Joshi and B. Srinivas. 1994. Disambiguation 
of super parts of speech (or supertags): Almost 
parsing. In Proceedings of the 15th International 
Conference on Computational Linguistics 
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 
1975. Tree Adjunct Grammars. Journal of Computer 
and System Science, 10:136?163. 
Evgeny Matusov, Nicola Ueffing, and Hermann Ney 
2006. Computing consensus translation from multiple 
machine translation systems using enhanced 
hypotheses alignment. In Proceedings of EACL 
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, and Dragomir Radev. 2004 A 
smorgasbord of features for statistical machine 
translation. In Proceedings of the Meeting of the 
North American chapter of the Association for 
Computational Linguistics 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine translation 
system combination using ITG-based alignments. In 
Proceedings of ACL-HLT 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency Treelet Translation: Syntactically 
Informed Phrasal SMT, In Proceedings of the 
Association for Computational Linguistics 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007. Improved word-level system 
combination for machine translation. In Proceedings 
of ACL 
Yves Schabes, Anne Abeille and Aravind K. Joshi. 
1988. Parsing strategies with 'lexicalized' grammars: 
Application to Tree Adjoining Grammars. In 
Proceedings of the 12th International Conference on 
Computational Linguistics 
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and P.C. 
Woodland .2007. Consensus Network Decoding for 
Statistical Machine Translation System Combination. 
In Proceedings of ICASSP 
Fei Xia. 1999. Extracting Tree Adjoining Grammars 
from Bracketed Corpora. In Proceedings of the 5th 
Natural Language Processing Pacific Rim 
Symposium (NLPRS-1999) 
Daguang Xu, Yuan Cao, Damianos Karakos. 2011. 
Description of the JHU System Combination Scheme 
for WMT 2011. In Proceedings of the Sixth 
Workshop on Statistical Machine Translation 
 
438
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138?147,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Extracting Social Networks from Literary Fiction
David K. Elson
Dept. of Computer Science
Columbia University
delson@cs.columbia.edu
Nicholas Dames
English Department
Columbia University
nd122@columbia.edu
Kathleen R. McKeown
Dept. of Computer Science
Columbia University
kathy@cs.columbia.edu
Abstract
We present a method for extracting so-
cial networks from literature, namely,
nineteenth-century British novels and se-
rials. We derive the networks from di-
alogue interactions, and thus our method
depends on the ability to determine when
two characters are in conversation. Our
approach involves character name chunk-
ing, quoted speech attribution and conver-
sation detection given the set of quotes.
We extract features from the social net-
works and examine their correlation with
one another, as well as with metadata such
as the novel?s setting. Our results provide
evidence that the majority of novels in this
time period do not fit two characterizations
provided by literacy scholars. Instead, our
results suggest an alternative explanation
for differences in social networks.
1 Introduction
Literary studies about the nineteenth-century
British novel are often concerned with the nature
of the community that surrounds the protagonist.
Some theorists have suggested a relationship be-
tween the size of a community and the amount of
dialogue that occurs, positing that ?face to face
time? diminishes as the number of characters in
the novel grows. Others suggest that as the social
setting becomes more urbanized, the quality of di-
alogue also changes, with more interactions occur-
ring in rural communities than urban communities.
Such claims have typically been made, however,
on the basis of a few novels that are studied in
depth. In this paper, we aim to determine whether
an automated study of a much larger sample of
nineteenth century novels supports these claims.
The research presented here is concerned with
the extraction of social networks from literature.
We present a method to automatically construct
a network based on dialogue interactions between
characters in a novel. Our approach includes com-
ponents for finding instances of quoted speech,
attributing each quote to a character, and iden-
tifying when certain characters are in conversa-
tion. We then construct a network where char-
acters are vertices and edges signify an amount
of bilateral conversation between those charac-
ters, with edge weights corresponding to the fre-
quency and length of their exchanges. In contrast
to previous approaches to social network construc-
tion, ours relies on a novel combination of pattern-
based detection, statistical methods, and adapta-
tion of standard natural language tools for the liter-
ary genre. We carried out this work on a corpus of
60 nineteenth-century novels and serials, includ-
ing 31 authors such as Dickens, Austen and Conan
Doyle.
In order to evaluate the literary claims in ques-
tion, we compute various characteristics of the
dialogue-based social network and stratify these
results by categories such as the novel?s setting.
For example, the density of the network provides
evidence about the cohesion of a large or small
community, and cliques may indicate a social frag-
mentation. Our results surprisingly provide evi-
dence that the majority of novels in this time pe-
riod do not fit the suggestions provided by liter-
ary scholars, and we suggest an alternative expla-
nation for our observations of differences across
novels.
In the following sections, we survey related
work on social networks as well as computational
studies of literature. We then present the literary
hypotheses in more detail. We describe the meth-
ods we use to extract dialogue and construct con-
versational networks, along with our approach to
analyzing their characteristics. After we present
the statistical results, we analyze their significance
from a literary perspective.
138
2 Related Work
Computer-assisted literary analysis has typically
occurred at the word level. This level of granular-
ity lends itself to studies of authorial style based
on patterns of word use (Burrows, 2004), and re-
searchers have successfully ?outed? the writers of
anonymous texts by comparing their style to that
of a corpus of known authors (Mostellar and Wal-
lace, 1984). Determining instances of ?text reuse,?
a type of paraphrasing, is also a form of analysis
at the lexical level, and it has recently been used to
validate theories about the lineage of ancient texts
(Lee, 2007).
Analysis of literature using more semantically-
oriented techniques has been rare, most likely be-
cause of the difficulty in automatically determin-
ing meaningful interpretations. Some exceptions
include recent work on learning common event se-
quences in news stories (Chambers and Jurafsky,
2008), an approach based on statistical methods,
and the development of an event calculus for char-
acterizing stories written by children (Halpin et al,
2004), a knowledge-based strategy. On the other
hand, literary theorists, linguists and others have
long developed symbolic but non-computational
models for novels. For example, Moretti (2005)
has graphically mapped out texts according to ge-
ography, social connections and other variables.
While researchers have not attempted the auto-
matic construction of social networks represent-
ing connections between characters in a corpus
of novels, the ACE program has involved entity
and relation extraction in unstructured text (Dod-
dington et al, 2004). Other recent work in so-
cial network construction has explored the use of
structured data such as email headers (McCallum
et al, 2007) and U.S. Senate bill cosponsorship
(Cho and Fowler, 2010). In an analysis of discus-
sion forums, Gruzd and Haythornthwaite (2008)
explored the use of message text as well as posting
data to infer who is talking to whom. In this pa-
per, we also explore how to build a network based
on conversational interaction, but we analyze the
reported dialogue found in novels to determine the
links. The kinds of language that is used to signal
such information is quite different in the two me-
dia. In discussion forums, people tend to use ad-
dresses such as ?Hi Tom,? while in novels, a sys-
tem must determine both the speaker of a quota-
tion and then the intended recipient of the dialogue
act. This is a significantly different problem.
3 Hypotheses
It is commonly held that the novel is a literary
form which tries to produce an accurate represen-
tation of the social world. Within literary stud-
ies, the recurring problem is how that represen-
tation is achieved. Theories about the relation
between novelistic form (the workings of plot,
characters, and dialogue, to take the most basic
categories) and changes to real-world social mi-
lieux abound. Many of these theories center on
nineteenth-century European fiction; innovations
in novelistic form during this period, as well as the
rapid social changes brought about by revolution,
industrialization, and transport development, have
traditionally been linked. These theories, however,
have used only a select few representative novels
as proof. By using statistical methods of analy-
sis, it is possible to move beyond this small corpus
of proof texts. We believe these methods are es-
sential to testing the validity of some core theories
about social interaction and their representation in
literary genres like the novel.
Major versions of the theories about the social
worlds of nineteenth-century fiction tend to cen-
ter on characters, in two specific ways: how many
characters novels tend to have, and how those
characters interact with one another. These two
?formal? facts about novels are usually explained
with reference to a novel?s setting. From the influ-
ential work of the Russian critic Mikhail Bakhtin
to the present, a consensus emerged that as nov-
els are increasingly set in urban areas, the num-
ber of characters and the quality of their interac-
tion change to suit the setting. Bakhtin?s term for
this causal relationship was chronotope: the ?in-
trinsic interconnectedness of temporal and spatial
relationships that are artistically expressed in liter-
ature,? in which ?space becomes charged and re-
sponsive to movements of time, plot, and history?
(Bakhtin, 1981, 84). In Bakhtin?s analysis, dif-
ferent spaces have different social and emotional
potentialities, which in turn affect the most basic
aspects of a novel?s aesthetic technique.
After Bakhtin?s invention of the chronotope,
much literary criticism and theory devoted itself
to filling in, or describing, the qualities of spe-
cific chronotopes, particularly those of the village
or rural environment and the city or urban en-
vironment. Following a suggestion of Bakhtin?s
that the population of village or rural fictions is
modeled on the world of the family, made up of
139
Author/Title/Year Persp. Setting Author/Title/Year Persp. Setting
Ainsworth, Jack Sheppard (1839) 3rd urban Gaskell, North and South (1854) 3rd urban
Austen, Emma (1815) 3rd rural Gissing, In the Year of Jubilee (1894) 3rd urban
Austen, Mansfield Park (1814) 3rd rural Gissing, New Grub Street (1891) 3rd urban
Austen, Persuasion (1817) 3rd rural Hardy, Jude the Obscure (1894) 3rd mixed
Austen, Pride and Prejudice (1813) 3rd rural Hardy, The Return of the Native (1878) 3rd rural
Braddon, Lady Audley?s Secret (1862) 3rd mixed Hardy, Tess of the d?Ubervilles (1891) 3rd rural
Braddon, Aurora Floyd (1863) 3rd rural Hughes, Tom Brown?s School Days (1857) 3rd rural
Bronte?, Anne, The Tenant of Wildfell Hall
(1848)
1st rural James, The Portrait of a Lady (1881) 3rd urban
Bronte?, Charlotte, Jane Eyre (1847) 1st rural James, The Ambassadors (1903) 3rd urban
Bronte?, Charlotte, Villette (1853) 1st mixed James, The Wings of the Dove (1902) 3rd urban
Bronte?, Emily, Wuthering Heights (1847) 1st rural Kingsley, Alton Locke (1860) 1st mixed
Bulwer-Lytton, Paul Clifford (1830) 3rd urban Martineau, Deerbrook (1839) 3rd rural
Collins, The Moonstone (1868) 1st urban Meredith, The Egoist (1879) 3rd rural
Collins, The Woman in White (1859) 1st urban Meredith, The Ordeal of Richard Feverel
(1859)
3rd rural
Conan Doyle, The Sign of the Four (1890) 1st urban Mitford, Our Village (1824) 1st rural
Conan Doyle, A Study in Scarlet (1887) 1st urban Reade, Hard Cash (1863) 3rd urban
Dickens, Bleak House (1852) mixed urban Scott, The Bride of Lammermoor (1819) 3rd rural
Dickens, David Copperfield (1849) 1st mixed Scott, The Heart of Mid-Lothian (1818) 3rd rural
Dickens, Little Dorrit (1855) 3rd urban Scott, Waverley (1814) 3rd rural
Dickens, Oliver Twist (1837) 3rd urban Stevenson, The Strange Case of Dr. Jekyll
and Mr. Hyde (1886)
1st urban
Dickens, The Pickwick Papers (1836) 3rd mixed Stoker, Dracula (1897) 1st urban
Disraeli, Sybil, or the Two Nations (1845) 3rd mixed Thackeray, History of Henry Esmond
(1852)
1st urban
Edgeworth, Belinda (1801) 3rd rural Thackeray, History of Pendennis (1848) 1st urban
Edgeworth, Castle Rackrent (1800) 3rd rural Thackeray, Vanity Fair (1847) 3rd urban
Eliot, Adam Bede (1859) 3rd rural Trollope, Barchester Towers (1857) 3rd rural
Eliot, Daniel Deronda (1876) 3rd urban Trollope, Doctor Thorne (1858) 3rd rural
Eliot, Middlemarch (1871) 3rd rural Trollope, Phineas Finn (1867) 3rd urban
Eliot, The Mill on the Floss (1860) 3rd rural Trollope, The Way We Live Now (1874) 3rd urban
Galt, Annals of the Parish (1821) 1st rural Wilde, The Picture of Dorian Gray (1890) 3rd urban
Gaskell, Mary Barton (1848) 3rd urban Wood, East Lynne (1860) 3rd mixed
Table 1: Properties of the nineteenth-century British novels and serials included in our study.
an intimately related set of characters, many crit-
ics analyzed the formal expression of this world
as constituted by a small set of characters who
express themselves conversationally. Raymond
Williams used the term ?knowable communities?
to describe this world, in which face-to-face rela-
tions of a restricted set of characters are the pri-
mary mode of social interaction (Williams, 1975,
166).
By contrast, the urban world, in this traditional
account, is both larger and more complex. To
describe the social-psychological impact of the
city, Franco Moretti argues, protagonists of urban
novels ?change overnight from ?sons? into ?young
men?: their affective ties are no longer vertical
ones (between successive generations), but hor-
izontal, within the same generation. They are
drawn towards those unknown yet congenial faces
seen in gardens, or at the theater; future friends,
or rivals, or both? (Moretti, 1999, 65). The re-
sult is two-fold: more characters, indeed a mass
of characters, and more interactions, although less
actual conversation; as literary critic Terry Eagle-
ton argues, the city is where ?most of our en-
counters consist of seeing rather than speaking,
glimpsing each other as objects rather than con-
versing as fellow subjects? (Eagleton, 2005, 145).
Moretti argues in similar terms. For him, the
difference in number of characters is ?not just a
matter of quantity... it?s a qualitative, morpho-
logical one? (Moretti, 1999, 68). As the number
of characters increases, Moretti argues (following
Bakhtin in his logic), social interactions of differ-
ent kinds and durations multiply, displacing the
family-centered and conversational logic of vil-
lage or rural fictions. ?The narrative system be-
comes complicated, unstable: the city turns into a
gigantic roulette table, where helpers and antago-
nists mix in unpredictable combinations? (Moretti,
1999, 68). This argument about how novelistic
setting produces different forms of social interac-
tion is precisely what our method seeks to evalu-
ate.
Our corpus of 60 novels was selected for its rep-
resentativeness, particularly in the following cate-
gories: authorial (novels from the major canoni-
140
cal authors of the period), historical (novels from
each decade), generic (from the major sub-genres
of nineteenth-century fiction), sociological (set in
rural, urban, and mixed locales), and technical
(narrated in first-person and third-person form).
The novels, as well as important metadata we as-
signed to them (the perspective and setting), are
shown in Table 1. We define urban to mean set
in a metropolitan zone, characterized by multi-
ple forms of labor (not just agricultural). Here,
social relations are largely financial or commer-
cial in character. We conversely define rural to
describe texts that are set in a country or vil-
lage zone, where agriculture is the primary activ-
ity, and where land-owning, non-productive, rent-
collecting gentry are socially predominant. Social
relations here are still modeled on feudalism (rela-
tions of peasant-lord loyalty and family tie) rather
than the commercial cash nexus. We also explored
other properties of the texts, such as literary genre,
but focus on the results found with setting and per-
spective. We obtained electronic encodings of the
texts from Project Gutenberg. All told, these texts
total more than 10 million words.
We assembled this representative corpus in or-
der to test two hypotheses, which are derived from
the aforementioned theories:
1. That there is an inverse correlation between
the amount of dialogue in a novel and the
number of characters in that novel. One ba-
sic, shared assumption of these theorists is
that as the network of characters expands?
as, in Moretti?s words, a quantitative change
becomes qualitative? the importance, and in
fact amount, of dialogue decreases. With
a method for extracting conversation from a
large corpus of texts, it is possible to test this
hypothesis against a wide range of data.
2. That a significant difference in the
nineteenth-century novel?s representation of
social interaction is geographical: novels set
in urban environments depict a complex but
loose social network, in which numerous
characters share little conversational interac-
tion, while novels set in rural environments
inhabit more tightly bound social networks,
with fewer characters sharing much more
conversational interaction. This hypothesis
is based on the contrast between Williams?s
rural ?knowable communities? and the
sprawling, populous, less conversational
urban fictions or Moretti?s and Eagleton?s
analyses. If true, it would suggest that the
inverse relationship of hypothesis #1 (more
characters means less conversation) can be
correlated to, and perhaps even caused by,
the geography of a novel?s setting. The
claims about novelistic geography and social
interaction have usually been based on
comparisons of a selected few novelists (Jane
Austen and Charles Dickens preeminently).
Do they remain valid when tested against a
larger corpus?
4 Extracting Conversational Networks
from Literature
In order to test these hypotheses, we developed
a novel approach to extracting social networks
from literary texts themselves, building on exist-
ing analysis tools. We defined ?social network?
as ?conversational network? for purposes of eval-
uating these literary theories. In a conversational
network, vertices represent characters (assumed to
be named entities) and edges indicate at least one
instance of dialogue interaction between two char-
acters over the course of the novel. The weight of
each edge is proportional to the amount of inter-
action. We define a conversation as a continuous
span of narrative time featuring a set of characters
in which the following conditions are met:
1. The characters are in the same place at the
same time;
2. The characters take turns speaking; and
3. The characters are mutually aware of each
other and each character?s speech is mutually
intended for the other to hear.
In the following subsections, we discuss the
methods we devised for the three problems in text
processing invoked by this approach: identifying
the characters present in a literary text, assigning
a ?speaker? (if any) to each instance of quoted
speech from among those characters, and con-
structing a social network by detecting conversa-
tions from the set of dialogue acts.
4.1 Character Identification
The first challenge was to identify the candi-
date speakers by ?chunking? names (such as Mr.
Holmes) from the text. We processed each novel
141
with the Stanford NER tagger (Finkel et al, 2005)
and extracted noun phrases that were categorized
as persons or organizations. We then clustered the
noun phrases into coreferents for the same entity
(person or organization). The clustering process is
as follows:
1. For each named entity, we generate varia-
tions on the name that we would expect to
see in a coreferent. Each variation omits cer-
tain parts of multi-word names, respecting ti-
tles and first/last name distinctions, similar to
work by Davis et al (2003). For example,
Mr. Sherlock Holmes may refer to the same
character as Mr. Holmes, Sherlock Holmes,
Sherlock and Holmes.
2. For each named entity, we compile a list of
other named entities that may be coreferents,
either because they are identical or because
one is an expected variation on the other.
3. We then match each named entity to the most
recent of its possible coreferents. In aggre-
gate, this creates a cluster of mentions for
each character.
We also pre-processed the texts to normalize
formatting, detect headings and chapter breaks, re-
move metadata, and identify likely instances of
quoted speech (that is, mark up spans of text that
fall between quotation marks, assumed to be a su-
perset of the quoted speech present in the text).
4.2 Quoted Speech Attribution
In order to programmatically assign a speaker to
each instance of quoted speech, we applied a high-
precision subset of a general approach we describe
elsewhere (Elson and McKeown, 2010). The first
step of this approach was to compile a separate
training and testing corpus of literary texts from
British, American and Russian authors of the nine-
teenth and twentieth centuries. The training cor-
pus consisted of about 111,000 words including
3,176 instances of quoted speech. To obtain gold-
standard annotations, we conducted an online sur-
vey via Amazon?s Mechanical Turk program. For
each quote, we asked three annotators to indepen-
dently choose a speaker from the list of contex-
tual candidates? or, choose ?spoken by an unlisted
character? if the answer was not available, or ?not
spoken by any character? for non-dialogue cases
such as sneer quotes.
We divided this corpus into training and testing
sets, and used the training set to develop a catego-
rizer that assigned one of five syntactic categories
to each quote. For example, if a quote is followed
by a verb that indicates verbal expression (such as
?said?), and then a character mention, a category
called Character trigram is assigned to the quote.
The fifth category is a catch-all for quotes that do
not fall into the other four. In many cases, the an-
swer can be reliably determined based solely on
its syntactic category. For instance, in the Char-
acter trigram category, the mentioned character is
the quote?s speaker in 99% of both the training and
testing sets.
In all, we were able to determine the speaker
of 57% of the testing set with 96% accuracy just
on the basis of syntactic categorization. This is
the technique we used to construct our conversa-
tional networks. In another study, we applied ma-
chine learning tools to the data (one model for
each syntactic category) and achieved an overall
accuracy of 83% over the entire test set (Elson
and McKeown, 2010). The other 43% of quotes
are left here as ?unknown? speakers; however, in
the present study, we are interested in conversa-
tions rather than individual quotes. Each conversa-
tion is likely to consist of multiple quotes by each
speaker, increasing the chances of detecting the in-
teraction. Moreover, this design decision empha-
sizes the precision of the social networks over their
recall. This tilts ?in favor? of hypothesis #1 (that
there are fewer social interactions in larger com-
munities); however, we shall see that despite the
emphasis of precision over recall, we identify a
sufficient mass of interactions in the texts to con-
stitute evidence against this hypothesis.
4.3 Constructing social networks
We then applied the results from our character
identification and quoted speech attribution meth-
ods toward the construction of conversational net-
works from literature. We derived one network
from each text in our corpus.
We first assigned vertices to character enti-
ties that are mentioned repeatedly throughout the
novel. Coreferents for the same name (such as
Mr. Darcy and Darcy) were grouped into the same
vertex. We found that a network that included in-
cidental or single-mention named entities became
too noisy to function effectively, so we filtered out
the entities that are mentioned fewer than three
142
times in the novel or are responsible for less than
1% of the named entity mentions in the novel.
We assigned undirected edges between vertices
that represent adjacency in quoted speech frag-
ments. Specifically, we set the weight of each
undirected edge between two character vertices to
the total length, in words, of all quotes that either
character speaks from among all pairs of adjacent
quotes in which they both speak? implying face to
face conversation. We empirically determined that
the most accurate definition of ?adjacency? is one
where the two characters? quotes fall within 300
words of one another with no attributed quotes in
between. When such an adjacency is found, the
length of the quote is added to the edge weight,
under the hypothesis that the significance of the re-
lationship between two individuals is proportional
to the length of the dialogue that they exchange.
Finally, we normalized each edge?s weight by the
length of the novel.
An example network, automatically constructed
in this manner from Jane Austen?s Mansfield Park,
is shown in Figure 1. The width of each vertex is
drawn to be proportional to the character?s share
of all the named entity mentions in the book (so
that protagonists, who are mentioned frequently,
appear in larger ovals). The width of each edge is
drawn to be proportional to its weight (total con-
versation length).
We also experimented with two alternate meth-
ods for identifying edges, for purposes of a base-
line:
1. The ?correlation? method divides the text
into 10-paragraph segments and counts the
number of mentions of each character in
each segment (excluding mentions inside
quoted speech). It then computes the Pear-
son product-moment correlation coefficient
for the distributions of mentions for each pair
of characters. These coefficients are used for
the edge weights. Characters that tend to ap-
pear together in the same areas of the novel
are taken to be more socially connected, and
have a higher edge weight.
2. The ?spoken mention? method counts occur-
rences when one character refers to another
in his or her quoted speech. These counts,
normalized by the length of the text, are used
as edge weights. The intuition is that charac-
ters who refer to one another are likely to be
in conversation.
??????????????????????????????????????????????????
?????????
??????????
?????
??????
???????????????????
?????????????
???
?????????
??????????????????
?????
??????????
???????????
???????
Figure 1: Automatically extracted conversation
network for Jane Austen?s Mansfield Park.
4.4 Evaluation
To check the accuracy of our method for extracting
conversational networks, we conducted an evalua-
tion involving four of the novels (The Sign of the
Four, Emma, David Copperfield and The Portrait
of a Lady). We did not use these texts when devel-
oping our method for identifying conversations.
For each book, we randomly selected 4-5 chap-
ters from among those with significant amounts
of quoted speech, so that all excerpts from each
novel amounted to at least 10,000 words. We then
asked three annotators to identity all the conversa-
tions that occur in all 44,000 words. We requested
that the annotators include both direct and indi-
rect (unquoted) speech, and define ?conversation?
as in the beginning of Section 4, but exclude ?re-
told? conversations (those that occur within other
dialogue).
We processed the annotation results by breaking
down each multi-way conversation into all of its
unique two-character interactions (for example, a
conversation between four people indicates six bi-
lateral interactions). To calculate inter-annotator
agreement, we first compiled a list of all possi-
ble interactions between all characters in each text.
In this model, each annotator contributed a set of
?yes? or ?no? decisions, one for every character
pair. We then applied the kappa measurement for
agreement in a binary classification problem (Co-
143
Method Precision Recall F
Speech adjacency .95 .51 .67
Correlation .21 .65 .31
Spoken-mention .45 .49 .47
Table 2: Precision, recall, and F-measure of three
methods for detecting bilateral conversations in
literary texts.
hen, 1960). In 95% of character pairs, annota-
tors were unanimous, which is a high agreement
of k = .82.
The precision and recall of our method for de-
tecting conversations is shown in Table 2. Preci-
sion was .95; this indicates that we can be con-
fident in the specificity of the conversational net-
works that we automatically construct. Recall was
.51, indicating a sensitivity of slightly more than
half. There were several reasons that we did not
detect the missing links, including indirect speech,
quotes attributed to anaphoras or coreferents, and
?diffuse? conversations in which the characters do
not speak in turn with one another.
To calculate precision and recall for the two
baseline social networks, we set a threshold t to
derive a binary prediction from the continuous
edge weights. The precision and recall values
shown for the baselines in Table 2 represent the
highest performance we achieved by varying t be-
tween 0 and 1 (maximizing F-measure over t).
Both baselines performed significantly worse in
precision and F-measure than our quoted speech
adjacency method for detecting conversations.
5 Data Analysis
5.1 Feature extraction
We extracted features from the conversational net-
works that emphasize the complexity of the social
interactions found in each novel:
1. The number of characters and the number of
speaking characters
2. The variance of the distribution of quoted
speech (specifically, the proportion of quotes
spoken by the n most frequent speakers, for
1 ? n ? 5)
3. The number of quotes, and proportion of
words in the novel that are quoted speech
4. The number of 3-cliques and 4-cliques in the
social network
5. The average degree of the graph, defined as
?
v?V |Ev|
|V |
=
2|E|
|V |
(1)
where |Ev| is the number of edges incident
on a vertex v, and |V | is the number of ver-
tices. In other words, this determines the
average number of characters connected to
each character in the conversational network
(?with how many people on average does a
character converse??).
6. A variation on graph density that normalizes
the average degree feature by the number of
characters:
?
v?V |Ev|
|V |(|V | ? 1)
=
2|E|
|V |(|V | ? 1)
(2)
By dividing again by |V | ? 1, we use this
as a metric for the overall connectedness of
the graph: ?with what percent of the entire
network (besides herself) does each charac-
ter converse, on average?? The weight of the
edge, as long as it is greater than 0, does not
affect either the network?s average degree or
graph density.
5.2 Results
We derived results from the data in two ways.
First, we examined the strengths of the correla-
tions between the features that we extracted (for
example, between number of character vertices
and the average degree of each vertex). We used
Pearson?s product-moment correlation coefficient
in these calculations. Second, we compared the
extracted features to the metadata we previously
assigned to each text (e.g., urban vs. rural).
Hypothesis #1, which we described in Section
3, claims that there is an inverse correlation be-
tween the amount of dialogue in a nineteenth-
century novel and the number of characters in that
novel. We did not find this to be the case. Rather,
we found a weak but positive correlation (r=.16)
between the number of quotes in a novel and
the number of characters (normalizing the quote
count for text length). There was a stronger pos-
itive correlation (r=.50) between the number of
unique speakers (those characters who speak at
least once) and the normalized number of quotes,
suggesting that larger networks have more conver-
sations than smaller ones. But because the first
144
correlation is weak, we investigated whether fur-
ther analysis could identify other evidence that
confirms or contradicts the hypothesis.
Another way to interpret hypothesis #1 is that
social networks with more characters tend to break
apart and be less connected. However, we found
the opposite to be true. The correlation between
the number of characters in each graph and the av-
erage degree (number of conversation partners) for
each character was a positive, moderately strong
r=.42. This is not a given; a network can easily, for
example, break into minimally connected or mutu-
ally exclusive subnetworks when more characters
are involved. Instead, we found that networks tend
to stay close-knit regardless of their size: even the
density of the graph (the percentage of the com-
munity that each character talks to) grows with
the total population size at r=.30. Moreover, as
the population of speakers grows, the density is
likely to increase at r=.49. A higher number of
characters (speaking or non-speaking) is also cor-
related with a higher rate of 3-cliques per charac-
ter (r=.38), as well as with a more balanced dis-
tribution of dialogue (the share of dialogue spo-
ken by the top three speakers decreases at r=?.61).
This evidence suggests that in nineteenth-century
British literature, it is the small communities,
rather than the large ones, that tend to be discon-
nected.
Hypothesis #2, meanwhile, posited that a
novel?s setting (urban or rural) would have an ef-
fect on the structure of its social network. After
defining ?social network? as a conversational net-
work, we did not find this to be the case. Sur-
prisingly, the numbers of characters and speakers
found in the urban novel were not significantly
greater than those found in the rural novel. More-
over, each of the features we extracted, such as
the rate of cliques, average degree, density, and
rate of characters? mentions of other characters,
did not change in a statistically significant man-
ner between the two genres. For example, Figure
2 shows the mean over all texts of each network?s
average degree, with confidence intervals, sepa-
rated by setting into urban and rural. The increase
in degree seen in urban texts is not significant.
Rather, the only type of metadata variable that
did impact the average degree with any signifi-
cance was the text?s perspective. Figure 2 also sep-
arates texts into first- and third-person tellings and
shows the means and confidence intervals for the
 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
 2 2.2
3rd1sturbanruralAverage De
gree
Setting  /  Perspective
Figure 2: The average degree for each character
as a function of the novel?s setting and its perspec-
tive.
???????????????????????????????????????????????????
?
??????????? ????????????? ?????????????? ????????????
?????
?????????????
????????????
???? ?????????????
Figure 3: Conversational networks for first-person
novels like Collins?s The Woman in White are less
connected due to the structure imposed by the per-
spective.
average degree measure. Stories told in the third
person had much more connected networks than
stories told in the first person: not only did the av-
erage degree increase with statistical significance
(by the homoscedastic t-test to p < .005), so too
did the graph density (p < .05) and the rate of
3-cliques per character (p < .05).
We believe the reason for this can be intuited
with a visual inspection of a first-person graph.
Figure 3 shows the conversational network ex-
tracted for Collins?s The Woman in White, which is
told in the first person. Not surprisingly, the most
oft-repeated named entity in the text is I, referring
to the narrator. More surprising is the lack of con-
versation connections between the auxiliary char-
acters. The story?s structure revolves around the
narrator and each character is understood in terms
of his or her relationship to the narrator. Private
conversations between auxiliary characters would
not include the narrator, and thus do not appear in a
145
first-hand account. An ?omniscient? third person
narrator, by contrast, can eavesdrop on any pair
of characters conversing. This highlights the im-
portance of detecting reported and indirect speech
in future work, as a first-person narrator may hear
about other connections without witnessing them.
6 Literary Interpretation of Results
Our data, therefore, markedly do not confirm hy-
pothesis #1. They also suggest, in relation to hy-
pothesis #2 (also not confirmed by the data), a
strong reason why.
One of the basic assumptions behind hypoth-
esis #2? that urban novels contain more charac-
ters, mirroring the masses of nineteenth-century
cities? is not borne out by our data. Our results do,
however, strongly correlate a point of view (third-
person narration) with more frequently connected
characters, implying tighter and more talkative so-
cial networks.
We would propose that this suggests that the
form of a given novel? the standpoint of the nar-
rative voice, whether the voice is ?omniscient? or
not? is far more determinative of the kind of so-
cial network described in the novel than where it
is set or even the number of characters involved.
Whereas standard accounts of nineteenth-century
fiction, following Bakhtin?s notion of the ?chrono-
tope,? emphasize the content of the novel as de-
terminative (where it is set, whether the novel fits
within a genre of ?village? or ?urban? fiction),
we have found that content to be surprisingly ir-
relevant to the shape of social networks within.
Bakhtin?s influential theory, and its detailed re-
workings by Williams, Moretti, and others, sug-
gests that as the novel becomes more urban, more
centered in (and interested in) populous urban set-
tings, the novel?s form changes to accommodate
the looser, more populated, less conversational
networks of city life. Our data suggests the op-
posite: that the ?urban novel? is not as strongly
distinctive a form as has been asserted, and that in
fact it can look much like the village fictions of the
century, as long as the same method of narration is
used.
This conclusion leads to some further consider-
ations. We are suggesting that the important ele-
ment of social networks in nineteenth-century fic-
tion is not where the networks are set, but from
what standpoint they are imagined or narrated.
Narrative voice, that is, trumps setting.
7 Conclusion
In this paper, we presented a method for char-
acterizing a text of literary fiction by extracting
the network of social conversations that occur be-
tween its characters. This allowed us to take a
systematic and wide look at a large corpus of
texts, an approach which complements the nar-
rower and deeper analysis performed by literary
scholars and can provide evidence for or against
some of their claims. In particular, we described
a high-precision method for detecting face-to-face
conversations between two named characters in a
novel, and showed that as the number of charac-
ters in a novel grows, so too do the cohesion, in-
terconnectedness and balance of their social net-
work. In addition, we showed that the form of the
novel (first- or third-person) is a stronger predictor
of these features than the setting (urban or rural).
Our results thus far suggest further review of our
methods, our corpus and our results for more in-
sights into the social networks found in this and
other genres of fiction.
8 Acknowledgments
This material is based on research supported in
part by the U.S. National Science Foundation
(NSF) under IIS-0935360. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the NSF.
References
Mikhail Bakhtin. 1981. Forms of time and of the
chronotope in the novel. In Trans. Michael Holquist
and Caryl Emerson, editors, The Dialogic Imagi-
nation: Four Essays, pages 84?258. University of
Texas Press, Austin.
John Burrows. 2004. Textual analysis. In Susan
Schreibman, Ray Siemens, and John Unsworth, ed-
itors, A Companion to Digital Humanities. Black-
well, Oxford.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In In
Proceedings of the 46th Annual Meeting of the As-
sociation of Com- putational Linguistics (ACL-08),
pages 789?797, Columbus, Ohio.
Wendy K. Tam Cho and James H. Fowler. 2010. Leg-
islative success in a small world: Social network
analysis and the dynamics of congressional legisla-
tion. The Journal of Politics, 72(1):124?135.
146
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Peter T. Davis, David K. Elson, and Judith L. Klavans.
2003. Methods for precise named entity matching
in digital collections. In Proceedings of the Third
ACM/IEEE Joint Conference on Digital Libraries
(JCDL ?03), Houston, Texas.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content ex-
traction (ace) program tasks, data, and evaluation.
In Proceedings of the Fourth International Confer-
ence on Language Resources and Evaluation (LREC
2004), pages 837?840, Lisbon.
Terry Eagleton. 2005. The English Novel: An Intro-
duction. Blackwell, Oxford.
David K. Elson and Kathleen R. McKeown. 2010. Au-
tomatic attribution of quoted speech in literary nar-
rative. In Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence (AAAI 2010),
Atlanta, Georgia.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In Proceedings of the 43nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pages 363?370.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social net-
works from threaded discussions. In International
Network of Social Network Analysis (INSNA) Con-
ference, St. Pete Beach, Florida.
Harry Halpin, Johanna D. Moore, and Judy Robertson.
2004. Automatic analysis of plot for story rewrit-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
?04), Barcelona.
John Lee. 2007. A computational model of text reuse
in ancient literary texts. In In Proceedings of the
45th Annual Meeting of the Association of Com-
putational Linguistics (ACL 2007), pages 472?479,
Prague.
Andrew McCallum, Xuerui Wang, and Andre?s
Corrada-Emmanual. 2007. Topic and role discovery
in social networks with experiments on enron and
academic email. Journal of Artificial Intelligence
Research, 30:249?272.
Franco Moretti. 1999. Atlas of the European Novel,
1800-1900. Verso, London.
Franco Moretti. 2005. Graphs, Maps, Trees: Abstract
Models for a Literary History. Verso, London.
Frederick Mostellar and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
The Federalist Papers. Springer, New York.
Raymond Williams. 1975. The Country and The City.
Oxford University Press, Oxford.
147
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 763?772,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Age Prediction in Blogs: A Study of Style, Content, and Online
Behavior in Pre- and Post-Social Media Generations
Sara Rosenthal
Department of Computer Science
Columbia University
New York, NY 10027, USA
sara@cs.columbia.edu
Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
kathy@cs.columbia.edu
Abstract
We investigate whether wording, stylistic
choices, and online behavior can be used
to predict the age category of blog authors.
Our hypothesis is that significant changes
in writing style distinguish pre-social me-
dia bloggers from post-social media blog-
gers. Through experimentation with a
range of years, we found that the birth
dates of students in college at the time
when social media such as AIM, SMS text
messaging, MySpace and Facebook first
became popular, enable accurate age pre-
diction. We also show that internet writing
characteristics are important features for
age prediction, but that lexical content is
also needed to produce significantly more
accurate results. Our best results allow for
81.57% accuracy.
1 Introduction
The evolution of the internet has changed the
way that people communicate. The introduction
of instant messaging, forums, social networking
and blogs has made it possible for people of ev-
ery age to become authors. The users of these
social media platforms have created their own
form of unstructured writing that is best char-
acterized as informal. Even how people com-
municate has dramatically changed, with multi-
tasking increasing and responses generated im-
mediately. We should be able to exploit those
differences to automatically determine from blog
posts whether an author is part of a pre- or post-
social media generation. This problem is called
age prediction and raises two main questions:
? Is there a point in time that proves to be
a significantly better dividing line between
pre and post-social media generations?
? What features of communication most di-
rectly reveal the generation in which a blog-
ger was born?
We hypothesize that the dividing line(s) oc-
cur when people in generation Y1, or the millen-
nial generation, (born anywhere from the mid-
1970s to the early 2000s) were typical college-
aged students (18-22). We focus on this gen-
eration due to the rise of popular social media
technologies such as messaging and online social
networks sites that occurred during that time.
Therefore, we experimented with binary clas-
sification into age groups using all birth dates
from 1975 through 1988, thus including students
from generation Y who were in college during
the emergence of social media technologies. We
find five years where binary classification is sig-
nificantly more accurate than other years: 1977,
1979, and 1982-1984. The appearance of social
media technologies such as AOL Instant Messen-
ger (AIM), weblogs, SMS text messaging, Face-
book and MySpace occurred when people with
these birth dates were in college.
We explore two of these years in more detail,
1979 and 1984, and examine a wide variety of
1http://en.wikipedia.org/wiki/Generation Y
763
features that differ between the pre-social me-
dia and post-social media bloggers. We examine
lexical-content features such as collocations and
part-of-speech collocations, lexical-stylistic fea-
tures such as internet slang and capitalization,
and features representing online behavior such
as time of post and number of friends. We find
that both stylistic and content features have a
significant impact on age prediction and show
that, for unseen blogs, we are able to classify
authors as born before or after 1979 with 80%
accuracy and born before or after 1984 with 82%
accuracy.
In the remainder of this paper, we first dis-
cuss work to date on age prediction for blogs
and then present the features that we extracted,
which is a larger set than previously explored.
We then turn separately to three experiments.
In the first, we implement a prior approach to
show that we can produce a similar outcome. In
the second, we show how the accuracy of age
prediction changes over time and pinpoint when
major changes occur. In the last experiment, we
describe our age prediction experiments in more
detail for the most significant years.
2 Related Work
In previous work, Mackinnon (2006) , used Live-
Journal data to identify a blogger?s age by ex-
amining the mean age of his peer group using
his social network and not just his immediate
friends. They were able to predict the correct
age within +/-5 years at 98% accuracy. This ap-
proach, however, is very different from ours as it
requires access to the age of each of the blogger?s
friends. Our approach uses only a body of text
written by a person along with his blogging be-
havior to determine which age group he is more
closely identified with.
Initial research on predicting age without us-
ing the ages of friends focuses on identifying im-
portant candidate features, including blogging
characteristics (e.g., time of post), text features
(e.g., length of post), and profile information
(e.g., interests) (Burger and Henderson, 2006).
They aimed at binary prediction of age, classify-
ing LiveJournal bloggers as either over or under
18, but were unable to automatically predict age
with more accuracy than a baseline model that
always chose the majority class. In our study on
determining the ideal age split we did not find
18 (bloggers born in 1986 in their dataset) to be
significant.
Prior work by Schler et al (2006) has ex-
amined metadata such as gender and age in
blogger.com bloggers. In contrast to our work,
they examine bloggers based on their age at the
time of the experiment, whether in the 10?s, 20?s
or 30?s age bracket. They identify interesting
changes in content and style features across cat-
egories, in which they include blogging words
(e.g., ?LOL?), all defined by the Linguistic In-
quiry and Word Count (LIWC) (Pennebaker et
al., 2007). They did not use characteristics of
online behavior (e.g., friends). They can distin-
guish between bloggers in the 10?s and in the 30?s
with relatively high accuracy (above 96%) but
many 30s are misclassified as 20s, which results
in a overall accuracy of 76.2%. We re-implement
Schler et al?s work in section 5.1 with similar
findings. Their work shows that ease of classi-
fication is dependent in part on what division
is made between age groups and in turn moti-
vates our decision to study whether the creation
of social media technologies can be used to find
the dividing line(s). Neither Schler et al, nor
we, attempt to determine how a person?s writ-
ing changes over his lifespan (Pennebaker and
Stone, 2003; Robins et al, 2002). Goswami et
al. (2009) add to Schler et al?s approach using
the same data and have a 4% increase in accu-
racy. However, the paper is lacking details and
it is entirely unclear how they were able to do
this with fewer features than Schler et al
In other work, Tam and Martell (2009) at-
tempt to detect age in the NPS chat corpus be-
tween teens and other ages. They use an SVM
classifier with only n-grams as features. They
achieve > 90% accuracy when classifying teens
vs 30s, 40s, 50s, and all adults and achieve at
best 76% when using 3 character gram features
in classifying teens vs 20s. This work shows that
n-grams are useful features for detecting age and
it is difficult to detect differences between con-
secutive groups such as teens and 20s, and this
764
Figure 1: Number of bloggers in 2010 by year of birth
from 1950-1996. A minimal amount of data occurred
in years not shown.
provides evidence for the need to find a good
classification split.
Other researchers have investigated weblogs
for differences in writing style depending on gen-
der identification (Herring and Paolillo, 2006;
Yan and Yan, 2006; Nowson and Oberlander,
2006). Herring et al(2006) found that the typi-
cal gender related features were based on genre
and independent of author gender. Yan et al
(2006) used text categorization and stylistic web
features, such as emoticons, to identify gender
and achieved 60% F-measure. Nowson et al
(2006) employed dictionary and n-gram based
content analysis and achieved 91.5% accuracy
using an SVM classifier. We also use a super-
vised machine learning approach, but classifica-
tion by gender is naturally a binary classification
task, while our work requires determining a nat-
ural dividing point.
3 Data Collection
Our corpus consists of blogs downloaded from
the virtual community LiveJournal. We chose
to use LiveJournal blogs for our corpus because
the website provides an easy-to-use format in
XML for downloading and crawling their site.
In addition, LiveJournal gives bloggers the op-
portunity to post their age on their profile. We
take advantage of this feature by downloading
blogs where the user chooses to publicly provide
this metadata.
We downloaded approximately 24,500 Live-
Journal blogs containing age. We represent age
as the year a person was born and not his age
at the time of the experiment. Since technol-
ogy has different effects in different countries,
we only analyze the blogs of people who have
listed US as their country. It is possible that
text written in a language other than English
is included in our corpus. However, in a man-
ual check of a small portion of text from 500
blogs, we only found English words. Each blog
was written by a unique individual and includes
a user profile and up to 25 recent posts written
between 2000-2010 with the most recent post be-
ing written in 2009-2010. The birth dates of the
bloggers range in years from 1940 to 2000 and
thus, their age ranges from 10 to 70 in 2010. Fig-
ure 1 shows the number of bloggers per age in
our group with birth dates from 1950 to 1996.
The majority of bloggers on LiveJournal were
born between 1978-1989.
4 Methods
We pre-processed the data to add Part-of-
Speech tags (POS) and dependencies (de Marn-
effe et al, 2006) between words using the Stan-
ford Parser (Klein and Manning, 2003a; Klein
and Manning, 2003b). The POS and syntactic
dependencies were only found for approximately
the first 90 words in each sentence. Our classifi-
cation method investigates 17 different features
that fall into three categories: online behavior,
lexical-stylistic and lexical-content. All of the
features we used are explained in Table 1 along
with their trend as age decreases where applica-
ble. Any feature that increased, decreased, or
fluctuated should have some positive impact on
the accuracy of predicting age.
4.1 Online Behavior and Interests
Online behavior features are blog specific, such
as number of comments and friends as described
in Table 1.1. The first feature, interests, is our
only feature that is specific to LiveJournal. In-
terests appear in the LiveJournal user profile,
but are not found on all blog sites. All other
online behavior features are typically available
in any blog.
765
Feature Explanation Example Trend as Age
Decreases
1 Interests Top3 interests provided on the profile page2 disney N/A
2
# of Friends Number of friends the blogger has 45 fluctuates
# of Posts Number of downloadable posts (0-25) 23 decrease
# of Lifetime Posts Number of posts written in total 821 decrease
Time Mode hour (00-23) and day the blogger posts 11/Monday no change
Comments Average number of comments per post 2.64 increase
3
Emoticons number of emoticons1 :) increase
Acronyms number of internet acronyms1 lol increase
Slang number of words that are not found in the dictionary1 wazzup increase
Punctuation number of stand-alone punctuation1 ... increase
Capitalization number of words (with length > 1) that are all CAPS1 YOU increase
Sentence Length average sentence length 40 decrease
Links/Images number of url and image links1 www.site.com fluctuates
4
Collocations Top3 Collocations in the age group. to [] the N/A
Syntax Collocations Top3 Syntax Collocations in the age group. best friends N/A
POS Collocations Top3 Part-of-Speech Collocations in the age group. this [] [] VB N/A
Words Top3 words in the age group his N/A
Table 1: List of all features used during classification divided into three categories (1,2) online behavior and
interests, (3) lexical - content, and (4) lexical - stylistic 1 normalized per sentence per entry, 2 available in
LiveJournal only, 3 pruned from top 200 features to include those that do not occur within +/- 10 position
in any other age group
We extracted the top 200 interests based on
occurrence in the profile page from 1500 random
blogs in three age groups. These age groups are
used solely to illustrate the differences that oc-
cur at different ages and are not used in our
classification experiments. We then pruned the
list of interests by excluding any interest that
occurred within a +/-10 window (based on its
position in the list) in multiple age groups. We
show the top interests in each age group in Ta-
ble 2. For example, ?disney? is the most popu-
lar unique interest in the 18-22 age group with
only 39 other non-unique interests in that age
group occurring more frequently. ?Fanfiction?
is a popular interest in all age groups, but it
is significantly more popular in the 18-22 age
group than in other age groups.
Amongst the other online behavior features,
the number of friends tends to fluctuate but
seems to be higher for older bloggers. The num-
ber of lifetime posts (Figure 2(d)), and posts de-
creases as bloggers get younger which is as one
would expect unless younger people were orders
of magnitude more prolific than older people.
The mode time (Figure 2(b)), refers to the most
18-22 28-32 38-42
disney 39 tori amos 49 polyamory 40
yaoi 40 hiking 55 sca 67
johnny depp 42 women 61 babylon 5 84
rent 44 gaming 62 leather 94
house 45 comic books 67 farscape 103
fanfiction 11 fanfiction 58 fanfiction 138
drawing 10 drawing 25 drawing 65
sci-fi 199 sci-fi 37 sci-fi 21
Table 2: Top interests for three different age groups.
The top half refers to the top 5 interests that are
unique to each age group. The value refers to the
position of the interest in its list
common hour of posting from 00-24 based on
GMT time. We didn?t compute time based on
the time zone because city/state is often not in-
cluded. We found time to not be a useful feature
in this manner and it is difficult to come to any
conclusions from its change as year of birth de-
creases.
4.2 Lexical - Stylistic
The Lexical-Stylistic features in Table 1.2, such
as slang and sentence length, are computed us-
766
Figure 2: Examples of change to features over time (a) Average number of emoticons in a sentence increases
as age decreases (b) The most common time fluctuates until 1982, where it is consistent (c) The number
of links/images in a sentence fluctuates (d) The average number of lifetime posts per year decreases as age
decreases
ing the text from all of the posts written by the
blogger. Other than sentence length, they were
normalized by sentence and post to keep the
numbers consistent between bloggers regardless
of whether the user wrote one or many posts in
his/her blog. The number of emoticons (Figure
2(a)), acronyms, and capital words increased as
bloggers got younger. Slang and punctuation,
which excludes the emoticons and acronyms
counted in the other features, increased as well,
but not as significantly. The length of sentences
decreased as bloggers got younger and the num-
ber of links/images varied across all years as
shown in Figure 2(c).
4.3 Lexical - Content
The last category of features described in Ta-
ble 1.3 consists of collocations and words, which
are content based lexical terms. The top words
are produced using a typical ?bag-of-words? ap-
proach. The top collocations are computed us-
ing a system called Xtract (Smadja, 1993).
We use Xtract to obtain important lexical col-
locations, syntactic collocations, and POS col-
locations as features from our text. Syntac-
tic collocations refer to significant word pairs
that have specific syntactic dependencies such
as subject/verb and verb/object. Due to the
length of time it takes to run this program, we
ran Xtract on 1500 random blogs from each age
group and examined the first 1000 words per
blog. We looked at 1.5 million words in total
and found approximately 2500-2700 words that
were repeated more than 50 times.
We extracted the top 200 words and colloca-
tions sorted by post frequency (pf), which is the
number of posts the term occurred in. Then,
similarly to interests, we pruned each list to
include the features that did not occur within
+/-10 window (based on its position in the list)
within each age group. Prior to settling on these
metrics, we also experimented with other met-
rics such as the number of times the collocation
767
18-22 28-32 38-42
ldquot (?) 101 great 166 may 164
t 152 find 167 old 183
school 172 many 177 house 191
x 173 years 179 world 192
anything 175 week 181 please 198
maybe 179 post 190 - -
because 68 because 80 because 93
him 59 him 85 him 73
Table 3: Top words for three age groups. The top
half refers to the top 5 words that are unique to each
age group. The value refers to the position of the
interest in its list
occurred in total, defined as collocation or term
frequency (tf), the number of blogs the colloca-
tion occurred in, defined as blog frequency (bf),
and variations of TF*IDF (Salton and Buck-
ley, 1988) where we tried using inverse blog fre-
quency and inverse post frequency as the value
for IDF. In addition, we also experimented with
looking at a different number of important words
and collocations ranging from the top 100-300
terms and experimented without pruning. None
of these variations improved accuracy in our
experiments, however, and thus, were dropped
from further experimentation.
Table 3 shows the top words for each age
group; older people tend to use words such as
?house? and ?old? frequently and younger peo-
ple talk about ?school?.
In our analysis of the top collocations, we
found that younger people tend to use first per-
son singular (I,me) in subject position while
older people tend to use first person plural (we)
in subject position, both with a variety of verbs.
5 Experiments and Results
We ran three separate experiments to determine
how well we can predict age: 1. classifying into
three distinct age groups (Schler et al (2006)
experiment), 2. binary classification with the
split at each birth year from 1975-1988 and 3.
Detailed classification on two significant splits
from the second experiment.
We ran all of our experiments in Weka (Hall et
al., 2009) using logistic regression over 10 runs
of 10-fold cross-validation. All values shown are
blogger.com livejournal.com
download
year
2004 2010
# of Blogs 19320 11521
# of Posts1 1.4 million 256,000
# of words1 295 million 50 million
age 13?17 23?27 33?37 18?22 28?32 38?42
size 8240 8086 2994 3518 5549 2454
majority
baseline
43.8% (13-17) 48.2% (22-32)
Table 4: Statistics for Schler et al?s data (blog-
ger.com) vs our data (livejournal.com) 1 is approxi-
mate amount.
the averages of the accuracies from the 10 cross-
validation runs and all results were compared
for statistical significance using the t-test where
applicable.
We use logistic regression as our classifier be-
cause it has been shown that logistic regression
typically has lower asymptotic error than naive
Bayes for multiple classification tasks as well as
for text classification (Ng and Jordan, 2002).
We experimented with an SVM classifier and
found logistic regression to do slightly better.
5.1 Age Groups
The first experiment implements a variation of
the experiment done by Schler et al (2006).
The differences between the two datasets are
shown in Tables 4. The experiment looks at
three age groups containing a 5-year gap be-
tween each group. Intermediate years were not
included to provide clear differentiation between
the groups because many of the blogs have been
active for several years and this will make it less
common for a blogger to have posts that fall into
two age groups (Schler et al, 2006).
We did not use the same age groups as Schler
et al because very few blogs on LiveJournal, in
2010, are in the 13-17 age group. Many early de-
mographic studies (Perseus Development, 2004;
Herring et al, 2004) show teens as the dom-
inant age group in all blogs. However, more
recent studies (Nowson and Oberlander, 2006;
Lenhart et al, 2010) show that less teens blog.
Furthermore, an early study on the LiveJournal
768
Figure 3: Style vs Content: Accuracy from 1975-
1988 for Style (Online-Behavior+Lexical-Stylistic)
vs Content (BOW)
demographic (Kumar et al, 2004) reported that
28.6% of blogs are written by bloggers between
the ages 13-18 whereas based on the current de-
mographic statistics, in 20102, only 6.96% of
blogs are written by that age group and the
number of bloggers in the 31-36 age group in-
creased from 3.9% to 12.08%. We chose the later
age groups because this study is based on blogs
updated in 2009-10 which is 5-6 years later and
thus, the 13-17 age group is now 18-22 and so
on.
We use style-based (lexical-stylistic) and
content-based features (BOW, interests) to
mimic Schler et al?s experiment as closely as
possible and also experimented with adding
online-behavior features. Our experiment with
style-based and content-based features had an
accuracy of 57%. However, when we added
online-behavior, we increased our accuracy to
67%. A more detailed look at the better results
show that our accuracies are consistently 7%
lower than the original work but we have similar
findings; 18-22s are distinguishable from 38-42s
with accuracy of 94.5%, and 18-22s are distin-
guishable from 28-32s with accuracy of 80.5%.
However, many 38-42s are misclassified as 28-
32s with an accuracy of 72.1%, yielding overall
accuracy of 67%. Due to our findings, we believe
that adding online-behavior features to Schler et
al.?s dataset would improve their results as well.
2http://www.livejournal.com/stats.bml
5.2 Social Media and Generation Y
In the first experiment we used the current age
of a blogger based on when he wrote his last
post. However, the age of a person changes;
someone who was in one age group now will be
in a different age group in 5 years. Furthermore,
a blogger?s posts can fall into two categories de-
pending on his age at the time. Therefore, our
second experiment looks at year of birth instead
of age, as that never changes. In contrast to
Schler et al?s experiment, our division does not
introduce a gap between age groups, we do bi-
nary classification, and we use significantly less
data.
We approach age prediction as attempting to
identify a shift in writing style over a 14 year
time span from birth years 1975-1988:
For each year X = 1975-1988:
? get 1500 blogs (?33,000 posts) balanced across
years BEFORE X
? get 1500 blogs (?33,000 posts) balanced across
years IN/AFTER X
? Perform binary classification between blogs BE-
FORE X and IN/AFTER X
The experiment focuses on the range of birth
years of bloggers from 1975-1888 to identify at
what point in time, if any, shift(s) in writing
style occurred amongst college-aged students in
generation Y. We were motivated to examine
these years due to the emergence of social me-
dia technologies during that time. Furthermore,
research by Pew Internet (Zickuhr, 2010) has
found that this generation (defined as 1977-
1992 in their research) uses social networking,
blogs, and instant messaging more than their
elders. The experiment is balanced to ensure
that each birth year is evenly represented. We
balance the data by choosing a blogger consec-
utively from each birth year in the category, re-
peating these sweeps through the category until
we have obtained 1500 blogs. We chose to use
1500 blogs from each group because of process-
ing power, time constraints, and the amount of
blogs needed to reasonably sample the age group
at each split. Due to the extensive running time,
we only examined variations of a combination of
769
Figure 4: Style and Content: Accuracy from 1975-
1988 using BOW, Online Behavior, and Lexical-
Stylistic features
online-behavior, lexical-stylistic, and BOW fea-
tures.
We found accuracy to increase as year of birth
increases in various feature experiments which is
consistent with the trends we found while exam-
ining the distribution of features such as emoti-
cons and lifetime posts in Figure 2. We ex-
perimented with style and content features and
found that both help improve accuracy. Figure 3
shows that content helps more than style, but
style helps more as age decreases. However, as
shown in Figure 4, style and content combined
provided the best results. We found 5 years to
have significant improvement over all prior years
for p ? .0005: 1977, 1979, and 1982-1984.
Generation Y is considered the social me-
dia generation, so we decided to examine how
the creation and/or popularity of social media
technologies compared to the years that had a
change in writing style. We looked at many pop-
ular social media technologies such as weblogs,
messaging, and social networking sites. Figure 5
compares the significant years 1977,1979, and
1982-1984 against when each technology was
created or became popular amongst college aged
students. We find that all the technologies had
an effect on one or more of those years. AIM and
weblogs coincide with the earlier shifts at 1977
and 1979, SMS messaging coincide with both
the earlier and later shifts at 1979 and 1982,
and the social networking sites, MySpace and
Facebook coincide with the later shifts of 1982-
Figure 5: The impact of social media technologies:
The arrows correspond to the years that generation
Yers were college aged students. The highlighted
years represent the significant years. 1Year it be-
came popular (Urmann, 2009)
1984. On the other hand, web forums and Twit-
ter each coincide with only one outlying year
which suggests that either they had less of an
impact on writing style or, in the case of Twit-
ter, the change has not yet been transferred to
other writing forms.
5.3 A Closer Look: 1979 and 1984
Our final experiment provides a more detailed
explanation of the results using various feature
combinations when splitting pre- and post- so-
cial media bloggers by year of birth at two of
the significant years found in the previous sec-
tion; 1979 and 1984. The results for all of the
experiments described are shown in Table 5.
We experimented against two baselines, on-
line behavior and interests. We chose these two
features as baselines because they are both easy
to generate and not lexical in nature. We found
that we were able to exceed the baselines sig-
nificantly using a simple bag-of-words (BOW)
approach. This means the BOW does a better
job of picking topics than interests. We found
that including all 17 features did not do well, but
we were able to get good results using a subset
of the lexical features. We found the best re-
sults to have an accuracy of 79.96% and 81.57%
for 1979 and 1984 respectively using BOW, in-
terests, online behavior, and all lexical-stylistic
features.
In addition, we show accuracy without in-
terests since they are not always available.
770
Experiment 1979 1984
Online-Behavior 59.66 61.61
Interests 70.22 74.61
Lexical-Stylistic 65.382 67.282
Slang+Emoticons+Acronyms 60.572 62.102
Online-Behavior + Lexical-
Stylistic
67.162 71.312
Collocations + Syntax Colloca-
tions
53.471 73.452
POS-Collocations + POS-
Syntax Collocations
55.541 74.002
BOW 75.26 77.76
BOW+Online-Behavior 76.39 79.22
BOW + Online-Behavior +
Lexical-Stylistic
77.45 80.88
BOW + Online-Behavior +
Lexical-Stylistic + Syntax Collo-
cations
74.8 80.36
BOW + Online-Behavior
+ Lexical-Stylistic + POS-
Collocations + POS Syntax
Collocations
74.73 80.54
Online-Behavior + Interests +
Lexical-Stylistic
74.39 77.20
BOW + Online-Behavior + In-
terests + Lexical-Stylistic
79.96 81.57
All Features 71.26 74.072
Table 5: Feature Accuracy. The top portion refers to
the baselines. The best accuracies are shown in bold.
Unless otherwise marked, all accuracies are statisti-
cally significant at p<=.0005 for both baselines. 1
not statistically significant over Online-Behavior and
Interests. 2 not statistically significant over Interests.
BOW, online-behavior, and lexical-stylistic fea-
tures combined did best achieving accuracy of
77.45% and 80.88% in 1979 and 1984 respec-
tively. This indicates that our classification
method could work well on blogs from any web-
site. It is interesting to note that colloca-
tions and POS-collocations were useful, but only
when we use 1984 as the split which implies that
bloggers born in 1984 and later are more homo-
geneous.
6 Conclusion and Future Work
We have shown that it is possible to predict the
age group of a person based on style, content,
and online behavior features with good accu-
racy; these are all features that are available
in any blog. While features representing writ-
ing practices that emerged with social media
(e.g., capitalized words, abbreviations, slang)
do not significantly impact age prediction on
their own, these features have a clear change of
value across time, with post-social media blog-
gers using them more often. We found that
the birth years that had a significant change
in writing style corresponded to the birth dates
of college-aged students at the time of the cre-
ation/popularity of social media technologies,
AIM, SMS text messaging, weblogs, Facebook
and MySpace.
In the future we plan on using age and other
metadata to improve results in larger tasks such
as identifying opinion, persuasion and power
by targeting our approach in those tasks to
the identified age of the person. Another ap-
proach that we will experiment with is the use
of ranking, regression, and/or clustering to cre-
ate meaningful age groups.
7 Acknowledgements
This research was funded by the Office of the
Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab.
All statements of fact, opinion or conclusions
contained herein are those of the authors and
should not be construed as representing the of-
ficial views or policies of IARPA, the ODNI or
the U.S. Government.
References
Shlomo Argamon, Moshe Koppel, Jonathan Fine,
and Anat Rachel Shimoni. 2003. Gender, genre,
and writing style in formal written texts. TEXT,
23:321?346.
John D. Burger and John C. Henderson. 2006. An
exploration of observable features related to blog-
ger age. In AAAI Spring Symposia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In In LREC 2006.
Sumit Goswami, Sudeshna Sarkar, and Mayur
Rustagi. 2009. Stylometric analysis of bloggers?
771
age and gender. In International AAAI Confer-
ence on Weblogs and Social Media.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
Susan C. Herring and John C. Paolillo. 2006. Gen-
der and genre variation in weblogs. Journal of
Sociolinguistics, 10(4):439?459.
Susan C. Herring, L.A. Scheidt, S. Bonus, and
E. Wright. 2004. Bridging the gap: A genre anal-
ysis of weblogs. In Proceedings of the 37th Hawaii
International Conference on System Sciences.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems, volume 15. MIT Press.
Ravi Kumar, Jasmine Novak, Prabhakar Raghavan,
and Andrew Tomkins. 2004. Structure and evolu-
tion of blogspace. Commun. ACM, 47:35?39, De-
cember.
Amanda Lenhart, Kristen Purcell, Aaron Smith, and
Kathryn Zickuhr. 2010. Social media and young
adults.
Ian Mackinnon. 2006. Age and geographic inferences
of the livejournal social network. In In Statistical
Network Analysis Workshop.
Andrew Y Ng and Michael I Jordan. 2002. On dis-
criminative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. Neural
Information Processing Systems, 2:841?848.
Scott Nowson and Jon Oberlander. 2006. The iden-
tity of bloggers: Openness and gender in personal
weblogs.
James W Pennebaker and Lori D Stone. 2003.
Words of wisdom: language use over the life span.
J Pers Soc Psychol, 85(2):291?301.
J.W. Pennebaker, R.E. Booth, and M.E. Fran-
cis. 2007. Linguistic inquiry and word count:
Liwc2007 ? operator?s manual. Technical report,
LIWC, Austin, TX.
Perseus Development. 2004. The blogging iceberg:
Of 4.12 million hosted weblogs, most little seen
and quickly abandoned. Technical report, Perseus
Development.
R.W. Robins, K. H. Trzesniewski, J.L. Tracy, S.D
Gosling, and J Potter. 2002. Global self-esteem
across the lifespan. Psychology and Aging, 17:423?
434.
Gerard Salton and Christopher Buckley. 1988.
Term-weighting approaches in automatic text re-
trieval. In Information Processing and Manage-
ment, pages 513?523.
J. Schler, M. Koppel, S. Argamon, and J. Pen-
nebaker. 2006. Effects of age and gender on blog-
ging. In AAAI Spring Symposium on Computa-
tional Approaches for Analyzing Weblogs.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19:143?
177.
Jenny Tam and Craig H. Martell. 2009. Age detec-
tion in chat. In Proceedings of the 2009 IEEE In-
ternational Conference on Semantic Computing,
ICSC ?09, pages 33?39, Washington, DC, USA.
IEEE Computer Society.
David H. Urmann. 2009. The history of text mes-
saging.
Xiang Yan and Ling Yan. 2006. Gender classification
of weblog authors. In AAAI Spring Symposium
Series on Computation Approaches to Analyzing
Weblogs, pages 228?230.
Kathryn Zickuhr. 2010. Generations 2010.
772
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254?259,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Optimal and Syntactically-Informed Decoding for Monolingual
Phrase-Based Alignment
Kapil Thadani and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{kapil,kathy}@cs.columbia.edu
Abstract
The task of aligning corresponding phrases
across two related sentences is an important
component of approaches for natural language
problems such as textual inference, paraphrase
detection and text-to-text generation. In this
work, we examine a state-of-the-art struc-
tured prediction model for the alignment task
which uses a phrase-based representation and
is forced to decode alignments using an ap-
proximate search approach. We propose in-
stead a straightforward exact decoding tech-
nique based on integer linear programming
that yields order-of-magnitude improvements
in decoding speed. This ILP-based decoding
strategy permits us to consider syntactically-
informed constraints on alignments which sig-
nificantly increase the precision of the model.
1 Introduction
Natural language processing problems frequently in-
volve scenarios in which a pair or group of related
sentences need to be aligned to each other, establish-
ing links between their common words or phrases.
For instance, most approaches for natural language
inference (NLI) rely on alignment techniques to es-
tablish the overlap between the given premise and a
hypothesis before determining if the former entails
the latter. Such monolingual alignment techniques
are also frequently employed in systems for para-
phrase generation, multi-document summarization,
sentence fusion and question answering.
Previous work (MacCartney et al, 2008) has pre-
sented a phrase-based monolingual aligner for NLI
(MANLI) that has been shown to significantly out-
perform a token-based NLI aligner (Chambers et
al., 2007) as well as popular alignment techniques
borrowed from machine translation (Och and Ney,
2003; Liang et al, 2006). However, MANLI?s use
of a phrase-based alignment representation appears
to pose a challenge to the decoding task, i.e. the
task of recovering the highest-scoring alignment un-
der some parameters. Consequently, MacCartney et
al. (2008) employ a stochastic search algorithm to
decode alignments approximately while remaining
consistent with regard to phrase segmentation.
In this paper, we propose an exact decoding tech-
nique for MANLI that retrieves the globally opti-
mal alignment for a sentence pair given some pa-
rameters. Our approach is based on integer lin-
ear programming (ILP) and can leverage optimized
general-purpose LP solvers to recover exact solu-
tions. This strategy boosts decoding speed by an
order of magnitude over stochastic search in our ex-
periments. Additionally, we introduce hard syntac-
tic constraints on alignments produced by the model,
yielding better precision and a large increase in the
number of perfect alignments produced over our
evaluation corpus.
2 Related Work
Alignment is an integral part of statistical MT (Vo-
gel et al, 1996; Och and Ney, 2003; Liang et al,
2006) but the task is often substantively different
from monolingual alignment, which poses unique
challenges depending on the application (MacCart-
ney et al, 2008). Outside of NLI, prior research has
also explored the task of monolingual word align-
254
ment using extensions of statistical MT (Quirk et al,
2004) and multi-sequence alignment (Barzilay and
Lee, 2002).
ILP has been used extensively for applications
ranging from text-to-text generation (Clarke and La-
pata, 2008; Filippova and Strube, 2008; Wood-
send et al, 2010) to dependency parsing (Martins
et al, 2009). It has also been recently employed for
finding phrase-based MT alignments (DeNero and
Klein, 2008) in a manner similar to this work; how-
ever, we further build upon this model through syn-
tactic constraints on the words participating in align-
ments.
3 The MANLI Aligner
Our alignment system is structured identically to
MANLI (MacCartney et al, 2008) and uses the same
phrase-based alignment representation. An align-
ment E between two fragments of text T1 and T2
is represented by a set of edits {e1, e2, . . .}, each be-
longing to one of the following types:
? INS and DEL edits covering unaligned words in
T1 and T2 respectively
? SUB and EQ edits connecting a phrase in T1 to
a phrase in T2. EQ edits are a specific case of
SUB edits that denote a word/lemma match; we
refer to both types as SUB edits in this paper.
Every token in T1 and T2 participates in exactly one
edit. While alignments are one-to-one at the phrase
level, a phrase-based representation effectively per-
mits many-to-many alignments at the token level.
This enables the aligner to properly link paraphrases
such as death penalty and capital punishment by ex-
ploiting lexical resources.
3.1 Dataset
MANLI was trained and evaluated on a corpus of
human-generated alignment annotations produced
by Microsoft Research (Brockett, 2007) for infer-
ence problems from the second Recognizing Tex-
tual Entailment (RTE2) challenge (Bar-Haim et al,
2006). The corpus consists of a development set
and test set that both feature 800 inference prob-
lems, each of which consists of a premise, a hy-
pothesis and three independently-annotated human
alignments. In our experiments, we merge the an-
notations using majority rule in the same manner as
MacCartney et al (2008).
3.2 Features
A MANLI alignment is scored as a sum of weighted
feature values over the edits that it contains. Fea-
tures encode the type of edit, the size of the phrases
involved in SUB edits, whether the phrases are con-
stituents and their similarity (determined by lever-
aging various lexical resources). Additionally, con-
textual features note the similarity of neighboring
words and the relative positions of phrases while
a positional distortion feature accounts for the dif-
ference between the relative positions of SUB edit
phrases in their respective sentences.
Our implementation uses the same set of fea-
tures as MacCartney et al (2008) with some mi-
nor changes: we use a shallow parser (Daume? and
Marcu, 2005) for detecting constituents and employ
only string similarity and WordNet for determining
semantic relatedness, forgoing NomBank and the
distributional similarity resources used in the orig-
inal MANLI implementation.
3.3 Parameter Inference
Feature weights are learned using the averaged
structured perceptron algorithm (Collins, 2002), an
intuitive structured prediction technique. We deviate
from MacCartney et al (2008) and do not introduce
L2 normalization of weights during learning as this
could have an unpredictable effect on the averaged
parameters. For efficiency reasons, we parallelize
the training procedure using iterative parameter mix-
ing (McDonald et al, 2010) in our experiments.
3.4 Decoding
The decoding problem is that of finding the highest-
scoring alignment under some parameter values for
the model. MANLI?s phrase-based representation
makes decoding more complex because the segmen-
tation of T1 and T2 into phrases is not known before-
hand. Every pair of phrases considered for inclusion
in an alignment must adhere to some consistent seg-
mentation so that overlapping edits and uncovered
words are avoided.
Consequently, the decoding problem cannot be
factored into a number of independent decisions
and MANLI searches for a good alignment using
a stochastic simulated annealing strategy. While
seemingly quite effective at avoiding local maxima,
255
System Data P% R% F1% E%
MANLI dev 83.4 85.5 84.4 21.7
(reported 2008) test 85.4 85.3 85.3 21.3
MANLI dev 85.7 84.8 85.0 23.8
(reimplemented) test 87.2 86.3 86.7 24.5
MANLI-Exact dev 85.7 84.7 85.2 24.6
(this work) test 87.8 86.1 86.8 24.8
Table 1: Performance of aligners in terms of precision, re-
call, F-measure and number of perfect alignments (E%).
this iterative search strategy is computationally ex-
pensive and moreover is not guaranteed to return the
highest-scoring alignment under the parameters.
4 Exact Decoding via ILP
Instead of resorting to approximate solutions, we
can simply reformulate the decoding problem as the
optimization of a linear objective function with lin-
ear constraints, which can be solved by well-studied
algorithms using off-the-shelf solvers1. We first de-
fine boolean indicator variables xe for every possible
edit e between T1 and T2 that indicate whether e is
present in the alignment or not. The linear objective
that maximizes the score of edits for a given param-
eter vector w is expressed as follows:
f(w) = max
?
e
xe ? scorew(e)
= max
?
e
xe ?w ? ?(e) (1)
where ?(e) is the feature vector over an edit. This
expresses the score of an alignment as the sum of
scores of edits that are present in it, i.e., edits e that
have xe = 1.
In order to address the phrase segmentation issue
discussed in ?3.4, we merely need to add linear con-
straints ensuring that every token participates in ex-
actly one edit. Introducing the notation e ? t to in-
dicate that edit e covers token t in one of its phrases,
this constraint can be encoded as:
?
e: e?t
xe = 1 ?t ? Ti, i = {1, 2}
On solving this integer program, the values of the
variables xe indicate which edits are present in the
1We use LPsolve: http://lpsolve.sourceforge.net/
Corpus Size Approximate Exact
Search ILP
RTE2 dev 800 2.58 0.11
test 800 1.67 0.08
McKeown et al
(2010)
297 61.96 2.45
Table 2: Approximate running time per decoding task in
seconds for the search-based approximate decoder and
the ILP-based exact decoder on various corpora (see text
for details).
highest-scoring alignment under w. A similar ap-
proach is employed by DeNero and Klein (2008) for
finding optimal phrase-based alignments for MT.
4.1 Alignment experiments
For evaluation purposes, we compare the perfor-
mance of approximate search decoding against ex-
act ILP-based decoding on a reimplementation of
MANLI as described in ?3. All models are trained
on the development section of the Microsoft Re-
search RTE2 alignment corpus (cf. ?3.1) using
the training parameters specified in MacCartney
et al (2008). Aligner performance is determined
by counting aligned token pairs per problem and
macro-averaging over all problems. The results are
shown in Table 1.
We first observe that our reimplemented version
of MANLI improves over the results reported in
MacCartney et al (2008), gaining 2% in precision,
1% in recall and 2-3% in the fraction of alignments
that exactly matched human annotations. We at-
tribute at least some part of this gain to our modified
parameter inference (cf. ?3.3) which avoids normal-
izing the structured perceptron weights and instead
adheres closely to the algorithm of Collins (2002).
Although exact decoding improves alignment per-
formance over the approximate search approach, the
gain is marginal and not significant. This seems to
indicate that the simulated annealing search strategy
is fairly effective at avoiding local maxima and find-
ing the highest-scoring alignments.
4.2 Runtime experiments
Table 2 contains the results from timing alignment
tasks over various corpora on the same machine us-
ing the models trained as per ?4.1. We observe a
256
twenty-fold improvement in performance with ILP-
based decoding. It is important to note that the spe-
cific implementations being compared2 may be re-
sponsible for the relative speed of decoding.
The short hypotheses featured in the RTE2 cor-
pus (averaging 11 words) dampen the effect of the
quadratic growth in number of edits with sentence
length. For this reason, we also run the aligners on
a corpus of 297 related sentence pairs which don?t
have a particular disparity in sentence lengths (McK-
eown et al, 2010). The large difference in decoding
time illustrates the scaling limitations of the search-
based decoder.
5 Syntactically-Informed Constraints
The use of an integer program for decoding pro-
vides us with a convenient mechanism to prevent
common alignment errors by introducting additional
constraints on edits. For example, function words
such as determiners and prepositions are often mis-
aligned just because they occur frequently in many
different contexts. Although MANLI makes use
of contextual features which consider the similar-
ity of neighboring words around phrase pairs, out-
of-context alignments of function words often ap-
pear in the output. We address this issue by adding
constraints to the integer program from ?4 that look
at the syntactic structure of T1 and T2 and prevent
matching function words from appearing in an align-
ment unless they are syntactically linked with other
words that are aligned.
To enforce token-based constraints, we define
boolean indicator variables yt for each token t in
text snippets T1 and T2 that indicate whether t is in-
volved in a SUB edit or not. The following constraint
ensures that yt = 1 if and only if it is covered by a
SUB edit that is present in the alignment.
yt ?
?
e: e?t,
e is SUB
xe = 0 ?t ? Ti, i = {1, 2}
We refer to tokens t with yt = 1 as being active in
the alignment. Constraints can now be applied over
any token with specific part-of-speech (POS) tag in
2Our Python reimplementation closely follows the original
Java implementation of MANLI and was optimized for perfor-
mance. MacCartney et al (2008) report a decoding time of
about 2 seconds per problem.
System Data P% R% F1% E%
MANLI-Exact with dev 86.8 84.5 85.6 25.3
M constraints test 88.8 85.7 87.2 29.9
MANLI-Exact with dev 86.1 84.6 85.3 24.5
L constraints test 88.2 86.4 87.3 27.6
MANLI-Exact with dev 87.1 84.4 85.8 25.4
M + L constraints test 89.5 86.2 87.8 33.0
Table 3: Performance of MANLI-Exact featuring addi-
tional modifier (M) and lineage (L) constraints. Figures
in boldface are statistically significant over the uncon-
strained MANLI reimplementation (p ? 0.05).
order to ensure that it can only be active if a differ-
ent token related to it in a dependency parse of the
sentence is also active. We consider the following
classes of constraints:
Modifier constraints: Tokens t that represent con-
junctions, determiners, modals and cardinals can
only be active if their parent tokens pi(t) are active.
yt ? ypi(t) <= 0
if POS(t) ? {CC, CD, MD, DT, PDT, WDT}
Lineage constraints: Tokens t that represent prepo-
sitions and particles (which are often confused by
parsers) can only be active if one of their ancestors
?(t) or descendants ?(t) is active. These constraints
are less restrictive than the modifier constraints in
order to account for attachment errors.
yt ?
?
a??(t)
ya ?
?
d??(t)
yd <= 0
if POS(t) ? {IN, TO, RP}
5.1 Alignment experiments
A TAG-based probabilistic dependency parser (Ban-
galore et al, 2009) is used to formulate the above
constraints in our experiments. The results are
shown in Table 3 and indicate a notable increase in
alignment precision, which is to be expected as the
constraints specifically seek to exclude poor edits.
Despite the simple and overly general restrictions
being applied, recall is almost unaffected. Most
compellingly, the number of perfect alignments pro-
duced by the system increases significantly when
257
compared to the unconstrained models from Table 1
(a relative increase of 35% on the test corpus).
6 Discussion
The results of our evaluation indicate that exact de-
coding via ILP is a robust and efficient technique for
solving alignment problems. Furthermore, the in-
corporation of simple constraints over a dependency
parse can help to shape more accurate alignments.
An examination of the alignments produced by our
system reveals that many remaining errors can be
tackled by the use of named-entity recognition and
better paraphrase corpora; this was also noted by
MacCartney et al (2008) with regard to the original
MANLI system. In addition, stricter constraints that
enforce the alignment of syntactically-related tokens
(rather than just their inclusion in the solution) may
also yield performance gains.
Although MANLI?s structured prediction ap-
proach to the alignment problem allows us to encode
preferences as features and learn their weights via
the structured perceptron, the decoding constraints
used here can be used to establish dynamic links be-
tween alignment edits which cannot be determined
a priori. The interaction between the selection of
soft features for structured prediction and hard con-
straints for decoding is an interesting avenue for fur-
ther research on this task. Initial experiments with
a feature that considers the similarity of dependency
heads of tokens in an edit (similar to MANLI?s con-
textual features that look at preceding and following
words) yielded some improvement over the base-
line models; however, this did not perform as well
as the simple constraints described above. Specific
features that approximate soft variants of these con-
straints could also be devised but this was not ex-
plored here.
In addition to the NLI applications considered in
this work, we have also employed the MANLI align-
ment technique to tackle alignment problems that
are not inherently asymmetric such as the sentence
fusion problems from McKeown et al (2010). Al-
though the absence of asymmetric alignment fea-
tures affects performance marginally over the RTE2
dataset, all the performance gains exhibited by exact
decoding with constraints appear to be preserved in
symmetric settings.
7 Conclusion
We present a simple exact decoding technique as an
alternative to approximate search-based decoding in
MANLI that exhibits a twenty-fold improvement in
runtime performance in our experiments. In addi-
tion, we propose novel syntactically-informed con-
straints to increase precision. Our final system im-
proves over the results reported in MacCartney et al
(2008) by about 4.5% in precision and 1% in recall,
with a large gain in the number of perfect alignments
over the test corpus. Finally, we analyze the align-
ments produced and suggest that further improve-
ments are possible through careful feature/constraint
design, as well as the use of named-entity recogni-
tion and additional resources.
Acknowledgments
The authors are grateful to Bill MacCartney for pro-
viding a reference MANLI implementation and the
anonymous reviewers for their useful feedback. This
material is based on research supported in part by
the U.S. National Science Foundation (NSF) under
IIS-05-34871. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the NSF.
References
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen
Rambow, and Beno??t Sagot. 2009. MICA: a prob-
abilistic dependency parser based on tree insertion
grammars. In Proceedings of HLT-NAACL, pages
185?188.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL Recognising Textual En-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Regina Barzilay and Lilian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of EMNLP.
Chris Brockett. 2007. Aligning the 2006 RTE cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
258
Christopher D. Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 165?170.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: an integer linear pro-
gramming approach. Journal of Artifical Intelligence
Research, 31:399?429, March.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1?8.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In Proceedings of ICML,
pages 169?176.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, pages 25?28.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177?185.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104?111.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP, pages 802?811.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP, pages 342?350.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of HLT-NAACL, pages 456?
464.
Kathleen McKeown, Sara Rosenthal, Kapil Thadani, and
Coleman Moore. 2010. Time-efficient creation of an
accurate sentence fusion corpus. In Proceedings of
HLT-NAACL, pages 317?320.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51, March.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of EMNLP, pages 142?149,
July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?841.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of EMNLP, pages 513?523.
259
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670?675,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Hierarchical Model of Web Summaries
Yves Petinot and Kathleen McKeown and Kapil Thadani
Department of Computer Science
Columbia University
New York, NY 10027
{ypetinot|kathy|kapil}@cs.columbia.edu
Abstract
We investigate the relevance of hierarchical
topic models to represent the content of Web
gists. We focus our attention on DMOZ,
a popular Web directory, and propose two
algorithms to infer such a model from its
manually-curated hierarchy of categories. Our
first approach, based on information-theoretic
grounds, uses an algorithm similar to recur-
sive feature selection. Our second approach
is fully Bayesian and derived from the more
general model, hierarchical LDA. We evalu-
ate the performance of both models against a
flat 1-gram baseline and show improvements
in terms of perplexity over held-out data.
1 Introduction
The work presented in this paper is aimed at lever-
aging a manually created document ontology to
model the content of an underlying document col-
lection. While the primary usage of ontologies is
as a means of organizing and navigating document
collections, they can also help in inferring a signif-
icant amount of information about the documents
attached to them, including path-level, statistical,
representations of content, and fine-grained views
on the level of specificity of the language used in
those documents. Our study focuses on the ontology
underlying DMOZ1, a popular Web directory. We
propose two methods for crystalizing a hierarchical
topic model against its hierarchy and show that the
resulting models outperform a flat unigram model in
its predictive power over held-out data.
1http://www.dmoz.org
To construct our hierarchical topic models, we
adopt the mixed membership formalism (Hofmann,
1999; Blei et al, 2010), where a document is rep-
resented as a mixture over a set of word multi-
nomials. We consider the document hierarchy H
(e.g. the DMOZ hierarchy) as a tree where internal
nodes (category nodes) and leaf nodes (documents),
as well as the edges connecting them, are known a
priori. Each node Ni in H is mapped to a multi-
nomial word distribution MultNi , and each path cd
to a leaf node D is associated with a mixture over
the multinonials (MultC0 . . .MultCk ,MultD) ap-
pearing along this path. The mixture components
are combined using a mixing proportion vector
(?C0 . . . ?Ck), so that the likelihood of string w be-
ing produced by path cd is:
p(w|cd) =
|w|?
i=0
|cd|?
j=0
?jp(wi|cd,j) (1)
where:
|cd|?
j=0
?j = 1,?d (2)
In the following, we propose two models that fit
in this framework. We describe how they allow the
derivation of both p(wi|cd,j) and ? and present early
experimental results showing that explicit hierarchi-
cal information of content can indeed be used as a
basis for content modeling purposes.
2 Related Work
While several efforts have focused on the DMOZ
corpus, often as a reference for Web summarization
670
tasks (Berger and Mittal, 2000; Delort et al, 2003)
or Web clustering tasks (Ramage et al, 2009b), very
little research has attempted to make use of its hier-
archy as is. The work by Sun et al (2005), where
the DMOZ hierarchy is used as a basis for a hierar-
chical lexicon, is closest to ours although their con-
tribution is not a full-fledged content model, but a
selection of highly salient vocabulary for every cat-
egory of the hierarchy. The problem considered in
this paper is connected to the area of Topic Modeling
(Blei and Lafferty, 2009) where the goal is to reduce
the surface complexity of text documents by mod-
eling them as mixtures over a finite set of topics2.
While the inferred models are usually flat, in that
no explicit relationship exists among topics, more
complex, non-parametric, representations have been
proposed to elicit the hierarchical structure of vari-
ous datasets (Hofmann, 1999; Blei et al, 2010; Li
et al, 2007). Our purpose here is more specialized
and similar to that of Labeled LDA (Ramage et al,
2009a) or Fixed hLDA (Reisinger and Pas?ca, 2009)
where the set of topics associated with a document is
known a priori. In both cases, document labels are
mapped to constraints on the set of topics on which
the - otherwise unaltered - topic inference algorithm
is to be applied. Lastly, while most recent develop-
ments have been based on unsupervised data, it is
also worth mentioning earlier approaches like Topic
Signatures (Lin and Hovy, 2000) where words (or
phrases) characteristic of a topic are identified using
a statistical test of dependence. Our first model ex-
tends this approach to the hierarchical setting, build-
ing actual topic models based on the selected vocab-
ulary.
3 Information-Theoretic Approach
The assumption that topics are known a-priori al-
lows us to extend the concept of Topic Signatures to
a hierarchical setting. Lin and Hovy (2000) describe
a Topic Signature as a list of words highly correlated
with a target concept, and use a ?2 estimator over
labeled data to decide as to the allocation of a word
to a topic. Here, the sub-categories of a node corre-
spond to the topics. However, since the hierarchy is
naturally organized in a generic-to-specific fashion,
2Here we use the term topic to describe a normalized distri-
bution over a fixed vocabulary V .
for each node we select words that have the least dis-
criminative power between the node?s children. The
rationale is that, if a word can discriminate well be-
tween one child and all others, then it belongs in that
child?s node.
3.1 Word Assignment
The algorithm proceeds in two phases. In the first
phase, the hierarchy tree is traversed in a bottom-up
fashion to compile word frequency information un-
der each node. In the second phase, the hierarchy
is traversed top-down and, at each step, words get
assigned to the current node based on whether they
can discriminate between the current node?s chil-
dren. Once a word has been assigned on a given
path, it can no longer be assigned to any other node
on this path. Thus, within a path, a word always
takes on the meaning of the one topic to which it has
been assigned.
The discriminative power of a term with respect
to node N is formalized based on one of the follow-
ing measures:
Entropy of the a posteriori children category dis-
tribution for a given w.
Ent(w) = ?
?
C?Sub(N)
p(C|w) log(p(C|w) (3)
Cross-Entropy between the a priori children cat-
egory distribution and the a posteriori children cate-
gories distribution conditioned on the appearance of
w.
CrossEnt(w) = ?
?
C?Sub(N)
p(C) log(p(C|w)) (4)
?2 score, similar to Lin and Hovy (2000) but ap-
plied to classification tasks that can involve an ar-
bitrary number of (sub-)categories. The number of
degrees of freedom of the ?2 distribution is a func-
tion of the number of children.
?2(w) =
?
i?{w,w}
?
C?Sub(N)
(nC(i)? p(C)p(i))2
p(C)p(i)
(5)
To identify words exhibiting an unusually low dis-
criminative power between the children categories,
we assume a gaussian distribution of the score used
and select those whose score is at least ? = 2 stan-
dard deviations away from the population mean3.
3Although this makes the decision process less arbitrary
671
Algorithm 1 Generative process for hLLDA
? For each topic t ? H
? Draw ?t = (?t,1, . . . , ?t,V )T ? Dir(?|?)
? For each document, d ? {1, 2 . . .K}
? Draw a random path assignment cd ? H
? Draw a distribution over levels along cd, ?d ?
Dir(?|?)
? Draw a document length n ? ?H
? For each word wd,i ? {wd,1, wd,2, . . . wd,n},
? Draw level zd,i ?Mult(?d)
? Draw word wd,i ?Mult(?cd [zd,i])
3.2 Topic Definition & Mixing Proportions
Based on the final word assignments, we estimate
the probability of word wi in topic Tk, as:
P (wi|Tk) =
nCk(wi)
nCk
(6)
with nCk(wi) the total number of occurrence of wi
in documents under Ck, and nCk the total number of
words in documents under Ck.
Given the individual word assignments we eval-
uate the mixing proportions using corpus-level esti-
mates, which are computed by averaging the mixing
proportions of all the training documents.
4 Hierarchical Bayesian Approach
The previous approach, while attractive in its sim-
plicity, makes a strong claim that a word can be
emitted by at most one node on any given path. A
more interesting model might stem from allowing
soft word-topic assignments, where any topic on the
document?s path may emit any word in the vocabu-
lary space.
We consider a modified version of hierarchical
LDA (Blei et al, 2010), where the underlying tree
structure is known a priori and does not have to
be inferred from data. The generative story for this
model, which we designate as hierarchical Labeled-
LDA (hLLDA), is shown in Algorithm 1. Just as
with Fixed Structure LDA4 (Reisinger and Pas?ca,
than with a hand-selected threshold, this raises the issue of iden-
tifying the true distribution for the estimator used.
4Our implementation of hLLDA was partially
based on the UTML toolkit which is available at
https://github.com/joeraii/
2009), the topics used for inference are, for each
document, those found on the path from the hierar-
chy root to the document itself. Once the target path
cd ? H is known, the model reduces to LDA over
the set of topics comprising cd. Given that the joint
distribution p(?, z, w|cd) is intractable (Blei et al,
2003), we use collapsed Gibbs-sampling (Griffiths
and Steyvers, 2004) to obtain individual word-level
assignments. The probability of assigning wi, the
ith word in document d, to the jth topic on path cd,
conditioned on all other word assignments, is given
by:
p(zi = j|z?i,w, cd) ?
nd?i,j + ?
|cd|(?+ 1)
?
nwi?i,j + ?
V (? + 1)
(7)
where nd?i,j is the frequency of words from docu-
ment d assigned to topic j, nwi?i,j is the frequency
of word wi in topic j, ? and ? are Dirichlet con-
centration parameters for the path-topic and topic-
word multinomials respectively, and V is the vocab-
ulary size. Equation 7 can be understood as defin-
ing the unormalized posterior word-level assignment
distribution as the product of the current level mix-
ing proportion ?i and of the current estimate of the
word-topic conditional probability p(wi|zi). By re-
peatedly resampling from this distribution we ob-
tain individual word assignments which in turn al-
low us to estimate the topic multinomials and the
per-document mixing proportions. Specifically, the
topic multinomials are estimated as:
?cd[j],i = p(wi|zcd[j]) =
nwizcd[j] + ??
n?zcd[j] + V ?
(8)
while the per-document mixing proportions ?d can
be estimated as:
?d,j ?
nd?,j + ?
nd + |cd|?
,?j ? 1, . . . , cd (9)
Although we experimented with hyper-parameter
learning (Dirichlet concentration parameter ?), do-
ing so did not significantly impact the final model.
The results we report are therefore based on stan-
dard values for the hyper-parameters (? = 1 and
? = 0.1).
5 Experimental Results
We compared the predictive power of our model to
that of several language models. In every case, we
672
compute the perplexity of the model over the held-
out dataW = {w1 . . .wn} given the modelM and
the observed (training) data, namely:
perplM(W) = exp(?
1
n
n?
i=1
1
|wi|
|wi|?
j=1
log pM(wi,j))
(10)
5.1 Data Preprocessing
Our experiments focused on the English portion of
the DMOZ dataset5 (about 2.1 million entries). The
raw dataset was randomized and divided according
to a 98% training (31M words), 1% development
(320k words), 1% testing (320k words) split. Gists
were tokenized using simple tokenization rules, with
no stemming, and were case-normalized. Akin to
Berger and Mittal (2000) we mapped numerical to-
kens to the NUM placeholder and selected the V =
65535 most frequent words as our vocabulary. Any
token outside of this set was mapped to the OOV to-
ken. We did not perform any stop-word filtering.
5.2 Reference Models
Our reference models consists of several n-gram
(n ? [1, 3]) language models, none of which makes
use of the hierarchical information available from
the corpus. Under these models, the probability of
a given string is given by:
p(w) =
|s|?
i=1
p(wi|wi?1, . . . ,wi?(n?1)) (11)
We used the SRILM toolkit (Stolcke, 2002), en-
abling Kneser-Ney smoothing with default param-
eters.
Note that an interesting model to include here
would have been one that jointly infers a hierarchy
of topics as well as the topics that comprise it, much
like the regular hierarchical LDA algorithm (Blei et
al., 2010). While we did not perform this experiment
as part of this work, this is definitely an avenue for
future work. We are especially interested in seeing
whether an automatically inferred hierarchy of top-
ics would fundamentally differ from the manually-
curated hierarchy used by DMOZ.
5We discarded the Top/World portion of the hierarchy.
5.3 Experimental Results
The perplexities obtained for the hierarchical and n-
gram models are reported in Table 1.
reg all
# documents 1153000 2083949
avg. gist length 15.47 15.36
1-gram 1644.10 1414.98
2-gram 352.10 287.09
3-gram 239.08 179.71
entropy 812.91 1037.70
cross-entropy 1167.07 1869.90
?2 1639.29 1693.76
hLLDA 941.16 983.77
Table 1: Perplexity of the hierarchical models and the
reference n-gram models over the entire DMOZ dataset
(all), and the non-Regional portion of the dataset (reg).
When taken on the entire hierarchy (all), the per-
formance of the Bayesian and entropy-based mod-
els significantly exceeds that of the 1-gram model
(significant under paired t-test, both with p-value <
2.2 ? 10?16) while remaining well below that of ei-
ther the 2 or 3 gram models. This suggests that, al-
though the hierarchy plays a key role in the appear-
ance of content in DMOZ gists, word context is also
a key factor that needs to be taken into account: the
two families of models we propose are based on the
bag-of-word assumption and, by design, assume that
words are drawn i.i.d. from an underlying distribu-
tion. While it is not clear how one could extend the
information-theoretic models to include such con-
text, we are currently investigating enhancements to
the hLLDA model along the lines of the approach
proposed in Wallach (2006).
A second area of analysis is to compare the per-
formance of the various models on the entire hier-
archy versus on the non-Regional portion of the tree
(reg). We can see that the perplexity of the proposed
models decreases while that of the flat n-grams mod-
els increase. Since the non-Regional portion of the
DMOZ hierarchy is organized more consistently in
a semantic fashion6, we believe this reflects the abil-
ity of the hierarchical models to take advantage of
6The specificity of the Regional sub-tree has also been dis-
cussed by previous work (Ramage et al, 2009b), justifying a
special treatment for that part of the DMOZ dataset.
673
Figure 1: Perplexity of the proposed algorithms against the 1-gram baseline for each of the 14 top level DMOZ cate-
gories: Arts, Business, Computer, Games, Health, Home, News, Recreation, Reference, Regional, Science, Shopping,
Society, Sports.
the corpus structure to represent the content of the
summaries. On the other hand, the Regional por-
tion of the dataset seems to contribute a significant
amount of noise to the hierarchy, leading to a loss in
performance for those models.
We can observe that while hLLDA outperforms
all information-theoretical models when applied to
the entire DMOZ corpus, it falls behind the entropy-
based model when restricted to the non-regional
section of the corpus. Also if the reduction in
perplexity remains limited for the entropy, ?2 and
hLLDA models, the cross-entropy based model in-
curs a more significant boost in performance when
applied to the more semantically-organized portion
of the corpus. The reason behind such disparity in
behavior is not clear and we plan on investigating
this issue as part of our future work.
Further analyzing the impact of the respective
DMOZ sub-sections, we show in Figure 1 re-
sults for the hierarchical and 1-gram models when
trained and tested over the 14 main sub-trees of
the hierarchy. Our intuition is that differences
in the organization of those sub-trees might af-
fect the predictive power of the various mod-
els. Looking at sub-trees we can see that the
trend is the same for most of them, with the best
level of perplexity being achieved by the hierar-
chical Bayesian model, closely followed by the
information-theoretical model using entropy as its
selection criterion.
6 Conclusion
In this paper we have demonstrated the creation of a
topic-model of Web summaries using the hierarchy
of a popular Web directory. This hierarchy provides
a backbone around which we crystalize hierarchical
topic models. Individual topics exhibit increasing
specificity as one goes down a path in the tree. While
we focused on Web summaries, this model can be
readily adapted to any Web-related content that can
be seen as a mixture of the component topics appear-
ing along a paths in the hierarchy. Such model can
become a key resource for the fine-grained distinc-
tion between generic and specific elements of lan-
guage in a large, heterogenous corpus.
Acknowledgments
This material is based on research supported in part
by the U.S. National Science Foundation (NSF) un-
der IIS-05-34871. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF.
674
References
A. Berger and V. Mittal. 2000. Ocelot: a system for
summarizing web pages. In Proceedings of the 23rd
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR?00), pages 144?151.
David M. Blei and J. Lafferty. 2009. Topic models. In A.
Srivastava and M. Sahami, editors, Text Mining: The-
ory and Applications. Taylor and Francis.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
David M. Blei, Thomas L. Griffiths, and Micheal I. Jor-
dan. 2010. The nested chinese restaurant process and
bayesian nonparametric inference of topic hierarchies.
In Journal of ACM, volume 57.
Jean-Yves Delort, Bernadette Bouchon-Meunier, and
Maria Rifqi. 2003. Enhanced web document sum-
marization using hyperlinks. In Hypertext 2003, pages
208?215.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Thomas Hofmann. 1999. The cluster-abstraction model:
Unsupervised learning of topic hierarchies from text
data. In Proceedings of IJCAI?99.
Wei Li, David Blei, and Andrew McCallum. 2007. Non-
parametric bayes pachinko allocation. In Proceedings
of the Proceedings of the Twenty-Third Conference An-
nual Conference on Uncertainty in Artificial Intelli-
gence (UAI-07), pages 243?250, Corvallis, Oregon.
AUAI Press.
C.-Y. Lin and E. Hovy. 2000. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics, pages 495?501.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009a. Labeled lda: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009), Singapore, pages 248?256.
Daniel Ramage, Paul Heymann, Christopher D. Man-
ning, and Hector Garcia-Molina. 2009b. Clustering
the tagged web. In Proceedings of the Second ACM In-
ternational Conference on Web Search and Data Min-
ing, WSDM ?09, pages 54?63, New York, NY, USA.
ACM.
Joseph Reisinger and Marius Pas?ca. 2009. Latent vari-
able models of concept-attribute attachment. In ACL-
IJCNLP ?09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2, pages 620?628, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pages 901?904, September.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In SIGIR 2005,
pages 194?201.
Hanna M. Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of the 23rd International
Conference on Machine Learning, Pittsburgh, Penn-
sylvania, U.S., pages 977?984.
675
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 69?73,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Aggregated Word Pair Features for Implicit Discourse Relation
Disambiguation
Or Biran
Columbia University
Department of Computer Science
orb@cs.columbia.edu
Kathleen McKeown
Columbia University
Department of Computer Science
kathy@cs.columbia.edu
Abstract
We present a reformulation of the word
pair features typically used for the task
of disambiguating implicit relations in the
Penn Discourse Treebank. Our word pair
features achieve significantly higher per-
formance than the previous formulation
when evaluated without additional fea-
tures. In addition, we present results
for a full system using additional features
which achieves close to state of the art per-
formance without resorting to gold syntac-
tic parses or to context outside the relation.
1 Introduction
Discourse relations such as contrast and causal-
ity are part of what makes a text coherent. Be-
ing able to automatically identify these relations
is important for many NLP tasks such as gener-
ation, question answering and textual entailment.
In some cases, discourse relations contain an ex-
plicit marker such as but or because which makes
it easy to identify the relation. Prior work (Pitler
and Nenkova, 2009) showed that where explicit
markers exist, the class of the relation can be dis-
ambiguated with f-scores higher than 90%.
Predicting the class of implicit discourse rela-
tions, however, is much more difficult. Without an
explicit marker to rely on, work on this task ini-
tially focused on using lexical cues in the form
of word pairs mined from large corpora where
they appear around an explicit marker (Marcu and
Echihabi, 2002). The intuition is that these pairs
will tend to represent semantic relationships which
are related to the discourse marker (for example,
word pairs often appearing around but may tend
to be antonyms). While this approach showed
some success and has been used extensively in
later work, it has been pointed out by multiple
authors that many of the most useful word pairs
are pairs of very common functional words, which
contradicts the original intuition, and it is hard to
explain why these are useful.
In this work we focus on the task of identi-
fying and disambiguating implicit discourse rela-
tions which have no explicit marker. In particular,
we present a reformulation of the word pair fea-
tures that have most often been used for this task
in the past, replacing the sparse lexical features
with dense aggregated score features. This is the
main contribution of our paper. We show that our
formulation outperforms the original one while re-
quiring less features, and that using a stop list of
functional words does not significantly affect per-
formance, suggesting that these features indeed
represent semantically related content word pairs.
In addition, we present a system which com-
bines these word pairs with additional features to
achieve near state of the art performance without
the use of syntactic parse features and of context
outside the arguments of the relation. Previous
work has attributed much of the achieved perfor-
mance to these features, which are easy to get in
the experimental setting but would be less reliable
or unavailable in other applications.1
2 Related Work
This line of research began with (Marcu and Echi-
habi, 2002), who used a small number of unam-
biguous explicit markers and patterns involving
them, such as [Arg1, but Arg2] to collect sets of
word pairs from a large corpus using the cross-
product of the words in Arg1 and Arg2. The au-
thors created a feature out of each pair and built a
naive bayes model directly from the unannotated
corpus, updating the priors and posteriors using
maximum likelihood. While they demonstrated
1Reliable syntactic parses are not always available in do-
mains other than newswire, and context (preceding relations,
especially explicit relations) is not always available in some
applications such as generation and question answering.
69
some success, their experiments were run on data
that is unnatural in two ways. First, it is balanced.
Second, it is constructed with the same unsuper-
vised method they use to extract the word pairs -
by assuming that the patterns correspond to a par-
ticular relation and collecting the arguments from
an unannotated corpus. Even if the assumption is
correct, these arguments are really taken from ex-
plicit relations with their markers removed, which
as others have pointed out (Blair-Goldensohn et
al., 2007; Pitler et al, 2009) may not look like true
implicit relations.
More recently, implicit relation prediction has
been evaluated on annotated implicit relations
from the Penn Discourse Treebank (Prasad et al,
2008). PDTB uses hierarchical relation types
which abstract over other theories of discourse
such as RST (Mann and Thompson, 1987) and
SDRT (Asher and Lascarides, 2003). It contains
40, 600 annotated relations from the WSJ corpus.
Each relation has two arguments, Arg1 and Arg2,
and the annotators decide whether it is explicit or
implicit.
The first to evaluate directly on PDTB in a re-
alistic setting were Pitler et al (2009). They used
word pairs as well as additional features to train
four binary classifiers, each corresponding to one
of the high-level PDTB relation classes. Although
other features proved to be useful, word pairs were
still the major contributor to most of these clas-
sifiers. In fact, their best system for comparison
included only the word pair features, and for all
other classes other than expansion the word pair
features alone achieved an f-score within 2 points
of the best system. Interestingly, they found that
training the word pair features on PDTB itself was
more useful than training them on an external cor-
pus like Marcu and Echihabi (2002), although in
some cases they resort to information gain in the
external corpus for filtering the word pairs.
Zhou et al (2010) used a similar method and
added features that explicitly try to predict the
implicit marker in the relation, increasing perfor-
mance. Most recently to the best of our knowl-
edge, Park and Cardie (2012) achieved the highest
performance by optimizing the feature set. An-
other work evaluating on PDTB is (Lin et al,
2009), who are unique in evaluating on the more
fine-grained second-level relation classes.
3 Word Pairs
3.1 The Problem: Sparsity
While Marcu and Echihabi (2002)?s approach of
training a classifier from an unannotated corpus
provides a relatively large amount of training data,
this data does not consist of true implicit relations.
However, the approach taken by Pitler et al (2009)
and repeated in more recent work (training directly
on PDTB) is problematic as well: when training a
model with so many sparse features on a dataset
the size of PDTB (there are 22, 141 non-explicit
relations overall), it is likely that many important
word pairs will not be seen in training.
In fact, even the larger corpus of Marcu and
Echihabi (2002) may not be quite large enough
to solve the sparsity issue, given that the num-
ber of word pairs is quadratic in the vocabulary.
Blair-Goldensohn et al (2007) report that using
even a very small stop list (25 words) significantly
reduces performance, which is counter-intuitive.
They attribute this finding to the sparsity of the
feature space. An analysis in (Pitler et al, 2009)
also shows that the top word pairs (ranked by
information gain) all contain common functional
words, and are not at all the semantically-related
content words that were imagined. In the case
of some reportedly useful word pairs (the-and; in-
the; the-of...) it is hard to explain how they might
affect performance except through overfitting.
3.2 The Solution: Aggregation
Representing each word pair as a single feature has
the advantage of allowing the weights for each pair
to be learned directly from the data. While pow-
erful, this approach requires large amounts of data
to be effective.
Another possible approach is to aggregate some
of the pairs together and learn weights from the
data only for the aggregated sets of words. For this
approach to be effective, the pairs we choose to
group together should have similar meaning with
regard to predicting the relation.
Biran and Rambow (2011) is to our knowledge
the only other work utilizing a similar approach.
They used aggregated word pair set features to
predict whether or not a sentence is argumentative.
Their method is to group together word pairs that
have been collected around the same explicit dis-
course marker: for every discourse marker such
as therefore or however, they have a single fea-
ture whose value depends only on the word pairs
70
collected around that marker. This is reasonable
given the intuition that the marker pattern is unam-
biguous and points at a particular relation. Using
one feature per marker can be seen as analogous
(yet complementary) to Zhou et al (2010)?s ap-
proach of trying to predict the implicit connective
by giving a score to each marker using a language
model.
This work uses binary features which only in-
dicate the appearance of one or more of the pairs.
The original frequencies of the word pairs are not
used anywhere. A more powerful approach is to
use an informed function to weight the word pairs
used inside each feature.
3.3 Our Approach
Our approach is similar in that we choose to ag-
gregate word pairs that were collected around the
same explicit marker. We first assembled a list of
all 102 discourse markers used in PDTB, in both
explicit and implicit relations.2
Next, we extract word pairs for each marker
from the Gigaword corpus by taking the cross
product of words that appear in a sentence around
that marker. This is a simpler approach than us-
ing patterns - for example, the marker because can
appear in two patterns: [Arg1 because Arg2] and
[because Arg1, Arg2], and we only use the first.
We leave the task of listing the possible patterns
for each of the 102 markers to future work because
of the significant manual effort required. Mean-
while, we rely on the fact that we use a very large
corpus and hope that the simple pattern [Arg1
marker Arg2] is enough to make our features use-
ful. There are, of course, markers for which this
pattern does not normally apply, such as by com-
parison or on one hand. We expect these features
to be down-weighted by the final classifier, as ex-
plained at the end of this section. When collect-
ing the pairs, we stem the words and discard pairs
which appear only once around the marker.
We can think of each discourse marker as hav-
ing a corresponding unordered ?document?, where
each word pair is a term with an associated fre-
quency. We want to create a feature for each
marker such that for each data instance (that is,
for each potential relation in the PDTB data) the
value for the feature is the relevance of the marker
document to the data instance.
2in implicit relations, there is no marker in the text but the
implicit marker is provided by the human annotators
Each data instance in PDTB consists of two ar-
guments, and can therefore also be represented
as a set of word pairs extracted from the cross-
product of the two arguments. To represent the rel-
evance of the instance to each marker, we set the
value of the marker feature to the cosine similarity
of the data instance and the marker?s ?document?,
where each word pair is a dimension.
While the terms (i.e. word pairs) of the
data instance are weighted by simple occurence
count, we weight the terms in each marker?s
document with tf-idf, where tf is defined in
one of two ways: normalized term frequency
( count(t)max{count(s,d):s?d}) and pointwise mutual infor-
mation (log count(t)count(w1)?count(w2)), where w1 and w2are the member words of the pair. Idf is calculated
normally given that the set of all documents is de-
fined as the 102 marker documents.
We then train a binary classifier (logistic regres-
sion) using these 102 features for each of the four
high-level relations in PDTB: comparison, con-
tingency, expansion and temporal. To make sure
our results are comparable to previous work, we
treat EntRel relations as instances of expansion
and use sections 2-20 for training and sections 21-
22 for testing. We use a ten fold stratified cross-
validation of the training set for development. Ex-
plicit relations are excluded from all data sets.
As mentioned earlier, there are markers that do
not fit the simple pattern we use. In particular,
some markers always or often appear as the first
term of a sentence. For these, we expect the list of
word pairs to be empty or almost empty, since in
most sentences there are no words on the left (and
recall that we discard pairs that appear only once).
Since the features created for these markers will
be uninformative, we expect them to be weighted
down by the classifier and have no significant ef-
fect on prediction.
4 Evaluation of Word Pairs
For our main evaluation, we evaluate the perfor-
mance of word pair features when used with no
additional features. Results are shown in Table 1.
Our word pair features outperform the previous
formulation (represented by the results reported by
(Pitler et al, 2009), but used by virtually all previ-
ous work on this task). For most relation classes,
tf is significantly better than pmi. 3
3Significance was verified for our own results in all exper-
iments shown in this paper with a standard t-test
71
Comparison Contingency Expansion Temporal
Pitler et al, 2009 21.96 (56.59) 45.6 (67.1) 63.84 (60.28) 16.21 (61.98)
tf-idf, no stop list 23 (61.72) 44.03 (66.78) 66.48 (60.93) 19.54 (68.09)
pmi-idf, no stop list 24.38 (61.72) 38.96 (61.52) 62.22 (57.26) 16 (65.53)
tf-idf, with stop list 23.77 44.33 65.33 16.98
Table 1: Main evaluation. F-measure (accuracy) for various implementations of the word pairs features
Comparison Contingency Expansion Temporal
Best System 25.4 (63.36) 46.94 (68.09) 75.87 (62.84) 20.23 (68.35)
features used pmi+1,2,3,6 tf+ALL tf+8 tf+3,9
Pitler et al, 2009 21.96 (56.59) 47.13 (67.3) 76.42 (63.62) 16.76 (63.49)
Zhou et al, 2010 31.79 (58.22) 47.16 (48.96) 70.11 (54.54) 20.3 (55.48)
Park and Cardie, 2012 31.32 (74.66) 49.82 (72.09) 79.22 (69.14) 26.57 (79.32)
Table 2: Secondary evaluation. F-measure (accuracy) for the best systems. tf and pmi refer to the word
pair features used (by tf implementation), and the numbers refer to the indeces of Table 3
Comp. Cont. Exp. Temp.
1 WordNet 20.07 34.07 52.96 11.58
2 Verb Class 14.24 24.84 49.6 10.04
3 MPN 23.84 38.58 49.97 13.16
4 Modality 17.49 28.92 13.84 10.72
5 Polarity 16.46 26.36 65.15 11.58
6 Affect 18.62 31.59 59.8 13.37
7 Similarity 20.68 34.5 43.16 12.1
8 Negation 8.28 22.47 75.87 11.1
9 Length 20.75 31.28 65.72 10.19
Table 3: F-measure for each feature category
We also show results using a stop list of 50 com-
mon functional words. The stop list has only a
small effect on performance except in the tempo-
ral class. This may be because of functional words
like was and will which have a temporal effect.
5 Other Features
For our secondary evaluation, we include addi-
tional features to complement the word pairs. Pre-
vious work has relied on features based on the gold
parse trees of the Penn Treebank (which overlaps
with PDTB) and on contextual information from
relations preceding the one being disambiguated.
We intentionally limit ourselves to features that do
not require either so that our system can be readily
used on arbitrary argument pairs.
WordNet Features: We define four features
based on WordNet (Fellbaum, 1998) - Synonyms,
Antonyms, Hypernyms and Hyponyms. The values
are the counts of word pairs in the cross-product of
the words in the arguments that have the particular
relation (synonymy, antonymy etc) between them.
Verb Class: This is the count of pairs of verbs
from Arg1 and Arg2 that share the same class, de-
fined as the highest level Levin verb class (Levin,
1993) from the LCS database (Dorr, 2001).
Money, Percentages and Numbers (MPN): The
counts of currency symbols/abbreviations, per-
centage signs or cues (?percent?, ?BPS?...) and
numbers in each argument.
Modality: Presence or absence of each English
modal in each argument.
Polarity: Based on MPQA (Wilson et al, 2005).
We include the counts of positive and negative
words according to the MPQA subjectivity lexicon
for both arguments. Unlike Pitler et al (2009), we
do not use neutral polarity features. We also do not
explicitly group negation with polarity (although
we do have separate negation features).
Affect: Based on the Dictionary of Affect in Lan-
guage (Whissell, 1989). Each word in the DAL
gets a score for three dimensions - pleasantness
(pleasant - unpleasant), activation (passive - ac-
tive) and imagery (hard to imagine - easy to imag-
ine). We use the average score for each dimension
in each argument as a feature.
Content Similarity: We use the cosine similarity
and word overlap of the arguments as features.
Negation: Presence or absence of negation terms
in each of the arguments.
Length: The ratio between the lengths (counts of
words) of the arguments.
6 Evaluation of Additional Features
For our secondary evaluation, we present results
for each feature category on its own in Table 3 and
for our best system for each of the relation classes
in Table 2. We show results for the best systems
from (Pitler et al, 2009), (Zhou et al, 2010) and
72
(Park and Cardie, 2012) for comparison.
7 Conclusion
We presented an aggregated approach to word pair
features and showed that it outperforms the previ-
ous formulation for all relation types but contin-
gency. This is our main contribution. With this
approach, using a stop list does not have a major
effect on results for most relation classes, which
suggests most of the word pairs affecting perfor-
mance are content word pairs which may truly be
semantically related to the discourse structure.
In addition, we introduced the new and useful
WordNet, Affect, Length and Negation feature cat-
egories. Our final system outperformed the best
system from Pitler et al (2009), who used mostly
similar features, for comparison and temporal and
is competitive with the most recent state of the
art systems for contingency and expansion with-
out using any syntactic or context features.
Acknowledgments
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Studies in Natural Language Process-
ing Series. Cambridge University Press.
Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialog by classifying text as argu-
mentative. International Journal of Semantic Com-
puting, 5(4):363?381, December.
Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and refin-
ing rhetorical-semantic relation models. In HLT-
NAACL, pages 428?435. The Association for Com-
putational Linguistics.
Bonnie J. Dorr. 2001. LCS Verb Database, Online
Software Database of Lexical Conceptual Structures
and Documentation. University Of Maryland Col-
lege Park.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University Of
Chicago Press.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 343?351.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In ACL, pages 368?375. ACL.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 108?112.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In ACL/IJCNLP (Short Papers), pages 13?16. The
Association for Computer Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In ACL/IJCNLP, pages 683?691. The
Association for Computer Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.
Cynthia M. Whissell. 1989. The dictionary of affect in
language.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347?354.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics.
73
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 13?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Corpus Creation for New Genres:
A Crowdsourced Approach to PP Attachment
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{mj2472,jda2129}@columbia.edu, {kapil,sara,kathy}@cs.columbia.edu
Abstract
This paper explores the task of building an ac-
curate prepositional phrase attachment corpus
for new genres while avoiding a large invest-
ment in terms of time and money by crowd-
sourcing judgments. We develop and present
a system to extract prepositional phrases and
their potential attachments from ungrammati-
cal and informal sentences and pose the subse-
quent disambiguation tasks as multiple choice
questions to workers from Amazon?s Mechan-
ical Turk service. Our analysis shows that
this two-step approach is capable of producing
reliable annotations on informal and poten-
tially noisy blog text, and this semi-automated
strategy holds promise for similar annotation
projects in new genres.
1 Introduction
Recent decades have seen rapid development in nat-
ural language processing tools for parsing, semantic
role-labeling, machine translation, etc., and much of
this success can be attributed to the study of statisti-
cal techniques and the availability of large annotated
corpora for training. However, the performance of
these systems is heavily dependent on the domain
and genre of their training data, i.e. systems trained
on data from a particular domain tend to perform
poorly when applied to other domains and adap-
tation techniques are not always able to compen-
sate (Dredze et al, 2007). For this reason, achiev-
ing high performance on new domains and genres
frequently necessitates the collection of annotated
training data from those domains and genres, a time-
consuming and frequently expensive process.
This paper examines the problem of collecting
high-quality annotations for new genres with a focus
on time and cost efficiency. We explore the well-
studied but non-trivial task of prepositional phrase
(PP) attachment and describe a semi-automated sys-
tem for identifying accurate attachments in blog
data, which is frequently noisy and difficult to parse.
PP attachment disambiguation involves finding a
correct attachment for a prepositional phrase in a
sentence. For example, in the sentence ?We went to
John?s house on Saturday?, the phrase ?on Satur-
day? attaches to the verb ?went?. In another exam-
ple, ?We went to John?s house on 12th Street?, the
PP ?on 12th street? attaches to the noun ?John?s
house?. This sort of disambiguation requires se-
mantic knowledge about sentences that is difficult
to glean from their surface form, a problem which
is compounded by the informal nature and irregular
vocabulary of blog text.
In this work, we investigate whether crowd-
sourced human judgments are capable of distin-
guishing appropriate attachments. We present a sys-
tem that simplifies the attachment problem and rep-
resents it in a format that can be intuitively tackled
by humans.
Our approach to this task makes use of a heuristic-
based system built on a shallow parser that identi-
fies the likely words or phrases to which a PP can
attach. To subsequently select the correct attach-
ment, we leverage human judgments from multi-
ple untrained annotators (referred to here as work-
ers) through Amazon?s Mechanical Turk 1, an online
marketplace for work. This two-step approach of-
1http://www.mturk.amazon.com
13
fers distinct advantages: the automated system cuts
down the space of potential attachments effectively
with little error, and the disambiguation task can be
reduced to small multiple choice questions which
can be tackled quickly and aggregated reliably.
The remainder of this paper focuses on the PP at-
tachment task over blog text and our analysis of the
resulting aggregate annotations. We note, however,
that this type of semi-automated approach is poten-
tially applicable to any task which can be reliably
decomposed into independent judgments that un-
trained annotators can tackle (e.g., quantifier scop-
ing, conjunction scope). This work is intended as
an initial step towards the development of efficient
hybrid annotation tools that seamlessly incorporate
aggregate human wisdom alongside effective algo-
rithms.
2 Related Work
Identifying PP attachments is an essential task for
building syntactic parse trees. While this task has
been studied using fully-automated systems, many
of them rely on parse tree output for predicting po-
tential attachments (Ratnaparkhi et al, 1994; Yeh
and Vilain, 1998; Stetina and Nagao, 1997; Zavrel
et al, 1997). However, systems that rely on good
parses are unlikely to perform well on new genres
such as blogs and machine translated texts for which
parse tree training data is not readily available.
Furthermore, the predominant dataset for eval-
uating PP attachment is the RRR dataset (Ratna-
parkhi et al, 1994) which consists of PP attach-
ment cases from the Wall Street Journal portion of
the Penn Treebank. Instead of complete sentences,
this dataset consists of sets of the form {V,N1,P,N2}
where {P,N2} is the PP and {V,N1} are the poten-
tial attachments. This simplification of the PP at-
tachment task to a choice between two alternatives
is unrealistic when considering the potential long-
distance attachments encountered in real-world text.
While blogs and other web text, such as discus-
sion forums and emails, have been studied for a va-
riety of tasks such as information extraction (Hong
and Davison, 2009), social networking (Gruhl et
al., 2004), and sentiment analysis (Leshed and
Kaye, 2006), we are not aware of any previous ef-
forts to gather syntactic data (such as PP attach-
ments) in the genre. Syntactic methods such as
POS tagging, parsing and structural disambiguation
are commonly used when analyzing well-structured
text. Including the use of syntactic information
has yielded improvements in accuracy in speech
recognition (Chelba and Jelenik, 1998; Collins et
al., 2005) and machine translation (DeNeefe and
Knight, 2009; Carreras and Collins, 2009). We an-
ticipate that datasets such as ours could be useful for
such tasks as well.
Amazon?s Mechanical Turk (MTurk) has become
very popular for manual annotation tasks and has
been shown to perform equally well over labeling
tasks such as affect recognition, word similarity, rec-
ognizing textual entailment, event temporal order-
ing and word sense disambiguation, when compared
to annotations from experts (Snow et al, 2008).
While these tasks were small in scale and intended to
demonstrate the viability of annotation via MTurk,
it has also proved effective in large-scale tasks in-
cluding the collection of accurate speech transcrip-
tions (Gruenstein et al, 2009). In this paper we ex-
plore a method for corpus building on a large scale
in order to extend annotation into new domains and
genres.
We previously evaluated crowdsourced PP attach-
ment annotation by using MTurk workers to repro-
duce PP attachments from the Wall Street Journal
corpus (Rosenthal et al, 2010). The results demon-
strated that MTurk workers are capable of identi-
fying PP attachments in newswire text, but the ap-
proach used to generate attachment options is de-
pendent on the existing gold-standard parse trees
and cannot be used on corpora where parse trees are
not available. In this paper, we build on the semi-
automated annotation principle while avoiding the
dependency on parsers, allowing us to apply this
technique to the noisy and informal text found in
blogs.
3 System Description
Our system must both identify PPs and generate a
list of potential attachments for each PP in this sec-
tion. Figure 1 illustrates the structure of the system.
First, the system extracts sentences from scraped
blog data. Text is preprocessed by stripping HTML
tags, advertisements, non-Latin and non-printable
14
PPs
PPs
Question 
 Builder
PP Identifier
Chunker
+
Preprocessor
sentences
Chunked 
Chunked 
sentences
point predictor
Attachment
attachments
Potential
Mechanical
     Turk
Questions
forNew domain
data (Blogs)
Figure 1: Overview of question generation system
characters. Emoticon symbols are removed using a
standard list. 2
The cleaned data is then partitioned into sentences
using the NLTK sentence splitter. 3 In order to
compensate for the common occurrence of informal
punctuation and web-specific symbols in blog text,
we replace all punctuation symbols between quo-
tation marks and parentheses with placeholder tags
(e.g. ?QuestionMark?) during the sentence splitting
process and do the same for website names, time
markers and referring phrases (e.g. @John). Ad-
ditionally, we attempt to re-split sentences at ellipsis
boundaries if they are longer than 80 words and dis-
card them if this fails.
As parsers trained on news corpora tend to per-
form poorly on unstructured texts like blogs, we
rely on a chunker to partition sentences into phrases.
Choosing a good chunker is essential to this ap-
proach: around 35% of the cases in which the cor-
rect attachment is not predicted by the system are
due to chunker error. We experimented with differ-
ent chunkers over a random sample of 50 sentences
before selecting a CRF-based chunker (Phan, 2006)
for its robust performance.
The chunker output is initially processed by fus-
ing together chunks in order to ensure that a single
chunk represents a complete attachment point. Two
consecutive NP chunks are fused if the first contains
an element with a possessive part of speech tag (e.g.
John?s book), while particle chunks (PRT) are fused
with the VP chunks that precede them (e.g. pack
up). These chunked sentences are then processed
to identify PPs and potential attachment points for
them, which can then be used to generate questions
2http://www.astro.umd.edu/?marshall/
smileys.html
3http://www.nltk.org
for MTurk workers.
3.1 PP Extraction
PPs can be classified into two broad categories based
on the number of chunks they contain. A simple
PP consists of only two chunks: a preposition and
one noun phrase, while a compound PP has multi-
ple simple PPs attached to its primary noun phrase.
For example, in the sentence ?I just made some last-
minute changes to the latest issue of our newsletter?,
the PP with preposition ?to? can be considered to be
either the simple PP ?to the latest issue? or the com-
pound PP ?to the latest issue of our newsletter?.
We handle compound PPs by breaking them down
into multiple simple PPs; compound PPs can be re-
covered by identifying the attachments of their con-
stituent simple PPs. Our simple PP extraction al-
gorithm identifies PPs as a sequence of chunks that
consist of one or more prepositions terminating in a
noun phrase or gerund.
3.2 Attachment Point Prediction
A PP usually attaches to the noun or verb phrase pre-
ceding it or, in some cases, can modify a following
clause by attaching to the head verb. We build a set
of rules based on this intuition to pick out the poten-
tial attachments in the sentence; these rules are de-
scribed in Table 1. The rules are applied separately
for each PP in a sentence and in the same sequence
as mentioned in the table (except for rule 4, which
is applied while choosing a chunk using any of the
other rules).
15
Rule Example
1 Choose closest NP and VP preceding the PP. I made modifications to our newsletter.
2 Choose next closest VP preceding the PP if the VP selected in (1)
contains a VBG.
He snatched the disk flying away with one hand.
3 Choose first VP following the PP. On his desk he has a photograph.
4 All chunks inside parentheses are skipped, unless the PP falls within
parentheses.
Please refer to the new book (second edition) for
more notes.
5 Choose anything immediately preceding the PP that is not out of
chunk and has not already been picked.
She is full of excitement.
6 If a selected NP contains the word and, expand it into two options,
one with the full expression and one with only the terms following
and.
He is president and chairman of the board.
7 For PPs in chains of the form P-NP-P-NP (PP-PP), choose all the
NPs in the chain preceding the PP and apply all the above rules
considering the whole chain as a single PP.
They found my pictures of them from the concert.
8 If there are fewer than four options after applying the above rules,
also select the VP preceding the last VP selected, the NP preceding
the last NP selected, and the VP following the last VP picked.
Table 1: List of rules for attachment point predictor. In the examples, PPs are denoted by boldfaced text and potential
attachment options are underlined.
4 Experiments
An experimental study was undertaken to test our
hypothesis that we could obtain reliable annotations
on informal genres using MTurk workers. Here we
describe the dataset and our methods.
4.1 Dataset and Interface
We used a corpus of blog posts made on LiveJour-
nal 4 for system development and evaluation. Only
posts from English-speaking countries (i.e. USA,
Canada, UK, Australia and New Zealand) were con-
sidered for this study.
The interface provided to MTurk workers showed
the sentence on a plain background with the PP high-
lighted and a statement prompting them to pick the
phrase in the sentence that the given PP modified.
The question was followed by a list of options. In
addition, we provided MTurk workers the option to
indicate problems with the given PP or the listed op-
tions. Workers could write in the correct attachment
if they determined that it wasn?t present in the list of
options, or the correct PP if the one they were pre-
sented with was malformed. This allowed them to
correct errors made by the chunker and automated
attachment point predictor. In all cases, workers
were forced to pick the best answer among the op-
tions regardless of errors. We also supplied a num-
4http://www.livejournal.com
ber of examples covering both well-formed and er-
roneous cases to aid them in identifying appropriate
attachments.
4.2 Experimental Setup
For our experiment, we randomly selected 1000
questions from the output produced by the system
and provided each question to five different MTurk
workers, thereby obtaining five different judgments
for each PP attachment case. Workers were paid four
cents per question and the average completion time
per task was 48 seconds. In total $225 was spent
on the full study with $200 spent on the workers and
$25 on MTurk fees.The total time taken for the study
was approximately 16 hours.
A pilot study was carried out with 50 sentences
before the full study to test the annotation interface
and experiment with different ways of presenting the
PP and attachment options to workers. During this
study, we observed that while workers were will-
ing to suggest correct answers or PPs when faced
with erroneous questions, they often opted to not
pick any of the options provided unless the question
was well-formed. This was problematic because, in
many cases, expert annotators were able to identify
the most appropriate attachment option. Therefore,
in the final study we forced them to pick the most
suitable option from the given choices before indi-
cating errors and writing in alternatives.
16
Workers in agreement Number of questions Accuracy Coverage
5 (unanimity) 389 97.43% 41.33%
? 4 (majority) 689 94.63% 73.22%
? 3 (majority) 887 88.61% 94.26%
? 2 (plurality) 906 87.75% 96.28%
Total 941 84.48% 100%
Table 2: Accuracy and coverage over agreement thresholds
5 Evaluation corpus
In order to determine if the MTurk results were re-
liable, worker responses had to be validated by hav-
ing expert annotators perform the same task. For
this purpose, two of the authors annotated the 1000
questions used for the experiment independently and
compared their judgments. Disagreements were ob-
served in 127 cases; these were then resolved by a
pool of non-author annotators. If all three annota-
tors on a case disagreed with each other the question
was discarded; this situation occured 43 times. An
additional 16 questions were discarded because they
did not have a valid PP. For example, ?I am painting
with my blanket on today?. Here ?on today? is in-
correctly extracted as a PP because the particle ?on?
is tagged as a preposition. The rest of the analysis
presented in this section was performed on the re-
maining 941 sentences.
The annotators? judgments were compared to the
answers provided by the MTurk workers and, in
the case of disagreement between the experts and
the majority of workers, the sentences were man-
ually inspected to determine the reason. In five
cases, more than one valid attachment was possi-
ble; for example, in the sentence ?The video below is
of my favourite song on the album - A Real Woman?,
the PP ?of my favourite song? could attach to either
the noun phrase ?the video? or the verb ?is? and con-
veys the same meaning. In such cases, both the ex-
perts and the workers were considered to have cho-
sen the correct answer.
In 149 cases, the workers also augmented their
choices by providing corrections to incomplete an-
swers and badly constructed PPs. For example,
the PP ?of the Rings and Mikey? in the sentence
?Samwise from Lord of the Rings and Mikey from
The Goonies are the same actor ?? was corrected to
?of the Rings?. In 34/39 of the cases where the cor-
rect answer was not present in the options provided,
at least one worker indicated correct attachment for
the PP.
5.1 Attachment Prediction Evaluation
We measure the recall for our attachment point pre-
dictor as the number of questions for which the cor-
rect attachment appeared among the generated op-
tions divided by the total number of questions. The
system achieves a recall of 95.85% (902/941 ques-
tions). We observed that in many cases where the
correct attachment point was not predicted, it was
due to a chunker error. For example, in the following
sentence, ?Stop all the clocks , cut off the telephone
, Prevent the dog from barking with a juicy bone...?,
the PP ?from barking? attaches to the verb ?Pre-
vent?; however, due to an error in chunking ?Pre-
vent? is tagged as a noun phrase and hence is not
picked by our system. The correct attachment was
also occasionally missed when the attachment point
was too far from the PP. For example, in the sentence
?Fitting as many people as possible on one sofa and
under many many covers and getting intimate?, the
correct attachment for the PP ?under many many
covers? is the verb ?Fitting? but it is not picked by
our system.
Even though the correct attachment was not al-
ways given, the workers could still provide their own
correct answer. In the first example above, 3/5 work-
ers indicated that the correct attachment was not in
the list of options and wrote it in.
6 Results
Table 2 summarizes the results of the experiment.
We assess both the coverage and reliability of
worker predictions at various levels of worker agree-
ment. This serves as an indicator of the effective-
ness of the MTurk results: the accuracy can be taken
17
Figure 2: The number of questions in which exactly x
workers provided the correct answer
as a general confidence measure for worker predic-
tions; when five workers agree we can be 97.43%
confident in the correctness of their prediction, when
at least four workers agree we can be 94.63% con-
fident, etc. Unanimity indicates that all workers
agreed on an answer, majority indicates that more
than half of workers agreed on an answer, and plu-
rality indicates that two workers agreed on a single
answer, while the remaining three workers each se-
lected different answers. We observe that at high
levels of worker agreement, we get extremely high
accuracy but limited coverage of the data set; as
we decrease our standard for agreement, coverage
increases rapidly while accuracy remains relatively
high.
Figure 2 shows the number of workers providing
the correct answer on a per-question basis. This
illustrates the distribution of worker agreements
across questions. Note that in the majority of cases
(69.2%), at least four workers provided the correct
answer; in only 3.6% of cases were no workers able
to select the correct attachment.
Figure 3 shows the distribution of worker agree-
ments. Unlike Table 2, these figures are not cumu-
lative and include non-plurality two-worker agree-
ments. Note that the number of agreements dis-
cussed in this figure is greater than the 941 evaluated
because in some cases there were multiple agree-
ments on a single question. As an example, three
workers may choose one answer while the remain-
ing two workers choose another; this question then
produces both a three-worker agreement as well as a
two-worker agreement.
Figure 3: The number of cases in which exactly x work-
ers agreed on an answer
No. of options No. of cases Accuracy
< 4 179 86.59%
4 718 84.26%
> 4 44 79.55%
Table 3: Variation in worker performance with the num-
ber of attachment options presented
All questions on which there is agreement also
produce a majority vote, with one exception: the
2/2/1 agreement. Although the correct answer was
selected by one set of two workers in every case of
2/2/1 agreement, this is not particularly useful for
corpus-building as we have no way to identify a pri-
ori which set is correct. Fortunately, 2/2/1 agree-
ments were also quite rare and occurred in only 3%
of cases.
Figure 3 appears to indicate that instances of
agreement between two workers are unlikely to pro-
duce good attachments; they have a an average ac-
curacy of 37.2%. However, this is due in large part
to cases of 3/2 agreement, in which the two workers
in the minority are usually wrong, as well as cases of
2/2/1 agreement which contain at least one incorrect
instance of two-worker agreement. However, if we
only consider cases in which the two-worker agree-
ment forms a plurality (i.e. all other workers dis-
agree amongst themselves), we observe an average
accuracy of 64.3% which is similar to that of cases
of three-worker agreement (67.7%).
We also attempted to study the variation in worker
performance based on the complexity of the task;
specifically looking at how response accuracy var-
ied depending on the number of options that workers
were presented with. Although our system aimed to
18
Figure 4: Variation in accuracy with sentence length.
generate four attachment options per case, fewer op-
tions were produced for small sentences and opening
PPs while additional options were generated in sen-
tences containing PP-NP chains (see Table 1 for the
complete list of rules). Table 3 shows the variation in
accuracy with the number of options provided to the
workers. We might expect that an increased number
of options may be correlated with decreased accu-
racy and the data does indeed seem to suggest this
trend; however, we do not have enough datapoints
for the cases with fewer or more than four options to
verify whether this effect is significant.
We also analyzed the relationship between the
length of the sentence (in terms of number of words)
and the accuracy. Figure 4 indicates that as the
length of the sentence increases, the average accu-
racy decreases. This is not entirely unexpected as
lengthy sentences tend to be more complicated and
therefore harder for human readers to parse.
7 Conclusions and Future Work
We have shown that by working in conjunction
with automated attachment point prediction sys-
tems, MTurk workers are capable of annotating PP
attachment problems with high accuracy, even when
working with unstructured and informal blog text.
This work provides an immediate framework for the
building of PP attachment corpora for new genres
without a dependency on full parsing.
More broadly, the semi-automated framework
outlined in this paper is not limited to the task of
annotating PP attachments; indeed, it is suitable for
almost any syntactic or semantic annotation task
where untrained human workers can be presented
with a limited number of options for selection. By
dividing the desired annotation task into smaller
sub-tasks that can be tackled independently or in a
pipelined manner, we anticipate that more syntac-
tic information can be extracted from unstructured
text in new domains and genres without the sizable
investment of time and money normally associated
with hiring trained linguists to build new corpora.
To this end, we intend to further leverage the advent
of crowdsourcing resources in order to tackle more
sophisticated annotation tasks.
Acknowledgements
The authors would like to thank Kevin Lerman for
his help in formulating the original ideas for this
work. This material is based on research supported
in part by the U.S. National Science Foundation
(NSF) under IIS-05-34871. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of EMNLP, pages 200?209.
Ciprian Chelba and Frederick Jelenik. 1998. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of ACL, pages
507?514.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
EMNLP, pages 727?736.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o Graca, and Fernando Pereira.
2007. Frustratingly hard domain adaptation for depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL, pages 1051?1055,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Alex Gruenstein, Ian McGraw, and Andrew Sutherland.
2009. A self-transcribing speech corpus: collecting
continuous speech with an online educational game.
In Proceedings of the Speech and Language Technol-
ogy in Education (SLaTE) Workshop.
19
Figure 5: HIT Interface for PP attachment task
Daniel Gruhl, R. Guha, David Liben-Nowell, and An-
drew Tomkins. 2004. Information diffusion through
blogspace. In Proceedings of WWW, pages 491?501.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering in
discussion boards. In Proceedings of SIGIR, pages
171?178.
Gilly Leshed and Joseph ?Jofish? Kaye. 2006. Under-
standing how bloggers feel: recognizing affect in blog
posts. In CHI ?06 extended abstracts on Human fac-
tors in computing systems, pages 1019?1024.
Xuan-Hieu Phan. 2006. CRFChunker: CRF
English phrase chunker. http://crfchunker.
sourceforge.net.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of HLT, pages 250?
255.
Sara Rosenthal, William J. Lipovsky, Kathleen McKe-
own, Kapil Thadani, and Jacob Andreas. 2010. Semi-
automated annotation for prepositional phrase attach-
ment. In Proceedings of LREC, Valletta, Malta.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP
attachment ambiguity resolution with a semantic dic-
tionary. In Proceedings of the Workshop on Very Large
Corpora, pages 66?80.
Alexander S. Yeh and Marc B. Vilain. 1998. Some prop-
erties of preposition and subordinate conjunction at-
tachments. In Proceedings of COLING, pages 1436?
1442.
Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1997. Resolving PP attachment ambiguities with
memory-based learning. In Proceedings of the Work-
shop on Computational Language Learning (CoNLL),
pages 136?144.
Appendix A: Mechanical Turk Interface
Figure 5 shows a screenshot of the interface pro-
vided to the Mechanical Turk workers for the PP at-
tachment task. By default, examples and additional
options are hidden but can be viewed using the links
provided. The screenshot illustrates a case in which
a worker is confronted with an incorrect PP and uses
the additional options to correct it.
20
Tense and Aspect Assignment in Narrative Discourse
David K. Elson and Kathleen R. McKeown
Department of Computer Science
Columbia University
{delson,kathy}@cs.columbia.edu
Abstract
We describe a method for assigning English
tense and aspect in a system that realizes sur-
face text for symbolically encoded narratives. Our
testbed is an encoding interface in which proposi-
tions that are attached to a timeline must be real-
ized from several temporal viewpoints. This in-
volves a mapping from a semantic encoding of
time to a set of tense/aspect permutations. The
encoding tool realizes each permutation to give
a readable, precise description of the narrative so
that users can check whether they have correctly
encoded actions and statives in the formal repre-
sentation. Our method selects tenses and aspects
for individual event intervals as well as subinter-
vals (with multiple reference points), quoted and
unquoted speech (which reassign the temporal fo-
cus), and modal events such as conditionals.
1 Introduction
Generation systems that communicate knowledge
about time must select tense and aspect carefully
in their surface realizations. An incorrect assign-
ment can give the erroneous impression that a con-
tinuous action has ended, or that a previous state
is the current reality. In this paper, we consider
English tense and aspect in the generation of nar-
rative discourse, where statives and actions occur
over connected intervals.
We describe two contributions: first, a general
application of theories of tense, aspect and inter-
val logic to a generation context in which we map
temporal relationships to specific tense/aspect se-
lections. Second, we describe an implementation
of this approach in an interactive environment with
a basic sentence planner and realizer. The first re-
sult does not depend on the second.
The purpose of the system is to allow users who
are na??ve to linguistics and knowledge representa-
tion to create semantic encodings of short stories.
To do this, they construct propositions (predicate-
argument structures) through a graphical, menu-
based interface, and assign them to intervals on a
timeline. Figure 1 shows a session in which the
user is encoding a fable of Aesop. The top-right
panel shows the original fable, and the left-hand
panel shows a graphical timeline with buttons for
constructing new propositions at certain intervals.
The left-hand and bottom-right panels contain au-
tomatically generated text of the encoded story, as
the system understands it, from different points of
view. Users rely on these realizations to check that
they have assigned the formal connections cor-
rectly. The tenses and aspects of these sentences
are a key component of this feedback. We describe
the general purpose of the system, its data model,
and the encoding methodology in a separate paper
(Elson and McKeown, 2010).
The paper is organized as follows: After dis-
cussing related work in Section 2, we describe our
method for selecting tense and aspect for single
events in Section 3. Section 4 follows with more
complex cases involving multiple events and shifts
in temporal focus. We then discuss the results.
2 Related Work
There has been intense interest in the interpre-
tation of tense and aspect into a formal under-
standing of the ordering and duration of events.
This work has been in both linguistics (Dowty,
1979; Nerbonne, 1986; Vlach, 1993) and natu-
ral language understanding. Early systems inves-
tigated rule-based approaches to parsing the du-
rations and orderings of events from the tenses
and aspects of their verbs (Hinrichs, 1987; Web-
ber, 1987; Song and Cohen, 1988; Passonneau,
1988). Allen (1984) and Steedman (1995) focus
on distinguishing between achievements (when an
event culminates in a result, such as John builds
a house) and processes (such as walking). More
Figure 1: Screenshot of our story encoding interface.
recent work has centered on markup languages
for complex temporal information (Mani, 2004)
and corpus-based (statistical) models for predict-
ing temporal relationships on unseen text (Mani et
al., 2006; Lapata and Lascarides, 2006).
Our annotation interface requires a fast realizer
that can be easily integrated into an interactive, on-
line encoding tool. We found that developing a
custom realizer as a module to our Java-based sys-
tem was preferable to integrating a large, general
purpose system such as KPML/Nigel (Matthiessen
and Bateman, 1991) or FUF/SURGE (Elhadad
and Robin, 1996). These realizers, along with Re-
alPro (Lavoie and Rambow, 1997), accept tense as
a parameter, but do not calculate it from a semantic
representation of overlapping time intervals such
as ours (though the Nigel grammar can calculate
tense from speech, event, and reference time or-
derings, discussed below). The statistically trained
FERGUS (Chen et al, 2002) contrasts with our
rule-based approach.
Dorr and Gaasterland (1995) and Grote (1998)
focus on generating temporal connectives, such as
before, based on the relative times and durations of
two events; Gagnon and Lapalme (1996) focus on
temporal adverbials (e.g., when to insert a known
time of day for an event). By comparison, we ex-
tend our approach to cover direct/indirect speech
and the subjunctive/conditional forms, which they
do not report implementing. While our work fo-
cuses on English, Yang and Bateman (2009) de-
scribe a recent system for generating Chinese as-
pect expressions based on a time interval represen-
tation, using KPML as their surface realizer.
Several other projects run tangential to our in-
teractive narrative encoding project. Callaway
and Lester?s STORYBOOK (2002) aims to im-
prove fluency and discourse cohesion in realiz-
ing formally encoded narratives; Ligozat and Zock
(1992) allow users to interactively construct sen-
tences in various temporal scenarios through a
graphical interface.
3 Expressing single events
3.1 Temporal knowledge
The propositions that we aim to realize take the
form of a predicate, one or more arguments, zero
or more attached modifiers (either a negation oper-
ator or an adverbial, which is itself a proposition),
and an assignment in time. Each argument is asso-
ciated with a semantic role (such as Agent or Ex-
periencer), and may include nouns (such as char-
acters) or other propositions. In our implemented
system, the set of predicates available to the an-
notator is adapted from the VerbNet (Kingsbury
and Palmer, 2002) and WordNet (Fellbaum, 1998)
linguistic databanks. These provide both durative
actions and statives (Dowty, 1979); we will refer
to both as events as they occur over intervals. For
example, here are an action and a stative:
walk(Mary, store, 2, 6) (1)
hungry(Julia, 1,?) (2)
The latter two arguments in (1) refer to time
states in a totally ordered sequence; Mary starts
walking to the store at state 2 and finishes walking
at state 6. (2) begins at state 1, but is unbounded
(Julia never ceases being hungry). While this pa-
per does not address the use of reference times
(such as equating a state to 6:00 or yesterday), this
is an area of ongoing work.
(1) and (2), depending on the situation, can be
realized in several aspects and tenses. We adapt
and extend Reichenbach?s (1947) famous system
of symbols for distinguishing between simple and
progressive aspect. Reichenbach identifies three
points that define the temporal position of the
event: the event time E, the speech time S, and
a reference time R which may or may not be in-
dicated by a temporal adverbial. The total order-
ing between these times dictates the appropriate
aspect. For example, the simple past John laughed
has the relation E < S. R = E because there is
no separate reference time involved. The past per-
fect John had laughed [by the end of the play] has
the relation E < R < S, in that it describe ?the
past of the past?, with the nearer ?past? being R
(the end of the play). R can be seen as the tempo-
ral focus of the sentence.
As Reichenbach does not address events with
intervals, we redefine E as the transition (E1..E2)
attached to the proposition (for example, (2,6)
for Mary?s walk). This definition deliberately as-
sumes that no event ever occurs over a single ?in-
stant? of time. The perception of an instantaneous
event, when it is needed, is instead created by di-
lating R into an interval large enough to contain
the entire event, as in Dowty (1979).
We also distinguish between two generation
modes: realizing the story as a complete discourse
(narration mode) and describing the content of a
single state or interval (snapshot mode). Our sys-
tem supports both modes differently. In discourse
mode, we realize the story as if all events occur be-
fore the speech time S, which is the style of most
literary fiction. (We shall see that this does not
preclude the use of the future tense.) In snapshot
mode, speech time is concurrent with reference
time so that the same events are realized as though
they are happening ?now.? The system uses this
mode to allow annotators to inspect and edit what
occurs at any point in the story. In Figure 1, for in-
stance, the lion?s watching of the bull is realized as
both a present, continuing event in snapshot mode
(the lion continues to watch the bull) and narrated
as a past, continuing event (the lion was watching
the bull). In both cases, we aim to precisely trans-
late the propositions and their temporal relation-
ships into text, even if the results are not elegant
rhetoric, so that annotators can see how they have
Diagram Relations Perspective
E 1 R E 2 
R < E1 Before
E 1 R 
E 2 R = E1R < E2 Begin
E 1 R 
E 2 E1 < RR < E2 During
E 1 R E 2 
R = E2
R > E1
Finish
E 1 R E 2 
R > E2 After
Table 1: Perspective assignment for viewing an
event from a reference state.
formally encoded the story. In the remainder of
this section, we describe our method for assigning
tenses and aspects to propositions such as these.
3.2 Reference state
In both snapshot and narration modes, we often
need to render the events that occur at some ref-
erence state R. We would like to know, for in-
stance, what is happening now, or what happened
at 6:00 yesterday evening. The tense and aspect
depend on the perspective of the reference state
on the event, which can be bounded or unbounded.
The two-step process for this scenario is to deter-
mine the correct perspective, then pick the tense
and aspect class that best communicates it.
We define the set of possible perspec-
tives to follow Allen (1983), who describes
seven relationships between two intervals: be-
fore/after, meets/met by, overlaps/overlapped by,
starts/started by, during/contains, finishes/finished
by, and equals. Not all of these map to a relation-
ship between a single reference point and an event
interval. Table 1 maps each possible interaction
between E and R to a perspective, for both
bounded and unbounded events, including the
defining relationships for each interaction. A dia-
mond for E1 indicates at or before, i.e., the event
is either anteriorly unbounded (E1 = ??) or
beginning at a state prior to R and E2. Similarly,
a diamond for E2 indicates at or after.
Once the perspective is determined, covering
Reichenbach?s E and R, speech time S is deter-
mined by the generation mode. Following the
guidelines of Reichenbach and Dowty, we then as-
sign a tense for each perspective/speech time per-
Perspective Generation mode English tense System?s construction Example
After Future Speech Past perfect had {PAST PARTICIPLE} She had walked.
Present Speech Present perfect has/have {PAST PARTICIPLE} She has walked.
Past Speech Future perfect will have {PAST PARTICIPLE} She will have walked.
Modal Infinitive to have {PAST PARTICIPLE} To have walked.
Finish Future Speech ?Finished? stopped {PROGRESSIVE} She stopped walking.
Present Speech ?Finishes? stops {PROGRESSIVE} She stops walking.
Past Speech ?Will finish? will stop {PROGRESSIVE} She will stop walking.
Modal Infinitive to stop {PROGRESSIVE} To stop walking.
During Future Speech Past progressive was/were {PROGRESSIVE} She was walking.
Present Speech Present pro-
gressive
am/is/are {PROGRESSIVE} She is walking.
Past Speech Future progres-
sive
will be {PROGRESSIVE} She will be walking.
Modal Infinitive to be {PROGRESSIVE} To be walking.
During-
After
Future Speech Past perfect
progressive
had been {PROGRESSIVE} She had been walking.
Present Speech Present perfect
progressive
has/have been {PROGRESSIVE} She has been walking.
Past Speech Future perfect
progressive
will have been {PROGRESSIVE} She will have been
walking.
Modal Infinitive to has/have been {PROGRESSIVE} To have been walking.
Begin Future Speech ?Began? began {INFINITIVE} She began to walk.
Present Speech ?Begins? begins {INFINITIVE} She begins to walk.
Past Speech ?Will begin? will begin {INFINITIVE} She will begin to walk.
Modal Infinitive to begin {PROGRESSIVE} To begin walking.
Contains Future Speech Simple past {SIMPLE PAST} She walked.
Present Speech Simple present {SIMPLE PRESENT} She walks.
Past speech Simple future will {INFINITIVE} She will walk.
Modal Infinitive {INFINITIVE} To walk.
Before Future Speech ?Posterior? was/were going {INFINITIVE} She was going to walk.
Present Speech Future am/is/are going {INFINITIVE} She is going to walk.
Past Speech Future-of-
future
will be going {INFINITIVE} She will be going to
walk.
Modal Infinitive to be going {INFINITIVE} To be going to walk.
Table 2: Tense/aspect assignment and realizer constructions for describing an action event from a partic-
ular perspective and speech time. ?Progressive? means ?present participle.?
mutation in Table 2. Not all permutations map to
actual English tenses. Narration mode is shown as
Future Speech, in that S is in the future with re-
spect to all events in the timeline. (This is the case
even if E is unbounded, with E2 = ?.) Snap-
shot mode is realized as Present Speech, in that
R = S. The fourth column indicates the syntac-
tic construction with which our system realizes the
permutation. Each is a sequence of tokens that are
either cue words (began, stopped, etc.) or conjuga-
tions of the predicate?s verb. These constructions
emphasize precision over fluency.
As we have noted, theorists have distinguished
between ?statives? that are descriptive (John was
hungry), ?achievement? actions that culminate in
a state change (John built the house), and ?activi-
ties? that are more continuous and divisible (John
read a book for an hour) (Dowty, 1979). Prior
work in temporal connectives has taken advantage
of lexical information to determine the correct sit-
uation and assign aspect appropriately (Moens and
Steedman, 1988; Dorr and Gaasterland, 1995). In
our case, we only distinguish between actions and
statives, based on information from WordNet and
VerbNet. We use a separate table for statives; it is
similar to Table 2, except the constructions replace
verb conjugations with insertions of be, been, be-
ing, was, were, felt, and so on (with the latter ap-
plying to affective states). We do not currently
distinguish between achievements and activities in
selecting tense and aspect, except that the anno-
tator is tasked with ?manually? indicating a new
state when an event culminates in one (e.g., The
house was complete). Recognizing an achieve-
ment action can benefit lexical choice (better to
say John finished building the house than John
stopped) and content selection for the discourse as
a whole (the house?s completion is implied by fin-
ished and does not need to be stated separately).
To continue our running examples, suppose
propositions (1) and (2) were viewed as a snap-
shot from state R = 2. Table 1 indicates Begin
Diagram Relations Perspective
E 2 R 2 E 1 R 1 
E 2 R 2 E 1 R 1 
R1 ? E2 After
E 2 R 2 E 1 R 1 
R1 > E1
E2 > R1
R2 > E2
Finish
E 2 R 2 E 1 R 1 
E 2 R 2 
E 1 R 1 
R1 ? E1
R2 ? E2
Contains
E 2 R 2 
E 1 R 1 
E1 < R1
E2 > R2
During
E 2 R 2 
E 1 R 1 
R1 < E1
R2 > E1
E2 > R2
Begin
E 2 R 2 
E 1 R 1 
E 2 R 2 
E 1 R 1 
E1 ? R2 Before
Table 3: Perspective assignment for describing an
event from an assigned perspective.
to be the perspective for (1), since E1 = R, and
Table 2 calls for a ?new? tense/aspect permutation
that means ?begins at the present time.? When the
appropriate construction is inserted into the over-
all syntax for walk(Agent, Destination), which we
derive from the VerbNet frame for walk, the result
is Mary begins to walk to the store; similarly, (2) is
realized as Julia is hungry via the During perspec-
tive. Narration mode invokes past-tense verbs.
3.3 Reference interval
Just as events occur over intervals, rather than sin-
gle points, so too can reference times. One may
need to express what occurred when ?Julia entered
the room? (a non-instantaneous action) or ?yes-
terday evening.? Our system allows annotators to
view intervals in snapshot mode to get a sense of
what happens over a certain time span.
The semantics of reference intervals have been
studied as extensions to Reichenbach?s point ap-
proach. Dowty (1979, p.152), for example, posits
that the progressive fits only if the reference in-
terval is completely contained within the event in-
terval. Following this, we construct an alternate
lookup table (Table 3) for assigning the perspec-
Diagram Relations Perspective
E 2 R 2 
E2 > R2
E1 = ??
R1 = ??
During (a priori)
E 2 R 2 
R2 > E2
E1 = ??
R1 = ??
After
E 1 R 1 
R1 > E1
E2 =?
R2 =?
Contains
E 1 R 1 
E1 > R1
E2 =?
R2 =?
Before
Table 4: Perspective assignment if event and ref-
erence intervals are unbounded in like directions.
tive of an event from a reference interval. Table
2 then applies in the same manner. In snapshot
mode, the speech time S also occurs over an inter-
val (namely, R), and Present Speech is still used.
In narration mode, S is assumed to be a point fol-
lowing all event and reference intervals. In our
running example, narrating the interval (1,7) re-
sults in Mary walked to the store and Julia began
to be hungry, using the Contains and Begin per-
spectives respectively.
The notion of an unbounded reference interval,
while unusual, corresponds to a typical perspec-
tive if the event is either bounded or unbounded
in the opposite direction. These scenarios are il-
lustrated in Table 3. Less intuitive are the cases
where event and reference intervals are unbounded
in the same direction. Perspective assignments for
these instances are described in Table 4 and em-
phasize the bounded end of R. These situations
occur rarely in this generation context.
3.4 Event Subintervals
We do not always want to refer to events in their
entirety. We may instead wish to refer to the be-
ginning, middle or end of an event, no matter when
it occurs with respect to the reference time. This
invokes a second reference point in the same inter-
val (Comrie, 1985, p.128), delimiting a subinter-
val. Consider John searches for his glasses versus
John continues to search for his glasses? both in-
dicate an ongoing process, but the latter implies a
subinterval during which time, we are expected to
know, John was already looking for his glasses.
Our handling of subintervals falls along four
alternatives that depend on the interval E1..E2,
the reference R and the subinterval E?1..E
?
2 of E,
where E?1 ? E1 and E
?
2 ? E2.
1. During-After. If E? is not a final subinter-
val of E (E?2 < E2), and R = E
?
2 or R is a
subinterval ofE that is met byE? (R1 = E?2),
the perspective of E? is defined as During-
After. In Table 2, this invokes the perfect-
progressive tense. For example, viewing ex-
ample (1) with E? = (2, 4) from R = 4 in
narration mode (Future Speech) would yield
Mary had been walking to the store.
2. Start. Otherwise, if E? is an initial subin-
terval of E (E?1 = E1 and E
?
2 < E2), the
perspective is defined as Start. These rows
are omitted from Table 2 for space reasons,
but the construction for this case reassigns the
perspective to that between R and E?. Our
realizer reassigns the verb predicate to begin
(or become for statives) with a plan to render
its only argument, the original proposition, in
the infinitive tense. For example, narrating
(2) with E? =(1,2) from R = 3 would yield
Julia had become hungry.
3. Continue. Otherwise, and similarly, if E
strictly contains E? (E?1 > E1 and E
?
2 < E2),
we assign the perspective Continue. To real-
ize this, we reassign the perspective to that
between R and E?, and reassign the verb
predicate to continue (or was still for statives)
with a plan to render its only argument, the
original proposition, in the infinitive.
4. End. Otherwise, if E? is a final subinterval
of E (E?1 > E1 and E
?
2 = E2), we assign the
perspective End. To realize this, we reassign
the perspective to that betweenR andE?, and
reassign the verb predicate to stop (or finish
for cumulative achievements). Similarly, the
predicate?s argument is the original proposi-
tion rendered in the infinitive.
4 Alternate timelines and modalities
This section covers more complex situations in-
volving alternate timelines? the feature of our rep-
resentation by which a proposition in the main
timeline can refer to a second frame of time. Other
models of time have supported similar encapsula-
tions (Crouch and Pulman, 1993; Mani and Puste-
jovsky, 2004). The alternate timeline can contain
references to actual events or modal events (imag-
ined, obligated, desired, planned, etc.) in the past
the future with respect to its point of attachment on
E speech 
R? 
R 
 E hunger 
E? buy 
E? hunger 
reality 
alternate 
S 
Figure 2: Schematic of a speech act attaching to
a alternate timeline with a hypothetical action. R?
and Espeech are attachment points.
the main timeline. This is primarily used in prac-
tice for modeling dialogue acts, but it can also be
used to place real events at uncertain time states
in the past (e.g., the present perfect is used in a
reference story being encoded).
4.1 Reassigning Temporal Focus
Ogihara (1995) describes dialogue acts involving
changes in temporal focus as ?double-access sen-
tences.? We now consider a method for planning
such sentences in such a way that the refocusing
of time (the reassignment of R into a new con-
text) is clear, even if it means changing tense and
aspect mid-sentence. Suppose Mary were to de-
clare that she would buy some eggs because of
Julia?s hunger, but before she returned from the
store, Julia filled up on snacks. If this speech act
is described by a character later in the story, then
we need to carefully separate what is known to
Mary at the time of her speech from what is later
known at R by the teller of the episode. Mary
sees her purchase of eggs as a possible future, even
though it may have already happened by the point
of retelling, and Mary does not know that Julia?s
hunger is to end before long.
Following Hornstein?s treatment of these sce-
narios (Hornstein, 1990), we attach R?, the ref-
erence time for Mary?s statement (in an alternate
timeline), to Espeech, the event of her speaking (in
the main timeline). The act of buying eggs is a
hypothetical event E?buy that falls after R
? on the
alternate (modal) timeline. S is not reassigned.
Figure 2 shows both timelines for this example.
The main timeline is shown on top; Mary?s speech
act is below. The attachment point on the main
timeline is, in this case, the speech event Espeech;
the attachment point on an alternate timeline is al-
ways R?. The placement of R, the main refer-
ence point, is not affected by the alternate time-
line. Real events, such as Julia?s hunger, can be
invoked in the alternate timeline (as drawn with a
vertical line from Ehunger to an E?hunger without
anE?2 known toMary) but they must preserve their
order from the main timeline.
The tense assignment for the event intervals in
the alternate timeline then proceeds as normal,
withR? substituting forR. The hypothetical ?buy?
event is seen in Before perspective, but past tense
(Future Speech), giving the ?posterior? (future-of-
a-past) tense. Julia?s hunger is seen as During as
per Table 1. Further, we assert that connectives
such as Because do not alterR (or in this situation,
R?), and that the E?buy is connected to E
?
hunger
with a causality edge. (Annotators can indicate
connectives between events for causality, motiva-
tion and other features of narrative cohesion.)
The result is: Mary had said that she
was going to buy eggs because Julia was hungry.
The subordinate clause following that sees E?buy
in the future, and E?hunger as ongoing rather than
in the past. It is appropriately ambiguous in both
the symbolic and rendered forms whetherE?buy oc-
curs at all, and if so, whether it occurs before, dur-
ing or after R. A discourse planner would have
the responsibility of pointing out Mary?s mistaken
assumption about the duration of Julia?s hunger.
We assign tense and aspect for quoted speech
differently than for unquoted speech. Instead of
holding S fixed, S? is assigned to R? at the attach-
ment point of the alternate timeline (the ?present
time? for the speech act). If future hypothetical
events are present, they invoke the Past Speech
constructions in Table 2 that have not been used
by either narration or snapshot mode. The content
of the quoted speech then operates totally indepen-
dently of the speech action, since both R? and S?
are detached: Mary said/says/was saying, ?I am
going to buy eggs because Julia is hungry.?
The focus of the sentence can be subsequently
reassigned to deeper nested timelines as necessary
(attaching E? to R??, and so on). Although the
above example uses subordinate clauses, we can
use this nesting technique to construct compos-
ite tenses such as those enumerated by Halliday
(1976). To this end, we conjugate the Modal In-
finitive construction in Table 2 for each alternate
timeline. For instance, Halliday?s complex form
?present in past in future in past in future? (as in
will have been going to have been taking) can be
generated with four timelines in a chain that in-
voke, in order and with Past Speech, the After, Be-
fore, After andDuring perspectives. There are four
Rs, all but the main one attached to a previous E.
4.2 Subjunctives and Conditionals
We finally consider tense and aspect in the case of
subjunctive and conditional statements (if-thens),
which can appear in alternate timelines. The re-
lationship between an if clause and a then clause
is not the same as the relationship between two
clauses joined by because or when. The then
clause? or set of clauses? is predicated on the truth
of the if clause. As linguists have noted (Horn-
stein, 1990, p.74), the if clause serves as an adver-
bial modifier, which has the effect of moving for-
ward the reference point to the last of the if event
intervals (provided that the if refers to a hypotheti-
cal future). Consider the sentence: If John were to
fly to Tokyo, he would have booked a hotel. A cor-
rect model would place E?book before E
?
fly on an
alternate timeline, with E?fly as the if. Since were
to fly is a hypothetical future, R? < E?fly. Dur-
ing regeneration, we set R? to E?fly after rendering
If John were to fly to Tokyo, because we begin to
assume that this event transpired. If R? is left un-
changed, it may be erroneously left before E?book:
Then he would be going to book a hotel.
Our encoding interface allows users to mark one
or more events in an alternate timeline as if events.
If at least one event is marked, all if events are ren-
dered in the subjunctive mood, and the remainder
are rendered in the conditional. For the if clauses
that follow R?, S? and R? itself are reassigned to
the interval for each clause in turn. R? and S? then
remain at the latest if interval (if it is after the origi-
nal R?) for purposes of rendering the then clauses.
In our surface realizer, auxiliary words were and
would are combined with theModal Infinitive con-
structions in Table 2 for events during or following
the original attachment point.
As an example, consider an alternate timeline
with two statives whose start and end points are the
same: Julia is hungry and Julia is unhappy. The
former is marked if. Semantically, we are saying
that hungry(Julia)?unhappy(Julia).
If R? were within these intervals, the rendering
would be: If Julia is hungry, then she is unhappy
(Contains/Present Speech for both clauses). If
R? were prior to these intervals, the rendering
would be: If Julia were to be hungry, then
she would be unhappy. This reassigns R? to
Ehungry, using were as a futurative and would
to indicate a conditional. Because R? and S? are
set to Ehungry, the perspective on both clauses
remains Contains/Present Speech. Finally, if both
intervals are before R?, describing Julia?s previous
emotional states, we avoid shifting R? and S?
backward: If Julia had been hungry, then she had
been unhappy (After perspective, Future Speech
for both statives).
The algorithm is the same for event intervals.
Take (1) and a prior event where Mary runs out of
eggs:
runOut(Mary, eggs, 0, 1) (3)
Suppose they are in an alternate timeline with
attachment point 0? and (1) marked if. We be-
gin by realizing Mary?s walk as an if clause: If
Mary were to walk to the store. We reassign R?
to Ewalk, (2,6), which diverts the perception of
(3) from Begins to After: She would have run out
of eggs. Conversely, suppose the conditional re-
lationship were reversed, with (3) as the only if
action. If the attachment point is 3?, we realize (3)
first in the After perspective, as R? does not shift
backward: If Mary had run out of eggs. The re-
mainder is rendered from the During perspective:
She would be walking to the store. Note that in
casual conversation, we might expect a speaker at
R = 3 to use the past simple: If Mary ran out
of eggs, she would be walking to the store. In this
case, the speaker is attaching the alternate timeline
at a reference interval that subsumes (3), invoking
the Contains perspective by casting a net around
the past. We ask our annotators to select the best
attachment point manually; automatically making
this choice is beyond the scope of this paper.
5 Discussion
As we mentioned earlier, we are describing two
separate methods with a modular relationship to
one another. The first is an abstract mapping from
a conceptual representation of time in a narrative,
including interval and modal logic, to a set of 11
perspectives, including the 7 listed in Table 2 and
the 4 introduced in Section 3.4. These 11 are
crossed with three scenarios for speech time to
give a total of 33 tense/aspect permutations. We
also use an infinitive form for each perspective.
One may take these results and map them from
other time representations with similar specifica-
tions.
The second result is a set of syntactic construc-
tions for realizing these permutations in our story
encoding interface. Our focus here, as we have
noted, is not fluency, but a surface-level render-
ing that reflects the relationships (and, at times,
the ambiguities) present in the conceptual encod-
ing. We consider variations in modality, such as
an indicative reading as opposed to a conditional
or subjunctive reading, to be at the level of the re-
alizer and not another class of tenses.
We have run a collection project with our en-
coding interface and can report success in the
tool?s usability (Elson and McKeown, 2009). Two
annotators each encoded 20 fables into the for-
mal representation, with their only exposure to the
semantic encodings being through the reference
text generator (as in Figure 1). Both annotators
became comfortable with the tool after a period
of training; in surveys that they completed after
each task, they gave Likert-scale usability scores
of 4.25 and 4.30 (averaged over 20 tasks, with
5 meaning ?easiest to use?). These scores are
not specific to the generation component, but they
suggest that annotators could derive satisfactory
tenses from their semantic structures. The most
frequently cited deficiency in the model in terms
of time was the inability to assign reference times
to states and intervals (such as the next morning).
6 Conclusion and Future Work
It has always been the goal in surface realization
to generate sentences from a purely semantic rep-
resentation. Our approach to the generation of
tense and aspect from temporal intervals takes us
closer to that goal. We have applied prior work in
linguistics and interval theory and tested our ap-
proach in an interactive narrative encoding tool.
Our method handles reference intervals and event
intervals, bounded and unbounded, and extends
into subintervals, modal events, conditionals, and
direct and indirect speech where the temporal fo-
cus shifts.
In the future, we will investigate extensions
to the current model, including temporal adver-
bials (which explain the relationship between two
events), reference times, habitual events, achieve-
ments, and discourse-level issues such as prevent-
ing ambiguity as to whether adjacent sentences oc-
cur sequentially (Nerbonne, 1986; Vlach, 1993).
7 Acknowledgments
This material is based on research supported in
part by the U.S. National Science Foundation
(NSF) under IIS-0935360. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the NSF.
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23(2):123?
154.
Charles Callaway and James Lester. 2002. Nar-
rative prose generation. Artificial Intelligence,
139(2):213?252.
John Chen, Srinivas Bangalore, Owen Rambow, and
Marilyn Walker. 2002. Towards automatic gen-
eration of natural language generation systems. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING 2002), Taipei,
Taiwan.
Bernard Comrie. 1985. Tense. Cambridge University
Press.
Richard Crouch and Stephen Pulman. 1993. Time and
modality in a natural language interface to a plan-
ning system. Artificial Intelligence, pages 265?304.
Bonnie J. Dorr and Terry Gaasterland. 1995. Select-
ing tense, aspect, and connecting words in language
generation. In Proceedings of the Fourteenth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-95), Montreal, Canada.
David R. Dowty. 1979. Word Meaning and Montague
Grammar. D. Reidel, Dordrecht.
Michael Elhadad and Jacques Robin. 1996. An
overview of surge: a reusable comprehensive syn-
tactic realization component. In INLG ?96 Demon-
strations and Posters, pages 1?4, Brighton, UK.
Eighth International Natural Language Generation
Workshop.
David K. Elson and Kathleen R. McKeown. 2009. A
tool for deep semantic encoding of narrative texts.
In Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 9?12, Suntec, Singapore.
David K. Elson and Kathleen R. McKeown. 2010.
Building a bank of semantically encoded narratives.
In Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation (LREC
2010), Malta.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Michel Gagnon and Guy Lapalme. 1996. From con-
ceptual time to linguistic time. Computational Lin-
guistics, 22(1):91?127.
Brigitte Grote. 1998. Representing temporal discourse
markers for generation purposes. In Proceedings
of the Discourse Relations and Discourse Markers
Workshop, pages 22?28, Montreal, Canada.
M.A.K. Halliday. 1976. The english verbal group. In
G. R. Kress, editor, Halliday: System and Function
in Language. Oxford University Press, London.
Erhard W. Hinrichs. 1987. A compositional semantics
of temporal expressions in english. In Proceedings
of the 25th Annual Conference of the Association for
Computational Linguistics (ACL-87), Stanford, CA.
Norbert Hornstein. 1990. As Time Goes By: Tense and
Universal Grammar. MIT Press, Cambridge, MA.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Canary Islands, Spain.
Mirella Lapata and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. Journal of Ar-
tificial Intelligence Research, 27:85?117.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Pro-
ceedings of the Fifth Conference on Applied Natural
Language Processing, Washington, DC.
Gerard Ligozat and Michael Zock. 1992. How to vi-
sualize time, tense and aspect? In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING ?92), pages 475?482, Nantes,
France.
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral discourse models for narrative structure. In
Proceedings of the ACL Workshop on Discourse An-
notation, Barcelona, Spain.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of COLING/ACL 2006, pages 753?760, Sydney,
Australia.
Inderjeet Mani. 2004. Recent developments in tempo-
ral information extraction. In Proceedings of the In-
ternational Conference on Recent Advances in Nat-
ural Language Processing (RANLP ?03), pages 45?
60, Borovets, Bulgaria.
Christian M. I. M. Matthiessen and John A. Bateman.
1991. Text generation and systemic-functional lin-
guistics: experiences from English and Japanese.
Frances Pinter Publishers and St. Martin?s Press,
London and New York.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational Lin-
guistics, 14(2):15?28.
John Nerbonne. 1986. Reference time and time in nar-
ration. Linguistics and Philosophy, 9(1):83?95.
Toshiyuki Ogihara. 1995. Double-access sentences
and reference to states. Natural Language Seman-
tics, 3:177?210.
Rebecca Passonneau. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, 14(2):44?60.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
MacMillan, London.
Fei Song and Robin Cohen. 1988. The interpretation
of temporal relations in narrative. In Proceedings of
the Seventh National Conference on Artificial Intel-
ligence (AAAI-88), St. Paul, Minnesota.
Mark Steedman. 1995. Dynamic semantics for tense
and aspect. In The 1995 International Joint Confer-
ence on AI (IJCAI-95), Montreal, Quebec, Canada.
Frank Vlach. 1993. Temporal adverbials, tenses and
the perfect. Linguistics and Philosophy, 16(3):231?
283.
Bonnie Lynn Webber. 1987. The interpretation of
tense in discourse. In Proceedings of the 25th An-
nual Meeting of the Association for Computational
Linguistics (ACL-87), pages 147?154, Stanford, CA.
Guowen Yang and John Bateman. 2009. The chinese
aspect generation based on aspect selection func-
tions. In Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP (ACL-
IJCNLP 2009), Singapore.
Workshop on Monolingual Text-To-Text Generation, pages 43?53,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43?53,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Towards Strict Sentence Intersection: Decoding and Evaluation Strategies
Kapil Thadani and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{kapil,kathy}@cs.columbia.edu
Abstract
We examine the task of strict sentence inter-
section: a variant of sentence fusion in which
the output must only contain the informa-
tion present in all input sentences and nothing
more. Our proposed approach involves align-
ment and generalization over the input sen-
tences to produce a generation lattice; we then
compare a standard search-based approach for
decoding an intersection from this lattice to an
integer linear program that preserves aligned
content while minimizing the disfluency in
interleaving text segments. In addition, we
introduce novel evaluation strategies for in-
tersection problems that employ entailment-
style judgments for determining the validity
of system-generated intersections. Our experi-
ments show that the proposed models produce
valid intersections a majority of the time and
that the segmented decoder yields advantages
over the search-based approach.
1 Introduction
In recent years, there has been growing interest
in text-to-text generation problems which transform
text according to specifications. Tasks such as sen-
tence compression, which strives to retain the most
salient content of an input sentence, and sentence fu-
sion, which attempts to combine the important con-
tent in related sentences, are useful components for
tackling larger natural language problems such as
abstractive summarization of documents. Systems
for these types of text-to-text problems are typically
evaluated on the informativeness of the output text
as judged by human annotators.
A natural aspect of most text generation systems
is that a given input can map to a range of lexi-
cally diverse outputs. However, text-to-text tasks
defined with vague criteria such as the preserva-
tion of the ?important? information in text can also
permit outputs that are semantically distinct. This
can make evaluation difficult; for instance, system-
generated sentences may differ (partially or com-
pletely) in informational content from reference
human-annotated text. This phenomenon has been
noted and discussed in the task of pairwise sentence
fusion (Daume? III and Marcu, 2004) and also in sen-
tence compression (McDonald, 2006). Some exam-
ples are listed in Table 1.
In this work, we examine the task of sentence in-
tersection: a variant of sentence fusion that does not
permit semantic variation in the output. A strict1 in-
tersection system is expected to produce a fused sen-
tence that contains all the information common to its
input sentences and avoid information that is in just
one of the inputs. In other words, a valid intersection
should only contain information that is substantiated
by all input sentences. The set-theoretic notions of
intersection (along with union) have been employed
to describe variants of sentence fusion tasks in previ-
ous work (Marsi and Krahmer, 2005; Krahmer et al,
2008) but, to our knowledge, this work is the first to
explicitly tackle and evaluate the strict intersection
task.
We focus on the case of unsupervised pairwise
sentence intersection and propose a strategy to yield
1We use the term strict to make explicit the distinction from
traditional fusion systems, which generally aim at notions of
intersection but are not formally evaluated with respect to it.
43
(a) Fusion example from
Daume? III and Marcu (2004)
(i) After years of pursuing separate and conflicting paths, AT&T and Digital Equip-
ment Corp. agreed in June to settle their computer-to-PBX differences.
(ii) The two will jointly develop an applications interface that can be shared by
computers and PBXs of any stripe.
Human fusion #1 AT&T and Digital Equipment Corp. agreed in June to settle their computer-to-
PBX differences and develop an applications interface that can be shared by any
computer or PBX.
Human fusion #2 After years of pursuing different paths, AT&T and Digital agreed to jointly develop
an applications interface that can be shared by computers and PBXs of any stripe.
(b) Compression example
from McDonald (2006)
TapeWare , which supports DOS and NetWare 286 , is a value-added process that
lets you directly connect the QA150-EXAT to a file server and issue a command
from any workstation to back up the server
Human compression #1 TapeWare supports DOS and NetWare 286
Human compression #2 TapeWare lets you connect the QA150-EXAT to a file server
(hypothesized)
Table 1: Examples of text-to-text generation problems with multiple valid human-generated outputs that differ signif-
icantly in semantic content. Italicized text is used to indicate fragments that are semantically identical.
valid intersections that follows the basic framework
of previous unsupervised fusion systems (Barzilay
and McKeown, 2005; Filippova and Strube, 2008b).
In our approach, the input sentences are first aligned
using a modified version of a recent phrase-based
alignment approach (MacCartney et al, 2008). We
assume the alignments that are produced define as-
pects of the input that must appear in the output fu-
sion and consider decoding strategies to recover in-
tersections that preserve these alignments. In addi-
tion to a search-based decoding strategy, we propose
a constrained integer linear programming (ILP) for-
mulation that attempts to decode the most fluent sen-
tence covering all these aspects while minimizing
the size and disfluency of interleaving text. This is a
fairly general model which can also be extended to
other alignment-based tasks such as pairwise union
and difference.
As this is a substantially more constrained task
than generic sentence fusion, we also present a
novel evaluation approach that avoids out-of-context
salience judgments. We make use of a recently-
released corpus of fusion candidates (McKeown et
al., 2010) and propose a crowdsourced entailment-
style evaluation to determine the validity of gener-
ated intersections, as well as the grammaticality of
the sentences produced. Additionally, automated
machine translation (MT) metrics are explored to
quantify the amount of information missing from
valid intersections. Our decoding strategies show
promise under these experiments and we discuss po-
tential directions for improving intersection perfor-
mance.
2 Related Work
The distinction between intersection and union of
text was introduced in the context of sentence fu-
sion (Krahmer et al, 2008; Marsi and Krahmer,
2005) in order to distinguish between traditional fu-
sion strategies that attempted to include only com-
mon content and fusions that attempted to include
all non-redundant content from the input. We fo-
cus here on strict sentence intersection, explicitly
incorporating a constraint that requires that a pro-
duced fusion must not contain information that is
not present in all input sentences. This distin-
guishes our approach from traditional sentence fu-
sion approaches (Jing and McKeown, 2000; Barzi-
lay and McKeown, 2005; Filippova and Strube,
2008b) which generally attempt to retain common
information but are typically evaluated in an abstrac-
tive summarization context in which additional in-
formation in the fusion output does not negatively
impact judgments.
This task is also related to the field of sentence
compression which has received much attention in
recent years (Turner and Charniak, 2005; McDon-
ald, 2006; Clarke and Lapata, 2008; Filippova and
Strube, 2008a; Cohn and Lapata, 2009; Marsi et al,
2010). Intersections can be viewed as guided com-
44
pressions in which the redundancy of information
content across input sentences in a multidocument
setting is assumed to directly indicate its salience,
thereby consigning it to the output.
Additionally, in this work, we frequently con-
sider the sentence intersection task from the per-
spective of textual entailment (cf. ?5.1). The textual
entailment task involves automatically determining
whether a given hypothesis can be inferred from a
textual premise (Dagan et al, 2005; Bar-Haim et al,
2006). Automatic construction of positive and neg-
ative entailment examples has been explored in the
past (Bensley and Hickl, 2008) to provide training
data for entailment systems; however the produc-
tion of text that is simultaneously entailed by two
(or more) sentences is a far more constrained and
difficult challenge.
ILP has been used extensively for text-to-text gen-
eration problems in recent years (Clarke and Lapata,
2008; Filippova and Strube, 2008b; Woodsend et al,
2010), including techniques which incorporate syn-
tax directly into the decoding to imporove the flu-
ency of the resulting text. In this paper, we focus on
generating valid intersections and do not incorporate
syntactic and semantic constraints into our ILP mod-
els; these are areas we intend to explore in the future.
3 The Intersection Task
The need for strict variants of fusion is motivated
by considerations of evaluation and utility in text-to-
text generation tasks. Without explicit constraints on
the semantic content of valid output, the operational
definition of fusion can encompass the full spectrum
from sentence intersection to sentence union. This
makes the comparison of different fusion systems
dependent on task-based utility2. In addition, inter-
section comprises an interesting problem in its own
right. It necessitates the use of generalization over
phrases in order to convey only the content of the
input sentences when different wording is used and
therefore involves more than just word deletion.
The analogy to set-theoretic intersection in this
task implies an underlying consideration of each
sentence as a set of informational concepts, sim-
2For instance, systems may trade off conciseness against
grammaticality, or informational content with degree of support
across the input sentences.
ilar to previous work in summarization and re-
dundancy (Filatova and Hatzivassiloglou, 2004;
Thadani and McKeown, 2008). While we don?t
commit to any semantic representation for such el-
ements of information, we can nevertheless attempt
to identify repeated information using well-studied
natural language analysis techniques such as align-
ment and paraphrase recognition, and furthermore
isolate this information through text-to-text genera-
tion techniques.
Consider, for example, the first sentence pair from
the examples in Table 2. A valid intersection for
these sentences must not contain any information
that is not substantiated by both of them, so a fu-
sion that mentions ?Mr Litvinenko?s poisoning?,
?Britain? or ?Sunday? would not satisfy this crite-
rion. In other words, a valid intersection must neces-
sarily be textually entailed by every input sentence.
Following this, we can interpret the sentence inter-
section task as one that requires the generation of
fluent text that is mutually entailed by all input sen-
tences3. We use this perspective in developing an
evaluation technique for strict intersection in ?5.1.
A major distinguishing factor between this work
and previous work on fusion is that simply adding
or deleting words in a sentence is not adequate; in
many cases, intersections require additional words
or phrases to be introduced in order to general-
ize over related but non-interchangeable aligned
terms (such as ?go? and ?expand?). Additionally,
we must attempt to avoid introducing additional
content-bearing text in the output while simultane-
ously striving to maintain the fluency of text.
3.1 Dataset
A corpus of sentence fusion instances was recently
made available by McKeown et al (2010), consist-
ing of 297 sentence pairs taken from newswire clus-
ters and manually judged as being good candidates
for fusion. Each sentence pair is accompanied by
human-produced intersections and unions collected
via Amazon?s Mechanical Turk service4. McKeown
et al (2010) noted that union responses are mostly
valid but intersections are frequently incorrect and
3From this perspective, the complementary task of sentence
union involves the generation of fluent text that entails all the
input sentences.
4http://www.mturk.com
45
1 (i) Home Secretary John Reid said Sunday the inquiry would go wherever ?the police take it.?
(ii) It comes as Home Secretary John Reid said the inquiry into Mr Litvinenko?s poisoning would expand beyond
Britain.
2 (i) Traces of polonium have been found on the planes on which they are believed to have travelled between
London and Moscow.
(ii) Small traces of radioactive substances had been found on the planes.
3 (i) Prosecutors allege that the accuser, who appeared in the program, was molested after the show aired.
(ii) Prosecutors allege that the boy, a cancer survivor, was molested twice after the program aired.
Table 2: Example sentence pairs from the McKeown et al (2010) corpus. Table 3 contains the corresponding system-
generated intersections for these sentence pairs.
hypothesized that the task is more confusing for
untrained annotators. A similar phenomenon was
noted by Krahmer et al (2008): while demonstrat-
ing that query-based human fusions exhibited less
variation than generic fusions, it was also observed
that intersections varied more than unions.
Due to the absence of adequate training data for
intersection, our approach to the task is unsuper-
vised, similar to previous work in fusion (Barzilay
and McKeown, 2005; Filippova and Strube, 2008b)
and sentence compression (Clarke and Lapata, 2008;
Filippova and Strube, 2008a). Additionally, we fo-
cus on the case of pairwise sentence intersection and
assume that the common information between the
input sentence pair can be represented within a sin-
gle output sentence. As a result, although the McK-
eown et al (2010) corpus cannot be used for training
an intersection model, we can make use of the sen-
tence pairs it contains for evaluation.
4 Models for intersection
Our proposed strategies for sentence intersection in-
volve phrase-based alignment, intermediate general-
ization steps that build a generation lattice and tech-
niques for decoding an output sentence, as described
below.
4.1 Phrase-based alignment
The alignment phase is a major component of any
intersection system as it is used to uncover the
common segments in the input that must be pre-
served in the output. We make use of an adapta-
tion of the supervised MANLI phrase-based align-
ment technique originally developed for textual en-
tailment systems (MacCartney et al, 2008); our
implementation replaces approximate search-based
decoding with exact ILP-based alignment decod-
ing and incorporates syntactic constraints to pro-
duce more precise alignments (Thadani and McKe-
own, 2011). The aligner is trained on a corpus of
human-generated alignment annotations produced
by Microsoft Research (Brockett, 2007) for infer-
ence problems from the second Recognizing Tex-
tual Entailment (RTE2) challenge (Bar-Haim et al,
2006).
Entailment problems are inherently asymmetric
because premise text is generally larger than hypoth-
esis text; however, this does not apply to our inter-
section problems and consequently our MANLI im-
plementation drops asymmetric indicator features.
The absence of these features impacts alignment
performance on RTE2 data but our reimplementa-
tion performs comparably to the original model un-
der the alignment evaluation from MacCartney et al
(2008).
4.2 Ontology-based generalization
An aligned phrase pair produced by the previous
step does not necessarily indicate that the phrases
are equivalent but merely that they are similar in
the given sentence context (such as ?accuser? and
?boy? in the third example from Table 2). We need
to generalize over these phrases as they are not inter-
changeable from the perspective of the intersection
task. We consider an alignment as containing three
types of aligned phrases:
1. Identical phrases or paraphrases: Either of
these may appear in the output
2. Entailed phrases: Only the entailed phrase
must appear in a valid intersection
3. Instances of a general concept: The common
concept must be lexicalized in the output
46
Although generalization of words within stan-
dalone sentences is usually hampered by word sense
ambiguity, our approach is less likely to encounter
this problem because we can generalize simultane-
ously over phrases which have already been aligned
using additional information (such as their neighbor-
ing context), thus avoiding generalizations that do
not fit the alignment.
For our experiments, we make use of the Wordnet
ontology (Miller, 1995) to find the hypernyms com-
mon to every aligned pair of non-identical phrases,
and only attempt to detect entailments which are
comprised of specific instances that entail general
concepts. This approach can be augmented by the
use of entailment corpora and distributional cluster-
ing which we intend to explore in future work. We
also use the lexical resource CatVar (Habash and
Dorr, 2003) to try to generate morphological vari-
ants of aligned words that enable them to be inter-
changed without creating disfluencies.
4.3 Pragmatic abstraction
Our strategy assumes that aligned text must be pre-
served in output intersections whereas unaligned
text must be minimized. However, unaligned text
cannot simply be dropped as it may contain vital
portions for generating fluent text. In addition, un-
aligned phrases can be caused by paraphrased or
metaphorical text that the aligner is not capable of
identifying. For example, the phrases ?polonium?
and ?radioactive substances? in the second sentence
pair from Table 2 fail to align with each other.
On the other hand, retaining unaligned text from
one of the input sentences for the sake of fluency
is likely to introduce information that is not sup-
ported by the other input sentence. We therefore
need to abstract away as much content from the un-
aligned portions of the text as possible. For this
purpose, we generate a large number of potential
compressions and abstractions for every unaligned
span that occurs between two consecutive aligned
phrases in each sentence. These compressions and
abstractions, referred to as interleaving paths, be-
tween pairs of aligned phrases essentially construct
a lattice over the input sentences that encodes all po-
tential intersection outputs.
Generation of interleaving paths is accomplished
through the application of rules on the dependency
parse structure over unaligned text spans from a sin-
gle sentence (as well as spans that occur before the
first aligned phrase and after the last aligned phrase
in each sentence). Interleaving paths are generated
by applying rules that:
1. Drop insignificant dependent words and un-
aligned prepositional phrases
2. Replace content-bearing verbs with tense-
adjusted generic variants such as ?did some-
thing? and ?happened?, with an exception for
statement verbs
3. Replace nouns with generic words such as
?someone? or ?something?, using Wordnet to
determine which generic variant fits a noun
4. Suggest connective text fragments such as
?something about? to cover long spans and
clause boundaries
Our abstraction rules are relatively simple but can
often generate reasonable interleaving paths. In gen-
eral, we note that shorter abstractions are less likely
to include glaring grammatical errors because long
unaligned spans are often indicative of problematic
alignments that either incorrectly relate unconnected
terms or fail to recognize paraphrases.
4.4 Decoding strategies
After sentence alignment, generalization over
aligned phrases and the construction of interleav-
ing paths, we are left with a lattice that encodes
potential intersections of the input sentence. Fig-
ure 1 describes the general structure of this lattice.
Every alignment link encompasses a set of aligned
phrases. Phrases may be identical or generaliza-
tions, in which case they can appear in the context
of either sentence, or they may be sentence-specific
(for example, verbs with different tenses or nominal-
izations like ?nominated? and ?nominations?). Ad-
ditionally, the abstraction phase generates interleav-
ing paths from unaligned spans between all pairs of
alignment links. These paths are generated from in-
dividual sentences and can only be used to connect
phrases that appear in the context of those sentences.
Our task now reduces to recovering a well-formed
intersection from this lattice. We make use of a lan-
guage model (LM) to judge fluency and propose two
techniques to decode high-scoring text from the lat-
tice: a simple beam-search technique and an ILP
47
Alignment link k
m
m
m
m
Phrases
from S1
Shared
phrases
Phrases
from S2
Alignment link l
Phrases
from S1
Shared
phrases
Phrases
from S2
Figure 1: The general structure of one segment of the
alignment lattice, illustrating the potential interleaving
paths between aligned phrases. Solid lines indicate paths
derived from sentence 1 and dashed lines indicate paths
derived from sentence 2
strategy that leverages our initial assumption that all
aligned phrases must appear in the output.
4.4.1 Beam search
Search-based decoding is often employed in phrase-
based MT systems (Och and Ney, 2003) and is
implemented in the Moses toolkit5; similar ap-
proaches have also been used in text-to-text gener-
ation tasks (Barzilay and McKeown, 2005; Soricut
and Marcu, 2006). This technique attempts to find
the highest-scoring sentence string under the LM by
unwrapping and searching through a lattice. Since
the dynamic programming search could require an
exponential number of search states, a fixed-width
beam can be used to control the number of search
states being actively considered at each step.
In order to decode an intersection problem, we
first pick a beam size B and initialize the list of can-
didate search states with the first interleaving paths
in each sentence. At every iteration, we consider the
B candidates with the highest normalized scores un-
der the LM and remove them from the candidate list.
Each candidate is then advanced, i.e., all aligned
phrases and interleaving paths following it are ex-
amined, scored and added to the candidate list. We
continue searching in this manner until B candidates
have covered all aligned phrases; the highest scoring
candidate is then retrieved as the target intersection.
4.4.2 Segmented decoding
While beam search is a viable strategy for decoding
intersections, its performance is contingent on the
5http://www.statmt.org/moses/
beam size parameter and it is not guaranteed to re-
turn the highest scoring sentence under the LM. For
instance, if a potential intersection starts with un-
usual text, it is unlikely to be explored by the search-
based approach even if it is the optimal solution to
the decoding problem. To address this, we also pro-
pose an alternative decoding problem that can be
formulated as the optimization of a linear objective
function with linear constraints. This can then be
solved exactly by well-studied algorithms using off-
the-shelf ILP solvers6.
This decoding problem does not look for the
highest scoring sentence under the LM; instead, it
attempts to find the set of interleaving paths and
aligned phrases that are most locally coherent7 un-
der the LM. Good phrase-path combinations that oc-
cur towards the tail end of an intersection can thus
be put on even footing with the combinations that
appear in the beginning. Although the two problems
consider different objective functions, they are both
engaged in the same overall goal: that of recovering
a fluent sentence from the lattice.
We first define boolean indicator variables aki ?
Ak for every aligned phrase in each aligned link Ak
present in the intersection problem I. We also in-
troduce indicator variables pklij for every possible in-
terleaving path between aligned phrases aki and a
l
j .
The linear objective for I that maximizes the local
coherence of all phrases can be expressed as
f = max
?
Ak,Al?I
|Ak|?
i=0
|Al|?
j=0
pklij ? score(p
kl
ij )
where score(pklij ) is the normalized LM score of the
fragment of text representing aki p
kl
ij a
l
j . In other
words, the score for each interleaving path is cal-
culated by appending it and the two phrases it con-
nects into a single fragment of text and determining
the score of that fragment under an LM8.
6We use LPsolve: http://lpsolve.sourceforge.net/
7As noted by Clarke and Lapata (2008), normalizing LM
scores cannot be easily accomplished with linear constraints
and we do not have training data to devise appropriate word-
insertion penalties as used in MT.
8If the fragment of text is smaller than the LM size, we
consider additional sentence context around the aligned phrases
rather than backing off to a smaller LM size to avoid a bias to-
wards short but ungrammatical interleaving paths.
48
We now introduce linear constraints to keep the
problem well-formed. First, we add a restriction
to ensure that only one phrase from each alignment
link is present in the solution.
?
aki ?Ak
aki = 1 ?Ak ? I
We can also ensure that interleaving paths are only in
the solution when the aligned phrases that they con-
nect together are themselves present using the fol-
lowing set of constraints.
aki ?
|Ak|?
i=0
pk?i? = 1 ?a
k
i ? Ak, Ak ? I
alj ?
|Al|?
j=0
p?l?j = 1 ?a
l
j ? Al, Al ? I
pklij ? a
k
i <= 0 ?i, j, k, l
pklij ? a
l
j <= 0 ?i, j, k, l
As we don?t restrict the structure of the lattice in any
way and allow crossing alignment links, the program
as defined thus far is capable of generating cyclic
and fragmented solutions. To combat this, we add
dummy start and end phrase variables and introduce
additional single commodity flow constraints (Mag-
nanti and Wolsey, 1994) adapted from Martins et
al. (2009) over the interleaving paths to guarantee
that the output will only involve a linear sequence of
aligned phrases and paths.
5 Evaluation
We now turn to the design of experiments for the
strict sentence intersection task and discuss the per-
formance of the proposed models using the corpus
provided by McKeown et al (2010). We use a beam
size of 50 for the beam search decoder and a 4-gram
LM for all experiments. Dependency parsing is ac-
complished with MICA, a TAG-based parser (Ban-
galore et al, 2009). Our primary considerations
for studying system-generated fusions are validity
(whether the output contains only the information
common to each sentence), coverage (whether the
output contains all the common information in the
input sentences) and the fluency of the output.
5.1 Evaluating Validity and Fluency
Evaluating the validity of an intersection involves
determining whether it contains only the informa-
tion contained in each sentence and nothing else. In
order to do this, we make use of the interpretation of
valid intersections as being mutually entailed by the
input sentences. It follows that the task of judging
the validity of an intersection can simply be decom-
posed into two tasks that judge whether the intersec-
tion is entailed by each input sentence.
We make use of Amazon?s Mechanical Turk
(AMT) platform to have humans evaluate the in-
tersections produced. Crowdsourcing annotations
and judgments in this manner has been shown to be
cheap and effective for natural language tasks (Snow
et al, 2008) and has recently been employed in sim-
ilar entailment-detection tasks (Negri and Mehdad,
2010; Buzek et al, 2010). Since we only seek judg-
ments on produced intersections and avoid present-
ing both input sentences to users, we do not antic-
ipate the noisiness that was noted by McKeown et
al. (2010) when asking AMT users to generate in-
tersections.
Each entailment task is framed as a multiple
choice question. An AMT user is shown just one
input sentence (the premise in entailment terminol-
ogy) along with a potential intersection (the hypoth-
esis) and is required to respond to whether there is
any new or different information in the latter that is
not in the former. They can respond on a 3-point
scale (yes/no/maybe) where maybe is clarified to in-
clude ambiguous rewording in the intersection. For
a given intersection instance, the responses9 using
each input sentence as the premise are averaged sep-
arately and then combined10 to give a measure of
how well the intersection is entailed by both sen-
tences.
A second question allows the user to specify the
grammaticality of the intersection on a 4-point scale.
As this measure doesn?t depend on the input sen-
tence presented to the AMT user, all scores provided
are simply averaged per intersection.
9Each instance is presented to 6 AMT users, 3 per premise.
Responses were automatically filtered for spam and removing
the largest outlier from each per-premise or per-intersection
group did not yield a notable change in relative performance.
10We use the harmonic mean for combination, but the results
are largely similar when using an arithmetic mean.
49
Intersection output Fluency Validity
Aligned words (i) Home Secretary John Reid said the inquiry would go. 0.667 0.800
(ii) Home Secretary John Reid said the inquiry would expand. 0.778
Beam search Home Secretary John Reid said something about the inquiry would move
wherever ?the something take it?.
0.389 0.667
Segmented decoder Home Secretary John Reid said the inquiry would change. 0.944 0.909
Aligned words (i) Traces of have been found on the planes. 0.445 1.000
(ii) traces of had been found on the planes. 0.556
Beam search Small traces of some things have been found on the planes. 0.611 0.909
Segmented decoder Small traces of had been found on the planes. 0.500 0.741
Aligned words (i) Prosecutors allege that the accuser the program was molested after aired. 0.167 0.800
(ii) Prosecutors allege that the boy was molested after the program aired. 1.000
Beam search Prosecutors allege that the being, who did something in the program, was
molested after something about aired.
0.400 0.909
Segmented decoder Prosecutors allege that the organism, who did something, was molested
after the program aired.
0.667 0.857
Table 3: Intersections produced for the examples introduced in Table 2 along with judgments from AMT users.
Validity Fluency Har. Mean
Other sentence 0.188 0.945 0.314
Aligned words 0.863 0.563? 0.682?
Beam search 0.729 0.450 0.557
Segmented decoder 0.812? 0.504 0.622
Oracle combination 0.813? 0.575? 0.674?
Table 4: Results of the AMT evaluation described in ?5.1.
Statistically insignificant differences within columns are
indicated with ?; all other entries are significantly distinct
at p ? 0.05.
5.2 Results of AMT evaluation
Table 4 contains the results from this evaluation
over the McKeown et al (2010) corpus11 and Ta-
ble 3 shows the system-produced intersections cor-
responding to the examples from ?3. We report nor-
malized scores of validity and fluency for ease of
comparison, as well as their unweighted harmonic
mean as a crude measure of combined human judg-
ment. In addition to the beam search and segmented
decoders, we report the performance of two upper-
bound systems that present artificial hypothesis sen-
tences to AMT users. Other sentence is simply the
sentence that is not the current premise from the sen-
tence pair; although this is rarely an appropriate in-
tersection in the data, it is useful as a measure of
how well humans judge grammaticality and infor-
11The first 20 sentence pairs of the corpus were examined
when devising abstraction rules and are therefore excluded from
these results.
mation content. Aligned words is the aligned subset
of the premise sentence; this is quite likely to be con-
sidered a valid entailment by AMT users as no new
words are introduced. Although the latter also scores
surprisingly well on fluency, we must note that this
is not an actual intersection solution: the aligned
words displayed to AMT users for a given intersec-
tion instance are different depending on which input
sentence is displayed as the premise.
Turning to the systems under study, we observe
that the ILP-based segmented decoder produces text
that is judged more fluent on average than the beam
search decoder. In order to judge the degree of over-
lap between the two systems, we also report the
performance of a pseudo-hybrid oracle combination
system which assumes the presence of an oracle that
runs both decoders and always chooses the output
intersection that is more grammatical. The improved
performance illustrates that each decoder has its ad-
vantages and that a real hybrid system might yield
improvements over either approach.
5.3 Evaluating Coverage
While validity experiments test whether the pro-
posed intersections contain extraneous or unsup-
ported information, we also need to check whether
the intersections contain all the information that is
shared between the input sentences. This cannot be
factored into a task that involves only one input sen-
tence and therefore cannot be easily accomplished
50
BLEU NIST
Aligned words 0.682 11.10
Beam search 0.726 10.53
Segmented decoder 0.818 11.56
Table 5: Results of the automated evaluation for coverage
of intersections described in ?5.3.
without annotators who understand the concept of
intersection.
We instead attempt to utilize the high-quality
human-generated union dataset from McKeown et
al. (2010) in evaluating the coverage of our inter-
section systems. Using the simple absorption law
A ? (A ? B) = A, we assume that the coverage
of intersection systems can be judged by how well
they can recover an input sentence from human-
generated unions. The resulting outputs are com-
pared to the original input sentences in an MT-
style evaluation under two commonly-used metrics:
BLEU (Papineni et al, 2002) and NIST (Dodding-
ton, 2002).
The results of this automated evaluation are
shown in Table 5. The aligned words system here
always considers words from the union sentence and
can therefore be seen as a baseline system. We ob-
serve that the segmented decoder produces output
that is judged most similar to the input sentences
under BLEU, which measures n-gram overlap, al-
though results under NIST (which gives additional
weight to rarer n-grams) are less conclusive.
6 Discussion
The experimental results indicate that the two sys-
tems we describe, particularly the segmented de-
coder, do a reasonable job of finding valid intersec-
tions with good coverage; however, producing fluent
output remains a challenge. Analysis of the inter-
sections produced leads us to note that the quality
of interleaving paths is the prime obstacle to im-
proving intersection output (cf. Table 3): produc-
ing syntactically-valid textual abstractions to con-
nect text is a challenge that is not met by our sim-
ple rule-based approach. Furthermore, we notice
that the quality of alignment also factors in to this
problem: systems that miss phrases which should
be aligned or systems that mistakenly align faraway
fragments both cause spans of unaligned text that
must be then abstracted over.
We hypothesize that these issues could be tackled
with the use of joint models: a system that aligns
as it decodes could reduce the need for abstrac-
tion over long unaligned spans, although care would
have to be taken to ensure that coverage is main-
tained. Additionally, richer lexical resources such
as wider-coverage ontologies (Snow et al, 2006)
and entailment/paraphrase dictionaries could aid in
improving coverage. Finally, previous work in fu-
sion (Filippova and Strube, 2008b; Filippova and
Strube, 2009) has noted that models based on syntax
outperform techniques that rely solely on LM scores
to determine fluency, and strict intersection appears
to be well-suited for further exploration in this vein.
7 Conclusion
We have examined the text-to-text generation task of
strict sentence intersection, which restricts semantic
variation in the output and necessarily invokes the
problems of generalization and abstraction in addi-
tion to the usual challenge of producing fluent text.
We tackle the task as lattice decoding and discuss
two decoding strategies for producing valid intersec-
tions. In addition, we assume that strict intersec-
tion tasks are best considered as problems of mu-
tual entailment generation and describe evaluation
strategies for this task that make use of both human
judgments as well as automated metrics run over a
related corpus. Experimental results indicate that
these systems are fairly effective at generating valid
intersections and that our novel segmented decoder
strategy outperforms the traditional beam search ap-
proach. Although fluency remains a challenge, we
hypothesize that the use of joint models, syntac-
tic constraints and lexical resources could bring im-
provements.
Acknowledgments
The authors are grateful to the anonymous reviewers
for their helpful feedback. This material is based on
research supported in part by the U.S. National Sci-
ence Foundation (NSF) under IIS-05-34871. Any
opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the views of the
NSF.
51
References
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen
Rambow, and Beno??t Sagot. 2009. MICA: a prob-
abilistic dependency parser based on tree insertion
grammars. In Proceedings of HLT-NAACL: Short Pa-
pers, pages 185?188.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL Recognising Textual En-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Jeremy Bensley and Andrew Hickl. 2008. Unsupervised
resource creation for textual inference applications. In
Proceedings of LREC.
Chris Brockett. 2007. Aligning the 2006 RTE cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.
Olivia Buzek, Philip Resnik, and Benjamin B. Bederson.
2010. Error driven paraphrase annotation using me-
chanical turk. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 217?221.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: an integer linear pro-
gramming approach. Journal of Artifical Intelligence
Research, 31:399?429, March.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Hal Daume? III and Daniel Marcu. 2004. Generic sen-
tence fusion is an ill-defined summarization task. In
Proceedings of the ACL Text Summarization Branches
Out Workshop, pages 96?103.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of HLT, pages 138?145.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of COLING,
page 397.
Katja Filippova and Michael Strube. 2008a. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference, INLG ?08, pages 25?32.
Katja Filippova and Michael Strube. 2008b. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177?185.
Katja Filippova and Michael Strube. 2009. Tree
linearization in English: improving language model
based approaches. In Proceedings of NAACL, pages
225?228.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for English. In Proceedings of NAACL,
NAACL ?03, pages 17?23.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of NAACL, pages 178?185.
Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Proceedings of ACL, pages 193?196.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP, pages 802?811.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees. In Technical Report 290-94,
Massechusetts Institute of Technology, Operations Re-
search Center.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European
Workshop on Natural Language Generation, pages
109?117.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Wal-
ter Daelemans. 2010. On the limits of sentence
compression by deletion. In Emiel Krahmer and
Marie?t Theune, editors, Empirical Methods in Natural
Language Generation, pages 45?66. Springer-Verlag,
Berlin, Heidelberg.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP, pages 342?350.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Kathleen McKeown, Sara Rosenthal, Kapil Thadani, and
Coleman Moore. 2010. Time-efficient creation of an
accurate sentence fusion corpus. In Proceedings of
NAACL-HLT.
George A. Miller. 1995. Wordnet: a lexical database
for english. Communications of the ACM, 38:39?41,
November.
Matteo Negri and Yashar Mehdad. 2010. Creating a
bi-lingual entailment corpus through translations with
mechanical turk: $100 for a 10-day rush. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
52
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 212?216.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
ACL ?02, pages 311?318, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL, pages 801?808.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
Radu Soricut and Daniel Marcu. 2006. Stochastic lan-
guage generation using widl-expressions and its appli-
cation in machine translation and summarization. In
Proceedings of ACL, pages 1105?1112.
Kapil Thadani and Kathleen McKeown. 2008. A frame-
work for identifying textual redundancy. In Proceed-
ings of COLING, pages 873?880.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and syntactically-informed decoding for monolingual
phrase-based alignment. In Proceedings of ACL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL, pages 290?297.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of EMNLP, EMNLP ?10, pages
513?523.
53
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 37?45,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Detecting Influencers in Written Online Conversations
Or Biran1* Sara Rosenthal1* Jacob Andreas1**
Kathleen McKeown1* Owen Rambow2?
1 Department of Computer Science, Columbia University, New York, NY 10027
2 Center for Computational Learning Systems, Columbia University, New York, NY 10027
* {orb, sara, kathy}@cs.columbia.edu
** jda2129@columbia.edu ?rambow@ccls.columbia.edu
Abstract
It has long been established that there is a cor-
relation between the dialog behavior of a par-
ticipant and how influential he or she is per-
ceived to be by other discourse participants.
In this paper we explore the characteristics of
communication that make someone an opinion
leader and develop a machine learning based
approach for the automatic identification of
discourse participants that are likely to be in-
fluencers in online communication. Our ap-
proach relies on identification of three types
of conversational behavior: persuasion, agree-
ment/disagreement, and dialog patterns.
1 Introduction
In any communicative setting where beliefs are ex-
pressed, some are more influential than others. An
influencer can alter the opinions of their audience,
resolve disagreements where no one else can, be rec-
ognized by others as one who makes important con-
tributions, and often continue to influence a group
even when not present. Other conversational par-
ticipants often adopt their ideas and even the words
they use to express their ideas. These forms of per-
sonal influence (Katz and Lazarsfeld, 1955) are part
of what makes someone an opinion leader. In this
paper, we explore the characteristics of communica-
tion that make someone an opinion leader and de-
velop a machine learning based approach for the au-
tomatic identification of discourse participants who
are likely to be influencers in online communication.
Detecting influential people in online conversa-
tional situations has relevance to online advertising
strategies which exploit the power of peer influence
on sites such as Facebook. It has relevance to analy-
sis of political postings, in order to determine which
candidate has more appeal or which campaign strat-
egy is most successful. It is also relevant for design-
ing automatic discourse participants for online dis-
cussions (?chatbots?) as it can provide insight into
effective communication. Despite potential applica-
tions, analysis of influence in online communication
is a new field of study in part because of the rela-
tively recent explosion of social media. Thus, there
is not an established body of theoretical literature in
this area, nor are there established implementations
on which to improve. Given this new direction for
research, our approach draws on theories that have
been developed for identifying influence in spoken
dialog and extends them for online, written dialog.
We hypothesize that an influencer, or an influencer?s
conversational partner, is likely to engage in the fol-
lowing conversational behaviors:
Persuasion: An influencer is more likely to express
personal opinions with follow-up (e.g., justification,
reiteration) in order to convince others.
Agreement/disagreement: A conversational partner
is more likely to agree with an influencer, thus im-
plicitly adopting his opinions.
Dialog Patterns: An influencer is more likely to par-
ticipate in certain patterns of dialog, for example
initiating new topics of conversation, contributing
more to dialog than others, and engendering longer
dialog threads on the same topic.
Our implementation of this approach comprises
a system component for each of these conversa-
tional behaviors. These components in turn provide
37
the features that are the basis of a machine learn-
ing approach for the detection of likely influencers.
We test this approach on two different datasets, one
drawn from Wikipedia discussion threads and the
other drawn from LiveJournal weblogs. Our results
show that the system performs better for detection
of influencer on LiveJournal and that there are in-
teresting differences across genres for detecting the
different forms of conversational behavior.
The paper is structured as follows. After review-
ing related work, we define influence, present our
data and methods. We present a short overview of
the black box components we use for persuasion and
detection of agreement/disagreement, but our focus
is on the development of the influencer system as a
whole and thus we spend most time exploring the
results of experimentation with the system on dif-
ferent data sets, analyzing which components have
most impact. We first review related work.
2 Related Work
It has long been established that there is a correlation
between the conversational behavior of a discourse
participant and how influential he or she is perceived
to be by the other discourse participants (Bales et al,
1951; Scherer, 1979; Brook and Ng, 1986; Ng et al,
1993; Ng et al, 1995). Specifically, factors such as
frequency of contribution, proportion of turns, and
number of successful interruptions have been identi-
fied as being important indicators of influence. Reid
and Ng (2000) explain this correlation by saying that
?conversational turns function as a resource for es-
tablishing influence?: discourse participants can ma-
nipulate the dialog structure in order to gain influ-
ence. This echoes a starker formulation by Bales
(1970): ?To take up time speaking in a small group
is to exercise power over the other members for at
least the duration of the time taken, regardless of the
content.? Simply claiming the conversational floor is
a feat of power. This previous work presents two is-
sues for a study aimed at detecting influence in writ-
ten online conversations.
First, we expect the basic insight ? conversation
as a resource for influence ? to carry over to written
dialog: we expect to be able to detect influence in
written dialog as well. However, some of the charac-
teristics of spoken dialog do not carry over straight-
forwardly to written dialog, most prominently the
important issue of interruptions: there is no interrup-
tion in written dialog. Our work draws on findings
for spoken dialog, but we identify characteristics of
written dialog which are relevant to influence.
Second, the insistence of Bales (1970) that power
is exercised through turn taking ?regardless of con-
tent? may be too strong. Reid and Ng (2000) discuss
experiments which address not just discourse struc-
ture features, but also a content feature which repre-
sents how closely a turn is aligned with the overall
discourse goal of one of two opposing groups (with
opposing opinions on a specific issue) participating
in the conversation. They show that interruptions are
more successful if aligned with the discourse goal.
They propose a model in which such utterances
?lead to participation which in turn predicts social
influence?, so that the correlation between discourse
structure and influence is really a secondary phe-
nomenon. However, transferring such results to
other types of interactions (for example, in which
there are not two well-defined groups) is challeng-
ing. In this study, we therefore examine two types of
features as they relate to influence: content-related
(persuasion and agreement/disagreement), and dis-
course structure-related.
So far, there has been little work in NLP related
to influencers. Quercia et al (2011) look at influ-
encers? language use in Twitter contrasted to other
users? groups and find some significant differences.
However, their analysis and definition relies quite
heavily on the particular nature of social activity
on Twitter. Rienks (2007) discusses detecting influ-
encers in a corpus of conversations. While he fo-
cuses entirely on non-linguistic behavior, he does
look at (verbal) interruptions and topic initiations
which can be seen as corresponding to some of our
Dialog Patterns Language Uses.
3 What is an Influencer?
Our definition of an influencer was collectively for-
mulated by a community of researchers involved in
the IARPA funded project on Socio Cultural Content
in Language (SCIL).
This group defines an influencer to be someone
who:
38
P1 by Arcadian <pc1>There seems to be a much better list at the National Cancer Institute than the one we?ve
got.</pc1><pa1>It ties much better to the actual publication (the same 11 sections, in the same order).</pa1>
I?d like to replace that section in this article. Any objections?
P2 by JFW <pc2><a1>Not a problem.</a1></pc2>Perhaps we can also insert the relative incidence as
published in this month?s wiki Blood journal
P3 by Arcadian I?ve made the update. I?ve included template links to a source that supports looking up
information by ICD-O code.
P4 by Emmanuelm Can Arcadian tell me why he/she included the leukemia classification to this lymphoma
page? It is not even listed in the Wikipedia leukemia page! <pc3>I vote for dividing the WHO classification
into 4 parts in 4 distinct pages: leukemia, lymphoma, histocytic and mastocytic neoplasms.</pc3><pa3>
Remember, Wikipedia is meant to be readable </pa3>by all. Let me know what you think before I delete
the non-lymphoma parts.
P5 by Arcadian Emmanuelm, aren?t you the person who added those other categories on 6 July 2005?
P6 by Emmanuelm <d1>Arcadian, I added only the lymphoma portion of the WHO classification.
You added the leukemias on Dec 29th.</d1>Would you mind moving the leukemia portion to the
leukemia page?
P7 by Emmanuelm <pc4>Oh, and please note that I would be very comfortable with a ?cross-coverage?
of lymphocytic leukemias in both pages.</pc4>My comment is really about myeloid, histiocytic and
mast cell neoplasms who share no real relationship with lymphomas.
P8 by Arcadian <pa5><a2>To simplify the discussion, I have restored that section to your version.
</a2></pa5>You may make any further edits, and <pc6>I will have no objection.</pc6>
P9 by JFW The full list should be on the hematological malignancy page, and the lymphoma part can be here.
<pc7>It would be defendable to list ALL and CLL here.</pc7><pa7>They fall under the lymphoproliferative
disorders.</pa7>
Table 1: Influence Example: A Wikipedia discussion thread displaying Emmanuelm as the influencer. Replies are
indicated by indentation (for example, P2 is a response to P1). All Language Uses are visible in this example: Attempt
to Persuade ({pci, pai}), Claims (pci), Argumentation (pai), Agreement (ai), Disagreement (di), and the five Dialog
Patterns Language Uses (eg. Arcadian has positive Initiative).
1. Has credibility in the group.
2. Persists in attempting to convince others, even if
some disagreement occurs
3. Introduces topics/ideas that others pick up on or
support.
By credibility, we mean someone whose ideas are
adopted by others or whose authority is explicitly
recognized. We hypothesize that this shows up
through agreement by other conversants. By per-
sists, we mean someone who is able to eventually
convince others and often takes the time to do so,
even if it is not quick. This aspect of our definition
corresponds to earlier work in spoken dialog which
shows that frequency of contributions and propor-
tion of turns is a method people use to gain influence
(Reid and Ng, 2000; Bales, 1970). By point 3, we
see that the influencer may be influential even in di-
recting where the conversation goes, discussing top-
ics that are of interest to others. This latter feature
can be measured through the discourse structure of
the interaction. The influencer must be a group par-
ticipant but need not be active in the discussion(s)
where others support/credit him.
The instructions that we provided to annotators
included this definition as well as examples of who
is not an influencer. We told annotators that if some-
one is in a hierarchical power relation (e.g., a boss),
then that person is not an influencer to sub-ordinates
(or, that is not the type of influencer we are look-
ing for). We also included someone with situational
power (e.g., authority to approve other?s actions) or
power in directing the communication (e.g., a mod-
erator) as negative examples.
We also gave positive examples of influencers. In-
fluencers include an active participant who argues
against a disorganized group and resolves a discus-
sion is an influencer, a person who provides an an-
swer to a posted question and the answer is accepted
after discussion, and a person who brings knowledge
to a discussion. We also provided positive and neg-
39
ative examples for some of these cases.
Table 1 shows an example of a dialog where there
is evidence of influence, drawn from a Wikipedia
Talk page. A participant (Arcadian) starts the thread
with a proposal and a request for support from other
participants. The influencer (Emmanuelm) later
joins the conversation arguing against Arcadian?s
proposal. There is a short discussion, and Arcadian
defers to Emmanuelm?s position. This is one piece
of dialog within this group where Emmanuelm may
demonstrate influence. The goal of our system is to
find evidence for situations like this, which suggests
that a person is more likely to be an influencer.
Since we attempt to find local influence (a per-
son who is influential in a particular thread, as op-
posed to influential in general), our notion of influ-
encer is consistent with diverse views on social in-
fluence. It is consistent with the definition of influ-
encer proposed by Gladwell (2001) and Katz (1957):
an exceptionally convincing and influential person,
set apart from everyone else by his or her ability to
spread opinions. While it superficially seems incon-
sistent with Duncan Watts? concept of ?accidental
influentials? (Watts, 2007), that view does not make
the assertion that a person cannot be influential in
a particular situation (in fact, it predicts that some-
one will) - only that one cannot in general identify
people who are always more likely to be influencers.
4 Data and Annotation
Our data set consists of documents from two differ-
ent online sources: weblogs from LiveJournal and
discussion forums from Wikipedia.
LiveJournal is a virtual community in which peo-
ple write about their personal experiences in a we-
blog. A LiveJournal entry is composed of a post
(the top-level content written by the author) and a
set of comments (written by other users and the au-
thor). Every comment structurally descends either
from the post or from another comment.
Each article on Wikipedia has a discussion forum
(called a Talk page) associated with it that is used
to discuss edits for the page. Each forum is com-
posed of a number of threads with explicit topics,
and each thread is composed of a set of posts made
by contributors. The posts in a Wikipedia discussion
thread may or may not structurally descend from
other posts: direct replies to a post typically descend
from it. Other posts can be seen as descending from
the topic of the thread.
For consistency of terms, from here on we refer to
each weblog or discussion forum thread as a thread
and to each post or comment as a post.
We have a total of 333 threads: 245 from Live-
Journal and 88 from Wikipedia. All were annotated
for influencers. The threads were annotated by two
undergraduate students of liberal arts. These stu-
dents had no prior training or linguistic background.
The annotators were given the full definition from
section 3 and asked to list the participants that they
thought were influencers. Each thread may in princi-
ple have any number of influencers, but one or zero
influencers per thread is the common case and the
maximal number of influencers found in our dataset
was two. The inter-annotator agreement on whether
or not a participant is an influencer (given by Co-
hen?s Kappa) is 0.72.
5 Method
Our approach is based on three conversational be-
haviors which are identified by separate system
components described in the following three sec-
tions. Figure 1 shows the pipeline of the Influencer
system and Table 1 displays a Wikipedia discussion
thread where there is evidence of an influencer and
in which we have indicated the conversational be-
haviors as they occur. Motivated by our definition,
each component is concerned with an aspect of the
likely influencer?s discourse behavior:
Persuasion examines the participant?s language to
identify attempts to persuade, such as {pc1, pa1} in
Table 1, which consist of claims (e.g. pc1) made
by the participant and supported by argumentations
(e.g. pa1). It also identifies claims and argumenta-
tions independently of one another (pc4 and pa5).
Agreement/Disagreement examines the other par-
ticipants? language to find how often they agree or
disagree with the participant?s statements. Examples
are a1 and d1 in Table 1.
Dialog Patterns examines how the participant inter-
acts in the discussion structurally, independently of
the content and the language used. An example of
this is Arcadian being the first poster and contribut-
ing the most posts in the thread in Table 1.
40
Figure 1: The influencer pipeline. Solid lines indicate
black-box components, which we only summarize in this
paper. Dashed lines indicate components described here.
Each component contributes a number of Lan-
guage Uses which fall into that category of conver-
sational behavior and these Language Uses are used
directly as features in a supervised machine learn-
ing model to predict whether or not a participant is
an influencer. For example, Dialog Patterns con-
tributes the Language Uses Initiative, Irrelevance,
Incitation, Investment and Interjection.
The Language Uses of the Persuasion and Agree-
ment/Disagreement components are not described in
detail in this paper, and instead are treated as black
boxes (indicated by solid boxes in Figure 1). We
have previously published work on some of these
(Biran and Rambow, 2011; Andreas et al, 2012).
The remainder of this section describes them briefly
and provides the results of evaluations of their per-
formance (in Table 2). The next section describes
the features of the Dialog Patterns component.
5.1 Persuasion
This component identifies three Language Uses: At-
tempt to Persuade, Claims and Argumentation.
We define an attempt to persuade as a set of con-
tributions made by a single participant which may
be made anywhere within the thread, and which are
all concerned with stating and supporting a single
claim. The subject of the claim does not matter:
an opinion may seem trivial, but the argument could
still have the structure of a persuasion.
Our entire data set was annotated for attempts to
persuade. The annotators labeled the text partici-
pating in each instance with either claim, the stated
opinion of which the author is trying to persuade
others or argumentation, an argument or evidence
that supports that claim. An attempt to persuade
must contain exactly one claim and at least one in-
stance of argumentation, like the {claim, argumen-
tation} pairs {pc1, pa1} and {pc3, pj3} in Table 1.
In addition to the complete attempt to persuade
Language Use, we also define the less strict Lan-
guage Uses claims and argumentation, which use
only the subcomponents as stand-alones.
Our work on argumentation, which builds on
Rhetorical Structure Theory (Mann and Thompson,
1988), is described in (Biran and Rambow, 2011).
5.2 Agreement/Disagreement
Agreement and disagreement are two Language
Uses that model others? acceptance of the partici-
pant?s statements. Annotation (Andreas et al, 2012)
is performed on pairs of phrases, {p1, p2}. A phrase
is a substring of a post or comment in a thread. The
annotations are directed since each post or comment
has a time stamp associated with it. This means that
p1 and p2 are not interchangeable. p1 is called the
?target phrase?, and p2 is called the ?subject phrase?.
A person cannot agree with him- or herself, so the
author of p1 and p2 cannot be the same. Each anno-
tation is also labeled with a type: either ?agreement?
or ?disagreement?.
6 Dialog Patterns
The Dialog Patterns component extracts features
based on the structure of the thread. Blogs and dis-
cussion threads have a tree structure, with a blog
post or a topic of discussion as the root and a set of
41
Component Wikipedia LiveJournal
P R F P R F
Attempt 79.1 69.6 74 57.5 48.2 52.4
to persuade
Claims 83.6 74.5 78.8 53.7 13.8 22
Argumentation 23.3 91.7 37.1 30.9 48.9 37.8
Agreement 12 31.9 17.4 20 50 28.6
Disagreement 8.7 9.5 9.1 6.3 14.3 8.7
Table 2: Performance of the black-box Language Uses in
terms of Precision (P), Recall (R), and F-measure(F).
Conversational
Behavior
Language Use
(Feature)
Users
Component A J E
Persuasion Claims 2/6 2/6 2/6
Argumentation Y Y Y
Attempt to Per-
suade
Y Y Y
Agreement/ Agreement 1/1 0/1 0/1
Disagreement Disagreement 1/1 0/1 0/1
Dialog Initiative Y N N
Patterns Irrelevance 2/4 1/2 1/3
Incitation 4 1 3
Interjection 1/9 2/9 4/9
Investment 4/9 2/9 3/9
Table 3: The feature values for each of the partici-
pants, Arcadian (A), JFW (J), and Emmanuelm (E), in
the Wikipedia discussion thread shown in Table 1.
comments or posts which are marked as a reply - ei-
ther to the root or to an earlier post. The hypothesis
behind Dialog Patterns is that influencers have typ-
ical ways in which they participate in a thread and
which are visible from the structure alone.
The Dialog Patterns component contains five sim-
ple Language Uses:
Initiative The participant is or is not the first poster
of the thread.
Irrelevance The percentage of the participant?s
posts that are not replied to by anyone.
Incitation The length of the longest branch of
posts which follows one of the participant?s posts.
Intuitively, the longest discussion started directly by
the participant.
Investment The participant?s percentage of all posts
in the thread.
Interjection The point in the thread, represented
as percentage of posts already posted, at which the
participant enters the discussion.
7 System and Evaluation
The task of the system is to decide for each partici-
pant in a thread whether or not he or she is an influ-
encer in that particular thread. It is realized with a
supervised learning model: we train an SVM with a
small number of features, namely the ten Language
Uses. One of our goals in this work is to evaluate
which Language Uses allow us to more accurately
classify someone as an influencer. Table 3 shows
the full feature set and feature values for the sample
discussion thread in Table 1. We experimented with
a number of different classification methods, includ-
ing bayesian and rule-based models, and found that
SVM produced the best results.
7.1 Evaluation
We evaluated on Wikipedia and LiveJournal sepa-
rately. The data set for each corpus consists of all
participants in all threads for which there was at least
one influencer. We exclude threads for which no in-
fluencer was found, narrowing our task to finding the
influencers where they exist. For each participant X
in each thread Y, the system answers the following
question: Is X an influencer in Y?
We used a stratified 10-fold cross validation of
each data set for evaluation, ensuring that the same
participant (from two different threads) never ap-
peared in both training and test at each fold, to elim-
inate potential bias from fitting to a particular partic-
ipant?s style. The system components were identical
when evaluating both data sets, except for the claims
system which was trained on sentiment-annotated
data from the corpus on which it was evaluated.
Table 4 shows the performance of the full system
and of systems using only one Language Use feature
compared against a baseline which always answers
positively (X is always an influencer in Y). It also
shows the performance for the best system, which
was found for each data set by looking at all possible
combinations of the features. The best system for
the Wikipedia data set is composed of four features:
Claims, Argumentation, Agreement and Investment.
The best LiveJournal system is composed of all five
Dialog Patterns features, Attempt to Persuade and
Argumentation. We found our results to be statis-
42
System Wikipedia LiveJournal
P R F P R F
Baseline: all-
yes
16.2 100 27.9 19.2 19.2 32.2
Full 40.5 80.5 53.9 61.7 82 70.4
Initiative 31.6 31.2 31.4 73.5 72.7 73.1
Irrelevance 21.7 77.9 34 19.2 100 32.2
Incitation 28.3 77.9 41.5 49.5 73.8 59.2
Investment 43 71.4 53.7 50.2 75.4 60.3
Interjection 24.7 88.3 38.6 36.9 91.3 52.5
Agreement 36 46.8 40.7 45.1 82.5 58.3
Disagreement 35.3 70.1 47 19.2 100 32.2
Claims 40 72.7 51.6 54.3 76 63.3
Argumentation 19 98.7 31.8 31.1 85.2 45.6
Attempt 23.7 79.2 36.5 37.4 48.1 42.1
to persuade
Best system 47 80.5 59.3 66.2 84.7 74.3
Table 4: Performance in terms of Precision (P), Recall
(R), and F-measure (F) using the baseline (everyone is an
influencer), all features (full), individual features one at a
time, and the best feature combination for each data set.
tically significant (with the Bonferroni adjustment)
in paired permutation tests between the best system,
the full system and the baseline of each data set.
When we first performed these experiments, we
used all threads in the data set. The performance on
this full set was lower, as shown in Table 5 due to
the presence of threads with no influencers. Threads
in which the annotators could not find a clear influ-
encer tend to be of a different nature: there is either
no clear topic of discussion, or no argument (every-
one is in agreement). We leave the task of distin-
guishing these threads from those which are likely
to have an influencer to future work.
7.2 Evaluating with Perfect Components
In a hierarchical system such as ours, errors can
be attributed to imperfect components or to a bad
choice of features, so it is important to look at the
potential contribution of the components. As an ex-
ample, Table 6 shows the difference between our
Attempt to Persuade system and a hypothetical per-
fect Attempt to Persuade component, simulated by
using the gold annotations, when predicting influ-
encer directly (i.e., a participant is an influencer iff
she makes an attempt to persuade).
Clearly, when predicting influencers, Attempt to
System Wikipedia LiveJournal
P R F P R F
Baseline 13.9 100 24.5 14.2 100 24.9
Full 36.7 79.2 50.2 46.3 79.8 58.6
Best 40.1 76.6 52.7 48.2 81.4 60.6
Table 5: Performance on the data set of all threads, in-
cluding those with no influencers. The ?Best System? is
the system that performed best on the filtered data set.
Data Set Our System Gold Answers
P R F P R F
Wikipedia 23.6 69.4 35.2 23.8 81.6 36.9
LiveJournal 37.5 48.1 42.1 40.7 61.8 49
Table 6: Performance of the Attempt to Persuade compo-
nent in directly predicting influencers. A comparison of
our system and the component?s gold annotation. These
experiments were run on the full data set, which is why
the system results are not exactly those of Table 4.
Persuade is a stronger indicator in LiveJournal than
it is in Wikipedia. However, as shown in Table 2,
our Attempt to Persuade system performs better on
Wikipedia. This situation is reflected in Table 6,
where the lower quality of the system component in
LiveJournal corresponds to a significantly lower per-
formance when applied to the influencer task. These
results demonstrate that Attempt to Persuade is a
good feature: a more precise feature value means
higher predictability of influencer. In the future we
will perform similar analyses for the other features.
8 Discussion
We evaluated our system on two corpora - Live-
Journal and Wikipedia discussions - which differ in
structure, context and discussion topics. As our re-
sults show, they also differ in the way influencers
behave and the way others respond to them. To
illustrate the differences, we contrast the sample
Wikipedia thread (Table 1) with an example from
LiveJournal (Table 7).
It is common in LiveJournal for the blogger to be
an influencer, as is the case in our example thread,
because the topic of the thread is set by the blog-
ger and comments are typically made by her friends.
This fact is reflected in our results: Initiative is a
very strong indicator in LiveJournal, but not so in
43
P1 by poconell <pc1>He really does make good on his promises! </pc1><pa1>Day three in office, and the
Global Gag Rule (A.K.A?The Mexico City Policy?) is gone!</pa1>I was holding my breath, hoping it
wouldn?t be left forgotte. He didn?t wait. <pc2>He can see the danger and risk in this policy, and the damage
it has caused to women and families.</pc2><pc3>I love that man!</pc3>
P2 by thalialunacy <a1>I literally shrieked ?HELL YES!? in my car when I heard. :D:D:D</a1>
P3 by poconell <a2>Yeah, me too</a2>
P4 by lunalovepotter <pc4><a3>He is SO AWESOME!</a3></pc4><pa4>Right down to business, no
ifs, ands, or buts! :D</pa4>
P5 by poconell <pc5>It?s amazing to see him so serious too!</pc5><pa5>This is one tough,
no-nonsense man!</pa5>
P6 by penny sieve My icon says it all :)
P7 by poconell <pc6>And I?m jealous of you with that President!</pc6><pa6>We tried to overthrow
our Prime Minister, but he went crying to the Governor General. </pa6>
Table 7: Influence Example: A LiveJournal discussion thread displaying poconell as the influencer. All the Language
Uses are visible in this example: agreement/disagreement (ai/di), persuasion ({pci, pai}, pci, pai), and dialog patterns
(eg. poconell has positive Initiative). This example is very different from the Wikipedia example in Table 1.
Wikipedia, where the discussion is between a group
of editors, all of whom are equally interested in the
topic. In general, the Dialog Patterns features are
stronger in LiveJournal. We believe this is due to the
fact that the tree structure in LiveJournal is strictly
enforced. In Wikipedia, people do not always reply
directly to the relevant post. Investment is the excep-
tion: it does not make use of the tree structure, and
is therefore an important indicator in Wikipedia.
Attempt to Persuade is useful in LiveJournal (the
influencer poconell makes three attempts to per-
suade in Table 7) but less so in Wikipedia. This is
explained by the precision of the gold system in Ta-
ble 6. Only 23.8% of those who attempt to persuade
in Wikipedia are influencers, compared with 40.7%
in LiveJournal. Attempts to Persuade are more com-
mon in Wikipedia (all participants attempt to per-
suade in Table 1), since people write there specifi-
cally to argue their opinion on how the article should
be edited. Conversely, agreement is a stronger pre-
dictor of influence in Wikipedia than in LiveJournal;
we believe that is because of a similar phenomenon,
that people in LiveJournal (who tend to know each
other) agree with each other more often. Disagree-
ment is not a strong indicator for either corpus which
may say something about influencers in general -
they can be disagreed with as often as anyone else.
9 Conclusion and Future Work
We have studied the relevance of content-related
conversational behavior (persuasion and agree-
ment/disagreement), and discourse structure-related
conversational behavior to detection of influence.
Identifying influencers is a hard task, but we are
able to show good results on the LiveJournal corpus
where we achieve an F-measure of 74.3%. Despite
a lower performance on Wikipedia, we are still able
to significantly outperform the baseline which yields
only 28.2%. Differences in performance between
the two seem to be attributable in part to the more
straightforward dialog structure in LiveJournal.
There are several areas for future work. In our
current work, we train and evaluate separately for
our two corpora. Alternatively, we could investigate
different training and testing combinations: train on
one corpus and evaluate on the other; a mixed cor-
pus for training and testing; genre-independent cri-
teria for developing different systems (e.g. length of
thread). We will also evaluate on new genres (such
as the Enron emails) in order to gain an appreciation
of how different genres of written dialog are.
Acknowledgment
This work has been supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Army Research Laboratory (ARL) contract num-
ber W911NF-09-C-0141. The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon.
44
References
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagreement
in threaded discussion. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Computation (LREC), Istanbul, Turkey, May.
R. F. Bales, Strodtbeck, Mills F. L., T. M., and M. Rose-
borough. 1951. Channels of communication in small
groups. American Sociological Review, pages 16(4),
461?468.
R. F. Bales. 1970. Personality and interpersonal be-
haviour.
Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialog. In Proceedings of the Fifth
IEEE International Conference on Semantic Comput-
ing.
M.E. Brook and S. H. Ng. 1986. Language and social
influence in small conversational groups. Journal of
Language and Social Psychology, pages 5(3), 201?
210.
Malcolm Gladwell. 2001. The tipping point: how little
things can make a big difference. Abacus.
Elihu Katz and Paul F. Lazarsfeld. 1955. Personal in-
fluence. Free Press, Glencoe, IL. by Elihu Katz and
Paul F. Lazarsfeld. With a foreword by Elmo Roper.
?A report of the Bureau of Applied Social Research,
Columbia University.? Bibliography: p. 381-393.
E. Katz. 1957. The Two-Step Flow of Communication:
An Up-To-Date Report on an Hypothesis. Bobbs-
Merrill Reprint Series in the Social Sciences, S137.
Ardent Media.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
S. H. Ng, D. Bell, and M. Brooke. 1993. Gaining turns
and achieving high in influence ranking in small con-
versational groups. British Journal of Social Psychol-
ogy, pages 32, 265?275.
S. H. Ng, M Brooke, and M. Dunne. 1995. Interruption
and in influence in discussion groups. Journal of Lan-
guage and Social Psychology, pages 14(4),369?381.
Daniele Quercia, Jonathan Ellis, Licia Capra, and Jon
Crowcroft. 2011. In the mood for being influential on
twitter. In SocialCom/PASSAT, pages 307?314. IEEE.
Scott A. Reid and Sik Hung Ng. 2000. Conversation as a
resource for in influence: evidence for prototypical ar-
guments and social identification processes. European
Journal of Social Psychology, pages 30, 83?100.
Rutger Joeri Rienks. 2007. Meetings in smart environ-
ments : implications of progressing technology. Ph.D.
thesis, Enschede, the Netherlands, July.
K. R. Scherer. 1979. Voice and speech correlates of per-
ceived social influence in simulated juries. In H. Giles
and R. St Clair (Eds), Language and social psychol-
ogy, pages 88?120. Oxford: Blackwell.
Duncan Watts. 2007. The accidental influentials. Har-
vard Business Review.
45
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 85?92,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Technologies in IBM WatsonTM
Alfio Gliozzo
IBM Watson Research Center
Yorktown Heights, NY 10598
gliozzo@us.ibm.com
Or Biran
Columbia University
New York, NY 10027
orb@cs.columbia.edu
Siddharth Patwardhan
IBM Watson Research Center
Yorktown Heights, NY 10598
siddharth@us.ibm.com
Kathleen McKeown
Columbia University
New York, NY 10027
kathy@cs.columbia.edu
Abstract
This paper describes a seminar course de-
signed by IBM and Columbia University
on the topic of Semantic Technologies,
in particular as used in IBM WatsonTM
? a large scale Question Answering sys-
tem which famously won at Jeopardy! R?
against two human grand champions. It
was first offered at Columbia University
during the 2013 spring semester, and will
be offered at other institutions starting in
the fall semester. We describe the course?s
first successful run and its unique features:
a class centered around a specific indus-
trial technology; a large-scale class project
which student teams can choose to par-
ticipate in and which serves as the ba-
sis for an open source project that will
continue to grow each time the course is
offered; publishable papers, demos and
start-up ideas; evidence that the course can
be self-evaluating, which makes it poten-
tially appropriate for an online setting; and
a unique model where a large company
trains instructors and contributes to creat-
ing educational material at no charge to
qualifying institutions.
1 Introduction
In 2007, IBM Research took on the grand chal-
lenge of building a computer system that can per-
form well enough on open-domain question an-
swering to compete with champions at the game of
Jeopardy! In 2011, the open-domain question an-
swering system dubbed Watson beat the two high-
est ranked players in a two-game Jeopardy! match.
To be successful at Jeopardy!, players must re-
tain enormous amounts of information, must have
strong language skills, must be able to understand
precisely what is being asked, and must accurately
determine the likelihood they know the right an-
swer. Over a four year period, the team at IBM
developed the Watson system that competed on
Jeopardy! and the underlying DeepQA question
answering technology (Ferrucci et al, 2010). Wat-
son played many games of Jeopardy! against cel-
ebrated Jeopardy! champions and, in games tele-
vised in February 2011, won against the greatest
players of all time, Ken Jennings and Brad Rutter.
DeepQA has applications well beyond Jeop-
ardy!, however. DeepQA is a software architec-
ture for analyzing natural language content in both
questions and knowledge sources. DeepQA dis-
covers and evaluates potential answers and gathers
and scores evidence for those answers in both un-
structured sources, such as natural language doc-
uments, and structured sources such as relational
databases and knowledge bases. Figure 1 presents
a high-level view of the DeepQA architecture.
DeepQA utilizes a massively parallel, component-
based pipeline architecture (Ferrucci, 2012) which
uses an extensible set of structured and unstruc-
tured content sources as well as a broad range of
pluggable search and scoring components that al-
low integration of many different analytic tech-
niques. Machine Learning techniques are used to
learn the weights for each scoring component in
order to combine them into a single final score.
Watson components include a large variety of state
of the art solutions originating in the fields of Nat-
ural Language Processing (NLP), Machine Learn-
ing (ML), Information Retrieval (IR), Semantic
Web and Cloud Computing. IBM is now aggres-
sively investing in turning IBM Watson from a re-
search prototype to an industry level highly adapt-
able system to be applied in dozens of business ap-
85
Figure 1: Overview of the DeepQA architecture
plications ranging from healthcare to finance (Fer-
rucci et al, 2012).
Finding that particular combination of skills in
the entry-level job market is hard: in many cases
students have some notion of Machine Learning
but are not strong in Natural Language Processing;
in other cases they have background in Knowledge
Management and some of the basics of Semantic
Web, but lack an understanding of statistical mod-
els and Machine Learning. In most cases semantic
integration is not a topic of interest, and so un-
derstanding sophisticated platforms like Apache
UIMATM (Ferrucci and Lally, 2004) is a chal-
lenge. Learning how to develop the large scale in-
frastructure and technology needed for IBM Wat-
son prepares students for the real-world challenges
of large-scale natural language projects that are
common in industry settings and which students
have little experience with before graduation.
Of course, IBM is interested in hiring entry-
level students as a powerful way of scaling Wat-
son. Therefore, it has resolved to start an ed-
ucational program focused on these topics. Ini-
tially, tutorials were given at scientific conferences
(NAACL, ISWC and WWW, among others), uni-
versities and summer schools. The great number
of attendees (usually in the range of 50 to 150)
and strongly positive feedback received from the
students was a motivation to transform the didac-
tic material collected so far into a full graduate-
level course, which has been offered for the first
time at Columbia University. The course (which
is described in the rest of this paper) received very
positive evaluations from the students and will be
used as a template to be replicated by other part-
ner universities in the following year. Our ultimate
goal is to develop high quality didactic material
for an educational curriculum that can be used by
interested universities and professors all over the
world.
2 Syllabus and Didactic Material
The syllabus1 is divided equally between classes
specifically on the Watson system, its architec-
ture and technologies used within it, and classes
on more general topics that are relevant to these
technologies. In particular, background classes on
Natural Language Processing; Distributional Se-
mantics; the Semantic Web; Domain Adaptation
and the UIMA framework are essential for under-
standing the Watson system and producing suc-
cessful projects.
The course at Columbia included four lectures
by distinguished guest speakers from IBM, which
were advertised to the general Columbia commu-
nity as open talks. Instead of exams, the course
included two workshop-style presentation days:
one at the mid term and another at the end of the
1The syllabus is accessible on line http://www.
columbia.edu/?ag3366
86
course. During these workshops, all student teams
gave presentations on their various projects. At the
mid-term workshop, teams presented their project
idea and timeline, as well as related work and the
state-of-the-art of the field. At the final workshop,
they presented their completed projects, final re-
sults and demos. This workshop was also made
open to the Columbia community and in particu-
lar to faculty and affiliates interested in start-ups.
The workshops will be discussed in further detail
in the following sections. The syllabus is briefly
detailed here.
? Introduction: The Jeopardy! Challenge
The motivation behind Watson, the task and
its challenges (Prager et al, 2012; Tesauro et
al., 2012; Lewis, 2012).
? The DeepQA Architecture Chu-Carroll et
al. (2012b), Ferrucci (2012), Chu-Carroll et
al. (2012a), Lally et al (2012).
? Natural Language Processing Background
Pre-processing, tokenization, POS tagging,
named entity recognition, syntactic parsing,
semantic role labeling, word sense disam-
biguation, evaluation best practices and met-
rics.
? Natural Language Processing in Watson
Murdock et al (2012a), McCord et al (2012).
? Structured Knowledge in Watson Murdock
et al (2012b), Kalyanpur et al (2012), Fan et
al. (2012).
? Semantic Web OWL, RDF, Semantic Web
resources.
? Domain Adaptation Ferrucci et al (2012).
? UIMA The UIMA framework, Annotators,
Types, Descriptors, tools. Hands-on exercise
with the class project architecture (Epstein et
al., 2012).
? Midterm Workshop Presentations of each
team?s project idea and their research into re-
lated work and the state of the art.
? Distributional Semantics Miller et al
(2012), Gliozzo and Isabella (2005).
? Machine Learning and Strategy in Watson
? What Watson Tells Us About Cognitive
Computing
? Final Workshop Presentations of each
team?s final project implementation, evalua-
tion, demo and future plans.
3 Watson-like Architecture for Projects
The goal of the class projects was for the stu-
dents to learn to design and develop language tech-
nology components in an environment very sim-
ilar to IBM?s Watson architecture. We provided
the students with a plug-in framework for seman-
tic search, into which they could integrate their
project code. Student projects will be described
in the following section. This section details the
framework that was made available to the students
in order to develop their projects.
Like the Watson system, the project framework
for this class was built on top of Apache UIMA
(Ferrucci and Lally, 2004)2 ? an open-source
software architecture for building applications that
handle unstructured information.
The Watson system makes extensive use of
UIMA to enable interoperability and scale-out of a
large question answering system. The architecture
(viz., DeepQA) of Watson (Ferrucci, 2012) defines
several high-level ?stages? of analysis in the pro-
cessing pipeline, such as Question and Topic Anal-
ysis, Primary Search, Candidate Answer Genera-
tion, etc. Segmentation of the system into high-
level stages enabled a group of 25 researchers at
IBM to independently work on different aspects
of the system with little overhead for interoper-
ability and system integration. Each stage of the
pipeline clearly defined the inputs and outputs ex-
pected of components developed for that particu-
lar stage. The researchers needed only to adhere
to these input/output requirements for their indi-
vidual components to easily integrate them into
the system. Furthermore, the high-level stages in
Watson, enabled massive scale-out of the system
through the use of the asynchronous scaleout ca-
pability of UIMA-AS.
Using the Watson architecture for inspitration,
we developed a semantic search framework for the
class projects. As shown in Figure 2, the frame-
work consists of a UIMA pipeline that has several
high-level stages (similar to those of the Watson
system):
2http://uima.apache.org
87
Figure 2: Overview of the class project framework
1. Query Analysis
2. Primary Document Search
3. Structured Data Search
4. Query Expansion
5. Expanded Query Analysis
6. Secondary Document Search
The input to this system is provided by a Query
Collection Reader, which reads a list of search
queries from a text file. The Query Collec-
tion Reader is a UIMA ?collection reader? that
reads the text queries into memory data struc-
tures (UIMA CAS structures) ? one for each
text query. These UIMA CASes flow through the
pipeline and are processed by the various process-
ing stages. The processing stages are set up so
that new components designed to perform the task
of each processing stage can easily be added to the
pipeline (or existing components easily modified).
The expected inputs and outputs of components in
each processing stage are clearly defined, which
makes the task of the team building the component
simpler: they no longer have to deal with man-
aging data structures and are spared the overhead
of converting from and into formats of data ex-
changed between various components. All of the
overhead is handled by UIMA. Furthermore, some
of the processing stages generate new CAS struc-
tures and the flow of all the UIMA CAS structures
through this pipeline is controlled by a ?Flow Con-
troller? designed by us for this framework.
The framework was made available to each of
the student teams, and their task was to build
their project by extending this framework. Even
though we built the framework to perform seman-
tic search over a text corpus, many of the teams
in this course had projects that went far beyond
just semantic search. Our hope was that each team
would be able to able independently develop inter-
esting new components for the processing stages
of the pipeline, and at the end of the course we
would be able to merge the most interesting com-
ponents to create a single useful application. In the
following section, we describe the various projects
undertaken by the student teams in the class, while
Section 5 discusses the integration of components
from student projects and the demo application
that resulted from the integrated system.
4 Class Projects
Projects completed for this course fall into three
types: scientific projects, where the aim is to
produce a publishable paper; integrated projects,
where the aim is to create a component that will be
integrated into the class open-source project; and
independent demo projects, where the aim is to
produce an independent working demo/prototype.
The following section describes the integrated
projects briefly.
4.1 Selected Project Descriptions
As described in section 3, the integrated class
project is a system with an architecture which, al-
though greatly simplified, is reminiscent of Wat-
son?s. While originally intended to be simply a
semantic search tool, some of the student teams
created additional components which resulted in
a full question answering system. Those projects
88
as well as a few other related ones are described
below.
Question Categorization: Using the DBPedia
ontology (Bizer et al, 2009) as a semantic
type system, this project classifies questions
by their answer type. It can be seen as a sim-
plified version of the question categorization
system in Watson. The classification is based
on a simple bag-of-words approach with a
few additional features.
Answer Candidate Ranking: Given the answer
type as well as additional features derived by
the semantic search component, this project
uses regression to rank the candidate an-
swers which themselves come from semantic
search.
Twitter Semantic Search: Search in Twitter is
difficult due to the huge variations among
tweets in lexical terms, spelling and style, and
the limited length of the tweets. This project
employs LSA (Landauer and Dumais, 1997)
to cluster similar tweets and increase search
accuracy.
Fine-Grained NER in the Open Domain: This
project uses DBPedia?s ontology as a type
system for named entities of type Person.
Given results from a standard NER system,
it attempts to find the fine-grained classifica-
tion of each Person entity by finding the most
similar type. Similarity is computed using
traditional distributional methods, using the
context of the entity and the contexts of each
type, collected from Wikipedia.
News Frame Induction: Working with a large
corpus of news data collected by Columbia
Newsblaster, this team used the Machine
Linking API to tag entities with semantic
types. From there, they distributionally col-
lected ?frames? prevalent in the news do-
main such as ?[U.S President] meeting with
[British Prime Minister]?.
Other projects took on problems such as Sense
Induction, NER in the Biomedical domain, Se-
mantic Role Labeling, Semantic Video Search,
and a mobile app for Event Search.
5 System Integration and Demonstration
The UIMA-based architecture described in section
3 allows us to achieve a relatively easy integra-
tion of different class projects, independently de-
veloped by different teams, in a common archi-
tecture and expose their functionality with a com-
bined class project demo. The demo is a collab-
oratively developed semantic search engine which
is able to retrieve knowledge from structured data
and visualize it for the user in a very concise way.
The input is a query; it can be a natural language
question or simply a set of keywords. The output
is a set of entities and their relations, visualized
as an entity graph. Figure 3 shows the results of
the current status of our class project demo on the
following Jeopardy! question.
This nation of 200 million has fought
small independence movements like
those in Aceh and East Timor.
The output is a set of DBPedia entities related to
the question, grouped by Type (provided by the
DBPedia ontology). The correct answer, ?Indone-
sia?, is among the candidate entities of type Place.
Note that only answers of type Place and Agent
have been selected: this is due to the question cate-
gorization component, implemented by one of the
student teams, that allows us to restrict the gener-
ated answer set to those answers having the right
types.
The demo will be hosted for one year fol-
lowing the end of the course at http://
watsonclass.no-ip.biz. Our goal is to
incrementally improve this demo, leveraging any
new projects developed in future versions of the
course, and to build an open source software com-
munity involving students taking the course.
6 Evaluation
The course at Columbia drew a relatively large au-
dience. A typical size for a seminar course on a
special topic is estimated at 15-20 students, while
ours drew 35. The vast majority were Master?s stu-
dents; there were also three PhD students and five
undergraduates.
During the student workshops, students were
asked to provide grades for each team?s presen-
tation and project. After the instructor indepen-
dently gave his own grades, we looked at the cor-
relation between the average grades given by the
students and those give by the instructor. While
89
Figure 3: Screenshot of the project demo
90
Team 1 2 3 4 5 6 7 8 9 10 11
Instructor?s grade B+ B C+ A- B- A+ B B- B+ A B-
TA?s grade B+ B B A B- A B- B+ B+ A C+
Class? average grade B/B+ B+/A- B/B+ A- B/B+ A- B+ A-/A B+/A- A-/A B/B+
Table 1: Grades assigned to class projects
the students tended to be more ?generous? (their
average grade for each team was usually half a
grade above the instructor?s), the agreement was
quite high. Table 1 shows the grades given by the
instructor, the teaching assistant and the class av-
erage for the midterm workshop.
Feedback about the course from the students
was very good. Columbia provides electonic
course evaluations to the students which are com-
pletely optional. Participation in the evaluation for
this course was just under 50% in the midterm
evaluation and just over 50% in the final eval-
uation. The scores (all in the 0-5 range) given
by the students in relevant categories were quite
high: ?Overall Quality? got an average score of
4.23, ?Amount Learned? got 4, ?Appropriateness
of Workload? 4.33 and ?Fairness of Grading Pro-
cess? got 4.42.
The course resulted in multiple papers that are
or will soon be under submission, as well as a few
projects that may be developed into start-ups. Al-
most all student teams agreed to share their code
in an open source project that is currently being
set up, and which will include the current question
answering and semantic search system as well as
additional side projects.
7 Conclusion
We described a course on the topic of Semantic
Technologies and the IBM Watson system, which
features a diverse curriculum tied together by its
relevance to an exciting, demonstrably successful
real-world system. Through a combined architec-
ture inspired by Watson itself, the students get the
experience of developing an NLP-heavy compo-
nent with specifications mandated by the larger
architecture, which requires a combination of re-
search and software engineering skills that is com-
mon in the industry.
An exciting result of this course is that the
class project architecture and many of the student
projects are to be maintained as an open source
project which the students can, if they choose,
continue to be involved with. The repository and
community of this project can be expected to grow
each time the class is offered. Even after one class,
it already contains an impressive semantic search
system.
Feedback for this course from the students
was excellent, and many teams have achieved
their personal goals as stated at the beginning of
the semester, including paper submissions, opera-
tional web demos and mobile apps.
Our long term goal is to replicate this course in
multiple top universities around the world. While
IBM does not have enough resources to always
do this with its own researchers, it is instead go-
ing to provide the content material and the open
source code generated so far to other universities,
encouraging professors to teach the course them-
selves. Initially we will work on a pilot phase
involving only a restricted number of professors
and researchers that are already in collaboration
with IBM Research, and eventually (if the posi-
tive feedback we have seen so far is repeated in
the pilot phase) give access to the same content to
a larger group.
References
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,
R. Cyganiak, and S. Hellmann. 2009. DBpedia?
Crystallization Point for the Web of Data. Journal
of Web Semantics: Science, Services and Agents on
the World Wide Web, 7(3):154?165, September.
J. Chu-Carroll, J. Fan, B. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012a. Finding Nee-
dles in the Haystack: Search and Candidate Gener-
ation. IBM Journal of Research and Development,
56(3.4):6:1?6:12.
J. Chu-Carroll, J. Fan, N. Schlaefer, and W. Zadrozny.
2012b. Textual Resource Acquisition and Engineer-
ing. IBM Journal of Research and Development,
56(3.4):4:1?4:11.
E. Epstein, M. Schor, B. Iyer, A. Lally, E. Brown, and
J. Cwiklik. 2012. Making Watson Fast. IBM Jour-
nal of Research and Development, 56(3.4):15:1?
15:12.
J. Fan, A. Kalyanpur, D. Gondek, and D. Ferrucci.
2012. Automatic Knowledge Extraction from Doc-
uments. IBM Journal of Research and Development,
56(3.4):5:1?5:10.
91
D. Ferrucci and A. Lally. 2004. UIMA: an Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan,
D. Gondek, A. Kalyanpur, A. Lally, J. W. Murdock,
E. Nyberg, J. Prager, N. Schlaefer, and C. Welty.
2010. Building Watson: An Overview of the
DeepQA project. AI magazine, 31(3):59?79.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and
E. Mueller. 2012. Watson: Beyond Jeopardy. Arti-
ficial Intelligence (in press).
D. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3.4):1:1?1:15.
A. Gliozzo and T. Isabella. 2005. Semantic Domains
in Computational Linguistics. Technical report.
A. Kalyanpur, B. Boguraev, S. Patwardhan, J. W.
Murdock, A. Lally, C. Welty, J. Prager, B. Cop-
pola, A. Fokoue-Nkoutche, L. Zhang, Y. Pan, and
Z. Qiu. 2012. Structured Data and Inference in
DeepQA. IBM Journal of Research and Develop-
ment, 56(3.4):10:1?10:14.
A. Lally, J. Prager, M. McCord, B. Boguraev, S. Pat-
wardhan, J. Fan, P. Fodor, and J. Chu-Carroll. 2012.
Question Analysis: How Watson Reads a Clue. IBM
Journal of Research and Development, 56(3.4):2:1?
2:14.
T. Landauer and S. Dumais. 1997. A Solution to
Plato?s Problem: the Latent Semantic Analysis The-
ory of Acquisition, Induction and Representation
of Knowledge. Psychological Review, 104(2):211?
240.
B. Lewis. 2012. In the Game: The Interface between
Watson and Jeopardy! IBM Journal of Research and
Development, 56(3.4):17:1?17:6.
M. McCord, J. W. Murdock, and B. Boguraev. 2012.
Deep Parsing in Watson. IBM Journal of Research
and Development, 56(3.4):3:1?3:15.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych.
2012. Using Distributional Similarity for Lexical
Expansion in Knowledge-based Word Sense Disam-
biguation. In Proceedings of the International Con-
ference on Computational Linguistics, pages 1781?
1796, Mumbai, India, December.
J. W. Murdock, J. Fan, A. Lally, H. Shima, and
B. Boguraev. 2012a. Textual Evidence Gathering
and Analysis. IBM Journal of Research and Devel-
opment, 56(3.4):8:1?8:14.
J. W. Murdock, A. Kalyanpur, C. Welty, J. Fan, D. Fer-
rucci, D. Gondek, L. Zhang, and H. Kanayama.
2012b. Typing Candidate Answers Using Type Co-
ercion. IBM Journal of Research and Development,
56(3.4):7:1?7:13.
J. Prager, E. Brown, and J. Chu-Carroll. 2012. Spe-
cial Questions and Techniques. IBM Journal of Re-
search and Development, 56(3.4):11:1?11:13.
G. Tesauro, D. Gondek, J. Lenchner, J. Fan, and
J. Prager. 2012. Simulation, Learning, and Op-
timization Techniques in Watson?s Game Strate-
gies. IBM Journal of Research and Development,
56(3.4):16:1?16:11.
92
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Sentence Compression with Joint Structural Inference
Kapil Thadani and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10025, USA
{kapil,kathy}@cs.columbia.edu
Abstract
Sentence compression techniques often
assemble output sentences using frag-
ments of lexical sequences such as n-
grams or units of syntactic structure such
as edges from a dependency tree repre-
sentation. We present a novel approach
for discriminative sentence compression
that unifies these notions and jointly pro-
duces sequential and syntactic represen-
tations for output text, leveraging a com-
pact integer linear programming formula-
tion to maintain structural integrity. Our
supervised models permit rich features
over heterogeneous linguistic structures
and generalize over previous state-of-the-
art approaches. Experiments on corpora
featuring human-generated compressions
demonstrate a 13-15% relative gain in 4-
gram accuracy over a well-studied lan-
guage model-based compression system.
1 Introduction
Recent years have seen increasing interest in text-
to-text generation tasks such as paraphrasing and
text simplification, due in large part to their direct
utility in high-level natural language tasks such as
abstractive summarization. The task of sentence
compression in particular has benefited from the
availability of a number of useful resources such
as the the Ziff-Davis compression corpus (Knight
and Marcu, 2000) and the Edinburgh compression
corpus (Clarke and Lapata, 2006b) which make
compression problems highly relevant for data-
driven approaches involving language generation.
The sentence compression task addresses the
problem of minimizing the lexical footprint of a
sentence, i.e., the number of words or characters
in it, while preserving its most salient informa-
tion. This is illustrated in the following example
from the compression corpus of Clarke and Lap-
ata (2006b):
Original: In 1967 Chapman, who had cultivated a
conventional image with his ubiquitous tweed jacket
and pipe, by his own later admission stunned a party
attended by his friends and future Python colleagues
by coming out as a homosexual.
Compressed: In 1967 Chapman, who had cultivated
a conventional image, stunned a party by coming out
as a homosexual.
Compression can therefore be viewed as analo-
gous to text summarization1 defined at the sen-
tence level. Unsurprisingly, independent selec-
tion of tokens for an output sentence does not
lead to fluent or meaningful compressions; thus,
compression systems often assemble output text
from units that are larger than single tokens such
as n-grams (McDonald, 2006; Clarke and Lap-
ata, 2008) or edges in a dependency structure (Fil-
ippova and Strube, 2008; Galanis and Androut-
sopoulos, 2010). These systems implicitly rely on
a structural representation of text?as a sequence
of tokens or as a dependency tree respectively?to
to underpin the generation of an output sentence.
In this work, we present structured transduc-
tion: a novel supervised framework for sen-
tence compression which employs a joint infer-
ence strategy to simultaneously recover sentence
compressions under both these structural repre-
sentations of text?a token sequence as well as a
tree of syntactic dependencies. Sentence genera-
tion is treated as a discriminative structured pre-
diction task in which rich linguistically-motivated
1To further the analogy, compression is most often formu-
lated as a word deletion task which parallels the popular view
of summarization as a sentence extraction task.
65
features can be used to predict the informative-
ness of specific tokens within the input text as well
as the fluency of n-grams and dependency rela-
tionships in the output text. We present a novel
constrained integer linear program that optimally
solves the joint inference problem, using the no-
tion of commodity flow (Magnanti and Wolsey,
1994) to ensure the production of valid acyclic se-
quences and trees for an output sentence.
The primary contributions of this work are:
? A supervised sequence-based compression
model which outperforms Clarke & Lapata?s
(2008) state-of-the-art sequence-based com-
pression system without relying on any hard
syntactic constraints.
? A formulation to jointly infer tree structures
alongside sequentially-ordered n-grams,
thereby permitting features that factor over
both phrases and dependency relations.
The structured transduction models offer addi-
tional flexibility when compared to existing mod-
els that compress via n-gram or dependency fac-
torizations. For instance, the use of commodity
flow constraints to ensure well-formed structure
permits arbitrary reorderings of words in the input
and is not restricted to producing text in the same
order as the input like much previous work (Mc-
Donald, 2006; Clarke and Lapata, 2008; Filippova
and Strube, 2008) inter alia.2
We ran compression experiments with the pro-
posed approaches on well-studied corpora from
the domains of written news (Clarke and Lapata,
2006b) and broadcast news (Clarke and Lapata,
2008). Our supervised approaches show signif-
icant gains over the language model-based com-
pression system of Clarke and Lapata (2008) un-
der a variety of performance measures, yielding
13-15% relative F1 improvements for 4-gram re-
trieval over Clarke and Lapata (2008) under iden-
tical compression rate conditions.
2 Joint Structure Transduction
The structured transduction framework is driven
by the fundamental assumption that generating
fluent text involves considerations of diverse struc-
tural relationships between tokens in both input
and output sentences. Models for sentence com-
pression often compose text from units that are
2We do not evaluate token reordering in the current work
as the corpus used for experiments in ?3 features human-
generated compressions that preserve token ordering.
larger than individual tokens, such as n-grams
which describe a token sequence or syntactic re-
lations which comprise a dependency tree. How-
ever, our approach is specifically motivated by the
perspective that both these representations of a
sentence?a sequence of tokens and a tree of de-
pendency relations?are equally meaningful when
considering its underlying fluency and integrity. In
other words, models for compressing a token se-
quence must also account for the compression of
its dependency representation and vice versa.
In this section, we discuss the problem of re-
covering an optimal compression from a sen-
tence as a linear optimization problem over het-
erogenous substructures (cf. ?2.1) that can be
assembled into valid and consistent representa-
tions of a sentence (cf. ?2.2). We then consider
rich linguistically-motivated features over these
substructures (cf. ?2.3) for which corresponding
weights can be learned via supervised structured
prediction (cf. ?2.4).
2.1 Linear Objective
Consider a single compression instance involving
a source sentence S containing m tokens. The no-
tation S? is used to denote a well-formed compres-
sion of S. In this paper, we follow the standard
assumption from compression research in assum-
ing that candidate compressions S? are assembled
from the tokens in S, thereby treating compression
as a word-deletion task. The inference step aims
to retrieve the output sentence S?? that is the most
likely compression of the given input S, i.e., the S?
that maximizes p(S?|S) ? p(S?, S) or, in an equiv-
alent discriminative setting, the S? that maximizes
a feature-based score for compression
S?? , argmax
S?
w>?(S, S?) (1)
where ?(S, S?) denotes some feature map param-
eterized by a weight vector w.
Let T , {ti : 1 ? i ? m} represent the set
of tokens in S and let xi ? {0, 1} represent a
token indicator variable whose value corresponds
to whether token ti is present in the output sen-
tence S?. The incidence vector x , ?x1, . . . , xm?>
therefore represents an entire token configuration
that is equivalent to some subset of T .
If we were to consider a simplistic bag-of-
tokens scenario in which the features factor en-
tirely over the tokens from T , the highest-scoring
66
compression under (1) would simply be the to-
ken configuration that maximizes a linear combi-
nation of per-token scores, i.e., ?ti?T xi ? ?tok(i)where ?tok : N? R denotes a linear scoring func-
tion which measures the relative value of retain-
ing ti in a compression of S based on its features,
i.e., ?tok(i) , w>tok?tok(ti). Although this can
be solved efficiently under compression-rate con-
straints, the strong independence assumption used
is clearly unrealistic: a model that cannot consider
any relationship between tokens in the output does
not provide a token ordering or ensure that the re-
sulting sentence is grammatical.
The natural solution is to include higher-order
factorizations of linguistic structures such as n-
grams in the objective function. For clarity of ex-
position, we assume the use of trigrams without
loss of generality. Let U represent the set of all
possible trigrams that can be constructed from the
tokens of S; in other words U , {?ti, tj , tk? : ti ?
T ? {START}, tj ? T, tk ? T ? {END}, i 6= j 6=
k}. Following the notation for token indicators, let
yijk ? {0, 1} represent a trigram indicator variable
for whether the contiguous sequence of tokens
?ti, tj , tk? is in the output sentence. The incidence
vector y , ?yijk??ti,tj ,tk??U hence representssome subset of the trigrams in U . Similarly, let V
represent the set of all possible dependency edges
that can be established among the tokens of S and
the pseudo-token ROOT, i.e., V , {?i, j? : i ?
T ? {ROOT}, j ? T, tj is a dependent of ti in S}.
As before, zij ? {0, 1} represents a dependency
arc indicator variable indicating whether tj is a di-
rect dependent of ti in the dependency structure of
the output sentence, and z , ?zij??ti,tj??V repre-
sents a subset of the arcs from V .
Using this notation, any output sentence S? can
now be expressed as a combination of some to-
ken, trigram and dependency arc configurations
?x,y, z?. Defining ?ngr and ?dep analogously to
?tok for trigrams and dependency arcs respectively,
we rewrite (1) as
S?? = argmax
x,y,z
?
ti?T
xi ? ?tok(i)
+
?
?ti,tj ,tk??U
yijk ? ?ngr(i, j, k)
+
?
?ti,tj??V
zij ? ?dep(i, j)
= argmax
x,y,z
x>?tok + y>?ngr + z>?dep (2)
where ?tok , ??tok(i)?ti?T denotes the vector of
token scores for all tokens ti ? T and ?ngr and
?dep represent vectors of scores for all trigrams
and dependency arcs in U and V respectively. The
joint objective in (2) is an appealingly straightfor-
ward and yet general formulation for the compres-
sion task. For instance, the use of standard sub-
structures like n-grams permits scoring of the out-
put sequence configuration y under probabilistic
n-gram language models as in Clarke and Lapata
(2008). Similarly, consideration of dependency
arcs allows the compressed dependency tree z to
be scored using a rich set of indicator features over
dependency labels, part-of-speech tags and even
lexical features as in Filippova and Strube (2008).
However, unlike the bag-of-tokens scenario,
these output structures cannot be constructed effi-
ciently due to their interdependence. Specifically,
we need to maintain the following conditions in
order to obtain an interpretable token sequence y:
? Trigram variables yijk must be non-zero if
and only if their corresponding word vari-
ables xi, xj and xk are non-zero.
? The non-zero yijk must form a sentence-like
linear ordering, avoiding disjoint structures,
cycles and branching.
Similarly, a well-formed dependency tree z will
need to satisfy the following conditions:
? Dependency variables zij must be non-zero if
and only if the corresponding word variables
xi and xj are.
? The non-zero zij must form a directed tree
with one parent per node, a single root node
and no cycles.
2.2 Constrained ILP Formulation
We now discuss an approach to recover exact so-
lutions to (2) under the appropriate structural con-
straints, thereby yielding globally optimal com-
pressions S? ? ?x,y, z? given some input sentence
S and model parameters for the scoring functions.
For this purpose, we formulate the inference task
for joint structural transduction as an integer linear
program (ILP)?a type of linear program (LP) in
which some or all of the decision variables are re-
stricted to integer values. A number of highly op-
timized general-purpose solvers exist for solving
ILPs thereby making them tractable for sentence-
level natural language problems in which the num-
ber of variables and constraints is described by a
low-order polynomial over the size of the input.
67
Recent years have seen ILP applied to many
structured NLP applications including depen-
dency parsing (Riedel and Clarke, 2006; Martins
et al, 2009), text alignment (DeNero and Klein,
2008; Chang et al, 2010; Thadani et al, 2012)
and many previous approaches to sentence and
document compression (Clarke and Lapata, 2008;
Filippova and Strube, 2008; Martins and Smith,
2009; Clarke and Lapata, 2010; Berg-Kirkpatrick
et al, 2011; Woodsend and Lapata, 2012).
2.2.1 Basic structural constraints
We start with constraints that define the behavior
of terminal tokens. Let y?jk, yij? and z?j denote
indicator variables for the sentence-starting tri-
gram ?START, tj , tk?, the sentence-ending trigram
?ti, tj , END? and the root dependency ?ROOT, tj?
respectively. A valid output sentence will started
and terminate with exactly one trigram (perhaps
the same); similarly, exactly one word should act
as the root of the output dependency tree.
?
j,k
y?jk = 1 (3)
?
i,j
yij? = 1 (4)
?
j
z?j = 1 (5)
Indicator variables for any substructure, i.e., n-
gram or dependency arc, must be kept consistent
with the token variables that the substructure is de-
fined over. For instance, we require constraints
which specify that tokens can only be active (non-
zero) in the solution when, for 1 ? p ? n, there
is exactly one active n-gram in the solution which
contains this word in position p.3 Tokens and de-
pendency arcs can similarly be kept consistent by
ensuring that a word can only be active when one
incoming arc is active.
xl ?
?
i,j,k:
l?{i,j,k}
yijk = 0, ?tl ? T (6)
xj ?
?
i
zij = 0, ?tj ? T (7)
3Note that this does not always hold for n-grams of or-
der n > 2 due to the way terminal n-grams featuring START
and END are defined. Specifically, in a valid linear ordering
of tokens and ?r ? 1 . . . n? 2, there can be no n-grams that
feature the last n?r?1 tokens in the r?th position or the first
n?r?1 tokens in the (n?r+1)?th position. However, this
is easily tackled computationally by assuming that the termi-
nal n-gram replaces these missing n-grams for near-terminal
tokens in constraint (6).
2.2.2 Flow-based structural constraints
A key challenge for structured transduction mod-
els lies in ensuring that output token sequences and
dependency trees are well formed. This requires
that output structures are fully connected and that
cycles are avoided. In order to accomplish this, we
introduce additional variables to establish single-
commodity flow (Magnanti and Wolsey, 1994) be-
tween all pairs of tokens, inspired by recent work
in dependency parsing (Martins et al, 2009). Lin-
ear token ordering is maintained by defining real-
valued adjacency commodity flow variables ?adjij
which must be non-zero whenever tj directly fol-
lows ti in an output sentence. Similarly, tree-
structured dependencies are enforced using addi-
tional dependency commodity flow variables ?depij
which must be non-zero whenever tj is the de-
pendent of ti in the output sentence. As with the
structural indicators, flow variables ?adj?j , ?adji? , ?dep?j
are also defined for the terminal pseudo-tokens
START, END and ROOT respectively.
Each active token in the solution consumes one
unit of each commodity from the flow variables
connected to it. In conjunction with the consis-
tency constraints from equations (6) and (7), this
ensures that cycles cannot be present in the flow
structure for either commodity.
?
i
?cij ?
?
k
?cjk = xj , ?tj ? T, (8)
?c ? {adj, dep}
By itself, (8) would simply set al token indica-
tors xi simultaneously to 0. However, since START
and ROOT have no incoming flow variables, the
amount of commodity in the respective outgo-
ing flow variables ?adj?j and ?dep?j remains uncon-
strained. These flow variables therefore provide
a point of origin for their respective commodities.
In order for commodity flow to be meaningful,
it should be confined to mirroring active structural
indicators; for this, we first restrict the amount of
commodity in any ?cij to be non-negative.
?cij ? 0, ?ti, tj ? T (9)
?c ? {adj, dep}
The adjacency commodity is then linked to the n-
grams that would actually establish an adjacency
relationship between two tokens, while the depen-
dency commodity is linked to its corresponding
dependency arcs. In conjunction with (8?9), these
68
START END
ROOT
Production was closed down at Ford last night for the Christmas period .
8 ?adj1,3 = 7 6 5 4 3 2 1
7
?dep3,1 = 1 2 1
2
1
1
Figure 1: An illustration of commodity values for a valid solution of the program. The adjacency com-
modity ?adj and dependency commodity ?dep are denoted by solid and dashed lines respectively.
constraints also serve to establish connectivity for
their respective structures.
?adjij ? Cmax
?
k
yijk ? 0, ?ti, tj ? T (10)
?adjjk ? Cmax
?
i
yijk ? 0, ?tj , tk ? T (11)
?depij ? Cmaxzij ? 0, ?ti, tj ? T (12)
where Cmax is the maximum amount of commod-
ity that the ?ij variables may carry and serves as an
upper bound on the number of tokens in the output
sentence. Since we use commodity flow to avoid
cyclical structure and not to specify spanning ar-
borescences (Martins et al, 2009), Cmax can sim-
ply be set to an arbitrary large value.
2.2.3 Compression rate constraints
The constraints specified above are adequate to en-
force structural soundness in an output compres-
sion. In addition, compression tasks often involve
a restriction on the size of the output sentence.
When measured in tokens, this can simply be ex-
pressed via constraints over token indicators.
?
i
xi ? Rmin (13)
?
i
xi ? Rmax (14)
where the compression rate is enforced by restrict-
ing the number of output tokens to [Rmin, Rmax].
2.3 Features
The scoring functions ? that guide inference for a
particular compression instance are defined above
as linear functions over structure-specific features.
We employ the following general classes of fea-
tures for tokens, trigrams and dependency arcs.
1. Informativeness: Good compressions might
require specific words or relationships be-
tween words to be preserved, highlighted, or
perhaps explicitly rejected. This can be ex-
pressed through features on token variables
that indicate a priori salience.4 For this pur-
pose, we rely on indicator features for part-
of-speech (POS) sequences of length up to 3
that surround the token and the POS tag of the
token?s syntactic governor conjoined with the
label. Inspired by McDonald (2006), we also
maintain indicator features for stems of verbs
(at or adjacent to the token) as these can be
useful indications of salience in compression.
Finally, we maintain features for whether to-
kens are negation words, whether they appear
within parentheses and if they are part of a
capitalized sequence of tokens (an approxi-
mation of named entities).
2. Fluency: These features are intended to cap-
ture how the presence of a given substructure
contributes to the overall fluency of a sen-
tence. The n-gram variables are scored with a
feature expressing their log-likelihood under
an LM. For n-gram variables, we include fea-
tures that indicate the POS tags and depen-
dency labels corresponding to the tokens it
covers. Dependency variable features involve
indicators for the governor POS tag con-
joined with the dependency direction. In ad-
dition, we also use lexical features for prepo-
sitions in the governor position of depen-
dency variables in order to indicate whether
certain prepositional phrases are likely to be
preserved in compressions.
3. Fidelity: One might reasonably expect that
many substructures in the input sentence will
appear unchanged in the output sentence.
Therefore, we propose boolean features that
indicate that a substructure was seen in the
input. Fidelity scores are included for all
n-gram variables alongside label-specific fi-
4Many compression systems (Clarke and Lapata, 2008;
Filippova and Strube, 2008) use a measure based on tf*idf
which derives from informativeness score of Hori and Furui
(2004), but we found this to be less relevant here.
69
delity scores for dependency arc variables,
which can indicate whether particular labels
are more or less likely to be dropped.
4. Pseudo-normalization: A drawback of us-
ing linear models for generation problems
is an inability to employ output sentence
length normalization in structure scoring. For
this purpose, we use the common machine
translation (MT) strategy of employing word
penalty features. These are essentially word
counts whose parameters are intended to bal-
ance out the biases in output length which are
induced by other features.
Each scale-dependent feature is recorded both ab-
solutely as well as normalized by the length of the
input sentence. This is done in order to permit the
model to acquire some robustness to variation in
input sentence length when learning parameters.
2.4 Learning
In order to leverage a training corpus to recover
weight parameters w? for the above features that
encourage good compressions for unseen data,
we rely on the structured perceptron of Collins
(2002). A fixed learning rate is used and param-
eters are averaged to limit overfitting.5 In our ex-
periments, we observed fairly stable convergence
for compression quality over held-out develop-
ment corpora, with peak performance usually en-
countered by 10 training epochs.
3 Experiments
In order to evaluate the performance of the
structured transduction framework, we ran com-
pression experiments over the newswire (NW)
and broadcast news transcription (BN) corpora
collected by Clarke and Lapata (2008). Sen-
tences in these datasets are accompanied by gold
compressions?one per sentence for NW and
three for BN?produced by trained human anno-
tators who were restricted to using word deletion,
so paraphrasing and word reordering do not play
a role. For this reason, we chose to evaluate the
systems using n-gram precision and recall (among
other metrics), following Unno et al (2006) and
standard MT evaluations.
We filtered the corpora to eliminate instances
with less than 2 and more than 110 tokens and used
5Given an appropriate loss function, large-margin struc-
tured learners such as k-best MIRA (McDonald et al, 2005)
can also be used as shown in Clarke and Lapata (2008).
the same training/development/test splits from
Clarke and Lapata (2008), yielding 953/63/603
sentences respectively for the NW corpus and
880/78/404 for the BN corpus. Dependency parses
were retrieved using the Stanford parser6 and ILPs
were solved using Gurobi.7 As a state-of-the-art
baseline for these experiments, we used a reim-
plementation of the LM-based system of Clarke
and Lapata (2008), which we henceforth refer to
as CL08. This is equivalent to a variant of our pro-
posed model that excludes variables for syntactic
structure, uses LM log-likelihood as a feature for
trigram variables and a tf*idf -based significance
score for token variables, and incorporates several
targeted syntactic constraints based on grammat-
ical relations derived from RASP (Briscoe et al,
2006) designed to encourage fluent output.
Due to the absence of word reordering in the
gold compressions, trigram variables y that were
considered in the structured transduction approach
were restricted to only those for which tokens
appear in the same order as the input as is the
case with CL08. Furthermore, in order to reduce
computational overhead for potentially-expensive
ILPs, we also excluded dependency arc variables
which inverted an existing governor-dependent re-
lationship from the input sentence parse.
A recent analysis of approaches to evaluating
compression (Napoles et al, 2011b) has shown a
strong correlation between the compression rate
and human judgments of compression quality,
thereby concluding that comparisons of systems
which compress at different rates are unreliable.
Consequently, all comparisons that we carry out
here involve a restriction to a particular compres-
sion rate to ensure that observed differences can
be interpreted meaningfully.
3.1 Results
Table 1 summarizes the results from compression
experiments in which the target compression rate
is set to the average gold compression rate for
each instance. We observe a significant gain for
the joint structured transduction system over the
Clarke and Lapata (2008) approach for n-gram F1.
Since n-gram metrics do not distinguish between
content words and function words, we also in-
clude an evaluation metric that observes the pre-
cision, recall and F-measure of nouns and verbs
6http://nlp.stanford.edu/software/
7http://www.gurobi.com
70
Corpus System n-grams F1% Content words Syntactic relations F1%n = 1 2 3 4 P% R% F1% Stanford RASP
NW CL08 66.65 53.08 40.35 31.02 73.84 66.41 69.38 51.51 50.21Joint ST 71.91 58.67 45.84 35.62 76.82 76.74 76.33 55.02 50.81
BN CL08 75.08 61.31 46.76 37.58 80.21 75.32 76.91 60.70 57.27Joint ST 77.82 66.39 52.81 42.52 80.77 81.93 80.75 61.38 56.47
Table 1: Experimental results under various quality metrics (see text for descriptions). Systems were
restricted to produce compressions that matched their average gold compression rate. Boldfaced entries
indicate significant differences (p < 0.0005) under the paired t-test and Wilcoxon?s signed rank test.
as a proxy for the content in compressed output.
From these, we see that the primary contribution
of the supervised joint approach is in enhancing
the recall of meaning-bearing words.
In addition to the direct measures discussed
above, Napoles et al (2011b) indicate that various
other metrics over syntactic relations such as those
produced by RASP also correlate significantly
with human judgments of compression quality.
Compressed sentences were therefore parsed with
RASP as well as the Stanford dependency parser
and their resulting dependency graphs were com-
pared to those of the gold compressions. These
metrics show statistically insignificant differences
except in the case of F1 over Stanford dependen-
cies for the NW corpus.8
Comparisons with CL08 do not adequately ad-
dress the question of whether the performance
gain observed is driven by the novel joint infer-
ence approach or the general power of discrimina-
tive learning. To investigate this, we also studied
a variant of the proposed model which eliminates
the dependency variables z and associated com-
modity flow machinery, thereby bridging the gap
between the two systems discussed above. This
system, which we refer to as Seq ST, is other-
wise trained under similar conditions as Joint ST.
Table 2 contains an example of incorrect system
output for the three systems under study and il-
lustrates some specific quirks of each, such as the
tendency of CL08 to preserve deeply nested noun
phrases, the limited ability of Seq ST to identify
heads of constituents and the potential for plausi-
ble but unusual output parses from Joint ST.
Figure 2 examines the variation of content word
F1% when the target compression rate is varied
for the BN corpus, which contains three refer-
8Our RASP F1 results for Clarke and Lapata (2008) in
Table 1 outperform their reported results by about 10% (ab-
solute) which may stem from our Gigaword-trained LM or
improvements in recent versions of RASP.
Input When Los Angeles hosted the Olympics in
1932 , Kurtz competed in high platform diving.
Gold When Los Angeles hosted the Olympics , Kurtz
competed in high diving .
CL08 When Los Angeles hosted Olympics in 1932 ,
in high platform diving .
Seq ST When Los Angeles hosted the Olympics , Kurtz
competed in high platform
Joint ST When Los Angeles hosted the Olympics in
1932 , Kurtz competed diving .
Table 2: Examples of erroneous system compres-
sions for a test instance from the NW corpus.
ence compressions per instance. Although the
gold compressions are often unreachable under
low rates, this provides a view into a model?s abil-
ity to select meaningful words under compression
constraints. We observe that the Joint ST model
consistently identifies content words more accu-
rately than the sequence-only models despite shar-
ing all token and trigram features with Seq ST.
Figure 3 studies the variation of RASP gram-
matical relation F1% with compression rate as an
approximate measure of grammatical robustness.
As all three systems track each other fairly closely,
the plot conveys the absolute difference of the ST
systems from the CL08 baseline, which reveals
that Joint ST largely outperforms Seq ST under
different compression conditions. We also note
that a high compression rate, i.e., minimal com-
pression, is generally favorable to CL08 under the
RASP F1 measure and conjecture that this may be
due to the hard syntactic constraints employed by
CL08, some of which are defined over RASP re-
lations. At higher compression rates, these con-
straints largely serve to prevent the loss of mean-
ingful syntactic relationships, e.g., that between a
preposition and its prepositional phrase; however,
a restrictive compression rate would likely result
in all such mutually-constrained components be-
ing dropped rather than simultaneously preserved.
71
Figure 2: Informativeness of compressions in the
BN test corpus indicated by noun and verb F1%
with respect to gold at different compression rates.
4 Related Work
An early notion of compression was proposed
by Dras (1997) as reluctant sentence paraphras-
ing under length constraints. Jing and McKe-
own (2000) analyzed human-generated summaries
and identified a heavy reliance on sentence re-
duction (Jing, 2000). The extraction by Knight
and Marcu (2000) of a dataset of natural com-
pression instances from the Ziff-Davis corpus
spurred interest in supervised approaches to the
task (Knight and Marcu, 2002; Riezler et al, 2003;
Turner and Charniak, 2005; McDonald, 2006;
Unno et al, 2006; Galley and McKeown, 2007;
Nomoto, 2007). In particular, McDonald (2006)
expanded on Knight & Marcu?s (2002) transition-
based model by using dynamic programming to
recover optimal transition sequences, and Clarke
and Lapata (2006a) used ILP to replace pairwise
transitions with trigrams. Other recent work (Fil-
ippova and Strube, 2008; Galanis and Androut-
sopoulos, 2010) has used dependency trees di-
rectly as sentence representations for compres-
sion. Another line of research has attempted to
broaden the notion of compression beyond mere
word deletion (Cohn and Lapata, 2009; Ganitke-
vitch et al, 2011; Napoles et al, 2011a). Finally,
progress on standalone compression tasks has also
enabled document summarization techniques that
jointly address sentence selection and compres-
sion (Daume? and Marcu, 2002; Clarke and Lapata,
2007; Martins and Smith, 2009; Berg-Kirkpatrick
et al, 2011; Woodsend and Lapata, 2012), a num-
ber of which also rely on ILP-based inference.
Monolingual text-to-text generation research
also faces many obstacles common to MT. Re-
Figure 3: Relative grammaticality of BN test cor-
pus compressions indicated by the absolute differ-
ence of RASP relation F1% from that of CL08.
cent work in MT decoding has proposed more ef-
ficient approaches than ILP to produced text op-
timally under syntactic and sequential models of
language (Rush and Collins, 2011). We are cur-
rently exploring similar ideas for compression and
other text-to-text generation problems.
5 Conclusion
We have presented a supervised discriminative
approach to sentence compression that elegantly
accounts for two complementary aspects of sen-
tence structure?token ordering and dependency
syntax. Our inference formulation permits rich,
linguistically-motivated features that factor over
the tokens, n-grams and dependencies of the out-
put. Structural integrity is maintained by linear
constraints based on commodity flow, resulting in
a flexible integer linear program for the inference
task. We demonstrate that this approach leads to
significant performance gains over a state-of-the-
art baseline compression system without resorting
to hand-picked constraints on output content.
Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
72
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-COLING Interactive Presenta-
tion Sessions.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning over
constrained latent representations. In Proceedings
of HLT-NAACL, pages 429?437.
James Clarke and Mirella Lapata. 2006a. Constraint-
based sentence compression: an integer program-
ming approach. In Proceedings of ACL-COLING,
pages 144?151.
James Clarke and Mirella Lapata. 2006b. Models
for sentence compression: a comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of ACL-COLING, pages 377?
384.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of EMNLP-CoNLL, pages 1?11.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: an integer linear
programming approach. Journal for Artificial Intel-
ligence Research, 31:399?429, March.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34(1):637?674, April.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1?8.
Hal Daume?, III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of ACL, pages 449?456.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, pages 25?28.
Mark Dras. 1997. Reluctant paraphrase: Textual re-
structuring under an optimisation model. In Pro-
ceedings of PacLing, pages 98?104.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of INLG, pages 25?32.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of HLT-NAACL, pages
885?893.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL, pages 180?
187, April.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP, pages 1168?1179.
Chiori Hori and Sadaoki Furui. 2004. Speech summa-
rization: an approach through word extraction and a
method for evaluation. IEICE Transactions on In-
formation and Systems, E87-D(1):15?25.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of NAACL, pages 178?185.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
Conference on Applied Natural Language Process-
ing, pages 310?315.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI, pages 703?710.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107, July.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees. In Technical Report 290-94,
Massechusetts Institute of Technology, Operations
Research Center.
Andre? F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1?9.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL-IJCNLP, pages 342?350.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL, pages 297?304.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011a. Paraphras-
tic sentence compression with a character-based
metric: tightening without deletion. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 84?90.
73
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011b. Evaluating sentence com-
pression: pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Infor-
mation Processing and Management, 43(6):1571?
1587, November.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proceedings of EMNLP, pages
129?137.
Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In Proceedings of HLT-NAACL, pages 118?125.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72?82.
Kapil Thadani, Scott Martin, and Michael White.
2012. A joint phrasal and dependency model for
paraphrase alignment. In Proceedings of COLING,
pages 1229?1238.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL, pages 290?297.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning
approaches. In Proceedings of ACL-COLING, pages
850?857.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP, pages 233?
243.
74
