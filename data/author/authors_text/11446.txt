Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 77?80,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Syntactic and Lexical-Based Discourse Segmenter
Milan Tofiloski
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
mta45@sfu.ca
Julian Brooke
Department of Linguistics
Simon Fraser University
Burnaby, BC, Canada
jab18@sfu.ca
Maite Taboada
Department of Linguistics
Simon Fraser University
Burnaby, BC, Canada
mtaboada@sfu.ca
Abstract
We present a syntactic and lexically based
discourse segmenter (SLSeg) that is de-
signed to avoid the common problem of
over-segmenting text. Segmentation is the
first step in a discourse parser, a system
that constructs discourse trees from el-
ementary discourse units. We compare
SLSeg to a probabilistic segmenter, show-
ing that a conservative approach increases
precision at the expense of recall, while re-
taining a high F-score across both formal
and informal texts.
1 Introduction?
Discourse segmentation is the process of de-
composing discourse into elementary discourse
units (EDUs), which may be simple sentences or
clauses in a complex sentence, and from which
discourse trees are constructed. In this sense, we
are performing low-level discourse segmentation,
as opposed to segmenting text into chunks or top-
ics (e.g., Passonneau and Litman (1997)). Since
segmentation is the first stage of discourse parsing,
quality discourse segments are critical to build-
ing quality discourse representations (Soricut and
Marcu, 2003). Our objective is to construct a dis-
course segmenter that is robust in handling both
formal (newswire) and informal (online reviews)
texts, while minimizing the insertion of incorrect
discourse boundaries. Robustness is achieved by
constructing discourse segments in a principled
way using syntactic and lexical information.
Our approach employs a set of rules for insert-
ing segment boundaries based on the syntax of
each sentence. The segment boundaries are then
further refined by using lexical information that
?This work was supported by an NSERC Discovery Grant
(261104-2008) to Maite Taboada. We thank Angela Cooper
and Morgan Mameni for their help with the reliability study.
takes into consideration lexical cues, including
multi-word expressions. We also identify clauses
that are parsed as discourse segments, but are not
in fact independent discourse units, and join them
to the matrix clause.
Most parsers can break down a sentence into
constituent clauses, approaching the type of out-
put that we need as input to a discourse parser.
The segments produced by a parser, however, are
too fine-grained for discourse purposes, breaking
off complement and other clauses that are not in a
discourse relation to any other segment. For this
reason, we have implemented our own segmenter,
utilizing the output of a standard parser. The pur-
pose of this paper is to describe our syntactic and
lexical-based segmenter (SLSeg), demonstrate its
performance against state-of-the-art systems, and
make it available to the wider community.
2 Related Work
Soricut and Marcu (2003) construct a statistical
discourse segmenter as part of their sentence-level
discourse parser (SPADE), the only implemen-
tation available for our comparison. SPADE is
trained on the RST Discourse Treebank (Carlson
et al, 2002). The probabilities for segment bound-
ary insertion are learned using lexical and syntac-
tic features. Subba and Di Eugenio (2007) use
neural networks trained on RST-DT for discourse
segmentation. They obtain an F-score of 84.41%
(86.07% using a perfect parse), whereas SPADE
achieved 83.1% and 84.7% respectively.
Thanh et al (2004) construct a rule-based
segmenter, employing manually annotated parses
from the Penn Treebank. Our approach is con-
ceptually similar, but we are only concerned with
established discourse relations, i.e., we avoid po-
tential same-unit relations by preserving NP con-
stituency.
77
3 Principles For Discourse Segmentation
Our primary concern is to capture interesting dis-
course relations, rather than all possible relations,
i.e., capturing more specific relations such as Con-
dition, Evidence or Purpose, rather than more gen-
eral and less informative relations such as Elabo-
ration or Joint, as defined in Rhetorical Structure
Theory (Mann and Thompson, 1988). By having a
stricter definition of an elementary discourse unit
(EDU), this approach increases precision at the ex-
pense of recall.
Grammatical units that are candidates for dis-
course segments are clauses and sentences. Our
basic principles for discourse segmentation follow
the proposals in RST as to what a minimal unit
of text is. Many of our differences with Carl-
son and Marcu (2001), who defined EDUs for the
RST Discourse Treebank (Carlson et al, 2002),
are due to the fact that we adhere closer to the orig-
inal RST proposals (Mann and Thompson, 1988),
which defined as ?spans? adjunct clauses, rather
than complement (subject and object) clauses. In
particular, we propose that complements of at-
tributive and cognitive verbs (He said (that)..., I
think (that)...) are not EDUs. We preserve con-
sistency by not breaking at direct speech (?X,? he
said.). Reported and direct speech are certainly
important in discourse (Prasad et al, 2006); we do
not believe, however, that they enter discourse re-
lations of the type that RST attempts to capture.
In general, adjunct, but not complement clauses
are discourse units. We require all discourse seg-
ments to contain a verb. Whenever a discourse
boundary is inserted, the two newly created seg-
ments must each contain a verb. We segment coor-
dinated clauses (but not coordinated VPs), adjunct
clauses with either finite or non-finite verbs, and
non-restrictive relative clauses (marked by com-
mas). In all cases, the choice is motivated by
whether a discourse relation could hold between
the resulting segments.
4 Implementation
The core of the implementation involves the con-
struction of 12 syntactically-based segmentation
rules, along with a few lexical rules involving a list
of stop phrases, discourse cue phrases and word-
level parts of speech (POS) tags. First, paragraph
boundaries and sentence boundaries using NIST?s
sentence segmenter1 are inserted. Second, a sta-
tistical parser applies POS tags and the sentence?s
syntactic tree is constructed. Our syntactic rules
are executed at this stage. Finally, lexical rules,
as well as rules that consider the parts-of-speech
for individual words, are applied. Segment bound-
aries are removed from phrases with a syntactic
structure resembling independent clauses that ac-
tually are used idiomatically, such as as it stands
or if you will. A list of phrasal discourse cues
(e.g., as soon as, in order to) are used to insert
boundaries not derivable from the parser?s output
(phrases that begin with in order to... are tagged as
PP rather than SBAR). Segmentation is also per-
formed within parentheticals (marked by paren-
theses or hyphens).
5 Data and Evaluation
5.1 Data
The gold standard test set consists of 9 human-
annotated texts. The 9 documents include 3 texts
from the RST literature2, 3 online product reviews
from Epinions.com, and 3 Wall Street Journal ar-
ticles taken from the Penn Treebank. The texts av-
erage 21.2 sentences, with the longest text having
43 sentences and the shortest having 6 sentences,
for a total of 191 sentences and 340 discourse seg-
ments in the 9 gold-standard texts.
The texts were segmented by one of the au-
thors following guidelines that were established
from the project?s beginning and was used as the
gold standard. The annotator was not directly in-
volved in the coding of the segmenter. To ensure
the guidelines followed clear and sound principles,
a reliability study was performed. The guidelines
were given to two annotators, both graduate stu-
dents in Linguistics, that had no direct knowledge
of the project. They were asked to segment the 9
texts used in the evaluation.
Inter-annotator agreement across all three anno-
tators using Kappa was .85, showing a high level
of agreement. Using F-score, average agreement
of the two annotators against the gold standard was
also high at .86. The few disagreements were pri-
marily due to a lack of full understanding of the
guidelines (e.g., the guidelines specify to break ad-
junct clauses when they contain a verb, but one
of the annotators segmented prepositional phrases
1http://duc.nist.gov/duc2004/software/
duc2003.breakSent.tar.gz
2Available from the RST website http://www.sfu.ca/rst/
78
Epinions Treebank Original RST Combined Total
System P R F P R F P R F P R F
Baseline .22 .70 .33 .27 .89 .41 .26 .90 .41 .25 .80 .38
SPADE (coarse) .59 .66 .63 .63 1.0 .77 .64 .76 .69 .61 .79 .69
SPADE (original) .36 .67 .46 .37 1.0 .54 .38 .76 .50 .37 .77 .50
Sundance .54 .56 .55 .53 .67 .59 .71 .47 .57 .56 .58 .57
SLSeg (Charniak) .97 .66 .79 .89 .86 .87 .94 .76 .84 .93 .74 .83
SLSeg (Stanford) .82 .74 .77 .82 .86 .84 .88 .71 .79 .83 .77 .80
Table 1: Comparison of segmenters
that had a similar function to a full clause). With
high inter-annotator agreement (and with any dis-
agreements and errors resolved), we proceeded to
use the co-author?s segmentations as the gold stan-
dard.
5.2 Evaluation
The evaluation uses standard precision, recall and
F-score to compute correctly inserted segment
boundaries (we do not consider sentence bound-
aries since that would inflate the scores). Precision
is the number of boundaries in agreement with the
gold standard. Recall is the total number of bound-
aries correct in the system?s output divided by the
number of total boundaries in the gold standard.
We compare the output of SLSeg to SPADE.
Since SPADE is trained on RST-DT, it inserts seg-
ment boundaries that are different from what our
annotation guidelines prescribe. To provide a fair
comparison, we implement a coarse version of
SPADE where segment boundaries prescribed by
the RST-DT guidelines, but not part of our seg-
mentation guidelines, are manually removed. This
version leads to increased precision while main-
taining identical recall, thus improving F-score.
In addition to SPADE, we also used the Sun-
dance parser (Riloff and Phillips, 2004) in our
evaluation. Sundance is a shallow parser which
provides clause segmentation on top of a basic
word-tagging and phrase-chunking system. Since
Sundance clauses are also too fine-grained for our
purposes, we use a few simple rules to collapse
clauses that are unlikely to meet our definition of
EDU. The baseline segmenter in Table 1 inserts
segment boundaries before and after all instances
of S, SBAR, SQ, SINV, SBARQ from the syntac-
tic parse (text spans that represent full clauses able
to stand alone as sentential units). Finally, two
parsers are compared for their effect on segmenta-
tion quality: Charniak (Charniak, 2000) and Stan-
ford (Klein and Manning, 2003).
5.3 Qualitative Comparison
Comparing the outputs of SLSeg and SPADE on
the Epinions.com texts illustrates key differences
between the two approaches.
[Luckily we bought the extended pro-
tection plans from Lowe?s,] # [so we
are waiting] [for Whirlpool to decide]
[if they want to do the costly repair] [or
provide us with a new machine].
In this example, SLSeg inserts a single bound-
ary (#) before the word so, whereas SPADE in-
serts four boundaries (indicated by square brack-
ets). Our breaks err on the side of preserving se-
mantic coherence, e.g., the segment for Whirlpool
to decide depends crucially on the adjacent seg-
ments for its meaning. In our opinion, the rela-
tions between these segments are properly the do-
main of a semantic, but not a discourse, parser. A
clearer example that illustrates the pitfalls of fine-
grained discourse segmenting is shown in the fol-
lowing output from SPADE:
[The thing] [that caught my attention
was the fact] [that these fantasy novels
were marketed...]
Because the segments are a restrictive relative
clause and a complement clause, respectively,
SLSeg does not insert any segment boundaries.
6 Results
Results are shown in Table 1. The combined in-
formal and formal texts show SLSeg (using Char-
niak?s parser) with high precision; however, our
overall recall was lower than both SPADE and the
baseline. The performance of SLSeg on the in-
formal and formal texts is similar to our perfor-
79
mance overall: high precision, nearly identical re-
call. Our system outperforms all the other systems
in both precision and F-score, confirming our hy-
pothesis that adapting an existing system would
not provide the high-quality discourse segments
we require.
The results of using the Stanford parser as an
alternative to the Charniak parser show that the
performance of our system is parser-independent.
High F-score in the Treebank data can be at-
tributed to the parsers having been trained on Tree-
bank. Since SPADE also utilizes the Charniak
parser, the results are comparable.
Additionally, we compared SLSeg and SPADE
to the original RST segmentations of the three
RST texts taken from RST literature. Performance
was similar to that of our own annotations, with
SLSeg achieving an F-score of .79, and SPADE
attaining .38. This demonstrates that our approach
to segmentation is more consistent with the origi-
nal RST guidelines.
7 Discussion
We have shown that SLSeg, a conservative rule-
based segmenter that inserts fewer discourse
boundaries, leads to higher precision compared to
a statistical segmenter. This higher precision does
not come at the expense of a significant loss in
recall, as evidenced by a higher F-score. Unlike
statistical parsers, our system requires no training
when porting to a new domain.
All software and data are available3. The
discourse-related data includes: a list of clause-
like phrases that are in fact discourse markers
(e.g., if you will, mind you); a list of verbs used
in to-infinitival and if complement clauses that
should not be treated as separate discourse seg-
ments (e.g., decide in I decided to leave the car
at home); a list of unambiguous lexical cues for
segment boundary insertion; and a list of attribu-
tive/cognitive verbs (e.g., think, said) used to pre-
vent segmentation of floating attributive clauses.
Future work involves studying the robustness of
our discourse segments on other corpora, such as
formal texts from the medical domain and other
informal texts. Also to be investigated is a quan-
titative study of the effects of high-precision/low-
recall vs. low-precision/high-recall segmenters on
the construction of discourse trees. Besides its use
in automatic discourse parsing, the system could
3http://www.sfu.ca/?mtaboada/research/SLSeg.html
assist manual annotators by providing a set of dis-
course segments as starting point for manual an-
notation of discourse relations.
References
Lynn Carlson and Daniel Marcu. 2001. Discourse
Tagging Reference Manual. ISI Technical Report
ISI-TR-545.
Lynn Carlson, Daniel Marcu and Mary E. Okurowski.
2002. RST Discourse Treebank. Philadelphia, PA:
Linguistic Data Consortium.
Eugene Charniak. 2000. A Maximum-Entropy In-
spired Parser. Proc. of NAACL, pp. 132?139. Seat-
tle, WA.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, Intentions, and the Structure of Discourse.
Computational Linguistics, 12:175?204.
Dan Klein and Christopher D. Manning. 2003. Fast
Exact Inference with a Factored Model for Natu-
ral Language Parsing. Advances in NIPS 15 (NIPS
2002), Cambridge, MA: MIT Press, pp. 3?10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8:243?281.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press,
Cambridge, MA.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse Segmentation by Human and Automated
Means. Computational Linguistics, 23(1):103?139.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi and Bonnie Webber. 2006. Attribution and its
Annotation in the Penn Discourse TreeBank. Traite-
ment Automatique des Langues, 47(2):43?63.
Ellen Riloff and William Phillips. 2004. An Introduc-
tion to the Sundance and AutoSlog Systems. Univer-
sity of Utah Technical Report #UUCS-04-015.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. Proc. of HLT-NAACL, pp. 149?156. Ed-
monton, Canada.
Rajen Subba and Barbara Di Eugenio. 2007. Auto-
matic Discourse Segmentation Using Neural Net-
works. Proc. of the 11th Workshop on the Se-
mantics and Pragmatics of Dialogue, pp. 189?190.
Rovereto, Italy.
Huong Le Thanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Automated Discourse Segmentation
by Syntactic Information and Cue Phrases. Proc. of
IASTED. Innsbruck, Austria.
80
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62?70,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Genre-Based Paragraph Classification for Sentiment Analysis 
 
 
Maite Taboada 
Department of Linguistics 
Simon Fraser University 
Burnaby, BC, Canada 
mtaboada@sfu.ca 
Julian Brooke 
Department of Computer Science 
University of Toronto 
Toronto, ON, Canada 
jbrooke@cs.toronto.edu 
Manfred Stede 
Institute of Linguistics 
University of Potsdam 
Potsdam, Germany 
stede@ling.uni-
potsdam.de 
 
  
 
 
Abstract 
We present a taxonomy and classification 
system for distinguishing between differ-
ent types of paragraphs in movie reviews: 
formal vs. functional paragraphs and, 
within the latter, between description and 
comment. The classification is used for 
sentiment extraction, achieving im-
provement over a baseline without para-
graph classification. 
1 Introduction 
Much of the recent explosion in sentiment-
related research has focused on finding low-level 
features that will help predict the polarity of a 
phrase, sentence or text. Features, widely unders-
tood, may be individual words that tend to ex-
press sentiment, or other features that indicate 
not only sentiment, but also polarity. The two 
main approaches to sentiment extraction, the se-
mantic or lexicon-based, and the machine learn-
ing or corpus-based approach, both attempt to 
identify low-level features that convey opinion. 
In the semantic approach, the features are lists of 
words and their prior polarity, (e.g., the adjective 
terrible will have a negative polarity, and maybe 
intensity, represented as -4; the noun masterpiece 
may be a 5). Our approach is lexicon-based, but 
we make use of information derived from ma-
chine learning classifiers. 
Beyond the prior polarity of a word, its local 
context obviously plays an important role in 
conveying sentiment. Polanyi and Zaenen (2006) 
use the term ?contextual valence shifters? to refer 
to expressions in the local context that may 
change a word?s polarity, such as intensifiers, 
modal verbs, connectives, and of course negation. 
Further beyond the local context, the overall 
structure and organization of the text, influenced 
by its genre, can help the reader determine how 
the evaluation is expressed, and where it lies. 
Polanyi and Zaenen (2006) also cite genre con-
straints as relevant factors in calculating senti-
ment.  
Among the many definitions of genre, we take 
the view of Systemic Functional Linguistics that 
genres are purposeful activities that develop in 
stages, or parts (Eggins and Martin, 1997), which 
can be identified by lexicogrammatical proper-
ties (Eggins and Slade, 1997). Our proposal is 
that, once we have identified different stages in a 
text, the stages can be factored in the calculation 
of sentiment, by weighing more heavily those 
that are more likely to contain evaluation, an ap-
proach also pursued in automatic summarization 
(Seki et al, 2006). 
To test this hypothesis, we created a taxonomy 
of stages specific to the genre of movie reviews, 
and annotated a set of texts. We then trained 
various classifiers to differentiate the stages. 
Having identified the stages, we lowered the 
weight of those that contained mostly description. 
Our results show that we can achieve improve-
ment over a baseline when classifying the polar-
ity of texts, even with a classifier that can stand 
to improve (at 71.1% accuracy). The best per-
formance comes from weights derived from the 
output of a linear regression classifier. 
We first describe our inventory of stages and 
the manual annotation (Section 2), and in Sec-
tion 3 turn to automatic stage classification. After 
describing our approach to sentiment classifica-
tion of texts in Section 4, we describe experi-
ments to improve its performance with the in-
formation on stages in Section 5. Section 6 dis-
62
cusses related work, and Section 7 provides con-
clusions.  
2 Stages in movie reviews 
Within the larger review genre, we focus on 
movie reviews. Movie reviews are particularly 
difficult to classify (Turney, 2002), because large 
portions of the review contain description of the 
plot, the characters, actors, director, etc., or 
background information about the film. 
Our approach is based on the work of Bieler et 
al. (2007), who identify formal and functional 
zones (stages) within German movie reviews. 
Formal zones are parts of the text that contribute 
factual information about the cast and the credits, 
and also about the review itself (author, date of 
publication and the reviewer?s rating of the mov-
ie). Functional zones contain the main gist of the 
review, and can be divided roughly into descrip-
tion and comment. Bieler et al showed that func-
tional zones could be identified using 5-gram 
SVM classifiers built from an annotated German 
corpus.  
2.1 Taxonomy 
In addition to the basic Describe/Comment dis-
tinction in Bieler et al, we use a De-
scribe+Comment label, as in our data it is often 
the case that both description and comment are 
present in the same paragraph. We decided that a 
paragraph could be labeled as De-
scribe+Comment when it contained at least a 
clause of each, and when the comment part could 
be assigned a polarity (i.e., it was not only sub-
jective, but also clearly positive or negative).  
Each of the three high-level tags has a subtag, 
a feature also present in Bieler et al?s manual 
annotation. The five subtags are: overall, plot, 
actors/characters, specific and general. ?Specific? 
refers to one particular aspect of the movie (not 
plot or characters), whereas ?general? refers to 
multiple topics in the same stage (special effects 
and cinematography at the same time). Outside 
the Comment/Describe scale, we also include 
tags such as Background (discussion of other 
movies or events outside the movie being 
reviewed), Interpretation (subjective but not 
opinionated or polar), and Quotes. Altogether, 
the annotation system includes 40 tags, with 22 
formal and 18 functional zones. Full lists of 
zone/stage labels are provided in Appendix A. 
2.2 Manual annotation 
We collected 100 texts from rottentomatoes.com, 
trying to include one positive and one negative 
review for the same movie. The reviews are part 
of the ?Top Critics? section of the site, all of 
them published in newspapers or on-line maga-
zines. We restricted the texts to ?Top Critics? 
because we wanted well-structured, polished 
texts, unlike those found in some on-line review 
sites. Future work will address those more in-
formal reviews. 
The 100 reviews contain 83,275 words and 
1,542 paragraphs. The annotation was performed 
at the paragraph level. Although stages may span 
across paragraphs, and paragraphs may contain 
more than one stage, there is a close relationship 
between paragraphs and stages. The restriction 
also resulted in a more reliable annotation, per-
formed with the PALinkA annotation tool (Ora-
san, 2003). 
The annotation was performed by one of the 
authors, and we carried out reliability tests with 
two other annotators, one another one of the au-
thors, who helped develop the taxonomy, and the 
third one a project member who read the annota-
tion guidelines1, and received a few hours? train-
ing in the labels and software. We used Fleiss? 
kappa (Fleiss, 1971), which extends easily to the 
case of multiple raters (Di Eugenio and Glass, 
2004). We all annotated four texts. The results of 
the reliability tests show a reasonable agreement 
level for the distinction between formal and 
functional zones (.84 for the 3-rater kappa). The 
lowest reliability was for the 3-way distinction in 
the functional zones (.68 for the first two raters, 
and .54 for the three raters). The full kappa val-
ues for all the distinctions are provided in Ap-
pendix B. After the reliability test, one of the 
authors performed the full annotation for all 100 
texts. Table 1 shows the breakdown of high-level 
stages for the 100 texts.  
 
Stage Count 
Describe 347 
Comment 237 
Describe+Comment 237 
Background 51 
Interpretation 22 
Quote 2 
Formal 646 
Table 1. Stages in 100 text RT corpus 
                                                 
1Available from http://www.sfu.ca/~mtaboada/nserc-
project.html 
63
3 Classifying stages 
Our first classification task aims at distinguishing 
the two main types of functional zones, Com-
ment and Describe, vs. Formal zones.  
3.1 Features 
We test two different sets of features. The first, 
following Bieler et al (2007), consists of 5-
grams (including unigrams, bigrams, 3-grams 
and 4-grams), although we note in our case that 
there was essentially no performance benefit 
beyond 3-grams. We limited the size of our fea-
ture set to n-grams that appeared at least 4 times 
in our training corpus. For the 2 class task (no 
formal zones), this resulted in 8,092 binary fea-
tures, and for the 3 and 4 class task there were 
9,357 binary n-gram features. 
The second set of features captures different 
aspects of genre and evaluation, and can in turn 
be divided into four different types, according to 
source. With two exceptions (features indicating 
whether a paragraph was the first or last para-
graph in text), the features were numerical (fre-
quency) and normalized to the length of the pa-
ragraph. 
The first group of genre features comes from 
Biber (1988), who attempted to characterize di-
mensions of genre. The features here include fre-
quency of first, second and third person pro-
nouns; demonstrative pronouns; place and time 
adverbials; intensifiers; and modals, among a 
number of others. 
The second category of genre features in-
cludes discourse markers, primarily from Knott 
(1996), that indicate contrast, comparison, causa-
tion, evidence, condition, and similar relations. 
The third type of genre features was a list of 
500 adjectives classified in terms of Appraisal 
(Martin and White, 2005) as indicating Apprec-
iation, Judgment or Affect. Appraisal categories 
have been shown to be useful in improving the 
performance of polarity classifiers (Whitelaw et 
al., 2005).  
Finally, we also include text statistics as fea-
tures, such as average length of words and sen-
tences and position of paragraphs in the text.  
3.2 Classifiers 
To classify paragraphs in the text, we use the 
WEKA suite (Witten and Frank, 2005), testing 
three popular machine learning algorithms: 
Na?ve Bayes, Support Vector Machine, and Li-
near Regression (preliminary testing with Deci-
sion Trees suggests that it is not appropriate for 
this task). Training parameters were set to default 
values. 
In order to use Linear Regression, which pro-
vides a numerical output based on feature values 
and derived feature weights, we have to conceive 
of Comment/Describe/Describe+Comment not as 
nominal (or ordinal) classes, but rather as corres-
ponding to a Comment/Describe ratio, with 
?pure? Describe at one end and ?pure? Comment 
at the other. For training, we assign a 0 value (a 
Comment ratio) to all paragraphs tagged De-
scribe and a 1 to all Comment paragraphs; for 
Describe+Comment, various options (including 
omission of this data) were tested. The time re-
quired to train a linear regression classifier on a 
large feature set proved to be prohibitive, and 
performance with smaller sets of features gener-
ally quite poor, so for the linear regression clas-
sifier we present results only for our compact set 
of genre features. 
3.3 Performance 
Table 2 shows the performance of classifi-
er/feature-set combinations for the 2-, 3-, and 4-
class tasks on the 100-text training set, with 10-
fold cross-validation, in terms of precision (P), 
recall (R) and F-measure 2 . SVM and Na?ve 
Bayes provide comparable performance, al-
though there is considerable variation, particular-
ly with respect to the feature set; the SVM is a 
significantly (p<0.05) better choice for our genre 
features 3 , while for the n-gram features the 
Bayes classification is generally preferred. The 
SVM-genre classifier significantly outperforms 
the other classifiers in the 2-class task; these ge-
nre features, however, are not as useful as 5-
grams at identifying Formal zones (the n-gram 
classifier, by contrast, can make use of words 
such as cast). In general, formal zone classifica-
tion is fairly straightforward, whereas identifica-
tion of Describe+Comment is quite difficult, and 
the SVM-genre classifier, which is more sensi-
tive to frequency bias, elects to (essentially) ig-
nore this category in order to boost overall accu-
racy.  
To evaluate a linear regression (LR) classifier, 
we calculate correlation coefficient ?, which re-
flects the goodness of fit of the line to the da-
ta. Table 3 shows values for the classifiers built 
from the corpus, with various Comment ratios
                                                 
2 For the 2- and 3-way classifiers, Describe+Comment pa-
ragraphs are treated as Comment. This balances the num-
bers of each class, ultimately improving performance. 
3 All significance tests use chi-square (?2). 
64
Classifier 
Comment Describe Formal Desc+Comm Overall 
Accuracy P R F P R F P R F P R F 
2-class-5-gram-Bayes .66 .79 .72 .70 .55 .62 - - - - - - 68.0 
2-class-5-gram-SVM .53 .63 .64 .68 .69 .69 - - - - - - 66.8 
2-class-genre-Bayes .66 .75 .70 .67 .57 .61 - - - - - - 66.2 
2-class-genre-SVM .71 .76 .74 .71 .65 .68 - - - - - - 71.1 
3-class-5-gram-Bayes .69 .49 .57 .66 .78 .71 .92 .97 .95 - - - 78.1 
3-class-5-gram-SVM .64 .63 .63 .68 .65 .65 .91 .97 .94 - - - 77.2 
3-class-genre-Bayes .68 .68 .66 .67 .46 .55 .84 .96 .90 - - - 74.0 
3-class-genre-SVM .66 .71 .68 .67 .56 .61 .90 .94 .92 - - - 76.8 
4-class-5-gram-Bayes .46 .35 .38 .69 .47 .56 .92 .97 .95 .42 .64 .51 69.0 
4-class-5-gram-SVM .43 .41 .44 .59 .62 .60 .91 .97 .94 .45 .41 .42 69.6 
4-class-genre-Bayes .38 .31 .34 .66 .30 .41 .86 .97 .90 .33 .60 .42 62.3 
4-class-genre-SVM .46 .32 .38 .53 .82 .65 .87 .94 .90 .26 .03 .06 67.4 
Table 2. Stage identification performance of various categorical classifiers 
 
(C) assigned to paragraphs with the De-
scribe+Comment tag, and with De-
scribe+Comment paragraphs removed from con-
sideration. 
 
Classifier ? 
LR, Des+Com C = 0 .37 
LR, Des+Com C = 0.25 .44 
LR, Des+Com C = 0.5 .47 
LR, Des+Com C = 0.75 .46 
LR, Des+Com C = 1 .43 
LR, No Des+Com .50 
Table 3. Correlation coefficients for LR 
classifiers 
The drop in correlation when more extreme 
values are assigned to Describe+Comment sug-
gests that Describe+Comment paragraphs do in-
deed belong in the middle of the Comment spec-
trum. Since there is a good deal of variation in 
the amount of comment across De-
scribe+Comment paragraphs, the best correlation 
comes with complete removal of these somewhat 
unreliable paragraphs. Overall, these numbers 
indicate that variations in relevant features are 
able to predict roughly 50% of the variation in 
Comment ratio, which is fairly good considering 
the small number and simplistic nature of the 
features involved. 
4 Sentiment detection: SO-CAL 
In this section, we outline our semantic orienta-
tion calculator, SO-CAL. SO-CAL extracts 
words from a text, and aggregates their semantic 
orientation value, which is in turn extracted from 
a set of dictionaries. SO-CAL uses five dictionar-
ies: four lexical dictionaries with 2,257 adjec-
tives, 1,142 nouns, 903 verbs, and 745 adverbs, 
and a fifth dictionary containing 177 intensifying 
expressions. Although the majority of the entries 
are single words, the calculator also allows for 
multiword entries written in regular expression-
like language.  
The SO-carrying words in these dictionaries 
were taken from a variety of sources, the three 
largest a corpus of 400 reviews from Epin-
ions.com, first used by Taboada and Grieve 
(2004), a 100 text subset of the 2,000 movie re-
views in the Polarity Dataset (Pang and Lee, 
2004), and words from the General Inquirer dic-
tionary (Stone, 1997). Each of the open-class 
words were given a hand-ranked SO value be-
tween 5 and -5 (neutral or zero-value words are 
not included in the dictionary) by a native Eng-
lish speaker. The numerical values were chosen 
to reflect both the prior polarity and strength of 
the word, averaged across likely interpretations. 
For example, the word phenomenal is a 5, nicely 
a 2, disgust a -3, and monstrosity a -5. The dic-
tionary was later reviewed by a committee of 
three other researchers in order to minimize the 
subjectivity of ranking SO by hand. 
Our calculator moves beyond simple averag-
ing of each word?s semantic orientation value, 
and implements and expands on the insights of 
Polanyi and Zaenen (2006) with respect to con-
textual valence shifters. We implement negation 
by shifting the SO value of a word towards the 
opposite polarity (not terrible, for instance, is 
calculated as -5+4 = -1). Intensification is mod-
eled using percentage modifiers (very engaging: 
4x125% = 5). We also ignore words appearing 
within the scope of irrealis markers such as cer-
tain verbs, modals, and punctuation, and de-
crease the weight of words which appear often in 
the text. In order to counter positive linguistic 
65
bias (Boucher and Osgood, 1969), a problem for 
lexicon-based sentiment classifiers (Kennedy and 
Inkpen, 2006), we increase the final SO of any 
negative expression appearing in the text. 
The performance of SO-CAL tends to be in 
the 76-81% range. We have tested on informal 
movie, book and product reviews and on the Po-
larity Dataset (Pang and Lee, 2004). The perfor-
mance on movie reviews tends to be on the lower 
end of the scale. Our baseline for movies, de-
scribed in Section 5, is 77.7%. We believe that 
we have reached a ceiling in terms of word- and 
phrase-level performance, and most future im-
provements need to come from discourse fea-
tures. The stage classification described in this 
paper is one of them.  
5 Results 
The final goal of a stage classifier is to use the 
information about different stages in sentiment 
classification. Our assumption is that descriptive 
paragraphs contain less evaluative content about 
the movie being reviewed, and they may include 
noise, such as evaluative words describing the 
plot or the characters. Once the paragraph clas-
sifier had assigned labels we used those labels to 
weigh paragraphs. 
5.1 Classification with manual tags 
Before moving on to automatic paragraph classi-
fication, we used the 100 annotated texts to see 
the general effect of weighting paragraphs with 
the ?perfect? human annotated tags on sentiment 
detection, in order to show the potential im-
provements that can be gained from this ap-
proach.  
Our baseline polarity detection performance 
on the 100 annotated texts is 65%, which is very 
low, even for movie reviews. We posit that for-
mal movie reviews might be particularly difficult 
because full plot descriptions are more common 
and the language used to express opinion less 
straightforward (metaphors are common). How-
ever, if we lower the weight on non-Comment 
and mixed Comment paragraphs (to 0, except for 
Describe+Comment, which is maximized by a 
0.1 weight), we are able to boost performance to 
77%, an improvement which is significant at the 
p<0.05 level. Most of the improvement (7%) is 
due to disregarding Describe paragraphs, but 2% 
comes from Describe+Comment, and 1% each 
from Background, Interpretation, and (all) For-
mal tags. There is no performance gain, however, 
from the use of aspect tags (e.g., by increasing 
the weight on Overall paragraphs), justifying our 
decision to ignore subtags for text-level polarity 
classification.  
5.2 Categorical classification 
We evaluated all the classifiers from Table 2, but 
we omit discussion of the worst performing. The 
evaluation was performed on the Polarity Dataset 
(Pang and Lee, 2004), a collection of 2,000 on-
line movie reviews, balanced for polarity. The 
SO performance for the categorical classifiers is 
given in Figure 1. When applicable, we always 
gave Formal Zones (which Table 2 indicates are 
fairly easy to identify) a weight of 0, however for 
Describe paragraphs we tested at 0.1 intervals 
between 0 and 1. Testing all possible values of 
Describe+Comment was not feasible, so we set 
the weights of those to a value halfway between 
the weight of Comment paragraphs (1) and the 
weight of the Describe paragraph. 
Most of the classifiers were able to improve 
performance beyond the 77.7% (unweighted) 
baseline. The best performing model (the 2-
class-genre-SVM) reached a polarity identifica-
tion accuracy of 79.05%, while the second best 
(the 3-class 5-gram-SVM) topped out at 78.9%. 
Many of the classifiers showed a similar pattern 
with respect to the weight on Describe, increas-
ing linearly as weight on Describe was decreased 
before hitting a maximum in the 0.4-0.1 range, 
and then dropping afterwards (often precipitous-
ly). Only the classifiers which were more con-
servative with respect to Describe, such as the 4-
class-5-gram-Bayes, avoided the drop, which can 
be attributed to low precision Describe identifi-
cation: At some point, the cost associated with 
disregarding paragraphs which have been mis-
tagged as Describe becomes greater that the ben-
efit of disregarding correctly-labeled ones. In-
deed, the best performing classifier for each class 
option is exactly the one that has the highest pre-
cision for identification of Describe, regardless 
of other factors. This suggests that improving 
precision is key, and, in lieu of that, weighting is 
a better strategy than simply removing parts of 
the text. 
In general, increasing the complexity of the 
task (increasing the number of classes) decreases 
performance. One clear problem is that the iden-
tification of Formal zones, which are much more 
common in our training corpus than our test cor-
pus, does not add important information, since 
most Formal zones have no SO valued words. 
The delineation of an independent De-
scribe+Comment class is mostly ineffective, 
66
 
Figure 1. SO Performance with various paragraph tagging classifiers, by weight on Describe 
 
probably because this class is not easily distin-
guishable from Describe and Comment (nor in 
fact should it be). 
We can further confirm that our classifier is 
properly distinguishing Describe and Comment 
by discounting Comment paragraphs rather than 
Describe paragraphs (following Pang and Lee 
2004). When Comment paragraphs tagged by the 
best performing classifier are ignored, SO-CAL?s 
accuracy drops to 56.65%, just barely above 
chance. 
5.3 Continuous classification 
Table 4 gives the results for the linear regression 
classifier, which assigns a Comment ratio to each 
paragraph used for weighting.  
 
Model Accuracy 
LR, Des+Com C = 0 78.75 
LR, Des+Com C = 0.25 79.35 
LR, Des+Com C = 0.5 79.00 
LR, Des+Com C = 0.75 78.90 
LR, Des+Com C = 1 78.95 
LR, No Des+Com 79.05 
Table 4. SO Performance with linear regression 
 
The linear regression model trained with a 
0.25 comment ratio on Describe+Comment para-
graphs provides the best performance of all clas-
sifiers we tested (an improvement of 1.65% from 
baseline). The correlation coefficients noted 
in Table 4 are reflected in these results, but the 
spike at C = 0.25 is most likely related to a gen-
eral preference for low (but non-zero) weights on 
Describe+Comment paragraphs also noted when 
weights were applied using the manual tags; 
these paragraphs are unreliable (as compared to 
pure Comment), but cannot be completely dis-
counted. There were some texts which had only 
Describe+Comment paragraphs.  
Almost a third of the tags assigned by the 2-
class genre feature classifier were different than 
the corresponding n-gram classifier, suggesting 
the two classifiers might have different strengths. 
However, initial attempts to integrate the various 
high performing classifiers?including collaps-
ing of feature sets, metaclassifiers, and double 
tagging of paragraphs?resulted in similar or 
worse performance. We have not tested all poss-
ible options (there are simply too many), but we 
think it unlikely that additional gains will be 
made with these simple, surface feature sets. Al-
though our testing with human annotated texts 
and the large performance gap between movie 
reviews and other consumer reviews both sug-
gest there is more potential for improvement, it 
will probably require more sophisticated and 
precise models. 
6 Related work 
The bulk of the work in sentiment analysis has 
focused on classification at either the sentence 
level, e.g., the subjectivity/polarity detection of 
Wiebe and Riloff (2005), or alternatively at the 
level of the entire text. With regards to the latter, 
two major approaches have emerged: the use of 
machine learning classifiers trained on n-grams 
77
78
79
80
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
SO
?C
al
cu
la
to
r?
A
cc
ur
ac
y
Weight?on?Describe?Paragraph
No?tagging?Baseline
2?Class?5?gram?SVM
2?Class?5?gram?Bayes
2?Class?genre?Bayes
2?Class?genre?SVM
3?Class?5?gram?Bayes
3?Class?5?gram?SVM
3?Class?genre?Bayes
4?Class?5?gram?Bayes
4?Class?5?gram?SVM
4?Class?genre?Bayes
67
or similar features (Pang et al, 2002), and the 
use of sentiment dictionaries (Esuli and Sebas-
tiani, 2006; Taboada et al, 2006). Support Vec-
tor Machine (SVM) classifiers have been shown 
to out-perform lexicon-based models within a 
single domain (Kennedy and Inkpen, 2006); 
however they have trouble with cross-domain 
tasks (Aue and Gamon, 2005), and some re-
searchers have argued for hybrid classifiers (An-
dreevskaia and Bergler, 2008). 
Pang and Lee (2004) attempted to improve the 
performance of an SVM classifier by identifying 
and removing objective sentences from the texts. 
Results were mixed: The improvement was mi-
nimal for the SVM classifier (though the perfor-
mance of a na?ve Bayes classifier was signifi-
cantly boosted), however testing with parts of the 
text classified as subjective showed that the elim-
inated parts were indeed irrelevant. In contrast to 
our findings, they reported a drop in performance 
when paragraphs were taken as the only possible 
boundary between subjective and objective text 
spans. 
Other research that has dealt with identifying 
more or less relevant parts of the text for the pur-
poses of sentiment analysis include Taboada and 
Grieve (2004), who improved the performance of 
a lexicon-based model by weighing words to-
wards the end of the text; Nigam and Hurst 
(2006), who detect polar expressions in topic 
sentences; and Voll and Taboada (2007), who 
used a topic classifier and discourse parser to 
eliminate potentially off-topic or less important 
sentences. 
7 Conclusions 
We have described a genre-based taxonomy for 
classifying paragraphs in movie reviews, with 
the main classification being a distinction be-
tween formal and functional stages, and, within 
those, between mainly descriptive vs. comment 
stages. The taxonomy was used to annotate 100 
movie reviews, as the basis for building classifi-
ers.  
We tested a number of different classifiers. 
Our results suggest that a simple, two-way or 
continuous classification using a small set of lin-
guistically-motivated features is the best for our 
purposes; a more complex system is feasible, but 
comes at the cost of precision, which seems to be 
the key variable in improving sentiment analysis. 
Ultimately, the goal of the classification was 
to improve the accuracy of SO-CAL, our seman-
tic orientation calculator. Using the manual an-
notations, we manage to boost performance by 
12% over the baseline. With the best automatic 
classifier, we still show consistent improvement 
over the baseline. Given the relatively low accu-
racy of the classifiers, the crucial factor involves 
using fine-grained weights on paragraphs, rather 
than simply ignoring Describe-labeled para-
graphs, as Pang and Lee (2004) did for objective 
sentences.  
An obvious expansion to this work would in-
volve a larger dataset on which to train, to im-
prove the performance of the classifier(s). We 
would also like to focus on the syntactic patterns 
and verb class properties of narration, aspects 
that are not captured with simply using words 
and POS labels. Connectives in particular are 
good indicators of the difference between narra-
tion (temporal connectives) and opinion (contras-
tive connectives). There may also be benefit to 
combining paragraph- and sentence-based ap-
proaches. Finally, we would like to identify 
common sequences of stages, such as plot and 
character descriptions appearing together, and 
before evaluation stages. This generic structure 
has been extensively studied for many genres 
(Eggins and Slade, 1997). 
Beyond sentiment extraction, our taxonomy 
and classifiers can be used for searching and in-
formation retrieval. One could, for instance, ex-
tract paragraphs that include mostly comment or 
description. Using the more fine-grained labels, 
searches for comment/description on actors, di-
rectors, or other aspects of the movie are possible. 
Acknowledgements 
This work was supported by SSHRC (410-2006-
1009) and NSERC (261104-2008) grants to 
Maite Taboada. 
References 
Andreevskaia, Alina & Sabine Bergler. 2008. When 
specialists and generalists work together: Domain 
dependence in sentiment tagging. Proceedings of 
46th Annual Meeting of the Association for Com-
putational Linguistics (pp. 290-298). Columbus, 
OH. 
Aue, Anthony & Michael Gamon. 2005. Customizing 
sentiment classifiers to new domains: A case study. 
Proceedings of the International Conference on 
Recent Advances in Natural Language Processing. 
Borovets, Bulgaria. 
Biber, Douglas. 1988. Variation across Speech and 
Writing. Cambridge: Cambridge University Press. 
68
Bieler, Heike, Stefanie Dipper & Manfred Stede. 
2007. Identifying formal and functional zones in 
film reviews. Proceedings of the 8th SIGdial 
Workshop on Discourse and Dialogue (pp. 75-78). 
Antwerp, Belgium. 
Boucher, Jerry D. & Charles E. Osgood. 1969. The 
Pollyanna hypothesis. Journal of Verbal Learning 
and Verbal Behaviour, 8: 1-8. 
Di Eugenio, Barbara & Michael Glass. 2004. The 
kappa statistic: A second look. Computational Lin-
guistics, 30(1): 95-101. 
Eggins, Suzanne & James R. Martin. 1997. Genres 
and registers of discourse. In Teun A. van Dijk 
(ed.), Discourse as Structure and Process. Dis-
course Studies: A Multidisciplinary Introduction 
(pp. 230-256). London: Sage. 
Eggins, Suzanne & Diana Slade. 1997. Analysing 
Casual Conversation. London: Cassell. 
Esuli, Andrea & Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for 
opinion mining. Proceedings of 5th International 
Conference on Language Resources and Evaluation 
(LREC) (pp. 417-422). Genoa, Italy. 
Fleiss, Joseph L. 1971. Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 76: 378-382. 
Kennedy, Alistair & Diana Inkpen. 2006. Sentiment 
classification of movie and product reviews using 
contextual valence shifters. Computational Intelli-
gence, 22(2): 110-125. 
Knott, Alistair. 1996. A Data-Driven Methodology for 
Motivating a Set of Coherence Relations. Edin-
burgh, UK: University of EdinburghThesis Type. 
Martin, James R. & Peter White. 2005. The Language 
of Evaluation. New York: Palgrave. 
Nigam, Kamal & Matthew Hurst. 2006. Towards a 
robust metric of polarity. In Janyce Wiebe (ed.), 
Computing Attitude and Affect in Text: Theory 
and Applications (pp. 265-279). Dordrecht: Sprin-
ger. 
Orasan, Constantin. 2003. PALinkA: A highly custo-
mizable tool for discourse annotation. Proceedings 
of 4th SIGdial Workshop on Discourse and Dialog 
(pp. 39 ? 43). Sapporo, Japan. 
Pang, Bo & Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. Proceedings of 
42nd Meeting of the Association for Computation-
al Linguistics (pp. 271-278). Barcelona, Spain. 
Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
Machine Learning techniques. Proceedings of Con-
ference on Empirical Methods in NLP (pp. 79-86). 
Polanyi, Livia & Annie Zaenen. 2006. Contextual 
valence shifters. In James G. Shanahan, Yan Qu & 
Janyce Wiebe (eds.), Computing Attitude and Af-
fect in Text: Theory and Applications (pp. 1-10). 
Dordrecht: Springer. 
Seki, Yohei, Koji Eguchi & Noriko Kando. 2006. 
Multi-document viewpoint summarization focused 
on facts, opinion and knowledge. In Janyce Wiebe 
(ed.), Computing Attitude and Affect in Text: 
Theory and Applications (pp. 317-336). Dordrecht: 
Springer. 
Stone, Philip J. 1997. Thematic text analysis: New 
agendas for analyzing text content. In Carl Roberts 
(ed.), Text Analysis for the Social Sciences. Mah-
wah, NJ: Lawrence Erlbaum. 
Taboada, Maite, Caroline Anthony & Kimberly Voll. 
2006. Creating semantic orientation dictionaries. 
Proceedings of 5th International Conference on 
Language Resources and Evaluation (LREC) (pp. 
427-432). Genoa, Italy. 
Taboada, Maite & Jack Grieve. 2004. Analyzing ap-
praisal automatically. Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text (AAAI Technical Report SS-04-07) (pp. 158-
161). Stanford University, CA. 
Turney, Peter. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews. Proceedings of 40th Meeting 
of the Association for Computational Linguistics 
(pp. 417-424). 
Voll, Kimberly & Maite Taboada. 2007. Not all 
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. Pro-
ceedings of the 20th Australian Joint Conference 
on Artificial Intelligence (pp. 337-346). Gold 
Coast, Australia. 
Whitelaw, Casey, Navendu Garg & Shlomo Arga-
mon. 2005. Using Appraisal groups for sentiment 
analysis. Proceedings of ACM SIGIR Conference 
on Information and Knowledge Management 
(CIKM 2005) (pp. 625-631). Bremen, Germany. 
Wiebe, Janyce & Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unan-
notated texts. Proceedings of Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics (CICLing-2005). Mex-
ico City, Mexico. 
Witten, Ian H. & Eibe Frank. 2005. Data Mining: 
Practical Machine Learning Tools and Techniques 
(2nd edn.). San Francisco: Morgan Kaufmann. 
 
69
Appendix A: Full lists of formal and functional zones 
 
 
Figure A1. Functional zones 
 
 
Figure A2. Formal zones 
 
Describe
Comment
Plot
Character
Specific
General
Content
Plot
Actors+characters
Specific
General
Overall
Plot
Actors+characters
Specific
General
Content
Structural
elements
Information
about the
film
Tagline
Structure
Off-topic
Title, Title+year, Runtime,
Country+year, Director,
Genre, Audience-restriction,
Cast, Credits, Show-Loc+date,
Misc-Movie-Info
Source, Author, Author-Bio,
Place, Date, Legal-Notice,
Misc-Review-Info, Rating
 
 
Appendix B: Kappa values for annotation task 
 
Classes 2-rater 
kappa 
3-rater 
kappa 
Describe/Comment/Describe+Comment/Formal .82 .73 
Describe/Comment/Formal .92 .84 
Describe/Comment/Describe+Comment .68 .54 
Describe/Comment .84 .69 
Table B1. Kappa values for stage annotations 
 
 
70
Coling 2010: Poster Volume, pages 90?98,
Beijing, August 2010
Automatic Acquisition of Lexical Formality
Julian Brooke, Tong Wang, and Graeme Hirst
Department of Computer Science
University of Toronto
{jbrooke,tong,gh}@cs.toronto.edu
Abstract
There has been relatively little work fo-
cused on determining the formality level
of individual lexical items. This study
applies information from large mixed-
genre corpora, demonstrating that signif-
icant improvement is possible over simple
word-length metrics, particularly when
multiple sources of information, i.e. word
length, word counts, and word associ-
ation, are integrated. Our best hybrid
system reaches 86% accuracy on an En-
glish near-synonym formality identifica-
tion task, and near perfect accuracy when
comparing words with extreme formality
differences. We also test our word as-
sociation method in Chinese, a language
where word length is not an appropriate
metric for formality.
1 Introduction
The derivation of lexical resources for use in
computational applications has been focused pri-
marily on the denotational relationships among
words, e.g. the synonym and hyponym relation-
ships encapsulated in WordNet (Fellbaum, 1998).
Largely missing from popular lexical resources
such as WordNet and the General Inquirer (Stone
et al, 1966) is stylistic information; there are,
for instance, no resources which provide com-
prehensive information about the formality level
of words, which relates to the appropriateness
of a word in a given context. Consider, for
example, the problem of choice among near-
synonyms: there are only minor denotational dif-
ferences among synonyms such as get, acquire,
obtain, and snag, but it is difficult to construct a
situation where any choice would be equally suit-
able. The key difference between these words is
their formality, with acquire the most formal and
snag the most informal.
In this work, we conceive of formality as
a continuous property. This approach is in-
spired by resources such as Choose The Right
Word (Hayakawa, 1994), in which differences be-
tween synonyms are generally described in rela-
tive rather than absolute terms, as well as linguis-
tic literature in which the quantification of stylis-
tic differences among genres is framed in terms of
dimensions rather than discrete properties (Biber,
1995). We begin by defining the formality score
for a word as a real number value in the range 1
to ?1, with 1 representing an extremely formal
word, and ?1 an extremely informal word. A
formality lexicon, then, gives a FS score to every
word within its coverage.
The core of our approach to the problem of
classifying lexical formality is the automated cre-
ation of formality lexicons from large corpora. In
this paper, we focus on the somewhat low-level
task of identifying the relative formality of word
pairs; we believe, however, that a better under-
standing of lexical formality is relevant to a num-
ber of problems in computational linguistics, in-
cluding sub-fields such as text generation, error
correction of (ESL) writing, machine translation,
text classification, text simplification, word-sense
disambiguation, and sentiment analysis. One con-
clusion of our research is that formality variation
is omnipresent in natural corpora, but it does not
follow that the identification of these differences
on the lexical level is a trivial one; nevertheless,
90
we are able to make significant progress using the
methods presented here, in particular the applica-
tion of latent semantic analysis to blog corpora.
2 Related Work
As far as we are aware, there are only a few
lines of research explicitly focused on the ques-
tion of linguistic formality. In linguistics proper,
the study of register and genre usually involves
a number of dimensions or clines, sometimes
explicitly identified as formality (Leckie-Tarry,
1995; Carter, 1998), or decomposed into notions
such as informational versus interpersonal con-
tent (Biber, 1995). Heyligen and Dewaele (1998)
provide a part-of-speech based quantification of
textual contextuality (which they argue is funda-
mental to the notion of formality); their metric
has been used, for instance, in a computational
investigation of the formality of online encyclo-
pedias (Emigh and Herring, 2005). In this kind
of quantification, however, there is little, if any,
focus on individual elements of the lexicon. In
computational linguistics, formality has received
attention in the context of text generation (Hovy,
1990); of particular note relevant to our research
is the work of Inkpen and Hirst (2006), who de-
rive boolean formality tags from Choose the Right
Word (Hayakawa, 1994). Like us, their focus was
improved word choice, though the approach was
much broader, also including dimensions such as
polarity. An intriguing example of formality rel-
evant to text classification is the use of infor-
mal language (slang) to help distinguish true news
from satire (Burfoot and Baldwin, 2009).
Our approach to this task is inspired and in-
formed by automatic lexical acquisition research
within the field of sentiment analysis (Turney
and Littman, 2003; Esuli and Sebastiani, 2006;
Taboada and Voll, 2006; Rao and Ravichandra,
2009). Turney and Littman (2003) apply latent
semantic analysis (LSA) (Landauer and Dumais,
1997) and pointwise mutual information (PMI) to
derive semantic orientation ratings for words us-
ing large corpora; like us, they found that LSA
was a powerful technique for deriving this lexical
information. The lexical database SentiWordNet
(Esuli and Sebastiani, 2006) provides 0?1 rank-
ings for positive, negative, and neutral polarity,
derived automatically using relationships between
words in WordNet (Fellbaum, 1998). Unfortu-
nately, WordNet synsets tend to cut across the for-
mal/informal distinction, and so the resource is
not obviously useful for our task.
The work presented here builds directly on a pi-
lot study (Brooke et al, 2010), the focus of which
was the construction of formality score (FS) lex-
icons. In that work, we employed less sophis-
ticated forms of some of the methods used here
in a relatively small dataset (the Brown Corpus),
providing a proof of concept, but with poor cov-
erage, and with no attempt to combine the meth-
ods to maximize performance. However, the small
dataset alowed us to do a thorough test of certain
options associated with our task. In particular we
found that using a similarity metric based on LSA
gave good performance across our test sets, es-
pecially when the term-document matrix was bi-
nary (unweighted), the k-value used for LSA was
small, and the method used to derive a formality
score was cosine similarity to our seed terms. A
metric using total word counts in corpora with di-
vergent formality also showed promise, with both
methods performing above our word-length base-
line for words within their coverage. PMI, by
comparison, proved less effective, and we do not
pursue it further here.
3 Data and Resources
3.1 Word Lists
All the word lists discussed here are publicly
available.1 We begin with two, one formal and
one informal, that we use both as seeds for our
lexicon construction methods and as test sets for
evaluation (our gold standard). We assume that
all slang terms are by their very nature informal
and so our 138 informal seeds were taken primar-
ily from an online slang dictionary2 (e.g. wuss,
grubby) and also include some contractions and
interjections (e.g. cuz, yikes). The 105 formal
seeds were selected from a list of discourse mark-
ers (e.g. moreover, hence) and adverbs from a sen-
timent lexicon (e.g. preposterously, inscrutably);
these sources were chosen to avoid words with
1 http://www.cs.toronto.edu/?jbrooke/FormalityLists.zip
2 http://onlineslangdictionary.com/
91
overt topic, and to ensure that there was some
balance of sentiment across formal and informal
seed sets. Part of speech, however, is not balanced
across our seed sets.
Another test set we use to evaluate our methods
is a collection of 399 pairs of near-synonyms from
Choose the Right Word (CTRW), a manual for as-
sisting writers with synonym word choice; each
pair was either explicitly or implicitly compared
for formality in the book. Implicit comparison in-
cluded statements such as this is the most formal
of these words; in those cases, and more gener-
ally, we avoided words appearing in more than
one comparison (there are no duplicate words in
our CTRW set), as well as multiword expressions
and words whose formality is strongly ambigu-
ous (i.e. word-sense dependent). An example of
this last phenomenon is the word cool, which is
used colloquially in the sense of good but more
formally as in the sense of cold. Partly as a re-
sult of this polysemy, which is clearly more com-
mon among informal words, our pairs are biased
toward the formal end of the spectrum; although
there are some informal comparisons, e.g. belly-
ache/whine, wisecrack/joke, more typical pairs
include determine/ascertain and hefty/ponderous.
Despite this imbalance, one obvious advantage
of using near-synonyms in our evaluation is that
factors other than linguistic formality (e.g. topic,
opinion) are less likely to influence performance.
In general, the CTRW allows for a more objective,
fine-grained evaluation of our methods, and is ori-
ented towards our primary interest, near-synonym
word choice.
To test the performance of our unsupervised
method beyond English, one of the authors (a na-
tive speaker of Mandarin Chinese) created two
sets of Chinese two-character words, one formal,
one informal, based on but not limited to the
words in the English sets. The Chinese seeds in-
clude 49 formal seeds and 43 informal seeds.
3.2 Corpora
Our corpora fall generally into three categories:
formal (written) copora, informal (spoken) cor-
pora, and mixed corpora. The Brown Corpus
(Francis and Kuc?era, 1982), our development cor-
pus, is used here both as a formal and mixed cor-
pus. Although extremely small by modern cor-
pus standards (only 1 million words), the Brown
Corpus has the advantage of being compiled ex-
plicitly to represent a range of American English,
though it is all of the published, written variety.
The Switchboard (SW) Corpus is a collection of
American telephone conversations (Godfrey et al,
1992), which contains roughly 2400 conversations
with over 2.6 million word tokens; we use it as an
informal counterpart to the Brown Corpus. Like
the Brown Corpus, The British National Corpus
(Burnard, 2000) is a manually-constructed mixed-
genre corpus; it is, however, much larger (roughly
100 million words). It contains a written portion
(90%), which we use as a formal corpus, and a
spontaneous spoken portion (4.3%), which we use
as an informal corpus. Our other mixed corpora
are two blog collections available to us: the first,
which we call our development blog corpus (Dev-
Blog) contains a total of over 900,000 English
blogs, with 216 million tokens.3 The second is the
?first tier? English blogs included in the publicly
available ICSWM 2009 Spinn3r Dataset (Burton
et al, 2009), a total of about 1.3 billion word to-
kens in 7.5 million documents. For our investiga-
tions in Chinese, we use the Chinese portion of the
ICSWM blogs, approximately 25.4 million char-
acter tokens in 86,000 documents.
4 Methods
4.1 Simple Formality Measures
The simplest kind of formality measure is based
on word length, which is often used directly as
an indicator of formality for applications such as
genre classification (Karlgren and Cutting, 1994).
Here, we use logarithmic scaling to derive a FS
score based on word length. Given a maximum
word length L4 and a word w of length l, the for-
mality score function, FS(w), is given by:
FS(w) =?1+2 log llogL
3These blogs were gathered by the University of Toronto
Blogscope project (www.blogscope.net) over a week in May
2008.
4We use an upper bound of 28 characters, which is
the length of antidisestablishmentarianism, the prototypical
longest word in English; this value of L provides an appropri-
ate formality/informality threshold, between 5- and 6-letter
words
92
For hyphenated terms, the length of each compo-
nent is averaged. Though this metric works rela-
tively well for English, we note that it is problem-
atic in a language with significant word aggluti-
nation (e.g. German) or without an alphabet (e.g.
Chinese, see below).
Another straightforward method is the assump-
tion that Latinate prefixes and suffixes are indica-
tors of formality in English (Kessler et al, 1997),
i.e. informal words will not have Latinate affixes
such as -ation and intra-. Here, we simply assign
words that appear to have such a prefix or suffix
an FS of 1, and all other words an FS of ?1.
Our frequency methods derive FS from word
counts in corpora. Our first, naive approach as-
sumes a single corpus, where either formal words
are common and informal words are rare, or vice
versa. To smooth out the Zipfian distribution, we
use the frequency rank of words as exponentials;
for a corpus with R frequency ranks, the FS for a
word of rank r under the formal is rare assumption
is given by:
FS(w) =?1+2 e
(r?1)
e(R?1)
Under the informal is rare assumption:
FS(w) = 1?2 e
(r?1)
e(R?1)
We have previously shown that these methods are
not particularly effective on their own (Brooke et
al., 2010), but we note that they provide useful
information for a hybrid system.
A more sophisticated method is to use two cor-
pora that are known to vary with respect to for-
mality and use the relative appearance of words in
each corpus as the metric. If word appears n times
in a (relatively) formal corpus and m times in an
informal corpus (and one of m, n is not zero), we
derive:
FS(w) =?1+2 nm?N +n
Here, N is the ratio of the size (in tokens) of the
informal corpus (IC) to the formal corpus (FC).
We need the constant N so that an imbalance in
the size of the corpora does not result in an equiv-
alently skewed distribution of FS.
4.2 Latent Semantic Analysis
Next, we turn to LSA, a technique for extracting
information from a large corpus of texts by (dras-
tically) reducing the dimensionality of a term?
document matrix, i.e. a matrix where the row vec-
tors correspond to the appearance or (weighted)
frequency of words in a set of texts. In essence,
LSA simplifies the variation of words across a col-
lection of texts, exploiting document?document
correlation to produce information about the k
most important dimensions of variation (k < to-
tal number of documents), which are generally
thought to represent semantic concepts, i.e. topic.
The mathematical basis for this transformation is
singular value decomposition5; for the details of
the matrix transformations, we refer the reader to
the discussion of Turney and Littman (2003). The
factor k, the number of columns in the compacted
matrix, is an important variable in any application
of LSA, one is generally determined by trial and
error (Turney and Littman, 2003).
LSA is computationally intensive; in order to
apply it to extremely large blog corpora, we need
to filter the documents and terms before build-
ing our term?document matrix. We adopt the
following strategy: to limit the number of docu-
ments in our term?document matrix, we first re-
move documents less than 100 tokens in length,
with the rationale that these documents provide
less co-occurrence information. Second, we re-
move documents that either do not contain any
target words (i.e. one of our seeds or CTRW test
words), or contain only target words which are
among the most common 20 in the corpus; these
documents are less likely to provide us with use-
ful information, and the very common target terms
will be well represented regardless. We further
shrink the set of terms by removing all hapax
legomena; a single appearance in a corpus is not
enough to provide reliable co-occurrence informa-
tion, and roughly half the words in our blog cor-
pora appear only once. Finally, we remove sym-
bols and all words which are not entirely lower
5We use the implementation included in Matlab; we take
the rows of the decomposed U matrix weighted by the sin-
gular values in ? for our word vectors. Using no weights
or ??1 generally resulted in worse performance, particularly
with the CTRW sets.
93
case; we are not interested, for instance, in num-
bers, acronyms, and proper nouns. We can esti-
mate the effect this filtering has on performance
by testing it both ways in a development corpus.
Once a k-dimensional vector for each relevant
word is derived using LSA, a standard method is
to use the cosine of the angle between a word vec-
tor and the vectors of seed words to identify how
similar the distribution of the word is to the distri-
bution of the seeds. To begin, each formal seed is
assigned a FS value of 1, each informal seed a FS
value of ?1, and then a raw seed similarity score
(FS?) is calculated for each word w:
FS?(w) = ?
s?S,s6=w
Ws?FS(s)? cos(?(w,s))
S is the set of all seeds. Note that seed terms are
excluded from their own FS calculation, this is
equivalent to leave-one-out cross-validation. Ws
is a weight that depends on whether s is a formal
or informal seed, Wi (for informal seeds) is calcu-
lated as:
Wi = ? f?F FS( f )|?i?I FS(i)|+? f?F FS( f )
and Wf (for formal seeds) is:
Wf = |?i?I FS(i)||?i?I FS(i)|+? f?F FS( f )
Here, I is the set of all informal seeds, and F is the
set of all formal seeds. These weights have the ef-
fect of countering any imbalance in the seed set,
as formal and informal seeds ultimately have the
same (potential) influence on each word, regard-
less of their count. This weighting is necessary for
the iterative extension of this method discussed in
the next section.
We calculate the final FS score as follows:
FS(w) = FS
?(w)?FS?(r)
Nw
The word r is a reference term, a common func-
tion word that has no formality.6 This has the ef-
fect of countering any (moderate) bias that might
6The particular choice of this word is relatively unimpor-
tant; common function words all have essentially the same
LSA vectors because they appear at least once in nearly ev-
ery document of any size. For English, we chose r = and,
and for Chinese, r = yinwei (because); there does not seem
to be an obvious two-character, formality-neutral equivalent
to and in Chinese.
exist in the corpus; in the Brown Corpus, for in-
stance, function words have positive formality be-
fore this step, simply because formal words oc-
curred more often in the corpus. Nw is a normal-
ization factor, either
Nw = maxwi?I? |FS
?(wi)?FS?(r)|
for all wi ? I? or
Nw = maxw f?F ? |FS
?(w f )?FS?(r)|
for all w f ? F ?. I? contains all words w such that
FS?(w)?FS?(r) < 0, and F ? contains all words w
such that FS?(w)?FS?(r) > 0. This ensures that
the resulting lexicon has terms exactly in the range
1 to?1, with the reference word r at the midpoint.
We also tested the LSA method in Chinese.
The only major relevant difference between Chi-
nese and English is word segmentation: Chinese
does not have spaces between words. To sidestep
this problem, we simply included all character bi-
grams found in our corpus. The drawback of this
approach in the inclusion of a huge number of
nonsense ?words? (1.3 million terms in just 86,000
documents), however we are at least certain to
identify all instances of our seeds.
4.3 Hybrid Methods
There are a number of ways to leverage the infor-
mation we derive from our basic methods. One
intriguing option is to use the basic FS measures
as the starting point for an iterative process using
the LSA cosine similarity. Under this paradigm,
all words in the starting FS lexicon are potential
seed words; we choose a cutoff value for inclu-
sion in the seed word set (e.g. words which have
at least .5 or ?.5 FS), and then carry out the co-
sine calculations, as above, to derive new FS val-
ues (a new FS lexicon). We can repeat this process
as many times as required, with the idea that the
connections between various words (as reflected
in their LSA-derived vectors) will cause the sys-
tem to converge towards the true FS values.
A simple hybrid method that combines the two
word count models uses the ratio of word counts
in two corpora to define the center of the FS spec-
trum, but single corpus methods to define the ex-
tremes. Formally, if m and n (word counts for the
94
informal corpus IC and formal corpus FC, respec-
tively) are both non-zero, then FS is given by:
FS(w) =?0.5+ nm?N +n
However, if n is zero, FS is given by:
FS(w) =?1+0.5 e
?rIC?1
e?RIC?1
where rIC is the frequency rank of the word in IC,
and RIC is the total number of ranks in IC. If m is
zero, FS is given by:
FS(w) = 1?0.5 e
?rFC?1
e?RFC?1
where i is the rank of the word in IC, and RIC is the
total number of frequency ranks in IC). This func-
tion is undefined in the case where m and n are
both zero. Intuitively, this is a kind of backoff, re-
lying on the idea that words of extreme formality
are rare even in a corpus of corresponding formal-
ity, whereas words in the core vocabulary (Carter,
1998), which are only moderately formal, will ap-
pear in all kinds of corpora, and thus are amenable
to the ratio method.
Finally, we explore a number of ways to com-
bine lexicons directly. The motivation for this
is that the lexicons have different strengths and
weaknesses, representing partially independent
information. An obvious method is an averag-
ing or other linear combination of the scores, but
we also investigate vote-based methods (requiring
agreement among n dictionaries). Beyond these
simple options, we test support vector machines
and naive Bayes classification using the WEKA
software suite (Witten and Frank, 2005), applying
10-fold cross-validation using default WEKA set-
tings for each classifier. The features here are task
dependent (see Section 5); for the pairwise task,
we use the difference between the FS value of the
words in each lexicon, rather than their individ-
ual scores. Finally, we can use the weights from
the SVM model of the CTRW (pairwise) task to
interpolate an optimal formality lexicon.
5 Evaluation
We evaluate our methods using the gold standard
judgments from the seed sets and CTRW word
pairs. To differentiate the two, we continue to use
the term seed for the former; in this context, how-
ever, these ?seed sets? are being viewed as a test
set (recall that our LSA method is equivalent to
leave-one-out cross-validation).
We derive the following measures: first, the
coverage (Cov.) is the percentage of words in the
set that are covered under the method. The class-
based accuracy (C-Acc.) of our seed sets is the
percentage of covered words which are correctly
classified as formal (FS > 0) or informal (FS <
0). The pair-based accuracy (P-Acc.) is the result
of exhaustively pairing words in the two seed sets
and testing their relative formality; that is, for all
wi ? I and w f ? F , the percentage of wi/w f pairs
where FS(wi) < FS(w f ). For the CTRW pairs
there are only two metrics, the coverage and the
pair-based accuracy; since the CTRW pairs repre-
sent relative formality of varying degrees, it is not
possible to calculate a class-based accuracy.
The first section of Table 1 provides the re-
sults for the basic methods in various corpora.
The word length (1) and morphology-based (2)
methods provide good coverage, but poor accu-
racy, while the word count ratio methods (3?4) are
fairly accurate, but suffer from low coverage. The
LSA results in Table 1 are the best for each corpus
across the k values we tested. When both cover-
age and accuracy are considered, there is a clear
benefit associated with increasing the amount of
data, though the difference between the Dev-Blog
and ICWSM suggests diminishing returns. The
performance of the filtered Dev-Blog is actually
slightly better than the unfiltered versions (though
there is a drop in coverage), suggesting that filter-
ing is a good strategy.
In our previous work (Brooke et al, 2010), we
noted that CTRW set performance in the Brown
dropped for k > 3, while performance on the seed
set was mostly steady as k increased. Figure 1
shows the pairwise performance of each test set
for various corpora across various k. The results
here are similar; all three corpora reach a CTRW
maximum at a relatively low k values (though
higher than Brown Corpus); however the seed set
performance in each corpus continues to improve
(though marginally) as k increases, while CTRW
performance drops. An explanation for this is that
95
Table 1: Seed coverage, class-based accuracy, pairwise accuracy, CTRW coverage, and pairwise accu-
racy for various FS lexicons and hybrid methods (%).
Seed set CTRW set
Method Cov. C-Acc. P-Acc. Cov. P-Acc.
Simple
(1) Word length 100 86.4 91.8 100 63.7
(2) Latinate affix 100 74.5 46.3 100 32.6
(3) Word count ratio, Brown and Switchboard 38.0 81.5 85.7 36.0 78.2
(4) Word count ratio, BNC Written vs. Spoken 60.9 89.2 97.3 38.8 74.3
(5) LSA (k=3), Brown 51.0 87.1 94.2 59.6 73.9
(6) LSA (k=10), BNC 94.7 83.0 98.3 96.5 69.4
(7) LSA (k=20), Dev-Blog 100 91.4 96.8 99.0 80.5
(8) LSA (k=20), Dev-Blog, filtered 99.0 92.1 97.0 97.7 80.5
(9) LSA (k=20), ICWSM, filtered 100 93.0 98.4 99.7 81.9
Hybrid
(10) BNC ratio with backoff (4) 97.1 78.8 75.7 97.0 78.8
(11) Combined ratio with backoff (3 + 4) 97.1 79.2 79.9 97.5 79.9
(12) BNC weighted average (10,6), ratio 2:1 97.1 83.5 90.0 97.0 83.2
(13) Blog weighted average (9,7), ratio 4:1 100 93.8 98.5 99.7 83.4
(14) Voting, 3 agree (1, 6, 7, 9, 11) 92.6 99.1 99.9 87.0 91.6
(15) Voting, 2 agree (1, 11, 13) 86.8 99.1 100 81.5 96.9
(16) Voting, 2 agree (1, 12, 13) 87.7 98.6 100 82.7 97.3
(17) SVM classifier (1, 2, 6, 7, 9, 11) 100 97.9 99.9 100 84.2
(18) Naive Bayes classifier (1, 2, 6, 7, 9, 11) 100 97.5 99.8 100 83.9
(19) SVM (Seed, class) weighted (1, 2, 6, 7, 9, 11) 100 98.4 99.8 100 80.5
(20) SVM (CTRW) weighted (1, 6, 7, 9, 11) 100 93.0 99.0 100 86.0
(21) Average (1, 6, 7, 9, 11) 100 95.9 99.5 100 84.5
Figure 1: Seed and CTRW pairwise accuracy,
LSA method for large corpora k, 10? k ? 200.
the seed terms represent extreme examples of for-
mality; thus there are numerous semantic dimen-
sions to distinguish them. However, the CTRW
set includes near-synonyms, many with only rel-
atively subtle differences in formality; for these
pairs, it is important to focus on the core di-
mensions relevant to formality, which are among
the first discovered in a factor analysis of mixed-
register texts (Biber, 1995).
With regards to hybrid methods, we first briefly
summarize our testing with the iterative model,
which included extensive experiments using ba-
sic lexicons and the LSA vectors derived from
the Brown Corpus, and some targeted testing with
the blog corpora (iteration on these corpora is
extraordinarily time-consuming). In general, we
found only that there were only small, inconsis-
tent benefits to be gained from the iterative ap-
96
proach. More generally, the intuition behind the
iterative method, i.e. that performance would in-
crease with an drastic increase in the number of
seeds, was found to be flawed: in other testing,
we found that we could randomly remove most
of the seeds without negatively affecting perfor-
mance. Even at relatively high k values, it seems
that a few seeds are enough to calibrate the model.
The ratio (with backoff) hybrid built from the
BNC (10) provides CTRW performance that is
comparable the best LSA models, though perfor-
mance in the seed sets is somewhat poor; supple-
menting with word counts from the Brown Cor-
pus and Switchboard Corpus provides a small im-
provement (11). The weighed hybrid dictionar-
ies in (12,13) demonstrate that it is possible to ef-
fectively combine lexicons built using two differ-
ent methods on the same corpus (12) or the same
method on different corpora (13); the former, in
particular, provides an impressive boost to CTRW
accuracy, indicating that word count and word as-
sociation methods are partially independent.
The remainder of Table 1 shows the best re-
sults using voting, averaging, and weighting. The
voting results (14?16) indicate that it is possible
to sacrifice some coverage for very high accu-
racy in both sets, including a near-perfect score
in the seed sets and significant gains in CTRW
performance. In general, the best accuracy with-
out a significant loss of coverage came from 2
of 3 voting (15?16), using dictionaries that rep-
resented our three basic sources of information
(word length, word count, and word associa-
tion). The machine learning hybrids (17?18) also
demonstrate a marked improvement over any sin-
gle lexicon, though it is important to note that
each accuracy score here reflects a different task-
specific model. Hybrid FS lexicons built with the
weights learned by the SVM models (19?20) pro-
vide superior performance on the task correspond-
ing to the model used, though the simple averag-
ing of the best dictionaries (21) also provides good
performance across all evaluation metrics.
Finally, the LSA results for Chinese are mod-
est but promising, given the relatively small scale
of our experiments: we saw a pairwise accuracy of
82.2%, with 79.3% class-based accuracy (k = 10).
We believe that the main reason for the generally
lower performance in Chinese (as compared to
English) is the modest size of the corpus, though
our simplistic character bigram term extraction
technique may also play a role. As mentioned,
smaller seed sets do not seem to be an issue. Inter-
estingly, the class-based accuracy is 10.8% lower
if no reference word is used to calibrate the divide
between formal and informal, suggesting a rather
biased corpus (towards informality); in English,
by comparison, the reference-word normalization
had a slightly negative effect on the LSA results,
though the effect mostly disappeared after hy-
bridization. The obvious next step is to integrate a
Chinese word segmenter, and use a larger corpus.
We could also try word count methods, though
finding appropriate (balanced) resouces similar to
the BNC might be a challenge; (mixed) blog cor-
pora, on the other hand, are easily collected.
6 Conclusion
In this work, we have experimented with a number
of different methods and source corpora for deter-
mining the formality level of lexical items, with
the implicit goal of distinguishing the formality of
near-synonym pairs. Our methods show marked
improvement over simple word-length metrics;
when multiple sources of information, i.e. word
length, word counts, and word association, are in-
tegrated, we are able to reach over 85% perfor-
mance on the near-synonym task, and close to
100% accuracy when comparing words with ex-
treme formality differences; our voting methods
show that even higher precision is possible. We
have also demonstrated that our LSA word associ-
ation method can be applied to a language where
word length is not an appropriate metric of for-
mality, though the results here are preliminary.
Other potential future work includes addressing a
wider range of phenomena, for instance assign-
ing formality scores to morphological elements,
syntactic cues, and multi-word expressions, and
demonstrating that a formality lexicon can be use-
fully applied to other NLP tasks.
Acknowledgements
This work was supported by Natural Sciences and
Engineering Research Council of Canada. Thanks
to Paul Cook for his ICWSM corpus API.
97
References
Biber, Douglas. 1995. Dimensions of Register Vari-
ation: A cross-linguistic comparison. Cambridge
University Press.
Brooke, Julian, Tong Wang, and Graeme Hirst. 2010.
Inducing lexicons of formality from corpora. In
Proceedings of the Language Resources and Eval-
uation Conference (LREC ?10), Workshop on Meth-
ods for the automatic acquisition of Language Re-
sources and their evaluation methods.
Burfoot, Clint and Timothy Baldwin. 2009. Auto-
matic satire detection: Are you having a laugh? In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computationsl
Linguistics and the 4th International Joint Confer-
ence on Nautral Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP ?09), Short Papers, Singapore.
Burnard, Lou. 2000. User reference guide for British
National Corpus. Technical report, Oxford Univer-
sity.
Burton, Kevin, Akshay Java, and Ian Soboroff. 2009.
The ICWSM 2009 Spinn3r Dataset. In Proceedings
of the Third Annual Conference on Weblogs and So-
cial Media (ICWSM 2009), San Jose, CA.
Carter, Ronald. 1998. Vocabulary: applied linguistic
perspectives. Routledge, London.
Emigh, William and Susan C. Herring. 2005. Col-
laborative authoring on the web: A genre analysis
of online encyclopedias. In Proceedings of the 38th
Annual Hawaii International Conference on System
Sciences (HICSS ?05).
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Interna-
tion Conference on Language Resources and Eval-
uation(LREC), Genova, Italy.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press.
Francis, Nelson and Henry Kuc?era. 1982. Frequency
Analysis of English Usage: Lexicon and Grammar.
Houghton Mifflin, Boston.
Godfrey, J.J., E.C. Holliman, and J. McDaniel. 1992.
Switchboard: telephone speech corpus for research
and development. IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
1:517?520.
Hayakawa, S.I., editor. 1994. Choose the Right Word.
HarperCollins Publishers, second edition. Revised
by Eugene Ehrlich.
Heylighen, Francis and Jean-Marc Dewaele. 2002.
Variation in the contextuality of language: An em-
pirical measure. Foundations of Science, 7(3):293?
340.
Hovy, Eduard H. 1990. Pragmatics and natural lan-
guage generation. Artificial Intelligence, 43:153?
197.
Inkpen, Diana and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym
differences. Computational Linguistics, 32(2):223?
262.
Karlgren, Jussi and Douglas Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proceedings of the 15th Con-
ference on Computational Linguistics, pages 1071?
1075.
Kessler, Brett, Geoffrey Nunberg, and Hinrich
Schu?tze. 1997. Automatic detection of text genre.
In Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics, pages
32?38.
Landauer, Thomas K. and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic anal-
ysis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104:211?240.
Leckie-Tarry, Helen. 1995. Language Context: a
functional linguistic theory of register. Pinter.
Rao, Delip and Deepak Ravichandra. 2009. Semi-
supervised polarity lexicon induction. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
gusitics, Athens, Greece.
Stone, Philip J., Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilivie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Taboada, Maite and Kimberly Voll. 2006. Methods
for creating semantic orientation dictionaries. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), Gen-
ova, Italy.
Turney, Peter and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco.
98
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 753?761, Dublin, Ireland, August 23-29 2014.
Unsupervised Multiword Segmentation of Large Corpora using
Prediction-Driven Decomposition of n-grams
Julian Brooke
*?
Vivian Tsang
?
Graeme Hirst
*
Fraser Shein
*?
*
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
gh@cs.toronto.edu
?
Quillsoft Ltd.
Toronto, Canada
vtsang@quillsoft.ca
fshein@quillsoft.ca
Abstract
We present a new, efficient unsupervised approach to the segmentation of corpora into multiword
units. Our method involves initial decomposition of common n-grams into segments which max-
imize within-segment predictability of words, and then further refinement of these segments into
a multiword lexicon. Evaluating in four large, distinct corpora, we show that this method cre-
ates segments which correspond well to known multiword expressions; our model is particularly
strong with regards to longer (3+ word) multiword units, which are often ignored or minimized
in relevant work.
1 Introduction
Identification of multiword units in language is an active but increasingly fragmented area of research, a
problem which can limit the ability of others to make use of units beyond the level of the word as input
to other applications. General research on word association metrics (Church and Hanks, 1990; Smadja,
1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010), though increasingly comprehensive in
its scope, has mostly failed to identify a single best choice, leading some to argue that the variety of
multiword phenomena must be tackled individually. For instance, there is a body of research focusing
specifically on collocations that are (to some degree) non-compositional, i.e. multiword expressions (Sag
et al., 2002; Baldwin and Kim, 2010), with individual projects often limited to a particular set of syntactic
patterns, e.g. verb-noun combinations (Fazly et al., 2009). A major issue with approaches involving
statistical association is that they rarely address expressions larger than 2 words (Heid, 2007); in corpus
linguistics, larger sequences referred to as lexical bundles are extracted using an n-gram frequency cutoff
(Biber et al., 2004), but the frequency threshold is typically set very high so that only a very limited set
is extracted. Another drawback, common to almost all these methods, is that they rarely offer an explicit
segmentation of a text into multiword units, which would be preferable for downstream uses such as
probabilistic distributional semantics. An exception is the Bayesian approach of Newman et al. (2012),
but their method does not scale well (see Section 2). Our own long-term motivation is to identify a wide
variety of multiword units for assisting language learning, since correct use of collocations is known to
pose a particular challenge to learners (Chen and Baker, 2010).
Here, we present a multiword unit segmenter
1
with the following key features:
? It is entirely unsupervised.
? It offers both segmentation of the input corpus and a lexicon which can be used to segment new
corpora.
? It is scalable to very large corpora, and works for a variety of corpora.
? It is language independent.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The software is available at http:/www.cs.toronto.edu/
?
jbrooke/ngram decomp seg.py .
753
? It does not inherently limit possible units with respect to part-of-speech or length.
? It has a bare minimum of parameters, and can be used off-the-shelf: in particular, it does not require
the choice of an arbitrary cutoff for some uninterpretable statistical metric.
? It does, however, include a parameter fixing the minimum number of times that a valid multiword
unit will appear in the corpus, which ensures sufficient usage examples for relevant applications.
Our method involves three major steps: extraction of common n-grams, initial segmentation of the
corpus, and a refinement of the resulting lexicon (and, by extension, the initial segmentation). The
latter two steps are carried out using a simple but novel heuristic based on maximizing word prediction
within multiword segments. Importantly, our method requires just a few iterations through the corpus,
and in practice these iterations can be parallelized. Evaluating with an existing set of multiword units
from WordNet in four large corpora from distinct genres, we show that our initial segmentation offers
extremely good subsumption of known collocations, and after lexicon refinement the model offers a
good trade-off between subsumption and exact matches. We also evaluate a sample of our multiword
vocabulary using crowdsourcing, and offer a qualitative analysis.
2 Related Work
In computational linguistics, there is a large body of research that proposes and/or evaluates lexical as-
sociation measures for the creation of multiword lexicons (Church and Hanks, 1990; Smadja, 1993;
Schone and Jurafsky, 2001; Evert, 2004): there are many more measures than can be addressed here?
work by Pecina (2010) considered 82 variations?but popular choices include the t-test, log likelihood,
and pointwise mutual information (PMI). In order to build lexicons using these methods, particular syn-
tactic patterns and thresholds for the metrics are typically chosen. Many of the statistical metrics do not
generalize at all beyond two words, but PMI (Church and Hanks, 1990), the log ratio of the joint proba-
bility to the product of the marginal probabilities, is a prominent exception. Other measures specifically
designed to address collocations of larger than two words include the c-value (Frantzi et al., 2000), a
metric designed for term extraction which weights term frequency by the log length of the n-gram while
penalizing n-grams that appear in frequent larger ones, and mutual expectation (Dias et al., 1999), which
produces a normalized statistic that reflects how much a candidate phrase resists the omission of any
particular word. Another approach is to simply to combine known n? 1 collocations to form n-length
collocations (Seretan, 2011), but this is based on the assumption that all longer collocations are built up
from shorter ones?idioms, for instance, do not usually work in that way.
An approach used in corpus linguistics which does handle naturally longer sequences is the study
of lexical bundles (Biber et al., 2004), which are simply n-grams that occur above a certain frequency
threshold. This includes larger phrasal chunks that would be missed by traditional collocation extraction,
and so research in this area has tended to focus on how particular phrases (e.g. if you look at) are indi-
ciative of particular genres (e.g. university lectures). In order to get very reliable phrases, the threshold
is typically set high enough (Biber et al. use 40 occurrences in 1 million words) to filter out the vast
majority of expressions in the process.
With respect to the features of our model, the work closest to ours is probably that of Newman et al.
(2012). Like us, they offer an unsupervised solution, in their case a generative Dirichlet Process model
which jointly creates a segmentation of the corpus and a multiword term vocabulary. Their method,
however, requires full Gibbs sampling with thousands of iterations through the corpus (Newman et al.
report using 5000), an approach which is simply not tractable for the large corpora that we address in
this paper (which are roughly 1000 times larger than theirs). Though the model is general, their focus is
limited to term extraction, and for larger terms they compare only with the c-value approach of Frantzi
et al. (2000). Other closely related work includes general tools available for creating multiword lexicons
using association measures or otherwise exploring the collocational behavior of words (Kilgarriff and
Tugwell, 2001; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Pedersen et al., 2011). Other related
but distinct tasks include syntactic chunking (Abney, 1991) and word segmentation for Asian languages,
in particular Chinese (Emerson, 2005).
754
3 Method
3.1 Prediction-based segmentation
Our full method consists of multiple independent steps, but it is based on one central and relatively simple
idea that we will introduce first. Given a sequence of words, w
1
. . .w
n
, and statistics (i.e. n-gram counts)
about the use of these words in a corpus, we first define p(w
i
|w
j,k
) as the conditional probability of some
word w
i
appearing with some contextual subsequence w
j
. . .w
i?1
,w
i+1
. . .w
k
,1 ? j ? i ? k ? n. In the
case i = j = k, this is simply the marginal probability, p(w
i
). We then define the word predictability
of some w
i
in the context w
1,n
as the log of the maximal conditional probability of the word across all
possible choices of j and k:
pred(w
i
,w
1,n
) = max
j,k
log p(w
i
|w
j,k
)
We can define predictability for the entire sequence then as:
pred(w
1,n
) =
n
?
i=1
pred(w
i
,w
1,n
)
Now we consider the case where we have a set of possible segmentations S of the sequence, where
each segmentation s ? S can be viewed as a (possibly empty) set of segment boundaries ?s
0
,s
1
, . . . ,s
m
?.
Among the available options, our optimal segmentation is:
argmax
s?S
m?1
?
i=0
pred(w
s
i
,s
i+1
?1
)
That is, we will prefer the segmentation which maximizes the overall predictability of each word in the
sequence, under the restriction that we only predict words using the context within their segments. This
reflects our basic assumption that words within a good segment, i.e. a multiword unit, are (much) more
predictive of each other than words outside a unit. Note that if our probabilities are calculated from the
full set of n-gram counts for the corpus being segmented and the set of possible segmentations S is not
constrained, a segmentation with a smaller number of breaks will generally be preferred over one with
more breaks. However, in practice we will be greatly constraining S and also using probabilities based
on only a subset of all the information in the corpus.
3.2 Extraction of n-grams
In order to carry out a segmentation of the corpus using this method, we first need to extract statistics
in the form of n-gram counts. Given a minimum occurrence threshold, this can be done efficiently even
for large corpora in an iterative fashion until all n-grams have been extracted. For all our experiments
here, we limit ourselves to n-grams that appear at least once in 10 million tokens, and we did not collect
n-grams for n > 10 (which are almost always the result of duplication of texts in the corpus). For the pur-
poses of calculating conditional probabilities given surrounding context in our predictive segmentation,
we collected both standard n-grams as well as (for n? 3) skip n-grams with a missing word (e.g. basic *
processes where the asterisk indicates that any word could appear in that slot). Here we use lower-cased
unlemmatized tokens, excluding punctuation, though for languages with more inflectional morphology
than English, lemmatization would be advised.
3.3 Initial segmentation
Given these n-gram statistics, our initial segmentation proceeds as follows: For each sentence in the
corpus, we identify all maximum length n-grams in the sentence, i.e. all those n-grams for n ? 2 where
there is no larger n-gram which contains them while still being above our threshold of occurrence. These
n-grams represent the upper bound of our segmentation: we will never break into segments larger than
these. However, there are many overlaps among these n-grams (in fact, with a low threshold the vast
majority of n-grams overlap with at least one other), and for proper segmentation we need to resolve
755
Figure 1: Three-step procedure for n-gram decomposition into multiword units. a) shows the maximal
n-grams identified in the sentence, b) is the segmentation after the initial pass of the corpora, and c)
shows further decomposition of segments after a pass through the lexicon resulting from b).
all overlaps between these maximal n-grams by inserting at least one break. For this we apply our
prediction-based decomposition technique. In our discussion in Section 3.1, we did not consider how
the possible segmentations were selected, but now we can be explicit: the set S consists of all possible
segmentations which minimally resolve all n-gram overlaps. By minimally resolve, we mean that the
removal of any breakpoint from our set would result in an unresolved overlap: in short, there are no
extraneous breaks, and therefore no cases where a possible set of breaks is a subset of another possible
set. Figure 1a shows a real example: if we just consider the last three maximal n-grams, there are two
possible minimal breaks: a single break between in and basic or two breaks, one between roles and in
and one between basic and cellular.
Rather than optimizing over all possible breaks over the whole sentence, which is computationally
problematic, we simplify the algorithm somewhat by moving sequentially through each n-gram over-
lap in the sentence, taking any previous breaks as given while considering only the minimum breaks
necessary to resolve any overlaps that directly influence the segmentation of the two overlapping spans
under consideration, which is to say any other overlapping spans which contain at least one word also
contained in at least one of overlapping spans under consideration. For example, in Figure 1a we first
deal independently with each of the first two overlaps (the spans modified with glucose and are enriched
in lipid rafts, and then we consider the final two overlaps together: The result is shown in Figure 1b. In
development, we tested including more context (i.e. considering second-order influence) and found no
benefit. Since we do not consider breaks other than those required to resolve overlapping n-grams, these
segments tend to be long. This is by design; our intention is that these segments will subsume as many
multiword units as possible, and therefore will be amenable to refinement by further decomposition in
the next step.
3.4 Lexicon decomposition
Based on the initial segmentation of the entire corpus, we extract a tentative lexicon, with corresponding
counts. Then, in order from longest to shortest, we consider decomposition of each entry. First, using
our prediction-based decomposition method, we find the best decomposition of the entry into two parts;
note that we only need to consider one break per lexicon entry, since breaks in the (smaller) parts will
be considered independently later in the process. If the count in our lexicon is below the occurrence
threshold, we always carry out this split, which means we remove the entry from the lexicon and (after
all n-grams of that length have been processed, so as to avoid ordering effects) add its counts to the
counts of n-grams of its best decomposition. If the count is above the threshold, we preserve the full
entry (for entries of length 3 or greater) only if the following inequality is true for each subsegment w
j,k
in the full entry w
1,n
:
k
?
i= j
pred(w
i
,w
1,n
)? pred(w
j,k
) > log p(w
j,k
)? log p(w
1,n
)
756
That is, the ratio (expressed as a difference of logarithms) between the count of the segment and the full
unsegmented entry (in our preliminary lexicon) is lower than the ratio of the predictability (as defined
in our discussion of prediction-based decomposition) of the words in the segment with the context of
the full entry to the predictability of words with only the context included within the segment (which
is just pred(w
j,k
)). In other words, we preserve only longer multiword sequences in our lexicon when
any decrease in the probability of the full entry relative to its smaller components
2
is fully offset by an
increase in the conditional probability of the individual words of that segment when the larger context
from the full segment is available. For example, after we have decided on a potential break in the phrase
basic | cellular process from our example in Figure 1, we compare the (marginal) probability ?lost? by
including basic in a larger phrase, i.e. the ratio of counts of basic to basic cellular process in our lexicon),
to the (conditional) probability ?gained? by how much more predictable the segment is in this context;
when the segment in question is a single word, as in this case, this is simply p(basic|cellular process)/
p(basic), and we break only when there is more gain than loss. This restriction could be parameterized
for more fine-grained control of the trade-off between larger and smaller segments in specific cases, but
in the interest of avoiding nuisance parameters we just use it directly. Once we have decomposed all
eligible entries to create a final lexicon, we apply these same decompositions to the segments in our
initial segmentation to produce a final segmentation (see Figure 1c).
4 Evaluation
Multiword lexicons are typically evaluated in one of two ways: direct comparison to an existing lexicon,
or precision of the top n candidates offered by the model. There are problems with both these meth-
ods, since there are no resources that offer a truly comprehensive treatment of multiword units, defined
broadly, and the top n candidates from a model for small n may not be a particularly representative sam-
ple: in particular, they might not include more common terms, which should be given more weight when
one is considering downstream applications. Given the dual output of our model, evaluation using seg-
mentation is another option, except that creating full gold standard segmentations would be a particularly
difficult annotation task, since our notion of multiword unit is a broad one.
In light of this, we evaluate by taking the best from these various approaches. Given an existing mul-
tiword lexicon, we can evaluate not by comparing our lexicon to it directly, but rather by looking at the
extent to which our segmentation preserves these known multiword units. There are several major ad-
vantages to this approach: first, it does not require a full lexicon or gold standard segmentation; second,
common units are automatically given more weight in the evaluation; third, we can use it for evaluation
in very large corpora. Our two main metrics are subsumption (Sub), namely the percentage of multiword
tokens that are fully contained with a segment, and exact matches (Exact), the percentage of multiword
tokens which correspond exactly to a segment. Exact matches would seem to be preferable to subsump-
tion, but in practice this is not necessarily the case, since our method often identifies valid compound
terms and larger constructions than our reference lexicon contains; for example, WordNet only contains
the expression a lot, but when appearing as part of a noun phrase our model typically segments this to a
lot of, which, in our opinion, is a preferable segmentation. To quantify overall performance, we calculate
a harmonic mean (Mean) of the two metrics. We also looked specifically at performance for terms of 3
or more words (Mean 3+), which are less studied and more relevant to our interests.
Our second evaluation focuses on the quality of these longer terms with a post hoc annotation of
output from our model and the best alternatives. We randomly extracted pairs of segments of three words
or more where our model mostly but not entirely overlapped with an alternative model (750 examples
per corpus per method), and asked CrowdFlower workers to choose which output seemed to be a better
multiword unit in the context; they were shown the entire sentence with the relevant span underlined,
and then the two individual chunks separately. To ensure quality, we used our multiword lexicon to
2
This probability is based on the respective counts in our preliminary lexicon at this step in the process, not the original n-
gram probability. One key advantage to doing the initial segmentation first is that words that appear consistently in larger units,
an extreme example is the bigram vector machine in the term support vector machine, already have low or zero probability, and
will not appear in the lexicon or be good candidate segments for decomposition. This rather intuitively accomplishes what the
c-value metric is modeling by applying negative weights to candidates appearing in larger n-grams.
757
create gold standard examples (comparing known multiword units to purposely bad segmentations which
overlapped with them), and used them to test and filter out unreliable workers: for inclusion in our final
set, we required a minimum 90% performance on the test questions. We also limited each contributor to
only 250 judgments, so that our results reflected a variety of opinions.
We considered a number of alternatives to our approach, though we limited the comparison to methods
which could predict segments greater than 2 words, those that were computationally feasible for large
corpora, and those which segment into single words only as a last resort: approaches which prefer single
words cannot do well under our evaluation because we have no negative examples, only positive ones.
The majority of our alternatives involve ranking all potential n-grams (not just the maximal) with n? 2
and then greedily segmenting them: big-n prefers longer n-grams (with a backoff to counts); c-value is
used for term extraction (Frantzi et al., 2000) and was also compared to by Newman et al. (2012); ME
refers to the Mutual Expectation metric (Dias et al., 1999); and PMI uses a standard extension of PMI to
more than 2 words. We also tested standard (pairwise) PMI as a metric for recursively joining contiguous
units (starting with words) into larger units until no larger units can be formed (PMI join), and a version
of our decomposition algorithm which selects the minimal breaks which maximize total word count
across segments rather than total word predictability (count decomp); the fact that traditional association
metrics are not defined for single words prevents us from using them as alternatives to predictability in
our decomposition approach. Finally, we also include an oracle which chooses the correct n-grams when
they are available for segmentation, but which still fails for units that are below our threshold.
We evaluated our model in four large English corpora: news articles from the Gigaword corpus (Graff
and Cieri, 2003) (4.9 billion tokens), out-of-copyright texts from the Gutenberg Project
3
(1.7 billion
tokens), a collection of abstracts from PubMed (2.2 billion tokens)
4
, and blogs from the ICWSM 2009
social media corpus (Burton et al., 2009) (1.1 million tokens). Our main comparison lexicon is WordNet
3.0, which contains a good variety of expressions appropriate to the various genres, but we also included
multiword terms from the Specialist Lexicon
5
for better coverage of the biomedical domain. One issue
with our evaluation is that it assumes all tokens are true instances of the multiword unit in question; we
carried out a manual inspection of multiword tokens identified by string match in our development sets
(5000 sentences set aside from each of the abstract and blog corpora), and excluded from the evaluation
a small set of idiomatic expressions (e.g. on it, do in) whose literal, non-MWE usage is too common for
the expression to be used reliably for evaluation; otherwise, we were satisfied that the vast majority of
multiword tokens were true matches. When one multiword token appeared within another, we ignored
the smaller of the two; when two overlapped in the text, we ignored both.
5 Results
All the results for the main evaluation are shown in Table 1. First, we observe that our initial segmentation
always provides the highest subsumption, and our final lexicon always provides the highest harmonic
mean, with a modest drop in subsumption but a huge increase in exact matches. The alternative models
fall roughly into two categories: those which have reasonably high subsumption, but few exact matches
(PMI rank seems to be the best of these) and those that have many exact matches (sometimes better
than either of our models) but are almost completely ineffective for identifying multiword units of length
greater than 2 (ME rank and c-value, with ME offering more exact matches): the latter phenomenon is
attributable to the predominance of two-word multiword tokens in our evaluation, which means a model
can do reasonably well by guessing mostly two-word units. For the corpora with more multiword units
of greater length, i.e. the PubMed abstracts and the Gutenberg corpus, our method also provides the most
exact matches. Our best results come in the PubMed corpus, probably because the texts are the most
uniform, though results are satisfactory in all four corpora tested here, which represent a considerable
range of genres.
3
http://www.gutenberg.org . Here we use the English texts from the 2010 image, with headers and footers filtered out using
some simple heuristics.
4
http://www.ncbi.nlm.nih.gov/pubmed/
5
http://www.nlm.nih.gov/research/umls/new users/online learning/LEX 001.htm
758
Table 1: Performance in segmenting multiword units of various segmentation methods in 4 large corpora.
Sub. = Subsumption (%); Exact = Exact Match (%); Mean = Harmonic mean of Sub and Exact; Mean 3+
= Harmonic mean of Sub and Exact for multiword tokens of at length 3 or more. Bold is best in column
for corpus, excluding the oracle.
Method
Gigaword news articles Gutenberg texts
Sub Exact Mean Mean 3+ Sub Exact Mean Mean 3+
Oracle 97.1 97.1 97.1 95.5 97.0 97.0 97.0 97.8
big-n rank 88.7 28.8 43.5 51.4 84.9 30.1 44.4 57.5
c-value rank 69.1 66.1 67.6 23.3 58.6 57.7 58.2 12.6
ME rank 75.3 70.0 72.6 14.4 63.2 61.0 62.1 10.9
PMI rank 90.8 30.0 45.1 53.5 86.9 32.8 47.7 61.2
PMI join 83.1 32.8 47.0 43.7 77.7 32.6 46.0 45.5
Count decomp 75.9 31.3 44.3 47.1 69.2 31.5 43.3 54.2
Prediction decomp, initial 92.2 36.4 52.2 64.4 89.3 38.7 54.0 71.6
Prediction decomp, final 85.6 66.4 75.2 63.8 78.9 62.8 70.0 61.6
Method
PubMed abstracts ICWSM blogs
Sub Exact Mean Mean 3+ Sub Exact Mean Mean 3+
Oracle 91.9 91.9 91.9 84.0 96.5 96.5 96.5 99.4
big-n rank 82.2 40.1 53.9 55.5 86.1 33.3 48.0 60.8
c-value rank 63.2 62.3 62.7 21.7 64.3 62.4 63.3 14.6
ME rank 68.5 65.8 67.1 9.1 69.7 66.2 67.9 11.7
PMI rank 87.0 41.4 56.1 58.3 88.4 35.7 50.8 63.4
PMI join 79.8 39.7 53.0 46.8 80.3 35.4 49.1 47.0
Count decomp 71.0 38.4 49.9 50.4 71.5 33.5 45.6 53.9
Prediction decomp, initial 88.6 50.3 64.1 67.2 90.5 40.3 55.8 70.9
Prediction decomp, final 85.2 73.4 78.8 69.5 83.2 64.9 72.9 66.9
Table 2: CrowdFlower pairwise preference evaluation, our full model versus a selection of alternatives
Comparison Preference for Prediction decomp, final
Prediction decomp, final vs. ME 57.9%
Prediction decomp, final vs. Multi PMI 71.0%
Prediction decomp, final vs. Prediction decomp, initial 70.5%
For our crowdsourced evaluation, we compared our final model to the best models of each of the two
major types from the first round, namely Mutual Expectation and PMI rank, as well as our initial seg-
mentation. The results are given in Table 2. Our full model is consistently preferred over the alternatives.
This is not surprising in the case of the high-subsumption, low-accuracy models, since the resulting seg-
ments often have extraneous words included: an example is in spite of my, which our model correctly
segmented to just in spite of. Given that the ME ranking rarely produces units larger than 2 words, how-
ever, we might have predicted that when it does it would be more precise than our model, but in fact
our model was somewhat preferred (a chi-square test confirmed that this result was statistically different
from chance, p < 0.001). An example of an instance where our model offered a better segmentation is
call for an end to as compared to for an end to from the ME model, though there are also many instances
where the ME segmentation is more sensible, e.g. what difference does it make as compared to difference
does it make from our model.
Looking closer at the output and vocabulary of our model across the various genres, we see a wide
range of multiword phenomena: in the medical abstracts, for instance, there is a lot of medical jargon (e.g.
daily caloric intake) but also other larger connective phrases and formulaic language (e.g. an alternative
explanation for, readily distinguished from). The blogs also have (very different) formulaic language of
759
the sort studied using lexical bundles (e.g. all I can say is that, where else can you) and lots of idiomatic
language (e.g. reinventing the wheel, look on the bright side). The idioms from the Gutenberg, not
surprisingly, tend to be less clich?ed and more evocative (e.g. ghost of a smile); there are rather stodgy
expressions like far be it from me and conjunctions we would not see in the other corpora (e.g. rocks
and shoals, masters and mistresses). By contrast, many of the larger expressions in the news articles are
from sports and finance (e.g. investor demand for, tied the game with), with many that would be filtered
out using the simple grammatical filters often applied in this space. However, for bigrams in particular,
some additional syntactic filtering is clearly warranted.
6 Conclusion
We have presented an efficient but effective method for segmenting a corpus into multiword collocational
units, with a particular focus on units of length greater than two. Our evaluation indicates that this
method results in high-quality segments that capture a variety of multiword phenomena, and is better
in this regard than alternatives based on relevant association measures. This result is consistent across
corpora, though we do particularly well with highly stereotyped language such as seen in the biomedical
domain.
Future work on improving the model will likely focus on extensions related to syntax, for instance
bootstrapped POS filtering and discounting of predictability that can be attributed solely to syntactic
patterns. Our method could also be adapted to decompose full syntactic trees rather than sequences of
words, offering tractable alternatives to Bayesian approaches that identify recurring tree fragments (Cohn
et al., 2009); this would allow us, for instance, to correctly identify constructions with long-distance
dependencies or other kinds of variation where relying on the surface form is insufficient (Seretan, 2011).
With regards to applications, we will be investigating how to help learners notice these chunks when
reading and then use them appropriately in their own writing; this work will eventually intersect with
the well-established areas of grammatical error correction (Leacock et al., 2014) and automated essay
scoring (Shermis and Burstein, 2003). As part of this, we will be building distributional lexical repre-
sentations of these multiword units, which is why our emphasis here was on a highly scalable method.
Part of our interest is of course in capturing the semantics of idiomatic phrases, but we note that even
in the case when a multiword unit is semantically compositional, it might provide de facto word sense
disambiguation or be stylistically distinct from its components, i.e. be very specific to a particular genre
or sub-genre. Therefore, provided we have enough examples to get reliable distributional statistics, these
larger units are likely to provide useful information for various downstream applications.
Acknowledgments
This work was supported by the Natural Sciences and Engineering Research Council of Canada and the
MITACS Elevate program. Thanks to our reviewers and also Tong Wang and David Jacob for their input.
References
Steven Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-
Based Parsing, pages 257?278. Kluwer Academic Publishers.
Vitor De Araujo, Carlos Ramisch, and Aline Villavicencio. 2011. Fast and flexible MWE candidate generation
with the mwetoolkit. In Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).
Timothy Baldwin and Su Nam Kim. 2010. Multiword expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, FL.
Douglas Biber, Susan Conrad, and Viviana Cortes. 2004. If you look at. . .: Lexical bundles in university teaching
and textbooks. Applied Linguistics, 25:371?405.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Media (ICWSM ?09), San Jose, CA.
760
Yu-Hua Chen and Paul Baker. 2010. Lexical bundles in L1 and L2 academic writing. Language Learning &
Technology, 14(2):30?49.
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution gram-
mars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics (NAACL ?09).
Ga?el Dias, Sylvie Guillor?e, and Jos?e Gabriel Pereira Lopes. 1999. Language independent automatic acquisition
of rigid multiword units from unrestricted text corpora. In Proceedings of Conf?erence Traitement Automatique
des Langues Naturelles (TALN) 1999.
Thomas Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Processing.
Stefan Evert. 2004. The statistics of word cooccurences?word pairs and collocatoins. Ph.D. thesis, University of
Stuttgart.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson. 2009. Unsupervised type and token identification of idiomatic
expressions. Computational Linguistics, 35(1):61?103.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima. 2000. Automatic recognition of multi-word terms: the
c-value/nc-value method. International Journal on Digital Libraries, 3:115?130.
David Graff and Christopher Cieri. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA.
Ulrich Heid. 2007. Compuational linguistic aspects of phraseology. In Harald Burger, Dmitrij Dobrovol?skij,
Peter K?uhn, and Neal R. Norrick, editors, Phraseology. An international handbook. Mouton de Gruyter, Berlin.
Adam Kilgarriff and David Tugwell. 2001. Word sketch: Extraction and display of significant collocations for
lexicography. In Proceedings of the ACL Workshop on Collocation: Computational Extraction, Analysis and
Exploitation.
Nidhi Kulkarni and Mark Finlayson. 2011. jMWE: A Java toolkit for detecting multi-word expressions. In
Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2014. Automated Grammatical Error
Detection for Language Learners (2nd Edition). Morgan & Claypool.
David Newman, Nagendra Koilada, Jey Han Lau, and Timothy Baldwin. 2012. Bayesian text segmentation for
index term identification and keyphrase extraction. In Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12).
Pavel Pecina. 2010. Lexical association measures and collocation extraction. Language Resources and Evalua-
tion, 44:137?158.
Ted Pedersen, Satanjeev Banerjee, Bridget McInnes, Saiyam Kohli, Mahesh Joshi, and Ying Liu. 2011. The
Ngram statistics package (text::nsp) : A flexible tool for identifying ngrams, collocations, and word associations.
In Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Proceedings of the 3rd International Conference on Intelligent Text Processing
and Computational Linguistics (CICLing ?02).
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a
solved problem? In Proceedings of Empirical Methods in Natural Language Processing (EMNLP ?01).
Violeta Seretan. 2011. Syntax-Based Collocation Extraction. Springer.
Mark D. Shermis and Jill Burstein, editors. 2003. Automated Essay Scoring: A Cross-Disciplinary Approach.
Lawrence Erlbaum Associates, Mahwah, NJ.
Frank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, pages 143?177.
761
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2172?2183, Dublin, Ireland, August 23-29 2014.
Supervised Ranking of Co-occurrence Profiles for Acquisition of
Continuous Lexical Attributes
Julian Brooke
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Certain common lexical attributes such as polarity and formality are continuous, creating chal-
lenges for accurate lexicon creation. Here we present a general method for automatically placing
words on these spectra, using co-occurrence profiles, counts of co-occurring words within a large
corpus, as a feature vector to a supervised ranking algorithm. With regards to both polarity and
formality, we show this method consistently outperforms commonly-used alternatives, both with
respect to the intrinsic quality of the lexicon and also when these newly-built lexicons are used
in downstream tasks.
1 Introduction
Lexicon acquisition represents one key way that the information in large corpora and other resources can
be leveraged in various NLP tasks, particularly when the range of lexical items involved in a particular
phenomenon is much more diverse than can typically be captured in manually-built resources. Another
property of the lexicon which might limit a manual approach is the fact that certain attributes are not
discrete, instead falling on a continuous spectrum; although there are manually-built dictionaries which
contain fine-grained judgments of spectra?an example is the MRC psychological database (Coltheart,
1980)?these tend to be very low in coverage, reflecting the difficulty in collecting this information.
Within computational linguistics, the continuous lexical attribute that has received the most attention is
undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity.
Much of the work focused on acquisition of this attribute at the lexical level has involved simplification
to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and
McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009;
Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification
(Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent
role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words
at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper,
we will focus on these two spectra; the method presented, however, is intended to be general, and as such
could be easily applied to other spectra such as those in the MRC database, e.g. abstractness (Turney et
al., 2011), and other kinds of variation captured in, for instance, Osgood?s semantic differential (Osgood
et al., 1957).
The typical approach to this problem involves semi-supervised methods using vector space and/or
graph representations and a set of seed terms. Our method is novel in that it uses fully supervised
SVM ranking of co-occurrence profiles, i.e. normalized counts of instances of binary text co-occurrence
between the target word and a large set of profiling words, selected on the basis of their frequency,
in a publicly-available blog corpus. The seed terms from earlier methods are now viewed as training
examples for building a supervised model that can connect the distributions of co-occurring words in this
wider vocabulary to relative locations on a continuous spectrum. This approach depends somewhat upon
improved manual lexical resources available for these tasks, such as the SO-CAL dictionary (Taboada
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2172
et al., 2011) in the case of polarity, but we limit our (word) training set size in order to show it will
work in resource-scarce situations, such as languages other than English. Our method is straightforward,
practical, and offers essentially full coverage, including words and lexicogrammtical patterns that are
simply not accessible by the many popular methods that are primarily based on WordNet.
To evaluate, we compare our method with popular alternatives in both polarity and formality, with
particular emphasis on other methods based on corpus co-occurrence that have also been shown to be
generalizable across various spectra, i.e. LSA and PMI. For both spectra of interest here, we evaluate both
intrinsically using pairwise comparisons from manually-built lexical resources, and also extrinsically in
downstream tasks such as text-level polarity classification and sentence-level formality judgments. We
show our method is consistently superior across our various evaluations. We also show that not only are
co-occurrence profiles a good source of information for supervised ranking, but that a focus on ranking
rather than regression in this space appears to be fundamental to the success of a supervised approach to
lexical spectra.
2 Related Work
Viewed primarily as a categorical task, the creation or expansion of lexical resources for sentiment anal-
ysis is a commonly-addressed problem. In addition to SentiWordNet (Baccianella et al., 2010), which
we will compare to directly to here, there are numerous mostly semi-supervised approaches based on
exploiting the glosses and/or the graph structure of WordNet to determine whether a word is positive
or negative (Kamps et al., 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Takamura et al., 2005; An-
dreevskaia and Bergler, 2006; Rao and Ravichandra, 2009; Hassan and Radev, 2010), or taking advan-
tage of some other lexicographic resources (Mohammad et al., 2009; Klebanov et al., 2013). The earliest
corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic in-
formation, i.e. conjunctions, to make connections between adjectives; other work that makes use of local
patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006).
Turney (2002) built a continuous polarity lexicon using PMI based on Internet hit counts as a useful
measure of relatedness between seeds, and Turney and Littman (2003) compared this approach with
LSA, which uses general patterns of co-occurrence based on dimensionality reduction. Velikovich et al.
(2010) combined web-scale corpora with a graph-based approach, assigning polarity scores to n-grams
on the basis of the maximum weighed path from an n-gram to the seed terms, using a small (6-word)
context around the word. Like us, Volkova et al. (2013) use social media, iteratively labeling tweets
and words for subjectivity and polarity. Fully-supervised approaches to polarity lexicon acquisition are
rare, but one example is the work of Chetviorkin and Loukachevitch (2012), who classify words as being
sentiment-relevant in Russian using a small set of statistical features, including ratios across disparate
corpora.
Our interest in the continuous aspect of polarity overlaps with work on deriving the semantic intensity
of lexical items from corpora (Sheinman and Tokunaga, 2009); in this task, small sets of synonyms are
ranked according to their intensity, including (but not limited to) polarity. De Melo and Bansal (2013)
use a Mixed Integer Linear Programing algorithm to combine information from multiple pairs into a
single coherent ranking. As with some of the work in polarity, the focus is on adjectives and local
patterns which explicitly distinguish degrees of intensity e.g. not only x but also y, which limits its range
of application; it would not, for instance, be useful for formality or other more pragmatic variations.
Beyond the our work in LSA-based formality lexicon creation (Brooke et al., 2010) and the sentence-
level formality annotation of Lahiri et al. (2011), which we discuss later in more detail, there is a rela-
tively small amount of computational research that directly addresses formality. At lexical level, Li and
Yarowsky (2008) identify formal and informal synonyms in Chinese. Heylighen and Dewaele (2002) and
Li et al. (2013) both offer text-level quantifications of formality; the former is based on POS frequency,
while the latter is based on the Coh-Metrix textual metrics. Using these kinds of metrics, formality
has been evaluated in social media (Mosquera and Moreda, 2012). A supervised text classification ap-
proach to formality is offered by Sheika and Inkpen (2012). Lexical formality is obviously related to
lexicon-based readability (Kidwell et al., 2009) and lexical simplification (Carroll et al., 1999), and is
2173
also relevant to the recent interest in identifying social relationships (Peterson et al., 2011) and shows of
politeness (Danescu-Niculescu-Mizil et al., 2013).
3 Method
Our approach to lexicon acquisition falls into the general category of corpus-based techniques. For both
attributes addressed in this paper, we use the same corpus, the 2009 ICWSM Spinn3r dataset (Burton et
al., 2009), a publicly-available blog corpus which we also used in our earlier work on lexical formality
(Brooke et al., 2010). Blogs are a good resource for broad lexical acquisition because they are very
broad in style and content, and are available in essentially unlimited amounts. We use the English Tier 1
(high-quality) blogs that have at least 100 word types, excluding duplicate texts; after this filtering, our
dataset contains a total of about 2.4 million blogs.
To build a lexicon for any continuous attribute of interest, we begin by creating co-occurrence profiles
as follows: First, we select a document frequency range min-df, max-df that determines a set of profile
words P in a corpus S, where for each p ? P, the document frequency df
S
p
of p in S is limited to be
min-df < df
S
p
< max-df ; that is, each profile word appears in more than min-df documents, but fewer
than max-df documents in our corpus. Then, given a sample size n and a target word w that we wish to
profile, we sample a set of n texts from S which contain the target word (or all the documents where the
word appears, if it appears fewer than n times), and count the document frequency of each profile word
p in this subcorpus, T
w
. We ignore the term frequencies within individual documents because a binary
representation is known to be preferred for stylistic dimensions like formality (Brooke et al., 2010),
and this seems to be also somewhat true in the domain of polarity, where better results can be obtained
when multiple instances of a polar word are discounted (Taboada et al., 2011). To avoid overfitting our
statistical model, we do not count a word as appearing with itself. Once we have sub-corpus document
frequences df
T
w
p
for each p, for each profile word p we define the element of our co-occurrence profile
vector v
p
as
v
p
=
df
T
w
p
?
q?P
df
T
w
q
That is, we normalize each count by the sum across all counts, such that the L1 norm of v is 1. For our
applications here, the dimension of the co-occurrence profile vector is typically in the tens of thousands,
but to illustrate the creation of this vector, suppose we choose an extremely narrow document frequency
band min-df, max-df, such there were only three co-occurrence profile words: p
1
,p
2
,p
3
. For some word
w, we sample n instances of texts from our corpus which contain w, and find that p
1
appears in 10
of these texts, p
2
in 40 of them, and p
3
in 50 of them. The resulting co-occurrence profile vector is
v = ?0.1,0.4,0.5?. This profile could be viewed as a distributional vector space representation of the
word (Turney and Pantel, 2010), or as an estimate of the probability of each p occurring with w; without
any further manipulation, however, we will use it directly as a feature vector for our supervised ranking.
In order to proceed with a supervised approach, we need a ranking of a set of words relevant to
the lexical attribute that we wish to acquire; this ranking is specific to the attribute in question, so we
discuss this in later sections. Given such a ranking (which, we note, may be partial), we apply SVM
rank
(Joachims, 2002), which is part of the SVM
light
set of SVM-based machine learning tools. SVM
rank
was developed for ranking web page results, and, to our knowledge, has not been applied in this space.
SVM
rank
uses an algorithm which optimizes the Kendall?s ? (Kendall, 1955) between a correct ranking r
a
and the automatically-generated ranking r
b
. The simplest version of ? is based on the number of pairwise
rankings which are in concord (C), i.e. both rankings rank the pair relative to each other and the pairwise
rankings are the same, or in discord (D), i.e. both rankings rank the pair relative to each other but the
rankings offered are contradictory. ? is defined as:
?(r
a
,r
b
) =
C?D
C +D
2174
In practice, this is accomplished in SVM
rank
by modifying the original SVM algorithm to use as feature
vectors the difference between ranked input vectors, rather than the input vectors directly. In the context
of this feature space, this means that the model is trained on vectors which represent the differences in the
co-occurrence profiles of ranked words; if the word with co-occurrence profile u is ranked higher than
a word with co-occurrence profile v in our annotation, then SVM
rank
will try to find a weight vector w
such that (u?v) ?w > 0, where w is constrained to be a sum of co-occurrence profile differences (i.e. the
support vectors). Like standard SVM, ranking SVM uses a C parameter which represents the trade-off
between margin size and classification errors, though the interpretation of the margin in ranking SVM
is less clear. The output of the classification step of SVM
rank
is a number for each word which can be
used directly to rank words, or which can be normalized across words into a scale. If the input rankings
also have a continuous numerical representation (which is true in our case for polarity), then this ranking
approach can be compared directly to a standard regression which is not directly sensitive to rankings; to
maximize comparability, we use the regression function included in SVM
light
for this purpose. For both,
we used a linear kernel.
There is a small number of parameters that need to be set: the sample size n, the frequency range
min-df, max-df, and the SVM C parameter. For each of the two lexical attributes of interest, we carried
out independent tuning of these parameters using 5-fold crossvalidation in the training set, carrying out a
grid search at powers of 10. We will discuss the values of parameters with respect to specific experiments
later, but we mention here that a higher-than-default C, which corresponds to more emphasis on avoiding
error rather than maximizing the margin, gave better results for both ranking and correlation, though with
diminishing returns. The role of n is primarily to make the method (much) more tractable, but we suspect
it might be beneficial to the training of the model for the profiles to be based on a uniform number of
examples across word types.
Before we move on to the experimental evaluation, we highlight some intrinsic advantages of this
model, independent of performance. As a technique based on large corpus co-occurrence, it has the im-
portant property that it can go beyond the limited vocabulary offered by, for instance, WordNet. Since
we rely only on co-occurrence, we are not at all limited to individual words (or specific types of words):
we could just as easily derive attribute values for n-grams, collocations, or full lexico-grammatical con-
structions (for instance, distinguishing high as related to price from high as related to quality); though
our interest here is in general lexical properties, there is no reason this approach could not be used
for domain-specific applications, for any lexical units that appear often enough to obtain a reliable co-
occurrence profile. Unlike many graph-based techniques, new vocabulary can be classified directly with-
out perturbing the model, potentially in an online fashion if the corpus is properly indexed (which, we
note, is by far the most time-consuming step of our method). Though some lemmatization may be re-
quired for highly inflectional languages, the method extends easily to any language for which blog data
is likely to be available in sufficient quantities. Our approach is more straightforward than most other
methods based on co-occurrence, which means fewer arbitrary choices and nuisance variables (such as
the dimensionality k or feature weighting typically used in dimensionality-reduction approaches such as
LSA); the parameters that we have are fairly well-behaved. Unlike methods which rely only on examples
from the extremes of a spectrum to derive a quantification of it, our method naturally integrates examples
from the middle of the spectrum (e.g. neutral examples in the case of polarity), but does not inherently
require fine-grained quantification of the entire spectrum; in fact, pairwise examples alone could be used
for training.
4 Polarity experiments
4.1 Word-level Evaluation
We first consider whether our model can be used to build a lexicon which reflects the polarity spectrum.
Our training set of words is taken from the SO-CAL dictionary (Taboada et al., 2011), which has man-
ually assigned SO (polarity) values for words at integer intervals in the range +5 to ?5. The entire
dictionary contains about 5000 words, but we do not use the entire set: first, we restrict our investigation
here to adjectives, which allows us to sidestep inflection issues (we do not consider comparative adjec-
2175
tives), and we randomly select only 50 words from each of the 11 possible SO ratings in the dictionary
(for a total of 550 words), so as to mimic a (relatively) low-resource situation as we might find working
in other languages, and to make it possible to keep the counts equal across SO ratings. Note that the SO-
CAL dictionary does not contain neutral words (words not in the dictionary are assumed to be neutral),
but we used a set of about 200 hand-marked neutral adjectives that had been excluded from the lexicon
during its creation from the words in a set of Epinions product reviews, and which were used for the
original dictionary evaluation by Taboada et al. (2011).
After training our model, we evaluate in two test sets. The first test set is the rest of the SO-CAL
dictionary, excluding words in the training set as well as those not given a rating by SentiWordNet (see
below). Note that this set is not balanced across SO values, since there are many more weakly positive
(SO 1 to 3) or weakly negative (SO ?1 to ?3) words than more-extreme or neutral words; we would
argue, though, that this reflects the actual situation in subjective corpora such as product reviews. To test
whether we might be overfitting to the product reviews domain, we also test using annotations from the
MPQA (Subjectivity) lexicon (Wilson et al., 2005), which was built primarily from news texts.
1
For this,
we again include only words that are in SentiWordNet. The MPQA lexicon uses a very different tagging
schema than the SO-CAL dictionary, with 3 polarity categories (positive, negative, and neutral) as well
as two degrees of subjectivity, weak or strong. Strong or weak subjectivity is defined as how reliable
an indicator of subjectivity the word is, which does not directly correspond to the rationale used for
the SO-CAL dictionary (which is closer to the notion of force or intensity); the results in Taboada et al.
(2011) and our own examination of the lexicon suggest, however, that there is some correlation.
2
Despite
this uncertainty, we combined the MPQA tags to form a polarity spectrum: strongly subjective negative,
weakly subjective negative, neutral, weakly subjective positive, strongly subjective positive. Given a
ranking by our SVM ranker, we evaluate overall pairwise accuracy by considering all possible pairings
of words across different ratings within the SO-CAL or MPQA test sets, and count the percentage of
those where the ordering of the pair with respect to the polarity spectrum is correctly predicted by the
ranking. For a more detailed breakdown, we divide these pairwise comparisons into 3 categories: polarity
(pairs which involve one positive and one negative word), neutrality (pairs which have one neutral word),
and intensity (pairs which have two words with the same polarity). Note that much work in bootstrapping
lexicons for sentiment analysis uses precision and recall, but this is not the most appropriate evaluation
metric in this case because our method can assign a rank (and, eventually, an SO value) to any word in the
2 million word vocabulary of our corpus.
3
Here, we are interested only in reliability of these rankings.
During parameter tuning in the development phase, we found that min-df = 10
3
, max-df = 10
5
was a
good choice: in other words, our profile words are words that appear less than once in 24 texts, but more
than once in 2400 texts. In the ICWSM, there are 30,852 words that fall into this category, so that is the
length of our feature vector. Based on results in the development set, we take n = 1000 as our default;
larger samples provided no appreciable benefit and were even slightly worse in some cases. For the SVM
C parameter, we used 100. We also test using SVM correlation, using the same parameters.
In addition to these variations on our co-occurrence profile technique, we also compare with three
independent alternatives. The first is SentiWordNet 3.0 (Baccianella et al., 2010), which uses a random
walk method in WordNet to derive positive, negative, and neutral values (which sum to 1) for each synset
in WordNet. We follow Taboada et al. (2011) in converting this to a single spectrum for each word by
subtracting the negative score from the positive score, and averaging the result across senses for each
1
There are of course other popular manually-built lexicons, for instance the General Inquirer (Stone et al., 1966), but they
tend to have only binary annotations.
2
One example of where these two dictionaries differ is the word nervous, which is tagged as a strongly subjective negative
word in the MPQA, but has only a?1 score in the SO-CAL dictionary, since it does not describe a particularly intense negative
emotion. An example of a ?5 word is horrendous, which is also a strongly subjective negative word in the MPQA. Instances
of discord where the SO-CAL dictionary is clearly stronger are rarer, but an example is comprehensive, which has an SO of 3
in SO-CAL, but is weakly subjective in the MPQA, probably because of its common descriptive uses, such as in the context of
insurance and (in the UK) education.
3
We have not yet build such a lexicon, but, to facilitate comparison, but we are making available raw scores for all the
adjectives already contained in at least one of the SO-CAL, MPQA, and SentiWordNet lists (excluding the 550 training words),
as well as lists of specific words used for training and testing. These resources can be found at http://www.cs.toronto.edu/
?
jbrooke/rankingpolarity.zip .
2176
Table 1: Results of polarity experiments. Left side of table shows pairwise accuracy (%) for various
sentiment lexicon ranking methods in SO-CAL and MPQA test sets. Pol. = Pairs with different polarity;
Neu. = Pairs with at least one neutral word; Int. = Pairs with the same polarity, but different intensi-
ties. Right side of table shows text polarity classification accuracy (%) in Epinions Corpus for various
adjective lexicons. Bold is best in column.
SO-CAL words MPQA words Epinions texts
Method Pol. Neu. Int. All Pol. Neu. Int. All Acc.
SentiWordNet 82.3 72.3 57.4 72.1 82.8 72.0 49.9 72.0 65.8
LSA 83.5 70.6 63.0 74.5 82.5 70.2 64.0 75.8 66.3
PMI 86.3 73.6 65.8 77.3 84.5 73.4 61.6 76.9 68.0
Profile regression 80.2 67.6 59.6 71.2 77.6 69.0 74.7 75.8 60.3
Profile ranking 88.6 75.7 67.5 79.4 87.5 74.0 56.5 77.0 71.8
word (in that paper, they also considered using only the most common sense but found the results to be
indistinguishable). The second alternative uses the semi-supervised LSA-based method of Turney and
Littman (2003). For the first step, singular value decomposition, we use a binary term-document matrix
with the same ICWSM texts as our supervised model, with k = 500 (a fairly standard choice). In the
second step, which involves calculating the cosine similarity with a set of seed terms using the LSA
vectors and then taking the difference, the positive and negative seeds are just the training instances for
our supervised model (neutral terms are discarded). Our third comparison is the PMI approach of Turney
(2002), which is still popular: for instance, PMI was used to built a Twitter sentiment lexicon in the
winning entry in a recent shared task (Mohammad et al., 2013). Because they have access to the same
corpus and even the same example words as our method, the LSA and PMI alternatives are most directly
comparable to ours.
The results for the word-level polarity experiments are shown in the left side of Table 1. In the SO-
CAL test set, the results are clear: our SVM ranking method is preferred over alternatives, across all the
different categories of pairwise comparison. The relative difficulty of each pair type reflects the average
distance between relevant pairs on the spectrum, as expected. Surprisingly, the correlation method,
despite using the same feature input as the ranking method, is the worst performing method here, though
SentiWordNet is only marginally better, while LSA falls roughly in the middle of the range, and PMI is
the strongest competitor. One potential criticism is that a ranking method is likely to have an advantage
when evaluating by rank. This is true, but we think that relative rank among words is fundamental to
the notion of a spectrum, whereas the bucketing of words into evenly spaced integer ratings is just an
annotation convenience. That said, our output ranking is perhaps too fine-grained in comparison to our
input (offering a full ranking for all words), and it would be desirable if our ranking algorithm allowed
us to encourage some words to be ranked the same.
Although SVM ranking is also the best method on the MPQA test set, the results are marginal as
compared to the SO-CAL test. Part of this could be a moderate amount of domain overfitting, or perhaps
the ranking method is better at fine-grained scales relative to the other methods. However, the most
obvious difference between the test sets appears relative to the intensity comparison, where the profile
ranking performance is relatively poor. This is likely attributable to the differences between the two
kinds of annotations: the SVM ranking method learns the SO-CAL intensity scale fairly well, but this
actually becomes a handicap when degree of subjectivity and not force is the deciding factor; on the other
hand, corpus-based models which did relatively poorly in all the other evaluations (profile regression,
LSA) actually do somewhat better in MPQA intensity than their most comparable alternatives (profile
ranking, PMI) to a degree that is in fact proportional to their relative inferiority elsewhere, suggesting
that sensitivity to degree of subjectivity might be interfering with acquisition of the SO-CAL polarity
spectrum. Interestingly, the value provided by SentiWordNet does not seem to correspond well to either
of these interpretations of intensity, since it does rather poorly with respect to both.
2177
4.2 Text-level Evaluation
The most common use of polarity lexicons is the task of text polarity classification (Turney, 2002),
identifying whether an opinionated text is positive or negative. In this section, we convert the initial
output of the models to polarity lexicons with an appropriate scale so that we can use the SO-CAL
software (Taboada et al., 2011) to carry out text sentiment analysis with our alternative adjective lexicons
rather than its original, manually-built one. SO-CAL is an unsupervised lexical sentiment analysis system
with a number of built-in features, e.g. handling of negation and intensification, that improve the accuracy
of the model, particularly when using a fine-grained, high-precision lexicon. Taboada et al. evaluate
across 4 corpora of balanced product reviews. For our evaluation, we use one of those corpora, a set of
400 product reviews from Epinions, with 50 balanced texts from each of 8 product categories (movies,
books, cars, computers, cookware, hotels, phones, and music). Taboada et al. call this corpus Epinions
2, to distinguish it from the Epinions corpus that the SO-CAL dictionary was built from. They report an
accuracy of 80% using SO-CAL with all word types and features enabled.
Our interest here is to test the influence of lexicon quality on polarity detection. Coverage, though
of course important, is actually a potential source of noise: Low coverage can naturally result in low
performance, but Taboada et al. point out that high coverage can also cause problems, when many of the
rarer words added to the lexicon, even when human-tagged, are not relevant to the primary sentiment of
the text, but rather irrelevant aspects like (in the movie or book domain) character descriptions. Steps
can be taken to mitigate this by identifying relevant sentiment (Scheible and Sch?utze, 2013), but here
we sidestep this problem by forcing our lexicons to have exactly the same coverage by limiting them to
words that appear in the static SentiWordNet lexicon. Again, we also consider only adjectives here.
To build the lexicon for this evaluation we used a different training set: it is not possible to take 50
samples from each SO rating in the SO-CAL adjective lexicon and not have training words that also
appear in the corpus, which we explicitly wanted to avoid.
4
Instead, we train using all the adjectives in
the SO-CAL dictionary that either don?t appear in the Epinions corpus or don?t appear in SentiWordNet
(since we are limiting our output lexicon to SentiWordNet words). This results in a much larger training
set than in the word-level evaluation (about 1500 words), but they are distributed unevenly across SO
ratings. Relative to the word-level evaluation, this is closer to the situation if we were using the entire
SO-CAL dictionary to expand the lexicon. We use the same set of training words as seeds for LSA.
To convert SentiWordNet to a SO-CAL-compatible dictionary, we simply multiply the raw score,
guaranteed to be between ?1 and +1, by 5, creating a range of ?5 to +5. For the raw scores for our
other three options (LSA, profile regression, and profile ranking), we linearly scale the raw score so that
the mean within the lexicon is 0, and a SO +5 word is at the third standard deviation away from the
mean; we choose a rather severe scaling so that there are only a handful of words in the lexicon whose
absolute value is over 5, which SO-CAL is not designed for.
The results of this evaluation are shown on right side of Table 1. Profile ranking is once again dom-
inant, almost 4 percentage points better than the second-best option. The ordering of the lexicons here
is exactly the same as we saw for the word-level evaluation in SO-CAL, though SentiWordNet does
somewhat better than would be expected from those scores. Again, regression does quite poorly despite
having access to the same feature vectors as the ranking method. We note that our results here are also
markedly better than all the other automatic lexicons compared by Taboada et al., namely a PMI-derived
lexicon based on Google counts (Taboada et al., 2006), and a binary lexicon built by expanding entries
in a thesaurus (Mohammad et al., 2009), and are even a bit better than using the human-tagged (binary)
annotations from the General Inquirer (Stone et al., 1966), though we are still quite a long way from what
is possible with the full manual SO-CAL dictionary. Since the quality of the lexicon is directly reflected
in our polarity classification scores here, it is not surprising that our gold-standard lexicon is superior;
in this context, it should be viewed as an upper bound. Nevertheless, we have strong evidence here that
our co-occurrence profile ranking method is a step in the right direction relative to other methods for
automatically building lexicons.
4
For all of our evaluations in this paper we were careful never to use the score of a word which appeared in our training set;
the drawback of this is that our training set size is not constant.
2178
5 Formality Experiments
5.1 Word-level Evaluation
Though the lexicon is perhaps more fundamental to distinctions of polarity than is the case for formality,
nevertheless formality is strongly expressed through word choice; for instance, in English using the
word dude to address a socially-powerful stranger would generally be unacceptable, and it would be
very strange to address a good friend as sir, except as a joke. These are not isolated examples: a huge
portion of the vocabulary is marked to some degree in this fashion, and requires special attention when
moving across text genres or social situations. Word length (in English, at least) and word frequency
can be used as a simple proxy (longer, rarer words are, on average, more formal), but the example above
belies this approach: sir is a shorter word than dude, and it is not immediately obvious that it would
appear less in, for instance, a news corpus, than dude.
In previous work (Brooke et al., 2010), we used the LSA co-occurrence method of Turney and Littman
(2003) discussed in the previous section to derive a formality lexicon using the ICWSM (which was the
best among various corpora tested, including the BNC). For testing, we used a set of 399 synonym
pairs that were pulled from a writing manual focused on word choice, Choose the Right Word (CTRW)
(Hayakawa, 1994), where the author explicitly compared words for their formality, showing that co-
occurrence was a better approach to identifying lexical formality than proxies related to word length or
word frequency. We note that many of the distinctions in the CTRW set are quite subtle, for example
determine vs. ascertain, both of which seem at least somewhat formal, though ascertain was judged by
the expert to be the more formal of the two. In this section we will build a formality ranking using our
profile ranking method, and show that it is better than the LSA method in the CTRW dataset. Here, we
follow our earlier work in using a much smaller k value (20) than is typical for topical uses of LSA,
which we found was better for this dataset, since major stylistic differences seem to be mostly captured
in the first few dimensions after dimensionality reduction, a result which is consistent with the work of
Biber (1988) looking at differences across registers.
Unlike for polarity, there is no resource available that offers a full scale of formality for a large number
of words, and the set used in our initial work on formality has only extreme, handpicked words. In more
recent work (Brooke and Hirst, 2013), we used a larger set of words (900) that included a variety of
different styles that had been tagged by a group of 5 annotators. In that work, we did not use the
term ?formality,? but one of our styles, colloquial, corresponds to the informal end of the spectrum,
and two other styles, objective and literary, can both be viewed as social-distancing language.
5
The
words tagged by annotators as belonging to neither of these categories will serve as the middle of this
spectrum. Compared to our polarity lexicon, our training set therefore is much more coarse-grained (with
only 3 rankings, as compared to 11 for polarity), but the pairwise relationships can be used to build a
fine-grained scale. As before, we remove all words that overlap with our test set.
With respect to parameters, we use the same n as in the polarity experiments, but in development we
saw better performance from a higher C (10,000) and a lower bottom bound on the document frequency
range, df-min = 10
2
. The latter might reflect the fact that rare vocabulary has a strong tendency to be
associated with extreme formality, though there is also a limit to this, since if a word is so rare that it
hardly ever co-occurs with anything, it cannot possibly be useful for training no matter how good an
example of extreme formality it is. The results using these settings are shown in the left side of Table 2.
Again, our profiling method is clearly better than lexicon-based alternatives. LSA outperforms PMI in
the word-level task, a result which is consistent with our other work in stylistic lexical induction (Brooke
and Hirst, 2013),
6
and all the co-occurrence methods are well above the word-length baseline.
Figure 1 contains a more detailed analysis of the influence of individual document frequency bands: we
5
The objective dimension corresponds roughly to the style of a technical document, while the literary dimension involves
flowery, even archaic language that suggests a literary sophistication. Contrast so with synonyms therefore (objective) and thus
(literary), which are both more formal, but in different ways.
6
We suspect this is due to the fact that LSA vectors encode information about word frequency: even when the vector norm is
controlled for, we have found that the LSA vectors of high- and low-frequency words have consistently different distributions,
which may help in identifying extremely low frequency, highly formal words appearing in the CTRW dataset; by contrast, PMI
and other probability-based approaches seem to behave more erratically when presented with low-frequency items.
2179
Table 2: Results of the formality experiments. The second column shows pairwise accuracy of different
models identifying the more formal of two synonyms in CTRW test set. The third column shows average
correlation with two human 5-point Likert-scale formality annotations of the 500 sentence test set.
Method CTRW words Sentence Evaluation
Word length 63.7 0.36
LSA 78.7 0.49
PMI 72.2 0.52
Profile ranking 86.5 0.55
Figure 1: Pairwise accuracy in the CTRW test set for various frequency bands. Dotted line represents
performance using best parameters from development phase, i.e. max-df = 10
2
, max-df = 10
5
.
built models for each of our frequency bands (ranges between two consecutive powers of ten), and tested
them in the CTRW corpus. The flat dotted line represents the larger band we used based on development
performance, 10
2
?10
5
. We see that accuracy peaks at 10
3
to 10
4
df band, at a value (87.3%) which is
higher than we saw with the larger band chosen based on the development set. The words in this band
are fairly uncommon, appearing less than once in 240 texts, but greater than once in 2400 texts; still, as
a group they provide enough evidence to make a strong determination of formalty.
5.2 Sentence-level Evaluation
Lahiri and Lu (2011) report on the creation of 5-point Likert scale annotations of sentence-level formality,
with two ratings for each of 500 sentences taken from separate texts in a diverse corpus which includes
news, blogs, forums, and academic papers (Lahiri et al., 2011). In this section, we use this annotation
to carry out an extrinsic evaluation of our lexical formality ratings. As far as we are aware, this is
the first use of this annotation for evaluating metrics of formality. We extract all lexical words (verbs,
adjectives, adverbs, and nouns, though we omit proper nouns) from the sentences and use the 3-way
formality annotation with these words removed to create LSA, PMI, and profile ranking models, which
are then used to create a formality lexicon for these words, using the same method we used to create the
SO lexicon. Given a lexicon, we averaged the formality score across each sentence (ignoring duplicate
items) to get a formality score for each sentence. We calculate Pearson?s correlation coefficient between
our score and each of the two annotators, and then average the result. For comparison, the correlation
between the two human annotators is 0.60.
The results in Table 2 indicate that the preference for the profile ranking method seen in the CTRW set
extends directly to sentence-level formality ranking, and the level of correlation reached by the profile
ranking method approaches correlation between humans. This supports our claim that lexical choice
is very important to formality: our results here indicate that humans with access to other indicators of
formality (for instance, use or avoidance of particular syntactic constructions) agree only slightly more
with each other than our lexicon-only model does with them.
2180
6 Conclusion
In this paper we have presented a novel approach to determining where a word lies on a spectrum, using
just counts of words that it tends to appear with and an SVM ranking algorithm, both of which are key
components to its success. We have shown that it can be applied to at least two continuous attributes of
interest in computational linguistics, namely polarity and formality, and that the benefits of this method
relative to established alternatives are visible not just in direct lexicon evaluation, but also in the NLP
tasks where these lexicons can be used. Even with a relatively small set of words to train with, we
see little sign of overfitting, and although we have focused on a small set of words here, our method is
efficient enough that it could easily be applied to a much larger set of lexicogrammatical units, though
we will also have to derive ways to filter out unreliable assignments to reduce overall noise. Other
future work will involve looking at other spectra, other languages, other supervised ranking models, and
improving our performance generally by being more selective of profile words or training examples or
by refining our rankings by including other sources of information such as WordNet.
Acknowledgements
This work was supported by the Natural Sciences and Engineering Research Council of Canada and the
MITACS Elevate program. Thanks to Shibamouli Lahiri and Maite Taboada for the use of their resources
and their feedback.
References
Alina Andreevskaia and Sabine Bergler. 2006. Semantic tag extraction from WordNet glosses. In Proceedings of
5th International Conference on Language Resources and Evaluation (LREC ?06).
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical re-
source for sentiment analysis and opinion mining. In Proceedings of the 7th Conference on International Lan-
guage Resources and Evaluation (LREC?10), Valletta, Malta, May.
Douglas Biber. 1988. Variation Across Speech and Writing. Cambridge University Press.
Julian Brooke and Graeme Hirst. 2013. Hybrid models for lexical acquisition of correlated styles. In Proceedings
of the 6th International Joint Conference on Natural Language Processing (IJCNLP ?13).
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Automatic acquisition of lexical formality. In Proceedings
of the 23rd International Conference on Computational Linguistics (COLING ?10), Beijing.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Media (ICWSM ?09), San Jose, CA.
John Carroll, Guido Minnen, Darren Pearce, Yvonne Canning, Siobhan Devlin, and John Tait. 1999. Simplifying
English text for language impaired readers. In Proceedings of the 9th Conference of the European Chapter of
the Association for Computational Linguistics (EACL?99).
Ilia Chetviorkin and Natalia Loukachevitch. 2012. Extraction of Russian sentiment lexicon for product meta-
domain. In Proceedings of the 24th International Conference on Computational Linguistics (COLING ?12).
Max Coltheart. 1980. MRC Psycholinguistic Database User Manual: Version 1. Birkbeck College.
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. 2013.
A computational approach to politeness with application to social factors. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (ACL ?13).
Gerard de Melo and Mohit Bansal. 2013. Good, great, excellent: Global inference of semantic intensities. Trans-
actions of the Association for Computational Linguistics, 1.
Ahmed Hassan and Dragomir Radev. 2010. Identifying text polarity using random walks. In Proceedings of the
48th Annual Meeting of the Association for Computational Linguistics, (ACL ?10).
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In
Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Confer-
ence of the European Chapter of the Association for Computational Linguistics (ACL/EACL ?97).
2181
S.I. Hayakawa, editor. 1994. Choose the Right Word. HarperCollins Publishers, second edition. Revised by
Eugene Ehrlich.
Francis Heylighen and Jean-Marc Dewaele. 2002. Variation in the contextuality of language: An empirical
measure. Foundations of Science, 7(3):293?340.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ?04).
Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In 9th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ?02).
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection
of HTML documents. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning (EMNLP-CoNLL ?07).
Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten de Rijke. 2004. Using WordNet to measure semantic
orientation of adjectives. In Proceedings of the 4th International Conference on Language Resources and
Evaluation (LREC ?04).
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented senti-
ment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing
(EMNLP ?06).
Maurice Kendall. 1955. Rank Correlation Methods. Hafner.
Paul Kidwell, Guy Lebanon, and Kevyn Collins-Thompson. 2009. Statistical estimation of word acquisition with
application to readability prediction. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP?09).
Soo Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th
International Conference on Computational Linguistics (COLING ?04).
Beata Beigman Klebanov, Nitin Madnani, and Jill Burstein. 2013. Using pivot-based paraphrasing and sentiment
profiles to improve a subjectivity lexicon for essay data. Transactions of the Association for Computational
Linguistics, 1.
Shibamouli Lahiri and Xiaofei Lu. 2011. Inter-rater agreement on sentence formality. http://arxiv.org/abs/1109.
0069 .
Shibamouli Lahiri, Prasenjit Mitra, and Xiaofei Lu. 2011. Informality judgment at sentence level and experiments
with formality score. In Proceedings of the 12th International Conference on Computational Linguistics and
Intelligent Text Processing (CICLing?11).
Zhifei Li and David Yarowsky. 2008. Mining and modeling relations between formal and informal Chinese
phrases from web corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing (EMNLP ?08).
Haiying Li, Arthur. C. Graesser, and Zhiqiang Cai. 2013. Comparing two measures of formality. In Proceedings
of the Twenty-sixth International Florida Artificial Intelligence Research Society Conference.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009. Generating high-coverage semantic orientation lexicons
from overtly marked words and a thesaurus. In Proceedings of the 2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP ?09).
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-the-art
in sentiment analysis of tweets. In Proceedings of the Second Joint Conference on Lexical and Computational
Semantics (SEMSTAR ?13).
Alejandro Mosquera and Paloma Moreda. 2012. A qualitative analysis of informality levels in web 2.0 texts: The
Facebook case study. In Proceedings of the LREC workshop:@NLP can u tag #user generated content, pages
23?29.
Charles E. Osgood, George Suci, and Percy Tannenbaum. 1957. The measurement of meaning. University of
Illinois Press, Urbana, IL.
2182
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011. Email formality in the workplace: A case study on the Enron
corpus. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ?11),
Portland, Oregon.
Delip Rao and Deepak Ravichandra. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the Association for Computational Lingusitics.
Christian Scheible and Hinrich Sch?utze. 2013. Sentiment relevance. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (ACL ?13).
Fadi Abu Sheika and Diana Inkpen. 2012. Learning to classify documents according to formal and informal style.
Linguistic Issues in Language Technology, 8.
Vera Sheinman and Takenobu Tokunaga. 2009. Adjscales: Visualizing differences between adjectives for language
learners. IEICE Transactions on Information and Systems, 92(8):1542?1550.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilivie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press.
Maite Taboada, Caroline Anthony, and Kimberly Voll. 2006. Methods for creating semantic orientation dictionar-
ies. In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC ?06).
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods
for sentiment analysis. Computational Linguistics, 37(2):267?307.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL
?05).
Peter D. Turney and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Systems, 21:315?346.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification
through concrete and abstract context. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP ?11).
Peter D. Turney. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL
?02).
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics (HLT/NAACL ?10).
Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring sentiment in social media: Bootstrap-
ping subjectivity clues from multilingual Twitter streams. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (ACL ?13).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in
Natural Language Processing (HLT/EMNLP ?05).
2183
Proceedings of NAACL-HLT 2013, pages 673?679,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Multi-Dimensional Bayesian Approach to Lexical Style
Julian Brooke
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
We adapt the popular LDA topic model (Blei
et al, 2003) to the representation of stylistic
lexical information, evaluating our model on
the basis of human-interpretability at the word
and text level. We show, in particular, that this
model can be applied to the task of inducing
stylistic lexicons, and that a multi-dimensional
approach is warranted given the correlations
among stylistic dimensions.
1 Introduction
In language, stylistic variation is a reflection of var-
ious contextual factors, including the backgrounds
of and relationship between the parties involved.
Although in the context of prescriptive linguistics
(Strunk and White, 1979), style is often assumed to
be a matter of aesthetics, the stylistic intuitions of
language users are inextricably linked to the conven-
tions of register and genre (Biber and Conrad, 2009).
Intentional or not, stylistic differences play a role
in numerous NLP tasks. Examples include genre
classification (Kessler et al, 1997), author profil-
ing (Garera and Yarowsky, 2009; Rosenthal and Mc-
Keown, 2011), social relationship classification (Pe-
terson et al, 2011), sentiment analysis (Wilson et al,
2005), readability classification (Collins-Thompson
and Callan, 2005), and text generation (Hovy, 1990;
Inkpen and Hirst, 2006). Following the classic work
of Biber (1988), computational modeling of style
has often focused on textual statistics and the fre-
quency of function words and syntactic categories.
When content words are considered, they are of-
ten limited to manually-constructed lists (Argamon
et al, 2007), or used as individual features for su-
pervised classification, which can be confounded by
topic (Petrenz and Webber, 2011) or fail in the face
of lexical variety. Our interest is models that offer
broad lexical coverage of human-identifiable stylis-
tic variation.
Research most similar to ours has focused on clas-
sifying the lexicon in terms of individual aspects rel-
evant to style (e.g. formality, specificity, readability)
(Brooke et al, 2010; Pan and Yang, 2010; Kidwell
et al, 2009) and a large body of research on the in-
duction of polarity lexicons, in particular from large
corpora (Turney, 2002; Kaji and Kitsuregawa, 2007;
Velikovich et al, 2010). Our work is the first to rep-
resent multiple dimensions of style in a single statis-
tical model, adapting latent Dirichlet alocation (Blei
et al, 2003), a Bayesian ?topic? model, to our stylis-
tic purposes; as such, our approach also follows on
recent interest in the interpretability of topic-model
topics (Chang et al, 2009; Newman et al, 2011).
We show that our model can be used for acquisition
of stylistic lexicons, and we also evaluate the model
relative to theories of register variation and the ex-
pected stylistic character of particular genres.
2 Model
2.1 Linguistic foundations
In English manuals of style and other prescriptivist
texts (Fowler and Fowler, 1906; Gunning, 1952;
Follett, 1966; Strunk and White, 1979; Kane, 1983;
Hayakawa, 1994), writers are urged to pay atten-
tion to various aspects of lexical style, including el-
ements such as clarity, familiarity, readability, for-
673
mality, fanciness, colloquialness, specificity, con-
creteness, objectivity, and naturalness; these stylis-
tic categories reflect common aesthetic judgments
about language. In descriptive studies of register,
some researchers have posited a few fixed styles
(Joos, 1961) or a small, discrete set of situational
constraints which determine style and register (Crys-
tal and Davy, 1969; Halliday and Hasan, 1976); by
contrast, the applied approach of Biber (1988) and
theoretical framework of Leckie-Tarry (1995) offer a
more continuous interpretation of register variation.
In Biber?s approach, functional dimensions such
as Involved vs. Informational, Argumentative vs.
Non-argumentative, and Abstract vs. non-Abstract
are derived in an unsupervised manner from a
mixed-genre corpus, with the labels assigned de-
pending on where features (a small set of known in-
dicators of register) and genres fall on each spec-
trum. The theory of Leckie-Tarry posits a single
main cline of register with one pole (the oral pole)
reflecting a full reliance on the context of the lin-
guistic situation, and the other (the literate pole) re-
flecting a reliance on cultural knowledge. The more
specific elements of register are represented as sub-
clines which are strongly influenced by this main
cline, creating probabilistic relationships between
related dimensions (Birch, 1995).
For the present study, we have chosen 3 dimen-
sions (6 styles) which are clearly represented in the
lexicon, which are discussed often in the relevant lit-
erature, and which fit well into the Leckie-Tarry con-
ception of related subclines: colloquial vs. literary,
concrete vs. abstract, and subjective vs. objective. In
addition to a negative correlation between opposing
styles, we also expect a positive correlation between
stylistic aspects that tend toward the same main pole,
situational (i.e. colloquial, concrete, subjective) or
cultural (i.e. literary, abstract, objective). These cor-
relations can potentially interfere with accurate lex-
ical acquisition.
2.2 Implementation
Our main model is an adaption of the popular latent
Dirichlet alocation topic model (Blei et al, 2003),
with each of the 6 styles corresponding to a topic.
Briefly, latent Dirichlet alocation (LDA) is a gener-
ative Bayesian model: for each document d, a dis-
tribution of topics ?d is drawn from a Dirichlet prior
(with parameter ?). For each topic z, there is a prob-
ability distribution ?z1 corresponding to the proba-
bility of that topic generating any given word in the
vocabulary. Words in document d are generated by
first selecting a topic z randomly according to ?d ,
and then randomly selecting a word w according to
?z. An extension of LDA, the correlated topic model
(CTM) (Blei and Lafferty, 2007), supposes a more
complex representation of topics: given a matrix ?
representing the covariance between topics and ?
representing the means, for each document a topic
distribution ? (analogous to ? ) is drawn from the
logistic normal distribution. Given a corpus, good
estimates for the relevant parameters can be derived
using Bayesian inference.
For both LDA and CTM we use the original
variational Bayes implementation of Blei. Varia-
tional Bayes (VB) works by approximating the true
posterior with a simpler distribution, minimizing
the Kullback-Leibler divergence between the two
through iterative updates of specially-introduced
free variables. The mathematical and algorithmic
details are omitted here; see Blei et al (2003; 2007).
Our early investigations used an online, batch ver-
sion of LDA (Hoffman et al, 2010), which is more
appropriate for large corpora because it requires
only a single iteration over the dataset. We discov-
ered, however, that batch models were markedly in-
ferior to more traditional models for our purposes
because the influence of the initial model diminishes
too quickly; here, we need particular topics in the
model to correspond to particular styles, and we ac-
complish this by seeding the model with known in-
stances of each style (see Section 3). Specifically,
our initial ? consists of distributions where the entire
probability mass is divided amongst the seeds for
each corresponding topic, and a full iteration over
the corpus occurs before ? is updated. Typically,
LDA iterates over the corpus until a convergence re-
quirement is met, but in this case this is neither prac-
tical (due to the size of our corpus) nor necessarily
desirable; the diminishing effects of the initial seed-
ing means that the model may not stabilize, in terms
of its likelihood, until after it has shifted away from
our desired stylistic dimensions towards some other
1Some versions of LDA smooth this distribution using a
Dirichlet prior; here, though, we use the original formulation
from Blei (2003), which does not.
674
variation in the data. Therefore, we treat the optimal
number of iterations as a variable to investigate.
The model is trained on a 1 million text por-
tion of the 2009 version of the ICWSM Spinn3r
dataset (Burton et al, 2009), a corpus of blogs we
have previously used for formality lexicon induction
(Brooke et al, 2010). Since our method relies on co-
occurrence, we followed our earlier work in using
only texts with at least 100 different word types. All
words were tokenized and converted to lower-case,
with no further lemmatization. Following Hoffman
et al (2010), we initialized the ? of our models to
1/k where k is the number of topics. Otherwise we
used the default settings; when they overlap they
were identical for the LDA and CTM models.
3 Lexicon Induction
Our primary evaluation is based on the stylistic in-
duction of held-out seed words. The words were
collected from various sources by the first author
and further reviewed by the second; we are both
native speakers of English with significant experi-
ence in English linguistics. Included words had to
be clear, extreme members of their stylistic cate-
gory, with little or no ambiguity with respect to their
style. The colloquial seeds consist of English slang
terms and acronyms, e.g. cuz, gig, asshole, lol. The
literary seeds were primarily drawn from web sites
which explain difficult language in texts such as the
Bible and Lord of the Rings; examples include be-
hold, resplendent, amiss, and thine. The concrete
seeds all denote objects and actions strongly rooted
in the physical world, e.g. shove and lamppost, while
the abstract seeds all involve concepts which require
significant human psychological or cultural knowl-
edge to grasp, for instance patriotism and noncha-
lant. For our subjective seeds, we used an edited
list of strongly positive and negative terms from a
manually-constructed sentiment lexicon (Taboada et
al., 2011), e.g. gorgeous and depraved, and for our
objective set we selected words from sets of near-
synonyms where one was clearly an emotionally-
distant alternative, e.g. residence (for home), jocu-
lar (for funny) and communicable (for contagious).
We filtered initial lists to 150 of each type, remov-
ing words which did not appear in the corpus or
which occurred in multiple lists. For evaluation we
used stratified 3-fold crossvalidation, averaged over
5 different (3-way) splits of the seeds, with the same
splits used for all evaluated conditions.
Given two sets of opposing seeds, we follow our
earlier work in evaluating our performance in terms
of the number of pairings of seeds from each set
which have the expected stylistic relationship rel-
ative to each other (the guessing baseline is 0.5).
Given a word w and two opposing styles (topics) p
and n, we place w on the PN dimension according to
the ? of our trained model as follows:
PNw =
?pw??nw
?pw +?nw
The normalization is important because otherwise
more-common words would tend to have higher
PN?s, when in fact the opposite is true (rare words
tend to be more stylistically prominent). We then
calculate pairwise accuracy as the percentage of
pairs ?wp,wn? (wp ? Pseeds and wn ? Nseeds) where
PNwp >PNwn . However, this metric does not address
the case where the degree of a word in one stylistic
dimension is overestimated because of its status on
a parallel dimension. Two more-holistic alternatives
are total accuracy, the percentage of seeds for which
the highest ?tw is the topic t for which w is a seed
(guessing baseline is 0.17), and the average rank of
the correct t as ordered by ?tw (in the range 1?6,
guessing baseline is 3.5); the latter is more forgiving
of near misses.
We tested a few options which involved straight-
forward modifications to model training. Standard
LDA produces all tokens in the document, but when
dealing with style rather than topic, the number of
times a word appears is much less relevant (Brooke
et al, 2010). Our binary model assumes an LDA
that generates types, not tokens.2 A key comparison
2At the theoretical level, this move is admittedly problem-
atic, since our LDA model is thus being trained under the as-
sumption that texts with multiple instances of the same type can
be generated, when of course such texts cannot by definition ex-
ist. We might address this by moving to Bayesian models with
very different generative assumptions, e.g. the spherical topic
model (Reisinger et al, 2010), but these methods involve a sig-
nificant increase of computational complexity and we believe
that on a practical level there are no real negatives associated
with directly using a binary representation as input to LDA; in
fact, we are avoiding what appears to be a much more serious
problem, burstiness (Doyle and Elkan, 2009), i.e. the fact that
675
Model
Pairwise Accuracy (%)
Total Acc. (%) Avg. Rank
Lit/Col Abs/Con Obj/Sub All
guessing baseline 50.0 50.0 50.0 50.0 16.6. 3.50
basic LDA (iter 2) 94.3 98.8 93.0 95.4 55.0 1.79
binary LDA (iter 2) 96.2 98.9 93.5 96.2 57.7 1.74
combo binary LDA (iter 1) 95.4 99.2 93.3 96.0 53.1 1.86
binary CTM (iter 1) 96.3 99.0 89.6 95.0 53.0 1.87
Table 1: Model performance in lexical induction of seeds. Bold indicates best in column.
here is with a combined LDA model (combo), an
amalgamation of three independently trained 2-topic
models, one for each dimension; this tests our key
hypothesis that training dimensions of style together
is beneficial. Finally, we test against the correlated
topic model (CTM), which offers an explicit repre-
sentation of style correlation, but which has done
poorly with respect to interpretability, despite offer-
ing better perplexity (Chang et al, 2009).
The results of the lexicon induction evaluation
are in Table 1. Since the number of optimal iter-
ations varies, we report the result from the best of
the first five iterations, as measured by total accu-
racy; the best iteration is shown in parenthesis. In
general, all the results are high enough?we are re-
liably above 90% for the pairwise task, and above
50% for the 6-way task?for us to conclude with
some confidence that our model is capturing a sig-
nificant amount of stylistic variation. As predicted,
using words as boolean features had a net positive
gain, consistent across all of our metrics, though this
effect was not as marked as we have seen previously.
The model with independent training of each dimen-
sion (combo) did noticeably worse, supporting our
conclusion that a multidimensional approach is war-
ranted here. Particularly striking is the much larger
drop in overall accuracy as compared to pairwise ac-
curacy, which suggests that the combo model is cap-
turing the general trends but not distinguishing cor-
related styles as well. However, the most complex
model, the CTM, actually does slightly worse than
the combo, which was contrary to our expectations
but nonetheless consistent with previous work on the
interpretability of topic models. The performance of
the full LDA models benefited from a second itera-
traditional LDA is influenced too much by multiple instances of
the same word.
tion, but this was not true of combo LDA or CTM,
and the performance of all models dropped after the
second iteration.
An analysis of individual errors reveals, unsur-
prisingly, that most of the errors occur across styles
on the same pole; by far the largest single com-
mon misclassification is objective words to abstract.
Of the words that consistently show this misclas-
sification across the runs, many of them, e.g. ani-
mate, aperture, encircle, and constrain are clearly
errors (if anything, these words tend towards con-
creteness), but in other cases the word in question
is arguably also fairly abstract, e.g. categorize and
predominant, and might not be labeled an error at
all. Other signs that our model might be doing bet-
ter than our total accuracy metric gives it credit for:
many of the subjective words that are consistently
mislabeled as literary have an exaggerated, literary
feel, e.g. jubilant, grievous, and malevolent.
4 Text-level Analysis
Our secondary analysis involved evaluating the ? ?s
of our best configuration (based on average pairwise
and total accuracy) on other texts. After training,
we carried out inference on the BNC corpus, aver-
aging the resulting ? ?s to see which styles are asso-
ciated with which genres. Appearances of the seed
terms for each model were disregarded during this
process; only the induced part of the lexicon was
used. The average differences relative to the mean
across the various stylistic dimensions (as measured
by the probabilities in ? ) are given for a selection of
genres in Table 2.
The most obvious pattern in table 2 is the domi-
nance of the medium: all written genres are positive
for our styles on the ?cultural? pole and negative for
styles on the ?situational? pole and the opposite is
676
Genre
Styles
Literary Abstract Objective Colloquial Concrete Subjective
News +0.67 +0.50 +0.43 ?0.31 ?0.72 ?0.57
Religious texts +0.38 +0.38 +0.28 ?0.27 ?0.44 ?0.32
Academic +0.18 +0.29 +0.26 ?0.20 ?0.36 ?0.18
Fiction +0.31 +0.09 +0.02 ?0.05 ?0.12 ?0.25
Meeting ?0.61 ?0.54 ?0.42 +0.35 +0.69 +0.55
Courtroom ?0.63 ?0.53 ?0.41 +0.32 +0.69 +0.57
Conversation ?0.56 ?0.63 ?0.54 +0.43 +0.80 +0.50
Table 2: Average differences from corpus mean of LDA-derived stylistic dimension probabilities for various genres in
the BNC, in hundredths.
true for spoken genres. The magnitude of this ef-
fect is more difficult to interpret: though it is clear
why fiction should sit on the boundary (since it con-
tains spoken dialogue), the appearance of news at
the written extreme is odd, though it might be due to
the fact that news blogs are the most prevalent for-
mal genre in the training corpus.
However, if we ignore magnitude and focus on the
relative ratios of the stylistic differences for styles
on the same pole, we can identify some individ-
ual stylistic effects among genres within the same
medium. Relative to the other written genres, for in-
stance, fiction is, sensibly, more literary and much
less objective, while academic texts are much more
abstract and objective; for the other two written gen-
res, the spread is more even, though relative to re-
ligious texts, news is more objective. At the sit-
uational pole, fiction also stands out, being much
more colloquial and concrete than other written gen-
res. Predictably, if we consider again the ratios
across styles, conversation is the most colloquial
genre here, though the difference is subtle.
We carried out a correlation analysis of the LDA-
reduced styles of all texts in the BNC and, con-
sistent with the genre results in Table 2, found a
strong positive correlation for all styles on the same
main pole, averaging 0.83. The average negative
correlation between opposing poles is even higher,
?0.88. This supports the Leckie-Tarry formulation.
The independence assumptions of the LDA model
did not prevent strong correlations from forming be-
tween these distinct yet clearly interrelated dimen-
sions; if anything, the correlations are stronger than
we would have predicted.
5 Conclusion
We have introduced a Bayesian model of stylistic
variation. Topic models like LDA are often evalu-
ated using information-theoretic measures, but our
emphasis has been on interpretibility: at the word
level we can use the model to induce stylistic lex-
icons which correspond to human judgement, and
at the text level we can use it distinguish genres in
expected ways. Another theme has been to offer ev-
idence that indeed a multi-dimensional approach is
strongly warranted: importantly, our results indicate
that separate unidimensional models of style are in-
ferior for identifying the core stylistic character of
each word, and in our secondary analysis we found
strong correlations among styles attributable to the
situational/cultural dichotomy. However, an off-the-
shelf model that integrates correlation among topics
did not outperform basic LDA.
One advantage of a Bayesian approach is in the
flexibility of the model: there are any number of
other interesting possible extensions at both the ?
and ? levels of the model, including alternative ap-
proaches to correlation (Li and McCallum, 2006).
Beyond Bayesian models, vector space and graphi-
cal approaches should be compared. More work is
clearly needed to improve evaluation: some of our
seeds could fall into multiple stylistic categories, so
a more detailed annotation would be useful.
Acknowledgements
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
677
References
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features. Journal of the American Society for In-
formation Science and Technology, 7:91?109.
Douglas Biber and Susan Conrad. 2009. Register, Genre,
and Style. Cambridge University Press.
Douglas Biber. 1988. Variation Across Speech and Writ-
ing. Cambridge University Press.
David Birch. 1995. Introduction. In Helen Leckie-Tarry,
editor, Language and Context: A Functional Linguis-
tic Theory of Register. Pinter.
David M. Blei and John D. Lafferty. 2007. Correlated
topic models. Annals of Applied Statistics, 1(1):17?
35.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet alocation.
Journal of Machine Learning Research, 3:993?1022.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of Neural Information Processing Systems
(NIPS ?09).
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science Technology, 56(13):1448?1462.
David Crystal and Derek Davy. 1969. Investigating En-
glish Style. Indiana University Press.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In International Confer-
ence on Machine Learning (ICML ?09).
Wilson Follett. 1966. Modern American Usage. Hill &
Wang, New York.
H. W. Fowler and F. G. Fowler. 1906. The King?s En-
glish. Clarendon Press, Oxford, 2nd edition.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP ?09), pages 710?718, Sin-
gapore.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill, New York.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman, London.
S.I. Hayakawa, editor. 1994. Choose the Right Word.
HarperCollins Publishers, second edition. Revised by
Eugene Ehrlich.
Matthew D. Hoffman, David M. Blei, and Francis R.
Bach. 2010. Online learning for latent Dirichlet al
location. In Neural Information Processing Systems
(NIPS ?10), pages 856?864.
Eduard H. Hovy. 1990. Pragmatics and natural language
generation. Artificial Intelligence, 43:153?197.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym dif-
ferences. Computational Linguistics, 32(2):223?262.
Martin Joos. 1961. The Five Clocks. Harcourt, Brace
and World, New York.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of HTML documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL ?07).
Thomas S. Kane. 1983. The Oxford Guide to Writing.
Oxford Univeristy Press.
Brett Kessler, Geoffrey Nunberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL ?97), pages 32?
38, Madrid, Spain.
Paul Kidwell, Guy Lebanon, and Kevyn Collins-
Thompson. 2009. Statistical estimation of word
acquisition with application to readability predic-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP?09), pages 900?909, Singapore.
Helen Leckie-Tarry. 1995. Language and Context: A
Functional Linguistic Theory of Register. Pinter.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In Proceedings of the 23rd International Con-
ference on Machine Learning, ICML ?06, pages 577?
584.
David Newman, Edwin V. Bonilla, and Wray Buntine.
2011. Improving topic coherence with regularized
topic models. In Proceedings of Advances in Neural
Information Processing Systems (NIPS ?11).
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10).
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study on
678
the Enron corpus. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?11), Portland, Oregon.
Philipp Petrenz and Bonnie Webber. 2011. Stable clas-
sification of text genres. Computational Linguistics,
37(2):385?393, June.
J. Reisinger, A. Waters, B. Silverthorn, and R. Mooney.
2010. Spherical topic models. In International Con-
ference on Machine Learning (ICML ?10).
Sara Rosenthal and Kathleen McKeown. 2011. Age pre-
diction in blogs: A study of style, content, and online
behavior in pre- and post-social media generations. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11), Port-
land, Oregon.
William Strunk and E.B. White. 1979. The Elements of
Style. Macmillan, 3rd edition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?02, pages 417?424, Philadelphia, Pennsyl-
vania.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 777?785, Los An-
geles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT/EMNLP
?05, pages 347?354.
679
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 33?39,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Building Readability Lexicons with Unannotated Corpora
Julian Brooke* Vivian Tsang? David Jacob? Fraser Shein*? Graeme Hirst*
*Department of Computer Science
University of Toronto
{jbrooke,gh}@cs.toronto.edu
?Quillsoft Ltd.
Toronto, Canada
{vtsang, djacob, fshein}@quillsoft.ca
Abstract
Lexicons of word difficulty are useful for var-
ious educational applications, including read-
ability classification and text simplification. In
this work, we explore automatic creation of
these lexicons using methods which go beyond
simple term frequency, but without relying on
age-graded texts. In particular, we derive infor-
mation for each word type from the readability
of the web documents they appear in and the
words they co-occur with, linearly combining
these various features. We show the efficacy of
this approach by comparing our lexicon with an
existing coarse-grained, low-coverage resource
and a new crowdsourced annotation.
1 Introduction
With its goal of identifying documents appropriate
to readers of various proficiencies, automatic anal-
ysis of readability is typically approached as a text-
level classification task. Although at least one pop-
ular readability metric (Dale and Chall, 1995) and
a number of machine learning approaches to read-
ability rely on lexical features (Si and Callan, 2001;
Collins-Thompson and Callan, 2005; Heilman et al,
2007; Petersen and Ostendorf, 2009; Tanaka-Ishii et
al., 2010), the readability of individual lexical items
is not addressed directly in these approaches. Nev-
ertheless, information about the difficulty of individ-
ual lexical items, in addition to being useful for text
readability classification (Kidwell et al, 2009), can
be applied to other tasks, for instance lexical simpli-
fication (Carroll et al, 1999; Burstein et al, 2007).
Our interest is in providing students with educa-
tional software that is sensitive to the difficulty of
particular English expressions, providing proactive
support for those which are likely to be outside a
reader?s vocabulary. However, our existing lexical
resource is coarse-grained and lacks coverage. In
this paper, we explore the extent to which an auto-
matic approach could be used to fill in the gaps of
our lexicon. Prior approaches have generally de-
pended on some kind of age-graded corpus (Kid-
well et al, 2009; Li and Feng, 2011), but this kind
of resource is unlikely to provide the coverage that
we require; instead, our methods here are based on
statistics from a huge web corpus. We show that
frequency, an obvious proxy for difficulty, is only
the first step; in fact we can derive key information
from the documents that words appear in and the
words that they appear with, information that can be
combined to give high performance in identifying
relative difficulty. We compare our automated lexi-
con against our existing resource as well as a crowd-
sourced annotation.
2 Related Work
Simple metrics form the basis of much readability
work: most involve linear combinations of word
length, syllable count, and sentence length (Kincaid
et al, 1975; Gunning, 1952), though the popular
Dale-Chall reading score (Dale and Chall, 1995) is
based on a list of 3000 ?easy? words; a recent re-
view suggests these metrics are fairly interchange-
able (van Oosten et al, 2010). In machine-learning
classification of texts by grade level, unigrams have
been found to be reasonably effective for this task,
outperforming readability metrics (Si and Callan,
2001; Collins-Thompson and Callan, 2005). Var-
33
ious other features have been explored, including
parse (Petersen and Ostendorf, 2009) and coherence
features (Feng et al, 2009), but the consensus seems
to be that lexical features are the most consistently
useful for automatic readability classification, even
when considering non-native readers (Heilman et
al., 2007).
In the field of readability, the work of Kidwell et
al. (2009) is perhaps closest to ours. Like the above,
their goal is text readability classification, but they
proceed by first deriving an age of acquisition for
each word based on its statistical distribution in age-
annotated texts. Also similar is the work of Li and
Feng (2011), who are critical of raw frequency as an
indicator and instead identify core vocabulary based
on the common use of words across different age
groups. With respect to our goal of lowering reliance
on fine-grained annotation, the work of Tanaka-Ishii
et al (2010) is also relevant; they create a readability
system that requires only two general classes of text
(easy and difficult), other texts are ranked relative to
these two classes using regression.
Other lexical acquisition work has also informed
our approach here. For instance, our co-occurrence
method is an adaption of a technique applied in
sentiment analysis (Turney and Littman, 2003),
which has recently been shown to work for formal-
ity (Brooke et al, 2010), a dimension of stylistic
variation that seems closely related to readability.
Taboada et al (2011) validate their sentiment lex-
icon using crowdsourced judgments of the relative
polarity of pairs of words, and in fact crowd sourcing
has been applied directly to the creation of emotion
lexicons (Mohammad and Turney, 2010).
3 Resources
Our primary resource is an existing lexicon, pre-
viously built under the supervision of the one of
authors. This resource, which we will refer to
as the Difficulty lexicon, consists of 15,308 words
and expressions classified into three difficulty cate-
gories: beginner, intermediate, and advanced. Be-
ginner, which was intended to capture the vocabu-
lary of early elementary school, is an amalgamation
of various smaller sources, including the Dolch list
(Dolch, 1948). The intermediate words, which in-
clude words learned in late elementary and middle
Table 1: Examples from the Difficulty lexicon
Beginner
coat, away, arrow, lizard, afternoon, rainy,
carpet, earn, hear, chill
Intermediate
bale, campground, motto, intestine, survey,
regularly, research, conflict
Advanced
contingency, scoff, characteristic, potent, myriad,
detracted, illegitimate, overture
school, were extracted from Internet-published texts
written by students at these grade levels, and then fil-
tered manually. The advanced words began as a list
of common words that were in neither of the origi-
nal two lists, but they have also been manually fil-
tered; they are intended to reflect the vocabulary un-
derstood by the average high school student. Table
1 contains some examples from each list.
For our purposes here, we only use a subset of the
Difficulty lexicon: we filtered out inflected forms,
proper nouns, and words with non-alphabetic com-
ponents (including multiword expressions) and then
randomly selected 500 words from each level for
our test set and 300 different words for our develop-
ment/training set. Rather than trying to duplicate our
arbitrary three-way distinction by manual or crowd-
sourced means, we instead focused on the relative
difficulty of individual words: for each word in each
of the two sets, we randomly selected three compar-
ison words, one from each of the difficulty levels,
forming a set of 4500 test pairs (2700 for the de-
velopment set): 1/3 of these pairs are words from
the same difficulty level, 4/9 are from adjacent dif-
ficulty levels, and the remaining 2/9 are at opposite
ends of our difficulty spectrum.
Our crowdsourced annotation was obtained using
Crowdflower, which is an interface built on top of
Mechanical Turk. For each word pair to be com-
pared, we elicited 5 judgments from workers. Rather
than frame the question in terms of difficulty or read-
ability, which we felt was too subjective, we instead
asked which of the two words the worker thought
he or she learned first: the worker could choose ei-
ther word, or answer ?about the same time?. They
34
were instructed to choose the word they did know if
one of the two words was unknown, and ?same? if
both were unknown. For our evaluation, we took the
majority judgment as the gold standard; when there
was no majority judgment, then the words were con-
sidered ?the same?. To increase the likelihood that
our workers were native speakers of English, we
required that the responses come from the US or
Canada. Before running our main set, we ran sev-
eral smaller test runs and manually inspected them
for quality; although there were outliers, the major-
ity of the judgments seemed reasonable.
Our corpus is the ICWSM Spinn3r 2009 dataset
(Burton et al, 2009). We chose this corpus because
it was used by Brooke et al (2010) to derive a lexi-
con of formality; they found that it was more effec-
tive for these purposes than smaller mixed-register
corpora like the BNC. The ICWSM 2009, collected
over several weeks in 2008, contains about 7.5 mil-
lion blogs, or 1.3 billion tokens, including well over
a million word types (more than 200,000 of which
which appear at least 10 times). We use only the
documents which have at least 100 tokens. The cor-
pus has been tagged using the TreeTagger (Schmid,
1995).
4 Automatic Lexicon Creation
Our method for lexicon creation involves first ex-
tracting a set of relevant numerical features for each
word type. We can consider each feature as defin-
ing a lexicon on its own, which can be evaluated us-
ing our test set. Our features can be roughly broken
into three types: simple features, document readabil-
ity features, and co-occurrence features. The first of
these types does not require much explanation: it in-
cludes the length of the word, measured in terms of
letters and syllables (the latter is derived using a sim-
ple but reasonably accurate vowel-consonant heuris-
tic), and the log frequency count in our corpus.1
The second feature type involves calculating sim-
ple readability metrics for each document in our cor-
pus, and then defining the relevant feature for the
word type as the average value of the metric for all
the documents that the word appears in. For exam-
1Though it is irrelevant when evaluating the feature alone,
the log frequency was noticeably better when combining fre-
quency with other features.
ple, if Dw is the set of documents where word type
w appears and di is the ith word in a document d,
then the document word length (DWL) for w can be
defined as follows:
DWL(w) = |Dw|
?1 ?
d?Dw
?
|d|
i=0 length(di)
|d|
Other features calculated in this way include: the
document sentence length, that is the average token
length of sentences; the document type-token ratio2;
and the document lexical density, the ratio of content
words (nouns, verbs, adjectives, and adverbs) to all
words.
The co-occurence features are inspired by the
semi-supervised polarity lexicon creation method of
Turney and Littman (2003). The first step is to build
a matrix consisting of each word type and the docu-
ments it appears in; here, we use a binary representa-
tion, since the frequency with which a word appears
in a particular document does not seem directly rel-
evant to readability. We also do not remove tradi-
tional stopwords, since we believe that the use of
certain common function words can in fact be good
indicators of text readability. Once the matrix is
built, we apply latent semantic analysis (Landauer
and Dumais, 1997); we omit the mathematical de-
tails here, but the result is a dimensionality reduc-
tion such that each word is represented as a vector
of some k dimensions. Next, we select two sets of
seed words (P and N) which will represent the ends
of the spectrum which we are interested in deriving.
We derive a feature value V for each word by sum-
ming the cosine similarity of the word vector with
all the seeds:
V (w) =
?p?P cos(?(w,p))
|P|
?
?n?N cos(?(w,n))
|N|
We further normalize this to a range of 1 to
?1, centered around the core vocabulary word and.
Here, we try three possible versions of P and N: the
first, Formality, is the set of words used by Brooke
et al (2010) in their study of formality, that is, a
2We calculate this using only the first 100 words of the docu-
ment, to avoid the well-documented influence of length on TTR.
35
set of slang and other markers of oral communica-
tion as N, and a set of formal discourse markers and
adverbs as P, with about 100 of each. The second,
Childish, is a set of 10 common ?childish? concrete
words (e.g. mommy, puppy) as N, and a set of 10
common abstract words (e.g. concept, philosophy)
as P. The third, Difficulty, consists of the 300 begin-
ner words from our development set as N, and the
300 advanced words from our development set as P.
We tested several values of k for each of the seed
sets (from 20 to 500); there was only small variation
so here we just present our best results for each set
as determined by testing in the development set.
Our final lexicon is created by taking a linear
combination of the various features. We can find an
appropriate weighting of each term by taking them
from a model built using our development set. We
test two versions of this: by default, we use a linear
regression model where for training beginner words
are tagged as 0, advanced words as 1, and intermedi-
ate words as 0.5. The second model is a binary SVM
classifier; the features of the model are the differ-
ence between the respective features for each of the
two words, and the classifier predicts whether the
first or second word is more difficult. Both models
were built using WEKA (Witten and Frank, 2005),
with default settings except for feature normaliza-
tion, which must be disabled in the SVM to get use-
ful weights for the linear combination which creates
our lexicon. In practice, we would further normalize
our lexicon; here, however, this normalization is not
relevant since our evaluation is based entirely on rel-
ative judgments. We also tested a range of other ma-
chine learning algorithms available in WEKA (e.g.
decision trees and MaxEnt) but the crossvalidated
accuracy was similar to or slightly lower than using
a linear classifier.
5 Evaluation
All results are based on comparing the relative dif-
ficulty judgments made for the word pairs in our
test set (or, more often, some subset) by the various
sources. Since even the existing Difficulty lexicon is
not entirely reliable, we report agreement rather than
accuracy. Except for agreement of Crowdflower
workers, agreement is the percentage of pairs where
the sources agreed as compared to the total num-
ber of pairs. For agreement between Crowdflower
workers, we follow Taboada et al (2011) in calcu-
lating agreement across all possible pairings of each
worker for each pair. Although we considered using
a more complex metric such as Kappa, we believe
that simple pairwise agreement is in fact equally in-
terpretable when the main interest is relative agree-
ment of various methods; besides, Kappa is intended
for use with individual annotators with particular bi-
ases, an assumption which does not hold here.
To evaluate the reliability of our human-annotated
resources, we look first at the agreement within the
Crowdflower data, and between the Crowdflower
and our Difficulty lexicon, with particular attention
to within-class judgments. We then compare the
predictions of various automatically extracted fea-
tures and feature combinations with these human
judgments; since most of these involve a continuous
scale, we focus only on words which were judged to
be different.3 For the Difficulty lexicon (Diff.), the
n in this comparison is 3000, while for the Crowd-
flower (CF) judgments it is 4002.
6 Results
We expect a certain amount of noise using crowd-
sourced data, and indeed agreement among Crowd-
flower workers was not extremely high, only 56.6%
for a three-way choice; note, however, that in these
circumstances a single worker disagreeing with the
rest will drop pairwise agreement in that judgement
to 60%.4 Tellingly, average agreement was rela-
tively high (72.5%) for words on the extremes of our
difficulty spectrum, and low for words in the same
difficulty category (46.0%), which is what we would
expect. As noted by Taboada et al (2011), when
faced with a pairwise comparison task, workers tend
to avoid the ?same? option; instead, the proximity of
the words on the underlying spectrum is reflected in
disagreement. When we compare the crowdsourced
judgements directly to the Difficulty lexicon, base
3A continuous scale will nearly always predict some differ-
ence between two words. An obvious approach would be to set
a threshold within which two words will be judged the same,
but the specific values depend greatly on the scale and for sim-
plicity we do not address this problem here.
4In 87.3% of cases, at least 3 workers agreed; in 56.2% of
cases, 4 workers agreed, and in 23.1% of cases all 5 workers
agreed.
36
agreement is 63.1%. This is much higher than
chance, but lower than we would like, considering
these are two human-annotated sources. However,
it is clear that much of this disagreement is due to
?same? judgments, which are three times more com-
mon in the Difficulty lexicon-based judgments than
in the Crowdflower judgments (even when disagree-
ment is interpreted as a ?same? judgment). Pairwise
agreement of non-?same? judgments for word pairs
which are in the same category in the Difficultly lex-
icon is high enough (45.9%)5 for us to conclude that
this is not random variation, strongly suggesting that
there are important distinctions within our difficulty
categories, i.e. that it is not sufficiently fine-grained.
If we disregard all words that are judged as same in
one (or both) of the two sources, the agreement of
the resulting word pairs is 91.0%, which is reason-
ably high.
Table 2 contains the agreement when feature val-
ues or a linear combination of feature values are used
to predict the readability of the unequal pairs from
the two manual sources. First, we notice that the
Crowdflower set is obviously more difficult, proba-
bly because it contains more pairs with fairly subtle
(though noticeable) distinctions. Other clear differ-
ences between the annotations: whereas for Crowd-
flower frequency is the key indicator, this is not true
for our original annotation, which prefers the more
complex features we have introduced here. A few
features did poorly in general: syllable count ap-
pears too coarse-grained to be useful on its own,
lexical density is only just better than chance, and
type-token ratio performs at or below chance. Oth-
erwise, many of the features within our major types
give roughly the same performance individually.
When we combine features, we find that simple
and document features combine to positive effect,
but the co-occurrence features are redundant with
each other and, for the most part, the document fea-
tures. A major boost comes, however, from combin-
ing either document or co-occurrence features with
the simple features; this is especially true for our
Difficulty lexicon annotation, where the gain is 7%
to 8 percentage points. It does not seem to matter
very much whether the weights of each feature are
determined by pairwise classifier or by linear regres-
5Random agreement here is 33.3%.
Table 2: Agreement (%) of automated methods with man-
ual resources on pairwise comparison task (Diff. = Diffi-
culty lexicon, CF = Crowdflower)
Features
Resource
Diff. CF
Simple
Syllable length 62.5 54.9
Word length 68.8 62.4
Term frequency 69.2 70.7
Document
Avg. word length 74.5 66.8
Avg. sentence length 73.5 65.9
Avg. type-token ratio 47.0 50.0
Avg. lexical density 56.1 54.7
Co-occurrence
Formality 74.7 66.5
Childish 74.2 65.5
Difficulty 75.7 66.1
Linear Combinations
Simple 79.3 75.0
Document 80.1 70.8
Co-occurrence 76.0 67.0
Document+Co-occurrence 80.4 70.2
Simple+Document 87.5 79.1
Simple+Co-occurrence 86.7 78.2
All 87.6 79.5
All (SVM) 87.1 79.2
sion: this is interesting because it means we can train
a model to create a readability spectrum with only
pairwise judgments. Finally, we took all the 2500
instances where our two annotations agreed that one
word was more difficult, and tested our best model
against only those pairs. Results using this selec-
tive test set were, unsurprisingly, higher than those
of either of the annotations alone: 91.2%, which is
roughly the same as the original agreement between
the two manual annotations.
7 Discussion
Word difficulty is a vague concept, and we have ad-
mittedly sidestepped a proper definition here: in-
stead, we hope to establish a measure of reliabil-
ity in judgments of ?lexical readability? by looking
for agreement across diverse sources of informa-
tion. Our comparison of our existing resources with
37
crowdsourced judgments suggests that some consis-
tency is possible, but that granularity is, as we pre-
dicted, a serious concern, one which ultimately un-
dermines our validation to some degree. An auto-
matically derived lexicon, which can be fully con-
tinuous or as coarse-grained as needed, seems like
an ideal solution, though the much lower perfor-
mance of the automatic lexicon in predicting the
more fine-grained Crowdflower judgments indicates
that automatically-derived features are limited in
their ability to deal with subtle differences. How-
ever, a visual inspection of the spectrum created by
the automatic methods suggests that, with a judi-
cious choice of granularity, it should be sufficient for
our needs. In future work, we also intend to evalu-
ate its use for readability classification, and perhaps
expand it to include multiword expressions and syn-
tactic patterns.
Our results clearly show the benefit of combin-
ing multiple sources of information to build a model
of word difficulty. Word frequency and word length
are of course relevant, and the utility of the docu-
ment context features is not surprising, since they
are merely a novel extension of existing proxies
for readability. The co-occurrence features were
also useful, though they seem fairly redundant and
slightly inferior to document features; we posit that
these features, in addition to capturing notions of
register such as formality, may also offer seman-
tic distinctions relevant to the acquisition process.
For instance, children may have a large vocabulary
in very concrete domains such as animals, includ-
ing words (e.g. lizard) that are not particularly fre-
quent in adult corpora, while very common words in
other domains (such as the legal domain) are com-
pletely outside the range of their experience. If we
look at some of the examples which term frequency
alone does not predict, they seem to be very much
of this sort: dollhouse/emergence, skirt/industry,
magic/system. Unsupervised techniques for identi-
fying semantic variation, such as LSA, can capture
these sorts of distinctions. However, our results indi-
cate that simply looking at the readability of the texts
that these sort of words appear in (i.e. our document
features) is mostly sufficient, and less than 10% of
the pairs which are correctly ordered by these two
feature sets are different. In any case, an age-graded
corpus is definitely not required.
There are a few other benefits of using word co-
occurrence that we would like to touch on, though
we leave a full exploration for future work. First, if
we consider readability in other languages, each lan-
guage may have different properties which render
proxies such as word length much less useful (e.g.
ideographic languages like Chinese or agglutinative
languages like Turkish). However, word (or lemma)
co-occurrence, like frequency, is essentially a uni-
versal feature across languages, and thus can be di-
rectly extended to any language. Second, if we con-
sider how we would extend difficulty-lexicon cre-
ation to the context of adult second-language learn-
ers, it might be enough to adjust our seed terms to
reflect the differences in the language exposure of
this population, i.e. we would expect difficulty in ac-
quiring colloquialisms that are typically learned in
childhood but are not part of the core vocabulary of
the adult language.
8 Conclusion
In this paper, we have presented an automatic
method for the derivation of a readability lexicon re-
lying only on an unannotated word corpus. Our re-
sults show that although term frequency is a key fea-
ture, there are other, more complex features which
provide competitive results on their own as well as
combining with term frequency to improve agree-
ment with manual resources that reflect word diffi-
culty or age of acquisition. By comparing our man-
ual lexicon with a new crowdsourced annotation, we
also provide a validation of the resource, while at
the same time highlighting a known issue, the lack
of fine-grainedness. Our manual lexicon provides a
solution for this problem, albeit at the cost of some
reliability. Although our immediate interest is not
text readability classification, the information de-
rived could be applied fairly directly to this task, and
might be particularly useful in the case when anno-
tated texts are not avaliable.
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
38
References
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10).
Jill Burstein, Jane Shore, John Sabatini, Yong-Won Lee,
and Matthew Ventura. 2007. The automated text
adaptation tool. In Proceedings of the Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics (NAACL ?07), Software
Demonstrations, pages 3?4.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999. Sim-
plifying English text for language impaired readers.
In Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL?99), pages 269?270.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science Technology, 56(13):1448?1462.
Edgar Dale and Jeanne Chall. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline Books, Cambridge, MA.
Edward William Dolch. 1948. Problems in Reading.
The Garrard Press.
Lijun Feng, Noe?mie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL ?09), pages 229?237.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill.
Michael J. Heilman, Kevyn Collins, and Jamie Callan.
2007. Combining lexical and grammatical features to
improve readability measures for first and second lan-
guage texts. In Proceedings of the Conference of the
North American Chapter of Association for Computa-
tional Linguistics (NAACL-HLT ?07).
Paul Kidwell, Guy Lebanon, and Kevyn Collins-
Thompson. 2009. Statistical estimation of word
acquisition with application to readability predic-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP?09), pages 900?909.
J. Peter Kincaid, Robert. P. Fishburne Jr., Richard L.
Rogers, and Brad. S. Chissom. 1975. Derivation of
new readability formulas for Navy enlisted personnel.
Research Branch Report 8-75, Millington, TN: Naval
Technical Training, U. S. Naval Air Station, Memphis,
TN.
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Hanhong Li and Alex C. Feng. 2011. Age tagging
and word frequency for learners? dictionaries. In Har-
ald Baayan John Newman and Sally Rice, editors,
Corpus-based Studies in Language Use, Language
Learning, and Language Documentation. Rodopi.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using Me-
chanical Turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26?34, Los Angeles.
Sarah E. Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter Speech and Language, 23(1):89?106.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop, pages 47?50.
Luo Si and Jamie Callan. 2001. A statistical model
for scientific readability. In Proceedings of the Tenth
International Conference on Information and Knowl-
edge Management (CIKM ?01), pages 574?576.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manifred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-
ada. 2010. Sorting texts by readability. Computa-
tional Linguistics, 36(2):203?227.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Philip van Oosten, Dries Tanghe, and Veronique Hoste.
2010. Towards an improved methodology for auto-
mated readability prediction. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation (LREC ?10).
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco.
39
Workshop on Computational Linguistics for Literature, pages 26?35,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Stylistic Segmentation of Poetry
with Change Curves and Extrinsic Features
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
The identification of stylistic inconsistency is a
challenging task relevant to a number of gen-
res, including literature. In this work, we
carry out stylistic segmentation of a well-known
poem, The Waste Land by T.S. Eliot, which
is traditionally analyzed in terms of numerous
voices which appear throughout the text. Our
method, adapted from work in topic segmen-
tation and plagiarism detection, predicts breaks
based on a curve of stylistic change which com-
bines information from a diverse set of features,
most notably co-occurrence in larger corpora via
reduced-dimensionality vectors. We show that
this extrinsic information is more useful than
(within-text) distributional features. We achieve
well above baseline performance on both artifi-
cial mixed-style texts and The Waste Land itself.
1 Introduction
Most work in automated stylistic analysis operates
at the level of a text, assuming that a text is stylis-
tically homogeneous. However, there are a number
of instances where that assumption is unwarranted.
One example is documents collaboratively created
by multiple authors, in which contributors may, ei-
ther inadvertently or deliberately (e.g. Wikipedia
vandalism), create text which fails to form a stylis-
tically coherent whole. Similarly, stylistic incon-
sistency might also arise when one of the ?contrib-
utors? is actually not one of the purported authors
of the work at all ? that is, in cases of plagia-
rism. More-deliberate forms of stylistic dissonance
include satire, which may first follow and then flout
the stylistic norms of a genre, and much narrative lit-
erature, in which the author may give the speech or
thought patterns of a particular character their own
style distinct from that of the narrator. In this paper,
we address this last source of heterogeneity in the
context of the well-known poem The Waste Land by
T.S. Eliot, which is often analyzed in terms of the
distinct voices that appear throughout the text.
T.S. Eliot (1888?1965), recipient of the 1948 No-
bel Prize for Literature, is among the most important
twentieth-century writers in the English language.
Though he worked in a variety of forms ? he was
a celebrated critic as well as a dramatist, receiving
a Tony Award in 1950 ? he is best remembered to-
day for his poems, of which The Waste Land (1922)
is among the most famous. The poem deals with
themes of spiritual death and rebirth. It is notable
for its disjunctive structure, its syncopated rhythms,
its wide range of literary allusions, and its incorpo-
ration of numerous other languages. The poem is di-
vided into five parts; in total it is 433 lines long, and
contains 3533 tokens, not including the headings.
A prominent debate among scholars of The Waste
Land concerns whether a single speaker?s voice pre-
dominates in the poem (Bedient, 1986), or whether
the poem should be regarded instead as dramatic
or operatic in structure, composed of about twelve
different voices independent of a single speaker
(Cooper, 1987). Eliot himself, in his notes to The
Waste Land, supports the latter view by referring to
?characters? and ?personage[s]? in the poem.
One of the poem?s most distinctive voices is that
of the woman who speaks at the end of its second
section:
26
I can?t help it, she said, pulling a long face,
It?s them pills I took, to bring it off, she said
[158?159]
Her chatty tone and colloquial grammar and lexis
distinguish her voice from many others in the poem,
such as the formal and traditionally poetic voice of a
narrator that recurs many times in the poem:
Above the antique mantel was displayed
As though a window gave upon the sylvan scene
The change of Philomel
[97?99]
While the stylistic contrasts between these and
other voices are apparent to many readers, Eliot
does not explicitly mark the transitions between
them. The goal of the present work is to investigate
whether computational stylistic analysis can identify
the transition between one voice and the next.
Our unsupervised approach, informed by research
in topic segmentation (Hearst, 1994) and intrinsic
plagiarism detection (Stamatatos, 2009), is based
on deriving a curve representing stylistic change,
where the local maxima represent likely transition
points. Notably, our curve represents an amalga-
mation of different stylistic metrics, including those
that incorporate external (extrinsic) knowledge, e.g.
vector representations based on larger corpus co-
occurrence, which we show to be extremely use-
ful. For development and initial testing we follow
other work on stylistic inconsistency by using arti-
ficial (mixed) poems, but the our main evaluation is
on The Waste Land itself. We believe that even when
our segmentation disagrees with expert human judg-
ment, it has the potential to inform future study of
this literary work.
2 Related work
Poetry has been the subject of extensive computa-
tional analysis since the early days of literary and
linguistic computing (e.g., Beatie 1967). Most of the
research concerned either authorship attribution or
analysis of metre, rhyme, and phonetic properties of
the texts, but some work has studied the style, struc-
ture, and content of poems with the aim of better un-
derstanding their qualities as literary texts. Among
research that, like the present paper, looks at varia-
tion with a single text, Simonton (1990) found quan-
titative changes in lexical diversity and semantic
classes of imagery across the components of Shake-
speare?s sonnets, and demonstrated correlations be-
tween some of these measures and judgments of the
?aesthetic success? of individual sonnets. Duggan
(1973) developed statistical measures of formulaic
style to determine whether the eleventh-century epic
poem Chanson de Ronald manifests primarily an
oral or a written style. Also related to our work,
although it concerned a novel rather than a poem,
is that of McKenna and Antonia (2001), who used
principal component analysis of lexical frequency
to discriminate different voices (dialogue, interior
monologue, and narrative) and different narrative
styles in sections of Ulysses by James Joyce.
More general work on identifying stylistic incon-
sistency includes that of Graham et al (2005), who
built artificial examples of style shift by concate-
nating Usenet postings by different authors. Fea-
ture sets for their neural network classifiers included
standard textual features, frequencies of function
words, punctuation and parts of speech, lexical en-
tropy, and vocabulary richness. Guthrie (2008) pre-
sented some general methods for identifying stylis-
tically anomalous segments using feature vector dis-
tance, and tested the effectiveness of his unsuper-
vised method with a number of possible stylistic
variations. He used features such as simple textual
metrics (e.g. word and sentence length), readability
measures, obscure vocabulary features, frequency
rankings of function words (which were not found
to be useful), and context analysis features from
the General Inquirer dictionary. The most effective
method ranked each segment according to the city-
block distance of its feature vector to the feature vec-
tor of the textual complement (the union of all other
segments in the text). Koppel et al (2011) used a
semi-supervised method to identify segments from
two different books of the Bible artificially mixed
into a single text. They first demonstrated that, in
this context, preferred synonym use is a key stylis-
tic feature that can serve as high-precision boot-
strap for building a supervised SVM classifier on
more general features (common words); they then
used this classifier to provide an initial prediction
for each verse and smooth the results over adjacent
segments. The method crucially relied on properties
of the King James Version translation of the text in
27
order to identify synonym preferences.
The identification of stylistic inconsistency or het-
erogeneity has received particular attention as a
component of intrinsic plagiarism detection ? the
task of ?identify[ing] potential plagiarism by analyz-
ing a document with respect to undeclared changes
in writing style? (Stein et al, 2011). A typical ap-
proach is to move a sliding window over the text
looking for areas that are outliers with respect to the
style of the rest of the text, or which differ markedly
from other regions in word or character-trigram fre-
quencies (Oberreuter et al, 2011; Kestemont et al,
2011). In particular, Stamatatos (2009) used a win-
dow that compares, using a special distance func-
tion, a character trigram feature vector at various
steps throughout the text, creating a style change
function whose maxima indicate points of interest
(potential plagarism).
Topic segmentation is a similar problem that has
been quite well-explored. A common thread in this
work is the importance of lexical cohesion, though
a large number of competing models based on this
concept have been proposed. One popular unsu-
pervised approach is to identify the points in the
text where a metric of lexical coherence is at a (lo-
cal) minimum (Hearst, 1994; Galley et al, 2003).
Malioutov and Barzilay (2006) also used a lexi-
cal coherence metric, but applied a graphical model
where segmentations are graph cuts chosen to max-
imize coherence of sentences within a segment, and
minimize coherence among sentences in different
segments. Another class of approaches is based
on a generative model of text, for instance HMMs
(Blei and Moreno, 2001) and Bayesian topic mod-
eling (Utiyama and Isahara, 2001; Eisenstein and
Barzilay, 2008); in such approaches, the goal is to
choose segment breaks that maximize the probabil-
ity of generating the text, under the assumption that
each segment has a different language model.
3 Stylistic change curves
Many popular text segmentation methods depend
crucially on a reliable textual unit (often a sentence)
which can be reliably classified or compared to oth-
ers. But, for our purposes here, a sentence is both
too small a unit ? our stylistic metrics will be more
accurate over larger spans ? and not small enough
? we do not want to limit our breaks to sentence
boundaries. Generative models, which use a bag-of-
words assumption, have a very different problem: in
their standard form, they can capture only lexical co-
hesion, which is not the (primary) focus of stylistic
analysis. In particular, we wish to segment using in-
formation that goes beyond the distribution of words
in the text being segmented. The model for stylis-
tic segmentation we propose here is related to the
TextTiling technique of Hearst (1994) and the style
change function of Stamatatos (2009), but our model
is generalized so that it applies to any numeric met-
ric (feature) that is defined over a span; importantly,
style change curves represent the change of a set of
very diverse features.
Our goal is to find the precise points in the text
where a stylistic change (a voice switch) occurs. To
do this, we calculate, for each token in the text, a
measure of stylistic change which corresponds to
the distance of feature vectors derived from a fixed-
length span on either side of that point. That is, if vi j
represents a feature vector derived from the tokens
between (inclusive) indices i and j, then the stylistic
change at point ci for a span (window) of size w is:
ci = Dist(v(i?w)(i?1),vi(i+w?1))
This function is not defined within w of the edge of
the text, and we generally ignore the possibility of
breaks within these (unreliable) spans. Possible dis-
tance metrics include cosine distance, euclidean dis-
tance, and city-block distance. In his study, Guthrie
(2008) found best results with city-block distance,
and that is what we will primarily use here. The fea-
ture vector can consist of any features that are de-
fined over a span; one important step, however, is to
normalize each feature (here, to a mean of 0 and a
standard deviation of 1), so that different scaling of
features does not result in particular features having
an undue influence on the stylistic change metric.
That is, if some feature is originally measured to be
fi in the span i to i+w?1, then its normalized ver-
sion f ?i (included in vi(i+w?1)) is:
f ?i =
fi? f
? f
The local maxima of c represent our best predic-
tions for the stylistic breaks within a text. However,
28
stylistic change curves are not well behaved; they
may contain numerous spurious local maxima if a
local maximum is defined simply as a higher value
between two lower ones. We can narrow our def-
inition, however, by requiring that the local max-
imum be maximal within some window w?. That
is, our breakpoints are those points i where, for all
points j in the span x?w?, x+w?, it is the case that
gi > g j. As it happens, w? = w/2 is a fairly good
choice for our purposes, creating spans no smaller
than the smoothed window, though w? can be low-
ered to increase breaks, or increased to limit them.
The absolute height of the curve at each local min-
imum offers a secondary way of ranking (and elim-
inating) potential breakpoints, if more precision is
required; however, in our task here the breaks are
fairly regular but often subtle, so focusing only on
the largest stylistic shifts is not necessarily desirable.
4 Features
The set of features we explore for this task falls
roughly into two categories: surface and extrinsic.
The distinction is not entirely clear cut, but we wish
to distinguish features that use the basic properties
of the words or their PoS, which have traditionally
been the focus of automated stylistic analysis, from
features which rely heavily on external lexical infor-
mation, for instance word sentiment and, in partic-
ular, vector space representations, which are more
novel for this task.
4.1 Surface Features
Word length A common textual statistic in reg-
ister and readability studies. Readability, in turn,
has been used for plagiarism detection (Stein et al,
2011), and related metrics were consistently among
the best for Guthrie (2008).
Syllable count Syllable count is reasonably good
predictor of the difficulty of a vocabulary, and is
used in some readability metrics.
Punctuation frequency The presence or absence
of punctuation such as commas, colons, semicolons
can be very good indicator of style. We also include
periods, which offer a measure of sentence length.
Line breaks Our only poetry-specific feature; we
count the number of times the end of a line appears
in the span. More or fewer line breaks (that is, longer
or shorter lines) can vary the rhythm of the text, and
thus its overall feel.
Parts of speech Lexical categories can indicate,
for instance, the degree of nominalization, which is
a key stylistic variable (Biber, 1988). We collect
statistics for the four main lexical categories (noun,
verb, adjective, adverb) as well as prepositions, de-
terminers, and proper nouns.
Pronouns We count the frequency of first-,
second-, and third-person pronouns, which can in-
dicate the interactiveness and narrative character of
a text (Biber, 1988).
Verb tense Past tense is often preferred in narra-
tives, whereas present tense can give a sense of im-
mediacy.
Type-token ratio A standard measure of lexical
diversity.
Lexical density Lexical density is the ratio of the
count of tokens of the four substantive parts of
speech to the count of all tokens.
Contextuality measure The contextuality mea-
sure of Heylighen and Dewaele (2002) is based on
PoS tags (e.g. nouns decrease contextuality, while
verbs increase it), and has been used to distin-
guish formality in collaboratively built encyclope-
dias (Emigh and Herring, 2005).
Dynamic In addition to the hand-picked features
above, we test dynamically including words and
character trigrams that are common in the text being
analyzed, particularly those not evenly distributed
throughout the text (we exclude punctuation). To
measure the latter, we define clumpiness as the
square root of the index of dispersion or variance-
to-mean ratio (Cox and Lewis, 1966) of the (text-
length) normalized differences between successive
occurrences of a feature, including (importantly) the
difference between the first index of the text and the
first occurrence of the feature as well as the last oc-
currence and the last index; the measure varies be-
tween 0 and 1, with 0 indicating perfectly even dis-
tribution. We test with the top n features based on
the ranking of the product of the feature?s frequency
29
in the text (tf ) or product of the frequency and its
clumpiness (tf-cl); this is similar to a tf-idf weight.
4.2 Extrinsic features
For those lexicons which include only lemmatized
forms, the words are lemmatized before their values
are retrieved.
Percent of words in Dale-Chall Word List A list
of 3000 basic words that is used in the Dale-Chall
Readability metric (Dale and Chall, 1995).
Average unigram count in 1T Corpus Another
metric of whether a word is commonly used. We use
the unigram counts in the 1T 5-gram Corpus (Brants
and Franz, 2006). Here and below, if a word is not
included it is given a zero.
Sentiment polarity The positive or negative
stance of a span could be viewed as a stylistic vari-
able. We test two lexicons, a hand-built lexicon for
the SO-CAL sentiment analysis system which has
shown superior performance in lexicon-based sen-
timent analysis (Taboada et al, 2011), and Senti-
WordNet (SWN), a high-coverage automatic lexicon
built from WordNet (Baccianella et al, 2010). The
polarity of each word over the span is averaged.
Sentiment extremity Both lexicons provide a
measure of the degree to which a word is positive or
negative. Instead of summing the sentiment scores,
we sum their absolute values, to get a measure of
how extreme (subjective) the span is.
Formality Average formality score, using a lex-
icon of formality (Brooke et al, 2010) built using
latent semantic analysis (LSA) (Landauer and Du-
mais, 1997).
Dynamic General Inquirer The General Inquirer
dictionary (Stone et al, 1966), which was used for
stylistic inconsistency detection by Guthrie (2008),
includes 182 content analysis tags, many of which
are relevant to style; we remove the two polarity tags
already part of the SO-CAL dictionary, and select
others dynamically using our tf-cl metric.
LSA vector features Brooke et al (2010) have
posited that, in highly diverse register/genre corpora,
the lowest dimensions of word vectors derived us-
ing LSA (or other dimensionality reduction tech-
niques) often reflect stylistic concerns; they found
that using the first 20 dimensions to build their for-
mality lexicon provided the best results in a near-
synonym evaluation. Early work by Biber (1988)
in the Brown Corpus using a related technique (fac-
tor analysis) resulted in discovery of several identi-
fiable dimensions of register. Here, we investigate
using these LSA-derived vectors directly, with each
of the first 20 dimensions corresponding to a sepa-
rate feature. We test with vectors derived from the
word-document matrix of the ICWSM 2009 blog
dataset (Burton et al, 2009) which includes 1.3 bil-
lion tokens, and also from the BNC (Burnard, 2000),
which is 100 million tokens. The length of the vector
depends greatly on the frequency of the word; since
this is being accounted for elsewhere, we normalize
each vector to the unit circle.
5 Evaluation method
5.1 Metrics
To evaluate our method we apply standard topic
segmentation metrics, comparing the segmentation
boundaries to a gold standard reference. The mea-
sure Pk, proposed by Beeferman et al (1997), uses a
probe window equal to half the average length of a
segment; the window slides over the text, and counts
the number of instances where a unit (in our case,
a token) at one edge of the window was predicted
to be in the same segment (according to the refer-
ence) as a unit at the other edge, but in fact is not; or
was predicted not to be in the same segment, but in
fact is. This count is normalized by the total number
of tests to get a score between 0 and 1, with 0 be-
ing a perfect score (the lower, the better). Pevzner
and Hearst (2002) criticize this metric because it
penalizes false positives and false negatives differ-
ently and sometimes fails to penalize false positives
altogether; their metric, WindowDiff (WD), solves
these problems by counting an error whenever there
is a difference between the number of segments in
the prediction as compared to the reference. Recent
work in topic segmentation (Eisenstein and Barzi-
lay, 2008) continues to use both metrics, so we also
present both here.
During initial testing, we noted a fairly serious
shortcoming with both these metrics: all else be-
ing equal, they will usually prefer a system which
30
predicts fewer breaks; in fact, a system that predicts
no breaks at all can score under 0.3 (a very com-
petitive result both here and in topic segmentation),
if the variation of the true segment size is reason-
ably high. This is problematic because we do not
want to be trivially ?improving? simply by moving
towards a model that is too cautious to guess any-
thing at all. We therefore use a third metric, which
we call BD (break difference), which sums all the
distances, calculated as fractions of the entire text,
between each true break and the nearest predicted
break. This metric is also flawed, because it can be
trivially made 0 (the best score) by guessing a break
everywhere. However, the relative motion of the two
kinds of metric provides insight into whether we are
simply moving along a precision/recall curve, or ac-
tually improving overall segmentation.
5.2 Baselines
We compare our method to the following baselines:
Random selection We randomly select bound-
aries, using the same number of boundaries in the
reference. We use the average over 50 runs.
Evenly spaced We put boundaries at equally
spaced points in the text, using the same number of
boundaries as the reference.
Random feature We use our stylistic change
curve method with a single feature which is created
by assigning a uniform random value to each token
and averaging across the span. Again, we use the
average score over 50 runs.
6 Experiments
6.1 Artificial poems
Our main interest is The Waste Land. It is, however,
prudent to develop our method, i.e. conduct an initial
investigation of our method, including parameters
and features, using a separate corpus. We do this by
building artificial mixed-style poems by combining
stylistically distinct poems from different authors, as
others have done with prose.
6.1.1 Setup
Our set of twelve poems used for this evaluation was
selected by one of the authors (an English literature
expert) to reflect the stylistic range and influences
of poetry at the beginning of the twentieth century,
and The Waste Land in particular. The titles were
removed, and each poem was tagged by an auto-
matic PoS tagger (Schmid, 1995). Koppel et al built
their composite version of two books of the Bible by
choosing, at each step, a random span length (from a
uniform distribution) to include from one of the two
books being mixed, and then a span from the other,
until all the text in both books had been included.
Our method is similar, except that we first randomly
select six poems to include in the particular mixed
text, and at each step we randomly select one of po-
ems, reselecting if the poem has been used up or the
remaining length is below our lower bound. For our
first experiment, we set a lower bound of 100 tokens
and an upper bound of 200 tokens for each span; al-
though this gives a higher average span length than
that of The Waste Land, our first goal is to test
whether our method works in the (ideal) condition
where the feature vectors at the breakpoint gener-
ally represent spans which are purely one poem or
another for a reasonably high w (100). We create 50
texts using this method. In addition to testing each
individual feature, we test several combinations of
features (all features, all surface features, all extrin-
sic features), and present the best results for greedy
feature removal, starting with all features (exclud-
ing dynamic ones) and choosing features to remove
which minimize the sum of the three metrics.
6.1.2 Results
The Feature Sets section of Table 1 gives the in-
dividual feature results for segmentation of the
artificially-combined poems. Using any of the fea-
tures alone is better than our baselines, though some
of the metrics (in particular type-token ratio) are
only a slight improvement. Line breaks are obvi-
ously quite useful in the context of poetry (though
the WD score is high, suggesting a precision/recall
trade-off), but so are more typical stylistic features
such as the distribution of basic lexical categories
and punctuation. The unigram count and formal-
ity score are otherwise the best two individual fea-
tures. The sentiment-based features did more mod-
estly, though the extremeness of polarity was use-
ful when paired with the coverage of SentiWord-
Net. Among the larger feature sets, the GI was the
least useful, though more effective than any of the
31
Table 1: Segmentation accuracy in artificial poems
Configuration Metrics
WD Pk BD
Baselines
Random breaks 0.532 0.465 0.465
Even spread 0.498 0.490 0.238
Random feature 0.507 0.494 0.212
Feature sets
Word length 0.418 0.405 0.185
Syllable length 0.431 0.419 0.194
Punctuation 0.412 0.401 0.183
Line breaks 0.390 0.377 0.200
Lexical category 0.414 0.402 0.177
Pronouns 0.444 0.432 0.213
Verb tense 0.444 0.433 0.202
Lexical density 0.445 0.433 0.192
Contextuality 0.462 0.450 0.202
Type-Token ratio 0.494 0.481 0.204
Dynamic (tf, n=50) 0.399 0.386 0.161
Dynamic (tf-cl, 50) 0.385 0.373 0.168
Dynamic (tf-cl, 500) 0.337 0.323 0.165
Dynamic (tf-cl, 1000) 0.344 0.333 0.199
Dale-Chall 0.483 0.471 0.202
Count in 1T 0.424 0.414 0.193
Polarity (SO-CAL) 0.466 0.487 0.209
Polarity (SWN) 0.490 0.478 0.221
Extremity (SO-CAL) 0.450 0.438 0.199
Extremity (SWN) 0.426 0.415 0.182
Formality 0.409 0.397 0.184
All LSA (ICWSM) 0.319 0.307 0.134
All LSA (BNC) 0.364 0.352 0.159
GI (tf, n=5) 0.486 0.472 0.201
GI (tf-cl, 5) 0.449 0.438 0.196
GI (tf-cl, 50) 0.384 0.373 0.164
GI (tf-cl, 100) 0.388 0.376 0.163
Combinations
Surface 0.316 0.304 0.150
Extrinsic 0.314 0.301 0.124
All 0.285 0.274 0.128
All w/o GI, dynamic 0.272 0.259 0.102
All greedy (Best) 0.253 0.242 0.099
Best, w=150 0.289 0.289 0.158
Best, w=50 0.338 0.321 0.109
Best, Diff=euclidean 0.258 0.247 0.102
Best, Diff=cosine 0.274 0.263 0.145
individual features, while dynamic word and char-
acter trigrams did better, and the ICWSM LSA vec-
tors better still; the difference in size between the
ICWSM and BNC is obviously key to the perfor-
mance difference here. In general using our tf-cl
metric was better than tf alone.
When we combine the different feature types, we
see that extrinsic features have a slight edge over the
surface features, but the two do complement each
other to some degree. Although the GI and dynamic
feature sets do well individually, they do not com-
bine well with other features in this unsupervised
setting, and our best results do not include them.
The greedy feature selector removed 4 LSA dimen-
sions, type-token ratio, prepositions, second-person
pronouns, adverbs, and verbs to get our best result.
Our choice of w to be the largest fully-reliable size
(100) seems to be a good one, as is our use of city-
block distance rather than the alternatives. Overall,
the metrics we are using for evaluation suggest that
we are roughly halfway to perfect segmentation.
6.2 The Waste Land
6.2.1 Setup
In order to evaluate our method on The Waste Land,
we first created a gold standard voice switch seg-
mentation. Our gold standard represents an amal-
gamation, by one of the authors, of several sources
of information. First, we enlisted a class of 140 un-
dergraduates in an English literature course to seg-
ment the poem into voices based on their own intu-
itions, and we created a combined student version
based on majority judgment. Second, our English
literature expert listened to the 6 readings of the
poem included on The Waste Land app (Touch Press
LLP, 2011), including two readings by T.S. Eliot,
and noted places where the reader?s voice seemed
to change; these were combined to create a reader
version. Finally, our expert amalgamated these two
versions and incorporated insights from independent
literary analysis to create a final gold standard.
We created two versions of the poem for evalua-
tion: for both versions, we removed everything but
the main body of the text (i.e. the prologue, dedi-
cation, title, and section titles), since these are not
produced by voices in the poem. The ?full? ver-
sion contains all the other text (a total of 68 voice
32
switches), but our ?abridged? version involves re-
moving all segments (and the corresponding voice
switches, when appropriate) which are 20 or fewer
tokens in length and/or which are in a language
other than English, which reduces the number of
voice switches to 28 (the token count is 3179). This
version allows us to focus on the segmentation for
which our method has a reasonable chance of suc-
ceeding and ignore the segmentation of non-English
spans, which is relatively trivial but yet potentially
confounding. We use w = 50 for the full version,
since there are almost twice as many breaks as in
the abridged version (and our artificially generated
texts).
6.2.2 Results
Our results for The Waste Land are presented in Ta-
ble 2. Notably, in this evaluation, we do not investi-
gate the usefulness of individual features or attempt
to fully optimize our solution using this text. Our
goal is to see if a general stylistic segmentation sys-
tem, developed on artificial texts, can be applied suc-
cessfully to the task of segmenting an actual stylis-
tically diverse poem. The answer is yes. Although
the task is clearly more difficult, the results for the
system are well above the baseline, particularly for
the abridged version. One thing to note is that using
the features greedily selected for the artificial sys-
tem (instead of just all features) appears to hinder,
rather than help; this suggests a supervised approach
might not be effective. The GI is too unreliable to
be useful here, whereas the dynamic word and tri-
gram features continue to do fairly well, but they do
not improve the performance of the rest of the fea-
tures combined. Once again the LSA features seem
to play a central role in this success. We manually
compared predicted with real switches and found
that there were several instances (corresponding to
very clear voices switches in the text) which were
nearly perfect. Moreover, the model did tend to pre-
dict more switches in sections with numerous real
switches, though these predictions were often fewer
than the gold standard and out of sync (because the
sampling windows never consisted of a pure style).
7 Conclusion
In this paper we have presented a system for auto-
matically segmenting stylistically inconsistent text
Table 2: Segmentation accuracy in The Waste Land
Configuration Metrics
WD Pk BD
Full text
Baselines
Random breaks 0.517 0.459 0.480
Even spread 0.559 0.498 0.245
Random feature 0.529 0.478 0.314
System (w=50)
Table 1 Best 0.458 0.401 0.264
GI 0.508 0.462 0.339
Dynamic 0.467 0.397 0.257
LSA (ICWSM) 0.462 0.399 0.280
All w/o GI 0.448 0.395 0.305
All w/o dynamic, GI 0.456 0.394 0.228
Abridged text
Baselines
Random breaks 0.524 0.478 0.448
Even spread 0.573 0.549 0.266
Random feature 0.525 0.505 0.298
System (w=100)
Table 1 Best 0.370 0.341 0.250
GI 0.510 0.492 0.353
Dynamic 0.415 0.393 0.274
LSA (ICWSM) 0.411 0.390 0.272
All w/o GI 0.379 0.354 0.241
All w/o dynamic, GI 0.345 0.311 0.208
and applied it to The Waste Land, a well-known
poem in which stylistic variation, in the form of dif-
ferent ?voices?, provides an interesting challenge to
both human and computer readers. Our unsuper-
vised model is based on a stylistic change curve de-
rived from feature vectors. Perhaps our most inter-
esting result is the usefulness of low-dimension LSA
vectors over surface features such as words and tri-
gram characters as well as other extrinsic features
such as the GI dictionary. In both The Waste Land
and our development set of artificially combined po-
ems, our method performs well above baseline. Our
system could probably benefit from the inclusion of
machine learning, but our main interest going for-
ward is the inclusion of additional features ? in par-
ticular, poetry-specific elements such as alliteration
and other more complex lexicogrammatical features.
33
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Bruce A. Beatie. 1967. Computer study of medieval Ger-
man poetry: A conference report. Computers and the
Humanities, 2(2):65?70.
Calvin Bedient. 1986. He Do the Police in Different
Voices: The Waste Land and its protagonist. Univer-
sity of Chicago Press.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models. In
In Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing (EMNLP
?97), pages 35?46.
Douglas Biber. 1988. Variation Across Speech and Writ-
ing. Cambridge University Press.
David M. Blei and Pedro J. Moreno. 2001. Topic seg-
mentation with an aspect hidden Markov model. In
Proceedings of the 24th annual international ACM SI-
GIR conference on Research and Development in In-
formation Retrieval, SIGIR ?01, pages 343?348.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Google Inc.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10).
Lou Burnard. 2000. User reference guide for British
National Corpus. Technical report, Oxford University.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Xiros Cooper. 1987. T.S. Eliot and the politics of
voice: The argument of The Waste Land. UMI Re-
search Press, Ann Arbor, Mich.
David R. Cox and Peter A.W. Lewis. 1966. The Sta-
tistical Analysis of Series of Events. Monographs on
Statistics and Applied Probability. Chapman and Hall.
Edgar Dale and Jeanne Chall. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline Books, Cambridge, MA.
Joseph J. Duggan. 1973. The Song of Roland: Formulaic
style and poetic craft. University of California Press.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?08, EMNLP ?08, pages
334?343.
William Emigh and Susan C. Herring. 2005. Collabo-
rative authoring on the web: A genre analysis of on-
line encyclopedias. In Proceedings of the 38th Annual
Hawaii International Conference on System Sciences
(HICSS ?05), Washington, DC.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL ?03), ACL ?03, pages 562?569.
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 2005.
Segmenting documents by stylistic character. Natural
Language Engineering, 11(4):397?415.
David Guthrie. 2008. Unsupervised Detection of
Anomalous Text. Ph.D. thesis, University of Sheffield.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?94), ACL ?94, pages 9?16.
Francis Heylighen and Jean-Marc Dewaele. 2002. Vari-
ation in the contextuality of language: An empirical
measure. Foundations of Science, 7(3):293?340.
Mike Kestemont, Kim Luyckx, and Walter Daelemans.
2011. Intrinsic plagiarism detection using character
trigram distance scores. In Proceedings of the PAN
2011 Lab: Uncovering Plagiarism, Authorship, and
Social Software Misuse.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011. Unsupervised decompo-
sition of a document into authorial components. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11).
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ?06), pages
25?32.
C. W. F. McKenna and A. Antonia. 2001. The statistical
analysis of style: Reflections on form, meaning, and
ideology in the ?Nausicaa? episode of Ulysses. Liter-
ary and Linguistic Computing, 16(4):353?373.
34
Gabriel Oberreuter, Gaston L?Huillier, Sebastia?n A. R??os,
and Juan D. Vela?squez. 2011. Approaches for intrin-
sic and external plagiarism detection. In Proceedings
of the PAN 2011 Lab: Uncovering Plagiarism, Author-
ship, and Social Software Misuse.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36, March.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop, pages 47?50.
Dean Keith Simonton. 1990. Lexical choices and aes-
thetic success: A computer content analysis of 154
Shakespeare sonnets. Computers and the Humanities,
24(4):251?264.
Efstathios Stamatatos. 2009. Intrinsic plagiarism detec-
tion using character n-gram profiles. In Proceedings
of the SEPLN?09 Workshop on Uncovering Plagia-
rism, Authorship and, Social Software Misuse (PAN-
09), pages 38?46. CEUR Workshop Proceedings, vol-
ume 502.
Benno Stein, Nedim Lipka, and Peter Prettenhofer. 2011.
Intrinsic plagiarism analysis. Language Resources
and Evaluation, 45(1):63?82.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilivie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manifred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Touch Press LLP. 2011. The Waste Land
app. http://itunes.apple.com/ca/app/the-waste-land/
id427434046?mt=8 .
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?01), pages
499?506.
35
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 1?8,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
A Tale of Two Cultures:
Bringing Literary Analysis and Computational Linguistics Together
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
There are cultural barriers to collaborative effort
between literary scholars and computational lin-
guists. In this work, we discuss some of these
problems in the context of our ongoing research
project, an exploration of free indirect discourse
in Virginia Woolf?s To The Lighthouse, ulti-
mately arguing that the advantages of taking
each field out of its ?comfort zone? justifies the
inherent difficulties.
1 Introduction
Within the field of English literature, there is a grow-
ing interest in applying computational techniques, as
evidenced by the growth of the Digital Humanities
(Siemens et al, 2004). At the same time, a subfield
in Computational Linguistics that addresses a range
of problems in the genre of literature is gaining mo-
mentum (Mani, 2013). Nevertheless, there are sig-
nificant barriers to true collaborative work between
literary and computational researchers. In this pa-
per, we discuss this divide, starting from the classic
rift between the two cultures of the humanities and
the sciences (Snow, 1959) and then focusing in on a
single aspect, the attitude of the two fields towards
ambiguity. Next, we introduce our ongoing collab-
orative project which is an effort to bridge this gap;
in particular, our annotation of Virginia Woolf?s To
the Lighthouse for free indirect discourse, i.e. mix-
tures of objective narration and subjective speech,
requires a careful eye to literary detail, and, while
novel, interacts in interesting ways with established
areas of Computational Linguistics.
2 Background
2.1 The ?Two Cultures? Problem
Since the publication of C. P. Snow?s influential
The Two Cultures and the Scientific Revolution
(Snow, 1959), the phrase ?the two cultures? been
used to signify the rift?perceived and generally
lamented?between scientific and humanities intel-
lectual cultures. The problem, of course, is the igno-
rance of each culture with regard to the methods and
assumptions of the other, and the resulting impos-
sibility of genuine dialogue between them, prevent-
ing them from working together to solve important
problems. Many scholars describing the recent rise
of the Digital Humanities?the area of research and
teaching concerned with the intersection of comput-
ing and humanities disciplines?have argued that it
effects a reconciliation of the two alienated spheres,
bringing scientific methodology to bear on problems
within the humanities, many of which had previ-
ously been addressed in a less-than-rigorous manner
(Hockey, 2004).
From within the discipline of English literature,
however, the application of computational meth-
ods to literary analysis has frequently been?and
continues to be?a matter of considerable contro-
versy (Hoover, 2007; Flanders, 2009). This con-
troversy arises from the perception of many tradi-
tional humanists that computational analysis, which
aims to resolve dilemmas, seeking singular truth
and hard-and-fast answers, is incompatible with the
aims of humanistic research, which is often focused
on opening up questions for debate rather than re-
solving them decisively, and often premised on the
1
idea that there are no right answers, only well- and
poorly-supported arguments. Critics have responded
to these views by arguing that the best computational
literary analysis participates in this project of open-
ing up meaning, arguing that it is not a rejection of
literary reading but rather a method for carrying it
out more efficiently and extending it to more texts
(Ramsay, 2007), and that computational modelling,
even when unsuccessful, allows for the application
of the scientific method and thus carries the poten-
tial for intellectual advancement not possible with
purely anecdotal evidence (McCarty, 2005). Despite
such counter-arguments, however, the fear remains
widespread among traditional literary scholars that
the rise of computational analysis will entail the loss
of certain sacred assumptions of humanistic inquiry.
2.2 Ambiguity Across the ?Cultures?
We argue, though, that these fears are not without
basis, particularly when one considers the very dif-
ferent approaches to the question of ambiguity in
the two specific disciplines involved in our project:
English Literature and Computational Linguistics.
Here, the rift of the two cultures remains evident.
A major focus of literary scholarship since the
early twentieth century has been the semantic mul-
tiplicity of literary language. Such scholarship has
argued that literature, distinct from other forms of
discourse, may be deliberately ambiguous or poly-
semous and that literary analysis, distinct from other
analytic schools, should thus aim not to resolve am-
biguity but to describe and explore it. This was a
central insight of the early twentieth-century school,
the New Criticism, advanced in such works as
William Empson?s Seven Types of Ambiguity (Emp-
son, 1930) and Cleanth Brooks?s The Well Wrought
Urn (Brooks, 1947), which presented ambiguity and
paradox not as faults of style but as important po-
etic devices. New Criticism laid out a method of
literary analysis centred on the explication of the
complex tensions created by ambiguity and para-
dox, without any effort to resolve them. Also in
the first half of the twentieth century, but indepen-
dently, the Russian critic Mikhail Bakhtin developed
his theory of dialogism, which valorized ?double-
voiced? or polyphonic works that introduce multi-
ple, competing perspectives?particularly voices?
that present conflicting ideologies (Bakhtin, 1981).
Bakhtin, who wrote his seminal work ?Discourse in
the Novel? under a Stalinist sentence of exile, par-
ticularly valued works that enacted the free compe-
tition of ideologically opposed voices. In a simi-
lar spirit, but independently of Bakhtin, the German
critic Erich Auerbach described the ?multi-personal
representation of consciousness?, a narrative tech-
nique in which the writer, typically the narrator of
objective facts, is pushed entirely into the back-
ground and the story proceeds by reflecting the in-
dividual consciousnesses of the characters; Auer-
bach argued that this was a defining quality of mod-
ernist (early twentieth-century) literature (Auerbach,
1953). In the second half of the twentieth century,
this critical emphasis on ambiguity and paradox de-
veloped in an extreme form into the school of de-
constructive criticism, which held a theory of the
linguistic sign according to which determinate lin-
guistic meaning is considered logically impossible.
Deconstructive literary analysis proceeds by seeking
out internal contradictions in literary texts to support
its theory of infinitely ambiguous signification.
In Computational Linguistics, by contrast, ambi-
guity is almost uniformly treated as a problem to be
solved; the focus is on disambiguation, with the as-
sumption that one true, correct interpretation exists.
In the sphere of annotation, for instance, there is
an expectation that agreement between annotators,
as measured by statistics such as kappa (Di Euge-
nio and Glass, 2004), reach levels (generally 0.67 or
higher) where disagreements can be reasonably dis-
missed as noise; the implicit assumption here is that
subjectivity is something to be minimized. The chal-
lenge of dealing with subjectivity in CL has been
noted (Alm, 2011), and indeed there are rare exam-
ples in the field where multiple interpretations have
been considered during evaluations?for instance,
work in lexical cohesion (Morris and Hirst, 2005)
and in using annotator disagreements as an indicator
that two words are of similar orientation (Taboada
et al, 2011)?but they are the exception. Work in
CL focused on literary texts tends towards aspects
of the texts which readers would not find particu-
larly ambiguous, for example identifying major nar-
rative threads (Wallace, 2012) or distinguishing au-
thor gender (Luyckx et al, 2006).
2
3 A Collaborative Research Agenda
The obvious solution to the problem of the ?two
cultures??and one that has often been proposed
(Friedlander, 2009)?is interdisciplinary collabora-
tion. But while there are many computational lin-
guists working in literary topics such as genre,
and many literary scholars performing computa-
tional analysis of literature, genuine collaboration
between the disciplines remains quite rare. Over the
past two years, we have undertaken two collabora-
tive projects?one mostly complete, one ongoing?
which aim at such genuine collaboration, and in so
doing seek to bridge the real rift between scientific
and humanities cultures.1 Each of these projects
is multi-faceted, seeking (a) to produce meaningful
research within both disciplines of Computational
Linguistics and English Literature; (b) to provide
educational experience which broadens the disci-
plinary horizons of the undergraduate students in-
volved in the projects; and (c) to provide a model
of collaborative research that will spur further such
?culture-spanning? projects.
Each of our projects was launched in the context
of a course entitled ?The Digital Text? offered by the
Department of English at the University of Toronto.
The first author, whose background is in English Lit-
erature, is instructor of the course, while the sec-
ond author, a graduate student in Computer Science,
was assigned as a teaching assistant. Working to-
gether with the third author, we have designed these
projects collaboratively.
The first project, which we call ?He Do the
Police in Different Voices?,2 was carried out in
2011?12 (Hammond, 2013). Focused on a ?multi-
personal? poem, The Waste Land (1922) by T.S.
Eliot, it encompassed each of the three aspects of
our projects outlined above; in particular, it was mo-
tivated by a research question of interest to both dis-
ciplines: could we identify the points in The Waste
Land where the style changes, where one ?voice?
gives way to another? A computational approach
1In addition, the third author was part of a separate collabo-
rative project between our departments (Le et al, 2011), though
the aim of that project was not literary analysis.
2This is a reference to Eliot?s working title for The Waste
Land, which in itself is a reference to a talented storyteller in
Our Mutual Friend by Charles Dickens; another Dickens novel
is alluded to in the title of this paper.
promised to bring added rigor as well as a degree
of objectivity to this question, which humanities
methods had proven unable to resolve in almost a
century of debate. Both because poetry is dense
in signification, and because the multiple voices in
The Waste Land are a deliberate effect achieved by
a single author rather than a disguised piecing to-
gether of the works of multiple authors, the ques-
tion provided a meaningful challenge to the com-
putational approach, an unsupervised vector-space
model which first segments by identifying points
of stylistic change (Brooke et al, 2012) and then
clusters the resulting segments together into voices
(Brooke et al, 2013).
This research project was tightly integrated into
the curriculum of ?The Digital Text?. Students were
instructed in the use of the Text Encoding Initiative
(TEI) XML guidelines,3 and each of the students
provided one annotation related to voice as part of
a marked assignment. Students also participated in
an online poll in which they indicated every instance
in which they perceived a vocal switch in the poem,
and their responses were used in the construction of
a gold standard for the evaluation of our computa-
tional approach.
Once they were complete, we developed our re-
sults into a publicly accessible website.4 This web-
site promises to encourage collaboration between
literary scholars and computational linguists by ex-
plaining the project and our results in language ac-
cessible to both, and by producing a new digital edi-
tion of the poem based on our findings. Human and
computer readings of the poem are presented side-
by-side on the website, to demonstrate that each in-
terprets the poem in different ways, but that neither
of these methods is absolutely valid. Rather, we en-
courage website visitors to decide for themselves
where they believe that the vocal switches occur,
and we provide an interactive interface for divid-
ing the poem up according to their own interpreta-
tion. In addition to serving as a model of collabora-
tion between English Literature and Computational
Linguistics?and also serving as a teaching tool for
instructors of The Waste Land at any level?the site
is thus useful to us as a source of further data.
3http://www.tei-c.org/Guidelines/
4http://www.hedothepolice.org
3
4 The ?Brown Stocking? Project
4.1 Free Indirect Discourse in To the
Lighthouse
Our second, ongoing project, ?The Brown Stock-
ing?, focuses on a literary text deliberately chosen
for its deeply ambiguous, polysemous, dialogic na-
ture: Virginia Woolf?s (1927) To the Lighthouse
(TTL). Woolf?s novel was produced at the same time
that critical theories of ambiguity and polyvocality
were being developed, and indeed was taken as a
central example by many critics. Our project takes
its title from the final chapter of Erich Auerbach?s
Mimesis, in which Auerbach presents TTL as the
representative text of modernist literature?s ?mul-
tipersonal representation of consciousness? (Auer-
bach, 1953). For Auerbach, there are two prin-
cipal distinguishing features in Woolf?s narrative
style. The first is the tendency, already noted, to ?re-
flect? incidents through the subjective perspectives
of characters rather than presenting them from the
objective viewpoint of the author; thus TTL becomes
a work in which there is more than one order and in-
terpretation. Woolf?s technique not only introduces
multiple interpretations, however, but also blurs the
transitions between individual perspectives, making
it difficult to know in many instances who is speak-
ing or thinking.
Woolf achieves this double effect?multiple sub-
jective impressions combined with obscuring of the
lines separating them from the narrator and from one
another?chiefly through the narrative technique of
free indirect discourse (also known as free indirect
style). Whereas direct discourse reports the actual
words or thoughts of a character, and indirect dis-
course summarizes the thoughts or words of a char-
acter in the words of the entity reporting them, free
indirect discourse (FID) is a mixture of narrative and
direct discourse (Abrams, 1999). As in indirect dis-
course, the narrator employs third-person pronouns,
but unlike indirect discourse, the narrator includes
words and expressions that indicate subjective or
personalized aspects clearly distinct from the narra-
tor?s style. For example, in the opening sentences of
TTL:
?Yes, of course, if it?s fine tomorrow,? said Mrs.
Ramsay. ?But you?ll have to be up with the
lark,? she added. To her son these words con-
veyed an extraordinary joy, as if it were settled,
the expedition were bound to take place, and
the wonder to which he had looked forward, for
years and years it seemed, was, after a night?s
darkness and a day?s sail, within touch.
we are presented with two spans of objective nar-
ration (said Mrs. Ramsay and she added) and two
passages of direct discourse, in which the narrator
introduces the actual words of Mrs. Ramsay (?Yes,
of course, if it?s fine tomorrow? and ?But you?ll have
to be up with the lark?). The rest of the passage is
presented in FID, mixing together the voices of the
narrator, Mrs. Ramsay, and her son James: while the
use of third-person pronouns and the past tense and
clearly indicates the voice of the narrator, phrases
such as for years and years it seemed clearly present
a subjective perspective.
In FID?s mixing of voices, an element of uncer-
tainty is inevitably present. While we can be con-
fident of the identity of the voice speaking certain
words, it remains unclear whether other words be-
long to the narrator or a character; in this case, it
is not clear whether for years and years it seemed
presents James?s actual thoughts, Mrs. Ramsay?s
summary of her son?s thoughts, the narrator?s sum-
mary of James?s thoughts, the narrator?s summary
of Mrs. Ramsay?s summary of James?s thoughts, etc.
Abrams (1999) emphasizes uncertainty as a defining
trait of FID: the term ?refers to the way, in many nar-
ratives, that the reports of what a character says and
thinks shift in pronouns, adverbs, and grammatical
mode, as we move?or sometimes hover?between
the direct narrated reproductions of these events as
they occur to the character and the indirect repre-
sentation of such events by the narrator?. FID, with
its uncertain ?hovering?, is used throughout TTL;
it is the principal technical means by which Woolf
produces ambiguity, dialogism, and polysemy in the
text. It is thus the central focus of our project.
In Literary Studies, Toolan (2008) was perhaps
the first to discuss the possibility of automatic recog-
nition of FID, but his work was limited to a very
small, very informal experiment using a few a pri-
ori features, with no implementation or quantita-
tive analysis of the results. Though we are not
aware of work in Computational Linguistics that
deals with this kind of subjectivity in literature?
FID is included in the narrative annotation schema
4
of Mani (2013), but it is not given any particular
attention within that framework?there are obvious
connections with sentence-level subjectivity analy-
sis (Wilson et al, 2005) and various other stylis-
tic tasks, including authorship profiling (Argamon
et al, 2007). Since the subjective nature of these
passages is often expressed through specific lexical
choice, it would be interesting to see if sentiment
dictionaries (Taboada et al, 2011) or other stylistic
lexical resources such as dictionaries of lexical for-
mality (Brooke et al, 2010) could be useful.
4.2 Our Approach
Our project is proceeding in four stages: an initial
round of student annotation, a second round of stu-
dent annotation, computational analysis of these an-
notations, and the development of a project website.
In the first stage, we had 160 students mark up a pas-
sage of between 100?150 words in accordance with
TEI guidelines. Students were instructed to use the
TEI said element to enclose any instance of char-
acter speech, to identify the character whose speech
is being introduced, and to classify each of these in-
stances as either direct, indirect, or free indirect dis-
course and as either spoken aloud or thought silently.
Because there are often several valid ways of inter-
preting a given passage, and because we are inter-
ested in how different students respond to the same
passage, each 100?150 word span was assigned to
three or four students. This first round of annotation
focused only on the first four chapters of TTL. Raw
average agreement of the various annotations at the
level of the word was slightly less than 70%,5 and
though we hope to do better in our second round,
levels of agreement typically required are likely to
be beyond our reach due to the nature of the task.
For example, all four sudents responsible for the
passage cited above agreed on the tagging of the first
two sentences; however, two students read the third
sentence as FID mixing the voices of the narrator
and Mrs. Ramsay, and two read it as FID mixing
the voice of the narrator and James. Though they
disagree, these are both valid interpretations of the
5Since each passage was tagged by a different set of stu-
dents, we cannot apply traditional kappa measures. Raw agree-
ment overestimates success, since unlike kappa it does not
discount random agreement, which in this case varies widely
across the different kinds of annotation.
passage.
In the second round of annotation, with 160 dif-
ferent student annotators assigned slightly longer
spans of 200?300 words, we are focusing on the
final seven chapters of TTL. We have made sev-
eral minor changes to our annotation guidelines, and
two significant changes. First, we now ask that in
every span of text which students identify as FID,
they explicitly identify the words that they regard
as clearly coming from the subjective perspective
of the character. We believe this will help students
make a valid, defensible annotation, and it may also
help with the computational analysis to follow. Sec-
ond, we are also allowing embedded tags, for in-
stances of direct or indirect discourse within spans
of FID, which were confusing to students in the ini-
tial round. For instance, students would now be able
to tag the above-cited passage of as a span of FID
mixing the narrator?s and Mrs. Ramsay?s words, in-
side of which Mrs. Ramsay introduces an indirect-
discourse rendering of her son?s thoughts. Moving
from a flat to a recursive representation will natu-
rally result in additional complexity, but we believe
it is necessary to capture what is happening in the
text.
Once this second round of tagging is complete, we
will begin our computational analysis. The aim is to
see whether we can use supervised machine learn-
ing to replicate the way that second-year students
enrolled in a rigorous English literature program re-
spond to a highly complex text such as TTL. We
are interested to see whether the subjective, messy
data of the students can be used to train a useful
model, even if it is inadequate as a gold standard.
If successful, this algorithm could be deployed on
the remaining, untagged sections of TTL (i.e. ev-
erything between the first four and last seven chap-
ters) and produce meaningful readings of the text.
It would proceed by (a) identifying passages of FID
(that is, passages in which it is unclear whether a
particular word belongs to the narrator or a char-
acter); (b) making an interpretation of that passage
(hypothesizing as to which particular voices are be-
ing mixed); and (c) judging the likely validity of
this interpretation. It would seek not only to identify
spans of vocal ambiguity, but also to describe them,
as far as possible. It would thus not aim strictly
at disambiguation?at producing a right-or-wrong
5
reading of the text?but rather at producing the best
possible interpretation. The readings thus generated
could then be reviewed by an independent expert as
a form of evaluation.
Finally, we will develop an interactive website for
the project. It will describe the background and aims
of the project, present the results from the first three
stages of the project, and also include an interface
allowing visitors to the site to annotate the text for
the same features as the students (via a Javascript in-
terface, i.e. without having to manipulate the XML
markup directly). This will provide further annota-
tion data for our project, as well as giving instruc-
tors in English Literature and Digital Humanities a
resource to use in their teaching.
5 Discussion
We believe our approach has numerous benefits on
both sides of the divide. From a research perspec-
tive, the inter-disciplinary approach forces partici-
pants from both English Literature and Computa-
tional Linguistics to reconsider some of their funda-
mental disciplinary assumptions. The project takes
humanities literary scholarship out of its ?comfort
zone? by introducing alien and unfamiliar method-
ologies such as machine learning, as well as by its
basic premise that FID?by definition, a moment
of uncertainty where the question of who is speak-
ing is unresolved?can be detected automatically.
Even though many of these problems can be linked
with classic Computational Linguistics research ar-
eas, the project likewise takes Computational Lin-
guistics out of its comfort zone by seeking not to
resolve ambiguity but rather to identify it and, as far
as possible, describe it. It presents an opportunity
for a computational approach to take into account a
primary insight of twentieth-century literary schol-
arship: that ambiguity and subjectivity are often de-
sirable, intentional qualities of literary language, not
problems to be solved. It promises literary scholar-
ship a method for extending time-consuming, labo-
rious human literary readings very rapidly to a vast
number of literary texts, the possible applications of
which are unclear at this early stage, but are surely
great.
While many current major projects in computer-
assisted literary analysis operate on a ?big-data?
model, drawing conclusions from analysis of vast
numbers of lightly annotated texts, we see advan-
tages in our own method of beginning with a few
heavily-annotated texts and working outward. Tra-
ditional literary scholars often object that ?big-data?
readings take little or no account of subjective, hu-
man responses to literary texts; likewise, they find
the broad conclusions of such projects (that the nine-
teenth century novel moves from telling to show-
ing (Heuser and Le-Khac, 2012); that Austen is
more influential than Dickens (Jockers, 2012)) dif-
ficult to test (or reconcile) with traditional literary
scholarship. The specific method we are pursuing?
taking a great number of individual human read-
ings of a complex literary text and using them as
the basis for developing a general understanding of
how FID works?promises to move literary analysis
beyond merely ?subjective? readings without, how-
ever, denying the basis of all literary reading in indi-
vidual, subjective responses. Our method indeed ap-
proaches the condition of a multi-voiced modernist
literary work like TTL, in which, as Erich Auerbach
perceived, ?overlapping, complementing, and con-
tradiction yield something that we might call a syn-
thesized cosmic view?. We too are building our syn-
thetic understanding out of the diverse, often contra-
dictory, responses of individual human readers.
Developing this project in an educational
context?basing our project on readings developed
by students as part of marked assignments for
?The Digital Text??is likewise beneficial to both
cultures. It forces humanities undergraduates
out of their comfort zone by asking them to turn
their individual close readings of the text into an
explicit, machine-readable representation (in this
case, XML). Recognizing the importance of a
sharable language for expressing literary features
in machine-readable way, we have employed the
standard TEI guidelines mark-up with as few
customizations as possible, rather than developing
our own annotation language from the ground up.
The assignment asks students, however, to reflect
critically on whether such explicit languages can
ever adequately capture the polyvalent structures
of meaning in literary texts; that is, whether there
will always necessarily be possibilities that can?t
be captured in the tag set, and whether, as such, an
algorithmic process can ever really ?read? literature
6
in a useful way. At the same time, this method
has potentially great benefits to the development
of such algorithmic readings, precisely by making
available machine-readable approximations of how
readers belonging to another ?culture??humanities
undergraduates?respond to a challenging literary
text. Such annotations would not be possible from
a pool of annotators trained in the sciences, but
could only come from students of the humanities
with a basic understanding of XML. We do not
believe, for example, workers on Amazon Mechan-
ical Turk could reliably be used for this purpose,
though it might be interesting to compare our
?studentsourcing? with traditional crowdsourcing
techniques.
Our approach also faces several important chal-
lenges. Certainly the largest is whether an algo-
rithmic criticism can be developed that could come
to terms with ambiguity. The discipline of literary
studies has long taught its students to accept what
the poet John Keats called ?negative capability, that
is, when a man is capable of being in uncertainties,
mysteries, doubts, without any irritable searching af-
ter fact and reason? (Keats, 2002). Computational
analysis may simply be too fundamentally premised
on ?irritable searching after fact and reason? to be
capable of ?existing in uncertainty? in the manner of
many human literary readers. Even if we are able to
develop a successful algorithmic method of detect-
ing FID in Woolf, this method may not prove appli-
cable to other literary texts, which may employ the
device in highly individual manners; TTL may prove
simply too complex?and employ too much FID?
to serve as a representative sample text. At a more
practical level, even trained literature students do not
produce perfect annotations: they make errors both
in XML syntax and in their literary interpretation of
TTL, a text that proves elusive even for some spe-
cialists. Since we do not want our algorithm to base
its readings on invalid student readings (for instance,
readings that attribute speech to a character clearly
not involved in the scene), we face the challenge of
weeding out bad student readings?and we will face
the same challenge once readings begin to be sub-
mitted by visitors to the website. These diverse read-
ings do, however, also present an interesting possi-
bility, which we did not originally foresee: the de-
velopment of a reader-response ?map? showing how
human readers actually interpret (and in many cases
misinterpret) complex modernist texts like TTL.
6 Conclusion
Despite the philosophical and technical chal-
lenges that face researchers in this growing multi-
disciplinary area, we are increasingly optimistic that
collaboration between computational and literary re-
searchers is not only possible, but highly desirable.
Interesting phenomena such as FID, this surprising
melding of objective and personal perspective that
is the subject of the current project, requires experts
in both fields working together to identify, annotate,
and ultimately model. Though fully resolving the
rift between our two cultures is not, perhaps, a feasi-
ble goal, we argue that even this early and tentative
collaboration has demonstrated the potential benefits
on both sides.
Acknowledgements
This work was financially supported by the So-
cial Sciences and Humanities Research Council of
Canada and the Natural Sciences and Engineering
Research Council of Canada.
References
M. H. Abrams. 1999. A Glossary of Literary Terms.
Harcourt Brace, Toronto, 7th edition.
Cecilia Ovesdotter Alm. 2011. Subjective natural lan-
guage problems: Motivations, applications, charac-
terizations, and implications. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 107?112.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features. Journal of the American Society for In-
formation Science and Technology, 7:91?109.
Erich Auerbach. 1953. Mimesis: The Representation
of Reality in Western Literature. Princeton University
Press, Princeton, NJ.
Mikhail Mikhailovich Bakhtin. 1981. Discourse in
the novel. In Michael Holquist, editor, The Dialogic
Imagination: Four Essays, pages 259?422. Austin:
Univeristy of Texas Press.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
7
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Julian Brooke, Adam Hammond, and Graeme Hirst.
2012. Unsupervised stylistic segmentation of poetry
with change curves and extrinsic features. In Proceed-
ings of the 1st Workshop on Computational Literature
for Literature (CLFL ?12), Montreal.
Julian Brooke, Graeme Hirst, and Adam Hammond.
2013. Clustering voices in the Waste Land. In Pro-
ceedings of the 2nd Workshop on Computational Lit-
erature for Literature (CLFL ?13), Atlanta.
Cleanth Brooks. 1947. The Well Wrought Urn. Harcourt
Brace, New York.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Computational Linguistics,
30(1):95?101, March.
T.S. Eliot. 1971. The Waste Land. In The Complete
Poems and Plays, 1909?1950, pages 37?55. Harcourt
Brace Jovanovich, New York.
William Empson. 1930. Seven Types of Ambiguity.
Chatto and Windus, London.
Julia Flanders. 2009. Data and wisdom: Electronic edit-
ing and the quantification of knowledge. Literary and
Linguistic Computing, 24(1):53?62.
Amy Friedlander. 2009. Asking questions and build-
ing a research agenda for digital scholarship. Work-
ing Together or Apart: Promoting the Next Generation
of Digital Scholarship. Report of a Workshop Cospon-
sored by the Council on Library and Information Re-
sources and The National Endowment for the Human-
ities, March.
Adam Hammond. 2013. He do the police in different
voices: Looking for voices in The Waste Land. Sem-
inar: ?Mapping the Fictional Voice? American Com-
parative Literature Association (ACLA).
Ryan Heuser and Long Le-Khac. 2012. A quantita-
tive literary history of 2,958 nineteenth-century British
novels: The semantic cohort method. Stanford Lit-
erary Lab Pamphlet No. 4. http://litlab.stanford.edu/
LiteraryLabPamphlet4.pdf .
Susan Hockey. 2004. The history of humanities com-
puting. In Ray Siemens, Susan Schreibman, and John
Unsworth, editors, A Companion to Digital Humani-
ties. Blackwell, Oxford.
David L. Hoover. 2007. Quantitative analysis and lit-
erary studies. In Ray Siemens and Susan Schreib-
man, editors, A Companion to Digital Literary Studies.
Blackwell, Oxford.
Matthew L. Jockers. 2012. Computing and visualiz-
ing the 19th-century literary genome. Presented at the
Digital Humanities Conference. Hamburg.
John Keats. 2002. Selected Letters. Oxford University
Press, Oxford.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina
Jokel. 2011. Longitudinal detection of dementia
through lexical and syntactic changes in writing: A
case study of three British novelists. Literary and Lin-
guistic Computing, 26(4):435?461.
Kim Luyckx, Walter Daelemans, and Edward Vanhoutte.
2006. Stylogenetics: Clustering-based stylistic analy-
sis of literary corpora. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC ?06), Genoa, Italy.
Inderjeet Mani. 2013. Computational Modeling of Nar-
rative. Morgan & Claypool.
Willard McCarty. 2005. Humanities Computing. Pal-
grave Macmillan, New York.
Jane Morris and Graeme Hirst. 2005. The subjectivity
of lexical cohesion in text. In James G. Shanahan, Yan
Qu, and Janyce M. Wiebe, editors, Computing Attitude
and Affect in Text. Springer, Dordrecht, The Nether-
lands.
Stephen Ramsay. 2007. Algorithmic criticism. In Ray
Siemens and Susan Schreibman, editors, A Companion
to Digital Literary Studies. Blackwell, Oxford.
Ray Siemens, Susan Schreibman, and John Unsworth,
editors. 2004. A Companion to Digital Humanities.
Blackwell, Oxford.
C. P. Snow. 1959. The Two Cultures and the Scientific
Revolution. Cambridge University Press, New York.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Michael Toolan. 2008. Narrative progression in the short
story: First steps in a corpus stylistic approach. Nar-
rative, 16(2):105?120.
Byron Wallace. 2012. Multiple narrative disentangle-
ment: Unraveling Infinite Jest. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1?10, Montre?al,
Canada, June. Association for Computational Linguis-
tics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT/EMNLP
?05, pages 347?354.
Virginia Woolf. 1927. To the Lighthouse. Hogarth, Lon-
don.
8
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 41?46,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Clustering voices in The Waste Land
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Abstract
T.S. Eliot?s modernist poem The Waste Land is
often interpreted as collection of voices which
appear multiple times throughout the text. Here,
we investigate whether we can automatically
cluster existing segmentations of the text into
coherent, expert-identified characters. We show
that clustering The Waste Land is a fairly dif-
ficult task, though we can do much better than
random baselines, particularly if we begin with
a good initial segmentation.
1 Introduction
Although literary texts are typically written by a sin-
gle author, the style of a work of literature is not nec-
essarily uniform. When a certain character speaks,
for instance, an author may shift styles to give the
character a distinct voice. Typically, voice switches
in literature are explicitly marked, either by the use
of quotation marks with or without a said quota-
tive, or, in cases of narrator switches, by a major
textual boundary (e.g. the novel Ulysses by James
Joyce). However, implicit marking is the norm in
some modernist literature: a well-known example is
the poem The Waste Land by T.S. Eliot, which is
usually analyzed in terms of voices that each appear
multiple times throughout the text. Our interest is
distinguishing these voices automatically.
One of the poem?s most distinctive voices is that
of the woman who speaks at the end of its second
section:
I can?t help it, she said, pulling a long face,
It?s them pills I took, to bring it off, she said
[158?159]
Her chatty tone and colloquial grammar and lexis
distinguish her voice from many others in the poem,
such as the formal and traditionally poetic voice of a
narrator that recurs many times in the poem:
Above the antique mantel was displayed
As though a window gave upon the sylvan scene
The change of Philomel
[97?99]
Although the stylistic contrasts between these and
other voices are clear to many readers, Eliot does
not explicitly mark the transitions, nor is it obvi-
ous when a voice has reappeared. Our previous
work focused on only the segmentation part of the
voice identification task (Brooke et al, 2012). Here,
we instead assume an initial segmentation and then
try to create clusters corresponding to segments of
the The Waste Land which are spoken by the same
voice. Of particular interest is the influence of the
initial segmentation on the success of this down-
stream task.
2 Related Work
There is a small body of work applying quantita-
tive methods to poetry: Simonton (1990) looked
at lexical and semantic diversity in Shakespearean
sonnets and correlated this with aesthetic success,
whereas Dugan (1973) developed statistics of for-
mulaic style and applied them to the Chanson de
Roland to determine whether it represents an oral
or written style. Kao and Jurafsky (2012) quantify
various aspects of poety, including style and senti-
ment, and use these features to distinguish profes-
sional and amateur writers of contemporary poetry.
41
With respect to novels, the work of McKenna and
Antonia (2001) is very relevant; they used principal
components analysis of lexical frequency to discrim-
inate different voices and narrative styles in sections
of Ulysses by James Joyce.
Clustering techniques have been applied to liter-
ature in general; for instance, Luyckx (2006) clus-
tered novels according to style, and recent work in
distinguishing two authors of sections of the Bible
(Koppel et al, 2011) relies crucially on an initial
clustering which is bootstrapped into a supervised
classifier which is applied to segments. Beyond lit-
erature, the tasks of stylistic inconsistency detec-
tion (Graham et al, 2005; Guthrie, 2008) and intrin-
sic (unsupervised) plagiarism detection (Stein et al,
2011) are very closely related to our interests here,
though in such tasks usually only two authors are
posited; more general kinds of authorship identifi-
cation (Stamatatos, 2009) may include many more
authors, though some form of supervision (i.e. train-
ing data) is usually assumed.
Our work here is built on our earlier work (Brooke
et al, 2012). Our segmentation model for The Waste
Land was based on a stylistic change curve whose
values are the distance between stylistic feature vec-
tors derived from 50 token spans on either side of
each point (spaces between tokens) in the text; the
local maxima of this curve represent likely voice
switches. Performance on The Waste Land was far
from perfect, but evaluation using standard text seg-
mentation metrics (Pevzner and Hearst, 2002) indi-
cated that it was well above various baselines.
3 Method
Our approach to voice identification in The Waste
Land consists first of identifying the boundaries of
voice spans (Brooke et al, 2012). Given a segmenta-
tion of the text, we consider each span as a data point
in a clustering problem. The elements of the vector
correspond to the best feature set from the segmen-
tation task, with the rationale that features which
were useful for detecting changes in style should
also be useful for identifying stylistic similarities.
Our features therefore include: a collection of read-
ability metrics (including word length), frequency
of punctuation, line breaks, and various parts-of-
speech, lexical density, average frequency in a large
external corpus (Brants and Franz, 2006), lexicon-
based sentiment metrics using SentiWordNet (Bac-
cianella et al, 2010), formality score (Brooke et al,
2010), and, perhaps most notably, the centroid of 20-
dimensional distributional vectors built using latent
semantic analysis (Landauer and Dumais, 1997), re-
flecting the use of words in a large web corpus (Bur-
ton et al, 2009); in previous work (Brooke et al,
2010), we established that such vectors contain use-
ful stylistic information about the English lexicon
(including rare words that appear only occasionally
in such a corpus), and indeed LSA vectors were the
single most promising feature type for segmentation.
For a more detailed discussion of the feature set, see
Brooke et al (2012). All the features are normalized
to a mean of zero and a standard deviation of 1.
For clustering, we use a slightly modified ver-
sion of the popular k-means algorithm (MacQueen,
1967). Briefly, k-means assigns points to a cluster
based on their proximity to the k cluster centroids,
which are initialized to randomly chosen points from
the data and then iteratively refined until conver-
gence, which in our case was defined as a change of
less than 0.0001 in the position of each centroid dur-
ing one iteration.1 Our version of k-means is distinct
in two ways: first, it uses a weighted centroid where
the influence of each point is based on the token
length of the underlying span, i.e. short (unreliable)
spans which fall into the range of some centroid will
have less effect on the location of the centroid than
larger spans. Second, we use a city-block (L1) dis-
tance function rather than standard Euclidean (L2)
distance function; in the segmentation task, Brooke
et al found that city-block (L1) distance was pre-
ferred, a result which is in line with other work
in stylistic inconsistency detection (Guthrie, 2008).
Though it would be interesting to see if a good k
could be estimated independently, for our purposes
here we set k to be the known number of speakers in
our gold standard.
4 Evaluation
We evaluate our clusters by comparing them to a
gold standard annotation. There are various met-
rics for extrinsic cluster evaluation; Amigo? et al
1Occasionally, there was no convergence, at which point we
halted the process arbitrarily after 100 iterations.
42
(2009) review various options and select the BCubed
precision and recall metrics (Bagga and Baldwin,
1998) as having all of a set of key desirable prop-
erties. BCubed precision is a calculation of the frac-
tion of item pairs in the same cluster which are also
in the same category, whereas BCubed recall is the
fraction of item pairs in the same category which
are also in the same cluster. The harmonic mean
of these two metrics is BCubed F-score. Typically,
the ?items? are exactly what has been clustered, but
this is problematic in our case, because we wish to
compare methods which have different segmenta-
tions and thus the vectors that are being clustered
are not directly comparable. Instead, we calculate
the BCubed measures at the level of the token; that
is, for the purposes of measuring performance we
act as if we had clustered each token individually,
instead of the spans of tokens actually used.
Our first evaluation is against a set of 20
artificially-generated ?poems? which are actually
randomly generated combinations of parts of 12 po-
ems which were chosen (by an English literature ex-
pert, one of the authors) to represent the time period
and influences of The Waste Land. The longest of
these poems is 1291 tokens and the shortest is just
90 tokens (though 10 of the 12 have at least 300 to-
kens); the average length is 501 tokens. Our method
for creating these poems is similar to that of Kop-
pel et al (2011), though generalized for multiple
authors. For each of the artificial poems, we ran-
domly selected 6 poems from the 12 source poems,
and then we concatenated 100-200 tokens (or all the
remaining tokens, if less than the number selected)
from each of these 6 poems to the new combined
poem until all the poems were exhausted or below
our minimum span length (20 tokens). This allows
us to evaluate our method in ideal circumstances, i.e.
when there are very distinct voices corresponding to
different poets, and the voice spans tend to be fairly
long.
Our gold standard annotation of The Waste Land
speakers is far more tentative. It is based on a
number of sources: our own English literature ex-
pert, relevant literary analysis (Cooper, 1987), and
also The Waste Land app (Touch Press LLP, 2011),
which includes readings of the poem by various ex-
perts, including T.S. Eliot himself. However, there
is inherently a great deal of subjectivity involved in
literary annotation and, indeed, one of the potential
benefits of our work is to find independent justifi-
cation for a particular voice annotation. Our gold
standard thus represents just one potential interpre-
tation of the poem, rather than a true, unique gold
standard. The average size of the 69 segments in
the gold standard is 50 tokens; the range, however,
is fairly wide: the longest is 373 tokens, while the
shortest consists of a single token. Our annotation
has 13 voices altogether.
We consider three segmentations: the segmen-
tation of our gold standard (Gold), the segmenta-
tion predicted by our segmentation model (Auto-
matic), and a segmentation which consists of equal-
length spans (Even), with the same number of spans
as in the gold standard. The Even segmentation
should be viewed as the baseline for segmentation,
and the Gold segmentation an ?oracle? represent-
ing an upper bound on segmentation performance.
For the automatic segmentation model, we use the
settings from Brooke et al (2012). We also com-
pare three possible clusterings for each segmenta-
tion: no clustering at all (Initial), that is, we assume
that each segment is a new voice; k-means clustering
(k-means), as outlined above; and random clustering
(Random), in which we randomly assign each voice
to a cluster. For these latter two methods, which both
have a random component, we averaged our metrics
over 50 runs. Random and Initial are here, of course,
to provide baselines for judging the effectiveness of
k-means clustering model. Finally, when using the
gold standard segmentation and k-means clustering,
we included another oracle option (Seeded): instead
of the standard k-means method of randomly choos-
ing them from the available datapoints, each cen-
troid is initialized to the longest instance of a dif-
ferent voice, essentially seeding each cluster.
5 Results
Table 1 contains the results for our first evaluation
of voice clustering, the automatically-generated po-
ems. In all the conditions, using the gold segmen-
tation far outstrips the other two options. The au-
tomatic segmentation is consistently better than the
evenly-spaced baseline, but the performance is actu-
ally worse than expected; the segmentation metrics
we used in our earlier work
43
Table 1: Clustering results for artificial poems
Configuration BCubed metrics
Prec. Rec. F-score
Initial Even 0.703 0.154 0.249
Initial Automatic 0.827 0.177 0.286
Initial Gold 1.000 0.319 0.465
Random Even 0.331 0.293 0.307
Random Automatic 0.352 0.311 0.327
Random Gold 0.436 0.430 0.436
k-means Even 0.462 0.409 0.430
k-means Automatic 0.532 0.479 0.499
k-means Gold 0.716 0.720 0.710
k-means Gold Seeded 0.869 0.848 0.855
Table 2: Clustering results for The Waste Land
Configuration BCubed metrics
Prec. Rec. F-score
Initial Even 0.792 0.069 0.128
Initial Automatic 0.798 0.084 0.152
Initial Gold 1.000 0.262 0.415
Random Even 0.243 0.146 0.183
Random Automatic 0.258 0.160 0.198
Random Gold 0.408 0.313 0.352
k-means Even 0.288 0.238 0.260
k-means Automatic 0.316 0.264 0.296
k-means Gold 0.430 0.502 0.461
k-means Gold Seeded 0.491 0.624 0.550
The results for The Waste Land are in Table 2.
Many of the basic patterns are the same, including
the consistent ranking of the methods; overall, how-
ever, the clustering is far less effective. This is par-
ticularly true for the gold-standard condition, which
only increases modestly between the initial and clus-
tered state; the marked increase in recall is balanced
by a major loss of precision. In fact, unlike with
the artificial text, the most promising aspect of the
clustering seems to be the fairly sizable boost to the
quality of clusters in automatic segmenting perfor-
mance. The effect of seeding is also very consistent,
nearly as effective as in the automatic case.
We also looked at the results for individual speak-
ers in The Waste Land; many of the speakers (some
of which appear only in a few lines) are very poorly
distinguished, even with the gold-standard segmen-
tation and seeding, but there are a few that cluster
quite well; the best two are in fact our examples from
Section 1,2 that is, the narrator (F-score 0.869), and
the chatty woman (F-score 0.605). The former re-
sult is particularly important, from the perspective
of literary analysis, since there are several passages
which seem to be the main narrator (and our ex-
pert annotated them as such) but which are definitely
open to interpretation.
6 Conclusion
Literature, by its very nature, involves combin-
ing existing means of expression in surprising new
ways, resisting supervised analysis methods that de-
pend on assumptions of conformity. Our unsuper-
vised approach to distinguishing voices in poetry of-
fers this necessary flexibility, and indeed seems to
work reasonably well in cases when the stylistic dif-
ferences are clear. The Waste Land, however, is a
very subtle text, and our results suggest that we are
a long way from something that would be a consid-
ered a possible human interpretation. Nevertheless,
applying quantitative methods to these kinds of texts
can, for literary scholars, bridge the gab between
abstract interpretations and the details of form and
function (McKenna and Antonia, 2001). In our own
case, this computational work is just one aspect of
a larger project in literary analysis where the ulti-
mate goal is not to mimic human behavior per se,
but rather to better understand literary phenomena
by annotation and modelling of these phenomena
(Hammond, 2013; Hammond et al, 2013).
With respect to future enhancements, improving
segmentation is obviously important; the best au-
tomated efforts so far provide only a small boost
over a baseline approach to segmentation. However,
independently of this, our experiments with gold-
standard seeding suggest that refining our approach
to clustering, e.g. a method that identifies good ini-
tial points for our centroids, may also pay dividends
in the long run. A more radical idea for future work
would be to remove the somewhat artificial delim-
2These passages are the original examples from our earlier
work (Brooke et al, 2012), selected by our expert for their dis-
tinctness, so the fact that they turned out to be the most easily
clustered is actually a result of sorts (albeit an anecdotal one),
suggesting that our clustering behavior does correspond some-
what to a human judgment of distinctness.
44
itation of the task into segmentation and clustering
phases, building a model which works iteratively
to produce segments that are sensitive to points of
stylistic change but that, at a higher level, also form
good clusters (as measured by intrinsic measures of
cluster quality).
Acknowledgements
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12:461?486, August.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th Conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics (ACL-COLING ?98), pages 79?85, Montreal,
Quebec, Canada.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Google Inc.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Julian Brooke, Adam Hammond, and Graeme Hirst.
2012. Unsupervised stylistic segmentation of poetry
with change curves and extrinsic features. In Proceed-
ings of the 1st Workshop on Computational Literature
for Literature (CLFL ?12), Montreal.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Xiros Cooper. 1987. T.S. Eliot and the politics of
voice: The argument of The Waste Land. UMI Re-
search Press, Ann Arbor, Mich.
Joseph J. Duggan. 1973. The Song of Roland: Formulaic
style and poetic craft. University of California Press.
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 2005.
Segmenting documents by stylistic character. Natural
Language Engineering, 11(4):397?415.
David Guthrie. 2008. Unsupervised Detection of
Anomalous Text. Ph.D. thesis, University of Sheffield.
Adam Hammond, Julian Brooke, and Graeme Hirst.
2013. A tale of two cultures: Bringing literary analy-
sis and computational linguistics together. In Proceed-
ings of the 2nd Workshop on Computational Literature
for Literature (CLFL ?13), Atlanta.
Adam Hammond. 2013. He do the police in different
voices: Looking for voices in The Waste Land. Sem-
inar: ?Mapping the Fictional Voice? American Com-
parative Literature Association (ACLA).
Justine Kao and Dan Jurafsky. 2012. A computational
analysis of style, sentiment, and imagery in contem-
porary poetry. In Proceedings of the 1st Workshop on
Computational Literature for Literature (CLFL ?12),
Montreal.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011. Unsupervised decompo-
sition of a document into authorial components. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11), Port-
land, Oregon.
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Kim Luyckx, Walter Daelemans, and Edward Vanhoutte.
2006. Stylogenetics: Clustering-based stylistic analy-
sis of literary corpora. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC ?06), Genoa, Italy.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, pages 281?297.
C. W. F. McKenna and A. Antonia. 2001. The statistical
analysis of style: Reflections on form, meaning, and
ideology in the ?Nausicaa? episode of Ulysses. Liter-
ary and Linguistic Computing, 16(4):353?373.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36, March.
Dean Keith Simonton. 1990. Lexical choices and aes-
thetic success: A computer content analysis of 154
Shakespeare sonnets. Computers and the Humanities,
24(4):251?264.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
45
Benno Stein, Nedim Lipka, and Peter Prettenhofer. 2011.
Intrinsic plagiarism analysis. Language Resources
and Evaluation, 45(1):63?82.
Touch Press LLP. 2011. The Waste Land
app. http://itunes.apple.com/ca/app/the-waste-land/
id427434046?mt=8 .
46
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 188?196,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Using Other Learner Corpora in the 2013 NLI Shared Task
Julian Brooke
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Our efforts in the 2013 NLI shared task fo-
cused on the potential benefits of external cor-
pora. We show that including training data
from multiple corpora is highly effective at ro-
bust, cross-corpus NLI (i.e. open-training task
1), particularly when some form of domain
adaptation is also applied. This method can
also be used to boost performance even when
training data from the same corpus is available
(i.e. open-training task 2). However, in the
closed-training task, despite testing a number
of new features, we did not see much improve-
ment on a simple model based on earlier work.
1 Introduction
Our participation in the 2013 NLI shared task
(Tetreault et al, 2013) follows on our recent work
exploring cross-corpus evaluation, i.e. using dis-
tinct corpora for training and testing (Brooke and
Hirst, 2011; Brooke and Hirst, 2012a; Brooke and
Hirst, 2012b), an approach that is now becoming
fairly standard alternative in relevant work (Bykh
and Meurers, 2012; Tetreault et al, 2012; Swan-
son and Charniak, 2013). Our promotion of cross-
corpus evaluation in NLI was partially motivated by
serious issues with the most popular corpus for na-
tive language identification work up to now, the In-
ternational Corpus of Learner English (Granger et
al., 2009). The new TOEFL-11 (Blanchard et al,
2013) used for this NLI shared task addresses some
of the problems with the ICLE (most glaringly, the
fact that some topics in the ICLE appeared only in
some L1 backgrounds), but, from the perspective of
topic, proficiency, and particularly genre, it is nec-
essarily limited in scope (perhaps even more so than
the ICLE); in short, it addresses only a small por-
tion of the space of learner texts. Our interest, then,
continues to be in robust models for NLI that are not
restricted to utility in a particular corpus, and in our
participation in this task we have focused our efforts
on the open-training tasks which allow the use of
corpora beyond the TOEFL-11. Since participation
in these tasks was low relative to the closed-training
task, fewer papers will address them, making our
emphasis here all the more relevant.
The models built for all of three of the tasks are
extensions of the model used in our recent work
(Brooke and Hirst, 2012b); we will discuss the as-
pects of this model common to all tasks in Section
2. Section 3 is a brief review of our methodology
and results in the closed-training task, which was fo-
cused exclusively on testing features (both new and
old); we found almost nothing that improved on our
best feature set from previous work, and most fea-
tures actually hurt performance. In Section 4, we
discuss the corpora we used for the open-training
tasks, some of which we collected and/or have not
been applied to NLI before. Our approach to the
open-training task 2 using these corpora is presented
in Section 5. In Section 6, we discuss how we used
domain adaption methods and our various external
corpora to create the (winning) model for the open-
training task 1, which did not permit usage of the
TOEFL-11; we also present some post hoc testing
(now that TOEFL-11 is no longer off limits). In Sec-
tion 7 we offer conclusions.
188
2 Basic Model
In our recent work on cross-corpus NLI (Brooke and
Hirst, 2012b), we tested a number of classifier and
feature options, and most of our choices there are
carried over to this work. In particular, we use the
Liblinear SVM 1va (one versus all) classifier (Fan et
al., 2008). Using the TOEFL-11 corpus, we briefly
tested the other options explored in that paper (in-
cluding SVM 1v1) as well as the logistic regression
classifier included in Liblinear, and found that the
SVM 1va classifier was still preferred (with our best
feature set, see below), though the differences in-
volved were marginal. Although small variations in
the choice of C parameter within the SVM model
did occasionally produce benefits (here and in our
previous work), these were not consistent, whereas
the default value of 1 showed consistently near opti-
mal results. We used a binary feature representation,
and then feature vectors were normalized to the unit
circle. With respect to feature selection, our earlier
work used a frequency cutoff of 5 for all features; we
continue to use frequency cutoffs here; other com-
mon feature selection methods (e.g. use of informa-
tion gain) were ineffective in our previous work, so
we did not explore them in detail here.
With regards to the features themselves, our ear-
lier work tested a fairly standard collection of distri-
butional features, including function words, word n-
grams (up to bigram), POS n-grams (up to trigram),
character n-grams (up to trigram), dependencies,
context-free productions, and ?mixed? POS/function
n-grams (up to trigram), i.e. n-grams with all lex-
ical words replaced with part of speech. Most of
these had appeared in previous NLI work (Koppel
et al, 2005; Wong and Dras, 2011; Wong et al,
2012), though until recently word n-grams had been
avoided because of ICLE topic bias. Our best model
used only two of these features, word n-grams and
the mixed POS/function n-grams. This was our
starting point for the present work. The Stanford
parser (Klein and Manning, 2003) was used for POS
tagging and parsing.
Obviously, the training set used varies through-
out the paper, and other differences in specific mod-
els built for each task will be mentioned as they
become relevant. For evaluation here, we primar-
ily use the test set for NLI shared task, though we
Table 1: Feature testing for closed-training task, previ-
ously investigated features; best result is in bold.
Feature Set Accuracy (%)
Word+mixed 76.8
Word+mixed+characters 72.0
Word+mixed+POS 76.6
Word+mixed+productions 77.9
Word+mixed+dependencies 78.9
Word+mixed+dep+prod 78.4
employ some other evaluation corpora, as appropri-
ate. During the preparation for the shared task, we
made our decisions regarding models for two tasks
with TOEFL-11 training according to the results in
two training/test sets (800 per language for training,
100 per language for testing) sampled from the re-
leased training data. Since our research was focused
on cross-corpus evaluation, we never created mecha-
nisms for cross-validation in our system, and in fact
it creates practical difficulties for the open-training
task 2, so we do not include cross-validated results
here.
3 Closed-training Task
Our approach to the closed-training task primarily
involved feature testing. Table 1 contains the re-
sults of testing our previously investigated features
from Brooke and Hirst (2012b) in the TOEFL-11,
pivoted around the best set (word n-grams + mixed
POS/Function n-grams) from that earlier work.
Some of the features we rejected in our previous
work also underperform here, in particular charac-
ter and POS n-grams. In fact, character n-grams had
a much more negative effect on performance here
than they had previously. Dependencies are clearly a
useful feature in the TOEFL-11, this is fully consis-
tent with out initial testing. CFG productions offer a
small benefit on top of our base feature set, but are
not useful when dependencies are also included, so
we discarded them. Thus, our feature set going for-
ward consists of word n-grams, mixed POS/function
n-grams, and dependencies.
Next, we evaluate our feature frequency cutoff us-
ing this feature set (Table 2). We used the rather high
cutoff of 5 (for all features) in the previous work be-
cause of our much larger training set. We looked at
189
Table 2: Feature frequency cutoff testing for closed-
training task; best result is in bold.
Cutoff Accuracy (%)
At least 5 occurrences 78.9
At least 3 occurrences 79.5
At least 2 occurrences 79.7
All features 80.2
higher values there, but for this task we focused on
testing lower values.
Lowering our frequency cutoff is indeed benefi-
cial, and we got our best result in the test set when
we had no feature selection at all. This was not con-
sistent with our preparatory testing, which showed
some benefit to removing hapax legomena, though
the difference was marginal. However, we did in-
clude a run with this option in our final submis-
sion, and so this last result represents our best per-
formance on the closed-training task.
We tested several other feature options that were
added to our system for this task. Inspired by Bykh
and Meurers (2012), we first considered n-grams
(up to trigrams) where at least one lexical word is
abstracted to its POS, and at least one isn?t (par-
tial abstraction). Since dependencies were found to
be a positive feature, we tried adding dependency
chains, which combine two dependencies, i.e. three
lexical words linked by two grammatical relations.
We tested productions with wild cards, e.g. S? NP
VP * matches any sentence production which starts
with NP VP. Tree Substitution grammar fragments
have been shown to be superior to CFG produc-
tions (Swanson and Charniak, 2012); we used raw
Tree Substitution Grammar (TSG) fragments for the
TOEFL-111 and tested a subset of those fragments
which involved at least two levels of the grammar
(i.e. those not already covered by n-grams or CFG
productions).
Our final feature option requires slightly more
explanation. Crossley and McNamara (2012) re-
port that metrics associated with word concreteness,
imagability, meaningfulness, and familiarity are use-
ful for NLI; the metrics they use are derived from the
MRC Psycholinguistic database (Coltheart, 1980),
1We thank Ben Swanson for letting us use his TSG frag-
ments.
Table 3: Feature testing for closed-training task, new fea-
tures; best result is in bold.
Feature Set Accuracy (%)
Best 80.2
Best+partial abstraction 79.7
Best+dependency chains 78.6
Best+wild card productions 78.8
Best+TSG fragments 78.1
Best+MRC lexicon 54.2
which assign values for each dimension to individ-
ual words. We used the scores in the MRC to get
an average score for each dimension for each text,
further normalized to the range 0?1; texts with no
words in the dictionaries were assigned the average
across the training set.
Table 3 indicates that all of these new features
were, to varying degrees, a drag on our model. The
strongly negative effect of the MRC lexicons is par-
ticularly surprising. We speculate that this might
might be due partially to problems with combining
a large number of binary features with a small num-
ber of continuous metrics directly in a single SVM.
A meta-classifier might solve this problem, but we
did not explore meta-classification for features here.
Finally, since that information was available to
us, we tested creating sub-models segregated by
topic and proficiency. The topic-segregated model
consisted of 8 SVMs, one for each topic; accu-
racy of this model was quite low, only 67.3%. The
proficiency-segregated model used two groups, high
and low/medium (there were few low texts, so we
did not think they would be sufficient by themselves
for a viable model). Results were higher, 74.9%, but
still well below the best unsegregated model.
4 External Corpora
In this section we review corpora which will be used
for the open-training tasks in the next two sections.
Including the TOEFL-11, there are at least six pub-
licly available multi-L1 learner text corpora for NLI,
with many of these corpora becoming available rel-
atively recently. Below, we introduce each corpus in
detail; a summary of the number of tokens from each
L1 background for each of the corpora is in Table 4.
190
Table 4: Number of tokens (in thousands) in external learner corpora, by L1.
L1
Corpus
Lang-8 (new) ICLE FCE ICCI ICNALE
Japanese 11694k 227k 33k 232k 199k
Chinese 7044k 552k 30k 243k 366k
Korean 5174k 0k 37k 0k 151k
French 536k 256k 61k 0k 0k
Spanish 861k 225k 83k 49k 0k
Italian 450k 251k 31k 0k 0k
German 331k 258k 29k 91k 0k
Turkish 51k 222k 22k 0k 0k
Arabic 218k 0k 0k 0k 0k
Hindi 11k 0k 0k 0k 0k
Telugu 2k 0k 0k 0k 0k
Lang-8 Lang-8 is a website where language learn-
ers write journal entries in their L2 to be corrected
by native speakers. We collected a large set of these
entries, which we?ve shown to be useful for NLI
(Brooke and Hirst, 2012b), despite the noisiness of
the corpus (for instance, some entries directly mix
L1 and L2). For this task we added more entries
written since the first version was collected (58k
on top of the existing 154k entries).2 The corpus
contains entries from all the L1 backgrounds in the
TOEFL-11, though the amounts for Hindi and par-
ticularly Telugu are small. Since many of the entries
are very short, as in our previous work we add en-
tries of the same L1 together to reach a minimum
size of 250 tokens.
ICLE Before 2011, nearly all work on NLI was
done in the International Corpus of Learner English
or ICLE (Granger et al, 2009), a collection of col-
lege student essays from 15 L1 backgrounds, 8 of
which overlap with the 11 L1s in the TOEFL-11.
Despite known issues that might cause problems
(Brooke and Hirst, 2011), it is probably the closest
match in terms of genre and writer proficiency to the
TOEFL-11.
FCE What we call the FCE corpus is a small
sample of the First Certificate in English portion
of the Cambridge Learner Corpus, which was re-
2We do not have permission to distribute the corpus directly;
however, we can offer a list of URLs together with software
which can be used to recreate the corpus.
leased for the purposes of essay scoring evaluation
(Yannakoudakis et al, 2011); 16 different L1 back-
grounds are represented, 9 of which overlap with the
TOEFL-11. Each of the texts consists of two short
answers in the form of a letter, a report, an article,
or a short story. Relative to the other corpora, the
actual amount of text in the FCE is small.
ICCI Like the ICLE and TOEFL-11, the Inter-
national Corpus of Crosslinguistic Interlanguage
(Tono et al, 2012) is also an essay corpus, though
in contrast with other corpora it is focused on young
learners, i.e. those in grade school. It includes both
descriptive and argumentative essays on a number of
topics. Only 4 of its L1s overlap with the TOEFL-
11.
ICANLE The International Corpus Network of
Asian Learners of English or ICANLE (Ishikawa,
2011) is a collection of essays from college students
in 10 Asian countries; 3 of the L1s overlap with the
TOEFL-11.3 Even more so than the TOEFL-11, this
corpus is strictly controlled for topic, it has only 2
topics (part-time jobs and smoking in restaurants).
One obvious problem with using the above cor-
pora to classify L1s in the TOEFL-11 is the lack
of Hindi and Telugu text, which we found were
the two most easily confused L1s in the closed-
3The ICANLE also contains 103K of Urdu text. Since Urdu
and Hindi are mutually intelligible, this could be a good substi-
tute for Hindi; we overlooked this possibility during our prepa-
ration for the task, unfortunately.
191
Table 5: Number of tokens (in thousands) in Indian cor-
pora, by expected L1.
L1
Indian Corpus
News Twitter Blog
Hindi 996k 146k 2089k
Telugu 998k 133k 76k
training task. We explored a few methods to get
data to fill this gap. First, we downloaded two
collections of English language Indian news arti-
cles, one from a Hindi newspaper, the Hindus-
tan Times, and one from a Telugu newspaper, the
Andhra Jyothy.4 Second, we extracted a collection
of English tweets from the WORLD twitter corpus
(Han et al, 2012) that were geolocated in the Hindi
and Telugu speaking areas; as with the Lang-8, these
were combined to create texts of at least 250 tokens.5
Our third Indian corpus consists of translations (by
Google Translate) of Hindi and Telugu blogs from
the ICWSM 2009 Spinn3r Dataset (Burton et al,
2009), which we used in other work on using L1
text for NLI (Brooke and Hirst, 2012a). The number
of tokens in each of these corpora are given in Table
5.
5 Open-training Task 2
Our approach to open-training task 2 is based on the
assumption that in many ways it is a direct extension
of the closed-training task. For example, we directly
use the best feature set from that task, with no further
testing. Based on the results in our initial testing,
we used a feature frequency cutoff of 2 during our
testing for open-training task 2; for consistency, we
continue with that cutoff in this section.
We first attempted to integrate information from
other corpora by using a meta-classifier, as was suc-
cessfully used for features by Tetreault et al (2012).
Briefly, classifiers were trained on each major exter-
nal corpus (including only the L1s in the TOEFL-
11), and then tested on the TOEFL-11 training set;
4As with the Lang-8, we cannot distribute the corpus di-
rectly but would be happy to provide URLs and scraping soft-
ware for those would like to build it themselves.
5We extracted India regions 07 and 36 for Hindi, and 02 and
25 for Telegu; We can provide a list of tweet ids for reconstruct-
ing the corpus if desired. Our thanks to Bo Han and Paul Cook
for helping us get these tweets.
TOEFL-11 training was accomplished using 10-fold
crossvalidation (by modifying the code for Liblin-
ear crossvalidation to output margins). With the
TOEFL-11 as the training set, the SVM margins
from each 1va classifier (across all L1s and all cor-
pora) were used as the feature input to the meta-
classifier (also an SVM). In addition to Liblinear,
we also outputted this meta-classification problem to
WEKA format (Witten and Frank, 2005), and tested
a number of other classifier options not available
in Liblinear (e.g. Na??ve Bayes, decision trees, ran-
dom forests). In addition to (continuous) margins,
we also tested using the classification directly. Ul-
timately, we came to the conclusion were that any
use of a meta-classifer came with a cost (a mini-
mum 2?3% drop in performance) that could not be
fully overcome with the additional information from
our external corpora. The result using SVM classi-
fiers, margin features, and an SVM meta-classifier
was 78.5%, well below the TOEFL-11?only base-
line.
The other approach to using these external cor-
pora is to add the data directly to the TOEFL-11 data
and train a single classifier. This is very straightfor-
ward; really the only variable is which corpora will
be included. However, we need to introduce, at this
point, a domain-adaptation technique from our most
recent work (Brooke and Hirst, 2012b), bias adap-
tion, which we used to greatly improve the accu-
racy of cross-corpus classification. Without getting
into the algorithmic details, bias adaption involves
changing the bias (constant) factor of a model until
the output of the model in some dataset is balanced
across classes (or otherwise fits the expected distri-
bution); it partially addresses skewed results due to
differences between training and testing corpora. In
the previous work, we used a separate development
set, but here we rely on the test set itself; since the
technique is unsupervised, we do not need to know
the classes. Table 6 shows model performance after
adding various corpora to the training set (TOEFL-
11 is always included), with and without bias adap-
tion (BA).
Many of the differences in Table 6 are modest,
but there are are few points to be made. First,
there is a small improvement using either the Lang-
8 or the ICLE as additional data. The ICCI, on the
other hand, has a clearly negative effect, perhaps be-
192
Table 6: Corpus testing for open-training task; best result
is in bold.
Training Set
Accuracy (%)
no BA with BA
TOEFL-11 only 79.7 79.2
+Lang-8 79.5 80.5
+ICLE 80.2 80.2
+FCE 79.6 79.3
+ICCI 77.3 76.7
+ICANLE 79.7 79.3
+Lang-8+ICLE 80.4 80.4
+all but ICCI 80.0 80.4
cause of the age or proficiency of the contributors to
that corpus. Bias adaption seems to help when the
(messy and highly unbalanced) Lang-8 is involved
(consistent with our previous work), but it does not
seem useful applied to other corpora, at least not in
this setting.
Our second adaptation technique involves training
data selection, which has been used, for instance in
cross-domain parsing (Plank and van Noord, 2011).
The method used here is very simple: we count the
number of times each word appears in a document in
our test data, rank the texts in our training data ac-
cording to the sum of counts (in the test data) each
word that appears in a training texts, and throw away
a certain numbers of low-ranked texts. For example,
if a training text consists solely of the two words I
agree6 and I appears in 1053 texts in the test set,
and agree appears in 325, then the value for that text
is 1378. This method simultaneously penalizes short
texts, those texts with low lexical diversity, and texts
that do not use the same words as our test set. We
use a fixed cutoff, r, which refers to the proportion
of training data that is thrown away for each L1 (al-
lowing this to work independent of L1 was not ef-
fective). We tested this on this method in tandem
with bias adaption on two corpus sets: The TOEFL-
11 and the Lang-8, and all corpora except the ICCI.
The results are in Table 7. The number in italics is
the best run that we submitted.
Again, it is difficult to come to any firm con-
clusions when the differences are this small, but
6This is not a made-up example; there is actually a text in
the TOEFL-11 corpus like this.
Table 7: Training set selection testing for open-training
task 2; best result is in bold, best submitted run is in ital-
ics.
Training Set
Accuracy (%)
no BA with BA
TOEFL-11 only 79.7 79.2
+Lang-8 79.5 80.5
+Lang-8 r = 0.1 81.4 81.6
+Lang-8 r = 0.2 80.6 81.5
+Lang-8 r = 0.3 81.0 80.6
+all but ICCI 80.0 80.4
+all but ICCI r = 0.1 81.5 82.5
+all but ICCI r = 0.2 81.0 81.6
+all but ICCI r = 0.3 80.9 81.3
our best results involve all of the corpora (except
the ICCI) and both adaptation techniques. Unfor-
tunately, our initial testing suggested r = 0.2 was
the better choice, so our official best result in this
task (81.6%) is not the best result in this table. Per-
formance clearly drops for r > 0.2. Nevertheless,
nearly all the results in the table show clear improve-
ment on our closed-training task model.
6 Open-training Task 1
The central challenge of open-training task 1 was
that the TOEFL-11 was completely off-limits, even
for testing. Therefore, a discussion of how we pre-
pared for this task is very distinct from a post hoc
analysis of the best method once we allowed our-
selves access to the TOEFL-11; we separate the two
here. We did use the feature set (and frequency cut-
off) from the closed-training (and open-training 2)
task; it was close enough to the feature set from our
earlier work (using the Lang-8, ICLE, and FCE) that
it did not seem like cheating to preserve it.
6.1 Method
Given our failure to create a meta-classifier in open-
training task 2, we did not pursue that option here,
focusing purely on adding corpora directly to a
mixed training set. The central question was which
corpora to add, and whether to use our domain-
adaptation methods. Our experience with the ICCI
in the open-training task 2 suggested that it might be
worth leaving it (or perhaps other corpora) out, but
193
Table 8: ICLE testing for Open-training task 1; best result
is in bold.
Training Set
Accuracy (%)
no BA with BA
Lang-8 47.0 57.1
Lang-8+FCE 47.9 58.2
Lang-8+ICCI 46.4 54.8
Lang-8+ICNALE 46.9 57.5
Lang-8+ICNALE+FCE 47.7 58.8
Lang-8+ICNALE+FCE r = 0.1 46.6 58.2
could we come to that conclusion independently?
Our approach involved considering each external
corpus as a test set, and seeing which other corpora
were useful when included in the training set; cor-
pora which were consistently useful would be in-
cluded in the final set. Our original exploration in-
volved looking at all of the corpora (as test sets),
but it was haphazard; here, we present results just
with the ICLE and the ICANLE, which are arguably
the two closest corpora to the TOEFL-11 in terms
of proficiency and genre. For this, we used a dif-
ferent selection of L1s, 12 for the ICLE, 7 for the
ICANLE; all of these languages appeared in at least
the Lang-8, and 2 of them (Chinese and Japanese)
appeared in all corpora. Both sets were balanced by
L1. Again, we report results with and without bias
adaption. The results for the ICLE are in Table 8.
The clearest result in Table 8 is the consistently
positive effect of bias adaption, at least 10 percent-
age points, which is line with our previous work.
Adding both ICLE and ICNALE to the Lang-8 cor-
pus gave a small boost in performance, but the effect
of the ICCI was once again negative, as was the ef-
fect of our training set selection.
The ICNALE results in Table 9 support many of
the conclusions that we reached in the ICLE (and
other sets like the FCE and ICCI, which are not in-
cluded here but gave similar results); the effect of
bias adaption is even more pronounced. Two dif-
ferences: the slightly positive effect of training data
selection and the positive effect of the ICCI, the lat-
ter of which we saw nowhere else. We speculate
that this might be due to that fact that although the
ICNALE is a college-level corpus, it is a corpus of
Table 9: ICNALE testing for open-training task 1; best
result is in bold.
Training Set
Accuracy
no BA with BA
Lang-8 37.2 59.6
Lang-8+FCE 37.9 61.3
Lang-8+ICCI 35.7 61.4
Lang-8+ICLE 37.3 61.4
Lang-8+ICLE+FCE 37.6 61.7
Lang-8+ICLE+FCE r = 0.1 37.7 61.9
Asian-language native speakers. Our theory is that
Europeans are, on average, more proficient users
of English (this is supported by, for instance, the
testing from Granger et al (2009)), and that there-
fore the European component of the low-proficiency
ICCI actually interferes with using high proficiency
as a way of distinguishing European L1s, a problem
which would obviously not extend to an Asian-L1-
only corpus. This is an interesting result, but we will
not explore it further here. In any case, it would lead
us to predict that including ICCI data would be a bad
idea for TOEFL-11 testing.
Since we did not have any way to evaluate our
Indian corpora (i.e. the news, twitter, and translated
blogs from Section 4) without using the TOEFL-11,
we instead took advantage of the option to submit
multiple runs, submitting runs which use each of the
corpora, and combining the blogs and news.
6.2 Post Hoc Analysis
With the TOEFL-11 data now visible to us, we first
ask whether our specially collected Indian corpora
can distinguish texts in the ICCI. The test set used
in Table 10 contains only Hindi and Telugu texts.
The results are quite modest (the guessing baseline
is 50%), but suggest that all three corpora contain
some information that distinguish Hindi and Telugu,
particularly if bias adaption is used.
The results for a selection of models on the full
set of TOEFL-11 languages is presented in Table
11. Since ours was the best-performing model in
this task, we include results for both the TOEFL-
11 training (including development set) and test set,
to facilitate future comparison. Again, there is little
doubt that bias adaption is of huge benefit, though
in fact our results in the Lang-8 alone, without bias
194
Table 11: 11-language testing on TOEFL-11 sets for open-training task 1; best result is in bold, best submitted run is
in italics.
Training Set
Accuracy (%)
TOEFL-11 test TOEFL-11 training
no BA with BA no BA with BA
Lang-8 39.5 53.2 37.2 48.2
Lang-8+ICCI 36.9 51.0 34.9 46.3
Lang-8+FCE+ICLE+ICNALE 44.5 55.8 44.9 53.1
Lang-8+FCE+ICLE+ICNALE+Indian news 45.2 56.5 45.5 54.9
Lang-8+FCE+ICLE+ICNALE+Indian tweets 44.9 56.4 45.1 53.4
Lang-8+FCE+ICLE+ICNALE+Indian translated blog 45.4 50.1 45.7 49.9
Lang-8+FCE+ICLE+ICNALE+News+Tweets 45.2 57.5 45.5 55.2
Lang-8+FCE+ICLE+ICNALE+News+Tweets r = 0.1 44.9 58.2 45.0 58.2
Table 10: Indian corpus testing for Open-training task 1;
best result is in bold.
Training Set
Accuracy (%)
no BA with BA
Indian news 50.0 54.0
Indian tweets 54.0 56.0
Indian blogs 51.5 56.0
adaption, would have been enough to take first place
in this task. Adding other corpora, including the In-
dian corpora but not the ICCI, did consistently im-
prove performance, as suggested by our testing in
other corpora. Although the translated blog data was
useful in distinguishing Hindi from Telugu alone, it
had an unpredictable effect in the main task, lower-
ing bias-adapted performance. Training set selection
does seem to have a small positive effect, though we
did not see this consistently in our original testing.
7 Conclusion
Our efforts in the 2013 NLI shared task focused on
the potential benefits of external corpora. We have
shown here that including training data from multi-
ple corpora is effective at creating good cross-corpus
NLI systems, particularly when domain adaptation,
i.e. bias adaption or training set selection, is also
applied; we were the highest-performing group in
open-training task 1 by a large margin. This ap-
proach can also be applied to improve performance
even when training data from the same corpus is
available, as in open-training task 2. However, in
the closed-training task, despite testing a number
of new features, we did not see much improvement
on our simple model based on earlier work. Other
teams clearly did find some ways to improve on
this straightforward approach, and we hope to see
to what extent those improvements are generalizable
across different NLI corpora.
Acknowledgements
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native English. Technical report, Edu-
cational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native lan-
guage detection with ?cheap? learner corpora. Pre-
sented at the 2011 Learner Corpus Research Con-
ference. Published in Sylviane Granger, Gae?tanelle
Gilquin and Fanny Meunier, editors, (2013) Twenty
Years of Learner Corpus Research: Looking back,
Moving ahead. Corpora and Language in Use - Pro-
ceedings 1, Louvain-la-Neuve: Presses universitaires
de Louvain.
Julian Brooke and Graeme Hirst. 2012a. Measuring in-
terlanguage: Native language identification with L1-
influence metrics. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC ?12), pages 779?784, Istanbul,
Turkey.
195
Julian Brooke and Graeme Hirst. 2012b. Robust, lexical-
ized native language identification. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING ?12).
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
Serhiy Bykh and Detmar Meurers. 2012. Native lan-
guage identification using recurring n-grams ? in-
vestigating abstraction and domain dependence. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12).
Max Coltheart. 1980. MRC Psycholinguistic Database
User Manual: Version 1. Birkbeck College.
Scott A. Crossley and Danielle S. McNamara. 2012. De-
tecting the first language of second language writers
using automated indicies of cohesion, lexical sophis-
tication, syntactic complexity and conceptual knowl-
edge. In Scott Jarvis and Scott A. Crossley, editors,
Approaching Language Transfer through Text Clas-
sification: Explorations in the Detection-based Ap-
proach. Multilingual Matters.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvain-la-Neuve.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Geolo-
cation prediction in social media data by finding loca-
tion indicative words. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING ?12).
Shin?ichiro Ishikawa, 2011. A new horizon in learner
corpus studies: The aim of the ICNALE project, pages
3?11. University of Strathclyde Press, Glasgow, UK.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423?430.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by min-
ing a text for errors. In Proceedings of the 11th
ACM SIGKDD International Conference on Knowl-
edge Discovery in Data Mining (KDD ?05), pages
624?628, Chicago, Illinois, USA.
Barbara Plank and Gertjan van Noord. 2011. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1566?1576, Portland, Oregon, USA,
June.
Ben Swanson and Eugene Charniak. 2012. Native lan-
guage detection with tree substitution grammars. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?12), pages
193?197, Jeju, Korea.
Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language acqui-
sition. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL HLT ?13).
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native lan-
guage identification. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING ?12).
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
Summary report on the first shared task on native lan-
guage identification. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NLP, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Yukio Tono, Yuji Kawaguchi, and Makoto Minegishi,
editors. 2012. Developmental and Cross-linguistic
Perspectives in Learner Corpus Research. John Ben-
jamins, Amsterdam/Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
?11), pages 1600?1610, Edinburgh, Scotland, UK.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring adaptor grammars for native lan-
guage identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL ?12), Jeju, Korea.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 180?189, Portland, Oregon.
196
