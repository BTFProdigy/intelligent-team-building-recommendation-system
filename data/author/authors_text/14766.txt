A word-grammar based morl)hoh)gieal nalyzer 
for agglutinative languages 
Aduriz 1.+, Agirre E., Aldezabal I., Alegria I., Arregi X., Arriohl J. M., Artola X., Gojenola K., 
Marilxalar A., Sarasola K., Urkia M.+ 
l)ept, of Colllptiier 1Aulgtlages and Systems, University of lhe Basqtlo Cotlnlry, 64.9 P. K., 
E-20080 1)onostia, Basque Counh'y 
tUZEI, Aldapeta 20, E-20009 1)onostia, Basque Country 
+Universidad de Barcelona, Grin Vfii de Isis Cortes CalaiallaS, 585, E-08007 Flarcelona 
j ipgogak @ si.elm, es. 
Abst rac l  
Agglutinative languages presenl rich 
morphology and for sonic applications 
they lleed deep analysis at word level. 
Tile work here presenled proposes a 
model for designing a full nlorpho- 
logical analyzer. 
The model integrates lhe two-level 
fornlalisnl alld a ullificalion-I)asod 
fornialisni. In contrast to other works, 
we propose to separate the treatment of 
sequential and non-sequetTtial mou)ho- 
lactic constraints. Sequential constraints 
are applied in lhe seglllenlalion phase, 
and non-seqtlontial OlleS ill the filial 
feature-combination phase. Early appli- 
cation of sequential nlorpholactic 
coilsli'aiills during tile segnloillaiioi/ 
process nlakes feasible :,ill officienl 
iinplenleilialion of tile full morpho- 
logical analyzer. 
The result of lhis research has been tile 
design and imi)len~entation of a full 
nlorphosynlactic analysis procedure for 
each word in unrestricted Basque texts. 
I n t roduct ion  
Morphological analysis of woMs is a basic 
tool for automatic language processing, and 
indispensable when dealing willl highly 
agglutinative languages like Basque (Aduriz el 
al., 98b). In lhis conlext, some applications, 
like spelling corfeclion, do ilOI need illOl'e lhan 
the seglllOlltation of each word inlo its 
different COlllponenl nlorphellles alollg with 
their morphological information, ltowever, 
there are oiher applications such as lemnializa- 
tion, lagging, phrase recognition, and 
delernlinaiion of clause boundaries (Aduriz el 
al., 95), which need an additional global 
morphological i)arsing j of the whole word. 
Such a complete nlorphological analyzer has 
lo consider three main aspects (l~,ilchie et al, 
92; Sproal, 92): 
1 Morl)hographenfics (also called morpho- 
phonology). This ternl covers orthographic 
variations that occur when linking 
I l lOfphellleS. 
2) morpholactics. Specil'ication of which 
nlorphenles can or cannot combine with 
each other lo form wflid words. 
3) Feature-combination. Specification of how 
these lnorphemes can be grouped and how 
their nlorphosyntactic features can be 
comlfined. 
The system here presented adopts, oil the one 
hand, tile lwo-level fornlalisnl to deal with 
morphogralfilemics and sequential morl)ho- 
lactics (Alegria el al., 96) and, on the other 
hand, a unification-based woM-grammar 2 to 
combine the grammatical information defined 
in nlorphemes and to  tackle complex 
nlorphotactics. This design allowed us to 
develop a full coverage analyzer that processes 
efl'iciently unrestricted texts in Basque. 
The remainder of tills paper is organized sis 
follows. After a brief' description of Basque 
nlorphology, section 2 describes tile 
architecture for morphological processing, 
where the morphosynlactic omponent is 
included. Section 3 specifies tile plaenomena 
covered by the analyzer, explains its desigi~ 
criteria, alld presents implementation and 
ewthialion details. Section d compares file 
I This has also been called mo*7)hOSh,ntactic 
parsitlg. When we use lhc \[(fill #11017~\]lOSyltl~/X WC 
will always refer to il~c lficrarchical structure at 
woM level, conlbining morphology and synlax. 
2 '\]'\]lt3 \[IDl'll\] WOl'd-gF(lllllllUl" should not be confused 
with the synlaclic lilcory presented in (Hudson, 84). 
system with previous works. Finally, the paper 
ends with some concluding renmrks. 
1 Brief description of Basque 
morphology 
These are the most important features of 
Basque morphology (Alegria et al, 96): 
? As prepositional functions are realized by 
case suffixes inside word-fornls, Basque 
presents a relatively high power to generate 
inflected word-forms. For instance, froth a 
single noun a minimum of 135 inflected 
forms can be generated. Therefore, the 
number of simple word-forms covered by 
the current 70,000 dictionary entries woukl 
not be less than 10 million. 
? 77 of the inflected forms are simple 
combinations of number, determination, 
and case marks, not capable of further 
inflection, but the other 58 word-forms 
ending in one of the two possible genitives 
(possessive and locative) can be further 
inflected with the 135 morphemes. This 
kind of recursive construction reveals a 
noun ellipsis inside a noun phrase and 
could be theoretically exteuded ad 
infinitum; however, in practice it is not 
usual to fiud more than two levels of this 
kind of recursion in a word-form. Taking 
into account a single level of noun ellipsis, 
the number of word-forum coukl be 
estimated over half a billion. 
? Verbs offer a lot of grammatical 
information. A verb tbrln conveys informa- 
tion about the subject, the two objects, as 
well as the tense and aspect. For example: 
diotsut (Eng.: 1 am telling you something). 
o Word-formation is very productive in 
Basque. It is very usual to create new 
compounds as well as derivatives. 
As a result of this wealth of infornmtion 
contained within word-forms, complex struc- 
tures have to be built to represent complete 
morphological information at word level. 
2 An architecture for the full 
morphological ana lyzer  
The framework we propose for the 
morphological treatment is shown in Figure 1. 
The morphological nalyzer is the fiont-end to 
all present applications for the processing of 
Basque texts. It is composed of two modules: 
the segmentation module and the 
morphosyntactic analyzer. 
conformant .................. ~ U~atabas N TEZ-conf~ 
\[Segmentation module 
____~| HorphograDhemics 
Morphotactics I 
TEI-FS .............. ~ ~ ~ ~  ~ - p ~  
conformant Cegmented TexN 
Morphosyntactic 
analyzer 
Feature- combination 
Morphotactics II 
TEI-FS \] .............. ~ actically 
Lermnatization, linguistic Analysis tagging tools 
Figure 1. Architecture 1"o1" morphological processing. 
The segmentation ,nodule was previously 
implemented in (Alegria et al, 96). This 
system applies two-level morphology 
(Koskenniemi, 83) for the morphological 
description and obtains, for each word, its 
possible segmentations (one or many) into 
component morphemes. The two-level system 
has the following components: 
? A set of 24 morphograf~hemic rules, 
compiled into transducers (Karttunen, 94). 
? A lexicon made up of around 70,000 items, 
grouped into 120 sublexicons and stored in 
a general lexical database (Aduriz et al, 
98a). 
This module has full coverage of free-running 
texts in Basque, giving an average number of 
2.63 different analyses per word. The result is 
the set of possible morphological segmenta- 
tions of a word, where each morpheme is 
associated with its corresponding features in 
the lexicon: part of speech (POS), 
subcategory, declension case, number, 
definiteness, as well as syntactic function and 
some semantic features. Therefore, the output 
of the segmeutation phase is very rich, as 
shown in Figure 2 with the word amarengan 
(Eng.: on the mother). 
grammar 
mother) 
POS noun) 
subc~t common 
:count: +) 
(an imate  +) 
(nleasurable "-) 
aren 
(of life) 
(POS decl-suffix) 
(definite +) 
(number sing) 
(case genitive) 
(synt-f @nouncomp) 
J gan \] 
(o.1 / 
(POS decl-suf fix) I 
(case inossivo) \] 
(synt-f @adverbial)I 
=> 
amarengan 
(o. the mother) 
POS noun) 
subcat common) 
number sing) 
definite +) 
case inessive) 
count +) 
animate +) 
measurable -) 
synt-f @adverbial) 
iq:e, ure 2. Morphosynlactic analysis eof (unureugun (l{ng.: (m 
The architecture is a modular envhoument that 
allows different ypes of output depending on 
the desired level of analysis. The foundation of 
the architecture lies in the fact lhat TEI- 
confommnt SGML has been adopted for the 
comnmnication allloIlg modules (Ide and 
VCFOIIiS, 95). l~'eature shucluleS coded 
accoMing TIU are used to represent linguistic 
information, illcluding tile input mM outl)ut of 
the morplaological analyzer. This reprcscnta- 
tion rambles the use of SGML-aware parsers 
and tools, and Call he easily filtered into 
different formats (Artola et ill., 00). 
3 Word level morl)hosyntactic analysis 
This section Hrst presents the l~henomena lhat 
must be covered by the morphosyntactic 
analyzer, then explains ils design criteria, and 
finally shows implementation and ewfluation 
details. 
3.1 Phenomena covered by the analyzer 
There are several features that emphasized the 
need of morphosyntactic almlysis in order to 
build up word level information: 
I) Multiplicity of values for the same feature 
in successive morphemes. In the analysis 
of Figure 2 there are two different values 
for the POS (noun and declension suffix), 
two for the case (genitive and inessive), 
and two for the syntactic function 
(@nouncomp and @adverbial). Multiple 
values at moq~hemc-level will have to be 
merged to obtain the word level infer 
mation. 
2) Words with phrase structure. Although the 
segmentation is done for isolated words, 
independently of context, in several cases 
3 l?calurc wtlues starling with the "@" character 
correspond to syntactic functions, like @noullcomp 
(norm complement) or @adverbial. 
the mother) 
tile resulting structure is oquiwflent o the 
aualysis of a phrase, as can be seen i, 
Figure 2. 111 this case, although there are 
two different cases (genitive and inessive), 
lhe case of the full word-form is simply 
inessive. 
3) Noun ellipsis inside word-lbrms. A noun 
ellipsis can occur withi, the word 
(oceasi(mally more than once). This 
information must be made explicit in the 
resulting analysis. For example, Figure 3 
shows the analysis of a single word-forln 
like diotsudumtrel&z (Eng.: with what I am 
lelling you). The first line shows its 
segmentation into four morphemes 
(die tsut+en+ 0 +arekin). The feature 
compl ill tile final analysis conveys the 
information for the verb (l um lelliHg you), 
that carries information about pc'rson, 
number and case o1' subject, object and 
indirect object. The feature comp2 
represents an elided noun and its 
declension stfffix (with). 
4) l)erivation and composition are productive 
in Basque. There arc more than 80 deri- 
w/tion morphemes (especially suffixes) 
intensively used in word-fornlatioll. 
3.2 Design of the word-grammar 
The need to impose hierarchical structure upon 
sequences of morphemes and to build complex 
constructions from them forced us to choose a 
unil'ication mechanism. This task is currently 
unsolwlble using finite-state techniques, clue to 
the growth in size of the resulting network 
(Beesley, 98). We have developed a unifica- 
tion based word-grammar, where each rule 
combines information flom different 
mot+lJlemes giving as a result a feature 
structure for each interpretation of a word- 
fol'nl, treating the previously mentioned cases. 
3 
diotsut 
I am tellh,g you) 
POS verb) 
(tense present) 
(pers-ergative is)\[ 
(pets-dative 2s) 
(pers-absol 3s) 
en 
(what) 
(POS relation) 
(subcat subord) 
(relator relative 
(synt-f @rel-clause 
0 
() 
(POS ellipsis) 
arekin 
(wire) 
(POS declension-suffix)) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
=> diotsudanarekin (wi~ what lamtel l ingyou) 
(POS verb-noun_ellipsis) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
(compl (POS verb) 
(subcat subord) 
(relator relative) 
(synt-f @tel-clause) 
(tense present) 
(pers-ergative is) 
(pets-dative 2s) 
(pers-absol 3s)) 
(comp2 (POS noun) 
(subcat common) 
(number sing) 
(definite+) 
(synt-f @adverbial)) 
Figure 3. Morphosyntactic analysis of diotxudanarekin (Eng.: with what I am tellittg you) 
As a consequence of the rich naorphology of 
Basque we decided to control morphotactic 
phenomena, as much as possible, in the 
morphological segmentation phase. Alterna- 
tively, a model with minimal morphotactic 
treatment (Ritchie et al, 92) would produce 
too many possible analyses after segmentation, 
which should be reiected in a second phase. 
Therefore, we propose to separate sequential 
morphotactics (i.e., which sequences of 
morphemes can or cannot combine with each 
other to form valid words), which will be 
recognized by the two-level system by means 
of continuation classes, and non-sequential 
morphotactics like long-distance dependencies 
that will be controlled by the word-gmnunar. 
The general linguistic principles used to define 
unification equations in the word-grannnar 
rules are the following: 
1) Information risen from the lemma. The 
POS and semantic features are risen flom 
the lemnm. This principle is applied to 
common nouns, adjectives and adverbs. 
The lemma also gives the mnnber in 
proper nouns, pronouns and determiners 
(see Figure 2). 
2) lnfornmtion risen from case suffixes. 
Simple case suffixes provide information 
on declension case, number and syntactic 
function. For example, tile singular 
genitive case is given by the suffix -tell in 
ama+ren (Eng.: of the mother). For 
compound case suffixes the number and 
determination are taken from the first 
suffix and the case from the second one. 
First, both suffixes are joined and after 
that they are attached to the lemma. 
3) Noun ellipsis. When an ellipsis occurs, the 
POS of the whole word-form is expressed 
by a compound, which indicates both the 
presence of the ellipsis (always a noun) 
and the main POS of the word. 
For instance, the resulting POS is 
verb-noun_e l l ips is  when a noun- 
ellipsis occurs after a verb. All the 
information corresponding to both units, 
the explicit lemma and the elided one, is 
stored (see Figure 3). 
4) Subordination morl~hemes. When a 
subordination morpheme is attached to a 
verb, the verb POS and its featm'es are 
risen as well as the subordhmte relation 
and the syntactic fnnction conveyed by the 
naorpheme. 
5) Degree morphemes attached to adjectives, 
past participles and adverbs. The POS and 
diotsudan 
(diotsut + en) 
(POS verb) 
(tense present) 
(relator relative) 
/ \ / 
diotsut 
(POS verb) 
(tense present 
diotsudanarekin 
(diotsut + en -I 0 + arekin) 
(POS verb-noun_ell ipsis) 
(case sociative) 
arekin 
(0 + arekin) 
(POS noun ellipsis) 
(case sociative) 
en 
(pos 
? . . 
o 
(POS e l l ips i s  re la t ion)  
arekin 
(case sociative) 
Figure 4. Parse tree for diotmuhmarekitl (Eng.: with what I am lellittg yott) 
main features arc taken from the lemma 
and the features corresponding to the 
degrees of comparison (comparative, 
supcrhttive) aft taken from the degree 
morphemes. 
6) l)efiwttion. 1)miwttion suffixes select tile 
POS of the base-form to create the deriw> 
tive anti in most cases to change its POS. 
For instance, the suffix -garri (Eng.: -able) 
is applied to verbs and the derived word is 
an adjective. When the derived form is 
obtained by means o1' a prefix, it does not 
change the POS of the base-form. In both 
cases the morphosyntactic rules add a new 
feature representing the structure of tile 
word as a derivative (root and affixes). 
7) Composition. At the moment, we only 
treat the most freqttent kind of 
composition (noun-noun). Since Basque is 
syntactically characterized as a right-head 
hmguage, the main information of the 
compound is taken from the second 
element. 
8) Order of application of the mofphosyn- 
tactic phenomena. When several morpho- 
syntactic phenomena are applied to the 
same leml l la ,  so as to eliminate 
nonsensical readings, the natural order to 
consider them in Basque is the following: 
lemmas, derbation prefixes, deriwltion 
suffixes, composition and inflection (see 
Figure 4). 
9) Morl)hotactic constraints. Elimination of 
illegal sequences of morphemes, such as 
those due to long-distance dependencies, 
which are difficult to restrict by means of 
conti.uation classes. 
The first and second principles are defined lo 
combine information of previously recognized 
mOrl~hemcs, but all the other principles arc 
related to both feature-combination a d non- 
sequential moq~hotactics. 
3.3 Implementation 
We have chosen the PATR formalism 
(Shiebcr, 86) for the definition of the moqflm- 
syntactic rules. There were two main reasons 
for this choice: 
? The formalism is based o.  unification. 
Unification is adequate for the treatment of 
complex phenomena (e.g., agreement of 
conslituents in case, tmmber and definite- 
hess) and complex linguistic structures. 
? Simplicity. The grammar is not linked to a 
linguistic theory, e.g. GPSG in (Ritchie et 
al., 92)? The fact that PATR is simpler than 
more sophisticated formalisms will allow 
that in @e future the grammar could be 
adapted to any of them. 
25 rules have been defined, distributed in the 
following way: 
? 11 rules for the merging of declension 
morphemes and their combination with the 
main categories, 
? 9 rules for the description of verbal 
subordination morphenles, 
? 2 general fulcs for derivation, 
? 1 rule for each of the following 
phenomeml: ellipsis, degree of COlnpavison 
of adjectives (comparative and SUl)erlative) 
and noun composition. 
3.4 Evaluat ion 
As a cousequence of the size of the lexical 
database and tile extensive treatment of 
nlorphosyntax, the resulting analyzer offers 
full coverage when applied to real texts, 
capable of treating unknown words and non- 
standard forms (dialectal wtriants and typical 
errors). 
We performed four experilnents to ewtluate 
tile efficiency of the implemented analyzer 
(see Table 1). A 10,832-word text was 
randomly selected from newspapers. We 
measured tile number of words per second 
analyzed by the morphosyntactic analyzer and 
also by the whole morphological analyzer 
(results taken on a Sun Ultra 10). Ill the first 
experiment all tile word-t'ornls were analyzed 
one-by-one; while ill tile other three experi- 
ments words with more than one occurrence 
were analyzed only once. Ill the last two 
experimeuts a memory with the analysis of tile 
most frequent word-forms (MFW) in Basque 
was used, so that only word-forms not found 
in the MFW were analyzed. 
Test 
description 
All 
word forms 
Diffcrent 
word forms 
MFW 
10,000 words 
(I 5 Mb) 
MFW 
50,000 words 
(75 mb) 
# words/scc 
analyzed Morphosynt. 
words analyzer 
10,832 
3,692 
1,483 
533 
15,13 
44 40 
111 95 
308 270 
words/see 
Full 
morphological 
analyzer 
13,5 
Table 1. Evaluation results. 
Even when our language is agglutinative, and 
its morphological phenomena need more 
computational resources to build complex and 
deep structures, the results prove tile feasibility 
of implementiug efficiently a fifll 
morphological analyzer, although efficiency 
was not the main concern of our 
implementation. The system is currently being 
applied to unrestricted texts in real-time 
applications. 
4 Related work 
(Koskeniemmi, 83) defined the formalism 
named two-level morphology. Its main 
contributiou was the treatment of 
morl)hographemics and morphotactics. The 
formalisnl has been stmcessfully applied to a 
wide wlriety ot' languages. 
(Karttunen, 94) speeds the two-level model 
compiling two-level rules into lexical 
transducers, also increasing the expressiveness 
of the model 
The morphological analyzer created by 
(Ritchie et al, 92) does not adopt finite state 
mechanisms to control morphotactic 
phenomena. Their two-level implementation 
incorporates a straightforward morphotactics, 
reducing tile number of sublexicons to the 
indispensable (prefixes, lemmas and suffixes). 
This approximation would be highly 
inefficient for agglutinative languages, as it 
would create lnany nonsensical interpretatiolas 
that should be rejected by tile unification 
phase. They use the word-grammar for both 
morphotactics and feature-conlbination. 
ill a similar way, (Trost, 90) make a proposal 
to combine two-level morphology and non- 
sequential morphotactics. 
The PC-Kimmo-V2 system (Antworth, 94) 
presents an architecture similar to ours applied 
to English, using a finite-state segmentation 
phase before applying a unification-based 
grammar. 
(Pr6szdky and Kis, 99) describe a morpho- 
syntactic analyzer for Hungarian, an agglu- 
tinative language. The system clots not use the 
two-level model for segmentation, precom- 
piling suffix-sequences to improve efficiency. 
They claim the need of a word-grammar, 
giving a first outline of its design, although 
they do not describe it in detail. 
(Oflazer, 99) presents a different approach for 
the treatment of Turkish, an agglutinative 
language, applying directly a dependency 
parsing scheme to morpheme groups, that is, 
merging morphosyntax and syntax. Although 
we are currently using a similar model to 
Basque, there are several applications that are 
word-based and need full morphological 
parsing of each word-t'orm, like the word- 
oriented Constraint Graminar formalism for 
disambiguation (Karlsson et aI., 95). 
Conc lus ion  
We propose a model for fllll morphological 
analysis iutegrating two different components. 
On tile one hand, the two-level formalism 
deals with morphographenfics and sequential 
morphotactics and, on the other hand, a 
unil\]cation-based word-grammar combines lhe 
granlll-iatical in\['ornlatioli defined in illoi'- 
phelllOS alld also handles COlllplcx illori)ho- 
tactics. 
Early application of sCqtloniial I/lOrl)hotactic 
conslraints dtu-ing the segmentation process 
avoids all excessive laUlllber of nleaningless 
segmentation possibilities before the 
coulputationally lllOlO expensive unification 
process. Unification permits lhe resohition of a 
wide variety of morl)hological phenonlena, 
like ellipsis, thal force the definition of: 
complex and deep structures Io roprosenl the 
output of the analyzer. 
This design allowed us io develop a full 
coverage allalyzor that processes efficiently 
unrestricted loxis in Basque, a strongly 
agglulinafive langttage. 
The anaiyzcl" has bccll integrated ill a gCllOl'al 
franlework for the l)lOCessing of l~asquc, with 
all the linguistic inodulos communicating by 
l l leallS O\[: foattll'C stltlClll l 'eS ill accord  {o the 
principles of ihe Text Encoding Initiative. 
Acknowledgements  
This research was partially supported by the 
Basque Government, the University of the 
\]71aS(lUe Cotlntry {/lid the CICYq' (Cotllisidn 
lntcrministorial de Ciencia y Tecnologfil). 
References 
Aduriz 1., Aldczabal I., Ansa ()., Arlola X., I)faz de 
Ilarraza A., Insau.~li .I.M. (1998a) EI)BL: a 
Mttlli-l~ttrposed Lexica/ Sttl)l)c;rl .lot the 
Treatment of Ba,s'que. Proceedings of the l;irst 
Inlernational Confcncncc on l Auiguagc Resources 
and Ewduation, Granada. 
Aduriz I., Agirre E., Aldczabal 1., Alegria 1., Ansa 
O., Arrcgi X., Arriola J.M., ArtolaX., I)faz de 
lhu'raza A., Ezciza N., Gqicnola K., Maritxahu" 
A., Maritxalar M., Oronoz M., Sarasola K., 
Soroa A., Urizar R., Urkia M. (1998b) A 
Framework .for the Automatic Pmce.vsi#~g (if" 
Basqtte. Proceedings o1 the First Ii~ternational 
Con \[elel i te on Lall.gtlagc Resources turf 
Evaluation, Granada. 
Aduriz I., Alcgria I., Arriohl J.M., Artola X., l)faz 
do Ilarraza A., Ecciza N., Gojcnola K., 
Maritxalar M. (1995) Di\[.ferelt! Issues in the 
Design qf a lemmatizer/Tagger fo Ba,s'qtte. From 
Tcxls to Tags: Issues in Mullilingual Language 
Analysis. ACL SIGI)AT Workshop, l)ublin. 
Alcgria 1., Art(Ha X., Sarasoht K., Urkia M. (1996) 
Automatic moqdzological analysis of Basque. 
IAtcrary and IAnguistic Computing, 11 (4): 193- 
203. Oxford University. 
Aniworlh E. I.. (1994) Morphological Par, ffng with 
a lhl(fication-ba,s'ed Word Grcmmutr. Norlh 
Te, xas Natural l~anguage Processing Workshop, 
Texas. 
Arlola X., Dfaz de \]larraza A., Ezciza N., Oo.icnohi 
K., Marilxahu' A., Soma A. (2000) A proposal 
for the integration of NLP tools using SGML- 
lagged documeHls. Proceedings of ll~e Second 
Cotfforence or1 Language Resources and 
Evaltmfion (IA~,EC 2000). Athens, Greece 2000. 
Bcesl%, K. (1998)AraDic Morphological Analysis 
(m the lnlernet, l'rocccdings of the International 
Conference on Mulii-IAngual Computing (Arabic 
& lhlglish), Cambridge. 
Hudson R. (1990) English Word Grammmar. 
Oxford: Basil Blackwcll. 
ldc N., Vcronis J. K. (1995) Text-Ettcoding hHtia- 
tire, Bac:kgmtmd and Context. Kluwcr Academic 
Publishers. 
Karlsson F., Voulilaincn A., Heikkiht J., Anltila A. 
(1995) Constrai, t Gnmmmr: A lxm,?tmge- 
i#ldcpcndent System Jor Pm:ffng Um'estricled 
Text, Mouton do Gruyicr ed.. 
Kartmnen 1,. (1994) Con,s'tructin~ l,e.vical 
7)'ansdttcers. Proc. of CO13NG'94, 406-411. 
Koskcnniemi, K, (1983) Two-level Mc;qdlo\[ogy: A 
ge,eral Comptttational Model ./br Word-Form 
Recognition and Pmduclioth University of 
Ilclsinki, l)clmrtmcnt of General IAnguisiics. 
l~ublications " 11. 
()flazcr K (1999) l)epetMe/t O' Parsing, with a, 
E.rtended I:inite State Approac\]t. ACL'99, 
Maryland. 
Pr6sz6ky G., Kis B (1999)A Unificati(m-hascd 
Apl~roach to Moqdto-syntactic I'arsitl<~ of 
Agghttinative and Other (Highly) lnjlectional 
Languages. ACtd99, Ma,yhmd. 
Ritchie G., Pulhnan S. G., FJlack A. W., Russcl G. 
J. (1992) Comlmtational Moudu)logy: Practical 
Mechanism,s'.fi)r the l#lglish l,exico,. ACL-MIT 
Series on Natural Language Processing, MIT 
Press. 
Shicbcr S. M. (1986) At/ lntroductiotz to 
Unification-Based Approaches to Grammar. 
CSLI, Slanford. 
Sproat R. (1992) Morphology anU Computcaion. 
ACL-MIT Press series in Natural Language 
Processing. 
Trost It. (1990) The application of two-level 
morldzo/ogy to rzon-concatenative German 
moqgtology. COIANG'90, Hclsinki. 
7 
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 59?64,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Strategies for sustainable MT for Basque:  
incremental design, reusability, standardization and open-source 
 I. Alegria, X. Arregi, X. Artola, A. Diaz de Ilarraza, G. Labaka,  
M. Lersundi, A. Mayor, K. Sarasola 
Ixa taldea.  
University of the Basque Country. 
i.alegria@ehu.es 
 
 
 
Abstract 
We present some Language Technology 
applications that have proven to be effec-
tive tools to promote the use of Basque, a 
European less privileged language. We also 
present the strategy we have followed for 
almost twenty years to develop those appli-
cations as the top of an integrated environ-
ment of language resources, language 
foundations, language tools and other ap-
plications. When we have faced a difficult 
task such as Machine Translation to 
Basque, our strategy has worked well. We 
have had good results in a short time just 
reusing previous works for Basque, reusing 
other open-source tools, and developing 
just a few new modules in collaboration 
with other groups. In addition, new reus-
able tools and formats have been produced.  
1 Introduction and Basque Language 
Basque is a highly inflected minority language 
with free order of sentence constituents. Machine 
Translation for Basque is thus both, a real need and 
a test bed for our strategy to develop NLP tools for 
Basque.          
Basque is an isolate language, and little is 
known of its origins. It is likely that an early form 
of the Basque language was already present in 
Western Europe before the arrival of the Indo-
European languages. 
Basque is an agglutinative language, with a rich 
flexional morphology. In fact for nouns, for 
example, at least 360 word forms are possible for 
each lemma. Each of the declension cases such as 
absolutive, dative, associative? has four different 
suffixes to be added to the last word of the noun 
phrase. These four suffix variants correspond to 
undetermined, determined singular, determined 
plural and ?close? determined plural.  
Basque is also an ergative-absolutive language. 
The subject of an intransitive verb is in the 
absolutive case (which is unmarked), and the same 
case is used for the direct object of a transitive 
verb. The subject of the transitive verb (that is, the 
agent) is marked differently, with the ergative case 
(shown by the suffix -k). This also triggers main 
and auxiliary verbal agreement. 
The auxiliary verb, which accompanies most 
main verbs, agrees not only with the subject, but 
with the direct object and the indirect object, if 
present. Among European languages, this 
polypersonal system (multiple verb agreement) is 
only found in Basque, some Caucasian languages, 
and Hungarian. The ergative-absolutive alignment 
is rare among European languages, but not 
worldwide. 
Although in last centuries Basque suffered 
continuous regression it still remains alive. The 
region in which Basque is spoken is smaller than 
what is known as the Basque Country, and the 
distribution of Basque speakers is not 
homogeneous there. The main reasons of this 
regression (Amorrortu, 2002) are that Basque was 
not an official language, and that it was out of 
educational system, out of media and out of 
industrial environments. Besides, the fact of being 
six different dialects made the wide development 
of written Basque difficult.  
However, after 1980, some of those features 
changed and many citizens and some local 
59
governments promote recovering of Basque 
Language.  
Today, Basque holds co-official language status 
in the Basque regions of Spain: the whole 
autonomous community of the Basque Country 
and some parts of Navarre. Basque has no official 
standing in the Northern Basque Country.   
In the past, Basque was associated with lack of 
education, stigmatized as uneducated, rural, or 
holding low economic and power resources. There 
is not such an association today; Basque speakers 
do not differ from Spanish or French monolinguals 
in any of these characteristics.  
Standard Basque, called Batua (unified) in 
Basque, was defined by the Academy of Basque 
Language (Euskaltzaindia) in 1968. At present, its 
morphology is completely standardized, but the 
lexical standardization process is still underway. 
Now this is the language model taught in most 
schools and used on some media and official 
papers published in Basque.  
Basque speakers are about 700,000, about 25% 
of the total population of the Basque Country, but 
they are not evenly distributed. Still the use of 
Basque in industry and specially in Information 
and Communication Technology is not 
widespread. A language that seeks to survive in the 
modern information society has to be present also 
in such field and this requires language technology 
products. Basque, as other minority languages, has 
to make a great effort to face this challenge (Petek, 
2000; Williams et al, 2001).  
2 Strategy to develop Human Language 
Technology (HLT) in Basque 
IXA group is a research Group created in 1986 by 
5 university lecturers in the computer science fac-
ulty of the University of the Basque Country with 
the aim of laying foundations for research and de-
velopment of NLP software mainly for Basque. 
We wanted to face the challenge of adapting 
Basque to language technology. 
Twenty one years later, now IXA is a group 
composed of 28 computer scientists, 13 linguists 
and 2 research assistants. It works in cooperation 
with more than 7 companies from Basque Country 
and 5 from abroad; it has been involved in the birth 
of two new spin-off companies; and it has devel-
oped more than seven language technology prod-
ucts. 
In recent years, several private companies and 
technology centers in the Basque Country have 
begun to get interested and to invest in this area. At 
the same time, more agents have come to be aware 
of the fact that collaboration is essential to the de-
velopment of language technologies for minority 
languages. One of the fruits of this collaboration 
are HIZKING21 (2002-2005) and ANHITZ (2006-
2008) projects. Both projects were accepted by the 
Government of the Basque Country in a new 
strategical research line called ?Language Infoen-
gineering?. 
At the very beginning, twenty years ago, our 
first goal was just to create a Spanish-Basque 
translation system, but after some preliminary 
work we realized that instead of wasting our time 
in creating an ad hoc MT system with small accu-
racy, we had to invest our effort in creating basic 
tools such as a morphological analyzer/generator 
for Basque, that could later be used to build not 
only a more robust MT system but also other ap-
plications. 
This thought was the seed to design our strategy 
to make progress in the adaptation of Basque to 
Language Technology. Basque language had to 
face up scarcity of resources and tools that could 
make possible its development in Language Tech-
nology at a reasonable and competitive rate. 
We presented an open proposal for making pro-
gress in Human Language Technology (Aduriz et 
al., 1998). Anyway, the steps proposed did not cor-
respond exactly with those observed in the history 
of the processing of English, because the high ca-
pacity and computational power of new computers 
allowed facing problems in a different way.  
Our strategy may be described in two points: 
1) The need for standardization of resources to 
be useful in different researches, tools and applica-
tions 
2) The need for incremental design and devel-
opment of language foundations, tools, and appli-
cations in a parallel and coordinated way in order 
to get the best benefit from them. Language foun-
dations and research are essential to create any tool 
or application; but in the same way tools and ap-
plications will be very helpful in the research and 
improvement of language foundations. 
Following this strategy, our steps on standardi-
zation of resources led us to adopt TEI and XML 
standards and also to define a methodology for 
60
stand-off corpus tagging based on TEI, feature 
structures and XML (Artola et al, 2005). 
In the same way, taking as reference our experi-
ence in incremental design and development we 
proposed four phases as a general strategy for lan-
guage processing. These are the phases defined 
with the products to be developed in each of them. 
1. Initial phase: Foundations. Corpus I (collection 
of raw text with no tagging mark). Lexical da-
tabase I (the first version could be a list of 
lemmas and affixes). Machine-readable dic-
tionaries. Morphological description.  
2. Second phase: Basic tools and applications. 
Statistical tools for the treatment of corpora. 
Morphological analyzer/generator. Lemma-
tizer/tagger. Spelling checker and corrector (al-
though in morphologically simple languages a 
word list could be enough). Speech processing 
at word level. Corpus II (word-forms are 
tagged with their part of speech and lemma). 
Lexical database II (lexical support for the con-
struction of general applications, including part 
of speech and morphological information). 
3. Third phase: Advanced tools and applications. 
An environment for tool integration. Web 
search engine.  A traditional search machine 
that integrates lemmatization and language 
identification. Surface syntax. Corpus III (syn-
tactically tagged text). Grammar and style 
checkers. Structured versions of dictionaries 
(they allow enhanced functionality not avail-
able for printed or raw electronic versions). 
Lexical database III (the previous version is en-
riched with multiword lexical units. Integration 
of dictionaries in text editors). Lexical-
semantic knowledge base. Creation of a con-
cept taxonomy (e.g.: Wordnet). Word-sense 
disambiguation. Speech processing at sentence 
level. Basic Computer Aided Language Learn-
ing (CALL) systems 
4. Fourth phase: Multilingualism and general 
applications. Information extraction. Transla-
tion aids (integrated use of multiple on-line 
dictionaries, translation of noun phrases and 
simple sentences). Corpus IV (semantically 
tagged text after word-sense disambiguation). 
Dialog systems. Knowledge base on multilin-
gual lexico-semantic relations and its applica-
tions.  
We will complete this strategy with some sug-
gestions about what shouldn?t be done when work-
ing on the treatment of minority languages. a) Do 
not start developing applications if linguistic foun-
dations are not defined previously; we recommend 
following the above given sequence: foundations, 
tools and applications. b) When a new system has 
to be planned, do not create ad hoc lexical or syn-
tactic resources; you should design those resources 
in a way that they could be easily extended to full 
coverage and reusable by any other tool or applica-
tion. c) If you complete a new resource or tool, do 
not keep it to yourself; there are many researchers 
working on English, but only a few on each minor-
ity language; thus, the few results should be public 
and shared for research purposes, for it is desirable 
to avoid needless and costly repetition of work. 
3 Machine Translation for Basque 
After years working on basic resources and tools 
we decided it was time to face  the MT task (Hut-
chins and Somers, 1992). Our general strategy was 
more specifically for Machine Translation defined 
bearing in mind the following concepts:  
? reusability of previous resources, specially 
lexical resources and morphology of Basque 
? standardization and collaboration: using a 
more general framework in collaboration 
with other groups working in NLP 
? open-source: this means that anyone having 
the necessary computational and linguistic 
skills will be able to adapt or enhance it to 
produce a new MT system,  
Due to the real necessity for translation in our 
environment the involved languages would be 
Basque, Spanish and English. 
From the beginning we wanted to combine the 
two basic approaches for MT (rule-based and cor-
pus-based) in order to build a hybrid system, be-
cause it is generally agreed that there are not 
enough corpora for a good corpus-based system in 
minority languages like Basque.  
Data-driven Machine Translation (example-
based or statistical) is nowadays the most prevalent 
trend in Machine Translation research. Translation 
results obtained with this approach have already 
reached a high level of accuracy, especially when 
the target language is English. But these Data-
driven MT systems base their knowledge on 
aligned bilingual corpora, and the accuracy of their 
61
output depends heavily on the quality and the size 
of these corpora. Large and reliable bilingual cor-
pora are unavailable for many language pairs. 
3.1 The rule-based approach 
First, we present the main architecture and the pro-
posed standards of an open source MT engine, the 
first implementation of which translates from 
Spanish into Basque using the traditional transfer 
model and based on shallow and dependency pars-
ing. 
The design and the programs are independent 
from the languages, so the software can be used for 
other projects in MT. Depending on the languages 
included in the adaptation, it will be necessary to 
add, reorder and change some modules, but this 
will not be difficult because a unique XML format 
is used for the communication among all the mod-
ules. 
The project has been integrated in the OpenTrad 
initiative (www.opentrad.com), a government-
funded project shared among different universities 
and small companies, which also include MT en-
gines for translation among the main languages in 
Spain. The main objective of this initiative is the 
construction of an open, reusable and interoperable 
framework. 
In the OpenTrad project, two different but coor-
dinated designs have been carried out: 
? A shallow-transfer machine translation en-
gine for similar languages (Spanish, Catalan 
and Galician by the the time being). The 
MT architecture uses finite-state transducers 
for lexical processing, hidden Markov mod-
els for part-of-speech tagging, and chunking 
based on finite-state for structural transfer. 
It is named Apertium and it can be 
downloaded from apertium.sourceforge.net. 
(Armentano-Oller et al, 2004) 
? A deeper-transfer engine for the Spanish-
Basque pair. It is named Matxin (Alegria et 
al., 2007) and it is stored in 
matxin.sourceforge.net. It is an extension of 
previous work in our group. In order to re-
use resources in this Spanish-Basque system 
the analysis module for similar languages 
was not included in Matxin; another open 
source engine, FreeLing (Carreras et al, 
2004), was used here, of course, and its out-
put had to be converted to the proposed in-
terchange format. 
Some of the components (modules, data formats 
and compilers) from the first architecture in Open-
Trad were used in the second one. Indeed, an im-
portant additional goal of this work was testing 
which modules from the first architecture could be 
integrated in deeper-transfer architectures for more 
difficult language pairs. 
The transfer module is also based on three main 
objects in the translation process: words or nodes, 
chunks or phrases, and sentences.  
? First, lexical transfer is carried out using a 
bilingual dictionary compiled into a finite-
state transducer. We use the XML specifica-
tion of Apertium engine.  
? Then, structural transfer at the sentence 
level is applied, and some information is 
transferred from some chunks to others, and 
some chunks may disappear. Grammars 
based on regular expressions are used to 
specify these changes. For example, in the 
Spanish-Basque transfer, the person and 
number information of the object and the 
type of subordination are imported from 
other chunks to the chunk corresponding to 
the verb chain. 
? Finally the structural transfer at the chunk 
level is carried out. This process can be 
quite simple (e.g. noun chains between 
Spanish and Basque) or more complex (e.g. 
verb chains between these same languages). 
The XML file coming from the transfer module 
is passed on the generation module. 
? In the first step, syntactic generation is per-
formed in order to decide the order of 
chunks in the sentence and the order of 
words in the chunks. Several grammars are 
used for this purpose.  
? Morphological generation is carried out in 
the last step. In the generation of Basque, 
the main inflection is added to the last word 
in the phrase (in Basque: the declension 
case, the article and other features are added 
to the whole noun phrase at the end of the 
last word), but in verb chains other words 
need morphological generation. A previous 
morphological analyzer/generator for 
Basque (Alegria et al, 1996) has been 
adapted and transformed to the format used 
in Apertium. 
The results for the Spanish/Basque system using 
FreeLing and Matxin are promising. The quantita-
62
tive evaluation uses the open source evaluation 
tool IQMT and figures are given using Bleu and 
NIST measures (Gim?nez et al, 2005). An user 
based evaluation has been carried out too. 
3.2 The corpus-based approach 
The corpus-based approach has been carried out in 
collaboration with the National Center for Lan-
guage Technology in Dublin.  
The system exploits both EBMT and SMT tech-
niques to extract a dataset of aligned chunks. We 
conducted Basque to English and Spanish to 
Basque translation experiments, evaluated on a 
large corpus (270, 000 sentence pairs).  
Some tools have been reused for this purpose: 
? GIZA++: for word/morpheme alignment we 
used the GIZA++ statistical word alignment 
toolkit, and following the ?refined? method 
of (Och and Ney, 2003), extracted a set of 
high-quality word/ morpheme alignments 
from the original unidirectional alignment 
sets. These along with the extracted chunk 
alignments were passed to the translation 
decoder.                                         
? Pharaoh/Moses decoder: the decoder is also 
a hybrid system which integrates EBMT 
and SMT. It is capable of retrieving already 
translated sentences and also provides a 
wrapper around the PHARAOH SMT de-
coder (Koehn, 2004). 
? MaTrEx: the MATREX (Machine Transla-
tion using Examples) system used in our 
experiments is a data-driven MT engine, 
built following an extremely modular de-
sign. It consists of a number of extensible 
and re-implementable modules (Way and 
Gough, 2005). 
   For this engine, we reuse a toolkit to chunk the 
Basque sentences. After this processing stage, a 
sentence is treated as a sequence of morphemes, in 
which chunk boundaries are clearly visible. Mor-
phemes denoting morphosyntactic features are re-
placed by conventional symbolic strings. After 
some adaptation, the chunks obtained in this man-
ner are actually very comparable to the English 
chunks obtained with the marker-based chunker. 
The experimental results have shown that our 
system significantly outperforms state-of-the-art 
approaches according to several common auto-
matic evaluation metrics: WER, Bleu and PER 
(Stroppa et al, 2006; Labaka et al, 2007). 
4 Conclusions 
A language that seeks to survive in the modern 
information society requires language technology 
products. "Minority" languages have to do a great 
effort to face this challenge. The Ixa group has 
been working since 1986 on adapting Basque to 
language technology, having developed several 
applications that are effective tools to promote the 
use of Basque. Now we are planning to define the 
BLARK for Basque (Krauwer, 2003).  
From our experience, we defend that research 
and development for a minority language should to 
be faced following these points: high standardiza-
tion,  reusing language foundations, tools, and ap-
plications, and their incremental design and devel-
opment. We know that any HLT project related to 
a less privileged language should follow those 
guidelines, but from our experience we know that 
in most cases they do not. We think that if Basque 
is now in an good position in HLT is because those 
guidelines have been applied even  when it was 
easier to define "toy" resources and tools useful to 
get good short term academic results, but not reus-
able in future developments.  
This strategy has been completely useful when 
we have created MT systems for Basque. Reusing 
previous works for Basque (that were defined fol-
lowing XML and TEI standards) and reusing other 
open-source tools have been the key to get satisfac-
tory results in a short time.  
Two results produced in the MT track are pub-
licly available:  
? matxin.sourceforge.net for the free code for 
the Spanish-Basque RBMT system 
? www.opentrad.org for the on-line demo  
Acknowledgments 
This work has been partially funded by the Spanish 
Ministry of Education and Science (OpenMT: 
Open Source Machine Translation using hybrid 
methods,TIN2006-15307-C03-01) and the Local 
Government of the Basque Country (AnHITZ 
2006: Language Technologies for Multingual In-
teraction in Intelligent Environments., IE06-185). 
Andy Way, Declan Groves and Nicolas Stroppa 
from National Centre for Language Technology in 
Dublin are kindly acknowledged for providing 
their expertise on the Matrex system and the 
evaluation of the output. 
63
References 
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, O. Ansa, 
X. Arregi, J. Arriola, X. Artola, A. D?az de Ilarraza, 
N. Ezeiza, K.Gojenola, M. Maritxalar, M. Oronoz, K. 
Sarasola, A. Soroa, R. Urizar. 1998. A framework for 
the automatic processing of Basque. Proceedings of 
Workshop on Lexical Resources for Minority Lan-
guages.  
I. Alegria, X. Artola, K. Sarasola. 1996.Automatic mor-
phological analysis of Basque. Literary & Linguistic 
Computing Vol. 11, No. 4, 193-203. Oxford Univer-
sity Press. Oxford. 1996. 
I. Alegria, A. D?az de Ilarraza, G. Labaka, M Lersundi, 
A. Mayor, K. Sarasola.  2007. Transfer-based MT 
from Spanish into Basque: reusability, standardiza-
tion and open source. LNCS 4394. 374-384. Cicling 
2007.  
E. Amorrortu. 2002. Bilingual Education in the Basque 
Country: Achievements and Challenges after Four 
Decades of Acquisition Planning. Journal of Iberian 
and Latin American Literary and Cultural Stud-
ies.Volume 2 Number 2 (2002) 
C. Armentano-Oller, A. Corb?-Bellot, M. L. Forcada, 
M. Ginest?-Rosell, B. Bonev, S. Ortiz-Rojas, J. A. 
P?rez-Ortiz, G. Ram?rez-S?nchez, F. S?nchez-
Mart?nez, 2005. An open-source shallow-transfer 
machine translation toolbox: consequences of its re-
lease and availability. Proceedings of OSMaTran: 
Open-Source Machine Translation workshop, MT 
Summit X. 
X. Artola, A. D?az de Ilarraza, N. Ezeiza, K. Gojenola, 
G. Labaka, A. Sologaistoa, A. Soroa.  2005. A 
framework for representing and managing linguistic 
annotations based on typed feature structures. Proc. 
of RANLP 2005. 
X. Carreras,, I. Chao, L. Padr? and M. Padr?. 2004. 
FreeLing: An open source Suite of Language Ana-
lyzers, in  Proceedings of the 4th International Con-
ference on Language Resources and Evaluation 
(LREC'04).  
J. Gim?nez, E. Amig?, C. Hori. 2005. Machine 
Translation Evaluation Inside QARLA. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Technology (IWSLT'05) 
W. Hutchins and H. Somers. 1992. An Introduction to 
Machine Translation. Academic Press. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els.  In Proceedings of AMTA-04, pages 115?124, 
Washington, District of Columbia. 
S. Krauwer. 2003. The Basic Language Resource Kit 
(BLARK) as the First Milestone for the Language 
Resources Roadmap. Proc. of the International 
Workshop  Speech and Computer. Moscow, Russia. 
G. Labaka, N. Stroppa, A. Way, K. Sarasola  2007 
Comparing Rule-Based and Data-Driven Approaches 
to Spanish-to-Basque Machine Translation Proc. of 
MT-Summit XI, Copenhagen 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1): 19?51. 
B. Petek. 2000. Funding for research into human lan-
guage technologies for less prevalent languages, Sec-
ond International Conference on Language Re-
sources and Evaluation (LREC 2000). Athens, 
Greece. 
N. Stroppa, D. Groves, A. Way, K. Sarasola K. 2006. 
Example-Based Machine Translation of the Basque 
Language. AMTA. 7th conference of the Association 
for Machine Translation in the Americas.. 
A. Way and N. Gough. 2005. Comparing Example-
Based and Statistical Machine Translation. Natural 
Language Engineering, 11(3):295?309. 
B. Williams, K. Sarasola, D. ??Cr?inin, B. Petek. 2001. 
Speech and Language Technology for Minority Lan-
guages. Proceedings of Eurospeech 2001 
 
 
64
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine Learning Techniques to Build a Comma Checker for 
Basque
I?aki Alegria Bertol Arrieta Arantza Diaz de Ilarraza Eli Izagirre Montse Maritxalar
Computer Engineering Faculty. University of the Basque Country.
Manuel de Lardizabal Pasealekua, 1
20018 Donostia, Basque Country, Spain.
{acpalloi,bertol,jipdisaa,jibizole,jipmaanm}@ehu.es
Abstract
In this paper, we describe the research 
using  machine  learning  techniques  to 
build a comma checker to be integrated 
in a grammar checker for Basque. After 
several experiments, and trained with a 
little corpus of 100,000 words, the sys?
tem guesses correctly not placing com?
mas with a precision of 96% and a re?
call of 98%. It also gets a precision of 
70% and a recall of 49% in the task of 
placing  commas.  Finally,  we  have 
shown  that  these  results  can  be  im?
proved using a bigger and a more ho?
mogeneous  corpus  to  train,  that  is,  a 
bigger corpus written by one unique au?
thor. 
1 Introduction
In the last years, there have been many studies 
aimed  at  building  a  grammar  checker  for  the 
Basque language (Ansa et al, 2004; Diaz De Il?
arraza et al, 2005). These works have been fo?
cused, mainly, on building rule sets ??taking into 
account syntactic information extracted from the 
corpus  automatically??  that  detect  some  erro?
neous grammar forms. The research here presen?
ted wants to complement the earlier work by fo?
cusing on  the  style  and the  punctuation of  the 
texts. To be precise, we have experimented using 
machine learning techniques for the special case 
of the comma, to evaluate their performance and 
to analyse the possibility of applying it in other 
tasks of the grammar checker.  
However,  developing  a  punctuation  checker 
encounters  one  problem  in  particular:  the  fact 
that the punctuation rules are not totally estab?
lished. In general, there is no problem when us?
ing the  full  stop,  the  question mark or  the ex?
clamation mark.  Santos (1998) highlights these 
marks are reliable punctuation marks, while all 
the rest are unreliable. Errors related to the reli?
able ones (putting or not the initial  question or 
exclamation mark depending on the language, for 
instance) are not so hard to treat. A rule set to 
correct some of these has already been defined 
for the Basque language (Ansa et al, 2004). In 
contrast, the comma is the most polyvalent and, 
thus, the least defined punctuation mark (Bayrak?
tar et al, 1998; Hill and Murray, 1998). The am?
biguity of the comma, in fact,  has been shown 
often (Bayraktar et  al.,  1998; Beeferman et al, 
1998;  Van  Delden  S.  and  Gomez  F.,  2002). 
These works have shown the lack of fixed rules 
about the comma. There are only some intuitive 
and  generally  accepted  rules,  but  they  are  not 
used in a standard way. In Basque, this problem 
gets even more evident, since the standardisation 
and  normalisation  of  the  language  began  only 
about twenty?five years ago and it  has not fin?
ished yet. Morphology is mostly defined, but, on 
the contrary, as far as syntax is concerned, there 
is  quite  work  to  do.  In  punctuation  and  style, 
some basic rules have been defined and accepted 
by the Basque Language Academy (Zubimendi, 
2004).  However,  there  are  not  final  decisions 
about the case of the comma. 
Nevertheless,  since  Nunberg?s  monograph 
(Nunberg, 1990), the importance of the comma 
has  been  undeniable,  mainly  in  these  two  as?
pects: i) as a due to the syntax of the sentence 
(Nunberg, 1990; Bayraktar et al, 1998; Garzia, 
1997), and ii) as a basis to improve some natural 
language  processing  tools  (syntactic  analysers, 
error  detection  tools?),  as  well  as  to  develop 
some  new  ones  (Briscoe  and  Carroll,  1995; 
Jones, 1996). The relevance of the comma for the 
syntax of the sentence may be easily proved with 
some clarifying examples where the sentence is 
understood in  one or  other  way,  depending on 
whether  a  comma  is  placed  or  not  (Nunberg, 
1990): 
a. Those students who can, contribute to the 
United Fund. 
b. Those students who can contribute to the 
United Fund. 
1
In the same sense,  it  is  obvious  that  a  well 
punctuated  text,  or  more  concretely,  a  correct 
placement of the commas, would help consider?
ably  in  the  automatic  syntactic  analysis  of  the 
sentence,  and, therefore,  in the development of 
more and better tools in the NLP field. Say and 
Akman (1997) summarise the research efforts in 
this direction.
As an important background for our work, we 
note  where  the  linguistic  information  on  the 
comma for the Basque language was formalised. 
This  information  was  extracted  after  analysing 
the  theories  of  some experts  in  Basque  syntax 
and punctuation (Aldezabal et al, 2003). In fact, 
although no final decisions have been taken by 
the Basque Language Academy yet,  the theory 
formalised in the above mentioned work has suc?
ceeded in unifying the main points of view about 
the  punctuation in  Basque.  Obviously,  this  has 
been the basis for our work. 
2 Learning commas
We have designed two different but combinable 
ways to get the comma checker:
? based on clause boundaries
? based directly on corpus
Bearing  in  mind  the formalised  theory  of 
Aldezabal et  al.  (2003)1,  we realised that if  we 
got to split the sentence into clauses, it would be 
quite easy to develop rules for detecting the exact 
places where commas would have to go. Thus, 
the best way to build a comma checker would be 
to get, first, a clause identification tool. 
Recent papers in this area report quite good 
results using machine learning techniques. Car?
reras and M?rquez (2003) get one of the best per?
formances in this  task (84.36% in test).  There?
fore, we decided to adopt this as a basis in order 
to  get  an  automatic  clause  splitting  tool  for 
Basque.  But  as  it  is  known,  machine  learning 
techniques cannot be applied if no training cor?
pus is available, and one year ago, when we star?
ted this  process,  Basque texts  with this  tagged 
clause splits were not available.
Therefore, we decided to use the second al?
ternative.  We  had  available  some  corpora  of 
Basque, and we decided to try learning commas 
from raw text, since a previous tagging was not 
needed. The problem with the raw text is that its 
commas are not the result of applying consistent 
rules.
1 From now on, we will speak about this as ?the accepted theory of Basque 
punctuation?. 
Related work
Machine learning techniques have been applied 
in many fields and for  many purposes,  but  we 
have found only one reference in the literature 
related to the use of machine learning techniques 
to assign commas automatically. 
Hardt (2001) describes research in using the 
Brill tagger (Brill 1994; Brill, 1995) to learn to 
identify incorrect commas in Danish. The system 
was developed by randomly inserting commas in 
a text, which were tagged as incorrect, while the 
original  commas  were  tagged  as  correct.  This 
system identifies incorrect commas with a preci?
sion  of  91%  and  a  recall  of  77%,  but  Hardt 
(2001) does not mention anything about identify?
ing correct commas. 
In  our  proposal,  we have tried  to  carry out 
both aspects, taking as a basis other works that 
also use machine learning techniques in similar 
problems  such  as  clause  splitting  (Tjong  Kim 
Sang E.F. and D?jean H., 2001) or detection of 
chunks (Tjong Kim Sang E.F. and Buchholz S., 
2000).
3 Experimental setup
Corpora
As we have mentioned before, some corpora 
in Basque are available. Therefore, our first task 
was to select the training corpora, taking into ac?
count that well punctuated corpora were needed 
to train the machine correctly. For that purpose, 
we looked for corpora that satisfied as much as 
possible our ?accepted theory of Basque punctu?
ation?.  The  corpora  of  the  unique  newspaper 
written in Basque, called  Egunkaria (nowadays 
Berria), were chosen, since they are supposed to 
use the ?accepted theory of Basque punctuation?. 
Nevertheless,  after  some brief  verifications, we 
realised that the texts of the corpora do not fully 
match with our theory. This can be understood 
considering that a lot of people work in a news?
paper. That is, every journalist can use his own 
interpretation of  the  ?accepted theory?,  even if 
all of them were instructed to use it in the same 
way. Therefore, doing this  research, we had in 
mind that the results we would get were not go?
ing to be perfect.
To counteract this problem, we also collected 
more  homogeneous  corpora  from  prestigious 
writers: a translation of a book of philosophy and 
a novel. Details about these corpora are shown in 
Table 1.
2
Size of the corpora
Corpora from the newspaper Egunkaria 420,000 words
Philosophy texts written by one unique author 25,000 words
Literature texts written by one unique author 25,000 words
Table 1. Dimensions of the used corpora
A short version of the first corpus was used in 
different experiments in order to tune the system 
(see section 4). The differences between the re?
sults  depending on the type of  the corpora are 
shown in section 5.
Evaluation
Results are shown using the standard measures in 
this area: precision, recall and f?measure2, which 
are calculated based on the test corpus. The res?
ults are shown in two colums ("0" and "1") that 
correspond to the result categories used. The res?
ults for the column ?0? are the ones for the in?
stances that are not followed by a comma. On the 
contrary, the results for the column ?1? are the 
results for the instances that should be followed 
by a comma. 
Since  our  final  goal  is  to  build  a  comma 
checker,  the precision in the column ?1? is the 
most  important  data  for  us,  although the recall 
for the same column is also relevant. In this kind 
of tools, the most important thing is to first ob?
tain all the comma proposals right (precision in 
columns ?1?), and then to obtain all the possible 
commas (recall in columns ?1?).
Baselines
In  the  beginning,  we  calculated  two  possible 
baselines based on a big part of the newspaper 
corpora in order to choose the best one. 
The  first  one  was  based  on  the  number  of 
commas  that  appeared  in  these  texts.  In  other 
words,  we  calculated  how  many  commas  ap?
peared in the corpora (8% out of all words), and 
then we put commas randomly in this proportion 
in the test corpus. The results obtained were not 
very good (see Table 2, baseline1), especially for 
the  instances  ?followed by  a  comma? (column 
?1?).
The second baseline was developed using the 
list  of  words appearing before a comma in the 
training corpora. In the test corpus, a word was 
tagged as ?followed by a comma? if it was one of 
the words of the mentioned list. The results (see 
baseline 2, in Table 2) were better, in this case, 
for the instances followed by a comma (column 
named  ?1?).  But,  on  the  contrary,  baseline  1 
provided us with better results for the instances 
not followed by a comma (column named ?0?). 
That is why we decided to take, as our baseline, 
2 f?measure = 2*precision*recall / (precision+recall)
the best data offered by each baseline (the ones 
in bold in table 2). 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
baseline 1 0.927 0.924 0.926 0.076 0.079 0.078
baseline 2 0.946 0.556 0.700 0,096 0.596 0.165
Table 2: The baselines
Methods and attributes
We  use  the  WEKA3 implementation  of  these 
classifiers: the Naive Bayes based classifier (Na?
iveBayes),  the  support  vector  machine  based 
classifier  (SMO)  and  the  decision?tree  (C4.5) 
based one (j48).
It  has  to  be  pointed  out  that  commas  were 
taken  away  from  the  original  corpora.  At  the 
same time, for each token, we stored whether it 
was followed by a  comma or not.  That  is,  for 
each  word  (token),  it  was  stored  whether  a 
comma was placed next to it or not. Therefore, 
each token in the corpus is equivalent to an ex?
ample (an instance). The attributes of each token 
are based on the token itself and some surround?
ing ones. The application window describes the 
number of tokens considered as information for 
each token.
Our initial application window was [?5, +5]; 
that means we took into account the previous and 
following 5 words (with their corresponding at?
tributes)  as  valid  information  for  each  word. 
However, we tuned the system with different ap?
plication windows (see section 4). 
Nevertheless, the attributes managed for each 
word can be as complex as we want. We could 
only use words, but we thought some morpho?
syntactic information would be beneficial for the 
machine to learn. Hence, we decided to include 
as much information as we could extract using 
the shallow syntactic parser of Basque (Aduriz et 
al.,  2004).  This  parser  uses  the  tokeniser,  the 
lemmatiser, the chunker and the morphosyntactic 
disambiguator  developed by  the  IXA4 research 
group. 
The attributes we chose to use for each token 
were the following:
? word?form
? lemma
? category 
? subcategory
? declension case
? subordinate?clause type
3 WEKA is a collection of machine learning algorithms for data mining tasks 
(http://www.cs.waikato.ac.nz/ml/weka/).
4 http://ixa.si.ehu.es
3
? beginning of chunk (verb, nominal, enti?
ty, postposition)
? end of chunk (verb, nominal, entity, post?
position)
? part of an apposition
? other  binary  features:  multiple  word  to?
ken,  full  stop,  suspension  points,  colon, 
semicolon,  exclamation  mark  and  ques?
tion mark 
We also included some additional  attributes 
which were automatically calculated: 
? number of verb chunks to the beginning 
and to the end of the sentence 
? number of nominal chunks to the begin?
ning and to the end of the sentence
? number  of  subordinate?clause  marks  to 
the beginning and to the end of the sen?
tence
? distance (in tokens) to the beginning and 
to the end of the sentence 
We also did other experiments using binary 
attributes that correspond to most used colloca?
tions (see section 4).
Besides, we used the result attribute ?comma? 
to store whether a comma was placed after each 
token. 
4 Experiments
Dimension of the corpus
In  this  test,  we  employed the  attributes  de?
scribed in section 3 and an initial window of [?5, 
+5], which means we took into account the pre?
vious 5 tokens and the following 5. We also used 
the C4.5 algorithm initially, since this algorithm 
gets very good results in other similar machine 
learning  tasks  related  to  the  surface  syntax 
(Alegria et al, 2004).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
100,000 train / 30,000 test 0,955 0,981 0,968 0,635 0,417 0,503
160,000 train / 45,000 test 0,947 0,981 0,964 0,687 0,43 0,529
330,000 train / 90,000 test 0,96 0,982 0,971 0,701 0,504 0,587
Table 3. Results depending on the size of corpora 
(C4.5 algorithm; [?5,+5] window).
As it  can be seen in table 3, the bigger the 
corpus,  the  better  the results,  but  logically,  the 
time expended to obtain the results also increases 
considerably. That is why we chose the smallest 
corpus  for  doing  the  remaining  tests  (100,000 
words  to  train  and  30,000  words  to  test).  We 
thought that the size of this corpus was enough to 
get good comparative results. This test, anyway, 
suggested that the best  results  we could obtain 
would  be  always  improvable  using  more  and 
more corpora. 
Selecting the window
Using the corpus and the attributes described be?
fore, we did some tests to decide the best applic?
ation window. As we have already mentioned, in 
some problems of this type, the information of 
the  surrounding  words  may  contain  important 
data to decide the result of the current word. 
In this test, we wanted to decide the best ap?
plication window for our problem. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
-5+5 0,955 0,981 0,968 0,635 0,417 0,503
-2+5 0,956 0,982 0,969 0,648 0,431 0,518
-3+5 0,957 0,979 0,968 0,627 0,441 0,518
-4+5 0,957 0,98 0,968 0,634 0,446 0,52
-5+2 0,956 0,982 0,969 0,65 0,424 0,514
-5+3 0,956 0,981 0,969 0,643 0,432 0,517
-5+4 0,955 0,982 0,968 0,64 0,417 0,505
-6+2 0,956 0,982 0,969 0,645 0,421 0,509
-6+3 0,956 0,982 0,969 0,646 0,426 0,514
-8+2 0,956 0,982 0,969 0,645 0,425 0,513
-8+3 0,956 0,979 0,967 0,615 0,431 0,507
-8+8 0,956 0,978 0,967 0,604 0,422 0,497
Table  4.  Results  depending  on  the  application 
window (C4.5 algorithm; 100,000 train / 30,000 
test)
As it can be seen, the best f?measure for the 
instances followed by a comma was obtained us?
ing the application window [?4,+5]. However, as 
we have said before, we are more interested in 
the precision. Thus, the application window [?5
,+2] gets the best precision, and, besides, its f?
measure is almost the same as the best one. This 
is the reason why we decided to choose the [?5
,+2] application window. 
Selecting the classifier
With  the  selected  attributes,  the  corpus  of 
130,000 words and the application window of [?5
, +2], the next step was to select the best classifi?
er for our problem. We tried the WEKA imple?
mentation of these classifiers:  the Naive Bayes 
based classifier (NaiveBayes), the support vector 
machine based classifier (SMO) and the decision 
tree based one (j48).  Table 5 shows the results 
obtained:
4
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
NB 0,948 0,956 0,952 0,376 0,335 0,355
SMO 0,936 0,994 0,965 0,672 0,143 0,236
J48 0,956 0,982 0,969 0,652 0,424 0,514
Table 5. Results depending on the classifier 
(100,000 train / 30,000 test; [?5, +2] window).
As we can see, the f?measure for the instances 
not followed by a comma (column ?0?) is almost 
the same for the three classifiers, but, on the con?
trary, there is a considerable difference when we 
refer  to  the  instances  followed  by  a  comma 
(column ?1?). The best f?measure gives the C4.5 
based classifier (J48) due to the better recall, al?
though the best precision is for the support vector 
machine  based  classifier  (SMO).  Definitively, 
the Na?ve Bayes (NB) based classifier was dis?
carded, but we had to think about the final goal 
of our research to choose between the other two 
classifiers.  Since our  final  goal  was to  build  a 
comma checker, we would have to have chosen 
the classifier that gave us the best precision, that 
is, the support vector machine based one. But the 
recall of the support vector machine based classi?
fier was not as good as expected to be selected. 
Consequently,  we  decided  to  choose  the  C4.5 
based classifier. 
Selecting examples
At this  moment,  the results  we get  seem to be 
quite good for the instances not  followed by a 
comma, but  not  so good for  the  instances  that 
should follow a comma. This could be explained 
by the fact that we have no balanced training cor?
pus. In other words, in a normal text, there are a 
lot  of  instances not  followed by a  comma, but 
there are not so many followed by it. Thus, our 
training  corpus,  logically,  has  very  different 
amounts of instances followed by a comma and 
not followed by a comma. That is the reason why 
the system will learn more easily to avoid the un?
necessary  commas  than  placing  the  necessary 
ones. 
Therefore,  we  resolved  to  train  the  system 
with a corpus where the number of instances fol?
lowed by a comma and not followed by a comma 
was the same. For that purpose, we prepared a 
perl program that changed the initial corpus, and 
saved only x words for each word followed by a 
comma. 
In  table  6,  we can see  the  obtained results. 
One to one means that in that case, the training 
corpus  had  one  instance  not  followed  by  a 
comma, for each instance followed by a comma. 
On the  other  hand,  one to  two means that  the 
training corpus had two instances not  followed 
by  a  comma  for  each  word  followed  by  a 
comma, and so on. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
normal 0,955 0,981 0,968 0,635 0,417 0,503
one to one 0,989 0,633 0,772 0,164 0,912 0,277
one to two 0,977 0,902 0,938 0,367 0,725 0,487
one to three 0,969 0,934 0,951 0,427 0,621 0,506
one to four 0,966 0,952 0,959 0,484 0,575 0,526
one to five 0,966 0,961 0,963 0,534 0,568 0,55
one to six 0,963 0,966 0,964 0,55 0,524 0,537
Table  6.  Results  depending  on  the  number  of 
words  kept  for  each  comma  (C4.5  algorithm; 
100,000 train / 30,000 test; [?5, +2] window). 
As  observed  in  the  previous  table,  the  best 
precision in the case of the instances followed by 
a comma is the original one: the training corpus 
where  no  instances  were  removed.  Note  that 
these results are referred as normal in table 6.
The corpus where a unique instance not fol?
lowed by a comma is kept for each instance fol?
lowed by a comma gets the best  recall  results, 
but the precision decreases notably. 
The  best  f?measure  for  the  instances  that 
should be followed by a comma is obtained by 
the one to five scheme, but as mentioned before, 
a comma checker must take care of offering cor?
rect comma proposals. In other words, as the pre?
cision of the original corpus is quite better (ten 
points better), we decided to continue our work 
with  the  first  choice:  the  corpus  where  no  in?
stances were removed. 
Adding new attributes
Keeping the best results obtained in the tests de?
scribed above (C4.5 with the [?5,  +2] window, 
and not removing any ?not comma? instances), 
we thought that giving importance to the words 
that appear normally before the comma would in?
crease our results. Therefore, we did the follow?
ing tests: 
1) To search a big corpus in order to extract 
the most  frequent  one hundred words  that  pre?
cede a  comma,  the  most  frequent  one hundred 
pairs of words (bigrams) that precede a comma, 
and the most frequent one hundred sets of three 
words (trigrams) that precede a comma, and use 
them as attributes in the learning process. 
2) To use only three attributes instead of the 
mentioned three hundred to encode the informa?
tion  about  preceding  words.  The  first  attribute 
would indicate whether a word is or not one of 
5
the  most  frequent  one  hundred  words.  The 
second attribute would mean whether a word is 
or not the last part of one of the most frequent 
one hundred pairs of words. And the third attrib?
ute would mean whether a word is or not the last 
part of one of the most frequent one hundred sets 
of three words. 
3) The case (1), but with a little difference: 
removing the attributes ?word? and ?lemma? of 
each instance. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
(0): normal 0,956 0,982 0,969 0,652 0,424 0,514
(1): 300 attributes + 0,96 0,983 0,972 0,696 0,486 0,572
(2): 3 attributes + 0,96 0,981 0,97 0,665 0,481 0,558
(3): 300 attributes +,  
no lemma, no word 0,955 0,987 0,971 0,71 0,406 0,517
Table 7. Results depending on the new attributes 
used (C4.5 algorithm; 100,000 train / 30,000 test; 
[?5, +2] window; not removed instances).
Table 7 shows that case number 1 (putting the 
300 data as attributes) improves the precision of 
putting  commas  (column  ?1?)  in  more  than  4 
points. Besides, it also improves the recall, and, 
thus, we improve almost 6 points its f?measure. 
The third test gives the best precision, but the 
recall decreases considerably. Hence, we decided 
to choose the case number 1, in table 7.
5 Effect of the corpus type
As we have said before (see section 3), depend?
ing on the quality of the texts, the results could 
be different.
In table 8, we can see the results using the dif?
ferent types of corpus described in table 1. Obvi?
ously,  to  give  a  correct  comparison,  we  have 
used the same size for all the corpora (20,000 in?
stances to train and 5,000 instances to test, which 
is the maximum size we have been able to ac?
quire for the three mentioned corpora).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
Newspaper 0.923 0.977 0.949 0.445 0.188 0.264
Philosophy 0.932 0.961 0.946 0.583 0.44 0.501
Literature 0.925 0.976 0.95 0.53 0.259 0.348
Table 8. Results depending on the type of corpo?
ra (20,000 train / 5,000 test).
The first line shows the results obtained using 
the short version of the newspaper. The second 
line  describes  the  results  obtained  using  the 
translation of a book of philosophy, written com?
pletely by one author. And the third one presents 
the  results  obtained  using  a  novel  written  in 
Basque. 
In any case, the results prove that our hypo?
thesis  was  correct.  Using  texts  written  by  a 
unique author improves the results. The book of 
philosophy has the best precision and the best re?
call.  It  could be  because it  has  very long sen?
tences  and  because  philosophical  texts  use  a 
stricter syntax comparing with the free style of a 
literature writer.  
As it was impossible for us to collect the ne?
cessary  amount  of  unique  author  corpora,  we 
could not go further in our tests.
6 Conclusions and future work
We have used machine learning techniques for 
the  task  of  placing  commas  automatically  in 
texts. As far as we know, it is quite a novel ap?
plication field. Hardt (2001) described a system 
which identified incorrect commas with a preci?
sion of 91% and a recall of 77% (using 600,000 
words  to  train).  These  results  are  comparable 
with the ones we obtain for the task of guessing 
correctly when not to place commas (see column 
?0? in the tables). Using 100,000 words to train, 
we obtain 96% of precision and 98.3% of recall. 
The main reason could be that we use more in?
formation to learn.
However, we have not obtained as good res?
ults as we hoped in the task of placing commas 
(we  get  a  precision  of  69.6%  and  a  recall  of 
48.6%). Nevertheless, in this particular task, we 
have  improved  considerably  with  the  designed 
tests, and more improvements could be obtained 
using more corpora and more specific corpora as 
texts written by a unique author or by using sci?
entific texts. 
Moreover,  we have detected some possible 
problems that could have brought these regular 
results in the mentioned task:
? No fixed rules for commas in the Basque 
language
? Negative influence when training using 
corpora from different writers
In this sense, we have carried out a little ex?
periment with some English corpora. Our hypo?
thesis was that a completely settled language like 
English,  where  comma  rules  are  more  or  less 
fixed, would obtain better results. Taking a com?
parative English corpus5 and similar learning at?
tributes6 to  Basque?s  one,  we  got,  for  the  in?
stances  followed  by  a  comma  (column  ?1?  in 
tables), a better precision (%83.3) than the best 
5 A newspaper corpus, from Reuters
6 Linguistic information obtained using Freeling (http://garraf.ep?
sevg.upc.es/freeling/)
6
one obtained for the Basque language. However, 
the recall was worse than ours: %38.7. We have 
to take into account that we used less learning at?
tributes with the English corpus and that we did 
not  change  the  application  window chosen  for 
the Basque experiment. Another application win?
dow would have been probably more suitable for 
English.  Therefore, we believe that with a few 
tests  we  easily  would  achieve  a  better  recall. 
These  results,  anyway,  confirm our  hypothesis 
and our diagnosis of the detected problems. 
Nevertheless,  we think the presented results 
for the Basque language could be improved. One 
way would  be  to  use  ?information  gain? tech?
niques in order to carry out the feature selection. 
On the other hand, we think that more syntactic 
information, concretely clause splits tags, would 
be especially beneficial to detect those commas 
named delimiters by Nunberg (1990).
In fact, our main future research will consist 
on clause identification. Based on the ?accepted 
theory of the comma?, we can assure that a good 
identification of clauses (together with some sig?
nificant linguistic information we already have) 
would enable us to put commas correctly in any 
text,  just  implementing some simple rules.  Be?
sides, a combination of both methods ??learning 
commas  and  putting  commas  after  identifying 
clauses??  would  probably  improve  the  results 
even more. 
Finally,  we contemplate building an ICALL 
(Intelligent Computer Assisted Language Learn?
ing) system to help learners to put commas cor?
rectly.
Acknowledgements
We would like to thank all the people who have 
collaborated in this research: Juan Garzia,  Joxe 
Ramon  Etxeberria,  Igone  Zabala,  Juan  Carlos 
Odriozola, Agurtzane Elorduy, Ainara Ondarra, 
Larraitz Uria and Elisabete Pociello. 
This research is supported by the University 
of  the  Basque  Country  (9/UPV00141.226?
14601/2002) and the Ministry of Industry of the 
Basque  Government  (XUXENG  project, 
OD02UN52).
References
Aduriz  I., Aranzabe  M., Arriola  J., D?az  de  Ilarraza 
A., Gojenola  K., Oronoz  M., Uria  L.   2004.
A  Cascaded  Syntactic  Analyser  for  Basque  
Computational  Linguistics  and  Intelligent  Text  
Processing. 2945  LNCS  Series.pg.  124?135. 
Springer Verlag. Berlin (Germany).
Aldezabal I., Aranzabe M., Arrieta B., Maritxalar M., 
Oronoz M. 2003.  Toward a punctuation checker 
for Basque. Atala Workshop on Punctuation. Paris 
(France).
Alegria I., Arregi  O., Ezeiza N., Fernandez I., Urizar 
R. 2004. Design and Development of a Named En?
tity  Recognizer  for  an  Agglutinative  Language. 
First International Joint Conference on NLP (IJC?
NLP?04). Workshop on Named Entity Recognition. 
Ansa O., Arregi X., Arrieta B., Ezeiza N., Fernandez 
I.,  Garmendia  A.,  Gojenola  K.,  Laskurain  B., 
Mart?nez  E.,  Oronoz  M.,  Otegi  A.,  Sarasola  K., 
Uria L. 2004. Integrating NLP Tools for Basque in  
Text Editors. Workshop on International Proofing 
Tools  and Language Technologies.  University  of 
Patras (Greece).
Aranzabe M., Arriola J.M., D?az de Ilarraza A.  2004.
Towards  a  Dependency  Parser  of  Basque.
Proceedings of the Coling 2004 Workshop on Re?
cent Advances in Dependency Grammar. Geneva 
(Switzerland).
Bayraktar M., Say B., Akman V. 1998. An Analysis of  
English Punctuation:  the special  case of  comma. 
International  Journal  of  Corpus  Linguistics 
3(1):pp. 33?57.  John  Benjamins  Publishing  Com?
pany. Amsterdam (The Netherlands).
Beeferman D.,  Berger  A.,  Lafferty  J.  1998.  Cyber?
punc: a lightweight punctuation annotation system 
for speech. Proceedings of the IEEE International 
Conference on Acoustics, Speech and Signal Pro?
cessing, pages 689?692, Seattle (WA).
Brill, E. 1994.  Some Advances in rule?based part of  
speech tagging. In Proceedings of the Twelfth Na?
tional Conference on Artificial Intelligence. Seattle 
(WA). 
Brill,  E.  1995.  Transformation?based  error?driven 
learning and natural language processing: a case 
study  in  part  of  speech  tagging. Computational 
Linguistics 21(4). MIT Press. Cambridge (MA).
Briscoe T., Carroll J. 1995.  Developing and evaluat?
ing a probabilistic lr parser of part?of?speech and  
punctuation  labels.  ACL/SIGPARSE 4th  interna?
tional Workshop on Parsing Technologies, Prague / 
Karlovy Vary (Czech Republic). 
Carreras X., M?rquez L. 2003. Phrase Recognition by 
Filtering and Ranking with Perceptrons. Proceed?
ings of the 4th RANLP Conference. Borovets (Bul?
garia).
D?az de  Ilarraza A., Gojenola K., Oronoz M.   2005.
Design and Development of a System for the De?
tection of Agreement Errors in Basque. CICLing?
2005, Sixth International Conference on Intelligent 
Text  Processing  and  Computational  Linguistics. 
Mexico City (Mexico).
Garzia  J.  1997.  Joskera  Lantegi. Herri  Arduralar?
itzaren Euskal Erakundea. Gasteiz, Basque Country 
(Spain).
7
Hardt D. 2001.  Comma checking in Danish.  Corpus 
linguistics. Lancaster (England). 
Hill R.L., Murray W.S. 1998.  Commas and Spaces: 
the Point of Punctuation. 11th Annual CUNY Con?
ference  on  Human  Sentence  Processing.  New 
Brunswick, New Jersey (USA). 
Jones B. 1996. Towards a Syntactic Account of Punc?
tuation. Proceedings of the 16th International Con?
ference on Computational Linguistics. Copenhagen 
(Denmark). 
Nunberg,  G.  1990.  The  linguistics  of  punctuation. 
Center for the Study of Language and Information. 
Leland Stanford Junior University (USA).
Say B., Akman V. 1996.  Information?Based Aspects 
of  Punctuation.  Proceedings  ACL/SIGPARSE In?
ternational  Meeting  on  Punctuation  in  Computa?
tional  Linguistics,  pages  pp. 49?56,  Santa  Cruz, 
California (USA). 
Tjong Kim Sang E.F. and Buchholz S. 2000.  Intro?
duction to the CoNLL?2000 shared task: chunking. 
In  proceedings  of  CoNLL?2000  and  LLL?2000. 
Lisbon (Portugal).
Tjong Kim Sang E.F. and D?jean H. 2001. Introduc?
tion to the CoNLL?2001 shared task: clause identi?
fication. In proceedings of CoNLL?2001. Tolouse 
(France).
Van Delden  S.,  Gomez  F.  2002.  Combining  Finite 
State Automata and a Greedy Learning Algorithm 
to Determine the Syntactic Roles of Commas. 14th 
IEEE International Conference on Tools with Arti?
ficial Intelligence. Washington, D.C. (USA)
Zubimendi,  J.R. 2004.  Ortotipografia.  Estilo liburu?
aren lehen atala. Eusko Jaurlaritzaren Argitalpen 
Zerbitzu  Nagusia.  Gasteiz,  Basque  Country 
(Spain).
8
Representation and Treatment of Multiword Expressions in Basque 
I?aki Alegria, Olatz Ansa, Xabier Artola 
Nerea Ezeiza, Koldo Gojenola and Ruben Urizar 
Ixa Group 
University of the Basque Country 
649 pk E-20.080 
Donostia. Basque Country 
rubenu@sc.ehu.es 
Abstract 
This paper describes the representation of 
Basque Multiword Lexical Units and the 
automatic processing of Multiword 
Expressions. After discussing and stating 
which kind of multiword expressions we 
consider to be processed at the current 
stage of the work, we present the 
representation schema of the 
corresponding lexical units in a general-
purpose lexical database. Due to its 
expressive power, the schema can deal 
not only with fixed expressions but also 
with morphosyntactically flexible 
constructions. It also allows us to 
lemmatize word combinations as a unit 
and yet to parse the components 
individually if necessary. Moreover, we 
describe HABIL, a tool for the automatic 
processing of these expressions, and we 
give some evaluation results. This work 
must be placed in a general framework of 
written Basque processing tools, which 
currently ranges from the tokenization 
and segmentation of single words up to 
the syntactic tagging of general texts. 
1 Introduction 
2 
Most texts are rich in multiword expressions, 
which must be necessarily processed if we want 
any NLP tool to perform accurately. Jackendoff 
(1997) estimates that their number in the speakers' 
lexicon ?is of the same order of magnitude as the 
number of single words?. 
There is no agreement among authors about the 
definition of the term Multiword Expression. 
However, in this article, Multiword Expressions 
(hereafter MWE) refer to any word combinations 
ranging from idioms, over proper names, 
compounds, lexical and grammatical 
collocations? to institutionalized phrases. MWEs 
comprise both semantically compositional and 
non-compositional combinations, and both 
syntactically regular and idiosyncratic phrases, 
including complex named entities such as proper 
nouns, dates and number expressions (see section 
2). 
In contrast, Multiword Lexical Units (hereafter 
MWLU) comprise lexicalized phrases ?
semantically non-compositional or syntactically 
idiosyncratic word combinations? which are 
represented and stored in the lexical database of 
Basque (EDBL). 
The remaining sections are organized as 
follows. Section 2 presents the main features of 
MWEs in Basque, and defines which are currently 
considered for automatic processing. Section 3 
describes the representation of MWLUs in the 
lexical database. Section 4 is devoted to the 
description and evaluation of the automatic 
treatment of MWEs by means of HABIL. Section 
5 summarizes future work. And, finally, section 6 
outlines some conclusions. 
Multiword Expressions in the 
processing of real texts in Basque 
The definition of the term Multiword Expression 
and the types of such MWEs to be treated in NLP 
may vary considerably depending on the purposes 
or "the depth of processing being undertaken" 
(Copestake et al, 2002). Multiword itself is a 
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 48-55
vague term. At text level, a word could be defined 
as "any string of characters between two blanks" 
(Fontenelle et al, 1994). This is not applicable to 
languages as Japanese, which are typically written 
without spaces. Besides, a great number of MWEs 
that in uninflected languages would be multiword, 
constitute a single typographic unit in agglutinative 
languages such as Basque (ziurrenik 'most 
probably', aurrerantzean 'from now on', aurretiaz 
'in advance'). Therefore, we consider them single 
words and they are included in the lexical database 
as such (or recognized by means of morphological 
analysis). 
In our case, when deciding which Basque 
MWEs to include in the database, we mostly rely 
on lexicographers' expertise since we consider 
lexicalized phrases have a top priority for both 
lemmatizing and syntactic purposes. So, the 
MWEs dealt with in the database comprise fixed 
expressions, which admit no morphosyntactic or 
internal modification ?including foreign 
expressions such as in situ, a priori, strictu sensu, 
etc.?, idioms, both decomposable and non-
decomposable, and lexicalized compounds. We 
also consider light verb constructions when they 
are syntactically idiosyncratic.  
However, currently we do not treat open 
collocations, proverbs, catch phrases and similes. 
Mostly, we don't include proper names in the 
database either, since complex named entities are 
given a separate treatment. Apart from proper 
nouns, also dates and number expressions are 
treated separately (see 4.1). 
So far we have described 2,270 MWLUs in our 
database. This work has been carried out in two 
phases. For the first phase, we made use of the 
Statistical Corpus of 20th Century Basque 
(http://www.euskaracorpusa.net) that contains 
about 4.7 million words. As a starting point, we 
chose the MWLUs that occurred more than 10 
times in this manually lemmatized corpus. This 
amounted to about 1,300 expressions. For the 
second phase, this list has been enlarged using the 
Hiztegi Batua, a dictionary of standard Basque that 
the Basque Language Academy updates regularly 
(http://www2.euskaltzaindia.net/hiztegibatua). 
2.1 
3 
Main features of lexicalized phrases 
Many of the lexicalized phrases are semantically 
non-compositional (or partially compositional), i.e. 
they can hardly be interpreted in terms of the 
meaning of their constituents (adarra jo 'to pull 
someone's leg', literally 'to play the horn'). 
Often, a component of these sequences hardly 
occurs in any other context and it is difficult to 
assign it a part of speech. For example, the word 
noizik is an archaism of modern noiztik 'from 
when', which occurs just in the expressions noizik 
behin, noizik behinean, noizik noizera, and noizik 
behinka all meaning 'once in a while'. Besides, it is 
not clear which is the part of speech of the words 
laprast in laprast egin 'to slip' or dir-dir in dir-dir 
egin 'to shine'. 
From a syntactic point of view, many of these 
MWEs present an unusual structure. For example, 
many complex verbs in Basque are light verb 
constructions, being the meaning of the compound 
quite compositional, e.g. lo egin 'to sleep' literally 
'to make (a) sleep' or lan egin 'to work' literally 'to 
make (a) work'. However, lo egin and lan egin can 
be considered 'syntactically idiomatic' since the 
nouns in these expressions, lo and lan, take no 
determiner, which would be completely 
ungrammatical for a noun functioning as a regular 
direct object (*arroz jan nuen 'I ate rice'). 
Morphosyntactic flexibility, being significant in 
this type of constructions in Basque, may vary 
considerably. For example in lo egin 'to sleep' the 
noun lo admits modification (lo asko egin zuen 'he 
slept very much') and may take the partitive 
assignment (ez dut lorik egin 'I haven't slept') while 
the verb egin can be subject to focalization (egin 
duzu lorik bart? 'did you sleep at all last night?'); 
besides, the components of the construction may 
change positions and some elements and phrases 
may be placed between them (mendian egin omen 
zuen lasai lo 'it is said that he slept peacefully in 
the mountain'). In contrast, alde egin 'to escape' is 
morphosyntactically quite rigid. In all the cases, 
the verb egin can take any inflection. 
For our database, we have worked out a single 
representation that covers all MWLUs ranging 
from fixed expressions to these of highest 
morphosyntactic flexibility. 
Representation of MWLUs in the lexical 
database 
In this section we explain how MWLUs are 
represented in EDBL (Aldezabal et al, 2001), a 
lexical database oriented to language processing 
that currently contains more than 80,000 entries, 
out of which 2,270 are MWLUs. Among these: 
? ~69% are always unambiguous. The average 
number of Surface Realization Schemas 
(SRS, see section 3.2) is 1.02. 
? ~23% are sometimes unambiguous and have 
3.6 SRSs in average, half of them 
ambiguous. 
? ~8% are always ambiguous and have 1.2 
SRSs in average. 
We want to point out that almost all of the 
unambiguous MWLUs have only one SRS, their 
components appearing in contiguous positions and 
always in the same order. About half of them are 
inflected, so, even if we discard the interpretations 
of the components, there is still some 
morphosyntactic ambiguity left. However, the 
identification of these MWLUs helps in 
disambiguation, as the input of tagging is more 
precise. 
 
The description of MWLUs within a general-
purpose lexical database must include, at least, two 
aspects (see Figure 1): (1) their composition, i.e. 
which the components of the MWLU are, whether 
each of them can be inflected or not, and according 
to which one-word lexical unit (OWLU 1 ) it 
inflects; and (2), what we call the surface 
realization, that is, the order in which the 
components may occur in the text, the mandatory 
or optional contiguousness of components, and the 
inflectional restrictions applicable to each one of 
the components. 
3.1 
                                                          
Composition 
As it has just been said, the description of the 
composition of MWLUs in EDBL gathers two 
aspects: on the one side, it depicts which the 
individual components of a MWLU are; on the 
other side, it links the inflectable components of a 
MWLU to the corresponding OWLU according to 
which each of them inflects. 
In Figure 1, we can see that the composed of 
relationship links every MWLU to up to 9 
individual components (MWLU_Components). 
Each component is characterized by the following 
attributes: 
1 We consider OWLUs lexical units with no spaces within its 
orthographical form; so, we also take hyphenated compounds 
as OWLUs. 
? Component_Position: this indicates 
the position of the component word-form in 
the canonical form of the MWLU. 
? Component_Form: i.e. the word-form 
itself as it appears in the canonical form of 
the MWLU. 
? Conveys_Morph_Info?: this is a 
Boolean value, indicating whether the 
component inflection conveys the 
morphological information corresponding to 
the whole MWLU or not2. 
(0,n)
(1,1)
(1,1)
(1,n)
(2,9)
(1,1)
 
MWLUs
Surface_Realization_Schemas
Order_Contiguousness
Sureness
Inflection_Restrictions
MWLU_Components
Component_Position
Component_Form
Conveys_Morph_Info?
OWLUs
corresp.
SR schemas
inflects
according to
composed of
Figure 1. Composition and surface realization of 
MWLUs. 
Moreover, the components of a MWLU are 
linked to its corresponding OWLU (according to 
which it inflects). This is represented by means of 
the inflects according to relationship 
(see Figure 1). 
                                                          
2 The morphological information that the attribute refers to is 
the set of morphological features the inflection takes in the 
current component instance. 
These two aspects concerning the composition 
of a MWLU are physically stored in a single table 
of the relational database in which EDBL resides.  
The columns of the table are the following: 
Entry, Homograph_Id, Component_ 
Position, Component_Form, Conveys_ 
Morph_Info?, OWLU_Entry, and OWLU_ 
Homograph_Id. In the example below, the 
composition of the MWLU begi bistan egon 'to be 
evident' is described. Note that one row is used per 
component: 
<begi bistan egon, 0, 1, begi, -, begi, 2> 
<begi bistan egon, 0, 2, bistan, -, bista, 1> 
<begi bistan egon, 0, 3, egon, +, egon, 1> 
This expression allows different realizations 
such as begi bistan dago 'it is evident' (literally 'it 
is at reach of the eyes'), begi bistan daude 'they are 
evident', begien bistan egon, 'to be evident', etc. In 
the table rows above, it can be seen that the last 
component egon 1 'to be' conveys the 
morphological information for the whole MWLU 
(+ in the corresponding column). 
3.2 Surface realization 
As for surface realization, we have already 
mentioned that the components of a MWLU can 
occur in a text either contiguously or dispersed. 
Besides, the order of the constituents may be fixed 
or not, and they may either inflect or occur in an 
invariable form. In the case of inflected 
components, some of them may accept any 
inflection according to its corresponding OWLU, 
whilst others may only inflect in a restricted way. 
Moreover, some MWLUs are unambiguous and 
some are not, since it cannot be certainly assured 
that the very same sequence of words in a text 
corresponds undoubtedly to a multiword entry in 
every context. For example, in the sentence Emilek 
buruaz baiezko keinu bat egin zuen 'Emile nodded 
his head' the words bat and egin do not correspond 
to the complex verb bat egin 'to unite' but to two 
separate phrases. 
According to these features, we use a formal 
description where different realization patterns 
may be defined for each MWLU. The corresp. 
SR schemas relationship in Figure 1 links every 
MWLU to one or more Surface_Realiza-
tion_Schemas. Each SRS is characterized by 
the following attributes: 
? Order_Contiguousness: an expression 
that indicates both the order in which the 
components may appear in the different 
instances of the MWLU and the 
contiguousness of these components. In 
these expressions the position of the digits 
indicate the position each component takes 
in a particular SRS, * indicates that 0 or 
more words may occur between two 
components, and ? indicates that at most 
one single word may appear between two 
given components of the MWLU. 
? Unambiguousness: a Boolean value, 
indicating whether the particular SRS 
corresponds to an unambiguous MWLU or 
not. It expresses whether the sequence of 
words matching this SRS must be 
unambiguously analyzed as an instance of 
the MWLU or, on the contrary, may be 
analyzed as separate OWLUs in some 
contexts. 
? Inflection_Restrictions: an 
expression that indicates the inflection 
paradigm according to which the MWLU 
may inflect in this specific SRS. In these 
expressions each component of the MWLU 
is represented by one list component (in the 
same order as the components of the 
MWLU appear in its canonical form): % 
indicates that the whole inflection paradigm 
of the corresponding inflectable component 
may occur; the minus sign (-) is used for 
non-inflectable components (no inflection at 
all may occur); finally, a logical expression 
(and, or, and not are allowed) composed 
of attribute-value pairs is used to express the 
inflectional restrictions and the 
morphotactics the component undergoes in 
this particular SRS of the MWLU (in 
brackets in the examples below). 
In the examples below, it can be seen that one 
row is used per SRS. The columns of the table are 
the following: Entry, Homograph_Id, Or-
der_Contiguousness, Unambiguousness, 
and Inflection_Restrictions: 
<begi bistan egon, 0, 123, +, 
 (((CAS=ABS) and (DEF=-)) or 
  ((CAS=GEN) and (NUM=PL)), -, %)> 
<begi bistan egon, 0, 312, +, 
 (((CAS=ABS) and (DEF=-)) or 
  ((CAS=GEN) and (NUM=PL)), -, %)> 
<begi bistan egon, 0, 3?12, +, 
 (((CAS=ABS) and (DEF=-)), -, %)> 
 
The first SRS matches occurrences such as begi 
bistan dago hau ez dela aski 'it is evident that it is 
not enough' or begien bistan zegoen honela 
bukatuko genuela 'it was evident that we would 
end up this way', where the components are 
contiguous and the analysis as an instance of the 
MWLU would be unambiguous. This SRS allows 
the inflection of the first component as absolutive 
case (non-definite) or as genitive (plural), and the 
whole set of inflection morphemes of the third one. 
The third SRS matches occurrences such as ez 
dago horren begi bistan 'it is not so evident', where 
the components are not contiguous (at most one 
word is allowed between the ?third? component 
and the ?first one?) and they occur in a non-
canonical order: 3?12. In this case, the 
interpretation as an instance of the MWLU would 
also be unambiguous. However, this SRS only 
allows the inflection of the first component as 
absolutive case (non-definite). 
3.3 
4 
Different information requirements in 
lemmatization and syntax processing 
The first prototype for the treatment of MWEs in 
Basque HABIL (Ezeiza et al, 1998; Ezeiza, 2003) 
was built for lemmatization purposes. However, 
we are nowadays involved in the construction of a 
deep syntactic parser (Aduriz et al, 2004) and the 
MWEs seem to need a different treatment. The fact 
that many MWEs may be syntactically regular but, 
above all, that an external element may have a 
dependency relation with one of the constituents, 
forces us to analyze the elements independently. 
For example, in the verb beldur izan 'to be afraid 
(of)' an external noun phrase may have a modifier-
noun dependency relation with beldur 'fear' as in 
sugeen beldur naiz 'I'm afraid of snakes'. In loak 
hartu 'to fall asleep' there is a subject-verb relation 
as in loak hartu nau 'I have fallen asleep', literally 
'sleep has caught me'; therefore subject-auxiliary 
verb agreement would fade if both components 
were analyzed as one. 
The MWLU representation we have adopted 
allows us to lemmatize the word combination as a 
unit and yet to parse the components individually 
whenever necessary. In order to do so, when 
describing each MWLU, we specify whether the 
elements in the MWLU must be analyzed 
separately or not3. 
Treatment of multiword expressions 
MWEs could be treated at different stages of the 
language process. Some approaches treat them at 
tokenization stage, identifying fixed phrases, such 
as prepositional phrases or compounds, included in 
a list (Carmona et al, 1998; Karlsson et al, 1995). 
Other approaches rely on morphological analysis 
to better identify the features of the MWE using 
finite state technology (Breidt et al, 1996). 
Finally, there is another approach that identifies 
them after the tagging process, allowing the 
correction of some tagging errors (Leech et al, 
1994). 
All of these approaches are based on the use of 
a closed set of MWLUs that could be included in a 
list or a database. However, some groups of MWEs 
are not subject to be included in a database, 
because they comprise an open class of 
expressions. That is the case of collocations, 
compounds or named entities. The group of 
collocations and compounds should be delimited 
using statistical approaches, such as Xtract 
(Smadja, 1993) or LocalMax (Silva et al, 1999), 
so that only the most relevant?those of higher 
frequency? are included in the database.  
Named entity recognition task has been solved 
for a large set of languages. Most of these works 
are linked to the Message Understanding 
Conference (Chinchor, 1997). There is a variety of 
methods that have been used in NE recognition, 
such as HMM, Maximum Entropy Models, 
Decision Trees, Boosting and Voted Perceptron 
(Collins, 2002), Syntactic Structure based 
approaches and WordNet-based approaches 
(Magnini et al, 2002; Ar?valo, 2002). Most 
references on NE task might be accessed at 
http://www.muc.saic.com. 
4.1 
                                                          
Processing MWEs with HABIL 
We have implemented HABIL, a tool for the 
treatment of multiword expressions (MWE), based 
3  Currently we are studying the MWLUs in the lexical 
database in order to determine which of them deserve to be 
parsed as separate elements. We have not defined yet how this 
will be formally represented in the database. 
on the features described in the lexical database. 
The most important features of HABIL are the 
following: 
? It deals with both contiguous and split 
MWEs. 
? It takes into account all the possible orders 
of the components (SRS). 
? It checks that inflectional restrictions are 
complied with. 
? It generates morphosyntactic interpretations 
for the MWE. 
This tool has two different components: on the 
one hand, there is a searching engine that identifies 
MWEs along the text, and, on the other hand, there 
is a morphosyntactic processor that assigns the 
corresponding interpretations to the components of 
the MWE. 
The morphosyntactic processor generates the 
interpretations for MWEs using category and 
subcategory information in the lexical database. 
When one of the components adds information to 
the MWE, the processor applies pattern-matching 
techniques to extract the corresponding 
morphological features of the analyses of that 
component, and these features are included in the 
interpretation of the MWE. Then, it replaces all the 
morphosyntactic interpretations of the components 
of unambiguous MWEs with the MWE 
interpretations. When MWEs are ambiguous, the 
new interpretations are added to the existing ones. 
HABIL also identifies and treats dates and 
numerical expressions. As they make up an open 
class, they are not obviously included in the lexical 
database. Furthermore, their components are 
always contiguous, have a very strict structure, and 
use a closed lexicon. Thus, it is quite easy to 
identify them using simple finite state transducers. 
For the morphosyntactic treatment of dates and 
numerical expressions, we use the morphosyntactic 
component of HABIL. These expressions may 
appear inflected and, in this case, the last 
component adds morphosyntactic features to the 
MWE. Finally, as they are unambiguous 
expressions, the processor discards the 
interpretations of the components and assigns them 
all the interpretations of the whole expression. 
4.2 Evaluation 
We performed several experiments using 650 
unambiguous, contiguous and ordered MWEs. We 
treated a reference corpus of around 36,000 tokens 
and there were 386 instances of 149 different 
MWEs. We also applied this process to a small test 
corpus of around 7,100 tokens in which there were 
87 instances of 45 MWEs. Taking both corpora 
into account, there were 473 instances of 167 
different MWEs, which amounted to 25% of the 
expressions considered, and 50% of the instances 
were ambiguous. Besides, only 14 dates and 12 
numerical expressions were found in the reference 
corpus, and 18 dates and 9 numerical expressions 
in the test corpus. 
 
  Ambiguity 
Rate 
Interpretations 
per Token 
Recall
word-
forms: 
before
after 
81.78% 
79.83% 
3.37 
3.30 
99.31%
99.31%
all 
tokens: 
before
after 
67.47% 
65.86% 
2.96 
2.89 
99.43%
99.43%
Table 1. Results of HABIL. 
The ambiguity measures of the test corpus are 
shown in Table 1. The ambiguity rate of word-
forms decreases by 2% and the average ambiguity 
rate by 1.5% after the processing of MWEs. It is 
important to point out that no error is made along 
the process. Furthermore, some important MWEs, 
more specifically, some complex sentence 
connectors that have highly ambiguous 
components, are correctly disambiguated. 
Bearing in mind the proportion of words treated 
by HABIL, these results help significantly in 
improving precision results of tagging and 
avoiding almost 10% of the errors, as shown in 
Table 2.  
 
 Precision Error 
before MWE processing 94.96% 5.04% 
after MWE processing 95.42% 4.58% 
Table 2. Tagging results. 
5 Future work 
After confirming the viability of the system and the 
good results in POS tagging, our main goal is to 
increase the number of MWLUs in the database, 
which will improve the identification of MWEs in 
corpora. 
A remaining difficulty that we are facing is the 
problem of ambiguous split MWEs. At present, we 
are creating a disambiguation grammar that will 
discard or select the multiword interpretations in 
ambiguous MWLUs. We are developing similar 
rules using both the Constraint Grammar 
formalism and finite state transducers (XFST tools, 
Kartunnen et al 1997). The very first rules seem to 
be quite effective. Soon, we will be assessing the 
first results, and then we will be able to choose the 
method that performs best with a lesser effort. 
Once we have chosen the best formalism, we 
intend to develop a comprehensive grammar that 
will disambiguate as many ambiguous MWLUs as 
possible. 
In addition, we are developing new processes 
after POS tagging in order to identify complex 
named entities and terminological units. These 
units constitute an open class and so their 
exhaustive inclusion in a database would not be 
viable. 
6 Conclusion 
7 
In this paper we have described a whole 
framework for the representation and treatment of 
MWEs, which is being currently used at the IXA 
Research Group to process this kind of expressions 
in general texts. Although it has been conceived 
and so far used for Basque, a highly inflected 
language, we think that it is general enough to be 
applied to other languages. 
A general representation schema for MWLUs at 
the lexical level has been proposed. This schema 
allows us to state which components a MWLU has 
and to formally encode all the different surface 
realizations it can adopt in the text. 
The problems that diverse information require-
ments in lemmatization and syntactic processing 
can eventually pose have been explained, and a 
possible solution for the representation of these 
phenomena has also been outlined. 
As for the processing aspects, we have 
described HABIL, the tool for the treatment of 
MWEs. HABIL processes MWEs based on their 
description in the lexical database, dealing also 
with some types of open class MWEs. 
One of the remaining problems when split and 
ambiguous MWEs are to be tagged is related with 
disambiguation procedures using Hidden Markov 
Models, which are not able to manage different 
paths with variable lengths. This problem can be 
solved using rule-based methods or lattice 
structures for tagging. 
Acknowledgements 
This research is being partially funded by the 
European Commission (MEANING project, IST-
2001-34460) and the Basque Government 
(Etortek-Hizking, Saiotek-Ihardetsi). 
References 
Aduriz I., Aranzabe M., Arriola J., D?az de Ilarraza A., 
Gojenola K., Oronoz M., Uria L. 2004. A cascaded 
syntactic analyser for Basque. Fifth International 
Conference on Intelligence Text Processing and 
Computational Linguistics (CICLing2004). Seoul, 
Korea. 
Aldezabal I., Ansa O., Arrieta B., Artola X., Ezeiza N., 
Hern?ndez G., Lersundi M. 2001. EDBL: a General 
Lexical Basis for the Automatic Processing of 
Basque. IRCS Workshop on Linguistic Databases. 
Philadelphia. 
Ar?valo M. 2002. MICE, un recurso para la resoluci?n 
de la an?fora. International Workshop on 
Computational Linguistics. http://www.lsi.upc.es/ 
~nlp/iwcl02. 
Breidt E., Segond F., Valetto G. 1996. Local grammars 
for the description of multi-word lexemes and their 
automatic recognition in texts. Proceedings of 
COMPLEX'96, 19-28. Budapest. 
Carmona J., Cervell S., M?rquez L., Mart? M.A., Padr? 
L., Placer R., Rodr?guez H., Taul? M., Turmo J. 
1998. An environment for morphosyntactic 
processing of unrestricted Spanish text. Proceedings 
of LREC'98. 915-922. 
Chinchor N. 1997. MUC-7 Named Entity Task 
Definition. Version 3.5. http://www.itl.nist.gov/iaui/ 
894.02/related_projects/muc/  
Collins M. 2002. Ranking Algorithms for Named-Entity 
Extraction: Boosting and the Voted Percetron. 
Proceedings of ACL-2002. 
Collins M., Singer Y. 1999. Unsupervised Models for 
Named Entity Classification. Proceedings of the 
Joint Conference on Empirical Methods in Natural 
Language Processing and Workshop on Very Large 
Corpora (EMNLP-VLC-99). 
Copestake A., Lambean F., Villavicencio A., Bond F., 
Baldwin T., Sag I., Flickinger D. 2002. Multiword 
Expressions: linguistic precision and reusability. 
Proceedings of the Third International Conference 
on Language Resources and Evaluation (LREC 
2002), Las Palmas, pp. 1941-7. 
Ezeiza N. 2003. Corpusak ustiatzeko tresna 
linguistikoak. Euskararen etiketatzaile sintaktiko 
sendo eta malgua. PhD thesis, University of the 
Basque Country. 
Ezeiza N., Aduriz I., Alegria I., Arriola J.M., Urizar R.  
1998. Combining Stochastic and Rule-Based 
Methods for Disambiguation in Agglutinative 
Languages. COLING-ACL'98, Montreal (Canada). 
Fontenelle T., Adriaens G., De Braekeleer G. 1994. The 
Lexical Unit in the Metal? MT System, MT. The 
Netherlands. v9. 1-19. 
Jackendoff R. 1997. The Architecture of the Language 
Faculty. Cambridge, MA MIT Press. 
Karlsson F., Voutilainen A., Heikkila J,. Anttila A. 
1995. Constraint Grammar: A Language-
independent System for Parsing Unrestricted Text. 
Mouton de Gruyter. 
Karttunen L., Chanod J-P., Grefenstette G., Schiller A. 
1997. Regular expressions for language engineering. 
Natural Language Engineering, Cambridge 
University Press. 
Leech G., Garside R., Bryan M. 1994. CLAWS4: The 
tagging of the British National Corpus. Proceedings 
of COLING-94, 622-628. 
Magnini B., Negri M., Prevete R., Tanev H. 2002. A 
WordNet Approach to Named Entities Recognition. 
Proceeding of the Workshop SemaNet'02: Binding 
and Using Semantic Networks. 
Silva J., Dias G., Guillor? S., Lopes G. 1999. Using 
localmaxs algorithm for the extraction of contiguous 
and non-contiguous multiword lexical units. 
Proceedings of 9th Portuguese Conference in 
Artificial Inteligence, 21-24. 
Smadja F. 1993. Retrieving Collocations from Text: 
Xtract. Computational Linguistics, 19(1), 143-177. 
Named Entities Translation Based on Comparable Corpora
In?aki Alegria
IXA NLP Group
EHU
Donostia, Basque Country
i.alegria@ehu.es
Nerea Ezeiza
IXA NLP Group
EHU
Donostia, Basque Country
n.ezeiza@ehu.es
Izaskun Fernandez
IXA NLP Group
EHU
Donostia, Basque Country
acbfegoi@si.ehu.es
Abstract
In this paper we present a system for
translating named entities from Basque
to Spanish based on comparable corpora.
For that purpose we have tried two ap-
proaches: one based on Basque linguis-
tic features, and a language-independent
tool. For both tools we have used Basque-
Spanish comparable corpora, a bilingual
dictionary and the web as resources.
1 Introduction
Person, location and organization names, main
types of named entities (NEs), are expressions
commonly used in all kinds of written texts. Re-
cently, these expressions have become indispens-
able units of information for many applications
in the area of information extraction as well as
for many searching engines. A lot of tools that
deal with the identification and classification of
named entities for a specific language have been
presented (CoNLL1). But there are few researches
for translation of NEs.
Our main goal is to get a multilingual NE data-
base, which can be very useful for translation
systems, multilingual information extraction tools
(i.e. Question Answering) or many multilingual
systems in general. As getting that multilingual
source is a complex task, we have started design-
ing a system for translating named entities from
Basque to Spanish based on comparable corpora.
Looking at the works published on NE trans-
lation, we can distinguish 3 types of systems: the
systems more often used are the ones based on par-
allel corpora; then the ones based on comparable
1http://www.cnts.ua.ac.be/conll2003/ner/
corpora; and finally the ones that only use the web
as an open corpus.
As we have mentioned before, most of the re-
lated works use parallel corpora. However and as
it is widely known, obtaining parallel corpus is not
an easy task, and it becomes harder when one of
the languages in the pair is a minority language,
as is the case of Basque. We can avoid working
with parallel corpora using comparable corpora.
Comparable corpora are those data sets which are
written in different languages, treat similar sub-
jects and are written in a similar style, but are not
necessarily texts? translations. Obtaining that kind
of corpora is much easier than obtaining parallel
one, although sometimes it is not possible to get
neither of them. In this case, we can use the web
as a multilingual corpus, in order to search it for
any possible entity translation.
We have a comparable data set available for
Basque and Spanish. But besides using that data
source, we decided also to resort to the web as a
complementary data set too, as in (Moore, 2003).
Apart from these two data sets, we have also
used some other information sources to develop
the Basque-Spanish bilingual NE translation sys-
tem. We have carried out two main different ex-
periments: one using a language-dependent gram-
mar, implementing transliteration transformations
(Al-Onaizan et al, 2002b) and rules related to
elements? order; and another one based on the
edition distance (Kukich, 1992) grammar, sim-
ulating simple cognates and transliteration trans-
formations, but in a language-independent way.
In both experiments, we have used a Basque-
Spanish bilingual dictionary for the words in
which transliteration transformations were not
enough to obtain the correct translated form.
Furthermore, we have always worked using
1
Basque as source language, and Spanish as target
language.
Since Basque and Spanish do not follow the
same syntactic pattern, entity elements may occur
in different positions in both languages. That is
why the elements need to be arranged when trans-
lating Basque entities into Spanish.
The paper is structured as follows. Section 2
presents the related works. Section 3 presents
the experimental settings. In section 4 we de-
scribe the development of NE translation system
explaining both possible systems, the language-
dependent system and the language-independent
one, and the system that combines both language-
dependent and independent sources. In section 5,
we present the results of the experiments, and fi-
nally, section 6 presents some conclusions and fu-
ture works.
2 Related Works
Despite the difficulty of getting bilingual parallel
corpus, most of the NE translation researches car-
ried out work with parallel data-sets. Furthermore,
those bilingual corpora are used to be aligned at
paragraph or even at phrase level. For example,
Moore?s work (Moore, 2003) uses a bilingual
parallel aligned English-French corpora, and ap-
plying different statistical techniques, he obtains a
French form for each English entity.
Although it has been less experimented with
comparable corpora there are some known sys-
tems designed to work with them as well. Most
of them deal with language pairs that have dif-
ferent kinds of alphabets. For instance, the
Chinese-English translation tool presented in ACL
2003 (Chen et al, 2003), or the one published
in the ACL 2002 edition for translating entity
names from Arabic to English (Al-Onaizan et
al., 2002a). The main goal of both systems is to
obtain the corresponding form for English, tak-
ing Chinese and Arabic respectively as source lan-
guages. Two kinds of translations can be distin-
guished in both systems: direct/simple translations
and transliterations (Al-Onaizan et al, 2002b).
However, the techniques used by each tool for
both kinds of translations are different. Frequency
based methods are used in Chinese-English trans-
lations, while in the Arabic-English language pair,
a more complex process is applied, which involves
the combination of different kinds of techniques.
In this paper, we present the research carried
out for translating entity names from Basque into
Spanish. For the first step, we have based on the
system presented by Y. Al Onaizan and K. Knight
in ACL 2002. With this system, they first obtain
a candidate translation list for the entity in the tar-
get language, using both monolingual and bilin-
gual resources. Once they have this list, they build
a ranking with candidates applying different meth-
ods (such as statistical measures, web-counting,
etc.). Finally, if they consider that the correct
translation does not appear in the list, they extract
an extended list version using the web and they
apply again the ranking step.
3 Experimental settings
We have obtained a Basque-Spanish comparable
corpora processing news from two newspapers,
one for each language: Euskaldunon Egunkaria,
the only newspaper written entirely in Basque for
Basque texts, and EFE for Spanish texts. We have
collected the articles written in the 2002 year in
both newspapers and we have obtained 40,648 ar-
ticles with 9,655,559 words for Basque and 16,914
with 5,192,567 words for Spanish. Both newspa-
pers deal with similar topics: international news,
sports, politics, economy, culture, local issues and
opinion articles, but with different scope.
In order to extract Basque NEs, we have used
Eihera (Alegria et al, 2003), a Basque NE rec-
ognizer developed in the IXA Group. Giving a
written text in Basque as input, this tool applies
a grammar based on linguistic features in order
to identify the entities in the text. For the clas-
sification of the identified expressions, we use a
heuristic that combines both internal and external
evidence. We labeled this corpus for the HER-
MES project2(news databases: cross-lingual infor-
mation retrieval and semantic extraction). Thus,
we obtained automatically 142,464 different per-
son, location and organization names.
Since we have participated at the HERMES
project, we have available labeled corpora for the
other languages processed by other participants. It
was the TALP3 research group the one that was in
charge of labeling EFE 2002 newspaper?s articles
for the Spanish version, in which 106,473 differ-
ent named entities were dealt with. We have built
the comparable corpus using this data-set together
with the Basque set mentioned above.
2http://nlp.uned.es/hermes/
3http://www.lsi.upc.edu/ nlp/web/
2
Being Basque an agglutinative language, entity
elements may contain more than just lexical infor-
mation. So before doing any translation attempt
a morphosyntactic analysis is required in order to
obtain all the information from each element. Fur-
thermore, Eihera works on a lemmatized text, so
lematizing the input text is a strong requirement.
For that purpose, we apply the lemmatizer/tagger
for Basque (Alegria et al, 1998) developed by the
IXA group.
The goal of our system is to translate Basque
person, location and organization names into
Spanish entities. These two languages share a
lot of cognates, that is, words that are similar in
both languages and only have small, usually pre-
dictable spelling differences. Two experts have re-
viewed an extended list of word pairs4 extracted
from EDBL (Basque Lexical Data-base) in order
to detect these differences. All the observed varia-
tions have been listed in a spelling-rule list. These
rules are in fact the ones that will be applied for the
translation of some of the words, but obviously not
for all.
When translating Basque words into Spanish,
usually the correct form is not obtained by ap-
plying the rules mentioned before, and a different
strategy is required. For these words in particu-
lar, we have used bilingual dictionaries as in Al-
Onaizan and Knight?s work.
We have used the Elhuyar 2000 bilingual dic-
tionary, one of the most popular for that language
pair. This dictionary has 74,331 Basque entries,
and it contains the corresponding Spanish syn-
onyms.
For the evaluation, we have used a set of 180
named entity-pairs. We have borrowed that set
from the Euskaldunon Egunkaria 2002 newspaper.
Concretely we applied Eihera, the Basque NE rec-
ognizer, to extract all the named entities in the cor-
pus. Then we estimated the normalized frequency
of each entity in the corpus, and we selected the
most common ones. Finally we translated them
manually into Spanish.
In order to carry out an evaluation starting from
correct Basque NEs, although the NEs were au-
tomatically extracted from the corpus, we verified
that all the entities were correctly identified. Be-
cause if the original entity was not a correct ex-
pression, the translation system could not get a
4One expert has revised adjective and nouns in general,
while the other one has only treated proper noun pairs
correct translation.
4 Systems? Development
As we have mentioned before, we have done two
different experiments in order to get a Basque-
Spanish NE translation tool. For both trials we
have used bilingual dictionaries and grammars to
translate and transliterate entity elements, respec-
tively. But the methodologies used to implement
each transliteration grammar are different: on the
one hand, we have used Basque linguistic knowl-
edge to develop the grammar; on the other hand,
we have defined a language-independent grammar
based on edition distance.
Those dictionaries and grammars have been
used in order to obtain translation proposals for
each entity element. But another methodology is
needed for the system to propose the translation of
whole entities. For the system based on linguistic
information, a specific arranging rule set has been
applied getting a candidate list. In order to decide
which is the most suitable one, we have created a
ranked list based on a simple web count.
For the language-independent system a more
simple methodology has been applied. We have
generated all the possible candidate combinations,
considering that every element can appear at any
position in the entity. Then, a comparable corpus
has been used in order to decide which is the most
probable candidate.
Now we will present the design of each experi-
ment in detail.
4.1 Linguistic Tool
We can see the pseudo-code of the linguistic tool
at Figure 1.
Figure 1: Linguistic Tool
The linguistic tool, first tries to obtain a transla-
tion proposal for each entity element using bilin-
gual dictionaries. If no candidate is obtained from
3
that search, the transliteration grammar is applied.
Once the system has obtained at least one proposal
for each element, the arranging grammar is ap-
plied, and finally, the resultant entire entity pro-
posals are ranked based on their occurrence on the
web.
4.1.1 Transliteration
Reviewing the extended list of words from
EDBL (a Basque Lexical Data-base) we have ob-
tained 24 common phonologic/spelling transfor-
mations, some of which depend on others, and can
usually be used together, although not always. We
have implemented these 24 transformations using
the XFST (Beesley and Karttunen, 2003) tool and
we have defined 30 rules. These rules have been
ordered in such a way that rules with possible in-
teractions are firstly applied and then the rest of
them. This way we have avoided interaction prob-
lems.
For instance, lets say that we want to translate
Kolonbia into Colombia and that our grammar has
the following two simple transformation rules: nb
? mb and b ? v. If we apply the first rule and
then the second one, the candidate we will obtain
is Colomvia, and this is not the correct translation.
However, if we do not allow to apply the second
rule after the nb ? mb transformation, the gram-
mar will propose the following candidates: Colon-
via and Colombia. So it would generate bad forms
but the correct forms too.
We can conclude from this fact that it is neces-
sary to apply the rules in a given order.
The possible combinations of rules are so wide
that it causes an overgeneration of candidates. To
avoid working with such a big number of can-
didates in the following steps, we have decided
to rank and select candidates using some kind of
measure.
We have estimated rules probabilities using the
bilingual dictionary Elhuyar 2000. We have sim-
ply apply all possible rule combinations on every
Basque word in the dictionary, and measured the
normalized frequency of each rule and each rule
pair. Thus, translation proposals are attached a
probability based on the probability of a rule be-
ing applied, and only the most probable ones are
proposed for the following steps.
4.1.2 Entire Entity Construction
At this point, we have N translation candidates
for each input entity element at the most, and they
have been obtained applying the grammar or from
the dictionary search. Our next goal is to create
entire entity translation proposals combining all
these candidates. But some words features, such
as gender and number, must be considered and
treated beforehand.
The number of an entity element will be re-
flected in the whole entity. Let?s say, for instance,
translate the organization name Nazio Batuak5.
The translation proposals from the previous mod-
ules for these two words are Nacio?n (for Nazio)
and Unida (for Batuak). If we do not consider that
the corresponding Basque word of the Unida ele-
ment is in the plural form, then the whole transla-
tion candidate will not be correct. In this case, we
will need to pluralize the corresponding Spanish
words.
Unlike Spanish, Basque has no morphological
gender. This means that for some Basque words
the generation of both male and female form is re-
quired. The word idazkari, for example, has no
morphological gender, and it has two correspond-
ing Spanish words: the masculine secretario and
the feminine secretaria. If we search for idazkari
on the bilingual dictionary, we will only obtain the
masculine form, but the feminine form is needed
for some entities , as it is the case with Janet Reno
Idazkaria6 . Since Janet Reno is a woman?s proper
name, the correct translation of Idazkaria would
be Secretaria. So before constructing the entire
entity translation, both male and female forms
have been generated for each element.
The simplest entities to construct are the ones
whose elements keep the same order in both the
Basque and the Spanish forms. Person names usu-
ally follow this pattern.
However, there are some translations that are
not as regular and easy to translate as the pre-
vious ones. Suppose that we want to translate
the Basque entity Lomeko Bake Akordio7 into the
Spanish form Acuerdo de Paz de Lome. After ap-
plying grammar and bilingual dictionaries, we ob-
tain the following translated elements (in order to
simplify the explanation, we have assumed that the
system will only return one translation candidate
per element): Lome Acuerdo and Paz. As you can
see, if we do not arrange those elements, the pro-
posal will not be the appropriate Spanish transla-
5United Nations
6Secretary Janet Reno
7Lome Peace Agreement
4
tion.
An expert?s manual work has been carried out in
order to define the element arranging needed when
turning from one language to the other. The mor-
phosyntactic information of the Basque entity el-
ements (such as PoS, declension, and so on) has
been used in this task.
Using this manual work, we have defined 10
element-arranging rules using the XFST tool. In
the example above, it is clear that some element-
arranging rules are needed in order to obtain the
correct translation. Let?s see how our grammar?s
rules arranges those elements.
When the system starts arranging the Lome
Acuerdo and Paz Spanish words to get the correct
translation for the Basque named entity Lomeko
Bake Akordio it starts from the right to the left us-
ing the Basque elements? morphosyntactic infor-
mation. So it will start arranging the translated
elements for Bake Akordio from right to left. Both
forms are common nouns with no declension case.
Looking at the grammar the system will find a rule
for this structure that switches position of the el-
ements and inserts the preposition de in between.
So the partial translation would be Acuerdo de Paz.
The next step is to find the correct position for the
translation of Lomeko, which is a location name
declined in genitive. There is a rule in the gram-
mar, that places the elements declined in genitive
at the end of the partial entity and adds the preposi-
tion de before this element. So, the system will ap-
ply that rule, obtaining the Spanish translation of
the whole entity Acuerdo de Paz de Lome, which
is the correct form.
4.1.3 Web Search
As we have explained, we combine at the most
the N translation candidates per entity elements
with each other using the corresponding arrang-
ing rule to get the translation of the whole entity.
So, at the most we will obtain NxN entity transla-
tion proposals. In order to know which candidate
is the correct one, the tool makes a web search, but
as the number of candidates is so high, we use the
same candidate selection technique applied previ-
ously for element selection.
This time we will use elements probability in or-
der to obtain a measured proposal list. The x can-
didates with the highest probability are searched
and ranked in a final candidate list of translated
entities.
In our experiments, we have used the Google
API to consult the web. Searching entities in
Google has the advantage of getting the most com-
mon forms for entities in any type of document.
But if you prefer to get a higher precision (rather
than a good recall), you can obtain a higher cer-
tainty rate by making a specialized search in the
web. For those specialized searches we have used
Wikipedia, a free encyclopedia written collabora-
tively by many of its readers in many languages.
4.2 Language Independent Tool
Since creating transformation rules for every lan-
guage pairs is not always a viable task, we have de-
signed a general transformation grammar, which
fits well for most language pairs that use the same
alphabetical system. All we need is a written cor-
pus for each language and a bilingual dictionary.
Figure 2: Language Independent Tool
We have constructed a NE translation tool based
on comparable corpora using that general gram-
mar. As you can see in Figure 2, the system
finds Basque translation proposals for entity ele-
ments applying the pseudo-transliteration module.
Once it gets at least one translation candidate for
each element, it applies the whole entity construc-
tion module obtaining all the possible whole entity
candidates. Finally, it searches each candidate in
the corresponding comparable corpus and returns
a ranked candidate list based on that search, in or-
der to obtain the correct translation form.
4.2.1 Pseudo-transliteration module
The pseudo-transliteration module has two
main sources: an edition distance (Kukich, 1992)
grammar and a Spanish lexicon.
The edition distance grammar is composed of
three main rules:
1. a character can be replaced in a word
2. a character can disappear from a word
3. a new character can be inserted in a word
5
There is no specific rule in the grammar for
switching adjacent characters, because we can
simulate that transformation just combining the
deleting and inserting rules mentioned above.
Since each rule can be applied n times for each
word, the set of all translated words that we ob-
tain, applying rules independently and combining
them, is too extent.
In order to reduce the output proposal-set, we
have combined the grammar with a Spanish lex-
icon, and we have restricted the transformation
rules to two applications. So words with more than
two transformations have been avoided. Thus,
when the system applies the resultant automa-
ton of this combination, only the Spanish words
that can be obtained with a maximum of two
transformations would be proposed as pseudo-
transliterations of a Basque entity element.
The Spanish lexicon has been constructed with
all the words of EFE 2002 (the Spanish corpus of
the 2002 year) and the bilingual dictionary Elhu-
yar 2000. And as we have considered this cor-
pus as a comparable corpus with regard to the Eu-
skaldunon Egunkaria 2002, Basque corpus ver-
sion, we assume that most of the Basque words
would have their corresponding translation in the
Spanish set.
However, there are some words that do not
have their corresponding translation at EFE 2002,
or their translation cannot be obtained applying
only two transformations. In order to obtain
their translations in a different way, we have used
the Basque-Spanish Elhuyar 2000 bilingual dic-
tionary. To be precise, we have converted the
bilingual dictionary into an automaton, and we
combined it with the resultant automaton obtained
from applying the transliteration grammar in the
Spanish lexicon.
In this way the system is able to translate not
only the transliterated words in EFE 2002 corpus,
but also, the words that cannot be translated us-
ing transformation knowledge and that need infor-
mation from a bilingual dictionary, such as ?Er-
akunde? vs. ?Organizacio?n?8 .
4.2.2 Entire Entity Construction
Since we want to build a language independent
system that works just having two different lan-
guage data-sets, we cannot use any linguistic fea-
ture for arranging entity elements and getting the
8Organization
correct whole translated entity.
We might use many approaches to arrange ele-
ments, but we have chosen the simplest one: com-
bining each proposed element with the rest, con-
sidering that each proposal can appear in any po-
sition within the entity. Thus, the system will re-
turn a large list of candidates, but we have ensured
that it will include the correct one, when the in-
dependent translation of all the elements has been
correctly done.
Although in some cases prepositions and arti-
cles are needed to obtain the correct Spanish form,
the translation candidates for the whole entity will
not contain any element apart from the translated
words of the original entity. So, in the following
step the lack of these elements will be taken into
account.
4.2.3 Comparable Corpus Search
Once the system has calculated all possible
translation candidates for the whole entity , the
following step is to select the most suitable pro-
posal. For that purpose, we have used the web in
the linguistic tool. But this time, we have made
used of the data-set in the Spanish-news articles,
in which entities were tagged. This set is smaller
and permits faster searching; furthermore, since
Basque and Spanish-sets are comparable, the cor-
rect translation form is expected to occur in this
smaller corpus, so it is very probable that the sys-
tem will propose us the right translation.
Therefore, every translation proposal will be
searched in the Spanish data-set and will be po-
sitioned at the ranked list according to their fre-
quency. Thus, the most repeated entities in the
corpus would appear on the top of the list.
4.2.4 Combining web and comparable corpus
rankings
Both Euskaldunon Egunkaria 2002 and EFE
2002 data-sets are 2002 year news-sets, and a lot
of named entities are due to occur in both sets. But
since they are articles taken from newspaper of
different countries, there may be some non-shared
named entities.
When the system finds these special entities in
the Spanish comparable corpus, it is very probable
that it will find none of the candidates, and so, the
list will not be arranged.
To avoid that random list ranking, when all
translation candidates have a very low frequency,
we propose to use the web to do a better rank-
6
ing. As we will present below, this optional second
ranking step improves final results.
5 Experiments
As we have mentioned before, we have first ex-
tracted a set of 180 person, location and organi-
zation name-pairs from Euskaldunon Egunkaria
2002 newspaper and then we have translated them
manually.
We have used three evaluation measures to
present the result of all the experiments:
? Precision = correctly translated NEsTranslated NEs
? Recall = correctly translated NEsAll NEs
? F ? score = 2?Precision?RecallPrecision+Recall
For the evaluation of the linguistic tool, we have
used a parameter (x in the tables) which deter-
mines how many translation candidates will be
used in each module at the most. This threshold is
necessary since the output of both transliteration
and arranging grammar is too big to work with in
the next modules.
The fr-min parameter in the tables specifies how
often a candidate must occur in a data-set to be
considered a likely NE translation proposal.
fr. min ? x Precision Recall F-score
10 ? 1 73.96% 69.44% 71.63%
100 ? 1 75.75% 69.44% 72.25%
250 ? 1 78.71% 67.77% 72.83%
500 ? 1 79.86% 61.66% 69.59%
10 ? 3 79.29% 74.44% 76.79%
100 ? 3 80.6% 73.88% 77.09%
250 ? 3 83.87% 72.22% 77.61%
500 ? 3 83.45% 64.4% 72.7%
10 ? 10 79.88% 75% 77.36%
100 ? 10 81.21% 74.44% 77.68%
250 ? 10 84.52% 72.78% 78.21%
500 ? 10 84.17% 65% 73.35%
Table 1: Linguistic knowledge + Google
Table 1 presents the results obtained applying
the linguistic tool, and searching its proposals in
Google. If we observe these results taking into
account the values of the x parameter, it seems
that the bigger the x value is, the better results we
get. But note that the best improvement is obtained
when we use the maximum of 3 candidate instead
of using just 1. We improved the system perfor-
mance in 5%. While using 10 candidates, the per-
formance increases in less than 1% compared to
the results obtained when x value is 3.
Regarding to the fr-min parameter, it seems that
the best value is around 250. Moreover, duplicat-
ing this value, performance decreases. So we can
say that when fr-min value exceeds 250, the sys-
tem performs worse.
For next comparatives, we will take the re-
sults given by the experiments using the values fr-
min=250 and x=1 as reference.
When we search Wikipedia instead of Google
(see Table 2), the system?s recall decreases from
69.44% to 66.67%. This time the only search-
ing restriction is that the candidate occurs at least
once, and not n times. This is because the data-set
offered by Wikipedia is significantly smaller than
the one given by Google. Moreover, precision re-
mains similar. So although it is a smaller data-set,
Wikipedia seems to be similar to Google as far as
the information significance of terms is concerned.
fr. min ? x Precision Recall F-score
1 ? 1 81.63% 66.67% 73.4%
1 ? 3 83.67% 68.33% 75.23%
1 ? 10 84.35% 68.88% 75.83%
Table 2: Linguistic knowledge + Wikipedia
When we use the comparable corpus instead of
the web, the linguistic tool performs a consider-
able enhancement in precision, a 13% improve-
ment, but gets worse coverage. On the other hand,
the language-independent tool achieves similar re-
sults with regard to the linguistic tool searching
in the web. So the language-independent tool
seems to be a good alternative for dealing with NE
translation without no exhaustive linguistic work.
Those results are detailed in Table 3.
System Precision Recall F-score
Ling. Tool 91.85% 68.8% 78.67%
Lang. Indep. 83.3% 72.2% 77.35%
Table 3: Results using comparable corpus
Finally, we have tried searching the proposals
from the linguistic tool first in the comparable
corpus. When no successful candidate is found
in it, the system tries searching the web, in both
Google and Wikipedia (See Table 4). In both ex-
periments, precision is significantly lower than the
one obtained when the system proposes candidates
found in the comparable corpus, without no fur-
ther search. However, the coverage increases in al-
most 5% in the trials carried out both with Google
and Wikipedia. Therefore, the system?s F-score
7
remains similar. Note that this time instead of per-
forming better when Google is used, the searches
done in Wikipedia give better results. Further-
more, the best results are obtained when combin-
ing comparable corpus and Wikipedia searches in
the Linguistic tool.
Web search Precision Recall F-score
Google, 250 81.36% 73.3% 77.12%
Wikipedia, 1 84.21% 73.3% 78.38%
Table 4: Ling. Tool + Comp. corpus + Web search
6 Conclusions and Further Works
We have presented an approach for the design and
development of an entity translation system from
Basque to Spanish and the different techniques
and resources we have used for this work.
On the one hand, we have combined bilingual
dictionaries with a phonologic/spelling grammar
for the entity elements? translation; on the other
hand, we have applied a language-independent
grammar based on edition distance. Both com-
binations perform well, and although the lin-
guistic tool obtains better results, the language-
independent grammar may be very useful for other
experiments carried out with language-pairs others
than Basque and Spanish.
Because of the differences of the syntactical
structures of Basque and Spanish, it is necessary to
arrange the entity elements for the correct transla-
tion of whole NEs; in particular, for those entities
with more than one element. For that purpose, we
have used two different techniques: probabilistic
rules and a simple combination method (all candi-
dates combined with all).
Finally, we have applied different resources and
techniques for the selection of the best candidates.
On the one hand, we have tried searching the web
(Google and Wikipedia); on the other hand, we
have used a comparable Basque-Spanish corpus.
We have verified, that although Google is a bigger
data-set, the significance of the information for NE
translation task is similar to the information given
by Wikipedia.
All the experiments carried out with compara-
ble corpus have performed very well, and the best
results have been obtained when combining it with
Wikipedia. So developing a NE translation system
based on comparable information have proved to
be a good way to build a robust system.
However, some modules can be improved.
Firstly, the methods to rank and select candidates
are very simple, so if we use more complex ones,
the number of candidates for the following mod-
ules would decrease considerably, and so, the sys-
tem?s final selection would be easier and more pre-
cise.
Regarding to the use of the web, actually we
have only used Google and Wikipedia. Searches
in Wikipedia are more precise than the ones made
in Google and so the information they offer can
be considered complementary. Furthermore, we
can obtain very valuable information for other en-
tity processes. For instance, since Wikipedia is
a topic-classified encyclopedia, when you do an
entity search, you can get information about the
kind of documents in which the entity can occur;
in other words, which is the most usual topic for
it to occur in. Besides, that classification category
can be very useful for entity disambiguation too.
With all the improvements presented so far, we
hope to get a stronger entity name translation sys-
tem in the future.
References
Aduriz I., Alegria I., Arriola J.M., Ezeiza N., Urizar R.
1998. Combining Stochastic and Rule-Based Meth-
ods for Disambiguation in Agglutinative Languages.
Proceedings of COLING-ACL?98.
Alegria I., Balza I., Ezeiza N., Fernandez I., Urizar R.
2003. Named Entity Recognition and Classification
for texts in Basque. Proceedings of JOTRI II.
Al-Onaizan Y., Knight K. 2002. Translating
Named Entities Using Monolingual and Bilingual
Resources. Proceedings of ACL 2002.
Al-Onaizan Y., Knight K. 2002. Machine Translitera-
tion of Names in Arabic Text. Proceedings of ACL
2002.
Beesley K.R., Karttunen L. 2003. Finite State Mor-
phology. CSLI
Chen H., Yang C., Lin Y. 2003. Learning Formulation
and Transformation Rules for Multilingual Named
Entities. Proceedings of the ACL 2003 Workshop
on Multilingual and Mixed-language Named Entity
Recognition.
Kukich K., 1992. Techniques for automatically cor-
recting word in text. ACM Computing Surveys Vol.
24 No. 4 377-439
Moore R. C., 2003. Learning Translations of Named-
Entity Phrases from Parallel Corpora. Proceedings
of EACL 2003.
8
A Multiclassifier based Document Categorization System: profiting from
the Singular Value Decomposition Dimensionality Reduction Technique
Ana Zelaia
UPV-EHU
Basque Country
ccpzejaa@si.ehu.es
In?aki Alegria
UPV-EHU
Basque Country
acpalloi@si.ehu.es
Olatz Arregi
UPV-EHU
Basque Country
acparuro@si.ehu.es
Basilio Sierra
UPV-EHU
Basque Country
ccpsiarb@si.ehu.es
Abstract
In this paper we present a multiclassifier
approach for multilabel document classifi-
cation problems, where a set of k-NN clas-
sifiers is used to predict the category of
text documents based on different training
subsampling databases. These databases
are obtained from the original training
database by random subsampling. In or-
der to combine the predictions generated
by the multiclassifier, Bayesian voting is
applied. Through all the classification pro-
cess, a reduced dimension vector represen-
tation obtained by Singular Value Decom-
position (SVD) is used for training and
testing documents. The good results of our
experiments give an indication of the po-
tentiality of the proposed approach.
1 Introduction
Document Categorization, the assignment of nat-
ural language texts to one or more predefined
categories based on their content, is an impor-
tant component in many information organization
and management tasks. Researchers have con-
centrated their efforts in finding the appropriate
way to represent documents, index them and con-
struct classifiers to assign the correct categories to
each document. Both, document representation
and classification method are crucial steps in the
categorization process.
In this paper we concentrate on both issues. On
the one hand, we use Latent Semantic Indexing
(LSI) (Deerwester et al, 1990), which is a vari-
ant of the vector space model (VSM) (Salton and
McGill, 1983), in order to obtain the vector rep-
resentation of documents. This technique com-
presses vectors representing documents into vec-
tors of a lower-dimensional space. LSI, which
is based on Singular Value Decomposition (SVD)
of matrices, has showed to have the ability to ex-
tract the relations among words and documents by
means of their context of use, and has been suc-
cessfully applied to Information Retrieval tasks.
On the other hand, we construct a multiclassi-
fier (Ho et al, 1994) which uses different train-
ing databases. These databases are obtained from
the original training set by random subsampling.
We implement this approach by bagging, and use
the k-NN classification algorithm to make the cat-
egory predictions for testing documents. Finally,
we combine all predictions made for a given doc-
ument by Bayesian voting.
The experiment we present has been evaluated
for Reuters-21578 standard document collection.
Reuters-21578 is a multilabel document collec-
tion, which means that categories are not mutu-
ally exclusive because the same document may be
relevant to more than one category. Being aware
of the results published in the most recent litera-
ture, and having obtained good results in our ex-
periments, we consider the categorization method
presented in this paper an interesting contribution
for text categorization tasks.
The remainder of this paper is organized as fol-
lows: Section 2, discusses related work on docu-
ment categorization for Reuters-21578 collection.
In Section 3, we present our approach to deal with
the multilabel text categorization task. In Section
4 the experimental setup is introduced, and details
about the Reuters database, the preprocessing ap-
plied and some parameter setting are provided. In
Section 5, experimental results are presented and
discussed. Finally, Section 6 contains some con-
clusions and comments on future work.
25
2 Related Work
As previously mentioned in the introduction, text
categorization consists in assigning predefined
categories to text documents. In the past two
decades, document categorization has received
much attention and a considerable number of ma-
chine learning based approaches have been pro-
posed. A good tutorial on the state-of-the-art of
document categorization techniques can be found
in (Sebastiani, 2002).
In the document categorization task we can find
two cases; (1) the multilabel case, which means
that categories are not mutually exclusive, because
the same document may be relevant to more than
one category (1 to m category labels may be as-
signed to the same document, being m the to-
tal number of predefined categories), and (2) the
single-label case, where exactly one category is
assigned to each document. While most machine
learning systems are designated to handle multi-
class data1, much less common are systems that
can handle multilabel data.
For experimentation purposes, there are stan-
dard document collections available in the pub-
lic domain that can be used for document catego-
rization. The most widely used is Reuters-21578
collection, which is a multiclass (135 categories)
and multilabel (the mean number of categories as-
signed to a document is 1.2) dataset. Many ex-
periments have been carried out for the Reuters
collection. However, they have been performed in
different experimental conditions. This makes re-
sults difficult to compare among them. In fact, ef-
fectiveness results can only be compared between
studies that use the same training and testing sets.
In order to lead researchers to use the same train-
ing/testing divisions, the Reuters documents have
been specifically tagged, and researchers are en-
couraged to use one of those divisions. In our
experiment we use the ?ModApte? split (Lewis,
2004).
In this section, we analize the category sub-
sets, evaluation measures and results obtained in
the past and in the recent years for Reuters-21578
ModApte split.
2.1 Category subsets
Concerning the evaluation of the classification
system, we restrict our attention to the TOPICS
1Categorization problems where there are more than two
possible categories.
group of categories that labels Reuters dataset,
which contains 135 categories. However, many
categories appear in no document and conse-
quently, and because inductive based learning
classifiers learn from training examples, these cat-
egories are not usually considered at evaluation
time. The most widely used subsets are the fol-
lowing:
? Top-10: It is the set of the 10 categories
which have the highest number of documents
in the training set.
? R(90): It is the set of 90 categories which
have at least one document in the training set
and one in the testing set.
? R(115): It is the set of 115 categories which
have at least one document in the training set.
In order to analyze the relative hardness of the
three category subsets, a very recent paper has
been published by Debole and Sebastiani (Debole
and Sebastiani, 2005) where a systematic, compar-
ative experimental study has been carried out.
The results of the classification system we pro-
pose are evaluated according to these three cate-
gory subsets.
2.2 Evaluation measures
The evaluation of a text categorization system is
usually done experimentally, by measuring the ef-
fectiveness, i.e. average correctness of the catego-
rization. In binary text categorization, two known
statistics are widely used to measure this effective-
ness: precision and recall. Precision (Prec) is the
percentage of documents correctly classified into a
given category, and recall (Rec) is the percentage
of documents belonging to a given category that
are indeed classified into it.
In general, there is a trade-off between preci-
sion and recall. Thus, a classifier is usually evalu-
ated by means of a measure which combines pre-
cision and recall. Various such measures have
been proposed. The breakeven point, the value at
which precision equals recall, has been frequently
used during the past decade. However, it has
been recently criticized by its proposer ((Sebas-
tiani, 2002) footnote 19). Nowadays, the F1 score
is more frequently used. The F1 score combines
recall and precision with an equal weight in the
following way:
F1 =
2 ? Prec ? Rec
Prec + Rec
26
Since precision and recall are defined only for
binary classification tasks, for multiclass problems
results need to be averaged to get a single perfor-
mance value. This will be done using microav-
eraging and macroaveraging. In microaveraging,
which is calculated by globally summing over all
individual cases, categories count proportionally
to the number of their positive testing examples.
In macroaveraging, which is calculated by aver-
aging over the results of the different categories,
all categories count the same. See (Debole and
Sebastiani, 2005; Yang, 1999) for more detailed
explanation of the evaluation measures mentioned
above.
2.3 Comparative Results
Sebastiani (Sebastiani, 2002) presents a table
where lists results of experiments for various train-
ing/testing divisions of Reuters. Although we are
aware that the results listed are microaveraged
breakeven point measures, and consequently, are
not directly comparable to the ones we present in
this paper, F1, we want to remark some of them.
In Table 1 we summarize the best results reported
for the ModApte split listed by Sebastiani.
Results reported by R(90) Top-10
(Joachims, 1998) 86.4
(Dumais et al, 1998) 87.0 92.0
(Weiss et.al., 1999) 87.8
Table 1: Microaveraged breakeven point results
reported by Sebastiani for the Reuters-21578
ModApte split.
In Table 2 we include some more recent re-
sults, evaluated according to the microaveraged
F1 score. For R(115) there is also a good result,
F1 = 87.2, obtained by (Zhang and Oles, 2001)2.
3 Proposed Approach
In this paper we propose a multiclassifier based
document categorization system. Documents in
the training and testing sets are represented in a
reduced dimensional vector space. Different train-
ing databases are generated from the original train-
2Actually, this result is obtained for 118 categories which
correspond to the 115 mentioned before and three more cat-
egories which have testing documents but no training docu-
ment assigned.
Results reported by R(90) Top-10
(Gao et al, 2003) 88.42 93.07
(Kim et al, 2005) 87.11 92.21
(Gliozzo and Strapparava, 2005) 92.80
Table 2: F1 results reported for the Reuters-21578
ModApte split.
ing dataset in order to construct the multiclassifier.
We use the k-NN classification algorithm, which
according to each training database makes a pre-
diction for testing documents. Finally, a Bayesian
voting scheme is used in order to definitively as-
sign category labels to testing documents.
In the rest of this section we make a brief re-
view of the SVD dimensionality reduction tech-
nique, the k-NN algorithm and the combination of
classifiers used.
3.1 The SVD Dimensionality Reduction
Technique
The classical Vector SpaceModel (VSM) has been
successfully employed to represent documents in
text categorization tasks. The newer method of
Latent Semantic Indexing (LSI) 3 (Deerwester et
al., 1990) is a variant of the VSM in which doc-
uments are represented in a lower dimensional
space created from the input training dataset. It
is based on the assumption that there is some
underlying latent semantic structure in the term-
document matrix that is corrupted by the wide va-
riety of words used in documents. This is referred
to as the problem of polysemy and synonymy. The
basic idea is that if two document vectors represent
two very similar topics, many words will co-occur
on them, and they will have very close semantic
structures after dimension reduction.
The SVD technique used by LSI consists in fac-
toring term-document matrix M into the product
of three matrices, M = U?V T where ? is a di-
agonal matrix of singular values in non-increasing
order, and U and V are orthogonal matrices of sin-
gular vectors (term and document vectors, respec-
tively). Matrix M can be approximated by a lower
rank Mp which is calculated by using the p largest
singular values of M . This operation is called
dimensionality reduction, and the p-dimensional
3http://lsi.research.telcordia.com,
http://www.cs.utk.edu/?lsi
27
space to which document vectors are projected is
called the reduced space. Choosing the right di-
mension p is required for successful application
of the LSI/SVD technique. However, since there
is no theoretical optimum value for it, potentially
expensive experimentation may be required to de-
termine it (Berry and Browne, 1999).
For document categorization purposes (Dumais,
2004), the testing document q is also projected to
the p-dimensional space, qp = qTUp??1p , and the
cosine is usually calculated to measure the seman-
tic similarity between training and testing docu-
ment vectors.
In Figure 1 we can see an ilustration of the doc-
ument vector projection. Documents in the train-
ing collection are represented by using the term-
document matrix M , and each one of the docu-
ments is represented by a vector in the Rm vec-
tor space like in the traditional vector space model
(VSM) scheme. Afterwards, the dimension p is se-
lected, and by applying SVD vectors are projected
to the reduced space. Documents in the testing
collection will also be projected to the same re-
duced space.
d1 d2
d3
d4d5
d2
d3
d4
d5d6d7
d9603 d1
d9603
d6d7
...
...
Rm
Reuters-21578, ModApte, Training
VSM
SVD
M
R p
d1 d2 d9603
Mp = Up?pV Tp
Figure 1: Vectors in the VSM are projected to the
reduced space by using SVD.
3.2 The k nearest neighbor classification
algorithm (k-NN)
k-NN is a distance based classification approach.
According to this approach, given an arbitrary test-
ing document, the k-NN classifier ranks its near-
est neighbors among the training documents, and
uses the categories of the k top-ranking neighbors
to predict the categories of the testing document
(Dasarathy, 1991). In this paper, the training and
testing documents are represented as reduced di-
mensional vectors in the lower dimensional space,
and in order to find the nearest neighbors of a
given document, we calculate the cosine similar-
ity measure.
In Figure 2 an ilustration of this phase can be
seen, where some training documents and a test-
ing document q are projected in the R p reduced
space. The nearest to the qp testing document are
considered to be the vectors which have the small-
est angle with qp. According to the category labels
of the nearest documents, a category label predic-
tion, c, will be made for testing document q.
d34
d61
d23
d135
d509
k?NN
R p
c
qp
Figure 2: The k-NN classifier is applied to qp test-
ing document and c category label is predicted.
We have decided to use the k-NN classifier be-
cause it has been found that on the Reuters-21578
database it performs best among the conventional
methods (Joachims, 1998; Yang, 1999) and be-
cause we have obtained good results in our pre-
vious work on text categorization for documents
written in Basque, a highly inflected language (Ze-
laia et al, 2005). Besides, the k-NN classification
algorithm can be easily adapted to multilabel cat-
egorization problems such as Reuters.
3.3 Combination of classifiers
The combination of multiple classifiers has been
intensively studied with the aim of improving the
accuracy of individual components (Ho et al,
1994). Two widely used techniques to implement
this approach are bagging (Breiman, 1996), that
uses more than one model of the same paradigm;
and boosting (Freund and Schapire, 1999), in
which a different weight is given to different train-
ing examples looking for a better accuracy.
In our experiment we have decided to construct
a multiclassifier via bagging. In bagging, a set of
training databases TDi is generated by selecting n
training examples drawn randomly with replace-
ment from the original training database TD of n
examples. When a set of n1 training examples,
28
n1 < n, is chosen from the original training col-
lection, the bagging is said to be applied by ran-
dom subsampling. This is the approach used in our
work. The n1 parameter has been selected via tun-
ing. In Section 4.3 the selection will be explained
in a more extended way.
According to the random subsampling, given a
testing document q, the classifier will make a la-
bel prediction ci based on each one of the train-
ing databases TDi. One way to combine the pre-
dictions is by Bayesian voting (Dietterich, 1998),
where a confidence value cvicj is calculated for
each training database TDi and category cj to be
predicted. These confidence values have been cal-
culated based on the original training collection.
Confidence values are summed by category. The
category cj that gets the highest value is finally
proposed as a prediction for the testing document.
In Figure 3 an ilustration of the whole ex-
periment can be seen. First, vectors in the
VSM are projected to the reduced space by using
SVD. Next, random subsampling is applied to the
training database TD to obtain different training
databases TDi. Afterwards the k-NN classifier is
applied for each TDi to make category label pre-
dictions. Finally, Bayesian voting is used to com-
bine predictions, and cj , and in some cases ck as
well, will be the final category label prediction of
the categorization system for testing document q.
In Section 4.3 the cases when a second category
label prediction ck is given are explained.
d1 d2 d9603
d1 d2
d3
d4d5
d2
d3
d4
d5d6d7
d9603 d1
Reuters?21578, ModApte, Test
d9603
d6d7
...
q1 q2 q3299
q
q
d34
d61
d23
d135
d509
TD2TD1 TD30
Reuters?21578, ModApte, Train
...
k?NN k?NN
d50
d778
d848d638d256
d98
d2787
d33
d1989
d55
d4612
d9
VSM
VSM
SVD
k?NN
Random
Subsampling
Bayesian voting 
TD
Rm R
m
R p R p R p
R p
M
Mp=Up?pV Tp
qp=qT Up??1p
c1 c2 c30 cj ,(ck)
qp
qpqp
Figure 3: Proposed approach for multilabel docu-
ment categorization tasks.
4 Experimental Setup
The aim of this section is to describe the document
collection used in our experiment and to give an
account of the preprocessing techniques and pa-
rameter settings we have applied.
When machine learning and other approaches
are applied to text categorization problems, a com-
mon technique has been to decompose the mul-
ticlass problem into multiple, independent binary
classification problems. In this paper, we adopt a
different approach. We will be primarily interested
in a classifier which produces a ranking of possi-
ble labels for a given document, with the hope that
the appropriate labels will appear at the top of the
ranking.
4.1 Document Collection
As previously mentioned, the experiment reported
in this paper has been carried out for the Reuters-
21578 dataset4 compiled by David Lewis and orig-
inally collected by the Carnegie group from the
Reuters newswire in 1987. We use one of the
most widely used training/testing divisions, the
?ModApte? split, in which 75 % of the documents
(9,603 documents) are selected for training and the
remaining 25 % (3299 documents) to test the ac-
curacy of the classifier.
Document distribution over categories in both
the training and the testing sets is very unbalanced:
the 10 most frequent categories, top-10, account
75% of the training documents; the rest is dis-
tributed among the other 108 categories.
According to the number of labels assigned to
each document, many of them (19% in training
and 8.48% in testing) are not assigned to any cat-
egory, and some of them are assigned to 12. We
have decided to keep the unlabeled documents in
both the training and testing collections, as it is
suggested in (Lewis, 2004)5.
4.2 Preprocessing
The original format of the text documents is in
SGML. We perform some preprocessing to fil-
ter out the unused parts of a document. We pre-
served only the title and the body text, punctua-
tion and numbers have been removed and all let-
ters have been converted to lowercase. We have
4http://daviddlewis.com/resources/testcollections
5In the ?ModApte? Split section it is suggested as fol-
lows: ?If you are using a learning algorithm that requires
each training document to have at least TOPICS category,
you can screen out the training documents with no TOPICS
categories. Please do NOT screen out any of the 3,299 docu-
ments - that will make your results incomparable with other
studies.?
29
used the tools provided in the web6 in order to ex-
tract text and categories from each document. We
have stemmed the training and testing documents
by using the Porter stemmer (Porter, 1980)7. By
using it, case and flection information are removed
from words. Consequently, the same experiment
has been carried out for the two forms of the doc-
ument collection: word-forms and Porter stems.
According to the dimension reduction, we have
created the matrices for the two mentioned doc-
ument collection forms. The sizes of the train-
ing matrices created are 15591 ? 9603 for word-
forms and 11114 ? 9603 for Porter stems. Differ-
ent number of dimensions have been experimented
(p = 100, 300, 500, 700).
4.3 Parameter setting
We have designed our experiment in order to op-
timize the microaveraged F1 score. Based on pre-
vious experiments (Zelaia et al, 2005), we have
set parameter k for the k-NN algorithm to k = 3.
This way, the k-NN classifier will give a category
label prediction based on the categories of the 3
nearest ones.
On the other hand, we also needed to decide
the number of training databases TDi to create. It
has to be taken into account that a high number of
training databases implies an increasing computa-
tional cost for the final classification system. We
decided to create 30 training databases. However,
this is a parameter that has not been optimized.
There are two other parameters which have been
tuned: the size of each training database and the
threshold for multilabeling. We now briefly give
some cues about the tuning performed.
4.3.1 The size of the training databases
As we have previously mentioned, documents
have been randomly selected from the original
training database in order to construct the 30 train-
ing databases TDi used in our classification sys-
tem. There are n = 9, 603 documents in the orig-
inal Reuters training collection. We had to decide
the number of documents to select in order to con-
struct each TDi. The number of documents se-
lected from each category preserves the propor-
tion of documents in the original one. We have
experimented to select different numbers n1 < n
6http://www.lins.fju.edu.tw/?tseng/Collections/Reuters-
21578.html
7http://tartarus.org/martin/PorterStemmer/
of documents, according to the following formula:
n1 =
115
?
i=1
2 + tij , j = 10, 20, . . . , 70,
where ti is the total number of training documents
in category i. In Figure 4 it can be seen the vari-
ation of the n1 parameter depending on the value
of parameter j. We have experimented different j
values, and evaluated the results. Based on the re-
sults obtained we decided to select j = 60, which
means that each one of the 30 training databases
will have n1 = 298 documents. As we can see,
the final classification system will be using train-
ing databases which are quite smaller that the orig-
inal one. This gives a lower computational cost,
and makes the classification system faster.
Pa
ra
m
et
er
 n
1
Parameter j
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 1100
 10  20  30  40  50  60  70
Figure 4: Random subsampling rate.
4.3.2 Threshold for multilabeling
The k-NN algorithm predicts a unique cate-
gory label for each testing document, based on the
ranked list of categories obtained for each training
database TDi8. As previously mentioned, we use
Bayesian voting to combine the predictions.
The Reuters-21578 is a multilabel database, and
therefore, we had to decide in which cases to as-
sign a second category label to a testing document.
Given that cj is the category with the highest value
in Bayesian voting and ck the next one, the second
ck category label will be assigned when the fol-
lowing relation is true:
cvck > cvcj ? r, r = 0.1, 0.2, . . . , 0.9, 1
In Figure 5 we can see the mean number of cate-
gories assigned to a document for different values
8It has to be noted that unlabeled documents have been
preserved, and thus, our classification system treats unlabeled
documents as documents of a new category
30
of r. Results obtained were evaluated and based
on them we decided to select r = 0.4, which cor-
responds to a ratio of 1.05 categories.
Parameter r
M
ul
til
ab
el
in
g 
Ra
tio
 0.98
 1
 1.02
 1.04
 1.06
 1.08
 1.1
 1.12
 1.14
 1.16
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Figure 5: Threshold for multilabeling.
5 Experimental Results
In Table 3 microaveraged F1 scores obtained
in our experiment are shown. As it could be
expected, a simple stemming process increases
slightly results, and it can be observed that the best
result for the three category subsets has been ob-
tained for the stemmed corpus, even though gain
is low (less than 0.6).
The evaluation for the Top-10 category subset
gives the best results, reaching up to 93.57%. In
fact, this is the expected behavior, as the number of
categories to be evaluated is small and the number
of documents in each category is high. For this
subset the best result has been obtained for 100
dimensions, although the variation is low among
results for 100, 300 and 500 dimensions. When
using higher dimensions results become poorer.
According to the R(90) and R(115) subsets, the
best results are 87.27% and 87.01% respectively.
Given that the difficulty of these subsets is quite
similar, their behavior is also analogous. As we
can see in the table, most of the best results for
these subsets have been obtained by reducing the
dimension of the space to 500.
6 Conclusions and Future Work
In this paper we present an approach for multilabel
document categorization problems which consists
in a multiclassifier system based on the k-NN al-
gorithm. The documents are represented in a re-
duced dimensional space calculated by SVD. We
want to emphasize that, due to the multilabel char-
acter of the database used, we have adapted the
Dimension reduction
Corpus 100 300 500 700
Words(10) 93.06 93.17 93.44 92.00
Porter(10) 93.57 93.20 93.50 92.57
Words(90) 84.90 86.71 87.09 86.18
Porter(90) 85.34 86.64 87.27 86.30
Words(115) 84.66 86.44 86.73 85.84
Porter(115) 85.13 86.47 87.01 86.00
Table 3: Microaveraged F1 scores for Reuters-
21578 ModApte split.
classification system in order for it to be multilabel
too. The learning of the system has been unique
(9603 training documents) and the category label
predictions made by the classifier have been eval-
uated on the testing set according to the three cat-
egory sets: top-10, R(90) and R(115). The mi-
croaveraged F1 scores we obtain are among the
best reported for the Reuters-21578.
As future work, we want to experiment with
generating more than 30 training databases, and
in a preliminary phase select the best among them.
The predictions made using the selected training
databases will be combined to obtain the final pre-
dictions.
When there is a low number of documents avail-
able for a given category, the power of LSI gets
limited to create a space that reflects interesting
properties of the data. As future work we want
to include background text in the training col-
lection and use an expanded term-document ma-
trix that includes, besides the 9603 training doc-
uments, some other relevant texts. This may in-
crease results, specially for the categories with less
documents (Zelikovitz and Hirsh, 2001).
In order to see the consistency of our classi-
fier, we also plan to repeat the experiment for the
RCV1 (Lewis et al, 2004), a new benchmark col-
lection for text categorization tasks which consists
of 800,000 manually categorized newswire stories
recently made available by Reuters.
7 Acknowledgements
This research was supported by the Univer-
sity of the Basque Country (UPV00141.226-T-
15948/2004) and Gipuzkoa Council in a European
31
Union Program.
References
Berry, M.W. and Browne, M.: Understanding Search
Engines: Mathematical Modeling and Text Re-
trieval. SIAM Society for Industrial and Applied
Mathematics, ISBN: 0-89871-437-0, Philadelphia,
(1999)
Breiman, L.: Bagging Predictors. Machine Learning,
24(2), 123?140, (1996)
Cristianini, N., Shawe-Taylor, J. and Lodhi, H.: Latent
Semantic Kernels. Proceedings of ICML?01, 18th
International Conference on Machine Learning, 66?
73, Morgan Kaufmann Publishers, (2001)
Dasarathy, B.V.: Nearest Neighbor (NN) Norms:
NN Pattern Recognition Classification Techniques.
IEEE Computer Society Press, (1991)
Debole, F. and Sebastiani, F.: An Analysis of the Rela-
tive Hardness of Reuters-21578 Subsets. Journal of
the American Society for Information Science and
Technology, 56(6),584?596, (2005)
Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer,
T.K. and Harshman, R.: Indexing by Latent Seman-
tic Analysis. Journal of the American Society for
Information Science, 41, 391?407, (1990)
Dietterich, T.G.: Machine-Learning Research: Four
Current Directions. The AI Magazine, 18(4), 97?
136, (1998)
Dumais, S.T., Platt, J., Heckerman, D. and Sahami,
M.: Inductive Learning Algorithms and Repre-
sentations for Text Categorization. Proceedings of
CIKM?98: 7th International Conference on Infor-
mation and Knowledge Management, ACM Press,
148?155 (1998)
Dumais, S.: Latent Semantic Analysis. ARIST, An-
nual Review of Information Science Technology, 38,
189?230, (2004)
Freund, Y. and Schapire, R.E.: A Short Introduction to
Boosting. Journal of Japanese Society for Artificial
Intelligence, 14(5), 771-780, (1999)
Gao, S., Wu, W., Lee, C.H. and Chua, T.S.: A Maxi-
mal Figure-of-Merit Learning Approach to Text Cat-
egorization. Proceedings of SIGIR?03: 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
174?181, ACM Press, (2003)
Gliozzo, A. and Strapparava, C.: Domain Kernels
for Text Categorization. Proceedings of CoNLL?05:
9th Conference on Computational Natural Language
Learning, 56?63, (2005)
Ho, T.K., Hull, J.J. and Srihari, S.N.: Decision Combi-
nation in Multiple Classifier Systems. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 16(1), 66?75, (1994)
Joachims, T. Text Categorization with Support Vector
Machines: Learning with Many Relevant Features.
Proceedings of ECML?98: 10th European Confer-
ence onMachine Learning, Springer 1398, 137?142,
(1998)
Kim, H., Howland, P. and Park, H.: Dimension Re-
duction in Text Classification with Support Vector
Machines. Journal of Machine Learning Research,
6, 37?53, MIT Press, (2005)
Lewis, D.D.: Reuters-21578 Text Catego-
rization Test Collection, Distribution 1.0.
http://daviddlewis.com/resources/testcollections
README file (v 1.3), (2004)
Lewis, D.D., Yang, Y., Rose, T.G. and Li, F.: RCV1: A
New Benchmark Collection for Text Categorization
Research. Journal of Machine Learning Research,
5, 361?397, (2004)
Porter, M.F.: An Algorithm for Suffix Stripping. Pro-
gram, 14(3), 130?137, (1980)
Salton, G. and McGill, M.: Introduction to Modern
Information Retrieval. McGraw-Hill, New York,
(1983)
Sebastiani, F.: Machine Learning in Automated Text
Categorization. ACM Computing Surveys, 34(1),
1?47, (2002)
Weiss, S.M., Apte, C., Damerau, F.J., Johnson, D.E.,
Oles, F.J., Goetz, T. and Hampp, T.: Maximizing
Text-Mining Performance. IEEE Intelligent Sys-
tems, 14(4),63?69, (1999)
Yang, Y. An Evaluation of Statistical Approaches to
Text Categorization. Journal of Information Re-
trieval. Kluwer Academic Publishers, 1,(1/2), 69?
90, (1999)
Zelaia, A., Alegria, I., Arregi, O. and Sierra, B.: An-
alyzing the Effect of Dimensionality Reduction in
Document Categorization for Basque. Proceedings
of L&TC?05: 2nd Language & Technology Confer-
ence, 72?75, (2005)
Zelikovitz, S. and Hirsh, H.: Using LSI for Text
Classification in the Presence of Background Text.
Proceedings of CIKM?01: 10th ACM International
Conference on Information and Knowledge Man-
agement, ACM Press, 113?118, (2001)
Zhang, T. and Oles, F.J.: Text Categorization Based
on Regularized Linear Classification Methods. In-
formation Retrieval, 4(1): 5?31, Kluwer Academic
Publishers, (2001)
32
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 2?7,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Automatic extraction of NV expressions in Basque: basic issues on
cooccurrence techniques
Antton Gurrutxaga
Elhuyar Foundation
a.gurrutxaga@elhuyar.com
In?aki Alegria
IXA group/Univ. of the Basque Country
i.alegria@ehu.es
Abstract
Taking as a starting-point the development
on cooccurrence techniques for several lan-
guages, we focus on the aspects that should
be considered in a NV extraction task for
Basque. In Basque, NV expressions are con-
sidered those combinations in which a noun,
inflected or not, is co-occurring with a verb, as
erabakia hartu (?to make a decision?), kontuan
hartu (?to take into account?) and buruz jakin
(?to know by heart?). A basic extraction sys-
tem has been developed and evaluated against
two references: a) a reference which includes
NV entries from several lexicographic works;
and b) a manual evaluation by three experts of
a random sample from the n-best lists.
1 Introduction
The last decade has witnessed great advances in the
automatic identification and processing of MWEs.
In the case of Basque, advances are limited to termi-
nology extraction and the tagging in corpora of the
MWEs represented in lexical databases.
Furthermore, the work on both theoretical and
practical phraseology in Basque has been mainly fo-
cused on idiomatic expressions, leaving aside col-
locations (Pe?rez Gaztelu et al, 2004). As a con-
sequence, Basque NLP and lexicography have not
benefited from the approach that emphasized the im-
portance of such units, and very important areas are
underdeveloped.
With the aim of taking steps to turn this situa-
tion, we undertake the task of extracting NV com-
binations from corpora. As a preliminary step, we
must face the morphosyntactic aspects of Basque
that might influence the efficiency of the process.
2 MWE: basic definition and extraction
techniques
As a basis for our work, we take idiomaticity as
the key feature for the definition and classifica-
tion of MWE. Idiomaticity could be described as a
non-discrete magnitude, whose ?value?, according
to recent investigations (Baldwin and Kim, 2010;
Fazly and Stevenson, 2007; Granger and Paquot,
2008), has turned to depend on a complex combi-
nation of features such as institutionalization, non-
compositionality and lexico-syntactic fixedness.
The idiomaticity of MWEs appears rather as a
continuum than as a scale of discrete values (Sin-
clair, 1996; Wulff, 2010). Thus, the classifica-
tion of MWEs into discrete categories is a difficult
task. Taking Cowie?s classification as an initial basis
(Cowie, 1998), our work is focused on phrase-like
units, aiming, at this stage, to differentiate MWEs
(idioms and collocations) from free combinations.
Specifically, NV combinations with the following
characteristics are considered as MWEs:
? Idioms: non-compositional combinations, as
opaque idioms (adarra jo: ?to pull somebody?s
leg?; lit: ?to play the horn?) and figurative id-
ioms (burua hautsi: ?to rack one?s brain?; lit:
?to break one?s head?).
? Collocations:
? Semicompositional combinations, in
which the noun keeps its literal meaning,
2
whereas the verb acts as a support verb
(lan egin: ?to work?; lit. ?to do work?),
or has a meaning which is specific to that
combination (atentzioa eman: ?to catch
someone?s eye?; lit. ?to give attention?
(sth to sb)); legea urratu: ?to break the
law?; lit. ?to tear the law?).
? Compositional combinations with lexical
restriction, in which it is not possible to
substitute the verb with its synonyms, or
that present a clear statistical idiosyncrasy
in favor of a given synonym choice (elka-
rtasuna adierazi: ?to express solidarity?;
konpromisoa berretsi: ?to confirm a com-
mitment?).
Among the different techniques that have been
proposed to extract and characterize MWEs, the
cooccurrence of the components is the most used
heuristic of institutionalization, and the use of asso-
ciation measures (AM) goes back to early research
on this field (Church and Hanks, 1990; Smadja,
1993). In recent years, the comparative analysis of
AMs has aroused considerable interest, as well as
the possibility of obtaining better results by com-
bining them (Pearce, 2002; Pecina, 2005). Cooc-
currence techniques are usually used in combination
with linguistic techniques, which allow the use of
lemmatized and POS-tagged corpora, or even syn-
tactic dependencies (Seretan, 2008).
3 Special features of Basque NV
combinations
These are some characteristics of the NV combina-
tions in Basque to be considered in order to design
the extraction process efficiently:
? Basque being an agglutinative language, MWE
extraction must work on tagged texts, in order
to identify different surface forms with their
corresponding lemma. Thus, pure statistical
methods working with raw text are not ex-
pected to yield acceptable results.
? Some combinations with a noun as first lemma
do not correspond to NV combinations in the
sense that is usually understood in English. For
example, the expression kontuan hartu can be
translated as take into account, where kontu is
a noun in the inessive case. We are interested in
all types of combinations that a noun can form
with verbs.
? Representing NV combinations as lemma-
lemma pairs is by no means satisfactory; we
would not be able to differentiate the aforemen-
tioned kontuan hartu from kontu hartu (?to ask
for an explanation?). So it is necessary to deal
with the form or type of every noun.
? In order to propose canonical forms for NV
combinations, we need case and number an-
notations for nouns in bigram data. The next
examples are different forms of the canoni-
cal erabakia hartu (?to make a decision?): ez
zuen erabakirik hartu (?he did not make any
decision?), zenbait erabaki hartu behar ditugu
(?we have to make some decisions?). Canonical
forms can be formulated by bigram normaliza-
tion (see section 4.5 for details).
4 Experimental setup
4.1 Corpora resources
In our experiments, we use a journalistic cor-
pus from two sources: (1) Issues published be-
tween 2001-2002 by the newspaper Euskaldunon
Egunkaria (28 Mw); and (2) Issues published be-
tween 2006-2010 by the newspaper Berria (47 Mw).
So, the overall size of the corpus is 75 Mw.
4.2 Corpus-processing
For linguistic tagging, we use EUSTAGGER by the
IXA group of the University of the Basque Country
(Aduriz et al, 1996). After linguistic processing, we
obtain information about the lemma, part-of-speech,
subcategory, case, number and other morphosyntac-
tic features.
We used EUSTAGGER without the module to de-
tect and annotate MWEs in order to evaluate the au-
tomatic extraction, regardless of wheter the candi-
dates are in the lexical database.
4.3 Preparing tagged corpora for bigram
generation
For bigram generation, we use the Ngram Statistics
Package-NSP (Banerjee and Pedersen, 2010). In
3
order to retain in the text sent to NSP the linguis-
tic information needed according to section 3, we
add different types of linguistic information to the
tokens, depending on the POS of the components of
the combination we are dealing with. In the case of
NV combinations, the nouns are represented in the
following form:
token lemma POS subcategory case number
In the case of verbs, only lemma and POS are
used, as verb inflection has no influence on the
canonical form of the expression. In future work,
verb inflection will be one of the parameters to mea-
sure syntactical flexibility. All other types of tokens
are discarded and considered as ?non-token? for NSP
processing.
Before this step, some surface-grammar rules are
defined to detect and filter the participle forms that
are not part of a NV combination, but must be ana-
lyzed as adjectives or nouns (eg. herrialde aurrerat-
uak ?developed countries?, and gobernuaren aliat-
uak, ?government?s allies?).
4.4 Bigram generation
We generated bigram sets for two different window
spans: ?1 and ?5. In both sets, the frequency cri-
terion for a bigram to be generated is f > 30. Also,
the following punctuation marks are interpreted as
a boundary for bigram generation: period, colon,
semicolon, and question and exclamation marks.
Then, all counts of bigrams in NV and VN order are
combined using NSP, and reordered in NV order.
Additionally, a heuristic is used to filter some
combinations. The first member of many ?com-
pound verbs? like nahi izan (?to want?), is a noun,
and some of them combine usually with a verb, in
VN order: ikusi nahi (zuen) (?he wanted to see?). In
order to reduce this noise, the combinations occur-
ring mostly in VN order are removed. The combi-
nations generated from passive constructions (hartu-
tako erabakien ondorioak, ?the consequences of the
decisions made?) are not affected by this filtering.
4.5 Bigram normalization
In order to get more representative statistics, and
to get information that would enable us to propose
a canonical form for each MWE candidate, differ-
ent inflection forms of the same case in nouns are
normalized to the most frequent form, and bigram
counts are recalculated. I.e. [ erabakia / erabakiak
/ erabakiok / erabakirik / erabaki ] hartu are col-
lapsed to erabakia hartu (?to make a decision?), be-
cause all the mentioned forms of the lemma erabaki
appear in the absolutive case. In contrast, the com-
binations kontu hartu (?to ask for an explanation?)
and kontuan hartu (?take into account?) are not nor-
malized, as their noun forms correspond to differ-
ent cases, namely, absolutive (kontu) and inessive
(kontuan). A Perl script detects in the dataset the
bigrams to be normalized, using the combined key
noun lemma/noun case+verb lemma, creates a sin-
gle bigram with the most frequent form, and sums
the frequencies of bigrams and those of the noun un-
igrams.
As an example, this is normalization data for
kalean ibili (?to walk on the street?):
kalean kale IZE ARR INE NUMS<>ibili ADI<>223 3354 10880
kaleetan kale IZE ARR INE NUMP<>ibili ADI<>119 243 10880
?
kalean kale IZE ARR INE NUMS<>ibili ADI<>342 3597 10880
Besides, ergative-singular ? absolutive-plural
normalization is carried out when the ratio is greater
than 1:5. This heuristic is used in order to repair
some mistakes from the tagger. Finally, partitive
case (PAR) is assimilated to absolutive (ABS) for bi-
gram normalization; partitive is a case used in neg-
ative, interrogative and conditional sentences with
subjects of intransitive verbs and objects of transi-
tive verbs. I.e. ez zuen erabakirik hartu (?he did not
make any decision?).
Thus, this is the normalization of erabakia hartu:
erabakia erabaki IZE ARR ABS NUMS<>hartu ADI<>2658 6329 88447
erabakiak erabaki IZE ARR ABS NUMP<>hartu ADI<>1632 2397 88447
erabakiak erabaki IZE ARR ERG NUMP<>hartu ADI<>88 141 88447
erabakirik erabaki IZE ARR PAR MG<>hartu ADI<>211 211 88447
?
erabakia erabaki IZE ARR ABS NUMS<>hartu ADI<>4589 9361 88447
4.6 AM calculation
The statistical analysis of cooccurrence data is car-
ried out using Stefan Evert?s UCS toolkit (Evert,
2005). The most common association measures are
calculated for each bigram: f , t-score (also t-test),
log-likelihood ratio, MI, MI3, and chi-square (?2).
4.7 Evaluation
In order to evaluate the results of the bigram extrac-
tion process, we use as a reference a collection of
4
NV expressions published in five Basque resources:
a) The Unified Basque Dictionary, b) Euskal Hizte-
gia (Sarasola, 1996); c) Elhuyar Hiztegia; d) Intza
project; and e) EDBL (Aldezabal et al, 2001).
The total number for NV expressions is 3,742.
Despite the small size of the reference, we believe
that it may be valid for a comparison of the perfor-
mance of different AMs. Nevertheless, even a su-
perficial analysis reveals that the reference is mostly
made up of two kinds of combinations, idioms and
typical ?compound verbs?1.
Every evaluation against a dictionary depends
largely on its recall and quality, and we envisage,
as recommended by Krenn (1999), to build a hand-
made gold standard. To this end, we extract an eval-
uation sample merging the 2,000-best candidates of
each AM ranking from the w = ?1 extraction set.
There are 4,334 different bigrams in this set. This
manual evaluation is an ongoing work by a group of
three experts (one of them is an author of this paper).
Annotators were provided with an evaluation man-
ual, with explanatory information about the evalua-
tion task and the guidelines that must be followed to
differentiate MWEs from free combinations, based
on the criteria mentioned in section 2. Illustrative
examples are included.
At present, a random sample of 600 has been eval-
uated (13.8%), with a Fleiss kappa of 0.46. Even
though some authors have reported lower agree-
ments on this task (Street et al, 2010), this level of
agreement is comparatively low (Fazly and Steven-
son, 2007; Krenn et al, 2004), and by no means sat-
isfactory. It is necessary to make further efforts to
improve the discriminatory criteria, and achieve a
better ?tuning? between the annotators.
5 Results
Figure 1 shows the precision curves obtained for
each AM in the automatic evaluation. Frequency
yields the best precision, followed by t-score, log-
likelihood and MI3. MI and ?2 have a very low
performance, even below the baseline2. These re-
1Support verbs with syntactic idiosyncrasy (anomalous use
of the indefinite noun), as lan egin (?to work?) and min hartu
(?to get hurt?).
2Following Evert (2005), our baseline corresponds to the
precision yielded by a random ranking of the n candidates from
thedata set?; and our topline is ?the precision achieved by an
sults are consistent with those reported by Krenn and
Evert (2001) for support-verbs (FVG). Accordingly,
this is the type of combination which is very much
present in our dictionary reference.
Figure 1: Precision results for the extraction set with w =
?1 and f > 30.
Figure 2 offers an evaluation of the influence of
window span and bigram normalization. The best
results are obtained by the f ranking with a narrow
window and without bigram normalization. Regard-
ing bigram normalization, it could be concluded, at
first sight, that the canonical forms included in the
dictionary are not the most frequent forms of their
corresponding MWEs. Thus, the frequency criteria
used to normalize different forms of the same case
and assign canonical forms must be reviewed. As for
window span, the hypothesis that, since Basque is
largely a free-word-order language, a wider window
would yield more significant cooccurrence statistics,
is not confirmed at the moment. Further analysis is
needed to interpret these results from a deeper lin-
guistic point of view.
Even though the manually evaluated random sam-
ple is small (600 combinations), some provisional
conclusions can be drawn from the results. The
amount of candidates validated by at least two of the
three evaluators is 153, whereas only 29 of them are
included in the dictionary reference. Even though
MWE classification has not yet been undertaken by
the annotator?s team, a first analysis by the authors
shows that most of the manually validated combina-
?ideal? measure that ranks all TPs at the top of the list?.
5
Figure 2: Precision results of f and t-score for three dif-
ferent extraction sets (f > 30): a) w = ?1 with bigram
normalization; b) w = ?1 without bigram normalization;
and c) w = ?5 with bigram normalization.
tions not included in the dictionary (108 out of 124)
are restricted collocations (mainly support-verb con-
structions that are not ?compound verbs?) or statis-
tically idiosyncratic units. This is the first clue that
confirms our suspicions about the limited coverage
and representativeness of the reference. At the same
time, it could be one of the possible explanations for
the low inter-annotator agreement achieved, as far as
those types of MWEs are the most difficult to differ-
entiate from free combinations.
Figure 3 presents the precision curves for the
complete evaluation set estimated from the manu-
ally evaluated random sample using the technique
proposed by Evert and Krenn (2005). As expected,
precision results increase compared with the evalu-
ation against the dictionary. Frequency and t-score
outperform the other AMs, but frequency is not the
best measure in the whole range, as it is overtaken
by t-score in the first 1,200 candidates.
6 Conclusions and Future work
The first results for the extraction of NV expressions
in Basque are similar to the figures in Krenn and
Evert (2001). Frequency and t-score are good mea-
sures and it seems difficult to improve upon them.
Nevertheless, in light of the results, it is essential to
complete the manual evaluation and build a repre-
sentative gold standard in order to have a more pre-
cise idea of the coverage of the reference, and get
Figure 3: Precision results estimated from a 13.8% ran-
don sample manually evaluated (600 conbinations).
a more accurate view of the behaviour of AMs in
function of several factors such as the type of combi-
nation, corpus size, frequency range, window span,
etc. Bigram normalization is, in principle, a reason-
able procedure to formulate representative canoni-
cal forms, but requires a deeper analysis of the si-
lence that it seems to generate in the results. Finally,
the first evaluation using a small gold-standard is en-
couraging, because it suggests that using AMs it is
possible to find new expressions that are not pub-
lished in Basque dictionaries.
In the near future, we want to carry out a more
comprehensive evaluation of the AMs, and study
how to combine them in order to improve the re-
sults (Pecina and Schlesinger, 2006). In addition of
this, we want to detect lexical, syntactic and seman-
tic features of the expressions, and use this informa-
tion to characterize them (Fazly et al, 2009).
Acknowledgments
This research was supported in part by the Span-
ish Ministry of Education and Science (OpenMT-2,
TIN2009-14675-C03-01) and by the Basque Gov-
ernment (Berbatek: Tools and Technologies to pro-
mote Language Industry. Etortek - IE09-262). Our
colleagues Ainara Estarrona and Larraitz Uria are
kindly acknowledged for providing their expertise as
linguists in the manual evaluation process.
6
References
Aduriz, I., I. Aldezabal, I. Alegria, X. Artola,
N. Ezeiza, and R. Urizar (1996). EUSLEM:
A lemmatiser/tagger for Basque. Proc. of EU-
RALEX?96, 17?26.
Aldezabal, I., O. Ansa, B. Arrieta, X. Artola,
A. Ezeiza, G. Herna?ndez, and M. Lersundi
(2001). EDBL: A general lexical basis for the au-
tomatic processing of Basque. In IRCS Workshop
on linguistic databases, pp. 1?10.
Baldwin, T. and S. Kim (2010). Multiword expres-
sions. Handbook of Natural Language Process-
ing, second edition. Morgan and Claypool.
Banerjee, S. and T. Pedersen (2010). The design,
implementation, and use of the Ngram Statistics
Package. Computational Linguistics and Intelli-
gent Text Processing, 370?381.
Church, K. and P. Hanks (1990). Word associa-
tion norms, mutual information, and lexicogra-
phy. Computational linguistics 16(1), 22?29.
Cowie, A. (1998). Phraseology: Theory, analysis,
and applications. Oxford University Press, USA.
Evert, S. (2005). The statistics of word cooccur-
rences: Word pairs and collocations. Ph. D. the-
sis, University of Stuttgart.
Evert, S. and B. Krenn (2005). Using small ran-
dom samples for the manual evaluation of statis-
tical association measures. Computer Speech &
Language 19(4), 450?466.
Fazly, A., P. Cook, and S. Stevenson (2009). Un-
supervised type and token identification of id-
iomatic expressions. Computational Linguis-
tics 35(1), 61?103.
Fazly, A. and S. Stevenson (2007). Distinguish-
ing subtypes of multiword expressions using
linguistically-motivated statistical measures. In
Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pp. 9?16. As-
sociation for Computational Linguistics.
Granger, S. and M. Paquot (2008). Disentangling
the phraseological web. Phraseology. An inter-
disciplinary perspective, 27?50.
Krenn, B. (1999). The usual suspects: Data-
oriented models for identification and represen-
tation of lexical collocations. German Research
Center for Artificial Intelligence.
Krenn, B. and S. Evert (2001). Can we do better than
frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Work-
shop on Collocations, pp. 39?46.
Krenn, B., S. Evert, and H. Zinsmeister (2004). De-
termining intercoder agreement for a collocation
identification task. In Proceedings of KONVENS,
pp. 89?96.
Pearce, D. (2002). A comparative evaluation of col-
location extraction techniques. In Proc. of LREC
2002, pp. 1530?1536.
Pecina, P. (2005). An extensive empirical study of
collocation extraction methods. In Proceedings of
the ACL Student Research Workshop, pp. 13?18.
Association for Computational Linguistics.
Pecina, P. and P. Schlesinger (2006). Combining as-
sociation measures for collocation extraction. pp.
651?658.
Pe?rez Gaztelu, E., I. Zabala, and L. Gra?cia (2004).
Las fronteras de la composicio?n en lenguas
roma?nicas y en vasco. San Sebastia?n: Universi-
dad de Deusto.
Sarasola, I. (1996). Euskal Hiztegia. Kutxa Fun-
dazioa / Fundacio?n Kutxa.
Seretan, V. (2008). Collocation extraction based on
syntactic parsing. Ph. D. thesis, University of
Geneva.
Sinclair, J. (1996). The search for units of meaning.
Textus 9(1), 75?106.
Smadja, F. (1993). Retrieving collocations from
text: Xtract. Computational linguistics 19(1),
143?177.
Street, L., N. Michalov, R. Silverstein, M. Reynolds,
L. Ruela, F. Flowers, A. Talucci, P. Pereira,
G. Morgon, S. Siegel, M. Barousse, A. Anderson,
T. Carroll, and A. Feldman (2010). Like finding a
needle in a haystack: Annotating the american na-
tional corpus for idiomatic expressions. In Proc.
of LREC 2010, Valletta, Malta.
Wulff, S. (2010). Rethinking Idiomaticity. Corpus
and Discourse. New York: Continuum Interna-
tional Publishing Group Ltd.
7
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 39?48,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning word-level dialectal variation as phonological replacement rules
using a limited parallel corpus
Mans Hulden
University of Helsinki
Language Technology
mans.hulden@helsinki.fi
In?aki Alegria
IXA taldea
UPV-EHU
i.alegria@ehu.es
Izaskun Etxeberria
IXA taldea
UPV-EHU
izaskun.etxeberria@ehu.es
Montse Maritxalar
IXA taldea
UPV-EHU
montse.maritxalar@ehu.es
Abstract
This paper explores two different methods of
learning dialectal morphology from a small
parallel corpus of standard and dialect-form
text, given that a computational description
of the standard morphology is available. The
goal is to produce a model that translates in-
dividual lexical dialectal items to their stan-
dard dialect counterparts in order to facili-
tate dialectal use of available NLP tools that
only assume standard-form input. The results
show that a learning method based on induc-
tive logic programming quickly converges to
the correct model with respect to many phono-
logical and morphological differences that are
regular in nature.
1 Introduction
In our work with the Basque language, a morpho-
logical description and analyzer is available for the
standard language, along with other tools for pro-
cessing the language (Alegria et al, 2002). How-
ever, it would be convenient to be able to analyze
variants and dialectal forms as well. As the dialectal
differences within the Basque language are largely
lexical and morphophonological, analyzing the di-
alectal forms would in effect require a separate mor-
phological analyzer that is able to handle the unique
lexical items in the dialect together with the differ-
ing affixes and phonological changes.
Morphological analyzers are traditionally hand-
written by linguists, most commonly using some
variant of the popular finite-state morphology ap-
proach (Beesley and Karttunen, 2002). This entails
having an expert model a lexicon, inflectional and
derivational paradigms as well as phonological al-
ternations, and then producing a morphological an-
alyzer/generator in the form of a finite-state trans-
ducer.
As the development of such wide-coverage mor-
phological analyzers is labor-intesive, the hope is
that an analyzer for a variant could be automatically
learned from a limited parallel standard/dialect cor-
pus, given that an analyzer already exists for the
standard language. This is an interesting problem
because a good solution to it could be applied to
many other tasks as well: to enhancing access to
digital libraries (containing diachronic and dialectal
variants), for example, or to improving treatment of
informal registers such as SMS messages and blogs,
etc.
In this paper we evaluate two methods of learning
a model from a standard/variant parallel corpus that
translates a given word of the dialect to its standard-
form equivalent. Both methods are based on finite-
state phonology. The variant we use for experiments
is Lapurdian,1 a dialect of Basque spoken in the La-
purdi (fr. Labourd) region in the Basque Country.
Because Basque is an agglutinative, highly in-
flected language, we believe some of the results can
be extrapolated to many other languages facing sim-
ilar challenges.
One of the motivations for the current work is
that there are a large number of NLP tools avail-
able and in development for standard Basque (also
called Batua): a morphological analyzer, a POS tag-
ger, a dependency analyzer, an MT engine, among
1Sometimes also called Navarro-Labourdin or Labourdin.
39
others (Alegria et al, 2011). However, these tools
do not work well in processing the different dialects
of Basque where lexical items have a different ortho-
graphic representation owing to slight differences in
phonology and morphology.
Here is a brief contrastive example of the kinds
of differences found in the (a) Lapurdian dialect and
standard Basque (b) parallel corpus:2
(a) Ez gero uste izan nexkatxa guziek tu egiten dautatela
(b) Ez gero uste izan neskatxa guztiek tu egiten didatela
As the example illustrates, the differences are mi-
nor overall?the word order and syntax are unaf-
fected, and only a few lexical items differ. This re-
flects the makeup of our parallel corpus quite well?
in it, slightly less than 20% of the word tokens
are distinct. However, even such relatively small
discrepancies cause great problems in the poten-
tial reuse of current tools designed for the standard
forms only.
We have experimented with two approaches that
attempt to improve on a simple baseline of mem-
orizing word-pairs in the dialect and the standard.
The first approach is based on work by Almeida
et al (2010) on contrasting orthography in Brazil-
ian Portuguese and European Portuguese. In this
approach differences between substrings in distinct
word-pairs are memorized and these transformation
patterns are then applied whenever novel words are
encountered in the evaluation. To prevent over-
generation, the output of this learning process is
later subject to a morphological filter where only ac-
tual standard-form outputs are retained. The second
approach is an Inductive Logic Programming-style
(ILP) (Muggleton and De Raedt, 1994) learning
algorithm where phonological transformation rules
are learned from word-pairs. The goal is to find a
minimal set of transformation rules that is both nec-
essary and sufficient to be compatible with the learn-
ing data, i.e. the word pairs seen in the training data.
The remainder of the paper is organized as fol-
lows. The characteristics of the corpus available to
us are described in section 2. In sections 3, 4, and 5,
we describe the steps and variations of the methods
we have applied and how they are evaluated. Sec-
tion 6 presents the experimental results, and finally,
2English translation of the example: Don?t think all girls spit
on me
we discuss the results and present possibilities for
potential future work in section 7.
1.1 Related work
The general problem of supervised learning of di-
alectal variants or morphological paradigms has
been discussed in the literature with various connec-
tion to computational phonology, morphology, ma-
chine learning, and corpus-based work. For exam-
ple, Kestemont et al (2010) presents a language-
independent system that can ?learn? intra-lemma
spelling variation. The system is used to produce
a consistent lemmatization of texts in Middle Dutch
literature in a medieval corpus, Corpus-Gysseling,
which contains manuscripts dated before 1300 AD.
These texts have enormous spelling variation which
makes a computational analysis difficult.
Koskenniemi (1991) provides a sketch of a dis-
covery procedure for phonological two-level rules.
The idea is to start from a limited number of
paradigms (essentially pairs of input-output forms
where the input is the surface form of a word and the
output a lemmatization plus analysis). The problem
of finding phonological rules to model morpholog-
ical paradigms is essentially similar to the problem
presented in this paper. An earlier paper, Johnson
(1984), presents a ?discovery procedure? for learning
phonological rules from data, something that can be
seen as a precursor to the problem dealt with by our
ILP algorithm.
Mann and Yarowsky (2001) present a method
for inducing translation lexicons based on transduc-
tion models of cognate pairs via bridge languages.
Bilingual lexicons within languages families are in-
duced using probabilistic string edit distance mod-
els. Inspired by that paper, Scherrer (2007) uses
a generate-and-filter approach quite similar to our
first method. He compares different measures of
graphemic similarity applied to the task of bilin-
gual lexicon induction between Swiss German and
Standard German. Stochastic transducers are trained
with the EM algorithm and using handmade trans-
duction rules. An improvement of 11% in F-score is
reported over a baseline method using Levenshtein
Distance.
40
Full corpus 80% part. 20% part.
Sentences 2,117 1,694 423
Words 12,150 9,734 2,417
Unique words
Standard Basque 3,553 3,080 1,192
Lapurdian 3,830 3,292 1,239
Filtered pairs 3,610 3,108 1,172
Identical pairs 2,532 2,200 871
Distinct pairs 1,078 908 301
Table 1: Characteristics of the parallel corpus used for
experiments.
2 The corpus
The parallel corpus used in this research is part of
?TSABL? project developed by the IKER group in
Baiona (fr. Bayonne).3 The researchers of the IKER
project have provided us with examples of the La-
purdian dialect and their corresponding forms in
standard Basque. Our parallel corpus then contains
running text in two variants: complete sentences of
the Lapurdian dialect and equivalent sentences in
standard Basque.
The details of the corpus are presented in table 1.
The corpus consists of 2,117 parallel sentences, to-
taling 12,150 words (roughly 3,600 types). In order
to provide data for our learning algorithms and also
to test their performance, we have divided the cor-
pus into two parts: 80% of the corpus is used for the
learning task (1,694 sentences) and the remaining
20% (423 sentences) for evaluation of the learning
process. As is seen, roughly 23% of the word-pairs
are distinct. Another measure of the average devi-
ation between the word pairs in the corpus is given
by aligning all word-pairs by minimum edit distance
(MED): aligning the 3,108 word-pairs in the learn-
ing corpus can be done at a total MED cost of 1,571.
That is, roughly every 14th character in the dialect
data is different from the standard form.
3 The baseline
The baseline of our experiments is a simple method,
based on a dictionary of equivalent words with the
list of correspondences between words extracted
3Towards a Syntactic Atlas of the Basque Language, web
site: http://www.iker.cnrs.fr/-tsabl-towards-a-syntactic-atlas-
of-.html
from the learning portion (80%) of the corpus. This
list of correspondences contains all different word
pairs in the variant vs. standard corpus. The baseline
approach consists simply of memorizing all the dis-
tinct word pairs seen between the dialectal and stan-
dard forms, and subsequently applying this knowl-
edge during the evaluation task. That is, if an in-
put word during the evaluation has been seen in the
training data, we provide the corresponding previ-
ously known output word as the answer. Otherwise,
we assume that the output word is identical to the
input word.
4 Overview of methods
We have employed two different methods to produce
an application that attempts to extract generaliza-
tions from the training corpus to ultimately be able
to produce the equivalent standard word correspond-
ing to a given dialectal input word. The first method
is based on already existing work by Almeida et al
(2010) that extracts all substrings from lexical pairs
that are different. From this knowledge we then pro-
duce a number of phonological replacement rules
that model the differences between the input and
output words. In the second method, we likewise
produce a set of phonological replacement rules, us-
ing an ILP approach that directly induces the rules
from the pairs of words in the training corpus.
The core difference between the two methods is
that while both extract replacement patterns from
the word-pairs, the first method does not consider
negative evidence in formulating the replacement
rules. Instead, the existing morphological analyzer
is used as a filter after applying the rules to unknown
text. The second method, however, uses negative
evidence from the word-pairs in delineating the re-
placement rules as is standard in ILP-approaches,
and the subsequent morphological filter for the out-
put plays much less of a role. Evaluating and com-
paring both approaches is motivated because the first
method may produce much higher recall by virtue
of generating a large number of input-output candi-
dates during application, and the question is whether
the corresponding loss in precision may be mitigated
by judicious application of post-processing filters.
41
4.1 Format of rules
Both of the methods we have evaluated involve
learning a set of string-transformation rules to
convert words, morphemes, or individual letters
(graphemes) in the dialectal forms to the stan-
dard variant. The rules that are learned are in
the format of so-called phonological replacement
rules (Beesley and Karttunen, 2002) which we have
later converted into equivalent finite-state transduc-
ers using the freely available foma toolkit (Hulden,
2009a). The reason for the ultimate conversion of
the rule set to finite-state transducers is twofold:
first, the transducers are easy to apply rapidly to
input data using available tools, and secondly, the
transducers can further be modified and combined
with the standard morphology already available to
us as a finite transducer.
In its simplest form, a replacement rule is of the
format
A? B || C D (1)
where the arguments A,B,C,D are all single sym-
bols or strings. Such a rule dictates the transfor-
mation of a string A to B, whenever the A occurs
between the strings C and D. Both C and D are
optional arguments in such a rule, and there may
be multiple conditioning environments for the same
rule.
For example, the rule:
h -> 0 || p , t , l , a s o
(2)
would dictate a deletion of h in a number of con-
texts; when the h is preceded by a p, t, or l, or suc-
ceeded by the sequence aso, for instance transform-
ing ongiethorri (Lapurdian) to ongietorri (Batua).
As we will be learning several rules that each tar-
get different input strings, we have a choice as to the
mode of application of the rules in the evaluation
phase. The learned rules could either be applied in
some specific order (sequentially), or applied simul-
taneously without regard to order (in parallel).
For example, the rules:
u -> i || z a (3)
k -> g || z a u (4)
would together (in parallel) change zaukun into zai-
gun. Note that if we imposed some sort of ordering
on the rules and the u ? i rule in the set would
apply first, for example, the conditioning environ-
ment for the second rule would no longer be met
after transforming the word into zaikun. We have
experimented with sequential as well as parallel pro-
cessing, and the results are discussed below.
4.2 Method 1 (lexdiff) details
The first method is based on the idea of identi-
fying sequences inside word pairs where the out-
put differs from the input. This was done through
the already available tool lexdiff which has been
used in automatic migration of texts between differ-
ent Portuguese orthographies (Almeida et al, 2010).
The lexdiff program tries to identify sequences of
changes from seen word pairs and outputs string cor-
respondences such as, for example: 76 ait ->
at ; 39 dautz -> diz (stemming from pairs
such as (joaiten/joaten and dautzut/dizut), indicating
that ait has changed into at 76 times in the cor-
pus, etc., thus directly providing suggestions as to
phonologically regular changes between two texts,
with frequency information included.
With such information about word pairs we gen-
erate a variety of replacement rules which are then
compiled into finite transducers with the foma ap-
plication. Even though the lexdiff program provides
a direct string-to-string change in a format that is
directly compilable into a phonological rule trans-
ducer, we have experimented with some possible
variations of the specific type of phonological rule
we want to output:
? We can restrict the rules by frequency and re-
quire that a certain type of change be seen at
least n times in order to apply that rule. For
example, if we set this threshold to 3, we will
only apply a string-to-string changing rule that
has been seen three or more times.
? We limit the number of rules that can be
applied to the same word. Sometimes the
lexdiff application divides the change be-
tween a pair of words into two separate rules.
For example the word-word correspondence
agerkuntza/agerpena is expressed by two rules:
rkun -> rpen and ntza -> na. Now,
given these two rules, we have to be able to
apply both to produce the correct total change
42
Figure 1: The role of the standard Basque (Batua) ana-
lyzer in filtering out unwanted output candidates created
by the induced rule set produced by method 1.
agerkuntza/agerpena. By limiting the number
of rules that can apply to a single input word we
can avoid creating many spurious outputs, but
also at the same time we may sacrifice some
ability to produce the desired output forms.
? We can also control the application mode of the
rules: sequential or parallel. If the previous
two rules are applied in parallel, the form ob-
tained from agerkuntza will not be correct
since the n overlaps with the two rules. That
is, when applying rules simultaneously in par-
allel, the input characters for two rules may not
overlap. However, if these two rules applied
in sequence (the order in this example is irrel-
evant), the output will be the correct: we first
change rkun -> rpen and later ntza ->
na. We have not a priori chosen to use parallel
or sequential rules and have decided to evaluate
both approaches.
? We can also compact the rules output by lex-
diff by eliminating redundancies and construct-
ing context-sensitive rules. For example: given
a rule such as rkun -> rpen, we can con-
vert this into a context-sensitive rule that only
changes ku into pe when flanked by r and n
to the left and right, respectively, i.e. producing
a rule:
k u -> p e || r n (5)
This has a bearing on the previous point and
will allow more rewritings within a single word
in parallel replacement mode since there are
fewer characters overlapping.
Once a set of rules is compiled with some instanti-
ation of the various parameters discussed above and
converted to a transducer, we modify the transducer
in various ways to improve on the output.
First, since we already have access to a large-scale
morphological transducer that models the standard
Basque (Batua), we restrict the output from the con-
version transducer to only allow those words as out-
put that are legitimate words in standard Basque.
Figure 1 illustrates this idea. In that figure, we see an
input word in the dialect (emaiten) produce a num-
ber of candidates using the rules induced. However,
after adding a morphological filter that models the
Batua, we retain only one output.
Secondly, in the case that even after applying
the Batua filter we retain multiple outputs, we sim-
ply choose the most frequent word (these unigram
counts are gathered from a separate newspaper cor-
pus of standard Basque).
4.3 Method 2 (ILP) details
The second method we have employed works
directly from a collection of word-pairs (di-
alect/standard in this case). We have developed an
algorithm that from a collection of such pairs seeks
a minimal hypothesis in the form of a set of replace-
ment rules that is consistent with all the changes
found in the training data. This approach is gener-
ally in line with ILP-based machine learning meth-
ods (Muggleton and De Raedt, 1994). However, in
contrast to the standard ILP, we do not learn state-
ments of first-order logic that fit a collection of data,
but rather, string-to-string replacement rules.4
4Phonological string-to-string replacement rules can be de-
fined as collections of statements in first-order logic and com-
piled into transducers through such logical statements as well;
43
The two parameters to be induced are (1) the col-
lection of string replacements X ? Y needed to
characterize the training data, and (2) the minimal
conditioning environments for each rule, such that
the collection of rules model the string transforma-
tions found in the training data.
The procedure employed for the learning task is
as follows:
(1) Align all word pairs (using minimum edit dis-
tance by default).
(2) Extract a collection of phonological rewrite
rules.
(3) For each rule, find counterexamples.
(4) For each rule, find the shortest conditioning en-
vironment such that the rule applies to all pos-
itive examples, and none of the negative exam-
ples. Restrict rule to be triggered only in this
environment.
The following simple example should illustrate
the method. Assuming we have a corpus of only
two word pairs:
emaiten ematen
igorri igorri
in step (1) we would perform the alignment and pro-
duce the output
e m a i t e n i g o r r i
e m a ? t e n i g o r r i
From this data we would in step (2) gather that
the only active phonological rule is i ? ?, since
all other symbols are unchanged in the data. How-
ever, we find two counterexamples to this rule (step
3), namely two i-symbols in igorri which do not al-
ternate with ?. The shortest conditioning environ-
ment that accurately models the data and produces
no overgeneration (does not apply to any of the is in
igorri) is therefore:
i -> ? || a (6)
see e.g. Hulden (2009b) for details. In other words, in this
work, we skip the intermediate step of defining our observa-
tions as logical statements and directly convert our observations
into phonological replacement rules.
the length of the conditioning environment being 1
(1 symbol needs to be seen to the left plus zero sym-
bols to the right). Naturally, in this example we have
two competing alternatives to the shortest general-
ization: we could also have chosen to condition the
i-deletion rule by the t that follows the i. Both con-
ditioning environments are exactly one symbol long.
To resolve such cases, we a priori choose to favor
conditioning environments that extend farther to the
left. This is an arbitrary decision?albeit one that
does have some support from phonology as most
phonological assimilation rules are conditioned by
previously heard segments?and very similar results
are obtained regardless of left/right bias in the learn-
ing. Also, all the rules learned with this method are
applied simultaneously (in parallel) in the evaluation
phase.
4.3.1 String-to-string vs. single-symbol rules
In some cases several consecutive input symbols
fail to correspond to the output in the learning data,
as in for example the pairing
d a u t
d i ? t
corresponding to the dialect-standard pair daut/dit.
Since there is no requirement in our formalism of
rewrite rules that they be restricted to single-symbol
rewrites only, there are two ways to handle this: ei-
ther one can create a string-to-string rewriting rule:
au? i / CONTEXT
or create two separate rules
a? i / CONTEXT , u? ? / CONTEXT
where CONTEXT refers to the minimal condition-
ing environment determined by the rest of the data.
We have evaluated both choices, and there is no no-
table difference between them in the final results.
5 Evaluation
We have measured the quality of different ap-
proaches by the usual parameters of precision, re-
call and the harmonic combination of them, the F1-
score, and analyzed how the different options in the
two approaches affect the results of these three pa-
rameters. Given that we, especially in method 1,
extract quite a large number of rules and that each
44
input word generates a very large number of candi-
dates if we use all the rules extracted, it is possible to
produce a high recall on the conversion of unknown
dialect words to the standard form. However, the
downside is that this naturally leads to low precision
as well, which we try to control by introducing a
number of filters to remove some of the candidates
output by the rules. As mentioned above, we use
two filters: (1) an obligatory filter which removes
all candidate words that are not found in the stan-
dard Basque (by using an existing standard Basque
morphological analyzer), and (2) using an optional
filter which, given several candidates in the standard
Basque, picks the most frequently occurring one by
a unigram count from the separate newspaper cor-
pus. This latter filter turns out to serve a much more
prominent role in improving the results of method 1,
while it is almost completely negligible for method
2.
6 Results
As mentioned above, the learning process has made
use of 80% of the corpus, leaving 20% of the corpus
for evaluation of the above-mentioned approaches.
In the evaluation, we have only tested those words
in the dialect that differ from words in the standard
(which are in the minority). In total, in the evalu-
ation part, we have tested the 301 words that differ
between the dialect and the standard in the evalua-
tion part of the corpus.
The results for the baseline?i.e. simple memo-
rization of word-word correspondences?are (in %):
P = 95.62, R = 43.52 and F1 = 59.82. As ex-
pected, the precision of the baseline is high: when
the method gives an answer it is usually the correct
one. But the recall of the baseline is low, as is ex-
pected: slightly less than half the words in the eval-
uation corpus have been encountered before.5
6.1 Results with the lexdiff method
Table 2 shows the initial experiment of method
1 with different variations on the frequency
5The reason the baseline does not show 100% precision is
that the corpus contains minor inconsistencies or accepted al-
ternative spellings, and our method of measuring the precision
suffers from such examples by providing both learned alterna-
tives to a dialectal word, while only one is counted as being
correct.
P R F1
f ? 1 38.95 66.78 49.20
f ? 2 46.99 57.14 51.57
f ? 3 49.39 53.82 51.51
Table 2: Values obtained for Precision, Recall and F-
scores with method 1 by changing the minimum fre-
quency of the correspondences to construct rules for
foma. The rest of the options are the same in all three
experiments: only one rule is applied within a word.
P R F1
f ? 1 70.28 58.13 63.64
f ? 2 70.18 53.16 60.49
f ? 3 71.76 51.50 59.96
Table 3: Values obtained for Precision, Recall and F-
score with method 1 by changing the threshold frequency
of the correspondences and applying a post-filter.
threshold?this is the limit on the number of times
we must see a string-change to learn it. The re-
sults clearly show that the more examples we extract
(frequency 1), the better results we obtain for recall
while at the same time the precision suffers since
many spurious outputs are given?even many differ-
ent ones that each legitimately correspond to a word
in the standard dialect. The F1-score doesn?t vary
very much and it maintains similar values through-
out. The problem with this approach is one which
we have noted before: the rules produce a large
number of outputs for any given input word and
the consequence is that the precision suffers, even
though only those output words are retained that cor-
respond to actual standard Basque.
With the additional unigram filter in place, the
results improve markedly. The unigram-filtered re-
sults are given in table 3.
We have also varied the maximum number of
possible rule applications within a single word as
well as applying the rules in parallel or sequentially,
and compacting the rules to provide more context-
sensitivity. We shall here limit ourselves to present-
ing the best results of all these options in terms of
the F1-score in table 4.
In general, we may note that applying more than
45
P R F1
Exp1 72.20 57.81 64.21
Exp2 72.13 58.47 64.59
Exp3 75.10 60.13 66.79
Table 4: Method 1. Exp1: frequency 2; 2 rules applied;
in parallel; without contextual conditioning. Exp2: fre-
quency 1; 1 rule applied; with contextual conditioning.
Exp3: frequency 2; 2 rules applied; in parallel; with con-
textual conditioning.
one rule within a word has a negative effect on
the precision while not substantially improving the
recall. Applying the unigram filter?choosing the
most frequent candidate?yields a significant im-
provement: much better precision but also slightly
worse recall. Choosing either parallel or sequential
application of rules (when more than one rule is ap-
plied to a word) does not change the results signifi-
cantly. Finally, compacting the rules and producing
context-sensitive ones is clearly the best option.
In all cases the F1-score improves if the unigram
filter is applied; sometimes significantly and some-
times only slightly. All the results of the table 4
which lists the best performing ones come from ex-
periments where the unigram filter was applied.
Figure 2 shows how precision and recall val-
ues change in some of the experiments done with
method 1. There are two different groups of points
depending on if the unigram filter is applied, illus-
trating the tradeoff in precision and recall.
6.2 Results with the ILP method
The ILP-based results are clearly better overall, and
it appears that the gain in recall by using method
1 does not produce F1-scores above those produced
with the ILP-method, irrespective of the frequency
filters applied. Crucially, the negative evidence
and subsequent narrowness of the replacement rules
learned with the ILP method is responsible for the
higher accuracy. Also, the results from the ILP-
based method rely very little on the post-processing
filters, as will be seen.
The only variable parameter with the ILP method
concerns how many times a word-pair must be seen
to be used as learning evidence for creating a re-
placement rule. As expected, the strongest result
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
Re
ca
ll
Precision
P vs R
without filter
with filter
Figure 2: Tradeoffs of precision and recall values in the
experiments with method 1 using various different pa-
rameters. When the unigram filter is applied the precision
is much better, but the recall drops.
P R F1
n = 1 85.02 (86.13) 58.47 (57.80) 69.29 (69.18)
n = 2 82.33 (83.42) 54.15 (53.49) 65.33 (65.18)
n = 3 80.53 (82.07) 50.83 (50.17) 62.32 (62.26)
n = 4 81.19 (82.32) 50.17 (49.50) 62.01 (61.83)
Table 5: Experiments with the ILP method using a thresh-
old of 1?4 (times a word-pair is seen) to trigger rule learn-
ing. The figures in parentheses are the same results with
the added postprocessing unigram filter that, given sev-
eral output candidates of the standard dialect, chooses the
most frequent one.
is obtained by using all word-pairs, i.e. setting the
threshold to 1. Table 5 shows the degradation of per-
formance resulting from using higher thresholds.
Interestingly, adding the unigram filter that im-
proved results markedly in method 1 to the output of
the ILP method slightly worsens the results in most
cases, and gives no discernible advantage in others.
In other words, in those cases where the method pro-
vides multiple outputs, choosing the most frequent
one on a unigram frequency basis gives no improve-
ment over not doing so.
Additionally, there is comparatively little advan-
tage with this method in adding the morphological
filter to the output of the words in method 2 (this
is the filter that rules out non-standard words). The
results in table 5 include the morphological filter,
but omitting it altogether brings down the best F1
46
P R F1
Baseline 95.62 43.52 59.82
Method 1 (lexdiff) 75.10 60.13 66.79
Method 2 (ILP) 85.02 58.47 69.29
Table 6: The best results (per F1-score of the two meth-
ods). The parameters of method 1 included using only
those string transformations that occur at least 2 times in
the training data, and limiting rule application to a maxi-
mum of 2 times within a word, and including a unigram
post-filter. Rules were contextually conditioned. For
method 2, all the examples (threshold 1) in the training
data were used as positive and negative evidence, with-
out a unigram filter.
to 56.14 from 69.29. By contrast, method 1 de-
pends heavily on it and omitting the filter brings
down the F1-score from 66.79 to 11.53 with the
otherwise strongest result of method 1 seen in ta-
ble 6. The most prominent difference between the
two approaches is that while method 1 can be fine-
tuned using frequency information and various fil-
ters to yield results close to method 2, the ILP ap-
proach provides equally robust results without any
additional information?in particular, frequency in-
formation of the target language. We also find a
much lower rate of errors of commission with the
ILP method; this is somewhat obvious as it takes ad-
vantage of negative evidence directly while the first
method only does so indirectly through filters added
later.
7 Conclusions and future work
We have presented a number of experiments to solve
a very concrete task: given a word in the Lapurdian
dialect of Basque, produce the equivalent standard
Basque word. As background knowledge, we have
a complete standard Basque morphological analyzer
and a small parallel corpus of dialect and standard
text. The approach has been based on the idea of
extracting string-to-string transformation rules from
the parallel corpus, and applying these rules to un-
seen words. We have been able to improve on the
results of a naive baseline using two methods to in-
fer phonological rules of the information extracted
from the corpus and applying them with finite state
transducers. In particular, the second method, in-
ferring minimal phonological rewrite rules using
an Inductive Logic Programming-style approach,
seems promising as regards inferring phonological
and morphological differences that are quite regu-
lar in nature between the two language variants. We
expect that a larger parallel corpus in conjunction
with this method could potentially improve the re-
sults substantially?with a larger set of data, thresh-
olds could be set so that morphophonological gener-
alizations are triggered only after a sufficient num-
ber of training examples (avoiding overgeneration),
and, naturally, many more unique, non-regular, lexi-
cal correspondences could be learned.
During the current work, we have also accumu-
lated a small but valuable training and test corpus
which may serve as a future resource for evaluation
of phonological and morphological rule induction
algorithms.
In order to improve the results, we plan to re-
search the combination of the previous methods with
other ones which infer dialectal paradigms and rela-
tions between lemmas and morphemes for the di-
alect and the standard. These inferred relations
could be contrasted with the information of a larger
corpus of the dialect without using an additional par-
allel corpus.
Acknowledgments
We are grateful for the insightful comments
provided by the anonymous reviewers. This re-
search has been partially funded by the Spanish
Science and Innovation Ministry via the OpenMT2
project (TIN2009-14675-C03-01) and the European
Commission?s 7th Framework Program under grant
agreement no. 238405 (CLARA).
References
Alegria, I., Aranzabe, M., Arregi, X., Artola, X.,
D??az de Ilarraza, A., Mayor, A., and Sarasola, K.
(2011). Valuable language resources and applica-
tions supporting the use of Basque. In Vetulani,
Z., editor, Lecture Notes in Artifitial Intelligence,
volume 6562, pages 327?338. Springer.
Alegria, I., Aranzabe, M., Ezeiza, N., Ezeiza, A.,
and Urizar, R. (2002). Using finite state tech-
nology in natural language processing of basque.
47
In LNCS: Implementation and Application of Au-
tomata, volume 2494, pages 1?12. Springer.
Almeida, J. J., Santos, A., and Simoes, A.
(2010). Bigorna?a toolkit for orthography migra-
tion challenges. In Seventh International Con-
ference on Language Resources and Evaluation
(LREC2010), Valletta, Malta.
Beesley, K. R. and Karttunen, L. (2002). Finite-state
morphology: Xerox tools and techniques. Stud-
ies in Natural Language Processing. Cambridge
University Press.
Hulden, M. (2009a). Foma: a finite-state compiler
and library. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association
for Computational Linguistics: Demonstrations
Session, pages 29?32, Athens, Greece. Associa-
tion for Computational Linguistics.
Hulden, M. (2009b). Regular expressions and pred-
icate logic in finite-state language processing. In
Piskorski, J., Watson, B., and Yli-Jyra?, A., edi-
tors, Finite-State Methods and Natural Language
Processing?Post-proceedings of the 7th Interna-
tional Workshop FSMNLP 2008, volume 191 of
Frontiers in Artificial Intelligence and Applica-
tions, pages 82?97. IOS Press.
Johnson, M. (1984). A discovery procedure for cer-
tain phonological rules. In Proceedings of the
10th international conference on Computational
linguistics, COLING ?84, pages 344?347. Asso-
ciation for Computational Linguistics.
Kestemont, M., Daelemans, W., and Pauw, G. D.
(2010). Weigh your words?memory-based
lemmatization for Middle Dutch. Literary and
Linguistic Computing, 25(3):287?301.
Koskenniemi, K. (1991). A discovery procedure for
two-level phonology. Computational Lexicology
and Lexicography: A Special Issue Dedicated to
Bernard Quemada, pages 451?446.
Mann, G. S. and Yarowsky, D. (2001). Multi-
path translation lexicon induction via bridge lan-
guages. In Proceedings of the second meeting of
the North American Chapter of the Association
for Computational Linguistics on Language tech-
nologies, NAACL ?01, pages 1?8.
Muggleton, S. and De Raedt, L. (1994). Inductive
Logic Programming: theory and methods. The
Journal of Logic Programming, 19:629?679.
Scherrer, Y. (2007). Adaptive string distance mea-
sures for bilingual dialect lexicon induction. In
Proceedings of the 45th Annual Meeting of the
ACL: Student Research Workshop, ACL ?07,
pages 55?60. Association for Computational Lin-
guistics.
48
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 13?17,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
BAD: An assistant tool for making verses in Basque
Manex Agirrezabal, In?aki Alegria, Bertol Arrieta
University of the Basque Country (UPV/EHU)
maguirrezaba008@ikasle.ehu.es, i.alegria@ehu.es, bertol@ehu.es
Mans Hulden
Ikerbasque (Basque Science Foundation)
mhulden@email.arizona.edu
Abstract
We present work on a verse-composition
assistant for composing, checking correct-
ness of, and singing traditional Basque
bertsoak?impromptu verses on particular
themes. A performing bertsolari?a verse
singer in the Basque Country?must ad-
here to strict rules that dictate the format
and content of the verses sung. To help
the aspiring bertsolari, we provide a tool
that includes a web interface that is able
to analyze, correct, provide suggestions and
synonyms, and tentatively also sing (using
text-to-speech synthesis) verses composed
by the user.
1 Introduction
In the Basque Country there exists a long-
standing live performance tradition of improvis-
ing verses?a type of ex tempore composition and
singing called bertsolaritza. Verses in bertsolar-
itza can be seen as discourses with strict rules
governing the technical structure of them: verses
must contain a certain number of lines and each
line must have a defined number of syllables, cer-
tain lines have to rhyme in certain patterns, and so
forth.
In this paper we present a web-based assistant
tool for constructing verses (bertsoak) according
to the rules of bertsolaritza (Garzia et al 2001).
If the reader is interested in this topic, we rec-
ommend watching the 2011 film Bertsolari1 2, di-
rected by Asier Altuna.
1IMDB: http://www.imdb.com/title/tt2058583
2Trailer on: http://vimeo.com/9355066
2 Relationship to earlier work
There exist some prior works dealing with Basque
verse-making and computer technologies, such as
BertsolariXa (Arrieta et al, 2001), which is a
rhyme search tool implemented as finite-state au-
tomata using the two-level morphology formal-
ism. The tool also contains other features, includ-
ing semantic categorization of words, narrowing
word-searches to certain themes, etc. While Bert-
solariXa focuses mostly on the word-level, the
current work also includes constraints on over-
all verse structure in its implementation as well
as a synonym search tool, a melody suggestion
system, and possibilities for plugging in text-to-
speech synthesis of verses.
2.1 The Bertsolari tradition
Bertsolaritza is very ingrained in the Basque
Country and championships, competitions and
get-togethers on bertsolaritza are quite common.
Usually the competitors in such event, called bert-
solaris, are given a theme to produce a verse on
under some very limited time constraints.
But the Basque Country is not the only place
that hosts such troubadour traditions?similar
customs are present in many other countries such
as Cuba, Brazil, Argentina, etc. The goal of the
current tool is to be generalizable, and so appli-
cable to various strategies of verse improvisation,
and possibly be useful not only for Basque speak-
ers, but also for others.
Below we briefly present an example of a verse
made in the Basque Country. In 1986 Andoni
Egan?a (a well-known bertsolari) was asked to
sing a bertso and assigned a topic. In the verse,
he was asked to play the role of an old person
who lived alone, and who realized that he could
13
not even tie his shoes. Within a few seconds he
composed and sang three verses. Here, we ana-
lyze the first verse.
Verse:
Gazte aroan ibili arren
gustora tirriki-tarra,
denbora honen joan etorriak
ederki jo dit gitarra,
gorputza daukat ximeldurikan
ta eskuen punta zaharra,
denborarekin seko galdu det
gazte aroko indarra,
ez al da pena gizon mardul bat
hola ibili beharra.
Translation:
Even when I was young
I was always on a spree
over time
I have been punished
I have a crumpled body
and the tip of the hands very old,
Over time I lost
the strength I had when I was young,
It?s a shame that a strong man
has to end up like me.
The special charm of bertsolaritza improvi-
sation is that people proficient in the art can
quickly express a variety of ideas, although they
are working with very restrictive rules concern-
ing the number of syllables in words they use,
and how the words must rhyme. We must take
into account that Andoni Egan?a was able to sing
this verse within a few seconds of being given
the topic, and also, that it complies exactly with
a certain metric. In this case, the verse contains
eight lines, each odd line consisting of ten sylla-
bles, and each even line of eight syllables, with
the even lines rhyming.
Formal training in the bertsolari tradition also
exists in the Basque Country. In the last 20 to
30 years, an important movement has developed
that aims to provide instruction to upcoming gen-
erations on how to create verses (orally or in
writing). This kind of instruction usually takes
place in learning centers called bertso-eskolak,
which in English roughly means, ?verse-making
schools.? The proliferation of this movement has
produced a strong base of young bertsolaris, of
whom many achieve an outstanding level of im-
provisation skills.
3 The BAD tool
BAD is the acronym for ?Bertsotarako Arbel Dig-
itala?, roughly ?Digital verse board.? The aim
of the tool is to serve as a general assistant for
bertsolari-style verse composition and help verse-
making learners in their learning process.
This tool has been developed using the PHP
programming language, but it contains certain
parts developed using finite-state technology. The
main functions of this tool, which will be dis-
cussed in more detail in the next five sections, are
the following: visualization of the verse structure,
structure checking, rhyme and synonym searching
and verse singing.
3.1 Verse structure
The main rules of the bertsolari verse are that a
verse must consist of a certain predefined number
of lines and each line in turn, of a predefined num-
ber of syllables. Traditionally, about a hundred
different schemes are used, and the tool provides
support for all these patterns. For example, the
structure called ?Hamarreko handia? has ten lines
and ten syllables in the odd-numbered lines, and
eight syllables in the even-numbered lines. In this
structure, the even-numbered lines have to rhyme.
Selecting this scheme, the tool will mark the cor-
responding lines with their requirements.
The web interface can be seen in figure 1,
which shows the general layout of the tool, illus-
trated with the example verse referred to above?
we see that each line has been approved in terms
of line length and syllable structure by the tool.
We have designed a database in which the main
verse structures are saved so that when the user se-
lects one verse schema, the system knows exactly
the number of lines it must contain, where must
it rhyme and how many syllables each line should
have. Those schemata are also linked to melodies,
each melody corresponding to one possible struc-
ture.
3.2 Structure checking
After writing the verse, the system can evaluate if
it is technically correct, i.e. if the overall structure
is correct and if each line in the form abides by the
required syllable count and rhyming scheme. The
syllable counter is implemented using the foma
software (Hulden, 2009), and the implementation
(Hulden, 2006) can be found on the homepage of
14
Figure 1: A verse written in the BAD web application.
foma.3
Separately, we have also developed a rhyme
checker, which extracts special patterns in the
lines that must rhyme and checks their confor-
mity.
These patterns are extracted using foma (see
section 3.4) after which some phonological rules
are applied. For example, an example rule era
? {era, eda, ega, eba}, models the fact that any
word ending in era, for example, etxera, will
rhyme with all words that end in era, eda, eba or
ega. These rhyming patterns have been extracted
according to the phonological laws described in
(Amuriza, 1981).
3.3 Synonym search
Usually, people who write verses tend to quickly
exhaust their vocabulary and ideas with to ex-
press what they want to say, or encounter prob-
lems with the number of syllables in various ten-
tative words they have in mind. For example,
if the verse-maker wants to say something con-
taining the word ?family,? (familia in Euskera, a
four-syllable word) but is forced to use a three-
syllable word in a particular context, the inter-
face provides for possibilities to look for three-
syllable synonyms of the word familia, producing
the word sendia? a word whose meaning is oth-
erwise the same, and made up of three syllables.
For developing the synonym search, we used a
modified version of the Basque Wordnet (Pociello
3http://foma.googlecode.com
et al, 2010), originally developed by the IXA
group at the University of the Basque Country.
Within Wordnet we search the synsets for the in-
coming word, and the words that correspond to
those synsets are returned.
3.4 Rhyme search
The classical and most well-known problem in
bertsolaritza concern the rhyming patterns. As
mentioned, various lines within a verse are re-
quired to rhyme, according to certain predefined
schemata. To search for words that rhyme with
other words in a verse, the BAD tool contains a
rhyme search engine. In the interface, this is lo-
cated in the right part of the BAD tool main view,
as seen in figure 2.
The rhyme searcher is built upon finite-state
technology, commonly used for developing mor-
phological and phonological analyzers, and calls
upon the freely available foma-tool, to calculate
matching and nonmatching rhyme schemes.
Its grammar is made up of regular expressions
that are used to identify phonological patterns in
final syllables in the input word. The result of
the search is the intersection of these patterns and
all the words generated from a morphological de-
scription of Basque (Alegria et al, 1996)?that
is, a list of all words that match both the required
phonological constraints given (rhyming) and a
morphological description of Basque.
Based upon figure 2, if we search rhymes for
the word landa (cottage), the system proposes a
15
Figure 2: The response of the rhyme search engine.
set of words that can be filtered depending on the
number of syllables required. Among this list of
words, we can find some words that end in anda,
such as, Irlanda (Ireland) or eztanda (explosion),
but through the application of phonological equiv-
alency rules we also find terms like ganga (vault).
3.5 Singing synthesis
Another characteristic, as mentioned, is that, in
the end, the verses are intended to be sung in-
stead of only being textually represented. Based
on other ongoing work in singing synthesis, we
have designed a system for singing the verses en-
tered into the system in Basque.
This is based on the ?singing mode? of the Fes-
tival text-to-speech system (Taylor et al, 1998).
The advantage of using this is that Festival is
open-source and has given us ample opportunities
to modify its behavior. However, as Festival does
not currently support Basque directly, we have re-
lied on the Spanish support of the Festival sys-
tem.4
4While morphologically and syntactically, Spanish and
Basque have no relationship whatsoever, phonetically the
languages are quite close, with only a few phonemes, syl-
Based on current work by the Aholab research
team in Bilbao?a lab that works on Basque
speech synthesis and recognition?we have im-
plemented a singing module for BAD, based on
the text-to-speech HTS engine (Erro et al, 2010).
Our application is able to sing the composed
verses entered into the system in Basque, with a
choice of various standard melodies for bertsolar-
itza.5
4 Discussion and future work
Now that the BAD tool has been developed, our
intention is to evaluate it. To make a qualita-
tive evaluation we have gotten in touch with some
verse-making schools (bertso-eskola), so that they
can test the system and send us their feedback us-
ing a form. Once the evaluation is made, we will
improve it according to the feedback and the sys-
tem will be made public.
Our ultimate goal is to develop a system able to
create verses automatically. To achieve this long-
term goal, there is plenty of work to do and ba-
sic research to be done. We have in our hands a
good corpus of 3,500 Basque verse transcriptions,
so we intend to study these verses from a mor-
phological, syntactical, semantical and pragmatic
point of view.
In the short term, we also plan to expand the
synonym search to be able to provide searches
for semantically related words and subjects (and
not just synonyms), like hypernyms or hyponyms.
The Basque WordNet provides a good opportu-
nity for this, as one is easily able to traverse the
WordNet to encounter words with varying degrees
of semantic similarity.
Another feature that we want to develop is a
system that receives as input a verse together with
a MIDI file, and where the system automatically
sings the verse to the music provided.
Finally, in order for the system to be
able to provide better proposals for the verse
artist?including perhaps humorous and creative
proposals?we intend to work with approaches
to computational creativity. We are considering
different approaches to this topic, such as in the
work on Hahacronym (Stock et al, 2005) or the
Standup riddle builder (Ritchie et al, 2001).
labification rules, and stress rules being different enough to
disturb the system?s behavior.
5However, this functionality is not available on the web
interface as of yet.
16
Figure 3: The BAD application before entering a verse, showing two possible rhyme patterns.
Acknowledgments
This research has been partially funded by the
Basque Government (Research Groups, IT344-
10).
References
In?aki Alegria, Xabier Artola, Kepa Sarasola and
Miriam Urkia, ?Automatic morphological analysis
of Basque?, Literary and Linguistic Computing,
ALLC, 1996
Xabier Amuriza, ?Hiztegi errimatua?, Alfabetatze Eu-
skalduntze Koordinakundea, 1981
Bertol Arrieta, In?aki Alegria, Xabier Arregi, ?An as-
sistant tool for Verse-Making in Basque based on
Two-Level Morphology?, 2001
Daniel Erro, In?aki Sainz, Ibon Saratxaga, Eva Navas,
Inma Herna?ez, ?MFCC+F0 Extraction and Wave-
form Reconstruction using HNM: Preliminary Re-
sults in an HMM-based Synthesizer?, 2010
Joxerra Garzia, Jon Sarasua, and Andoni Egan?a,
?The art of bertsolaritza: improvised Basque verse
singing?, Bertsolari liburuak, 2001
John E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman,
?Introduccio?n a la teor??a de Auto?matas, Lenguajes
y Computacio?n?, Pearson educacio?n, 2002
Mans Hulden, ?Foma: a finite-state compiler and li-
brary?, Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Demonstrations Session, p. 29?
32, 2009
Mans Hulden, ?Finite-state syllabification?, Finite-
State Methods and Natural Language Processing, p.
86 ? 96, Springer, 2006
Kimmo Koskenniemi. ?Two-level morphology: A
general computational model for word-form recog-
nition and production?. Publication 11, University
of Helsinki, Department of General Linguistics,
Helsinki, 1983.
Eli Pociello, Eneko Agirre, Izaskun Aldezabal,
?Methodology and construction of the Basque
WordNet?, 2010
Graeme Ritchie, ?Current directions in computational
humour?, Artificial Intelligence Review, Volume
16, Number 2, p. 119 ? 135, Springer, 2001
Graeme Ritchie, Ruli Manurung, Helen Pain, Annalu
Waller, Dave O?Mara, ?The STANDUP interactive
riddle builder?, Volume 2, Number 2, p. 67 ? 69,
IEEE Intelligent Systems, 2006
Oliviero Stock and Carlo Strapparava, ?Hahacronym:
A computational humor system?, Proceedings of
the ACL 2005 on Interactive poster and demonstra-
tion sessions, p. 113 ? 116, Association for Compu-
tational Linguistics, 2005
Paul Taylor, Alan W. Black and Richard Caley, ?The
architecture of the Festival speech synthesis sys-
tem?, International Speech Communication Asso-
ciation, 1998
17
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 35?39,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Finite-state technology in a verse-making tool
Manex Agirrezabal, In?aki Alegria, Bertol Arrieta
University of the Basque Country (UPV/EHU)
maguirrezaba008@ikasle.ehu.es, i.alegria@ehu.es, bertol@ehu.es
Mans Hulden
Ikerbasque (Basque Science Foundation)
mhulden@email.arizona.edu
Abstract
This paper presents a set of tools designed to
assist traditional Basque verse writers during
the composition process. In this article we
are going to focus on the parts that have been
created using finite-state technology: this in-
cludes tools such as syllable counters, rhyme
checkers and a rhyme search utility.
1 The BAD tool and the Basque singing
tradition
The BAD tool is an assistant tool for verse-makers
in the Basque bertsolari tradition. This is a form
of improvised verse composition and singing where
participants are asked to produce impromptu com-
positions around themes which are given to them
following one of many alternative verse formats.
The variety of verse schemata that exist all impose
fairly strict structural requirements on the composer.
Verses in the bertsolari tradition must consist of a
specified number of lines, each with a fixed num-
ber of syllables. Also, strict rhyme patterns must
be followed. The structural requirements are con-
sidered the most difficult element in the bertsolar-
itza?however, well-trained bertsolaris can usually
produce verses that fulfill the structural prerequisites
in a very limited time.
The BAD tool presented here is mainly di-
rected at those with less experience in the tradi-
tion such as students. One particular target group
are the bertso-eskola-s (verse-making schools) that
have been growing in popularity?these are schools
found throughout the Basque Country that train
young people in the art of bertsolaritza.
The primary functionality of the tool is illustrated
in figure 1 which shows the main view of the util-
ity. The user is offered a form in which a verse
can be written, after which the system checks the
technical correctness of the poem. To perform this
task, several finite state transducer-based modules,
are used, some of them involving the metrics (syl-
lable counter) of the verse, and others the rhyme
(rhyme searcher and checker). The tool has support
for 150 well known verse meters.
In the following sections, we will outline the tech-
nology used in each of the parts in the system.
2 Related work
Much of the existing technology for Basque mor-
phology and phonology uses finite-state technology,
including earlier work on rhyme patterns (Arrieta
et al, 2001). In our work, we have used the Basque
morphological description (Alegria et al, 1996) in
the rhyme search module. Arrieta et al (2001) de-
velop a system where, among other things, users can
search for words that rhyme with an introduced pat-
tern. It is implemented in the formalism of two-level
morphology (Koskenniemi, 1983) and compiled into
finite-state transducers.
We have used the open-source foma finite-state
compiler to develop all the finite-state based parts
of our tool.1. After compiling the transducers, we
use them in our own application through the C/C++
API provided with foma.
3 Syllable counter
As mentioned, each line in a verse must contain a
specified number of syllables. The syllable counter
module that checks whether this is the case consists
of a submodule that performs the syllabification it-
self as well as a module that yields variants produced
by optional apocope and syncope effects. For the
syllabification itself, we use the approach described
in Hulden (2006), with some modifications to cap-
ture Basque phonology.
1In our examples, FST expressions are written using foma
syntax. For details, visit http://foma.googlecode.com
35
Figure 1: A verse written in the BAD web application.
3.1 Syllabification
Basque syllables can be modeled by assuming a
maximum onset principle together with a sonority
hierarchy where obstruents are the least sonorous el-
ement, followed in sonority by the liquids, the nasals
and the glides. The syllable nuclei are always a sin-
gle vowel (a,e,i,o,u) or a combination of a low vowel
(a,e) and a high vowel (i,o,u) or a high vowel and an-
other high vowel.
The syllabifier relies on a chain of composed re-
placement rules (Beesley and Karttunen, 2003) com-
piled into finite-state transducers. These defini-
tions are shown in figure 2. The overall strategy
is to first mark off the nuclei in a word by the rule
MarkNuclei which takes advantage of a left-to-
right longest replacement rule. This is to ensure that
diphthongs do not get split into separate syllables
by the subsequent syllabification process. Follow-
ing this, syllables are marked off by the markSyll-
rule, which inserts periods after legitimate syllables.
This rule takes advantage of the shortest-leftmost re-
placement strategy?in effect minimizing the coda
and maximizing the size of the onset of a syllable to
the extent permitted by the allowed onsets and co-
das, defined in Onset and Coda, respectively.
To illustrate this process, supposing that we
are syllabifying the Basque word intransitiboa.
The first step in the syllabification process is
to mark the nuclei in the word, resulting in
{i}ntr{a}ns{i}t{i}b{o}{a}. In the more com-
plex syllabification step, the markSyll rule as-
sures that the juncture ntr gets divided as n.tr be-
cause nt.r would produce a non-maximal onset,
and i.ntr would in turn produce an illegal onset in
define Obs [f|h|j|k|p|s|t|t s|t z|t x|x|
z|b|d|g|v|d d|t t];
define LiqNasGli [l|r|r r|y|n|m];
define LowV [a|e|o];
define HighV [i|u];
define V LowV | HighV;
define Nucleus [V | LowV HighV |
[HighV HighV - [i i] - [u u]]];
define Onset (Obs) (LiqNasGli);
define Coda C?<4;
define MarkNuclei Nucleus @-> %{ ... %};
define Syll Onset %{ Nucleus %} Coda;
define markSyll Syll @> ... "." || _ Syll ;
define cleanUp %{|%} -> 0;
regex MarkNuclei .o. markSyll .o. cleanUp;
Figure 2: Syllable definition
the second syllable. The final syllabification, af-
ter markup removal by the Cleanup rule, is then
in.tran.si.ti.bo.a. This process is illustrated in fig-
ure 3
In bertsolaritza, Basque verse-makers follow this
type of syllable counting in the majority if cases;
however, there is some flexibility as regards the syl-
labification process. For example, suppose that the
phrase ta lehenengo urtian needs to fit a line which
must contain six syllables. If we count the sylla-
bles using the algorithm shown above, we receive a
count of eight (ta le.hen.en.go ur.ti.an). However,
in the word lehenengo we can identify the syncope
pattern vowel-h-vowel, with the two vowels being
identical. In such cases, we may simply replace
the entire sequence by a single vowel (ehe ? e).
This is phonetically equivalent to shortening the ehe-
sequence (for those dialects where the orthographi-
cal h is silent). With this modification, we can fit
36
the line in a 7 syllable structure. We can, however,
further reduce the line to 6 syllables by a second
type of process that merges the last syllable of one
word with the first of the next one and then resyl-
labifying. Hence, ta lehenengo urtian, using the
modifications explained above, could be reduced to
ta.le.nen.gour.ti.an, which would fit the 6 syllable
structure. This production of syllabification variants
is shown in figure 4.
transformazioei
tr{a}nsf{o}rm{a}z{i}{o}{ei}
markNuclei
syllabify
tr{a}ns.f{o}r.m{a}.z{i}.{o}.{ei}
cleanUp
trans.for.ma.zi.o.ei
Figure 3: Normal syllabification.
trarnsfomn
trzarnzsfzomn
marrkNuculkeuis
kreybskeym
trzarnzsfzomn trnzsfzomn
tratnsftronnm
tzratznsftzroznnm
marrkNuculkeuis
kreybskeym
tzratznsftzroznnm tzratznstzroznnm
Figure 4: Flexible syllabification.
4 Finite-state technology for rhymes
4.1 Basque rhyme patterns and rules
Similar to the flexibility in syllabification, Basque
rhyme schemes also allows for a certain amount
of leeway that bertsolaris can take advantage of.
The widely consulted rhyming dictionary Hiztegi
Errimatua (Amuriza, 1981) contains documented a
number of phonological alternations that are accept-
able as off-rhymes: for example the stops p, t, and k
are often interchangeable, as are some other phono-
logical groups. Figure 5 illustrates the definitions
for interchangeable phonemes when rhyming. The
interchangeability is done as a prelude to rhyme
checking, whereby phonemes in certain groups,
such as p, are replaced by an abstract symbol de-
noting the group (e.g. PTK).
4.2 Rhyme checker
The rhyme checker itself in BAD was originally de-
veloped as a php-script, and then reimplemented as
define plosvl [p | t | k];
define rplosv [b | d | g | r];
define sib [s | z | x];
define nas [n | m];
define plosvlconv ptk -> PTK;
define rplosvconv bdgr -> BDGR;
define sibconv sib -> SZX;
define nasconv nas -> NM;
define phoRules plosvlconv .o. rplosvconv .o.
sibconv .o. nasconv ;
Figure 5: Conflation of consonant groups before rhyme
checking.
a purely finite-state system. In this section we will
focus on the finite-state based one.
As the php version takes advantage of syllabifica-
tion, the one developed with transducers does not.
Instead, it relies on a series of replacement rules and
the special eq() operator available in foma. An
implementation of this is given in figure 6. As input
to the system, the two words to be checked are as-
sumed to be provided one after the other, joined by
a hyphen. Then, the system (by rule rhympat1)
identifies the segments that do not participate in the
rhyme and marks them off with ?{? and ?}? symbols
(e.g. landa-ganga ? <{l}anda>-<{g}anga>).
The third rule (rhympat3) removes everything
that is between ?{? and ?}?, leaving us only with
the segments relevant for the rhyming pattern (e.g.
<anda>-<anga>). Subsequent to this rule, we
apply the phonological grouping reductions men-
tioned above in section 4.1, producing, for example
(<aNMBDGRa>-<aNMBDGRa>).
After this reduction, we use the eq(X,L,R)-
operator in foma, which from a transducer X, filters
out those words in the output where material be-
tween the specified delimiter symbols L and R are
unequal. In our case, we use the < and > symbols
as delimiters, yielding a final transducer that does
not accept non-rhyming words.
4.3 Rhyme search
The BAD tool also includes a component for search-
ing words that rhyme with a given word. It is devel-
oped in php and uses a finite-state component like-
wise developed with foma.
Similarly to the techniques previously described,
it relies on extracting the segments relevant to the
37
define rhympat1 [0:"{" ?* 0:"}"
[[[V+ C+] (V) V] | [(C) V V]] C* ];
# constraining V V C pattern
define rhympat2 ?[?* V "}" V C];
# cleaning non-rhyme part
define rhympat3 "{" ?* "}" -> 0;
define rhympat rhympat1 .o. rhympat2 .o.
rhympat3;
# rhyming pattern on each word
# and phonological changes
define MarkPattern rhympat .o.
phoRules .o. patroiak;
# verifying if elements between < and >
# are equal
define MarkTwoPatterns
0:%< MarkPattern 0:%> %-
0:%< MarkPattern 0:%> ;
define Verify _eq(MarkTwoPatterns, %<, %>)
regex Verify .o. Clean;
Figure 6: Rhyme checking using foma.
rhyme, after which phonological rules are applied
(as in 4.1) to yield phonetically related forms. For
example, introducing the pattern era, the system re-
turns four phonetically similar forms era, eda, ega,
and eba. Then, these responses are fed to a trans-
ducer that returns a list of words with the same end-
ings. To this end, we take advantage of a finite-state
morphological description of Basque (Alegria et al,
1996).
As this transducer returns a set of words which
may be very comprehensive?including words not
commonly used, or very long compounds?we then
apply a frequency-based filter to reduce the set of
possible rhymes. To construct the filter, we used
a newspaper corpus, (Egunkaria2) and extracted the
frequencies of each word form. Using the frequency
counts, we defined a transducer that returns a word?s
frequency, using which we can extract only the n-
most frequent candidates for rhymes. The system
also offers the possibility to limit the number of syl-
lables that desired rhyming words may contain. The
syllable filtering system and the frequency limiting
parts have been developed in php. Figure 7 shows
the principle of the rhyme search?s finite-state com-
ponent.
5 Evaluation
As we had available to us a rhyme checker written
in php before implementing the finite-state version,
2http://berria.info
regex phoRules .o. phoRules.i .o.
0:?* ?* .o. dictionary ;
Figure 7: Rhyme search using foma
it allowed for a comparison of the application speed
of each. We ran an experiment introducing 250,000
pairs of words to the two rhyme checkers and mea-
sured the time each system needed to reply. The
FST-based checker was roughly 25 times faster than
the one developed in php.
It is also important to mention that these tools
are going to be evaluated in an academic environ-
ment. As that evaluation has not been done yet, we
made another evaluation in our NLP group in or-
der to detect errors in terms of syllabification and
rhyme quality. The general feeling of the experiment
was that the BAD tool works well, but we had some
efficiency problems when many people worked to-
gether. To face this problem some tools are being
implemented as a server.
6 Discussion & Future work
Once the main tools of the BAD have been devel-
oped, we intend to focus on two different lines of
development. The first one is to extend to flexibil-
ity of rhyme checking. There are as of yet patterns
which are acceptable as rhymes to bertsolaris that
the system does not yet recognize. For example,
the words filma and errima will not be accepted by
the current system, as the two rhymes ilma and ima
are deemed to be incompatible. In reality, these two
words are acceptable as rhymes by bertsolaris, as
the l is not very phonetically prominent. However,
adding flexibility also involves controlling for over-
generation in rhymes. Other reduction patterns not
currently covered by the system include phenomena
such as synaloepha?omission of vowels at word
boundaries when one word ends and the next one
begins with a vowel.
Also, we intend to include a catalogue of melodies
in the system. These are traditional melodies that
usually go along with a specific meter. Some 3,000
melodies are catalogued (Dorronsoro, 1995). We are
also using the components described in this article in
another project whose aim is to construct a robot ca-
pable to find, generate and sing verses automatically.
38
Acknowledgments
This research has been partially funded by the Span-
ish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and partially funded by the
Basque Government (Research Groups, IT344-10).
We would like to acknowledge Aitzol Astigarraga
for his help in the development of this project. He
has been instrumental in our work, and we intend to
continue working with him. Also we must mention
the Association of Friends of Bertsolaritza, whose
verse corpora has been used to test and develop these
tools and to develop new ones.
References
Alegria, I., Artola, X., Sarasola, K., and Urkia,
M. (1996). Automatic morphological analysis
of Basque. Literary and Linguistic Computing,
11(4):193?203.
Amuriza, X. (1981). Hiztegi errimatua [Rhyme Dic-
tionary]. Alfabetatze Euskalduntze Koordinakun-
dea.
Arrieta, B., Alegria, I., and Arregi, X. (2001). An
assistant tool for verse-making in Basque based
on two-level morphology. Literary and linguistic
computing, 16(1):29?43.
Beesley, K. R. and Karttunen, L. (2003). Finite state
morphology. CSLI.
Dorronsoro, J. (1995). Bertso doinutegia [Verse
melodies repository]. Euskal Herriko Bertsolari
Elkartea.
Hulden, M. (2006). Finite-state syllabification.
Finite-State Methods and Natural Language Pro-
cessing, pages 86?96.
Koskenniemi, K. (1983). Two-level morphology:
A general computational model for word-form
production and generation. Publications of the
Department of General Linguistics, University of
Helsinki. Helsinki: University of Helsinki.
39
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 116?125,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Combining Different Features of Idiomaticity for the Automatic
Classification of Noun+Verb Expressions in Basque
Antton Gurrutxaga
Elhuyar Foundation
Zelai Haudi 3, Osinalde industrialdea
Usurbil 20170. Basque Country
a.gurrutxaga@elhuyar.com
In?aki Alegria
IXA group, Univ. of the Basque Country
Manuel Lardizabal 1
Donostia 20018. Basque Country
i.alegria@ehu.es
Abstract
We present an experimental study of how dif-
ferent features help measuring the idiomatic-
ity of noun+verb (NV) expressions in Basque.
After testing several techniques for quantify-
ing the four basic properties of multiword ex-
pressions or MWEs (institutionalization, se-
mantic non-compositionality, morphosyntac-
tic fixedness and lexical fixedness), we test
different combinations of them for classifica-
tion into idioms and collocations, using Ma-
chine Learning (ML) and feature selection.
The results show the major role of distribu-
tional similarity, which measures composi-
tionality, in the extraction and classification
of MWEs, especially, as expected, in the case
of idioms. Even though cooccurrence and
some aspects of morphosyntactic flexibility
contribute to this task in a more limited mea-
sure, ML experiments make benefit of these
sources of knowledge, allowing to improve
the results obtained using exclusively distribu-
tional similarity features.
1 Introduction
Idiomaticity is considered the defining feature of the
concept of multiword expressions (MWE). It is de-
scribed as a non-discrete magnitude, whose ?value?
depends on a combination of features like in-
stitutionalization, non-compositionality and lexico-
syntactic fixedness (Granger and Paquot, 2008).
Idiomaticity appears as a continuum rather than as
a series of discrete values. Thus, the classification of
MWEs into discrete categories is a difficult task. A
very schematic classification that has achieved a fair
degree of general acceptance among experts distin-
guishes two main types of MWEs at phrase-level:
idioms and collocations.
This complexity of the concept of idiomaticity has
posed a challenge to the development of methods
addressing the measurement of the aforementioned
four properties. Recent research has resulted in
this issue nowadays being usually addressed through
measuring the following phenomena: (i) cooccur-
rence, for institutionalization; (ii) distributional sim-
ilarity, for non-compositionality; (iii) deviation from
the behavior of free combinations, for morphosyn-
tactic fixedness; and (iv) substitutability, for lexical
fixedness. This is the broad context of our experi-
mental work on the automatic classification of NV
expressions in Basque.
2 Related Work
2.1 Statistical Idiosyncrasy or
Institutionalization
Using the cooccurrence of the components of a com-
bination as a heuristic of its institutionalization goes
back to early research on this field (Church and
Hanks, 1990), and is computed using association
measures (AM), usually in combination with lin-
guistic techniques, which allows the use of lemma-
tized and POS-tagged corpora, or the use of syntac-
tic dependencies (Seretan, 2011). In recent years,
the comparative analysis of AMs (Evert, 2005) and
the combination of them (Lin et al, 2008; Pecina,
2010) have aroused considerable interest.
This approach has been recently explored in
Basque (Gurrutxaga and Alegria, 2011).
116
2.2 Compositionality
The central concept in characterizing compositional-
ity is the hypothesis of distributional similarity (DS)
As proposed by Baldwin and Kim (2010), ?the un-
derlying hypothesis is that semantically idiomatic
MWEs will occur in markedly different lexical con-
texts to their component words.?
Berry-Rogghe (1974) proposed R-value to mea-
sure the compositionality of verb-particle construc-
tions (VPCs), by dividing the overlap between the
sets of collocates associated with the particle by
the total number of collocates of the VPC. Wulff
(2010) proposes two extensions to the R-value
in her research on verb-preposition-noun construc-
tions, combining and weighting in different ways in-
dividual R-values of each component.
The Vector Space Model (VSM) is applied,
among others, by Fazly and Stevenson (2007), who
use the cosine as a similarity measure. The shared
task Distributional Semantics and Compositionality
(DiSCo) at ACL-HLT 2011 shows a variety of tech-
niques for this task, mainly association measures
and VSM (Biemann and Giesbrecht, 2011). LSA
(Latent Semantic Analysis) is used in several stud-
ies (Baldwin et al, 2003; Katz and Giesbrecht, 2006;
Schone and Jurafsky, 2001).
Those approaches have been applied recently to
Basque (Gurrutxaga and Alegria, 2012)
2.3 Morphosyntactic Flexibility (MSFlex)
Morphosyntactic fixedness is usually computed in
terms of relative flexibility, as the statistical dis-
tance between the behavior of the combination and
(i) the average behavior of the combinations with
equal POS composition (Fazly and Stevenson, 2007;
Wulff, 2010), or (ii) the average behavior of the
combinations containing each one of the compo-
nents of the combination (Bannard, 2007).
Fazly and Stevenson (2007) use Kullback-
Leibler divergence (KL-div) to compute this dis-
tance. They analyze a set of patterns: determination
(a/the), demonstratives, possessives, singular/plural
and passive. They compute two additional measure-
ments (dominant pattern and presence of absence of
adjectival modifiers preceding the noun).
Wulff (2010) considers (i) tree-syntactic, (ii)
lexico-syntactic and (iii) morphological flexibilities,
and implements two metrics for these features: (i) an
extension of Barkema proposal (NSSD, normalized
sum of squared deviations), (ii) a special conception
of ?relative entropy? (Hrel).
Bannard (2007), using CPMI (conditional point-
wise mutual information), analyses these variants:
(i) variation, addition or dropping of a determiner;
(ii) internal modification of the noun phrase; and (iii)
verb passivation.
2.4 Lexical Flexibility (LFlex)
The usual procedure for measuring lexical flexibility
is to compute the substitutability of each component
of the combination using as substitutes its synony-
mous, quasi-synonyms, related words, etc.
The pioneering work in this field is Lin (1999),
who uses a thesaurus automatically built from text.
This resource is used in recent research (Fazly and
Stevenson, 2007). They assume that the target pair
is lexically fixed to the extent that its PMI deviates
from the average PMI of its variants generated by
lexical substitution. They compute flexibility using
the z-score.
In Van de Cruys and Moiro?n (2007), a technique
based on KL-div is used for Duch. They define Rnv
as the ratio of noun preference for a particular verb
(its KL-div), compared to the other nouns that are
present in the cluster of substitutes. Similarly for
Rvn. The substitute candidates are obtained from
the corpus using standard distributional similarity
techniques.
2.5 Other Methods
Fazly and Stevenson (2007) consider two other fea-
tures: (i) the verb itself; and (ii) the semantic cate-
gory of the noun according to WordNet.
2.6 Combined Systems
In order to combine several sources of knowledge,
several studies have experimented with using Ma-
chine Learning methods (ML).
For Czech, Pecina (2010) combines only AMs us-
ing neural networks, logistic regression and SVM
(Support Vector Machine). Lin et al (2008) employ
logistic linear regression model (LLRM) to combine
scores of AMs.
Venkatapathy and Joshi (2005) propose a mini-
mally supervised classification scheme that incorpo-
117
rates a variety of features to group verb-noun combi-
nations. Their features drawn from AM and DS, but
some of each type are tested and combined. They
compute ranking correlation using SVM, achieving
results of about 0.45.
Fazly and Stevenson (2007) use all the types of
knowledge, and decision trees (C5.0) as a learning
method, and achieve average results (F-score) near
to 0.60 for 4 classes (literal, abstract, light verbs and
idioms). The authors claim that the syntactic and
combined fixedness measures substantially outper-
form measures of collocation extraction.
3 Experimental Setup
3.1 Corpus and Preprocessing
We use a journalistic corpus of 75 million words
(MW) from two sources: (1) Issues published
in 2001-2002 by the newspaper Euskaldunon
Egunkaria (28 MW); and (2) Issues published in
2006-2010 by the newspaper Berria (47 MW).
The corpus is annotated with lemma, POS, fine
grained POS (subPOS), case and number informa-
tion using Eustagger developed by the IXA group of
the University of the Basque Country. A precision of
95.42% is reported for POS + subPOS + case analy-
sis (Oronoz et al, 2010).
3.2 Extraction of Bigram Candidates
The key data for defining a Basque NV bigram are
lemma and case for the noun, and lemma for the
verb. Case data is needed to differentiate, for exam-
ple, kontu hartu (?to ask for an explanation?) from
kontuan hartu (?to take into account?), where kontu
is a noun lemma in the inessive case.
In order to propose canonical forms, we need, for
nouns, token, case and number annotations in bi-
gram data. Those canonical forms can be formulated
using number normalization, as described in Gur-
rutxaga and Alegria (2011). Bigrams belonging to
the same key noun lemma/noun case+verb lemma
are normalized; a single bigram with the most fre-
quent form is created, and the frequencies of bi-
grams and those of the noun unigrams summed.
We use the Ngram Statistics Package-
NSP (Banerjee and Pedersen, 2010) to generate NV
bigrams from a corpus generated from the output of
Eustagger. Taking into account our previous results
(Gurrutxaga and Alegria, 2011), we use a window
span of ?1 and a frequency threshold of f > 30.
Before generation, some surface-grammar rules are
applied to correct annotations that produce noise.
For example, in most Basque AdjN combinations,
the adjetive is a verb in a participe form (eg. indar
armatuak, ?armed forces?). Similarly, those kind
of participles can function as nouns (gobernuaren
aliatuak, ?the allies of the government?). Not
tagging those participles properly would introduce
noise in the extraction of NV combinations.
3.3 Experiments Using Single Knowledge
Sources
3.3.1 Cooccurrence
The cooccurrence data provided by NSP in the bi-
gram extraction step is processed to calculate AMs.
To accomplish this, we use Stefan Evert?s UCS
toolkit (Evert, 2005). The most common AMs are
calculated: f , t-score, log-likelihood ratio, MI, MI3,
and chi-square (?2).
3.3.2 Distributional Similarity
The idea is to compare the contexts of each NV
bigram with the contexts of its corresponding com-
ponents, by means of different techniques. The
more similar the contexts, the more compositional
the combination.
Context Generation We extract the context words
of each bigram from the sentences with contiguous
cooccurrences of the components. The noun has to
occur in the grammatical case in which it has been
defined after bigram normalization.
The contexts of the corresponding noun and verb
are extracted separately from sentences where they
did not occur together. Only content-bearing lem-
mas are included in the contexts (nouns, verbs and
adjectives).
Context Comparison We process the contexts in
two different ways:
First, we construct a VSM model, representing
the contexts as vectors. As similarity measures, we
use Berry-Roghe?s R-value (RBR) and the two ex-
tensions to it proposed by Wulff (RW1 and RW2),
Jaccard index and cosine. For the cosine, different
AMs have been tested for vector weights (f , t-score,
118
LLR and PMI). We experiment with different per-
centages of the vector and different numbers of col-
locates, using the aforementioned measures to rank
the collocates. The 100 most frequent words in the
corpus are stopped.
Second, we represent the same contexts as doc-
uments, and compare them by means of differ-
ent indexes using the Lemur Toolkit (Allan et al,
2003). The contexts of the bigrams are used as
queries against a document collection containing the
context-documents of all the members of the bi-
grams. This can be implemented in different ways;
the best results were obtained using the following:
? Lemur 1 (L1): As with vectors, the contexts of
a bigram are included in a single query docu-
ment, and the same is done for the contexts of
its members
? Lemur 2 (L2): The context sentences of bi-
grams are treated as individual documents, but
the contexts of each one of its members are rep-
resented in two separate documents
Due to processing reasons, the number of context
sentences used in Lemur to generate documents is
limited to 2,000 (randomly selected from the whole
set of contexts).
We further tested LSA (using Infomap1), but the
above methods yielded better results.
3.3.3 Morphosyntactic Flexibility
We focus on the variation of the N slot, dis-
tinguishing the main type of extensions and num-
ber inflections. Among left-extensions, we take
into account relative clauses. In addition, we con-
sider the order of components as a parameter. We
present some examples of the free combination libu-
rua irakurri (?to read a book?)
? Determiner: liburu bat irakurri dut (?I have
read one book?), zenbat liburu irakurri dituzu?
(?how many books have you read??)
? Postnominal adjective: liburu interesgarria
irakurri nuen (?I read an interesting book?)
? Prenominal adjective: italierazko liburua
irakurri (?to read a book in Italian?)
1http://infomap-nlp.sourceforge.net/
? Relative clause: irakurri dudan liburua (?the
book I have read?), anaiak irakurritako liburu
batzuk (?some books read by my brother?)
? Number inflection: liburua/liburuak/
liburu/liburuok irakurri (?to read
a/some/?/these book(s)?)
? Order of components (NV / VN): liburua
irakurri dut / irakurri dut liburua (?I have read
a book?)
We count the number of variations for each bi-
gram, for all NV bigrams, and for each combination
of the type bigram component+POS of the other
component (e.g, for liburua irakurri, the variations
of all the combinations liburua+V and N+irakurri).
To calculate flexibility, we experiment with all the
measures described in section 2.3: Fazly?s KL-div,
Wulff?s NSSD and Hrel (relative entropy), and Ban-
nard?s CPMI.
3.3.4 Lexical Flexibility
In order to test the substitutability of the compo-
nents of bigrams, we use two resources: (i) ELH:
Sinonimoen Kutxa, a Basque dictionary of syn-
onyms, published by the Elhuyar Foundation (for
nouns and verbs, 40,146 word-synomyn pairs); (ii)
WN: the Basque version of WordNet2(68,217 word-
synomyn pairs). First, we experimented with both
resources on their own, but the results show that
in many cases there either was no substitute candi-
date, or the corpus lacked combinations containing
a substitute. In order to ensure a broader coverage,
we combined both resources (ELHWN), and we ex-
panded the set of substitutes including the siblings
retrieved from Basque WordNet (ELHWNexpand).
To calculate flexibility, we experiment with the
two measures described in section 2.4: z-score and
KL-div based R.
3.4 Combining Knowledge Sources Using
Machine Learning
We use some ML methods included in the Weka
toolkit (Hall et al, 2009) in order to combine re-
sults obtained in experiments using single knowl-
edge sources (described in section 3.3). The values
2http://ixa2.si.ehu.es/cgi-bin/mcr/public/wei.consult.perl
119
of the different measures obtained in those experi-
ments were set as features.
We have selected five methods corresponding to
different kind of techniques which have been used
successfully in this field: Naive Bayes, C4.5 deci-
sion tree (j48), Random Forest, SVM (SMO algo-
rithm) and Logistic Regression. Test were carried
out using either all features, the features from each
type of knowledge, and some subsets, obtained af-
ter manual and automatic selection. Following Fa-
zly and Stevenson (2007), verbs are also included as
features.
Since, as we will see in section 3.5, the amount of
instances in the evaluation dataset is not very high
(1,145), cross-validation is used in the experiments
for model validation (5 folds). In the case of auto-
matic attribute selection, we use AttributeSelected-
Classifier, which encapsulates the attribute selection
process with the classifier itself, so the attribute se-
lection method and the classifier only see the data in
the training set of each fold.
3.5 Evaluation
3.5.1 Reference Dataset and Human
Judgments
As an evaluation reference, we use a subset of
1,200 combinations selected randomly from a ex-
tracted set of 4,334 bigrams, that is the result of
merging the 2,000-best candidates of each AM rank-
ing from the w = ?1 and f > 30 extraction set.
The subset has been manually classified by three
lexicographers into idioms, collocations and free
combinations. Annotators were provided with an
evaluation manual, containing the guidelines for
classification and illustrative examples.
The agreement among evaluators was calculated
using Fleiss? ?. We obtained a value of 0.58, which
can be considered moderate, close to fair, agree-
ment. Although this level of agreement is relatively
low when compared to Krenn et al (2004), it is
comparable to the one reported by Pecina (2010),
who attributed his ?relatively low? value to the fact
that ?the notion of collocation is very subjective,
domain-specific, and also somewhat vague.? Street
et al (2010) obtain quite low inter-annotator agree-
ment for annotation of idioms in the ANC (Ameri-
can National Corpus). Hence, we consider that the
level of agreement we have achieved is acceptable.
For the final classification of the evaluation set,
cases where agreement was two or higher were au-
tomatically adopted, and the remaining cases were
classified after discussion. We removed 55 combina-
tions that did not belong to the NV category, or that
were part of larger MWEs. The final set included
1,145 items, out of which 80 were idioms 268 collo-
cations, and 797 free combinations.
3.5.2 Procedure
In order to compare the results of the individual
techniques, we based our evaluation on the rank-
ings provided by each measure. If we were to have
an ideal measure, the set of bigram categories (?id?,
?col? and ?free?) would be an ordered set, with ?id?
values on top of the ranking, ?col? in the middle, and
?free? at the bottom. Thus, the idea is to compute
the distance between a rank derived from the ideally
ordered set, which contains a high number of ties,
and the rank yielded by each measure. To this end,
we use Kendall?s ?B as a rank-correlation measure.
Statistical significance of the Kendall?s ?B correla-
tion coefficient is tested with the Z-test. The realistic
topline, yielded by a measure that ranks candidates
ideally, but without ties, would be 0.68.
In addition, average precision values (AP) were
calculated for each ranking.
In the case of association measures, similarity
measures applied to VSM, and measures of flexibil-
ity, the bigrams were ranked by means of the val-
ues of the corresponding measure. In the case of ex-
periments with Lemur, the information used to rank
the bigrams consisted of the positions of the docu-
ments corresponding to each member of the bigram
in the document list retrieved (?rank? in Table 1). For
the experiments in which the context sentences have
been distributed in different documents, average po-
sitions were calculated and weighted, in relation to
the amount of documents for each bigram analysis
(?rank weight?). The total number of documents in
the list (or ?hits?) is weighted in the same manner
(?hit rel?).
When using ML techniques, several measures
provided by Weka were analyzed: percentage of
Correctly Classified Instances (CCI), F-measures
for each class (id, col, free), Weighted Average F-
measure and Average F-measure.
120
measure ?B AP MWE AP id AP col
random rank (-0.02542) 0.30879 0.0787 0.23358
AM
f 0.18853 0.43573 0.07391 0.37851
t-score 0.19673 0.45461 0.08442 0.38312
log-likelihood 0.15604 0.42666 0.10019 0.33480
PMI (-0.12090) 0.25732 0.08648 0.18234
chi-squared (-0.03699) 0.30227 0.11853 0.20645
DS
RBR NV (MI -50%) 0.27034 0.47343 0.21738 0.30519
RW1(2000 MI f3 50%) 0.26206 0.47152 0.19664 0.30967
L1 Indri rankNV 0.31438 0.53536 0.22785 0.35299
L1 KL rankNV 0.29559 0.51694 0.23558 0.33607
L2 Indri hit rel NV 0.32156 0.56612 0.29416 0.35389
L2 KL hit rel NV 0.30848 0.55146 0.31977 0.33241
L2 Indri rankN weight 0.21387 0.45567 0.26148 0.28025
L2 Indri rankV weight 0.31398 0.55208 0.12837 0.43143
MSFlex
Hrel Det 0.07295 0.38995 0.12749 0.27704
Hrel PostAdj (-0.05617) 0.31673 0.04401 0.29597
Hrel PreAdj 0.11459 0.38561 0.09897 0.29223
Hrel Rel 0.09115 0.40502 0.12913 0.29012
Hrel Num 0.11861 0.43381 0.13387 0.31318
Hrel ord (0.02319) 0.31661 0.08124 0.24052
CPMI (components) 0.05785 0.41917 0.12630 0.30831
LFlex
Rnv ELHWN (0.08998) 0.36717 0.07521 0.29896
Rvn ELHWN (0.03306) 0.31752 0.08689 0.24369
z-score V ELHWNexpand 0.10079 0.35687 0.12232 0.25019
z-score N ELHWNexpand 0.08412 0.35534 0.07245 0.29005
Table 1: Kendall?s ?B rank-correlations relative to an ideal idiomaticity ranking, obtained by different idiomaticity
measures. Non-significant values of ?B in parentheses (p > 0.05). Average precisions for MWEs in general, and
specific values for idioms and collocations.
4 Experimental Results
4.1 Single Knowledge Experiments
The results for Kendall?s ?B and AP for MWEs and
separate AP values for idioms and collocations are
summarized in Table 1 (only the experiments with
the most noteworthy results are included).
The best results are obtained in the Lemur exper-
iments, most notably in the Lemur 2 type, using ei-
ther Indri or KL-div indexes. In the MWE rankings,
measures of the R-value type only slightly outper-
form AMs.
In the case of idioms, DS measures obtain signif-
icantly better ranks than the other measures. Idioms
being the least compositional expressions, his result
is expected, and supports the hypothesis that seman-
tic compositionality can better be characterized us-
ing measures of DS than using AMs.
Regarding collocations, no such claim can be
made, as the AP values for t-score and f outper-
form DS values, with a remarkable exception: the
best AP is obtained by an Indri index that com-
pares the semantic similarity between the verb in
combination with the noun and the verb in contexts
without the noun (L2 Indri rankV weight), accord-
ingly with the claim that the semantics of the verb
contribute to the semicompositionality of colloca-
tions. By contrast, the corresponding measure for
the noun (L2 Indri rankN weight) works quite a bit
better with idioms than the previous verb measure.
Figure 1 shows the precision curves for the extrac-
tion of MWEs by the best measure of each compo-
nent of idiomaticity.
In Figure 2 and 3, we present separately the preci-
121
Figure 1: Precision results for the compositionality rank-
ings of MWEs.
sion curves for idioms and collocations. We plot the
measures with the best precision values.
Figure 2: Precision results for the compositionality rank-
ings of idioms.
Regarding the precision for collocations in Fig-
ure 3, the differences are not obviously significant.
Even though the DS measure has the better perfor-
mance, precision values for the t-score are not too
much lower, and the t-score has a similar perfor-
mance at the beginning of the ranking (n < 150).
4.2 Machine Learning Experiments
We report only the results of the three methods with
the best overall performance: Logistic Regression
(LR), SMO and RandomForest (RF).
In Table 2, we present the results obtained with
datasets containing only DS attributes (the source
of knowledge with the best results in single ex-
Figure 3: Precision results for the compositionality rank-
ings of collocations.
periments); datasets containing all features corre-
sponding to the four properties of idiomaticity; and
datasets obtained adding the verb of the bigram as a
string-type attribute.
As the figures show, it is difficult to improve the
results obtained using only DS. The results of SMO
are better when the features of the four components
of idiomaticity are used, and even better when the
verb is added, especially for idioms. The verb causes
the performance of RF be slightly worse; in the case
of LR, it generates considerable noise.
It can be observed that the figures for LR are
more unstable. Using SMO and RF, convergence
does not depend on how many noisy variables are
present (Biau, 2012). Thus, feature selection could
improve the results when LR is used.
In a complementary experiment, we observed the
impact of removing the attributes of each source of
knowledge (without including verbs). The most ev-
ident result was that the exclusion of LFlex features
contributes the most to improving F. This was an ex-
pected effect, considering the poor results for LFlex
measures described in section 4.1. More interest-
ing is the fact that removing MSFlex features had a
higher negative impact on F than not taking AMs as
features.
Table 3 shows the results for two datasets gener-
ated through two manual selection of attributes: (1)
manual 1: the 20 attributes with best AP average re-
sults; and (2) manual 2: a manual selection of the
attributes from each knowledge source with the best
AP MWE, best AP id and best AP col. The third
122
Features Method CCI F id F col F free F W.Av. F Av.
DS
LR 72.489 0.261 0.453 0.838 0.707 0.517
SMO 74.061 0.130 0.387 0.824 0.575 0.447
RF 71.441 0.295 0.440 0.821 0.695 0.519
all idiom. properties
LR 71.703 0.339 0.514 0.821 0.716 0.558
SMO 76.507 0.367 0.505 0.857 0.740 0.576
RF 74.498 0.323 0.486 0.844 0.724 0.551
all + verb
LR 60.000 0.240 0.449 0.726 0.627 0.472
SMO 75.808 0.400 0.540 0.848 0.744 0.596
RF 74.061 0.243 0.459 0.846 0.713 0.516
Table 2: Results of Machine Learning experiments combining knowledge sources in three ways: (i) DS: distributional
similarity features; (ii) knowledge related to the four components of idiomaticity (AM+DS+MSFlex+LFlex); (iii)
previous features+verb components of bigrams.
section presents the results obtained with AttributeS-
electedClassifier using CfsSubsetEval (CS) as evalu-
ator3 and BestFirst (BS) as search method. Looking
at the results of the selection process in each fold, we
saw that the attributes selected in more than 2 folds
are 36: 1 AM, 20 from DS, 7 from MSFlex, 1 from
LFlex and 7 verbs.
Features Method F W.Av. F Av.
manual 1
LR 0.709 0.525
SMO 0.585 0.304
RF 0.680 0.485
manual 2
LR 0.696 0.518
SMO 0.581 0.286
RF 0.688 0.519
CS-BF
LR 0.727 0.559
SMO 0.693 0.485
RF 0.704 0.531
Table 3: F Weighted average and F average results for ex-
periments using: (1) the 20 attributes with best AP aver-
age results; (2) a manual selection of the 3 best attributes
from each knowledge source; and (3) AttributeSelected-
Classifier with automatic attribute selection using Cfs-
SubsetEval as evaluator and BestFirst as search method
The results show that, for each method, auto-
matic selection outperforms the two manual selec-
tions. Most of the attributes automatically selected
are DS measures, but it is interesting to observe that
MSFlex and the verb slot contribute to improving
the results. Using automatic attribute selection and
3http://wiki.pentaho.com/display/
DATAMINING/CfsSubsetEval
LR, the results are close to the best figure of F W.Av.
using SMO and all the features (0.727 vs 0.744).
5 Discussion
The most important conclusions from our experi-
ments are the following:
? In the task of ranking the candidates, the best
results are obtained using DS measures, and,
in particular, Indri and KL-div in L2 experi-
ments. This is true for both type of MWEs, and
is ratified in ML experiments when automatic
attribute filtering is carried out. It is, however,
particularly notable with regard to idioms; in
the case of collocations, the difference between
the performance of DS and that of and MS and
AM were not that significant.
? MSFlex contributes to the classification task
when used in combination with DS, but get
poor results by themselves. The most relevant
parameter MSFlex is number inflection.
? SMO is the most precise method when a high
amount of features is used. It gets the best over-
all F-score. The other methods need feature se-
lection to obtain similar results.
? Automatic attribute selection using CS-BF fil-
ter yields better results than manual selections.
The method that takes the most advantage is
LR, whose scores are little bit worse than those
of SMO using the whole set of attributes.
123
Some of these conclusions differ from those reached
by earlier works. In particular, the claims in Fazly
and Stevenson (2007) and Van de Cruys and Moiro?n
(2007) that syntactic as well as lexical flexibility out-
perform other techniques of MWE characterization
are not confirmed in this work for Basque. Some
hypothesis could be formulated to explain those
differences: (1) Basque idioms could be syntacti-
cally more flexible, whereas some free combinations
could present a non-negligible level of fixedness; (2)
Basque, especially in the journalistic register, could
be sociolinguistically less fixed than, say, English
or Spanish; thus, the lexical choice of the collocate
could be not so clearly established; (3) the Basque
lexical resources to test substitutability could have
insufficient coverage; and (4) Fazly and Stevenson
(2007) use the cosine for DS, a measure which in our
experiments is clearly below other measures. Those
hypotheses require experimental testing and deeper
linguistic analysis.
6 Conclusions and Future Work
We have presented an in-depth analysis of the per-
formance of different features of idiomaticity in the
characterization of NV expressions, and the results
obtained combining them using ML methods. The
results confirm the major role of DS, especially, as
expected, in the case of idioms. It is remarkable that
the best results have been obtained using Lemur, an
IR tool. ML experiments show that other features
contribute to improve the results, especially some
aspects of MSFlex, the verb of the bigram and, to
a more limited extent, AMs. The performance of
DS being the best one for idioms confirm previous
research on other languages, but MSFlex and LFlex
behave below the expected. The explanations pro-
posed for this issue require further verification.
We are planning experiments using these tech-
niques for discriminating between literal and id-
iomatic occurrences of MWEs in context. Work on
parallel corpora is planned for the future.
Acknowledgments
This research was supported in part by the Span-
ish Ministry of Education and Science (TACARDI-
TIN2012-38523-C02-011) and by the Basque
Government (Berbatek project, Etortek-IE09-262;
KONBITZ project, Saiotek 2012). Ainara Estar-
rona and Larraitz Uria (IXA group) and Ainara On-
darra and Nerea Areta (Elhuyar) are acknowledged
for their work as linguists in the manual evaluation.
Maddalen Lopez de la Calle and In?aki San Vicente
(Elhuyar) and Oier Lopez de la Calle (IXA group)
have contributed with their expertise to the design of
the experiments with Lemur and Infomap. Finally,
special thanks goes to Olatz Arregi (IXA group)
for having guided us in the experiments with Weka,
and to Yosu Yurramendi from the University of the
Basque Country, for his advice on the statistics in
the evaluation step.
References
Allan, J., J. Callan, K. Collins-Thompson, B. Croft,
F. Feng, D. Fisher, J. Lafferty, L. Larkey,
T. Truong, P. Ogilvie, et al (2003). The Lemur
Toolkit for language modeling and information
retrieval.
Baldwin, T., C. Bannard, T. Tanaka, and D. Wid-
dows (2003). An empirical model of multi-
word expression decomposability. In Proceed-
ings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pp. 96.
Baldwin, T. and S. Kim (2010). Multiword expres-
sions. Handbook of Natural Language Process-
ing, second edition. Morgan and Claypool.
Banerjee, S. and T. Pedersen (2010). The design,
implementation, and use of the Ngram Statistics
Package. Computational Linguistics and Intelli-
gent Text Processing, 370?381.
Bannard, C. (2007). A measure of syntactic flexi-
bility for automatically identifying multiword ex-
pressions in corpora. In Proceedings of the Work-
shop on a Broader Perspective on Multiword Ex-
pressions, pp. 1?8.
Berry-Rogghe, G. (1974). Automatic identification
of phrasal verbs. Computers in the Humanities,
16?26.
Biau, G. (2012). Analysis of a random forests
model. The Journal of Machine Learning Re-
search 98888, 1063?1095.
Biemann, C. and E. Giesbrecht (2011). Distri-
butional semantics and compositionality 2011:
124
Shared task description and results. Workshop
on Distributional semantics and compositionality
2011. ACL HLT 2011, 21.
Church, K. and P. Hanks (1990). Word associa-
tion norms, mutual information, and lexicogra-
phy. Computational linguistics 16(1), 22?29.
Evert, S. (2005). The statistics of word cooccur-
rences: Word pairs and collocations. Ph. D. the-
sis, University of Stuttgart.
Fazly, A. and S. Stevenson (2007). Distinguish-
ing subtypes of multiword expressions using
linguistically-motivated statistical measures. In
Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pp. 9?16. As-
sociation for Computational Linguistics.
Granger, S. and M. Paquot (2008). Disentangling
the phraseological web. Phraseology. An inter-
disciplinary perspective, 27?50.
Gurrutxaga, A. and I. Alegria (2011). Automatic
extraction of NV expressions in Basque: basic
issues on cooccurrence techniques. Proc. of the
Workshop on Multiword Expressions. ACL HLT
2011, 2?7.
Gurrutxaga, A. and I. Alegria (2012). Measuring
the compositionality of nv expressions in basque
by means of distributional similarity techniques.
LREC2012.
Hall, M., E. Frank, G. Holmes, B. Pfahringer,
P. Reutemann, and I. H. Witten (2009). The weka
data mining software: an update. Volume 11, pp.
10?18. ACM.
Katz, G. and E. Giesbrecht (2006). Automatic
identification of non-compositional multi-word
expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying
Properties, pp. 12?19. Association for Computa-
tional Linguistics.
Krenn, B., S. Evert, and H. Zinsmeister (2004). De-
termining intercoder agreement for a collocation
identification task. In Proceedings of KONVENS,
pp. 89?96.
Lin, D. (1999). Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the ACL, pp. 317?324. Associ-
ation for Computational Linguistics.
Lin, J., S. Li, and Y. Cai (2008). A new colloca-
tion extraction method combining multiple asso-
ciation measures. In Machine Learning and Cy-
bernetics, 2008 International Conference on, Vol-
ume 1, pp. 12?17. IEEE.
Oronoz, M., A. D. de Ilarraza, and K. Gojenola
(2010). Design and evaluation of an agreement er-
ror detection system: testing the effect of ambigu-
ity, parser and corpus type. In Advances in Natu-
ral Language Processing, pp. 281?292. Springer.
Pecina, P. (2010). Lexical association measures and
collocation extraction. Language resources and
evaluation 44(1), 137?158.
Schone, P. and D. Jurafsky (2001). Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem. In Proc. of the 6th
EMNLP, pp. 100?108. Citeseer.
Seretan, V. (2011). Syntax-Based Collocation Ex-
traction. Text, Speech and Language Technology.
Dordrecht: Springer.
Street, L., N. Michalov, R. Silverstein, M. Reynolds,
L. Ruela, F. Flowers, A. Talucci, P. Pereira,
G. Morgon, S. Siegel, et al (2010). Like finding a
needle in a haystack: Annotating the american na-
tional corpus for idiomatic expressions. In Proc.
of LREC?2010.
Van de Cruys, T. and B. Moiro?n (2007). Semantics-
based multiword expression extraction. In Pro-
ceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pp. 25?32. Asso-
ciation for Computational Linguistics.
Venkatapathy, S. and A. Joshi (2005). Measuring the
relative compositionality of verb-noun (vn) collo-
cations by integrating features. In Proceedings of
HLT/EMNLP, pp. 899?906. Association for Com-
putational Linguistics.
Wulff, S. (2010). Rethinking Idiomaticity. Corpus
and Discourse. New York: Continuum Interna-
tional Publishing Group Ltd.
125
