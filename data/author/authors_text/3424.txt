Classifying Biological Full-Text Articles for Multi-Database Curation 
Wen-Juan Hou, Chih Lee and Hsin-Hsi Chen
Department of Computer Science and Information Engineering, 
National Taiwan University, Taipei, Taiwan 
{wjhou, clee}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw
Abstract
In this paper, we propose an approach 
for identifying curatable articles from a 
large document set.  This system 
considers three parts of an article (title 
and abstract, MeSH terms, and captions) 
as its three individual representations 
and utilizes two domain-specific 
resources (UMLS and a tumor name list) 
to reveal the deep knowledge contained 
in the article.  An SVM classifier is 
trained and cross-validation is employed 
to find the best combination of 
representations.  The experimental 
results show overall high performance. 
1 Introduction 
Organism databases play a crucial role in 
genomic and proteomic research.  It stores the 
up-to-date profile of each gene of the species 
interested.  For example, the Mouse Genome 
Database (MGD) provides essential integration 
of experimental knowledge for the mouse 
system with information annotated from both 
literature and online sources (Bult et al, 2004).  
To provide biomedical scientists with easy 
access to complete and accurate information, 
curators have to constantly update databases 
with new information.  With the rapidly 
growing rate of publication, it is impossible for 
curators to read every published article.  Since 
fully automated curation systems have not met 
the strict requirement of high accuracy and recall, 
database curators still have to read some (if not 
all) of the articles sent to them.  Therefore, it 
will be very helpful if a classification system can 
correctly identify the curatable or relevant 
articles in a large number of biological articles. 
Recently, several attempts have been made to 
classify documents from biomedical domain 
(Hirschman et al, 2002).  Couto et al (2004) 
used the information extracted from related web 
resources to classify biomedical literature.  Hou 
et al (2005) used the reference corpus to help 
classifying gene annotation.  The Genomics 
Track (http://ir.ohsu.edu/genomics) of TREC 
2004 and 2005 organized categorization tasks.  
The former focused on simplified GO terms 
while the latter included the triage for "tumor 
biology", "embryologic gene expression", 
"alleles of mutant phenotypes" and "GO" articles.  
The increase of the numbers of participants at 
Genomics Track shows that biological 
classification problems attracted much attention. 
This paper employs the domain-specific 
knowledge and knowledge learned from full-text 
articles to classify biological text.  Given a 
collection of articles, various methods are 
explored to extract features to represent a 
document.  We use the experimental data 
provided by the TREC 2005 Genomics Track to 
evaluate different methods. 
The rest of this paper is organized as follows.  
Section 2 sketches the overview of the system 
architecture.  Section 3 specifies the test bed 
used to evaluate the proposed methods.  The 
details of the proposed system are explained in 
Section 4.  The experimental results are shown 
and discussed in Section 5.  Finally, we make 
conclusions and present some further work. 
2 System Overview 
Figure 1 shows the overall architecture of the 
proposed system.  At first, we preprocess each 
training article, and divide it into three parts, 
including (1) title and abstract, (2) MeSH terms 
assigned to this article, and (3) captions of 
figures and tables.  They are denoted as 
"Abstract", "MeSH", and "Caption" in this paper, 
respectively.  Each part is considered as a 
representation of an article.  With the help of 
domain-specific knowledge, we obtain more 
detail representations of an article.  In the 
model selection phase, we perform feature 
ranking on each representation of an article and 
employ cross-validation to determine the 
number of features to be kept.  Moreover, we 
use cross-validation to obtain the best 
combination of all the representations.  Finally, 
a support vector machine (SVM) (Vapnik, 1995; 
Hsu et al, 2003) classifier is obtained. 
159
3 Experimental Data
We train classifiers for classifying biomedical 
articles on the Categorization Task of the TREC 
2005 Genomics Track. The task uses data from 
the Mouse Genome Informatics (MGI) system
(http://www.informatics.jax.org/) for four
categorization tasks, including tumor biology,
embryologic gene expression, alleles of mutant
phenotypes and GO annotation. Given a 
document and a category, we have to identify
whether it is relevant to the given category.
The document set consists of some full-text 
data obtained from three journals, i.e., Journal of
Biological Chemistry, Journal of Cell Biology
and Proceedings of the National Academy of 
Science in 2002 and 2003.  There are 5,837
training documents and 6,043 testing documents.
4 Methods 
4.1 Document Preprocessing
In the preprocessing phase, we perform acronym
expansion on the articles, remove the remaining
tags from the articles and extract three parts of 
interest from each article.  Abbreviations are 
often used to replace long terms in writing 
articles, but it is possible that several long terms
share the same short form, especially for
gene/protein names. To avoid ambiguity and
enhance clarity, the acronym expansion 
operation replaces every tagged abbreviation 
with its long form followed by itself in a pair of 
parentheses.
4.2 Employing Domain-Specific Knowledge 
With the help of domain-specific knowledge, we 
can extract the deeper knowledge in an article. 
For example, with a gene name dictionary, we
can identify the gene names contained in an 
article.  Moreover, by further consulting
organism databases, we can get the properties of
the genes. Two domain-specific resources are
exploited in this study.  One is the Unified 
Medical Language System (UMLS) (Humphreys
et al, 1998) and the other is a list of tumor
names obtained from Mouse Tumor Biology
Database (MTB)1.
UMLS contains a huge dictionary of
biomedical terms ? the UMLS Metathesaurus
and defines a hierarchy of semantic types ? the 
UMLS Semantic Network. Each concept in the
Metathesaurus contains a set of strings, which
are variants of each other and belong to one or
more semantic types in the Semantic Network.
Therefore, given a string, we can obtain a set of 
semantic types to which it belongs. Then we 
obtain another representation of the article by 
gathering the semantic types found in the part of 
the article. Consequently, we get another three 
much deeper representations of an article after
this step. They are denoted as "AbstractSEM",
"MeSHSEM" and "CaptionSEM". 
We use the list of tumor names on the Tumor
task. We first tokenize all the tumor names and 
stem each unique token. With the resulting list 
of unique stemmed tokens, we use it as a filter to 
remove the tokens not in the list from the 
"Abstract" and "Caption", which produce 
"AbstractTM" and "CaptionTM".
4.3 Model Selection
As mentioned above, we generate several 
representations for an article. In this section, 
we explain how feature selection is done and
how the best combination of the representations 
1 http://tumor.informatics.jax.org/mtbwi/tumorSearch.do
A New 
Full-Text
Article
Full-Text
Training
Articles
Abstract
MeSH
Caption
Model
Selection
AbsSEM/TM
Preprocessing
MeSHSEM
CapSEM/TM
Domain-Specific
Knowledge
SVM
Classifier
Yes/NoPartsSEM/TMPreprocessing Multiple
Parts
Figure 1. System Architecture
160
of an article is obtained. 
For each representation, we first rank all the 
tokens in the training documents via the 
chi-square test of independence.  Postulating 
the ranking perfectly reflects the effectiveness of 
the tokens in classification, we then decide the 
number of tokens to be used in SVM 
classification by 4-fold cross-validation.  In 
cross-validation, we use the TF*IDF weighting 
scheme.  Each feature vector is then 
normalized to a unit vector.  We set C+ to ur* C-
because of the relatively small number of 
positive examples, where C+ and C- are the 
penalty constants on positive and negative 
examples in SVMs.  After that, we obtain the 
optimal number of tokens and the corresponding 
SVM parameters C- and gamma, a parameter in 
the radial basis kernel.  In the rest of this paper, 
"Abstract30" denotes the "Abstract" 
representation with top-30 tokens, 
"CaptionSEM10" denotes "CaptionSEM" with 
top-10 tokens, and so forth. 
After feature selection is done for each 
representation, we try to find the best 
combination by the following algorithm. 
Given the candidate representations with 
selected features, we start with an initial set 
containing some or zero representation.  For 
each iteration, we add one representation to the 
set by picking the one that enhances the 
cross-validation performance the most.  The 
iteration stops when we have exhausted all the 
representations or adding more representation to 
the set doesn?t improve the cross-validation 
performance. 
For classifying the documents with better 
features, we run the algorithm twice.  We first 
start with an empty set and obtain the best 
combination of the basic three representations, 
e.g., "Abstract10", "MeSH30" and "Caption10".  
Then, starting with this combination, we attempt 
to incorporate the three semantic representations, 
e.g., "Abstract30SEM", "MeSH30SEM" and 
"Caption10SEM", and obtain the final 
combination.  Instead of using this algorithm to 
incorporate the "AbstractTM" and "CaptionTM" 
representations, we use them to replace their 
unfiltered counterparts "Abstract" and "Caption" 
when the cross-validation performance is better. 
5 Results and Discussions 
Table 1 lists the cross-validation results of each 
representation for each category (in Normalized 
Utility (NU)2 measure).  For category Allele, 
"Caption" and "AbstractSEM" perform the best 
among the basic and semantic representations, 
respectively.  For category Expression, 
"Caption" plays an important role in identifying 
relevant documents, which agrees with the 
finding by the winner of KDD CUP 2002 task 1 
(Regev et al, 2002).  Similarly, MeSH terms 
are crucial to the GO category, which are used 
by top-performing teams (Dayanik et al, 2004; 
Fujita, 2004) in TREC Genomics 2004.  For 
category Tumor, MeSH terms are important, but 
after semantic type extraction, "AbstractSEM" 
exhibits relatively high cross-validation 
performance.  Since only 10 features are 
selected for the "AbstractSEM", using this 
representation alone may be susceptible to 
over-fitting.  Finally, by comparing the 
performance of the "AbstractTM" and 
"Abstract", we find the list of tumor names 
helpful for filtering abstracts. 
We list the results for the test data in Table 2.  
Column "Experiment" identifies our proposed 
methods.  We show six experiments in Table 2: 
one for Allele (AL), one for Expression (EX), 
one for GO (GO) and three for Tumor (TU, TN 
and TS).  Column "cv NU" shows the 
cross-validation NU measure, "NU" shows the 
performance on the test data and column 
"Combination" lists the combination of the 
representations used for each experiment.  In 
this table, "M30" is the abbreviation for 
"MeSH30", "CS10" is for "CaptionSEM10", and 
so on.  The combinations for the first 4 
experiments, i.e., AL, EX, GO and TU, are 
obtained by the algorithm described in Section 
4.3, while the combination for TN is obtained by 
substituting "AbstractTM30" for "Abstract30" in 
the combination for TU.  The experiment TS 
only uses the "AbstractSEM10" because its 
cross-validation performance beats all other 
combinations for the Tumor category. 
The combinations of the first 5 experiments 
illustrate that adding other inferior 
representations to the best one enhances the 
performance, which implies that the inferior 
ones may contain important exclusive 
information.  The cross-validation performance 
fairly predicts the performance on the test data, 
except for the last experiment TS, which relies 
on only 10 features and is therefore susceptible 
to over-fitting. 
                                                 
2 Please refer to the TREC 2005 Genomics Track Protocol 
(http://ir.ohsu.edu/genomics/2005protocol.html).
161
Allele Expression GO Tumor 
# Tokens / NU # Tokens / NU # Tokens / NU # Tokens / NU 
Abstract 10 / 0.7707 10 / 0.5586 10 / 0.4411 10 / 0.8055 
MeSH 10 / 0.7965 10 / 0.6044 10 / 0.4968 30 / 0.8106 
Caption 10 / 0.8179 10 / 0.7192 10 / 0.4091 10 / 0.7644 
AbstractSEM 10 / 0.7209 10 / 0.4811 10 / 0.3493 10 / 0.8814 
MeSHSEM 10 / 0.6942 10 / 0.4563 10 / 0.4403 10 / 0.7047 
CaptionSEM 30 / 0.6789 10 / 0.5433 10 / 0.2551 30 / 0.7160 
AbstractTM 30 / 0.8325 
CaptionTM 10 / 0.7498 
Table 1. Partial Cross-validation Results. 
Experiment cv NU NU Recall Precision F-score Combination 
AL (for Allele) 0.8717 0.8423 0.9488 0.3439 0.5048 M30+C10+A10+CS10+AS10+MS10 
EX (for Expression) 0.7691 0.7515 0.8190 0.1593 0.2667 M10+C10+CS10+MS10 
GO (for GO) 0.5402 0.5332 0.8803 0.1873 0.3089 M10+C10+MS10 
TU (for Tumor) 0.8742 0.8299 0.9000 0.0526 0.0994 M30+C30+A30+AS10+CS30 
TN (for Tumor) 0.8764 0.8747 0.9500 0.0518 0.0982 M30+C30+AT30+AS10+CS30 
TS (for Tumor) 0.8814 0.5699 0.6500 0.0339 0.0645 AS10
Table 2. Evaluation Results. 
Subtask NU (Best/Median) Recall (Best/Median) Precision (Best/Median) F-score (Best/Median) 
Allele 0.8710/0.7773 0.9337/0.8720 0.4669/0.3153 0.6225/0.5010 
Expression 0.8711/0.6413 0.9333/0.7286 0.1899/0.1164 0.3156/0.2005 
GO Annotation 0.5870/0.4575 0.8861/0.5656 0.2122/0.3223 0.3424/0.4107 
Tumor 0.9433/0.7610 1.0000/0.9500 0.0709/0.0213 0.1325/0.0417 
Table 3. Best and Median Results for Each Subtask on TREC 2005 (Hersh et al, 2005). 
To compare with our performance, we list the 
best and median results for each subtask on the 
genomics classification task of TREC 2005 in 
Table 3.  Comparing to Tables 2 and 3, it shows 
our experimental results have overall high 
performance. 
6 Conclusions and Further Work 
In this paper, we demonstrate how our system is 
constructed.  Three parts of an article are 
extracted to represent its content.  We 
incorporate two domain-specific resources, i.e., 
UMLS and a list of tumor names.  For each 
categorization work, we propose an algorithm to 
get the best combination of the representations 
and train an SVM classifier out of this 
combination.  Evaluation results show overall 
high performance in this study. 
Except for MeSH terms, we can try other 
sections in the article, e.g., Results, Discussions 
and Conclusions as targets of feature extraction 
besides the abstract and captions in the future.  
Finally, we will try to make use of other 
available domain-specific resources in hope of 
enhancing the performance of this system. 
Acknowledgements
Research of this paper was partially supported by 
National Science Council, Taiwan, under the 
contracts NSC94-2213-E-002-033 and 
NSC94-2752-E-001-001-PAE. 
References 
Bult, C.J., Blake, J.A., Richardson, J.E., Kadin, J.A., Eppig, 
J.T. and the Mouse Genome Database Group. The Mouse 
Genome Database (MGD): Integrating Biology with the 
Genome. Nucleic Acids Research, 32, D476?D481, 2004. 
Couto, F.M., Martins, B. and Silva, M.J. Classifying Biological 
Articles Using Web Resources. Proceedings of the 2004 
ACM Symposium on Applied Computing, 111-115, 2004. 
Dayanik, A., Fradkin, D., Genkin, A., Kantor, P., Lewis, D.D., 
Madigan, D. and Menkov, V. DIMACS at the TREC 2004 
Genomics Track. Proceedings of the Thirteenth Text 
Retrieval Conference, 2004. 
Fujita, S., Revisiting Again Document Length Hypotheses 
TREC-2004 Genomics Track Experiments at Patolis. 
Proceedings of the Thirteenth Text Retrieval Conference,
2004.
Hersh, W., Cohen, A., Yang, J., Bhuptiraju, R.T., Toberts, P. 
and Hearst, M. TREC 2005 Genomics Track Overview. 
Proceedings of the Fourteenth Text Retrieval Conference,
2005.
Hirschman, L., Park, J., Tsujii, J., Wong, L. and Wu, C.H. 
Accomplishments and Challenges in Literature Data 
Mining for Biology. Bioinformatics, 18(12): 1553-1561, 
2002.
Hou, W.J., Lee, C., Lin, K.H.Y. and Chen, H.H. A Relevance 
Detection Approach to Gene Annotation. Proceedings of the 
First International Symposium on Semantic Mining in 
Biomedicine, http://ceur-ws.org, 148: 15-23, 2005. 
Hsu, C.W., Chang, C.C. and Lin, C.J. A Practical Guide to 
Support Vector Classification. http://www.csie.ntu.edu.tw 
/~cjlin/libsvm/index.html, 2003. 
Humphreys, B.L., Lindberg, D.A., Schoolman, H.M. and 
Barnett, G.O. The Unified Medical Language System: an 
Informatics Research Collaboration. Journal of American 
Medical Information Association, 5(1):1-11, 1998. 
Regev, Y., Finkelstein-Landau, M. and Feldman, R. Rule-based 
Extraction of Experimental Evidence in the Biomedical 
Domain - the KDD Cup (Task 1). SIGKDD Explorations,
4(2):90-92, 2002. 
Vapnik, V. The Nature of Statistical Learning Theory,
Springer-Verlag, 1995. 
162
Enhancing Performance of Protein Name Recognizers Using Collocation 
Wen-Juan Hou 
Department of Computer Science 
and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
wjhou@nlg.csie.ntu.edu.tw 
Hsin-Hsi Chen 
Department of Computer Science 
and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
hh_chen@csie.ntu.edu.tw 
 
 
Abstract 
Named entity recognition is a fundamental task in 
biological relationship mining.  This paper 
employs protein collocates extracted from a 
biological corpus to enhance the performance of 
protein name recognizers.  Yapex and KeX are 
taken as examples.  The precision of Yapex is 
increased from 70.90% to 81.94% at the low 
expense of recall rate (i.e., only decrease 2.39%) 
when collocates are incorporated.  We also 
integrate the results proposed by Yapex and KeX, 
and employs collocates to filter the merged results.  
Because the candidates suggested by these two 
systems may be inconsistent, i.e., overlap in partial, 
one of them is considered as a basis.  The 
experiments show that Yapex-based integration is 
better than KeX-based integration. 
1 Introduction 
Named entities are basic constituents in a 
document.  Recognizing named entities is a 
fundamental step for document understanding.  In 
a famous message understanding competition 
MUC (Darpa, 1998), named entities extraction, 
including organizations, people, and locations, 
along with date/time expressions and monetary and 
percentage expressions, is one of the evaluation 
tasks.  Several approaches have been proposed to 
capture these types of terms.  For example, 
corpus-based methods are employed to extract 
Chinese personal names, and rule-based methods 
are used to extract Chinese date/time expressions 
and monetary and percentage expressions (Chen 
and Lee, 1996; Chen, et al, 1998).  Corpus-based 
approach is adopted because a large personal name 
database is available for training.  In contrast, 
rules which have good coverage exist for date/time 
expressions, so the rule-based approach is adopted. 
In the past, named entities extraction mainly 
focuses on general domains.  Recently, large 
amount of scientific documents has been published, 
in particular for biomedical domains.  Several 
attempts have been made to mine knowledge from 
biomedical documents (Hirschman, et al, 2002).  
One of their goals is to construct a knowledge base 
automatically and to find new information 
embedded in documents (Craven and Kumlien, 
1999).  Similar information extraction works have 
been explored on this domain.  Named entities 
like protein names, gene names, drug names, 
disease names, and so on, were recognized (Collier, 
et al, 2000; Fukuda, et al, 1998; Olsson, et al, 
2002; Rindflesch, et al, 2000).  Besides, the 
relationships among these entities, e.g., 
protein-protein, protein-gene, drug-gene, 
drug-disease, etc., were extracted (Blaschke, et al, 
1999; Frideman, et al, 2001; Hou and Chen, 2002; 
Marcotte, et al, 2001; Ng and Wong, 1999; Park, 
et al, 2001; Rindflesch, et al, 2000; Thomas, et al, 
2000; Wong, 2001). 
Collocation denotes two or more words having 
strong relationships (Manning and Schutze, 1999).  
The related technologies have been applied to 
terminological extraction, natural language 
generation, parsing, and so on.  This paper deals 
with a special collocation in biological domain ? 
say, protein collocation.  We will find out those 
keywords that co-occur with protein names by 
using statistical methods.  Such terms, which are 
called collocates of proteins hereafter, will be 
considered as restrictions in protein name 
extraction.  To improve the precision rate at the 
low expense of recall rate is the main theme of this 
approach. 
The rest of the paper is organized as follows.  
The protein name recognizers used in this study are 
introduced in Section 2.  The collocation method 
we adopted is shown in Section 3.  The filtering 
and integration strategies are explained in Sections 
4 and 5, respectively.  Finally, Section 6 
concludes the remarks and lists some future works. 
2 
3 
Protein Name Recognizers 
The detection of protein names presents a 
challenging task because of their variant structural 
characteristics, their resemblance to regular noun 
phrases and their similarity with other kinds of 
biological substances.  Previous approaches on 
biological named entities extraction can be 
classified into two types ? say, rule-based (Fukuda, 
et al, 1998; Humphreys, et al, 2000; Olsson, et al, 
2002) and corpus-based (Collier, et al, 2000).  
KeX developed by Fukuda, et al (1998) and 
Yapex developed by Olsson, et al (2002) were 
based on handcrafted rules for extracting protein 
names.  Collier, et al (2000) trained a Hidden 
Markov Model with a small corpus of 100 
MEDLINE abstracts to extract names of gene and 
gene products. 
Different taggers have their specific features.  
KeX was evaluated by using 30 abstracts on SH3 
domain and 50 abstracts on signal transduction, 
and achieved 94.70% precision and 98.84% recall.  
Yapex was applied to a test corpus of 101 abstracts.   
Of these, 48 documents were queried from protein 
binding and interaction, and 53 documents were 
randomly chosen from GENIA corpus.  The 
performance of tagging protein names is 67.8% 
precision and 66.4% recall.  While the same test 
corpus was applied to KeX, it got 40.4% precision 
and 41.1% recall.  It reveals that each tagger has 
its own characteristics.  Changing the domain 
may result in the variant performance.  
Consequently, how to select the correct molecular 
entities proposed from the existing taggers is an 
interesting issue. 
Statistical Methods for Collocation 
The overall flow of our method is shown in Figure 
1.  To extract protein collocates, we need a corpus 
in which protein names have been tagged.  Thus, 
we prepare a tagged biological corpus by looking 
up the protein lexicon in the first step.  Then, 
common stop words are removed and the 
stemming procedure is applied to gather and group 
more informative words.  Next, the collocation 
values of proteins and their surrounding words are 
calculated.  Finally, we use these values to tell 
which neighbouring words are the desired 
collocates.  The major modules are specified in 
detail in the following subsections. 
 
 
 Protein 
lexicon 
Tag the raw material  
 
 
 Preprocessing 
1. Remove stopwords 
2. Stem 
Stop w
ord
list  
 
 
 
 Calculate collocation value 
 
 
 Extract significant protein 
collocates  
 
 
 
   
Figure 1. Flow of Mining Protein Collocates 
3.1 Step 1: Tagging the Corpus 
On the one hand, to calculate the collocation 
values of words with proteins from a corpus, it is 
necessary to recognize protein names at first.  On 
the other hand, the goal of this paper deals with 
performance issue of protein name tagging.  
Hence, preparing a protein name tagged corpus and 
developing a high performance protein name 
tagger seem to be a chicken-egg problem.  
Because the corpus developed in the first step is 
used to extract the contextual information of 
proteins, a completely tagged corpus is not 
necessary at the first step.  Dictionary-based 
approach for name tagging, i.e., full pattern 
matching between the dictionary entries and the 
words in the corpus, is simple.  The major 
argument is its coverage.  Those protein names 
which are not listed in the dictionary, but appear in 
the corpus will not be recognized.  Thus this 
approach only produces a partial-tagged corpus, 
but it is enough to acquire contextual information 
for latter use. 
3.2 Step 2: Preprocessing 
3.2.1 Step 2.1: Exclusion of Stopwords 
Stopwords are common English words (such as 
preposition ?in? and article ?the?) that frequently 
appear in the text but are not helpful in 
discriminating special classes.  Because they are 
distributed largely in the corpus, they should be 
filtered out.  The stopword list in this study was 
collected with reference to the stoplists of Fox 
(1992), but the words also appearing in the protein 
lexicon are removed.  For example, ?of? is a 
constituent of the protein name ?capsid of the 
lumazine?, so that ?of? is excluded from the 
stoplist.  Finally, 387 stopwords were used. 
3.2.2 Step 2.2: Stemming 
Stemming is a procedure of transforming an 
inflected form to its root form.  For example, 
?inhibited? and ?inhibition? will be mapped into 
the root form ?inhibit? after stemming.  
Stemming can group the same word semantics and 
reflect more information around the proteins. 
3.3 Step 3: Computing Collocation Statistics 
The collocates of proteins are those terms that 
often co-occur with protein names in the corpus.  
In this step, we calculate three collocation statistics 
to find the significant terms around proteins. 
Frequency 
The collocates are selected by frequency.  In 
order to gather more flexible relationships, here we 
define a collocation window that has five words on 
each side of protein names.  And then collocation 
bigrams at a distance are captured.  In general, 
more occurrences in the collocation windows are 
preferred, but the standard criteria for frequencies 
are not acknowledged.  Hence, other collocation 
models are also considered. 
Mean and Variance 
The mean value of collocations can indicate how 
far collocates are typically located from protein 
names.  Furthermore, variance shows the 
deviation from the mean.  The standard deviation 
of value zero indicates that the collocates and the 
protein names always occur at exactly the same 
distance equal to the mean value.  If the standard 
deviation is low, two words usually occur at about 
the same distance, i.e., near the mean value.  If 
the standard deviation is high, then the collocates 
and the protein names occur at random distance. 
t-test Model 
When the values of mean and variance have been 
computed, it is necessary to know if two words do 
not co-occur by chance.  Moreover, we also have 
to know if the standard deviation is low enough.  
In other words, we have to set a threshold in the 
above approach.  To get the statistical confidence 
that two words have a collocation relationship, 
t-test hypothesis testing is adopted. 
The t-value for each word i is formulated as 
follows: 
Ns
uxt
i
ii
i
/2
?=  
Where 
N = 4n - 15, 
N
countnx ii _= , 
)1(2 iii pps ??= , 
ncountnp ii /_= , 
iproteini ppu ?= , and 
proteinp  is the probability of protein. 
When ? (confidence level) is equal to 0.005, the 
value of t is 2.576.  In the t-test model, if the 
t-value is larger than 2.576, the word is regarded as 
a good collocate of protein with 99.5% confidence. 
3.4 Step 4: Extraction of Collocates 
We applied the above procedure to a corpus 
downloaded from the PASTA website in Sheffield 
University with 1,514 MEDLINE abstracts 
[http://www.dcs.shef.ac.uk/nlp/pasta].  Of the 
4,782 different stemmed words appearing in the 
collocation windows, there are 541 collocations 
generated in Step 3.  The collocates are not 
tagged with parts of speech, so that the output may 
contain nouns, prepositions, numbers, verbs, etc. 
The collocates extracted in a corpus cannot only 
serve as conditions of protein names, but also 
facilitate the relationship discovery between 
proteins.  From the past papers on the extraction 
of the biological information, such as Blaschke, et 
al. (1999), Ng, et al (1999), and Ono, et al (2001) 
etc., verbs are the major targets.  This is because 
many of the subjects and the objects related to 
these verbs are names of genes or proteins.  To 
assure that the collocates selected in Step 3 are 
verbs, we assign parts of speech to these words.  
Appendix A lists the collocates and their 
variations. 
4 Filtering Strategies 
For protein name recognition, rule-based systems 
and dictionary-based systems are usually 
complementary.  Rule-based systems can 
recognize those protein names not listed in a 
dictionary, but some false entities may also pass at 
the same time.  Dictionary-based systems can 
recognize those proteins in a dictionary, but the 
coverage is its major deficiency.  In this section, 
we will employ collocates of proteins mined earlier 
to help identify the molecular entities.  Yapex 
system (Olsson et al, 2002) is adopted to propose 
candidates, and collocates are served as restrictions 
to filter out less possible protein names. 
The following filtering strategies are proposed.  
Assume the candidate set M0 is the output 
generated by Yapex. 
z M1: For each candidate in M0, check if a 
collocate is found in its collocation window.  
If yes, tag the candidate as a protein name.  
Otherwise, discard it. 
z M2: Some of the collocates may be 
substrings of protein names.  We relax the 
restriction in M1 as follows.  If a 
collocate appears in the candidate or in the 
collocation window of the candidate, then 
tag the candidate as a protein name; 
otherwise, discard it. 
z M3: Some protein names may appear more 
than once in a document.  They may not 
always co-occur with some collocate in 
each occurrence.  In other words, the 
protein candidate and some collocates may 
co-occur in the first occurrence, the second 
occurrence, or even the last occurrence.  
We revise M1 and M2 as follows to 
capture this phenomenon.  During 
checking if there exists a collocate 
co-occurring with a protein candidate, the 
candidate without any collocate is kept 
undecidable instead of definite no.  After 
all the protein names are examined, those 
undecidable candidates may be considered 
as protein names when one of their 
co-occurrences containing any collocate.  
In other words, as long as a candidate has 
been confirmed once, it is assumed to be a 
protein throughout.  In this way, there are 
two filtering alternatives M31 and M32 
from M1 and M2, respectively. 
To get more objective evaluation, we utilized 
another corpus of 101 abstracts used by Yapex 
[http://www.sics.se/humle/projects/prothalt].  
Using the test corpus and answer keys supported in 
Yapex project, the evaluation results on filtering 
strategies are listed in Table 1. 
 
Table 1.  Evaluation on Filtering Strategies 
 Precision Recall F-score 
M0 70.90% 69.53% 70.22% 
M1 79.18% 56.10% 67.64% 
M2 79.29% 56.66% 67.98% 
M31 81.97% 66.84% 74.41% 
M32 81.94% 67.14% 74.54% 
 
Compared with the baseline model M0, the 
precision rates of all the four models using 
collocates were improved more than 8%.  The 
recall rates of M1 and M2 decreased about 13%. 
Thus, the overall F-scores of M1 and M2 
decreased about 2% compared to M0.  In contrast, 
if the decision of tagging was deferred until all the 
information were considered, then the recall rate 
decreased only 2% and the overall F-scores of M31 
and M32 increased 4% relative to M0.  The best 
one, M32, improved the precision rate from 
70.90% to 81.94%, and the F-score from 70.22% 
to 74.54%.  That meets our expectation, i.e., to 
enhance the precision rate, but not to reduce the 
significant recall rate. 
5 Integration Strategies 
Now we consider how to improve the recall rates. 
Integration strategies based on a hybrid concept are 
introduced.  The basic idea is that different 
protein name taggers have their own specific 
features such that they can recognize some tagging 
objects according to their rules or recognition 
methods.  Among the proposed protein names by 
different recognizers, there may exist some 
overlaps and some differences.  In other words, a 
protein name recognizer may tag a protein name 
that another recognizer cannot identify, or both of 
them may accept certain common proteins.  The 
integration strategies are used to select correct 
protein names proposed by multiple recognizers.  
In this study, we made experiments on Yapex and 
KeX because they are freely available on the web. 
Because protein candidates are proposed by two 
named entity extractors independently, they may 
be totally separated, totally overlap, overlapped in 
between, overlapped in the beginning, and 
overlapped in the end.  Figure 2 demonstrates 
these five cases. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
The integration strategies shown as follows 
combine the results from two sources. 
z When the protein names produced from 
two recognizers are totally separated (i.e., 
type A), retain them as the protein 
candidates.  This integration strategy 
postulates that one protein name 
recognizer may extract some proteins that 
another one cannot identify. 
z When the protein names produced from 
two recognizers are exactly the same (i.e., 
type B), retain them as the protein 
candidates.  Because both taggers accept 
the same protein names, there must exist 
some special features that fit protein 
names. 
z When the protein names tagged by two 
taggers have partial overlap (i.e., types C, 
D and E), two additional integration 
strategies are employed, i.e., Yapex-based 
and KeX-based strategies.  In the former 
strategy, we adopt protein names tagged 
by Yapex as candidates and discard the 
ones produced by KeX.  In contrast, the 
names tagged by KeX are kept in the latter 
strategy.  The integration strategy is 
made because each recognizer has its own 
characteristics, and we do not know which 
one is performed better in advance. 
Type A: totally separated 
The above integration strategies put together all 
the possible protein candidates except the 
ambiguous cases (i.e., types C, D and E).  That 
tends to increase the recall rate.  To avoid 
decreasing the precision rate, we also employ the 
collocates mentioned in Section 3 to filter out the 
less possible protein candidates.  Furthermore, to 
objectively evaluate the performance of the 
proposed collocates, we employ the same 
strategies to the same test corpus with some terms 
suggested by human experts.  Total 48 verbal 
keywords which were used to find the pathway of 
proteins are used and listed in Appendix B. 
Type B: totally overlap 
Type C: overlapped in between 
Type D: overlapped in the beginning 
Four sets of experiments were designed as 
follows for Yapex- and KeX-based integration 
strategies, respectively. 
Type E: overlapped in the end 
(1)YA and KA: Use the collocates automatically 
extracted in Section 3 to filter out the candidates as 
described in Section 4. 
(2)YB and KB: Use the terms suggested by 
human experts for the filtering strategies. 
Figure 2. Candidates Proposed by Two Systems 
(3)YA-C and KA-C: If Yapex and KeX 
recommend the same protein names (i.e., type B), 
regard them as protein names without 
consideration of collocates.  Otherwise, use the 
collocates proposed in this study to make filtering. 
(4)YB-C and KB-C: Similar to (3) except that 
the collocates are replaced by the terms suggested 
by human experts. 
The experimental results are listed in Tables 2 
and 3.  The tendency M32>M31>M2>M1 is still 
kept in the new experiments.  The strategy of 
delaying the decision until clear evidence is found 
is workable.  The performances of YA, YA-C, KA, 
and KA-C are better than the performances of the 
corresponding models (i.e., YB, YB-C, KB, and  
 
Table 2. Evaluation Results on Yapex-based 
Integration Strategy 
YA Precision Recall F-score 
M0 61.98% 77.52% 69.75% 
M1 64.97% 62.82% 63.90% 
M2 65.02% 63.53% 64.28% 
M31 65.94% 74.26% 70.10% 
M32 65.90% 74.62% 70.26% 
YB    
M1 66.79% 44.30% 55.55% 
M2 66.79% 44.81% 55.80% 
M31 70.20% 65.06% 67.63% 
M32 70.19% 65.51% 67.85% 
YA-C    
M1 65.76% 69.18% 67.47% 
M2 65.88% 69.84% 67.86% 
M31 65.39% 75.43% 70.41% 
M32 65.38% 75.69% 70.54% 
YB-C    
M1 68.92% 58.09% 63.51% 
M2 68.78% 58.49% 63.64% 
M31 69.07% 69.08% 69.13% 
M32 69.07% 69.63% 69.35% 
 
Table 3. Evaluation Results on KeX-based 
Integration Strategy 
KA Precision Recall F-score 
M0 60.43% 70.60% 65.52% 
M1 63.82% 56.61% 60.22% 
M2 63.52% 57.22% 60.37% 
M31 64.39% 65.56% 64.98% 
M32 64.03% 65.92% 64.98% 
KB    
M1 67.56% 41.20% 54.38% 
M2 66.99% 41.71% 54.35% 
M31 69.57% 55.70% 61.64% 
M32 69.25% 56.26% 62.76% 
KA-C    
M1 64.72% 63.17% 63.95% 
M2 64.44% 63.68% 64.06% 
M31 63.83% 66.79% 65.31% 
M32 63.49% 67.04% 65.27% 
KB-C    
M1 69.57% 55.60% 62.59% 
M2 69.15% 56.10% 64.06% 
M31 68.36% 60.22% 64.29% 
M32 68.09% 60.78% 64.44% 
KB-C).  It shows that the set of collocates 
proposed by our system is more complete than the 
set of terms suggested by human experts.  
Compared with the recall rate of M0 in Table 1 
(i.e., 69.53%), the recall rates of both Yapex- and 
KeX-based integration are increased, i.e., 77.52% 
and 70.60%, respectively.  That matches our 
expectation.  However, the precision rates are 
decreased more than the increase of recall rates.  
In particular, the F-score of KeX-based integration 
strategy is 4.70% worse than that of the baseline 
M0.  It shows that KeX performed not well in this 
test set, so it cannot recommend good candidates in 
the integration stage.  Moreover, the F-scores of 
M31 and M32 of YA and YA-C are better than that 
of M0 in Table 1.  It reveals that Yapex 
performed better in this test corpus, so that we can 
enhance the performance by both the filtering and 
integration strategies.  Nevertheless, the models 
in Tables 2 and 3 still cannot compete to M32 in 
Table 1.  The reason may be some heuristic rules 
used in Yapex are modified from KeX (Olsson et 
al., 2002). 
6 Concluding Remarks 
This paper shows a fully automatic way of mining 
collocates from scientific text in the protein 
domain, and employs them to improve the 
performance of protein name recognition 
successfully.  The same approach can be extended 
to other domains like gene, DNA, RNA, drugs, and 
so on.  The collocates extracted from a domain 
corpus are also important keywords for pathway 
discovery, so that a systematic way from basic 
named entities finding to complex relationships 
discovery can be established. 
Applying filtering strategy only demonstrates 
better performance than applying both filtering and 
integration strategies together in this paper.  One 
of the possible reasons is that the adopted systems 
are similar, i.e., both systems are rule-based, and 
some heuristic steps used in one system are 
inherited from another.  The effects of combining 
different types of protein name taggers, e.g., 
rule-based and corpus-based, will be investigated 
in the future. 
Acknowledgements 
Part of research results was supported by National 
Science Council under the contract 
NSC-91-2213-E-002-088.  We also thank Dr. 
George Demetriou in the Department of the 
Computer Science of the University of Sheffield, 
who kindly supported the resources in this work. 
References 
Blaschke, C., Andrade, M.A., Ouzounis, C. and 
Valencia, A. (1999) ?Automatic Extraction of 
Biological Information from Scientific Text: 
Protein-Protein Interactions,? Proceedings of 7th 
International Conference on Intelligent Systems for 
Molecular Biology, pp. 60-67. 
Chen, H.H. and Lee, J.C. (1996) ?Identification and 
Classification of Proper Nouns in Chinese Texts,? 
Proceedings of 16th International Conference on 
Computational Linguistics, pp. 222-229. 
Chen, H.H.; Ding, Y.W. and Tsai, S.C. (1998) ?Named 
Entity Extraction for Information Retrieval,? 
Computer Processing of Oriental Languages, 
Special Issue on Information Retrieval on Oriental 
Languages, 12(1), 1998, pp. 75-85. 
Collier, N., Park, H.S., Ogata, N., Tateishi, Y., Nobata, 
C. and Ohta, T. (1999) ?The GENIA project: 
Corpus-based Knowledge Acquisition and 
Information Extraction from Genome Research 
Papers,? Proceedings of the Annual Meeting of the 
European Chapter of the Association for 
Computational Linguistics (EACL?99), June. 
Collier, N., Nobata, C. and Tsujii J.I. (2000) ?Extracting 
the Names of Genes and Gene Products with a 
Hidden Markov Model,? Proceedings of 18th 
International Conference on Computational 
Linguistics, pp. 201-207. 
Craven, M. and Kumlien, J. (1999) ?Constructing 
Biological Knowledge Bases by Extracting 
Information from Text Sources, Proceedings of 7th 
International Conference on Intelligent Systems for 
Molecular Biology, pp. 77-86. 
DARPA (1998) Proceedings of 7th Message 
Understanding Conference. 
Fox, C. Lexical Analysis and Stoplists. In Information 
Retrieval: Data Structures and Algorithms, Frakes, 
W. B. and Baeza-Yates, R., ed., Prentice Hall, 
102-130, 1992. 
Friedman, C., Kra, P., Yu, H., Krauthammer, M. and 
Rzhetsky, A. (2001) ?GENIES: A Natural Language 
Processing System for the Extraction of Molecular 
Pathways from Journal Articles,? Bioinformatics, 
17(S1), pp. 74-82. 
Fukuda, K., Tsunoda, T., Tamura, A., and Takagi, T. 
(1998) ?Toward Information Extraction: Identifying 
Protein Names from Biological Papers,? 
Proceedings of Pacific Symposium on Biocomputing, 
pp. 707-718. 
Hirschman, L., Park, J.C., Tsujii, J., Wong, L. and Wu, 
C.H. (2002) ?Accomplishments and Challenges in 
Literature Data mining for Biology,? Bioinformatics, 
18(12), pp. 1553-1561. 
Hou, W.J. and Chen, H.H. (2002) ?Extracting Biological 
Keywords from Scientific Text,? Proceedings of 13th 
International Conference on Genome Informatics, 
pp. 571-573. 
Humphreys, K., Demetriou, G. and Gaizauskas, R. 
(2000) ?Two Applications of Information Extraction 
to Biological Science Journal Articles: Enzyme 
Interactions and Protein Structures,? Proceedings of 
Pacific Symposium on Biocomputing, 5, pp. 
502-513. 
Manning, C.D. and Schutze, H. (1999) Foundations of 
Statistical Natural Language Processing, The MIT 
Press. 
Marcotte, E.M., Xenarios, I. and Eisenberd, D. (2001) 
?Mining Literature for Protein-protein Interactions,? 
Bioinformatics, 17(4), pp. 359-363. 
Ng, S.-K. and Wong, M. (1999) ?Toward Routine 
Automatic Pathway Discovery from On-line 
Scientific Text Abstracts,? Proceedings of 10th 
International Conference on Genome Informatics, 
pp. 104-112. 
Olsson, F., Eriksson, G., Franzen, K., Asker, L. and 
Liden P. (2002) ?Notions of Correctness when 
Evaluating Protein Name Taggers,? Proceedings of 
the 19th International Conference on Computational 
Linguistics, pp. 765-771. 
Ono, T., Hishigaki, H., Tanigami, A., and Takagi, T. 
?Automated Extraction of Information on 
Protein-Protein Interactions from the Biological 
Literature,? Bioinformatics, 17(2), pp.155-161. 
Park, J.C., Kim, H.S., and Kim, J.J. (2001) 
?Bidirectional Incremental Parsing for Automatic 
Pathway Identification with Combinatory Categorial 
Grammar,? Proceedings of Pacific Symposium on 
Biocomputing, 6, pp. 396-407. 
Rindflesch, T.C., Tanabe, L., Weinstein, J.N. and Hunter, 
L. (2000) ?EDGAR: Extraction of Drugs, Genes, 
and Relations from Biomedical Literature,? 
Proceedings of Pacific Symposium on Biocomputing, 
5, pp. 517-528. 
Thomas, J., Milward, D., Ouzounis, C., Pulman, S., and 
Carroll, M. (2000) ?Automatic Extraction of Protein 
Interactions from Scientific Abstracts,? Proceedings 
of Pacific Symposium on Biocomputing, 5, pp. 
538-549. 
Wong, L. (2001) ?PIES, a Protein Interaction Extraction 
System,? Proceedings of Pacific Symposium on 
Biocomputing, 6, pp. 520-531. 
Appendix A. Collocates mined from corpus 
act (-, -ed, -ing, -ion, -ive, -ivities, -ivity, -s), 
activat (-e, -ed, -es, -ing, -ion, -or) , adopt (-,ed, -s), 
affect (-, -ed, -ing, -s), allow (-, -ed, -s), analy (-sed, 
-ses, -sis, -zed, -zing), appear (-, -s), arrange (-d, 
-ment), assembl (-ing, -y), associat (-e, -ed, -ion), 
bas (-e, -ed, -is), belong (-, -ing, -s), bind (-, -ing, 
-s) / bound, bond (-, -ed, -ing, -s), bridge (-, -d, -s), 
calculat (-ed, -ion), called, carr (-ied, -ier, -ies), 
cataly (-sed, -ses, -stic, -ze, -zed, -zes, -zing), cause 
(-, -d, -s), center (-, -ed) / centre (-, -s), chang (-e, 
-ed, -es, -ing), characteriz (-ation, -e, -ed, -es, -ing), 
charg (-e, -ed), class (-, -es, -ified, -ifying), cleav 
(-e, -ed, -es, -ing), clos (-e, -ed, -ing), coil (-, -ed), 
compar (-e, -ed, -ing, -ison, -isons), complex (-, -ed, 
-es), composed, compris (-es, -ing), conclu (-de, 
-ded, -sion, -sions), conserved, consist (-, -ed, -ent, 
-ing, -s), constitut (-e, -ed, -es), contact (-, -s), 
contain (-, -ed, -ing, -s), coordinat (-e, -ed, -es, 
-ion), correlat (-e, -ed), correspond (-, -ing), crystal 
(-, -lize, -lized, -lizes, -s), cycl (-e, -es, -ing), define 
(-d, -s), demonstrat (-e, -ed, -es, -ing), depend (-, 
-ent, -ing), derived, describe (-, -d), design (-, -ed, 
-ing), detail (-, -ed, -s), determin (-ation, -ations, -e, 
-ed, -es, -ing), differ (-ence, -ences, -s), diffract 
(-ing, -ion), digest (-ed, -s), dimer (-, -ic, -ization, 
-ize), direct (-, -ed, -s), discuss (-, -ed), display (-, 
-s), disrupt (-, -ed, -ing, -s), effect (-, -s), encod (-e, 
-ed, -ing), enhanc (-e, -ed, -er, -es, -ing), exhibit (-, 
-ed, -s), exist (-, -s), explain (-, -ed, -ing, -s), 
express (-ed, -ing), extend (-, -ed), facilitat (-e, -es, 
-ing), finding / found, fold (-, -ed, -ing, -s), form (-, 
-ed, -ing, -s), function (-, -al, -ing, -s), groove (-, 
-s), hydroly (-sis, -zed, -zes), identif (-ied, -ies, -y), 
implicat (-e, -ed, -ions), inactiv (-ated, -ates, -e), 
includ (-ed, -es, -ing), indicat (-e, -ed, -es, -ing), 
induc (-e, -ed, -es, -ing), inhibit (-, -ed, -ing, -ion, 
-or, -ors, -s), initiat (-ed, -es), insert (-, -ed, -ing), 
interact (-, -ing, -ion, -ions, -s), involv (-e, -ed, -es, 
-ing), isolated, lack (-, -s), lead (-, -ing, -s), ligand 
(-, -ed, -s), like, link (-, -ed, -ing), located, loop (-, 
-ing, -s), mediat (-e, -ed, -es, -ing), model (-, -ed, 
-ing, -s), modul (-ate, -ates, -ating, -e, -es), mutat 
(-ed, -ions), observ (-e, -ed), obtain (-, -ed), occup 
(-ied, -ies), occur (-, -red, -s), organiz (-ation, -ed), 
oxidiz (-ed, -ing), phosphorylate (-d, -s), play (-, 
-s), position (-, -ed, -ing, -s), predict (-, -ed, -ing), 
presen (-ce, -ted, -ting), produc (-e, -ed, -es, -ing), 
promot (-e, -er, -es, -ing), proposed, proton (-, 
-ated, -s), provid (-e, -ed, -es, -ing), purif (-ied, -y), 
react (-, -ion, -tive, -s), recogni (-tion, -zed, -zes, 
-ing), reduc (-ed, -es, -ing, -tase, -tion), refined, 
regulat (-e, -ed, -es, -ing, -ion, -ory), relat (-ed, -es, 
-ive), repeat (-, -ed, -s), replaced, report (-, -ed), 
represent (-, -ed, -ing, -s), requir (-e, -ed, -es, -ing), 
resembl (-e, -ed, -es, -ing), resol (-ution, -ve), 
result (-, -ed, -ing, -s), reveal (-, -ed, -s), select (-ed, 
-ive, -ively), sequence (-, -d, -s), serve (-, -s), shape 
(-, -d), share (-, -d, -s), show (-, -n, -s), signal (-, 
-ing, , -ling, -s), sol (-ution, -ved), stabili (sed, -ty, 
-ze, -zed, -zes, -zing), stimulat (-e, -ed, -es, -ion, 
-ory), strain (-, -s), strand (-, -ed, -s), structur (-al, 
-ally, -e, -ed, -es), stud (-ied, -ies, -y, -ying), 
substitut (-e, -es, -ion, -ions), substrate (-, -s), 
suggest (-, -ed, -ing, -ion, -s), support (-, -ing, -s), 
switch (-, -es), synthesi (-s, -ze, -zed), target (-, -ed, 
-ing, -s), transfer (-, -red), transport (-, -s), 
understand (-, -ing) / understood, unexpected, us 
(-e, -ing) 
Appendix B. Terms suggested by an expert 
accompan (-ied, -ies, -y, -ying), activat (-e, -ed, -es, 
-ing, -ion, -or, -ors, -ory), affect (-, -ed, -ing, -s), 
aggregat (-e, -ed, -es, -ing, -ion), assembl (-e, -ed, 
-es, -ing, -y), associat (-e, -ed, -es, -ing, -ion), 
attract (-, -ed, -ing, -ion, -s), bind (-, -ing, -s) / 
bound, catalys (-e, -ed, -es, -ing, -tic), catalyz (-e, 
-ed, -es, -ing), cluster (-, -ed, -ing, -s), communicat 
(-e, -ed, -es, -ing, -ion), complex (-, -ed, -es, -ing), 
construct (-, -ed, -ing, -ion, -s), control (-, -ed, -ing, 
-led, -ling, -s), cooperat (-e, -ed, -es, -ing, -ion, -or, 
-ors), correlat (-e, -ed, -es, -ing, -ion), coupl (-e, 
-ed, -es, -ing), crosslink (-, -ed, -ing, -s), 
deglycosylat (-e, -ed, -es, -ing, -ion, -ory), 
demethylat (-e, -ed, -es, -ing, -ion, -ory), 
dephosphorylat (-e, -ed, -es, -ing, -ion, -ory), effect 
(-, -ed, -ing, -s), eliminat (-e, -ed, -es, -ing, -ion), 
enabl (-e, -ed, -es, -ing), enhanc (-e, -ed, -er, -es, 
-ing), glycosylat (-e, -ed, -es, -ing, -ion, -ory), 
group (-, -ed, -ing, -s), help (-, -ed, -ing, -s), hinder 
(-, -ed, -ing, -s), inactivat (-e, -ed, -es, -ing, -ion, 
-or, -ors, -ory), inhibit (-, -ed, -ing, -ion, -or, -ors, 
-ory, -s), integrat (-e, -ed, -es, -ing, -ion), interact (-, 
-ed, -ing, -ion, -s), link (-, -ed, -ing, -s), methylat 
(-e, -ed, -es, -ing, -ion), obstacl (-e, -ed, -es, -ing), 
participat (-e, -ed, -es, -ing, -ion), phosphorylat (-e, 
-ed, -es, -ing, -ion, -ory), prim (-e, -ed, -es, -ing), 
process (-, -ed, -es, -ing), react (-, -ed, -ing, -ion, 
-or, -ors, -ory, s), regulat (-e, -ed, -es, -ing, 
-ion, ,-or, -ory), relat (-e, -ed, -es, -ing, -ion), signal 
(-, -ed, -ing, , -led, -ling, -s), stimulat (-e, -ed, -es, 
-ing, -ion, ,-or, -ory), suppress (-, -ed, -es, -ing, 
-ion), transduc (-e, -ed, -es, -ing, -tion, ,-tor, -tory), 
trigger (-, -ed, -ing, -s) 
Support Vector Machine Approach to Extracting  
Gene References into Function from Biological Documents 
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen 
Natural Language Processing Laboratory 
Department of Computer Science and Information Engineering 
National Taiwan University 
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw 
 
Abstract 
In the biological domain, extracting newly 
discovered functional features from the 
massive literature is a major challenging issue.  
To automatically annotate Gene References 
into Function (GeneRIF) in a new literature is 
the main goal of this paper.  We tried to find 
GRIF words in a training corpus, and then 
applied these informative words to annotate the 
GeneRIFs in abstracts with several different 
weighting schemes.  The experiments showed 
that the Classic Dice score is at most 50.18%, 
when the weighting schemes proposed in the 
paper (Hou et al, 2003) were adopted.  In 
contrast, after employing Support Vector 
Machines (SVMs) and the definition of classes 
proposed by Jelier et al (2003), the score 
greatly improved to 56.86% for Classic Dice 
(CD).  Adopting the same features, SVMs 
demonstrated advantage over the Na?ve Bayes 
Classifier.  Finally, the combination of the 
former two models attained a score of 59.51% 
for CD. 
1 Introduction 
Text Retrieval Conference (TREC) has been 
dedicated to information retrieval and information 
extraction for years.  TREC 2003 introduced a new 
track called Genomics Track (Hersh and 
Bhupatiraju, 2003) to address the information 
retrieval and information extraction issues in the 
biomedical domain.  For the information extraction 
part, the goal was to automatically reproduce the 
Gene Reference into Function (GeneRIF) resource 
in the LocusLink database (Pruitt et al, 2000.) 
GeneRIF associated with a gene is a sentence 
describing the function of that gene, and is currently 
manually generated. 
This paper presents the post-conference work on 
the information extraction task (i.e., secondary task).  
In the official runs, our system (Hou et al, 2003) 
adopted several weighting schemes (described in 
Section 3.2) to deal with this problem.  However, 
we failed to beat the simple baseline approach, 
which always picks the title of a publication as the 
candidate GeneRIF.  Bhalotia et al (2003) 
converted this task into a binary classification 
problem and trained a Na?ve  Bayes classifier with 
kernels, achieving 53.04% for CD.  In their work, 
the title and last sentence of an abstract were 
concatenated and features were then extracted from 
the resulting string.  Jelier et al (2003) observed the 
distribution of target GeneRIFs in 9 sentence 
positions and converted this task into a 9-class 
classification problem, attaining 57.83% for CD.  
Both works indicated that the sentence position is 
of great importance.  We therefore modified our 
system to incorporate the position information with 
the help of SVMs and we also investigated the 
capability of SVMs versus Na?ve  Bayes on this 
problem. 
The rest of this paper is organized as follows.  
Section 2 presents the architecture of our extracting 
procedure.  The basic idea and the experimental 
methods in this study are introduced in Section 3.  
Section 4 shows the results and makes some 
discussions.  Finally, Section 5 concludes the 
remarks and lists some future works. 
2 Architecture Overview 
A complete annotation system may be done at two 
stages, including (1) extraction of molecular 
function for a gene from a publication and (2) 
alignment of this function with a GO term.  Figure 
1 shows an example.  The left part is an MEDLINE 
abstract with the function description highlighted.  
The middle part is the corresponding GeneRIF.  
The matching words are in bold, and the similar 
words are underlined.  The right part is the GO 
annotation.  This figure shows a possible solution of 
maintaining the knowledge bases and ontology 
using natural language processing technology.  We 
addressed automation of the first stage in this paper. 
The overall architecture is shown in Figure 2.  
First, we constructed a training corpus in such a 
way that GeneRIFs were collected from LocusLink 
and the corresponding abstracts were retrieved from 
54
MEDLINE.  ?GRIF words? and their weights were 
derived from the training corpus.  Then Support 
Vector Machines were trained using the derived 
corpus.  Given a new abstract, a sentence is selected 
from the abstract to be the candidate GeneRIF. 
3 Methods  
We adopted several weighting schemes to locate the 
GeneRIF sentence in an abstract in the official runs 
(Hou et al, 2003).  Inspired by the work by Jelier et 
al. (2003), we incorporated their definition of 
classes into our weighting schemes, converting this 
task into a classification problem using SVMs as 
the classifier.  We ran SVMs on both sets of 
features proposed by Hou et al (2003) and Jelier et 
al. (2003), respectively.  Finally, all the features 
were combined and some feature selection methods 
were applied to train the classifier. 
3.1 Training and test material preparation 
Since GeneRIFs are often cited verbatim from 
abstracts, we decided to reproduce the GeneRIF by 
selecting one sentence in the abstract.  Therefore, 
for each abstract in our training corpus, the sentence 
most similar to the GeneRIF was labelled as the 
GeneRIF sentence using Classic Dice coefficient as 
similarity measure.  Totally, 259,244 abstracts were 
used, excluding the abstracts for testing.  The test 
data for evaluation are the 139 abstracts used in 
TREC 2003 Genomics track. 
3.2 GRIF words extraction and weighting 
scheme  
We called the matched words between GeneRIF 
and the selected sentence as GRIF words in this 
paper.  GRIF words represent the favorite 
vocabulary that human experts use to describe gene 
functions.  After stop word removal and stemming 
operation, 10,506 GRIF words were extracted. 
In our previous work (Hou et al, 2003), we first 
generated the weight for each GRIF word.  Given 
an abstract, the score of each sentence is the sum of 
weights of all the GRIF words in this sentence.  
Finally, the sentence with the highest score is 
selected as the  candidate GeneRIF.  This method is 
denoted as OUR weighting scheme, and several 
heuristic weighting schemes were investigated.  
Here, we only present the weighting scheme used in 
SVMs classification.  The weighting scheme is as 
follows. For GRIF word i, the number of 
occurrence Gin  in all the GeneRIF sentences and the 
number of occurrence Ain  in all the abstracts were 
computed and AiGi nn /  was assigned to GRIF word i 
as its weight. 
3.3 Classification 
3.3.1 Class definition and feature extraction 
The distribution of GeneRIF sentences showed that 
the position of a sentence in an abstract is an 
important clue to where the answer sentence is.  
Jelier et al (2003) considered only the title, the first 
three and the last five sentences, achieving the best 
performance in TREC official runs.  Their Na?ve 
Bayes model is as follows.  An abstract a is 
assigned a class vj by calculating vNB: 
 
Existing 
GeneRIFs 
on 
LocusLink 
Corresponding 
Medline 
Abstracts  
GRIF Word 
Extractor 
Weighted GRIF 
Words 
Generating 
Training Data 
Training SVMs 
New 
Abstract 
GeneRIF 
 Sentence Locator  
Candidate 
GeneRIF 
 
Figure 2: Architecture of Extracting Candidate GeneRIF 
Figure 1: An Example of Complete Annotation from a Literature to Gene Ontology 
 
extraction 
alignm
ent 
The Bcl10 gene was recently isolated 
from the breakpoint region of 
t(1;14)(p22;q32) in mucosa-associated 
lymphoid tissue (MALT) lymphomas. 
Somatic mutations of Bcl10 were found 
in not only t(1;14)-bearing MALT 
lymphomas, but also a wide range of 
other tumors. ? ? Our results strongly 
suggest that somatic mutations  of Bcl10 
are extremely rare in malignant 
cartilaginous tumors  and do not 
commonly contribute to their molecular 
pathogenesis. 
PMID: 11836626 
MEDLINE abstract 
Mutations, 
relatively 
common in 
lymphomas, 
are extremely 
rare in 
malignant 
cartilaginous 
tumors. 
GeneRIF 
l GO:0005515  
term: protein binding 
definition: Interacting selectively with any protein, or 
protein complex (a complex of two or more proteins that 
may include other nonprotein molecules). 
l GO:0008181  
term: tumor suppressor 
l GO:0006917  
term: induction of apoptosis 
l GO:0005622 
term: intracellular 
l GO:0016329  
term: apoptosis regulator activity 
definition: The function held by products which directly 
regulate any step in the process of apoptosis. 
l GO:0045786  
term: negative regulation of cell cycle 
GO annotation 
55
  CD MUD MBD MBDP 
1 Jelier (Sentence-wise bag of words + Na?ve  Bayes) 57.83% 59.63% 46.75% 49.11% 
2 Sentence-wise bag of words + SVMs 58.92% 61.46% 47.86% 50.84% 
3 OUR Weighting scheme 50.18% 46.71% 33.47% 38.83% 
4 OUR Weighting scheme + SVMs 56.86% 58.81% 45.08% 48.10% 
5 Combined 59.51% 62.16% 48.17% 51.25% 
6 Combined + gene/protein names 57.59% 59.95% 46.69% 49.68% 
7 Combined + BWRatio feature selection 57.59% 59.90% 47.11% 50.08% 
8 Combined + Graphical feature selection 58.81% 61.09% 47.98% 50.92% 
9 Optimal Classifier 67.60% 70.74% 59.28% 62.09% 
10 Baseline 50.47% 52.60% 34.82% 37.91% 
Table 2: Comparison of performances on the 139 abstracts 
,
,argmax ( ) ( | )
j a i
NB j k i j
v V i S k W
v P v P w v
? ? ?
= ?? ?
 
where vj is one of the nine positions aforementioned, 
S is the set of 9 sentence positions, Wa,i is the set of 
all word positions in sentence i in abstract a, wk,i is 
the occurrence of the normalized word at position k 
in sentence i and V is the set of 9 classes. 
We, therefore, represented each abstract by a 
feature vector composed of the scores of 9 
sentences.  Furthermore, with a list of our 10,506 
GRIF words at hand, we also computed the 
occurrences of these words in each sentence, given 
an abstract.  Each abstract is then represented by the 
number of occurrences of these words in the 9 
sentences respectively, i.e., the feature vector is 
94,554 in length.  Classification based on this type 
of features is denoted the sentence-wise bag of 
words model in the rest of this paper.  Combining 
these two models, we got totally 94,563 features. 
Since we are extracting sentences discussing gene 
functions, it?s reasonable to expect gene or protein 
names in the GeneRIF sentence.  Therefore, we 
employed Yapex (Olsson et al, 2002) and 
GAPSCORE (Chang et al, 2004) protein/gene 
name detectors to count the number of protein/gene 
names in each of the 9 sentences, resulting in 
94,581 features.  
3.3.2 Training SVMs 
The whole process related to SVM was done via 
LIBSVM ? A Library for Support Vector Machines 
(Hsu et al, 2003).  Radial basis kernel was adopted 
based on our previous experience.  However, 
further verification showed that the combined 
model with either linear or polynomial kernel only 
slightly surpassed the baseline, attaining 50.67% for 
CD.  In order to get the best-performing classifier, 
we tuned two parameters, C and gamma.  They are 
the penalty coefficient in optimization and a 
parameter for the radial basis kernel, respectively.  
Four-fold cross validation accuracy was used to 
select the best parameter pair. 
3.3.3 Picking up the answe r sentence  
Test instances were first fed to the classifier to get 
the predicted positions of GeneRIF sentences.  In 
case that the predicted position doesn?t have a 
sentence, which would happen when the abstract 
doesn?t have enough sentences, the sentence with 
the highest score is picked for the weighting 
scheme and the combined model, otherwise the title 
is picked for the sentence-wise bag of words model. 
4 Results and Discussions  
The performance measures are based on Dice 
coefficient, which calculates the overlap between 
the candidate GeneRIF and actual GeneRIF.  
Classic Dice (CD) is the classic Dice formula using 
a common stop word list and the Porter stemming 
algorithm.  Due to lack of space, we referred you to 
the Genomics track overview for the other three 
modifications of CD (Hersh and Bhupatiraju, 2003). 
The evaluation results are shown in Table 2.  The 
1st row shows the official run of Jelier?s team, the 
first place in the official runs.  The 2nd row shows 
the performance when the Na?ve Bayes classifier 
adopted by Jelier is replaced with SVMs.  The 3rd 
row is the performance of our weighting scheme 
without a classifier.  The 4th row then lists the 
performance when our weighting scheme is 
combined with SVMs.  The 5th row is the result 
when our weighting scheme and the sentence-wise 
bag of words model are combined together.  The 6th 
row is the result when two gene/protein name 
detectors are incorporated into the combined model.  
The next two rows were obtained after two feature 
selection methods were applied.  The 9th row shows 
the performance when the classifier always 
proposes a sentence most similar to the actual 
GeneRIF.  The last row lists the baseline, i.e., title 
is always picked. 
A comparative study on text categorization 
(Joachims, 1998) showed that SVMs outperform 
other classification methods, such as Na?ve  Bayes, 
C4.5, and k-NN.  The reasons would be that SVMs 
are capable of handling large feature space, text 
categorization has few irrelevant features, and 
document vectors are sparse.  The comparison 
56
between SVMs and the Na?ve  Bayes classifier again 
demonstrated the superiority of SVMs in text 
categorization (rows 1, 2). 
The performance greatly improved after 
introducing position information (rows 3, 4), 
showing the sentence position plays an important 
role in locating the GeneRIF sentence.  The 2% 
difference between rows 2 and 4 indicates that the 
features under sentence-wise bag of words model 
are more informative than those under our 
weighting scheme.  However, with only 9 features, 
our weighting scheme with SVMs performed fairly 
well.  Comparing the performance before and after 
combining our weighting scheme and the sentence-
wise bag of words model (rows 2, 5 and rows 4, 5), 
we can infer from the performance differences that 
both models provide mutually exclusive 
information in the combined model.  The result 
shown in row 6 indicates that the information of 
gene/protein name occurrences did not help identify 
the GeneRIF sentences in these 139 test abstracts. 
We performed feature selection on the combined 
model to reduce the dimension of feature space.  
There were two methods applied: a supervised 
heuristic method (denoted as BWRatio feature 
selection in Table 2) (S. Dutoit et al, 2002) and 
another unsupervised method (denoted as Graphical 
feature selection in Table 2) (Chang et al, 2002).  
The number of features was then reduced to about 
4,000 for both methods.  Unfortunately, the 
performance did not improve after either method 
was applied.  This may be attributed to over-fitting 
training data, because the cross-validation 
accuracies are indeed higher than those without 
feature selection.  The result may also imply there 
are little irrelevant features in this case. 
5 Conclusion and Future work 
This paper proposed an automatic approach to 
locate the GeneRIF sentence in an abstract with the 
assistance of SVMs, reducing the human effort in 
updating and maintaining the GeneRIF field in the 
LocusLink database. 
We have to admit that the 139 abstracts provided 
in TREC 2003 are too few to verify the 
performance among models, and the results based 
on these 139 abstracts may be slightly biased.  Our 
next step would aim at measuring the cross-
validation performances using Dice coefficient. 
The syntactic  information is worth exploring, 
since the sentences describing gene functions may 
share some common structural patterns.  Moreover, 
how the weighting scheme affects the performance 
is also very interesting.  We are currently trying to 
obtain a weighting scheme that can best distinguish 
GeneRIF sentence from non-GeneRIF sentence 
without classifiers. 
References  
G. Bhalotia, P.I. Nakov, A.S. Schwartz, and M.A. 
Hearst. 2003. BioText Team Report for the TREC 
2003 Genomics Track. TREC 2003 work notes: 
158-166. 
Y.C. I. Chang, H. Hsu and L.Y. Chou. 2002. 
Graphical Features Selection Method. Intelligent 
Data Engineering and Automated Learning, 
Edited by H. Yin, N. Allinson, R. Freeman, J. 
Keane, and S. Hubband. 
J.T. Chang, H. Schutze, R.B. Altman. 2004. 
GAPSCORE: finding gene and protein names one 
word at a time. Bioinformatics, 20(2):216-225. 
S. Dutoit, Y.H. Yang, M.J. Callow and T.P. Speed. 
2002. Statistical methods for identifying 
differentially expressed genes in replicated cDNA 
microarray experiments. J. Amer. Statis. Assoc. 
97:77-86. 
W. Hersh and Ravi Teja Bhupatiraju. 2003. TREC 
Genomics Track Overview. TREC 2003 work 
notes. 
W.J. Hou, C.Y. Teng, C. Lee and H.H. Chen. 2003. 
SVM Approach to GeneRIF Annotation. 
Proceedings of TREC 2003. 
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A 
Practical Guide to Support Vector Classification.  
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.ht
ml. 
R. Jelier, M. Schuemie, C.V.E. Eijk, M. Weeber, 
E.V. Mulligen, B. Schijvenaars, B. Mons and J. 
Kors. 2003. Searching for geneRIFs: concept-
based query expansion and Bayes classification. 
TREC 2003 work notes: 167-174. 
T. Joachims. 1998. Text Categorization with 
Support Vector Machines: Learning with Many 
Relevant Features. Proceedings of ECML-98, 
137-142. 
F. Olsson, G. Eriksson, K. Franz?n, L. Asker and P. 
Lid?n. 2002. Notions of Correctness when 
Evaluating Protein Name Taggers. Proceedings of 
the 19th International Conference on 
Computational Linguistics 2002, 765-771. 
K.D. Pruitt, K.S. Katz, H. Sicotte and D.R. Maglott. 
2000. Introducing RefSeq and LocusLink: 
Curated Human Genome Resources at the NCBI. 
Trends Genet, 16(1):44-47. 
T. Sekimizu, H.S. Park and J. Tsujji. 1998. 
Identifying the Interaction Between Genes and 
Gene Products Based on Frequently Seen Verbs 
in Medline Abstracts. Genome Information, 9:62-
71 
57
Annotating Multiple Types of Biomedical Entities:  
A Single Word Classification Approach 
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen 
Natural Language Processing Laboratory 
Department of Computer Science and Information Engineering 
National Taiwan University  
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw 
 
Abstract 
Named entity recognition is a fundamental 
task in biomedical data mining.  Multiple -class 
annotation is more challenging than single -
class annotation.  In this paper, we took a 
single word classification approach to dealing 
with the multiple -class annotation problem 
using Support Vector Machines (SVMs).  
Word attributes, results of existing 
gene/protein name taggers, context, and other 
information are important features for 
classification.  During training, the size of 
training data and the distribution of named 
entities are considered.  The preliminary 
results showed that the approach might be 
feasible when more training data is used to 
alleviate the data imbalance problem. 
1 Introduction 
The volumn of on-line material in the biomedical 
field has been growing steadily for more than 20 
years.  Several attempts have been made to mine 
knowledge from biomedical documents, such as 
identifying gene/protein names, recognizing 
protein interactions, and capturing specific 
relations in databases.  Among these, named entity 
recognition is a fundamental step to mine 
knowledge from biological articles. 
Previous approaches on biological named entity 
extraction can be classified into two types ? rule-
based (Fukuda et al, 1998; Olsson et al, 2002; 
Tanabe and Wilbur, 2002) and corpus-based 
(Collier et al, 2000; Chang et al, 2004).  Yapex 
(Olsson et al, 2002) implemented some heuristic 
steps described by Fukuda, et al, and applied 
filters and knowledge bases to remove false alarms.  
Syntactic information obtained from the parser was 
incorporated as well.  GAPSCORE (Chang et al, 
2004) scored words on the basis of statistical 
models that quantified their appearance, 
morphology and context.  The models includes 
Naive Bayes (Manning and Schutze, 1999), 
Maximum Entropy (Ratnaparkhi, 1998) and 
Support Vector Machines (Burges, 1998).  
GAPSCORE also used Brill?s tagger (Brill, 1994) 
to get the POS tag to filter out some words that are 
clearly not gene or protein names.  Efforts have 
been made (Hou and Chen, 2002, 2003; Tsuruoka 
and Tsujii, 2003) to improve the performance.  The 
nature of classification makes it possible to 
integrate existing approaches by extracting good 
features from them.  Several works employing 
SVM classifier have been done (Kazama et al, 
2002; Lee et al, 2003; Takeuchi and Collier, 2003; 
Yamamoto et al, 2003), and will be discussed 
further in the rest of this paper. 
Collocation denotes two or more words having 
strong relationships (Manning and Schutze, 1999).  
Hou and Chen (2003) showed that protein/gene 
collocates are capable of assisting existing 
protein/gene taggers.  In this paper, we addressed 
this task as a multi-class classification problem 
with SVMs and extended the idea of collocation to 
generate features at word and pattern level in our 
method.  Existing protein/gene recognizers were 
used to perform feature extraction as well. 
The rest of this paper is organized as follows.  
The methods used in this study are introduced in 
Section 2.  The experimental results are shown and 
discussed in Section 3.  Finally, Section 4 
concludes the remarks and lists some future works. 
2 Methods  
Most of the works in the past on recognizing 
named entities in the biomedical domain focused 
on identifying a single type of entities like protein 
and/or gene names.  It is obviously more 
challenging to annotate multiple types of named 
entities simultaneously.  Intuitively, one can 
develop a specific recognizer for each type of 
named entities, run the recognizers one by one to 
annotate all types of named entities, and merge the 
results.  The problem results from the boundary 
decision and the annotation conflicts.  Instead of 
constructing five individual recognizers, we 
regarded the multiple -class annotation as a 
classification problem, and tried to learn a 
80
classifier capable of identifying all the five types of 
named entities. 
Before classification, we have to decide the unit 
of classification.  Since it is difficult to correctly 
mark the boundary of a name to be identified, the 
simplest way is to consider an individual word as 
an instance and assign a type to it.  After the type 
assignment, continuous words of the same type 
will be marked as a complete named entity of that 
type.  The feature extraction process will be 
described in the following subsections. 
2.1 Feature Extraction 
The first step in classification is to extract 
informative and useful features to represent an 
instance to be classified.  In our work, one word is 
represented by the attributes carried per se, the 
attributes contributed by two surrounding words, 
and other contextual information.  The details are 
as follows. 
2.1.1 Word Attributes 
The word ?attribute? is sometimes used 
interchangeably with ?feature?, but in this article 
they denote two different concepts.  Features are 
those used to represent a classification instance, 
and the information enclosed in the features is not 
necessarily contributed by the word itself.  
Attributes are defined to be the information that 
can be derived from the word alone in this paper. 
The attributes assigned to each word are whether 
it is part of a gene/protein name, whether it is part 
of a species name, whether it is part of a tissue 
name, whether it is a stop word, whether it is a 
number, whether it is punctuation, and the part of 
speech of this word.  Instead of using a lexicon for 
gene/protein name annotation, we employed two 
gene/protein name taggers, Yapex and 
GAPSCORE, to do this job.  As for part of speech 
tagging, Brill?s part of speech tagger was adopted. 
2.1.2 Context Information Preparation 
Contextual information has been shown helpful in 
annotating gene/protein names, and therefore two 
strategies for extracting contextual information at 
different levels are used.  One is the usual practice 
at a word level, and the other is at a pattern level.  
Since the training data released in the beginning 
does not define the abstract boundary, we have to 
assume that sentences are independent of each 
other, and the contextual information extraction 
was thus limited to be within a sentence. 
For contextual information extraction at word 
level (Hou and Chen, 2003), collocates along with 
4 statistics including frequency, the average and 
standard error of distance between word and entity 
and t-test score, were extracted.  The frequency 
and t-test score were normalized to [0, 1].  Five 
lists of collocates were obtained for cell-line, cell-
type, DNA, RNA, and protein, respectively. 
As for contextual information extraction at 
pattern level, we first gathered a list of words 
constituting a specific type of named entities.  
Then a hierarchical clustering with cutoff threshold 
was performed on the words. Edit distance was 
adopted as the measure of dissimilarity (see Figure 
1). Afterwards, common substrings were obtained 
to form the list of patterns.  With a list of patterns 
at hand, we estimated the pattern distribution, the 
occurrence frequencies at and around the current 
position, given the type of word at the current 
position.  Figure 2 showed an example of the 
estimated distribution.  The average KL-
Divergence between any two distributions was 
computed to discriminate the power of each pattern.  
The formula is as follows: 
1 1,
1 ( || )
( 1)
n n
i j
i j j i
D p p
n n = = ?- ? ? , where pi and pj 
are the distributions of a pattern given the word at 
position 0 being type i and j, respectively. 
 
Figure 1: Example of common substring extraction 
 
Figure 2: Pattern distributions given the type of 
word at position 0 
2.2 Constructing Training Data 
For each word in a sentence, the attributes of the 
word and the two adjacent words are put into the 
feature vector.  Then, the left five and the right five 
words are searched for previously extracted 
collocates.  The 15 variables thus added are shown 
below. 
5
5, 0
( | )i
i i
Freq w type
=- ?
?
 
5
5, 0
_ ( | )i
i i
t test score w type
=- ?
-?  
81
5, ,
5, 0
??( | , )
i iw type w type
i i
f i m s
=- ?
? , where f is the pdf of 
normal distribution, type is one of the five types, wi 
denotes the surrounding words,
,
?
itypew
m and 
,
?
itypew
s are 
the maximum likelihood estimates of mean and 
standard deviation for wi given the type. Next, the 
left three and right three words along with the 
current word are searched for patterns, adding 6 
variables to the feature vector. 
3
3
Prob ( | )
wi
p
i p P
i type
=- ?
? ? , where type is one of the 
six types including ?O?, 
iwP is the set of patterns 
matching wi, Prob p  denotes the pmf for pattern p.  
Finally, the type of the previous word is added to 
the feature vector, mimicking the concept of a 
stochastic model. 
2.3 Classification 
Support Vector Machines classification with radial 
basis kernel was adopted in this task, and the 
package LIBSVM ? A Library for Support Vector 
Machines (Hsu et al, 2003) was used for training 
and prediction. The penalty coefficient C in 
optimization and gamma in kernel function were 
tuned using a script provided in this package. 
The constructed training data contains 492,551 
instances, which is too large for training.  Also, the 
training data is extremely unbalanced (see Table 1) 
and this is a known problem in SVMs 
classification.  Therefore, we performed stratified 
sampling to form a smaller and balanced data set 
for training. 
Type # of instances (words) 
cell-type 15,466 
DNA 25,307 
cell-line 11,217 
RNA 2,481 
protein 55,117 
O 382,963 
Table 1: Number of instances for each type 
3 Results and Discussion 
Since there is a huge amount of training instances 
and we do not have enough time to tune the 
parameters and train a model with all the training 
instances available, we first randomly selected one 
tenth and one fourth of the complete training data.  
The results, as we expected, showed that model 
trained with more instances performed better (see 
Table 2).  However, we noticed that the 
performances vary among the 6 types and one of 
the possible causes is the imbalance of training 
data among classes (see Table 1). Therefore we 
decided to balance the training data. 
First, the training data was constructed to 
comprise equal number of instances from each 
class.  However, it didn?t perform well and lots of 
type ?O? words were misclassified, indicating that 
using only less than 1% of type ?O? training 
instances is not sufficient to train a good model.  
Thus two more models were trained to see if the 
performance can be enhanced.  One model has 
slightly more type ?O? instances than the equally 
balanced one, and the other model has the ratio 
among classes being 4:8:4:1:8:16.  The results 
showed increase in recall but drop in precision. 
Kazama et al (2002) addressed the data 
imbalance problem and sped up the training 
process by splitting the type ?O? instances into sub-
classes using part-of-speech information.  However, 
we missed their work while we were doing this 
task, and hence didn?t have the chance to use and 
extend this idea. 
After carefully examining the classification 
results, we found that many of the ?DNA? 
instances were classified as ?protein? and many of 
the ?protein? instances were classified as ?DNA?.  
For example, 904 out of 2,845 ?DNA? instances 
were categorized as ?protein? under ?model 1/4?.  
The reason may be that Yapex and GAPSCORE do 
not distinguish gene name from protein names.  
Even humans don?t do very well at this 
(Krauthammer et al, 2002). 
We originally planned to verify the contribution 
of each type of features. For example, how much 
noise was introduced by using existing taggers 
instead of lexicons. This would have helped gain 
more insights into the proposed features. 
4 Conclusion and Future work 
This paper presented the preliminary results of our 
study.  We introduced the use of existing taggers 
and presented a way to collect common substrings 
shared by entities.  Due to lack of time, the models 
were not well tuned against the two parameters ? C 
and gamma, influencing the capabilities of the 
models.  Further, not all of the training instances 
provided were used to train the model, and it will 
be interesting and worthwhile to investigate.  How 
to deal with data imbalance is another important 
issue.  By solving this problem, further evaluation 
of feature effectiveness would be facilitated.  We 
believe there is much left for our approach to 
improve and it may perform better if more time is 
given. 
82
References  
E. Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. Proceedings of 
the National Conference on Artificial 
Intelligence. AAAI Press; 722-727. 
C. Burges. 1998. A Tutorial on Support Vector 
Machines for Pattern Recognition. Data Mining 
and Knowledge Discovery, 2: 121-167. 
J.T. Chang, H. Schutze and R.B. Altman. 2004. 
GAPSCORE: Finding Gene and Protein Names 
One Word at a Time. Bioinformatics, 20(2): 216-
225. 
N. Collier, C. Nobata and J.I. Tsujii. 2000. 
Extracting the Names of Genes and Gene 
Products with a Hidden Markov Model. 
Proceedings of 18 th International Conference on 
Computational Linguistics, 201-207. 
K. Fukuda, T. Tsunoda, A. Tamura and T. Takagi. 
1998. Toward Information Extraction: 
Identifying Protein Names from Biological 
Papers. Proceedings of Pacific Symposium on 
Biocomputing, 707-718. 
W.J. Hou and H.H. Chen 2002. Extracting 
Biological Keywords from Scientific Text. 
Proceedings of 13 th International Conference on 
Genome Informatics; 571-573. 
W.J. Hou and H.H. Chen. 2003. Enhancing 
Performance of Protein Name Recognizers 
Using Collocation. Proceedings of the ACL 2003 
Workshop on NLP in Biomedicine, 25-32. 
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A 
Practical Guide to Support Vector Classification. 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.h
tml. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition. Proceedings of the 
ACL 2002 workshop on NLP in the Biomedical 
Domain , 1-8. 
M. Krauthammer, P. Kra, I. Iossifov, S.M. Gomez, 
G. Hripcsak, V. Hatzivassiloglou, C. Friedman 
and A. Rzhetsky. 2002. Of truth and pathways: 
chasing bits of information through myriads of 
articles. Bioinformatics, 18(sup.1):S249-S257. 
K.J. Lee, Y.S. Hwang and H.C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs. Proceedings of the ACL 2003 Workshop 
on NLP in Biomedicine, 33-40. 
C.D. Manning and H. Schutze. 1999. Foundations 
of Statistical Natural Language Processing. MIT 
Press. 
F. Olsson, G. Eriksson, K. Franzen, L. Asker and P. 
Liden. 2002. Notions of Correctness when 
Evaluating Protein Name Taggers. Proceedings 
of the 19th International Conference on 
Computational Linguistics, 765-771. 
A. Ratnaparkrhi. 1998. Maximum Entropy Models 
for Natural Language Ambiguity Resolution. 
PhD Thesis, University of Pennsylvania. 
K. Takeuchi and N. Collier. 2003. Bio-Medical 
Entity Extraction using Support Vector 
Machines. Proceedings of the ACL 2003 
workshop on NLP in Biomedicine, 57-64. 
L. Tanabe and W.J. Wilbur. 2002. Tagging Gene 
and Protein Names in Biomedical Text. 
Bioimformatics, 18(8) : 1124-1132. 
Y. Tsuruoka and J. Tsujii. 2003. Boosting 
Precision and Recall of Dictionary-based Protein 
Name Recognition. Proceedings of the ACL 
2003 Workshop on NLP in Biomedicine, 41-48. 
K. Yamamoto, T. Kudo, A. Konagaya and Y. 
Matsumoto. 2003. Protein Name Tagging for 
Biomedical Annotation in Text. Proceedings of 
the ACL 2003 workshop on NLP in Biomedicine, 
65-72.
 Model 1/10 Model 1/4    
 Recall Prec. F-score Recall Prec. F-score Recall Prec. F-score 
Full (Object) 0.4756 0.4399 0.4571 0.5080 0.4759 0.4914    
Full (protein) 0.5846 0.4392 0.5016 0.6213 0.4614 0.5296    
Full (cell-line) 0.2420 0.2909 0.2642 0.2820 0.3341 0.3059    
Full (DNA) 0.2784 0.3249 0.2998 0.2888 0.4479 0.3512    
Full (cell-type) 0.3863 0.5752 0.4622 0.4196 0.6115 0.4977    
Full (RNA) 0.0085 0.1000 0.0156 0.0000 0.0000 0.0000    
 Model balanced equally Model slightly more ?O? Model 4:8:4:1:8:16 
Full (Object) 0.1480 0.0990 0.1186 0.1512 0.1002 0.1206 0.5036 0.3936 0.4419 
Full (protein) 0.1451 0.1533 0.1491 0.1458 0.1527 0.1492 0.5629 0.4280 0.4863 
Full (cell-line) 0.1580 0.0651 0.0922 0.2280 0.0319 0.0560 0.4060 0.2261 0.2904 
Full (DNA) 0.1326 0.0466 0.0690 0.1591 0.0582 0.0852 0.3759 0.2457 0.2972 
Full (cell-type) 0.1650 0.1375 0.1500 0.1494 0.1908 0.1676 0.4701 0.4900 0.4798 
Full (RNA) 0.0932 0.0067 0.0126 0.0169 0.0075 0.0104 0.0593 0.1148 0.0782 
Table 2: Performance of each model (only FULL is shown) 
83
