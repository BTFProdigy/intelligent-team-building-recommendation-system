183
184
185
186
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145?148,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Hidden Markov Tree Model in Dependency-based Machine Translation
?
Zden
?
ek
?
Zabokrtsk?y
Charles University in Prague
Institute of Formal and Applied Linguistics
zabokrtsky@ufal.mff.cuni.cz
Martin Popel
Charles University in Prague
Institute of Formal and Applied Linguistics
popel@matfyz.cz
Abstract
We would like to draw attention to Hid-
den Markov Tree Models (HMTM), which
are to our knowledge still unexploited in
the field of Computational Linguistics, in
spite of highly successful Hidden Markov
(Chain) Models. In dependency trees,
the independence assumptions made by
HMTM correspond to the intuition of lin-
guistic dependency. Therefore we suggest
to use HMTM and tree-modified Viterbi
algorithm for tasks interpretable as label-
ing nodes of dependency trees. In par-
ticular, we show that the transfer phase
in a Machine Translation system based
on tectogrammatical dependency trees can
be seen as a task suitable for HMTM.
When using the HMTM approach for
the English-Czech translation, we reach a
moderate improvement over the baseline.
1 Introduction
Hidden Markov Tree Models (HMTM) were intro-
duced in (Crouse et al, 1998) and used in appli-
cations such as image segmentation, signal classi-
fication, denoising, and image document catego-
rization, see (Durand et al, 2004) for references.
Although Hidden Markov Models belong to the
most successful techniques in Computational Lin-
guistics (CL), the HMTM modification remains to
the best of our knowledge unknown in the field.
The first novel claim made in this paper is that
the independence assumptions made by Markov
Tree Models can be useful for modeling syntactic
trees. Especially, they fit dependency trees well,
because these models assume conditional depen-
dence (in the probabilistic sense) only along tree
?
The work on this project was supported by the grants
MSM 0021620838, GAAV
?
CR 1ET101120503, and M
?
SMT
?
CR LC536. We thank Jan Haji?c and three anonymous review-
ers for many useful comments.
edges, which corresponds to intuition behind de-
pendency relations (in the linguistic sense) in de-
pendency trees. Moreover, analogously to applica-
tions of HMM on sequence labeling, HMTM can
be used for labeling nodes of a dependency tree,
interpreted as revealing the hidden states
1
in the
tree nodes, given another (observable) labeling of
the nodes of the same tree.
The second novel claim is that HMTMs are
suitable for modeling the transfer phase in Ma-
chine Translation systems based on deep-syntactic
dependency trees. Emission probabilities rep-
resent the translation model, whereas transition
(edge) probabilities represent the target-language
tree model. This decomposition can be seen as
a tree-shaped analogy to the popular n-gram ap-
proaches to Statistical Machine Translation (e.g.
(Koehn et al, 2003)), in which translation and lan-
guage models are trainable separately too. More-
over, given the input dependency tree and HMTM
parameters, there is a computationally efficient
HMTM-modified Viterbi algorithm for finding the
globally optimal target dependency tree.
It should be noted that when using HMTM, the
source-language and target-language trees are re-
quired to be isomorphic. Obviously, this is an un-
realistic assumption in real translation. However,
we argue that tectogrammatical deep-syntactic de-
pendency trees (as introduced in the Functional
Generative Description framework, (Sgall, 1967))
are relatively close to this requirement, which
makes the HMTM approach practically testable.
As for the related work, one can found a num-
ber of experiments with dependency-based MT
in the literature, e.g., (Boguslavsky et al, 2004),
(Menezes and Richardson, 2001), (Bojar, 2008).
However, to our knowledge none of the published
systems searches for the optimal target representa-
1
HMTM looses the HMM?s time and finite automaton in-
terpretability, as the observations are not organized linearly.
However, the terms ?state? and ?transition? are still used.
145
	
	 		
	
 Valency Frames of Czech Verbs in VALLEX 1.0
Zdene?k Z? abokrtsky?
Center for Computational Linguistics,
Charles University,
Malostranske? na?m. 25,
CZ-11800 Prague, Czech Republic
zabokrtsky@ckl.mff.cuni.cz
Marke?ta Lopatkova?
Center for Computational Linguistics,
Charles University,
Malostranske? na?m. 25,
CZ-11800 Prague, Czech Republic
lopatkova@ckl.mff.cuni.cz
Abstract
The Valency Lexicon of Czech Verbs, Version
1.0 (VALLEX 1.0) is a collection of linguisti-
cally annotated data and documentation, resul-
ting from an attempt at formal description of
valency frames of Czech verbs. VALLEX 1.0
is closely related to Prague Dependency Tre-
ebank. In this paper, the context in which
VALLEX came into existence is briefly outli-
ned, and also three similar projects for English
verbs are mentioned. The core of the paper is
the description of the logical structure of the
VALLEX data. Finally, we suggest a few di-
rections of the future research.
1 Introduction
The Prague Dependency Treebank1 (PDT) meets the
wide-spread aspirations of building corpora with rich an-
notation schemes. The annotation on the underlying (tec-
togrammatical) level of language description ((Hajic?ova?
et al, 2000)) ? serving among other things for training
stochastic processes ? allows to acquire a considerable
amount of data for rule-based approaches in computati-
onal linguistics (and, of course, for ?traditional? linguis-
tics). And valency belongs undoubtedly to the core of all
rule-based methods.
PDT is based on Functional Generative Description
of Czech (FGD), being developed by Petr Sgall and his
collaborators since the 1960s ((Sgall et al, 1986)). Wi-
thin FGD, the theory of valency has been studied since
the 1970s (see esp. (Panevova?, 1992)). Its modification
is used as the theoretical background in VALLEX 1.0
(see (Lopatkova?, 2003) for a detailed description of the
framework).
Valency requirements are considered for autosemantic
words ? verbs, nouns, adjectives, and adverbs. Now, its
1http://ufal.mff.cuni.cz/pdt
principles are applied to a huge amount of data ? that
means a great opportunity to verify the functional criteria
set up and the necessity to expand the ?center?, ?core? of
the language being described.
Within the massive manual annotation in PDT, the pro-
blem of consistency of assigning the valency structure
increased. This was the first impulse leading to the deci-
sion of creating a valency lexicon. However, the potential
usability of the valency lexicon is certainly not limited to
the context of PDT ? several possible applications have
been illustrated in ((Stran?a?kova?-Lopatkova? and Z? abokrt-
sky?, 2002)).
The Valency Lexicon of Czech Verbs, Version 1.0
(VALLEX 1.0) is a collection of linguistically annota-
ted data and documentation, resulting from this attempt
at formal description of valency frames of Czech verbs.
VALLEX 1.0 contains roughly 1400 verbs (counting only
perfective and imperfective verbs, but not their iterative
counterparts).2 They were selected as follows: (1) We star-
ted with about 1000 most frequent Czech verbs, according
to their number of occurrences in a part of the Czech Nati-
onal Corpus3 (only ?by?t? (to be) and some modal verbs
were excluded from this set, because of their non-trivial
status on the tectogrammatical level of FGD). (2) Then we
added their perfective or imperfective aspectual counter-
parts, if they were missing; in other words, the set of verbs
in VALLEX 1.0 is closed under the relation of ?aspectual
pair?.
The preparation of the first version of VALLEX has
taken more than two years. Although it is still a work
in progress requiring further linguistic research, the first
2Besides VALLEX, a larger valency lexicon (called
PDT-VALLEX, (Hajic? et al, 2003)) has been created during the
annotation of PDT. PDT-VALLEX contains more verbs (5200
verbs), but only frames occuring in PDT, whereas in VALLEX
the verbs are analyzed in the whole complexity, in all their me-
anings. Moreover, richer information is assigned to particular
valency frames in VALLEX.
3http://ucnk.ff.cuni.cz
version has been already publically released. The whole
VALLEX 1.0 can be downloaded from the Internet af-
ter filling the on-line registration form at the following
address: http://ckl.mff.cuni.cz/zabokrtsky/vallex/1.0/
From the very beginning, VALLEX 1.0 was designed
with an emphasis on both human and machine readability.
Therefore both linguists and developers of applications
within the Natural Language Processing domain can use
and critically evaluate its content. In order to satisfy diffe-
rent needs of these different potential users, VALLEX 1.0
contains the data in the following three formats:
  Browsable version. HTML version of the data
allows for an easy and fast navigation through the
lexicon. Verbs and frames are organized in several
ways, following various criteria.
  Printable version. For those who prefer to have a
paper version in hand. For a sample from the prin-
table version, see the Appendix.
  XML version. Programmers can run sophisticated
queries (e.g. based on XPATH query language) on
this machine-tractable data, or use it in their appli-
cations. Structure of the XML file is defined using a
DTD file (Document Type Definition), which natu-
rally mirrors logical structure of the data (described
in Sec. 3).
2 Similar Projects for English Verbs4
2.1 FrameNet
FrameNet ((Fillmore, 2002)) groups lexical units
(pairings of words and senses) into sets according to whe-
ther they permit parallel semantic descriptions. The verbs
belonging to a particular set share the same collection of
frame-relevant semantic roles. The ?general-purpose? se-
mantic roles (as Agent, Patient, Theme, Instrument, Goal,
and so on) are replaced by more specific ?frame-specific?
role names (e.g. Speaker, Addressee, Message and Topic
for ?speaking verbs?).
2.2 Levin Verb Classes
Levin semantic classes ((Levin, 1993)) are constructed
from verbs which undergo a certain number of alternations
(where an alternation means a change in the realization
of the argument structure of a verb, as e.g. ?conative al-
ternation? Edith cuts the bread ? Edith cuts at the bread).
These alternations are specific to English. For Czech, e.g.
particular types of diatheses can be considered as useful
alternations.
Both FrameNet and Levin classification are focused (at
least for the time being) only on selected meanings of
verbs.
4For comparison of PropBank, Lexical Conceptual Data-
base, and PDT, see (Hajic?ova? and Kuc?erova?, 2002).
2.3 PropBank
In the PropBank corpus ((Kingsbury and Palmer,
2002)) sentences are annotated with predicate-argument
structure. The human annotators use the lexicon conta-
ining verbs and their ?frames? ? lists of their possible
complementations. The lexicon is called ?Frame Files?.
Frame Files are mapped to individual members of Levin
classes.
There is only a minimal specification of the connecti-
ons between the argument types and semantic roles ? in
principle, a one-argument verb has arg0 in its frame, a
two-argument verb has arg0 and arg1, etc. Frame Files
store all the meanings of the verbs, with their description
and examples.
3 Logical Structure of the VALLEX Data
3.1 Word Entries
On the topmost level, VALLEX 1.0 is divided into word
entries (the HTML ?graphical? layout of a word entry
is depicted on Fig. 1). Each word entry relates to one
or more headword lemmas5 (Sec. 3.2). The word entry
consists of a sequence of frame entries (Sec. 3.5) relevant
for the lemma(s) in question (where each frame entry
usually corresponds to one of the lemma?s meanings).
Information about the aspect (Sec. 3.16) of the lemma(s)
is assigned to each word entry as a whole.
Figure 1: HTML layout of a word entry.
Most of the word entries correspond to lemmas in a
simple one-to-one manner, but the following two non-
trivial situations (and even combinations of them) appear
as well in VALLEX 1.0:
5Remark on terminology: The terms used here either belong
to the broadly accepted linguistic terminology, or come from the
Functional Generative Description (FGD), which we have used
as the background theory, or are defined somewhere else in this
text.
  lemma variants (Sec. 3.3)
  homonyms (Sec. 3.4)
The content of a word entry roughly corresponds to the
traditional term of lexeme.
3.2 Lemmas
Under the term of lemma (of a verb) we understand the
infinitive form of the respective verb, in case of homonym
(Sec. 3.4) followed by a Roman number in superscript
(which is to be considered as an inseparable part of the
lemma in VALLEX 1.0!).
Reflexive particles se or si are parts of the infinitive
only if the verb is reflexive tantum, primary (e.g. ba?t se)
as well as derived (e.g. zab??t se, s???r?it se, vra?tit se).
3.3 Lemma Variants
Lemma variants are groups of two (or more) lemmas that
are interchangable in any context without any change of
the meaning (e.g. dove?de?t se/dozve?de?t se). The only diffe-
rence usually is just a small alternation in the morphologi-
cal stem, which might be accompanied by a subtle stylis-
tic shift (e.g. myslet/myslit, the latter one being bookish).
Moreover, although the infinitive forms of the variants di-
ffer in spelling, some of their conjugated forms are often
identical (mysli (imper.sg.) both for myslet and myslit).
The term ?lemma variants? should not be confused with
the term ?synonymy?.
3.4 Homonyms
There are pairs of word entries in VALLEX 1.0, the lem-
mas of which have the same spelling, but considerably
differ in their meanings (there is no obvious semantic re-
lation between them). They also might differ as to their
etymology (e.g. nakupovat   - to buy vs. nakupovat    - to
heap), aspect (Sec. 3.16) (e.g. stac?it   pf. - to be enough
vs. stac?it
  
impf. - to catch up with), or conjugated forms
(z?ilo (past.sg.fem) for z???t   - to live vs. z?alo(past.sg.fem)
z???t
  
- to mow). Such lemmas (homonyms)6 are distingu-
ished by Roman numbering in superscript. These numbers
should be understood as an inseparable part of lemma in
VALLEX 1.0.
3.5 Frame Entries
Each word entry consists of a non-empty sequence of
frame entries, typically corresponding to the individual
meanings (senses) of the headword lemma(s) (from this
point of view, VALLEX 1.0 can be classified as a Sense
Enumerated Lexicon).
6Note on terminology: we have adopted the term ?homo-
nyms? from Czech linguistic literature, where it traditionally
stands for what was stated above (words identical in the spelling
but considerably different in the meaning); in English literature
the term ?homographs? is sometimes used to express the same
notion.
The frame entries are numbered within each word en-
try; in the VALLEX 1.0 notation, the frame numbers are
attached to the lemmas as subscripts.
The ordering of frames is not completely random, but
it is not perfectly systematic either. So far it is based only
on the following weak intuition: primary and/or the most
frequent meanings should go first, whereas rare and/or idi-
omatic meanings should go last. (We do not guarantee that
the ordering of meanings in this version of VALLEX 1.0
exactly matches their frequency of the occurrences in con-
temporary language.)
Each frame entry7 contains a description of the va-
lency frame itself (Sec. 3.6) and of the frame attributes
(Sec. 3.13).
3.6 Valency Frames
In VALLEX 1.0, a valency frame is modeled as a sequence
of frame slots. Each frame slot corresponds to one (either
required or specifically permitted) complementation8 of
the given verb.
The following attributes are assigned to each slot:
  functor (Sec. 3.7)
  list of possible morphemic forms (realizations)
(Sec. 3.8)
  type of complementation (Sec. 3.11)
Some slots tend to systematically occur together. In
order to capture this type of regularity, we introduced the
mechanism of slot expansion (Sec. 3.12) (full valency
frame will be obtained after performing these expansions).
3.7 Functors
In VALLEX 1.0, functors (labels of ?deep roles?; similar
to theta-roles) are used for expressing types of relations
between verbs and their complementations. According to
FGD, functors are divided into inner participants (actants)
and free modifications (this division roughly corresponds
to the argument/adjunct dichotomy). In VALLEX 1.0,
we also distinguish an additional group of quasi-valency
complementations.
Functors which occur in VALLEX 1.0 are listed in the
following tables (for Czech sample sentences see (Lopat-
kova? et al, 2002), page 43):
Inner participants:
  ACT (actor): Peter read a letter.
  ADDR (addressee): Peter gave Mary a book.
7Note on terminology: The content of ?frame entry? rou-
ghly corresponds to the term of lexical unit (?lexie? in Czech
terminology).
8Note on terminology: in this text, the term ?complemen-
tation? (dependent item) is used in its broad sense, not related to
the traditional argument/adjunct (complement/modifier) dicho-
tomy (or, if you want, covering both ends of the dichotomy).
  PAT (patient): I saw him.
  EFF (effect): We made her the secretary.
  ORIG (origin): She made a cake from apples.
Quasi-valency complementations:
  DIFF (difference): The number has swollen by 200.
  OBST(obstacle): The boy stumbled over a stumb.
  INTT (intent): He came there to look for Jane.
Free modifications:
  ACMP (accompaniement): Mother came
with her children.
  AIM (aim): John came to a bakery
for a piece of bread.
  BEN (benefactive): She made this for her children.
  CAUS (cause): She did so since they wanted it.
  COMPL (complement): They painted the wall blue.
  DIR1 (direction-from): He went from the forest to
the village.
  DIR2 (direction-through): He went
through the forest to the village.
  DIR3 (direction-to): He went from the forest
to the village.
  DPHR (dependent part of a phraseme): Peter talked
horse again.
  EXT (extent): The temperatures reached
an all time high.
  HER (heritage): He named the new villa
after his wife.
  LOC (locative): He was born in Italy.
  MANN (manner): They did it quickly.
  MEANS (means): He wrote it by hand.
  NORM (norm): Peter has to do it
exactly according to directions.
  RCMP (recompense): She bought a new shirt
for 25 $.
  REG (regard): With regard to George she asked his
teacher for advice.
  RESL (result): Mother protects her children
from any danger.
  SUBS (substitution): He went to the theatre
instead of his ill sister.
  TFHL (temporal-for-how-long): They interrupted
their studies for a year.
  TFRWH (temporal-from-when): His bad reminis-
cences came from this period.
  THL (temporal-how-long ): We were there
for three weeks.
  TOWH (temporal-to when): He put it over
to next Tuesday.
  TSIN (temporal-since-when): I have not heard about
him since that time.
  TWHEN (temporal-when): His son was born
last year.
Note 1: Besides the functors listed in the tables above,
also value DIR occurs in the VALLEX 1.0 data. It is used
only as a special symbol for slot expansion (Sec. 3.12).
Note 2: The set of functors as introduced in FGD is
richer than that shown above, moreover, it is still being
elaborated within the Prague Dependency Treebank. We
do not use its full (current) set in VALLEX 1.0 due to se-
veral reasons. Some functors do not occur with a verb at
all (e.g. APP - appuertenace, ?my.APP dog?), some other
functors can occur there, but represent other than depen-
dency relation (e.g. coordination, ?Jim or.CONJ Jack?).
And still others can occur with verbs as well, but their be-
haviour is absolutely independent of the head verb, thus
they have nothing to do with valency frames (e.g. ATT -
attitude, ?He did it willingly.ATT?).
3.8 Morphemic Forms
In a sentence, each frame slot can be expressed by a li-
mited set of morphemic means, which we call forms. In
VALLEX 1.0, the set of possible forms is defined either
explicitly (Sec. 3.9), or implicitly (Sec. 3.10). In the for-
mer case, the forms are enumerated in a list attached to
the given slot. In the latter case, no such list is specified,
because the set of possible forms is implied by the functor
of the respective slot (in other words, all forms possibly
expressing the given functor may appear).
3.9 Explicitly Declared Forms
The list of forms attached to a frame slot may contain
values of the following types:
  Pure (prepositionless) case. There are seven mor-
phological cases in Czech. In the VALLEX 1.0 no-
tation, we use their traditional numbering: 1 - no-
minative, 2 - genitive, 3 - dative, 4 - accusative, 5 -
vocative, 6 - locative, and 7 - instrumental.
  Prepositional case. Lemma of the preposition (i.e.,
preposition without vocalization) and the number of
the required morphological case are specified (e.g.,
z+2, na+4, o+6. . . ). The prepositions occurring in
VALLEX 1.0 are the following: bez, do, jako, k,
kolem, kvu?li, mezi, m??sto, na, nad, na u?kor, o, od,
ohledne?, okolo, oproti, po, pod, podle, pro, proti,
pr?ed, pr?es, pr?i, s, u, v, ve prospe?ch, vu?c?i, v za?jmu,
z, za. (?jako? is traditionally considered as a con-
junction, but it is included in this list, as it requires a
particular morphological case in some valency fra-
mes).
  Subordinating conjunction. Lemma of the con-
junction is specified. The following subordinating
conjunctions occur in VALLEX 1.0: aby, at?, az?, jak,
zda,9 z?e.
  Infinitive construction. The abbreviation ?inf?
stands for infinitive verbal complementation. ?inf?
can appear together with a preposition (e.g.
?nez?+inf?), but it happens very rarely in Czech.
  Construction with adjectives. Abbreviation ?adj-
digit? stands for an adjective complementation in the
given case, e.g. adj-1 (C??t??m se slaby? - I feel weak).
  Constructions with ?by?t? . Infinitive of verb ?by?t? (to
be) may combine with some of the types above, e.g.
by?t+adj-1 (e.g. zda? se to by?t dostatec?ne? - it seems to
be sufficient).
  Part of phraseme. If the set of the possible le-
xical values of the given complementation is very
small (often one-element), we list these values di-
rectly (e.g. ?napospas? for phraseme ?ponechat na-
pospas? - to expose).
3.10 Implicitly Declared Forms
If no forms are listed explicitly for a frame slot, then the
list of possible forms implicitly results from the functor of
the slot according to the following (yet incomplete) lists:
  LOC: adverb, na+6, v+6, u+2, pr?ed+7, za+7, nad+7,
pod+7, okolo+2, kolem+2, pr?i+6, vedle+2, mezi+7,
mimo+4, naproti+3, pode?l+2 . . .
  MANN: adverb, 7, na+4, . . .
  DIR3: adverb, na+4, v+4, do+2, pr?ed+4, za+4,
nad+4, pod+4, vedle+2, mezi+4, po+4, okolo+2, ko-
lem+2, k+3, mimo+4, naproti+3 . . .
  DIR1: adverb, z+2, od+2, zpod+2, zpoza+2, zpr?ed+2
. . .
  DIR2: adverb, 7, pr?es+4, pode?l+2, mezi+7, . . .
  TWHEN: adverb, 2, 4, 7, pr?ed+7, za+4, po+6, pr?i+6,
za+2, o+6, k+3, mezi+7, v+4, na+4, na+6, kolem+2,
okolo+2, . . .
  THL: adverb, 4, 7, po+4, za+4, . . .
  EXT: adverb, 4, na+4, kolem+2, okolo+2, . . .
  REG: adverb, 7, na+6, v+6, k+3, pr?i+6, ohledne?+2,
nad+7, na+4, s+7, u+2, . . .
9Note: form ?zda? is in fact an abbreviation for couple of
conjunctions ?zda? and ?jestli?.
  TFRWH: z+2, od+2, . . .
  AIM: k+3, na+4, do+2, pro+4, proti+3, aby, at?, z?e,
. . .
  TOWH: na+4 . . .
  TSIN: od+2 . . .
  TFHL: na+4, pro+4, . . .
  NORM: podle+2, v duchu+2, po+6, . . .
  MEANS: 7, v+6,na+6,po+6, z+2, z?e, s+7, na+4,
za+4, pod+7, do+2, . . .
  CAUS: 7, za+4, z+2, kvu?li+2, pro+4, k+3, na+4, z?e,
. . .
3.11 Types of Complementations
Within the FGD framework, valency frames (in a narrow
sense) consist only of inner participants (both obligatory10
and optional, ?obl? and ?opt? for short) and obligatory free
modifications; the dialogue test was introduced by Pane-
vova? as a criterium for obligatoriness. In VALLEX 1.0,
valency frames are enriched with quasi-valency comple-
mentations. Moreover, a few non-obligatory free modi-
fications occur in valency frames too, since they are ty-
pically (?typ?) related to some verbs (or even to whole
classes of them) and not to others. (The other free modi-
fications can occur with the given verb too, but are not
contained in the valency frame, as it was mentioned above
(Sec. 3.7) )
The attribute ?type? is attached to each frame slot and
can have one of the following values: ?obl? or ?opt? for
inner participants and quasi-valency complementations,
and ?obl? or ?typ? for free modifications.
3.12 Slot Expansion
Some slots tend systematically to occur together. For
instance, verbs of motion can be often modified with
direction-to and/or direction-through and/or direction-
from modifier. We decided to capture this type of regula-
rity by introducing the abbreviation flag for a slot. If this
flag is set (in the VALLEX 1.0 notation it is marked with
an upward arrow), the full valency frame will be obtained
after slot expansion.
If one of the frame slots is marked with the upward
arrow (in the XML data, attribute ?abbrev? is set to 1), then
the full valency frame will be obtained after substituting
this slot with a sequence of slots as follows:
   DIR  DIR1  DIR2  DIR3 
10It should be emphasized that in this context the term obliga-
toriness is related to the presence of the given complementation
in the deep (tectogrammatical) structure, and not to its (surface)
deletability in a sentence (moreover, the relation between deep
obligatoriness and surface deletability is not at all straightfor-
ward in Czech).
   DIR1  

 DIR1  

DIR2   DIR3 
   DIR2  

 DIR1  DIR2  

DIR3 
   DIR3  

 DIR1  DIR2  DIR3  

   TSIN  

 TSIN  

THL  TTIL 
   THL  TSIN  THL  TTIL 
3.13 Frame Attributes
In VALLEX 1.0, frame attributes (more exactly, attribute-
value pairs) are either obligatory or optional. The former
ones have to be filled in every frame. The latter ones
might be empty, either because they are not applicable
(e.g. some verbs have no aspectual counterparts), or be-
cause the annotation was not finished (e.g. attribute class
(Sec. 3.15) is filled only in roughly one third of frames).
Obligatory frame attributes:
  gloss ? verb or paraphrase roughly synonymous with
the given frame/meaning; this attribute is not suppo-
sed to serve as a source of synonyms or even of
genuine lexicographic definition ? it should be used
just as a clue for fast orientation within the word
entry!
  example ? sentence(s) or sentence fragment(s) con-
taining the given verb used with the given valency
frame.
Optional frame attributes:
  control (Sec. 3.14)
  class (Sec. 3.15)
  aspectual counterparts (Sec. 3.16)
  idiom flag (Sec. 3.17)
3.14 Control
The term ?control? relates in this context to a certain
type of predicates (verbs of control)11 and two corre-
ferential expressions, a ?controller? and a ?controllee?. In
VALLEX 1.0, control is captured in the data only in the
situation where a verb has an infinitive modifier (regar-
dless of its functor). Then the controllee is an element that
would be a ?subject? of the infinitive (which is structurally
excluded on the surface), and controller is the co-indexed
expression. In VALLEX 1.0, the type of control is stored
in the frame attribute ?control? as follows:
  if there is a coreferential relation between the (unex-
pressed) subject (?controllee?) of the infinitive verb
and one of the frame slots of the head verb, then the
attribute is filled with the functor of this slot (?cont-
roller?);
11Note on terminology: in English literature the terms ?equi
verbs? and ?raising verbs? are used in a similar context.
  otherwise (i.e., if there is no such co-reference) value
?ex.? is used.
Examples:
  pokusit se (to try) - control: ACT
  slys?et (to hear), e.g. ?slys?et ne?koho pr?icha?zet? (to hear
somebody come) - control: PAT
  j??t, in the sense ?jde to ude?lat? (it is possible to do it)
- control: ex
3.15 Class
Some frames are assigned semantic classes like ?mo-
tion?, ?exchange?, ?communication?, ?perception?, etc.
However, we admit that this classification is tentative and
should be understood merely as an intuitive grouping of
frames, rather than a properly defined ontology.
The motivation for introducing such semantic classi-
fication in VALLEX 1.0 was the fact that it simplifies
systematic checking of consistency and allows for ma-
king more general observations about the data.
3.16 Aspect, Aspectual Counterparts
Perfective verbs (in VALLEX 1.0 marked as ?pf.? for
short) and imperfective verbs (marked as ?impf.?) are dis-
tinguished between in Czech; this characteristic is called
aspect. In VALLEX 1.0, the value of aspect is attached to
each word entry as a whole (i.e., it is the same for all its
frames and it is shared by the lemma variants, if any).
Some verbs (i.e. informovat - to inform, charakterizo-
vat - to characterize) can be used in different contexts
either as perfective or as imperfective (obouvidova? slo-
vesa, ?biasp.? for short).
Within imperfective verbs, there is a subclass of of ite-
rative verbs (iter.). Czech iterative verbs are derived more
or less in a regular way by affixes such as -va- or -iva-, and
express extended and repetitive actions (e.g. c???ta?vat, cho-
d??vat). In VALLEX 1.0, iterative verbs containing double
affix -va- (e.g. chod??va?vat) are completely disregarded,
whereas the remaining iterative verbs occur as aspectual
counterparts in frame entries of the corresponding non-
iterative verbs (but have no own word entries, still).
A verb in its particular meaning can have aspectual
counterpart(s) - a verb the meaning of which is almost the
same except for the difference in aspect (that is why the
counterparts constitute a single lexical unit on the tecto-
grammatical level of FGD; however, each of them has its
own word entry in VALLEX 1.0, because they have di-
fferent morphemic forms). The aspectual counterpart(s)
need not be the same for all the meanings of the given
verb, e.g., odpove?de?t is a counterpart of odpov??dat - to
answer, but not of odpov??dat - to correspond. Therefore
the aspectual counterparts (if any) are listed in frame at-
tribute ?asp. counterparts? in VALLEX 1.0. Moreover, for
perfective or imperfective counterparts, not only the lem-
mas are specified within the list, but (more specifically)
also the frame numbers of the counterpart frames (which
is of course not the case for the iterative counterparts, for
they have no word entries of their own as stated above).
One frame might have more than one counterpart be-
cause of two reasons. Either there are two counterparts
with the same aspect (impf. pu?sobit and impf. zpu?sobo-
vat for pf. zpu?sobit), or there are two counterparts with
different aspects (impf. scha?zet, pf. sej??t, iter. scha?z??vat).
3.17 Idiomatic frames
When building VALLEX 1.0, we focused mainly on pri-
mary or usual meanings of verbs. We also noted many fra-
mes corresponding to peripheral usages of verbs, however
their coverage in VALLEX 1.0 is not exhaustive. We call
such frames idiomatic and mark them with label ?idiom?.
An idiomatic frame is tentatively characterized either by
a substantial shift in meaning (with respect to the primary
sense), or by a small and strictly limited set of possi-
ble lexical values in one of its complementations, or by
occurence of another types of irregularity or anomaly.
4 Future Work
We plan to extend VALLEX in both quantitative and qua-
litative aspects. At this moment, word entries for 500
new verbs are being created, and further batches of verbs
will follow in near future (selected with respect to their
frequency, again). As for the theoretical issues, we in-
tend to focus on capturing the structure on the set of
frames/senses (e.g. the relations between primary and me-
taphorical usages of a verb), on improving the semantic
classification of frames, and on exploring the influence of
word-formative process on valency frames (for example,
regularities in the relations between valency frames of a
basic verb and of a verb derived from it by prefixing, are
expected).
Acknowledgements
VALLEX 1.0 has been created under the financial sup-
port of the projects MSMT LN00A063 and GACR
405/04/0243.
We would like to thank for an extensive linguistic and
also technical advice to our colleagues from CKL and
UFAL, especially to professor Jarmila Panevova?.
References
Charles Fillmore. 2002. Framenet and the linking be-
tween semantic and syntactic relations. In Proceedings
of COLING 2002, pages xxviii?xxxvi.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?, and Petr Pajas. 2003.
PDT-VALLEX: Creating a Large-coverage Valency
Lexicon for Treebank Annotation. In Proceedings of
The Second Workshop on Treebanks and Linguistic
Theories, volume 9 of Mathematical Modeling in Phys-
ics, Engineering and Cognitive Sciences, pages 57?68.
Vaxjo University Press, November 14?15, 2003.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/Valency Structure in PropBank, LCS Database
and Prague Dependency Treebank: A Comparative Pi-
lot Study. In Proceedings of the Third International
Conference on Language Resources and Evaluation
(LREC 2002), pages 846?851. ELRA.
Eva Hajic?ova?, Jarmila Panevova?, and Petr Sgall, 2000. A
Manual for Tectogrammatical Tagging of the Prague
Dependency Treebank.
Paul Kingsbury and Martha Palmer. 2002. From Tre-
ebank to PropBank. In Proceedings of the 3rd Inter-
national Conference on Language Resources and Eva-
luation, Las Palmas, Spain.
Beth C. Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Marke?ta Lopatkova?, Zdene?k Z? abokrtsky?, Karolina Skwar-
ska, and Va?clava Benes?ova?. 2002. Tektogramaticky
anotovany? valenc?n?? slovn??k c?esky?ch sloves. Technical
Report TR-2002-15.
Marke?ta Lopatkova?. 2003. Valency in the Prague Depen-
dency Treebank: Building the Valency Lexicon. Pra-
gue Bulletin of Mathematical Linguistics, (79?80).
Jarmila Panevova?. 1992. Valency frames and the me-
aning of the sentence. In Ph. L. Luelsdorff, editor,
The Prague School of Structural and Functional Lingu-
istics, pages 223?243, Amsterdam-Philadelphia. John
Benjamins.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel Publishing Company, Dord-
recht.
Hana Skoumalova?. 2002. Verb frames extracted from
dictionaries. The Prague Bulletin of Mathematical Lin-
guistics 77.
Marke?ta Stran?a?kova?-Lopatkova? and Zdene?k Z? abokrtsky?.
2002. Valency Dictionary of Czech Verbs: Complex
Tectogrammatical Annotation. In Proceedings of the
Third International Conference on Language Resour-
ces and Evaluation (LREC 2002), volume 3, pages 949?
956. ELRA.
Nad?a Svozilova?, Hana Prouzova?, and Anna Jirsova?. 1997.
Slovesa pro praxi. Academia, Praha.
APPENDIX ? Sample from the printed version of VALLEX 1.0
  	
 




Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 171?178,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving Parsing Accuracy by Combining Diverse 
Dependency Parsers 
 
 
Daniel Zeman and Zden k ?abokrtsk? 
?stav form?ln? a aplikovan? lingvistiky, Univerzita Karlova 
Malostransk? n?m st? 25, CZ-11800  Praha 
{zeman|zabokrtsky}@ufal.mff.cuni.cz 
 
 
 
 
 
Abstract 
This paper explores the possibilities of 
improving parsing results by combining 
outputs of several parsers. To some ex-
tent, we are porting the ideas of Hender-
son and Brill (1999) to the world of 
dependency structures. We differ from 
them in exploring context features more 
deeply. All our experiments were con-
ducted on Czech but the method is lan-
guage-independent. We were able to 
significantly improve over the best pars-
ing result for the given setting, known so 
far. Moreover, our experiments show that 
even parsers far below the state of the art 
can contribute to the total improvement. 
1 Introduction 
Difficult and important NLP problems have the 
property of attracting whole range of researchers, 
which often leads to the development of several 
different approaches to the same problem. If these 
approaches are independent enough in terms of not 
producing the same kinds of errors, there is a hope 
that their combination can bring further improve-
ment to the field. While improving any single ap-
proach gets more and more difficult once some 
threshold has been touched, exploring the potential 
of approach combination should never be omitted, 
provided three or more approaches are available. 
Combination techniques have been successfully 
applied to part of speech tagging (van Halteren et 
al., 1998; Brill and Wu, 1998; van Halteren et al, 
2001). In both cases the investigators were able to 
achieve significant improvements over the previ-
ous best tagging results. Similar advances have 
been made in machine translation (Frederking and 
Nirenburg, 1994), speech recognition (Fiscus, 
1997), named entity recognition (Borthwick et al, 
1998), partial parsing (Inui and Inui, 2000), word 
sense disambiguation (Florian and Yarowsky, 
2002) and question answering (Chu-Carroll et al, 
2003). 
Brill and Hladk? (Haji  et al, 1998) have first 
explored committee-based dependency parsing. 
However, they generated multiple parsers from a 
single one using bagging (Breiman, 1994). There 
have not been more sufficiently good parsers 
available. A successful application of voting and of 
a stacked classifier to constituent parsing followed 
in (Henderson and Brill, 1999). The authors have 
investigated two combination techniques (constitu-
ent voting and na?ve Bayes), and two ways of their 
application to the (full) parsing: parser switching, 
and similarity switching. They were able to gain 
1.6 constituent F-score, using their most successful 
technique. 
In our research, we focused on dependency pars-
ing. One of the differences against Henderson and 
Brill?s situation is that a dependency parser has to 
assign exactly one governing node (parent word) to 
each word. Unlike the number of constituents in 
constituency-based frameworks, the number of 
dependencies is known in advance, the parser only 
has to assign a link (number 0 through N) to each 
word. In that sense, a dependency parser is similar 
to classifiers like POS taggers. Unless it deliber-
ately fails to assign a parent to a word (or assigns 
171
several alternate parents to a word), there is no 
need for precision & recall. Instead, a single metric 
called accuracy is used. 
On the other hand, a dependency parser is not a 
real classifier: the number of its ?classes? is theo-
retically unlimited (natural numbers), and no gen-
eralization can be drawn about objects belonging 
to the same ?class? (words that ? sometimes ? ap-
peared to find their parent at the position i). 
A combination of dependency parsers does not 
necessarily grant the resulting dependency struc-
ture being cycle-free. (This contrasts to not intro-
ducing crossing brackets in constituent parsing, 
which is granted according to Henderson and 
Brill.) We address the issue in 4.4. 
The rest of this paper is organized as follows: in 
Sections 2 and 3 we introduce the data and the 
component parsers, respectively. In Section 4 we 
discuss several combining techniques, and in Sec-
tion 5 we describe the results of the corresponding 
experiments. We finally compare our results to the 
previous work and conclude. 
2 The data 
To test our parser combination techniques, we use 
the Prague Dependency Treebank 1.0 (PDT; Haji  
et al 2001). All the individual parsers have been 
trained on its analytical-level training section 
(73,088 sentences; 1,255,590 tokens). 
The PDT analytical d-test section has been parti-
tioned into two data sets, Tune (last 77 files; 3646 
sentences; 63,353 tokens) and Test (first 76 files; 
3673 sentences; 62,677 tokens). We used the Tune 
set to train the combining classifiers if needed. The 
Test data were used to evaluate the approach. Nei-
ther the member parsers, nor the combining classi-
fier have seen this data set during their respective 
learning runs. 
3 Component parsers 
The parsers involved in our experiments are sum-
marized in Table 1. Most of them use unique 
strategies, the exception being thl and thr, which 
differ only in the direction in which they process 
the sentence. 
The table also shows individual parser accura-
cies on our Test data. There are two state-of-the art 
parsers, four not-so-good parsers, and one quite 
poor parser. We included the two best parsers 
(ec+mc) in all our experiments, and tested the con-
tributions of various selections from the rest. 
The necessary assumption for a meaningful 
combination is that the outputs of the individual 
parsers are sufficiently uncorrelated, i.e. that the 
parsers do not produce the same errors. If some 
Accuracy Par-
ser 
Author Brief description 
Tune Test 
ec 
Eugene 
Charniak 
A maximum-entropy inspired parser, home in constituency-based 
structures. English version described in Charniak (2000), Czech ad-
aptation 2002 ? 2003, unpublished. 
83.6 85.0 
mc 
Michael 
Collins 
Uses a probabilistic context-free grammar, home in constituency-
based structures. Described in (Haji  et al, 1998; Collins et al, 
1999). 
81.7 83.3 
z? Zden k ?abokrtsk? 
Purely rule-based parser, rules are designed manually, just a few lexi-
cal lists are collected from the training data. 2002, unpublished. 74.3 76.2 
dz Daniel Zeman 
A statistical parser directly modeling syntactic dependencies as word 
bigrams. Described in (Zeman, 2004). 73.8 75.5 
thr 71.0 72.3 
thl 69.5 70.3 
thp 
Tom?? 
Holan 
Three parsers. Two of them use a sort of push-down automata and 
differ from each other only in the way they process the sentence (left-
to-right or right-to-left). Described in (Holan, 2004). 62.0 63.5 
 
Table 1. A brief description of the tested parsers. Note that the Tune data is not the data used to train the 
individual parsers. Higher numbers in the right column reflect just the fact that the Test part is slightly 
easier to parse. 
172
parsers produced too similar results, there would 
be the danger that they push all their errors 
through, blocking any meaningful opinion of the 
other parsers. 
To check the assumption, we counted (on the 
Tune data set) for each parser in a given parser se-
lection the number of dependencies that only this 
parser finds correctly. We show the results in Ta-
ble 2. They demonstrate that all parsers are inde-
pendent on the others at least to some extent. 
4 Combining techniques 
Each dependency structure consists of a number of 
dependencies, one for each word in the sentence. 
Our goal is to tell for each word, which parser is 
the most likely to pick its dependency correctly. 
By combining the selected dependencies we aim at 
producing a better structure. We call the complex 
system (of component parsers plus the selector) the 
superparser. 
Although we have shown how different strate-
gies lead to diversity in the output of the parsers, 
there is little chance that any parser will be able to 
push through the things it specializes in. It is very 
difficult to realize that a parser is right if most of 
the others reject its proposal. Later in this section 
we assess this issue; however, the real power is in 
majority of votes. 
4.1 Voting 
The simplest approach is to let the member parsers 
vote. At least three parsers are needed. If there are 
exactly three, only the following situations really 
matter: 1) two parsers outvote the third one; 2) a 
tie: each parser has got a unique opinion. It would 
be democratic in the case of a tie to select ran-
domly. However, that hardly makes sense once we 
know the accuracy of the involved parsers on the 
Tune set. Especially if there is such a large gap 
between the parsers? performance, the best parser 
(here ec) should get higher priority whenever there 
Parsers compared All 7 4 best 3 best ec+mc+dz 2 best 3 worst 
Who is correct How many times correct 
ec 1.7 % 3.0 % 4.1 % 4.5 % 8.1 %  
z? 1.2 % 2.0 % 3.3 %    
mc 0.9 % 1.7 % 2.7 % 2.9 % 6.2 %  
thr 0.4 %     4.9 % 
thp 0.4 %     4.4 % 
dz 0.3 % 1.0 %  2.2 %   
a single parser 
(all other wrong) 
thl 0.3 %     4.3 % 
all seven parsers 42.5 %      
at least six 58.1 %      
at least five 68.4 %      
at least four 76.8 % 58.0 %     
at least three 84.0 % 75.1 % 63.6 % 64.7 %  50.6 % 
at least two 90.4 %  82.9 % 82.4 % 75.5 % 69.2 % 
at least one 95.8 % 94.0 % 93.0 % 92.0 % 89.8 % 82.7 % 
 
Table 2: Comparison of various groups of parsers. All percentages refer to the share of the total words in 
test data, attached correctly. The ?single parser? part shows shares of the data where a single parser is the 
only one to know how to parse them. The sizes of the shares should correlate with the uniqueness of the 
individual parsers? strategies and with their contributions to the overall success. The ?at least? rows give 
clues about what can be got by majority voting (if the number represents over 50 % of parsers compared) 
or by hypothetical oracle selection (if the number represents 50 % of the parsers or less, an oracle would 
generally be needed to point to the parsers that know the correct attachment). 
 
173
is no clear majority of votes. Van Halteren et al 
(1998) have generalized this approach for higher 
number of classifiers in their TotPrecision voting 
method. The vote of each classifier (parser) is 
weighted by their respective accuracy. For in-
stance, mc + z? would outvote ec + thr, as 81.7 + 
74.3 = 156 > 154.6 = 83.6 + 71.0. 
4.2 Stacking 
If the world were ideal, we would have an oracle, 
able to always select the right parser. In such situa-
tion our selection of parsers would grant the accu-
racy as high as 95.8 %. We attempt to imitate the 
oracle by a second-level classifier that learns from 
the Tune set, which parser is right in which situa-
tions. Such technique is usually called classifier 
stacking. Parallel to (van Halteren et al, 1998), we 
ran experiments with two stacked classifiers, 
Memory-Based, and Decision-Tree-Based. This 
approach roughly corresponds to (Henderson and 
Brill, 1999)?s Na?ve Bayes parse hybridization. 
4.3 Unbalanced combining 
For applications preferring precision to recall, un-
balanced combination ? introduced by Brill and 
Hladk? in (Haji  et al, 1998) ? may be of inter-
est. In this method, all dependencies proposed by 
at least half of the parsers are included. The term 
unbalanced reflects the fact that now precision is 
not equal to recall: some nodes lack the link to 
their parents. Moreover, if the number of member 
parsers is even, a node may get two parents. 
4.4 Switching 
Finally, we develop a technique that considers the 
whole dependency structure rather than each de-
pendency alone. The aim is to check that the result-
ing structure is a tree, i.e. that the dependency-
selecting procedure does not introduce cycles.1 
Henderson and Brill prove that under certain con-
ditions, their parse hybridization approach cannot 
                                                   
1
 One may argue that ?treeness? is not a necessary condition 
for the resulting structure, as the standard accuracy measure 
does not penalize non-trees in any way (other than that there is 
at least one bad dependency). Interestingly enough, even some 
of the component parsers do not produce correct trees at all 
times. However, non-trees are both linguistically and techni-
cally problematic, and it is good to know how far we can get 
with the condition in force. 
introduce crossing brackets. This might seem an 
analogy to our problem of introducing cycles ? 
but unfortunately, no analogical lemma holds. As a 
workaround, we have investigated a crossbreed 
approach between Henderson and Brill?s Parser 
Switching, and the voting methods described 
above. After each step, all dependencies that would 
introduce a cycle are banned. The algorithm is 
greedy ? we do not try to search the space of de-
pendency combinations for other paths. If there are 
no allowed dependencies for a word, the whole 
structure built so far is abandoned, and the struc-
ture suggested by the best component parser is 
used instead.2 
5 Experiments and results 
5.1 Voting 
We have run several experiments where various 
selections of parsers were granted the voting right. 
In all experiments, the TotPrecision voting scheme 
of (van Halteren et al, 1998) has been used. The 
voting procedure is only very moderately affected 
by the Tune set (just the accuracy figures on that 
set are used), so we present results on both the Test 
and the Tune sets. 
 
Accuracy Voters Tune Test 
ec (baseline) 83.6 85.0 
all seven 84.0 85.4 
ec+mc+dz 84.9 86.2 
all but thp 84.9 86.3 
ec+mc+z?+dz+thr 85.1 86.5 
ec+mc+z? 85.2 86.7 
ec+mc+z?+dz 85.6 87.0 
Table 3: Results of voting experiments. 
 
According to the results, the best voters pool 
consists of the two best parsers, accompanied by 
                                                   
2
 We have not encountered such situation in our test data. 
However, it indeed is possible, even if all the component pars-
ers deliver correct trees, as can be seen from the following 
example. Assume we have a sentence #ABCD and parsers P1 
(85 votes), P2 (83 votes), P3 (76 votes). P1 suggests the tree 
A?D?B?C?#, P2 suggests B?D?A?C?#, P3 suggests 
B?D?A?#, C?#. Then the superparser P gradually intro-
duces the following dependencies: 1. A?D; 2. B?D; 
3. C?#; 4. D?A or D?B possible but both lead to a cycle. 
174
the two average parsers. The table also suggests 
that number of diverse strategies is more important 
than keeping high quality standard with all the 
parsers. Apart from the worst parser, all the other 
together do better than just the first two and the 
fourth. (On the other hand, the first three parsers 
are much harder to beat, apparently due to the ex-
treme distance of the strategy of z? parser from all 
the others.) 
Even the worst performing parser combination 
(all seven parsers) is significantly3 better than the 
best component parser alone. 
We also investigated some hand-invented voting 
schemes but no one we found performed better 
than the ec+mc+z?+dz combination above. 
Some illustrative results are given in the Ta-
ble 4. Votes were not weighted by accuracy in 
these experiments, but accuracy is reflected in the 
priority given to ec and mc by the human scheme 
inventor. 
 
Accuracy Voters Selection 
scheme Tune Test 
all seven most votes 
or ec 
82.8 84.3 
all seven 
at least 
half, or ec 
if there is 
no absolute 
majority 
84.4 85.8 
all seven 
absolute 
majority, 
or ec+2, or 
mc+2, or 
ec 
84.6 85.9 
Table 4: Voting under hand-invented schemes. 
 
5.2 Stacking ? using context 
We explored several ways of using context in 
pools of three parsers.4 If we had only three parsers 
we could use context to detect two kinds of situa-
tions: 
                                                   
3
 All significance claims refer to the Wilcoxon Signed Ranks 
Test at the level of p = 0.001. 
4
 Similar experiments could be (and have been) run for sets of 
more parsers as well. However, the number of possible fea-
tures is much higher and the data sparser. We were not able to 
gain more accuracy on context-sensitive combination of more 
parsers. 
1. Each parser has its own proposal and a 
parser other than ec shall win. 
2. Two parsers agree on a common pro-
posal but even so the third one should 
win. Most likely the only reasonable in-
stance is that ec wins over mc + the 
third one. 
?Context? can be represented by a number of 
features, starting at morphological tags and ending 
up at complex queries on structural descriptions. 
We tried a simple memory-based approach, and a 
more complex approach based on decision trees. 
Within the memory-based approach, we use just 
the core features the individual parsers themselves 
train on: the POS tags (morphological tags or m-
tags in PDT terminology). We consider the m-tag 
of the dependent node, and the m-tags of the gov-
ernors proposed by the individual parsers. 
We learn the context-based strengths and weak-
nesses of the individual parsers on their perform-
ance on the Tune data set. In the following table, 
there are some examples of contexts in which ec is 
better than the common opinion of mc + dz. 
 
Dep. 
tag 
Gov. 
tag 
(ec) 
Context 
occurrences 
No. of 
times 
ec was 
right 
Percent 
cases ec 
was 
right 
J^ # 67 44 65.7 
Vp J^ 53 28 52.8 
VB J^ 46 26 56.5 
N1 Z, 38 21 55.3 
Rv Vp 25 13 52.0 
Z, Z, 15 8 53.3 
A1 N1 15 8 53.3 
Vje J^ 14 9 64.3 
N4 Vf 12 9 75.0 
Table 5: Contexts where ec is better than mc+dz. 
J^ are coordination conjunctions, # is the root, V* 
are verbs, Nn are nouns in case n, R* are preposi-
tions, Z* are punctuation marks, An are adjectives. 
 
For the experiment with decision trees, we used 
the C5 software package, a commercial version of 
the well-known C4.5 tool (Quinlan, 1993). We 
considered the following features: 
For each of the four nodes involved (the de-
pendent and the three governors suggested by the 
three component parsers): 
175
? 12 attributes derived from the morpho-
logical tag (part of speech, subcategory, 
gender, number, case, inner gender, in-
ner number, person, degree of compari-
son, negativeness, tense and voice) 
? 4 semantic attributes (such as Proper-
Name, Geography etc.) 
For each of the three governor-dependent pairs 
involved: 
? mutual position of the two nodes (Left-
Neighbor, RightNeighbor, LeftFar, 
RightFar) 
? mutual position expressed numerically 
? for each parser pair a binary flag 
whether they do or do not share opin-
ions 
The decision tree was trained only on situations 
where at least one of the three parsers was right 
and at least one was wrong. 
 
Voters Scheme Accuracy 
ec+mc+dz context free 86.2 
ec+mc+dz memory-based 86.3 
ec+mc+z? context free 86.7 
ec+mc+z? decision tree 86.9 
Table 6: Context-sensitive voting. Contexts trained 
on the Tune data set, accuracy figures apply to the 
Test data set. Context-free results are given for the 
sake of comparison. 
 
It turns out that there is very low potential in the 
context to improve the accuracy (the improvement 
is significant, though). The behavior of the parsers 
is too noisy as to the possibility of formulating 
some rules for prediction, when a particular parser 
is right. C5 alone provided a supporting evidence 
for that hypothesis, as it selected a very simple tree 
from all the features, just 5 levels deep (see Fig-
ure 1). 
Henderson and Brill (1999) also reported that 
context did not help them to outperform simple 
voting. Although it is risky to generalize these ob-
servations for other treebanks and parsers, our en-
vironment is quite different from that of Henderson 
and Brill, so the similarity of the two observations 
is at least suspicious. 
5.3 Unbalanced combining 
Finally we compare the balanced and unbalanced 
methods. Expectedly, precision of the unbalanced 
combination of odd number of parsers rose while 
recall dropped slightly. A different situation is ob-
served if even number of parsers vote and more 
than one parent can be selected for a node. In such 
case, precision drops in favor of recall. 
 
Method Precision Recall F-measure 
ec only 
(baseline) 85.0 
balanced 
(all seven) 85.4 
unbalanced 
(all seven) 90.7 78.6 84.2 
balanced 
(best four) 87.0 
unbalanced 
(best four) 85.4 87.7 86.5 
balanced 
(ec+mc+dz) 86.2 
unbalanced 89.5 84.0 86.7 
 agreezzmc = yes: zz (3041/1058) 
 agreezzmc = no: 
 :...agreemcec = yes: ec (7785/1026) 
     agreemcec = no: 
     :...agreezzec = yes: ec (2840/601) 
         agreezzec = no: 
         :...zz_case = 6: zz (150/54) 
             zz_case = 3: zz (34/10) 
             zz_case = X: zz (37/20) 
             zz_case = undef: ec (2006/1102) 
             zz_case = 7: zz (83/48) 
             zz_case = 2: zz (182/110) 
             zz_case = 4: zz (108/57) 
             zz_case = 1: ec (234/109) 
             zz_case = 5: mc (1) 
             zz_case = root: 
             :...ec_negat = A: mc (117/65) 
                 ec_negat = undef: ec (139/65) 
                 ec_negat = N: ec (1) 
                 ec_negat = root: ec (2) 
 
Figure 1. The decision tree for ec+mc+z?, 
learned by C5. Besides pairwise agreement be-
tween the parsers, only morphological case and 
negativeness matter. 
176
Method Precision Recall F-measure 
(ec+mc+dz) 
balanced 
(ec+mc+z?) 86.7 
unbalanced 
(ec+mc+z?) 90.2 84.7 87.3 
Table 7: Unbalanced vs. balanced combining. All 
runs ignored the context. Evaluated on the Test 
data set. 
 
5.4 Switching 
Out of the 3,673 sentences in our Test set, 91.6 % 
have been rendered as correct trees in the balanced 
decision-tree based stacking of ec+mc+z?+dz (our 
best method). 
After we banned cycles, the accuracy dropped 
from 97.0 to 96.9 %. 
6 Comparison to related work 
Brill and Hladk? in (Haji  et al, 1998) were able to 
improve the original accuracy of the mc parser on 
PDT 0.5 e-test data from 79.1 to 79.9 (a nearly 4% 
reduction of the error rate). Their unbalanced5 vot-
ing pushed the F-measure from 79.1 to 80.4 (6% 
error reduction). We pushed the balanced accuracy 
of the ec parser from 85.0 to 87.0 (13% error re-
duction), and the unbalanced F-measure from 85.0 
to 87.7 (18% reduction). Note however that there 
were different data and component parsers (Haji  
et al found bagging the best parser better than 
combining it with other that-time-available pars-
ers). This is the first time that several strategically 
different dependency parsers have been combined. 
(Henderson and Brill, 1999) improved their best 
parser?s F-measure of 89.7 to 91.3, using their na-
?ve Bayes voting on the Penn TreeBank constituent 
structures (16% error reduction). Here, even the 
framework is different, as has been explained 
above. 
7 Conclusion 
We have tested several approaches to combining of 
dependency parsers. Accuracy-aware voting of the 
four best parsers turned out to be the best method, 
as it significantly improved the accuracy of the 
best component from 85.0 to 87.0 % (13 % error 
                                                   
5
 Also alternatively called unrestricted. 
rate reduction). The unbalanced voting lead to the 
precision as high as 90.2 %, while the F-measure 
of 87.3 % outperforms the best result of balanced 
voting (87.0). 
At the same time, we found that employing con-
text to this task is very difficult even with a well-
known and widely used machine-learning ap-
proach. 
The methods are language independent, though 
the amount of accuracy improvement may vary 
according to the performance of the available pars-
ers. 
Although voting methods are themselves not 
new, as far as we know we are the first to propose 
and evaluate their usage in full dependency pars-
ing. 
8 Acknowledgements 
Our thanks go to the creators of the parsers used 
here for making their systems available. 
The research has been supported by the Czech 
Academy of Sciences, the ?Information Society? 
program, project No. 1ET101470416. 
References  
Andrew Borthwick, John Sterling, Eugene Agichtein, 
Ralph Grishman. 1998. Exploiting Diverse Knowl-
edge Sources via Maximum Entropy in Named Entity 
Recognition. In: Eugene Charniak (ed.): Proceedings 
of the 6th Workshop on Very Large Corpora, pp. 
152?160. Universit? de Montr?al, Montr?al, Qu?bec. 
Leo Breiman. 1994. Bagging Predictors. Technical Re-
port 421, Department of Statistics, University of 
California at Berkeley, Berkeley, California. 
Eric Brill, Jun Wu. 1998. Classifier Combination for 
Improved Lexical Combination. In: Proceedings of 
the 17th International Conference on Computational 
Linguistics (COLING-98), pp. 191?195. Universit? 
de Montr?al, Montr?al, Qu?bec. 
Eugene Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. In: Proceedings of NAACL. Seattle, Wash-
ington. 
Jennifer Chu-Carroll, Krzysztof Czuba, John Prager, 
Abraham Ittycheriah. 2003. In Question Answering, 
Two Heads Are Better Than One. In: Proceedings of 
the HLT-NAACL. Edmonton, Alberta. 
Michael Collins, Jan Haji   , Eric Brill, Lance Ramshaw, 
Christoph Tillmann. 1999. A Statistical Parser of 
Czech. In: Proceedings of the 37th Meeting of the 
177
ACL, pp. 505?512. University of Maryland, College 
Park, Maryland. 
Jonathan G. Fiscus. 1997. A Post-Processing System to 
Yield Reduced Word Error Rates: Recognizer Output 
Voting Error Reduction (ROVER). In: EuroSpeech 
1997 Proceedings, vol. 4, pp. 1895?1898. Rodos, 
Greece. 
Radu Florian, David Yarowsky. 2002. Modeling Con-
sensus: Classifier Combination for Word Sense Dis-
ambiguation. In: Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP), pp. 25?32. Philadelphia, Pennsylvania. 
Robert Frederking, Sergei Nirenburg. 1994. Three 
Heads Are Better Than One. In: Proceedings of the 
4th Conference on Applied Natural Language Proc-
essing, pp. 95?100. Stuttgart, Germany. 
Jan Haji   , Eric Brill, Michael Collins, Barbora Hladk?, 
Douglas Jones, Cynthia Kuo, Lance Ramshaw, Oren 
Schwartz, Christoph Tillmann, Daniel Zeman. 1998. 
Core Natural Language Processing Technology Ap-
plicable to Multiple Languages. The Workshop 98 
Final Report. http://www.clsp.jhu.edu/ws98/projects/ 
nlp/report/. Johns Hopkins University, Baltimore, 
Maryland. 
Jan Haji   , Barbora Vidov? Hladk?, Jarmila Panevov?, 
Eva Haji   ov?, Petr Sgall, Petr Pajas. 2001. Prague 
Dependency Treebank 1.0 CD-ROM. Catalog # 
LDC2001T10, ISBN 1-58563-212-0. Linguistic Data 
Consortium, Philadelphia, Pennsylvania. 
Hans van Halteren, Jakub Zav  el, Walter Daelemans. 
1998. Improving Data-Driven Wordclass Tagging by 
System Combination. In: Proceedings of the 17th In-
ternational Conference on Computational Linguistics 
(COLING-98), pp. 491?497. Universit? de Montr?al, 
Montr?al, Qu?bec. 
Hans van Halteren, Jakub Zav  el, Walter Daelemans. 
2001. Improving Accuracy in Word Class Tagging 
through the Combination of Machine Learning Sys-
tems. In: Computational Linguistics, vol. 27, no. 2, 
pp. 199?229. MIT Press, Cambridge, Massachusetts. 
John C. Henderson, Eric Brill. 1999. Exploiting Diver-
sity in Natural Language Processing: Combining 
Parsers. In: Proceedings of the Fourth Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP-99), pp. 187?194. College Park, Maryland. 
Tom?? Holan. 2004. Tvorba z?vislostn?ho syntaktick?ho 
analyz?toru. In: David Obdr??lek, Jana Teskov? 
(eds.): MIS 2004 Josef v D  l, Sborn?k semin?  e. 
Matfyzpress, Praha, Czechia. 
Inui Takashi, Inui Kentaro. 2000. Committee-Based 
Decision Making in Probabilistic Partial Parsing. In: 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING 2000), pp. 
348?354. Universit?t des Saarlandes, Saarbr?cken, 
Germany. 
J. Ross Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, California. 
Daniel Zeman. 2004. Parsing with a Statistical Depend-
ency Model (PhD thesis). Univerzita Karlova, Praha, 
Czechia. 
178
Proceedings of the Third Workshop on Statistical Machine Translation, pages 167?170,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
TectoMT: Highly Modular MT System
with Tectogrammatics Used as Transfer Layer?
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, Petr Pajas
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
{zabokrtsky,ptacek,pajas}@ufal.mff.cuni.cz
Abstract
We present a new English?Czech machine
translation system combining linguistically
motivated layers of language description (as
defined in the Prague Dependency Treebank
annotation scenario) with statistical NLP ap-
proaches.
1 Introduction
We describe a new MT system (called Tec-
toMT) based on the conventional analysis-transfer-
synthesis architecture. We use the layers of language
description defined in the Prague Dependency Tree-
bank 2.0 (PDT for short, (Hajic? and others, 2006)),
namely (1) word layer ? raw text, no linguistic
annotation, (2) morphological layer ? sequence of
tagged and lemmatized tokens, (3) analytical layer
? each sentence represented as a surface-syntactic
dependency tree, and (4) tectogrammatical layer ?
each sentence represented as a deep-syntactic de-
pendency tree in which only autosemantic words do
have nodes of their own; prefixes w-, m-, a-, or t-
will be used for denoting these layers.1
We use ?Praguian? tectogrammatics (introduced
in (Sgall, 1967)) as the transfer layer because
we believe that, first, it largely abstracts from
language-specific (inflection, agglutination, func-
tional words. . . ) means of expressing non-lexical
?The research reported in this paper is financially supported
by grants GAAV C?R 1ET101120503 and MSM0021620838.
1In addition, we use also p-layer (phrase structures) as an
a-layer alternative, the only reason for which is that we do not
have a working a-layer parser for English at this moment.
meanings, second, it allows for a natural transfer
factorization, and third, local tree contexts in t-trees
carry more information (esp. for lexical choice) than
local linear contexts in the original sentences.
In order to facilitate separating the transfer of lex-
icalization from the transfer of syntactization, we in-
troduce the concept of formeme. Each t-node?s has
a formeme attribute capturing which morphosyntac-
tic form has been (in the case of analysis) or will
be (synthesis) used for the t-node in the surface sen-
tence shape. Here are some examples of formemes
we use for English: n:subj (semantic noun (sn) in
subject position), n:for+X (sn with preposition for),
n:X+ago (sn with postposition ago), n:poss (posses-
sive form of sn), v:because+fin (semantic verb (sv)
as a subordinating finite clause introduced by be-
cause), v:without+ger (sv as a gerund after without),
adj:attr (semantic adjective (sa) in attributive posi-
tion), adj:compl (sa in complement position).
The presented system intensively uses the PDT
technology (data formats, software tools). Special
attention is paid to modularity: the translation is im-
plemented (in Perl) as a long sequence of processing
modules (called blocks) with relatively tiny, well-
defined tasks, so that each module is independently
testable, improvable, or substitutable. TectoMT al-
lows to easily combine blocks based on different
approaches, from blocks using complex probabilis-
tic solutions (e.g., B2, B6, B35, see the next section),
through blocks applying simpler Machine Learning
techniques (e.g., B69) or empirically based heuris-
tics (e.g., B7, B25, B36, B71), to blocks implementing
?crisp? linguistic rules (e.g., B48-B51, B59). There are
also blocks for trivial technical tasks (e.g., B33, B72).
167
English m-layerSheshe PRP hashave VBZ nevernever RB laughedlaugh VBN inin IN herher PRP$ newnew JJ bossboss NN 's's POS officeoffice NN .. .
NPBShe has
English p-layerS
ADVPnever
VP
laughedin
VP
her new
PP
NPBboss 's
NPB
office .
English a-layer
She has neverlaughedin
her newboss's
office.
English t-layer
#PersPronn:subj neveradv:
laughv:fin
#PersPronn:poss newadj:attr
bossn:poss
officen:in+X
Czech t-layer
#PersPronn:1 nikdyadv:
sm?t_sev:fin ??adn:v+6
#PersPronadj:attr nov?adj:attr
??fn:2
Czech a-layer
NikdyD........1A... seP7
nesm?laVpFS...3..NA..vR??aduN.IS6.....A...
sv?hoP8MS2......... nov?hoAAMS2....1A...
??faN.MS2.....A...
.Z
Figure 1: MT ?pyramid? as implemented in TectoMT. All the representations are rooted with artificial nodes, serving
only as labels. Virtually, the pyramid is bottomed with the input sentence on the source side (She has never laughed in
her new boss?s office.) and its automatic translation on the target side (Nikdy se nesma?la v u?r?adu sve?ho nove?ho s?e?fa.).
2 Translation Procedure
The structure of this section directly renders the se-
quence of blocks currently used for English-Czech
translation in TectoMT. The intermediate stages of
the translation process are illustrated in Figure 1;
identifiers of the blocks affecting on the translation
of the sample sentence are typeset in bold.
2.1 From English w-layer to English m-layer
B1: Segment the source English text into sentences.
B2: Split the sentences into sequences of tokens,
roughly according to Penn Treebank (PTB for short;
(Marcus et al, 1994)) conventions. B3: Tag the
tokens with PTB-style POS tags using a tagger
(Brants, 2000). B4: Fix some tagging errors sys-
tematically made by the tagger using a rule-based
corrector. B5: Lemmatize the tokens using morpha,
(Minnen et al, 2000).
2.2 From English m-layer to English p-layer
B6: Build PTB-style phrase-structure tree for each
sentence using a parser (Collins, 1999).
2.3 From English p-layer to English a-layer
B7: In each phrase, mark the head node (using a set
of heuristic rules). B8: Convert phrase-structure trees
to a-trees. B9: Apply some heuristic rules to fix ap-
position constructions. B10: Apply another heuris-
tic rules for reattaching incorrectly positioned nodes.
B11: Unify the way in which multiword prepositions
(such as because of ) and subordinating conjunctions
(such as provided that) are treated. B12: Assign an-
alytical functions (only if necessary for a correct
treatment of coordination/apposition constructions).
2.4 From English a-layer to English t-layer
B13: Mark a-nodes which are auxiliary (such as
prepositions, subordinating conjunctions, auxiliary
verbs, selected types of particles, etc.) B14: Mark not
as an auxiliary node too (but only if it is connected to
a verb form). B15: Build t-trees. Each a-node cluster
formed by an autosemantic node and possibly sev-
eral associated auxiliary nodes is ?collapsed? into a
single t-node. T-tree dependency edges are derived
from a-tree edges connecting the a-node clusters.
B16: Explicitely distinguish t-nodes that are mem-
bers of coordination (conjuncts) from shared modi-
fiers. It is necessary as they all are attached below
the coordination conjunction t-node. B17: Modify
t-lemmas in specific cases. E.g., all kinds of per-
sonal pronouns are represented by the ?artificial? t-
lemma #PersPron. B18: Assign functors that are nec-
essary for proper treatment of coordination and ap-
position constructions. B19: Distribute shared auxil-
iary words in coordination constructions. B20: Mark
t-nodes that are roots of t-subtrees corresponding to
finite verb clauses. B21: Mark passive verb forms.
B22: Assign (a subset of) functors. B23: Mark t-nodes
corresponding to infinitive verbs. B24: Mark t-nodes
which are roots of t-subtrees corresponding to rel-
ative clauses. B25: Identify coreference links be-
tween relative pronouns (or other relative pronom-
inal word) and their nominal antecedents. B26: Mark
168
t-nodes that are the roots of t-subtrees correspond-
ing to direct speeches. B27: Mark t-nodes that are
the roots of t-subtrees corresponding to parenthe-
sized expressions. B28: Fill the nodetype attribute
(rough classification of t-nodes). B29: Fill the sem-
pos attribute (fine-grained classification of t-nodes).
B30: Fill the grammateme attributes (semantically in-
dispensable morphological categories, such as num-
ber for nouns, tense for verbs). B31: Determine the
formeme of each t-node. B32: Mark personal names,
distinguish male and female first names if possible.
2.5 From English t-layer to Czech t-layer
B33: Initiate the target-side t-trees, simply by cloning
the source-side t-trees. B34: In each t-node, trans-
late its formeme.2 B35: Translate t-lemma in each
t-node as its most probable target-language counter-
part (which is compliant with the previously chosen
formeme), according to a probabilistic dictionary.3
B36: Apply manual rules for fixing the formeme and
lexeme choices, which are otherwise systematically
wrong and are reasonably frequent. B37: Fill the gen-
der grammateme in t-nodes corresponding to deno-
tative nouns (it follows from the chosen t-lemma).4
B38: Fill the aspect grammateme in t-nodes corre-
sponding to verbs. Information about aspect (perfec-
tive/imperfective) is necessary for making decisions
about forming complex future tense in Czech. B39:
Apply rule-based correction of translated date/time
expressions (several templates such as 1970?s, July
1, etc.). B40: Fix grammateme values in places where
the English-Czech grammateme correspondence is
not trivial (e.g., if an English gerund expression
is translated using Czech subordinating clause, the
2The translation mapping from English formemes to Czech
formemes was obtained as follows: we analyzed 10,000 sen-
tence pairs from the WMT?08 training data up to the t-layer
(using a tagger shipped with the PDT and parser (McDonald et
al., 2005) for Czech), added formemes to t-trees on both sides,
aligned the t-trees (using a set of weighted heuristic rules, simi-
larly to (Menezes and Richardson, 2001)), and from the aligned
t-node pairs extracted for each English formeme its most fre-
quent Czech counterpart.
3The dictionary was created by merging the translation dic-
tionary from PCEDT ((Cur???n and others, 2004)) and a trans-
lation dictionary extracted from a part of the parallel corpus
Czeng ((Bojar and Z?abokrtsky?, 2006)) aligned at word-level by
Giza++ ((Och and Ney, 2003)).
4Czech nouns have grammatical gender which is (among
others) important for resolving grammatical agreement.
tense grammateme has to be filled). B41: Negate
verb forms where some arguments of the verbs bear
negative meaning (double negation in Czech). B42:
Verb t-nodes in active voice that have transitive t-
lemma and no accusative object, are turned to re-
flexives. B43: The t-nodes with genitive formeme
or prepositional-group formeme, whose counterpart
English t-nodes are located in pre-modification po-
sition, are moved to post-modification position. B44:
Reverse the dependency orientation between nu-
meric expressions and counted nouns, if the value
of the numeric expression is greater than four and
the noun without the numeral would be expressed in
nominative or accusative case. B45: Find coreference
links from personal pronouns to their antecedents,
if the latter are in subject position (needed later for
reflexivization).
2.6 From Czech t-layer to Czech a-layer
B46: Create initial a-trees by cloning t-trees. B47:
Fill the surface morphological categories (gender,
number, case, negation, etc.) with values derived
from values of grammatemes, formeme, seman-
tic part of speech etc. B48: Propagate the values
of gender and number of relative pronouns from
their antecedents (along the coreference links). B49:
Propagate the values of gender, number and person
according to the subject-predicate agreement (i.e.,
from subjects to the finite verbs). B50: Resolve agree-
ment of adjectivals in attributive positions (copying
gender/number/case from their governing nouns).
B51: Resolve complement agreement (copying gen-
der/number from subject to adjectival complement).
B52: Apply pro-drop ? deletion of personal pronouns
in subject positions. B53: Add preposition a-nodes
(if implied by the t-node?s formeme). B54: Add a-
nodes for subordinating conjunction (if implied by
the t-node?s formeme). B55: Add a-nodes corre-
sponding to reflexive particles for reflexiva tantum
verbs. B56: Add an a-node representing the auxiliary
verb by?t (to be) in the case of compound passive
verb forms. B57: Add a-nodes representing modal
verbs, accordingly to the deontic modality gram-
mateme. B58: Add the auxiliary verb by?t in imperfec-
tive future-tense complex verb forms. B59: Add verb
forms such as by/bys/bychom expressing conditional
verb modality. B60: Add auxiliary verb forms such
as jsem/jste in past-tense complex verb forms. B61:
169
Partition a-trees into finite clauses (a-nodes belong-
ing to the same clause are coindexed). B62: In each
clause, a-nodes which represent clitics are moved to
the so called second position in the clause (accord-
ing to Wackernagel?s law). B63: Add a-nodes cor-
responding to sentence-final punctuation mark. B64:
Add a-nodes corresponding to commas on bound-
aries between governing and subordinated clauses.
B65: Add a-nodes corresponding to commas in front
of conjunction ale and also commas in multiple co-
ordinations. B66: Add pairs of parenthesis a-nodes.
B67: Choose morphological lemmas in a-nodes cor-
responding to personal pronouns. B68: Generate
the resulting word forms (derived from lemmas and
tags) using Czech word form generator described in
(Hajic?, 2004). B69: Vocalize prepositions k, s, v, and
z (accordingly to the prefix of the following word).
B70: Capitalize the first word in each sentence as well
as in each direct speech.
2.7 From Czech a-layer to Czech w-layer
B71: Create the resulting sentences by flattening the
a-trees. Heuristic rules for proper spacing around
punctuation marks are used. B72: Create the resulting
text by concatenating the resulting sentences.
3 Final remarks
We believe that the potential contribution of tec-
togrammatical layer of language representation for
MT is the following: it abstracts from many
language-specific phenomena (which could reduce
the notorious data-sparsity problem) and offers a
natural factorization of the translation task (which
could be useful for formulating independence as-
sumptions when building probabilistic models). Of
course, the question naturally arises whether these
properties can ever outbalance the disadvantages, es-
pecially cumulation and interference of errors made
on different layers, considerable technical complex-
ity, and the need for detailed linguistic insight. In
our opinion, this question still remains open. On
one hand, the translation quality offered now by Tec-
toMT is below the state-of-the-art system according
to the preliminary evaluation of the WMT08 Shared
Task. But on the other hand, the potential of tec-
togrammatics has not been used fully, and more-
over there are still many components with only pilot
heuristic implementation which increase the number
of translation errors and which can be relatively eas-
ily substituted by corpus-based solutions. In the near
future, we plan to focus especially on the transfer
blocks, which are currently based on the naive as-
sumption of isomorphism of the source and target
t-trees and which do not make use of the target lan-
guage model, so far.
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2006. CzEng:
Czech-English Parallel Corpus, Release version 0.5.
Prague Bulletin of Mathematical Linguistics, 86:59?
62.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger . pages 224?231, Seattle.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Jan Cur???n et al 2004. Prague Czech - English Depen-
dency Treebank, Version 1.0. CD-ROM, Linguistics
Data Consortium, LDC Catalog No.: LDC2004T25,
Philadelphia.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of HTL/EMNLP, pages 523?530, Vancouver, Canada.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the workshop on Data-driven methods in ma-
chine translation, volume 14, pages 1?8.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust Applied Morphological Generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference, pages 201?208, Israel.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague.
170
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125?129,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
English-Czech MT in 2008 ?
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin Popel,
Jan Pta?c?ek, Jan Rous?, Zdene?k ?Zabokrtsky?
Charles University, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz
{popel,jan.rous}@matfyz.cz
Abstract
We describe two systems for English-to-
Czech machine translation that took part
in the WMT09 translation task. One of
the systems is a tuned phrase-based system
and the other one is based on a linguisti-
cally motivated analysis-transfer-synthesis
approach.
1 Introduction
We participated in WMT09 with two very dif-
ferent systems: (1) a phrase-based MT based
on Moses (Koehn et al, 2007) and tuned for
English?Czech translation, and (2) a complex
system in the TectoMT platform ( ?Zabokrtsky? et
al., 2008).
2 Data
2.1 Monolingual Data
Our Czech monolingual data consist of (1)
the Czech National Corpus (CNC, versions
SYN200[056], 72.6%, Kocek et al (2000)), (2)
a collection of web pages downloaded by Pavel
Pecina (Web, 17.1%), and (3) the Czech mono-
lingual data provided by WMT09 organizers
(10.3%). Table 1 lists sentence and token counts
(see Section 2.3 for the explanation of a- and t-
layer).
Sentences 52 M
with nonempty t-layer 51 M
a-nodes (i.e. tokens) 0.9 G
t-nodes 0.6 G
Table 1: Czech monolingual training data.
? The work on this project was supported by the grants
MSM0021620838, 1ET201120505, 1ET101120503, GAUK
52408/2008, M?SMT ?CR LC536 and FP6-IST-5-034291-STP
(EuroMatrix).
2.2 Parallel Data
As the source of parallel data we use an internal
release of Czech-English parallel corpus CzEng
(Bojar et al, 2008) extended with some additional
texts. One of the added sections was gathered
from two major websites containing Czech sub-
titles to movies and TV series1. The matching of
the Czech and English movies is rather straight-
forward thanks to the naming conventions. How-
ever, we were unable to reliably determine the se-
ries number and the episode number from the file
names. We employed a two-step procedure to au-
tomatically pair the TV series subtitle files. For
every TV series:
1. We clustered the files on both sides to remove
duplicates
2. We found the best matching using a provi-
sional translation dictionary. This proved to
be a successful technique on a small sample
of manually paired test data. The process was
facilitated by the fact that the correct pairs of
episodes usually share some named entities
which the human translator chose to keep in
the original English form.
Table 2 lists parallel corpus sizes and the distri-
bution of text domains.
English Czech
Sentences 6.91 M
with nonempty t-layer 6.89 M
a-nodes (i.e. tokens) 61 M 50 M
t-nodes 41 M 33 M
Distribution: [%] [%]
Subtitles 68.2 Novels 3.3
Software Docs 17.0 Commentaries/News 1.5
EU (Legal) Texts 9.5 Volunteer-supplied 0.4
Table 2: Czech-English data sizes and sources.
1www.opensubtitles.org and titulky.com
125
2.3 Data Preprocessing using TectoMT
platform: Analysis and Alignment
As we believe that various kinds of linguistically
relevant information might be helpful in MT, we
performed automatic analysis of the data. The
data were analyzed using the layered annotation
scheme of the Prague Dependency Treebank 2.0
(PDT 2.0, Hajic? and others (2006)), i.e. we used
three layers of sentence representation: morpho-
logical layer, surface-syntax layer (called analyti-
cal (a-) layer), and deep-syntax layer (called tec-
togrammatical (t-) layer).
The analysis was implemented using TectoMT,
( ?Zabokrtsky? et al, 2008). TectoMT is a highly
modular software framework aimed at creating
MT systems (focused, but by far not limited to
translation using tectogrammatical transfer) and
other NLP applications. Numerous existing NLP
tools such as taggers, parsers, and named entity
recognizers are already integrated in TectoMT, es-
pecially for (but again, not limited to) English and
Czech.
During the analysis of the large Czech mono-
lingual data, we used Jan Hajic??s Czech tagger
shipped with PDT 2.0, Maximum Spanning Tree
parser (McDonald et al, 2005) with optimized set
of features as described in Nova?k and ?Zabokrtsky?
(2007), and a tool for assigning functors (seman-
tic roles) from Klimes? (2006), and numerous other
components of our own (e.g. for conversion of an-
alytical trees into tectogrammatical ones).
In the parallel data, we analyzed the Czech side
using more or less the same scenario as used for
the monolingual data. English sentences were an-
alyzed using (among other tools) Morce tagger
Spoustova? et al (2007) and Maximum Spanning
Tree parser.2
The resulting deep syntactic (tectogrammatical)
Czech and English trees are then aligned using T-
aligner?a feature based greedy algorithm imple-
mented for this purpose (Marec?ek et al, 2008). T-
aligner finds corresponding nodes between the two
given trees and links them. For deciding whether
to link two nodes or not, T-aligner makes use of
a bilingual lexicon of tectogrammatical lemmas,
morphosyntactic similarities between the two can-
didate nodes, their positions in the trees and other
similarities between their parent/child nodes. It
2In some previous experiments (e.g. ?Zabokrtsky? et al
(2008)), we used phrase-structure parser Collins (1999) with
subsequent constituency-dependency conversion.
also uses word alignment generated from surface
shapes of sentences by GIZA++ tool, Och and Ney
(2003). We use acquired aligned tectogrammatical
trees for training some models for the transfer.
As analysis of such amounts of data is obvi-
ously computationally very demanding, we run it
in parallel using Sun Grid Engine3 cluster of 40
4-CPU computers. For this purpose, we imple-
mented a rather generic tool that submits any Tec-
toMT pipeline to the cluster.
3 Factored Phrase-Based MT
We essentially repeat our experiments from last
year (Bojar and Hajic?, 2008): GIZA++ align-
ments4 on a-layer lemmas (a-layer nodes corre-
spond 1-1 to surface tokens), symmetrized using
grow-diag-final (no -and) heuristic5 .
Probably due to the domain difference (the test
set is news), including Subtitles in the parallel data
and Web in the monolingual data did not bring any
improvement that would justify the additional per-
formance costs. For most of the phrase-based ex-
periments, we thus used only 2.2M parallel sen-
tences (27M Czech and 32M English tokens) and
43M Czech sentences (694 M tokens).
In Table 3 below, we report the scores for the
following setups selected from about 50 experi-
ments we ran in total:
Moses T is a simple phrase-based translation (T)
with no additional factors. The translation is
performed on truecased word forms (i.e. sen-
tence capitalization removed unless the first
word seems to be a name). The 4-gram lan-
guage model is based on the 43M sentences.
Moses T+C is a factored setup with form-to-form
translation (T) and target-side morphological
coherence check following Bojar and Hajic?
(2008). The setup uses two language mod-
els: 4-grams of word forms and 7-grams of
morphological tags.
Moses T+C+C&T+T+G 84k is a setup desirable
from the linguistic point of view. Two in-
dependent translation paths are used: (1)
form?form translation with two target-side
checks (lemma and tag generated from the
target-side form) as a fine-grained baseline
3http://gridengine.sunsource.net/
4Default settings, IBM models and iterations: 153343.
5Later, we found out that the grow-diag-final-and heuris-
tic provides insignificantly superior results.
126
with the option to resort to (2) an independent
translation of lemma?lemma and tag?tag
finished by a generation step that combines
target-side lemma and tag to produce the fi-
nal target-side form.
We use three language models in this setup
(3-grams of forms, 3-grams of lemmas, and
10-grams of tags).
Due to the increased complexity of the setup,
we were able to train this model on 84k par-
allel sentences only (the Commentaries sec-
tion) and we use the target-side of this small
training data for language models, too.
For all the setups we perform standard MERT
training on the provided development set.6
4 Translation Setup Based on
Tectogrammatical Transfer
In this translation experiment, we follow the tradi-
tional analysis-transfer-synthesis approach, using
the set of PDT 2.0 layers: we analyze the input
English sentence up to the tectogrammatical layer
(through the morphological and analytical ones),
then perform the tectogrammatical transfer, and
then synthesize the target Czech sentence from its
tectogrammatical representation. The whole pro-
cedure consists of about 80 steps, so the following
description is necessarily very high level.
4.1 Analysis
Each sentence is tokenized (roughly according to
the Penn Treebank conventions), tagged by the En-
glish version of the Morce tagger Spoustova? et al
(2007), and lemmatized by our lemmatizer. Then
the dependency parser (McDonald et al, 2005) is
applied. Then the analytical trees resulting from
the parser are converted to the tectogrammatical
ones (i.e. functional words are removed, only
morphologically indispensable categories are left
with the nodes using a sequence of heuristic proce-
dures). Unlike in PDT 2.0, the information about
the original syntactic form is stored with each t-
node (values such as v:inf for an infinitive verb
form, v:since+fin for the head of a subor-
dinate clause of a certain type, adj:attr for
an adjective in attribute position, n:for+X for a
given prepositional group are distinguished).
6We used the full development set of 2k sentences for
?Moses T? and a subset of 1k sentences for the other two
setups due to time constraints.
One of the steps in the analysis of English is
named entity recognition using Stanford Named
Entity Recognizer (Finkel et al, 2005). The nodes
in the English t-layer are grouped according to the
detected named entities and they are assigned the
type of entity (location, person, or organization).
This information is preserved in the transfer of the
deep English trees to the deep Czech trees to al-
low for the appropriate capitalization of the Czech
translation.
4.2 Transfer
The transfer phase consists of the following steps:
? Initiate the target-side (Czech) t-trees sim-
ply by ?cloning? the source-side (English) t-
trees. Subsequent steps usually iterate over
all t-nodes. In the following, we denote a
source-side t-node as S and the correspond-
ing target-side node as T.
? Translate formemes using
two probabilistic dictionaries
(p(T.formeme|S.formeme, S.parent.lemma)
and p(T.formeme|S.formeme)) and a few
manual rules. The formeme translation
probability estimates were extracted from a
part of the parallel data mentioned above.
? Translate lemmas using a probabilistic dictio-
nary (p(T.lemma|S.lemma)) and a few rules
that ensure compatibility with the previously
chosen formeme. Again, this probabilistic
dictionary was obtained using the aligned
tectogrammatical trees from the parallel cor-
pus.
? Fill the grammatemes (deep-syntactic equiv-
alent of morphological categories) gender
(for denotative nouns) and aspect (for verbs)
according to the chosen lemma. We also
fix grammateme values where the English-
Czech grammateme correspondence is non-
trivial (e.g. if an English gerund expression is
translated to Czech as a subordinating clause,
the tense grammateme has to be filled). How-
ever, the transfer of grammatemes is defi-
nitely much easier task than the transfer of
formemes and lemmas.
4.3 Synthesis
The transfer step yields an abstract deep
syntactico-semantical tree structure. Firstly,
127
we derive surface morphological categories
from their deep counterparts taking care of their
agreement where appropriate and we also remove
personal pronouns in subject positions (because
Czech is a pro-drop language).
To arrive at the surface tree structure, auxil-
iary nodes of several types are added, including
(1) reflexive particles, (2) prepositions, (3) subor-
dinating conjunctions, (4) modal verbs, (5) ver-
bal auxiliaries, and (6) punctuation nodes. Also,
grammar-based node ordering changes (imple-
mented by rules) are performed: e.g. if an English
possessive attribute is translated using Czech gen-
itive, it is shifted into post-modification position.
After finishing the inflection of nouns, verbs,
adjectives and adverbs (according to the values of
morphological categories derived from agreement
etc.), prepositions may need to be vocalized: the
vowel -e or -u is attached to the preposition if the
pronunciation of prepositional group would be dif-
ficult otherwise.
After the capitalization of the beginning of each
sentence (and each named entity instance), we ob-
tain the final translation by flattening the surface
tree.
4.4 Preliminary Error Analysis
According to our observations most errors happen
during the transfer of lemmas and formemes.
Usually, there are acceptable translations of
lemma and formeme in respective n-best lists
but we fail to choose the best one. The sce-
nario described in Section 4.2 uses quite a
primitive transfer algorithm where formemes
and lemmas are translated separately in two
steps. We hope that big improvements could
be achieved with more sophisticated algo-
rithms (optimizing the probability of the whole
tree) and smoothed probabilistic models (such
as p(T.lemma|S.lemma, T.parent.lemma) and
p(T.formeme|S.formeme, T.lemma, T.parent.lemma)).
Other common errors include:
? Analysis: parsing (especially coordinations
are problematic with McDonald?s parser).
? Transfer: the translation of idioms and col-
locations, including named entities. In these
cases, the classical transfer at the t-layer
is not appropriate and utilization of some
phrase-based MT would help.
? Synthesis: reflexive particles, word order.
5 Experimental Results and Discussion
Table 3 reports lowercase BLEU and NIST scores
and preliminary manual ranks of our submissions
in contrast with other systems participating in
English?Czech translation, as evaluated on the
official WMT09 unseen test set. Note that auto-
matic metrics are known to correlate quite poorly
with human judgements, see the best ranking but
?lower scoring? PC Translator this year and also
in Callison-Burch et al (2008).
System BLEU NIST Rank
Moses T 14.24 5.175 -3.02 (4)
Moses T+C 13.86 5.110 ?
Google 13.59 4.964 -2.82 (3)
U. of Edinburgh 13.55 5.039 -3.24 (5)
Moses T+C+C&T+T+G 84k 10.01 4.360 -
Eurotran XP 09.51 4.381 -2.81 (2)
PC Translator 09.42 4.335 -2.77 (1)
TectoMT 07.29 4.173 -3.35 (6)
Table 3: Automatic scores and preliminary human
rank for English?Czech translation. Systems in
italics are provided for comparison only. Best re-
sults in bold.
Unfortunately, this preliminary evaluation sug-
gests that simpler models perform better, partly
because it is easier to tune them properly both
from computational point of view (e.g. MERT
not stable and prone to overfitting with more fea-
tures7), as well as from software engineering point
of view (debugging of complex pipelines of tools
is demanding). Moreover, simpler models run
faster: ?Moses T? with 12 sents/minute is 4.6
times faster than ?Moses T+C?. (Note that we have
not tuned either of the models for speed.)
While ?Moses T? is probably nearly identical
setup as Google and Univ. of Edinburgh use,
the knowledge of correct language-dependent to-
kenization and the use of relatively high quality
large language model data seems to bring moder-
ate improvements.
6 Conclusion
We described our experiments with a complex lin-
guistically motivated translation system and vari-
ous (again linguistically-motivated) setups of fac-
tored phrase-based translation. An automatic eval-
uation seems to suggest that simpler is better, but
we are well aware that a reliable judgement comes
only from human annotators.
7For ?Moses T+C+C&T+T+G?, we observed BLEU
scores on the test set varying by up to five points absolute
for various weight settings yielding nearly identical dev set
scores.
128
References
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143?146, Columbus, Ohio, June. Association for
Computational Linguistics.
Ondr?ej Bojar, Miroslav Jan??c?ek, Zdene?k ?Zabokrtsky?,
Pavel ?Ces?ka, and Peter Ben?a. 2008. CzEng 0.7:
Parallel Corpus with Community-Supplied Transla-
tions. In Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, May. ELRA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T0 1, Philadelphia.
Va?clav Klimes?. 2006. Analytical and Tectogrammat-
ical Analysis of a Natural Language. Ph.D. thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity, Prague, Czech Rep.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of European Machine Translation Confer-
ence (EAMT 08), pages 102?111, Hamburg, Ger-
many.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT
?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancou-
ver, British Columbia, Canada.
Va?clav Nova?k and Zdene?k ?Zabokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, ed-
itors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th I nternational Conference on
Text, Speech and Dialogue, Lecture Notes in Com-
puter Science, pages 92?98, Pilsen, Czech Repub-
lic. Springer Science+Business Media Deutschland
GmbH.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular Hybrid MT System
with Tectogrammatics Used as Transfer Layer. In
Proc. of the ACL Workshop on Statistical Machine
Translation, pages 167?170, Columbus, Ohio, USA.
129
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 194?201,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Czech Named Entity Corpus and SVM-based Recognizer
Jana Kravalova?
Charles University in Prague
Institute of Formal and Applied Linguistics
kravalova@ufal.mff.cuni.cz
Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
zabokrtsky@ufal.mff.cuni.cz
Abstract
This paper deals with recognition of
named entities in Czech texts. We present
a recently released corpus of Czech sen-
tences with manually annotated named en-
tities, in which a rich two-level classifica-
tion scheme was used. There are around
6000 sentences in the corpus with roughly
33000 marked named entity instances. We
use the data for training and evaluating a
named entity recognizer based on Support
Vector Machine classification technique.
The presented recognizer outperforms the
results previously reported for NE recog-
nition in Czech.
1 Introduction
After the series of Message Understanding
Conferences (MUC; (Grishman and Sundheim,
1996)), processing of named entities (NEs) be-
came a well established discipline within the NLP
domain, usually motivated by the needs of Infor-
mation Extraction, Question Answering, or Ma-
chine Translation. For English, one can find liter-
ature about attempts at rule-based solutions for the
NE task as well as machine-learning approaches,
be they dependent on the existence of labeled data
(such as CoNLL-2003 shared task data), unsuper-
vised (using redundancy in NE expressions and
their contexts, see e.g. (Collins and Singer, 1999))
or a combination of both (such as (Talukdar et al,
2006), in which labeled data are used as a source
of seed for an unsupervised procedure exploiting
huge unlabeled data). A survey of research on
named entity recognition is available in (Ekbal and
Bandyopadhyay, 2008).
There has been considerably less research
done in the NE field in Czech, as discussed in
(S?evc???kova? et al, 2007b). Therefore we focus on
it in this paper, which is structured as follows. In
Section 2 we present a recently released corpus
of Czech sentences with manually annotated in-
stances of named entities, in which a rich classi-
fication scheme is used. In Section 3 we describe
a new NE recognizer developed for Czech, based
on the Support Vector Machine (SVM) classifi-
cation technique. Evaluation of such approach is
presented in Section 4. The summary is given in
Section 5.
2 Manually Annotated Corpus
2.1 Data Selection
We have randomly selected 6000 sentences
from the Czech National Corpus1 from the re-
sult of the query ([word=".*[a-z0-9]"]
[word="[A-Z].*"]). This query makes the
relative frequency of NEs in the selection higher
than the corpus average, which makes the sub-
sequent manual annotation much more effective,
even if it may slightly bias the distribution of NE
types and their observed density.2
2.2 Annotation NE Instances with Two-level
NE Classification
There is no generally accepted typology of Named
Entities. One can see two trends: from the view-
point of unsupervised learning, it is advantageous
to have just a few coarse-grained categories (cf.
the NE classification developed for MUC confer-
ences or the classification proposed in (Collins
and Singer, 1999), where only persons, locations,
and organizations were distinguished), whereas
those interested in semantically oriented applica-
tions prefer more informative (finer-grained) cat-
egories (e.g. (Fleischman and Hovy, 2002) with
1http://ucnk.ff.cuni.cz
2The query is trivially motivated by the fact that NEs in
Czech (as well as in many other languages) are often marked
by capitalization of the first letter. Annotation of NEs in a cor-
pus without such selection would lower the bias, but would
be more expensive due to the lower density of NE instances
in the annotated material.
194
Types of NE
a - Numbers in addresses
c - Bibliographic items
g - Geographical names
i - Institutions
m - Media names
n - Specific number usages
o - Artifact names
p - Personal names
q - Quantitative expressions
t - Time expressions
ah - street numbers at - phone/fax numbers
az - zip codes
cb - volume numbers cn - chapt./sect./fig. numbers
cp - page numbers cr - legisl. act numberscs - article titles
gc - states gh - hydronyms
gl - nature areas / objects gp - planets, cosmic objectsgq - urban parts gr - territorial namesgs - streets, squares gt - continentsgu - cities/towns g_ - underspecifiedia - conferences/contests ic - cult./educ./scient. inst.if - companies, concerns... io - government/political inst.i_ - underspecified
mi - internet links mn - periodical
mr - radio stations mt - TV stationsna - age nc - sport scoreni - itemizer nm - in formulanp - part of personal name nq - town quarternr - ratio nw - flat sizen_ - underspecified
oa - cultural artifacts (books, movies) oc - chemical
oe - measure units om - currency unitsop - products or - directives, normso_ - underspecified
pb - animal names pc - inhabitant names
pd - (academic) titles pf - first names
pm - second names pp - relig./myth personsps - surnames p_ - underspecified
qc - cardinal numbers qo - ordinal numbers
tc - centuries td - daystf - feasts th - hours
tm - months tn - minutes
tp - epochs ts - seconds
ty - years
Figure 1: Two-level hierarchical classification of NEs used in the corpus. Note that the (detailed) NE
types are divided into two columns just because of the space reasons here.
195
eight types of person labels, or Sekine?s Extended
NE Hierarchy, cf. (Sekine, 2003)).
In our corpus, we use a two-level NE classifi-
cation depicted in Figure 1. The first level corre-
sponds to rough categories (called NE supertypes)
such as person names, geographical names etc.
The second level provides a more detailed classi-
fication: e.g. within the supertype of geographi-
cal names, the NE types of names of cities/towns,
names of states, names of rivers/seas/lakes etc.
are distinguished.3 If more robust processing is
necessary, only the first level (NE supertypes)
can be used, while the second level (NE types)
comes into play when more subtle information is
needed. Each NE type is encoded by a unique two-
character tag (e.g., gu for names of cities/towns,
gc for names of states; a special tag, such as g ,
makes it possible to leave the NE type underspec-
ified).
Besides the terms of NE type and supertype, we
use also the term NE instance, which stands for a
continuous subsequence of tokens expressing the
entity in a given text. In the simple plain-text for-
mat, which we use for manual annotations, the NE
instances are marked as follows: the word or the
span of words belonging to the NE is delimited by
symbols < and >, with the former one immediately
followed by the NE type tag (e.g. <pf John> loves
<pf Mary>).
The annotation scheme allows for the embed-
ding of NE instances. There are two types of em-
bedding. In the first case, the NE of a certain
type can be embedded in another NE (e.g., the
river name can be part of a name of a city as in
<gu U?st?? nad <gh Labem>>). In the second case,
two or more NEs are parts of a (so-called) con-
tainer NE (e.g., two NEs, a first name and a sur-
name, form together a person name container NE
such as in <P<pf Paul> <ps Newman>>). The
container NEs are marked with a capital one-letter
tag: P for (complex) person names, T for tempo-
ral expressions, A for addresses, and C for biblio-
graphic items. A more detailed description of the
NE classification can be found in (S?evc???kova? et al,
2007b).
3Given the size of the annotated data, further subdivi-
sion into even finer classes (such as persons divided into cat-
egories such as lawyer, politician, scientist used in (Fleis-
chman and Hovy, 2002)) would result in too sparse annota-
tions.
2.3 Annotated Data Cleaning
After collecting all the sentences annotated by the
annotators, it was necessary to clean the data in or-
der to improve the data quality. For this purpose,
a set of tests was implemented. The tests revealed
wrong or ?suspicious? spots in the data (based e.g.
on the assumption that the same lemma should
manifest an entity of the same type in most its oc-
currences), which were manually checked and cor-
rected if necessary. Some noisy sentences caused
e.g. by wrong sentence segmentation in the origi-
nal resource were deleted; the final size of the cor-
pus is 5870 sentences.
2.4 Morphological Analysis of Annotated
Data
The sentences have been enriched with morpho-
logical tags and lemmas using Jan Hajic??s tagger
shipped with Prague Dependency Treebank 2.0
(Hajic? et al, 2006) integrated into the TectoMT
environment (Z?abokrtsky? et al, 2008). Motivation
for this step was twofold
? Czech is a morphologically rich language,
and named entities might be subject to
paradigms with rich inflection too. For
example, male first name Toma?s? (Thomas)
migh appear also in one of the following
forms: Toma?s?e, Toma?s?ovi, Toma?s?i, Toma?s?em,
Toma?s?ove?, Toma?s?u?m . . . (according to gram-
matical case and number), which would make
the training data without lemmatization much
sparser.
? Additional features (useful for SVM as well
as for any other Machine Learning approach)
can be mined from the lemma and tag se-
quences, as shown in Section 3.2.
2.5 Public Data Release
Manually annotated and cleaned 6000 sentences
with roughly 33000 named entities were released
as Czech Named Entity Corpus 1.0. The corpus
consists of manually annotated sentences and mor-
phological analysis in several formats: a simple
plain text format, a simple xml format, a more
complex xml format based on the Prague Markup
Language (Pajas and S?te?pa?nek, 2006) and contain-
ing also the above mentioned morphological anal-
ysis, and the html format with visually highlighted
NE instances.
For the purposes of supervised machine learn-
ing, division of data into training, development
196
and evaluation subset is provided in the corpus.
The division into training, development and evalu-
ation subsets was made by random division of sen-
tences into three sets, in proportion 80% (training),
10% (development) and 10% (evaluation), see Ta-
ble 1. Other basic quantitative properties are sum-
marized in Table 2 and Table 3.
The resulting data collection, called
Czech Named Entity Corpus 1.0, is
now publicly available on the Internet at
http://ufal.mff.cuni.cz/tectomt.
Set #Sentences #Words #NE instances
train 4696 119921 26491
dtest 587 14982 3476
etest 587 15119 3615
total 5870 150022 33582
Table 1: Division of the annotated corpus into
training, development test, and evaluation test sets.
Lenght #Occurrences Proportion
one-word 23057 68.66%
two-word 6885 20.50%
three-word 1961 5.84%
longer 1679 5.00%
total 33582 100.00%
Table 2: Occurrences of NE instances of different
length in the annotated corpus.
3 SVM-based Recognizer
3.1 NER as a classification task
In this section, we formulate named entity recog-
nition as a classification problem. The task of
named entity recognition as a whole includes sev-
eral problems to be solved:
? detecting ?basic? one-word, two-word and
multiword named entities,
? detecting complex entities containing other
entities (e.g. an institution name containing
a personal name).
Furthermore, one can have different require-
ments on what a correctly recognized named entity
is (and train a separate recognizer for each case):
? an entity whose span and type are correctly
recognized,
NE type #Occurrences Proportion
ps 4040 12.03%
pf 3072 9.15%
P 2722 8.11%
gu 2685 8.00%
qc 2040 6.07%
oa 1695 5.05%
ic 1410 4.20%
ty 1325 3.95%
th 1325 3.95%
s 1285 3.83%
gc 1107 3.30%
if 834 2.48%
io 830 2.47%
tm 559 1.66%
n 512 1.52%
f 506 1.51%
Table 3: Distribution of several most frequent NE
types in the annotated corpus.
? an entity whose span and supertype are cor-
rectly recognized,
? an entity whose span is correctly recognized
(without regard to its type).
Therefore, we subdivide the classification prob-
lem into a few subproblems. Firstly, we indepen-
dently evaluate the recognition system for one-
word named entities, for two-word named enti-
ties and for multiword named entities. For each
of these three problems, we define three tasks, or-
dered from the easiest to the most difficult:
? Named entity span recognition ? all words of
named entity must be found but the type is
not relevant. For one-word entities, this re-
duces to 0/1 classification problem, that is,
each word is either marked as named entity
(1) or as regular word (0). For two-word en-
tities, this 0/1 decision is made for each cou-
ple of subsequent words (bigram) in the sen-
tence.
? Named entity supertype recognition ? all
words of named entity must be found and the
supertype must be correct. This is a multi-
class classification problem, where classes
are named entity classes of the first level in
hierarchy (p, g, i, ...) plus one class
for regular words.
197
? Named entity type recognition ? all words
of named entity must be found and the type
must be correct.
In our solution, a separate SVM classifier
is built for one-word named entities, two-word
named entities and three-word named entities.
Then, as we proceed through the text, we apply the
classifier on each ?window? or ?n-gram? of words
? one-word, two-word and three-word, classifying
the n-gram with the corresponding SVM classi-
fier. We deliberately omit named entities contain-
ing four and more words, as they represent only a
small portion of the instances (5%).
3.2 Features
Classification features which were used by the
SVM classifier(s), are as follows:
? morphological features ? part of speech, gen-
der, case and number,
? orthographic features ? boolean features
such as capital letter at the beginning of the
word or regular expression for time and year
,
? lists of known named entities ? boolean fea-
tures describing whether the word is listed
in lists of Czech most used names and sur-
names, Czech cities, countries or famous in-
stitutions,
? lemma ? some lemmas contain shortcuts de-
scribing the property of lemma, for example
?Prahou? (Prague, 7th case) would lemma-
tize to ?Praha ;G? with mark ? ;G? hinting
that ?Praha? is a geographical name,
? context features ? similar features for pre-
ceding and following words, that is, part of
speech, gender, case and number for the pre-
ceding and following word, orthographic fea-
tures, membership in a list of known entities
and lemma hints for the preceding and fol-
lowing word.
All classification features were transformed into
binary (boolean) features, resulting in roughly
200-dimensional binary feature space.
3.3 Classifier implementation
For the classification task, we decided to use Sup-
port Vector Machine classification method. First,
this solution has been repeatedly shown to give
better scores in NE recognition in comparison to
other Machine Learning methods, see e.g. (Isozaki
and Kazawa, 2002) and (Ekbal and Bandyopad-
hyay, 2008). Second, in our preliminary experi-
ments on our data it outperformed all other solu-
tions too (based on naive Bayes, k nearest neigh-
bors, and decision trees).
As an SVM classifier, we used its CPAN Perl
implementation Algorithm-SVM.4
Technically, the NE recognizer is implemented
as a Perl module included into TectoMT, which is
a modular open source software framework for im-
plementing NLP applications, (Z?abokrtsky? et al,
2008).5
4 Evaluation
4.1 Evaluation metrics
We use the following standard quantities for eval-
uating performance of the presented classifier:
? precision ? the number of correctly predicted
NEs divided by the number of all predicted
NEs,
? recall ? the number of correctly predicted
NEs divided by the number of all NEs in the
data,
? f-score ? harmonic mean of precision and re-
call.
In our opinion, simpler quantities such as accu-
racy (the percentage of correctly marked words)
are not suitable for this task, since the number
of NE instances to be found is not known in ad-
vance.6
4.2 Results
The results for SVM classifier when applied on
the evaluation test set of the corpus are summa-
rized in Table 4. The table evaluates all subtasks
as defined in Section 3.1, that is, for combination
4http://www.cpan.org/authors/id/L/LA/LAIRDM/
5One of the reasons for integrating the classifier into Tec-
toMT is the fact that it requires the input texts to be sentence-
segmented, tokenized, tagged and lemmatized; all the nec-
essary tools for such preprocessing are already available in
TectoMT.
6Counting also all non-NEwords predicted as non-entities
as a success would lead to very high accuracy value without
much information content (obviously most words are not NE
instances).
198
All NEs One-word NEs Two-word NEs
P R F P R F P R F
span+type 0.75 0.62 0.68 0.80 0.71 0.75 0.68 0.62 0.65
span+supertype 0.75 0.67 0.71 0.87 0.78 0.82 0.71 0.64 0.67
span 0.84 0.70 0.76 0.89 0.80 0.84 0.76 0.69 0.72
Table 4: Summary of the SVM classifier performance (P=precision, R=recall, F=f-measure). Recogni-
tion of NEs of different length is evaluated separately. The other dimension corresponds to the gradually
released correctness requirements.
true type predicted type true type description predicted type description errors
oa x cultural artifacts (books, movies) no entity 184
ic x cult./educ./scient. inst. no entity 74
x gu no entity cities/towns 71
x P no entity personal name container 66
if x companies, concerns . . . no entity 60
x ic no entity cult./educ./scient. inst. 59
io x government/political inst. no entity 57
x ps no entity surnames 47
P x personal name container no entity 43
ps x surnames no entity 41
gu x cities/towns no entity 37
x td no entity days 35
op x products no entity 33
x pf no entity first names 31
T x time container no entity 30
Table 5: The most frequent types of errors in NE recognition made by the SVM classifier.
of subtask defined for all entities, one-word enti-
ties and two-word entities and with gradually re-
leased requirements for correctness: correct span
and correct (detailed) type, correct span and cor-
rect supertype, correct span only.
The most common SVM classification errors
are shown in Table 5.
4.3 Discussion
As we can see in Table 4, the classifier recognizes
span and type of all named entities in text with
f-measure = 0.68. This improves the results re-
ported on this data in (S?evc???kova? et al, 2007a),
which was 0.62. For one-word named entities, the
improvement is also noticeable, from 0.70 to 0.75.
In our opinion, the improvement is caused by
better feature selection on one hand. We do not
use as many classification features as the authors
of (S?evc???kova? et al, 2007a), instead we made a
preliminary manual selection of features we con-
sidered to be helpful. For example, we do not use
the whole variety of 15 Czech morphological cat-
egories for every word in context, but we use only
part of speech, gender, case and number. Also,
we avoided using features based on storing words
which occurred in training data, such as boolean
feature, which is true for words, which appeared
in training data as named entity. We tried employ-
ing such features, but in our opinion, they result in
sparsity in space searched by SVM.
It would be highly difficult to correctly compare
the achieved results with results reported on other
languages (such as f-score 88.76% achieved for
English in (Zhang and Johnson, 2003)), especially
because of different task granularity (and obvi-
ously highly different baselines). Furthermore, in
Czech the task is more complicated due to inflec-
tion: many named entities can appear in several
many different forms. For example, the Czech
capital city ?Praha? appeared in these forms in
training data: Praha, Prahy, Prahou, Prahu.
Table 5 describes the most common errors made
by classifier. Clearly, the most problematic classes
are objects (oa) and institutions (ic, if, io),
199
which mostly remain unrecognized. The problem
is that, cultural artifacts like books or movies, or
institutions, tend to have quite new and unusual
names, as opposed to personal names, for which
fairly limited amount of choice exists, and cities,
which do not change and can be listed easily.
Institutions also tend to have long and com-
plicated names, for which it is especially diffi-
cult to find the ending frontier. We believe that
dependency syntax analysis (such as dependency
trees resulting from the maximum spanning tree
parser, (McDonald et al, 2005)) might provide
some clues here. By determining the head of the
clause, e.g. theatre, university, gallery and it?s de-
pendants, we might get some hints about which
words are part of the name and which are not.
Yet another improvement in overall perfor-
mance could be achieved by incorporating hyper-
nym discovery (making use e.g. of Wikipedia) as
proposed in (Kliegr et al, 2008).
5 Conclusions
We have presented a new recently published cor-
pus of Czech sentences with manually annotated
named entities with fine-grained two-level annota-
tion. We used the data for training and evaluating a
named entity recognizer based on Support Vector
Machines classification technique. Our classifier
reached f-measure 0.68 in recognizing and classi-
fying Czech named entities into 62 categories and
thus outperformed the results previously reported
for NE recognition in Czech in (S?evc???kova? et al,
2007a).
We intend to further improve our classifier,
especially recognition of institution and object
names, by employing dependency syntax features.
Another improvement is hoped to be achieved us-
ing WWW-based ontologies.
Acknowledgments
This research was supported by MSM
0021620838, GAAV C?R 1ET101120503, and
MS?MT C?R LC536.
References
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC), pages 189?196.
Asif Ekbal and Sivaji Bandyopadhyay. 2008. Named
Entity Recognition using Support Vector Machine:
A Language Independent Approach . International
Journal of Computer Systems Science and Engineer-
ing, 4(2):155?170.
Michael Fleischman and Eduard Hovy. 2002. Fine
Grained Classification of Named Entities . In Pro-
ceedings of the 19th International Conference on
Computational Linguistics (COLING), volume I,
pages 267?273.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference - 6: A Brief History.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING), volume I,
pages 466?471.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova?. 2006. Prague Dependency Treebank
2.0.
Hideki Isozaki and Hideto Kazawa. 2002. Effi-
cient Support Vector Classifiers For Named Entity
Recognition. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING?02).
Tomas Kliegr, Krishna Chandramouli, Jan Nemrava,
Vojtech Svatek, and Ebroul Izquierdo. 2008.
Wikipedia as the premiere source for targeted hy-
pernym discovery. WBBT ECML08.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of Human Langauge Technology Conference
and Conference on Empirical Methods in Natural
Language Processing (HTL/EMNLP), pages 523?
530, Vancouver, BC, Canada.
Petr Pajas and Jan S?te?pa?nek. 2006. XML-based rep-
resentation of multi-layered annotation in the PDT
2.0. In Richard Erhard Hinrichs, Nancy Ide, Martha
Palmer, and James Pustejovsky, editors, Proceed-
ings of the LREC Workshop on Merging and Layer-
ing Linguistic Information (LREC 2006), pages 40?
47, Paris, France.
Satoshi Sekine. 2003. Sekine?s Extended Named En-
tity Hierarchy. http://nlp.cs.nyu.edu/ene/.
Magda S?evc???kova?, Zdene?k Z?abokrtsky?, and Oldr?ich
Kru?za. 2007. Named Entities in Czech: Annotat-
ing Data and Developing NE Tagger. In Va?clav Ma-
tous?ek and Pavel Mautner, editors, Lecture Notes in
Artificial Intelligence, Proceedings of the 10th Inter-
national Conference on Text, Speech and Dialogue,
volume 4629 of Lecture Notes in Computer Science,
pages 188?195, Pilsen, Czech Republic. Springer
Science+Business Media Deutschland GmbH.
200
Partha Pratim Talukdar, Thorsten Brants, Mark Liber-
man, and Fernando Pereira. 2006. A Context Pat-
tern Induction Method for Named Entity Extraction.
In Proceedings of the 10th Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 141?148.
Magda S?evc???kova?, Zdene?k Z?abokrtsky?, and Oldr?ich
Kru?za. 2007. Zpracova?n?? pojmenovany?ch entit
v c?esky?ch textech. Technical report, U?FAL MFF
UK, Praha.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition sys-
tem. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of CoNLL-2003, pages 204?207.
Edmonton, Canada.
201
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276?285,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Comparison of Classification and Ranking Approaches
to Pronominal Anaphora Resolution in Czech?
Ngu. y Giang Linh, Va?clav Nova?k, Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, CZ-11800
{linh,novak,zabokrtsky}.ufal.mff.cuni.cz
Abstract
In this paper we compare two Ma-
chine Learning approaches to the task
of pronominal anaphora resolution: a
conventional classification system based
on C5.0 decision trees, and a novel
perceptron-based ranker. We use coref-
erence links annotated in the Prague De-
pendency Treebank 2.0 for training and
evaluation purposes. The perceptron sys-
tem achieves f-score 79.43% on recogniz-
ing coreference of personal and possessive
pronouns, which clearly outperforms the
classifier and which is the best result re-
ported on this data set so far.
1 Introduction
Anaphora Resolution (AR) is a well established
task in Natural Language Processing (Mitkov,
2002). Classification techniques (e.g., single can-
didate model aimed at answering: ?Is there a
coreference link between the anaphor and this
antecedent candidate, or not??) are very often
used for the task, e.g. in Mccarthy and Lehnert
(1995) and Soon et al (2001). However, as ar-
gued already in Yang et al (2003), better results
are achieved when the candidates can compete in
a pairwise fashion. It can be explained by the
fact that in this approach (called twin-candidate
model), more information is available for the de-
cision making. If we proceed further along this
direction, we come to the ranking approach de-
scribed in Denis and Baldridge (2007), in which
the entire candidate set is considered at once and
?The work on this project was supported by the
grants MSM 0021620838, GAAV C?R 1ET101120503 and
1ET201120505, MS?MT C?R LC536, and GAUK 4383/2009
which leads to further significant shift in perfor-
mance, more recently documented in Denis and
Baldridge (2008).
In this paper we deal with supervised ap-
proaches to pronominal anaphora in Czech.1 For
training and evaluation purposes, we use corefer-
ences links annotated in the Prague Dependency
Treebank, (Jan Hajic?, et al, 2006). We limit our-
selves only to textual coreference (see Section 2)
and to personal and possessive pronouns. We
make use of a rich set of features available thanks
to the complex annotation scenario of the tree-
bank.
We experiment with two of the above men-
tioned techniques for AR: a classifier and a ranker.
The former is based on a top-down induction of
decision trees (Quinlan, 1993). The latter uses
a simple scoring function whose optimal weight
vector is estimated using perceptron learning in-
spired by Collins (2002). We try to provide both
implementations with as similar input information
as possible in order to be able to compare their
performance for the given task.
Performance of the presented systems can be
compared with several already published works,
namely with a rule-based system described in
Kuc?ova? and Z?abokrtsky? (2005), some of the ?clas-
sical? algorithms implemented in Ne?mc???k (2006),
a system based on decision trees (Ngu. y, 2006),
and a rule-based system evaluated in Ngu. y and
Z?abokrtsky? (2007). To illustrate the real complex-
ity of the task, we also provide performance eval-
uation of a baseline solution.
1Currently one can see a growing interest in unsupervised
techniques, e.g. Charniak and Elsner (2009) and Ng (2008).
However, we make only a very tiny step in this direction:
we use a probabilistic feature based on collocation counts in
large unannotated data (namely in the Czech National Cor-
pus).
276
The most important result claimed in this pa-
per is that, to the best of our knowledge, the pre-
sented ranker system outperforms all the previ-
ously published systems evaluated on the PDT
data. Moreover, the performance of our ranker (f-
score 79.43%) for Czech data is not far from the
performance of the state-of-the-art system for En-
glish described in Denis and Baldridge (2008) (f-
score for 3rd person pronouns 82.2 %).2
A side product of this work lies in bringing
empirical evidence ? for a different language and
different data set ? for the claim of Denis and
Baldridge (2007) that the ranking approach is
more appropriate for the task of AR than the clas-
sification approach.
The paper is structured as follows. The data
with manually annotated links we use are de-
scribed in Section 2. Section 3 outlines prepro-
cessing the data for training and evaluation pur-
poses. The classifier-based and ranker-based sys-
tems are described in Section 4 and Section 5 re-
spectively. Section 6 summarizes the achieved re-
sults by evaluating both approaches using the test
data. Conclusions and final remarks follow in Sec-
tion 7.
2 Coreference links in the Prague
Dependency Treebank 2.0
The Prague Dependency Treebank 2.03 (PDT 2.0,
Jan Hajic?, et al (2006)) is a large collection of
linguistically annotated data and documentation,
based on the theoretical framework of Functional
Generative Description (FGD; introduced by Sgall
(1967) and later elaborated, e.g. in by Sgall et al
(1986)). The PDT 2.0 data are Czech newspaper
texts selected from the Czech National Corpus4
(CNC).
The PDT 2.0 has a three-level structure. On the
lowest morphological level, a lemma and a posi-
tional morphological tag are added to each token.
The middle analytical level represents each sen-
tence as a surface-syntactic dependency tree. On
the highest tectogrammatical level, each sentence
is represented as a complex deep-syntactic depen-
2However, it should be noted that exact comparison is not
possible here, since the tasks are slightly different for the
two languages, especially because of typological differences
between Czech and English (frequent pro-drop in Czech)
and different information available in the underlying data re-
source on the other hand (manually annotated morphological
and syntactical information available for Czech).
3http://ufal.mff.cuni.cz/pdt2.0/
4http://ucnk.ff.cuni.cz/
dency tree, see Mikulova? and others (2005) for de-
tails. This level includes also annotation of coref-
erential links.
The PDT 2.0 contains 3,168 newspaper texts
(49,431 sentences) annotated on the tectogram-
matical level. Coreference has been annotated
manually in all this data. Following the FGD,
there are two types of coreference distinguished:
grammatical coreference and textual coreference
(Panevova?, 1991). The main difference between
the two coreference types is that the antecedent in
grammatical coreference can be identified using
grammatical rules and sentence syntactic struc-
ture, whereas the antecedent in textual coreference
can not.
The further division of grammatical and textual
coreference is based on types of anaphors:
Grammatical anaphors: relative pronouns, re-
flexive pronouns, reciprocity pronouns, re-
stored (surface-unexpressed) ?subjects? of
infinitive verbs below verbs of control,
Textual anaphors: personal and possessive pro-
nouns, demonstrative pronouns.
The data in the PDT 2.0 are divided into three
groups: training set (80%), development test set
(10%), and evaluation test set (10%). The training
and development test set can be freely exploited,
while the evaluation test data should serve only for
the very final evaluation of developed tools.
Table 1 shows the distribution of each anaphor
type. The total number of coreference links in the
PDT 2.0 data is 45,174.5 Personal pronouns in-
cluding those zero ones and possessive pronouns
form 37.4% of all anaphors in the entire corpus
(16,888 links).
An example tectogrammatical tree with de-
picted coreference links (arrows) is presented in
Figure 1. For the sake of simplicity, only three
node attributes are displayed below the nodes: tec-
togrammatical lemma, functor, and semantic part
of speech (tectogrammatical nodes themselves are
complex data structures and around twenty at-
tributes might be stored with them).
Tectogrammatical lemma is a canonical word
form or an artificial value of a newly created node
5In terms of the number of coreference links, PDT 2.0
is one of the largest existing manually annotated resources.
Another comparably large resource is BBN Pronoun Coref-
erence and Entity Type Corpus (Weischedel and Brunstein,
2005), which contains a stand-off annotation of coreference
links in the Penn Treebank texts.
277
Type/Count train dtest etest
Personal pron. 12,913 1,945 2,030
Relative pron. 6,957 948 1,034
Under-control pron. 6,598 874 907
Reflexive pron. 3,381 452 571
Demonstrative pron. 2,582 332 344
Reciprocity pron. 882 110 122
Other 320 35 42
Total 34,983 4,909 5,282
Table 1: Distribution of the different anaphor
types in the PDT 2.0.
on the tectogrammatical level. E.g. the (artifi-
cial) tectogrammatical lemma #PersPron stands
for personal (and possessive) pronouns, be they
expressed on the surface (i.e., present in the orig-
inal sentence) or restored during the annotation
of the tectogrammatical tree structure (zero pro-
nouns).
Functor captures the deep-syntactic dependency
relation between a node and its governor in the
tectogrammatical tree. According to FGD, func-
tors are divided into actants (ACT ? actor, PAT ?
patient, ADDR ? addressee, etc.) and free modi-
fiers (LOC ? location, BEN ? benefactor, RHEM
? rhematizer, TWHEN ? temporal modifier, APP
? appurtenance, etc.).
Semantic parts of speech correspond to ba-
sic onomasiological categories (substance, fea-
ture, factor, event). The main semantic POS dis-
tinguished in PDT 2.0 are: semantic nouns, se-
mantic adjectives, semantic adverbs and semantic
verbs (for example, personal and possessive pro-
nouns belong to semantic nouns).
3 Training data preparation
The training phase of both presented AR systems
can be outlined as follows:
1. detect nodes which are anaphors (Sec-
tion 3.1),
2. for each anaphor ai, collect the set of an-
tecedent candidates Cand(ai) (Section 3.2),
3. for each anaphor ai, divide the set of
candidates into positive instances (true an-
tecedents) and negative instances (Sec-
tion 3.3),
4. for each pair of an anaphor ai and an an-
tecedent candidate cj ? Cand(ai), compute
the feature vector ?(c, ai) (Section 3.4),
5. given the anaphors, their sets of antecedent
candidates (with related feature vectors), and
the division into positive and negative candi-
dates, train the system for identifying the true
antecedents among the candidates.
Steps 1-4 can be seen as training data prepro-
cessing, and are very similar for both systems.
System-specific details are described in Section 4
and Section 5 respectively.
3.1 Anaphor selection
In the presented work, only third person per-
sonal (and possessive) pronouns are considered,6
be they expressed on the surface or reconstructed.
We treat as anaphors all tectogrammatical nodes
with lemma #PersPron and third person stored in
the gram/person grammateme. More than 98 %
of such nodes have their antecedents (in the sense
of textual coreference) marked in the training data.
Therefore we decided to rely only on this highly
precise rule when detecting anaphors.7
In our example tree, the node #PersPron rep-
resenting his on the surface and the node #Per-
sPron representing the zero personal pronoun he
will be recognized as anaphors.
3.2 Candidate selection
In both systems, the predicted antecedent of a
given anaphor ai is selected from an easy-to-
compute set of antecedent candidates denoted as
Cand(ai). We limit the set of candidates to se-
mantic nouns which are located either in the same
sentence before the anaphor, or in the preced-
ing sentence. Table 2 shows that if we disregard
cataphoric and longer anaphoric links, we loose
a chance for correct answer with only 6 % of
anaphors.
6The reason is that antecedents of most other types of
anaphors annotated in PDT 2.0 can be detected ? given
the tree topology and basic node attributes ? with precision
higher than 90 %, as it was shown already in Kuc?ova? and
Z?abokrtsky? (2005). For instance, antecedents of reflexive
pronouns are tree-nearest clause subjects in most cases, while
antecedents of relative pronouns are typically parents of the
relative clause heads.
7It is not surprising that no discourse status model (as used
e.g. in Denis and Baldridge (2008)) is practically needed
here, since we limit ourselves to personal pronouns, which
are almost always ?discourse-old?.
278
Antecedent location Percnt.
Previous sentence 37 %
Same sentence, preceding the anaphor 57 %
Same sentence, following the anaphor 5 %
Other 1 %
Table 2: Location of antecedents with respect to
anaphors in the training section of PDT 2.0.
3.3 Generating positive and negative
instances
If the true antecedent of ai is not present in
Cand(ai), no training instance is generated. If it is
present, the sets of negative and positive instances
are generated based on the anaphor. This prepro-
cessing step differs for the two systems, because
the classifier can be easily provided with more
than one positive instance per anaphor, whereas
the ranker can not.
In the classification-based system, all candi-
dates belonging to the coreferential chain are
marked as positive instances in the training data.
The remaining candidates are marked as negative
instances.
In the ranking-based system, the coreferential
chain is followed from the anaphor to the nearest
antecedent which itself is not an anaphor in gram-
matical coreference.8 The first such node is put on
the top of the training rank list, as it should be pre-
dicted as the winner (E.g., the nearest antecedent
of the zero personal pronoun he in the example
tree is the relative pronoun who, however, it is a
grammatical anaphor, so its antecedent Brien is
chosen as the winner instead). All remaining (neg-
ative) candidates are added to the list, without any
special ordering.
3.4 Feature extraction
Our model makes use of a wide range of features
that are obtained not only from all three levels of
the PDT 2.0 but also from the Czech National Cor-
pus and the EuroWordNet. Each training or test-
ing instance is represented by a feature vector. The
features describe the anaphor, its antecedent can-
didate and their relationship, as well as their con-
8Grammatical anaphors are skipped because they usually
do not provide sufficient information (e.g., reflexive pronouns
provide almost no cues at all). The classification approach
does not require such adaptation ? it is more robust against
such lack of information as it treats the whole chain as posi-
tive instances.
texts. All features are listed in Table 4 in the Ap-
pendix.
When designing the feature set on personal pro-
nouns, we take into account the fact that Czech
personal pronouns stand for persons, animals and
things, therefore they agree with their antecedents
in many attributes and functions. Further we use
the knowledge from the Lappin and Leass?s al-
gorithm (Lappin and Leass, 1994), the Mitkov?s
robust, knowledge-poor approach (Mitkov, 2002),
and the theory of topic-focus articulation (Kuc?ova?
et al, 2005). We want to take utmost advantage of
information from the antecedent?s and anaphor?s
node on all three levels as well.
Distance: Numeric features capturing the dis-
tance between the anaphor and the candidate, mea-
sured by the number of sentences, clauses, tree
nodes and candidates between them.
Morphological agreement: Categorial features
created from the values of tectogrammatical gen-
der and number9 and from selected morphological
categories from the positional tag10 of the anaphor
and of the candidate. In addition, there are features
indicating the strict agreement between these pairs
and features formed by concatenating the pair of
values of the given attribute in the two nodes (e.g.,
masc neut).
Agreement in dependency functions: Catego-
rial features created from the values of tec-
togrammatical functor and analytical functor (with
surface-syntactic values such as Sb, Pred, Obj) of
the anaphor and of the candidate, their agreement
and joint feature. There are two more features in-
dicating whether the candidate/anaphor is an ac-
tant and whether the candidate/anaphor is a sub-
ject on the tectogrammatical level.11
Context: Categorial features describing the con-
text of the anaphor and of the candidate:
? parent ? tectogrammatical functor and the se-
mantic POS of the effective parent12 of the
9Sometimes gender and number are unknown, but we can
identify the gender and number of e.g. relative or reflexive
pronouns on the tectogrammatical level thanks to their an-
tecedent.
10A positional tag from the morphological level is a string
of 15 characters. Every positions encodes one morphological
category using one character.
11A subject on the tectogrammatical level can be a node
with the analytical functor Sb or with the tectogrammatical
functor Actor in a clause without a subject.
12The ?true governor? in terms of dependency relations.
279
anaphor and the candidate, their agreement
and joint feature; a feature indicating the
agreement of both parents? tectogrammatical
lemma and their joint feature; a joint feature
of the pair of the tectogrammatical lemma
of the candidate and the effective parent?s
lemma of the anaphor; and a feature indicat-
ing whether the candidate and the anaphor are
siblings.13
? coordination ? a feature that indicates
whether the candidate is a member of a coor-
dination and a feature indicating whether the
anaphor is a possessive pronoun and is in the
coordination with the candidate
? collocation ? a feature indicating whether the
candidate has appeared in the same colloca-
tion as the anaphor within the text14 and a
feature that indicates the collocation assumed
from the Czech National Corpus.15
? boundness ? features assigned on the ba-
sis of contextual boundness (available in the
tectogrammatical trees) {contextually bound,
contrastively contextually bound, or contex-
tually non-bound}16 for the anaphor and the
candidate; their agreement and joint feature.
? frequency ? 1 if the candidate is a denotative
semantic noun and occurs more than once
within the text; otherwise 0.
Semantics: Semantically oriented feature that
indicates whether the candidate is a person name
for the present and a set of 63 binary ontologi-
cal attributes obtained from the EuroWordNet.17
These attributes determine the positive or negative
13Both have the same effective parent.
14If the anaphor?s effective parent is a verb and the can-
didate is a denotative semantic noun and has appeared as a
child of the same verb and has had the same functor as the
anaphor.
15The probability of the candidate being a subject preced-
ing the verb, which is the effective parent of the anaphor.
16Contextual boundness is a property of an expression (be
it expressed or absent in the surface structure of the sentence)
which determines whether the speaker (author) uses the ex-
pression as given (for the recipient), i.e. uniquely determined
by the context.
17The Top Ontology used in EuroWordNet (EWN) con-
tains the (structured) set of 63 basic semantic concepts like
Place, Time, Human, Group, Living, etc. For the majority of
English synsets (set of synonyms, the basic unit of EWN), the
appropriate subset of these concepts are listed. Using the In-
ter Lingual Index that links the synsets of different languages,
the set of relevant concepts can be found also for Czech lem-
mas.
relation between the candidate?s lemma and the se-
mantic concepts.
4 Classifier-based system
Our classification approach uses C5.0, a succes-
sor of C4.5 (Quinlan, 1993), which is probably the
most widely used program for inducing decision
trees. Decision trees are used in many AR sys-
tems such as Aone and Bennett (1995), Mccarthy
and Lehnert (1995), Soon et al (2001), and Ng
and Cardie (2002).18
Our classifier-based system takes as input a set
of feature vectors as described in Section 3.4 and
their classifications (1 ? true antecedent, 0 ? non-
antecedent) and produces a decision tree that is
further used for classifying new pairs of candidate
and anaphor.
The classifier antecedent selection algorithm
works as follows. For each anaphor ai, feature
vectors ?(c, ai) are computed for all candidates
c ? Cand(ai) and passed to the trained decision
tree. The candidate classified as positive is re-
turned as the predicted antecedent. If there are
more candidates classified as positive, the nearest
one is chosen.
If no candidate is classified as positive, a sys-
tem of handwritten fallback rules can be used. The
fallback rules are the same rules as those used in
the baseline system in Section 6.2.
5 Ranker-based system
In the ranker-based AR system, every training ex-
ample is a pair (ai, yi), where ai is the anaphoric
expression and yi is the true antecedent. Using
the candidate extraction function Cand, we aim
to rank the candidates so that the true antecedent
would always be the first candidate on the list. The
ranking is modeled by a linear model of the fea-
tures described in Section 3.4. According to the
model, the antecedent y?i for an anaphoric expres-
sion ai is found as:
y?i = argmax
c?Cand(ai)
?(c, ai) ?
??w
The weights ??w of the linear model are trained
using a modification of the averaged perceptron al-
18Besides C5.0, we plan to use also other classifiers in the
future (especially Support Vector Machine, which is often
employed in AR experiments, e.g. by Ng (2005) and Yang
et al (2006)) in order to study how the classifier choice in-
fluences the AR system performance on our data and feature
sets.
280
gorithm (Collins, 2002). This is averaged percep-
tron learning with a modified loss function adapted
to the ranking scenario. The loss function is tai-
lored to the task of correctly ranking the true an-
tecedent, the ranking of other candidates is irrel-
evant. The algorithm (without averaging the pa-
rameters) is listed as Algorithm 1. Note that the
training instances where yi /? Cand(ai) were ex-
cluded from the training.
input : N training examples (ai, yi),
number of iterations T
init : ??w ?
??
0 ;
for t? 1 to T , i? 1 to N do
y?i ? argmaxc?Cand(ai) ?(c, ai) ?
??w ;
if y?i 6= yi then
??w = ??w + ?(yi, ai)? ?(y?i, ai);
end
end
output: weights ??w
Algorithm 1: Modified perceptron algorithm
for ranking. ? is the feature extraction func-
tion, ai is the anaphoric expression, yi is the
true antecedent.
Antecedent selection algorithm using a ranker:
For each third person pronoun create a feature vec-
tor from the pronoun and the semantic noun pre-
ceding the pronoun and is in the same sentence or
in the previous sentence. Use the trained ranking
features weight model to get out the candidate?s
total weight. The candidate with the highest fea-
tures weight is identified as the antecedent.
6 Experiments and evaluation
6.1 Evaluation metrics
For the evaluation we use the standard metrics:19
Precision = number of correctly predicted anaphoric third person pronounsnumber of all predicted third person pronouns
Recall = number of correctly predicted anaphoric third person pronounsnumber of all anaphoric third person pronouns
F-measure = 2?Precision?RecallPrecision+Recall
We consider an anaphoric third person pronoun
to be correctly predicted when we can success-
19Using simple accuracy would not be adequate, as there
can be no link (or more than one) leading from an anaphor
in the annotated data. In other words, finding whether a pro-
noun has an antecedent or not is a part of the task. A deeper
discussion about coreference resolution metrics can be found
in Luo (2005).
fully indicate its antecedent, which can be any an-
tecedent from the same coreferential chain as the
anaphor.
Both the AR systems were developed and tested
on PDT 2.0 training and development test data. Fi-
nally they were tested on evaluation test data for
the final scoring, summarized in Section 6.3.
6.2 Baseline system
We have made some baseline rules for the task of
AR and tested them on the PDT 2.0 evaluation test
data. Their results are reported in Table 3. Base-
line rules are following: For each third person pro-
noun, consider all semantic nouns which precede
the pronoun and are not further than the previous
sentence, and:
? select the nearest one as its antecedent
(BASE 1),
? select the nearest one which is a clause sub-
ject (BASE 2),
? select the nearest one which agrees in gender
and number (BASE 3),
? select the nearest one which agrees in gen-
der and number; if there is no such noun,
choose the nearest clause subject; if no clause
subject was found, choose the nearest noun
(BASE 3+2+1).
6.3 Experimental results and discussion
Scores for all three systems (baseline, clasifier
with and without fallback, ranker) are given in Ta-
ble 3. Our baseline system based on the combina-
tion of three rules (BASE 3+2+1) reports results
superior to the ones of the rule-based system de-
scribed in Kuc?ova? and Z?abokrtsky? (2005). Kuc?ova?
and Z?abokrtsky? proposed a set of filters for per-
sonal pronominal anaphora resolution. The list of
candidates was built from the preceding and the
same sentence as the personal pronoun. After ap-
plying each filter, improbable candidates were cut
off. If there was more than one candidate left at
the end, the nearest one to the anaphor was cho-
sen as its antecedent. The reported final success
rate was 60.4 % (counted simply as the number of
correctly predicted links divided by the number of
pronoun anaphors in the test data section).
An interesting point of the classifier-based sys-
tem lies in the comparison with the rule-based
281
Rule P R F
BASE 1 17.82% 18.00% 17.90%
BASE 2 41.69% 42.06% 41.88%
BASE 3 59.00% 59.50% 59.24%
BASE 3+2+1 62.55% 63.03% 62.79%
CLASS 69.9% 70.44% 70.17%
CLASS+3+2+1 76.02% 76.60% 76.30%
RANK 79.13% 79.74% 79.43%
Table 3: Precision (P), Recall (R) and F-measure
(F) results for the presented AR systems.
system of Ngu. y and Z?abokrtsky? (2007). With-
out the rule-based fallback (CLASS), the clas-
sifier falls behind the Ngu. y and Z?abokrtsky??s
system (74.2%), while including the fallback
(CLASS+3+2+1) it gives better results.
Overall, the ranker-based system (RANK) sig-
nificantly outperforms all other AR systems for
Czech with the f-score of 79.43%. Comparing
with the model for third person pronouns of Denis
and Baldridge (2008), which reports the f-score of
82.2%, our ranker is not so far behind. It is im-
portant to say that our system relies on manually
annotated information20 and we solve the task of
anaphora resolution for third person pronouns on
the tectogrammatical level of the PDT 2.0. That
means these pronouns are not only those expressed
on the surface, but also artificially added (recon-
structed) into the structure according to the princi-
ples of FGD.
7 Conclusions
In this paper we report two systems for AR in
Czech: the classifier-based system and the ranker-
based system. The latter system reaches f-score
79.43% on the Prague Dependency Treebank test
data and significantly outperforms all previously
published results. Our results support the hypoth-
esis that ranking approaches are more appropriate
for the AR task than classification approaches.
References
Chinatsu Aone and Scott William Bennett. 1995.
Evaluating automated and manual acquisition of
20In the near future, we plan to re-run the experiments us-
ing sentence analyses created by automatic tools (all needed
tools are available in the TectoMT software framework
(Z?abokrtsky? et al, 2008)) instead of manually created analy-
ses, in order to examine the sensitivity of the AR system to
annotation quality.
anaphora resolution strategies. In Proceedings of the
33rd annual meeting on Association for Computa-
tional Linguistics, pages 122?129, Morristown, NJ,
USA. Association for Computational Linguistics.
Anto?nio Branco, Tony McEnery, Ruslan Mitkov, and
Fa?tima Silva, editors. 2007. Proceedings of the 6th
Discourse Anaphora and Anaphor Resolution Col-
loquium (DAARC 2007), Lagos (Algarve), Portugal.
CLUP-Center for Linguistics of the University of
Oporto.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 148?156, Athens, Greece,
March. Association for Computational Linguistics.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of EMNLP, volume 10, pages 1?8.
Pascal Denis and Jason Baldridge. 2007. A ranking
approach to pronoun resolution. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI2007), pages 1588?1593, Hyder-
abad, India, January 6?12.
Pascal Denis and Jason Baldridge. 2008. Special-
ized models and ranking for coreference resolu-
tion. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP2008), pages 660?669, Honolulu, Hawaii,
USA, October 25?27.
Jan Hajic?, et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Lucie Kuc?ova? and Zdene?k Z?abokrtsky?. 2005.
Anaphora in Czech: Large Data and Experiments
with Automatic Anaphora. LNCS/Lecture Notes in
Artificial Intelligence/Proceedings of Text, Speech
and Dialogue, 3658:93?98.
Lucie Kuc?ova?, Kater?ina Vesela?, Eva Hajic?ova?, and
Jir??? Havelka. 2005. Topic-focus articulation and
anaphoric relations: A corpus based probe. In Klaus
Heusinger and Carla Umbach, editors, Proceedings
of Discourse Domains and Information Structure
workshop, pages 37?46, Edinburgh, Scotland, UK,
Aug. 8-12.
Shalom Lappin and Herbert J. Leass. 1994. ?an algo-
rithm for pronominal anaphora resolution?. Compu-
tational Linguistics, 20(4):535?561.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 25?32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
282
J Mccarthy and Wendy G. Lehnert. 1995. Using de-
cision trees for coreference resolution. In In Pro-
ceedings of the Fourteenth International Joint Con-
ference on Artificial Intelligence, pages 1050?1055.
Marie Mikulova? et al 2005. Anotace na tektogra-
maticke? rovine? Praz?ske?ho za?vislostn??ho korpusu.
Anota?torska? pr???ruc?ka (t-layer annotation guide-
lines). Technical Report TR-2005-28, U?FAL MFF
UK, Prague, Prague.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
Va?clav Ne?mc???k. 2006. Anaphora Resolution. Mas-
ter?s thesis, Faculty of Informatics, Masaryk Univer-
sity.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104?111, Morristown, NJ, USA. Association
for Computational Linguistics.
Vincent Ng. 2005. Supervised ranking for pro-
noun resolution: Some recent improvements. In
Manuela M. Veloso and Subbarao Kambhampati,
editors, AAAI, pages 1081?1086. AAAI Press / The
MIT Press.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2008), pages 640?649, Hon-
olulu, Hawaii, USA.
Giang Linh Ngu. y and Zdene?k Z?abokrtsky?. 2007.
Rule-based approach to pronominal anaphora reso-
lution applied on the prague dependency treebank
2.0 data. In Branco et al (Branco et al, 2007), pages
77?81.
Giang Linh Ngu. y. 2006. Proposal of a Set of Rules
for Anaphora Resolution in Czech. Master?s thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity.
Jarmila Panevova?. 1991. Koreference gramaticka? nebo
textova?? In Etudes de linguistique romane et slave.
Krakow.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing Company,
Dordrecht.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague, Czech Republic.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Comput. Linguist., 27(4):521?544.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL.
Ralph Weischedel and Ada Brunstein. 2005. BBN
Pronoun Coreference and Entity Type Corpus. CD-
ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2005T33, Philadelphia.
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competition learning approach. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 176?
183, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL2006), pages 41?48, Sydney, Australia, July
17?21.
283
A Appendix
t-ln95049-047-p3s1
root
O - O
RSTR
n.denot
Brien - BRIEN
ACT
n.denot
kter? - WHO
ACT
n.pron.indef
Louganis - LOUGANIS
PAT
n.denot
tr novat - TO TRAIN
RSTR
v
rok - YEAR
THL
n.denot
deset - TEN
RSTR
adj.quant.def
#PersPron - HIS
ACT
n.pron.def.pers
onemocn?n - INJURY
PAT
n.denot.neg
vdt - TO KNOW
PRED
v
ale - BUT enunc
ADVS
coap
zavzat_se - TO TIE SOMEONE'S SELF
PRED
v
#PersPron - (HE)
ACT
n.pron.def.pers
ml?en - SECRECY
PAT
n.denot.neg
.
Figure 1: Simplified tectogrammatical tree representing the sentence O?Brien, ktery? Louganise tre?noval
deset let, o jeho onemocne?n?? ve?de?l, ale zava?zal se mlc?en??m. (Lit.: O?Brien, who Louganis trained for
ten years, about his injury knew, but (he) tied himself to secrecy.) Note two coreferential chains {Brien,
who, (he)} and {Louganis, his}.
284
Distance
sent dist sentence distance between c and ai
clause dist clause distance between c and ai
node dist tree node distance between c and ai
cand ord mention distance between c and ai
Morphological Agreement
gender t-gender of c and ai, agreement, joint
number t-number of c and ai, agreement, joint
apos m-POS of c and ai, agreement, joint
asubpos detailed POS of c and ai, agreement, joint
agen m-gender of c and ai, agreement, joint
anum m-number of c and ai, agreement, joint
acase m-case of c and ai, agreement, joint
apossgen m-possessor?s gender of c and ai, agreement, joint
apossnum m-possessor?s number of c and ai, agreement, joint
apers m-person of c and ai, agreement, joint
Functional Agreement
afun a-functor of c and ai, agreement, joint
fun t-functor of c and ai, agreement, joint
act c/ai is an actant, agreement
subj c/ai is a subject, agreement
Context
par fun t-functor of the parent of c and ai, agreement, joint
par pos t-POS of the parent of c and ai, agreement, joint
par lemma agreement between the parent?s lemma of c and ai, joint
clem aparlem joint between the lemma of c and the parent?s lemma of ai
c coord c is a member of a coordination
app coord c and ai are in coordination & ai is a possessive pronoun
sibl c and ai are siblings
coll c and ai have the same collocation
cnk coll c and ai have the same CNC collocation
tfa contextual boundness of c and ai, agreement, joint
c freq c is a frequent word
Semantics
cand pers c is a person name
cand ewn semantic position of c?s lemma within the EuroWordNet Top Ontology
Table 4: Features used by the perceptron-based model
285
Coling 2010: Demonstration Volume, pages 9?12,
Beijing, August 2010
Annotation Tool for Discourse in PDT
Ji?? M?rovsk?, Lucie Mladov?, Zden?k ?abokrtsk?
Charles University in Prague
Institute of Formal and applied Linguistics
{mirovsky,mladova,zabokrtsky}@ufal.mff.cuni.cz
Abstract
We present a tool for annotation of se?
mantic  inter?sentential  discourse  rela?
tions  on  the  tectogrammatical  layer  of 
the  Prague  Dependency  Treebank 
(PDT).  We present  the way of helping 
the annotators by several useful features 
implemented in the annotation tool, such 
as a possibility to combine surface and 
deep  syntactic  representation  of  sen?
tences during the annotation, a possibili?
ty  to  define,  display and connect  arbi?
trary  groups  of  nodes,  a  clause?based 
compact  depiction  of  trees,  etc.  For 
studying differences among parallel an?
notations, the tool offers a simultaneous 
depiction of parallel  annotations of the 
data.
1 Introduction
The  Prague  Dependency  Treebank  2.0  (PDT 
2.0; Haji? et al, 2006) is a manually annotated 
corpus of Czech. It belongs to the most complex 
end elaborate linguistically annotated treebanks 
in the world. The texts  are annotated on three 
layers  of  language description:  morphological, 
analytical (which expresses the surface syntactic 
structure),  and  tectogrammatical  (which  ex?
presses the deep syntactic structure). On the tec?
togrammatical layer, the data consist of almost 
50 thousand sentences.
For the future release of PDT, many addition?
al  features  are  planned,  coming  as  results  of 
several  projects.  Annotation  of  semantic  in?
ter?sentential  discourse  relations  (Mladov?  et 
al., 2009)1 is one of the planned additions. The 
1 It is performed in the project From the structure of a sen?
tence to textual relations (GA405/09/0729), as one of sev?
goal is not only to annotate the data, but also to 
compare the representation of these relations in 
the Prague Dependency Treebank with the an?
notation done at the Penn Treebank, which was 
carried  out  at  University  of  Pennsylvania 
(Prasad et al, 2008).
Manual  annotation  of  data  is  an  expensive 
and time consuming task. A sophisticated anno?
tation  tool  can  substantially  increase  the  effi?
ciency of the annotations and ensure a higher in?
ter?annotator agreement. We present such a tool.
2 Tree  Editor  TrEd  and  the  Annota?
tion Extension
The primary format of PDT 2.0 is called PML. 
It is an abstract XML?based format designed for 
annotation of linguistic corpora, and especially 
treebanks.  Data  in  the  PML  format  can  be 
browsed  and  edited  in  TrEd,  a  fully 
customizable  tree  editor  (Pajas  and  ?t?p?nek, 
2008).
TrEd is completely written in Perl and can be 
easily customized to a desired purpose by exten?
sions that are included into the system as mod?
ules.  In this paper,  we describe the main fea?
tures of an extension that has been implemented 
for  our  purposes.  The  data  scheme  used  in 
PDT 2.0 has been enriched too, to support the 
annotation of the discourse relations.
2.1 Features of the Annotation Tool
A tool for the annotation of discourse needs to 
offer several features:
? creation of a link between arguments of 
a relation
? exact specification of the arguments of 
the relation
eral tasks.
9
? assigning a connective to the relation
? adding additional information to the re?
lation (a type, a source, a comment etc.)
Links between arguments:  The annotation 
of discourse relations in PDT is performed on 
top of the tectogrammatical (deep syntactic) lay?
er of the treebank.  Similarly to another exten?
sion of TrEd, dedicated to the annotation of the 
textual  coreference  and the  bridging  anaphora 
(M?rovsk? et al, 2010), a discourse relation be?
tween nodes is  represented  by  a dedicated  at?
tribute  at the initial  node of the  relation,  con?
taining a unique identifier of the target node of 
the relation.2 Each relation has two arguments 
and is oriented ? one of the arguments is initial, 
the other one is a target of the link. The link is 
depicted as a curved arrow between the nodes, 
see Figure 1. Although the arrow connects the 
two nodes, it does not mean that the two nodes 
themselves equal the two arguments of the rela?
tion ? more about it later.
Figure 1. An arrow represents a link.
Additional  information  about  the  relation  is 
also  kept  at  the  initial  node  ?  there  is  an  at?
tribute for the type, an attribute for the source 
(who annotated it) and an attribute for a com?
ment.
Extent of the arguments:  Usually, an argu?
ment  of  a  discourse  relation  corresponds  to  a 
subtree  of a tectogrammatical  tree  and can be 
represented simply by the root node of the sub?
tree.  However,  there  are  exceptions  to  this 
2 The data representation allows for several discourse links 
starting at a single node ? there is a list of structured dis?
course elements representing the individual relations.
?rule?. Sometimes it is necessary to exclude a 
part of the subtree of a node from the argument, 
sometimes the argument consists of more than 
one tree and sometimes it is even impossible to 
set exactly the borders of the argument. To al?
low for  all  these  variants,  each discourse  link 
has two additional attributes specifying range of 
the initial/target argument (both are stored at the 
initial node of the link). The possible values are:
? ?0? (zero) ? the argument corresponds 
to the subtree of the node
? N (a  positive  integer)  ?  the  argument 
consists of the subtree of the node and of 
N subsequent (whole) trees
? ?group? ? the argument consists of an 
arbitrary set of nodes (details below); this 
should only be used if the previous op?
tions are not applicable
? ?forward?  ?  the  argument  consists  of 
the subtree of the node and an unspeci?
fied number of subsequent trees; should 
only be used if more specific options are 
not applicable
? ?backward?  ?  similarly,  the  argument 
consists of the subtree of the node and an 
unspecified  number  of  preceding  trees; 
should only be used if more specific op?
tions are not applicable
Groups: An argument of a discourse relation 
can consist of an arbitrary group of nodes, even 
from  several  trees.  The  fact  is  indicated  in  a 
range  attribute  of  the  relation  (by  value 
?group?).  Another  attribute  then  tells  which 
group it  is.  Groups of nodes inside one docu?
ment are identified  by numbers  (positive inte?
gers).  Each node can be a member  of several 
groups; a list of identifiers of groups a node be?
longs to is kept at the node. Every group has a 
representative  node  ?  if  a  discourse  link 
starts/ends at a group, graphically it starts/ends 
at the representative node of the group, which is 
the depth?first  node of the group belonging to 
the leftmost tree of the group. Figure 2 shows an 
example of a group. In the example,  the right 
son (along with its subtree) of the target node of 
the relation has been excluded from the target 
argument of the relation (by specifying the tar?
get group of nodes, which is graphically high?
lighted). The right son (and its subtree) is actu?
ally the initial argument of the relation.
10
Figure 2. A group of nodes.
Connectives: A connective of a discourse re?
lation  is  represented  as  a  list  of  identifiers  of 
(usually)  tectogrammatical  nodes  that  corre?
spond to the surface tokens of the connective; 
the list is kept at the initial node of the relation. 
It is often only one node, sometimes it consists 
of several nodes. However, some tokens (like a 
colon  ?  ?:?)  are  not  represented  on  the  tec?
togrammatical  layer  (at  least  not  as  a  node). 
Therefore, identifiers of nodes from the analyti?
cal layer are allowed as well.
Collapsed trees: To be able to display more 
information using less space, a collapsed mode 
of depicting trees has been implemented.
Figure 3. A collapsed mode of depicting trees.
 A simple algorithm based on the tectogram?
matical  annotation  has  been  employed  to  col?
lapse  each  subtree  representing  an  individual 
clause of the sentence into one node. Figure 3 
shows an example of two collapsed trees.
Discourse  relations  most  often  start/end  at 
nodes representing roots of the clauses. In those 
rare  cases  when  the  discourse  relation  should 
lead inside a clause, the annotators can un?col?
lapse  the  trees,  create  the  link,  and  collapse 
back. Such a link would then be depicted with a 
dotted arrow.
Other  features:  The  tool  also  incorporates 
some other features that make the annotation of 
discourse relations easier. Based on their prefer?
ence,  the annotators  can annotate the relations 
either on the trees or on the linear form of the 
sentences in the text window of the tool. In the 
sentences,  the  tokens  that  represent  the 
initial/target nodes of the relations are highlight?
ed and easily visible.
2.2 Parallel Annotations
To study discrepancies in parallel annotations, a 
mode for depicting parallel annotations exists. It 
can display annotations of the same data from 
two or more annotators. Figure 4 shows parallel 
annotations from two annotators. In this exam?
ple, the two annotators (?JZ? and ?PJ?) agreed 
on the relation on the top of the figure, they also 
marked the same connective (?Pot??),  and se?
lected the same type of the relation (?preced(?
ence)?). They also agreed on the range of both 
the  arguments  (?0?,  i.e.  the  subtrees  of  the 
nodes). The other relation (on the left, below the 
first one) has only been recognized by one an?
notator (?JZ?).
Figure 4. Parallel annotations.
11
3 Conclusion
From the technical point of view, we have de?
scribed features of an annotation tool for seman?
tic  inter?sentential  discourse  relations  in  the 
Prague  Dependency  Treebank  2.0.  We  have 
shown how it (hopefully in a simple and intu?
itive manner) allows for quite complex configu?
rations  of  arguments,  and  offers  features  that 
make the annotation easier. A mode for study?
ing  parallel  annotations  has  also  been  imple?
mented.
Evaluation of such a tool designed for a high?
ly specific task is difficult, as the tool does not 
produce any direct results (apart from the anno?
tated data) and is highly adapted to our ? given 
the tectogrammatical trees ? quite unique needs. 
(The annotated data themselves, of course, can 
be (and have been,  see Zik?nov? et  al., 2010) 
evaluated in various ways.) Bird and Liberman 
(2001) listed some very general requirements on 
annotation tools for linguistic corpora, namely:
? generality, specificity, simplicity,
? searchability, browsability,
? maintainability and durability.
The first requirement applies both to the an?
notation tool and the annotation framework. As 
described e.g. in Mladov? et al (2009), the an?
notation framework that we use is based on the 
knowledge obtained from studying various oth?
er systems, especially the Penn Discourse Tree?
bank (Prasad et al, 2008), but naturally it has 
been  adjusted  to  specific  needs  of  the  Czech 
language and PDT. The inter?connection of our 
system with the tectogrammatical layer of PDT 
helps  in  some  annotation  decisions,  as  many 
ambiguities have already been solved in the tec?
togrammatical annotation.
The second requirement  ? searchability  and 
browsability  ?  is  very  easily  fulfilled  in  our 
framework.  A  very  powerful  extension  for 
searching in PML?formatted  data,  called PML 
Tree  Query,  is  available  in  TrEd  (Pajas  and 
?t?p?nek, 2009).
PML is  a  well  defined  formalism  that  has 
been  used  extensively  for  large  variations  of 
data  annotation.  It  can be processed automati?
cally using btred, a command?line tool for ap?
plying Perl scripts to PML data, as well as inter?
actively using TrEd. Therefore, we believe that 
our  annotation  framework  and  the  annotation 
tool fulfill also the third requirement.
Acknowledgments
We gratefully  acknowledge  support  from  the 
Czech  Ministry  of  Education  (grant  MSM?
0021620838),  and  the  Grant  Agency  of  the 
Czech  Republic  (grants  405/09/0729  and 
P406/2010/0875).
References
Bird S. and M. Liberman. 2001. A formal framework 
for linguistic annotation. Speech Communication 
33, pp. 23?60.
Haji?, J., Panevov?, J., Haji?ov?, E., Sgall, P., Pajas, 
P.,  ?t?p?nek,  J.,  Havelka,  J.,  Mikulov?,  M., 
?abokrtsk?,  Z.,  and  M.  ?ev??kov??Raz?mov?. 
2006.  Prague  Dependency  Treebank  2.0. CD?
ROM,  LDC2006T01,  Linguistic  Data  Consor?
tium, Philadelphia, USA.
Mladov?,  L.,  Zik?nov?,  ?.,  Bed?ichov?,  Z.,  and E. 
Haji?ov?. 2009.  Towards a Discourse Corpus of  
Czech. Proceedings of the fifth Corpus Linguistics 
Conference, Liverpool, UK.
M?rovsk?, J.,  Pajas,  P.,  and A.  Nedoluzhko.  2010. 
Annotation  Tool  for  Extended  Textual  Corefer?
ence  and  Bridging  Anaphora. Proceedings  of 
LREC 2010, European Language Resources As?
sociation, Valletta, Malta.
Pajas, P. and J. ?t?p?nek. 2008. Recent advances in  
a feature?rich framework for treebank annotation. 
Proceedings  of  Coling  2008.  Manchester,  pp. 
673?680.
Pajas, P. and J. ?t?p?nek. 2009. System for Querying  
Syntactically Annotated Corpora. Proceedings of 
the ACL?IJCNLP 2009 Software Demonstrations, 
Association  for  Computational  Linguistics,  Sun?
tec, Singapore, pp. 33?36.
Prasad R., Dinesh N., Lee A., Miltsakaki E., Robal?
do L., Joshi A., and B. Webber. 2008.  The Penn  
Discourse  Treebank  2.0. Proceedings  of  the  6th 
International Conference on Language Resources 
and Evaluation (LREC 2008), Marrakech.
Zik?nov?,  ?.,  Mladov?,  L.,  M?rovsk?,  J.,  and  P. 
J?nov?. 2010.  Typical Cases  of  Annotators'  Dis?
agreement  in  Discourse  Annotations  in  Prague  
Dependency  Treebank.  Proceedings  of  LREC 
2010, European Language Resources Association, 
Valletta, Malta.
12
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 297?307, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Reducibility in Unsupervised Dependency Parsing
David Marec?ek and Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, 11800 Prague, Czech Republic
{marecek,zabokrtsky}@ufal.mff.cuni.cz
Abstract
The possibility of deleting a word from a sen-
tence without violating its syntactic correct-
ness belongs to traditionally known manifes-
tations of syntactic dependency. We introduce
a novel unsupervised parsing approach that is
based on a new n-gram reducibility measure.
We perform experiments across 18 languages
available in CoNLL data and we show that
our approach achieves better accuracy for the
majority of the languages then previously re-
ported results.
1 Introduction
The true nature of the notion of dependency (af-
ter removing sedimentary deposits of rules imposed
only by more or less arbitrary conventions) remains
still somewhat vague and elusive. This holds in spite
of a seemingly strong background intuition and even
after a decade of formalized large-scale dependency-
based resources being available to the research com-
munity. It is undeniable that a huge progress has
been reached in the field of supervised dependency
parsing, especially due to the CoNLL shared task se-
ries. However, when it comes to unsupervised pars-
ing, there are surprisingly few clues we could rely
on.
As mentioned e.g. by Ku?bler et al2009), one of
the traditional linguistic criteria for recognizing de-
pendency relations (including their head-dependent
orientation) is that a head H of a construction C de-
termines the syntactic category of C and can often
replace C. Or, in words of Dependency Analysis by
Reduction (Lopatkova? et al2005), stepwise dele-
tion of dependent elements within a sentence pre-
serves its syntactic correctness. A similar idea of
dependency analysis by splitting a sentence into all
possible acceptable fragments is used by Gerdes and
Kahane (2011).
Of course, all the above works had to respond to
the notorious fact that there are many language phe-
nomena precluding the ideal (word by word) sen-
tence reducibility (e.g. in the case of prepositional
groups, or in the case of subjects in English finite
clauses). However, we disregard their solutions ten-
tatively and borrow only the very core of the re-
ducibility idea: if a word can be removed from a
sentence without damaging it, then it is likely to be
dependent on some other (still present) word.
As it is usual with dichotomies in natural lan-
guages, it seems more adequate to use a continuous
scale instead of the reducible-irreducible opposition.
That is why we introduce a simple reducibility mea-
sure based on n-gram corpus statistics. We employ
this reducibility measure as the main feature in our
unsupervised parsing procedure.
The procedure is based on a commonly used
Bayesian inference technique called Gibbs sampling
(Gilks et al1996). In our sampler, the more re-
ducible a given token is, the more likely it is to
be sampled as a dependant and not as a head. Af-
ter certain number of sampling iterations, for each
sentence a final dependency tree is created (one to-
ken per node, including punctuation) that maximizes
the product of edge probabilities gathered along the
sampling history.
Our approach allows to utilize information from
297
very large corpora. While the computationally de-
manding sampling procedure can be applied only
on limited data, the unrepeated precomputation of
statistics for reducibility estimates can easily exploit
much larger data.
We are not aware of any other published work
on unsupervised parsing employing reducibility or
a similar idea. Dominating approaches in unsuper-
vised parsing are typically based on repeated pat-
terns, and not on the possibility of a deletion inside
a pattern. It seems that the two views of depen-
dency (frequent co-occurrence of head-dependant
pair, versus reducibility of the dependant) are rather
complementary, so fruitful combinations can be
hopefully expected in future.
The remainder of this paper is structured as fol-
lows. Section 2 briefly outlines the state of the art in
unsupervised dependency parsing. Our measure of
reducibility based on a large monolingual corpus is
presented in Section 3. Section 4 shows our models
which serve for generating probability estimates for
edge sampling described in Section 5. Experimen-
tal parsing results for languages included in CoNLL
shared task treebanks are summarized in Section 6.
Section 7 concludes this article.
2 Related Work
The most popular approach in unsupervised de-
pendency parsing of the recent years is to employ
Dependency Model with Valence (DMV), which
was introduced by Klein and Manning (2004).
The inference algorithm was further improved by
Smith (2007) and Cohen et al2008). Headden,
Johnson and McClosky (2009) introduced the Ex-
tended Valence Grammar (EVG) and added lexical-
ization and smoothing. Blunsom and Cohn (2010)
use tree substitution grammars, which allow learn-
ing larger dependency fragments.
Unfortunately, many of these works show results
only for English.1 However, the main feature of
unsupervised methods should be their applicabil-
ity across a wide range of languages. Such exper-
iments were done by Spitkovsky (2011b; 2011c),
where the parsing algorithm was evaluated on all 19
languages included in CoNLL 2006 (Buchholz and
1The state-of-the-art unsupervised parsers achieve more
than 50% of attachment score measured on the Penn Treebank.
Marsi, 2006) and 2007 (Nivre et al2007) shared
tasks.
The fully unsupervised linguistic analysis
(Spitkovsky et al2011a) shows that the unsuper-
vised part-of-speech tags may be more useful for
this task than the supervised ones.
Another possibility for obtaining dependency
structures for languages without any linguistically
annotated resources can be the projection using a
parallel treebank with a resource-rich language (typ-
ically English). McDonald et al2011) showed that
such projection produce better structures than the
current unsupervised parsers do. However, our task
is different. We would like to produce structures that
are not burdened by any linguistic conventions.
In this paper, we describe a novel approach to un-
supervised dependency parsing. Our model differs
from DMV, since we employ the reducibility feature
and use fertility of nodes instead of generating STOP
signs.
We use Gibbs sampling procedure for inference
instead of Variational Bayes, which has been more
common for induction of linguistic strucures. Gibbs
sampling algorithm for grammar induction was used
also by Marec?ek and Z?abokrtsky? (2011). However,
their sampling algorithm produces generally non-
projective trees. Our sampler, which is described in
Section 5, introduces a completely different small-
change operator that guarantees projective edges.
3 Computing Reducibility scores
We call a word (or a sequence of words) in a sen-
tence reducible, if the sentence after removing the
word remains grammatically correct. Although we
cannot automatically recognize grammaticality of
such newly created sentence, we can search for it
in a large corpus. If we find it, we assume the word
was reducible in the original sentence.
Since the number of such reducible word se-
quences found in any corpus will be low, we de-
termine the reducibility scores from their individual
types (part-of-speech tags). This then implicitly al-
lows some sharing of the scores between different
word sequences.
The necessity to search for the whole sentences
in the corpus and not only for some smaller context
(considering, for example, just left and right neigh-
298
bor), which would lead to lower sparsity, is rational-
ized by the following example:
Their children went to school.
I took their children to school.
The verb ?went? would be reducible in the context
?their children went to school?, because the sequence
?their children to school? occurs in the second sen-
tence. One could find such examples frequently even
for large contexts. For instance, verbs in free word-
order languages can be placed almost at any posi-
tion in a sentence; therefore, without the full sen-
tence context, they would have to be considered as
reducible. To prevent this, we decided to work ex-
clusively with the full sentence context instead of
shorter contexts.
Other way that would lead to lower sparsity would
be searching for sequences of part-of-speech tags in-
stead of sequences of word forms. However, this
also does not bring desired results. For instance, the
two following sentence patterns
DT NNS VBD IN DT NN .
DT NNS VBD DT NN .
are quite frequent in English and we can deduce
from them that the preposition IN is reducible. But
this is of course a wrong deduction, since the prepo-
sition cannot be removed from the prepositional
phrase. Using part-of-speech tags instead of word
forms is thus not suitable for computing reducibility
scores.
Although we search for reducible sequences of
word forms in the corpus, we compute reducibil-
ity scores for sequences of part-of-speech tags. This
requires to have the corpus morphologically disam-
biguated. A sequences of part-of-speech tags will be
denoted as ?PoS n-gram? in the following text.
Assume a PoS n-gram g = [t1, . . . , tn]. We go
through the corpus and search for all its occurrences.
For each such occurrence, we remove the respec-
tive words from the current sentence and check in
the corpus whether the rest of the sentence occurs at
least once elsewhere in the corpus.2 If so, then such
occurrence of PoS n-gram is reducible, otherwise it
is not. We denote the number of such reducible oc-
2We do not take into account sentences with less then 10
words, because they could be nominal (without any verb) and
might influence the reducibility scores of verbs.
unigrams R bigrams R trigrams R
VB 0.04 VBN IN 0.00 IN DT JJ 0.00
TO 0.07 IN DT 0.02 JJ NN IN 0.00
IN 0.11 NN IN 0.04 NN IN NNP 0.00
VBD 0.12 NNS IN 0.05 VBN IN DT 0.00
CC 0.13 JJ NNS 0.07 JJ NN . 0.00
VBZ 0.16 NN . 0.08 DT JJ NN 0.04
NN 0.22 DT NNP 0.09 DT NNP NNP 0.05
VBN 0.24 DT NN 0.09 NNS IN DT 0.14
. 0.32 NN , 0.11 NNP NNP . 0.15
NNS 0.38 DT JJ 0.13 NN IN DT 0.23
DT 0.43 JJ NN 0.14 NNP NNP , 0.46
NNP 0.78 NNP . 0.15 IN DT NNP 0.55
JJ 0.84 NN NN 0.22 DT NN IN 0.59
RB 2.07 IN NN 0.67 NNP NNP NNP 0.64
, 3.77 NNP NNP 0.76 IN DT NN 0.80
CD 55.6 IN NNP 1.81 IN NNP NNP 4.27
Table 1: Reducibility scores of the most frequent
English n-grams. (V* are verbs, N* are nouns, DET
are determiners, IN are prepositions, JJ are adjec-
tives, RB are adverbs, CD are numerals, and CC are
coordinating conjunctions)
currences of PoS n-gram g by r(g). The number of
all its occurrences is c(g).
The relative reducibility R(g) of a PoS n-gram g
is then computed as
R(g) =
1
N
r(g) + ?1
c(g) + ?2
, (1)
where the normalization constant N , which ex-
presses relative reducibility over all the PoS n-grams
(denoted by G), causes the scores are concentrated
around the value 1.
N =
?
g?G(r(g) + ?1)
?
g?G(c(g) + ?2)
(2)
Smoothing constants ?1 and ?2, which prevent re-
ducibility scores from being equal to zero, are set
to
?1 =
?
g?G r(g)
?
g?G c(g)
, ?2 = 1 (3)
This setting causes that even if a given PoS n-gram is
not reducible anywhere in the corpus, its reducibility
score is 1/(c(g) + 1).
Tables 1, 2, and 3 show reducibility scores of the
most frequent PoS n-grams of three selected lan-
guages: English, German, and Czech. If we consider
only unigrams, we can see that the scores for verbs
are often among the lowest. Verbs are followed by
prepositions and nouns, and the scores for adjectives
299
unigrams R bigrams R trigrams R
VVPP 0.00 NN APPR 0.00 NN APPR NN 0.01
APPR 0.27 APPR ART 0.00 ADJA NN APPR 0.01
VVFIN 0.28 ART ADJA 0.00 APPR ART ADJA 0.01
APPRART 0.32 NN VVPP 0.00 NN KON NN 0.01
VAFIN 0.37 NN $( 0.01 ADJA NN $. 0.01
KON 0.37 NN NN 0.01 NN ART NN 0.32
NN 0.43 NN ART 0.21 ART NN ART 0.49
ART 0.49 ADJA NN 0.28 NN ART ADJA 0.90
$( 0.57 NN $, 0.67 ADJA NN ART 0.95
$. 1.01 NN VAFIN 0.85 NN APPR ART 0.95
NE 1.14 NN VVFIN 0.89 NN VVPP $. 1.01
CARD 1.38 NN $. 0.95 ART NN APPR 1.35
ADJA 2.38 ART NN 1.07 ART ADJA NN 1.58
$, 2.94 NN KON 2.41 APPR ART NN 2.60
ADJD 3.54 APPR NN 2.65 APPR ADJA NN 2.65
ADV 7.69 APPRART NN 3.06 ART NN VVFIN 9.51
Table 2: Reducibility scores of the most frequent
German n-grams. (V* are verbs, N* are nouns, ART
are articles, APPR* are prepositions, ADJ* are ad-
jectives, ADV are adverbs, CARD are numerals, and
KON are conjunctions)
and adverbs are very high for all three examined lan-
guages. That is desired, because the reducible uni-
grams will more likely become leaves in dependency
trees. Considering bigrams, the couples [determiner
? noun], [adjective ? noun], and [preposition ? noun]
obtained reasonably high scores. However, there
are also n-grams such as the German trigram [de-
terminer ? noun ? preposition] (ART-NN-APPR)
whose reducibility score is undesirably high.3
4 Models
We introduce a new generative model that is dif-
ferent from the widely used Dependency Model
with Valence (DMV). In DMV (Klein and Manning,
2004) and in the extended model EVG (Headden III
et al2009), there is a STOP sign indicating that no
more dependents in a given direction will be gener-
ated. Given a certain head, all its dependents in left
direction are generated first, then the STOP sign in
that direction, then all its right dependents and then
STOP in the other direction. This process continues
recursively for all generated dependents.
Our model introduces fertility of a node, which
substitutes the STOP sign. For a given head, we first
generate the number of its left and right children
3The high reducibility score of ART-NN-APPR was proba-
bly caused by German particles, which have the same PoS tag
as prepositions.
unigrams R bigrams R trigrams R
P4 0.00 RR AA 0.00 RR NN Z: 0.00
RV 0.00 Z: J, 0.00 NN RR AA 0.00
Vp 0.06 Vp NN 0.00 NN AA NN 0.16
Vf 0.06 VB NN 0.12 AA NN RR 0.23
P7 0.16 NN Vp 0.13 NN RR NN 0.46
J, 0.24 NN VB 0.18 NN J? NN 0.46
RR 0.28 NN RR 0.22 AA NN NN 0.47
VB 0.33 NN AA 0.23 NN Z: Z: 0.48
NN 0.72 NN J? 0.62 NN Z: NN 0.52
J? 1.72 AA NN 0.62 NN NN NN 0.70
C= 1.85 NN NN 0.70 AA AA NN 0.72
PD 2.06 NN Z: 0.97 AA NN Z: 0.86
AA 2.22 Z: NN 1.72 NN NN Z: 1.38
Dg 3.21 Z: Z: 1.97 RR NN NN 2.26
Z: 4.01 J? NN 2.05 RR AA NN 2.65
Db 4.62 RR NN 2.20 Z: NN Z: 8.32
Table 3: Reducibility scores of the most frequent
Czech n-grams. (V* are verbs, N* are nouns, P* are
pronouns, R* are prepositions, A* are adjectives, D*
are adverbs, C* are numerals, J* are conjunctions,
and Z* is punctuation)
(fertility model) and then we fill these positions by
generating its individual dependents (edge model).
If a zero fertility is generated in both the directions,
the head becomes a leaf.
Besides the fertility model and the edge model,
we use two more models (subtree model and dis-
tance model), which force the generated trees to
have more desired shape.4
4.1 Fertility Model
We express a fertility of a node by a pair of num-
bers: the number of its left dependents and the num-
ber of its right dependents. For example, fertility
?1-3? means that the node has one left and three
right dependents, fertility ?0-0? indicates that it is
a leaf. Fertility is conditioned by part-of-speech tag
of the node and it is computed following the Chi-
nese restaurant process. This means that if a specific
fertility has been frequent for a given PoS tag in the
past, it is more likely to be generated again. The
formula for computing probability of fertility fi of a
word on the position i in the corpus is as follows:
Pf (fi|ti) =
c?i(?ti, fi?) + ?P0(fi)
c?i(?ti?) + ?
, (4)
4In fact, the subtree model and the distance model disrupt a
bit the generative story, because the probabilites do not sum up
to one when they are used. However, they proved to help with
inducing better linguistic structures.
300
where ti is part-of-speech tag of the word on the po-
sition i, c?i(?ti, fi?) stands for the count of words
with PoS tag ti and fertility fi in the history, and
P0 is a prior probability for the given fertility which
depends on the total number of node dependents de-
noted by |fi| (the sum of numbers of left and right
dependents):
P0(fi) =
1
2|fi|+1
(5)
This prior probability has a nice property: for a
given number of nodes, the product of fertility prob-
abilities over all the nodes is equal for all possible
dependency trees. This ensures the stability of this
model during the inference.
Besides the basic fertility model, we introduce
also an extended fertility model, which uses fre-
quency of a given word form for generating number
of children. We assume that the most frequent words
are mostly function words (e.g. determiners, prepo-
sitions, auxiliary verbs, conjunctions). Such words
tend to have a stable number of children, for exam-
ple (i) some function words are exclusively leaves,
(ii) prepositions have just one child, and (iii) attach-
ment of auxiliary verbs depends on the annotation
style, but number of their children is also not very
variable. The higher the frequency of a word form,
the higher probability mass is concentrated on one
specific number of children and the lower Dirichlet
hyperparameter ? in Equation 4 is needed. The ex-
tended fertility is described by equation
P ?f (fi|ti, wi) =
c?i(?ti, fi?) + ?eF (wi)P0(fi)
c?i(?ti?) + ?eF (wi)
, (6)
where F (wi) is a frequency of the word wi, which
is computed as a number of words wi in our corpus
divided by number of all words.
4.2 Edge Model
After the fertility (number of left and right depen-
dents) is generated, the individual slots are filled us-
ing the edge model. A part-of-speech tag of each de-
pendent is conditioned by part-of-speech tag of the
head and the edge direction (position of the depen-
dent related to the head).5
5For the edge model purposes, the PoS tag of the technical
root is set to ?<root>? and it is in the zero-th position in the
Similarly as for the fertility model, we employ
Chinese restaurant process to assign probabilities of
individual dependent.
Pe(tj |ti, dj) =
c?i(?ti, tj , dj?) + ?
c?i(?ti, dj?) + ?|T |
, (7)
where ti and tj are the part-of-speech tags of the
head and the generated dependent respectively; dj is
a direction of edge between the words i and j, which
can have two values: left and right. c?i(?ti, tj , dj?)
stands for the count of edges ti ? tj with the direc-
tion dj in the history, |T | is a number of unique tags
in the corpus and ? is a Dirichlet hyperparameter.
4.3 Distance Model
Distance model is an auxiliary model that prevents
the resulting trees from being too flat. Ideally, it
would not be needed, but experiments showed that
it helps to infer better trees. This simple model says
that shorter edges are more probable than longer
ones. We define probability of a distance between
a word and its parent as its inverse value,6 which is
then normalized by the normalization constant d.
Pd(i, j) =
1
d
(
1
|i? j|
)?
(8)
The hyperparameter ? determines the weight of this
model.
4.4 Subtree Model
The subtree model uses the reducibility measure. It
plays an important role since it forces the reducible
words to be leaves and reducible n-grams to be sub-
trees. Words with low reducibility are forced to-
wards the root of the tree. We define desc(i) as a
sequence of tags [tl, . . . , tr] that corresponds to all
the descendants of the word wi including wi, i.e. the
whole subtree of wi. The probability of such sub-
tree is proportional to its reducibility R(desc(i)).
The hyperparameter ? determines the weight of the
model; s is a normalization constant.
Ps(i) =
1
s
R(desc(i))? (9)
sentence, so the head word of the sentence is always its right
dependent.
6Distance between any word and the technical root of the
dependency tree was set to 10. Since each technical root has
only one dependent, this value does not affect the model.
301
4.5 Probability of the Whole Treebank
We want to maximize the probability of the whole
generated treebank, which is computed as follows:
Ptreebank =
n?
i=1
(P ?f (fi|ti, wi) (10)
Pe(ti|tpi(i), di) (11)
Pd(i, pi(i)) (12)
Ps(i)), (13)
where pi(i) denotes the parent of the word on the
position i. We multiply the probabilities of fertil-
ity, edge, distance from parent, and subtree over all
words (nodes) in the corpus. The extended fertility
model P ?f can be substituted by its basic variant Pf .
5 Sampling Algorithm
For stochastic searching for the most probable de-
pendency trees, we employ Gibbs sampling, a stan-
dard Markov Chain Monte Carlo technique (Gilks et
al., 1996). In each iteration, we loop over all words
in the corpus in a random order and change the de-
pendencies in their neighborhood (a small change
described in Section 5.2). In the end, ?average? trees
based on the whole sampling are built.
5.1 Initialization
Before the sampling starts, we initialize the projec-
tive trees randomly. For doing so, we tried the fol-
lowing two initializers:
? For each sentence, we choose randomly one
word as the head and attach all other words to
it.
? We are picking one word after another in a ran-
dom order and we attach it to the nearest left (or
right) neighbor that has not been attached yet.
The left-right choice is made by a coin flip. If it
is not possible to attach a word to one side, we
attach it to the other side. The last unattached
word becomes the head of the sentence.
While the first method generates only flat trees,
the second one can generate all possible projective
trees. However, the sampler converges to similar re-
sults for both the initializations. Therefore we con-
clude that the choice of the initialization mechanism
The   dog   was   in   the   park  .
(((The) dog) was (in ((the) park)) (.))
Figure 1: Arrow and bracketing notation of a projec-
tive dependency tree.TTThe dogwadosinootpoTTre doki.(dooT)dd
TTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddTTd d
TTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddT Td d
Figure 2: An example of small change in a projec-
tive tree. The bracket (in the park) is removed and
there are five possibilities how to replace it.
is not so important here and we choose the first one
due to its simplicity.
5.2 Small Change Operator
We use the bracketing notation for illustrating the
small change operator. Each projective dependency
tree consisting of n words can be expressed by n
pairs of brackets. Each bracket pair belongs to one
node and delimits its descendants from the rest of
the sentence. Furthermore, each bracketed segment
contains just one word that is not embedded deeper;
this node is the segment head. An example of this
notation is in Figure 1.
The small change is then very simple. We remove
one pair of brackets and add another, so that the con-
ditions defined above are not violated. An example
of such change is in Figure 2.
From the perspective of dependency structures,
the small change can be described as follows:
1. Pick a random non-root word w (the word in
in our example) and find its parent p (the word
was).
2. Find all other children of w and p (the words
dog, park, and .) and denote this set by C.
3. Choose the new head out of w and p. Mark the
new head as g and the second candidate as d.
Attach d to g.
302
4. Select a neighborhoodD adjacent to the word d
as a continuous subset ofC and attach all words
from D to d. D may be also empty.
5. Attach the remaining words from C that were
not in D to the new head g.
5.3 Building ?Average? Trees
The ?burn-in? period is set to 10 iterations. After
this period, we begin to count how many times an
edge occurs at a particular location in the corpus.
These counts are collected over the whole corpus
with the collection-rate 0.01.7
When the samling is finished, we build final de-
pendency trees based on the edge counts obtained
during the sampling. We employ the maximum
spanning tree (MST) algorithm (Chu and Liu, 1965)
to find them; the weights of edges for computing
MST correspond to the number of times they were
present during the sampling. This averaging method
was used also by Marec?ek and Z?abokrtsky? (2011).
Other possibilities for obtaining final depen-
dency trees would be using Eisner?s projective al-
gorithm (Eisner, 1996) or using annealing method
(favoring more likely changes) at the end of the sam-
pling. However, the general non-projective MST al-
gorithm enable non-projective edges, which are by
no means negligible in treebanks (Havelka, 2007).
6 Experiments and Evaluation
We evaluate our parser on 20 treebanks (18
languages) included in CoNLL shared tasks
2006 (Buchholz and Marsi, 2006) and 2007 (Nivre
et al2007).
Similarly to some previous papers on unsuper-
vised parsing (Gillenwater et al2011; Spitkovsky
et al2011b), the tuning experiments were per-
formed on English only. We used English for check-
ing functionality of the individual models and for
optimizing hyperparameter values. The best config-
uration of the parser achieved on English develop-
ment data was then used for parsing all other lan-
guages. This simulates the situation in which we
have only one treebank (English) on which we can
tune our parser and we want to parse other languages
for which we have no manually annotated treebanks.
7After each small change is made, the edges from the whole
corpus are collected with a probability 0.01.
language tokens (mil.) language tokens (mil.)
Arabic 19.7 Greek 20.9
Basque 14.1 Hungarian 26.3
Bulgarian 18.8 Italian 39.7
Catalan 27.0 Japanese 2.6
Czech 20.3 Portuguese 31.7
Danish 15.9 Slovenian 13.7
Dutch 27.1 Spanish 53.4
English 85.0 Swedish 19.2
German 56.9 Turkish 16.5
Table 4: Wikipedia texts statistics
6.1 Data
We need two kinds of data for our experiments: a
smaller treebank, which is used for sampling and for
evaluation, and a large corpus, from which we com-
pute n-gram reducibility scores.
The treebanks are taken from the CoNLL shared
task 2006 and 2007. The experiments are per-
formed for all languages except for Chinese.8 We
use only the testing parts of the treebanks (the files
test.conll) for the dependency tree induction.
As a source of the part-of-speech tags, we use the
fine-grained gold PoS tags, which are in the fifth col-
umn in the CoNLL format.
For obtaining reducibility scores, we used the
W2C corpus9 of Wikipedia articles, which was
downloaded by Majlis? and Z?abokrtsky? (2012). Their
statistics across languages are shown in Table 4. To
make them useful, the necessary preprocessing steps
must have been done. The texts were first automati-
cally segmented and tokenized10 and then they were
part-of-speech tagged by TnT tagger (Brants, 2000),
which was trained on the respective CoNLL train-
ing data (the files train.conll). The quality of
such tagging is not very high, since we do not use
any lexicons11 or pretrained models. However, it is
sufficient for obtaining good reducibility scores.
8We do not have appropriate Chinese segmenter that would
segment Chinese texts in the same way as in CoNLL.
9http://ufal.mff.cuni.cz/?majlis/w2c/
10The segmentation to sentences and tokenization was per-
formed using the TectoMT framework (Popel and Z?abokrtsky?,
2010).
11Using lexicons or another pretrained models for tagging
means using other sources of human annotated data, which is
not allowed if we want to compare our results with others.
303
6.2 Setting the Hyperparameters
The applicability of individual models and their pa-
rameters were tested on development data set of
English (the file en/dtest.conll in CoNLL
shared task 2007).
After several experiments, we have observed that
the extended fertility model provides better results
than the basic fertility model; the parser using the
basic fertility model achieved 44.1% attachment
score for English, whereas the extended fertility
model increased the score to 46.8%. The four hy-
perparameters ?e (extended fertility model), ? (edge
model), ? (distance model), and ? (subtree model),
were set by a grid search algorithm,12 which found
the following optimal values:
?e = 0.01, ? = 1, ? = 1.5, ? = 1
In informal experiments, parameters were tuned
also for other treebanks and we found out that they
vary across languages. Therefore, adjusting the hy-
perparameters on another language would probably
change the scores significantly.
6.3 Evaluation
The best setting from the experiments on English is
now used for evaluating our parser on all CoNLL
languages. To be able to compare our parser attach-
ment score to previously published results, the fol-
lowing steps must be done:
? We take the testing part of each treebank (the
file test.conll) and remove all the punctu-
ation marks. If the punctuation node is not a
leaf, its children are attached to the parent of
the removed node.
? Some previous papers report results on up-to-
10-words sentences only. Therefore we extract
such sentences from the test data and evaluate
on this subsets as well.
12Here we make use of manually annotated trees. However,
we use only English treebank an we are setting only four num-
bers out of several previously given values (e.g ?e out of 0.01,
0.1, 1, 10). These numbers could be tuned also by inspecting
the outputs. So we believe this method can be treated as unsu-
pervised.
CoNLL ? 10 tokens all sentences
language year gil11 our spi11 our
Arabic 06 ? 40.5 16.6 26.5
Arabic 07 ? 48.0 49.5 27.9
Basque 07 ? 30.8 24.0 26.8
Bulgarian 06 58.3 53.2 43.9 46.0
Catalan 07 ? 63.5 59.8 47.0
Czech 06 53.2 58.9 27.7 49.5
Czech 07 ? 63.7 28.4 48.0
Danish 06 45.9 49.5 38.3 38.6
Dutch 06 33.5 48.8 27.8 44.2
English 07 ? 64.1 45.2 49.2
German 06 46.7 60.8 30.4 44.8
Greek 07 ? 30.2 13.2 20.2
Hungarian 07 ? 61.8 34.7 51.8
Italian 07 ? 50.5 52.3 43.3
Japanese 06 57.7 65.4 50.2 50.8
Portuguese 06 54.0 62.3 36.7 50.6
Slovenian 06 50.9 21.0 32.2 18.1
Spanish 06 57.9 67.3 50.6 51.9
Swedish 06 45.0 60.5 50.0 48.2
Turkish 07 ? 13.0 35.9 15.7
Average: 50.3? 54.7? 37.4 40.0
Table 5: Comparison of directed attachment scores
with previously reported results on CoNLL tree-
banks. The column ?gil11? contains results reported
by Gillenwater et al011) (see the best configura-
tion in Table 7 in their paper). They provided only
results on sentences of up to 10 tokens from CoNLL
2006 treebanks. Results in the column ?spi11? are
taken from Spitkovsky et al011b), best configu-
ration in Table 6 in their paper. The average score
in the last line is computed across all comparable
results, i.e. for comparison with ?gil11? only the
CoNLL?06 results are averaged (?). Our parser was
not evaluated on Turkish CoNLL?06 data and Chi-
nese data, because we have not them available.
The resulting scores are given in Table 5. We
compare our results with results previously reported
by Gillenwater (2011) and Spitkovsky (2011b), who
used the CoNLL data for evaluation too. Since they
provide results for several configurations of their
parsers, we choose only the best one from each the
paper. We define the best configuration as the one
304
with the highest average attachment score across all
the tested languages.
We can see that our parser outperforms the pre-
viously published ones. In one case, it is better for
8 out of 10 data sets, in the other case, it is better
for 14 out of 20 data sets. The average attachment
scores, which are computed only from the results
present for both compared parsers, also confirm the
improvement.
However, it is important to note that we used an
additional source of information, namely large unan-
notated corpora for computing reducibility scores,
while the others used only the CoNLL data.
6.4 Error Analysis
Our main motivation for developing an unsupervised
dependency parser was that we wanted to be able
to parse any language. However, the experiments
show that our parser fails for some languages. In
this section, we try to analyze and explain some of
the most substantial types of errors.
Auxiliary verbs in Slovenian ? In the Slovenian
treebank, many verbs are composed of two words:
main verb (marked as Verb-main) and auxiliary
verb (Verb-copula). Our parser choose the aux-
iliary verb as the head and the main verb and all its
dependants become its children. That is why the at-
tachment score is so poor (only 18.1%). In fact, the
induced structure is not so bad. The main verb is
switched with the auxiliary one which causes also
the wrong attachment of all its dependants.
Articles in German ? Attachment of about one
half of German articles is wrong. Instead of the ar-
ticle being attached below the appropriate noun, the
noun is attached below the article. It is a similar
problem as the aforementioned Slovenian auxiliary
verbs. The dependency between content and func-
tion word is switched and the dependants of the con-
tent word are attached to the function word. Klein
and Manning (2004) observed a similar behavior in
their experiments with DMV.
Noun phrases in English ? The structure of
phrases that consist of more nouns are often induced
badly. This is caused probably by ignoring word
forms. For example, the structure of the sequence
?NN NN NN? can be hardly recognized by our parser.
fert. edge dist. subtr. en de cs
(random baseline) 19.8 18.4 26.7
X 8.71 13.7 14.9
X 18.9 20.2 26.5
X 23.6 19.5 25.3
X 28.2 23.7 33.5
X X 21.2 22.9 23.5
X X 19.9 19.7 25.5
X X 7.8 17.5 22.7
X X 24.1 19.5 27.1
X X 25.5 27.5 40.7
X X 31.2 25.2 33.1
X X X 30.7 26.2 22.0
X X X 14.1 18.1 34.6
X X X 36.1 32.2 38.9
X X X 34.8 26.7 42.4
X X X X 46.8 36.5 47.2
Table 6: Ablation analysis. Unlabeled attachment
scores for different combinations of model compo-
nents (fertility model, edge model, distance model
and subtree model). The scores are computed on all
sentences of the development data. Punctuation is
included into the evaluation.
6.5 Ablation Analysis
To investigate the impact of individual components
of the model, we run the parser for all possible com-
ponent combinations. We choose three languages
along the scale of word order freedom: English
(very rigid word order), Czech (relatively free word
order), and German (somewhere in the middle). The
attachment scores are shown in Table 6. If no model
is used for the inference and the sampling algorithm
samples completely random trees, we get the ran-
dom baseline score, which is 19.8% for English13.
From the perspective of the subtree model, which
implements the reducibility feature, we can see that
it is the most useful model here. Alone, it improves
the score for English to 28.2%. If we do not use
it, the score decreases from 46.8% (when all mod-
els are used) to 30.7%. Very important is also the
distance model which eliminates the possibility of
attaching all words to one head word. If we omit
13This relatively high baseline scores are caused by the MST
algorithm, which chooses the most frequent edges from random
trees i.e. the shortest ones.
305
it, the score for English falls drastically to 14.1%.
Some combinations of models have their scores far
below the baseline. This is caused by the fact that
some regularities have been found but the structures
are induced differently and thus all attachments are
wrong.
6.6 Induction without Wikipedia Corpus
We have performed also experiments using exclu-
sively the CoNLL data. However, the numbers of
reducible words in CoNLL training set were very
low (50 words at maximum in CoNLL 2006 train-
ing data and 10 words at maximum in CoNLL 2007
training data). This led to completely unreliable re-
ducibility scores and the consequent poor results.
7 Conclusions and Future Work
We have shown that employing the reducibility fea-
ture is useful in unsupervised dependency parsing
task. We extracted the n-gram reducibility scores
from a large corpus, and then made the computation-
ally demanding inference on smaller data using only
these scores. We evaluated our parser on 18 lan-
guages included in CoNLL and for 14 of them, we
achieved higher attachment scores than previously
published results.
The most errors were caused by function words,
which sometimes take over the dependents of adja-
cent content words. This can be caused by the fact
that the reducibility cannot handle function words
correctly, because they must be reduced together
with a content word, not one after another.
In future work, we would like to estimate the
hyperparameters automatically. Furthermore, we
would like to get rid of manually designed PoS tags
and use some kind of unsupervised clusters in order
to have all the annotation process completely unsu-
pervised. We would also like to employ lexicalized
models that should help in situations in which the
PoS tags are too coarse.
Finally, we would like to move towards deeper
syntactic structures, where the tree would be formed
only by content words and the function words would
be treated in a different way.
Software
The source code of our unsupervised dependency
parser including the script for computing reducibil-
ity scores from large corpora is available at
http://ufal.mff.cuni.cz/?marecek/udp.
Acknowledgement
This research was supported by the grants
GAUK 116310, GA201/09/H057 (Res Infor-
matica), LM2010013, MSM0021620838, and
by the European Commission?s 7th Framework
Program (FP7) under grant agreement n? 247762
(FAUST). We thank anonymous reviewers for their
valuable comments and suggestions.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 1204?1213, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. Proceedings of the sixth conference
on Applied natural language processing, page 8.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Y. J. Chu and T. H. Liu. 1965. On the Shortest Arbores-
cence of a Directed Graph. Science Sinica, 14:1396?
1400.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Neural Information
Processing Systems, pages 321?328.
Jason Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?345,
Copenhagen, August.
Kim Gerdes and Sylvain Kahane. 2011. Defining depen-
dencies (and constituents). In Proceedings of Depen-
dency Linguistics 2011, Barcelona.
Walter R. Gilks, S. Richardson, and David J. Spiegelhal-
ter. 1996. Markov chain Monte Carlo in practice. In-
terdisciplinary statistics. Chapman & Hall.
306
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-
nando Pereira, and Ben Taskar. 2011. Posterior
Sparsity in Unsupervised Dependency Parsing. The
Journal of Machine Learning Research, 12:455?490,
February.
Jir??? Havelka. 2007. Beyond Projectivity: Multilin-
gual Evaluation of Constraints and Measures on Non-
Projective Structures. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, pages 608?615.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 101?109, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Marke?ta Lopatkova?, Martin Pla?tek, and Vladislav Kubon?.
2005. Modeling syntax of free word-order languages:
Dependency analysis by reduction. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors,
Lecture Notes in Artificial Intelligence, Proceedings of
the 8th International Conference, TSD 2005, volume
3658 of Lecture Notes in Computer Science, pages
140?147, Berlin / Heidelberg. Springer.
Martin Majlis? and Zdene?k Z?abokrtsky?. 2012. Language
richness of the web. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), Istanbul, Turkey, May. Eu-
ropean Language Resources Association (ELRA).
David Marec?ek and Zdene?k Z?abokrtsky?. 2011. Gibbs
Sampling with Treeness constraint in Unsupervised
Dependency Parsing. In Proceedings of RANLP Work-
shop on Robust Unsupervised and Semisupervised
Methods in Natural Language Processing, pages 1?8,
Hissar, Bulgaria.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 62?72, Edinburgh, Scotland, UK., July. Associ-
ation for Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Depen-
dency Parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Noah Ashton Smith. 2007. Novel estimation methods
for unsupervised discovery of latent structure in natu-
ral language text. Ph.D. thesis, Baltimore, MD, USA.
AAI3240799.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang,
and Daniel Jurafsky. 2011a. Unsupervised depen-
dency parsing without gold part-of-speech tags. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011b. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011c. Punctuation: Making a point in unsu-
pervised dependency parsing. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL-2011).
307
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 517?527,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Coordination Structures in Dependency Treebanks
Martin Popel, David Marec?ek, Jan S?te?pa?nek, Daniel Zeman, Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics (U?FAL)
Malostranske? na?me?st?? 25, CZ-11800 Praha, Czechia
{popel|marecek|stepanek|zeman|zabokrtsky}@ufal.mff.cuni.cz
Abstract
Paratactic syntactic structures are noto-
riously difficult to represent in depen-
dency formalisms. This has painful con-
sequences such as high frequency of pars-
ing errors related to coordination. In other
words, coordination is a pending prob-
lem in dependency analysis of natural lan-
guages. This paper tries to shed some
light on this area by bringing a system-
atizing view of various formal means de-
veloped for encoding coordination struc-
tures. We introduce a novel taxonomy of
such approaches and apply it to treebanks
across a typologically diverse range of 26
languages. In addition, empirical obser-
vations on convertibility between selected
styles of representations are shown too.
1 Introduction
In the last decade, dependency parsing has grad-
ually been receiving visible attention. One of
the reasons is the increased availability of depen-
dency treebanks, be they results of genuine depen-
dency annotation projects or converted automat-
ically from previously existing phrase-structure
treebanks.
In both cases, a number of decisions have to be
made during the construction or conversion of a
dependency treebank. The traditional notion of
dependency does not always provide unambiguous
solutions, e.g. when it comes to attaching func-
tional words. Worse, dependency representation is
at a loss when it comes to representing paratactic
linguistic phenomena such as coordination, whose
nature is symmetric (two or more conjuncts play
the same role), as opposed to the head-modifier
asymmetry of dependencies.1
1We use the term modifier (or child) for all types of de-
pendent nodes including arguments.
The dominating solution in treebank design is to
introduce artificial rules for the encoding of coor-
dination structures within dependency trees using
the same means that express dependencies, i.e., by
using edges and by labeling of nodes or edges. Ob-
viously, any tree-shaped representation of a coor-
dination structure (CS) must be perceived only as
a ?shortcut? since relations present in coordination
structures form an undirected cycle, as illustrated
already by Tesnie`re (1959). For example, if a noun
is modified by two coordinated adjectives, there
is a (symmetric) coordination relation between the
two conjuncts and two (asymmetric) dependency
relations between the conjuncts and the noun.
However, as there is no obvious linguistic in-
tuition telling us which tree-shaped CS encoding
is better and since the degree of freedom has sev-
eral dimensions, one can find a number of distinct
conventions introduced in particular dependency
treebanks. Variations exist both in topology (tree
shape) and labeling. The main goal of this pa-
per is to give a systematic survey of the solutions
adopted in these treebanks.
Naturally, the interplay of dependency and co-
ordination links in a single tree leads to serious
parsing issues.2 The present study does not try to
decide which coordination style is the best from
the parsing point of view.3 However, we believe
that our survey will substantially facilitate experi-
ments in this direction in the future, at least by ex-
ploring and describing the space of possible can-
didates.
2CSs have been reported to be one of the most frequent
sources of parsing errors (Green and Z?abokrtsky?, 2012; Mc-
Donald and Nivre, 2007; Ku?bler et al, 2009; Collins, 2003).
Their impact on quality of dependency-based machine trans-
lation can also be substantial; as documented on an English-
to-Czech dependency-based translation system (Popel and
Z?abokrtsky?, 2009), 39% of serious translation errors which
are caused by wrong parsing have to do with coordination.
3There might be no such answer, as different CS conven-
tions might serve best for different applications or for differ-
ent parser architectures.
517
The rest of the paper is structured as follows.
Section 2 describes some known problems related
to CS. Section 3 shows possible ?styles? for rep-
resenting CS. Section 4 lists treebanks whose CS
conventions we studied. Section 5 presents empir-
ical observations on CS convertibility. Section 6
concludes the paper.
2 Related work
Let us first recall the basic well-known character-
istics of CSs.
In the simplest case of a CS, a coordinating
conjunction joins two (usually syntactically and
semantically compatible) words or phrases called
conjuncts. Even this simplest case is difficult to
represent within a dependency tree because, in the
words of Lombardo and Lesmo (1998): Depen-
dency paradigms exhibit obvious difficulties with
coordination because, differently from most lin-
guistic structures, it is not possible to characterize
the coordination construct with a general schema
involving a head and some modifiers of it.
Proper formal representation of CSs is further
complicated by the following facts:
? CSs with more than two conjuncts (multi-
conjunct CSs) exist and are frequent.
? Besides ?private? modifiers of individual
conjuncts, there are modifiers shared by
all conjuncts, such as in ?Mary came and
cried?. Shared modifiers may appear along-
side with private modifiers of particular con-
juncts.
? Shared modifiers can be coordinated, too:
?big and cheap apples and oranges?.
? Nested (embedded) coordinations are possi-
ble: ?John and Mary or Sam and Lisa?.
? Punctuation (commas, semicolons, three
dots) is frequently used in CSs, mostly with
multi-conjunct coordinations or juxtaposi-
tions which can be interpreted as CSs with-
out conjunctions (e.g. ?Don?t worry, be
happy!?).
? In many languages, comma or other punctu-
ation mark may play the role of the main co-
ordinating conjunction.
? The coordinating conjunction may be a mul-
tiword expression (?as well as?).
? Deficient CSs with a single conjunct exist.
? Abbreviations like ?etc.? comprise both the
conjunction and the last conjunct.
? Coordination may form very intricate struc-
tures when combined with ellipsis. For ex-
ample, a conjunct can be elided while its ar-
guments remain in the sentence, such as in
the following traditional example: ?I gave
the books to Mary and the records to Sue.?
? The border between paratactic and hypotactic
surface means of expressing coordination re-
lations is fuzzy. Some languages can use en-
clitics instead of conjunctions/prepositions,
e.g. Latin ?Senatus Populusque Romanus?.
Purely hypotactic surface means such as the
preposition in ?John with Mary? occur too.4
? Careful semantic analysis of CSs discloses
additional complications: if a node is mod-
ified by a CS, it might happen that it is
the node itself (and not its modifiers) what
should be semantically considered as a con-
junct. Note the difference between ?red and
white wine? (which is synonymous to ?red
wine and white wine?) and ?red and white
flag of Poland?. Similarly, ?five dogs and
cats? has a different meaning than ?five dogs
and five cats?.
Some of these issues were recognized already
by Tesnie`re (1959). In his solution, conjuncts are
connected by vertical edges directly to the head
and by horizontal edges to the conjunction (which
constitutes a cycle in every CS). Many different
models have been proposed since, out of which the
following are the most frequently used ones:
? MS = Mel?c?uk style used in the Meaning-
Text Theory (MTT): the first conjunct is the
head of the CS, with the second conjunct at-
tached as a dependent of the first one, third
conjunct under the second one, etc. Coor-
dinating conjunction is attached under the
penultimate conjunct, and the last conjunct
is attached under the conjunction (Mel?c?uk,
1988),
? PS = Prague Dependency Treebank (PDT)
style: all conjuncts are attached under the
coordinating conjunction (along with shared
modifiers, which are distinguished by a spe-
cial attribute) (Hajic? et al, 2006),
4As discussed by Stassen (2000), all languages seem to
have some strategy for expressing coordination. Some of
them lack the paratactic surface means (the so called WITH-
languages), but the hypotactic surface means are present al-
most always.
518
? SS = Stanford parser style:5 the first conjunct
is the head and the remaining conjuncts (as
well as conjunctions) are attached under it.
One can find various arguments supporting the
particular choices. MTT possesses a complex
set of linguistic criteria for identifying the gov-
ernor of a relation (see Mazziotta (2011) for an
overview), which lead to MS. MS is preferred in
a rule-based dependency parsing system of Lom-
bardo and Lesmo (1998). PS is advocated by
S?te?pa?nek (2006) who claims that it can represent
shared modifiers using a single additional binary
attribute, while MS would require a more complex
co-indexing attribute. An argumentation of Tratz
and Hovy (2011) follows a similar direction: We
would like to change our [MS] handling of coordi-
nating conjunctions to treat the coordinating con-
junction as the head [PS] because this has fewer
ambiguities than [MS]. . .
We conclude that the influence of the choice of
coordination style is a well-known problem in de-
pendency syntax. Nevertheless, published works
usually focus only on a narrow ad-hoc selection of
few coordination styles, without giving any sys-
tematic perspective.
Choosing a file format presents a different prob-
lem. Despite various efforts to standardize lin-
guistic annotation,6 no commonly accepted stan-
dard exists. The primitive format used for CoNLL
shared tasks is widely used in dependency parsing,
but its weaknesses have already been pointed out
(cf. Stran?a?k and S?te?pa?nek (2010)). Moreover, par-
ticular treebanks vary in their contents even more
than in their format, i.e. each treebank has its own
way of representing prepositions or different gran-
ularity of syntactic labels.
3 Variations in representing
coordination structures
Our analysis of variations in representing coordi-
nation structures is based on observations from a
set of dependency treebanks for 26 languages.7
5We use the already established MS-PS-SS distinction to
facilitate literature overview; as shown in Section 3, the space
of possible coordination styles is much richer.
6For example, TEI (TEI Consortium, 2013), PML (Hana
and S?te?pa?nek, 2012), SynAF (ISO 24615, 2010).
7The primary data sources are the following: Ancient
Greek: Ancient Greek Dependency Treebank (Bamman and
Crane, 2011), Arabic: Prague Arabic Dependency Tree-
bank 1.0 (Smrz? et al, 2008), Basque: Basque Dependency
Treebank (larger version than CoNLL 2007 generously pro-
In accordance with the usual conventions, we as-
sume that each sentence is represented by one de-
pendency tree, in which each node corresponds
to one token (word or punctuation mark). Apart
from that, we deliberately limit ourselves to CS
representations that have shapes of connected sub-
graphs of dependency trees.
We limit our inventory of means of expressing
CSs within dependency trees to (i) tree topology
(presence or absence of a directed edge between
two nodes, Section 3.1), and (ii) node labeling
(additional attributes stored insided nodes, Sec-
tion 3.2).8 Further, we expect that the set of pos-
sible variations can be structured along several di-
mensions, each of which corresponds to a certain
simple characteristic (such as choosing the left-
most conjunct as the CS head, or attaching shared
modifiers below the nearest conjunct). Even if it
does not make sense to create the full Cartesian
product of all dimensions because some values
cannot be combined, it allows to explore the space
of possible CS styles systematically.9
3.1 Topological variations
We distinguish the following dimensions of topo-
logical variations of CS styles (see Figure 1):
Family ? configuration of conjuncts. We di-
vide the topological variations into three main
groups, labeled as Prague (fP), Moscow (fM), and
vided by IXA Group) (Aduriz and others, 2003), Bulgarian:
BulTreeBank (Simov and Osenova, 2005), Czech: Prague
Dependency Treebank 2.0 (Hajic? et al, 2006), Danish: Dan-
ish Dependency Treebank (Kromann et al, 2004), Dutch:
Alpino Treebank (van der Beek and others, 2002), English:
Penn TreeBank 3 (Marcus et al, 1993), Finnish: Turku De-
pendency Treebank (Haverinen et al, 2010), German: Tiger
Treebank (Brants et al, 2002), Greek (modern): Greek De-
pendency Treebank (Prokopidis et al, 2005), Hindi, Ben-
gali and Telugu: Hyderabad Dependency Treebank (Husain
et al, 2010), Hungarian: Szeged Treebank (Csendes et al,
2005), Italian: Italian Syntactic-Semantic Treebank (Mon-
temagni and others, 2003), Latin: Latin Dependency Tree-
bank (Bamman and Crane, 2011), Persian: Persian Depen-
dency Treebank (Rasooli et al, 2011), Portuguese: Floresta
sinta?(c)tica (Afonso et al, 2002), Romanian: Romanian De-
pendency Treebank (Ca?la?cean, 2008), Russian: Syntagrus
(Boguslavsky et al, 2000), Slovene: Slovene Dependency
Treebank (Dz?eroski et al, 2006), Spanish: AnCora (Taule?
et al, 2008), Swedish: Talbanken05 (Nilsson et al, 2005),
Tamil: TamilTB (Ramasamy and Z?abokrtsky?, 2012), Turk-
ish: METU-Sabanci Turkish Treebank (Atalay et al, 2003).
8Edge labeling can be trivially converted to node labeling
in tree structures.
9The full Cartesian product of variants in Figure 1 would
result in topological 216 variants, but only 126 are applicable
(the inapplicable combinations are marked with ??? in Fig-
ure 1). Those 126 topological variants can be further com-
bined with labeling variants defined in Section 3.2.
519
Main family Prague family (code fP)[14 treebanks]
Moscow family (code fM)
[5 treebanks]
Stanford family (code fS)
[6 treebanks]
Choice of head
Head on left (code hL)
[10 treebanks]
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Head on right (code hR)
[14 treebanks]
Mixed head (code hM) [1 treebank] A mixture of hL and hR
Attachment of shared modifiers
Shared modifier
below the nearest conjunct
(code sN)
[15 treebanks]
Shared modifier below head
(code sH)
[11 treebanks]
dogs
dogsaaan, caaaaataaaaaarocaaaaaoc
on
dogs an, cct do
dogs
an, cct
orao 
o 
dogsaaan, caaaataaaarocaaaon
oc
Attachment of coordinating conjunction
Coordinating conjunction
below previous conjunct (code cP)
[2 treebanks]
?
dogs
and,, acst,,racs dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
below following conjunct (code cF)
[1 treebank]
?
dogssadn,
g c,
adn,tssrdn,dog dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
between two conjuncts (code cB)
[8 treebanks]
?
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Coordinating conjunction as the head (code cH)
is the only applicable style for the Prague family [14 treebanks] ? ?
Placement of punctuation
values pP [7 treebanks], pF [1 treebank] and pB [15 treebanks] are analogous to cP, cF and cB
(but applicable also to the Prague family)
Figure 1: Different coordination styles, variations in tree topology. Example phrase: ?(lazy) dogs, cats
and rats?. Style codes are described in Section 3.1.
Stanford (fS) families.10 This first dimension dis-
tinguishes the configuration of conjuncts: in the
Prague family, all the conjuncts are siblings gov-
erned by one of the conjunctions (or a punctuation
fulfilling its role); in the Moscow family, the con-
juncts form a chain where each node in the chain
depends on the previous (or following) node; in
the Stanford family, the conjuncts are siblings ex-
cept for the first (or last) conjunct, which is the
10Names are chosen purely as a mnemonic device, so that
Prague Dependency Treebank belongs to the Prague family,
Mel?c?uk style belongs to the Moscow family, and Stanford
parser style belongs to the Stanford family.
head.11
Choice of head ? leftmost or rightmost. In
the Prague family, the head can be either the left-
most12 (hL) or the rightmost (hR) conjunction or
punctuation. Similarly, in the Moscow and Stan-
ford families, the head can be either the leftmost
(hL) or the rightmost (hR) conjunct. A third op-
11Note that for CSs with just two conjuncts, fM and fS
may look exactly the same (depending on the attachment of
conjunctions and punctuation as described below).
12For simplicity, we use the terms left and right even if
their meaning is reversed for languages with right-to-left
writing systems such as Arabic or Persian.
520
tion (hM) is to mix hL and hR based on some cri-
terion, e.g. the Persian treebank uses hR for coor-
dination of verbs and hL otherwise. For the exper-
iments in Section 5, we choose the head which is
closer to the parent of the whole CS, with the mo-
tivation to make the edge between CS head and its
parent shorter, which may improve parser training.
Attachment of shared modifiers. Shared mod-
ifiers may appear before the first conjunct or after
the last one. Therefore, it seems reasonable to at-
tach shared modifiers either to the CS head (sH),
or to the nearest (i.e. first or last) conjunct (sN).
Attachment of coordinating conjunctions. In
the Moscow family, conjunctions may be either
part of the chain of conjuncts (cB), or they may be
put outside of the chain and attached to the previ-
ous (cP) or following (cF) conjunct. In the Stan-
ford family, conjunctions may be either attached
to the CS head (and therefore between conjuncts)
(cB), or they may be attached to the previous (cP)
or the following (cF) conjunct. The cB option in
both Moscow and Stanford families, treats con-
junctions in the same way as conjuncts (with re-
spect to topology only). In the Prague family, there
is just one option available (cH) ? one of the con-
junctions is the CS head while the others are at-
tached to it.
Attachment of punctuation. Punctuation to-
kens separating conjuncts (commas, semicolons
etc.) could be treated the same way as conjunc-
tions. However, in most treebanks it is treated
differently, so we consider it as well. The val-
ues pP, pF and pB are analogous to cP, cF and
cB except that punctuation may be also attached
to the conjunction in case of pP and pF (other-
wise, a comma before the conjunction would be
non-projectively attached to the member follow-
ing the conjunction).
The three established styles mentioned in Sec-
tion 2 can be defined in terms of the newly intro-
duced abbreviations: PS = fPhRsHcHpB, MS =
fMhLsNcBp?, and SS = fShLsNcBp?.13
3.2 Labeling variations
Most state-of-the-art dependency parsers can pro-
duce labeled edges. However, the parsers produce
only one label per edge. To fully capture CSs,
we need more than one label, because there are
several aspects involved (see the initial assump-
13The question marks indicate that the original Mel?c?uk
and Stanford parser styles ignore punctuation.
tions in Section 3): We need to identify the co-
ordinating conjunction (its POS tag might not be
enough), conjuncts, shared modifiers, and punctu-
ation that separates conjuncts. Besides that, there
should be a label classifying the dependency rela-
tion between the CS and its parent.
Some of the information can be retrieved from
the topology of the tree and the ?main label? of
each node, but not everything. The additional in-
formation can be attached to the main label, but
such approach obscures the logical structure.
In the Prague family, there are two possible
ways to label a conjunction and conjuncts:
Code dU (?dependency labeled at the upper
level of the CS?). The dependency relation of the
whole CS to its parent is represented by the label
of the conjunction, while the conjuncts are marked
with a special label for conjuncts (e.g. ccof in the
Hyderabad Dependency Treebank).
Code dL (?lower level?). The CS is represented
by a coordinating conjunction (or punctuation if
there is no conjunction) with a special label (e.g.
Coord in PDT). Subsequently, each conjunct has
its own label that reflects the dependency relation
towards the parent of the whole CS, therefore, con-
juncts of the same CS can have different labels,
e.g. ?Who[SUBJ] and why[ADV] did it??
Most Prague family treebanks use sH, i.e.
shared modifiers are attached to the head (coor-
dinating conjunction). Each child of the head has
to belong to one of three sets: conjuncts, shared
modifiers, and punctuation or additional conjunc-
tions. In PDT, conjuncts, punctuation and addi-
tional conjunctions are recognized by specific la-
bels. Any other children of the head are shared
modifiers.
In the Stanford and Moscow families, one of
the conjuncts is the head. In practice, it is never la-
beled as a conjunct explicitly, because the fact that
it is a conjunct can be deduced from the presence
of conjuncts among its children. Usually, the other
conjuncts are labeled as conjuncts; conjunctions
and punctuation also have a special label. This
type of labeling corresponds to the dU type.
Alternatively (as found in the Turkish treebank,
dL), all conjuncts in the Moscow chain have their
own dependency labels and the fact that they are
conjuncts follows from the COORDINATION la-
bels of the conjunction and punctuation nodes be-
tween them.
To represent shared modifiers in the Stan-
521
ford and Moscow families, an additional label
is needed again to distinguish between private
and shared modifiers since they cannot be distin-
guished topologically. Moreover, if nested CSs
are allowed, a binary label is not sufficient (i.e.
?shared? versus ?private?) because it also has to
indicate which conjuncts the shared modifier be-
longs to.14
We use the following binary flag codes for cap-
turing which CS participants are distinguished in
the annotation: m01 = shared modifiers anno-
tated; m10 = conjuncts annotated; m11 = both
annotated; m00 = neither annotated.
4 Coordination Structures in Treebanks
In this section, we identify the CS styles defined
in the previous section as used in the primary tree-
bank data sources; statistical observations (such
as the amount of annotated shared modifiers) pre-
sented here, as well as experiments on CS-style
convertibility presented in Section 5.2, are based
on the normalized shapes of the treebanks as con-
tained in the HamleDT 1.0 treebank collection
(Zeman et al, 2012).15
Some of the treebanks were downloaded indi-
vidually from the web, but most of them came
from previously published collections for depen-
dency parsing campaigns: six languages from
CoNLL-2006 (Buchholz and Marsi, 2006), seven
languages from CoNLL-2007 (Nivre et al, 2007),
two languages from CoNLL-2009 (Hajic? and oth-
ers, 2009), three languages from ICON-2010 (Hu-
sain et al, 2010). Obviously, there is a certain
risk that the CS-related information contained in
the source treebanks was slightly biased by the
properties of the CoNLL format upon conversion.
In addition, many of the treebanks were natively
dependency-based (cf. the 2nd column of Table 1),
but some were originally based on constituents
and thus specific converters to the CoNLL for-
mat had to be created (for instance, the Span-
ish phrase-structure trees were converted to de-
pendencies using a procedure described by Civit
et al (2006); similarly, treebank-specific convert-
ers have been used for other languages). Again,
14This is not needed in Prague family where shared modi-
fiers are attached to the conjunction provided that each shared
modifier is shared by conjuncts that form a full subtree to-
gether with their coordinating conjunctions; no exceptions
were found during the annotation process of the PDT.
15A subset of the treebanks whose license
terms permit redistribution is available directly at
http://ufal.mff.cuni.cz/hamledt/.
Danish Romanian
dogsa
n,  anctttr  attt,

ddog
san,n           cntnrnsn     c,n
Hungarian
dogsadnnnn,nnnn ctrdadnnnnrncgdasd
Figure 2: Annotation styles of a few treebanks do
not fit well into the multidimensional space de-
fined in Section 3.1.
there is some risk that the CS-related information
contained in treebanks resulting from such conver-
sions is slightly different from what was intended
in the very primary annotation.
There are several other languages (e.g. Esto-
nian or Chinese) which are not included in our
study, despite of the fact that constituency tree-
banks do exist for them. The reason is that the
choice of their CS style would be biased, because
no independent converters exist ? we would have
to convert them to dependencies ourselves. We
also know about several more dependency tree-
banks that we have not processed yet.
Table 1 shows 26 languages whose treebanks
we have studied from the viewpoint of their CS
styles. It gives the basic quantitative properties of
the treebanks, their CS style in terms of the tax-
onomy introduced in Section 3, as well as statis-
tics related to CSs: the average number of CSs per
100 tokens, the average number of conjuncts per
one CS, the average number of shared modifiers
per one CS,16 and the percentage of nested CSs
among all CSs. The reader can return to Figure
1 to see the basic statistics on the ?popularity? of
individual design decisions among the developers
of dependency treebanks or constituency treebank
converters.
CS styles of most treebanks are easily classifi-
able using the codes introduced in Section 3, plus
a few additional codes:
? p0 = punctuation was removed from the tree-
bank.
16All non-Prague family treebanks are marked sN and
m00 or m10, (i.e. shared modifiers not marked in the origi-
nal annotation, but attached to the head conjunct) because we
found no counterexamples (modifiers attached to a conjunct,
but not the nearest one). The HamleDT normalization proce-
dure contains a few heuristics to detect shared modifiers, but
it cannot recover the missing distinction reliably, so the num-
bers in the ?SMs/CJ? column are mostly underestimated.
522
Language Orig. Data Sents. Tokens Original CS CSs / CJs / SMs / Nested RT
type set style code 100 tok. CS CS CS[%] UAS
Ancient
Greek dep prim. 31 316 461 782 fP hR sH cH pB dL m11 6.54 2.17 0.16 10.3 97.86
Arabic dep C07 3 043 116 793 fP hL sH cH pB dL m00 3.76 2.42 0.13 10.6 96.69
Basque dep prim. 11 225 151 593 fP hR sN cH pP dU m00 3.37 2.09 0.03 5.1 99.32
Bengali dep I10 1 129 7 252 fP hR sH cH pP dU m11 4.87 1.71 0.05 24.1 99.97
Bulgarian phr C06 13 221 196 151 fS hL sN cB pB dU m10 2.99 2.19 0.00 0.0 99.74
Czech dep C07 25 650 437 020 fP hR sH cH pB dL m11 4.09 2.16 0.20 14.6 99.42
Danish dep C06 5 512 100 238 fS* hL sN cP pB dU m10 3.68 1.93 0.13 7.5 99.76
Dutch phr C06 13 735 200 654 fP hR sN cH pP dU m10 2.06 2.17 0.05 3.3 99.47
English phr C07 40 613 991 535 fP hR sH cH pB dU m10 2.07 2.33 0.05 6.3 99.84
Finnish dep prim. 4 307 58 576 fS hL sN cB pB dU m10 4.06 2.41 0.00 6.4 99.70
German phr C09 38 020 680 710 fM hL sN cP pP dU m10 2.79 2.09 0.01 0.0 99.73
Greek dep C07 2 902 70 223 fP hR sH cH pB dL m11 3.25 2.48 0.18 7.2 99.43
Hindi dep I10 3 515 77 068 fP hR sH cH pP dU m11 2.45 1.97 0.04 10.3 98.35
Hungarian phr C07 6 424 139 143 fT hX sN cX pX dL m00 2.37 1.90 0.01 2.2 99.84
Italian dep C07 3 359 76 295 fS hL sN cB pB dU m10 3.32 2.02 0.03 3.8 99.51
Latin dep prim. 3 473 53 143 fP hR sH cH pB dL m11 6.74 2.24 0.41 12.3 97.45
Persian dep prim. 12 455 189 572 fM*hM sN cB pP dU m00 4.18 2.10 0.18 3.7 99.82
Portuguese phr C06 9 359 212 545 fS hL sN cB pB dU m10 2.51 1.95 0.26 11.1 99.16
Romanian dep prim. 4 042 36 150 fP* hR sN cH p0 dU m10 1.80 2.00 0.00 0.0 100.00
Russian dep prim. 34 895 497 465 fM hL sN cB p0 dU m10 4.02 2.02 0.07 3.9 99.86
Slovene dep C06 1 936 35 140 fP hR sH cH pB dL m00 4.31 2.49 0.00 10.8 98.87
Spanish phr C09 15 984 477 810 fS hL sN cB pB dU m10 2.79 1.98 0.14 12.7 99.24
Swedish phr C06 11 431 197 123 fM hL sN cF pF dU m10 3.94 2.19 0.13 0.7 99.66
Tamil dep prim. 600 9 581 fP hR sH cH pB dL m11 1.66 2.46 0.22 3.8 99.67
Telugu dep I10 1 450 5 722 fP hR sH cH pP dU m11 3.48 1.59 0.06 5.0 100.00
Turkish dep C07 5 935 69 695 fM hR sN cB pB dL m10 3.81 2.04 0.00 34.3 99.23
Table 1: Overview of analyzed treebanks. prim. = primary source; C06?C09 = CoNLL 2006?2009;
I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in
nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip
experiment described in Section 5. Style codes are defined in Sections 3 and 4.
? fM* = Persian treebank uses a mix of fM and
fS: fS for coordination of verbs and fM oth-
erwise.
Figure 2 shows three other anomalies:
? fS* = Danish treebank employs a mixture of
fS and fM, where the last conjunct is attached
indirectly via the conjunction.
? fP* = Romanian treebank omits punctuation
tokens and multi-conjunct coordinations get
split.
? fT = Hungarian Szeged treebank uses
?Tesnie`re family? ? disconnected graphs for
CSs where conjuncts (and conjunction and
punctuation) are attached directly to the par-
ent of CS, and so the other style dimensions
are not applicable (hX, cX, pX).
5 Empirical Observations on
Convertibility of Coordination Styles
The various styles cannot represent the CS-related
information to the same extent. For example,
it is not possible to represent nested CSs in the
Moscow and Stanford families without signifi-
cantly changing the number of possible labels.17
The dL style (which is most easily applicable to
the Prague family) can represent coordination of
different dependency relations. This is again not
possible in the other styles without adding e.g. a
special ?prefix? denoting the relations.
We can see that the Prague family has a greater
expressive power than the other two families: it
can represent complex CSs using just one addi-
tional binary label, distinguishing between shared
modifiers and conjuncts. A similar additional label
is needed in the other styles to distinguish between
shared and private modifiers.
Because of the different expressive power, con-
verting a CS from one style to another may
lead to a loss of information. For example, as
17Mel?c?uk uses ?grouping? to nest CSs ? cf. related so-
lutions involving coindexing or bubble trees (Kahane, 1997).
However, these approaches were not used in any of the re-
searched treebanks. To combine grouping with shared modi-
fiers, each group in a tree should have a different identifier.
523
there is no way of representing shared modifiers
in the Moscow family without an additional at-
tribute, converting a CS with shared modifiers
from Prague to Moscow family makes the modi-
fiers private. When converting back, one can use
certain heuristics to handle the most obvious cases,
but sometimes the modifiers will stay private (very
often, the nature of a modifier depends on context
or is debatable even for humans, e.g. ?Young boys
and girls?).
5.1 Transformation algorithm
We developed an algorithm to transform one CS
style to another. Two subtasks must be solved by
the algorithm: identification of individual CSs and
their participants, and transforming of the individ-
ual CSs.
Obviously, the individual CSs cannot be trans-
formed independently because of coordination
nesting. For instance, when transforming a nested
coordination from the Prague style to the Moscow
style (e.g. to fMhL), the leftmost conjunct in the
inner (lower) coordination must climb up to be-
come the head of the inner CS, but then it must
climb up once again to become the head of the
outer (upper) CS too. This shows that inner CSs
must be transformed first.
We tackle this problem by a depth-first recur-
sion. When going down the tree, we only recog-
nize all the participants of the CSs, classify them
and gather them in a separate data structure (one
for each visited CS). The following four types
of CS participants are distinguished: coordinat-
ing conjunctions, conjuncts, shared modifiers, and
punctuations that separate conjuncts.18 No change
of the tree is performed during these descent steps.
When returning back from the recursion (i.e.,
when climbing from a node back up to its par-
ent), we test whether the abandoned node is the
topmost node of some CS. If so, then this CS is
transformed, which means that its participants are
rehanged and relabelled according the the target
CS style.
This procedure naturally guarantees that the in-
18Conjuncts are explicitly marked in most styles. Coordi-
nating conjunctions can be usually identified with the help of
dependency labels and POS tags. Punctuation separating con-
juncts can be detected with high accuracy using simple rules.
If shared modifiers are not annotated (code m00 or m10),
one can imagine rule-based heuristics or special classifiers
trained to distinguish shared modifiers. For the experiments
in this section, we use the HamleDT gold annotation attribute
is shared modifier.
ner CSs are transformed first and that all CSs are
transformed when the recursions returns to the
root.
5.2 Roundtrip experiment
The number of possible conversion directions ob-
viously grows quadratically with the number of
styles. So far, we limited ourselves only to con-
versions from/to the style of the HamleDT tree-
bank collection, which contains all the treebanks
under our study already converted into a com-
mon scheme. The common scheme is based
on the conventions of PDT, whose CS style is
fPhRsHcHpB.19
We selected nine styles (3 families times 3 head
choices) and transformed all the HamleDT scheme
treebanks to these nine styles and back, which we
call a roundtrip. Resulting averaged unlabeled at-
tachment scores (UAS, evaluated against the Ham-
leDT scheme) in the last column of Table 1 indi-
cate that the percentage of transformation errors
(i.e. tokens attached to a different parent after the
roundtrip) is lower than 1% for 20 out of the 26
languages.20 A manual inspection revealed two
main error sources. First, as noted above, the Stan-
ford and Moscow families have lower expressive
power than the Prague family, so naturally, the in-
verse transformation was ambiguous and the trans-
formation heuristics were not capable of identify-
ing the correct variant every time. Second, we also
encountered inconsistencies in the original tree-
banks (which we were not trying to fix in Ham-
leDT for now).
6 Conclusions and Future Work
We described a (theoretically very large) space of
possible representations of CSs within the depen-
dency framework. We pointed out a range of de-
tails that make CSs a really complex phenomenon;
anyone dealing with CSs in treebanking should
take these observations into account.
We proposed a taxonomy of those approaches
19As documented in Zeman et al (2012), the normalization
procedures used in HamleDT embrace many other phenom-
ena as well (not only those related to coordination), and in-
volve both structural transformation and dependency relation
relabeling.
20Table 1 shows that Latin and Ancient Greek treebanks
have on average more than 6 CSs per 100 tokens, more than
2 conjuncts per CS, and Latin has also the highest number of
shared modifiers per CS. Therefore the percentage of nodes
affected by the roundtrip is the highest for these languages
and the lower roundtrip UAS is not surprising.
524
that have been argued for in literature or employed
in real treebanks.
We studied 26 existing treebanks of different
languages. For each value of each dimension in
Figure 1, we found at least one treebank where the
value is used; even so, several treebanks take their
own unique path that cannot be clearly classified
under the taxonomy (the taxonomy could indeed
be extended, for the price of being less clearly ar-
ranged).
We discussed the convertibility between the var-
ious styles and implemented a universal tool that
transforms between any two styles of the taxon-
omy. The tool achieves a roundtrip accuracy close
to 100%. This is important because it opens the
door to easily switching coordination styles for
parsing experiments, phrase-to-dependency con-
version etc.
While the focus of this paper is to explore and
describe the expressive power of various annota-
tion styles, we did not address the learnability of
the styles by parsers. That will be a complemen-
tary point of view, and thus a natural direction of
future work for us.
Acknowledgments
We thank the providers of the primary data re-
sources. The work on this project was sup-
ported by the Czech Science Foundation grants
no. P406/11/1499 and P406/2010/0875, and by
research resources of the Charles University in
Prague (PRVOUK). This work has been using lan-
guage resources developed and/or stored and/or
distributed by the LINDAT-Clarin project of the
Ministry of Education of the Czech Republic
(project LM2010013). Further, we would like to
thank Jan Hajic?, Ondr?ej Dus?ek and four anony-
mous reviewers for many useful comments on the
manuscript of this paper.
References
Itzair Aduriz et al 2003. Construction of a Basque de-
pendency treebank. In Proceedings of the 2nd Work-
shop on Treebanks and Linguistic Theories.
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In LREC, pages 1968?1703.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank. In
Proceedings of the 4th Intern. Workshop on Linguis-
tically Interpreteted Corpora (LINC).
David Bamman and Gregory Crane. 2011. The An-
cient Greek and Latin dependency treebanks. In
Language Technology for Cultural Heritage, Theory
and Applications of Natural Language Processing,
pages 79?98. Springer Berlin Heidelberg.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency treebank for Russian: Concept, tools,
types of information. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 987?991. Association for Computational Lin-
guistics Morristown, NJ, USA.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Montserrat Civit, Maria Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: From constituents to
dependencies. In FinTAL, volume 4139 of Lec-
ture Notes in Computer Science, pages 141?152.
Springer.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged treebank. In
TSD, volume 3658 of Lecture Notes in Computer
Science, pages 123?131. Springer.
Mihaela Ca?la?cean. 2008. Data-driven dependency
parsing for Romanian. Master?s thesis, Uppsala
University, August.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k Z?abokrtsky?, and Andreja Z?ele. 2006.
Towards a Slovene dependency treebank. In LREC
2006, pages 1388?1391, Genova, Italy. European
Language Resources Association (ELRA).
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hy-
brid combination of constituency and dependency
trees into an ensemble dependency parser. In Pro-
ceedings of the Workshop on Innovative Hybrid Ap-
proaches to the Processing of Textual Data, pages
19?26, Avignon, France. Association for Computa-
tional Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova?-Raz??mova?. 2006. Prague Dependency
Treebank 2.0. CD-ROM, Linguistic Data Consor-
tium, LDC Catalog No.: LDC2006T01, Philadel-
phia.
525
Jan Hajic? et al 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in mul-
tiple languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
Jirka Hana and Jan S?te?pa?nek. 2012. Prague
markup language framework. In Proceedings of the
Sixth Linguistic Annotation Workshop, pages 12?
21, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics, Association for Computational
Linguistics.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking Finnish. In Proceedings of
the Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9), pages 79?90.
Samar Husain, Prashanth Mannem, Bharat Ambati,
and Phani Gadde. 2010. The ICON-2010 tools
contest on Indian language dependency parsing. In
Proceedings of ICON-2010 Tools Contest on Indian
Language Dependency Parsing, Kharagpur, India.
ISO 24615. 2010. Language resource management ?
Syntactic annotation framework (SynAF).
Sylvain Kahane. 1997. Bubble trees and syntactic
representations. In Proceedings of the 5th Meeting
of the Mathematics of the Language, DFKI, Saar-
brucken.
Matthias T. Kromann, Line Mikkelsen, and Stine Kern
Lynge. 2004. Danish dependency treebank.
Sandra Ku?bler, Erhard Hinrichs, Wolfgang Maier, and
Eva Klett. 2009. Parsing coordinations. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 406?414,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Vincenzo Lombardo and Leonardo Lesmo. 1998. Unit
coordination and gapping in dependency theory. In
Processing of Dependency-Based Grammars; pro-
ceedings of the workshop. COLING-ACL, Montreal.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Nicolar Mazziotta. 2011. Coordination of verbal de-
pendents in Old French: Coordination as a specified
juxtaposition or apposition. In Proceedings of In-
ternational Conference on Dependency Linguistics
(DepLing 2011).
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Simonetta Montemagni et al 2003. Building the Ital-
ian syntactic-semantic treebank. In Building and us-
ing Parsed Corpora, Language and Speech series,
pages 189?210, Dordrecht. Kluwer.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of the
NODALIDA Special Session on Treebanks.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
2007 Shared Task. EMNLP-CoNLL, June.
Martin Popel and Zdene?k Z?abokrtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Prokopis Prokopidis, Elina Desipri, Maria Koutsom-
bogera, Harris Papageorgiou, and Stelios Piperidis.
2005. Theoretical and practical issues in the con-
struction of a Greek dependency treebank. In Pro-
ceedings of the 4th Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 149?160.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2012.
Prague dependency style treebank for Tamil. In
Proceedings of LREC 2012, pages 23?25, I?stanbul,
Turkey. European Language Resources Association.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
Persian verbs: The first steps towards Persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231, Poznan?, Poland.
Kiril Simov and Petya Osenova. 2005. Extending
the annotation of BulTreeBank: Phase 2. In The
Fourth Workshop on Treebanks and Linguistic Theo-
ries (TLT 2005), pages 173?184, Barcelona, Decem-
ber.
Otakar Smrz?, Viktor Bielicky?, Iveta Kour?ilova?, Jakub
Kra?c?mar, Jan Hajic?, and Petr Zema?nek. 2008.
Prague Arabic dependency treebank: A word on the
million words. In Proceedings of the Workshop on
Arabic and Local Languages (LREC) 2008, pages
16?23, Marrakech, Morocco. European Language
Resources Association.
Leon Stassen. 2000. And-languages and with-
languages. Linguistic Typology, 4(1):1?54.
Jan S?te?pa?nek. 2006. Capturing a Sentence Struc-
ture by a Dependency Relation in an Annotated Syn-
tactical Corpus (Tools Guaranteeing Data Consis-
tence) (in Czech). Ph.D. thesis, Charles Univer-
526
sity in Prague, Faculty of Mathematics and Physics,
Prague, Czech Republic.
Pavel Stran?a?k and Jan S?te?pa?nek. 2010. Represent-
ing layered and structured data in the CoNLL-ST
format. In Alex Fang, Nancy Ide, and Jonathan
Webster, editors, Proceedings of the Second Inter-
national Conference on Global Interoperability for
Language Resources, pages 143?152, Hong Kong,
China. City University of Hong Kong, City Univer-
sity of Hong Kong.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel annotated cor-
pora for Catalan and Spanish. In LREC. European
Language Resources Association.
TEI Consortium. 2013. TEI P5: Guidelines for Elec-
tronic Text Encoding and Interchange.
Lucien Tesnie`re. 1959. Ele?ments de syntaxe struc-
turale. Paris.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP, pages 1257?1268, Edin-
burgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Leonoor van der Beek et al 2002. Chapter 5. The
Alpino dependency treebank. In Algorithms for Lin-
guistic Processing NWO PIONIER Progress Report,
Groningen, The Netherlands.
Daniel Zeman, David Marec?ek, Martin Popel,
Loganathan Ramasamy, Jan S?te?pa?nek, Zdene?k
Z?abokrtsky?, and Jan Hajic?. 2012. HamleDT: To
parse or not to parse? In Proceedings of LREC 2012,
pages 2735?2741, I?stanbul, Turkey. European Lan-
guage Resources Association.
527
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201?206,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Maximum Entropy Translation Model
in Dependency-Based MT Framework
David Marec?ek, Martin Popel, Zdene?k Z?abokrtsky?
Charles University in Prague, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{marecek,popel,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Maximum Entropy Principle has been
used successfully in various NLP tasks. In
this paper we propose a forward transla-
tion model consisting of a set of maxi-
mum entropy classifiers: a separate clas-
sifier is trained for each (sufficiently fre-
quent) source-side lemma. In this way
the estimates of translation probabilities
can be sensitive to a large number of fea-
tures derived from the source sentence (in-
cluding non-local features, features mak-
ing use of sentence syntactic structure,
etc.). When integrated into English-to-
Czech dependency-based translation sce-
nario implemented in the TectoMT frame-
work, the new translation model signif-
icantly outperforms the baseline model
(MLE) in terms of BLEU. The perfor-
mance is further boosted in a configuration
inspired by Hidden Tree Markov Mod-
els which combines the maximum entropy
translation model with the target-language
dependency tree model.
1 Introduction
The principle of maximum entropy states that,
given known constraints, the probability distri-
bution which best represents the current state of
knowledge is the one with the largest entropy.
Maximum entropy models based on this princi-
ple have been widely used in Natural Language
Processing, e.g. for tagging (Ratnaparkhi, 1996),
parsing (Charniak, 2000), and named entity recog-
nition (Bender et al, 2003). Maximum entropy
models have the following form
p(y|x) =
1
Z(x)
exp
?
i
?ifi(x, y)
where fi is a feature function, ?i is its weight, and
Z(x) is the normalizing factor
Z(x) =
?
y
exp
?
i
?ifi(x, y)
In statistical machine translation (SMT), trans-
lation model (TM) p(t|s) is the probability that the
string t from the target language is the translation
of the string s from the source language. Typical
approach in SMT is to use backward translation
model p(s|t) according to Bayes? rule and noisy-
channel model. However, in this paper we deal
only with the forward (direct) model.1
The idea of using maximum entropy for con-
structing forward translation models is not new. It
naturally allows to make use of various features
potentially important for correct choice of target-
language expressions. Let us adopt a motivat-
ing example of such a feature from (Berger et al,
1996) (which contains the first usage of maxent
translation model we are aware of): ?If house ap-
pears within the next three words (e.g., the phrases
in the house and in the red house), then dans might
be a more likely [French] translation [of in].?
Incorporating non-local features extracted from
the source sentence into the standard noisy-
channel model in which only the backward trans-
lation model is available, is not possible. This
drawback of the noisy-channel approach is typi-
cally compensated by using large target-language
n-gram models, which can ? in a result ? play a
role similar to that of a more elaborate (more con-
text sensitive) forward translation model. How-
ever, we expect that it would be more beneficial to
exploit both the parallel data and the monolingual
data in a more balance fashion, rather than extract
only a reduced amount of information from the
parallel data and compensate it by large language
model on the target side.
1A backward translation model is used only for pruning
training data in this paper.
201
A deeper discussion on the potential advantages
of maximum entropy approach over the noisy-
channel approach can be found in (Foster, 2000)
and (Och and Ney, 2002), in which another suc-
cessful applications of maxent translation models
are shown. Log-linear translation models (instead
of MLE) with rich feature sets are used also in
(Ittycheriah and Roukos, 2007) and (Gimpel and
Smith, 2009); the idea can be traced back to (Pap-
ineni et al, 1997).
What makes our approach different from the
previously published works is that
1. we show how the maximum entropy trans-
lation model can be used in a dependency
framework; we use deep-syntactic depen-
dency trees (as defined in the Prague Depen-
dency Treebank (Hajic? et al, 2006)) as the
transfer layer,
2. we combine the maximum entropy transla-
tion model with target-language dependency
tree model and use tree-modified Viterbi
search for finding the optimal lemmas label-
ing of the target-tree nodes.
The rest of the paper is structured as follows. In
Section 2 we give a brief overview of the trans-
lation framework TectoMT in which the experi-
ments are implemented. In Section 3 we describe
how our translation models are constructed. Sec-
tion 4 summarizes the experimental results, and
Section 5 contains a summary.
2 Translation framework
We use tectogrammatical (deep-syntactic) layer of
language representation as the transfer layer in the
presented MT experiments. Tectogrammatics was
introduced in (Sgall, 1967) and further elaborated
within the Prague Dependency Treebank project
(Hajic? et al, 2006). On this layer, each sentence
is represented as a tectogrammatical tree, whose
main properties (from the MT viewpoint) are fol-
lowing: (1) nodes represent autosemantic words,
(2) edges represent semantic dependencies (a node
is an argument or a modifier of its parent), (3) there
are no functional words (prepositions, auxiliary
words) in the tree, and the autosemantic words ap-
pear only in their base forms (lemmas). Morpho-
logically indispensable categories (such as number
with nouns or tense with verbs, but not number
with verbs as it is only imposed by agreement) are
stored in separate node attributes (grammatemes).
The intuition behind the decision to use tec-
togrammatics for MT is the following: we be-
lieve that (1) tectogrammatics largely abstracts
from language-specific means (inflection, agglu-
tination, functional words etc.) of expressing
non-lexical meanings and thus tectogrammatical
trees are supposed to be highly similar across lan-
guages,2 (2) it enables a natural transfer factor-
ization,3 (3) and local tree contexts in tectogram-
matical trees carry more information (especially
for lexical choice) than local linear contexts in the
original sentences.4
In order to facilitate transfer of sentence ?syn-
tactization?, we work with tectogrammatical nodes
enhanced with the formeme attribute (Z?abokrtsky?
et al, 2008), which captures the surface mor-
phosyntactic form of a given tectogrammatical
node in a compact fashion. For example, the
value n:pr?ed+4 is used to label semantic nouns
that should appear in an accusative form in a
prepositional group with the preposition pr?ed in
Czech. For English we use formemes such as
n:subj (semantic noun (SN) in subject position),
n:for+X (SN with preposition for), n:X+ago (SN
with postposition ago), n:poss (possessive form of
SN), v:because+fin (semantic verb (SV) as a sub-
ordinating finite clause introduced by because),
v:without+ger (SV as a gerund after without), adj:attr
(semantic adjective (SA) in attributive position),
adj:compl (SA in complement position).
We have implemented our experiments in the
TectoMT software framework, which already of-
fers tool chains for analysis and synthesis of Czech
and English sentences (Z?abokrtsky? et al, 2008).
The translation scenario proceeds as follows.
1. The input English text is segmented into sen-
tences and tokens.
2. The tokens are lemmatized and tagged with
Penn Treebank tags using the Morce tagger
(Spoustova? et al, 2007).
2This claim is supported by error analysis of output of
tectogrammatics-based MT system presented in (Popel and
Z?abok/rtsky?, 2009), which shows that only 8 % of translation
errors are caused by the (obviously too strong) assumption
that the tectogrammatical tree of a sentence and the tree rep-
resenting its translation are isomorphic.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
?denser?, especially when translating from/to a language with
rich inflection such as Czech.
4Recall the house-is-somewhere-around feature in the in-
troduction; again, the fact that we know the dominating (or
dependent) word should allow to construct a more compact
translation model, compared to n-gram models.
202
Figure 1: Intermediate sentence representations when translating the English sentence ?However, this
very week, he tried to find refuge in Brazil.?, leading to the Czech translation ?Pr?esto se tento pra?ve?
ty?den snaz?il naj??t u?toc?is?te? v Braz??lii.?.
3. Then the Maximum Spanning Tree parser
(McDonald et al, 2005) is applied and a
surface-syntax dependency tree (analytical
tree in the PDT terminology) is created for
each sentence (Figure 1a).
4. This tree is converted to a tectogrammatical
tree (Figure 1b). Each autosemantic word
with its associated functional words is col-
lapsed into a single tectogrammatical node,
labeled with lemma, formeme, and seman-
tically indispensable morphologically cate-
gories; coreference is also resolved. Collaps-
ing edges are depicted by wider lines in the
Figure 1a.
5. The transfer phase follows, whose most dif-
ficult part consists in labeling the tree with
target-side lemmas and formemes5 (changes
of tree topology are required relatively infre-
quently). See Figure 1c.
6. Finally, surface sentence shape (Figure 1d) is
synthesized from the tectogrammatical tree,
which is basically a reverse operation for the
5In this paper we focus on using maximum entropy
for translating lemmas, but it can be used for translating
formemes as well.
tectogrammatical analysis: adding punctua-
tion and functional words, spreading mor-
phological categories according to grammat-
ical agreement, performing inflection (using
Czech morphology database (Hajic?, 2004)),
arranging word order etc.
3 Training the two models
In this section we describe two translation mod-
els used in the experiments: a baseline translation
model based on maximum likelihood estimates
(3.2), and a maximum entropy based model (3.3).
Both models are trained using the same data (3.1).
In addition, we describe a target-language tree
model (3.4), which can be combined with both
the translation models using the Hidden Tree
Markov Model approach and tree-modified Viterbi
search, similarly to the approach of (Z?abokrtsky?
and Popel, 2009).
3.1 Data preprocessing common for both
models
We used Czech-English parallel corpus CzEng 0.9
(Bojar and Z?abokrtsky?, 2009) for training the
translation models. CzEng 0.9 contains about
8 million sentence pairs, and also their tectogram-
matical analyses and node-wise alignment.
203
We used only trees from training sections (about
80 % of the whole data), which contain around 30
million pairs of aligned tectogrammatical nodes.
From each pair of aligned tectogrammatical
nodes, we extracted triples containing the source
(English) lemma, the target (Czech) lemma, and
the feature vector.
In order to reduce noise in the training data,
we pruned the data in two ways. First, we dis-
regarded all triples whose lemma pair did not oc-
cur at least twice in the whole data. Second,
we computed forward and backward maximum
likelihood (ML) translation models (target lemma
given source lemma and vice versa) and deleted
all triples whose probability according to one of
the two models was lower than the threshold 0.01.
Then the forward ML translation model was
reestimated using only the remaining data.
For a given pair of aligned nodes, the feature
vector was of course derived only from the source-
side node or from the tree which it belongs to. As
already mentioned in the introduction, the advan-
tage of the maximum entropy approach is that a
rich and diverse set of features can be used, with-
out limiting oneself to linearly local context. The
following features (or, better to say, feature tem-
plates, as each categorical feature is in fact con-
verted to a number of 0-1 features) were used:
? formeme and morphological categories of the
given node,
? lemma, formeme and morphological cate-
gories of the governing node,
? lemmas and formemes of all child nodes,
? lemmas and formemes of the nearest linearly
preceding and following nodes.
3.2 Baseline translation model
The baseline TM is basically the ML translation
model resulting from the previous section, lin-
early interpolated with several translation models
making use of regular word-formative derivations,
which can be helpful for translating some less fre-
quent (but regularly derived) lemmas. For exam-
ple, one of the derivation-based models estimates
the probability p(zaj??mave?|interestingly) (possibly
unseen pair of deadjectival adverbs) by the value
of p(zaj??mavy?|interesting). More detailed descrip-
tion of these models goes beyond the scope of this
paper; their weights in the interpolation are very
small anyway.
3.3 MaxEnt translation model
The MaxEnt TM was created as follows:
1. training triples (source lemma, target lemma,
feature vector) were disregarded if the source
lemma was not seen at least 50 times (only
the baseline model will be used for such lem-
mas),
2. the remaining triples were grouped by the En-
glish lemma (over 16 000 groups),
3. due to computational issues, the maximum
number of triples in a group was reduced to
1000 by random selection,
4. a separate maximum entropy classifier
was trained for each group (i.e., one
classifier per source-side lemma) using
AI::MaxEntropy Perl module,6
5. due to the more aggressive pruning of the
training data, coverage of this model is
smaller than that of the baseline model; in or-
der not to loose the coverage, the two mod-
els were combined using linear interpolation
(1:1).
Selected properties of the maximum entropy
translation model (before the linear interpolation
with the baseline model) are shown in Figure 2.
We increased the size of the training data from
10 000 training triples up to 31 million and eval-
uated three relative quantities characterizing the
translation models:
? coverage - relative frequency of source lem-
mas for which the translation model offers at
least one translation,
? first - relative frequency of source lemmas for
which the target lemmas offered as the first
by the model (argmax) are the correct ones,
? oracle - relative frequency of source lemmas
for which the correct target lemma is among
the lemmas offered by the translation model.
As mentioned in Section 3.1, there are context
features making use both of local linear context
and local tree context. After training the MaxEnt
model, there are about 4.5 million features with
non-zero weight, out of which 1.1 million features
6http://search.cpan.org/perldoc?AI::
MaxEntropy
204
Figure 2: Three measures characterizing the Max-
Ent translation model performance, depending on
the training data size. Evaluated on aligned node
pairs from the dtest portion of CzEng 0.9.
are derived from the linear context and 2.4 million
features are derived from the tree context. This
shows that the MaxEnt translation model employs
the dependency structure intensively.
A preliminary analysis of feature weights seems
to support our intuition that the linear context
is preferred especially in the case of more sta-
ble collocations. For example, the most impor-
tant features for translating the lemma bare are
based on the lemma of the following noun: tar-
get lemma bosy? (barefooted) is preferred if the fol-
lowing noun on the source side is foot, while holy?
(naked, unprotected) is preferred if hand follows.
The contribution of dependency-based features
can be illustrated on translating the word drop.
The greatest weight for choosing kapka (a droplet)
as the translation is assigned to the feature captur-
ing the presence of a node with formeme n:of+X
among the node?s children. The greatest weights
in favor of odhodit (throw aside) are assigned to
features capturing the presence of words such as
gun or weapon, while the greatest weights in favor
of klesnout (to come down) are assigned to fea-
tures saying that there is the lemma percent or the
percent sign among the children.
Of course, the lexical choice is influenced also
by the governing lemmas, as can be illustrated
with the word native. One can find a high-
value feature for rodily? (native-born) saying that
the source-side parent is speaker; similarly for
mater?sky? (mother) with governing tongue, and
rodny? (home) with land.
Linear and tree features are occasionally used
simultaneously: there are high-valued positive
configuration BLEU NIST
baseline TM 10.44 4.795
MaxEnt TM 11.77 5.135
baseline TM + TreeLM 11.77 5.038
MaxEnt TM + TreeLM 12.58 5.250
Table 1: BLEU and NIST evaluation of four con-
figurations of our MT system; the WMT 2010 test
set was used.
weights for translating order as objednat (reserve,
give an order for st.) assigned both to tree-based
features saying that there are words such as pizza,
meal or goods and to linear features saying that the
very following word is some or two.
3.4 Target-language tree model
Although the MaxEnt TM captures some contex-
tual dependencies that are covered by language
models in the standard noisy-channel SMT, it may
still be beneficial to exploit target-language mod-
els, because these can be trained on huge mono-
lingual corpora. We use a target-language depen-
dency tree model differing from standard n-gram
model in two aspects:
? it uses tree context instead of linear context,
? it predicts tectogrammatical attributes (lem-
mas and formemes) instead of word forms.
In particular, our target-language tree model
(TreeLM) predicts the probability of node?s
lemma and formeme given its parent?s lemma and
formeme. The optimal (lemma and formeme) la-
beling is found by tree-modified Viterbi search;
for details see (Z?abokrtsky? and Popel, 2009).
4 Experiments
When included into the above described transla-
tion scenario, the MaxEnt TM outperforms the
baseline TM, be it used together with or with-
out TreeLM. The results are summarized in Ta-
ble 1. The improvement is statistically signif-
icant according to paired bootstrap resampling
test (Koehn, 2004). In the configuration without
TreeLM the improvement is greater (1.33 BLEU)
than with TreeLM (0.81 BLEU), which confirms
our hypothesis that MaxEnt TM captures some of
the contextual dependencies resolved otherwise by
language models.
205
5 Conclusions
We have introduced a maximum entropy transla-
tion model in dependency-based MT which en-
ables exploiting a large number of feature func-
tions in order to obtain more accurate translations.
The BLEU evaluation proved significant improve-
ment over the baseline solution based on the trans-
lation model with maximum likelihood estimates.
However, the performance of this system still be-
low the state of the art (which is around BLEU 16
for the English-to-Czech direction).
Acknowledgments
This research was supported by the grants
MSM0021620838, MS?MT C?R LC536, FP7-ICT-
2009-4-247762 (Faust), FP7-ICT-2007-3-231720
(EuroMatrix Plus), GA201/09/H057, and GAUK
116310. We thank two anonymous reviewers for
helpful comments.
References
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proceedings of CoNLL 2003, pages
148?151.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathemati-
cal Linguistics, 92:63?83.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the ACL conference, pages
132?139, San Francisco, USA.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52, Morristown, USA.
Association for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2009. Feature-
rich translation by quasi-synchronous lattice pars-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 219?228, Morristown, USA. Association for
Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Candace L. Sidner, Tanja
Schultz, Matthew Stone, and ChengXiang Zhai, edi-
tors, HLT-NAACL, pages 57?64. The Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, volume 4, pages 388?395.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT / EMNLP, pages 523?530, Vancouver,
Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295?302.
Kishore A. Papineni, Salim Roukos, and Todd R.
Ward. 1997. Feature-based language understand-
ing. In European Conference on Speech Commu-
nication and Technology (EUROSPEECH), pages
1435?1438, Rhodes, Greece, September.
Martin Popel and Zdene?k Z?abok/rtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In In Proceedings
of EMNLP?96, pages 133?142.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k Z?abokrtsky? and Martin Popel. 2009. Hidden
markov tree model in dependency-based machine
translation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 145?148, Sun-
tec, Singapore.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL, pages 167?170.
206
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 433?439,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Influence of Parser Choice on Dependency-Based MT
Martin Popel, David Marec?ek, Nathan Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{popel,marecek,green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Accuracy of dependency parsers is one of the
key factors limiting the quality of dependency-
based machine translation. This paper deals
with the influence of various dependency pars-
ing approaches (and also different training
data size) on the overall performance of an
English-to-Czech dependency-based statisti-
cal translation system implemented in the
Treex framework. We also study the relation-
ship between parsing accuracy in terms of un-
labeled attachment score and machine transla-
tion quality in terms of BLEU.
1 Introduction
In the last years, statistical n-gram models domi-
nated the field of Machine Translation (MT). How-
ever, their results are still far from perfect. Therefore
we believe it makes sense to investigate alternative
statistical approaches. This paper is focused on an
analysis-transfer-synthesis translation system called
TectoMT whose transfer representation has a shape
of a deep-syntactic dependency tree. The system has
been introduced by Z?abokrtsky? et al (2008). The
translation direction under consideration is English-
to-Czech.
It has been shown by Popel (2009) that the current
accuracy of the dependency parser employed in this
translation system is one of the limiting factors from
the viewpoint of its output quality. In other words,
the parsing phase is responsible for a large portion
of translation errors. The biggest source of trans-
lation errors in the referred study was (and prob-
ably still is) the transfer phase, however the pro-
portion has changed since and the relative impor-
tance of the parsing phase has grown, because the
tranfer phase errors have already been addressed by
improvements based on Hidden Markov Tree Mod-
els for lexical and syntactic choice as shown by
Z?abokrtsky? and Popel (2009), and by context sensi-
tive translation models based on maximum entropy
as described by Marec?ek et al (2010).
Our study proceeds along two directions. First,
we train two state-of-the-art dependency parsers on
training sets with varying size. Second, we use
five parsers based on different parsing techniques.
In both cases we document the relation between
parsing accuracy (in terms of Unlabeled Attachment
Score, UAS) and translation quality (estimated by
the well known BLEU metric).
The motivation behind the first set of experiments
is that we can extrapolate the learning curve and try
to predict how new advances in dependency parsing
can affect MT quality in the future.
The second experiment series is motivated by
the hypothesis that parsers based on different ap-
proaches are likely to have a different distribution
of errors, even if they can have competitive perfor-
mance in parsing accuracy. In dependency parsing
metrics, all types of incorrect edges typically have
the same weight,1 but some incorrect edges can be
more harmful than others from the MT viewpoint.
For instance, an incorrect attachment of an adverbial
node is usually harmless, while incorrect attachment
of a subject node might have several negative conse-
1This issue has been tackled already in the parsing literature;
for example, some authors disregard placement of punctuation
nodes within trees in the evaluation (Zeman, 2004).
433
quences such as:
? unrecognized finiteness of the governing verb,
which can lead to a wrong syntactization on the
target side (an infinitive verb phrase instead of
a finite clause),
? wrong choice of the target-side verb form (be-
cause of unrecognized subject-predicate agree-
ment),
? missing punctuation (because of wrongly rec-
ognized finite clause boundaries),
? wrong placement of clitics (because of wrongly
recognized finite clause boundaries),
? wrong form of pronouns (personal and posses-
sive pronouns referring to the clause?s subject
should have reflexive forms in Czech).
Thus it is obvious that the parser choice is im-
portant and that it might not be enough to choose a
parser, for machine translation, only according to its
UAS.
Due to growing popularity of dependency syntax
in the last years, there are a number of dependency
parsers available. The present paper deals with
five parsers evaluated within the translation frame-
work: three genuine dependency parsers, namely the
parsers described in (McDonald et al, 2005), (Nivre
et al, 2007), and (Zhang and Nivre, 2011), and two
constituency parsers (Charniak and Johnson, 2005)
and (Klein and Manning, 2003), whose outputs were
converted to dependency structures by Penn Con-
verter (Johansson and Nugues, 2007).
As for the related literature, there is no published
study measuring the influence of dependency parsers
on dependency-based MT to our knowledge.2
The remainder of this paper is structured as fol-
lows. The overall translation pipeline, within which
the parsers are tested, is described in Section 2. Sec-
tion 3 lists the parsers under consideration and their
main features. Section 4 summarizes the influence
of the selected parsers on the MT quality in terms of
BLEU. Section 5 concludes.
2However, the parser bottleneck of the dependency-based
MT approach was observed also by other researchers (Robert
Moore, personal communication).
2 Dependency-based Translation in Treex
We have implemented our experiments in the Treex
software framework (formerly TectoMT, introduced
by Z?abokrtsky? et al (2008)), which already offers
tool chains for analysis and synthesis of Czech and
English sentences.
We use the tectogrammatical (deep-syntactic)
layer of language representation as the transfer layer
in the presented MT experiments. Tectogrammat-
ics was introduced by Sgall (1967) and further
elaborated within the Prague Dependency Treebank
project (Hajic? et al, 2006). On this layer, each
sentence is represented as a tectogrammatical tree,
whose main properties (from the MT viewpoint) are
the following:
1. nodes represent autosemantic words,
2. edges represent semantic dependencies (a node
is an argument or a modifier of its parent),
3. there are no functional words (prepositions,
auxiliary words) in the tree, and the autose-
mantic words appear only in their base forms
(lemmas). Morphologically indispensable cat-
egories (such as number with nouns or tense
with verbs, but not number with verbs as it is
only imposed by agreement) are stored in sep-
arate node attributes (grammatemes).
The intuitions behind the decision to use tec-
togrammatics for MT are the following: we be-
lieve that (1) tectogrammatics largely abstracts from
language-specific means (inflection, agglutination,
functional words etc.) of expressing non-lexical
meanings and thus tectogrammatical trees are sup-
posed to be highly similar across languages, (2)
it enables a natural transfer factorization,3 (3) and
local tree contexts in tectogrammatical trees carry
more information (especially for lexical choice) than
local linear contexts in the original sentences.
The translation scenario is outlined in the rest of
this section.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
?denser?, especially when translating from/to a language with
rich inflection such as Czech.
434
2.1 Analysis
The input English text is segmented into sentences
and tokens. The tokens are lemmatized and tagged
with Penn Treebank tags using the Morce tagger
(Spoustova? et al, 2007). Then one of the studied
dependency parsers is applied and a surface-syntax
dependency tree (analytical tree in the PDT termi-
nology) is created for each sentence.
This tree is converted to a tectogrammatical tree.
Each autosemantic word with its associated func-
tional words is collapsed into a single tectogram-
matical node, labeled with a lemma, formeme,4 and
semantically indispensable morphologically cate-
gories; coreference is also resolved.
2.2 Transfer
The transfer phase follows, whose most difficult part
consists especially in labeling the tree with target-
side lemmas and formemes. There are also other
types of changes, such as node addition and dele-
tion. However, as shown by Popel (2009), changes
of tree topology are required relatively infrequently
due to the language abstractions on the tectogram-
matical layer.
Currently, translation models based on Maxi-
mum Entropy classifiers are used both for lemmas
and formemes (Marec?ek et al, 2010). Tree label-
ing is optimized using Hidden Tree Markov Mod-
els (Z?abokrtsky? and Popel, 2009), which makes
use of target-language dependency tree probabilistic
model.
All models used in the transfer phase are trained
using training sections of the Czech-English parallel
corpus CzEng 0.9 (Bojar and Z?abokrtsky?, 2009).
2.3 Synthesis
Finally, surface sentence shape is synthesized from
the tectogrammatical tree, which is basically the
reverse operation of the tectogrammatical analy-
sis. It consists of adding punctuation and functional
4Formeme captures the morphosyntactic means which are
used for expressing the tectogrammatical node in the surface
sentence shape. Examples of formeme values: v:that+fin ?
finite verb in a subordinated clause introduced with conjunction
that, n:sb ? semantic noun in a subject position, n:for+X ?
semantic noun in a prepositional group introduced with prepo-
sition for, adj:attr ? semantic adjective in an attributive po-
sition.
words, spreading morphological categories accord-
ing to grammatical agreement, performing inflection
(using Czech morphology database (Hajic?, 2004)),
arranging word order etc.
The difference from the analysis phase is that
there is not very much space for optimization in the
synthesis phase. In other words, final sentence shape
is determined almost uniquely by the tectogrammat-
ical tree (enriched with formemes) resulting from
the transfer phase. However, if there are not enough
constraints for a unique choice of a surface form of
a lemma, then a unigram language model is used for
the final decision. The model was trained using 500
million words from the Czech National Corpus.5
3 Involved Parsers
We performed experiments with parsers from
three families: graph-based parsers, transition-
based parsers, and phrase-structure parsers (with
constituency-to-dependency postprocessing).
3.1 Graph-based Parser
In graph-based parsing, we learn a model for scoring
graph edges, and we search for the highest-scoring
tree composed of the graph?s edges. We used Max-
imum Spanning Tree parser (Mcdonald and Pereira,
2006) which is capable of incorporating second or-
der features (MST for short).
3.2 Transition-based Parsers
Transition-based parsers utilize the shift-reduce al-
gorithm. Input words are put into a queue and
consumed by shift-reduce actions, while the out-
put parser is gradually built. Unlike graph-based
parsers, transition-based parsers have linear time
complexity and allow straightforward application of
non-local features.
We included two transition-based parsers into our
experiments:
? Malt ? Malt parser introduced by Nivre et al
(2007) 6
5http://ucnk.ff.cuni.cz
6We used stackeager algorithm, liblinear learner, and
the enriched feature set for English (the same configu-
ration as in pretrained English models downloadable at
http://maltparser.org.
435
? ZPar ? Zpar parser7 which is basically an al-
ternative implementation of the Malt parser,
employing a richer set of non-local features as
described by Zhang and Nivre (2011).
3.3 CFG-based Tree Parsers
Another option how to obtain dependency trees is
to apply a constituency parser, recognize heads in
the resulting phrase structures and apply a recur-
sive algorithm for converting phrase-structure trees
into constituency trees (the convertibility of the two
types of syntactic structures was studied already by
Gaifman (1965)).
We used two constituency parsers:
? Stanford ? The Stanford parser (Klein and
Manning, 2003),8
? CJ ? a MaxEnt-based parser combined with
discriminative reranking (Charniak and John-
son, 2005).9
Before applying the parsers on the text, the system
removes all spaces within tokens. For instance U. S.
becomes U.S. to restrict the parsers from creating
two new tokens. Tokenization built into both parsers
is bypassed and the default tokenization in Treex is
used.
After parsing, Penn Converter introduced by Jo-
hansson and Nugues (2007) is applied, with the
-conll2007 option, to change the constituent
structure output, of the two parsers, into CoNLL de-
pendency structure. This allows us to keep the for-
mats consistent with the output of both MST and
MaltParser within the Treex framework.
There is an implemented procedure for cre-
ating tectogrammatical trees from the English
phrase structure trees described by Kuc?erova? and
Z?abokrtsky? (2002). Using the procedure is more
straightforward, as it does not go through the
CoNLL-style trees; English CoNLL-style trees dif-
fer slightly from the PDT conventions (e.g. in at-
taching auxiliary verbs) and thus needs additional
7http://sourceforge.net/projects/zpar/ (version 0.4)
8Only the constituent, phrase based, parsed output is used in
these experiments.
9We are using the default settings from the August 2006 ver-
sion of the software.
postprocessing for our purposes. However, we de-
cided to stick to Penn Converter, so that the similar-
ity of the translation scenarios is maximized for all
parsers.
3.4 Common Preprocessing: Shallow Sentence
Chunking
According to our experience, many dependency
parsers have troubles with analyzing sentences that
contain parenthesed or quoted phrases, especially if
they are long.
We use the assumption that in most cases the con-
tent of parentheses or quotes should correspond to
a connected subgraph (subtree) of the syntactic tree.
We implemented a very shallow sentence chunker
(SentChunk) which recognizes parenthesed word
sequences. These sequences can be passed to a
parser first, and be parsed independently of the rest
of the sentence. This was shown to improve not only
parsing accuracy of the parenthesed word sequence
(which is forced to remain in one subtree), but also
the rest of the sentence.10
In our experiments, SentChunk is used only
in combination with the three genuine dependency
parsers.
4 Experiments and Evaluation
4.1 Data for Parsers? Training and Evaluation
The dependency trees needed for training the parsers
and evaluating their UAS were created from the
Penn Treebank data (enriched first with internal
noun phrase structure applied via scripts provided
by Vadas and Curran (2007)) by Penn Converter (Jo-
hansson and Nugues, 2007) with the -conll2007
option (PennConv for short).
All the parsers were evaluated on the same data ?
section 23.
All the parsers were trained on sections 02?21,
except for the Stanford parser which was trained
on sections 01?21. We were able to retrain the
parser models only for MST and Malt. For the
other parsers we used pretrained models available on
the Internet: CJ?s default model ec50spfinal,
Stanford?s wsjPCFG.ser.gz model, and
10Edge length is a common feature in dependency parsers, so
?deleting? parenthesed words may give higher scores to correct
dependency links that happened to span over the parentheses.
436
ZPar?s english.tar.gz. The model of ZPar
is trained on data converted to dependencies using
Penn2Malt tool,11 which selects the last member of
a coordination as the head. To be able to compare
ZPar?s output with the other parsers, we postpro-
cessed it by a simple ConjAsHead code that con-
verts this style of coordinations to the one used in
CoNLL2007, where the conjuction is the head.
4.2 Reference Translations Used for Evaluation
Translation experiments were evaluated using refer-
ence translations from the new-dev2009 data set,
provided by the organizors of shared translation task
with the Workshop on Statistical Machine Transla-
tion.
4.3 Influence of Parser Training Data Size
We trained a sequence of parser models for MST and
Malt, using a roughly exponentially growing se-
quence of Penn Treebank subsets. The subsets are
contiguous and start from the beginning of section
02. The results are collected in Tables 1 and 2.12
#tokens UAS BLEU NIST
100 0.362 0.0579 3.6375
300 0.509 0.0859 4.3853
1000 0.591 0.0995 4.6548
3000 0.623 0.1054 4.7972
10000 0.680 0.1130 4.9695
30000 0.719 0.1215 5.0705
100000 0.749 0.1232 5.1193
300000 0.776 0.1257 5.1571
990180 0.793 0.1280 5.1915
Table 1: The effect of training data size on parsing accu-
racy and on translation performance with MST.
The trend of the relation between the training data
size and BLEU is visible also in Figure 1. It is ob-
vious that increasing the training data has a positive
effect on the translation quality. However, the pace
of growth of BLEU is sublogarithmic, and becomes
unconvincing above 100,000 training tokens. It in-
dicates that given one of the two parsers integrated
11http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
12To our knowledge, the best system participating in the
shared task reaches BLEU 17.8 for this translation direction.
#tokens UAS BLEU NIST
100 0.454 0.0763 4.0555
300 0.518 0.0932 4.4698
1000 0.591 0.1042 4.6769
3000 0.616 0.1068 4.7472
10000 0.665 0.1140 4.9100
30000 0.695 0.1176 4.9744
100000 0.723 0.1226 5.0504
300000 0.740 0.1238 5.1005
990180 0.759 0.1253 5.1296
Table 2: The effect of training data size on parsing accu-
racy and on translation performance with Malt.
 
0.05
 
0.06
 
0.07
 
0.08
 
0.09 0.1
 
0.11
 
0.12
 
0.13
 
100
 
100
0
 
100
00
 
100
000
 
1e+
06
BLEU
train
ing 
toke
ns
MS
T Mal
t
Figure 1: The effect of parser training data size of BLEU
with Malt and MST parsers.
into our translation framework, increasing the parser
training data alone would probably not lead to a sub-
stantial improvement of the translation performance.
4.4 Influence of Parser Choice
Table 3 summarizes our experiments with the five
parsers integrated into the tectogrammatical transla-
tion pipeline. Two configurations (with and without
SentChunk) are listed for the genuine dependency
parsers. The relationship between UAS and BLEU
for (the best configurations of) all five parsers is de-
picted also in Figure 2.
Additionally, we used paired bootstrap 95% con-
fidence interval testing (Zhang et al, 2004), to check
which BLEU differences are significant. For the
five compared parser (with SentChunk if appli-
cable), only four comparisons are not significant:
MST-CJ, MST-Stanford, Malt-Stanford,
and CJ-Stanford.
437
Parser Training data Preprocessing Postprocessing UAS BLEU NIST TER
MST PennTB + PennConv SentChunk ? 0.793 0.1280 5.192 0.735
MST PennTB + PennConv ? ? 0.794 0.1236 5.149 0.739
Malt PennTB + PennConv SentChunk ? 0.760 0.1253 5.130 0.740
Malt PennTB + PennConv ? ? 0.761 0.1214 5.088 0.744
Zpar PennTB + Penn2Malt SentChunk ConjAsHead 0.793 0.1176 5.039 0.749
Zpar PennTB + Penn2Malt ? ConjAsHead 0.792 0.1127 4.984 0.754
CJ PennTB ? PennConv 0.904 0.1284 5.189 0.737
Stanford PennTB ? PennConv 0.825 0.1277 5.137 0.740
Table 3: Dependency parsers tested in the translation pipeline.
 
0.1
 
0.10
5
 
0.11
 
0.11
5
 
0.12
 
0.12
5
 
0.13
 
0.13
5
 
0.14
 
0.14
5
 
0.15
 
0.74
 
0.76
 
0.78
 
0.8
 
0.82
 
0.84
 
0.86
 
0.88
 
0.9
 
0.92
BLEU
UAS
MS
T Mal
t
Zpa
r
Sta
nfor
d CJ
Figure 2: Unlabeled Attachment Score versus BLEU.
Even if BLEU grows relatively smoothly with
UAS for different parsing models of the same parser,
one can see that there is no obvious relation be-
tween UAS and BLEU accross all parsers. MST and
Zpar have the same UAS but quite different BLEU,
whereas MST and CJ have very similar BLEU but
distant UAS. It confirms the original hypothesis that
it is not only the overall UAS, but also the parser-
specific distribution of errors what matters.
4.5 Influence of Shallow Sentence Chunking
Table 3 confirms that parsing the contents paren-
theses separately from the rest of the sentence
(SentChunk) has a positive effect with all three
dependency parsers. Surprisingly, even if the effect
on UAS is negligible, the improvement is almost
half of BLEU point which is significant for all the
three parsers.
4.6 Discussion on Result Comparability
We tried to isolate the effects of the properties of
selected parsers, however, the separation from other
influencing factors is not perfect due to several tech-
nical issues:
? So far, we were not able to retrain the models
for all parsers ourselves and therefore their pre-
trained models (one of them based on slightly
different Penn Treebank division) must have
been used.
? Some parsers make their own choice of POS
tags within the parsed sentences, while other
parsers require the sentences to be tagged al-
ready on their input.
? The trees in the CzEng 0.9 parallel treebank
were created using MST. CzEng 0.9 was used
for training translation models used in the
transfer phase of the translation scenario; thus
these translation models might compensate for
some MST?s errors, which might handicap other
parsers. So far we were not able to reparse 8
million sentence pairs in CzEng 0.9 by all stud-
ied parsers.
5 Conclusions
This paper is a study of how the choice of a de-
pendency parsing technique influences the quality of
English-Czech dependency-based translation. Our
main observations are the following. First, BLEU
grows with the increasing amount of training depen-
dency trees, but only in a sublogarithmic pace. Sec-
ond, what seems to be quite effective for translation
438
is to facilitate the parsers? task by dividing the sen-
tences into smaller chunks using parenthesis bound-
aries. Third, if the parsers are based on different
approaches, their UAS does not correlate well with
their effect on the translation quality.
Acknowledgments
This research was supported by the
grants MSM0021620838, GAUK 116310,
GA201/09/H057, and by the European Com-
mission?s 7th Framework Program (FP7) under
grant agreements n? 238405 (CLARA), n? 247762
(FAUST), and n? 231720 (EuroMatrix Plus).
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathematical
Linguistics, 92:63?83.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
Association for Computational Linguistics, ACL ?05,
pages 173?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Haim Gaifman. 1965. Dependency systems and phrase-
structure systems. Information and Control, pages
304?337.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of Association for Computational Lin-
guistics, pages 423?430.
Ivona Kuc?erova? and Zdene?k Z?abokrtsky?. 2002. Trans-
forming Penn Treebank Phrase Trees into (Praguian)
Tectogrammatical Dependency Trees. The Prague
Bulletin of Mathematical Linguistics, (78):77?94.
David Marec?ek, Martin Popel, and Zdene?k Z?abokrtsky?.
2010. Maximum entropy translation model in
dependency-based MT framework. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 201?201, Uppsala,
Sweden. Association for Computational Linguistics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT / EMNLP, pages 523?530, Vancouver, Canada.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Martin Popel. 2009. Ways to Improve the Quality of
English-Czech Machine Translation. Master?s thesis,
Institute of Formal and Applied Linguistics, Charles
University, Prague, Czech Republic.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing, ACL
2007, pages 67?74, Praha.
David Vadas and James Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Zdene?k Z?abokrtsky? and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 145?148, Suntec, Sin-
gapore.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceedings
of the 3rd Workshop on Statistical Machine Transla-
tion, ACL, pages 167?170.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph.D. thesis, Faculty of Mathematics
and Physics, Charles University in Prague.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In To
appear in the Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores: How much improvement
do we need to have a better system. In Proceedings of
LREC, volume 4, pages 2051?2054.
439
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19?26,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Hybrid Combination of Constituency and Dependency Trees into an
Ensemble Dependency Parser
Nathan David Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
Prague, Czech Republic
{green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Dependency parsing has made many ad-
vancements in recent years, in particu-
lar for English. There are a few de-
pendency parsers that achieve compara-
ble accuracy scores with each other but
with very different types of errors. This
paper examines creating a new depen-
dency structure through ensemble learn-
ing using a hybrid of the outputs of var-
ious parsers. We combine all tree out-
puts into a weighted edge graph, using 4
weighting mechanisms. The weighted edge
graph is the input into our ensemble sys-
tem and is a hybrid of very different parsing
techniques (constituent parsers, transition-
based dependency parsers, and a graph-
based parser). From this graph we take a
maximum spanning tree. We examine the
new dependency structure in terms of accu-
racy and errors on individual part-of-speech
values.
The results indicate that using a greater
number of more varied parsers will improve
accuracy results. The combined ensemble
system, using 5 parsers based on 3 different
parsing techniques, achieves an accuracy
score of 92.58%, beating all single parsers
on the Wall Street Journal section 23 test
set. Additionally, the ensemble system re-
duces the average relative error on selected
POS tags by 9.82%.
1 Introduction
Dependency parsing has made many advance-
ments in recent years. A prime reason for the
quick advancement has been the CoNLL shared
task competitions. These competitions gave the
community a common training/testing framework
along with many open source systems. These sys-
tems have, for certain languages, achieved fairly
high accuracy. Many of the top systems have
comparable accuracy but vary on the types of
errors they make. The approaches used in the
shared task vary from graph-based techniques to
transition-based techniques to the conversion of
constituent trees produced by state-of-the-art con-
stituent parsers. This varied error distribution
makes dependency parsing a prime area for the
application of new hybrid and ensemble algo-
rithms.
Increasing accuracy of dependency parsing of-
ten is in the realm of feature tweaking and opti-
mization. The idea behind ensemble learning is to
take the best of each parser as it currently is and
allow the ensemble system to combine the outputs
to form a better overall parse using prior knowl-
edge of each individual parser. This is often done
by different weighting or voting schemes.
2 Related Work
Ensemble learning (Dietterich, 2000) has been
used for a variety of machine learning tasks and
recently has been applied to dependency pars-
ing in various ways and with different levels of
success. (Surdeanu and Manning, 2010; Haf-
fari et al, 2011) showed a successful combina-
tion of parse trees through a linear combination
of trees with various weighting formulations. To
keep their tree constraint, they applied Eisner?s al-
gorithm for reparsing (Eisner, 1996).
Parser combination with dependency trees has
been examined in terms of accuracy (Sagae and
Lavie, 2006; Sagae and Tsujii, 2007; Zeman and
Z?abokrtsky?, 2005). However, the various tech-
niques have generally examined similar parsers
19
or parsers which have generated various different
models. To the best of our knowledge, our ex-
periments are the first to look at the accuracy and
part of speech error distribution when combining
together constituent and dependency parsers that
use many different techniques. However, POS
tags were used in parser combination in (Hall et
al., 2007) for combining a set of Malt Parser mod-
els with success.
Other methods of parser combinations have
shown to be successful such as using one parser
to generate features for another parser. This was
shown in (Nivre and McDonald, 2008), in which
Malt Parser was used as a feature to MST Parser.
The result was a successful combination of a
transition-based and graph-based parser, but did
not address adding other types of parsers into the
framework.
3 Methodology
The following sections describe the process flow,
choice of parsers, and datasets needed for oth-
ers to recreate the results listed in this paper.
Although we describe the specific parsers and
datasets used in this paper, this process flow
should work for any number of hybrid combina-
tions of parsers and datasets.
3.1 Process Flow
To generate a single ensemble parse tree, our sys-
tem takes N parse trees as input. The inputs are
from a variety of parsers as described in 3.2.
All edges in these parse trees are combined into
a graph structure. This graph structure accepts
weighted edges. So if more than one parse tree
contains the same tree edge, the graph is weighted
appropriately according to a chosen weighting al-
gorithm. The weighting algorithms used in our
experiments are described in 3.5.
Once the system has a weighted graph, it then
uses an algorithm to find a corresponding tree
structure so there are no cycles. In this set of ex-
periments, we constructed a tree by finding the
maximum spanning tree using ChuLiu/Edmonds?
algorithm, which is a standard choice for MST
tasks. Figure 1 graphically shows the decisions
one needs to make in this framework to create an
ensemble parse.
Figure 1: General flow to create an ensemble parse
tree.
3.2 Parsers
To get a complete representation of parsers in
our ensemble learning framework we use 5 of
the most commonly used parsers. They range
from graph-based approaches to transition-based
approaches to constituent parsers. Constituency
output is converted to dependency structures us-
ing a converter (Johansson and Nugues, 2007).
All parsers are integrated into the Treex frame-
work (Z?abokrtsky? et al, 2008; Popel et al, 2011)
using the publicly released parsers from the re-
spective authors but with Perl wrappers to allow
them to work on a common tree structure.
? Graph-Based: A dependency tree is a spe-
cial case of a weighted edge graph that
spawns from an artificial root and is acyclic.
Because of this we can look at a large history
of work in graph theory to address finding
the best spanning tree for each dependency
graph. In this paper we use MST Parser
(McDonald et al, 2005) as an input to our
ensemble parser.
? Transition-Based: Transition-based parsing
creates a dependency structure that is pa-
rameterized over the transitions used to cre-
ate a dependency tree. This is closely re-
lated to shift-reduce constituency parsing al-
gorithms. The benefit of transition-based
parsing is the use of greedy algorithms which
have a linear time complexity. However, due
to the greedy algorithms, longer arc parses
can cause error propagation across each tran-
sition (Ku?bler et al, 2009). We make use
20
of Malt Parser (Nivre et al, 2007b), which
in the shared tasks was often tied with the
best performing systems. Additionally we
use Zpar (Zhang and Clark, 2011) which is
based on Malt Parser but with a different set
of non-local features.
? Constituent Transformation While not a
true dependency parser, one technique of-
ten applied is to take a state-of-the-art con-
stituent parser and transform its phrase based
output into dependency relations. This has
been shown to also be state-of-the-art in ac-
curacy for dependency parsing in English. In
this paper we transformed the constituency
structure into dependencies using the Penn
Converter conversion tool (Johansson and
Nugues, 2007). A version of this converter
was used in the CoNLL shared task to create
dependency treebanks as well. For the fol-
lowing ensemble experiments we make use
of both (Charniak and Johnson, 2005) and
Stanford?s (Klein and Manning, 2003) con-
stituent parsers.
In addition to these 5 parsers, we also report
the accuracy of an Oracle Parser. This parser is
simply the best possible parse of all the edges of
the combined dependency trees. If the reference,
gold standard, tree has an edge that any of the 5
parsers contain, we include that edge in the Or-
acle parse. Initially all nodes of the tree are at-
tached to an artificial root in order to maintain
connectedness. Since only edges that exist in a
reference tree are added, the Oracle Parser main-
tains the acyclic constraint. This can be viewed
as the maximum accuracy that a hybrid approach
could achieve with this set of parsers and with the
given data sets.
3.3 Datasets
Much of the current progress in dependency pars-
ing has been a result of the availability of common
data sets in a variety of languages, made avail-
able through the CoNLL shared task (Nivre et al,
2007a). This data is in 13 languages and 7 lan-
guage families. Later shared tasks also released
data in other genres to allow for domain adap-
tation. The availability of standard competition,
gold level, data has been an important factor in
dependency based research.
For this study we use the English CoNLL data.
This data comes from the Wall Street Journal
(WSJ) section of the Penn treebank (Marcus et al,
1993). All parsers are trained on sections 02-21 of
the WSJ except for the Stanford parser which uses
sections 01-21. Charniak, Stanford and Zpar use
pre-trained models ec50spfinal, wsjPCFG.ser.gz,
english.tar.gz respectively. For testing we use sec-
tion 23 of the WSJ for comparability reasons with
other papers. This test data contains 56,684 to-
kens. For tuning we use section 22. This data is
used for determining some of the weighting fea-
tures.
3.4 Evaluation
As an artifact of the CoNLL shared tasks
competition, two standard metrics for com-
paring dependency parsing systems emerged.
Labeled attachment score (LAS) and unlabeled
attachment score (UAS). UAS studies the struc-
ture of a dependency tree and assesses whether the
output has the correct head and dependency arcs.
In addition to the structure score in UAS, LAS
also measures the accuracy of the dependency la-
bels on each arc. A third, but less common met-
ric, is used to judge the percentage of sentences
that are completely correct in regards to their LAS
score. For this paper since we are primarily con-
cerned with the merging of tree structures we only
evaluate UAS (Buchholz and Marsi, 2006).
3.5 Weighting
Currently we are applying four weighting algo-
rithms to the graph structure. First we give each
parser the same uniform weight. Second we ex-
amine weighting each parser output by the UAS
score of the individual parser taken from our tun-
ing data. Third we use plural voting weights
(De Pauw et al, 2006) based on parser ranks from
our tuning data. Due to the success of Plural vot-
ing, we try to exaggerate the differences in the
parsers by using UAS10 weighting. All four of
these are simple weighting techniques but even in
their simplicity we can see the benefit of this type
of combination in an ensemble parser.
? Uniform Weights: an edge in the graph gets
incremented +1 weight for each matching
edge in each parser. If an edge occurs in 4
parsers, the weight is 4.
? UAS Weighted: Each edge in the graph gets
21
incremented by the value of it?s parsers in-
dividual accuracy. So in the UAS results
in Table 1 an edge in Charniak?s tree gets
.92 added while MST gets .86 added to ev-
ery edge they share with the resulting graph.
This weighting should allow us to add poor
parsers with very little harm to the overall
score.
? Plural Voting Weights: In Plural Voting
the parsers are rated according to their rank
in our tuning data and each gets a ?vote?
based on their quality. With N parsers the
best parser gets N votes while the last place
parser gets 1 vote. In this paper, Charniak
received 5 votes, Stanford received 4 votes,
MST Parser received 3 votes, Malt Parser
received 2 votes, and Zpar received 1 vote.
Votes in this case are added to each edge as
a weight.
? UAS10: For this weighting scheme we took
each UAS value to the 10th power. This gave
us the desired affect of making the differ-
ences in accuracy more apparent and giving
more distance from the best to worse parser.
This exponent was empirically selected from
results with our tuning data set.
4 Results
Table 1 contains the results of different parser
combinations of the 5 parsers and Table 2 shows
the baseline scores of the respective individual
parsers. The results indicate that using two
parsers will result in an ?average? score, and no
combination of 2 parsers gave an improvement
over the individual parsers, these were left out
of the table. Ensemble learning seems to start to
have a benefit when using 3 or more parsers with a
few combinations having a better UAS score than
any of the baseline parsers, these cases are in bold
throughout the table. When we add a 4th parser
to the mix almost all configurations lead to an
improved score when the edges are not weighted
uniformly. The only case in which this does not
occur is when Stanford?s Parser is not used.
Uniform voting gives us an improved score in a
few of the model combinations but in most cases
does not produce an output that beats the best in-
dividual system. UAS weighting is not the best
overall but it does give improved performance in
the majority of model combinations. Problemati-
cally UAS weighted trees do not give an improved
accuracy when all 5 parsers are used. Given the
slight differences in UAS scores of the baseline
models in Table 2 this is not surprising as the
best graph edge can be outvoted as the number
of N parsers increases. The slight differences in
weight do not seem to change the MST parse dra-
matically when all 5 parsers are used over Uni-
form weighting. Based on the UAS scores learned
in our tuning data set, we next looked to amplify
the weight differences using Plural Voting. For
the majority of model combinations in Plural vot-
ing we achieve improved results over the individ-
ual systems. When all 5 parsers are used together
with Plural Voting, the ensemble parser improves
over the highest individual parser?s UAS score.
With the success of Plural voting we looked to
amplify the UAS score differences in a more sys-
tematic way. We looked at using UASx where
x was found experimentally in our tuning data.
UAS10 matched Plural voting in the amount of
system combinations that improved over their in-
dividual components. The top overall score is
when we use UAS10 weighting with all parsers.
For parser combinations that do not feature Char-
niak?s parser, we also find an increase in over-
all accuracy score compared to each individual
parser, although never beating Charniak?s individ-
ual score.
To see the maximum accuracy a hybrid combi-
nation can achieve we include an Oracle Ensem-
ble Parser in Table 1. The Oracle Parser takes
the edges from all dependency trees and only adds
each edge to the Oracle Tree if the corresponding
edge is in the reference tree. This gives us a ceil-
ing on what ensemble learning can achieve. As
we can see in Table 1, the ceiling of ensemble
learning is 97.41% accuracy. Because of this high
value with only 5 parsers, ensemble learning and
other hybrid approaches should be a very prosper-
ous area for dependency parsing research.
In (Ku?bler et al, 2009) the authors confirm that
two parsers, MST Parser and Malt Parser, give
similar accuracy results but with very different
errors. MST parser, a maximum spanning tree
graph-based algorithm, has evenly distributed er-
rors while Malt Parser, a transition based parser,
has errors on mainly longer sentences. This re-
22
System Uniform UAS Plural UAS10 Oracle
Weighting Weighted Voting Weighted UAS
Charniak-Stanford-Mst 91.86 92.27 92.28 92.25 96.48
Charniak-Stanford-Malt 91.77 92.28 92.3 92.08 96.49
Charniak-Stanford-Zpar 91.22 91.99 92.02 92.08 95.94
Charniak-Mst-Malt 88.80 89.55 90.77 92.08 96.3
Charniak-Mst-Zpar 90.44 91.59 92.08 92.08 96.16
Charniak-Malt-Zpar 88.61 91.3 92.08 92.08 96.21
Stanford-Mst-Malt 87.84 88.28 88.26 88.28 95.62
Stanford-Mst-Zpar 89.12 89.88 88.84 89.91 95.57
Stanford-Malt-Zpar 88.61 89.57 87.88 87.88 95.47
Mst-Malt-Zpar 86.99 87.34 86.82 86.49 93.79
Charniak-Stanford-Mst-Malt 90.45 92.09 92.34 92.56 97.09
Charniak-Stanford-Mst-Zpar 91.57 92.24 92.27 92.26 96.97
Charniak-Stanford-Malt-Zpar 91.31 92.14 92.4 92.42 97.03
Charniak-Mst-Malt-Zpar 89.60 89.48 91.71 92.08 96.79
Stanford-Mst-Malt-Zpar 88.76 88.45 88.95 88.44 96.36
All 91.43 91.77 92.44 92.58 97.41
Table 1: Results of the maximum spanning tree algorithm on a combined edge graph. Scores are in bold when
the ensemble system increased the UAS score over all individual systems.
Parser UAS
Charniak 92.08
Stanford 87.88
MST 86.49
Malt 84.51
Zpar 76.06
Table 2: Our baseline parsers and corresponding UAS
used in our ensemble experiments
sult comes from the approaches themselves. MST
parser is globally trained so the best mean solu-
tion should be found. This is why errors on the
longer sentences are about the same as the shorter
sentences. Malt Parser on the other hand uses a
greedy algorithm with a classifier that chooses a
particular transition at each vertex. This leads to
the possibility of the propagation of errors further
in a sentence. Along with this line of research,
we look at the error distribution for all 5 parsers
along with our best ensemble parser configura-
tion. Much like the previous work, we expect dif-
ferent types of errors, given that our parsers are
from 3 different parsing techniques. To examine
if the ensemble parser is substantially changing
the parse tree or is just taking the best parse tree
and substituting a few edges, we examine the part
of speech accuracies and relative error reduction
in Table 3.
As we can see the range of POS errors varies
dramatically depending on which parser we ex-
amine. For instance for CC, Charniak has 83.54%
accuracy while MST has only 71.16% accuracy.
The performance for certain POS tags is almost
universally low such as the left parenthesis (.
Given the large difference in POS errors, weight-
ing an ensemble system by POS would seem like
a logical choice in future work. As we can see
in Figure 2, the varying POS accuracies indicate
that the parsing techniques we have incorporated
into our ensemble parser, are significantly differ-
ent. In almost every case in Table 3, our ensemble
parser achieves the best accuracy for each POS,
while reducing the average relative error rate by
9.82%.
The current weighting systems do not simply
default to the best parser or to an average of all er-
rors. In the majority of cases our ensemble parser
obtains the top accuracy. The ability of the en-
semble system to use maximum spanning tree on
a graph allows the ensemble parser to connect
nodes which might have been unconnected in a
subset of the parsers for an overall gain, which
is preferable to techniques which only select the
best model for a particular tree. In all cases,
our ensemble parser is never the worst parser. In
23
POS Charniak Stanford MST Malt Zpar Best Relative Error
Ensemble Reduction
CC 83.54 74.73 71.16 65.84 20.39 84.63 6.62
NNP 94.59 92.16 88.04 87.17 73.67 95.02 7.95
VBN 91.72 89.81 90.35 89.17 88.26 93.81 25.24
CD 94.91 92.67 85.19 84.46 82.64 94.96 0.98
RP 96.15 95.05 97.25 95.60 94.51 97.80 42.86
JJ 95.41 92.99 94.47 93.90 89.45 95.85 9.59
PRP 97.82 96.21 96.68 95.64 95.45 98.39 26.15
TO 94.52 89.44 91.29 90.73 88.63 94.35 -3.10
WRB 63.91 60.90 68.42 73.68 4.51 63.91 0.00
RB 86.26 79.88 81.49 81.44 80.61 87.19 6.77
WDT 97.14 95.36 96.43 95.00 9.29 97.50 12.59
VBZ 91.97 87.35 83.86 80.78 57.91 92.46 6.10
( 73.61 75.00 54.17 58.33 15.28 73.61 0.00
POS 98.18 96.54 98.54 98.72 0.18 98.36 9.89
VB 93.04 88.48 91.33 90.95 84.37 94.24 17.24
MD 89.55 82.02 83.05 78.77 51.54 89.90 3.35
NNS 93.10 89.51 90.68 88.65 78.93 93.67 8.26
NN 93.62 90.29 88.45 86.98 83.84 94.00 5.96
VBD 93.25 87.20 86.27 82.73 64.32 93.52 4.00
DT 97.61 96.47 97.30 97.01 92.19 97.97 15.06
RBS 90.00 76.67 93.33 93.33 86.67 90.00 0.00
IN 87.80 78.66 83.45 80.78 73.08 87.48 -2.66
) 70.83 77.78 96.46 55.56 12.50 72.22 4.77
VBG 85.19 82.13 82.74 82.25 81.27 89.35 28.09
Average 9.82
Table 3: POS accuracies for each of our systems that are used in the ensemble system. We use these accuracies
to obtain the POS error distribution for our best ensemble system, which is the combination of all parsers using
UAS10 weighting. Relative error reduction is calculated between our best ensemble system against the Charniak
Parser which had the best individual scores.
24
Figure 2: POS errors of all 5 parsers and the best en-
semble system
cases where the POS is less frequent, our ensem-
ble parser appears to average out the error distri-
bution.
5 Conclusion
We have shown the benefits of using a maxi-
mum spanning tree algorithm in ensemble learn-
ing for dependency parsing, especially for the
hybrid combination of constituent parsers with
other dependency parsing techniques. This en-
semble method shows improvements over the cur-
rent state of the art for each individual parser. We
also show a theoretical maximum oracle parser
which indicates that much more work in this field
can take place to improve dependency parsing ac-
curacy toward the oracle score of 97.41%.
We demonstrated that using parsers of differ-
ent techniques, especially including transformed
constituent parsers, can lead to the best accuracy
within this ensemble framework. The improve-
ments in accuracy are not simply due to a few
edge changes but can be seen to improve the ac-
curacy of the majority of POS tags over all indi-
vidual systems.
While we have only shown this for English,
we expect the results to be similar for other lan-
guages since our methodology is language in-
dependent. Future work will contain different
weighting mechanisms as well as application to
other languages which are included in CoNLL
data sets.
6 Acknowledgments
This research has received funding from the
European Commission?s 7th Framework Pro-
gram (FP7) under grant agreement n? 238405
(CLARA)
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Guy De Pauw, Gilles-Maurice de Schryver, and Peter
Wagacha. 2006. Data-driven part-of-speech tag-
ging of kiswahili. In Petr Sojka, Ivan Kopecek, and
Karel Pala, editors, Text, Speech and Dialogue, vol-
ume 4188 of Lecture Notes in Computer Science,
pages 197?204. Springer Berlin / Heidelberg.
Thomas G. Dietterich. 2000. Ensemble methods in
machine learning. In Proceedings of the First In-
ternational Workshop on Multiple Classifier Sys-
tems, MCS ?00, pages 1?15, London, UK. Springer-
Verlag.
Jason Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In
Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), pages
340?345, Copenhagen, August.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
710?714, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen
Eryigit, Bea?ta Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
a study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 933?939.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
25
English. In Proceedings of NODALIDA 2007,
pages 105?112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis lectures on human lan-
guage technologies. Morgan & Claypool, US.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the Penn Treebank. Com-
put. Linguist., 19:313?330, June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June. Association for
Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task
on dependency parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gulsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Martin Popel, David Marec?ek, Nathan Green, and
Zdene?k Z?abokrtsky?. 2011. Influence of parser
choice on dependency-based mt. In Proceedings of
the Sixth Workshop on Statistical Machine Trans-
lation, pages 433?439, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Kenji Sagae and Alon Lavie. 2006. Parser combi-
nation by reparsing. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 129?132,
New York City, USA, June. Association for Com-
putational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Depen-
dency parsing and domain adaptation with LR mod-
els and parser ensembles. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 1044?1050, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, HLT ?10, pages 649?652, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL, pages 167?170.
Daniel Zeman and Zdene?k Z?abokrtsky?. 2005. Im-
proving parsing accuracy by combining diverse de-
pendency parsers. In In: Proceedings of the 9th In-
ternational Workshop on Parsing Technologies.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
26
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 84?89,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Dependency Parsing
using Reducibility and Fertility features?
David Marec?ek and Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{marecek,zabokrtsky}@ufal.mff.cuni.cz
Abstract
This paper describes a system for unsuper-
vised dependency parsing based on Gibbs
sampling algorithm. The novel approach in-
troduces a fertility model and reducibility
model, which assumes that dependent words
can be removed from a sentence without vio-
lating its syntactic correctness.
1 Introduction
One of the traditional linguistic criteria for recog-
nizing dependency relations (including their head-
dependent orientation) is that stepwise deletion of
dependent elements within a sentence preserves
its syntactic correctness (Lopatkova? et al, 2005;
Ku?bler et al, 2009; Gerdes and Kahane, 2011).1 If a
word can be removed from a sentence without dam-
aging it, then it is likely to be dependent on some
other (still present) word.
Our approach allows to utilize information from
very large corpora. While the computationally de-
manding sampling procedure can be applied only
on limited data, the unrepeated precomputation of
? This research was supported by the grants
GA201/09/H057 (Res Informatica), MSM0021620838,
GAUK 116310, and by the European Commission?s 7th
Framework Program (FP7) under grant agreement n? 247762
(FAUST).
1Of course, all the above works had to respond to the noto-
rious fact that there are many language phenomena precluding
the ideal (word by word) sentence reducibility (e.g. in the case
of prepositional groups, or in the case of subjects in English fi-
nite clauses). However, we borrow only the very core of the
reducibility idea.
statistics for reducibility estimates can easily exploit
much larger data.
2 Precomputing PoS tag reducibility scores
We call a word (or a sequence of words) in a sen-
tence reducible, if the sentence after removing the
word remains grammatically correct. Although we
cannot automatically recognize grammaticality of
such newly created sentence, we can search for it
in a large corpus. If we find it, we assume the word
was reducible in the original sentence. It is obvi-
ous that the number of such reducible sequences of
words found in a corpus is relatively low. However,
it is sufficient for determining reducibility scores at
least for individual types of words (part-of-speech
tags).2
Assume a PoS n-gram g = [t1, . . . , tn]. We go
through the corpus and search for all its occurrences.
For each such occurrence, we remove the respec-
tive words from the current sentence and check in
the corpus whether the rest of the sentence occurs at
least once elsewhere in the corpus.3 If so, then such
occurrence of PoS n-gram is reducible, otherwise it
is not. We denote the number of such reducible oc-
currences of PoS n-gram g by r(g). The number of
all its occurrences is c(g). The relative reducibility
2Although we search for reducible sequences of word forms
in the corpus, we compute reducibility scores for sequences of
part-of-speech tags. This requires to have the corpus morpho-
logically disambiguated.
3We do not take into account sentences with less then 10
words, because they could be nominal (without any verb) and
might influence the reducibility scores of verbs.
84
unigrams R bigrams R trigrams R
VB 0.04 VBN IN 0.00 IN DT JJ 0.00
TO 0.07 IN DT 0.02 JJ NN IN 0.00
IN 0.11 NN IN 0.04 NN IN NNP 0.00
VBD 0.12 NNS IN 0.05 VBN IN DT 0.00
CC 0.13 JJ NNS 0.07 JJ NN . 0.00
VBZ 0.16 NN . 0.08 DT JJ NN 0.04
NN 0.22 DT NNP 0.09 DT NNP NNP 0.05
VBN 0.24 DT NN 0.09 NNS IN DT 0.14
. 0.32 NN , 0.11 NNP NNP . 0.15
NNS 0.38 DT JJ 0.13 NN IN DT 0.23
DT 0.43 JJ NN 0.14 NNP NNP , 0.46
NNP 0.78 NNP . 0.15 IN DT NNP 0.55
JJ 0.84 NN NN 0.22 DT NN IN 0.59
RB 2.07 IN NN 0.67 NNP NNP NNP 0.64
, 3.77 NNP NNP 0.76 IN DT NN 0.80
CD 55.6 IN NNP 1.81 IN NNP NNP 4.27
Table 1: Reducibility scores of the most frequent PoS tag
English n-grams.
R(g) of a PoS n-gram g is then computed as
R(g) =
1
N
r(g) + ?1
c(g) + ?2
, (1)
where the normalization constant N , which ex-
presses relative reducibility over all the PoS n-grams
(denoted by G), causes the scores are concentrated
around the value 1.
N =
?
g?G(r(g) + ?1)
?
g?G(c(g) + ?2)
(2)
Smoothing constants ?1 and ?2, which prevent re-
ducibility scores from being equal to zero, are set as
follows: 4
?1 =
?
g?G r(g)
?
g?G c(g)
, ?2 = 1 (3)
Table 1 shows reducibility scores of the most fre-
quent English PoS n-grams. If we consider only uni-
grams, we can see that the scores for verbs are often
among the lowest. Verbs are followed by preposi-
tions and nouns, and the scores for adjectives and ad-
verbs are very high for all three examined languages.
That is desired, because the reducible unigrams will
more likely become leaves in dependency trees.
3 Dependency Models
We introduce a new generative model that is differ-
ent from the widely used Dependency Model with
4This setting causes that even if a given PoS n-gram is
not reducible anywhere in the corpus, its reducibility score is
1/(c(g) + 1).
Valence (DMV).5 Our generative model introduces
fertility of a node. For a given head, we first gener-
ate the number of its left and right children (fertility
model) and then we fill these positions by generat-
ing its individual dependents (edge model). If a zero
fertility is generated, the head becomes a leaf.
Besides the fertility model and the edge model,
we use two more models (subtree model and dis-
tance model), which force the generated trees to
have more desired shape.
3.1 Fertility model
We express a fertility of a node by a pair of num-
bers: the number of its left dependents and the num-
ber of its right dependents.6 Fertility is conditioned
by part-of-speech tag of the node and it is computed
following the Chinese restaurant process.7 The for-
mula for computing probability of fertility fi of a
word on the position i in the corpus is as follows:
Pf (fi|ti) =
c?i(?ti, fi?) + ?P0(fi)
c?i(?ti?) + ?
, (4)
where ti is part-of-speech tag of the word on the po-
sition i, c?i(?ti, fi?) stands for the count of words
with PoS tag ti and fertility fi in the history, and
P0 is a prior probability for the given fertility which
depends on the total number of node dependents de-
noted by |fi| (the sum of numbers of left and right
dependents):
P0(fi) =
1
2|fi|+1
(5)
This prior probability has a nice property: for a
given number of nodes, the product of fertility prob-
abilities over all the nodes is equal for all possi-
ble dependency trees. This ensures balance of this
model during inference.
5In DMV (Klein and Manning, 2004) and in the extended
model EVG (Headden III et al, 2009), there is a STOP sign in-
dicating that no more dependents in a given direction will be
generated. Given a certain head, all its dependents in left di-
rection are generated first, then the STOP sign in that direction,
then all its right dependents and then STOP in the other direc-
tion. This process continues recursively for all generated de-
pendents.
6For example, fertility ?1-3? means that the node has one
left and three right dependents, fertility ?0-0? indicates that it is
a leaf.
7If a specific fertility has been frequent for a given PoS tag
in the past, it is more likely to be generated again.
85
Besides the basic fertility model, we introduce
also an extended fertility model, which uses fre-
quency of a given word form for generating number
of children. We assume that the most frequent words
are mostly function words (e.g. determiners, prepo-
sitions, auxiliary verbs, conjunctions). Such words
tend to have a stable number of children, for exam-
ple (i) some function words are exclusively leaves,
(ii) prepositions have just one child, and (iii) attach-
ment of auxiliary verbs depends on the annotation
style, but number of their children is also not very
variable. The higher the frequency of a word form,
the higher probability mass is concentrated on one
specific number of children and the lower Dirichlet
hyperparameter ? in Equation 4 is needed. The ex-
tended fertility is described by equation
P ?f (fi|ti, wi) =
c?i(?ti, fi?) + ?eF (wi)P0(fi)
c?i(?ti?) + ?eF (wi)
, (6)
where F (wi) is a frequency of the word wi, which
is computed as a number of words wi in our corpus
divided by number of all words.
3.2 Edge model
After the fertility (number of left and right depen-
dents) is generated, the individual slots are filled us-
ing the edge model. A part-of-speech tag of each de-
pendent is conditioned by part-of-speech tag of the
head and the edge direction (position of the depen-
dent related to the head).8
Similarly as for the fertility model, we employ
Chinese restaurant process to assign probabilities of
individual dependent.
Pe(tj |ti, dj) =
c?i(?ti, tj , dj?) + ?
c?i(?ti, dj?) + ?|T |
, (7)
where ti and tj are the part-of-speech tags of the
head and the generated dependent respectively; dj is
a direction of edge between the words i and j, which
can have two values: left and right. c?i(?ti, tj , dj?)
stands for the count of edges ti ? tj with the direc-
tion dj in the history, |T | is a number of unique tags
in the corpus and ? is a Dirichlet hyperparameter.
8For the edge model purposes, the PoS tag of the technical
root is set to ?<root>? and it is in the zero-th position in the
sentence, so the head word of the sentence is always its right
dependent.
3.3 Distance model
Distance model is an auxiliary model that prevents
the resulting trees from being too flat.9 This sim-
ple model says that shorter edges are more probable
than longer ones. We define probability of a distance
between a word and its parent as its inverse value,10
which is then normalized by the normalization con-
stant d.
Pd(i, j) =
1
d
(
1
|i? j|
)?
(8)
The hyperparameter ? determines the weight of this
model.
3.4 Subtree model
The subtree model uses the reducibility measure. It
plays an important role since it forces the reducible
words to be leaves and reducible n-grams to be sub-
trees. Words with low reducibility are forced to-
wards the root of the tree. We define desc(i) as a
sequence of tags [tl, . . . , tr] that corresponds to all
the descendants of the word wi including wi, i.e. the
whole subtree of wi. The probability of such sub-
tree is proportional to its reducibility R(desc(i)).
The hyperparameter ? determines the weight of the
model; s is a normalization constant.
Ps(i) =
1
s
R(desc(i))? (9)
3.5 Probability of the whole treebank
We want to maximize the probability of the whole
generated treebank, which is computed as follows:
Ptreebank =
n?
i=1
(P ?f (fi|ti, wi) (10)
Pe(ti|tpi(i), di) (11)
Pd(i, pi(i)) (12)
Ps(i)), (13)
where pi(i) denotes the parent of the word on the
position i. We multiply the probabilities of fertil-
ity, edge, distance from parent, and subtree over all
9Ideally, the distance model would not be needed, but exper-
iments showed that it helps to infer better trees.
10Distance between any word and the technical root of the
dependency tree was set to 10. Since each technical root has
only one dependent, this value does not affect the model.
86
The   dog   was   in   the   park  .
(((The) dog) was (in ((the) park)) (.))
Figure 1: Arrow and bracketing notation of a projective
dependency tree.
words (nodes) in the corpus. The extended fertility
model P ?f can be substituted by its basic variant Pf .
4 Sampling algorithm
For stochastic searching for the most probable de-
pendency trees, we employ Gibbs sampling, a stan-
dard Markov Chain Monte Carlo technique (Gilks et
al., 1996). In each iteration, we loop over all words
in the corpus in a random order and change the de-
pendencies in their neighborhood (a small change
described in Section 4.2). In the end, ?average? trees
based on the whole sampling are built.
4.1 Initialization
Before the sampling starts, we initialize the projec-
tive trees randomly in a way that for each sentence,
we choose randomly one word as the head and attach
all other words to it.11
4.2 Small Change Operator
We use the bracketing notation for illustrating the
small change operator. Each projective dependency
tree consisting of n words can be expressed by n
pairs of brackets. Each bracket pair belongs to one
node and delimits its descendants from the rest of
the sentence. Furthermore, each bracketed segment
contains just one word that is not embedded deeper;
this node is the segment head. An example of this
notation is in Figure 1.
The small change is then very simple. We remove
one pair of brackets and add another, so that the con-
ditions defined above are not violated. An example
of such change is in Figure 2.
From the perspective of dependency structures,
the small change can be described as follows:
11More elaborated methods for generating random trees con-
verges to similar results. Therefore we conclude that the choice
of the initialization mechanism is not so important here.
TTThe dogwadosinootpoTTre doki.(dooT)dd
TTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddTTd d
TTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddT Td d
Figure 2: An example of small change in a projective
tree. The bracket (in the park) is removed and there are
five possibilities how to replace it.
1. Pick a random non-root word w (the word in in
our example) and find its parent p (the word was).
2. Find all other children of w and p (the words
dog, park, and .) and denote this set by C.
3. Choose the new head out of w and p. Mark the
new head as g and the second candidate as d. Attach
d to g.
4. Select a neighborhood D adjacent to the word
d as a continuous subset of C and attach all words
from D to d. D may be also empty.
5. Attach the remaining words from C that were
not in D to the new head g.
4.3 Building ?average? trees
The ?burn-in? period is set to 10 iterations. After
this period, we begin to count how many times an
edge occurs at a particular location in the corpus.
This counts are collected over the whole corpus with
the collection-rate 0.01.12
When the sampling is finished, the final depen-
dency trees are built using such edges that belonged
to the most frequent ones during the sampling. We
employ the maximum spanning tree (MST) algo-
rithm (Chu and Liu, 1965) to find them.13 Tree pro-
jectivity is not guaranteed by the MST algorithm.
5 Experiments
We evaluated our parser on 10 treebanks included in
the WILS shared-task data. Similarly to some previ-
ous papers on unsupervised parsing (Gillenwater et
al., 2011; Spitkovsky et al, 2011), the tuning exper-
iments were performed on English only. We used
12After each small change is made, the edges from the whole
corpus are collected with a probability 0.01.
13The weights of edges needed in MST algorithm correspond
to the number of times they were present during the sampling.
87
language tokens (mil.) language tokens (mil.)
Arabic 19.7 English 85.0
Basque 14.1 Portuguese 31.7
Czech 20.3 Slovenian 13.7
Danish 15.9 Swedish 19.2
Dutch 27.1
Table 2: Wikipedia texts statistics
English development data for checking functional-
ity of the individual models and for optimizing hy-
perparameter values. The best configuration of the
parser achieved on English was then used for pars-
ing all other languages. This simulates the situation
in which we have only one treebank (English) on
which we can tune our parser and we want to parse
other languages for which we have no manually an-
notated treebanks.
5.1 Data
For each experiment, we need two kinds of data: a
smaller treebank, which is used for sampling and
for evaluation, and a large corpus, from which we
compute n-gram reducibility scores. The induction
of dependency trees and evaluation is done only on
WILS testing data.
For obtaining reducibility scores, we used
Wikipedia articles downloaded by Majlis? and
Z?abokrtsky? (2012). Their statistics across languages
are showed in Table 2. To make them useful, the
necessary preprocessing steps must have been done.
The texts were first automatically segmented and to-
kenized14 and then they were part-of-speech tagged
by TnT tagger (Brants, 2000), which was trained on
the respective WILS training data. The quality of
such tagging is not very high, since we do not use
any lexicons15 or pretrained models. However, it is
sufficient for obtaining good reducibility scores.
5.2 Setting the hyperparameters
The applicability of individual models and their pa-
rameters were tested on English development data
14The segmentation to sentences and tokenization was per-
formed using the TectoMT framework (Popel and Z?abokrtsky?,
2010).
15Using lexicons or another pretrained models for tagging
means using other sources of human annotated data, which is
not allowed if we want to compare our results with others.
and full part-of-speech tags. The four hyperparame-
ters ? (fertility model), ? (edge model), ? (distance
model), and ? (subtree model), were set by a grid
search algorithm, which found the following opti-
mal values:
?e = 0.01, ? = 1, ? = 1.5, ? = 1
5.3 Results
The best setting from the experiments on English
is now used for evaluating our parser across all
WILS treebanks. Besides using the standard part-of-
speech tags (POS), the experiments were done also
on coarse tags (CPOS) and universal tags (UPOS).
From results presented in Table 3, it is obvious that
our systems outperforms all the given baselines con-
sidering the average scores across all the testing lan-
guages. However, it is important to note that we
used an additional source of information, namely
large unannotated corpora for computing reducibil-
ity scores, while the baseline system (Gillenwater et
al., 2011) use only the WILS datasets.
6 Conclusions and Future Work
We have described a novel unsupervised depen-
dency parsing system employing new features, such
as reducibility or fertility. The reducibility scores are
extracted from a large corpus, and the computation-
ally demanding inference is then done on smaller
data.
In future work, we would like to estimate the
hyperparameters automatically, for example by the
Metropolis-Hastings technique (Gilks et al, 1996).
Furthermore, we would like to add lexicalized mod-
els and automatically induced word classes instead
of the PoS tags designed by humans. Finally, we
would like to move towards deeper syntactic struc-
tures, where the tree would be formed only by con-
tent words and the function words would be treated
in a different way.
References
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. Proceedings of the sixth conference
on Applied natural language processing, page 8.
Y. J. Chu and T. H. Liu. 1965. On the Shortest Arbores-
cence of a Directed Graph. Science Sinica, 14:1396?
1400.
88
baselines Gillenwater et al (2011) our approach
Treebank random left br. right br. CPOS POS UPOS CPOS POS UPOS
Arabic PADT 37.0 5.2 58.7 14.9 14.5 23.4 12.7 57.2 52.0
Basque 3LB 9.8 32.4 22.5 48.9 41.8 30.0 21.0 25.5 22.3
Czech PDT 10.8 23.9 29.1 25.6 31.6 27.5 49.1 42.9 44.1
Danish CDT 24.7 13.3 47.4 31.8 27.2 26.3 48.4 41.4 49.7
Dutch Alpino 17.1 27.9 24.6 23.3 21.2 23.5 28.3 44.2 29.9
English Childes 12.8 34.8 22.2 48.8 50.4 36.2 54.2 44.2 49.3
English PTB 13.2 31.4 20.4 30.9 34.4 25.9 41.0 50.3 37.5
Portuguese Floresta 33.1 25.9 31.1 26.0 31.1 25.5 50.2 49.5 29.3
Slovene JOS 9.5 31.1 10.8 36.6 53.3 36.6 30.4 40.8 26.7
Swedish Talbanken 23.9 25.8 28.0 27.2 27.2 27.1 48.4 50.6 52.6
Average: 19.2 25.2 29.5 31.4 33.3 28.2 38.4 44.7 39.3
Table 3: Directed attachment scores on the WILS testing data (all sentences). Punctuation was excluded. Comparison
with simple baselines and the given baseline system (Gillenwater et al, 2011).
Kim Gerdes and Sylvain Kahane. 2011. Defining depen-
dencies (and constituents). In Proceedings of Depen-
dency Linguistics 2011, Barcelona.
W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. 1996.
Markov chain Monte Carlo in practice. Interdisci-
plinary statistics. Chapman & Hall.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-
nando Pereira, and Ben Taskar. 2011. Posterior
Sparsity in Unsupervised Dependency Parsing. The
Journal of Machine Learning Research, 12:455?490,
February.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 101?109, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Marke?ta Lopatkova?, Martin Pla?tek, and Vladislav Kubon?.
2005. Modeling syntax of free word-order languages:
Dependency analysis by reduction. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors,
Lecture Notes in Artificial Intelligence, Proceedings of
the 8th International Conference, TSD 2005, volume
3658 of Lecture Notes in Computer Science, pages
140?147, Berlin / Heidelberg. Springer.
Martin Majlis? and Zdene?k Z?abokrtsky?. 2012. Language
richness of the web. In Proceedings of LREC2012, Is-
tanbul, Turkey, May. ELRA, European Language Re-
sources Association. In print.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011).
89
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267?274,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Formemes in English-Czech Deep Syntactic MT ?
Ondr?ej Du?ek, Zdene?k ?abokrtsk?, Martin Popel,
Martin Majli?, Michal Nov?k, and David Marec?ek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?me?st? 25, Prague
{odusek,zabokrtsky,popel,majlis,mnovak,marecek}@ufal.mff.cuni.cz
Abstract
One of the most notable recent improve-
ments of the TectoMT English-to-Czech trans-
lation is a systematic and theoretically sup-
ported revision of formemes?the annotation
of morpho-syntactic features of content words
in deep dependency syntactic structures based
on the Prague tectogrammatics theory. Our
modifications aim at reducing data sparsity,
increasing consistency across languages and
widening the usage area of this markup.
Formemes can be used not only in MT, but in
various other NLP tasks.
1 Introduction
The cornerstone of the TectoMT tree-to-tree ma-
chine translation system is the deep-syntactic lan-
guage representation following the Prague tec-
togrammatics theory (Sgall et al, 1986), and its ap-
plication in the Prague Dependency Treebank (PDT)
2.01 (Hajic? et al, 2006), where each sentence is
analyzed to a dependency tree whose nodes corre-
spond to content words. Each node has a number
of attributes, but the most important (and difficult)
for the transfer phase are lemma?lexical informa-
tion, and formeme?surface morpho-syntactic infor-
? This research has been supported by the grants
FP7-ICT-2009-4-247762 (FAUST), FP7-ICT-2009-4-249119
(Metanet), LH12093 (Kontakt II), DF12P01OVV022 (NAKI),
201/09/H057 (Czech Science Foundation), GAUK 116310, and
SVV 265 314. This work has been using language resources de-
veloped and/or stored and/or distributed by the LINDAT-Clarin
project of the Ministry of Education of the Czech Republic
(project LM2010013).
1http://ufal.mff.cuni.cz/pdt2.0
mation, including selected auxiliary words (Pt?c?ek
and ?abokrtsk?, 2006; ?abokrtsk? et al, 2008).
This paper focuses on formemes?their definition
and recent improvements of the annotation, which
has been thoroughly revised in the course of prepa-
ration of the CzEng 1.0 parallel corpus (Bojar et al,
2012b), whose utilization in TectoMT along with the
new formemes version has brought the greatest ben-
efit to our English-Czech MT system in the recent
year. However, the area of possible application of
formemes is not limited to MT only or to the lan-
guage pair used in our system; the underlying ideas
are language-independent.
We summarize the development of morpho-
syntactic annotations related to formemes (Sec-
tion 2), provide an overview of the whole TectoMT
system (Section 3), then describe the formeme an-
notation (Section 4) and our recent improvements
(Section 5), as well as experimental applications, in-
cluding English-Czech MT (Section 6). The main
asset of the formeme revision is a first systematic re-
organization of the existing practical aid, providing
it with a solid theoretical base, but still bearing its
intended applications in mind.
2 Related Work
Numerous theoretical approaches had been made
to morpho-syntactic description, mainly within va-
lency lexicons, starting probably with the work by
Helbig and Schenkel (1969). Perhaps the best one
for Czech is PDT-VALLEX (Hajic? et al, 2003), list-
ing all possible subtrees corresponding to valency
arguments (Ure?ov?, 2009). ?abokrtsk? (2005)
gives an overview of works in this field.
267
This kind of information has been most exploited
in structural MT systems, employing semantic re-
lations (Menezes and Richardson, 2001) or surface
tree substructures (Quirk et al, 2005; Marcu et al,
2006). Formemes, originally developed for Natural
Language Generation (NLG) (Pt?c?ek and ?abokrt-
sk?, 2006), have been successfully applied to MT
within the TectoMT system. Our revision of for-
meme annotation aims to improve the MT perfor-
mance, keeping other possible applications in mind.
3 The TectoMT English-Czech Machine
Translation System
The TectoMT system is a structural machine trans-
lation system with deep transfer, first introduced
by ?abokrtsk? et al (2008). It currently supports
English-to-Czech translation. Its analysis stage
follows the Prague tectogrammatics theory (Sgall,
1967; Sgall et al, 1986), proceeding over two layers
of structural description, from shallow (analytical)
to deep (tectogrammatical) (see Section 3.1).
The transfer phase of the system is based on Max-
imum Entropy context-sensitive translation models
(Marec?ek et al, 2010) and Hidden Tree Markov
Models (?abokrtsk? and Popel, 2009). It is factor-
ized into three subtasks: lemma, formeme and gram-
matemes translation (see Sections 3.2 and 3.3).
The subsequent generation phase consists of rule-
based components that gradually change the deep
target language representation into a shallow one,
which is then converted to text (cf. Section 6.1).
The version of TectoMT submitted to WMT122
builds upon the WMT11 version. Several rule-based
components were slightly refined. However, most of
the effort was devoted to creating a better and bigger
parallel treebank?CzEng 1.03 (Bojar et al, 2012b),
and re-training the statistical components on this re-
source. Apart from bigger size and improved filter-
ing, one of the main differences between CzEng 0.9
(Bojar and ?abokrtsk?, 2009) (used in WMT11) and
CzEng 1.0 (used in WMT12) is the revised annota-
tion of formemes.
2http://www.statmt.org/wmt12
3http://ufal.mff.cuni.cz/czeng
3.1 Layers of structural analysis
There are two distinct structural layers used in the
TectoMT system:
? Analytical layer. A surface syntax layer, which
includes all tokens of the sentence, organized
into a labeled dependency tree. The labels cor-
respond to surface syntax functions.
? Tectogrammatical layer. A deep syntax/se-
mantic layer describing the linguistic meaning
of the sentence. Its dependency trees include
only content words as nodes, assigning to each
of them a deep lemma (t-lemma), a semantic
role label (functor), and other deep linguistic
features (grammatemes), such as semantic part-
of-speech, person, tense or modality.
The analytical layer can be obtained using differ-
ent dependency parsers (Popel et al, 2011); the tec-
togrammatical representation is then created by rule-
based modules from the analytical trees.
In contrast to the original PDT annotation,
the TectoMT tectogrammatical layer also includes
formemes describing the surface morpho-syntactic
realization of the nodes (cf. also Section 3.3).
3.2 Transfer: Translation Factorization and
Symmetry
Using the tectogrammatical representation in struc-
tural MT allows separating the problem of translat-
ing a sentence into relatively independent simpler
subtasks: lemma, functors, and grammatemes trans-
lation (Bojar et al, 2009; ?abokrtsk?, 2010). Since
topology changes to deep syntax trees are rare in MT
transfer, each of these three subtasks allows a vir-
tually symmetric source-target one-to-one mapping,
thus simplifying the initial n-to-m mapping of word
phrases or surface subtrees.
?abokrtsk? et al (2008) obviated the need for
transfer via functors (i.e. semantic role detection)
by applying a formeme transfer instead. While
formeme values are much simpler to obtain by au-
tomatic processing, this approach preserved the ad-
vantage of symmetric one-to-one value translation.
Moreover, translations of a given source morpho-
syntactic construction usually follow a limited num-
ber of patterns in the target language regardless of
268
their semantic functions, e.g. a finite clause will
most often be translated as a finite clause.
3.3 Motivation for the Introduction of
Formemes
Surface-oriented formemes have been introduced
into the semantics-oriented tectogrammatical layer,
as it proves beneficial to combine the deep syntax
trees, smaller in size and more consistent across lan-
guages, with the surface morphology and syntax to
provide for a straightforward transition to the surface
level (?abokrtsk?, 2010).
The three-fold factorization of the transfer phase
(see Section 3.2) helps address the data sparsity is-
sue faced by today?s MT systems. As the translation
of lemmas and their morpho-syntactic forms is sepa-
rated, combinations unseen in the training data may
appear on the output.
To further reduce data sparsity, only minimal in-
formation needed to reconstruct the surface form is
stored in formemes; morphological categories deriv-
able from elsewhere, i.e. morphological agreement
or grammatemes, are discarded.
4 Czech and English Formemes in
TectoMT
A formeme is a concise description of relevant
morpho-syntactic features of a node in a tectogram-
matical tree (deep syntactic tree whose nodes usu-
ally correspond to content words). The general
shape of revised Czech and English formemes, as
implemented within the Treex4 NLP framework
(Popel and ?abokrtsk?, 2010) for the TectoMT sys-
tem, consists of three main parts:
1. Syntactic part-of-speech.5 The number of syn-
tactic parts-of-speech is very low, as only con-
tent words are used on the deep layer and the
categories of pronouns and numerals have been
divided under nouns and adjectives accord-
ing to syntactic behavior (?evc??kov?-Raz?mov?
and ?abokrtsk?, 2006). The possible values are
v for verbs, n for nouns, adj for adjectives,
and adv for adverbs.
4http://ufal.mff.cuni.cz/treex/,
https://metacpan.org/module/Treex
5Cf. Section 5.2 for details.
2. Subordinate conjunction/preposition. Applies
only to formemes of prepositional phrases and
subordinate clauses introduced by a conjunc-
tion and contains the respective conjunction or
preposition; e.g. if, on or in_case_of.
3. Form. This part represents the morpho-
syntactic form of the node in question and de-
pends on the part-of-speech (see Table 1).
The two or three parts are concatenated into
a human-readable string to facilitate usage in
hand-written rules as well as statistical systems
(?abokrtsk?, 2010), producing values such as
v:inf, v:if+fin or n:into+X. Formeme val-
ues of nodes corresponding to uninflected words are
atomic.
Formemes are detected by rule-based modules op-
erating on deep and surface trees. Example deep
syntax trees annotated with formemes are shown in
Fig. 1. A listing of all possible formeme values is
given in Table 1.
Verbal formemes remain quite consistent in both
languages, except for the greater range of forms in
English (Czech uses adjectives or nouns instead of
gerunds and verbal attributes). Nominal formemes
differ more significantly: Czech is a free-word order
language with rich morphology, where declension
is important to syntactic relations?case is therefore
included in formemes. As English makes its syntac-
tic relations visible rather with word-order than with
morphology, English formemes indicate the syntac-
tic position instead. The same holds for adjecti-
val complements to verbs. Posession is expressed
mostly using nouns in English and adjectives in
Czech, which is also reflected in formemes.
5 Recent Markup Improvements
Our following markup innovations address several
issues found in the previous version and aim to adapt
the range of values more accurately to the intended
applications.
5.1 General Form Changes
The relevant preposition and subordinate conjunc-
tion nodes had been selected based on their depen-
dency labels; we use a simple part-of-speech tag fil-
ter instead in order to minimize the influence of pars-
ing errors and capture more complex prepositions,
269
Figure 1: An example English and Czech deep sentence structure annotated with formemes (in typewriter font).
Formeme Language Definition
v:(P+)fin both Verbs as heads of finite clauses
v:rc both Verbs as heads of relative clauses
v:(P+)inf both Infinitive clauses; typically with the particle to in English?
v:(P+)ger EN Gerunds, e.g. I like reading (v:ger), but I am tired of arguing (v:of+ger).
v:attr EN Present or past participles (i.e. -ing or -ed forms) in the attributive syntactic
position, e.g. Striking (v:attr) teachers hate bored (v:attr) students.
n:[1..7] CS Bare nouns; the numbers indicate morphological case?
n:X CS Bare nouns that cannot be inflected
n:subj EN Nouns in the subject position (i.e. in front of the main verb of the clause)
n:obj EN Nouns in the object position (i.e. following the verb with no preposition)
n:obj1, n:obj2 EN Nouns in the object position; distinguishing the two objects of ditransitive
verbs (e.g. give, consider)
n:adv EN Nouns in an adverbial position, e.g. The sales went up by 1 % last month
n:P+X EN Prepositional phrases
n:P+[1..7] CS Prepositional phrases; the preposition surface form is combined with the re-
quired case?
n:attr both Nominal attributes, e.g. insurance company or president Smith in English
and prezident Smith in Czech
n:poss EN English possessive pronouns and nouns with the ?s suffix
adj:attr both Adjectival attributes (Czech inflection forms need not be stored thanks to
congruency with the parent noun)
adj:compl EN Direct adjectival complements to verbs
adj:[1..7] CS Direct adjectival complements to verbs (morphological case must be stored
in Czech, as it is determined by valency)
adj:poss CS Czech possesive adjectives and pronouns; a counterpart to English n:poss
adv both Adverbs (not inflected, can take no prepositions etc.)
x both Coordinating conjunctions, other uninflected words
drop both Deep tree nodes which do not appear on the surface (e.g. pro-drop pronouns)
?I.e. infinitives as head of clauses, not infinitives as parts of compound verb forms with finite auxiliary verbs.
?Numbers are traditionally used to mark morphological case in Czech; 1 stands for nominative, 2 for genitive etc.
?Since many prepositions may govern multiple cases in Czech, the case number is necessary.
Table 1: A listing of all possible formeme values, indicating their usage in Czech, English or both languages. ?P+?
denotes the (lowercased) surface form of a preposition or a subordinate conjunction. Round brackets denote optional
parts, square brackets denote a set of alternatives.
270
e.g. in case of. Our revision also allows combining
prepositions with all English gerunds and infinitives,
preventing a loss of important data.
We also use the lowercased surface form in the
middle formeme part instead of lemmas to allow for
a more straightforward surface form generation.
5.2 Introducing Syntactic Part-of-Speech
Formemes originally contained the semantic part-of-
speech (sempos) (Raz?mov? and ?abokrtsk?, 2006)
as their first part. We replaced it with a syntac-
tic part-of-speech (syntpos), since it proved compli-
cated to assign sempos reliably by a rule-based mod-
ule and morpho-syntactic behavior is more relevant
to formemes than semantics.
The syntpos is assigned in two steps:
1. A preliminary syntpos is selected, using our
categorization based on the part-of-speech tag
and lemma.
2. The final syntpos is selected according to the
syntactic position of the node, addressing nom-
inal usage of adjectives and cardinal numerals
(see Sections 5.4 and 5.5).
5.3 Capturing Czech Nominal Attributes
Detecting the attributive usage of nouns is straight-
forward for English, where any noun depending di-
rectly on another noun is considered an attribute.
In Czech, one needs to distinguish case-congruent
attributes from others that have a fixed case. We
aimed at assigning the n:attr formeme only in the
former case and thus replaced the original method
based on word order with a less error-prone one
based on congruency and named entity recognition.
5.4 Numerals: Distinguishing Usage and
Correcting Czech Case
The new formemes now distinguish adjectival and
nominal usage of cardinal numerals (cf. also Sec-
tion 5.2), e.g. the number in 5 potatoes is now as-
signed the adj:attr formeme, whereas Apollo 11
is given n:attr. The new situation is analogous
in Czech, with nominal usages of numerals having
their morphological case marked in formemes.
To reduce data sparsity in the new formemes ver-
sion, we counter the inconsistent syntactic behavior
of Czech cardinal numerals, where 1-4 behave like
The word ban?n is in genitive (n:2), but would have an ac-
cusative (n:4) form if the numeral behaved like an adjective.
Figure 2: Case correction with numerals in Czech.
adjectives but other numerals behave like nouns and
shift their semantically governing noun to the po-
sition of a genitive attribute. An example of this
change is given in Fig. 2.
5.5 Adjectives: Nominal Usage and Case
The new formemes address the usage of adjectives
in the syntactic position of nouns (cf. Section 5.2),
which occurs only rarely, thus preventing sparse val-
ues, namely in these syntactic positions:
? The subject. We replaced the originally as-
signed adj:compl value, which was impos-
sible to tell from adjectival objects, with the
formeme a noun would have in the same po-
sition, e.g. in the sentence Many of them were
late, the subject many is assigned n:subj.
? Prepositional phrases. Syntactic behavior of
adjectives is identical to nouns here; we thus
assign them the formeme values a noun would
receive in the same position, e.g. n:of+X in-
stead of adj:of+X in He is one of the best at
school.
In Czech, we detect nominal usage of adjectives
in verbal direct objects as well, employing large-
coverage valency lexicons (Lopatkov? et al, 2008;
Hajic? et al, 2003).
Instead of assigning the compl value in Czech,
our formemes revision includes the case of adjecti-
val complements, which depends on the valency of
the respective verb.
5.6 Mutual Information Across Languages
The changes described above have been motivated
not only by theoretical linguistic description of the
languages in question, but also by the intended us-
age within the TectoMT translation system. Instead
271
of retraining the translation model after each change,
we devised a simpler and faster estimate to measure
the asset of our innovations: using Mutual Informa-
tion (MI) (Manning and Sch?tze, 1999, p. 66) of
formemes in Czech and English trees.
We expect that an inter-language MI increase will
lead to lower noise in formeme-to-formeme transla-
tion dictionary (Bojar et al, 2009, cf. Section 3.2),
thus achieving higher MT output quality.
Using the analysis pipeline from CzEng1.0, we
measured the inter-language MI on sentences from
the Prague Czech-English Dependency Treebank
(PCEDT) 2.0 (Bojar et al, 2012a). The overall re-
sults show an MI increase from 1.598 to 1.687 (Bo-
jar et al, 2012b). Several proposed markup changes
have been discarded as they led to an inter-language
MI drop; e.g. removing the v:rc relative clause
formeme or merging the v:attr and adj:attr
values in English.
6 Experimental Usage
We list here our experiments with the newly de-
veloped annotation: an NLG experiment aimed at
assessing the impact of formemes on the synthesis
phase of the TectoMT system, and the usage in the
English-Czech MT as a whole.
6.1 Czech Synthesis
The synthesis phase of the TectoMT system relies
heavily on the information included in formemes, as
its rule-based blocks use solely formemes and gram-
mar rules to gradually change a deep tree node into
a surface subtree.
To directly measure the suitability of our changes
for the synthesis stage of the TectoMT system, we
used a Czech-to-Czech round trip?deep analysis of
Czech PDT 2.0 development set sentences using the
CzEng 1.0 pipeline (Bojar et al, 2012b), followed
directly by the synthesis part of the TectoMT sys-
tem. The results were evaluated using the BLEU
metric (Papineni et al, 2002) with the original sen-
tences as reference; they indicate a higher suitability
of the new formemes for deep Czech synthesis (see
Table 2).
6.2 English-Czech Machine Translation
To measure the influence of the presented formeme
revision on the translation quality, we compared
Version BLEU
Original formemes 0.6818
Revised formemes 0.7092
Table 2: A comparison of formeme versions in Czech-to-
Czech round trip.
Version BLEU
Original formemes 0.1190
Revised formemes 0.1199
Table 3: A comparison of formeme versions in English-
to-Czech TectoMT translation on the WMT12 test set.
two translation scenarios?one using the origi-
nal formemes and the second using the revised
formemes in the formeme-to-formeme translation
model. Due to time reasons, we were able to
train both translation models only on 1/2 of the
CzEng 1.0 training data.
The results in Table 3 demonstrate a slight6 BLEU
gain when using the revised formemes version. The
gain is expected to be greater if several rule-based
modules of the transfer phase are adapted to the re-
visions.
7 Conclusion and Further Work
We have presented a systematic and theoretically
supported revision of a surface morpho-syntactic
markup within a deep dependency annotation sce-
nario, designed to facilitate the TectoMT transfer
phase. Our first practical experiments proved the
merits of our innovations in the tasks of Czech syn-
thesis and deep structural MT as a whole. We have
also experimented with formemes in the functor as-
signment (semantic role labelling) task and gained
moderate improvements (ca. 1-1.5% accuracy).
In future, we intend to tune the rule-based parts
of our MT transfer for the new version of formemes
and examine further possibilities of data sparsity re-
duction (e.g. by merging synonymous formemes).
We are also planning to create formeme annotation
modules for further languages to widen the range of
language pairs used in the TectoMT system.
6Significant at 90% level using pairwise bootstrap resam-
pling test (Koehn, 2004).
272
References
O. Bojar and Z. ?abokrtsk?. 2009. CzEng 0.9: Large
Parallel Treebank with Rich Annotation. Prague Bul-
letin of Mathematical Linguistics, 92.
O. Bojar, D. Marec?ek, V. Nov?k, M. Popel, J. Pt?c?ek,
J. Rou?, and Z. ?abokrtsk?. 2009. English-Czech MT
in 2008. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 125?129. As-
sociation for Computational Linguistics.
O. Bojar, J. Hajic?, E. Hajic?ov?, J. Panevov?, P. Sgall,
S. Cinkov?, E. Fuc??kov?, M. Mikulov?, P. Pajas,
J. Popelka, J. Semeck?, J. ?indlerov?, J. ?te?p?nek,
J. Toman, Z. Ure?ov?, and Z. ?abokrtsk?. 2012a.
Announcing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of LREC 2012, Istanbul,
Turkey, May. ELRA, European Language Resources
Association. In print.
O. Bojar, Z. ?abokrtsk?, O. Du?ek, P. Galu?c??kov?,
M. Majli?, D. Marec?ek, J. Mar??k, M. Nov?k,
M. Popel, and A. Tamchyna. 2012b. The Joy of Par-
allelism with CzEng 1.0. In Proceedings of LREC
2012, Istanbul, Turkey, May. ELRA, European Lan-
guage Resources Association. In print.
J. Hajic?, J. Panevov?, Z. Ure?ov?, A. B?mov?,
V. Kol?rov?, and P. Pajas. 2003. PDT-VALLEX: Cre-
ating a large-coverage valency lexicon for treebank an-
notation. In Proceedings of The Second Workshop on
Treebanks and Linguistic Theories, volume 9, pages
57?68.
J. Hajic?, J. Panevov?, E. Hajic?ov?, P. Sgall, P. Pajas,
J. ?te?p?nek, J. Havelka, M. Mikulov?, Z. ?abokrtsk?,
and M. ?evc??kov?-Raz?mov?. 2006. Prague Depen-
dency Treebank 2.0. CD-ROM LDC2006T01, LDC,
Philadelphia.
G. Helbig and W. Schenkel. 1969. W?rterbuch zur
Valenz und Distribution deutscher Verben. VEB Bib-
liographisches Institut, Leipzig.
P. Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
M. Lopatkov?, Z. ?abokrtsk?, V. Kettnerov?, and
K. Skwarska. 2008. Valenc?n? slovn?k c?esk?ch sloves.
Karolinum, Prague.
C.D. Manning and H. Sch?tze. 1999. Foundations of
statistical natural language processing. MIT Press.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntacti-
fied target language phrases. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 44?52. Association for
Computational Linguistics.
D. Marec?ek, M. Popel, and Z. ?abokrtsk?. 2010. Maxi-
mum entropy translation model in dependency-based
MT framework. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and Met-
rics (MATR), pages 201?206. Association for Compu-
tational Linguistics.
A. Menezes and S. D. Richardson. 2001. A best-first
alignment algorithm for automatic extraction of trans-
fer mappings from bilingual corpora. In Proceed-
ings of the workshop on Data-driven methods in ma-
chine translation - Volume 14, DMMT ?01, pages 1?8,
Stroudsburg, PA. Association for Computational Lin-
guistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
M. Popel and Z. ?abokrtsk?. 2010. TectoMT: modular
NLP framework. Advances in Natural Language Pro-
cessing, pages 293?304.
M. Popel, D. Marec?ek, N. Green, and Z. ?abokrtsk?.
2011. Influence of parser choice on dependency-based
MT. In Chris Callison-Burch, Philipp Koehn, Christof
Monz, and Omar Zaidan, editors, Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 433?439, Edinburgh, UK. Association for Com-
putational Linguistics.
J. Pt?c?ek and Z. ?abokrtsk?. 2006. Synthesis of
Czech sentences from tectogrammatical trees. In Text,
Speech and Dialogue, pages 221?228. Springer.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 271?279. Association for Computational
Linguistics.
M. Raz?mov? and Z. ?abokrtsk?. 2006. Annotation
of grammatemes in the Prague Dependency Treebank
2.0. In Proceedings of the LREC 2006 Workshop on
Annotation Science, pages 12?19.
M. ?evc??kov?-Raz?mov? and Z. ?abokrtsk?. 2006. Sys-
tematic parameterized description of pro-forms in the
Prague Dependency Treebank 2.0. In J. Hajic? and
J. Nivre, editors, Proceedings of the Fifth Workshop on
Treebanks and Linguistic Theories (TLT), pages 175?
186, Prague.
P. Sgall, E. Hajic?ov?, J. Panevov?, and J. Mey. 1986. The
meaning of the sentence in its semantic and pragmatic
aspects. Springer.
P. Sgall. 1967. Generativn? popis jazyka a c?esk? dekli-
nace. Academia, Prague.
Z. Ure?ov?. 2009. Building the PDT-VALLEX valency
lexicon. In On-line proceedings of the fifth Corpus
Linguistics Conference. University of Liverpool.
273
Z. ?abokrtsk?, J. Pt?c?ek, and P. Pajas. 2008. Tec-
toMT: highly modular MT system with tectogrammat-
ics used as transfer layer. In Proceedings of the Third
Workshop on Statistical Machine Translation, StatMT
?08, pages 167?170, Stroudsburg, PA. Association for
Computational Linguistics.
Z. ?abokrtsk?. 2005. Valency Lexicon of Czech Verbs.
Ph.D. thesis, Charles University in Prague.
Z. ?abokrtsk?. 2010. From Treebanking to Machine
Translation. Habilitation thesis, Charles University in
Prague.
Z. ?abokrtsk? and M. Popel. 2009. Hidden Markov
Tree Model in Dependency-based Machine Transla-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 145?148, Suntec, Singa-
pore.
274
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 72?77,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using an SVM Ensemble System for Improved Tamil Dependency Parsing
Nathan Green, Loganathan Ramasamy and Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
Prague, Czech Republic
{green,ramasamy,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Dependency parsing has been shown to im-
prove NLP systems in certain languages and
in many cases helps achieve state of the art re-
sults in NLP applications, in particular appli-
cations for free word order languages. Mor-
phologically rich languages are often short on
training data or require much higher amounts
of training data due to the increased size of
their lexicon. This paper examines a new
approach for addressing morphologically rich
languages with little training data to start.
Using Tamil as our test language, we cre-
ate 9 dependency parse models with a lim-
ited amount of training data. Using these
models we train an SVM classifier using only
the model agreements as features. We use
this SVM classifier on an edge by edge deci-
sion to form an ensemble parse tree. Using
only model agreements as features allows this
method to remain language independent and
applicable to a wide range of morphologically
rich languages.
We show a statistically significant 5.44%
improvement over the average dependency
model and a statistically significant 0.52% im-
provement over the best individual system.
1 Introduction
Dependency parsing has made many advancements
in recent years. A prime reason for the quick ad-
vancement has been the CoNLL shared task compe-
titions, which gave the community a common train-
ing/testing framework along with many open source
systems. These systems have, for certain languages,
achieved high accuracy ranging from on average
from approximately 60% to 80% (Buchholz and
Marsi, 2006). The range of scores are more of-
ten language dependent rather than system depen-
dent, as some languages contain more morpholog-
ical complexities. While some of these languages
are morphologically rich, we would like to addition-
ally address dependency parsing methods that may
help under-resourced languages as well, which often
overlaps with morphologically rich languages. For
this reason, we have chosen to do the experiments
in this paper using the Tamil Treebank (Ramasamy
and Z?abokrtsky?, 2012).
Tamil belongs to Dravidian family of languages
and is mainly spoken in southern India and also in
parts of Sri Lanka, Malaysia and Singapore. Tamil
is agglutinative and has a rich set of morphologi-
cal suffixes. Tamil has nouns and verbs as two ma-
jor word classes, and hundreds of word forms can
be produced by the application of concatenative and
derivational morphology. Tamil?s rich morphology
makes the language free word order except that it is
strictly head final.
When working with small datasets it is often very
difficult to determine which dependency model will
best represent your data. One can try to pick the
model through empirical means on a tuning set but
as the data grows in the future this model may no
longer be the best choice. The change in the best
model may be due to new vocabulary or through a
domain shift. If the wrong single model is chosen
early on when training is cheap, when the model is
applied in semi supervised or self training it could
lead to significantly reduced annotation accuracy.
72
For this reason, we believe ensemble combinations
are an appropriate direction for lesser resourced lan-
guages, often a large portion of morphologically
rich languages. Ensemble methods are robust as
data sizes grow, since the classifier can easily be re-
trained with additional data and the ensemble model
chooses the best model on an edge by edge basis.
This cost is substantially less than retraining multi-
ple dependency models.
2 Related Work
Ensemble learning (Dietterich, 2000) has been used
for a variety of machine learning tasks and recently
has been applied to dependency parsing in various
ways and with different levels of success. (Surdeanu
and Manning, 2010; Haffari et al, 2011) showed
a successful combination of parse trees through a
linear combination of trees with various weight-
ing formulations. Parser combination with depen-
dency trees have been examined in terms of accu-
racy (Sagae and Lavie, 2006; Sagae and Tsujii,
2007; Zeman and Z?abokrtsky?, 2005; S?gaard and
Rish?j, 2010). (Sagae and Lavie, 2006; Green and
Z?abokrtsky?, 2012) differ in part since their method
guarantees a tree while our system can, in some sit-
uations, produce a forest. POS tags were used in
parser combination in (Hall et al, 2007) for combin-
ing a set of Malt Parser models with an SVM clas-
sifier with success, however we believe our work is
novel in its use of an SVM classifier solely on model
agreements. Other methods of parse combinations
have shown to be successful such as using one parser
to generate features for another parser. This was
shown in (Nivre and McDonald, 2008; Martins et
al., 2008), in which Malt Parser was used as a fea-
ture to MST Parser.
Few attempts were reported in the literature on the
development of a treebank for Tamil. Our exper-
iments are based on the openly available treebank
(TamilTB) (Ramasamy and Z?abokrtsky?, 2012). De-
velopment of TamilTB is still in progress and the ini-
tial results for TamilTB appeared in (Ramasamy and
Z?abokrtsky?, 2011). Previous parsing experiments in
Tamil were done using a rule based approach which
utilized morphological tagging and identification of
clause boundaries to parse the sentences. The results
were also reported for Malt Parser and MST parser.
Figure 1: Process Flow for one run of our SVM Ensemble
system. This Process in its entirety was run 100 times for
each of the 8 data set splits.
When the morphological tags were available during
both training and testing, the rule based approach
performed better than Malt and MST parsers. For
other Indian languages, treebank development is ac-
tive mainly for Hindi and Telugu. Dependency pars-
ing results for them are reported in (Husain et al,
2010).
3 Methodology
3.1 Process Flow
When dealing with small data sizes it is often
not enough to show a simple accuracy increase.
This increase can be very reliant on the train-
ing/tuning/testing data splits as well as the sam-
pling of those sets. For this reason our experi-
ments are conducted over 7 training/tuning/testing
data split configurations. For each configuration
we randomly sample without replacement the train-
ing/tuning/testing data and rerun the experiment 100
times. These 700 runs, each on different samples,
allow us to better show the overall effect on the ac-
curacy metric as well as the statistically significant
changes as described in Section 3.5. Figure 1 shows
this process flow for one run of this experiment.
73
3.2 Parsers
A dependency tree is a special case of a depen-
dency graph that spawns from an artificial root, is
connected, follows a single-head constraint and is
acyclic. Because of this we can look at a large his-
tory of work in graph theory to address finding the
best spanning tree for each dependency graph. The
most common form of this type of dependency pars-
ing is Graph-Based parsing also called arc-factored
parsing and deals with the parameterization of the
edge weights. The main drawback of these meth-
ods is that for projective trees, the worst case sce-
nario for most methods is a complexity of O(n3)
(Eisner, 1996). However, for non-projective pars-
ing Chu-Liu-Edmond?s algorithm has a complexity
of O(n2) (McDonald et al, 2005). The most com-
mon tool for doing this is MST parser (McDonald et
al., 2005). For this parser we generate two models,
one projective and one non-projective to use in our
ensemble system.
Transition-based parsing creates a dependency
structure that is parameterized over the transitions.
This is closely related to shift-reduce constituency
parsing algorithms. The benefit of transition-based
parsing is the use greedy algorithms which have a
linear time complexity. However, due to the greedy
algorithms, longer arc parses can cause error propa-
gation across each transition (Ku?bler et al, 2009).
We make use of Malt Parser (Nivre et al, 2007),
which in the CoNLL shared tasks was often tied
with the best performing systems. For this parser
we generate 7 different models using different train-
ing parameters, seen in Table 1, and use them as
input into our ensemble system along with the two
Graph-based models described above. Each parser
has access to gold POS information as supplied by
the TamilTB described in 3.4.
Dependency parsing systems are often optimized
for English or other major languages. This opti-
mization, along with morphological complexities,
lead other languages toward lower accuracy scores
in many cases. The goal here is to show that
while the corpus is not the same in size or scope of
most CoNLL data, a successful dependency parser
can still be trained from the annotated data through
model combination for morphologically rich lan-
guages.
Training Parameter Model Description
nivreeager Nivre arc-eager
nivrestandard Nivre arc-standard
stackproj Stack projective
stackeager Stack eager
stacklazy Stack lazy
planar Planar eager
2planar 2-Planar eager
Table 1: Table of the Malt Parser Parameters used during
training. Each entry represents one of the parsing algo-
rithms used in our experiments. For more information see
http://www.maltparser.org/options.html
3.3 Ensemble SVM System
We train our SVM classifier using only model agree-
ment features. Using our tuning set, for each cor-
rectly predicted dependency edge, we create
(
N
2
)
features where N is the number of parsing models.
We do this for each model which predicted the cor-
rect edge in the tuning data. So for N = 3 the
first feature would be a 1 if model 1 and model 2
agreed, feature 2 would be a 1 if model 1 and model
3 agreed, and so on. This feature set is novel and
widely applicable to many languages since it does
not use any additional linguistic tools.
For each edge in the ensemble graph, we use our
classifier to predict which model should be correct,
by first creating the model agreement feature set
for the current edge of the unknown test data. The
SVM predicts which model should be correct and
this model then decides to which head the current
node is attached. At the end of all the tokens in a
sentence, the graph may not be connected and will
likely have cycles. Using a Perl implementation of
minimum spanning tree, in which each edge has a
uniform weight, we obtain a minimum spanning for-
est, where each subgraph is then connected and cy-
cles are eliminated in order to achieve a well formed
dependency structure. Figure 2 gives a graphical
representation of how the SVM decision and MST
algorithm create a final Ensemble parse tree which
is similar to the construction used in (Hall et al,
2007; Green and Z?abokrtsky?, 2012). Future itera-
tions of this process could use a multi-label SVM
or weighted edges based on the parser?s accuracy on
tuning data.
74
Figure 2: General flow to create an Ensemble parse tree
3.4 Data Sets
Table 2 shows the statistics of the TamilTB Tree-
bank. The last 2 rows indicate how many word types
have unique tags and how many have two tags. Also,
Table 2 illustrates that most of the word types can
be uniquely identified with single morphological tag
and only around 120 word types take more than one
morphological tag.
Description Value
#Sentences 600
#Words 9581
#Word types 3583
#Tagset size 234
#Types with unique tags 3461
#Types with 2 tags 112
Table 2: TamilTB: data statistics
Since this is a relatively small treebank and in or-
der to confirm that our experiments are not heavily
reliant on one particular sample of data we try a va-
riety of data splits. To test the effects of the train-
ing, tuning, and testing data we try 7 different data
splits. The tuning data in the Section 4 use the for-
mat training-tuning-testing. So 70-20-10 means we
used 70% of the TamilTB for training, 20% for tun-
ing the SVM classifier, and 10% for evaluation.
3.5 Evaluation
Made a standard in the CoNLL shared tasks com-
petition, two standard metrics for comparing depen-
dency parsing systems are typically used. Labeled
attachment score (LAS) and unlabeled attachment
score (UAS). UAS studies the structure of a depen-
dency tree and assesses whether the output has the
correct head and dependency arcs. In addition to the
structure score in UAS, LAS also measures the accu-
racy of the dependency labels on each arc (Buchholz
and Marsi, 2006). Since we are mainly concerned
with the structure of the ensemble parse, we report
only UAS scores in this paper.
To test statistical significance we use Wilcoxon
paired signed-rank test. For each data split we have
100 iterations each with different sampling. Each
model is compared against the same samples so a
paired test is appropriate in this case. We report sta-
tistical significance values for p < 0.01 and p <
0.05.
4 Results and Discussion
Data Average % Increase % Increase
Split SVM UAS over Avg over Best
70-20-10 76.50% 5.13% 0.52%
60-20-20 76.36% 5.68% 0.72%
60-30-10 75.42% 5.44% 0.52%
60-10-30 75.66% 4.83% 0.10%
85-5-10 75.33% 3.10% -1.21%
90-5-5 75.42% 3.19% -1.10%
80-10-10 76.44% 4.84% 0.48%
Table 3: Average increases and decreases in UAS score
for different Training-Tuning-Test samples. The average
was calculated over all 9 models while the best was se-
lected for each data split
For each of the data splits, Table 3 shows the per-
cent increase in our SVM system over both the av-
erage of the 9 individual models and over the best
individual model. As the Table 3 shows, our ap-
proach seems to decrease in value along with the de-
crease in tuning data. In both cases when we only
used 5% tuning data we did not get any improve-
ment in our average UAS scores. Examining Table
4, shows that the decrease in the 90-5-5 split is not
statistically significant however the decrease in 85-
5-10 is a statistically significant drop. However, the
increases in all data splits are statistically significant
except for the 60-20-20 data split. It appears that
75
Model 70-20-10 60-20-20 60-30-10 60-10-30 85-5-10 90-5-5 80-10-10
2planar * * * * * * **
mstnonproj * * * * * * **
mstproj * * * * * * **
nivreeager * * * * ** x *
nivrestandard * * ** x * * *
planar * * * * * * **
stackeager * * * x * ** *
stacklazy * * * x * ** *
stackproj ** * * x ** ** **
Table 4: Statistical Significance Table for different Training-Tuning-Test samples. Each experiment was sampled
100 times and Wilcoxon Statistical Significance was calculated for our SVM model?s increase/decrease over each
individual model. ? = p < 0.01 , ? ? p =< 0.05, x = p ? 0.05
the size of the tuning and training data matter more
than the size of the test data given the low variance
in Table 5. Since the TamilTB is relatively small
when compared to other CoNLL treebanks, we ex-
pect that this ratio may shift more when additional
data is supplied since the amount of out of vocab-
ulary, OOV, words will decrease as well. As OOV
words decrease, we expect the use of additional test
data to have less of an effect.
Data Splits SVM Variance
70-20-10 0.0011
60-20-20 0.0005
60-30-10 0.0010
60-10-30 0.0003
85-5-10 0.0010
90-5-5 0.0028
80-10-10 0.0010
Table 5: Variance of the UAS Scores of our Ensemble
SVM System over 100 data splits
The traditional approach of using as much data as
possible for training does not seem to be as effec-
tive as partitioning more data for tuning an SVM.
For instance the highest training percentage we use
is 90% applied to training with 5% for tuning and
testing each. In this case the best individual model
had a UAS of 76.25% and the SVM had a UAS of
75.42%. One might think using 90% of the data
would achieve a higher overall UAS than using less
training data. On the contrary, we achieve a better
UAS score on average using only 60%, 70%, 80%,
and 85% of the data towards training. This addi-
tional data spent for tuning appears to be worth the
cost.
5 Conclusion
We have shown a new SVM based ensemble parser
that uses only dependency model agreement fea-
tures. The ability to use only model agreements al-
lows us to keep this approach language independent
and applicable to a wide range of morphologically
rich languages. We show a statistically significant
5.44% improvement over the average dependency
model and a statistically significant 0.52% improve-
ment over the best individual system.
In the future we would like to examine how our
data splits? results change as more data is added.
This might be a prime use for self training. Since
the tuning data size for the SVM seems most impor-
tant, the UAS may be improved by only adding self
training data to our tuning sets. This would have the
additional benefit of eliminating the need to retrain
the individual parsers, thus saving computation time.
The tuning size may have a reduced effect for larger
treebanks but in our experiments it is critical to the
smaller treebank. Additionally, a full comparison of
various ensemble parsing error distributions will be
needed.
6 Acknowledgments
This research has received funding from the Euro-
pean Commission?s 7th Framework Program (FP7)
under grant agreement n? 238405 (CLARA)
76
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. In Proceedings of the First Interna-
tional Workshop on Multiple Classifier Systems, MCS
?00, pages 1?15, London, UK. Springer-Verlag.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?345,
Copenhagen, August.
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hybrid
Combination of Constituency and Dependency Trees
into an Ensemble Dependency Parser. In Proceedings
of the Workshop on Innovative Hybrid Approaches to
the Processing of Textual Data, pages 19?26, Avignon,
France, April. Association for Computational Linguis-
tics.
Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar.
2011. An ensemble model that combines syntactic
and semantic clustering for discriminative dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 710?714, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single Malt or Blended? A Study in Mul-
tilingual Parser Optimization. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 933?939.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The icon-2010 tools contest
on indian language dependency parsing. In Proceed-
ings of ICON-2010 Tools Contest on Indian Language
Dependency Parsing, pages 1?8.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis lectures on hu-
man language technologies. Morgan & Claypool, US.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 157?166, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2011.
Tamil dependency parsing: results using rule based
and corpus based approaches. In Proceedings of the
12th international conference on Computational lin-
guistics and intelligent text processing - Volume Part I,
CICLing?11, pages 82?95, Berlin, Heidelberg.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2012.
Prague dependency style treebank for Tamil. In Pro-
ceedings of LREC 2012, I?stanbul, Turkey.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 129?132, New
York City, USA, June. Association for Computational
Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Anders S?gaard and Christian Rish?j. 2010. Semi-
supervised dependency parsing using generalized tri-
training. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 1065?1073, Beijing, China, August.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: cheap and
good? In HLT: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 649?652,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel Zeman and Zdene?k Z?abokrtsky?. 2005. Improving
parsing accuracy by combining diverse dependency
parsers. In In: Proceedings of the 9th International
Workshop on Parsing Technologies.
77
Proceedings of the First Workshop on Multilingual Modeling, pages 18?24,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
The Study of Effect of Length in Morphological Segmentation of
Agglutinative Languages
Loganathan Ramasamy and Zdene?k Z?abokrtsky?
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics, Charles University in Prague
{ramasamy, zabokrtsky}@ufal.mff.cuni.cz
Sowmya Vajjala
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
sowmya@sfs.uni-tuebingen.de
Abstract
Morph length is one of the indicative feature
that helps learning the morphology of lan-
guages, in particular agglutinative languages.
In this paper, we introduce a simple unsu-
pervised model for morphological segmenta-
tion and study how the knowledge of morph
length affect the performance of the seg-
mentation task under the Bayesian frame-
work. The model is based on (Goldwater et
al., 2006) unigram word segmentation model
and assumes a simple prior distribution over
morph length. We experiment this model
on two highly related and agglutinative lan-
guages namely Tamil and Telugu, and com-
pare our results with the state of the art Mor-
fessor system. We show that, knowledge of
morph length has a positive impact and pro-
vides competitive results in terms of overall
performance.
1 Introduction
Most of the NLP tasks require one way or an-
other the handling of morphology. The task be-
comes very crucial when the language in ques-
tion is morphologically rich as is the case in many
Indo-European languages. The application of mor-
phology is evident in applications such as Statis-
tical Machine Translation (SMT) (Lee, 2004), de-
pendency parsing, information retrieval and so on.
Apart from the morphological analysis as in the tra-
ditional linguistic sense, morphological segmenta-
tion is also widely used as an easy alternative to
full fledged morphological analysis. In this paper
we mainly focus on the task of morphological seg-
mentation.
The main task in morphological segmentation is
to segment the given token or wordform into set
of morphs or identifying the location of each mor-
pheme boundary within the token. Morphological
segmentation is most suitable for agglutinative lan-
guages (such as Finnish or Turkish) than fusional
languages (such as Semitic languages).
Though both supervised (Koskenniemi, 1983)
and unsupervised methods (Goldsmith, 2001;
Creutz and Lagus, 2005) are extensively studied for
morphological segmentation, unsupervised tech-
niques have the appeal of application to multilin-
gual data with cost effective manner. Within un-
supervised paradigm, various methods have been
explored. Minimum Description Length (MDL)
(Goldsmith, 2001; Creutz and Lagus, 2005) based
approaches are most popular in which the best seg-
mentation corresponds to the compact represen-
tation of morphology and the resulting lexicon.
(Goldwater et al, 2009; Snyder and Barzilay, 2008)
attempted word segmentation and joint segmenta-
tion of related languages using Bayesian approach.
(Demberg, 2007; Dasgupta and Ng, 2007) applied
various probabilistic measures to discover affixes
of wordforms. (Naradowsky and Goldwater, 2009;
Yarowsky and Wicentowski, 2000) explored ways
to model orthographic rules of wordforms.
In this work, we are mainly going to focus on
Bayesian approach. Bayesian approaches provide
natural way of modeling subjective knowledge as
well as separating problem specific aspects from
general aspects. In the case of agglutinative lan-
18
guages, the number of morphemes in a word as well
as morph length play a major role in morpholog-
ical process. The main rationale for this work is
to study linguistic factors (mainly morph length),
so that language specific priors can be applied over
different languages. This will especially be use-
ful when modeling resource poor languages (RPL)
with little or no data, as well as building resources
for RPL from resource rich languages (RRL).
Towards that objective, our main contribution in
this work is, we introduce a simple unsupervised
segmentation model based on Bayesian approach
and we study the effect of morph length prior for
two agglutinative languages.
2 Previous Work
In this section, we briefly survey earlier works that
utilized the morph length information, then we pro-
vide basis for our unsupervised morphological seg-
mentation model and finally we list some prior
works on morphological analysis/segmentation of
Telugu and Tamil.
Snover (2001) used an exponential like distri-
bution for morph length that decreased over word
length, thus favoring shorter morph lengths. Our
work is directly related to (Creutz, 2003) as it
made use of prior distributions on morph length and
frequency of morphs under maximum a posteriori
(MAP) framework. Gamma distribution was used
as a prior distribution for morph length. The main
difference between (Creutz, 2003) and our work is
that, we are going to experiment different morph
lengths under Bayesian framework.
Naradowsky (2011) introduced an exponential
length penalty to prevent the model from under seg-
mentation results. It also emphasized that avoiding
length penalty seriously affected the model. (Poon
et al , 2009) indirectly specified about the morph
length by restricting the number of morphemes per
word.
In this work, we mainly rely on Goldwater (2009;
2006) which conducted an extensive study on the
application of Bayesian approach to word segmen-
tation in child-directed speech utterances. It in-
cluded both unigram and bigram models (based on
Hierarchical Dirichlet Processes) for word segmen-
tation. Gibbs sampling was used to extract sam-
ples (utterances with word boundaries) from pos-
terior distribution. We apply the unigram model
(Goldwater et al, 2009) to morphological segmen-
tation where the word boundaries in speech utter-
ances correspond to morpheme boundaries in word-
forms.
Before we describe unsupervised morphological
segmentation model, we briefly survey the existing
work on Telugu and Tamil morphological segmen-
tation/analysis.
Rao et al (2011) described in detail, the prepara-
tion of a linguistic database for Telugu morpholog-
ical analysis, compiling 2800 morphological cate-
gories and reported a coverage of 95-97%. They
followed a word and paradigm model, which was
considered to be better suited for agglutinative lan-
guages. The issue of out-of-vocabulary words was
handled better in the rule based approach by (Gana-
pathiraju and Levin, 2006). They describe a rule-
based morphological analyzer TelMore for Telugu
nouns and verbs.
Aksharbharathi et al (2004) describes the devel-
opment of a generic morphological analysis shell
that uses dictionaries along with Finite State Trans-
ducers based feature structures, to perform the mor-
phological analysis of a word. The feature struc-
tures were derived from the standard rules of the
grammar in respective languages. This was tested
with Hindi, Telugu, Tamil and Russian.
Kiranmai et al (2010) describe a supervised
morphological analyzer with support vector ma-
chines.
For Tamil, morphological segmentation is rarely
studied. Most of the work is done for morpholog-
ical analysis of wordforms. Most of the analyz-
ers use rule based approaches. Dhanalakshmi et al
(2009) used sequence labeling approach to morpho-
logical analysis of wordforms.
3 Unsupervised Morphological
Segmentation
Consider a wordform (w) of length n composed of
characters from alphabet LA,
w = c1c2c3...cn
The main objective is to identify the character po-
sitions where morpheme boundaries occur. The
19
model we describe here is similar to the cache
model described in (Goldwater et al, 2006) for
word segmentation. We apply the same model to
identify morpheme boundaries. The model makes
decision at every character position in the wordform
for the entire corpus. The hypothesis probability
that no morpheme boundary at position i in word-
form w is calculated as follows,
P (w?i |h) =
nma + ?P0(ma)
Nm + ?
(1)
ma is a substring or a morph in the wordform
w which contains the character position position i.
nma refers to number of times the morph ma oc-
curs in the history of morph counts Nm. In the case
of having a boundary at position i, we will have
two morphs to consider, one morph (ma) to the left
of position i (including i), and another morph (mb)
starting after i. The probability of having a mor-
pheme boundary at position i is calculated in the
same way as Equation 1, but this time with two
morphs,
P (w+i |h) =
nma + ?P0(ma)
Nm + ?
.
nmb + I(ma == mb) + ?P0(mb)
(Nm + 1) + ?
(2)
I(ma == mb) takes the value 1 if both morphs
are same, otherwise the value is 0. Also note that
the additional 1 (due to previous factor) in the de-
nominator of the second part of the equation. In
both the equations, P0 is a base distribution which
can be utilized to put a bias over certain hypothe-
ses. In our case, the base distribution (P0) mainly
assigns probability distribution over morph length.
Additional linguistic factors can also be modeled
this way. ? is a concentration parameter which can
be used to control P0. Overall, the model (in equa-
tion 1 and 2) uses only unigram morph counts.
Every character position (except the last posi-
tion) in a given word is a potential candidate that
can have a morpheme boundary. To determine
whether they really have morpheme boundary or
not, for every character position i inw, we calculate
hypothesis probabilities b+i (i.e. has a morpheme
boundary) and b?i (has no morpheme boundary).
Having calculated the hypothesis probabilities, we
choose the hypothesis by using a weighted coin flip.
In our problem, we have only two hypotheses: (i) a
morpheme boundary and (ii) no morpheme bound-
ary. If the new hypothesis is different from the char-
acter?s previous status, then appropriate data struc-
tures are updated. This procedure is repeated for
many number of iterations.
3.1 Modeling morpheme length
We encode our beliefs about morph length via
base distribution P0. We chose Poisson distribu-
tion for modeling the length of the morphs. Pois-
son distribution utilizing morph length is defined as
P (l, k) = l
ke?l
k! , where l is an expected length of
the morph and when supplied k, it returns the prob-
ability density of a morph having length k. We de-
fine two base distributions based on morph length
prior,
PA0 (m) = p(l, k)
=
lke?l
k!
(3)
PB0 (m) = p(m)p(l, k)
=
nm
| lm |
lke?l
k!
(4)
p(m) is probability of the morph itself. | lm | -
total number of substrings of length equal to the
length of morph m. Morfessor (Creutz and Lagus,
2005) uses Zipfian distribution for frequencies and
gamma length prior for modeling the length of the
morphs. Setting a particular expected morph length
effectively puts a bias towards that particular morph
length (l). We experiment both our base distribu-
tions over different morph lengths.
3.2 Inferencing
Gibbs sampling (Gilks et al, 1996) uses iterative
procedure to repeatedly draw value of a variable
given the current state of all other variables in the
model. In our case, drawing a value is equal to
determining whether there is a boundary at the
character position, thus obtaining individual mor-
phemes. We iteratively segment the given corpus or
list of words into morphological segments. The in-
tuitive idea is that, when we sample enough number
of times i.e. drawing morphological segments of
words given history of segments of all other words,
20
the sampler converges to the posterior distribution
of the morphological segments of the entire corpus.
The Algorithm 1 gives a general outline of how the
Gibbs sampling procedure is applied to morpholog-
ical segmentation.
Algorithm 1: Basic Sampling Procedure
Data: words, model
Result: Segmented words
begin
RandSeg ?? InitializeSegments(words)
Baseline?? Evaluate(RandSeg)
CurrSeg ?? RandSeg
MorphCounts?? GetCounts(CurrSeg)
for i ? iterations do
for j ? size(words) do
for k ? length(words[j]) do
b?k ?? Calculate(P (words[j]
?
k ))
b+k ?? Calculate(P (words[j]
+
k ))
if HasNoBoundaryAt(k) then
add boundary at k with
probability
b+k
b?k +b
+
k
no change at k with probability
b?k
b?k +b
+
k
if HasBoundaryAt(k) then
remove boundary at k with
probability
b?k
b?k +b
+
k
no change at k with probability
b+k
b?k +b
+
k
UpdateCurrSeg(CurrSeg)
AdjustMorphCounts(MorphCounts)
We use temperature (T) settings (not shown in
the algorithm) to make the sampling procedure con-
verge faster. We use 10 values (from 0.1 to 1.0) for
T and raise the probability values of hypotheses to
( 1T ). Also, we make the collection rate very small,
so that only few and substantially different samples
(or morphological segmentation of the entire cor-
pus) are collected.
4 Experimental Setup
The experiments are carried out for the unigram
segmentation model (unsup-uni) as described in
Section 3 and Morfessor system (Creutz and Lagus,
2005). For both Tamil and Telugu, we perform the
following experiments: (i) baseline (ii) unsup-uni
with base distribution PA0 (unsup-uni-p0-len) (iii)
unsup-uni with base distribution PB0 (unsup-uni-
p0-lex-len) and (iv) with Morfessor. For each sys-
tem, we add some knowledge about morph length
(l) and report the accuracy.
The experiments (ii), (iii) and (iv) use additional
dataset known as extra-data. Extra-data is an unan-
notated/unsegmented data which augments the test
data while training the systems. As test data with
gold segmentation is very small, we feel this step is
necessary to make the evaluation credible. The fol-
lowing subsection describes the datasets in detail.
Baseline system corresponds to random segmen-
tation. We evaluate baseline system for morph
lengths 1 to 10. For each morph length (l) experi-
ment, we change the probability of adding a bound-
ary at each character position to be (1l ) except at
l = 1 where the probability is 0.75.
Unsup-uni-p0-len experiment uses base distribu-
tion PA0 (see Section 3.1). We conduct this experi-
ment in 2 steps: (i) running the Gibbs sampler with
the extra-data and (ii) use the parameters (includ-
ing morph counts) from step (i) and run the Gibbs
sampler on test data. We set the expected morph
length (l) in the base distribution PA0 every time we
run the experiment for different morph length. For
the step (i), the Gibbs sampler is run for 10000 iter-
ations with different concentration parameter (?).
We collect samples every 1000 iterations and we
store the last sample as our model along with other
parameters. For step (ii), we use the model from
step (i) and run the Gibbs sampler on test data. We
collect the final sample as our predicted segmen-
tation of the test data and perform evaluation on
the predicted segmentation. In unsup-uni-p0-lex-
len experiment, we use the base distribution PB0
(see Section 3.1). PB0 includes morpheme proba-
bility apart from the length prior. Experiments for
unsup-uni-p0-lex-len is carried out in the same way
as that of unsup-uni-p0-len.
We use gamma distribution length prior for ex-
periments with Morfessor. We train Morfessor on
extra-data for morph lengths 1 to 10. We change
the expected length in the gamma prior for each
morph length experiment. Then we run the Mor-
fessor on test data with same parameters created
during the training.
We use Precision (P), Recall (R) and F-score (F)
21
Lang. Words Chars Morphs Avg. m.(l)
Tamil 1500 12642 3280 3.85
Telugu 998 10303 1733 5.95
Table 1: Gold segmentation: statistics
for evaluating our predicted segmentation with gold
segmentation. Our evaluation is same as (Creutz
and Linde?n, 2004).
4.1 Data
We use EMILLE corpus (Xiao et al , 2004) for
our experiments. The EMILLE corpus contains
monolingual, parallel and annotated data for var-
ious Indian languages. We randomly selected ar-
ticles from monolingual section of Tamil and Tel-
ugu data. The original data were in utf-8 and
we transliterated the data into latin format. The
transliteration step is an important step as it avoids
confusion in specifying morph length (l). As we
already mentioned earlier, we use two sets (extra-
data and test data) of data for each language. For
training of extra-data, we use 30000 unique words
list for each language. For test data, we make words
list from real sentences thus it can contain multi-
ple occurrences of a same wordform. The Table 1
provides the statistics of the test data for which we
have manually performed gold segmentations. At
present, our gold segmentation does not take into
account multiple possible segmentations.
The Figure 1 shows morph counts distribution
of both Tamil and Telugu (derived from gold seg-
ments) according to their morph lengths. Tamil has
more morphs that are shorter in length than Telugu.
5 Results
The Table 2 shows evaluation results for the exper-
imental setup described in the previous section.
For Tamil, most of the morphs have the length 1-
4. The models unsup-uni-p0-len and unsup-uni-p0-
lex-len perform quite well near to that length range.
For the same range (l = 1 to 4), both the models
together perform better than Morfessor in terms of
F-score. The performance of unsup-uni-p0-len and
unsup-uni-p0-lex-len are constantly decreasing and
start to perform worse than Morfessor after length
5. This is somewhat expected that unsup-uni mod-
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18
Tamil
morph length (l)
mo
rph c
ount
0
200
400
600
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Telugu
morph length (l)
mo
rph c
ount
0
100
200
300
Figure 1: Morph counts according to morph length (l)
els are quite sensitive to length priors and may per-
form poorly if we assume morph lengths far from
the true range. Whereas, Morfessor has a consis-
tent performance over the entire length range (l = 1
to 10). This implies that, Morfessor is less sensitive
to length priors even if we drastically change the
expected morph length. Unsup-uni-p0-len gave the
best overall performance (F-score - 48.83%) com-
pared to other models in this task.
Telugu?s common morph length ranges from 2-
8. Except at l = 1 & 2, Morfessor beats both
unsup-uni-p0-len and unsup-uni-p0-lex-len in all
other remaining length ranges. Unsup-uni models
perform quite poorly over different length ranges
when comparing with Tamil for the same range. In
this task, Morfessor?s overall performance (F-score
43.63%) is better than unsup-uni models. Mor-
fessor also performs better near the most frequent
morph length range (5-8).
6 Some Observations on (l)
? The results (Table 2) suggest that unsup-uni
model is quite sensitive to morph length pa-
rameter in the prior distributions.
? For Tamil, unsup-uni model performs well
near to the true morph length range. But the
performance deteriorates when the expected
morph length parameter is too different from
22
Language System P/R/F
Morph length (l)
1 2 3 4 5 6 7 8 9 10
Tamil
baseline
P 15.79 15.86 17.04 17.11 15.33 16.33 15.98 14.75 17.63 16.65
R 73.98 50.08 34.92 26.25 19.64 15.50 13.82 11.47 12.31 10.24
F 26.02 24.09 22.91 20.72 17.22 15.91 14.82 12.91 14.50 12.68
unsup-uni-p0-len
P 63.61 62.17 67.99 69.68 69.22 72.77 72.29 68.70 66.73 64.08
R 39.62 40.01 36.49 33.18 28.82 26.47 24.23 22.10 20.65 20.76
F 48.83 48.69 47.49 44.96 40.7 38.82 36.30 33.45 31.54 31.36
unsup-uni-p0-lex-len
P 46.51 59.48 63.79 63.69 56.10 54.58 50.29 48.18 45.99 50.39
R 41.35 41.07 39.34 38.28 36.04 33.69 34.25 34.08 33.02 28.65
F 43.78 48.59 48.67 47.82 43.88 41.66 40.75 39.92 38.44 36.53
Morfessor
P 48.54 48.32 48.61 49.01 50.24 49.07 49.93 49.21 49.42 48.93
R 41.75 40.18 40.07 40.24 40.46 39.84 40.35 39.84 40.40 39.62
F 44.89 43.87 43.93 44.19 44.82 43.98 44.63 44.03 44.64 43.78
Telugu
baseline
P 07.88 08.05 07.91 07.38 07.70 07.54 07.62 08.52 08.96 07.91
R 75.69 51.59 32.97 23.86 20.00 16.00 13.66 13.38 12.97 10.07
F 14.28 13.93 12.76 11.27 11.12 10.25 09.78 10.41 10.60 10.07
unsup-uni-p0-len
P 36.67 37.29 36.2 39.71 41.87 40.58 41.34 39.15 38.10 33.65
R 53.10 51.17 48.14 38.07 29.1 19.31 16.14 11.45 11.03 9.66
F 43.38 43.14 41.33 38.87 34.34 26.17 23.21 17.72 17.11 15.01
unsup-uni-p0-lex-len
P 22.27 26.55 32.46 35.76 28.29 19.31 19.83 18.3 18.17 17.26
R 66.9 58.34 44.41 35.17 35.31 55.17 42.21 49.79 55.45 52.28
F 33.41 36.5 37.51 35.47 31.41 28.6 26.98 26.76 27.37 25.95
Morfessor
P 29.32 29.59 30.48 30.72 30.88 30.85 31.31 30.34 29.88 30.40
R 70.30 69.48 69.48 69.75 70.17 70.30 71.96 70.99 70.58 71.96
F 41.38 41.50 42.38 42.65 42.89 42.88 43.63 42.51 41.99 42.74
Table 2: Results for Tamil and Telugu
the true frequent morph length range.
? However for Telugu, morph length parameter
did not improve the results at the most frequent
morph length range (5-8).
? Concentration parameter (?) too influences
the effect of base distribution as a whole, but
at present, our study does not take into account
?. For small ? values, the base distribution
will not have much effect.
7 Conclusion
In this paper, we mainly studied the effect of knowl-
edge of morph length that could have on the ac-
curacy of morphological segmentation of aggluti-
native languages. Towards that goal, we intro-
duced a simple unsupervised morphological seg-
mentation model based on Bayesian approach that
utilized prior distribution over morph length. The
results showed that the knowledge of length cer-
tainly has a positive impact on the accuracy. Also,
the model provided competitive results in general
and achieved best overall performance (F-score:
48.83%) for Tamil against Morfessor. As a future
work, it would be interesting to see the model and
priors that handle sandhi changes.
Acknowledgements
The research leading to these results has re-
ceived funding from the European Commission?s
7th Framework Program (FP7) under grant agree-
ment n? 238405 (CLARA). We would like to thank
David Marec?ek for useful suggestions about theory
and implementation of the system. We also would
like to thank anonymous reviewers for their useful
comments.
References
Akshar Bharathi, Rajeev Sangal, Dipti M Sharma and
Radhika Mamidi. 2004. Generic Morphological
Analysis Shell. In Proceedings of LREC 2004.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 737?745. 2008.
David Yarowsky and Richard Wicentowski. Minimally
Supervised Morphological Analysis by Multimodal
Alignment. In Proceedings of the 38th Annual Meet-
ing on Association for Computational Linguistics
(ACL), 2000.
Dhanalakshmi V, AnandKumar M, Rekha RU and Ra-
jendran S. 2009. Morphological Analyzer for Ag-
23
glutinative Languages Using Machine Learning Ap-
proaches. In Advances in Recent Technologies in
Communication and Computing, 2009, ARTCom?09,
2009.
Hoifung Poon, Colin Cherry and Kristina Toutanova.
2009. Unsupervised Morphological Segmentation
with Log-Linear Models. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the ACL
(NAACL-HLT), pages 209?217, Boulder, Colorado,
June 2009.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving Morphology Induction by Learning Spelling
Rules. In Proceedings of 21st International Joint
Conference on Artificial Intelligence (IJCAI), 2009.
Jason Naradowsky and Kristina Toutanova. 2011. Un-
supervised Bilingual Morpheme Segmentation and
Alignment with Context-rich Hidden Semi-Markov
Models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 895?904,
June, 2011.
John Goldsmith. 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2): pages 153?198, 2001.
Kimmo Koskenniemi. 1983. Two-level morphol-
ogy: A general computational model for word-form
recognition and production. Publication 11, Univer-
sity of Helsinki, Department of General Linguistics,
Helsinki. 1983.
Madhavi Ganapathiraju and Lori Levin. 2006. Tel-
More: Morphological Generator for Telugu Nouns
and Verbs. In Proceedings of the Second Interna-
tional Conference on Digital Libraries. 2006.
Mathias Creutz. 2003. Unsupervised Segmentation of
Words Using Prior Distributions of Morph Length
and Frequency. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 280?287, July 2003.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
Morpheme Segmentation and Morphology Induction
from Text Corpora Using Morfessor 1.0. In Publica-
tions in Computer and Information Science, Report
A81, Helsinki University of Technology, 2005.
Mathias Creutz and Krister Linde?n. 2004. Morpheme
Segmentation Gold Standards for Finnish and En-
glish. Publications in Computer and Information Sci-
ence, Report A77, Helsinki University of Technology,
October, 2004.
Matthew G. Snover and Michael R. Brent. 2001. A
Bayesian model for morpheme and paradigm identi-
fication. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics (ACL),
pages 490?498, 2001.
Sai Kiranmai G., K. Mallika, M. Anand Kumar, V.
Dhanalakshmi and K. P. Soman. 2010. Morpho-
logical Analyzer for Telugu using support vector ma-
chines. In Proceedings of ICT 2010.
Sajib Dasgupta and Vincent Ng. 2007. High-
Performance, Language-Independent Morphological
Segmentation. In Proceedings of NAACL HLT 2007,
pages 155?163, 2007.
Sharon Goldwater, Thomas L. Griffiths and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2006.
Sharon Goldwater, Thomas L. Griffiths and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112 (1), pp. 21?54, 2009.
Uma Maheshwar Rao G., Amba Kulkarni P. and Christo-
pher Mala. 2011. A Telugu Morphological Analyzer.
International Telugu Internet Conference Proceed-
ings, Milpitas, California, USA, 28th - 30th Septem-
ber, 2011
Vera Demberg. 2007. A Language-Independent Unsu-
pervised Model for Morphological Segmentation. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
920?927, Prague, Czech Republic, June 2007.
Walter R. Gilks, Sylvia Richardson and David Spiegel-
halter. 1996. Markov Chain Monte Carlo in Practice.
Chapman and Hall. 1996.
Xiao Z., McEnery A., Baker P. and Hardie A. 2004.
Developing Asian language corpora: standards and
practice. In Proceedings of the Fourth Workshop on
Asian Language Resources, pp. 1?8, 2004.
Young-Suk Lee. 2004. Morphological Analysis for Sta-
tistical Machine Translation. In Proceedings of the
HLT-NAACL 2004, pp. 57?60, Boston, USA, 2004.
24
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 19?24,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Improvements to Syntax-based Machine Translation using Ensemble
Dependency Parsers
Nathan David Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
Prague, Czech Republic
{green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Dependency parsers are almost ubiqui-
tously evaluated on their accuracy scores,
these scores say nothing of the complex-
ity and usefulness of the resulting struc-
tures. The structures may have more com-
plexity due to their coordination structure
or attachment rules. As dependency parses
are basic structures in which other systems
are built upon, it would seem more reason-
able to judge these parsers down the NLP
pipeline.
We show results from 7 individual parsers,
including dependency and constituent
parsers, and 3 ensemble parsing tech-
niques with their overall effect on a Ma-
chine Translation system, Treex, for En-
glish to Czech translation. We show that
parsers? UAS scores are more correlated
to the NIST evaluation metric than to the
BLEU Metric, however we see increases
in both metrics.
1 Introduction
Ensemble learning (Dietterich, 2000) has been
used for a variety of machine learning tasks and
recently has been applied to dependency parsing
in various ways and with different levels of suc-
cess. (Surdeanu and Manning, 2010; Haffari
et al, 2011) showed a successful combination of
parse trees through a linear combination of trees
with various weighting formulations. To keep
their tree constraint, they applied Eisner?s algo-
rithm for reparsing (Eisner, 1996).
Parser combination with dependency trees has
been examined in terms of accuracy (Sagae and
Lavie, 2006; Sagae and Tsujii, 2007; Zeman
and Z?abokrtsky?, 2005; Holan and Z?abokrtsky?,
2006). Other methods of parser combinations
have shown to be successful such as using one
parser to generate features for another parser. This
was shown in (Nivre and McDonald, 2008), in
which Malt Parser was used as a feature to MST
Parser. The result was a successful combination of
a transition-based and graph-based parser, but did
not address adding other types of parsers into the
framework.
We will use three ensemble approaches. First a
fixed weight ensemble approach in which edges
are added together in a weighted graph. Sec-
ond, we added the edges using weights learned
through fuzzy clustering based on POS errors.
Third, we will use a meta-classifier that uses an
SVM to predict the correct model for edge using
only model agreements without any linguistic in-
formation added. Parsing accuracy and machine
translation has been examined in terms of BLEU
score (Quirk and Corston-Oliver, 2006). How-
ever, we believe our work is the first to examine
the NLP pipeline for ensemble parsing for both de-
pendency and constituent parsers as well as exam-
ining both BLEU and NIST scores? relationship to
their Unlabeled Accuracy Score(UAS).
2 Methodology
2.1 Annotation
To find the maximum effect that dependency pars-
ing can have on the NLP pipeline, we annotated
English dependency trees to form a gold standard.
Annotation was done with two annotators using
a tree editor, Tred (Pajas and Fabian, 2011), on
data that was preprocessed using MST parser. For
the annotation of our gold data, we used the stan-
dard developed by the Prague Dependency Tree-
bank (PDT) (Hajic?, 1998). PDT is annotated on
three levels, morphological, analytical, and tec-
togrammatical. For our gold data we do not touch
the morphological layer, we only correct the ana-
lytical layer (i.e. labeled dependency trees). For
machine translation experiments later in the paper
19
we allow the system to automatically generate a
new tectogrammatical layer based on our new an-
alytical layer annotation. Because the Treex ma-
chine translation system uses a tectogrammatical
layer, when in doubt, ambiguity was left to the tec-
togrammatical (t-layer in Figure 1) to handle.
2.1.1 Data Sets
For the annotation experiments we use data pro-
vided by the 2012 Workshop for Machine Trans-
lation (WMT2012). The data which consists
of 3,003 sentences was automatically tokenized,
tagged, and parsed. This data set was also chosen
since it is disjoint from the usual dependency train-
ing data, allowing researchers to use it as a out-of-
domain testing set. The parser used was an imple-
mentation of MST parser. We then hand corrected
the analytical trees to have a ?Gold? standard de-
pendency structure. Analytical trees were anno-
tated on the PDT standard. Most changes involved
coordination construction along with prepositional
phrase attachment. We plan to publicly release this
data and corresponding annotations in the near fu-
ture1.
Having only two annotators has limited us
to evaluating our annotation only through spot
checking and through comparison with other base-
lines. Annotation happened sequentially one after
another. Possible errors were additionally detected
through automatic means. As a comparison we
will evaluate our gold data set versus other parsers
in respect to their performance on previous data
sets, namely the Wall Street Journal (WSJ) section
23.
2.2 Translation
2.2.1 Data Sets
All the parsers were trained on sections 02-21 of
the WSJ, except the Stanford parser which also
uses section 01. We retrained MST and Malt
parsers and used pre-trained models for the other
parsers. Machine translation data was used from
WMT 2010, 2011, and 2012. Using our gold
standard we are able to evaluate the effective-
ness of different parser types from graph-base,
transition-based, constituent conversion to ensem-
ble approaches on the 2012 data while finding data
trends using previous years data.
1When available the data and description will be at
www.nathangreen.com/wmtdata
2.2.2 Translation Components
To examine the effects of dependency parsing
down the NLP pipeline, we now turn to syntax
based machine translation. Our dependency mod-
els will be evaluated using the Treex translation
system (Popel and Z?abokrtsky?, 2010). This sys-
tem, as opposed to other popular machine transla-
tion systems, makes direct use of the dependency
structure during the conversion from source to tar-
get languages via a tectogrammatical tree transla-
tion approach.
Figure 1: Treex syntax-based translation scenario
(Popel and Z?abokrtsky?, 2010)
We use the different parsers in separate trans-
lation runs each time in the same Treex parsing
block. So each translation scenario only differs in
the parser used and nothing else. As can be seen
in Figure 1, we are directly manipulating the An-
alytical portion of Treex. The parsers used are as
follows:
? MST: Implementation of Ryan McDonald?s
Minimum spanning tree parser (McDonald et
al., 2005)
? MST with chunking: Same implementation
as above but we parse the sentences based on
chunks and not full sentences. For instance
this could mean separating parentheticals or
separating appositions (Popel et al, 2011)
? Malt: Implementation of Nivre?s Malt Parser
trained on the Penn Treebank (Nivre, 2003)
? Malt with chunking: Same implementation
as above but with chunked parsing
? ZPar: Yue Zhang?s statistical parser. We
used the pretrained English model (en-
glish.tar.gz) available on the ZPar website for
all tests (Zhang and Clark, 2011)
? Charniak: A constituent based parser
(ec50spfinal model) in which we transform
20
the results using the Pennconverter (Johans-
son and Nugues, 2007)
? Stanford: Another constituent based
parser (Klein and Manning, 2003) whose
output is converted using Pennconverter as
well (wsjPCFG.ser.gz model)
? Fixed Weight Ensemble: A stacked en-
semble system combining five of the parsers
above (MST, Malt, ZPar, Charniak, Stan-
ford). The weights for each tree are as-
signed based on UAS score found in tun-
ing data, section 22 of the WSJ (Green and
Z?abokrtsky?, 2012)
? Fuzzy Cluster: A stacked ensemble system
as well but weights are determined by a clus-
ter analysis of POS errors found in the same
tuning data as above (Green and Z?abokrtsky?,
2012)
? SVM: An ensemble system in which each in-
dividual edge is picked by a meta classifier
from the same 5 parsers as the other ensemble
systems. The SVM meta classifier is trained
on results from the above tuning data (Green
et al, 2012a; Green et al, 2012b).
2.2.3 Evaluation
For Machine Translation we report two automatic
evaluation scores, BLEU and NIST. We examine
parser accuracy using UAS. This paper compares
a machine translation system integrating 10 differ-
ent parsing systems against each other, using the
below metrics.
The BLEU (BiLingual Evaluation Understudy)
and NIST(from the National Institute of Standards
and Technology), are automatic scoring mecha-
nisms for machine translation that are quick and
can be reused as benchmarks across machine
translation tasks. BLEU and NIST are calculated
as the geometric mean of n-grams multiplied by a
brevity penalty, comparing a machine translation
and a reference text (Papineni et al, 2002). NIST
is based upon the BLEU n-gram approach how-
ever it is also weighted towards discovering more
?informative? n-grams. The more rare an n-gram
is, the higher the weight for a correct translation of
it will be.
Made a standard in the CoNLL shared tasks
competition, UAS studies the structure of a depen-
dency tree and assesses how often the output has
the correct head and dependency arcs (Buchholz
and Marsi, 2006). We report UAS scores for each
parser on section 23 of the WSJ.
3 Results and Discussion
3.1 Type of Changes in WMT Annotation
Since our gold annotated data was preprocessed
with MST parser, our baseline system at the time,
we started with a decent baseline and only had
to change 9% of the dependency arcs in the data.
These 9% of changes roughly increase the BLEU
score by 7%.
3.2 Parser Accuracy
As seen in previous Ensemble papers (Farkas and
Bohnet, 2012; Green et al, 2012a; Green et al,
2012b; Green and Z?abokrtsky?, 2012; Zeman and
Z?abokrtsky?, 2005), parsing accuracy can be im-
proved by combining parsers? outputs for a variety
of languages. We apply a few of these systems, as
described in Section 2.2.2, to English using mod-
els trained for both dependencies and constituents.
3.2.1 Parsers vs our Gold Standard
On average our gold data differed in head agree-
ment from our base parser 14.77% of the time.
When our base parsers were tested on the WSJ
section 23 data they had an average error rate of
12.17% which is roughly comparable to the differ-
ence with our gold data set which indicates overall
our annotations are close to the accepted standard
from the community. The slight difference in per-
centage fits into what is expect in annotator error
and in the errors in the conversion process of the
WSJ by Pennconverter.
3.3 Parsing Errors Effect on MT
3.3.1 MT Results in WMT with Ensemble
Parsers
WMT 2010
As seen in Table 1, the highest resulting BLEU
score for the 2010 data set is from the fixed weight
ensemble system. The other two ensemble sys-
tems are beaten by one component system, Char-
niak. However, this changes when comparing
NIST scores. Two of the ensemble method have
higher NIST scores than Charniak, similar to their
UAS scores.
WMT 2011
The 2011 data corresponded the best with UAS
scores. While the BLEU score increases for all
21
Parser UAS NIST(10/11/12) BLEU(10/11/12)
MST 86.49 5.40/5.58/5.19 12.99/13.58/11.54
MST w chunking 86.57 5.43/5.63/5.23 13.43/14.00/11.96
Malt 84.51 5.37/5.57/5.14 12.90/13.48/11.27
Malt w chunking 87.01 5.41/5.60/5.19 13.39/13.80/11.73
ZPar 76.06 5.26/5.46/5.08 11.91/12.48/10.53
Charniak 92.08 5.47/5.65/5.28 13.49/13.95/12.26
Stanford 87.88 5.40/5.59/5.18 13.23/13.63/11.74
Fixed Weight 92.58 5.49/5.68/5.29 13.53/14.04/12.23
Fuzzy Cluster 92.54 5.47/5.68/5.26 13.47/14.06/12.06
SVM 92.60 5.48/5.68/5.28 13.45/14.11/12.22
Table 1: Scores for each machine translation run for each dataset (WMT 2010, 2011 and 2012)
the ensemble systems, the order of systems by
UAS scores corresponds exactly to the systems or-
dered by NIST score and corelates strongly (Table
2). Unlike the 2010 data, the MST parser was the
highest base parser in terms of the BLEU metric.
WMT 2012
The ensemble increases are statistically significant
for both the SVM and the Fixed Weight system
over the MST with chunking parser with 99% con-
fidence, our previous baseline and best scoring
base system from 2011 in terms of BLEU score.
We examine our data versus MST with chunking
instead of Charniak since we have preprocessed
our gold data set with MST, allowing us a direct
comparison in improvements. The fuzzy cluster
system achieves a higher BLEU evaluation score
than MST, but is not significant. In pairwise tests
it wins approximately 78% of the time. This is the
first dataset we have looked at where the BLEU
score is higher for a component parser and not an
ensemble system, although the NIST score is still
higher for the ensemble systems.
NIST BLEU
2010 0.98 0.93
2011 0.98 0.94
2012 0.95 0.97
Table 2: Pearson correlation coefficients for each
year and each metric when measured against UAS.
Statistics are taken from the WMT results in Table
1. Overall NIST has the stronger correlation to
UAS scores, however both NIST and BLEU show
a strong relationship.
3.3.2 Human Manual Evaluation: SVM vs
the Baseline System
We selected 200 sentences at random from our an-
notations and they were given to 7 native Czech
speakers. 77 times the reviewers preferred the
SVM system, 48 times they preferred the MST
system, and 57 times they said there was no differ-
ence between the sentences. On average each re-
viewer looked at 26 sentences with a median of 30
sentences. Reviewers were allowed three options:
sentence 1 is better, sentence 2 is better, both sen-
tences are of equal quality. Sentences were dis-
played in a random order and the systems were
randomly shuffled for each question and for each
user.
+ = -
+ 12 12 0
= 3 7
- 7
Table 3: Agreement for sentences with 2 or more
annotators for our baseline and SVM systems. (-,-)
all annotators agreed the baseline was better, (+,+)
all annotators agreed the SVM system was better,
(+,-) the annotators disagreed with each other
Table 3 indicates that the SVM system was pre-
ferred. When removing annotations marked as
equal, we see that the SVM system was preferred
24 times to the Baseline?s 14.
Although a small sample, this shows that using
the ensemble parser will at worse give you equal
results and at best a much improved result.
22
3.3.3 MT Results with Gold Data
In the perfect situation of having gold standard de-
pendency trees, we obtained a NIST of 5.30 and
a BLEU of 12.39. For our gold standard system
run, the parsing component was removed and re-
placed with our hand annotated data. These are
the highest NIST and BLEU scores we have ob-
tained including using all base parsers or any com-
binations of parsers. This indicates that while an
old problem which is a ?solved? problem for some
languages, Parsing is still worth researching and
improving for its cascading effects down the NLP
pipeline.
4 Conclusion
We have shown that ensemble parsing techniques
have an influence on syntax-based machine trans-
lation both in manual and automatic evaluation.
Furthermore we have shown a stronger correlation
between parser accuracy and the NIST rather than
the more commonly used BLEU metric. We have
also introduced a gold set of English dependency
trees based on the WMT 2012 machine translation
task data, which shows a larger increase in both
BLEU and NIST. While on some datasets it is in-
conclusive whether using an ensemble parser with
better accuracy has a large enough effect, we do
show that practically you will not do worse using
one and in many cases do much better.
5 Acknowledgments
This research has received funding from the
European Commission?s 7th Framework Pro-
gram (FP7) under grant agreement n? 238405
(CLARA). Additionally, this work has been us-
ing language resources developed and/or stored
and/or distributed by the LINDAT-Clarin project
of the Ministry of Education of the Czech Repub-
lic (project LM2010013).
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, CoNLL-X
?06, pages 149?164, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Thomas G. Dietterich. 2000. Ensemble Methods
in Machine Learning. In Proceedings of the First
International Workshop on Multiple Classifier Sys-
tems, MCS ?00, pages 1?15, London, UK. Springer-
Verlag.
Jason Eisner. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), pages
340?345, Copenhagen, August.
Richa?rd Farkas and Bernd Bohnet. 2012. Stacking of
Dependency and Phrase Structure Parsers. In Pro-
ceedings of COLING 2012, pages 849?866, Mum-
bai, India, December. The COLING 2012 Organiz-
ing Committee.
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hy-
brid Combination of Constituency and Dependency
Trees into an Ensemble Dependency Parser. In Pro-
ceedings of the EACL 2012 Workshop on Innovative
hybrid approaches to the processing of textual data,
Avignon, France.
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Ensem-
ble Parsing and its Effect on Machine Translation.
Technical Report 48.
Nathan Green, Septina Dian Larasati, and Zdene?k
Z?abokrtsky?. 2012a. Indonesian Dependency
Treebank: Annotation and Parsing. In Proceed-
ings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 137?
145, Bali,Indonesia, November. Faculty of Com-
puter Science, Universitas Indonesia.
Nathan Green, Loganathan Ramasamy, and Zdene?k
Z?abokrtsky?. 2012b. Using an SVM Ensemble Sys-
tem for Improved Tamil Dependency Parsing. In
Proceedings of the ACL 2012 Joint Workshop on
Statistical Parsing and Semantic Processing of Mor-
phologically Rich Languages, pages 72?77, Jeju,
Republic of Korea, July 12. Association for Com-
putational Linguistics.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An Ensemble Model that Combines
Syntactic and Semantic Clustering for Discrimina-
tive Dependency Parsing. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 710?714, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.
Toma?s? Holan and Zdene?k Z?abokrtsky?. 2006. Com-
bining Czech Dependency Parsers. In Proceedings
of the 9th international conference on Text, Speech
and Dialogue, TSD?06, pages 95?102, Berlin, Hei-
delberg. Springer-Verlag.
23
Richard Johansson and Pierre Nugues. 2007. Ex-
tended Constituent-to-dependency Conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, HLT ?05, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing Graph-Based and Transition-Based Dependency
Parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June. Association for
Computational Linguistics.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT, pages 149?160.
Petr Pajas and Peter Fabian. 2011. TrEd 2.0 - newly
refactored tree editor. http://ufal.mff.cuni.cz/tred/,
Institute of Formal and Applied Linguistics, MFF
UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ?02, pages 311?
318, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: modular NLP framework. In Proceedings of
the 7th international conference on Advances in nat-
ural language processing, IceTAL?10, pages 293?
304, Berlin, Heidelberg. Springer-Verlag.
Martin Popel, David Marec?ek, Nathan Green, and
Zdenek Zabokrtsky. 2011. Influence of parser
choice on dependency-based mt. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 433?439, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed sta-
tistical machine translation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 62?69,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Kenji Sagae and Alon Lavie. 2006. Parser Combina-
tion by Reparsing. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 129?132,
New York City, USA, June. Association for Compu-
tational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
Parsing and Domain Adaptation with LR Models
and Parser Ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 649?652, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Daniel Zeman and Zdene?k Z?abokrtsky?. 2005. Improv-
ing Parsing Accuracy by Combining Diverse Depen-
dency Parsers. In In: Proceedings of the 9th Inter-
national Workshop on Parsing Technologies.
Yue Zhang and Stephen Clark. 2011. Syntactic Pro-
cessing Using the Generalized Perceptron and Beam
Search. Computational Linguistics, 37(1):105?151.
24
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 51?59,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Translation of ?It? in a Deep Syntax Framework
Michal Nova?k, Anna Nedoluzhko and Zdene?k ?Zabokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, CZ-11800
{mnovak,nedoluzko,zabokrtsky}@ufal.mff.cuni.cz
Abstract
We present a novel approach to the trans-
lation of the English personal pronoun it
to Czech. We conduct a linguistic analysis
on how the distinct categories of it are usu-
ally mapped to their Czech counterparts.
Armed with these observations, we design
a discriminative translation model of it,
which is then integrated into the TectoMT
deep syntax MT framework. Features in
the model take advantage of rich syntac-
tic annotation TectoMT is based on, exter-
nal tools for anaphoricity resolution, lex-
ical co-occurrence frequencies measured
on a large parallel corpus and gold coref-
erence annotation. Even though the new
model for it exhibits no improvement in
terms of BLEU, manual evaluation shows
that it outperforms the original solution in
8.5% sentences containing it.
1 Introduction
After it has long been neglected, retaining cohe-
sion of a text larger than a single sentence in Ma-
chine Translation (MT) has recently become a dis-
cussed topic. Correct translation of referential ex-
pressions is in many cases essential for humans to
grasp the meaning of a translated text.
Especially, the translation of pronouns attracts
a higher rate of interest. In the previous works
of Le Nagard and Koehn (2010), Hardmeier and
Federico (2010) and Guillou (2012), it has been
shown that current MT systems perform poorly in
producing the correct forms of pronouns. As re-
gards English, the personal pronoun it is the most
complicated case. Not only can it corefer with al-
most any noun phrase (making it hard to pick the
correct gender and number if the target language is
morphologically rich), but it can also corefer with
a larger discourse segment or play the role of a
filler in certain grammatical constructions.
In this work, we turn our attention to the transla-
tion of the English personal pronoun it into Czech.
Even if we ignore morphology and merge all re-
lated surface forms into one, we cannot find a
single Czech expression that would comprise all
functions of the English it. Moreover, there is no
simple one-to-one mapping from categories of it
to Czech expressions. For instance, one would ex-
pect that the translation of it which is coreferen-
tial with a noun phrase has to agree in number and
gender with the translation of its antecedent. How-
ever, there are cases when it is more suitable to
translate it as the demonstrative pronoun to, whose
gender is always neuter.
The aim of this work is to build an English-to-
Czech translation model for the personal pronoun
it within the TectoMT framework ( ?Zabokrtsky? et
al., 2008). TectoMT is a tree-to-tree translation
system with transfer via tectogrammatical layer,
a deep syntactic layer which follows the Prague
tectogrammatics theory (Sgall, 1967; Sgall et al,
1986) Therefore, its translation model outputs the
deep syntactic representation of a Czech expres-
sion. Selecting the correct grammatical categories
and thus producing a concrete surface form of a
deep syntactic representation is provided by the
translation synthesis stage, which we do not focus
on in this work.
The mapping between it and corresponding
Czech expressions depends on many aspects. We
address them by introducing features based on
syntactic annotation and anaphoricity resolver out-
put. Furthermore, we make use of lexical co-
occurrence counts aggregated on a large auto-
matically annotated Czech-English parallel corpus
CzEng 1.0 (Bojar et al, 2012). Coreference links
also appear to be a source of valuable features.1
In contrast to the related work, we prefer a dis-
criminative model to a commonly used generative
1However, we excluded them from the final model used
in MT as they originate from gold standard annotation.
51
model. The former allows us to feed it with many
syntactic and lexical features that may affect the
output, which would hardly be possible in the lat-
ter.
2 Related Work
Our work addresses a similar issue that has been
explored by Le Nagard and Koehn (2010), Hard-
meier and Federico (2010) and Guillou (2012).
These works attempted to incorporate informa-
tion on coreference relations into MT, aiming to
improve the translation of English pronouns into
morphologically richer languages. The poor re-
sults in the first two works were mainly due to im-
perfect automatic coreference annotation.
The work of Guillou (2012) is of special interest
to this work because it is also focused on English
to Czech translation and makes an extensive use
of the Prague Czech-English Dependency Tree-
bank 2.0 (PCEDT). Instead of automatic corefer-
ence links, they employed gold annotation, reveal-
ing further reasons of small improvements ? the
number of occurrences in the tranining data weak-
ened by including grammatical number and gen-
der in the annotation and availability of only a sin-
gle reference translation.
The first issue is a consequence of the assump-
tion that a Czech pronoun must agree in gen-
der and number with its antecedent. There are
cases, though, when demonstrative pronoun to fits
better and grammatical categories are not propa-
gated. Keeping grammatical information on its
antecedent may in this case result in probably not
harmful but still superfluous partitioning the train-
ing data.
Our work deals also with the second issue, how-
ever, at the cost of partial manual annotating.
The most significant difference of our work
compared to the abovementioned ones lies in the
MT systems used. Whereas they tackle the issue
of pronoun translation within the Moses phrase-
based system (Koehn et al, 2003), we rely on the
translation via deep syntax with TectoMT system
( ?Zabokrtsky? et al, 2008). Our approach is more
linguistically oriented, working with deep syntac-
tic representations and postponing the decisions
about the concrete forms to the synthesis stage.
3 Linguistic Analysis
In English, three main coarse-grained types of
it are traditionally distinguished. Referential it
points to a noun phrase in the preceding or the fol-
lowing context:
(1) Peter has finished writing an article and
showed it to his supervisor.
Anaphoric it refers to a verbal phrase or larger dis-
course segments (so-called discourse deixis).
(2) Peter has discussed the issue with his su-
pervisor and it helped him to finish the ar-
ticle.
Pleonastic it has no antecedent in the preced-
ing/following context and its presence is imposed
only by the syntactic rules of English.
(3) It is difficult to give a good example.
From the perspective of Czech, there are also
three prevailing types of how it can be translated.
The most frequent are personal pronouns or zero
forms.2 In Prague tectogrammatics theory zero
anaphors are reconstructed on the tectogrammat-
ical layer. Same as expressed personal pronouns,
they are represented by a node with the #PersPron
symbol, e.g.
(4) Bushova vla?da ozna?mila, z?e se svu?j pla?n
#PersPron pokus?? vzkr???sit.
The Bush administration has said it will
try to resurrect its plan.
The second typical possibility is the Czech demon-
strative pronoun to (= it, this), which is a form of
a pronoun ten in its neuter singular form, e.g.
(5) Analytik r?ekl, z?e to byla tato moz?nost
poz?adavku, ktera? pevne?js???m cena?m po-
mohla.
The analyst said that it was the possibility
of this demand that helped firm prices.
In many cases, it has no lexical counterpart in
the Czech translation, the English and Czech sen-
tences thus having a different syntactic structure.
These are cases like, for instance:
(6) Obchodn??ci uvedli, z?e je obt??z?ne? nove?
emise REMIC strukturovat, kdyz? se ceny
tolik me?n??.
Dealers noted that it?s difficult to struc-
ture new Remics when prices are moving
widely.
2Czech is a pro-drop language.
52
Figure 1: The mapping of the types of English it
to Czech translations.
There are also some other possibilities of how it
can be translated into Czech, such as the repeti-
tion of the antecedent noun, different genders of
the demonstrative ten (=it, this) in the anaphoric
position, using synonyms and hyperomyns. How-
ever, these cases are not so frequent and they rarely
cannot be converted to one of the three broader
categories.
The correspondence between the course-
grained types of English it and its possible Czech
translations is not one-to-one. As seen from
Figure 1, a personal pronoun/zero anaphora
translates to the referential it (see example 4) and
no lexical counterpart is used when translating the
pleonastic it (see example 6).
However, all types of it can be translated as a
neuter demonstrative to. The typical case ?it refer-
ring to VPs/larger discourse segments = to? was
demonstrated in (5).
The mapping ?referential it = to? is common for
cases where the referent is attributed some further
characteristics, mostly in constructions with a verb
to be like ?It is something.?, such as (7).3 This
is an interesting case for Czech, because a gen-
der and number agreement between the antecedent
and the anaphoric to is generally absent.
(7) Some investors say Friday?s sell-off was a
good thing. ?It was a healthy cleansing,?
says Michael Holland.
Ne?kter??? investor?i r???kaj??, z?e pa?tec?n??
vy?prodej byla dobra? ve?c. ?Byla to zdrava?
oc?ista,? r???ka? Michael Holland.
The ?cleft sentences? (see example 8) and some
other syntactic constructions are the case when
pleonastic it is translated into Czech with the
demonstrative to.
3We suspect that it holds also for he/she/they but such a
claim is not yet empirically supported. For the sake of sim-
plicity, we conduct our research only for it.
(8) But it is Mr. Lane, as movie director, who
has been obsessed with refitting Chaplin?s
Little Tramp in a contemporary way.
Ale je to Lane jako filmovy? rez?ise?r, kdo je
posedly? t??m, z?e zmodernizuje Chaplinu?v
film ?Little Tramp (Maly? tula?k)?.
In some cases, both translations of pleonastic it
are possible: neuter demonstrative to or a different
syntactic construction with no lexical counterpart
of it. Compare the examples from PCEDT where
it with similar syntactic function was translated by
changing the syntactic structure in (9) and using a
neuter to in (10):
(9) ?It was great to have the luxury of time,?
Mr. Rawls said.
?Bylo skve?le?, z?e jsme me?li dostatek c?asu,?
r?ekl Rawls.
(10) ?On days that I?m really busy,? says Ms.
Foster, ?it seems decadent to take time off
for a massage.?
?Ve dnech, kdy ma?m opravdu mnoho
pra?ce,? r???ka? pan?? Fosterova?, ?to vypada?
zvrhle, kdyz? si vyhrad??m c?as na masa?z?.?
4 Translation via Deep Syntax
Following a phrase-based statistical MT approach,
it may be demanding to tackle issues that arise
when translating between typologically different
languages. Translation from English to Czech is a
typical example. One has to deal with a rich mor-
phology, less constrained word order, changes in
clauses bindings, pro-drops etc.
In this work, we make use of the English to
Czech translation implemented within the Tec-
toMT system, first introduced by ?Zabokrtsky? et al
(2008). In contrast to the phrase-based approach,
TectoMT performs a tree-to-tree machine transla-
tion. Given an input English sentence, the trans-
lation process is divided into three stages: analy-
sis, transfer and synthesis. TectoMT at first con-
ducts an automatic analysis including POS tag-
ging, named entity recognition, syntactic parsing,
semantic role labeling, coreference resolution etc.
This results in a deep syntactic representation of
the English sentence, which is subsequently trans-
ferred into Czech, with the translation of lexical
and grammatical information being provided via
several factors. The process proceeds with a rule-
53
based synthesis stage, when a surface Czech sen-
tence is generated from its deep syntactic struc-
ture.
Deep syntactic representation of a sentence fol-
lows the Prague tectogrammatics theory (Sgall,
1967; Sgall et al, 1986). It is a dependency
tree whose nodes correspond to the content words
in the sentence. Personal pronouns missing on
the surface are reconstructed in special nodes.
Nodes are assigned semantic roles (called func-
tors) and grammatical information is comprised in
so called grammatemes. Furthermore, tectogram-
matical representation is a place where corefer-
ence relations are annotated.
4.1 Model of it within TectoMT
The transfer stage, which maps an English tec-
togrammatical tree to a Czech one, is a place
where the translation model of it is applied. For
every English node corresponding to it, a feature
vector is extracted and fed into a discriminative re-
solver that assigns one of the three classes to it ?
PersPron, To and Null, corresponding to the
main Czech types introduced in Section 3.
If labeled as PersPron, the English node
is mapped to a Czech #PersPron node and the
English coreference link is projected. During
the synthesis, it is decided whether the pronoun
should be expressed on a surface, its gender and
number are copied from the antecedent?s head and
finally the correct form (if any) is generated.
Obtaining class To makes things easier. The
English node is only mapped to a Czech node con-
taining the pronoun ten with its gender and num-
ber set to neuter singular, so that later the correct
form to will be generated.
Last, if it is assigned Null, no corresponding
node on the Czech side is generated, but the Czech
counterpart of the governing verb is forced to be in
neuter singular.
5 Prague Czech-English Dependency
Treebank as a source of data
The Prague Czech-English Dependency Treebank
(Hajic? et al, 2011, PCEDT) is a manually parsed
Czech-English parallel corpus comprising over 1.2
million words for each language in almost 50,000
sentence pairs. The English part contains the en-
tire Penn Treebank?Wall Street Journal Section
(Linguistic Data Consortium, 1999). The Czech
part consists of translations of all the texts from
the English part. The data from both parts are
annotated on three layers following the theory of
Prague tectogrammatics ? the morphological layer
(where each token from the sentence gets a lemma
and a POS tag), the analytical layer (surface syn-
tax in the form of a dependency tree, where each
node corresponds to a token in the sentence) and
the tectogrammatical representation (see Section
4).
Sentences of PCEDT have been automatically
morphologically annotated and parsed into ana-
lytical dependency trees.4 The tectogrammatical
trees in both language parts have been annotated
manually (Hajic? et al, 2012). The nodes of Czech
and English trees have been automatically aligned
on analytical as well as tectogrammatical layer
(Marec?ek et al, 2008).
5.1 Extraction of Classes
The shortcomings of the automatic alignment
is particularly harmful for pronouns and zero
anaphors, which can replace a whole range of con-
tent words and their meaning is inferred mainly
from the context. The situation is better for verbs
as their usual parents in dependency trees: since
they carry meaning in a greater extent, their auto-
matic alignment is of a higher quality.
Thus, we did not search for a Czech counterpart
of it by following the alignment of it itself. Using
the fact that the verb alignment is more reliable
and functors in tectogrammatical trees have been
manually corrected, we followed the alignment of
the parent of it (a verb) and selected the Czech sub-
tree with the same tectogrammatical functor as it
had on the English side. If the obtained subtree
is a single node of type #PersPron or ten, we as-
signed class PersPron or To, respectively, to the
corresponding it. This approach relies also on the
assumption that semantic roles do not change in
the translation.
The automatic acquisition of classes covered
more than 60% of instances, the rest had to be la-
beled manually. During the annotation, we obeyed
the following rules:
1. If a demonstrative pronoun to is present in the
Czech sentence or if a personal pronoun is
either present or unexpressed, assign the in-
stance to the corresponding class.
4The English dependency trees were built by automati-
cally transforming the original phrase-structure annotation of
the Penn Treebank.
54
2. Otherwise, ignore the Czech translation pro-
vided in the corpus and follow the most sim-
plistic possible translation which would still
be correct. Assign the instance to the class
which fits it the best.
Note that it may happen that none of the three
options fits, because it is either an idiomatic ex-
pression or larger structural modifications are re-
quired. Such cases are very rare and we left them
out of the data.
The manual annotation was a bottleneck. We
managed to tag the complete testing data, but were
only able to annotate more than just 1/6 of the
training data due to time reasons. We only use
a corresponding proportion of the automatically
labeled training instances in order to respect the
overall distribution.
5.2 Extraction of Features
Given the linguistically supported observation on
both manually and automatically annotated tree-
banks, we designed features to differentiate be-
tween the ways it is translated.
Since this work focuses on MT with transfer via
deep-syntactic layer, it is possible for the proposed
features to exploit morphological, syntactic and a
little of semantic information present on various
annotation layers.
Unlike the target classes, which have to be as-
signed as accurately as possible, extracted fea-
tures must follow the real-world scenario of MT
? the only information that is given is the source
sentence. Thus, whereas extracting classes may
exploit the gold standard linguistic annotation, it
cannot be employed in feature extraction. We ex-
tract them from text automatically annotated by
the same pipeline that is used in the TectoMT anal-
ysis stage.
However, there is an exception where we violate
this approach ? coreference. Performance of state-
of-the-art coreference resolvers is still far from the
ideal, especially for distinguishing between pro-
nouns referring to noun phrases and those refer-
ring to clauses or wider discourse segments. Sim-
ilarly to the work of Guillou (2012) we wanted
to isolate the problem of translating referential
expressions from the task of resolving the entity
they refer to. Therefore, we opted for extracting
the coreferential features from the gold annotation
projected onto automatically analyzed trees. Note
that the results achieved using these features have
to be considered an upper bound for a given set-
ting.
Although the mapping between Czech transla-
tion of it and English categories of it does not al-
low to translate it directly, the category of it es-
timated by an anaphoricity resolver might be a
promising feature. We therefore constructed a bi-
nary feature based on the output of a system iden-
tifying whether a pronoun it is coreferential or
not. We employed the NADA resolver (Bergsma
and Yarowsky, 2011)5 exploiting the web-scale n-
gram data and its tree-based extension presented
in (Veselovska? et al, 2012).
Some verbs are more likely to bind with it that
refers to a longer utterance. Such it is quite con-
sistently translated as a demonstrative to. This
motivated incorporating a parent lemma of an oc-
currence of it into the feature set. However, the
training data is too small to be a sufficient sample
from a distribution over lexical properties. Hence,
we took advantage of the automatically annotated6
Czech-English corpus CzEng 1.0 (Bojar et al,
2012) that comprises more than 15 million sen-
tence pairs. In the manner described in Section
5.1, we collected co-occurrence counts between
a functor that the given it possesses concatenated
with a lemma of its verbal parent and a Czech
counterpart having the same functor (denoted as
csit). We filtered out all occurrences where csit
was neither #PersPron nor ten. Then, for both val-
ues of csit a feature is constructed by looking up
counts for a concrete occurrence in the collected
counts and quantized into 4-5 bins (Bansal and
Klein, 2012) following the formula:
bin(log(
count(functor : parent ? csit)
count(functor : parent)count(csit)
)).
Linguistic analysis carried out in Section 3 sug-
gests the following syntax-oriented features re-
lated to the verb to be. Some nominal predicates
tend to be translated as to, even though it is usually
coreferential in such expressions (see example 7).
So the corresponding binary feature fires if it is a
subject and its parent is the verb to be having an
object (Figure 2a).
Similarly, adjectival predicates that are not fol-
lowed by a subordinating clause connected with
5A probability value returned by this tool was binarized at
a threshold 0.5
6Using the same annotation layers as in PCEDT and Tec-
toMT, i.e. in accordance with the Prague tectogrammatics
theory.
55
Figure 2: Syntactic features capturing typical con-
structions with a verb be.
the main clause by the English connectives to or
that are usually referential and translated as to,
too. We proposed a feature describing these cases,
illustrated in Figure 2b.
In contrast, if an adjectival predicate is followed
by a subordinating clause with the verb being finite
and connected to the main clause by a conjunction
that, in majority of cases it is a pleonastic usage of
it translated as a null subject (see example 6). A
schema of the feature is depicted in Figure 2c.
Being definitely pleonastic, it in cleft sentences
is expressed in Czech either by to or by sentence
rearranging (see example 8). We target this phe-
nomenon by another feature being fired if it is a
subject of the verb to be and if this verb has an ob-
ject and is followed by a relative clause (see Figure
2d).
Finally, we designed two features exploiting
coreference relations. The first one simply indi-
cates if it has an antecedent, while the second fires
if any of the antecedents in the coreferential chain
is a verb phrase. As we noted above, these fea-
tures are based on the gold standard annotation of
coreference.
5.3 Data Description
The data for training and testing a discriminative
translation model of the personal pronoun it were
extracted from PCEDT with classes and features
obtained as described in Section 5.1 and 5.2, re-
spectively. Due to the limited amount of manually
annotated training data, the training set extracted
from sections 00 ? 19 was reduced from 5841 to
940 instances, though. The testing set was an-
notated thoroughly, thus containing 543 instances
extracted from sections 20 ? 21. Every instance
represents an occurrence of it in PCEDT. The dis-
Class Train Test
PersPron 576 322
To 231 138
Null 133 83
Table 1: Distribution of classes in the data sets.
tribution of target classes in the data is shown in
Table 1.
6 Experiments
Experiments were conducted in two settings that
differ in the usage of features extracted from gold
coreferential relations.
To mitigate a possible error caused by a wrong
classifier choice, we built several models based on
various Machine Learning classification methods.
If not explicitly mentioned, the methods below are
applied with default parameters:
? Vowpal Wabbit (Langford, 2012). Binary
logistic regression with one-against-all strat-
egy for handling multiple classes. The opti-
mum has been found using the online method
(Stochastic Gradient Descent). We varied the
parameters of the number of passes over the
data and the L2 regularization weight.
? AI::MaxEntropy.7 Multiclass logistic re-
gression.8 The optimum has been found us-
ing the batch method (L-BFGS).
? sklearn.neighbors.9 k-nearest neighbors
classifier with the parameter k being varied.
? sklearn.tree. Decision tree classifier.
? sklearn.SVC. Support Vector Machines with
one-against-one strategy to handle multiple
classes. We varied the choice of a kernel.
The accuracy evaluated on both training and test
sets is shown in Table 2 (columns Acc:Train and
Acc:Test). The baseline resolver simply picks the
most frequent class in the training set, which is
PersPron. For both experimental settings, the
standard deviation measured on the test set is less
than 1% in total, if the method?s best configuration
of parameters is taken and the result on decision
trees, which we did not tune, is excluded. This
shows that all classifiers are consistent in their de-
cisions.
7http://search.cpan.org/
?
laye/
AI-MaxEntropy-0.20/
8In the field of NLP also called Maximum Entropy.
9All classifiers labeled as sklearn.* are implemented in
the Scikit-learn Python library (Pedregosa et al, 2011).
56
all feats all feats + coref
ML Method Acc:Train Acc:Test BLEU Acc:Train Acc:Test
Baseline 60.70 59.30 0.1401 60.70 59.30
Original TectoMT ? ? 0.1404 ? ?
Vowpal Wabbit (passes=30) 90.62 75.69 ? 90.83 75.87
Vowpal Wabbit (passes=20) 89.99 76.43 0.1403 90.20 76.98
Vowpal Wabbit (passes=10) 87.78 76.24 ? 87.78 76.61
Vowpal Wabbit (passes=30, l2=0.001) 71.23 66.11 ? 83.03 77.16
Vowpal Wabbit (passes=20, l2=0.001) 82.19 74.95 ? 78.19 74.40
Vowpal Wabbit (passes=10, l2=0.001) 75.03 70.17 ? 72.81 70.17
Vowpal Wabbit (passes=30, l2=0.00001) 90.52 75.69 ? 90.94 76.06
Vowpal Wabbit (passes=20, l2=0.00001) 89.99 76.43 ? 90.09 76.98
Vowpal Wabbit (passes=10, l2=0.00001) 87.67 76.24 ? 87.67 76.61
AI::MaxEntropy 85.99 76.61 0.1403 86.09 76.98
sklearn.neighbors (k=1) 91.57 71.64 ? 93.36 72.19
sklearn.neighbors (k=3) 84.62 72.01 ? 84.93 71.82
sklearn.neighbors (k=5) 84.93 74.77 0.1403 84.72 75.87
sklearn.neighbors (k=10) 82.51 73.30 ? 83.14 75.87
sklearn.tree 93.36 73.66 0.1403 94.10 71.82
sklearn.SVC (kernel=linear) 90.83 75.51 0.1402 91.15 76.80
sklearn.SVC (kernel=poly) 60.70 59.30 ? 60.70 59.30
sklearn.SVC (kernel=rbf) 71.23 68.69 ? 73.76 71.27
Table 2: Intrinsic (accuracy on the training and test data) and extrinsic (BLEU score) evaluation of
translation model of it in configuration with (all feats) and without gold coreferential features (all feats
+ coref).
By introducing linguistically motivated features
exploiting the deep-syntactic description of the
sentence, we gained 17% in total over the base-
line. Moreover, adding features based on the gold
coreference annotation results in a further 0.5%
improvement.
7 Evaluation on MT
Although intrinsic evaluation as performed in Sec-
tion 6 can give us a picture of how accurate the
translation model might be, the main purpose of
this work is to integrate it in a full-fledged MT
system. As explained in Section 4, this component
is tailored for TectoMT ? an MT system where the
transfer is provided through a deep-syntactic layer.
The extrinsic evaluation of the proposed method
was carried out on the English-Czech test set for
WMT 2011 Shared Translation Task (Callison-
Burch et al, 2011).10 This data set contains 3,003
English sentences with one Czech reference trans-
lation, out of which 430 contain at least one occur-
rence of it.
Since this test set is provided with no annota-
tion of coreferential links, the model of it that is
involved in experiments on the end-to-end transla-
tion was trained on a complete feature set exclud-
10http://www.statmt.org/wmt11/test.tgz
ing the coreferential features using the Machine
Learning method that performed best in the intrin-
sic test, i.e. AI::MaxEntropy (see Section 6).
The new method was compared to the rule-
based approach originally used in TectoMT, which
works as follows. In the transfer stage, all occur-
rences of it are translated to a demonstrative ten.
In the synthesis stage, another rule is fired, which
determines whether ten is omitted on the surface.
Then, omitting it corresponds either to a structural
change (Null class) or an unexpressed personal
pronoun (a subset of PersPron class). It makes
this original approach difficult to compare with the
scores in Table 2, as the translation model of it
is applied in the transfer stage, where we do not
know yet if a personal pronoun is to be expressed
or not. Thus, we consider it the most appropriate
to use final translated sentences produced by two
versions of TectoMT in order to compare the dif-
ferent way they handle it.
The shift from the original settings to a new
model for it results in 166 changed sentences. In
terms of BLEU score, we observe a marginal drop
from 0.1404 to 0.1403 when using the new ap-
proach.11 Other classifiers achieved the same or
11For comparison, the best system so far ? Chimera (Bojar
et al, 2013) achieves 0.1994 on the same test set. Chimera
combines Moses, TectoMT and rule-based corrections.
57
new better than old 24
old better than new 13
both equally wrong 9
both equally correct 4
Table 3: The results of manual evaluation con-
ducted on 50 sentences translated by TectoMT in
the original settings (old) and with the new trans-
lation model for it (new)
similar score which correlates with the findings
from intrinsic evaluation (see Table 2). It accords
with a similar experience of Le Nagard and Koehn
(2010) and Guillou (2012) and gives another evi-
dence that the BLEU metric is inaccurate for mea-
suring pronoun translation.
Manual evaluation gives a more realistic view.
We randomly sampled 50 out of the 166 sentences
that differ and one annotator assessed which of
the two systems gave a better translation. Table
3 shows that in almost half of the cases the change
was an improvement. Including the sentences that
are acceptable for both settings, the new approach
picked the correct Czech counterpart of it in 22%
more sentences than the original approach. Since
the proportion of the changed sentences accounts
for almost 39% of all sentences containing it, the
overall proportion of improved sentences with it is
around 8.5% in total.
8 Discussion
Inspecting the manually evaluated translation for
types of improvements and losses, we have found
that in none of the changed sentences the original
system decided to omit ten (obtained by the rule)
on the surface. It shows that the new approach
agrees with the original one on the way of omit-
ting personal pronouns and mainly addresses the
overly simplistic assignment of the demonstrative
ten.
The distribution of target classes over cor-
rected sentences is almost uniform. In 13 out
of 24 improvements, the new system succeeded
in correctly resolving the Null class while in
the remaining 11 cases, the corrected class was
PersPron. It took advantage mostly of the
syntax-based features in the former and sugges-
tions given by the NADA anaphoricity resolver in
the latter.
Examining the errors, we observed that the ma-
jority of them are incurred in the structures with
?it is?. These errors stem mostly from incorrect
activation of syntactic features due to parsing and
POS tagging errors. Example 11 (the Czech sen-
tence is an MT output) shows the latter, when the
POS tagger erroneously labeled the word soy as an
adjective. That resulted in activating the feature
for adjectival predicates followed by that (Figure
2c) instead of a feature indicating cleft structures
(Figure 2d), thus preferring the label Null to the
correct To.
(11) SOURCE: It is just soy that all well-known
manufacturers use now.
TECTOMT: Je to jen so?jove?, z?e zna?m??
vy?robci vs?ech pouz???vaj?? te?d.
9 Conclusion
In this work we presented a novel approach to
dealing with the translation of the English personal
pronoun it. We have shown that the mapping be-
tween the categories of it and the ways of trans-
lating it to Czech is not one-to-one. In order to
deal with this, we designed a discriminative trans-
lation model of it for the TectoMT deep syntax MT
framework.
We have built a system that outperforms its pre-
decessor in 8.5% sentences containing it, taking
advantage of the features based on rich syntactic
annotation the MT system provides, external tools
for anaphoricity resolution and features capturing
lexical co-occurrence in a massive parallel corpus,
The main bottleneck that hampered bigger im-
provements is the manual annotation of the train-
ing data. We managed to accomplish it just on 1/6
of the data, which did not provide sufficient evi-
dence for some specific features.
Our main objective of the future work is thus
to reduce a need for manual annotation by dis-
covering ways of automatic extraction of reliable
classes from a semi-manually annotated corpus
such as PCEDT.
Acknowledgments
This work has been supported by the Grant
Agency of the Czech Republic (grants
P406/12/0658 and P406/2010/0875), the grant
GAUK 4226/2011 and EU FP7 project Khresmoi
(contract no. 257528). This work has been using
language resources developed and/or stored and/or
distributed by the LINDAT-Clarin project of the
Ministry of Education of the Czech Republic
(project LM2010013).
58
References
Mohit Bansal and Dan Klein. 2012. Coreference Se-
mantics from Web Features. In Proceedings of the
50th Annual Meeting of the ACL: Long Papers ? Vol-
ume 1, pages 389?398, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Shane Bergsma and David Yarowsky. 2011. NADA:
A Robust System for Non-Referential Pronoun De-
tection. In DAARC, pages 12?23, Faro, Portugal,
October.
Ondr?ej Bojar, Zdene?k ?Zabokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proceedings of LREC 2012, Istanbul, Turkey,
May. ELRA, European Language Resources Associ-
ation.
Ondr?ej Bojar, Rudolf Rosa, and Ales? Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proceedings of the Eight Workshop on Sta-
tistical Machine Translation. Under review.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Liane Guillou. 2012. Improving Pronoun Translation
for Statistical Machine Translation. In Proceedings
of the Student Research Workshop at the 13th Con-
ference of the EACL, pages 1?10, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Silvie Cinkova?, Eva Fuc???kova?, Marie Mikulova?, Petr
Pajas, Jan Popelka, Jir??? Semecky?, Jana ?Sindlerova?,
Jan ?Ste?pa?nek, Josef Toman, Zden?ka Ures?ova?, and
Zdene?k ?Zabokrtsky?. 2011. Prague Czech-English
Dependency Treebank 2.0.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr
Sgall, Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?,
Marie Mikulova?, Petr Pajas, Jan Popelka, Jir???
Semecky?, Jana ?Sindlerova?, Jan ?Ste?pa?nek, Josef
Toman, Zden?ka Ures?ova?, and Zdene?k ?Zabokrtsky?.
2012. Announcing Prague Czech-English Depen-
dency Treebank 2.0. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3153?3160. ELRA.
Christian Hardmeier and Marcello Federico. 2010.
Modelling Pronominal Anaphora in Statistical Ma-
chine Translation. In Marcello Federico, Ian Lane,
Michael Paul, and Franc?ois Yvon, editors, Proceed-
ings of the seventh International Workshop on Spo-
ken Language Translation (IWSLT), pages 283?289.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the 2003 Conference of the NAACL HLT
? Volume 1, pages 48?54, Stroudsburg, PA, USA.
Association for Computational Linguistics.
John Langford. 2012. Vowpal Wabbit.
Ronan Le Nagard and Philipp Koehn. 2010. Aid-
ing Pronoun Translation with Co-Reference Resolu-
tion. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 252?261, Uppsala, Sweden, July. Association
for Computational Linguistics.
Linguistic Data Consortium. 1999. Penn Treebank 3.
LDC99T42.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of the Twelfth EAMT Conference, pages
102?111.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ?Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Jour-
nal of Machine Learning Research, 12:2825?2830,
November.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing Company,
Dordrecht.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague, Czech Republic.
Kater?ina Veselovska?, Giang Linh Nguy, and Michal
Nova?k. 2012. Using Czech-English Parallel Cor-
pora in Automatic Identification of It. In The Fifth
Workshop on Building and Using Comparable Cor-
pora, pages 112?120.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 167?170, Stroudsburg, PA, USA.
Association for Computational Linguistics.
59
