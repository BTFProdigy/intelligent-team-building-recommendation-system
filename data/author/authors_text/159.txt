Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 579?586, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Emotions from text: machine learning for text-based emotion prediction
Cecilia Ovesdotter Alm?
Dept. of Linguistics
UIUC
Illinois, USA
ebbaalm@uiuc.edu
Dan Roth
Dept. of Computer Science
UIUC
Illinois, USA
danr@uiuc.edu
Richard Sproat
Dept. of Linguistics
Dept. of Electrical Eng.
UIUC
Illinois, USA
rws@uiuc.edu
Abstract
In addition to information, text con-
tains attitudinal, and more specifically,
emotional content. This paper explores
the text-based emotion prediction prob-
lem empirically, using supervised machine
learning with the SNoW learning archi-
tecture. The goal is to classify the emo-
tional affinity of sentences in the narra-
tive domain of children?s fairy tales, for
subsequent usage in appropriate expres-
sive rendering of text-to-speech synthe-
sis. Initial experiments on a preliminary
data set of 22 fairy tales show encourag-
ing results over a na??ve baseline and BOW
approach for classification of emotional
versus non-emotional contents, with some
dependency on parameter tuning. We
also discuss results for a tripartite model
which covers emotional valence, as well
as feature set alernations. In addition, we
present plans for a more cognitively sound
sequential model, taking into considera-
tion a larger set of basic emotions.
1 Introduction
Text does not only communicate informative con-
tents, but also attitudinal information, including
emotional states. The following reports on an em-
pirical study of text-based emotion prediction.
Section 2 gives a brief overview of the intended
application area, whereas section 3 summarizes re-
lated work. Next, section 4 explains the empirical
study, including the machine learning model, the
corpus, the feature set, parameter tuning, etc. Sec-
tion 5 presents experimental results from two classi-
fication tasks and feature set modifications. Section
6 describes the agenda for refining the model, before
presenting concluding remarks in 7.
2 Application area: Text-to-speech
Narrative text is often especially prone to having
emotional contents. In the literary genre of fairy
tales, emotions such as HAPPINESS and ANGER and
related cognitive states, e.g. LOVE or HATE, become
integral parts of the story plot, and thus are of par-
ticular importance. Moreover, the story teller read-
ing the story interprets emotions in order to orally
convey the story in a fashion which makes the story
come alive and catches the listeners? attention.
In speech, speakers effectively express emotions
by modifying prosody, including pitch, intensity,
and durational cues in the speech signal. Thus, in
order to make text-to-speech synthesis sound as nat-
ural and engaging as possible, it is important to con-
vey the emotional stance in the text. However, this
implies first having identified the appropriate emo-
tional meaning of the corresponding text passage.
Thus, an application for emotional text-to-speech
synthesis has to solve two basic problems. First,
what emotion or emotions most appropriately de-
scribe a certain text passage, and second, given a text
passage and a specified emotional mark-up, how to
render the prosodic contour in order to convey the
emotional content, (Cahn, 1990). The text-based
emotion prediction task (TEP) addresses the first of
these two problems.
579
3 Previous work
For a complete general overview of the field of af-
fective computing, see (Picard, 1997). (Liu, Lieber-
man and Selker, 2003) is a rare study in text-
based inference of sentence-level emotional affin-
ity. The authors adopt the notion of basic emotions,
cf. (Ekman, 1993), and use six emotion categories:
ANGER, DISGUST, FEAR, HAPPINESS, SADNESS,
SURPRISE. They critique statistical NLP for being
unsuccessful at the small sentence level, and instead
use a database of common-sense knowledge and cre-
ate affect models which are combined to form a rep-
resentation of the emotional affinity of a sentence.
At its core, the approach remains dependent on an
emotion lexicon and hand-crafted rules for concep-
tual polarity. In order to be effective, emotion recog-
nition must go beyond such resources; the authors
note themselves that lexical affinity is fragile. The
method was tested on 20 users? preferences for an
email-client, based on user-composed text emails
describing short but colorful events. While the users
preferred the emotional client, this evaluation does
not reveal emotion classification accuracy, nor how
well the model generalizes on a large data set.
Whereas work on emotion classification from
the point of view of natural speech and human-
computer dialogues is fairly extensive, e.g. (Scherer,
2003), (Litman and Forbes-Riley, 2004), this ap-
pears not to be the case for text-to-speech synthe-
sis (TTS). A short study by (Sugimoto et al, 2004)
addresses sentence-level emotion recognition for
Japanese TTS. Their model uses a composition as-
sumption: the emotion of a sentence is a function of
the emotional affinity of the words in the sentence.
They obtain emotional judgements of 73 adjectives
and a set of sentences from 15 human subjects and
compute words? emotional strength based on the ra-
tio of times a word or a sentence was judged to fall
into a particular emotion bucket, given the number
of human subjects. Additionally, they conducted an
interactive experiment concerning the acoustic ren-
dering of emotion, using manual tuning of prosodic
parameters for Japanese sentences. While the au-
thors actually address the two fundamental problems
of emotional TTS, their approach is impractical and
most likely cannot scale up for a real corpus. Again,
while lexical items with clear emotional meaning,
such as happy or sad, matter, emotion classifica-
tion probably needs to consider additional inference
mechanisms. Moreover, a na??ve compositional ap-
proach to emotion recognition is risky due to simple
linguistic facts, such as context-dependent seman-
tics, domination of words with multiple meanings,
and emotional negation.
Many NLP problems address attitudinal mean-
ing distinctions in text, e.g. detecting subjective
opinion documents or expressions, e.g. (Wiebe et
al, 2004), measuring strength of subjective clauses
(Wilson, Wiebe and Hwa, 2004), determining word
polarity (Hatzivassiloglou and McKeown, 1997) or
texts? attitudinal valence, e.g. (Turney, 2002), (Bai,
Padman and Airoldi, 2004), (Beineke, Hastie and
Vaithyanathan, 2003), (Mullen and Collier, 2003),
(Pang and Lee, 2003). Here, it suffices to say that
the targets, the domain, and the intended application
differ; our goal is to classify emotional text passages
in children?s stories, and eventually use this infor-
mation for rendering expressive child-directed sto-
rytelling in a text-to-speech application. This can be
useful, e.g. in therapeutic education of children with
communication disorders (van Santen et al, 2003).
4 Empirical study
This part covers the experimental study with a for-
mal problem definition, computational implementa-
tion, data, features, and a note on parameter tuning.
4.1 Machine learning model
Determining emotion of a linguistic unit can be
cast as a multi-class classification problem. For
the flat case, let T denote the text, and s an em-
bedded linguistic unit, such as a sentence, where
s ? T . Let k be the number of emotion classes E =
{em1, em2, .., emk}, where em1 denotes the special
case of neutrality, or absence of emotion. The goal
is to determine a mapping function f : s ? emi,
such that we obtain an ordered labeled pair (s, emi).
The mapping is based on F = {f1, f2, .., fn}, where
F contains the features derived from the text.
Furthermore, if multiple emotion classes can
characterize s, then given E? ? E, the target of the
mapping function becomes the ordered pair (s,E?).
Finally, as further discussed in section 6, the hier-
archical case of label assignment requires a sequen-
580
tial model that further defines levels of coarse ver-
sus fine-grained classifiers, as done by (Li and Roth,
2002) for the question classification problem.
4.2 Implementation
Whereas our goal is to predict finer emotional mean-
ing distinctions according to emotional categories in
speech; in this study, we focus on the basic task of
recognizing emotional passages and on determining
their valence (i.e. positive versus negative) because
we currently do not have enough training data to ex-
plore finer-grained distinctions. The goal here is to
get a good understanding of the nature of the TEP
problem and explore features which may be useful.
We explore two cases of flat classification, us-
ing a variation of the Winnow update rule imple-
mented in the SNoW learning architecture (Carl-
son et al, 1999),1 which learns a linear classifier
in feature space, and has been successful in sev-
eral NLP applications, e.g. semantic role labeling
(Koomen, Punyakanok, Roth and Yih, 2005). In
the first case, the set of emotion classes E consists
of EMOTIONAL versus non-emotional or NEUTRAL,
i.e. E = {N,E}. In the second case, E has been
incremented with emotional distinctions according
to the valence, i.e. E = {N,PE,NE}. Experi-
ments used 10-fold cross-validation, with 90% train
and 10% test data.2
4.3 Data
The goal of our current data annotation project is
to annotate a corpus of approximately 185 children
stories, including Grimms?, H.C. Andersen?s and B.
Potter?s stories. So far, the annotation process pro-
ceeds as follows: annotators work in pairs on the
same stories. They have been trained separately and
work independently in order to avoid any annota-
tion bias and get a true understanding of the task
difficulty. Each annotator marks the sentence level
with one of eight primary emotions, see table 1, re-
flecting an extended set of basic emotions (Ekman,
1993). In order to make the annotation process more
focused, emotion is annotated from the point of view
of the text, i.e. the feeler in the sentence. While the
primary emotions are targets, the sentences are also
1Available from http://l2r.cs.uiuc.edu/?cogcomp/
2Experiments were also run for Perceptron, however the re-
sults are not included. Overall, Perceptron performed worse.
marked for other affective contents, i.e. background
mood, secondary emotions via intensity, feeler, and
textual cues. Disagreements in annotations are re-
solved by a second pass of tie-breaking by the first
author, who chooses one of the competing labels.
Eventually, the completed annotations will be made
available.
Table 1: Basic emotions used in annotation
Abbreviation Emotion class
A ANGRY
D DISGUSTED
F FEARFUL
H HAPPY
Sa SAD
Su+ POSITIVELY SURPRISED
Su- NEGATIVELY SURPRISED
Emotion annotation is hard; interannotator agree-
ment currently range at ? = .24 ? .51, with the ra-
tio of observed annotation overlap ranging between
45-64%, depending on annotator pair and stories as-
signed. This is expected, given the subjective nature
of the annotation task. The lack of a clear defini-
tion for emotion vs. non-emotion is acknowledged
across the emotion literature, and contributes to dy-
namic and shifting annotation targets. Indeed, a
common source of confusion is NEUTRAL, i.e. de-
ciding whether or not a sentence is emotional or
non-emotional. Emotion perception also depends on
which character?s point-of-view the annotator takes,
and on extratextual factors such as annotator?s per-
sonality or mood. It is possible that by focusing
more on the training of annotator pairs, particularly
on joint training, agreement might improve. How-
ever, that would also result in a bias, which is prob-
ably not preferable to actual perception. Moreover,
what agreement levels are needed for successful ex-
pressive TTS remains an empirical question.
The current data set consisted of a preliminary an-
notated and tie-broken data set of 1580 sentence, or
22 Grimms? tales. The label distribution is in table
2. NEUTRAL was most frequent with 59.94%.
Table 2: Percent of annotated labels
A D F H
12.34% 0.89% 7.03% 6.77%
N SA SU+ SU.-
59.94% 7.34% 2.59% 3.10%
581
Table 3: % EMOTIONAL vs. NEUTRAL examples
E N
40.06% 59.94%
Table 4: % POSITIVE vs. NEGATIVE vs. NEUTRAL
PE NE N
9.87% 30.19% 59.94%
Next, for the purpose of this study, all emotional
classes, i.e. A, D, F, H, SA, SU+, SU-, were com-
bined into one emotional superclass E for the first
experiment, as shown in table 3. For the second ex-
periment, we used two emotional classes, i.e. pos-
itive versus negative emotions; PE={H, SU+} and
NE={A, D, F, SA, SU-}, as seen in table 4.
4.4 Feature set
The feature extraction was written in python. SNoW
only requires active features as input, which resulted
in a typical feature vector size of around 30 features.
The features are listed below. They were imple-
mented as boolean values, with continuous values
represented by ranges. The ranges generally over-
lapped, in order to get more generalization coverage.
1. First sentence in story
2. Conjunctions of selected features (see below)
3. Direct speech (i.e. whole quote) in sentence
4. Thematic story type (3 top and 15 sub-types)
5. Special punctuation (! and ?)
6. Complete upper-case word
7. Sentence length in words (0-1, 2-3, 4-8, 9-15,
16-25, 26-35, >35)
8. Ranges of story progress (5-100%, 15-100%,
80-100%, 90-100%)
9. Percent of JJ, N, V, RB (0%, 1-100%, 50-
100%, 80-100%)
10. V count in sentence, excluding participles (0-1,
0-3, 0-5, 0-7, 0-9, > 9)
11. Positive and negative word counts ( ? 1, ? 2,
? 3, ? 4, ? 5, ? 6)
12. WordNet emotion words
13. Interjections and affective words
14. Content BOW: N, V, JJ, RB words by POS
Feature conjunctions covered pairings of counts of
positive and negative words with range of story
progress or interjections, respectively.
Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are ex-
tracted automatically from the sentences in the sto-
ries; with the SNoW POS-tagger used for features
9, 10, and 14. Group 10 reflects how many verbs
are active in a sentence. Together with the quotation
and punctuation, verb domination intends to capture
the assumption that emotion is often accompanied
by increased action and interaction. Feature group
4 is based on Finish scholar Antti Aarne?s classes
of folk-tale types according to their informative the-
matic contents (Aarne, 1964). The current tales
have 3 top story types (ANIMAL TALES, ORDINARY
FOLK-TALES, and JOKES AND ANECDOTES), and
15 subtypes (e.g. supernatural helpers is a subtype
of the ORDINARY FOLK-TALE). This feature intends
to provide an idea about the story?s general affective
personality (Picard, 1997), whereas the feature re-
flecting the story progress is hoped to capture that
some emotions may be more prevalent in certain
sections of the story (e.g. the happy end).
For semantic tasks, words are obviously impor-
tant. In addition to considering ?content words?, we
also explored specific word lists. Group 11 uses
2 lists of 1636 positive and 2008 negative words,
obtained from (Di Cicco et al, online). Group 12
uses lexical lists extracted from WordNet (Fellbaum,
1998), on the basis of the primary emotion words
in their adjectival and nominal forms. For the ad-
jectives, Py-WordNet?s (Steele et al, 2004) SIMI-
LAR feature was used to retrieve similar items of
the primary emotion adjectives, exploring one addi-
tional level in the hierarchy (i.e. similar items of all
senses of all words in the synset). For the nouns and
any identical verbal homonyms, synonyms and hy-
ponyms were extracted manually.3 Feature group 13
used a short list of 22 interjections collected manu-
ally by browsing educational ESL sites, whereas the
affective word list of 771 words consisted of a com-
bination of the non-neutral words from (Johnson-
Laird and Oatley, 1989) and (Siegle, online). Only a
subset of these lexical lists actually occurred.4
3Multi-words were transformed to hyphenated form.
4At this point, neither stems and bigrams nor a list of ono-
matopoeic words contribute to accuracy. Intermediate resource
processing inserted some feature noise.
582
The above feature set is henceforth referred to as
all features, whereas content BOW is just group 14.
The content BOW is a more interesting baseline than
the na??ve one, P(Neutral), i.e. always assigning the
most likely NEUTRAL category. Lastly, emotions
blend and transform (Liu, Lieberman and Selker,
2003). Thus, emotion and background mood of im-
mediately adjacent sentences, i.e. the sequencing,
seems important. At this point, it is not implemented
automatically. Instead, it was extracted from the
manual emotion and mood annotations. If sequenc-
ing seemed important, an automatic method using
sequential target activation could be added next.
4.5 Parameter tuning
The Winnow parameters that were tuned included
promotional ?, demotional ?, activation threshold
?, initial weights ?, and the regularization parame-
ter, S, which implements a margin between positive
and negative examples. Given the currently fairly
limited data, results from 2 alternative tuning meth-
ods, applied to all features, are reported.
? For the condition called sep-tune-eval, 50%
of the sentences were randomly selected and
set aside to be used for the parameter tuning
process only. Of this subset, 10% were subse-
quently randomly chosen as test set with the re-
maining 90% used for training during the auto-
matic tuning process, which covered 4356 dif-
ferent parameter combinations. Resulting pa-
rameters were: ? = 1.1, ? = 0.5, ? = 5,
? = 1.0, S = 0.5. The remaining half of
the data was used for training and testing in the
10-fold cross-validation evaluation. (Also, note
the slight change for P(Neutral) in table 5, due
to randomly splitting the data.)
? Given that the data set is currently small, for the
condition named same-tune-eval, tuning was
performed automatically on all data using a
slightly smaller set of combinations, and then
manually adjusted against the 10-fold cross-
validation process. Resulting parameters were:
? = 1.2, ? = 0.9, ? = 4, ? = 1, S = 0.5. All
data was used for evaluation.
Emotion classification was sensitive to the selected
tuning data. Generally, a smaller tuning set resulted
in pejorative parameter settings. The random selec-
tion could make a difference, but was not explored.
5 Results and discussion
This section first presents the results from exper-
iments with the two different confusion sets de-
scribed above, as well as feature experimentation.
5.1 Classification results
Average accuracy from 10-fold cross validation for
the first experiment, i.e. classifying sentences as ei-
ther NEUTRAL or EMOTIONAL, are included in ta-
ble 5 and figure 1 for the two tuning conditions on
the main feature sets and baselines. As expected,
Table 5: Mean classification accuracy: N vs. E, 2 conditions
same-tune-eval sep-tune-eval
P(Neutral) 59.94 60.05
Content BOW 61.01 58.30
All features except BOW 64.68 63.45
All features 68.99 63.31
All features + sequencing 69.37 62.94
degree of success reflects parameter settings, both
for content BOW and all features. Nevertheless, un-
der these circumstances, performance above a na??ve
baseline and a BOW approach is obtained. More-
over, sequencing shows potential for contributing
in one case. However, observations also point to
three issues: first, the current data set appears to
be too small. Second, the data is not easily separa-
ble. This comes as no surprise, given the subjective
nature of the task, and the rather low interannota-
tor agreement, reported above. Moreover, despite
the schematic narrative plots of children?s stories,
tales still differ in their overall affective orientation,
which increases data complexity. Third and finally,
the EMOTION class is combined by basic emotion
labels, rather than an original annotated label.
More detailed averaged results from 10-fold
cross-validation are included in table 6 using all
features and the separated tuning and evaluation
data condition sep-tune-eval. With these parame-
ters, approximately 3% improvement in accuracy
over the na??ve baseline P(Neutral) was recorded,
and 5% over the content BOW, which obviously did
poorly with these parameters. Moreover, precision is
583
0 10 20 30 40 50 60 70
same-tune-eval
sep-tune-eval
Tuning sets
% Accuracy
P(Neutral) Content BOWAll features except BOW All featuresAll features + sequencing
Figure 1: Accuracy under different conditions (in %)
Table 6: Classifying N vs. E (all features, sep-tune-eval)
Measure N E
Averaged accuracy 0.63 0.63
Averaged error 0.37 0.37
Averaged precision 0.66 0.56
Averaged recall 0.75 0.42
Averaged F-score 0.70 0.47
higher than recall for the combined EMOTION class.
In comparison, with the same-tune-eval procedure,
the accuracy improved by approximately 9% over
P(Neutral) and by 8% over content BOW.
In the second experiment, the emotion category
was split into two classes: emotions with positive
versus negative valence. The results in terms of pre-
cision, recall, and F-score are included in table 7, us-
ing all features and the sep-tune-eval condition. The
decrease in performance for the emotion classes mir-
rors the smaller amounts of data available for each
class. As noted in section 4.3, only 9.87% of the
sentences were annotated with a positive emotion,
and the results for this class are worse. Thus, perfor-
mance seems likely to improve as more annotated
story data becomes available; at this point, we are
experimenting with merely around 12% of the total
texts targeted by the data annotation project.
5.2 Feature experiments
Emotions are poorly understood, and it is espe-
cially unclear which features may be important for
their recognition from text. Thus, we experimented
Table 7: N, PE, and NE (all features, sep-tune-eval)
N NE PE
Averaged precision 0.64 0.45 0.13
Averaged recall 0.75 0.27 0.19
Averaged F-score 0.69 0.32 0.13
Table 8: Feature group members
Word lists interj., WordNet, affective lists, pos/neg
Syntactic length ranges, % POS, V-count ranges
Story-related % story-progress, 1st sent., story type
Orthographic punctuation, upper-case words, quote
Conjunctions Conjunctions with pos/neg
Content BOW Words (N,V,Adj, Adv)
with different feature configurations. Starting with
all features, again using 10-fold cross-validation for
the separated tuning-evaluation condition sep-tune-
eval, one additional feature group was removed un-
til none remained. The feature groups are listed in
table 8. Figure 2 on the next page shows the accu-
racy at each step of the cumulative subtraction pro-
cess. While some feature groups, e.g. syntactic, ap-
peared less important, the removal order mattered;
e.g. if syntactic features were removed first, accu-
racy decreased. This fact also illustrated that fea-
tures work together; removing any group degraded
performance because features interact and there is
no true independence. It was observed that fea-
tures? contributions were sensitive to parameter tun-
ing. Clearly, further work on developing features
which fit the TEP problem is needed.
6 Refining the model
This was a ?first pass? of addressing TEP for TTS.
At this point, the annotation project is still on-going,
and we only had a fairly small data set to draw on.
Nevertheless, results indicate that our learning ap-
proach benefits emotion recognition. For example,
the following instances, also labeled with the same
valence by both annotators, were correctly classified
both in the binary (N vs. E) and the tripartite polar-
ity task (N, NE, PE), given the separated tuning and
evaluation data condition, and using all features:
(1a) E/NE: Then he offered the dwarfs money, and prayed and
besought them to let him take her away; but they said, ?We will
not part with her for all the gold in the world.?
584
Cumulative removal of feature groups
61.81
63.31
62.57
57.95
58.30
58.93
59.56
55
60
65
All features
- Word lists
- Syntactic
- Story-related
- Orthographic
- Conjunctions
- Content words
% A
ccur
acy
All features P(Neutral) BOW
Figure 2: Averaged effect of feature group removal, using sep-tune-eval
(1b) N: And so the little girl really did grow up; her skin was as
white as snow, her cheeks as rosy as the blood, and her hair as
black as ebony; and she was called Snowdrop.
(2a) E/NE: ?Ah,? she answered, ?have I not reason to weep?
(2b) N: Nevertheless, he wished to try him first, and took a stone
in his hand and squeezed it together so that water dropped out
of it.
Cases (1a) and (1b) are from the well-known FOLK
TALE Snowdrop, also called Snow White. (1a)
and (1b) are also correctly classified by the sim-
ple content BOW approach, although our approach
has higher prediction confidence for E/NE (1a); it
also considers, e.g. direct speech, a fairly high verb
count, advanced story progress, connotative words
and conjunctions thereof with story progress fea-
tures, all of which the BOW misses. In addition, the
simple content BOW approach makes incorrect pre-
dictions at both the bipartite and tripartite levels for
examples (2a) and (2b) from the JOKES AND ANEC-
DOTES stories Clever Hans and The Valiant Little
Tailor, while our classifier captures the affective dif-
ferences by considering, e.g. distinctions in verb
count, interjection, POS, sentence length, connota-
tions, story subtype, and conjunctions.
Next, we intend to use a larger data set to conduct
a more complete study to establish mature findings.
We also plan to explore finer emotional meaning dis-
tinctions, by using a hierarchical sequential model
which better corresponds to different levels of cog-
nitive difficulty in emotional categorization by hu-
mans, and to classify the full set of basic level emo-
tional categories discussed in section 4.3. Sequential
modeling of simple classifiers has been successfully
employed to question classification, for example by
(Li and Roth, 2002). In addition, we are working
on refining and improving the feature set, and given
more data, tuning can be improved on a sufficiently
large development set. The three subcorpora in the
annotation project can reveal how authorship affects
emotion perception and classification.
Moreover, arousal appears to be an important
dimension for emotional prosody (Scherer, 2003),
especially in storytelling (Alm and Sproat, 2005).
Thus, we are planning on exploring degrees of emo-
tional intensity in a learning scenario, i.e. a prob-
lem similar to measuring strength of opinion clauses
(Wilson, Wiebe and Hwa, 2004).
Finally, emotions are not discrete objects; rather
they have transitional nature, and blend and overlap
along the temporal dimension. For example, (Liu,
Lieberman and Selker, 2003) include parallel esti-
mations of emotional activity, and include smooth-
585
ing techniques such as interpolation and decay to
capture sequential and interactive emotional activity.
Observations from tales indicate that some emotions
are more likely to be prolonged than others.
7 Conclusion
This paper has discussed an empirical study of the
text-based emotion prediction problem in the do-
main of children?s fairy tales, with child-directed ex-
pressive text-to-speech synthesis as goal. Besides
reporting on encouraging results in a first set of com-
putational experiments using supervised machine
learning, we have set forth a research agenda for
tackling the TEP problem more comprehensively.
8 Acknowledgments
We are grateful to the annotators, in particular A.
Rasmussen and S. Siddiqui. We also thank two
anonymous reviewers for comments. This work was
funded by NSF under award ITR-#0205731, and NS
ITR IIS-0428472. The annotation is supported by
UIUC?s Research Board. The authors take sole re-
sponsibility for the work.
References
Antti Aarne. 1964. The Types of the Folk-Tale: a Classification
and Bibliography. Helsinki: Suomalainen Tiedeakatemia.
Cecilia O. Alm, and Richard Sproat. 2005. Perceptions of emo-
tions in expressive storytelling. INTERSPEECH 2005.
Xue Bai, Rema Padman, and Edoardo Airoldi. 2004. Sen-
timent extraction from unstructured text using tabu search-
enhanced Markov blankets. In MSW2004, Seattle.
Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan.
2004. The sentimental factor: improving review classifi-
cation via human-provided information. In Proceedings of
ACL, 263?270.
Janet Cahn. 1990. The generation of affect in synthesized
Speech. Journal of the American Voice I/O Society, 8:1?19.
Andrew Carlson, Chad Cumby, Nicholas Rizzolo, Jeff Rosen,
and Dan Roth. 1999. The SNoW Learning Architecture.
Technical Report UIUCDCS-R-99-2101, UIUC Comp. Sci.
Stacey Di Cicco et al General Inquirer Pos./Neg. lists
http://www.webuse.umd.edu:9090/
Paul Ekman. 1993. Facial expression and emotion. American
Psychologist, 48(4), 384?392.
Christiane Fellbaum, Ed. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, Mass.
Vasileios Hatzivassiloglou, and Kathleen McKeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of ACL, 174?181.
Philip Johnson-Laird, and Keith Oatley. 1989. The language
of emotions: an analysis of a semantic field. Cognition and
Emotion, 3:81?123.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2005. Generalized inference with multiple semantic role la-
beling systems. In Proceedings of the Annual Conference on
Computational Language Learning (CoNLL), 181?184.
Diane Litman, and Kate Forbes-Riley. 2004. Predicting stu-
dent emotions in computer-human tutoring dialogues. In
Proceedings of ACL, 351?358.
Xin Li, and Dan Roth. 2002. Learning question classifiers: the
role of semantic information. In Proc. International Confer-
ence on Computational Linguistics (COLING), 556?562.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A model of
textual affect sensing using real-world knowledge. In ACM
Conference on Intelligent User Interfaces, 125?132.
Tony Mullen, and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse information
sources. In Proceedings of EMNLP, 412?418.
Bo Pang, and Lillian Lee. 2004. A sentimental education: sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL, 271?278.
Rosalind Picard. 1997. Affective computing. MIT Press, Cam-
bridge, Mass.
Dan Roth. 1998. Learning to resolve natural language ambigu-
ities: a unified approach. In AAAI, 806?813.
Klaus Scherer. 2003. Vocal communication of emotion: a
review of research paradigms. Speech Commununication,
40(1-2):227?256.
Greg Siegle. The Balanced Affective Word List
http://www.sci.sdsu.edu/CAL/wordlist/words.prn
Oliver Steele et al Py-WordNet
http://osteele.com/projects/pywordnet/
Futoshi Sugimoto et al 2004. A method to classify emotional
expressions of text and synthesize speech. In IEEE, 611?
614.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of ACL, 417?424.
Jan van Santen et al 2003. Applications of computer gen-
erated expressive speech for communication disorders. In
EUROSPEECH 2003, 1657?1660.
Janyce Wiebe et al 2004. Learning subjective language. Jour-
nal of Computational Linguistics, 30(3):277?308.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just
how mad are you? Finding strong and weak opinion clauses.
In Proceedings of the Nineteenth National Conference on Ar-
tificial Intelligence (AAAI), 761?769.
586
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 547?554,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminating image senses by clustering with multimodal features
Nicolas Loeff
Dept. of Computer Science
University of Illinois, UC
loeff@uiuc.edu
Cecilia Ovesdotter Alm
Dept. of Linguistics
University of Illinois, UC
ebbaalm@uiuc.edu
David A. Forsyth
Dept. of Computer Science
University of Illinois, UC
daf@uiuc.edu
Abstract
We discuss Image Sense Discrimination
(ISD), and apply a method based on spec-
tral clustering, using multimodal features
from the image and text of the embedding
web page. We evaluate our method on a
new data set of annotated web images, re-
trieved with ambiguous query terms. Ex-
periments investigate different levels of
sense granularity, as well as the impact of
text and image features, and global versus
local text features.
1 Introduction and problem clarification
Semantics extends beyond words. We focus on im-
age sense discrimination (ISD)1 for web images
retrieved from ambiguous keywords, given a mul-
timodal feature set, including text from the doc-
ument which the image was embedded in. For
instance, a search for CRANE retrieves images of
crane machines, crane birds, associated other ma-
chinery or animals etc., people, as well as images
of irrelevant meanings. Current displays for im-
age queries (e.g. Google or Yahoo!) simply list
retrieved images in any order. An application is
a user display where images are presented in se-
mantically sensible clusters for improved image
browsing. Another usage of the presented model
is automatic creation of sense discriminated image
data sets, and determining available image senses
automatically.
ISD differs from word sense discrimination and
disambiguation (WSD) by increased complexity
in several respects. As an initial complication,
both word and iconographic sense distinctions
1Cf. (Schu?tze, 1998) for a definition of sense discrimina-
tion in NLP.
matter. Whereas a search term like CRANE can
refer to, e.g. a MACHINE or a BIRD; iconographic
distinctions could additionally include birds stand-
ing, vs. in a marsh land, or flying, i.e. sense-
distinctions encoded by further descriptive modi-
fication in text. Therefore, as the number of text
senses grow with corpus size, the iconographic
senses grow even faster, and enumerating icono-
graphic senses is extremely challenging; espe-
cially since dictionary senses do not capture icono-
graphic distinctions. Thus, we focus on image-
driven word senses for ISD, but we acknowledge
the importance of iconography for visual meaning.
Also, an image often depicts a related mean-
ing. E.g. a picture retrieved for SQUASH may
depict a squash bug (i.e. an insect on a leaf of
a squash plant) instead of a squash vegetable,
whereas this does not really apply in WSD, where
each instance concerns the ambiguous term itself.
Therefore, it makes sense to consider the divi-
sion between core sense, related sense, and un-
related sense in ISD, and, as an additional com-
plication, their boundaries are often blurred. Most
importantly, whereas the one-sense-per-discourse
assumption (Yarowsky, 1995) also applies to dis-
criminating images, there is no guarantee of
a local collocational or co-occurrence context
around the target image. Design or aesthetics may
instead determine image placement. Thus, con-
sidering local text around the image may not be as
helpful as local context is for standard WSD. In
fact, the query term may even not occur in the
text body. On the other hand, one can assume that
an image spotlights the web page topic and that it
highlights important document information. Also,
images mostly depict concrete senses. Lastly, ISD
from web data is complicated by web pages being
more domain-independent than news wire, the fa-
547
(a) squash flower (b) tennis? (c) hook (d) food (e) bow (f) speaker
Figure 1: Example RELATED images for (a) vegetable and (b) sports senses for SQUASH, and for (c-d) fish and (e-f) musical
instrument for BASS. Related senses are associated with the semantic field of a core sense, but the core sense is visually absent
or undeterminable.
Figure 2: Which fish or instruments are BASS? Image sense annotation is more vague and subjective than in text.
vored corpus for WSD. As noted by (Yanai and
Barnard, 2005), whereas current image retrieval
engines include many irrelevant images, a data set
of web images gives a more real-world point of
departure for image recognition.
Outline Section 2 discusses the corpus data and
image annotation. Section 3 presents the feature
set and the clustering model. Subsequently, sec-
tion 4 introduces the evaluation used, and dis-
cusses experimental work and results. In section
5, this work is positioned with respect to previous
work. We conclude with an outline of plans for
future work in section 6.
2 Data and annotation
Yahoo!?s image query API was used to obtain a
corpus of pairs of semantically ambiguous images,
in thumbnail and true size, and their correspond-
ing web sites for three ambiguous keywords in-
spired by (Yarowsky, 1995): BASS, CRANE, and
SQUASH. We apply query augmentation (cf. Ta-
ble 1), and exact duplicates were filtered out by
identical image URLs, but cases occurred where
both thumbnail and true-size image were included.
Also, some images shared the same webpage or
came from the same site. Generally, the lat-
ter gives important information about shared dis-
course topic, however the images do not necessar-
ily depict the same sense (e.g. a CRANE bird vs.
a meadow), and image features can separate them
into different clusters.
Annotation overview The images were anno-
tated with one of several labels by one of the au-
thors out of context (without considering the web
site and its text), after applying text-based filter-
ing (cf. section 3.1). For annotation purposes, im-
ages were numbered and displayed on a web page
in thumbnail size. In case the thumbnail was not
sufficient for disambiguation, the image linked at
its true size to the thumbnail was inspected.2 The
true-size view depended on the size of the orig-
inal picture and showed the image and its name.
However, the annotator tried to resist name influ-
ence, and make judgements based just on the im-
age. For each query, 2 to 4 core word senses (e.g.
squash vegetable and squash sport for SQUASH)
were distinguished from inspecting the data. How-
ever, because ?context? was restricted to the image
content, and there was no guarantee that the image
actually depicts the query term, additional anno-
tator senses were introduced. Thus, for most core
senses, a RELATED label was included, accounting
for meanings that seemed related to core meaning
but lacked a core sense object in the image. Some
examples for RELATED senses are in Fig. 1. In ad-
dition, for each query term, a PEOPLE label was
included because such images are common due to
the nature of how people take pictures (e.g. por-
traits of persons or group pictures of crowds, when
core or related senses did not apply), as was an
2We noticed a few cases where Yahoo! retrieved a thumb-
nail image different from the true size image.
548
Word (#Annot. images) QueryTerms Senses Coverage Examples of visual annotation cues
BASS
(2881)
5: bass, bass guitar,
bass instrument,
bass fishing, sea
bass
1. fish 35% any fish, people holding catch
2. musical instrument 28% any bass-looking instrument, playing
3. related: fish 10% fishing (gear, boats, farms), rel. food, rel. charts/maps
4. related: musical instrument 8% speakers, accessories, works, chords, rel. music
5. unrelated 12% miscellaneous (above senses not applicable)
6. people 7% faces, crowd (above senses not applicable)
CRANE
(2650)
5: crane,
construction cranes,
whooping crane,
sandhill crane,
origami cranes
1. machine 21% machine crane, incl. panoramas
2. bird 26% crane bird or chick
3. origami 4% origami bird
4. related: machine 11% other machinery, construction, motor, steering, seat
5. related: bird 11% egg, other birds, wildlife, insects, hunting, rel. maps/charts
6. related: origami 1% origami shapes (stars, pigs), paper folding
7. people 7% faces, crowd (above senses not applicable)
8. unrelated 18% miscellaneous (above senses not applicable)
9. karate 1% martial arts
SQUASH
(1948)
10: squash+: rules,
butternut, vegetable,
grow, game of,
spaghetti, winter,
types of, summer
1. vegetable 24% squash vegetable
2. sport 13% people playing, court, equipment
3. related:vegetable 31% agriculture, food, plant, flower, insect, vegetables
4. related:sport 6% other sports, sports complex
5. people 10% faces, crowd (above senses not applicable)
6. unrelated 16% miscellaneous (above senses not applicable)
Table 1: Web images for three ambiguous query terms were annotated manually out of context (without considering the
web page document). For each term, the number of annotated images, the query retrieval terms, the senses, their distribution,
and rough sample annotation guidelines are provided, with core senses marked in bold face. Because image retrieval engines
restrict hits to 1000 images, query expansion was conducted by adding narrowing query terms from askjeeves.com to
increase corpus size. We selected terms relevant to core senses, i.e. the main discrimination phenomenon.
UNRELATED label for irrelevant images which did
not fit other labels or were undeterminable.
For a human annotator, even when using more
natural word senses, assigning sense labels to im-
ages based on image alone is more challenging
and subjective than labeling word senses in tex-
tual context. First of all, the annotation is heav-
ily dependent on domain-knowledge and it is not
feasible for a layperson to recognize fine-grained
semantics. For example, it is straightforward for
the layperson to distinguish between a robin and a
crane, but determining whether a given fish should
have the common name bass applied to it, or
whether an instrument is indeed a bass instrument
or not, is extremely difficult (see Fig. 2; e.g. de-
ciding if a picture of a fish fillet is a picture of a
fish is tricky). Furthermore, most images display
objects only partially; for example just the neck
of a classical double bass instead of the whole in-
strument. In addition, scaling, proportions, and
components are key cues for object discrimina-
tion in real-life, e.g. for singling out an electric
bass from an electric guitar, but an image may
not provide these detail. Thus, senses are even
fuzzier for ISD than WSD labeling. Given that
laypeople are in the majority, it is fair to assume
their perspective and naiveness. This latter fact
also led to annotations? level of specificity differ-
ing according to search term. Annotation criteria
depended on the keyword term and its senses and
their coverage, as shown in Table 1. Neverthe-
less, several border-line cases for label assignment
occurred. Considering that the annotation task is
Keywordquery Filtering
Image feature 
extraction Text feature extraction
1. Compute pair-wise document affinities2. Compute eigenvalues3. Embed and cluster
Evaluation of purity
Figure 3: Overview of algorithm
quite subjective, this is to be expected. In fact,
one person?s labeling often appears as justifiable
as a contradicting label provided by another per-
son. We explore the vagueness and subjective na-
ture of image annotation further in a companion
paper (Alm, Loeff, Forsyth, 2006).
3 Model
Our goal is to provide a mapping between im-
ages and a set of iconographically coherent clus-
ters for a given query word, in an unsupervised
framework. Our approach involves extracting
and weighting unordered bags-of-words (BOWs;
henceforth) features from the webpage text, sim-
ple local and global features from the image, and
running spectral clustering on top. Fig. 3 shows an
overview of the implementation.
549
3.1 Feature extraction
Document and text filtering A pruning process
was used to filter out image-document pairs based
on e.g. language specification, exclusion of ?In-
dex of? pages, pages lacking an extractable target
image, or a cutoff threshold of number of tokens
in the body. For remaining documents, text was
preprocessed (e.g. lower-casing, removing punc-
tuation, tokens being very short, having numbers
or no vowels, etc.). We used a stop word list, but
avoided stemming to make the algorithm language
independent in other respects. When using image
features, grayscale images (no color histograms)
and images without salient regions (no keypoints
detected) were also removed.
Text features We used the following BOWs:
(a) tokens in the page body; (b) tokens in a ?10
window around the target image (if multiple, the
first was considered); (c) tokens in a ?10 window
around any instances of the query keyword (e.g.
squash); (d) tokens of the target image?s alt at-
tribute; (e) tokens of the title tag; (f) some meta
tokens.3 Tf-idf was applied to a weighted aver-
age of the BOWs. Webpage design is flexible, and
some inconsistencies and a certain degree of noise
remained in the text features.
Image features Given the large variability in
the retrieved image set for a given query, it is dif-
ficult to model images in an unsupervised fash-
ion. Simple features have been shown to provide
performance rivaling that of more elaborate mod-
els in object recognition (Csurka et al 2004) and
(Chapelle, Haffner, and Vapnik, 1999), and the
following image bags of features were considered:
Bags of keypoints: In order to obtain a compact
representation of the textures of an image, patches
are extracted automatically around interesting re-
gions or keypoints in each image. The keypoint
detection algorithm (Kadir and Brady, 2001) uses
a saliency measure based on entropy to select re-
gions. After extraction, keypoints were repre-
sented by a histogram of gradient magnitude of
the pixel values in the region (SIFT) (Lowe, 2004).
These descriptors were clustered using a Gaussian
Mixture with ? 300 components, and the result-
ing global patch codebook (i.e. histogram of code-
book entries) was used as lookup table to assign
each keypoint to a codebook entry.
3Adding to META content, keywords was an attribute, but
is irregular. Embedded BODY pairs are rare; thus not used.
Color histograms: Due to its similarity to
how humans perceive color, HSV (hue, saturation,
brightness) color space was used to bin pixel color
values for each image. Eight bins were used per
channel, obtaining an 83 dimensional vector.
3.2 Measuring similarity between images
For the BOWs text representation, we use the com-
mon measure of cosine similarity (cs) of two tf-
idf vectors (Jurafsky and Martin, 2000). The co-
sine similarity measure is also appropriate for key-
point representation as it is also an unordered bag.
There are several measures for histogram compar-
ison (i.e. L1, ?2). As in (Fowlkes et al 2004) we
use the ?2 distance measure between histograms
hi and hj .
?2i,j =
1
2
512?
k=1
(hi(k)? hj(k))2
hi(k) + hj(k)
(1)
3.3 Spectral Clustering
Spectral clustering is a powerful way to sepa-
rate non-convex groups of data. Spectral meth-
ods for clustering are a family of algorithms that
work by first constructing a pairwise-affinity ma-
trix from the data, computing an eigendecomposi-
tion of the data, embedding the data into this low-
dimensional manifold, and finally applying tradi-
tional clustering techniques (i.e. k-means) to it.
Consider a graph with a set of n vertices each
one representing an image document, and the
edges of the graph represent the pairwise affinities
between the vertices. Let W be an n?n symmet-
ric matrix of pairwise affinities. We define these
as the Gaussian-weighted distance
Wij = exp
(
??t(1? csti,j)? ?
k(1? cski,j)? ?
c?2i,j
)
,
(2)
where {?t, ?k, ?c} are scaling parameters for text,
keypoints, and color features.
It has been shown that the use of multiple eigen-
vectors of W is a valid space onto which the data
can be embedded (Ng, Jordan, Weiss, 2002). In
this space noise is reduced while the most signif-
icant affinities are preserved. After this, any tra-
ditional clustering algorithm can be applied in this
new space to get the final clusters. Note that this
is a nonlinear mapping of the original space. In
particular, we employ a variant of k-means, which
includes a selective step that is quasi-optimal in
a Vector Quantization sense (Ueda and Nakano,
1994). It has the added advantage of being more
550
robust to initialization than traditional k-means.
The algorithm follows,
1. For given documents, compute the affinity
matrix W as defined in equation 2.
2. Let D be a diagonal matrix whose (i, i)-th
element is the sum of W ?s i-th row, and de-
fine L = D?1/2WD?1/2.
3. Find the k largest eigenvectors V of L.
4. Define E as V , with normalized rows.
5. Perform clustering on the columns of E,
which represent the embedding of each im-
age into the new space, using a selective step
as in (Ueda and Nakano, 1994).
Why Spectral Clustering? Why apply a vari-
ant of k-means in the embedded space as opposed
to the original feature space? The k-means algo-
rithm cannot separate non-convex clusters. Fur-
thermore, it is unable to cope with noisy dimen-
sions (this is especially true in the case of the text
data) and highly non-ellipsoid clusters. (Ng, Jor-
dan, Weiss, 2002) stated that spectral clustering
outperforms k-means not only on these high di-
mensional problems, but also in low-dimensional,
multi-class data sets. Moreover, there are prob-
lems where Euclidean measures of distance re-
quired by k-means are not appropriate (for in-
stance histograms), or others where there is not
even a natural vector space representation. Also,
spectral clustering provides a simple way of com-
bining dissimilar vector spaces, like in this case
text, keypoint and color features.
4 Experiments and results
In the first set of experiments, we used all features
for clustering. We considered three levels of sense
granularity: (1) all senses (All), (2) merging re-
lated senses with their corresponding core sense
(Meta), (3) just the core senses (Core). For ex-
periments (1) and (2), we used 40 clusters and all
labeled images. For (3), we considered only im-
ages labeled with core senses, and thus reduced the
number of clusters to 20 for a more fair compari-
son. Results were evaluated according to global
cluster purity, cf. Equation 3.4
Global purity =
?
clusters
# of most common sense in cluster
total # images
(3)
4Purity did not include the small set of outlier images, de-
fined as images whose ratio of distances to the second closest
and closest clusters was below a threshold.
Word All senses Meta senses Core senses
BASS 6 senses 4 senses 2 senses
Median 0.60 0.73 0.94
Range 0.03 0.02 0.02
Baseline 0.35 0.45 0.55
CRANE 9 senses 6 senses 4 senses
Median 0.49 0.65 0.86
Range 0.05 0.07 0.07
Baseline 0.27 0.37 0.50
SQUASH 6 senses 4 senses 2 senses
Median 0.52 0.71 0.94
Range 0.03 0.04 0.03
Baseline 0.32 0.56 0.64
Table 2: Median and range of global clustering purity
for 5 runs with different initializations. For each keyword, the
table lists the number of senses, median, and range of global
cluster purity, followed by the baseline. All senses used the
full set of sense labels and 40 clusters. Meta senses merged
core senses with their respective related senses, considering
all images and using 40 clusters. Core senses were clustered
into 20 clusters, using only images labeled with core sense la-
bels. Purity was stable across runs, and peaked for Core. The
baseline reflected the frequency of the most common sense.
Word Img TxtWin BodyTxt Baseline
BASS
Median 0.71 0.83 0.93 0.55
Range 0.05 0.03 0.05
CRANE
Median 0.61 0.84 0.85 0.50
Range 0.07 0.04 0.05
SQUASH
Median 0.71 0.91 0.96 0.64
Range 0.05 0.04 0.03
Table 3: Global and local features? performance. Core
sense images were grouped into 20 clusters, on the basis of
individual feature types, and global cluster purity was mea-
sured. The table lists the median and range from 5 runs with
different initializations. Img included just image features;
TxtWin local tokens in a ?10 window around the target im-
age anchor; BodyTxt global tokens in the page BODY; and
Baseline uses the most common sense. Text performed bet-
ter than image features, and global text appeared better than
local. All features performed above the baseline.
Median and range results are reported for five
runs, given each condition, comparing against the
baseline (i.e. choosing the most common sense).
Table 2 shows that purity was surprisingly good,
stable across query terms, and that it was high-
est when only core sense data was considered. In
addition, purity tended to be slightly higher for
BASS, which may be related to the annotator being
less confident about its fine-grained sense distinc-
tions, and thus less strict for assigning core sense
labels for this query term.5 In addition, we looked
at the relative performance of individual global
and local features using 20 clusters and only core
5A slightly modified HTML extractor yielded similar re-
sults (?0-2% median, ?0-5% range cf. to Tables 2 - 4).
551
Figure 4: First 30 images from a CRANE BIRD cluster consisting of 81 images in the median run. Individual cluster purity
for all senses was 0.67, and for meta senses 0.83. Not all clusters were as pure as this one; global purity for all 40 cluster was
0.49. This cluster appeared to show some iconography; mostly standing cranes. Interestingly, another cluster contained several
images of flying cranes. Most weighted tokens: cranes whooping birds wildlife species. Table 1 has sense labels.
Figure 5: Global purity does not tell the whole story SQUASH VEGETABLE cluster of 22 images in the median run.
Individual cluster purity for all senses was 0.5, and for meta senses 1.0. Global purity for all 40 cluster was 0.52. This cluster
both shows visually coherent images, and a sensible meta semantic field. Most weighted tokens: chayote calabaza add bitter
cup. Presumably, some tokens reflect the vegetable?s use within the cooking domain.
sense data based on a particular feature. Table 3
shows that global text features were most infor-
mative (although not homogenously), but also that
each feature type performed better than the base-
line in isolation. This indicates that an optimal fea-
ture combination may improve over current per-
formance, using manually selected parameters. In
addition, purity is not the whole story. Figs. 4
and 5 show examples of two selected interesting
clusters obtained for CRANE and SQUASH, respec-
tively, using combined image and text features and
all individual senses.6 Inspection of image clus-
ters indicated that image features, both in isolation
and when used in combination, appeared to con-
6The UIUC-ISD data set and results are currently at
http://www.visionpc.cs.uiuc.edu/isd/.
tribute to more visually balanced clusters, espe-
cially in terms of colors and shading. This shows
that further exploring image features may be vi-
tal for attaining more subtle iconographic senses.
Moreover, as discussed in the introduction, images
are not necessarily anchored in the immediate text
which they refer to. This could explain why lo-
cal text features do not perform as well as global
ones. Lastly, in addition, Fig. 6 shows an example
of a partial cluster where the algorithm inferred a
specific related sense.
We also experimented with different number of
clusters for BASS. The results are in Table 4, lack-
ing a clear trend, with comparable variation to dif-
ferent initializations. This is surprising, since we
would expect purity to increase with number of
552
Figure 6: RELATED: SQUASH VEGETABLE cluster, consisting of 27 images. The algorithm discovered a specific SQUASH
BUG-PLANT sense, which appears iconographic. Individual cluster purity for all senses was 0.85, and individual meta purity:
1.0. Global purity for all 40 clusters: 0.52. Most weighted tokens: bugs bug beetle leaf-footed kentucky.
# Clusters 6 10 20 40 80
All
Median 0.61 0.55 0.58 0.60 0.61
Range 0.03 0.05 0.03 0.03 0.04
Meta
Median 0.75 0.70 0.70 0.73 0.72
Range 0.04 0.07 0.04 0.02 0.04
Table 4: Impact of cluster size? We ran BASS for different
number of clusters (5 runs each with distinct initializations),
and recorded median and range of global purity for all six
senses of the query term, and for the four meta senses, with-
out a clear trend.
clusters (Schu?tze, 1998), but may be due to the
spectral clustering. Inspection showed that 6 clus-
ters were dominated by core senses, whereas with
40 clusters a few were also dominated by RE-
LATED senses or PEOPLE. No cluster was domi-
nated by an UNRELATED label, which makes sense
since semantic linkage should be absent between
unrelated items.
5 Comparison to previous work
Space does not allow a complete review of the
WSD literature. (Yarowsky, 1995) demonstrated
that semi-supervised WSD could be successful.
(Schu?tze, 1998) and (Lin and Pantel, 2002a, b)
show that clustering methods are helpful in this
area.
While ISD has received less attention, image
categorization has been approached previously
by adding text features. For example, (Frankel,
Swain, and Athitsos, 1996)?s WebSeer system
attempted to mutually distinguish photos, hand-
drawn, and computer-drawn images, using a com-
bination of HTML markup, web page text, and im-
age information. (Yanai and Barnard, 2005) found
that adding text features could benefit identifying
relevant web images. Using text-annotated images
(i.e. images annotated with relevant keywords),
(Barnard and Forsyth, 2001) clustered them ex-
ploring a semantic hierarchy; similarly (Barnard,
Duygulu, and Forsyth, 2002) conducted art clus-
tering, and (Barnard and Johnson, 2005) used text-
annotated images to improve WSD. The latter pa-
per obtained best results when combining text and
image features, but contrary to our findings, im-
age features performed better in isolation than just
text. They did use a larger set of image features
and segmentation, however, we suspect that dif-
ferences can rather be attributed to corpus type. In
fact, (Yanai, Shirahatti, and Barnard, 2005) noted
that human evaluators rated images obtained via
a keyword retrieval method higher compared to
image-based retrieval methods, which they relate
to the importance of semantics for what humans
regard as matching, and because pictorial seman-
tics is hard to detect.
(Cai et al 2004) use similar methods to rank
visual search results. While their work does not
focus explicitly on sense and does not provide in-
depth discussion of visual sense phenomena, these
do appear in, for example, figs. 7 and 9 of their pa-
per. An interesting aspect of their work is the use
of page layout segmentation to associate text with
images in web documents. Unfortunately, the au-
553
thors only provide an illustrative query example,
and no numerical evaluation, making any com-
parison difficult. (Wang et al 2004) use similar
features with the goal to improve image retrieval
through similarity propagation, querying specific
web sites. (Fuji and Ishikawa, 2005) deal with
image ambiguity for establishing an online mul-
timedia encyclopedia, but their method does not
integrate image features, and appears to depend
on previous encyclopedic background knowledge,
limited to a domain set.
6 Conclusion
It is remarkable how high purity is, considering
that we are using relatively simple image and text
representation. In most corpora used to date for re-
search on illustrated text, word sense is an entirely
secondary phenomenon, whereas our data set was
collected as to emphasize possible ambiguities as-
sociated with word sense. Our results suggest that
a surprisingly degree of the meaning of an illus-
trated object is exposed on the surface.
This work is an initial attempt at addressing
the ISD problem. Future work will involve learn-
ing the algorithm?s parameters without supervi-
sion, and develop a semantically meaningful im-
age taxonomy. In particular, we intend to explore
the notion of iconographic senses; surprisingly
good results on image classification by (Chapelle,
Haffner, and Vapnik, 1999) using image features
suggest that iconography plays an important role
in the semantics of images. An important aspect
is to enhance our understanding of the interplay
between text and image features for this purpose.
Also, it remains an unsolved problem how to enu-
merate iconographic senses, and use them in man-
ual annotation and classification. Experimental
work with humans performing similar tasks may
provide increased insight into this issue, and can
also be used to validate clustering performance.
7 Acknowledgements
We are grateful to Roxana Girju and Richard
Sproat for helpful feedback, and to Alexander
Sorokin.
References
C. O. Alm, N. Loeff, and D. Forsyth. 2006. Challenges for
annotating images for sense disambiguation. ACL work-
shop on Frontiers in Linguistically Annotated Corpora.
K. Barnard and D. Forsyth. 2001. Learning the semantics of
words and pictures. ICCV, 408?415.
K. Barnard, P. Duygulu, and D. Forsyth. 2002. Modeling the
statistics of image features and associated text. SPIE.
K. Barnard and M. Johnson. 2005. Word sense disambigua-
tion with pictures. Artificial Intelligence, 167, 13?30.
D. Cai et al 2004. Hierarchical clustering of WWW image
search results using visual, textual and link information.
ACM Multimedia, 952-959.
O. Chapelle and P. Haffner and V. Vapnik. 1999. Support
vector machines for histogram-based image classification.
IEEE Neural Networks, 10(5), 1055?1064.
G. Csurka et al 2004. Visual categorization with bags
of keypoints. ECCV Int. Workshop on Stat. Learning in
Computer Vision.
C. Frankel, M. Swain, and V. Athitsos. 1996. WebSeer: an
image search engine for the World Wide Web. Univ. of
Chicago, Computer Science, Technical report #96-14.
C. Fowlkes, S. Belongie, F. Chung, and J. Malik. 2004.
Spectral grouping using the Nystro?m method. IEEE
PAMI, 26(2),214-225.
A. Fuji and T. Ishikawa. 2005. Toward the automatic com-
pilation of multimedia encyclopedias: associating images
with term descriptions on the web. IEEE WI, 536-542.
D. Jurafsky and J. Martin 2000. Speech and Language Pro-
cessing, Prentice Hall.
T. Kadir and M. Brady. 2001. Scale, saliency and image
description. Int. Journal of Computer Vision, 45 (2):83?
105.
D. Lin and P. Pantel. 2002a. Concept discovery from text.
COLING, 577?583.
D. Lowe. 2004. Distinctive image features from scale-
invariant keypoints. Int. Journal of Computer Vision,
60(2), 91?110.
A. Ng, M. Jordan, and Y. Weiss. 2002. On spectral cluster-
ing: analysis and an algorithm. NIPS 14.
P. Pantel and D. Lin. 2002b. Discovering word senses from
text. KDD, 613?619.
H. Schuetze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
J. Shi and J. Malik. 2000. Normalized cuts and image seg-
mentation. IEEE PAMI, 22(8):888?905.
N. Ueda. and R. Nakano. 1994. A new competitive learn-
ing approach based on an equidistortion principle for
designing optimal vector quantizers. Neural Networks,
7(8):1211?1227.
X.-J. Wang et al 2004. Multi-model similarity propagation
and its application for image retrieval. MM,944?951.
K. Yanai and K. Barnard. 2005. Probabilistic web image
gathering. SIGMM, 57?64.
K. Yanai, N. V. Shirahatti, and K. Barnard. 2005. Evaluation
strategies for image understanding and retrieval. SIGMM,
217-226.
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. ACL, 189?196.
554
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 1?4,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Challenges for annotating images for sense disambiguation
Cecilia Ovesdotter Alm
Dept. of Linguistics
University of Illinois, UC
ebbaalm@uiuc.edu
Nicolas Loeff
Dept. of Computer Science
University of Illinois, UC
loeff@uiuc.edu
David A. Forsyth
Dept. of Computer Science
University of Illinois, UC
daf@uiuc.edu
Abstract
We describe an unusual data set of thou-
sands of annotated images with interest-
ing sense phenomena. Natural language
image sense annotation involves increased
semantic complexities compared to dis-
ambiguating word senses when annotating
text. These issues are discussed and illus-
trated, including the distinction between
word senses and iconographic senses.
1 Introduction
We describe a set of annotated images, each asso-
ciated with a sense of a small set of words. Build-
ing this data set exposes important sense phenom-
ena which not only involve natural language but
also vision. The context of our work is Image
Sense Discrimination (ISD), where the task is to
assign one of several senses to a web image re-
trieved by an ambiguous keyword. A compan-
ion paper introduces the task, presents an unsuper-
vised ISD model, drawing on web page text and
image features, and shows experimental results
(Loeff et al, 2006). The data was subject to single-
annotator labeling, with verification judgements
on a part of the data set as a step toward study-
ing agreement. Besides a test bed for ISD, the
data set may be applicable to e.g. multimodal word
sense disambiguation and cross-language image
retrieval. The issues discussed concern concepts,
and involve insights into semantics, perception,
and knowledge representation, while opening up a
bridge for interdisciplinary work involving vision
and NLP.
2 Related work
The complex relationship between annotations
and images has been explored by the library com-
munity, who study management practices for im-
age collections, and by the computer vision com-
munity, who would like to provide automated im-
age retrieval tools and possibly learn object recog-
nition methods.
Commercial picture collections are typically an-
notated by hand, e.g. (Enser, 1993; Armitage and
Enser, 1997; Enser, 2000). Subtle phenomena can
make this very difficult, and content vs. interpreta-
tion may differ; an image of the Eiffel tower could
be annotated with Paris or even love, e.g. (Ar-
mitage and Enser, 1997), and the resulting annota-
tions are hard to use, cf. (Markkula and Sormunen,
2000), or Enser?s result that a specialized indexing
language gives only a ?blunt pointer to regions of
the Hulton collections?, (Enser, 1993), p. 35.
Users of image collections have been well stud-
ied. Important points for our purposes are: Users
request images both by object kinds, and individ-
ual identities; users request images both by what
they depict and by what they are about; and that
text associated with images is extremely useful in
practice, newspaper archivists indexing largely on
captions (Markkula and Sormunen, 2000).
The computer vision community has stud-
ied methods to predict annotations from images,
e.g. (Barnard et al, 2003; Jeon et al, 2003; Blei
and Jordan, 2002). The annotations that are pre-
dicted most successfully tend to deal with ma-
terials whose identity can be determined without
shape analysis, like sky, sea and the like. More
complex annotations remain difficult. There is no
current theory of word sense in this context, be-
cause in most current collections, words appear in
the most common sense only. Sense is known to
be important, and image information can disam-
biguate word senses (Barnard and Johnson, 2005).
1
Word (#Annot. images) QueryTerms Senses Coverage Examples of visual annotation cues
BASS
(2881)
5: bass, bass guitar,
bass instrument,
bass fishing, sea
bass
1. fish 35% any fish, people holding catch
2. musical instrument 28% any bass-looking instrument, playing
3. related: fish 10% fishing (gear, boats, farms), rel. food, rel. charts/maps
4. related: musical instrument 8% speakers, accessories, works, chords, rel. music
5. unrelated 12% miscellaneous (above senses not applicable)
6. people 7% faces, crowds (above senses not applicable)
CRANE
(2650)
5: crane,
construction cranes,
whooping crane,
sandhill crane,
origami cranes
1. machine 21% machine crane, incl. panoramas
2. bird 26% crane bird or chick
3. origami 4% origami bird
4. related: machine 11% other machinery, construction, motor, steering, seat
5. related: bird 11% egg, other birds, wildlife, insects, hunting, rel. maps/charts
6. related: origami 1% origami shapes (stars, pigs), paper folding
7. people 7% faces, crowds (above senses not applicable)
8. unrelated 18% miscellaneous (above senses not applicable)
9. karate 1% martial arts
SQUASH
(1948)
10: squash+: rules,
butternut, vegetable,
grow, game of,
spaghetti, winter,
types of, summer
1. vegetable 24% squash vegetable
2. sport 13% people playing, court, equipment
3. related:vegetable 31% agriculture, food, plant, flower, insect, vegetables
4. related:sport 6% other sports, sports complex
5. people 10% faces, crowds (above senses not applicable)
6. unrelated 16% miscellaneous (above senses not applicable)
Table 1: Overview of annotated images for three ambiguous query terms, inspired by the WSD literature. For each term,
the number of annotated images, the expanded query retrieval terms (taken terms from askjeeves.com), the senses, their
distribution coverage, and rough sample annotation guidelines are provided, with core senses marked in bold.
(a) machine (b)
bird
(c) origami (d)
karate
(e) rel. to a (f) rel. to b (g)
rel. to c
(h)
people
(i) unrel.
Figure 1: CRANE images with clear senses: (a-d) core senses, (e-g) related senses, (h) people and (i) unrelated. Related
senses are associated with the semantic field of a core sense, but the core sense is visually absent or undeterminable.
3 Data set
The data set has images retrieved from a web
search engine. We deliberately focused on three
keywords, which cover a range of phenomena in
semantic ambiguity: BASS, CRANE, and SQUASH.
Table 1 gives an overview of the data set, anno-
tated by one author (CA).1 The webpage was not
considered to avoid bias, given the ISD task.
For each query, 2 to 4 core word senses were
distinguished from inspecting the data using com-
mon sense. We chose this approach rather than
ontology senses which tend to be incomplete or
too specific for our purposes. For example, the
origami sense of CRANE is not included in Word-
Net under CRANE, but for BASS three different
senses appear with fish. WordNet contains bird
as part of the description for the separate entry
origami, and some query expansion terms are hy-
ponyms which occur as separate WordNet entries
(e.g. bass guitar, sea bass, summer squash). Im-
ages may show multiple objects; a general strategy
preferred a core sense if it was included.
An additional complication is that given that the
images are retrieved by a search engine there is no
guarantee that they depict the query term, so ad-
ditional senses were introduced. Thus, for most
1We call the data set the UIUC-ISD data set. It is currently
at http://www.visionpc.cs.uiuc.edu/isd/.
core senses, a RELATED label was included for
meanings related to the semantic field of a core
sense. Also, a PEOPLE label was included since
such images may occur due to how people take
pictures (e.g. portraits of persons, group pictures,
or other representations of people outside core and
related senses). An UNRELATED label accounted
for images that did not fit other labels, or were ir-
relevant or undeterminable. In fact, distinguish-
ing between PEOPLE and UNRELATED was not al-
ways straightforward. Fig. 1 shows examples of
CRANE when sense assignment was quite straight-
forward. However, distinguishing image senses
was often not this clear. In fact, many border-line
cases occurred when one could argue for different
label assignments. Also, annotation cues are sub-
ject to interpretation, and disagreements between
judges are expected. They simply reflect that im-
age senses are located on a semantic continuum.
4 Why annotating image senses is hard
In general, annotating images involves special
challenges, such as what to annotate and how ex-
tensively. We assign an image one sense. Never-
theless, compared to disambiguating a word, sev-
eral issues are added for annotation. As noted
above, a core sense may not occur, and judge-
ments are characterized by increased subjectivity,
with semantics beyond prototypical and peripheral
2
(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)
(k) (l) (m) (n) (o) (p) (q)
Figure 2: Annotating images is often challenging for different reasons. Are these images of CRANE birds? (a-c) depiction
(d-f) gradient change (g-h) partial display (i-j) domain knowledge (k) unusual appearance (l-n) distance (o-q) not animate.
exemplars. Also, the disambiguating context is
limited to image contents, rather than collocations
of an ambiguous token. Fig. 2 illustrates selected
challenging judgement calls for assigning or not
the bird sense of CRANE, as discussed below.
Depiction: Images may include man-made de-
pictions of an object in artistic depictions, and the
question is whether this counts as the object or
not, e.g. Fig. 2(a-c). Gradient changes: Recog-
nition is complicated by objects taking different
forms and shapes, cf. the insight by (Labov, 1973)
on gradual categories.2 For example, as seen in
Fig. 2(d-f), birds change with age; an egg may be
a bird, but a chick is, as is a fledgeling. Partial
display: Objects may be rendered in incomplete
condition. For example, Fig. 2(g-h) show merely
feathers or a bird neck. Domain knowledge: Peo-
ple may disagree due to differences in domain
knowledge, e.g. some non-experts may have a dif-
ficult time determining whether or not other sim-
ilar bird species can be distinguished from a bird
crane, cf. Fig. 2(i-j). This also affected annota-
tions? granularity depending on keyword, see Ta-
ble 1?s example cues. Unusual appearance: Ob-
jects may occur in less frequent visual appear-
ance, or lack distinguishing properties. For in-
stance, Fig. 2(k) illustrates how sunset background
masks birds? color information. Scale: The dis-
tance to objects may render them unclear and in-
fluence judgement accuracy, and people may dif-
fer in the degree of certainty required for assign-
ing a sense. For example, Fig. 2(l-n) show flying
or standing potential cranes at distance. Animate:
Fig. 2(o-q) raise the question whether dead, skele-
tal, or artificial objects are instantiations or not.
Other factors complicating the annotation task in-
clude image crowdedness disguising objects, cer-
tain entities having less salience, and lacking or
unclear reference to object proportions. Senses
2Function or properties may also influence (Labov, 1973).
may also be etymologically related or blend occa-
sionally, or be guided by cultural interpretations,
and so on.
Moreover, related senses are meant to capture
images associated with the semantic field of a core
sense. However, because the notion and borders of
a semantic field are non-specific, related senses
are tricky. Annotators may build associations
quite wildly, based on personal experience and
opinion, thus what is or is not a related sense may
very quickly get out of hand. For instance, a per-
son may by association reason that if bird cranes
occur frequently in fields, then an image of a field
alone should be marked as related. To avoid this,
guidelines attempted to restrict related senses, as
exemplified in Table 1, with some data-driven re-
visions during the annotation process. However,
guidelines are also based on judgement calls. Be-
sides, for abstract concepts like LOVE, differenti-
ating core versus related sense is not really valid.
Lastly, an additional complexity of image
senses is that in addition to traditional word
senses, images may also capture repeatedly oc-
curring iconographic patterns or senses. As illus-
trated in Fig. 3, the iconography of flying cranes
is quite different from that of standing cranes, as
regards motion, shape, identity, and color of figure
and ground, respectively. Mixed cases also occur,
e.g. when bird cranes are taking off or are about
to land in relation to flight. Iconographic senses
may compare to more complex linguistic struc-
tures than nominal categories, e.g. a modified NP
or clause, but are represented by image properties.
A policy for annotating iconographic senses is
still lacking. Image groups based on iconographic
senses seem to provide increased visual and se-
mantic harmony for the eye, but experiments are
needed to confirm how iconographic senses cor-
respond to humans? perception of semantic image
similarity, and at what level of semantic differen-
3
(a) (b) (c) (d) (e) (f) (g) (h)
Figure 3: Iconographic bird CRANE senses: (a-c) flying cranes, (d-f) standing cranes, and (g-h) mixed cases in-between.
(a) 5/2 (b) 1/4 (c) 4/1 (d) 4/1 (e) 4/8 (f) 8/2 (g) 8/1 (h) 6/8,5 (i) 4/1
Figure 4: Disagreement examples (sense numbers in Table 1): (a) crane or other bird? (b) toy crane or scales? (c) crane or
other steel structure/elevator? (d) crane or other machine? (e) company is related or not? (f) bird or abstract art? (g) crane in
background or not? (h) origami-related paper? (i) inside of crane? (and is inside sufficient to denote image as machine crane?)
tiation they become relevant for sense assessment.
Lastly, considering the challenges of image an-
notation, it is interesting to look at annotation dis-
agreements. Thus, another author (NL) inspected
CRANE annotations, and recorded disagreement
candidates, which amounted to 5%. Rejecting or
accepting a category label seems less hard than
independent annotation but still can give insights
into disagreement tendencies. Several disagree-
ments involved a core category vs. its related label
vs. unrelated, rather than two core senses. Also,
some disagreement candidates had tiny, fuzzy,
partial or peripheral potential sense objects, or
lacked distinguishing object features, so interpre-
tation became quite idiosyncratic. The disagree-
ment candidates were discussed together, result-
ing in 2% being true disagreements, 2% false dis-
agreements (resolved by consensus on CA?s la-
bels), and 1% annotation mistakes. Examples of
true disagreements are in Fig. 4. Often, both par-
ties could see each others? points, but opted for an-
other interpretation; this confirms that border lines
tend to merge, indicating that consistency is chal-
lenging and not always guaranteed. As the annota-
tion procedure advances, criteria may evolve and
modify the fuzzy sense boundaries.
5 Conclusion
This work draws attention to the need for consid-
ering natural language semantics in multi-modal
settings. Annotating image senses adds increased
complexity compared to word-sense annotation
in text due to factors such as image proper-
ties, subjective perception, and annotator domain-
knowledge. Moreover, the concept of related
senses as well as iconographic senses go beyond
and diversify the notion of word sense. In the fu-
ture, we would like to perform experimentation
with human subjects to explore both similarity
judgements for image pairs or groups, as well as
issues in interannotator agreement for image dis-
ambiguation, and, finally, to better understand the
role of iconography for semantic interpretation.
6 Acknowledgements
Thanks to anonymous reviewers, R. Girju and R.
Sproat for feedback. Any fallacies are our own.
References
L. H. Armitage and P. G. B. Enser. 1997. Analysis
of user need in image archives. J. of Inform. Sci.,
23(4):287?299.
K. Barnard and M. Johnson. 2005. Word sense disam-
biguation with pictures. Artif. Intel., 167:13?30.
K. Barnard, P. Duygulu, N. Freitas, D. Forsyth, D. Blei,
and M. I. Jordan. 2003. Matching words and pic-
tures. J. of Mach. Learn. Research, 3:1107?1135.
D. M. Blei and M. I. Jordan. 2002. Modeling anno-
tated data. Technical Report CSD-02-1202, Div. of
Computer Science, Univ. of California, Berkeley.
P. G. B. Enser. 1993. Query analysis in a visual infor-
mation retrieval context. J. of Doc. and Text Man-
agement, 1(1):25?52.
P. G. B. Enser. 2000. Visual image retrieval: seek-
ing the alliance of concept based and content based
paradigms. J. of Inform. Sci., 26(4):199?210.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Auto-
matic image annotation and retrieval using crossme-
dia relevance models. In SIGIR, pages 119?126.
W. Labov. 1973. The boundaries of words and their
meanings. In C. J. Baily and R. Shuy, editors, New
ways of analyzing variation in English, pages 340?
373. Washington D.C: Georgetown Univ. Press.
N. Loeff, C. O. Alm, and D. A. Forsyth. 2006. Dis-
criminating image senses by clustering with multi-
modal features. In ACL (forthcoming).
M. Markkula and E. Sormunen. 2000. End-user
searching challenges indexing practices in the digital
newspaper photo archive. Inform. Retr., 1:259?285.
4
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1718?1727, Dublin, Ireland, August 23-29 2014.
Towards multimodal modeling of physicians? diagnostic
confidence and self-awareness using medical narratives
Joseph Bullard
?
Cecilia Ovesdotter Alm
?
Qi Yu
?
Pengcheng Shi
?
Anne Haake
?
?
College of Computing and Information Sciences
?
College of Liberal Arts
Rochester Institute of Technology
jtb4478@cs.rit.edu
coagla|qi.yu|spcast|arhics@rit.edu
Abstract
Misdiagnosis is a problem in the medical field, often related to physicians? cognitive errors.
Overconfidence is considered a major cause of such errors. Intelligent diagnostic support sys-
tems could benefit from understanding how aware physicians are of their performance when they
estimate their confidence in a diagnosis (i.e. a physician?s diagnostic self-awareness). Shed-
ding light on the cognitive processes related to such awareness could also help improve medical
education. We use a multimodal dataset of medical narratives to computationally model diagnos-
tic confidence and self-awareness based on physicians? linguistic and eye movement behaviors.
Dermatologists viewed images of cutaneous conditions, providing a description, diagnosis, and
certainty level for each image case, while their speech and eye movements were recorded. We
define both a generalized and a personalized approach to binning confidence levels, used in clas-
sification experiments. We also introduce truly multimodal features, which focus on combining
linguistic and eye movement data into multimodal attributes. Results indicate that combinations
of multiple modalities can outperform their constituent modalities in isolation for these problems.
1 Introduction
Misdiagnosis in the medical field is estimated to be as high as 10%-15% (Berner and Graber, 2008;
Croskerry, 2009). Such errors can result in incorrect or delayed treatment, causing patients to experience
additional suffering. Graber et al. (2002) describe three types of diagnostic errors: no-fault errors, result-
ing from atypical disease presentation or limitations of medical knowledge; system errors, resulting from
problems with the health care system; and cognitive errors, resulting from biases or faulty interpretation
on the part of a physician. Cognitive errors in particular have potential for substantial reduction through
education and training aimed at developing clinicians? metacognitive skills. Understanding the cognitive
processes of physicians during diagnosis is also of critical importance for building human-centered di-
agnostic support systems, which could help detect and flag problematic diagnostic self-awareness cases.
Examples of cognitive errors include settling on a final diagnosis too early, without ever considering the
correct diagnosis (Berner and Graber, 2008), or confirmation bias, in which only evidence to confirm a
diagnostic hypothesis is considered (Croskerry, 2003). Overconfidence is generally thought to be a major
cause of such errors (Berner and Graber, 2008; Croskerry, 2008). For example, an overconfident physi-
cian may not question her original thoughts or explore alternative diagnoses until later in the treatment
process. In general, overconfidence may be a systemic problem, reinforced by patients? preferences for
confident doctors, and by a professional environment that favors decisive actions (Katz, 1984). Similarly,
underconfidence can erode patients? trust in their providers. In this study, we view the interplay between
confidence
1
and correctness as a two-dimensional problem (see Figure 1). Ideally, physicians would
have high confidence when correct and low confidence when incorrect, indicated by the upper-left and
lower-right quadrants in Figure 1.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
For consistency, this paper uses the term confidence, treated as interchangeable with certainty and similar synonymous
expressions which may have been used by clinicians in the medical narratives, such as sure, certain, confident, etc.
1718
Appropriate
Confidence
Overconfidence
Underconfidence
Appropriate
Confidence
Confident
Not confident
Correct Incorrect
Figure 1: Two-dimensional view of the confidence and correctness relationship as it relates to diagnostic
self-awareness. A similar conceptual model is presented by Pon-Barry and Shieber (2011). Ideally,
physicians should have high confidence when they are correct and low confidence when incorrect.
Contribution Diagnostic self-awareness is an important phenomenon with implications for clinical
training and practice, yet has received little focus from a computational perspective. We report on com-
putational modeling for predicting the confidence and correctness interplay in diagnosis using features
of physicians? speech, eye movements, and combinations thereof, as dermatologists performed medical
image inspection tasks while narrating their diagnostic thought process. In dermatology, visual expertise
and clinical knowledge are both important. A motivation behind our multimodal approach is that medi-
cal image inspection relies on both the physician?s visual perceptual expertise and conceptual knowledge
base, each of which can be regarded as expressed by eye movement behavior and linguistic behavior,
respectively. We aim to apply this decision modeling to intelligent diagnostic support and clinical tutor-
ing systems. Here we solve a foundational problem by successfully modeling the complex relationship
between physicians? confidence in and correctness of their diagnoses. We also make contributions in
multimodal and linguistic feature analysis: carefully assessing feature modalities that represent physi-
cians? behaviors, and introducing a novel multimodal feature type that focuses on fusing eye movement
and verbal data.
2 Previous Work
Although there are many causes of diagnostic errors (Graber et al., 2005), those resulting from cognitive
errors may be the most challenging to reduce (Croskerry, 2003; Graber et al., 2002), while their reduction
provides high impact. Examples of such errors include flawed perception, biased heuristics, and settling
on a final diagnosis too early (Graber et al., 2002), all of which can be caused by overconfidence (Berner
and Graber, 2008; Croskerry, 2008). Underconfidence may also be a problem if it prevents a physician
from pursuing a correct diagnosis (Friedman et al., 2005).
There is evidence for links between speech and confidence in terms of prosodic features, such as
pitch and loudness (Scherer et al., 1973; Pon-Barry and Shieber, 2011; Kimble and Seidel, 1991), as
well as other characteristics of spoken language, such as speech disfluencies (Womack et al., 2012)
and hedges (Smith and Clark, 1993). Prosodic features have been identified and successfully used in
intelligent tutoring systems (Liscombe et al., 2005), where a student?s confidence (or lack thereof) can
play a key role in effective system response. In medical diagnosis, prosodic and lexical features have
been useful indicators of physicians? confidence and diagnostic correctness, individually (Womack et al.,
2013; McCoy et al., 2012). Other potentially useful information may be evident in speech as well. In
a study by Womack et al. (2012) on a similar dataset, the authors found a relationship between speech
characteristics and physician experience: attending (experienced) physicians used more filled pauses and
spoke more than resident (in-training) physicians. Additionally, verbal features may expose differences
in diagnostic reasoning that may be useful predictors of confidence. Rogers (1996) analyzed a dataset of
spoken chest X-ray examinations by radiologists, remarking that reasoning styles influence physicians?
expectations, and confirmations or contradictions of those expectations can affect their self-reported
confidence levels.
1719
Most relevant literature focuses on linguistic features. Language, as the primary form of human ex-
pression, is certainly critical. However, analyzing meaning may require going beyond linguistic infer-
ence, depending on the context or application. Previous studies have successfully incorporated multiple
expressive modalities when examining linguistic and cognitive processes, such as facial expressions for
video sentiment analysis (P?erez-Rosas et al., 2013) and pointing gestures for referring actions (Gatt and
Paggio, 2013). In such studies, the additional modalities were carefully chosen based on the nature of
the performed tasks. Here, we deal with experts (dermatologists) inspecting images (skin conditions) for
diagnostic purposes, a task that heavily involves their use of visual perceptual expertise, in addition to
conceptual domain knowledge. For this reason, we incorporate features of their eye movements in our
study. There is evidence for ties between perceptual expertise and eye movements during image inspec-
tion tasks (Li et al., 2012b), and we explore if such ties may also relate to a physician?s confidence and
diagnostic self-awareness.
Integrating different expressive modalities is challenging. Previous work involving multimodality has
predominantly treated each in isolation. We further address this challenge by identifying and exploring
truly multimodal features that focus on combining verbal and eye movement data into complex multi-
modal attributes, as it seems reasonable that the two modalities together could be more informative if
linked, and that such complex features represent a natural interactive extension of multimodal semantics.
Evidence for ties between speech and eye movements specifically was found by Li et al. (2012a), in
which sequences of fixations and saccadic eye movements were identified to predominantly align with
particular conceptual units of thought (e.g. primary lesion type) expressed verbally in medical narratives.
3 Data Description and Analysis
This study takes advantage of a dataset previously reported on by Womack et al. (2013), which is briefly
described here for clarity, as Womack et al.?s work ignored the eye movement data. A group of 29 derma-
tologists (11 attending physicians, 18 residents) were each shown a series of 30 images of dermatological
conditions in random order and asked to narrate their diagnosis of each condition. They were asked to
provide a description of the case, a list of differential diagnoses to consider, a final diagnosis, and their
certainty of their final diagnosis, as a percentage. The physicians? verbal descriptions were recorded as
audio and later manually transcribed in detail, including pauses, disfluencies, and other speech phenom-
ena.
2
During this process, the physicians? eye movements were also tracked. Each image was displayed
on a 22? LCD monitor (1650x1050 pixels) with an attached 250Hz SensoMotoric Instruments RED
remote eye-tracker while IViewX software was recording the eye movements.
In this study, the time-aligned pair of verbal description and eye movements for one physician viewing
one image is henceforth called a narrative. Figure 2a shows an example of a verbal description for
one narrative and Figure 2b shows a visualization of the corresponding eye movements. The correct
diagnoses for all images were known for the experiment and each narrative was assigned a binary label
of correct or incorrect.
3
For the purposes of this multimodal study, 238 of the 870 narratives were
excluded due to technical issues that had occurred with the eye tracking or audio capture equipment,
or because the physicians had provided no confidence values for their diagnoses. The remaining 632
narratives were used for the analysis and experimentation reported on in this paper.
3.1 Case Studies towards Understanding Physicians? Confidence and Correctness
The physicians tended to evaluate their confidence towards the upper end of the spectrum, with a me-
dian of 70% confident over all narratives. But diagnostic confidence may be affected by many factors,
including professional experience, case difficulty, and personality. We examine both individual images
and physicians at the extremes of confidence to gain insight into the relationship between confidence and
correctness in the dataset. Table 1 summarizes information for the three image cases that received the
2
Some transcription imperfections may occur.
3
A limited number of narratives in the dataset were labeled half correct if one of two final diagnoses given was correct, and
partially correct if the final diagnosis was too broad. Here, we consider half to be correct, because in such cases the correct
diagnosis was still identified, but partial to be incorrect, because the correct diagnosis was technically not identified.
1720
... um two ... pa- ... pink to purple
macules ... on the ... volar wrist
differential diagnosis ... um fixed
drug eruption ... bites ... urticaria
... uh ... diagnosis fixed drug erup-
tion percent certainty fifty percent
next ...
(a) Sample verbal description. Ellipses
(?...?) show pauses.
(b) Sample eye movement visualization. Circles represent fixations, where the
center is the point of fixation and the radius is proportional to the time fixating at
that point. Lines represent saccades (movements) between fixation points.
Figure 2: Sample verbal description and eye movements for one narrative. The final diagnosis is correct
and the physician was 50% confident.
Confidence Conf. % Correct Rank
Highest
100 100 2
90 100 5
90 100 1
Lowest
50 24 25
50 35 29
45 0 20
Table 1: Images receiving highest and lowest me-
dian confidence values. Difficulty ranking pro-
vided by a dermatology expert with 1 reflecting
the easiest image and 30 the most difficult.
Confidence Conf. % Correct Exp.
Highest
90 53 R
85 50 A
85 41 R
Lowest
38 39 A
30 48 R
15 37 R
Table 2: Most and least confident physicians by
median confidence values given over all images.
The last column shows experience level: experi-
enced attending (A) or resident (R) physician.
highest median confidence values and the three that received the lowest. A domain expert (dermatolo-
gist and clinical educator) who was not a subject in the experiment gave each image a unique difficulty
ranking from 1 to 30, where the image ranked number 1 was considered the easiest to support a correct
diagnosis, and 30 the most difficult. As expected, the highest confidence images were among the easiest,
and vice versa. Accordingly, the higher confidence images were correctly diagnosed by every physician,
while those receiving the lowest confidence were correctly diagnosed much less often. The negative
correlation between image difficulty and median physician confidence was significant using Spearman?s
rank correlation (r
s
= ?0.544, p < 0.005). In other words, higher levels of case difficulty were associ-
ated with lower levels of physician confidence. In contrast, examination of the most and least confident
physicians yields less intuitive results. The physicians with the highest and lowest median confidence
values are shown in the top and bottom halves of Table 2, respectively. Notably, each of the two groups
contained both resident dermatologists-in-training and attending physicians with careers spanning mul-
tiple decades. Also, the most confident physicians were only correct roughly half of the time, and the
least confident physicians? correctness appears quite similar. While this may reflect the sample size, the
observation is interesting nonetheless. Clearly, this points to how complicated diagnostic self-awareness
is, and how potentially useful it would be to computationally infer a physician?s self-awareness for diag-
nostic cases based on their behaviors.
1721
3.2 Confidence Binning
Nearly all confidence values given were multiples of five, or simply numbers close to 100, such as 99%.
4
This makes discretization preferable to using real-numbered values for confidence. Additionally, the
analyses in Section 3.1 revealed patterns of over- or underconfidence in individual physicians. What this
indicates is that ?high? and ?low? confidence involve different numerical values in the minds of different
physicians. This subjectivity could be problematic in doctor-patient interactions and it adds complexity
for predictive modeling involving confidence. To explore the impact, we devise two alternative binary
binning schemes: generalized bins, based on the performance of all physicians in the dataset, and per-
sonalized bins, based on each individual physician?s performance in the training data only. In terms of
application, consider a diagnostic support system which could establish a history for each physician who
uses it. Such a system could implement a generalized binning scheme and predictive model for new
users, and later, after learning from repeated exposure to a given physician, switch to a model based on
that physician?s individual performance. In addition, binning choice may be influenced by context: in a
clinical tutoring system, it may be preferable to compare learners to experienced physicians as a target
population. For the generalized binning scheme, a confidence value greater than or equal to the median
over all physicians is considered high, while a value below is considered low. This results in a slight
imbalance towards high confidence (56% of narratives).
5
We construct the personalized binning scheme
similarly, but using a given physician?s own median confidence in the training data as the dividing line.
In this case, high confidence accounts for 58% of the narratives, similar to that of the generalized bins.
Calling a physician?s median confidence high lets us better distinguish the problem cases: cases of under-
confidence should be strictly less than their ?typical? confidence, while cases of overconfidence should
be at or above typical. The binning scheme used does not affect the correctness value for each narrative,
but it does change the distribution of high and low confidence, with the generalized scheme favoring
over- and underconfidence, and the personalized scheme favoring appropriate confidence. Arguably, the
latter is a better reflection of the expected: over- and underconfidence as the minority classes.
4 Approach and Methodology
There are many ways to approach the problem of predicting physicians? diagnostic self-awareness. Here
we formulate two classification problems, each tested under both binning schemes, yielding a total of
four classification models. We also outline the performance evaluation experiments for the models.
4.1 Classification Problems
We define two classification problems based on the chart in Figure 1 (above). First, we define Confidence
Only, which ignores correctness (the horizontal dimension of Figure 1) and predicts only confidence as
a binary high or low. Intuitively, low confidence might be considered a warning sign for a diagnosis,
alerting a physician to seek additional insight or information.
6
This first problem was used as a stepping
stone to explore and better understand confidence, before incorporating correctness. Next, we define
Confidence & Correctness, which relates confidence with the correctness of the diagnosis (considering
all four quadrants in Figure 1, individually) to better address the more problematic, but interesting, cases.
Distinguishing these four classes could be of use to intelligent tutoring or clinical support systems, which
could respond differently to over- or underconfident users. In general, the full separation of these classes
could ultimately allow for deeper analysis of physician self-awareness.
4.2 Model Evaluation
Before any development took place, the 632 narratives were randomly divided into three subsets: 442
(70%) for training (dev-train), 95 (15%) for testing during development and tuning (dev-test), and 95
4
There were only a few exceptions: one physician gave three values of 3%, another gave a 33% and a 66% (rounded down
from ?two-thirds?), and a third gave a 33%. The latter three cases could also seem intuitive depending on how many conditions
were listed in the differential diagnosis. For example, 66% might indicate that one disease seemed twice as likely as a another.
5
Other simple binning schemes dividing up the 0-100% range were explored, but this binary version allowed for a more
systematic approach to both generalized and personalized binning, without sacrificing performance.
6
Normally, a physician would likely administer tests after the differential diagnosis, before reaching a final diagnosis.
1722
(15%) for final summative evaluation after all development was completed (heldout-test). All three
subsets have similar class distributions. Each of the four classification models were evaluated in two
ways: (1) by training the model on the union of the dev-train and dev-test sets and testing on the heldout-
test set, and (2) by running 50 randomized iterations of 10-fold cross-validation on the entire collection
of 632 narratives. The first evaluation experiment addresses the problem of overfitting by excluding the
heldout-test set from all development, while the second addresses the problem of sampling bias in the
initial set divisions. The results are described in Section 5.2.
5 Models and Results
Here we describe the development and performance of each of the four computational models outlined
in Section 4. We report on logistic regression, which had the best performance in all metrics for all
experiments, after dimensionality reduction (see Section 5.1). The feature selection and modeling was
implemented in Python with the scikit-learn machine learning library (Pedregosa et al., 2011).
5.1 Feature Extraction and Selection
A total of 60 features were examined (see Table 3). The features represented three modalities, moti-
vated by the task the physicians performed and knowledge about dimensions of clinical expertise in this
domain: verbal, composed of lexical, prosodic, and structural features of the narratives; eye movement,
consisting of features of fixations and saccadic eye movements; and truly multimodal features, consisting
of overlapping or simultaneously occurring features from the other two modalities, to reflect integrated
multimodal semantics. Continuing with the theme of personalization, we also created a fourth category
of personal features, with demographics of the physician and statistics about their confidence and cor-
rectness in the training data, in order to model their ?past? performance. The latter simulates how a
system could learn from experience with a particular physician.
As discussed in Section 2, verbal features of confidence have been studied before, and many of the
verbal features used here are inspired by previous work. Some verbal features are based on word choice,
such as amplifiers (e.g. definitely, sure) and modals (e.g. could, might),
7
while other have to do with
silences (or pauses) or prosody. The eye movement and multimodal features are mostly concerned with
fixations, as it seems intuitive that fixation may be associated with thoughtfulness about a particular area
of the image, which may in turn reflect a physician?s confidence.
Initial feature selection was performed on the development data (dev-train and dev-test) using
scikit-learn?s random forest ensemble classifier. This allowed for human-friendly inspection of
useful features. Random forests (Breiman, 2001) are an ensemble method in which numerous decision
trees are constructed, each trained on a randomized subset of the development data, which allows for the
utility of features to be evaluated on many sub-distributions of the data. The importance of a feature can
then be approximated as the sum of the error reduction at each node that splits on that feature, weighted
by the population size at that node. This reflects the fact that features used near the root of the tree often
handle a larger number of individuals. The importance values for all features will sum to 1. We consider
any feature that appeared in the top 20 of the ranked features for any model to be important, and all such
types of features are marked in bold in Table 3. Interestingly, the useful features for all classification
models were almost the same, with a few transpositions in the ordering. The exception was past confi-
dence, which was useful under generalized, but disappeared under personalized, as expected, since the
personalized scheme effectively normalizes each physician?s confidence values.
Interpreting the results for the verbal features, silence duration (statistics about the durations of all
silences) and the duration of narrative were most useful. Intuitively, this may relate to thoughtfulness or
contemplation. Additionally, words per second, or speech rate, was also useful, again perhaps relating to
more careful or thorough inspection/diagnosis. As discussed earlier, ties between speech and confidence
have been well-studied, while eye movements are underreported. It seems intuitive that eye movement
7
Such word-choice features were mostly based on lexical lists, and some overlap may occur. The cutaneous
conditions feature contained multiword expressions. These could be improved by using resources such as UMLS
(http://www.nlm.nih.gov/research/umls/) or WordNet (http://wordnet.princeton.edu/).
1723
Verbal (29)
Duration of narrative
Number of silences
Silence duration (?, ?, ?)
Duration of initial silence
Number of filled pauses
Word type-token ratio
Words per second
Cutaneous conditions (n, %)
Pronouns 1st (n, %)
Pronouns 3rd (n, %)
Modals (n, %)
Amplifier words (n, %)
Speculative words (n, %)
Negations (n, %)
Pitch (m, M , ?)
Intensity (m, M , ?)
Eye movement (11)
Fixation duration (?, ?, ?) Number of fixations
Saccade duration (?, ?, ?) % image area fixated
Saccade amplitude (?, ?, ?)
Multimodal (14)
% of initial silence time fixating
% of total silent time fixating
% of total fixation time silent
Words per second during fixation
Pitch during fixations (?, range)
Intensity during fixations (?, range)
Pitch of filled pauses (m, M , ?)
Intensity of filled pauses (m, M , ?)
Personal (6)
Attending vs. Resident Past correctness
Years of experience
Past confidence (m, M , ?)
Table 3: Features examined for classification (60 total), grouped by modality. Symbols in parentheses
indicate statistics over all occurrences of a feature in a narrative: raw count (n), raw count divided by
the total number of words (%), sum (?), mean (?), standard deviation (?), min (m), max (M ), range
(range). Useful features are boldfaced. If a feature has multiple statistics, the useful ones are underlined.
features may be more related to correctness. For example, the most useful eye movement feature was
% image area fixated, computed using a grid overlaid onto the image. If more of the image was fixated
upon, then it may have contained more areas of interest, or more visual evidence may have been sought,
which may also be related to case difficulty. Similarly, features of saccade amplitude (the angle of a
saccadic eye movement) may reflect physicians feeling a need to explore additional visual evidence by
switching focus between distant areas in an image. It is not surprising that the useful individual features
from verbal and eye movement modalities were also useful when combined as multimodal features. In
particular, simultaneous silence and fixation were the most useful, which again might indicate contem-
plation and analytical cognitive processing. This suggests that expression of confidence and diagnostic
self-awareness is at least partially a multimodal phenomenon.
Although the random forest method could be used for dimensionality reduction, we instead use Princi-
ple Component Analysis (PCA) in evaluation below, as it gave better performance gains in development.
The purpose of the random forest method was to examine which verbal, eye movement, and multimodal
features were most informative for classification, as we are interested in understanding how these modal-
ities relate to confidence and correctness. The latent features resulting from PCA are linear combinations
of the features, and thus would not allow for such inspection. The number of PCA components was
optimized for classification accuracy in cross-validation for each of the four classification models. Each
problem had a different number of principal components, indicating that both the binning scheme and the
classification problem type affected which features were identified as more collectively discriminative
by PCA.
5.2 Results and Evaluation
Heldout narratives We addressed the problem of overfitting by withholding 15% (n = 95) of the
narratives as an unseen final evaluation set. All predictive models performed well above their respective
majority class baselines (see Table 4). The Confidence Only models were able to reach higher accuracy,
precision, and recall than the joint Confidence & Correctness models. The exception is the accuracy
relative to baseline for personalized Confidence Only, which may be due to its higher baseline. As men-
tioned in Section 3.2, the generalized binning scheme is biased towards over- and underconfidence, and
the personalized towards appropriate confidence. The per-class metrics (not shown here) reflect this fact,
with overconfidence having higher precision and recall under generalized binning than under personal-
ized. Additionally, under the personalized scheme underconfidence is particularly underrepresented and
thus more difficult to predict.
1724
Binning Problem N Majority Class % BL % Acc. P R
Generalized
Conf. Only 2 High Confidence 53 76 (+23) 0.76 0.76
Conf. & Corr. 4 Overconfidence 37 53 (+16) 0.42 0.42
Personalized
Conf. Only 2 High Confidence 65 77 (+12) 0.75 0.73
Conf. & Corr. 4 Appropriate High 37 53 (+16) 0.38 0.42
Table 4: Performance metrics for the heldout-test set under each binning scheme with logistic regression
and PCA. All four models performed well above the majority class baselines (% BL) of their respective
problems (each with N many class labels). Precision (P) and recall (R) are each macro-averaged.
Random cross-validation A potential drawback of the initial development strategy used here is that
the initial random splits may bias classification models. To address this problem, after the heldout testing,
50 randomized iterations of 10-fold cross-validation were performed on the total collection of narratives,
the results of which are in Table 5. The personalized binning scheme was designed to mimic a sys-
tem that could adapt to a physician?s performance history, and thus the statistics used for personalized
confidence binning were recomputed on the training data within each individual cross-validation fold.
It is therefore not possible to establish a baseline for the personalized confidence binning outside of
a given fold. Instead, we take the mean of the percent accuracy above baseline from each test fold
(
1
k
?
k
i=1
(accuracy
i
? baseline
i
)). All models performed well above their respective baselines, which
is in line with observations from heldout testing.
Binning Generalized Personalized
Problem C.O. C&C C.O. C&C
Acc. above
baseline
+14 +9 +13 +12
Precision 0.70 0.25 0.69 0.32
Recall 0.70 0.38 0.57 0.37
Table 5: Performance metrics for logistic re-
gression with 50 randomized iterations of cross-
validation using all narratives for Confidence Only
(C.O.) and Confidence & Correctness (C&C). We
average the accuracy above baseline from each in-
dividual fold. Precision and recall are each macro-
averaged for each problem.
Feature Generalized Personalized
modality C.O. C&C C.O. C&C
V +13 +9 +12 +11
E +7 +6 +11 +10
MM +7 +4 +6 +5
V+E +13 +9 +13 +11
V+MM +14 +8 +11 +11
E+MM +10 +6 +13 +11
V+E+MM +14 +9 +13 +12
Table 6: Modality study with cross-validation for
Verbal (V), Eye movement (E), and Multimodal
(MM) features, measured in accuracy above re-
spective baselines, averaged over all folds. Most
modality combinations equaled or slightly im-
proved on constituent modalities in isolation.
Modality study We also performed a study within the cross-validation testing to investigate the impact
of different feature modality combinations on classification (see Table 6). Importantly, the verbal modal-
ity alone was more powerful than the eye movement or multimodal features, but most combinations of
modalities resulted in slightly higher or equal accuracy compared to their isolated constituent modali-
ties. This suggests that, as we projected, considering multiple modalities of a physician?s behavior can
help reveal their confidence and self-awareness, but also that verbal features are the most informative,
likely since verbal expression is the primary means to tap into physicians? rich and tacit conceptual un-
derstanding of a diagnostic case. The multimodal features, which focused on combining verbal and eye
movement data, did not improve performance over baselines as much as the simple combination of the
individual verbal and eye movement features. One reason for this could be that a person?s speech and
eye movements are not perfectly temporally aligned (Vaidyanathan et al., 2012), and this asynchronous
relationship may affect the meaningfulness of our multimodal feature measurements. Additionally, these
eye movement features may be at a much finer spatial or temporal scale than the verbal features.
1725
6 Conclusions
This study examined a dataset of medical narratives consisting of verbal descriptions, eye movements,
and self-reported confidence values, and used it to model physicians? confidence in diagnosis, as well
as their diagnostic self-awareness. The Confidence Only problem involves the expression of confidence
based on clinicians? belief, but it is important to understand the relationship to clinicians? actual diag-
nostic performance. This distinction is key because, while predicting confidence alone is a stepping
stone, self-awareness is the ability to additionally align one?s confidence with unknown correctness,
which involves human intuitive and analytical reasoning (another topic of interest to the medical field,
see Hochberg et al. (2014)). Case studies of the most and least confident physicians revealed a com-
plex relationship between confidence and correctness, and highlighted the need for exploring clinical
self-awareness. We also defined a personalized binning scheme for physician confidence levels, taking
into account a physician?s past confidence when drawing the line between high and low confidence, and
compared this to a generalized binning scheme based on performance of all physicians. In tandem, these
approaches to confidence binning could be used by an intelligent diagnostic support system.
We incorporated previously unused eye movement information from this dataset, and introduced truly
multimodal features which directly combined physicians? verbal and eye movement behaviors. While
physicians? eye movement and multimodal features were not individually as powerful as verbal features,
combinations of the three groups mostly produced classification improvements that were slightly better
than, or at least as good as, their constituent feature groups in isolation. The best performance for the
majority of models was achieved by considering features from all three modalities. This suggests that
eye movements help convey confidence and diagnostic self-awareness. The multimodal features did not
help as much, which we believe is explained by the more flexible temporal relationship between speech
and eye movements in the human mind. We leave the multimodal alignment challenge to future work.
Some pitch features implemented without speaker-dependent analysis were useful for classification, but
future work may benefit from pitch feature representations that adapt to demographic variation. Another
area for future work beyond the scope of this study includes examining alternative ways of combining
confidence and correctness classes, such as merging the diagonals of Figure 1 into a binary classifica-
tion of appropriate vs. inappropriate (i.e. the union of over- and underconfidence). Such alternatives
may present additional challenges for classification, but could also provide benefits for simpler clinical
support applications that may not be concerned with differentiating all four classes.
Acknowledgements
This work was supported by a seed award, and its dissemination partially by a Kodak Endowed Chair
award, both from the Golisano College of Computing and Information Sciences at RIT. The original data
collection was supported by NIH grant 1 R21 LM01003901A1. The content is solely the responsibility
of the authors and does not necessarily represent the official views of the National Institutes of Health.
The authors also thank Rui Li, and appreciate the helpful comments from reviewers.
References
Eta S. Berner and Mark L. Graber. 2008. Overconfidence as a cause of diagnostic error in medicine. The American
Journal of Medicine, 121(5A):S2?S23.
Leo Breiman. 2001. Random forests. Machine Learning, 45:5?32.
Pat Croskerry. 2003. The importance of cognitive errors in diagnosis and strategies to minimize them. Academic
Medicine, 78(8):775?780, August.
Pat Croskerry. 2008. Overconfidence in clinical decision making. The American Journal of Medicine,
121(5A):S24?S29.
Pat Croskerry. 2009. A universal model of diagnostic reasoning. Academic Medicine, 84(8):1022?1028, August.
Charles P. Friedman, Guido G. Gatti, Timothy M. Franz, Gwendolyn C. Murphy, Frederic M. Wolf, Paul S. Heck-
erling, Paul L. Fine, Thomas M. Miller, and Arthur S. Elstein. 2005. Do physicians know when their diagnoses
are correct? Journal of General Internal medicine, 20:334?339, April.
1726
Albert Gatt and Patrizia Paggio. 2013. What and where: An empirical investigation of pointing gestures and de-
scriptions in multimodal referring actions. In Proceedings of the 14th European Workshop on Natural Language
Generation, pages 82?91, Sofia, Bulgaria, August 8-9.
Mark Graber, Ruthanna Gordon, and Nancy Franklin. 2002. Reducing diagnostic errors in medicine: What?s the
goal? Academic Medicine, 77(10):981?992, October.
Mark L. Graber, Nancy Franklin, and Ruthanna Gordon. 2005. Diagnostic error in internal medicine. Archives of
Internal Medicine, 165:1493?1499, July 11.
Limor Hochberg, Cecilia Ovesdotter Alm, Esa M. Rantanen, Caroline M. DeLong, and Anne Haake. 2014. Deci-
sion style in a clinical reasoning corpus. BioNLP 2014.
Jay Katz. 1984. Why doctors don?t disclose uncertainty. Hastings Center Report, 14:35?44.
Charles E. Kimble and Steven D. Seidel. 1991. Vocal signs of confidence. Journal of Nonverbal Behavior,
15:99?105.
Rui Li, Jeff Pelz, Pengcheng Shi, Cecilia Ovesdotter Alm, and Anne Haake. 2012a. Learning eye movement
patterns for characterization of perceptual expertise. In ETRA 2012 Proceedings of the Symposium on Eye
Tracking Research and Applications, pages 393?396, Santa Barbara, CA, March 28-30.
Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake. 2012b. Learning image-derived eye movement patterns for
characterization of perceptual expertise. In Proceedings of CogSci 2012, pages 1900?1905.
Jackson Liscombe, Julia Hirschberg, and Jennifer J. Venditti. 2005. Detecting certainness in spoken tutorial
dialogues. In Proceedings of Interspeech 2005, pages 1837?1840, Lisbon, Portugal.
Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne Haake. 2012. Link-
ing uncertainty in physicians? narratives to diagnostic correctness. In Proceedings of the ACL-2012 Workshop
on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), pages 19?27, Jeju,
Republic of Korea, 13 July.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
Ver?onica P?erez-Rosas, Rada Mihalcea, and Louis-Phillippe Morency. 2013. Utterance-level multimodal sentiment
analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages
973?982, Sofia, Bulgaria, August 4-9.
Heather Pon-Barry and Stuart M. Shieber. 2011. Recognizing uncertainty in speech. EURASIP Journal on
Advances in Signal Processing, 2011(251753).
Erika Rogers. 1996. A study of visual reasoning in medical diagnosis. In Proceedings of the Eighteenth Annual
Conference of the Cognitive Science Society, pages 213?218, La Jolla, California, 12-15 July.
Klaus R. Scherer, Harvey London, and Jared J. Wolf. 1973. The voice of confidence: Paralinguistic cues and
audience evaluation. Journal of Research in Personality, 7:31?44, June.
Vicki L. Smith and Herbert H. Clark. 1993. On the course of answering questions. Journal of Memory and
Language, 32(1):25?38.
Preethi Vaidyanathan, Jeff Pelz, Wilson McCoy, Cara Calvelli, Cecilia Ovesdotter Alm, Pengcheng Shi, and Anne
Haake. 2012. Visualinguistic approach to medical image understanding. In Proceedings of the AMIA 2012
Annual Symposium, Chicago, Illinois, November.
Kathryn Womack, Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne
Haake. 2012. Disfluencies as extra-propositional indicators of cognitive processing. In Proceedings of the
ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 1?9, Jeju, Republic of Korea, 13 July.
Kathryn Womack, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne Haake. 2013.
Markers of confidence and correctness in spoken medical narratives. In Proceedings of Interspeech 2013, pages
2549?2553, Lyon, France, August 25-29.
1727
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 107?112,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Subjective Natural Language Problems:
Motivations, Applications, Characterizations, and Implications
Cecilia Ovesdotter Alm
Department of English
College of Liberal Arts
Rochester Institute of Technology
coagla@rit.edu
Abstract
This opinion paper discusses subjective natu-
ral language problems in terms of their mo-
tivations, applications, characterizations, and
implications. It argues that such problems de-
serve increased attention because of their po-
tential to challenge the status of theoretical
understanding, problem-solving methods, and
evaluation techniques in computational lin-
guistics. The author supports a more holis-
tic approach to such problems; a view that
extends beyond opinion mining or sentiment
analysis.
1 Introduction
Interest in subjective meaning and individual, inter-
personal or social, poetic/creative, and affective di-
mensions of language is not new to linguistics or
computational approaches to language. Language
analysts, including computational linguists, have
long acknowledged the importance of such topics
(Bu?hler, 1934; Lyons, 1977; Jakobson, 1996; Halli-
day, 1996; Wiebe et al 2004; Wilson et al 2005). In
computational linguistics and natural language pro-
cessing (NLP), current efforts on subjective natural
language problems are concentrated on the vibrant
field of opinion mining and sentiment analysis (Liu,
2010; Ta?ckstro?m, 2009), and ACL-HLT 2011 lists
Sentiment Analysis, Opinion Mining and Text Clas-
sification as a subject area. The terms subjectivity or
subjectivity analysis are also established in the NLP
literature to cover these topics of growing inquiry.
The purpose of this opinion paper is not to pro-
vide a survey of subjective natural language prob-
lems. Rather, it intends to launch discussions about
how subjective natural language problems have a vi-
tal role to play in computational linguistics and in
shaping fundamental questions in the field for the
future. An additional point of departure is that a
continuing focus on primarily the fundamental dis-
tinction of facts vs. opinions (implicitly, denotative
vs. connotative meaning) is, alas, somewhat limit-
ing. An expanded scope of problem types will bene-
fit our understanding of subjective language and ap-
proaches to tackling this family of problems.
It is definitely reasonable to assume that problems
involving subjective perception, meaning, and lan-
guage behaviors will diversify and earn increased at-
tention from computational approaches to language.
Banea et alalready noted: ?We have seen a surge
in interest towards the application of automatic tools
and techniques for the extraction of opinions, emo-
tions, and sentiments in text (subjectivity)? (p. 127)
(Banea et al 2008). Therefore, it is timely and use-
ful to examine subjective natural language problems
from different angles. The following account is an
attempt in this direction. The first angle that the pa-
per comments upon is what motivates investigatory
efforts into such problems. Next, the paper clarifies
what subjective natural language processing prob-
lems are by providing a few illustrative examples of
some relevant problem-solving and application ar-
eas. This is followed by discussing yet another an-
gle of this family of problems, namely what some
of their characteristics are. Finally, potential im-
plications for the field of computational linguistics
at large are addressed, with the hope that this short
piece will spawn continued discussion.
107
2 Motivations
The types of problems under discussion here are
fundamental language tasks, processes, and phe-
nomena that mirror and play important roles in peo-
ple?s daily social, interactional, or affective lives.
Subjective natural language processing problems
represent exciting frontier areas that directly re-
late to advances in artificial natural language be-
havior, improved intelligent access to information,
and more agreeable and comfortable language-based
human-computer interaction. As just one example,
interactional systems continue to suffer from a bias
toward ?neutral?, unexpressive (and thus commu-
nicatively cumbersome) language.
From a practical, application-oriented point of
view, dedicating more resources and efforts to sub-
jective natural language problems is a natural step,
given the wealth of available written, spoken or mul-
timodal texts and information associated with cre-
ativity, socializing, and subtle interpretation. From
a conceptual and methodological perspective, auto-
matic subjective text analysis approaches have po-
tential to challenge the state of theoretical under-
standing, problem-solving methods, and evaluation
techniques. The discussion will return to this point
in section 5.
3 Applications
Subjective natural language problems extend well
beyond sentiment and opinion analysis. They in-
volve a myriad of topics?from linguistic creativity
via inference-based forecasting to generation of so-
cial and affective language use. For the sake of illus-
tration, four such cases are presented below (bearing
in mind that the list is open-ended).
3.1 Case 1: Modeling affect in language
A range of affective computing applications apply
to language (Picard, 1997). One such area is au-
tomatically inferring affect in text. Work on auto-
matic affect inference from language data has gener-
ally involved recognition or generation models that
contrast a range of affective states either along af-
fect categories (e.g. angry, happy, surprised, neu-
tral, etc.) or dimensions (e.g. arousal and pleasant-
ness). As one example, Alm developed an affect
dataset and explored automatic prediction of affect
in text at the sentence level that accounted for differ-
ent levels of affective granularity (Alm, 2008; Alm,
2009; Alm, 2010). There are other examples of the
strong interest in affective NLP or affective interfac-
ing (Liu et al 2003; Holzman and Pottenger, 2003;
Francisco and Gerva?s, 2006; Kalra and Karahalios,
2005; Ge?ne?reux and Evans, 2006; Mihalcea and Liu,
2006). Affective semantics is difficult for many au-
tomatic techniques to capture because rather than
simple text-derived ?surface? features, it requires so-
phisticated, ?deep? natural language understanding
that draws on subjective human knowledge, inter-
pretation, and experience. At the same time, ap-
proaches that accumulate knowledge bases face is-
sues such as the artificiality and limitations of trying
to enumerate rather than perceive and experience hu-
man understanding.
3.2 Case 2: Image sense discrimination
Image sense discrimination refers to the problem of
determining which images belong together (or not)
(Loeff et al 2006; Forsyth et al 2009). What counts
as the sense of an image adds subjective complex-
ity. For instance, images capture ?both word and
iconographic sense distinctions ... CRANE can re-
fer to, e.g. a MACHINE or a BIRD; iconographic
distinctions could additionally include birds stand-
ing, vs. in a marsh land, or flying, i.e. sense distinc-
tions encoded by further descriptive modication in
text.? (p. 547) (Loeff et al 2006). In other words,
images can evoke a range of subtle, subjective mean-
ing phenomena. Challenges for annotating images
according to lexical meaning (and the use of verifi-
cation as one way to assess annotation quality) have
been discussed in depth, cf. (Alm et al 2006).
3.3 Case 3: Multilingual communication
The world is multilingual and so are many human
language technology users. Multilingual applica-
tions have strong potential to grow. Arguably, future
generations of users will increasingly demand tools
capable of effective multilingual tasking, communi-
cation and inference-making (besides expecting ad-
justments to non-native and cross-linguistic behav-
iors). The challenges of code-mixing include dy-
namically adapting sociolinguistic forms and func-
tions, and they involve both flexible, subjective
sense-making and perspective-taking.
108
3.4 Case 4: Individualized iCALL
A challenging problem area of general interest
is language learning. State-of-the-art intelligent
computer-assisted language learning (iCALL) ap-
proaches generally bundle language learners into a
homogeneous group. However, learners are individ-
uals exhibiting a vast range of various kinds of dif-
ferences. The subjective aspects here are at another
level than meaning. Language learners apply per-
sonalized strategies to acquisition, and they have a
myriad of individual communicative needs, motiva-
tions, backgrounds, and learning goals. A frame-
work that recognizes subjectivity in iCALL might
exploit such differences to create tailored acquisition
flows that address learning curves and proficiency
enhancement in an individualized manner. Counter-
ing boredom can be an additional positive side-effect
of such approaches.
4 Characterizations
It must be acknowledged that a problem such as
inferring affective meaning from text is a substan-
tially different kind of ?beast? compared to predict-
ing, for example, part-of-speech tags.1 Identifying
such problems and tackling their solutions is also
becoming increasingly desirable with the boom of
personalized, user-generated contents. It is a use-
ful intellectual exercise to consider what the gen-
eral characteristics of this family of problems are.
This initial discussion is likely not complete; that is
also not the scope of this piece. The following list is
rather intended as a set of departure points to spark
discussion.
? Non-traditional intersubjectivity Subjective
natural language processing problems are gen-
erally problems of meaning or communication
where so-called intersubjective agreement does
not apply in the same way as in traditional
tasks.
? Theory gaps A particular challenge is that sub-
jective language phenomena are often less un-
derstood by current theory. As an example, in
the affective sciences there is a vibrant debate?
indeed a controversy?on how to model or even
define a concept such as emotion.
1No offense intended to POS tagger developers.
? Variation in human behavior Humans often
vary in their assessments of these language be-
haviors. The variability could reflect, for exam-
ple, individual preferences and perceptual dif-
ferences, and that humans adapt, readjust, or
change their mind according to situation de-
tails. Humans (e.g. dataset annotators) may
be sensitive to sensory demands, cognitive fa-
tigue, and external factors that affect judge-
ments made at a particular place and point in
time. Arguably, this behavioral variation is part
of the given subjective language problem.
? Absence of real ?ground truth?? For such
problems, acceptability may be a more useful
concept than ?right? and ?wrong?. A partic-
ular solution may be acceptable/unacceptable
rather than accurate/erroneous, and there may
be more than one acceptable solution. (Rec-
ognizing this does not exclude that acceptabil-
ity may in clear, prototypical cases converge
on just one solution, but this scenario may not
apply to a majority of instances.) This central
characteristic is, conceptually, at odds with in-
terannotator agreement ?targets? and standard
performance measures, potentially creating an
abstraction gap to be filled. If we recog-
nize that (ground) truth is, under some circum-
stances, a less useful concept?a problem reduc-
tion and simplification that is undesirable be-
cause it does not reflect the behavior of lan-
guage users?how should evaluation then be ap-
proached with rigor?
? Social/interpersonal focus Many problems in
this family concern inference (or generation)
of complex, subtle dimensions of meaning and
information, informed by experience or socio-
culturally influenced language use in real-
situation contexts (including human-computer
interaction). They tend to tie into sociolin-
guistic and interactional insights on language
(Mesthrie et al 2009).
? Multimodality and interdisciplinarity Many
of these problems have an interactive and hu-
manistic basis. Multimodal inference is ar-
guably also of importance. For example, writ-
ten web texts are accompanied by visual mat-
109
ter (?texts?), such as images, videos, and text
aesthetics (font choices, etc.). As another ex-
ample, speech is accompanied by biophysical
cues, visible gestures, and other perceivable in-
dicators.
It must be recognized that, as one would expect,
one cannot ?neatly? separate out problems of this
type, but core characteristics such as non-traditional
intersubjectivity, variation in human behavior, and
recognition of absence of real ?ground truth? may be
quite useful to understand and appropriately model
problems, methods, and evaluation techniques.
5 Implications
The cases discussed above in section 3 are just se-
lections from the broad range of topics involving
aspects of subjectivity, but at least they provide
glimpses at what can be done in this area. The list
could be expanded to problems intersecting with the
digital humanities, healthcare, economics or finance,
and political science, but such discussions go be-
yond the scope of this paper. Instead the last item on
this agenda concerns the broader, disciplinary im-
plications that subjective natural language problems
raise.
? Evaluation If the concept of ?ground truth?
needs to be reassessed for subjective natural
language processing tasks, different and al-
ternative evaluation techniques deserve care-
ful thought. This requires openness to alterna-
tive assessment metrics (beyond precision, re-
call, etc.) that fit the problem type. For ex-
ample, evaluating user interaction and satis-
faction, as Liu et al(2003) did for an affec-
tive email client, may be relevant. Similarly,
analysis of acceptability (e.g. via user or anno-
tation verification) can be informative. MOS
testing for speech and visual systems has such
flavors. Measuring pejoration and ameliora-
tion effects on other NLP tasks for which stan-
dard benchmarks exist is another such route.
In some contexts, other measures of quality
of life improvements may help complement
(or, if appropriate, substitute) standard evalua-
tion metrics. These may include ergonomics,
personal contentment, cognitive and physical
load (e.g. counting task steps or load bro-
ken down into units), safety increase and non-
invasiveness (e.g. attention upgrade when per-
forming a complex task), or. Combining stan-
dard metrics of system performance with alter-
native assessment methods may provide espe-
cially valuable holistic evaluation information.
? Dataset annotation Studies of human annota-
tions generally report on interannotator agree-
ment, and many annotation schemes and ef-
forts seek to reduce variability. That may
not be appropriate (Zaenen, 2006), consid-
ering these kinds of problems (Alm, 2010).
Rather, it makes sense to take advantage of
corpus annotation as a resource, beyond com-
putational work, for investigation into actual
language behaviors associated with the set of
problems dealt with in this paper (e.g. vari-
ability vs. trends and language?culture?domain
dependence vs. independence). For exam-
ple, label-internal divergence and intraannota-
tor variation may provide useful understand-
ing of the language phenomenon at stake; sur-
veys, video recordings, think-alouds, or inter-
views may give additional insights on human
(annotator) behavior. The genetic computation
community has theorized concepts such as user
fatigue and devised robust algorithms that in-
tegrate interactional, human input in effective
ways (Llora` et al 2005; Llora` et al 2005).
Such insights can be exploited. Reporting on
sociolinguistic information in datasets can be
useful properties for many problems, assuming
that it is feasible and ethical for a given context.
? Analysis of ethical risks and gains Overall,
how language and technology coalesce in so-
ciety is rarely covered; but see Sproat (2010)
for an important exception. More specifically,
whereas ethics has been discussed within the
field of affective computing (Picard, 1997),
how ethics applies to language technologies re-
mains an unexplored area. Ethical interroga-
tions (and guidelines) are especially important
as language technologies continue to be refined
and migrate to new domains. Potential prob-
lematic implications of language technologies?
110
or how disciplinary contributions affect the lin-
guistic world?have rarely been a point of dis-
cussion. However, there are exceptions. For
example, there are convincing arguments for
gains that will result from an increased engage-
ment with topics related to endangered lan-
guages and language documentation in compu-
tational linguistics (Bird, 2009), see also Ab-
ney and Bird (2010). By implication, such ef-
forts may contribute to linguistic and cultural
sustainability.
? Interdisciplinary mixing Given that many
subjective natural language problem have a hu-
manistic and interpersonal basis, it seems par-
ticularly pivotal with investigatory ?mixing? ef-
forts that reach outside the computational lin-
guistics community in multidisciplinary net-
works. As an example, to improve assess-
ment of subjective natural language process-
ing tasks, lessons can be learned from the
human-computer interaction and social com-
puting communities, as well as from the digi-
tal humanities. In addition, attention to multi-
modality will benefit increased interaction as it
demands vision or tactile specialists, etc.2
? Intellectual flexibility Engaging with prob-
lems that challenge black and white, right vs.
wrong answers, or even tractable solutions,
present opportunities for intellectual growth.
These problems can constitute an opportunity
for training new generations to face challenges.
6 Conclusion
To conclude: there is a strong potential?or, as this
paper argues, a necessity?to expand the scope of
computational linguistic research into subjectivity.
It is important to recognize that there is a broad fam-
ily of relevant subjective natural language problems
with theoretical and practical, real-world anchoring.
The paper has also pointed out that there are certain
aspects that deserve special attention. For instance,
there are evaluation concepts in computational lin-
guistics that, at least to some degree, detract atten-
2When thinking along multimodal lines, we might stand a
chance at getting better at creating core models that apply suc-
cessfully also to signed languages.
tion away from how subjective perception and pro-
duction phenomena actually manifest themselves in
natural language. In encouraging a focus on efforts
to achieve ?high-performing? systems (as measured
along traditional lines), there is risk involved?the
sacrificing of opportunities for fundamental insights
that may lead to a more thorough understanding of
language uses and users. Such insights may in fact
decisively advance language science and artificial
natural language intelligence.
Acknowledgments
I would like to thank anonymous reviewers and col-
leagues for their helpful comments.
References
Abney, Steven and Steven Bird. 2010. The Human Lan-
guage Project: Building a Universal Corpus of the
worlds languages. Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 8897.
Alm, Cecilia Ovesdotter. 2009. Affect in Text and
Speech. VDM Verlag: Saarbrcken.
Alm, Cecilia Ovesdotter. 2010. Characteristics of high
agreement affect annotation in text. Proceedings of the
LAW IV workshop at the 48th Annual Meeting of the
Association for Computational Linguistics, Uppsala,
Sweden, 118-122.
Alm, Cecilia Ovesdotter. 2008. Affect Dataset. GNU
Public License.
Alm, Cecilia Ovesdotter and Xavier Llora?. 2006.
Evolving emotional prosody Proceedings of INTER-
SPEECH 2006 - ICSLP, Ninth International Confer-
ence on Spoken Language Processing, Pittsburgh, PA,
USA, 1826-1829.
Alm, Cecilia Ovesdotter, Nicolas Loeff, and David
Forsyth. 2006. Challenges for annotating images for
sense disambiguation. Proceedings of the Workshop
on Frontiers in Linguistically Annotated Corpora, at
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, Sydney, 1-4.
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity anal-
ysis using machine translation. Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, 127-135.
Bird, Steven. 2009. Last words: Natural language pro-
cessing and linguistic fieldwork. Journal of Computa-
tional Linguistics, 35 (3), 469-474.
111
Bu?hler, Karl. 1934. Sprachtheorie: Die Darstellungs-
funktion der Sprache. Stuttgart: Gustav Fischer Ver-
lag.
Forsyth, David, Tamana Berg, Cecilia Ovesdotter Alm,
Ali Farhadi, Julia Hockenmaier, Nicolas Loeff, and
Gang Wang. Words and pictures: categories, modi-
fiers, depiction, and iconography. In S. J. Dickinson,
et al(Eds.). Object Categorization: Computer and Hu-
man Vision Perspectives, 167-181. Cambridge: Cam-
bridge Univ. Press.
Francisco, Virginia and Pablo Gerva?s. 2006. Explor-
ing the compositionality of emotions in text: Word
emotions, sentence emotions and automated tagging.
AAAI-06 Workshop on Computational Aesthetics: Ar-
tificial Intelligence Approaches to Beauty and Happi-
ness.
Ge?ne?reux, Michel and Roger Evans. 2006. Distinguish-
ing affective states in weblog posts. AAAI Spring
Symposium on Computational Approaches to Analyz-
ing Weblogs, 40-42.
Halliday, Michael A. K. 1996. Linguistic function and
literary style: An inquiry into the language of William
Golding?s The Inheritors. Weber, Jean Jacques (ed).
The Stylistics Reader: From Roman Jakobson to the
Present. London: Arnold, 56-86.
Holzman, Lars E. and William Pottenger. 2003. Classifi-
cation of emotions in Internet chat: An application of
machine learning using speech phonemes. LU-CSE-
03-002, Lehigh University.
Jakobson, Roman. 1996. Closing statement: Linguistics
and poetics. Weber, Jean Jacques (ed). The Stylistics
Reader: From Roman Jakobson to the Present. Lon-
don: Arnold, 10-35.
Karla, Ankur and Karrie Karahalios. 2005. TextTone:
Expressing emotion through text. Interact 2005, 966-
969.
Liu, Bing. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, second
edition. Nitin Indurkhya and Fred J. Damerau (Eds.).
Boca Raton: CRC Press, 627-666.
Liu, Hugo, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge International Conference on Intelligent
User Interfaces, 125-132.
Llora`, Xavier, Kumara Sastry, David E. Goldberg, Abhi-
manyu Gupta, and Lalitha Lakshmi. 2005. Combating
user fatigue in iGAs: Partial ordering, Support Vec-
tor Machines, and synthetic fitness Proceedings of the
Genetic and Evolutionary Computation Conference.
Llora`, Xavier, Francesc Al??as, Llu??s Formiga, Kumara
Sastry and David E. Goldberg. Evaluation consis-
tency in iGAs: User contradictions as cycles in partial-
ordering graphs IlliGAL TR No 2005022, University
of Illinois at Urbana-Champaign.
Loeff, Nicolas, Cecilia Ovesdotter Alm, and David
Forsyth. 2006. Discriminating image senses by clus-
tering with multimodal features. Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th ACL, Sydney, Australia, 547-554.
Lyons, John. 1977. Semantics volumes 1, 2. Cambridge:
Cambridge University Press.
Mesthrie, Rajend, Joan Swann, Ana Deumert, and
William Leap. 2009. Introducing Sociolinguistics,
2nd ed. Amsterdam: John Benjamins.
Mihalcea, Rada and Hugo Liu. 2006. A corpus-based ap-
proach to finding happiness. AAAI Spring Symposium
on Computational Approaches to Analyzing Weblogs,
139-144.
Picard, Rosalind W. 1997. Affective Computing. Cam-
bridge, Massachusetts: MIT Press.
Sproat, Richard. 2010. Language, Technology, and Soci-
ety. Oxford: Oxford University Press.
Ta?ckstro?m, Oscar. 2009. A literature survey of methods
for analysis of subjective language. SICS Technical
Report T2009:08, ISSN 1100-3154.
Wiebe, Janyce, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learning
subjective language. Journal of Computational Lin-
guistics 30 (3), 277-308.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffman.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. Proceedings of the Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing, 347-
354.
Zaenen, Annie. 2006. Mark-up barking up the wrong
tree. Journal of Computational Linguistics 32 (4),
577-580.
112
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 118?122,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Characteristics of high agreement affect annotation in text
Cecilia Ovesdotter Alm
Cornell University, USA
cissioalm@gmail.com
Abstract
The purpose of this paper is to present an
unusual English dataset for affect explo-
ration in text. It describes a corpus of fairy
tales from three sources that have been
annotated for affect at the sentence level.
Special attention is given to data marked
by high annotator agreement. A quali-
tative analysis of characteristics of high
agreement sentences from H. C. Ander-
sen reveals several interesting trends, illus-
trated by examples.
1 Introduction
Meaning is essential to language. The impor-
tance of expressive, attitudinal/emotive, or so-
cial/interpersonal meaning has been noted by
prominent linguists (Bu?hler, 1934; Lyons, 1977;
Jakobson, 1996; Halliday, 1996). However, affect
is still an understudied phenomenon in linguistics,
although many affective computing applications
actually apply to language (Picard, 1997).
The motivation behind this discussion is to
bring a special and rather unique dataset to the
attention of reseachers in the field of natural lan-
guage processing, affective computing, and re-
lated areas. This paper discusses affect represen-
tation, presents an affect dataset, and then focuses
on clear-cut cases of affective meaning and expres-
sion in text with a summary of an analysis of data
for which human annotators highly agreed on the
assignment of affect labels. For dataset results in
supervised classification (including experimenta-
tion on high agreement data), cf. Alm (2009).1
2 Affect representation
Affect can be modeled, e.g. as categories (Ek-
man, 1994), dimensions (Osgood, 1969), by fo-
1For details on this dataset and experimentation con-
ducted with it, readers should consult my book (Alm, 2009),
which exceeds this paper in scope and depth.
cus on appraisal (Ortony et al 1988), or on ex-
perience of physical and bodily responses (Cor-
nelius, 2000). There is a lack of consensus on a
model of affect (Picard, 1997; Scherer, 2003) and
controversy surrounds such modeling. Pragmati-
cally, different views of affect complement each
other and jointly create a basis for understanding
affective language phenomena. Affect modeling
decisions are arguably application dependent. For
a detailed literature review on previous work on
how to characterize affect, affect in text-based lin-
guistics and in subjective NLP or speech technol-
ogy, and tales and oral narratives, see Alm (2009).
Also see http://emotion-research.net/.
Resulting originally from an interest in text
analysis for child-directed expressive text-to-
speech synthesis, this dataset relies on a categor-
ical annotation scheme of basic emotions; a model
supported by the compelling observation that emo-
tive facial expressions were cross-culturally rec-
ognized well above chance (Ekman and Friesen,
1998). In vision and speech research ?the Big
Six? (Cornelius, 2000) (i.e. happiness, fear, anger,
surprise, disgust, and sadness) appear quite often.
Nevertheless, the Ekmanian view remains contro-
versial. For instance, Russel and Ferna?ndez-Dols
(1998) have critiqued the relevance, methods, and
rigor of the ?Facial Expression Program? for emo-
tion. One alternative is free labeling (i.e. anno-
tators may come up with their own labels), but
that may result in impractical, large label sets. A
study grouping items from open-ended responses
to a perception test on characterizing certain fairy
tale sentences noted that although other cases oc-
curred, Big Six emotions were frequent in answers
(Brale` et al 2005).
As regards the dataset?s use of affect cate-
gories, several empirical studies have shown above
chance performance for recognition of categorical
emotions in classification tasks involving prosody.
Categorical labels may be more straightforward
118
for annotators to conceptualize compared to di-
mensional scales, as participants pointed out in a
study (Francisco and Gervas, 2006). Also, cate-
gories are arguably suitable for pedagogy, and they
naturally fit computational classification. A basic
affect category is also broad enough to span re-
lated affect states, e.g. the emotion family (Ek-
man, 1994) of angry could also cover concepts
such as irritated, annoyed and enraged.2 Finally,
the foundational nature of basic, categorical af-
fects intuitively seems to fit a child-directed con-
text and fairy tales contents, which may include
certain canonical topics and behaviors, compared
to more spontaneous discourse.3
3 Corpus data overview
The affect dataset consists of 176 stories (more
than 15,000 sentences) by Beatrix Potter, the
Brothers Grimm and H. C. Andersen, manually
annotated at the sentence level by pairs of annota-
tors.4 For the annotation process, annotators read
tales and had to make a choice from a set of affect
categories for sentences. Each sentence was given
four affect labels since each of two annotators as-
signed both a primary emotion (guided by the pre-
cence of a feeler, mostly a character or character
type in the text) and a background mood to a sen-
tence. The four labels were then combined into a
sentence?s affect labels. For more details on the
annotation process, cf. (Alm, 2009). The label set
consisted of a set of categorical affect labels. Prior
to the analysis below, ANGRY and DISGUSTED
were merged (motivated by data sparsity and re-
lated semantics) into one category, as were POSI-
TIVELY and NEGATIVELY SURPRISED, yielding a
merged set of affect labels: ANGRY-DISGUSTED,
FEARFUL, HAPPY, NEUTRAL, SAD, SURPRISED.
Interannotator agreement can be an artifact of
annotation scheme and procedure. For exam-
ple, pairs might be trained to annotate similarly,
across-the-board rules (e.g. questions are nega-
tive) might ignore subtle decisions, or problem-
atic items might be removed. Such approaches
may yield higher agreement, cleaner data, and
perhaps better performance and more consistent
2Categories do not exclude adding intensity for approxi-
mating an arousal dimension, arguably relevant for speech.
3Naturally, tales also encompass narrative complexity.
4The annotated data are available at the author?s website
(both the full dataset and the high agreement subsets). For in-
stance, for the high agree affect data, a storyname is followed
by its corresponding high agree affective sentences in the fol-
lowing format: sentence-id-in-story@label-code@sentence.
Figure 1: (Dis)agreement: merged labels
trained applications. But, the relevance of that
for study of linguistic behavior is less clear. Za-
enen (2006) noted that ?[f]or interannotator agree-
ment, it suffices that all annotators do the same
thing. But even with full annotator agreement it
is not sure that the task captures what was origi-
nally intended? (577); this should not be confused
with understanding a linguistic issue. Fig. 1 re-
ports on a diagnostic alternative with the ratios of
(dis)agreement types. This avoids the concept of
ground truth, which may not hold for all language
phenomena. Affect, which is highly subjective, is
arguably better captured by flexible acceptability.5
Fig. 1 shows that sentences only labeled NEU-
TRAL were frequent, as were disagreements,
which were more common for sentences marked
both with NEUTRAL and one or more affect
classes. This parallels findings for polarity expres-
sions in subjective texts (Wilson et al 2005), and
shows that the border between affective and neu-
tral is fuzzy. (Affect perception lacks clear defini-
tions and is subjective, and neutrality suffers from
the same dilemma.) A sentence with high agree-
ment affect was defined as all four primary emo-
tion and mood labels having the same affective la-
bel (given the merged label set). These were more
common than mixed affective labels.
4 High agreement in H. C. Andersen
This section examines the subset of high agree-
ment sentences in the H. C. Andersen data from
a qualitative-interpretive perspective. The anal-
ysis is not intended as rigid categorization, but
rather to get an overall idea of why high agreement
might occur on affect labels across annotators.
Isolated sentences were extracted and mostly ex-
amined that way, rarely considering context. This
5Regular agreement scores for the corpus would be low.
119
Figure 2: Distribution of 460 H. C. Andersen high
agreement affective sentences across affect labels
focused the analytical scope.6 Five annotators en-
gaged with the overall H. C. Andersen subcor-
pus of 77 tales. 460 sentences were marked by
affective high agreement, given the five affective
classes. The distribution of affective classes for
this subset is in Fig. 2, with HAPPY and SAD being
most frequent.
4.1 Characteristics: high agreement affect
The below overview lists characteristics observed
in an analysis on the H. C. Andersen high agree-
ment data. It briefly describes each characteristic
and lets an example illustrate it. For more discus-
sion, examples, word lists etc., see Alm (2009).
The characteristics occur in some and not all sen-
tences; some frequently, others more rarely. Often,
several jointly characterize a sentence.
The illustrative sentence examples in this sec-
tion use the following format: Affect labels
are in small caps and sentences are in italics.
Also, phrases in bold-face illustrate the discussed
characteristic, whereas phrases that annotators
noted are underlined (single underscore for non-
overlapping vs. double underscore for overlap-
ping mark-up), and their feeler/s for the primary
emotion annotation is/are included (with annotator
subscripts to show if they had indicated the same
or not) in parenthesis in small caps.
4.1.1 Affect words
Content words that directly name an affective
state (e.g. reflecting a particular intensity) are
common in high agreement sentences, cf.:
6Annotators? noted feeler and emotional/connotative
phrases for the sentences were inspected.
ANGRY-DISGUSTED: They buzzed round
the prince and stung his face and hands;
angrily he drew his sword and brandished it, but
he only touched the air and did not hit the gnats.
(VILLAIN1,2)
That narration can directly announce affective
states is an indication of the important narrative
role affect can play in stories. Also, Wilson and
Wiebe (2003) interestingly noted that annotators
agreed more strongly with strong subjective ex-
pressions, which affect words are examples of.
Some illustrative affect words from the examined
data are (for SURPRISED): alarmed, astonished,
astonishment, shocked, shocking, startled, sur-
prised. Special cases include negation (e.g. not
happy for SAD); figurative/idiomatic phrases (e.g.
one of his heartstrings had broken for SAD); or ap-
pearance with more than one affect (e.g. anguish
for SAD or FEARFUL).
4.1.2 Words for related/contrastive affect
states
Expressions in the sentential context naming re-
lated or contrastive affective states not in the label
set (e.g. dull, pride, relief, or shame) may also help
evoke a particular affect, as in:
HAPPY: They looked at Little Claus ploughing
with his five horses, and he was so proud that he
smacked his whip, and said, ?Gee-up, my five
horses.? (HERO1,2)
4.1.3 Affect related words or expressions
Lexical items or phrases which describe actions,
properties, behaviors, cognitive states, or objects
associated with particular affects occur frequently
in the examined high agreement subset, e.g. as in:
HAPPY: They laughed and they wept; and Peter
embraced the old Fire-drum. (HERO1, (TRUE)
MOTHER2, (TRUE) FATHER2)
Some more prominent affect related lexical
items include weep, kiss, laugh, cry (= weep), and
forms of pleasure, tears, and smile. Expressions
of weeping or tears often appear with sadness, but
may also depict happiness. Negations may occur.
4.1.4 Polarity words and expressions
Words or expressions of positive or negative po-
larity can help to set the scene with a particular af-
fective mode, in particular with relation to context
and acquired knowledge. Expressions of opposing
polarity may be used as a contrast, as in:
HAPPY: It became a splendid flower-garden
120
to the sick boy, and his little treasure upon earth.
(SICK BOY1,2)
Modifiers can intensify the affective load. Lex-
ical words and phrases may have permanent vs.
occasional attitudinal meaning (Hedquist, 1978).
4.1.5 Knowledge and human experience
Readers may from experience associate aquired
knowledge about situations, visualizations, and
behaviors with particular affects. For example, it
is common knowledge that starving is traumatic:
SAD: He was hungry and thirsty, yet no one gave
him anything; and when it became dark, and they
were about to close the gardens, the porter turned
him out. (HERO1,2).
Story worlds tend to involve canonical represen-
tations of characters, actions, functions, situations
and objects. Surrounding context can be impor-
tant for affective interpretations. Scenarios may
include, e.g. an inspiration from weather, flow-
ers, nature, or God; singing (or dancing, jump-
ing); physical lack and need; sleep deprivation
or allowance; addiction; incapability; unexpected
observation; appearance/posture (or intonation);
contextual guidance; or relate to marriage (see
(Alm, 2009) for examples). In fact, arguably most
discussed characteristics can be traced to acquired
knowledge, experience, associations, or context.
4.1.6 Speech acts
Speech acts reflect a certain kind of communica-
tive knowledge that can have affective meaning
(such as cursing, insulting, commanding), e.g.:
ANGRY-DISGUSTED:
Let her be expelled from the congregation and the
Church. (VILLAIN1,2)
4.1.7 Types of direct speech
Direct speech may be used by characters in tales to
express affect. This might include speaking excit-
edly, (WH)-exclamations or (WH)-questions, short
utterances, interjections (and sound effects), such
as ah, alas, hurrah, o God, sorry, thump, ugh. Di-
rect speech can be introduced by words of speak-
ing, as in:
FEARFUL: ?Mercy!? cried Karen. (HEROINE1,2)
4.1.8 Mixed emotions
Affective high agreement sentences also include
cases of mixed emotions, e.g. affect or affect-
related words referring to more than one affect.
The ?winning? affect may be inferred. Contrast
might make it more prominent, as in:
HAPPY (mixed SAD): He now felt glad at
having suffered sorrow and trouble, because
it enabled him to enjoy so much better all the
pleasure and happiness around him; for the
great swans swam round the new-comer, and
stroked his neck with their beaks, as a welcome.
(MAIN CHARACTER/HERO1,2)
4.2 Tendencies of particular affect categories
Lastly, there may be trends for particular charac-
teristics associating more or less with a particular
affect. For example, in this subset, FEARFUL sen-
tences seem often to contain affect or affect related
words, whereas SURPRISED sentences may quite
often be characterized by various types of direct
speech or involve unexpected observations.
5 Conclusion
This paper brought attention to an affect dataset,
and discussed (mostly surface) characteristics in
its H. C. Andersen high agreement subset, il-
lustrating the complexity of affect cues, without
claiming an exhaustive analysis. It also tentatively
hypothesized that some characteristics may show
particular affinity with certain affects.
The high agreement sentence data may be par-
ticularly interesting for affect research, while other
parts of the annotated, larger corpus may reveal
insights on affect variation in text and perception
thereof (bearing in mind that the dataset is not
necessarily representative across domains and text
types, nor of contemporary texts).
Lastly, as noted above, developed knowledge,
experience, associations, and context appear very
important for affect understanding. This is also
a substantial part of what makes the problem of
automatically predicting affect from text so chal-
lenging; it involves levels of deep cognitive under-
standing rather than just extractable surface fea-
tures. Whereas the discussed characteristics nat-
urally do not consistute the answer to affect un-
derstanding, they may inform future search for it.
Deep understanding and continuous, as opposed
to static, computational development of affective
understanding remain crucial areas of future work
for expressive NLP applications.
Acknowledgments
Thanks to R. Sproat, R. Proan?o, and reviewers.
Project funded by NSF (award ITR-#0205731).
121
References
Alm, Cecilia Ovesdotter. 2009. Affect in Text and
Speech. VDM Verlag: Saarbrcken.
Brale`, Ve?ronique, Vale?rie Maffiolo, Ioannis Kanellos,
and Thierry Moudenc. 2005. Towards an expres-
sive typology in storytelling: A perceptive approach.
In Jianhua Tao, Tieniu Tan, and Rosalind W. Picard
(Eds.), Affective Computing and Intelligent Inter-
action, First International Conference, ACII 2005,
Beijing, China, October 22-24, 2005, Proceedings,
858-865.
Bu?hler, Karl. 1934. Sprachtheorie: Die Darstellungs-
funktion der Sprache. Stuttgart: Gustav Fischer Ver-
lag.
Cahn, Janet E. 1990. The generation of affect in syn-
thesized speech. Journal of the American Voice I/O
Society 8, 1-19.
Cornelius, Randolph R. 2000. Theoretical approaches
to emotion. In Proceedings of the ISCA Workshop on
Speech and Emotion, 3-10.
Ekman, Paul. 1994. All emotions are basic. In P. Ek-
man and R. J. Davidson (Eds.), The Nature of Emo-
tion: Fundamental Questions. Oxford: Oxford Uni-
versity Press, 15-19.
Ekman, Paul and Wallace V. Friesen. 1998 [1971]
Constants across culture in the face and emo-
tion. Jenkins, Jennifer M and Oatley, Keith and
Stein, Nancy L. (eds). Human Emotions: A Reader.
Malden, Massachussetts: Blackwell, 63-72.
Francisco, Virginia and Pablo Gerva?s 2006. Explor-
ing the compositionality of emotions in text: Word
emotions, sentence emotions and automated tag-
ging. In AAAI-06 Workshop on Computational Aes-
thetics: Artificial Intelligence Approaches to Beauty
and Happiness.
Halliday, Michael A. K. 1996. Linguistic function
and literary style: An inquiry into the language
of William Golding?s The Inheritors. Weber, Jean
Jacques (ed). The Stylistics Reader: From Roman
Jakobson to the Present. London: Arnold, 56-86.
Hedquist, Rolf. 1978. Emotivt spa?k: En studie i
dagstidningarnas ledare. Ph.D. Thesis. Umea?.
Jakobson, Roman. 1996. Closing statement: Lin-
guistics and poetics. Weber, Jean Jacques (ed). The
Stylistics Reader: From Roman Jakobson to the
Present. London: Arnold, 10-35.
Lyons, John. 1977. Semantics volumes 1, 2. Cam-
bridge: Cambridge University Press.
Ortony, Andrew, Gerlad L. Clore, and Allan Collins.
1988. The Cognitive Structure of Emotions. Cam-
bridge: Cambridge University Press.
Osgood, Charles E. 1969. On the whys and wherefores
of E, P, and A. Journal of Personality and Social
Psychology 12 (3), 194-199.
Picard, Rosalind W. 1997. Affective computing. Cam-
bridge, Massachusetts: MIT Press.
Russell, James A. and Jose? M. Ferna?ndez-Dols 1998
[1997]. What does a facial expression mean? Jenk-
ins, Jennifer M and Oatley, Keith and Stein, Nancy
L. (eds). Human Emotions: A Reader. Malden,
Massachussetts: Blackwell, 63-72.
Scherer, Klaus R. 2003. Vocal communication of emo-
tion: A review of research paradigms. Speech Com-
munication 40 (1-2), 227256.
Wilson, Theresa, Janyce Wiebe, and Paul Hoff-
man. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. Proceedings of
HLT/EMNLP, 347-354.
Wilson, Theresa and Janyce Wiebe. 2003. Annotating
opinions in the world press. 4th SigDial workshop
on Discourse and Dialogue.
Zaenen, Annie. 2006. Mark-up barking up the wrong
tree. Journal of Computational Linguistics 32 (4),
577-580.
122
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 9?18,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Detecting Distressed and Non-distressed Affect States  in Short Forum Texts 
Michael Thaul Lehrman  Cecilia Ovesdotter Alm  Rub?n A. Proa?o Rochester Institute of Technology michael.lehrman@alum.rit.edu  coagla@rit.edu  rpmeie@rit.edu  Abstract 
Improving mental wellness with preventive measures can help people at risk of experiencing mental health conditions such as depression or post-traumatic stress disorder. We describe an encouraging study on how automatic analysis of short written texts based on relevant linguistic text features can be used to identify whether the authors of such texts are experiencing distress. Such a computational model can be useful in developing an early warning system able to analyze writing samples for signs of mental distress. This could serve as a red flag, signaling when someone might need a professional assessment by a clinician. This paper reports on classification of distressed and non-distressed short, written excerpts from relevant web forums, using features automatically extracted from input text. Varying the value of k in k-fold cross-validation shows that both coarse-grained and fine-grained automatic classification of affect states are generally 20% more accurate in detecting affect state than randomly assigning a distress label to a text. The study also compares the importance of bundled linguistic super-factors with a 2k factorial model. Analyzing the importance of different linguistic features for this task indicates main effects of affect word list matches, pronouns, and parts of speech in the predictive model. Excerpt length contributed to interaction effects. 1 Introduction Many people today deal with depression, post-traumatic stress disorder, and other mental disorders involving anxiety or distress, both diagnosed and undiagnosed. The societal costs of treating mental health are staggering. Sultz and Young (2011) estimate that the total mental health care treatment costs in the United States amount to more than USD 100 billion per year. The health care system in the United States generally focuses 
on treating patients? illnesses rather than on preventing their occurrence, and mental health care is no exception. Mental health diagnosis typically takes place after patients already show behavioral and physical symptoms associated with mental distress. Moreover, there are 33,000 suicides every year in the United States and, according to Matykiewicz et al (2009; referencing Kung et al (2008)), ?[i]n the United States, suicide ranks second as the leading cause of death among 25-34 year-olds and the third leading cause of death among 15-25 year-olds? (p. 179). Diagnosing mental illnesses is difficult. For example, depression has a prevalence of 19.5%, according to Mitchell et al (2009), and is mostly diagnosed and treated by general practitioners. However, it is diagnosed correctly in only 47.3% of cases. Commonly, the initial assessment of mental distress does not rely on clinical tests or advanced technology, and the evaluation of a patient is typically performed through the use of standardized questionnaires. A patient's answers are then compiled and compared with disease classification guidelines, such as the International Classification of Diseases or the Diagnostic and Statistical Manual, to guide the patient?s diagnosis. However, these diagnostic methods are not precise and have high rates of false positives and false negatives. For example, in the United States, half of those who received mental health treatment did not meet the diagnostic criteria for a mental disorder (Kessler et al, 2005). In addition, societal and financial barriers prevent many people from seeking medical attention. In fact, in the USA, between 1990 and 2003, two-thirds of those with mental disorders did not receive treatment (Kessler et al, 2005). Many societies around the world stigmatize and discriminate against people with mental disorders, contributing to the unwillingness of individuals to acknowledge the problem and seek help (Michels et al, 2006; Fabrega, 1991). 
9
It would be helpful if, e.g., military clinicians could effectively and non-invasively analyze soldiers? writing samples, social media posts, or email correspondence to screen service members for trouble coping with combat-related stress, to complement self-reporting or patient surveys. Careful thought would be required for access to such information so that it helps and not hurts. It seems useful as additional information for doctors. We report on an initial study in which we analyze a smaller balanced dataset and experiment with inference of affect states at two different levels of affective granularity. Our work is based on Natural Language Processing (NLP) using supervised machine learning. We also discuss 2k factorial, a method commonly used in engineering statistics, which has been successfully applied to many domains within engineering and product design for feature selection. Our work contributes initial reference values for what can be achieved by applying four fundamental supervised classification methods and text-based features to the challenging task of automatically classifying mental affect states in short texts based on just a small dataset. We discuss performance both in terms of different experimental setups, which linguistic features matter, and how labels confuse with each other. 2 Relevant previous work Computational linguistics approaches have been applied to a range of challenging problems with impact outside the language technology field, e.g., to predict pricing movements on the stock market (Schumaker, 2010) or opinions on political candidates in event prediction markets (Lerman et al, 2008). In psychology, psychiatry, and criminology, studies with natural language data have found differences in behaviors for mental health patients or inmates with various mental health disorders (e.g., Andreasen and Pfohl, 1976; Harvey, 1983; Ragin and Oltmanns, 1983; Fraser et al, 1986; Endres, 2004; Gawda, 2010). Recently, computational linguists have increasingly tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. Jha and Elhadad (2010) predicted how far breast cancer patients had progressed in their disease, based on discourse available in postings 
on web forums. As another example, Roark et al (2007) explored the use of structural aspects of the language of individuals with mild cognitive impairment in assisting with such diagnostics. More specifically in mental health, Yu et al (2009) classified five forms of ?negative life events? in text (p. 202). Pestian et al (2008) were able to use machine learning, taking advantage of text characteristics to classify suicide notes as written by either ?simulators? or ?completers? as accurately as mental health experts (p. 96). The authors also found that emotional content was useful for the expert clinicians, but not for the automatic inference methods. However, this might indicate that the study did not consider an appropriate feature set. In comparison, Alm (2009) explored a more comprehensive feature set for automatic affect prediction in text. Matykiewicz et al (2009) discriminated between suicide notes and control texts using automatic clustering techniques, and discovered sub-clusters within suicide writings. In 2011, Pestian et al (2012) organized a challenge to determine emotions and meaningful information in notes by suicide completers. These latter investigatory efforts, while valuable, involved computationally analyzing suicide notes of individuals with advanced rather than earlier stages of mental distress. Our work links fundamental NLP classification methods with a standard engineering statistics method. Since the publication of ?Building a Better Delivery System: A New Engineering/Health Care Partnership? by the Institute of Medicine (IOM) and the National Academy of Engineering (NAE) in 2005, there has been increased attention to the potential of engineering to broadly improve U.S. health care delivery. The IOM-NAE report identifies the use of optimization techniques to support decision making as one of the most promising engineering tools and technologies that could help the health care system deliver ?safe, effective, timely, patient-centered, efficient, and equitable? care (Reid et al, 2005, p. 1). 3 Conceptual model We conceptualize the task of determining affect state as a classification problem. Formally, let t denote a text that expresses an affect state. Let k be the number of affect state classes C = {c1, c2, c3, ?, ck}, where ci denotes a specific class label. The goal is to decide a mapping function f : t ? ci to 
10
obtain an ordered labeled pair (t, ci). The mapping is based on Ft = {f1, f2, ?, fn}, describing n feature values, automatically extracted from the text t. The label hierarchy is shown below in Figure 1. The coarse-grained level represents a binary classification problem: distressed vs. non-distressed. At a more fine-grained level, we distinguish four classes (see section 4 below): high distress, low distress, response, and happy.  
 Figure 1. Class label hierarchy with two levels of granularity (binary vs. quaternary division of labels). 4 Dataset There is currently no readily available text dataset for this problem. For this initial study, we prepared a small, annotated dataset of short written texts that represented relevant distinct, yet related, affect states. We manually collected a convenience (i.e., non-random) sample consisting of 200 posts from various public online forums dealing with mental well-being. 1  Forum posts were chosen because they are similar to other short digital social media texts, such as e-mails, online community posts, blog entries, or brief reflective writing that could be quickly gathered during a clinical session. We considered the text in the posts but not their titles. 4.1 Data annotation Distressed and happy posts naturally divided into categories given the titles of the forums from which they were taken. Based on observation, we assumed that the distressed posts, all of which initiated new threads, were affectively distinct from responses to such threads, which had another polarity as they were meant to be reassuring and supportive. Therefore, we treated such responses as non-distressed posts. We recognize that a response represents a turn following an initial post. It is                                                 1 Excerpts were culled from forums that dealt with mental health states at BreastCancer.org and reddit.com. Manually inspecting data ensured that relevant texts were included, but we also acknowledge that data obtained by such a selection process might differ from data obtained by random selection. 
useful to explore how dialogic threading becomes part of affective language behaviors in social media (forums). The happy posts were included to represent the other extreme end of the affect spectrum.2 The dataset 3  was balanced such that 100 excerpts were distressed, 50 were non-distressed responses, and 50 were non-distressed happy. The distressed excerpts were then split further according to their distress intensity into high and low based on the annotator's perception, as seen in Figure 1. In an attempt to reduce personal bias, any post stating an active intent to harm someone or oneself was classified as high distress, while posts simply discussing bad feelings were usually classified as low distress. There were slightly more excerpts with low as opposed to high distress. Alm (2009) noted that expression of affect in language is often non-extreme. In a study of affective language in tales, Alm (2010) showed that affect is more often than not located in the gray zone between neutral and emotional. Table 1 shows the distribution of the excerpts according to four assigned class labels.   Class Raw count and  % of total excerpts High Distress 39 (19.5%) Low Distress 61 (30.5%) Response 50 (25.0%) Happy 50 (25.0%) Total 200 (100%) Table 1. Distribution of excerpts by four classes.  Figure 2 provides affect class distribution by source. As expected, subforum topic seems related 
                                                2 Short happy post example: ?I now have my foot in the door of the custom cake decorating business. I start in customer service as a cashier/barista, work my way through frosting, and then either into wedding, birthday, or sculpted cakes! I have been unemployed for 3 months now and this is huge. It means I can start saving money again, paying my bills and loans, and all the while doing something I love!? 3 Posts were self-annotated according to the title of the forum to which they were submitted (e.g., r/depression posts as distress, and r/happy posts as happy and non-distress). Self-annotation acknowledges that people experience subjective	 ?differences in their tolerance levels for distress. Only distressed posts were perceptually sorted into high or low distress based on data observations. Texts were also inspected to block invalid posts, spam, or irrelevant responses. 
11
to the distribution of intensity of distressed posts (high vs. low). 
 Figure 2. Excerpts by class and source. 5 Corpus linguistic analysis of dataset Since this was an exploratory study, we conducted corpus linguistic analysis of the dataset by exploring descriptive statistics of linguistic and textual dimensions of the dataset. 4  As Table 2 shows, the collected corpus had 3,140 sentences, and totaled 49,850 words. There were on average 16 sentences or roughly 250 words in an excerpt.  Total excerpts 200 Total sentences 3,140 Total words 49,850 Average sentences per excerpt 15.70 Average words per excerpt 249.25 Average words per sentence 15.88 Table 2. Basic dataset statistics.  Table 3 shows basic statistics on text length.  
Affect state and source Sentences / excerpt Words  / excerpt H Distress /r/SuicideWatch 19.8 300.0 H Distress /r/depression 31.1 399.7 L Distress breast cancer forum 16.5 297.5 L Distress /r/SuicideWatch 21.0 355.7 L Distress /r/depression 19.9 308.0 Response breast cancer forum 9.8 163.6 Response /r/SuicideWatch 14.3 218.2 Response /r/depression 13.4 219.9 Happy /r/happy 8.5 144.9 Table 3. Sentences and words per excerpt by affect state and source. 
                                                4 We recognize that it would have been preferable to compute corpus statistics on a separate development dataset. 
The statistics indicate that happy posts have the fewest sentences and words per excerpt, followed by the responses, ending with the distressed posts.5 In Table 4, we consider words per sentence as a metric independent of excerpt length, therefore avoiding potential selection bias. The average sentence length tended to be similar across forums.  
Affect state and source No. of excerpts Words per sentence H Distress /r/SuicideWatch 29 15.1 H Distress /r/depression 9 12.8 L Distress breast cancer forum 11 18.0 L Distress /r/SuicideWatch 26 16.9 L Distress /r/depression 24 15.5 Response breast cancer forum 13 16.6 Response /r/SuicideWatch 30 15.3 Response /r/depression 7 16.4 Happy /r/happy 50 17.1 Table 4. Length statistics by affect state and source.  We also examined exact lexical matches in polarity word lists,6 with words having positive and negative connotation, which had been used before in Alm?s work (2009). Positive words seemed favored in non-distressed posts (i.e., responses and happy posts). The opposite did not hold for distressed posts. Results are in Table 5. We additionally examined the number of affect words present in each excerpt by considering four relevant affect word lists from Alm (2009), which were slightly expanded for this analysis (but less extensive than the polarity ones, yielding fewer matches overall).  Affect state and source Positive Negative H Distress /r/SuicideWatch 18.0 20.0 H Distress /r/depression 24.0 24.0 L Distress breast cancer forum 21.0 15.0 L Distress /r/SuicideWatch 24.0 22.0 L Distress /r/depression 19.0 20.0 Response breast cancer forum 14.0 8.1 Response /r/SuicideWatch 17.0 13.0 Response /r/depression 17.0 13.0 Happy /r/happy 9.8 4.7 Table 5. Average polarity word list matches by affect state and source. 
                                                5 Because only one BreastCancer.org post was classified as high distress, it was considered an outlier and thus excluded in presenting and discussing these tables. 6 Positive and negative word lists contained 1915 and 2294 lexical items, respectively. 
50	 ?
9	 ?
24	 ?
7	 ?
29	 ?
26	 ?
30	 ?
1	 ?
11	 ?
13	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
H	 ?Distress	 ? L	 ?Distress	 ? Response	 ? Happy	 ?
N
um
be
r	 ?
of
	 ?e
xc
er
pt
s	 ?
/r/happy	 ? /r/depression	 ?
/r/SuicideWatch	 ? BreastCancer	 ?
12
The average numbers of exact lexical matches from the word lists in all excerpts are shown in Table 6. For each affect word list (cf. columns), the highest and lowest values are in bold font. Table 6 shows that the number of average matches was low overall, and that in general, there were more matches with sad and afraid wordlists. However, happy posts showed slightly more overlap with the happy word list.  Affect state  and source Happy Sad Afraid Angry H Distress /r/SuicideWatch  0.9 1.8 2.1 1.0 H Distress /r/depression  1.1 3.6 3.3 1.0 L Distress breast cancer forum  1.8 1.6 1.9 0.5 L Distress /r/SuicideWatch  1.5 2.9 4.0 0.7 L Distress /r/depression  1.4 2.5 2.7 0.8 Response breast cancer forum  1.2 0.5 1.0 0.3 Response /r/SuicideWatch  1.3 2.0 2.4 0.5 Response /r/depression  0.6 1.1 1.3 0.0 Happy  /r/happy  1.4 0.4 0.5 0.1 Table 6. Average emotion word list matches by affect state and source.  Lastly, because pronouns have been found important for linguistic analysis of mental health disorders or socio-cognitive processes (e.g., Andreasen and Pfohl, 1976; Pennebaker 2011), we explored this in the dataset based on the part of speech output from an NLTK-based tagger (Bird et al, 2009). Table 7 shows percentages of first-, second-, and third-person pronouns in the dataset.  
Affect state and source 1st person 2nd person 3rd person H Distress /r/SuicideWatch 77.1 0.9 22.0 H Distress /r/depression 56.1 12.0 31.9 L Distress breast cancer forum 63.0 10.9 26.1 L Distress /r/SuicideWatch 68.6 1.6 29.8 L Distress /r/depression 76.9 1.6 21.5 Response breast cancer forum 39.1 33.1 27.8 Response /r/SuicideWatch 23.1 46.1 30.8 Response /r/depression 21.3 56.9 21.8 Happy /r/happy 72.1 4.1 23.8 Table 7. % pronoun by person, affect state, and source.  
There were few second-person pronouns in distressed and happy posts, but more in the responses, which had fewer first-person pronouns. This observation confirms that distressed and happy posts are self-oriented, but that responses, which reassure and reply to a thread initiator, are other-oriented. Perspective is thus another meaningful dimension of this affect dataset. 6 Computational modeling experiments This initial study used three fundamental supervised classification methods: Na?ve Bayes, Maximum Entropy, and Decision Tree (Bird et al, 2009). These allowed us to derive initial reference values which can be improved upon with more advanced techniques in future work. We also provide results for a fourth approach, Perkins? Max Vote method (2010), using the other three algorithms? predictions to give a joint prediction. 6.1 Feature set used for modeling We developed a set of features based on the scholarly literature (e.g., Alm, 2009; Andreasen and Pfohl, 1976; Endres, 2004; Yu et al, 2009). The following features were automatically extracted from text, using Python, NLTK (Bird et al, 2009), and Perkins (2010): ?bag of words? (BOW) with unique unigrams; excerpt length in sentences; excerpt and sentence lengths in words; positive vs. negative polarity word list matches; happy, sad, afraid, and angry affect word list matches; first-, second-, and third-person pronouns; and, finally, nouns, verbs, adjectives, adverbs, and pronouns. 7  Most features were initially examined both as a raw number and as a per sentence average. Features were discretized by considering how they deviated (more vs. less) from average values calculated from the corpus as a whole.8 This resulted in 42 distinct feature types. Feature extraction was conducted the same way for train and test sets.                                                 7 Part of speech ratios were included due to an indication by Fraser et al (1986) that verb patterns could be useful in discriminating manic patients from schizophrenics and the control group. 8 The absence of a separate dataset for computing the averages allows a possibility of overfitting the data. However, we assume the averages are representative for similar texts and will be useful in future expanded model development. 
13
6.2 Experiment 1: Classification at two levels The computational experimental process is illustrated in Figure 3. In these experiments, the dataset is initially randomized and then evaluated with k-fold cross-validation, by repeating the classification process k times. Performance is thus reported as the average over k accuracy scores. The experiment explored five scenarios with k = {5, 10, 20, 100, 200}. The last scenario corresponds to a leave-one-out cross-validation (i.e., where the train set consists of (N-1) instances and the test set of one instance, and the procedure is repeated N times, where N is the total instances in the dataset). 
 Figure 3. Computational experimentation process.  Figure 4 shows the accuracy for the coarse-grained binary classification problem which involved assigning either a distressed or a non-distressed label to a text excerpt. The majority class baseline for this is 50%, as half of the excerpts belonged to each of the two classes. Figure 4 shows that the classifiers average performance has a stable range with around 73-76% accuracy, across varying k-folds and across algorithms. This performance improves more than 20% over the majority class baseline, which is indicated by a line in Figure 4. 
 Figure 4. Classification accuracy for the coarse-grained classification scenario that considers two affect states: distressed and non-distressed.  
Next, Figure 5 shows the results for classification at the fine-grained level which considers four affect classes: high distress, low distress, response, and happy. Here the majority class baseline is 30.5%. Four states yield around 54-57% accuracy. Again, that is more than a 20% improvement over the majority class baseline. The exception is Maximum Entropy, which performs poorly on this classification task. 
 Figure 5. Classification accuracy for the fine-grained classification scenario that considers four states.  Inspecting the most relevant features from runs over the course of the study indicates that the number of second-person pronouns, which usually identified responses, and the number of verbs and fearful affect words per sentence are particularly important. In responding to a post, one uses more second-person pronouns in order to address the original poster. Again, this indicates that turn-taking impacts affective language behaviors. A confusion matrix in Table 8 shows misclassification results for a select test fold of fine-grained classification. The shaded cells along the diagonal show how often the model correctly predicted an affect state. The other cells show where the model misclassified the affect state.   Predicted Actual H Distress L Distress Response Happy H Distress 7.6% 3.0% 1.5% 3.0% L Distress 7.6% 4.5% 4.5% 9.1% Response . . 28.8% . Happy 10.6% 3.0% 3.0% 13.6% Table 8. A select confusion matrix.9  Looking at the response class, for example, the classifier correctly classified all of the actual                                                 9 This table shows results from a single test of a classifier. Due to the random test set, totals do not match the corpus totals. 
0	 ?
20	 ?
40	 ?
60	 ?
80	 ?
100	 ?
5	 ? 10	 ? 20	 ? 100	 ? 200	 ?
A
ve
ra
ge
	 ?%
	 ?a
cc
ur
ac
y	 ?
Cross-??valida?n	 ?folds	 ?
Na?ve	 ?Bayes	 ? Max	 ?Entropy	 ? Decision	 ?Tree	 ? Max	 ?Vote	 ?
0	 ?
20	 ?
40	 ?
60	 ?
80	 ?
100	 ?
5	 ? 10	 ? 20	 ? 100	 ? 200	 ?
A
ve
ra
ge
	 ?%
	 ?a
cc
ur
ac
y	 ?
Cross-??valida?n	 ?folds	 ?
Na?ve	 ?Bayes	 ? Max	 ?Entropy	 ? Decision	 ?Tree	 ? Max	 ?Vote	 ?
14
response excerpts. This is likely due to the importance of second-person pronouns found in particular in the response excerpts. However, the classifier incorrectly labeled some excerpts in each of the other classes as response. Although this classifier was not as accurate for the other affect classes, the accurate option was the most commonly predicted class for both high distress and happy. This was not the case for low distress, however, which was more often predicted as high distress or happy. This can reflect the challenge of affect analysis in the gray zone between affect and neutrality, as lower emotional intensity decreases perceptual clarity. This finding is consistent with the previous literature, discussed above. A way to deal with this issue is to combine text analysis with other data analysis. 6.3 Experiment 2: Ablation study An ablation study was performed to assess the accuracy with different features given the four fine-grained classes, using a k = 5 cross-validation. We ignore bag of words, which can result in many sparse features, to examine other types. In Table 9, the first ablation step represents only length variables; the second adds polarity variables; the third adds affect variables; the fourth adds pronoun variables; and the fifth adds part of speech variables (in each case, to the features added in previous steps). Each test was done on all four supervised classification algorithms. The results with this split of train and test data show that each addition to the feature set improved the accuracy of the model's predictions, except the part-of-speech features. This could be due to the particular data split, the order of the ablation steps, or the ablation feature groupings. Additionally, excluding BOW features did not have a clear negative effect on performance. Considering only length averaged 25.1% accuracy across classifiers; adding five feature types resulted in 54.5%.   Classifier type   NB ME DT MV Mean Length .260 .225 .255 .265 .251 + polarity .295 .295 .390 .320 .325 + affect .430 .395 .365 .415 .401 + pronouns .590 .530 .485 .580 .546 + POS .595 .505 .505 .575 .545 Table 9. Ablation study results: four affect states fine-grained classification scenario (NB=Na?ve Bayes, ME=Max Entropy, DT=Decision Tree, MV=MaxVote). 
7 Engineering statistics applied to NLP Choosing the right feature set remains a difficult, poorly understood process. Here, we report on a separate analysis using a 2k factorial design, which is a common method from engineering statistics that can be used to quantitatively and systematically determine the effect and interactions that different linguistic feature types have on the assessment of the affect state of a text. 10  The outcome of this factorial design is a response formula that can be used to classify excerpts. A 2k experimental design assumes that a decision maker wants to determine how to express the effect of k different factors and their interactions on a response of interest. Given that the factors can take any possible value, the number of necessary experiments to statistically deduce such an expression can be quite large and expensive. Instead, a 2k design limits each factor to only two levels (a high and a low value). The minimum number of experiments needed to deduce a model that explains the direct and interaction effects of k factors is 2k. For example, a problem in which 5 factors are assumed to affect the value of a response requires executing 25=32 experiments, each with a unique arrangement of factor levels. Replications of these experiments are recommended to increase accuracy in the estimation of the term coefficients. Having 42 candidate linguistic features that could influence an evaluator?s decisions to categorize the distress state of a text would have required at least 242 (over 4 trillion!) tests with different configurations of features. Therefore, we grouped related linguistic and textual features into five super-factors. For example, sentences per excerpt, words per excerpt, and words per sentence were all combined into a length factor. The super-factors chosen were:  y1 = length, y2 = polarity, y3 = affect, y4 = pronoun, and y5 = parts of speech.11 Using five super-factors resulted in 32 (25) possible experimental combinations. We assessed the 200 text excerpts based on all 42 linguistic features to get a numerical value for                                                 10  We adapt the regular terminology used in engineering statistics for discussing this approach. This means that k is used in a different sense in this section compared to above. 11 BOW features were excluded here as well. The ablation study in section 6.3 also justifies their exclusion. 
15
each super-factor. We then labeled each of these numerical values as high or low, based on the median of all 200 values for each factor and for each text. The super-factor label combinations for each of the 200 excerpts were then mapped to these 32 possible combinations. This mapping was used to generate a response formula (similar to a multi-attribute regression expression) that found the direct effect of the super-factors and their interactions on the distress evaluation. We found that three main effects of the super-factors and four of their interactions were statistically significant. The significant super-factors were affect, pronoun, and part of speech. Although the main effect of length was not significant, its interactions with the affect and pronoun super-factors were significant. The obtained expression for predicting the class of an excerpt is below. Each factor is a positive or negative 1, for high or low values, respectively:  Response = ?0.377 + 0.2062y3 + 0.355y4 ? 0.276y5 + 0.1983y1y3 + 0.1928y3y4 ? 0.197y1y3y4 ? 0.1704y1y3y4y5  Responses can range from ?2 to 1, with ?2 predicting high distress, ?1 predicting low distress, 0 predicting response, and 1 predicting happy. This response formula could be tested as a prediction method on future data not used in its estimation. We further propose using the 2k factorial mechanism to systematically reduce the super-factors into simpler features. For example, because one of the super-factors did not show a significant main effect, we can assume that its linguistic features do not individually reflect distress or non-distress. Thus, one could reconfigure new super-features, assigning new values to the 200 excerpts, and repeat the analysis and remove any super-feature whose main and secondary effects are not significant. This iterative process should halt when we have new, redefined super-features that are significant in predicting the distressed and non-distressed states of the 200 excerpts. An analysis of residuals will serve as a control mechanism to reduce the number of iterations in the process. 8 Conclusion If there were a way to automatically identify individuals with undiagnosed mental illnesses, it 
would be possible to recommend a clinical visit. The problem addressed by this paper was how to discriminate related affect states via computational linguistic analysis of short online writings. We reported on an initial dataset from forums and corpus linguistic analysis, and found patterns in the data that merit further study. To predict distress states, we used supervised classification and explored super-features? importance with a 2k factorial design, an engineering statistics method. We approach this problem from a linguistic perspective and pay extra attention to linguistic analysis and how distress is linguistically encoded. Not only do we report on effects by forum, distress state, emotion and polarity lexicon, etc., but our 2k factorial analysis also rigorously clarifies which linguistic feature types contribute in statistically significant ways. Additionally, the ablation study conducted largely verified these findings. Leave-one-out cross-validation is common with small datasets; we also show that varying k in the cross-validation does not impact results.	 ?There are benefits with smaller datasets and shorter texts. In clinical settings, data can be especially hard to obtain, and it is useful to understand the limitations and affordances of modeling with limited data. Similarly, it is important to understand how models perform on fundamental algorithms and shallow features extracted from text that can generalize to, for example, resource-poor languages. While this data was adequate for exploratory investigation, a larger, clinical dataset would be less prone to selection bias. Combining text with other analysis information seems key in future work. Also, more advanced algorithms could yield more accurate predictions, as could iterations of the 2k factorial analysis. Other aspects left for future study include the relationship between the individual affect states and their predictive linguistic features and experimentation with unbalanced data scenarios. Lastly, another area to pursue is using affect features for identifying linguistic patterns unique to online communication. Acknowledgments This work was supported by an RIT Seed Funding Award. We thank anonymous reviewers for comments. We also thank W. McCoy and R. Lehrman.  
16
References Cecilia Ovesdotter Alm. 2009. Characteristics of high agreement affect annotation in text. Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, Uppsala, Sweden, 15-16 July 2010, 118-122. Cecilia Ovesdotter Alm. 2009. Affect in Text and Speech. VDM Verlag, Saarbr?cken. Nancy J.C. Andreasen and Bruce Pfohl. 1976. Linguistic analysis of speech in affective disorders. Archives of General Psychiatry, 33:1361-1367. Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O'Reilly Media, Sebastopol, CA. Natural Language Toolkit, http://www.nltk.org/book. Anna Dixon, David McDaid, Martin Knapp, and Claire Curran. 2006. Financing mental health services in low and middle income countries: equity and efficiency concerns. Health Policy Plan, 21:71-82. Johann Endres. 2004. The language of the psychopath: characteristics of prisoners' performance in a sentence completion test. Criminal Behaviour and Mental Health, 14:214-226. Horacio Fabrega, Jr. 1991. Psychiatric stigma in non-Western societies. Comprehensive Psychiatry, 32:534-551. William I. Fraser, Kathleen M. King, Philip Thomas, and Robert E. Kendell. 1986. The diagnosis of schizophrenia by language analysis. British Journal of Psychiatry, 148:275-278. Barbara Gawda. 2010. Syntax of emotional narratives of persons diagnosed with antisocial personality. Journal of Psycholinguistic Research, 39:273-283. Philip D. Harvey. 1983. Speech competence in manic and schizophrenic psychoses: The association between clinically rated thought disorder and cohesion and reference performance. Journal of Abnormal Psychology, 92(3):368-377. Mukund Jha and No?mie Elhadad. 2010. Cancer stage prediction based on patient online discourse. Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, Uppsala, Sweden, 64-71. Ronald C. Kessler, Olga Demler, Richard G. Frank, et al 2005. Prevalence and treatment of mental disorders, 1990 to 2003. New England Journal of Medicine, 352:2515-2523. Hsiang-Ching Kung, Donna L. Hoyert, Jiaquan Xu, and Sherry L. Murphy. 2008. Deaths: Final data for 2005. National Vital Statistics Report, 56:1-121. 
Kevin Lerman, Ari Gilder, Mark Dredze, and Fernando Pereira. 2008. Reading the markets: Forecasting public opinion of political candidates by news analysis. Proceedings of the 22rd International Conference on Computational Linguistics (Coling 2008), 473-480. Mark Lutz. 2009. Learning Python (4th Edition). O'Reilly Media, Sebastopol, CA. Pawel Matykiewicz, Wlodzilaw Duch, and John P. Pestian. 2009. Clustering semantic spaces of spaces of suicide notes and newsgroup articles. Proceedings of the Workshop on BioNLP, Boulder, Colorado, 179-184. Kathleen M. Michels, Karen J. Hofman, Gerald T. Keusch, Sharon H. Hrynhow. 2006. Stigma and global health: Looking forward. Lancet, 367:538-539. Alex J. Mitchell, Amol Vaze, and Sanjay Rao. 2009. Clinical diagnosis of depression in primary care: A meta analysis. Lancet, 374:609-619. Elias Mossialos, Anna Dixon, Josep Figueras, and Joe Kutzin. 2002. Funding Health Care: Options for Europe. Open University Press, Buckingham, UK. James W. Pennebaker. 2011. The Secret Life of Pronouns: What our Words Say about us. Bloomsbury Press, New York. Jacob Perkins. 2010. Python Text Processing with NLTK 2.0 Cookbook. Packt Publishing, Birmingham. John P. Pestian, Pawel Matykiewicz, Michelle Linn-Gus, Brett South, Ozlem Uzner, Jan Wiebe, Kevin B. Cohen, and Christopher Brew. (2012). Sentiment analysis of suicide notes: A shared task. Biomedical Informatics Insights. 5 (Suppl. 1), 3-16. John P. Pestian, Pawel Matykiewicz, and Jacqueline Grupp-Phelan. 2008. Using natural language processing to classify suicide notes. BioNLP 2008: Current Trends in Biomedical Natural Language Processing, Columbus, Ohio, 96-97. Ann Barnett Ragin and Thomas F. Oltmanns. 1983. Predictability as an index of impaired verbal communication in schizophrenic and affective disorders. British Journal of Psychiatry, 143:578-583. Proctor P. Reid, W. Dale Compton, Jerome H. Grossman, and Gary Fanjiang. 2005. Building a Better Delivery System: A New Engineering/Health Care Partnership. The National Academies Press. Brian Roark, Margaret Mitchell, and Kristy Hollingshead. 2007. Syntactic complexity measures 
17
for detecting Mild Cognitive Impairment. BioNLP 2007: Biological, translational, and clinical language processing, Prague, 1-8. Shekhar Saxena, Graham Thornicroft, Martin Knapp, and Harvey Whiteford. 2007. Resources for mental health: scarcity, inequity, and inefficiency. Lancet, 370:878-889. Robert P. Schumaker. 2010. An analysis of verbs in financial news articles and their impact on stock price. Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, Los Angeles, California, 3-4. David M. Shepard, Michael C. Ferris, Gustavo H. Olivera, and T. Rockweel Mackie. 1999. Optimizing the delivery of radiation therapy to cancer patients. SIAM Review, 41(4):721-744. Harry A. Sultz and Kristina M. Young. 2011. Health care USA: Understanding its Organization and Delivery (7th Edition). Jones and Barlett Learning, LLC, Sudbury. C. Turrina, R. Caruso, R. Este, et al 1994. Affective disorders among elderly general practice patients: a two-phase survey in Brescia, Italy. British Journal of Psychiatry, 165:533-537. World Health Organization. 2005. Mental Health Atlas. WHO, Geneva, Switzerland. Liang-Chih Yu, Chien-Lung Chan, Chung-Hsien Wu, and Chao-Cheng Lin. 2009. Mining association language patterns for negative life event classification. Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, 201-204. Yitao Zhang and Jon Patrick. 2006. Extracting patient clinical profiles from case reports. Proceedings of the 2006 Australasian Language Technology Workshop, 167-168. 
18
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 1?9, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Disfluencies as Extra-Propositional Indicators of Cognitive Processing
Kathryn Womack
Dept. of ASL
& Interpreting Edu.
kaw8159@rit.edu
Wilson McCoy
Dept. of Interactive
Games & Media
wgm4143@rit.edu
Cecilia Ovesdotter Alm
Dept. of English
coagla@rit.edu
Cara Calvelli
College of Health
Sciences & Tech.
cfcscl@rit.edu
Jeff B. Pelz
Center for
Imaging Science
pelz@cis.rit.edu
Pengcheng Shi
Computing &
Information Sciences
spcast@rit.edu
Anne Haake
Computing &
Information Sciences
anne.haake@rit.edu
Rochester Institute of Technology
Abstract
We explore filled pause usage in spontaneous
medical narration. Expert physicians viewed
images of dermatological conditions and pro-
vided a description while working toward a
diagnosis. The narratives were analyzed for
differences in filled pauses used by attending
(experienced) and resident (in-training) physi-
cians and by male and female physicians. At-
tending physicians described more and used
more filled pauses than residents. No differ-
ence was found by speaker gender. Acoustic
speech features were examined for two types
of filled pauses: nasal (e.g. um) and non-nasal
(e.g. uh). Nasal filled pauses were more of-
ten followed by longer silent pauses. Scores
capturing diagnostic correctness and diagnos-
tic thoroughness for each narrative were com-
pared against filled pauses. The number of
filled and silent pauses trends upward as cor-
rectness scores increase, indicating a tentative
relationship between filled pause usage and
expertise. Also, we report on a computational
model for predicting types of filled pause.
1 Introduction
Although they are often not consciously realized,
disfluencies are common in everyday speech. In an
overview of several studies, Fox Tree (1995) esti-
mates that approximately 6% of speech is disflu-
ent. Disfluencies include filled pauses, silent pauses,
edited or repeated words, and sounds such as clear-
ing one?s throat or click noises. Disfluencies affect
the way that listeners comprehend speech in learn-
ing situations (Barr, 2003), formulate opinions of
the speaker as being more or less fluent (Lo?vgren
and van Doorn, 2005), and even parse grammatically
complex sentences (Bailey and Ferreira, 2003).
Since disfluencies are generally absent in writ-
ten text, they are irrelevant when analyzing text for
extra-propositional meaning, such as uncertainty or
modality (Vincze et al, 2008, for example). In con-
trast, when studying meaning in spoken language,
disfluencies provide information about a speaker?s
cognitive state. For example, they might indicate
cognitive load, uncertainty, confidence, thoughtful-
ness, problems in reasoning, or stylistic preferences
between individuals or groups of individuals. We
study filled pauses (e.g. um and uh) and leave other
disfluency types for future work.
The presence of filled pauses could indicate
context-dependent facets of cognitive reasoning pro-
cesses. We examine filled pauses present in the
speech of highly-trained dermatologists who were
shown images of dermatological conditions and
asked to provide a description and diagnosis. We
look at the difference between two different types
of filled pauses: those with nasal consonants, such
as um; and those without nasal consonants, such as
uh. We build a computational model to confirm find-
ings that nasal and non-nasal filled pauses differ by
prosodic and contextual features. In addition, we
first compare whether there is a difference between
filled pause use for variables such as level of physi-
cian expertise and gender. We also examine the rela-
tionship of correctness in the diagnostic process with
respect to filled pause use.
There is evidence that filled pauses indicate cog-
nitive processing difficulties and could change the
1
speaker?s intended meaning or the listener?s per-
ceived meaning of an utterance. However, such im-
plicit meanings are severely understudied in previ-
ous work, especially in specialized, high-stakes do-
mains such as medical diagnostics. Little is under-
stood about what factors impact the linguistic behav-
ior of using certain filled pauses rather than others,
and how the use of filled pauses differs based on
level of expertise, gender, or diagnostic correctness.
Looking into these differences is useful to form a
better understanding of the relationship between lan-
guage and specialized decision-making processes.
More specifically, it is necessary to improve the un-
derstanding of how speakers? use of filled pauses
differs based on the context of speech and how
they change the meaning and reception of speech in
extra-propositional ways.
2 Previous Work
Filled pauses in English include monosyllables with
and without nasal consonants, such as um and uh re-
spectively. Filled pauses are most common in un-
structured, spontaneous speech, but they are also
present in prompted, structured speech; and occur
in both monologues and dialogues.
Much research has been done into hedging, nega-
tion, and other propositional features that change
the meaning or modality of phrases (Morante and
Sporleder, in press). Less research has been done
into the usage of filled pauses and their relation-
ship to certainty and speculation. It has been shown
that disfluencies are used to indicate uncertainty in
speakers? forthcoming statements or to indicate that
the speaker is engaged in the discourse but working
to formulate their response (Brennan and Williams,
1995; Smith and Clark, 1993). These studies found
that speakers less confident of their answers take
longer to answer and use more disfluencies.
Recent studies have suggested that disfluencies
provide meaningful information about the speaker?s
cognitive or linguistic processes (Arnold et al,
2003; Bortfeld et al, 2001; Corley and Stewart,
2008; Oviatt, 1995, for example), and are uninten-
tional indications that the speaker is having difficulty
formulating upcoming speech.
More specifically, it has been shown that the two
major categories of filled pauses, i.e. nasal and non-
nasal, are specific indicators of the level of cognitive
load, with nasal filled pauses indicating higher load
and non-nasal filled pauses indicating lower load.
Barr (2001) performed an experiment in which a
speaker described one of several visible images to
a listener who then selected the image being de-
scribed. In this study as well as in Barr and Seyfid-
dinipur (2010), listeners focused on a topic that was
new to the discourse or exceptionally complex when
they heard the speaker say um. Although they did
not differentiate between nasal and non-nasal filled
pauses, Arnold et al (2003; 2007) found in similar
experiments that filled pauses often preceded unfa-
miliar or complex objects.
There is evidence that speakers use filled pauses
to indicate different processing difficulties. Clark
and Fox Tree (2002) describe four different filled
pauses that are annotated in the corpora they use.
These are uh, um, and their elongated versions u:h
and u:m. They argue that each of these corresponds
to a different following pause time with uh being
followed by the shortest pause time, then u:h, um,
and u:m followed by the longest. It is important to
note that their primary corpus is the London-Lund
Corpus of Spoken English, in which the pause times
were annotated based on the transcriber?s estimate of
pause time in units of ?one light foot? or ?one stress
unit? (Clark and Fox Tree, 2002, p. 80) rather than
measured in seconds.1
However, studies on filled pauses by Barr (2001)
and Smith and Clark (1993) measured the duration
of silent pauses in seconds and confirm that um was
followed by longer silent pauses than uh. The hy-
pothesis suggested by Barr, Clark and Fox Tree, and
Smith and Clark is that uh indicates a minor delay
and lower level of cognitive difficulty while um in-
dicates a major delay due to higher level of difficulty
in speech planning and production.
On the other hand, a study by O?Connell and
Kowal (2005) refuted the findings of Clark and Fox
Tree and showed that specific filled pauses could
not predict pause time in their corpus of TV inter-
views. O?Connell and Kowal?s corpus was six in-
terviews conducted by various TV personnel with
1The difference between listeners? perception of duration
and actual duration is an important one because perceptual and
actual duration do not always match (Megyesi and Gustafson-
Capkova, 2002; Spinos et al, 2002).
2
Hillary Clinton because these ?professional speak-
ers? (O?Connell and Kowal, 2005, p. 560) should
be more likely to use filled pauses according to con-
vention. However, speech in public TV interviews is
likely to be pre-planned and highly self-monitored
by the speakers, and it may not be appropriate to
consider this situation a model for spontaneous, less
formal, and less public speech. It has been shown
that rate and use of filled pauses can vary widely
within certain fields (Schachter et al, 1991), in situ-
ations that are more or less structured (Oviatt, 1995),
and depending on the formality of the situational
context (Bortfeld et al, 2001).
3 Data, Annotation, and Methods
Data were acquired from a study involving 16 der-
matologists, including 12 attending physicians and 4
residents. The participants were evenly split for gen-
der. These physicians were shown 50 images of dif-
ferent dermatological conditions and asked to pro-
vide a description and diagnosis of each. In a mod-
ification of the Master-Apprentice scenario (Beyer
and Holtzblatt, 1997), each observer explained his
or her thoughts and processes to a student who was
silent. These are monologues; however, the Master
has the feeling of interaction and of dialogue.
Audio of each description was recorded while
eye-movements were tracked. The relationship be-
tween eye-movements and extra-propositional fea-
tures will be the topic of a later study. The audio files
were manually single-annotated and time-aligned at
the word level in Praat, a software for acoustic and
phonetic analysis (Boersma, 2001). A section of
the spoken narrative with time-alignment is pictured
in Figure 1. Praat and Python scripts were used to
computationally extract measurements of pitch, in-
tensity, and duration for words, silent pauses, and
narratives. In total, there were 800 audio-recorded
narratives. At this time, 707 of these narratives have
been time-aligned and annotated and only these are
used in this study.
Four transcribers worked independently on time-
alignment, and they were given instructions by one
coordinator. Every spoken token was included in
the transcriptions, including filled pauses, extra-
linguistic sounds such as clicks, repairs, and silent
pauses. Annotators were instructed to mark only
Figure 1: Screenshot of the program Praat which was
used to time-align each narrative and extract acoustic
prosodic information about the physicians? speech.
silent pauses that were longer than 30 milliseconds,
because it has been shown that pauses under 20-30
ms are not consistently perceived by listeners in dis-
course (Kirsner et al, 2002; Lo?vgren and van Doorn,
2005).
After word-level time-alignment, each narrative
was independently annotated by three expert derma-
tologists who did not participate in the original data
elicitation procedure. Each narrative was examined
for medical lesion morphology (the description of
the condition), differential diagnosis (possible diag-
nostic conditions), and final diagnosis (the diagno-
sis that the observer found most likely). These inde-
pendent experts annotated the physicians? diagnostic
correctness for the three steps of the diagnostic pro-
cess. They annotated medical lesion morphology as
correct, incorrect, correct but incomplete, or none,
indicating that no medical morphology was given.
Final diagnosis was labeled as correct, incorrect, or
none, and differential diagnosis was rated as yes, no,
or no differential given. An analysis of the annotated
data set is discussed by McCoy et al (Forthcoming
2012).
4 Results and Discussion
4.1 Types of Filled Pauses
Nasal filled pauses included hm and um and non-
nasal filled pauses included ah, er, and uh. We an-
alyzed nasal and non-nasal filled pauses as groups
rather than each individual filled pause because the
number of filled pauses within each category was not
balanced. Higher token counts of uh and um were
identified, with fewer ah, er, and hm filled pauses. In
comparing use of nasal and non-nasal filled pauses,
3
FPs No. Dur. St. Dev. %
hm 78 0.48 s 0.20 2%
um 1439 0.51 s 0.19 36%
Total
(nasal)
1517 0.50 s 0.19 38%
ah 23 0.46 s 0.23 1%
er 9 0.26 s 0.09 <1%
uh 2401 0.36 s 0.16 61%
Total (non-
nasal)
2433 0.36 s 0.16 62%
Total (all) 3950 0.42 s 0.19 100%
Table 1: Total number of each type of filled pause (FPs)
with mean duration in seconds, standard deviation of the
mean duration, and percentage of all filled pauses.
we considered all 707 narratives. The number of to-
kens and average duration for each filled pause is
given in Table 1.
The average filled pause duration was slightly
longer for nasal than for non-nasal, likely due to the
segmental quality.
In total, 38% of the filled pauses in our data set are
nasal. However, observers vary widely in their indi-
vidual usage, from one observer who used 22 non-
nasal (10%) and 189 nasal (90%) filled pauses to an
observer at the other extreme who used 562 non-
nasal (97%) and only 19 nasal (3%) filled pauses.
Some people seem to have a tendency to use one
type of filled pause over the other.
Clark and Fox Tree (2002) found that nasal filled
pauses were more often followed by silent pauses
and that those silences were on average longer than
that of non-nasal filled pauses. Our data are consis-
tent with this as shown in Tables 2 and 3,2 and Fig-
ure 2. Of the total nasal filled pauses, 70% were fol-
lowed by a silent pause, whereas only 41% of non-
nasal filled pauses were followed by a silent pause.
The mean duration of silent pauses following
nasal filled pauses was 1.5 s while non-nasal was 1.1
s, which indicates a difference significant enough
that it could be recognized by a listener. These find-
ings show that nasal filled pauses are good indica-
tors of continuing delay, which supports Clark and
Fox Tree?s hypothesis that nasal and non-nasal filled
2The data were analyzed using two-sample t-tests assuming
unequal variances.
Nasal
(hm, um)
Non-nasal
(ah, er, uh)
p
Dur. of FPs 0.50 s 0.36 s < 0.01
Dur. of FPs +
SILs
2.46 s 1.37 s < 0.01
No. of FPs 1517 2433 n/a
Table 2: Mean duration in seconds of filled pauses (FPs),
and mean duration of the filled pause including the span
of any preceding and following silences. If there were no
silences, only the duration of the filled pause was used to
calculate the mean.
Nasal
(hm, um)
Non-nasal
(ah, er, uh)
p
Dur. of pre.
SILs
1.19 s 1.15 s 0.4
No. of pre.
SILs
1167 1197 n/a
Dur. of foll.
SILs
1.50 s 1.07 s < 0.01
No. of foll.
SILs
1059 1006 n/a
Table 3: Mean duration in seconds of silent pauses (SILs)
preceding filled pauses, silent pauses following filled
pauses, and the number of tokens for each. Durations
were only considered if there was a silence, so the num-
ber of silences was different for each calculation.
Figure 2: The percentage of nasal and non-nasal filled
pauses with a preceding silent pause, following silent
pause, and a silent pause both preceding and following.
pauses are used to indicate different levels of diffi-
culty in speech planning. Taken with the results of
experiments by Barr (2001) that nasal filled pauses
are more often used before a topic that is relatively
4
complex or new to discourse, it seems that nasal
filled pauses indicate a higher level of cognitive dif-
ficulty than non-nasal filled pauses.
In their previously-mentioned study, Clark and
Fox Tree also found that nasal filled pauses were
more often preceded by delays and that those delays
were longer. Similarly, in our data 77% of the nasal
filled pauses were preceded by silences, compared
with 49% of non-nasal.
No difference was found in the mean duration of
preceding silences, however. Although this conclu-
sion is tentative, it seems that the duration of the
preceding pause could be the maximum length of
silence a speaker feels is permissible before needing
to indicate their continuing participation in the dis-
course. This supports Jefferson?s (1989) findings of
a ?standard maximum silence? of around 1 second
in discourse. At that point, the speaker could need
to signal that they have more to say, using a nasal
filled pause if they anticipate a long delay or a non-
nasal filled pause if they anticipate a shorter delay.
The longer duration of surrounding silent pauses for
nasal filled pauses also supports the conclusion that
they indicate higher cognitive load and more pre-
planning. This critical finding highlights the im-
portance of considering filled pauses in computa-
tional modeling and hint at their potential usefulness
across phenomena of extra-propositional meaning.
4.2 Gender
Traditional stereotypes have held that women are
less confident speakers than men. When women and
men use the same number of hedge words or mod-
ifiers, women are judged more harshly as sounding
passive or uncertain (Bradley, 1981). Although dif-
ferent rates and ratios of filled pauses were identi-
fied, Acton (2011), Binnenpoorte et al (2005), and
Bortfeld et al (2001) all found that women used a
lower rate of filled pauses than men. Acton also
found that women consistently used a higher ratio
of nasal filled pauses.
Our data were analyzed at the level of diagnostic
narrative based on the means of: number of filled
pauses, filled pauses per second, the percentage of
filled pauses (i.e. the rate per 100 words), the num-
ber of nasal filled pauses, and the percentage of nasal
filled pauses. The difference between the means was
not statistically significant, confirmed by the com-
puted p-score.3 Hence, our data do not support a dif-
ference in men?s and women?s use of filled pauses.
There are several possible explanations for this.
For example, it has been shown that women tend
to be more conscious of their speaking style than
men because they are aware of the stereotyping men-
tioned previously (Gordon, 1994), and they may
make more effort to speak clearly. Acton (2011) and
Bortfeld et al (2001) noted different usage of filled
pauses by men and women in different situations.
Whereas our results point to gender neutrality and
refute the common gender bias as well as findings
of previous studies, we recognize that our results
could reflect that this study involved a largely ho-
mogeneous professional and educational group. The
studies mentioned thus far used corpora consisting
of casual conversations in various situations with in-
dividuals of various backgrounds. Further research
into gender differences in expert fields could clarify
this factor further.
4.3 Level of Expertise
Our data were analyzed based on the means per nar-
rative, similar to Section 4.2, but comparing levels
of expertise (attending versus resident physicians).
Attending physicians? narratives had a longer mean
duration and significantly more words. Attending
physicians also used more filled pauses, a higher rate
of filled pauses per 100 words, and a higher percent-
age of nasal filled pauses (see Table 4).4
One probable explanation for the difference is that
the experienced attendings noticed more about the
image, leading them to give more information about
their thought processes and go into more detail than
residents. It is possible also that the attendings?
experience could have provided them with a larger
conceptual space and options to explore. This ex-
plains the longer narrative time and the higher num-
ber of words used. Many of the dermatological
terms used are highly complex and may require ex-
planation on the part of the observer, and other stud-
3The mean of each category was determined for each ob-
server, and then analyzed using a two-sample t-test. In total, we
had 355 narratives from males and 352 from females.
4These results were calculated using the mean of each ob-
server and each narrative. A paired t-test was used to compare
means for residents on each image against means for attendings
on each image.
5
For Narra-
tives
Attendings?
Means
Residents?
Means
p
Total Dur. 46.1 s 33.8 s < 0.01
No. of Words 85.7 50.9 < 0.01
No. of FPs 6.3 1.9 < 0.01
% FPs 8% 4% < 0.01
% Nasal FPs 0.4% 0.2% < 0.01
Table 4: Analysis considered, at the narrative level, at-
tending and resident physicians? mean total duration,
number of words (including filled and silent pauses),
number of filled pauses (FPs), percentage of filled pauses
of total words (total words includes pauses; without
pauses, this rate would be higher), and percentage of
nasal filled pauses of total filled pauses.
ies have found that the filled pause rate increases as
the utterance length increases (Oviatt, 1995; Bort-
feld et al, 2001), so one would expect to see more
filled pauses used in longer descriptions.
One issue with our data is that the number of at-
tending physicians and the number of resident physi-
cians is not balanced. We had 592 narratives done by
12 attendings and 115 done by 4 residents. All val-
ues were calculated using means so the values are
not weighted based on the number of narratives ana-
lyzed. However, we have previously mentioned that
personal preference plays a role in the usage of filled
pauses, and we have a wider variety of attending ob-
servers than resident observers. It could be that our
resident observers happened to be the kinds of peo-
ple who do not use many filled pauses.
4.4 Diagnostic Correctness
Three scores were determined for each narrative.
The first score was the holistic expert score provided
by the expert annotators, based on ?relevancy, thor-
oughness, and accuracy? of each narrative from 1
to 3 with 3 being the best. The second score was
an overall correctness score which spanned from
0 to 3, with one-third of a point given per inde-
pendent annotator for each step (i.e. medical lesion
morphology, differential diagnosis, and final diag-
nosis) if correct and 13 ? 0.5 points given for cor-
rect but incomplete. The last score was the not-
given score which, similar to the correctness score,
spanned from 0 to 3 with one-third of a point given
per annotator for each step if the original observer
Figure 3: Average number of filled pauses per narrative
by observer (y-axis) against the holistic expert score, cor-
rectness score, and not-given score (x-axis).
did not provide that information.5
Correlation between these three scores and the
number or rate of words, filled pauses, and silent
pauses was not strong enough to make predictions,
indicating that more factors than just the scores
should be considered. However, certain trends were
evident. As the holistic expert and correctness
scores improved, the means of narratives? total du-
ration in seconds and total number of words also in-
creased. This finding, combined with the fact that
experienced physicians spoke more and had higher
average correctness and expert scores, indicates that
verbal behavior can reflect both heightened concep-
tual knowledge and level of expertise.
The number of filled pauses per narrative, num-
ber of silent pauses per narrative, and the total dura-
tion of filled and silent pauses (per narrative) also in-
creased as the holistic expert and correctness scores
improved and the not-given score decreased. The
graph of filled pauses in Figure 3 indicates that the
increase in the number of filled and silent pauses in-
volve more cognitive processing. That the not-given
score tends to inversely decrease could indicate very
little cognitive processing (e.g., if an observer was
so unsure that they did not even hazard a guess).
The number and percentage of nasal filled pauses,
as opposed to non-nasal filled pauses, increased at
5There was not a strong correlation between the holistic ex-
pert, correctness, and not-given scores, but each score measured
different criteria. The mean holistic expert score was 2.3 with
a standard deviation of 0.5; the mean correctness score was 1.6
with a standard deviation of 0.8; and the mean not-given score
was 0.26 with a standard deviation of 0.16.
6
a slightly higher rate as the holistic expert and cor-
rectness scores increased. This could indicate that
nasal filled pauses indicate a higher cognitive load
and therefore more consideration in the decision-
making process. However, as discussed in Section
4.1, this corpus has more non-nasal than nasal filled
pauses and some observers have a particular prefer-
ence, so this would need to be controlled and inves-
tigated further.
5 Computational Model of Filled Pauses
Based on Speech Features
A computational model was developed to classify
filled pauses as either nasal or non-nasal,6 based
on features discussed in our analysis and in previ-
ous work. This model performs above a majority
class baseline, supporting our findings that there are
differences between the two types of filled pauses,
given the features that we have examined, which can
be captured by a computational model.
The features considered for classification were
total duration and number of words in the narra-
tive; duration, intensity, mean pitch, minimum pitch,
and maximum pitch of the filled pause;7 the filled
pause?s time and word position in the narrative; time
and word position as a percentage of the total narra-
tive; and length of silent pauses8 on each side of the
filled pause. The CFS subset evaluation features se-
lection algorithm was first applied. The filled pause
duration, maximum pitch, left silence length, and
right silence length were maintained as features for
classification; other features were not used further.
The widely used J48 decision tree algorithm in
Weka9 was used to classify our data, which allowed
us to visualize our model. The experimental ap-
proach was guided by the relatively small size of
the dataset. We wanted to avoid over- or under-
interpretation of results based on just a small held-
out test set. The data were shuffled and partitioned
differently during tuning and testing to ensure dis-
6We also made a fine-grained model to classify specific
filled pauses ah, er, hm, uh, and um. It had 70% accuracy but
was generally unable to identify the least-often occurring ah, er,
and hm filled pauses, so it is not reported on here.
7Pitch features were extracted considering gender: 75-300
Hz for men and a 100-500 Hz for women.
8If there was no silence, the value was 0.
9See http://www.cs.waikato.ac.nz/ml/weka/.
Predicted
Nasal Non-nasal
Actual Nasal 900 617Non-nasal 462 1971
Table 5: Confusion matrix of classification results.
tinct identities of the data splits so that parameters
were not tuned on test folds. The algorithm?s pa-
rameters were tuned using 5-fold cross-validation;
the best-performing fold?s parameters were chosen.
The data were then shuffled anew and split into 10
folds with each fold being the test set for one experi-
mental run. Results are reported on the final 10-fold
cross-validation case.
The baseline for this model was 62% because the
majority class, non-nasal filled pauses, comprised
that percentage of the data set. Our model cor-
rectly classified 73% of the instances, performing
11% above the baseline. A confusion matrix of the
classifier output is shown in Table 5. The model per-
forms best for non-nasal filled pauses, likely because
they are more common.
The output of the decision tree indicated that du-
ration of the filled pause was the most important fea-
ture. As discussed in Section 4.1, this corresponds
with our previous statistical findings as well as those
of Clark and Fox Tree (2002) that there is a differ-
ence in duration of filled pauses. The next most
important features were the left and right silence
lengths, also supported by our analysis as well as
by Clark and Fox Tree (2002) and Barr (2001). The
last selected feature was the maximum pitch of the
filled pause, possibly due to phonemic qualities.
This computational model mirrors the findings of
Section 4.1 that the duration of filled pauses and of
surrounding silent pauses are a differentiating fac-
tor between nasal and non-nasal filled pauses and
that the contextual surroundings of each filled pause
type are different. The finding that the two distinct
types of filled pauses behave differently in this do-
main could also aid language processing systems for
clinicians in the medical field. Further research into
filled pause and other speech phenomena in each
step of the diagnostic process (i.e. medical lesion
morphology, differential diagnosis, and final diag-
nosis) could also be explored in future work.
7
6 Conclusion
The results of this study underscore the need for fur-
ther research into the production of disfluencies, es-
pecially in decision making situations and in special-
ized fields such as dermatology. Future work will
further explore their connection with highly relevant
extra-propositional meaning phenomena in diagnos-
tic verbal behaviors such as certainty, confidence,
correctness, and thoroughness.
This study has shown that the two main types of
filled pauses, nasal and non-nasal, differ in their us-
age. Nasal filled pauses are more likely to be pre-
ceded and followed by silent pauses, and these fol-
lowing silent pauses are more likely to be longer.
These findings are reinforced by the computational
model which identified the duration of the filled
pause, duration of surrounding silences, and pitch
as important for classification of filled pause type.
That longer and more frequent silent pauses sur-
round nasal filled pauses supports the hypothesis
that nasal filled pauses indicate a higher level of cog-
nitive load (Clark and Fox Tree, 2002) or a topic that
is new to the discourse or unusually complex (Barr,
2001; Barr and Seyfiddinipur, 2010).
The lack of differences in use of filled pauses by
speaker gender given the differences found by Ac-
ton (2011), Binnenpoorte et al (2005), and Bortfeld
et al (2001) shows that more research is needed to
understand gender variation in speech.
Another finding was that level of expertise in-
fluenced the use of filled pauses and overall narra-
tive length. On average, attending physicians spoke
longer, said more, used more filled pauses, and had
a higher percentage of nasal filled pauses. Attend-
ing physicians also had slightly higher holistic ex-
pert and correctness scores and were more likely to
provide medical lesion morphology, differential di-
agnosis, and final diagnosis. We believe that attend-
ing physicians likely noticed more about the images
due to their experience.
The differences by level of expertise (in our study,
between attending and resident physicians) need to
be verified and compared with more data and in non-
medical fields. The differences could also be re-
lated to teaching experience of the attending physi-
cians, so further research could compare experi-
enced physicians who are also teachers with those
who are not, and if their speaking style affects stu-
dents? comprehension. In general, differences in lin-
guistic behaviors in relation to levels of expertise
deserve more research, and might have long-term
implications for development of clinical decision-
support and training systems.
The information used by the physicians in our
study was limited; they were only shown images
of dermatological conditions without being able to
examine the patient, run diagnostic tests, or have
a patient history. This may have changed their
the behavior, along with factors such as the dif-
ficulty of diagnosis of each image and their role
in the Master-Apprentice scenario. Understanding
how these variables affect the diagnostic process
of physicians could help us understand how disflu-
encies are impacted by the contexts of diagnostic
decision-making.
The differences found between the use of filled
pauses based on level of expertise and on the correct-
ness of narratives seem to indicate that filled pauses
could provide partial information about the experts?
decision-making process as well as level of confi-
dence and certainty. This is especially important
in the medical domain in order to understand how
physicians? verbal behaviors are interpreted by other
physicians as well as by patients and students.
We recently collected a similar, larger data set and
we plan to further examine differences based on ex-
pertise in this new corpus. In the recent data collec-
tion, observers were also asked to rate their level of
certainty about the diagnosis. This provides the op-
portunity to examine the relationship between disflu-
encies and certainty. We have eye-tracking data for
both studies and future work will also look at eye-
movements in relation to the use of filled and silent
pauses, certainty, expertise level, and cognitive load.
Acknowledgements
Supported in part by NIH 1 R21 LM010039-01A1,
NSF IIS-0941452, RIT GCCIS Seed Funding, and
RIT Research Computing (http://rc.rit.edu). We
thank Lowell A. Goldsmith, M.D. and the anony-
mous reviewers for their comments, and Dr. Rube?n
Proan?o for input on statistical analysis.
8
References
Eric K. Acton. 2011. On gender differences in the dis-
tribution of um and uh. University of Pennsylvania
Working Papers in Linguistics, 17(2).
Jennifer E. Arnold, Maria Fagnano, and Michael K.
Tanenhaus. 2003. Disfluencies signal theee, um, new
information. Journal of Psycholinguistic Research,
32(1):25?36.
Jennifer E. Arnold, Carla L. Hudson Kam, and
Michael K. Tanenhaus. 2007. If you say thee uh you
are describing something hard: The on-line attribution
of disfluency during reference comprehension. Jour-
nal of Experimental Psychology: Learning, Memory,
and Cognition, 33(5):914?930.
Karl G.D. Bailey and Fernanda Ferreira. 2003. Disfluen-
cies affect the parsing of garden-path sentences. Jour-
nal of Memory and Language, 49:183?200.
Dale J. Barr and Mandana Seyfiddinipur. 2010. The role
of fillers in listener attributes for speaker disfluency.
Language and Cognitive Processes, 25(4):441?455.
Dale J. Barr. 2001. Trouble in mind: Paralinguistic
indices of effort and uncertainty in communication.
Oralite? and gestualite?: Communication Multimodale,
Interaction, pages 597?600.
Dale J. Barr. 2003. Paralinguistic correlates of con-
ceptual structure. Psychonomic Bulletin & Review,
10(2):462?467.
Hugh Beyer and Karen Holtzblatt. 1997. Contextual De-
sign: Defining Customer-Centered Systems. Morgan
Kaufmann.
Diana Binnenpoorte, Christophe Van Bael, Els den Os,
and Lou Boves. 2005. Gender in everyday speech and
language: A corpus-based study. Interspeech, pages
2213?2216.
Paul Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, pages 341?345.
Heather Bortfeld, Silvia D. Leon, Johnathan E. Bloom,
Michael F. Schober, and Susan E. Brennan. 2001.
Disfluency rates in conversation: Effects of age, re-
lationship, topic, role, and gender. Language and
Speech, 44(2):123?147.
Patricia Hayes Bradley. 1981. The folk-linguistics of
women?s speech: an empirical investigation. Commu-
nication Monographs, 48(1):78?91.
Susan E. Brennan and Maurice Williams. 1995. The
feeling of another?s knowing: Prosody and filled
pauses as cues to listeners about the metacognitive
states of speakers. Journal of Memory and Language,
34:383?398.
Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh
and um in spontaneous speaking. Cognition, 84:73?
111.
Martin Corley and Oliver W. Stewart. 2008. Hesitation
disfluencies in spontaneous speech: The meaning of
um. Lang. and Linguistics Compass, 2(4):589?602.
Jean E. Fox Tree. 1995. The effects of false starts and
repetitions on the processing of subsequent words in
spontaneous speech. Journal of Memory and Lan-
guage, 34:709?738.
Elizabeth Gordon. 1994. Sex differences in language:
Another explanation? American Speech, 69(2):215?
221.
Gail Jefferson. 1989. Notes on a possible metric which
provides for a ?standard maximum? silence of approx-
imately one second in conversation. In Derek Roger
and Peter Bull, editors, Conversation, chapter 8, pages
166?196. Multilingual Matters, Clevedon, UK.
Kim Kirsner, John Dunn, Kathryn Hird, Tim Parkin, and
Craig Clark. 2002. Time for a pause. Proc. of the
9th Australian Int?l. Conf. on Speech Science & Tech.,
pages 52?57.
Tobias Lo?vgren and Jan van Doorn. 2005. Influence of
manipulation of short silent pause duration on speech
fluency. Proceedings of DiSS05, pages 123?126.
Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli,
Jeff Pelz, Pengcheng Shi, and Anne Haake.
Forthcoming-2012. Linking uncertainty in physi-
cians? narratives to diagnostic correctness. Proc. of
the ExProM 2012 Workshop.
Beata Megyesi and Sofia Gustafson-Capkova. 2002.
Production and perception of pauses and their linguis-
tic context in read and spontaneous speech in Swedish.
ICSLP 7.
Roser Morante and Caroline Sporleder. in press. Modal-
ity and negation: An introduction to the special issue.
Computational Linguistics.
Daniel C. O?Connell and Sabine Kowal. 2005. uh
and um revisited: Are they interjections for signal-
ing delay? Journal of Psycholinguistic Research,
34(6):555?576.
Sharon Oviatt. 1995. Predicting and managing spo-
ken disfluencies during human-computer interaction.
Computer Speech and Language, 9:19?35.
Stanley Schachter, Nicholas Christenfeld, Bernard Rav-
ina, and Frances Bilous. 1991. Speech disfluency and
the structure of knowledge. JPSP, 60(3):362?367.
Vicki L. Smith and Herbert H. Clark. 1993. On the
course of answering questions. Journal of Memory
and Language, 32:25?38.
Anna-Marie R. Spinos, Daniel C. O?Connell, and Sabine
Kowal. 2002. An empirical investigation of pause no-
tation. Pragmatics, 12(1):1?9.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The bioscope
corpus: Biomedical texts annotated for uncertainty,
negation, and their scopes. BMC Bioinformatics, 9.
9
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 19?27, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Linking Uncertainty in Physicians? Narratives to Diagnostic Correctness
Wilson McCoy
Department of Interactive
Games and Media
wgm4143@rit.edu
Cecilia Ovesdotter Alm
Department of English
coagla@rit.edu
Cara Calvelli
College of Health
Sciences and Technology
cfcscl@rit.edu
Jeff B. Pelz
Center for
Imaging Science
pelz@cis.rit.edu
Pengcheng Shi
Computing and
Information Sciences
pengcheng.shi@rit.edu
Rochester Institute of Technology
Anne Haake
Computing and
Information Sciences
anne.haake@rit.edu
Abstract
In the medical domain, misdiagnoses and di-
agnostic uncertainty put lives at risk and in-
cur substantial financial costs. Clearly, medi-
cal reasoning and decision-making need to be
better understood. We explore a possible link
between linguistic expression and diagnostic
correctness. We report on an unusual data set
of spoken diagnostic narratives used to com-
putationally model and predict diagnostic cor-
rectness based on automatically extracted and
linguistically motivated features that capture
physicians? uncertainty. A multimodal data
set was collected as dermatologists viewed im-
ages of skin conditions and explained their di-
agnostic process and observations aloud. We
discuss experimentation and analysis in initial
and secondary pilot studies. In both cases,
we experimented with computational model-
ing using features from the acoustic-prosodic
and lexical-structural linguistic modalities.
1 Introduction
Up to 20% of post-mortem diagnoses in the United
States are inconsistent with the diagnosis before
death (Graber, 2005). These misdiagnoses cost both
human lives and estimated millions of dollars every
year. To find where and why misdiagnoses occur, it
is necessary to improve our understanding of doc-
tors? diagnostic reasoning and how it is linked to di-
agnostic uncertainty and correctness. Our contribu-
tion begins to explore the computational modeling
of this phenomenon in diagnostic narratives. From a
cognitive science perspective, we are contributing to
the research on medical reasoning and how it is lin-
guistically expressed. In the long term, this area of
work could be a useful decision-making component
for flagging diagnoses that need further review.
The study used an unusual multimodal data set
collected in a modified Master-Apprentice interac-
tion scenario. It comprises both gaze and linguistic
data. The present study focuses on the linguistic data
which in turn can be conceptualized as consisting of
both acoustic-prosodic and lexical-structural modal-
ities. This data set can further be used to link vision
and language research to understand human cogni-
tion in expert decision-making scenarios.
We report on a study conducted in two phases.
First, an initial pilot study involved a preliminary an-
notation of a small subset of the collected diagnos-
tic narratives and also investigated the prediction of
diagnostic correctness using a set of linguistic fea-
tures from speech recordings and their verbal tran-
scriptions. This provided initial features relevant to
classification, helped us identify annotation issues,
and gave us insight on how to improve the annota-
tion scheme used for annotating ground truth data.
Next, a second pilot study was performed, build-
ing on what was learned in the initial pilot study.
The second pilot study involved a larger data set
with a revised and improved annotation scheme that
considered gradient correctness at different steps of
the diagnostic reasoning process: (1) medical lesion
morphology (e.g. recognizing the lesion type as a
scaly erythematous plaque), (2) differential diagno-
sis (i.e. providing a set of possible final diagnoses),
and (3) final diagnosis (e.g. identifying the disease
condition as psoriasis). We also experiment with
19
classification using an expanded feature set moti-
vated by the initial pilot study and by previously
published research. We report on results that con-
sider different algorithms, feature set modalities, di-
agnostic reasoning steps, and coarse vs. fine grained
classes as explained below in Section 4.3.
2 Previous Work
Much work has been done in the area of medi-
cal decision-making. Pelaccia et al (2011) have
viewed clinical reasoning through the lens of dual-
process theory. They posit that two systems are at
work in the mind of a clinician: the intuitive system
which quickly produces a response based on expe-
rience and a holistic view of the situation, versus
the analytic system which slowly and logically steps
through the problem with conscious use of knowl-
edge. Croskerry (2009) stated that ?[i]f the presen-
tation is not recognized, or if it is unduly ambiguous
or there is uncertainty, [analytic] processes engage
instead? (p. 1022); for instance, if a clinician is un-
familiar with a disease or unsure of their intuitive
answer. We assume that different reasoning systems
may cause changes in linguistic behaviors. For ex-
ample, when engaging the slower analytic system, it
seems reasonable that frequent pausing could appear
as an indication of, e.g., uncertainty or thoughtful-
ness.
Several studies have explored the task of detect-
ing uncertainty through language. Uncertainty de-
tection necessitates inference of extra-propositional
meaning and is arguably a subjective natural lan-
guage problem, i.e. part of a family of problems
that are increasingly receiving attention in compu-
tational linguistics. These problems involve more
dynamic classification targets and different perfor-
mance expectations (Alm, 2011). Pon-Barry and
Shieber (2009) have shown encouraging results in
finding uncertainty using acoustic-prosodic features
at the word, word?s local context, and whole utter-
ance levels. Henriksson and Velupillai (2010) used
?speculative words? (e.g., could, generally, should,
may, sort of, etc.) as well as ?certainty ampli-
fiers? (e.g., definitely, positively, must, etc.) to deter-
mine uncertainty in text. Velupillai (2010) also ap-
plied the same approach to medical texts and noted
that acoustic-prosodic features should be considered
alongside salient lexical-structural features as indi-
cators of uncertainty. In this work, we draw on the
insight of such previous work, but we also extend
the types of linguistic evidence considered for iden-
tifying possible links to diagnostic correctness.
As another type of linguistic evidence, disfluen-
cies make up potentially important linguistic evi-
dence. Zwarts and Johnson (2011) found that the
occurrence of disfluencies that had been removed
could be predicted to a satisfactory degree. Pakho-
mov (1999) observed that such disfluencies are just
as common in monologues as in dialogues even
though there is no need for the speakers to indicate
that they wish to continue speaking. This finding is
important for the work presented here because our
modified use of the Master-Apprentice scenario re-
sults in a particular dialogic interaction with the lis-
tener remaining silent. Perhaps most importantly,
Clark and Fox Tree (2002) postulated that filled
pauses (e.g., um, uh, er, etc.) play a meaningful
role in speech. For example, they may signal that
the speaker is yet to finish speaking or searching for
a word. There is some controversy about this claim,
however, as explained by Corley and Stewart (2008).
The scholarly controversy about the role of disfluen-
cies indicates that more research is needed to under-
stand the disfluency phenomenon, including how it
relates to extra-propositional meaning.
3 Data Set
The original elicitation experiment included 16
physicians with dermatological expertise. Of these,
12 were attending physicians and 4 were residents
(i.e. dermatologists in training). The observers were
shown a series of 50 images of dermatological con-
ditions. The summary of this collected data is shown
in Table 1, with reference to the pilot studies.
The physicians were instructed to narrate, in En-
glish, their thoughts and observations about each im-
age to a student, who remained silent, as they arrived
at a differential diagnosis or a possible final diagno-
sis. This data elicitation approach is a modified ver-
sion of the Master-Apprentice interaction scenario
(Beyer and Holtzblatt, 1997). This elicitation setup
is shown in Figure 1. It allows us to extract in-
formation about the Master?s (i.e. in this case, the
physician?s) cognitive process by coaxing them to
20
Data parameters Quantity
# of participating doctors 16
# of images for which
narratives were collected 50
# of time-aligned narratives
in the initial pilot study 160
# of time-aligned narratives
in the second pilot study 707
Table 1: This table summarizes the data. Of the collected
narratives, 707 are included in this work; audio is unavail-
able for some narratives.
vocalize their thoughts in rich detail. This teaching-
oriented scenario really is a monologue, yet induces
a feeling of dialogic interaction in the Master.
Figure 1: The Master-Apprentice interaction scenario al-
lows us to extract information about the Master?s (here:
doctor?s) cognitive processes.
The form of narratives collected can be analyzed
in many ways. Figure 2 shows two narratives, re-
cently elicited and similar to the ones in the study?s
data set, that are used here with permission as ex-
amples. In terms of diagnostic reasoning styles, re-
ferring to Pelaccia et al (2011), we can propose that
observer A may be using the intuitive system and
that observer B may be using the analytical system.
Observer A does not provide a differential diagnosis
and jumps straight to his/her final diagnosis, which
in this case is correct. We can postulate that observer
A looks at the general area of the lesion and uses
previous experience or heuristic knowledge to come
to the correct diagnosis. This presumed use of the
intuitive system could potentially relate to the depth
of previous experience with a disease, for example.
Observer B, on the other hand, might be using the
A. This patient has a pinkish papule with
surrounding hypopigmentation in a field of
other cherry hemagiomas and nevoid type
lesions. The only diagnosis that comes to
mind to me is Sutton?s nevus.
B. I think I?m looking at an abdomen, possibly.
I see a hypopigmented oval-shaped patch in
the center of the image. I see that there
are two brown macules as well. In the center
of the hypopigmented oval patch there
appears to be an area that may be a pink
macule. Differential diagnosis includes
halo nevus, melanoma, post-inflammatory
hypopigmentation. I favor a diagnosis of
maybe post-inflammatory hypopigmentation.
Figure 2: Two narratives collected in a recent elicitation
setup and used here with permission. Narratives A and B
are not part of the studied data set, but exemplify data set
narratives which could not be distributed. Observers A
and B are both looking at an image of a halo or Sutton?s
nevus as seen in Figure 3. Disfluencies are considered in
the experimental work but have been removed for read-
ability in these examples.
Figure 3: The image of a halo or Sutton?s nevus viewed
by the observers and the subject of example narratives.
analytical system. Observer B steps through the di-
agnosis in a methodical process and uses evidence
presented to rationalize the choice of final diagno-
sis. Observer B also provides a differential diagno-
sis unlike observer A. This suggests that observer
B is taking advantage of a process of elimination to
decide on a final diagnosis.
Another way to evaluate these narratives is in
terms of correctness and the related concept of diag-
21
nostic completeness. Whereas these newly elicited
narrative examples have not been annotated by doc-
tors, some observations can still be made. From the
point of view of final diagnosis, observer A is cor-
rect, unlike observer B. Assessment of diagnostic
correctness and completeness can also be made on
intermediate steps in the diagnostic process (e.g. dif-
ferential diagnoses or medical lesion morphological
description). Including such steps in the diagnos-
tic process is considered good practice. Observer A
does not supply a differential diagnosis and instead
skips to the final diagnosis. Observer B provides
the correct answer in the differential diagnosis but
gives the incorrect final diagnosis. Observer B fully
describes the medical lesion morphology presented.
Observer A, however, only describes the pink lesion
and does not discuss the other two brown lesions.
The speech of the diagnostic narratives was
recorded. At the same time, the observers? eye-
movements were tracked; the eye-tracking data
are considered in another report (Li et al, 2010).
We leave the integration of the linguistic and eye-
tracking data for future work.
After the collection of the raw audio data, the
utterances were manually transcribed and time-
aligned at the word level with the speech anal-
ysis tool Praat (Boersma, 2001).1 A sample of
the transcription process output is shown in Fig-
ure 4. Given our experimental context, off-the-shelf
automatic speech recognizers could not transcribe
the narratives to the desired quality and resources
were not available to create our own automatic tran-
1See http://www.fon.hum.uva.nl/praat/.
Figure 4: Transcripts were time-aligned in Praat which
was also used to extract acoustic-prosodic features.
scriber. Manual transcription also preserved disflu-
encies, which we believe convey meaningful infor-
mation. Disfluencies were transcribed to include
filled pauses (e.g. uh, um), false starts (e.g. pur-
reddish purple), repetitions, and click sounds.
This study is strengthened by its involvement of
medical experts. Trained dermatologists were re-
cruited in the original elicitation experiment as well
as the creation and application of both annotation
schemes. This is crucial in a knowledge-rich domain
such as medicine because the annotation scheme
must reflect the domain knowledge. Another study
reports on annotation details (McCoy et al, Forth-
coming 2012).
4 Classification Study
This section discusses the classification work, first
explaining the methodology for the initial pilot study
followed by interpretation of results. Next, the
methodology of the second pilot study is described.
4.1 Generic Model Overview
This work applies computational modeling de-
signed to predict diagnostic correctness in physi-
cians? narratives based on linguistic features from
the acoustic-prosodic and lexical-structural modali-
ties of language, shown in Table 2. Some tests dis-
cussed in 4.2 and 4.3 were performed with these
modalities separated. These features are inspired
by previous work conducted by Szarvas (2008),
Szarvas et al (2008), Litman et al (2009), Liscombe
et al (2005), and Su et al (2010).
We can formally express the created model in the
following way: Let ni be an instance in a set of nar-
ratives N , let j be a classification method, and let
li be a label in a set of class labels L. We want to
establish a function f(ni, j) : li where li is the label
assigned to the narrative based on linguistic features
from a set F , where F = f1, f2, ...fk, as described
in Table 2. The baseline for each classifier is de-
fined as the majority class ratio. Using scripts in
Praat (Boersma, 2001), Python, and NLTK (Bird et
al., 2009), we automatically extracted features for
each narrative. Each narrative was annotated with
multiple labels relating to its diagnostic correctness.
The labeling schemes used in the initial and second
pilot studies, respectively, are described in subsec-
22
tions 4.2 and 4.3.
4.2 Initial Pilot Study
The initial pilot classification study allowed the op-
portunity to refine the prediction target annotation
scheme, as well as to explore a preliminary set of lin-
guistic features. 160 narratives were assigned labels
Linguistic Feature at the narrative level
Modality
Acoustic- Total duration
prosodic Percent silence
Time silent
# of silences *
Time speaking
# of utterances *
Initial silence length
F0 mean (avg. pitch) ?
F0 min (min. pitch) ?
F0 max (max. pitch) ?
dB mean (avg. intensity) ?
dB max (max. intensity) ?
Lexical- # of words
structural words per minute
# of disfluencies ?
# of certainty amplifiers * ?
# of speculative words * ?
# of stop words * ?
# of content words * ?
# of negations * ?
# of nouns ?
# of verbs ?
# of adjectives ?
# of adverbs ?
Unigram of tokens
Bigram of tokens
Trigram of tokens
Table 2: Features used by their respective modalities.
Features marked with a * were only included in the sec-
ond pilot study. Features marked with ? were included
twice; once as their raw value and again as a z-score nor-
malized to its speaker?s data in the training set. Features
marked with ?were also included twice; once as their raw
count and again as their value divided by the total number
of words in that narrative. Disfluencies were considered
as words towards the total word count, silences were not.
No feature selection was applied.
of correct or incorrect for two steps of the diagnos-
tic process: diagnostic category and final diagno-
sis. These annotations were done by a dermatologist
who did not participate in the elicitation study (co-
author Cara Calvelli). For final diagnosis, 70% were
marked as correct, and for diagnostic category, 80%
were marked as correct. An outcome of the anno-
tation study was learning that the initial annotation
scheme needed to be refined. For example, diagnos-
tic category had a fuzzy interpretation, and correct-
ness and completeness of diagnoses are found along
a gradient in medicine. This led us to pursue an im-
proved annotation scheme with new class labels in
the second pilot study, as well as the adoption of a
gradient scale of correctness.
For the initial pilot study, basic features were ex-
tracted from the diagnostic narratives in two modal-
ities: acoustic-prosodic and lexical-structural (see
Table 2). To understand the fundamental aspects
of the problem, the initial pilot study experimented
with the linguistic modalities separately and to-
gether, using three foundational algorithms, as im-
plemented in NLTK (Naive Bayes, Maximum En-
tropy, Decision Tree), and a maximum vote classi-
fier based on majority consensus of the three basic
classifiers. The majority class baselines were 70%
for diagnosis and 80% for diagnostic category. The
small pilot data set was split into an 80% training set
and a 20% testing set. The following results were
obtained with the maximum vote classifier.
Utilizing only acoustic-prosodic features, the
maximum vote classifier performed 5% above the
baseline when testing final diagnosis and 6% below
it for diagnostic category. F0 min and initial silence
length appeared as important features. This initial si-
lence length could signal that the observers are able
to glean more information from the image, and us-
ing this information, they can make a more accurate
diagnosis.
Utilizing only lexical-structural features, the
model performed near the baseline (+1%) for final
diagnosis and 9% better than the baseline for diag-
nostic category. When combining acoustic-prosodic
and lexical-structural modalities, the majority vote
classifier performed above the baseline by 5% for fi-
nal diagnosis and 9% for diagnostic category. We
are cautious in our interpretation of these findings.
For example, the small size of the data set and the
23
particulars of the data split may have guided the re-
sults, and the concept of diagnostic category turned
out to be fuzzy and problematic. Nevertheless, the
study helped us refine our approach for the second
pilot study and redefine the annotation scheme.
4.3 Second Pilot Study
For the second pilot study, we hoped to gain further
insight into primarily two questions: (1) How accu-
rately do the tested models perform on three steps of
the diagnostic process, and what might influence the
performance? (2) In our study scenario, is a certain
linguistic modality more important for the classifi-
cation problem?
The annotation scheme was revised according to
findings from the initial pilot study. These revisions
were guided by dermatologist and co-author Cara
Calvelli. The initial pilot study scheme only anno-
tated for diagnostic category and final diagnosis. We
realized that diagnostic category was too slippery of
a concept, prone to misunderstanding, to be useful.
Instead, we replaced it with two new and more ex-
plicit parts of the diagnostic process: medical lesion
morphology and differential diagnosis.
For final diagnosis, the class label options of cor-
rect and incorrect could not characterize narratives
in which observers had not provided a final diag-
nosis. Therefore, a third class label of none was
added. New class labels were also created that cor-
responded to the diagnostic steps of medical lesion
morphology and differential diagnosis. Medical le-
sion morphology, which is often descriptively com-
plex, allowed the label options correct, incorrect,
and none, as well as correct but incomplete to deal
with correct but under-described medical morpholo-
gies. Differential diagnosis considered whether or
not the final diagnosis appeared in the differential
and thus involved the labels yes, no, and no differ-
ential given. Table 3 summarizes the refined anno-
tation scheme.
The examples in Figure 2 above can now be ana-
lyzed according to the new annotation scheme. Ob-
server A has a final diagnosis which should be la-
beled as correct but does not give a differential diag-
nosis, so the differential diagnosis label should be no
differential given. Observer A also misses parts of
the morphological description so the assigned med-
ical lesion morphology would likely be correct but
incomplete. Observer B provides what seems to be
a full morphological description as well as lists the
correct final diagnosis in the differential diagnosis,
yet is incorrect regarding final diagnosis. This narra-
tive?s labels for medical lesion morphology and dif-
ferential diagnosis would most likely be correct and
yes respectively. Further refinements may turn out
useful as the data set expands.
Diagnostic step Possible labels Count Ratio
Medical Correct 537 .83
Lesion Incorrect 36 .06
Morphology None Given 40 .06
Incomplete 32 .05
Differential Yes 167 .24
Diagnosis No 101 .14
No Differential 434 .62
Final Correct 428 .62
Diagnosis Incorrect 229 .33
None Given 35 .05
Table 3: Labels for various steps of the diagnostic process
as well as their count and ratios of the total narratives, af-
ter eliminating those with no annotator agreement. These
labels are explained in section 4.3.
Three dermatologists annotated the narratives, as-
signing a label of correctness for each step in the
diagnostic process for a given narrative. Table 3
shows the ratios of labels in the collected annota-
tions. Medical lesion morphology is largely correct
with only smaller ratios being assigned to other cat-
egories. Secondly, a large ratio of narratives were
assigned no differential given but of those that did
provide a differential diagnosis, the correct final di-
agnosis was more likely to be included than not. Re-
garding final diagnosis, a label of correct was most
often assigned and few narratives did not provide
any final diagnosis. These class imbalances, exist-
ing at each level, indicated that the smaller classes
with fewer instances would be quite challenging for
a computational classifier to learn.
Any narrative for which there was not agreement
for at least 2 of the 3 dermatologists in a diagnostic
step was discarded from the set of narratives consid-
ered in that diagnostic step.2
2Because narratives with disagreement were removed, the
total numbers of narratives in the experiment sets differ slightly
on the various step of the diagnostic process.
24
Comparing classification in terms of algorithms,
diagnostic steps, and individual classes
Weka (Witten and Frank, 2005)3 was used with
four classification algorithms, which have a widely
accepted use in computational linguistics.4
Standard performance measures were used to
evaluate the classifiers. Both acoustic-prosodic and
lexical-structural features were used in a leave-one-
out cross-validation scenario, given the small size of
the data set. The results are shown in Table 4. Ac-
curacy is considered in relation to the majority class
baseline in each case. With this in mind, the high
accuracies found when testing medical lesion mor-
phology are caused by a large class imbalance. Dif-
ferential diagnosis? best result is 5% more accurate
than its baseline while final diagnosis and medical
lesion morphology are closer to their baselines.
Final Dx Diff. Dx M. L. M.
Baseline .62 .62 .83
C4.5 .57 .62 .77
SVM .63 .67 .83
Naive Bayes .55 .61 .51
Log Regression .53 .64 .66
Table 4: Accuracy ratios of four algorithms (implemented
in Weka) as well as diagnostic steps? majority class base-
lines. Experiments used algorithms? default parameters
for final diagnosis (3 labels), differential diagnosis (3 la-
bels), and medical lesion morphology (4 labels) using
leave-one-out cross-validation.
In all scenarios, the SVM algorithm reached or
exceeded the majority class baseline. For this rea-
son, other experiments used SVM. The results for
the SVM algorithm when considering precision and
recall for each class label, at each diagnostic step,
are shown in Table 5. Precision is calculated as the
number of true positives for a given class divided by
the number of narratives classified as the given class.
Recall is calculated as the number of true positives
for a given class divided by the number of narra-
tives belonging to the given class. As Table 5 shows,
and as expected, labels representing large propor-
tions were better identified than labels representing
3See http://www.cs.waikato.ac.nz/ml/weka/.
4In this initial experimentation, not all features used were
independent, although this is not ideal for some algorithms.
Dx step Labels Precision Recall
Medical Correct .83 .99
Lesion Incorrect 0 0
Morphology None Given 0 0
Incomplete 0 0
Differential Yes .49 .44
Diagnosis No .26 .10
No Diff. .76 .89
Final Correct .67 .84
Diagnosis Incorrect .32 .47
None Given 0 0
Table 5: Precision and recall of class labels. These were
obtained using the Weka SVM algorithm with default pa-
rameters using leave-one-out cross-validation. These cor-
respond to the experiment for SVM in Table 4.
Final Diagnosis Diff. Diagnosis
Baseline .62 .62
Lex.-struct. .62 .67
Acous.-pros. .65 .62
All .63 .67
Table 6: Accuracy ratios for various modalities. Tests
were performed for final diagnosis and differential diag-
nosis tags with Weka?s SVM algorithm using a leave-
out-out cross-validation method. Lexical-structural and
acoustic-prosodic cases used only features in their respec-
tive set.
intermediate proportions, and classes with few in-
stances did poorly.
Experimentation with types of feature
To test if one linguistic modality was more impor-
tant for classification, experiments were run in each
of three different ways: with only lexical-structural
features, with only acoustic-prosodic features, and
with all features. We considered the final diagnosis
and differential diagnosis scenarios. It was decided
not to run this experiment in terms of medical lesion
morphology because of its extreme class imbalance
with a high baseline of 83%. Medical lesion mor-
phology also differs in being a descriptive step un-
like the other two which are more like conclusions.
Again, a leave-one-out cross-validation method was
used. The results are shown in Table 6.
These results show that, regarding final diagnosis,
considering only acoustic-prosodic features seemed
25
to yield somewhat higher accuracy than when fea-
tures were combined. This might reflect that, con-
ceptually, final diagnosis captures a global end step
in the decision-making process, and we extracted
voice features at a global level (across the narrative).
In the case of differential diagnosis, the lexical-
structural features performed best, matching the ac-
curacy of the combined feature set (5% over the ma-
jority class baseline). Future study could determine
which individual features in these sets were most im-
portant.
Experiments with alternative label groupings for
some diagnostic steps
Another set of experiments examined perfor-
mance for adjusted label combinations. To learn
more about the model, experiments were run in
which selected classes were combined or only cer-
tain classes were considered. The class proportions
thus changed due to the combinations and/or re-
moval of classes. This was done utilizing all fea-
tures, the Weka SVM algorithm, and a leave-one-
out methodology. Only logically relevant tests that
increased class balance are reported here.5
An experiment was run on the differential diagno-
sis step. The no differential given label was ignored
to allow the binary classification of narratives that
included differential diagnoses. The new majority
class baseline for this test was 62% and this classi-
fication performed 1% over its baseline. A similar
experiment was run on the final diagnosis diagnos-
tic step. Class labels of incorrect and none given
were combined to form binary set of class labels
with a 62% baseline. This classification performed
6% over the baseline, i.e., slightly improved perfor-
mance compared to the scenario with three class la-
bels.
5 Conclusion
In these pilot studies, initial insight has been gained
regarding the computational linguistic modeling of
extra-propositional meaning but we acknowledge
that these results need to be confirmed with new
data.
This paper extracted features, which could pos-
sibly relate to uncertainty, at the global level of a
5Other experiments were run but are not reported because
they have no use in future implementations.
narrative to classify correctness of three diagnostic
reasoning steps. These steps are in essence local
phenomena and a better understanding of how un-
certainty is locally expressed in the diagnostic pro-
cess is needed. Also, this work does not consider
parametrization of algorithms or the role of feature
selection. In future work, by considering only the
features that are most important, a better understand-
ing of linguistic expression in relation to diagnostic
correctness could be achieved, and likely result in
better performing models. One possible future adap-
tation would be the utilization of the Unified Medi-
cal Language System to improve the lexical features
used Woods et al (2006).
Other future work includes integrating eye move-
ment data into prediction models. The gaze modal-
ity informs us as to where the observers were look-
ing when they were verbalizing their diagnostic pro-
cess. We can thus map the narratives to how gaze
was positioned on an image. Behavioral indicators
of doctors? diagnostic reasoning likely extend be-
yond language. By integrating gaze and linguistic
information, much could be learned regarding per-
ceptual and conceptual knowledge.
Through this study, we have moved towards un-
derstanding reasoning in medical narratives, and we
have come one step closer to linking the spoken
words of doctors to their cognitive processes. In a
much more refined, future form, certainty or cor-
rectness detection could become useful to help un-
derstanding medical reasoning or help guide medi-
cal reasoning or detect misdiagnosis.
Acknowledgements
This research supported by NIH 1 R21 LM010039-
01A1, NSF IIS-0941452, RIT GCCIS Seed Fund-
ing, and RIT Research Computing (http://rc.rit.edu).
We would like to thank Lowell A. Goldsmith, M.D.
and the anonymous reviewers for their comments.
References
Cecilia Ovesdotter Alm. 2011. Subjective Natural Lan-
guage Problems: Motivations, Applications, Charac-
terizations, and Implications. Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 107?112.
26
Hugh Beyer and Karen Holtzblatt. 1997. Contextual De-
sign: Defining Customer-Centered Systems. Morgan
Kaufmann.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia.
Paul Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, pages 341?345.
Herbert Clark and Jean Fox Tree. 2002. Using uh and um
in spontaneous speaking. Cognition, pages 73?111.
Martin Corley and Oliver Stewart. 2008. Hesitation dis-
fluencies in spontaneous speech: The meaning of um.
Language and Linguistics Compass, 5(2):589?602.
Pat Croskerry. 2009. A universal model of diagnostic
reasoning. Academic Medicine, pages 1022?1028.
Mark Graber. 2005. Diagnostic errors in medicine: A
case of neglect. The Joint Commission Journal on
Quality and Patient Safety, pages 106?113.
Aron Henriksson and Sumithra Velupillai. 2010. Levels
of certainty in knowledge-intensive corpora: An ini-
tial annotation study. Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, pages 41?45.
Rui Li, Preethi Vaidyanathan, Sai Mulpuru, Jeff Pelz,
Pengcheng Shi, Cara Calvelli, and Anne Haake. 2010.
Human-centric approaches to image understanding
and retrieval. Image Processing Workshop, Western
New York, pages 62?65.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. Proceedings of Interspeech, pages 1837?
1840.
Diane Litman, Mihail Rotaru, and Greg Nicholas. 2009.
Classifying turn-level uncertainty using word-level
prosody. Proceedings of Interspeech, pages 2003?
2006.
Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli,
Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake.
Forthcoming-2012. Annotation schemes to encode
domain knowledge in medical narratives. Proceedings
of the Sixth Linguistic Annotation Workshop.
Sergey Pakhomov. 1999. Modeling filled pauses in med-
ical dictations. Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, pages 619?624.
Thierry Pelaccia, Jacques Tardif, Emmanuel Triby, and
Bernard Charlin. 2011. An analysis of clinical rea-
soning through a recent and comprehensive approach:
the dual-process theory. Medical Education Online,
16:5890.
Heather Pon-Barry and Stuart Shieber. 2009. The im-
portance of sub-utterance prosody in predicting level
of certainty. Proceedings of NAACL HLT, pages 105?
108.
Qi Su, Chu-Ren Huang, and Helen Kai-yun Chen. 2010.
Evidentiality for text trustworthiness detection. Pro-
ceedings of the 2010 Workshop on NLP and Linguis-
tics: Finding the Common Ground ACL 2010, pages
10?17.
Gyorgy Szarvas, Veronika Vincze, Richard Farkas, and
Janos Csirik. 2008. The bioscope corpus: annotation
for negation, uncertainty and their scope in biomedical
texts. BioNLP 2008: Current Trends in Biomedical
Natural Language Processing, pages 38?45.
Gyorgy Szarvas. 2008. Hedge classification in biomed-
ical texts with a weakly supervised selection of key-
words. Proceedings of 46th Annual Meeting of the
Association of Computational Linguistics, pages 281?
289.
Sumithra Velupillai. 2010. Towards a better understand-
ing of uncertainties and speculations in Swedish clin-
ical text - analysis of an initial annotation trial. Pro-
ceedings of the Workshop on Negation and Speculation
in Natural Language Processing, pages 14?22.
Ian H.Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
James Woods, Charles Sneiderman, Karam Hameed,
Michael Ackerman, and Charlie Hatton. 2006. Using
umls metathesaurus concepts to describe medical im-
ages: dermatology vocabulary. Computers in Biology
and Medicine 36, pages 89?100.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics,
pages 703?711.
27
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 11?16,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Computational analysis to explore authors? depiction of characters
Joseph Bullard
Dept. of Computer Science
Rochester Institute of Technology
jtb4478@cs.rit.edu
Cecilia Ovesdotter Alm
Dept. of English
Rochester Institute of Technology
coagla@rit.edu
Abstract
This study involves automatically identi-
fying the sociolinguistic characteristics of
fictional characters in plays by analyz-
ing their written ?speech?. We discuss
three binary classification problems: pre-
dicting the characters? gender (male vs.
female), age (young vs. old), and socio-
economic standing (upper-middle class vs.
lower class). The text corpus used is
an annotated collection of August Strind-
berg and Henrik Ibsen plays, translated
into English, which are in the public do-
main. These playwrights were chosen for
their known attention to relevant socio-
economic issues in their work. Linguis-
tic and textual cues are extracted from the
characters? lines (turns) for modeling pur-
poses. We report on the dataset as well
as the performance and important features
when predicting each of the sociolinguis-
tic characteristics, comparing intra- and
inter-author testing.
1 Introduction
A speech community has sociolinguistic proper-
ties. Social variables influencing verbal inter-
action include, for example, geographical back-
ground, gender, age, ethnicity, and class. Writ-
ers and playwrights, in turn, use their knowledge
of social verbal markers to generate credible and
compelling characters. The focus of this study is
the creation of an annotated dataset and computa-
tional model for predicting the social-biographical
aspects of fictional characters based on features
of their written ?speech? in dramatic plays. The
plays used here are authored by August Strind-
berg and Henrik Ibsen, two Scandinavian play-
wrights known for creating characters and stories
that acted as social commentary and were contro-
versial when they were first written. These authors
are also recognized for their contributions in shap-
ing modern drama. Their attention to social issues
makes these plays and characters highly relevant
in constructing such a model to shed light on how
these authors? translated texts portray social vari-
ables. Interlocutors? social attributes (such as their
gender, age, social class, and ethnicity) are known
to correlate with language behavior, and they tap
into dimensions of language behavior that are of
central interest to the humanities. For instance,
anecdotal evidence suggests that large-scale cor-
pus analysis can show how society collectively as-
cribes certain roles to male versus female referents
in text (cf. Lindquist, 2009).
Studying these authors and texts from the point
of view of corpus-oriented computational soci-
olinguistics can also help us examine the authors?
differences in production, descriptively. This is
useful as a complementary approach to the more
traditional close reading methodology common in
literary research, through which their texts are
usually approached. On a broader scale, the study
can contribute valuable insights to a theory of lin-
guistic text criticism. These authors are part of a
global literary canon, and their plays are arguably
more often performed in translation than in their
Scandinavian originals. Accordingly, we focus on
analyzing texts translated into English.
We focus on sociolinguistic characteristics that
are assigned to each character and that can be
described as translating into three binary classifi-
cation problems: predicting the characters? gen-
der (male vs. female), age (young vs. old), and
socioeconomic standing or class (upper-middle
class vs. lower class). The text corpus is
annotated by assigning each of the characters
that match specified criteria a value in each of
the characteristics. We do this at the charac-
ter level, joining all dialogic lines of a charac-
ter into one instance. The work was accom-
plished through the use of computational tools
11
for natural language processing, including Python
(http://www.python.org/), the Natural Language
Toolkit (http://www.nltk.org/) for part of the pre-
processing, and the scikit-learn machine
learning library for the computational modeling.
Translated texts that reside in the public do-
main were collected from the Gutenberg Archive
(http://www.gutenburg.org/wiki/Main Page/).
2 Previous Work
A pilot study by Hota et al. (2006) on automatic
gender identification in Shakespeare?s texts, as
well as a few primarily gender-oriented studies
surveyed in Garera and Yarowsky (2009), have set
the stage for further inquiry. The latter study ex-
panded on previous work by exploring three at-
tributes: gender, age, and native/non-native speak-
ers. There have been previous avenues of re-
search into categorizing speakers based on differ-
ent individual sociolinguistic factors. However,
not many studies have attempted this categoriza-
tion with fictional characters. Literary texts are
complex, reflecting authors? decision-making and
creative processes. From the perspective of digi-
tal humanities, such a focus complements compu-
tational sociolinguistic modeling of contemporary
user-generated text types (such as emails, or blogs
(Rosenthal and McKeown, 2011)). As Lindquist
(2009) points out, social data for interlocutors is
less often attached to openly available linguistic
corpora, and interest is strong in developing cor-
pus methods to help explore social language be-
havior (see Lindquist (2009) and Baker (2010)).
Previous investigation into social dimensions of
language has established strong links between lan-
guage and social attributes of speech communi-
ties (for an overview, see Mesthrie et al. (2009)).
However, such inquiry has generally had a firm
foundation in field-based research and has usually
focused on one or just a few linguistic variables
(such as how the pronunciation of certain sounds
aligns with social stratification (Labov, 1972)).
Moreover, previous scholarship has chiefly fo-
cused on the spoken rather than the written mode.
Garera and Yarowsky (2009) and Boulis and Os-
tendorf (2005) take into account the interlocutors?
speech for analysis. In contrast, we experiment
with the challenge of using only sociolinguisti-
cally relevant knowledge coded in the text of char-
acters? lines. Thus, our approach is more simi-
lar to Hota et al.?s (2006) work on Shakespeare.
The characters? lines do not include the metadata
needed for considering spoken features, since usu-
ally these are added at the discretion of the per-
former. This may make our problem more chal-
lenging, since some of these indicators may be
reliable for identifying gender, such as backchan-
nel responses and affirmations from females, and
assertively ?holding the floor? with filled pauses
from males (Boulis and Ostendorf, 2005). More-
over, there are prosodic features that clearly dif-
fer between males and females due to physical
characteristics (e.g. F
0
, predominant for pitch per-
ception). We do not take advantage of acous-
tic/prosodic cues in this work. Our text is also
artificial discourse, as opposed to natural speech;
therefore these characters? lines may rather ex-
press how writers choose to convey sociolinguistic
attributes of their characters.
In terms of features, we have explored observa-
tions from previous studies. For instance, com-
mon lexical items have been shown successful,
with males tending to use more obscenities, espe-
cially when talking to other males (Boulis and Os-
tendorf, 2005), and females tending to use more
third-person pronouns. Phrases also tended to be
more useful than unigrams, though whether the
commonly-used words tend to be content-bearing
remains a question according to Boulis and Os-
tendorf (2005). Tackling another form of text,
Kao and Jurafksy (2012) examined the statisti-
cal properties of 20th century acknowledged ver-
sus amateur poets in terms of style and content
substance, finding, for example, that lexical afflu-
ence and properties coherent with imagism, as an
aesthetic theorized ideal, distinguished contempo-
rary professionals? poetics, while sound phenom-
ena played a lesser role, and amateurs preferred
the use of more explicit negative vocabulary than
professionals. In our study, we focus on data col-
lection, corpus analysis, and exploratory experi-
mentation with classification algorithms.
3 Data
The texts used were freely available transcriptions
from the Gutenberg Archive. English transla-
tions of public-domain plays by August Strindberg
and Henrik Ibsen were collected from the archive,
from various translators and years of release. As
noted above, these plays are often performed in
English, and we assume that the translations will
convey relevant linguistic cues, as influenced by
12
Strindberg Ibsen Total
# of plays 11 12 23
# of characters 65 93 158
# of lines 6555 12306 18861
Table 1: Distribution of plays, characters, and
lines between Strindberg and Ibsen in the dataset.
Character Gender Age Class
Christine Female Young Upper
Jean Male Young Lower
Miss Julia Female Young Lower
Table 2: Example annotations from Miss Julia.
authors, as well as translators. We assume that
the translators intended to replicate as closely as
possible the voice of the original author, as this is
generally the function of literary translation, but
we recognize the potential for loss of information.
The texts were minimally pre-processed (such
as removing licensing and introduction text), leav-
ing only the written lines-to-be-spoken of the char-
acters. Each character?s lines were automatically
extracted and aggregated using a Python script.
Characters should have a significant number of
lines (equal to or greater than fifteen in his or her
respective play) to be considered.
1
We also record
metadata per character, such as the play title, the
play translator, and the URL of the original play
text on Gutenberg. The basic characteristics of the
resulting dataset are shown in Table 1.
In terms of annotation, characters from each
play were annotated by a third party and assigned
characteristics primarily according to the plot de-
scriptions on Wikipedia of their respective plays of
origin. The characteristics considered were gen-
der (male vs. female), age (young vs. old), and so-
cioeconomic standing or class (upper-middle class
vs. lower class). For example, for age, characters
with children are considered old, and those chil-
dren are considered young. A childless character
whose peers have children or who has experienced
life-changing events typically associated with age
(e.g. widows/widowers) is also old, unless sepa-
rately noted otherwise. The gender annotations
were validated by a project-independent person
1
The only exception to this rule is Mrs. X from Strind-
berg?s The Stronger. She has only 11 separate ?lines?, but
also has the only speaking part for the entire play, which is a
single act of substantial length. We also note that while an ad
hoc threshold for lines was used, future work could explore
principled ways to set it.
Attribute Annotation Strindberg Ibsen
Gender Male / Female 42 / 23 61 / 32
Age Old / Young 46 / 19 61 / 32
Class Upper / Lower 57 / 8 83 / 10
Table 3: Character attribute distributions for gen-
der, age, and class for each author.
in Scandinavia (Swedish native speaker) based on
her knowledge of Scandinavian naming conven-
tions. Example character annotations for Strind-
berg?s well-known naturalistic play Miss Julia (or
Miss Julie) are shown in Table 2. As seen in Table
3, the imbalance of class labels presents the great-
est problem for our model. Baselines of 88% and
89% upper class for Strindberg and Ibsen, respec-
tively, indicate that there may be less information
to be extracted for class.
4 Models
Here we describe the design and performance of
computational models for predicting a character?s
gender, age, and class for Strindberg and Ibsen,
yielding six models in total. Logistic regression,
implemented in Python with the scikit-learn
machine learning library (Pedregosa et al., 2011),
is used for all classification models.
4.1 Feature Extraction
Many features were examined, some inspired
by previous analyses in the literature, such as
type-token ratio, subordinate clauses, and wh-
questions, as well as some exploratory features,
such as honorific terms of address. A full list
of the features examined is shown in Table 4.
All features were automatically extracted using
Python. We use honorifics here to mean com-
mon formal terms of address during the time pe-
riod (sir, madam, lord, Mr., Mrs., etc.). It seems
intuitive that such terms may be used differently
based on class or possibly age (e.g. lower class
using more higher terms of address when speaking
to their superiors). We use family words to mean
anything that indicates a familial relationship (fa-
ther, daughter, nephew, etc.). The use of such
words may be affected by gender roles (Hota et al.,
2006). Part-of-speech tagging was accomplished
using the Natural Language Toolkit (NLTK) (Bird
et al., 2009).
13
Linguistic features
Family words
Honorifics
Pronouns 1st
Pronouns 2nd
Pronouns 3rd
Pronouns all
Wh- questions
Type-token ratio
Determiners
Adjectives
Prepositions
For/with
Modals
Personal pronouns
Nouns singular
Nouns plural
Verbs past
Verbs past part.
Verbs sing. pres. non-3rd
Mean line length
Number of lines
% short lines (?5 words)
Table 4: List of linguistic features examined for
the models. All features, with the exception of the
last three in the right column, were measured once
as raw counts and once as the fraction of the over-
all words for a given character.
4.2 Cross-Author Validation
We compared translations of Strindberg and Ib-
sen?s use of language to convey sociolinguistic
attributes. This was done for each of the three
attributes of interest (gender, age, and class) by
training one model for each author, then using it to
classify the other author?s characters. We accom-
plish this by defining a cross-author validation
procedure, a variation of the standard k-fold cross-
validation procedure in which the trained model in
each fold is used to predict both its own test set
and the test set of the other author. This proce-
dure is explained visually in Figure 1. The pro-
cedure is especially interesting as these two au-
thors were contemporaries and dealt with topics of
social commentary in their works, although from
their own perspectives.
The results of cross-author validation are shown
in Table 5 as a matrix where the row is the au-
thor used for training, the column is the author
used for testing, and the value inside a cell is
the average accuracy over all iterations of cross-
author validation. Majority class baselines are also
shown. As expected, the models for each author?s
texts were better at predicting themselves than the
other author, with a couple of exceptions. For
age, the Strindberg-trained model was still able to
improve on Ibsen?s baseline, but not vice versa.
One possible explanation could be that common
features between their depictions of age might be
more useful for one author than the other. An-
other interesting exception is in the class models
Test
Test
I
S
Train
Train
Figure 1: Example of one fold of cross-author
validation for Strindberg (S) and Ibsen (I). Ar-
rows indicate testing. Each author has its own 5-
fold cross-validation, but in each fold, the trained
model is tested on both its own test set and the test
set of the other author.
Gender Age Class
S I S I S I
Strindberg (S) 68 60 74 70 89 90
Ibsen (I) 61 67 70 74 91 90
Baseline 65 66 71 66 88 89
Table 5: Results of cross-author validation (see
Figure 1). Rows are the author used for training,
columns are the author used for testing, and the
value in the cell is the average accuracy over 500
iterations of 5-fold cross-validation. Accuracies
above majority class baselines are shown in bold.
for both authors, which performed slightly above
high baselines for the opposite authors as well as
their own. While class improvements are recog-
nizably marginal (and not claimed to be signifi-
cant), these results might indicate that the two au-
thors? translated texts are using similar character-
istics to convey social class of their characters. It is
important to note that the baselines for class were
extremely high, making prediction of this attribute
more difficult. At least in the intra-author testing,
the gender and age models were generally able to
improve accuracy over their respective baselines
more so than the class models, with age being the
best overall.
4.3 Comparison of Useful Features
Since the experimentation used a linear model
(logistic regression), we can inspect the coeffi-
cients/weights of a trained classifier to determine
which features contributed particularly to the clas-
sification. The absolute value of a coefficient in-
dicates how influential its feature is, and the sign
(+/-) of the coefficient indicates which class the
feature is associated with. During cross-author
14
Strindberg Ibsen
Gender Pronouns 3rd
Honorifics
Determiners
Female
Female
Male
Pronouns 3rd
Family words
Modals
Female
Female
Male
Age Nouns singular
Family words
Modals
Old
Young
Young
Family words
Verbs sing. pres. non-3rd
Prepositions
Young
Young
Old
Class For/with
Verbs past part.
Honorifics
Lower
Upper
Lower
For/with
Honorifics
Nouns singular
Lower
Lower
Lower
Table 6: Most useful features for gender, age, and class for each author, determined by examining the
coefficients of classifiers that performed above baseline during cross-author testing. The pairs in the
table consist of a linguistic feature and the label indicated by more frequent use of that feature (e.g. for
Strindberg, third-person pronoun usage contributed to predicting gender, with greater usage indicating a
female character). Features marked in bold are shared between authors for a given attribute.
validation, if the trained classifier for a given fold
performed above the baseline of its own test set,
then we record its three most heavily weighted fea-
tures. At the end, we have a tally of which fea-
tures most often appeared in the top three features
for such better-performing classifiers. We can use
this to compare which features were more consis-
tently involved for each author and attribute pair,
as shown in Table 6.
Some of the useful features are more intuitive
than others. For example, as mentioned in an
earlier section, it seems reasonable that family
words may relate to depictions of gender roles
of the time period in which the plays were writ-
ten, with women being expected to take on so-
cial roles more confined to the home. This ap-
pears to be true for Ibsen, but not for Strindberg.
We also see family words suggesting young char-
acters for both authors? texts. It seems intuitive
that authors may have chosen to depict children as
spending more time around family members, in-
cluding using family terminology as terms of ad-
dress. The use of honorifics is also as predicted
earlier in the paper: lower class characters use
more higher terms of address, presumably when
interacting with their superiors. Another inter-
esting result is the frequency of third-person pro-
nouns being the most useful predictor of gender,
indicating female characters for both authors. Pos-
sibly, women may have spoken more about other
people than men did in these texts.
Some other results are not as easy to explain.
For example, the use of the prepositions for and
with was consistently the most useful predictor of
lower class characters (which could explain why
the models performed comparably on opposite au-
thors in Table 5). An interesting result was the
more frequent use of singular, present tense, non-
third person verbs among young characters in the
Ibsen texts. This suggests that young characters
used more verbs centered around I and you in the
present tense. One possible explanation is that
children were depicted as being more involved in
their own personal world, speaking less about peo-
ple they were not directly interacting with in a
given moment.
5 Conclusion
We have presented a dataset of translated plays by
August Strindberg and Henrik Ibsen, along with
computational models for predicting the sociolin-
guistic attributes of gender, age, and social class
of characters using the aggregation of their tex-
tual lines-to-be-spoken. We compared the per-
formance and important features of the models in
both intra- and inter-author testing using a cross-
author validation procedure, finding that models
generally performed above challenging baselines
for their own authors, but less so for the other,
as one would expect. The exception was the so-
cial class variable, which was consistently slightly
above baseline regardless of the author used for
testing. While this could indicate that the trans-
lated Strindberg and Ibsen texts conveyed social
class using similar linguistic cues, this remains a
topic for future exploration, given the class im-
balance for that attribute. We also examine some
indicative features for each attribute and author
pair, identifying similarities and differences be-
15
tween the depictions in each set of texts. This anal-
ysis supported the trends seen in the cross-author
testing.
Future work would include exploring other au-
thors and literary genres, or extending the scope
to non-literary domains. When expanding this
initial work to larger datasets, there is an op-
portunity to better understand the intricacies of
performance through other metrics (e.g. preci-
sion, recall). There is certainly much opportu-
nity to expand sociolinguistic features on fictional
texts and to explore other potentially simpler or
more advanced modeling frameworks. Alterna-
tives for assigning annotation of sociolinguistic
variables, such as socioeconomic standing, also
deserve further attention. Additionally, it would
be interesting to verify the preservation of linguis-
tic/sociolinguistic cues in translation by repeating
this work using different translations of the same
texts.
Acknowledgements
We thank the Swedish Institute (http://eng.si.se)
for partially supporting this work. We also thank
the reviewers for valuable comments that were
considered in the revision of this paper.
References
Paul Baker. 2010. Sociolinguistics and Corpus Lin-
guistics. Edinburgh University Press, Edinburgh.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python ? An-
alyzing Text with the Natural Language Toolkit.
O?Reilly Media, Sebastopol.
Constantinos Boulis and Mari Ostendorf. 2005. A
quantitative analysis of lexical differences between
genders in telephone conversations. In Proceedings
of the 43rd Annual Meeting of the ACL, pages 435?
442, Ann Arbor, MI, USA, June.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of the 47th Annual Meeting of the
ACL and 4th IJCNLP of the AFNLP, pages 719?718,
Suntec, Singapore, August.
Sobhan Raj Hota, Shlomo Argamon, and Rebecca
Chung. 2006. Gender in Shakespeare: Automatic
stylistics gender character classification using syn-
tactic, lexical and lemma features. In Digital Hu-
manities and Computer Science (DHCS 2006).
Justine Kao and Dan Jurafsky. 2012. A computational
analysis of style, affect, and imagery in contempo-
rary poetry. In Workshop on Computational Linguis-
tics for Literature, pages 8?17, Montr?eal, Canada,
June 8.
William Labov. 1972. Sociolinguistic Patterns. Uni-
versity of Pennsylvania Press, Philadelphia, PA.
Hans Lindquist. 2009. Corpus Linguistics and the De-
scription of English. Edinburgh University Press,
Edinburgh.
Rajend Mesthrie, Joan Swann, Anna Deumert, and
William Leap. 2009. Introducing Sociolinguistics
(2nd ed.). Jon Benjamins, Amsterdam.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Sara Rosenthal and Kathleen McKeown. 2011. Age
prediction in blogs: A study of style, content, and
online behavior in pre- and post-social media gen-
erations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 763?772, Portland, Oregon, June 19-24.
16
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 107?117,
Baltimore, Maryland USA, June 27, 2014.
c
?2014 Association for Computational Linguistics
Toward Macro-Insights for Suicide Prevention:
Analyzing Fine-Grained Distress at Scale
Christopher M. Homan
1
Ravdeep Johar
1
Tong Liu
1
Megan Lytle
2
Vincent Silenzio
2
Cecilia O. Alm
3
1
Golisano College of Computing and Information Sciences, Rochester Institute of Technology
2
Department of Psychiatry, University of Rochester Medical Center
3
College of Liberal Arts, Rochester Institute of Technology
{cmh
?
| rsj7209
$
| tl8313
$
| Megan Lytle
?
| Vincent Silenzio
?
| coagla
$
}
?
@cs.rit.edu
$
@rit.edu
?
@urmc.rochester.edu
Abstract
Suicide is a leading cause of death in the
United States. One of the major chal-
lenges to suicide prevention is that those
who may be most at risk cannot be re-
lied upon to report their conditions to clin-
icians. This paper takes an initial step
toward the automatic detection of suici-
dal risk factors through social media ac-
tivity, with no reliance on self-reporting.
We consider the performance of annota-
tors with various degrees of expertise in
suicide prevention at annotating microblog
data for the purpose of training text-based
models for detecting suicide risk behav-
iors. Consistent with crowdsourcing liter-
ature, we found that novice-novice anno-
tator pairs underperform expert annotators
and outperform automatic lexical analysis
tools, such as Linguistic Inquiry and Word
Count.
1 Introduction
Suicide is among the leading causes of death for
individuals 10?44 years of age in the United States
(Heron and Tejada-Vera, 2009). Indeed, while
mortality rates for most illnesses decreased be-
tween 2008 and 2009, the rate of suicide increased
by 2.4% (Heron and Tejada-Vera, 2009). The life-
time prevalence for suicidal ideation is 5.6?14.3%
in the general population, and as high as 19.8?
24.0% among youth (Nock et al., 2008).
The first step toward suicide prevention is to
identify, ideally in consultation with clinical ex-
perts, the risk factors associated with suicide. Due
to social stigma among other sociocultural fac-
tors (Crosby et al., 2011), individuals at risk for
committing suicide may not always reach out to
professionals or, if they do, provide them with
accurate information. They may not even real-
ize their own level of suicide risk before it is too
late. Self-reporting, then, is not an entirely reliable
means of detecting and assessing suicide risk, and
research on suicide prevention can benefit from
also exploring other channels for assessing risk.
For instance, individuals may be more inclined
to seek support from informal resources, such as
social media, instead of seeking treatment (Crosby
et al., 2011; Bruffaerts et al., 2011; Ryan et al.,
2010). Evidence suggests that youth and emerg-
ing adults usually prefer to seek help from their
friends and families; however, higher levels of
suicidal ideation are associated with lower levels
of help-seeking from both formal or informal re-
sources (Deane et al., 2001).
These patterns in help-seeking behavior sug-
gest that social media might be an impor-
tant channel for discovering those at risk for?
and even preventing?suicide. Internet- and
telecommunications-driven activity is revolution-
izing the social sciences by providing data, much
of it publicly available, on human activity in situ,
at volumes and a level of time and space granu-
larity never before approached. Can such data im-
prove clinical preventative study and measures by
providing access to at-risk individuals who would
otherwise go undetected, and by leading to better
science about suicide risk behaviors?
The stress-diathesis model for suicidal behav-
ior (Mann et al., 1999) suggests that they might. It
says that (1) objective states, such as depression or
life events, as well as subjective states and traits,
such as substance abuse or family history of de-
pression, suicide, or substance abuse, are among
the risk factors that contribute to suicidal ideation
and (2) the presence of these factors could even-
tually lead to either externalizing (e.g., interper-
107
sonal violence) or internalizing aggression (e.g.,
attempting suicide).
Since the stress-diathesis model was developed
using risk factors for suicidal behavior, and be-
cause it makes a connection between internalized
and externalized acts, it is a suitable framework for
analyzing publicly available linguistic data from
social media outlets such as Twitter. Data from so-
cial media can be seen as a kind of natural exper-
iment on depression and suicidal ideation that is
unburdened by such sample biases as the willing-
ness of individuals to take part in research and/or
seek out formal sources of support. Moreover, this
approach may provide information about individ-
uals who are unlikely to engage in formal help-
seeking behaviors, or may inform effective meth-
ods of natural helping. Thus, this macro-level ap-
proach to monitoring suicidal behaviors may have
future implications not only for identifying indi-
viduals who have a higher prevalence for suicidal
behaviors but it could eventually lead to additional
methods for enhancing protective factors against
suicide.
In this paper, we take steps toward the auto-
matic detection of suicide risk among individuals
via social media. Suicide ideation is a complex be-
havior and its connection to suicide itself remains
poorly understood. We focus on a particular aspect
of suicidality, namely distress. While not equiva-
lent to suicide ideation, according to Nock et al.
(2010) distress is an important risk factor in sui-
cide, and one that is observable from microblog
text, though admittedly observing suicide risk be-
havior is a subjective and noisy venture.
Lehrman et al. (2012) conducted an early study
on the computational modeling of distress based
on short forum texts, yet left many areas wide open
for continued study. For example, analysis at scale
is one such open issue. More specifically, Pestian
and colleagues (Matykiewicz et al., 2009; Pestian
et al., 2008) used computational methods to under-
stand suicide notes. However, when it comes to
preventive contexts, such data are less insightful.
For preventive health, access to real-time health-
related data that dynamically evolve can allow us
to address macro-level analysis. Social media pro-
vide an additional opportunity to model the phe-
nomena of interest at scale.
We use methods that take advantage of lexical
analysis to retrieve microblog posts (tweets) from
Twitter and compare the performance of human
annotators?one being an expert, and others not?
to rate the level of distress of each tweet.
Clinical expert annotation, rather than general-
purpose tools for content and sentiment analy-
sis such as LIWC (Linguistic Inquiry and Word
Count) by Pennebaker et al. (2001), provides a ba-
sis for text-based statistical modeling. We show
that expertise-based keyword retrieval, departing
from knowledge about contributing risk factors,
results in better interannotator agreement in both
novice-novice and novice-expert annotation when
the keywords reflect the task at hand.
2 Related Work
Data on suicide traditionally comes from health-
care organizations, large-scale studies, or self re-
porting (Crosby et al., 2011; Horowitz and Bal-
lard, 2009). These sources are limited by sociocul-
tural barriers (Crosby et al., 2011), such as stigma
and shame. Moreover, data on suicide is never par-
ticularly reliable because suicide is a fundamen-
tally subjective, complex phenomenon with a low
base rate. For these reasons, many researchers
tend to focus on the relationship between risk fac-
tors and suicidal behavior, without relying heavily
on theoretical models (Nock et al., 2008).
Approximately one-third of all individuals who
reported suicidal ideation in their lifetime made a
plan to commit suicide. Nearly three-quarters of
those who reported making a suicide plan actu-
ally attempted. The odds of attempting suicide in-
creased exponentially when individuals endorsed
three or more risk factors, e.g., having a mood or
substance abuse disorder (Kessler et al., 1999).
Demographics, previous suicide attempts, men-
tal health concerns (i.e., depression, substance
abuse, suicidal ideation, self-harm, or impulsiv-
ity), family history of suicide, interpersonal con-
flicts (i.e., family violence or bullying), and means
for suicidal behavior (e.g., firearms), are com-
monly cited risk factors for suicidal behavior
(Nock et al., 2008; Crosby et al., 2011; Gaynes
et al., 2004; Harriss and Hawton, 2005; Shaffer et
al., 2004; Brown et al., 2000).
Regarding the use of annotation for predictive
modeling, evidence suggests that when it comes
to judgments that involve clinical phenomena, ex-
perts and novices behave differently (Li et al.,
2012; Womack et al., 2012). Such distinctions in-
tuitively make sense, as the learning of medical
domain knowledge requires advanced education in
108
conjunction with substantial practical field experi-
ence.
In a task such as medical image inspection, the
subtle cues that point an observer to evidence that
allow them to identify a clinical condition, while
accessible to experts with training and perceptual
expertise to guide their exploration, are likely to be
missed by novices who lack that background and
clinical understanding. Such expertise can then be
integrated into human-centered health-IT systems
(Guo et al., 2014), in order to introduce novel ways
to retrieve medical images and take advantage of
an understanding of which information is useful.
It is reasonable to assume that this knowledge gap
also applies to other knowledge-intensive clinical
domains such as mental health. In this study, we
explore this question and study if novice vs. ex-
pert annotation makes a difference for identifying
distress in social media texts, as well as what the
impact of expert vs. novice annotation is for subse-
quent computational modeling with the annotated
data.
Affect in language is a phenomenon that has
been studied in the speech and text analysis do-
mains, and in many others (Calvo and D?Mello,
2010). Clearly, emotion is a key element in the
human experience, but it is notoriously difficult
to pin down and scholars in the affective sciences
lack a single agreed-upon definition for emotion.
Accordingly, different theoretical constructs have
been proposed to describe affect and affect-related
behaviors (Picard, 1997). In addition, research on
affect in language has shown that such phenom-
ena tend to be subjective, lack real ground truth
(often resulting in moderate kappa scores), and
have particularly fuzzy semantics in the gray zone
where neutrality and emotion meet (Alm, 2008).
These kinds of problem characteristics bring with
them their own set of demanding challenges from
a computational perspective (Alm, 2011). Yet, the
nature of such problems make them incredibly im-
portant to study, despite the challenges involved.
Sentiment analysis has been widely studied in
a number of computational settings, including on
various social networking sites. A rather substan-
tial body of work already exists on the use of
Twitter to study emotion (Bollen et al., 2011b;
Dodds et al., 2011; Wang et al., 2012; Pfitzner et
al., 2012; Kim et al., 2012; Bollen et al., 2011a;
Pfitzner et al., 2012; Bollen et al., 2011c; Moham-
mad, 2012; Golder and Macy, 2011; De Choud-
hury et al., 2012a; De Choudhury et al., 2012b;
De Choudhury et al., 2013; De Choudhury and
Counts, 2013; Hannak et al., 2012; Thelwall et
al., 2011; Pak and Paroubek, 2010). For in-
stance, Golder and and Macy study aggregate
global trends in ?mood,? and show, among other
things, that people wake up in a relatively good
mood that decays as the day progresses (Golder
and Macy, 2011). Bollen et al. (2011c) show that
tweets from users who took a standard diagnos-
tic instrument for mood are often tied to current
events, such as elections and holidays.
Relatively little of this work has focused on sui-
cide or related psychological conditions. Masuda
et al. (2013) study suicide on mixi (a Japanese
social networking service). Cheng et al. (2012)
consider the ethical and political implications
of online data collection for suicide prevention.
Jashinsky et al. (2013) show correlations between
frequency in tweets related to suicide and ac-
tual suicide in the 50 United States of Amer-
ica. Sadilek et al. (2014) study depression on
Twitter. De Choudhury and collaborators studied
depression?in general and post-partum?in Twit-
ter (De Choudhury et al., 2012a; De Choudhury et
al., 2012b; De Choudhury et al., 2013; De Choud-
hury and Counts, 2013) and Facebook (De Choud-
hury et al., 2014). Homan et al. (2014) investigate
depression in TrevorSpace. A number of social
theories of suicide have been proposed (Wray et
al., 2011), but most of this work was with respect
to offline social systems.
3 Methods
Our methods involve four main phases: (1) We fil-
tered a corpus, obtained from Sadilek et al. (2012),
of approximately 2.5 million tweets from 6,237
unique users in the New York City area that were
sent during a 1-month period between May and
June, 2010, into a set of 2,000 tweets that are rela-
tively likely to be centered around suicide risk fac-
tors. (2) We annotated each of these 2,000 tweets
with their level of distress, and also analyzed the
annotations in detail. (3) We then trained sup-
port vector machines and topic models with the
annotated data, except for a held-out subset of 200
tweets. (4) Finally, we assessed the effectiveness
of these methods on the held-out data.
109
Source
tweets
Number of tweets 2,535,706
Unique geo-active users 6,237
?Follows? relationships 102,739
?Friends? relationships 31,874
Filtered
tweets
Number of tweets 2,000
Unique users 1,467
Unique unigrams 1,714,167
Unique bigrams 9,246,715
Unique trigrams 1,306,1142
Categories
distribution
LIWC sad 1,370
Depressive feeling 283
Suicide ideation 123
Depression symptoms 72
Self harm 67
Family violence/discord 47
Bullying 10
Gun ownership 10
Drug abuse 6
Impulsivity 6
Prior suicide attempts 2
Suicide around individual 2
Psychological disorders 2
Table 1: Summary statistics and thematic cate-
gory distributions of the collected dataset. The
data were collected from NYC. Geo-active users
are those who geo-tag (i.e., automatically post the
GPS location of) their tweets relatively frequently
(more than 100 times per month).
3.1 Filtering tweets
In order to facilitate the discovery of distress-
related tweets, we first (a) converted all text to
lower case; (b) stripped out punctuation and spe-
cial characters; and (c) mapped informal terms
(such as abbreviations and netspeak) to more stan-
dard ones, based on the noslang dictionary.
1
We then used two different methods to filter
tweets that are relatively likely to center on sui-
cide risk factors. We used LIWC to capture 1,370
tweets by sampling randomly from among the
2,000 tweets with the highest LIWC sad score.
LIWC has been widely used to estimate emotion
in online social networks, and specifically to mood
on Twitter. This slight amount of randomness in
filtering tweets this way was intended to avoid se-
lecting obvious false positives, such as the use of
?sad? in nicknames.
Next, we adopted a collection of inclusive
search terms/phrases from Jashinsky et al. (2013),
which was designed specifically for capturing
tweets related to suicide risk factors, and applied
them to our source corpus. We added to these
more terms, from (Crosby et al., 2011) (see Ta-
ble 2). These terms yielded 630 tweets.
1
http://www.noslang.com/dictionary
depressive
feeling
tired of living, leave this world,
wanna die, hate my job,
feeling guilty, deserve to die,
desire to end own life,
feeling ignored,
tired of everything, feeling blue,
have blues
depression
symptoms
sleeping pill, have insomnia,
sleep forever, sleep disorder
drug
abuse
clonazepam, drug overdose,
imipramine
prior suicide
attempts
tried suicide
suicide
ideation
commit suicide,
committing suicide,
feeling suicidal, want to suicide,
shoot myself, a gun to head,
hang myself, intention to die
self
harm
hurt myself, cut myself
psychological
disorders
sleep apnea
family
violence
discord
lost my friend,
argument with wife,
argument with husband,
shouted at each other
Table 2: Filtering terms added to those
from Jashinsky et al. (2013).
3.2 Novice and Expert Tweet Annotation
We then divided the resulting set of 2,000 fil-
tered tweets (1,370 from the LIWC sad dimension
and 630 from suicide-specific search terms), into
two randomized sets of 1,000 tweets each. Both
sets had the same proportion of LIWC-filtered and
suicide-specific-filtered tweets. A novice anno-
tated the first set and a counseling psychologist
with experience in suicide related research anno-
tated the second set. A second novice annotated
a subset of 250 tweets of the first set, to reveal
interannotator agreement between novices, as one
might expect a novice without training to be less
systematic. (The annotators were among the au-
thors.) Each tweet in each set was rated on a four-
point scale (H, ND, LD, HD) according to the level
of distress evident (Table 3).
Each tweet to be annotated was provided with
context in the form of the three tweets before and
after the tweet to be annotated that the tweeter
made, along with the timestamp of those tweets
and the thematic categories to which the tweet be-
longed, based on the filtering process (Figure 1).
3.3 Modeling
We then mapped each tweet to a feature space
composed of the unigrams, bigrams, and trigrams
in the corpus. For example, a simple tweet ?I am
110
978: Date: XXXX
-3: dat man on maury is overreacting!!
he juss doin dat cuz he on
tv [-0:24:39]
-2: @XXXX cedes!!! [-0:21:25]
-1: yesssss! da weatherman was wronq
no rainy ass prom days!! yesss
prom is 2day guys!! class
of 2010! [-0:02:56]
>>> @XXXX awwww thanks trae-trae
1: rt @XXXX: abt 2 hop in a kab
to skool i wouldn?t dare spend
over 2 dollars to get somewhere
i dnt wanna be n da first
place! [+0:00:57]
2: @XXXX yeaa [+0:03:59]
3: @XXXX wassup? [+0:05:28]
Msg_id: XXXX [Distress: ND, LIWC Sad: No]
Figure 1: Example input for annotator. The tweet
to be annotated is indicated by >>>. Annotators
were given context in the form of the three tweets
immediately preceding?and the three tweets im-
mediately following?the tweet to be annotated
that the tweeter made, along with the relative time
at which each tweet was made. Each numerical la-
bel denotes one of these context tweets. (Tweeter
information has been blanked out.)
Code Distress Level
H happy
ND no distress
LD low distress
HD high distress
Table 3: Distress-related categories used to anno-
tate the tweets.
so happy? was represented as the following feature
vector: {I, am, so, happy, I am, am so, so happy,
I am so, am so happy}. Each feature is associated
with its tf-idf score (Manning et al., 2008).
We performed topic modeling on our dataset. A
topic is a set of lexical items that are likely to occur
in the same tweet. Topic models are capable of as-
sociating words with similar meanings and distin-
guishing among the different meanings of a single
word. We used latent Dirichlet allocation (LDA)
(Blei et al., 2003) to create these topics. Before
doing so, we removed stop words and words that
occur only once in the dataset. We then applied
LDA algorithm on the data to discover three top-
ics using 100 iterations.
We used support vector machines (SVMs)
(Joachims, 1998), a machine learning method that
is used to train a classification model that can as-
sign class labels to previously unseen tweets, to
assess the power of our annotations. SVMs treat
each tweet as a point in an extremely high dimen-
sional space (one dimension per uni-, bi-, and tri-
gram in the corpus). SVMs are a form of linear
separator that can also distinguish between non-
linearly separable classes of data by warping the
feature space (though in our case we perform no
such warping, or kernelization). They have proven
to be an extremely effective tool in classifying text
in numerous settings, including Twitter.
4 Results
Figure 2: Distribution of distress level annota-
tions on the tweets annotated by Novices 1 and 2
(N=250, identical set).
Figure 3: Distribution of distress level annota-
tions from Novice 1 and Expert. Note the these
two datasets are disjoint (N = 1000 tweets, respec-
tively).
Figure 2 shows the distribution of annotation la-
bels for the subset of tweets that Novices 1 and 2
both annotated, and Figure 3 compares the over-
all annotation distributions between Novice 1 and
the Expert. Interestingly, the novices are relatively
conservative, compared to the expert, in assign-
ing distressed labels, whereas the expert exhibits
a higher sensitivity toward low distress than either
of the novices. This suggests that it is important in
this domain not to rely too much on novice judg-
111
ments, as novices are not trained to pick up on sub-
tle cues?in contrast to the clinically trained eye.
Note that there are very few happy tweets,
which confirms that our filtering was effective in
removing tweets of the opposite polarity.
Filtering method Kappa
LIWC sad 0.4
Thematic suicide risk factors 0.6
Both 0.5
Table 4: Cohen kappa interannotator agreement
between Novice 1 and 2.
H ND LD HD
H 0 2 0 0
ND 1 85 2 1
LD 0 22 9 0
HD 0 1 0 2
Table 5: Confusion matrix between Novices 1 and
2 on annotations of the LIWC-sad-based filtered
tweets.
H ND LD HD
H 4 6 0 0
ND 0 55 12 1
LD 0 12 22 5
HD 0 1 3 4
Table 6: Confusion matrix between Novices 1 and
2 on annotations of tweets filtered by Jashinsky et
al. (2013)?s thematic suicide risk factors inclusion
terms.
Table 4 shows the Cohen kappa score between
Novices 1 and 2, when high and low distress vs.
no distress and happy, are grouped in a single cate-
gory and Tables 5?7 show the confusion matrices
between Novices 1 and 2. In all cases the kappa
score is moderate. However, it clearly improves
when annotation is restricted to just those tweets
filtered using the suicide-thematic inclusion terms
of Jashinsky et al. (2013). This again seems to
point to the usefulness of including clinical experts
into the training process.
Due to their sensitive nature, we decided not to
provide examples of high distress tweets. Here are
two examples of tweets labeled as low distress by
two annotators.
? insomnia night#56325897521365!!
sheesh can?t deal w/ this shit!
i have class in the morning got
dammit....
H ND LD HD
H 4 8 0 0
ND 1 140 14 2
LD 0 34 31 5
HD 0 2 3 6
Table 7: Confusion matrix between Novices 1 and
2 on annotations of all common tweets between
the two annotators.
? @XXXX i?m still sad thoo. i feel
neglected! and i miss XXXX
And here are two examples of tweets labeled as
no distress by two annotators.
? i did mad push-ups tryna get that
cut up look, then look at myself
after a shower ... #plandidntwork;
thats #whyiaintgotomiami
? my son is gonna have blues eyes and
nappy hair! yes yes yes
The above examples are rather clear cut, how-
ever in many cases the tweets were more ambigu-
ous, even when annotators had the preceding and
succeeding three tweets from the user of the tweet
to be annotated to rely on for context. While con-
text and time offset information was useful for an-
notators, distress annotation is clearly a challeng-
ing task, as the confusion matrices in Tables 5?6
reveal. The lower agreement levels, and particu-
larly the fuzzy border between ?no distress? and
?low distress? are completely in line with prior
research, discussed above, on affective language
phenomena.
Another filtering and annotation challenge in-
volves tweets with mixed emotion, such as:
? as much as i hate my job some of the
people i work with are amazing.
Beyond the targeted annotation categories of
distress level, there were emerging themes of
aggression, privilege and oppression, and daily
struggles, among others. For instance, jobs were
a popular source of distress:
? i friggin hate these bastards my
job grimey ass bastards knew i
wanted the day off and tell me some
next shit
? hate my job wit a passion! hate
every1 there.. they better do
sumthin about it, or im out!
Personal bias may have impacted annotation de-
cisions. For instance, numerous tweets contained
112
irony and dark humor, which may result in anno-
tators underestimating or overlooking actual dis-
tress. In addition, by pulling data from Twitter,
any non-Twitter context behind the tweets is lost.
For example, a few individuals retweeted in a sar-
castic manner about what individuals should say
to someone who is considering suicide:
? you wish!!! rt @XXXX: i think
suicide is funny. especially once
my mom does it
? rt @XXXX: what do i say to a person
thats asking me for advice becuz
they thinking bout committing
suicide when i see there point?
lmao
Without knowing the circumstances of the original
message (beyond the provided context window) it
is difficult to classify such tweets.
Finally, a number of tweets seemed to show
compassion or empathy for others experiencing
stress. This suggests to us the profound role that
social support places in well-being and depression,
that one?s friends and associates can also provide
clues into one?s emotional state, and that social
media can reveal such behavior.
? rt @XXXX: damn now what do i do? i
feel empty as f$% damit!! breathe
ocho,
*
tears
*
from liberty city to
(cont) http://XXXX
? @XXXX that?s just sad i feel for you
High Distress Random
feel like, wanna cry, get
hurt, miss 2, ima miss, win
lose, tired everything, broke
bitches, gun range, one
person
good morning, last
night, happy birthday,
look like, bout 2, can?t
wait, video , know
(cont), chris brown, jus
got
commit suicide, miss you!,
miss baby, feel empty,
committing suicide, tired
living, sleep forever, lost
phone, left alone, :( miss
feel like, let know,
make sure, bout go,
time get, don?t get, wats
good, . ., don?t want,
jus saw
hate job, feel sad, tummy
hurts, lost friend, feel
helpless, leave alone, don?t
wanna, worst feeling, leave
world, don?t let
don?t know, let?s go,
looks like, what?s good,
go sleep, even tho, hell
yea, new single, r u?,
don?t wanna
Table 8: Topic analysis on bigrams of tweets la-
beled as high distress vs. randomly selected tweets
from the larger, unlabeled dataset. The high dis-
tress tweets clearly convey strong negative affect.
Table 8 shows the results of a 3-category topic
model on bigrams. The first column is taken just
from tweets labeled high distress by any one of
the three annotators (72 tweets total). The sec-
ond column comes from a randomly-chosen sam-
ple of 2000 tweets from the 2.3 million tweet cor-
pus. These results show that the lexical contents
of the annotated tweets are recognizeably differ-
ent from the random sample. By our judgement,
the topical groupings in the rows of the high dis-
tress column are all clearly marked by strong neg-
ative affect, and additionally they could arguably
be labeled?from top to bottom?as: ?failure and
defeat,? ?loss,? and ?loneliness.? The rows of the
second column are less clear cut, and appear to
reflect a much broader scope of topics. One inter-
esting aspect of the second, random column is that
recording artist Chris Brown had released a new
album during the collection period, which seems
to explain why his name appeared.
Training Testing Precision Recall F-Measure
N1 N1 0.53 0.63 0.58
N1 E 0.58 0.27 0.37
E E 0.59 0.71 0.64
E N1 0.34 0.85 0.48
N1 + E N1 + E 0.33 0.41 0.37
Table 9: Performance of SVM-based classification
when the training and testing sets are alternately
Novice 1 (N1) or the Expert (E). Because we fo-
cus on distress classification, we report precision,
recall and F-measure for the distress class, which
combines LD and HD into a single class with re-
spect to binary (distress vs. non-distress) classifi-
cation. In each case, a held-out set of 100 ran-
domly selected tweets compose the test set and
the remaining 900 tweets from that annotator com-
pose the training set. The last row shows when the
two training sets (respectively, test sets) are com-
bined into a single set of 1800 (respectively, 200)
tweets.
For classification, because we are most inter-
ested in being able to separate distressed from
non-distressed tweets, we combine low distress
and high distress into a single distress class, and
no distress and happy into a non-distress class. Ta-
ble 9 shows the performance of the SVM-based
classifier when trained and tested on the Expert
and Novice 1 training sets. Four themes emerge:
(1) the SVM classifier is much more accurate (in
terms of F-measure) when the testing and training
data come from the same annotator (test and train-
ing data are disjoint), and the best performance
comes from the expert-annotated data. (2) When
113
testing and training data are from different anno-
tators, the F-measure performance of the SVM
is lower when the training set is from the novice
rather than the expert. (3) When testing and train-
ing data are from different annotators, the SVM
has lower recall and higher precision when the
training set is from the novice rather than the ex-
pert. This is in part because the Expert was more
sensitive to distress than Novice 1. It is premature
to draw conclusions from this observation, but per-
haps this shows that training with expert-labeled
annotations is preferable to using novice-labeled
data, espectially when our goal is to discover dis-
tressful tweets for the purpose of identifying at-
risk individuals and err on the side of caution (high
recall). (4) Integrating more but mixed data does
not improve performance.
5 Discussion
As previously mentioned, many of the risk fac-
tors for suicidal behavior may be linked to other
expressions of distress, such as aggression and
interpersonal violence (Mann et al., 1999). The
goal of this study is to determine the feasibility
of classifying distress to enable further study of
expressed suicidal behaviors. Consistent with the
stress diathesis model for suicidal behavior, ag-
gression was an emerging theme that arose from
the data. Here are some examples:
? @XXXX i don?t feel sad 4 him. he
gets pissed n says wat he wants then
sends out fony apologies
? @XXXX cuz he?s n a relationship
with that horseface bitch &amp; he
lied 2 me &amp; i feel so used &amp;
worthless now
Some individuals tweeted about feeling empty,
hopeless, angry, frustrated, and alone. Behaviors
indicating bullying and schadenfreude were also
observed. While these are all risk factors for inter-
nalizing aggression (i.e., suicidal behavior), they
are also associated with externalized aggression.
In addition to overt expressions of anger and vi-
olence, many of the humorous, ironic tweets also
had an aggressive undertone.
5.1 Limitations
As ground truth, we rely on tweets hand-annotated
by expert and novice for classification. However,
the mental state of another individual, observed
from a few lines of text often written in an in-
formal register is necessarily hard to discern and,
even under less noisy conditions, extremely sub-
jective; even the observers? personal understand-
ings of such concepts as ?distress? may differ
drastically. This makes annotation quite a chal-
lenge, and does not reveal in an objective fashion a
tweeter?s true mental state. As we have mentioned
earlier, self-reporting has its own limitations, yet
it is often regarded as the gold standard for ground
truth about emotional state. Part of the problem in
assessing the effectiveness of self-reporting is the
relative rareness by which suicide occurs, and by
the inherent subjectivity of the act, which makes
any data on suicide fuzzy. We hope to explore in
future work the relationship between clinical ob-
servation in both on- and off-line settings and self-
reporting, including the integration of natural lan-
guage data of patients from clinical settings. We
also hope to explore distress annotation from dif-
ferent perspectives and levels of context.
Higher levels of suicidal ideation have an in-
verse relationship with all types of help-seeking
and a positive correlation with the decision to not
seek support (Deane et al., 2001). Thus, we would
expect suicidal individuals to generally be less ac-
tive on social media than those who are not. Nev-
ertheless, a number of studies have shown a posi-
tive correlation between online social network use
and negative mood. Perhaps this means in part that
individuals who are depressed are slower to disen-
gage on- rather than off-line.
6 Conclusion
We studied the performance of different ap-
proaches to training systems to detect evidence
of suicide risk behavior in microblog data. We
showed that both the methods used to automat-
ically collect training sets, as well as the ex-
pertise level of the annotator affect greatly the
performance of automatic systems for detecting
suicide risk factors. In general, our study and
its results?from filtering via data annotation to
classification?confirmed the critical importance
of bringing clinical expertise into the computa-
tional modeling loop.
Acknowledgments
This work was supported by a grant of the Kodak
Endowed Chair Fund from the Golisano College
of Computing and Information Sciences at RIT
and NSF award SES-1111016.
114
References
Cecilia Ovesdotter Alm. 2008. Affect in Text and
Speech. Ph.D. thesis, University of Illinois at Ur-
bana Champaign.
Cecilia Ovesdotter Alm. 2011. Subjective natural
language problems: Motivations, applications, char-
acterizations, and implications. In Proceedings of
49th Annual Meeting of the Assoc. for Computa-
tional Linguistics: Human Language Technologies,
Portland, OR, pages 107?112.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal Mach-
ince Learning Research, 3:993?1022, March.
Johan Bollen, Bruno Gonc?alves, Guangchen Ruan, and
Huina Mao. 2011a. Happiness is assortative in on-
line social networks. Artificial Life, 17(3):237?251.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011b.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8.
Johan Bollen, Alberto Pepe, and Huina Mao. 2011c.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In Proceed-
ings of the Fifth International AAAI Conference on
Weblogs and Social Media, pages 450?453.
Gregory K Brown, Aaron T Beck, Robert A Steer, and
Jessica R Grisham. 2000. Risk factors for sui-
cide in psychiatric outpatients: A 20-year prospec-
tive study. Journal of Consulting and Clinical Psy-
chology, 68(3):371.
Ronny Bruffaerts, Koen Demyttenaere, Irving Hwang,
Wai-Tat Chiu, Nancy Sampson, Ronald C Kessler,
Jordi Alonso, Guilherme Borges, Giovanni de Giro-
lamo, Ron de Graaf, et al. 2011. Treatment of suici-
dal people around the world. The British Journal of
Psychiatry, 199(1):64?70.
Rafael A. Calvo and Sidney D?Mello. 2010. Affect
detection: An interdisciplinary review of models,
methods, and their applications. IEEE Transactions
on Affective Computing, 1(1):18?37.
Qijin Cheng, Shu-Sen Chang, and Paul SF Yip.
2012. Opportunities and challenges of online data
collection for suicide prevention. The Lancet,
379(9830):e53?e54.
Alex E Crosby, LaVonne Ortega, and Cindi Melanson.
2011. Self-directed violence surveillance: Uniform
definitions and recommended data elements. Cen-
ters for Disease Control and Prevention, National
Center for Injury Prevention and Control, Division
of Violence Prevention.
Munmun De Choudhury and Scott Counts. 2013. Un-
derstanding affect in the workplace via social media.
In 16th ACM Conference on Computer Supported
Cooperative Work and Social Media (CSCW 2013),
pages 303?316. ACM.
Munmun De Choudhury, Scott Counts, and Michael
Gamon. 2012a. Not all moods are created equal!
Exploring human emotional states in social media.
In 6th International AAAI Conference on Weblogs
and Social Media.
Munmun De Choudhury, Michael Gamon, and Scott
Counts. 2012b. Happy, nervous or surprised? Clas-
sification of human affective states in social media.
In 6th International AAAI Conference on Weblogs
and Social Media.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Major life changes and behavioral
markers in social media: case of childbirth. In Pro-
ceedings of the 2013 conference on Computer sup-
ported cooperative work, pages 1431?1442. ACM.
Munmun De Choudhury, Scott Counts, Eric J Horvitz,
and Aaron Hoff. 2014. Characterizing and pre-
dicting postpartum depression from shared facebook
data. In Proceedings of the 17th ACM conference
on Computer Supported Cooperative Work & Social
Computing, pages 626?638. ACM.
Frank P Deane, Coralie J Wilson, and Joseph Ciarrochi.
2001. Suicidal ideation and help-negation: Not just
hopelessness or prior help. Journal of Clinical Psy-
chology, 57:901?914.
Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M Kloumann, Catherine A Bliss, and Christo-
pher M Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and twitter. PloS one, 6(12):e26752.
Bradley N Gaynes, Suzanne L West, Carol A Ford,
Paul Frame, Jonathan Klein, and Kathleen N Lohr.
2004. Screening for suicide risk in adults: A sum-
mary of the evidence for the US Preventive Ser-
vices Task Force. Annals of Internal Medicine,
140(10):822?835.
S.A. Golder and M.W. Macy. 2011. Diurnal and sea-
sonal mood vary with work, sleep, and daylength
across diverse cultures. Science, 333(6051):1878?
1881.
Xuan Guo, Rui Li, Cecilia Ovesdotter Alm, Qi Yu, Jeff
Pelz, Pengcheng Shi, and Anne Haake. 2014. Infus-
ing perceptual expertise and domain knowledge into
a human-centered image retrieval system: A proto-
type application. In Proceedings of the Symposium
on Eye Tracking Research and Applications, pages
275?278. ACM.
Aniko Hannak, Eric Anderson, Lisa Feldman Barrett,
Sune Lehmann, Alan Mislove, and Mirek Riede-
wald. 2012. Tweetin in the rain: Exploring societal-
scale effects of weather on mood. In Proceedings of
the 6th International AAAI Conference on Weblogs
and Social Media (ICWSM12).
Louise Harriss and Keith Hawton. 2005. Suicidal in-
tent in deliberate self-harm and the risk of suicide:
The predictive power of the suicide intent scale.
Journal of Affective Disorders, 86(2):225?233.
115
Melonie Heron and Betzaida Tejada-Vera. 2009.
Deaths: Leading causes for 2005. National Vital
Statistics Reports: From the Centers for Disease
Control and Prevention, National Center for Health
Statistics, National Vital Statistics System, 58(8):1?
97.
Christopher M Homan, Naiji Lu, Xin Tu, Megan C Ly-
tle, and Vincent Silenzio. 2014. Social structure
and depression in TrevorSpace. In Proceedings of
the 17th ACM Conference on Computer Supported
Cooperative Work & Social Computing, pages 615?
625. ACM.
Lisa M Horowitz and Elizabeth D Ballard. 2009. Sui-
cide screening in schools, primary care and emer-
gency departments. Current Opinion in Pediatrics,
21(5):620?627.
Jared Jashinsky, Scott H Burton, Carl L Hanson, Josh
West, Christophe Giraud-Carrier, Michael D Barnes,
and Trenton Argyle. 2013. Tracking suicide risk
factors through Twitter in the US. Crisis, pages 1?
9.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learn-
ing (ECML), pages 137?142, Berlin. Springer.
Ronald C Kessler, Guilherme Borges, and Ellen E Wal-
ters. 1999. Prevalence of and risk factors for life-
time suicide attempts in the national comorbidity
survey. Archives of General Psychiatry, 56(7):617?
626.
Suin Kim, J Bak, and Alice Oh. 2012. Do you feel
what I feel? Social aspects of emotions in Twitter
conversations. In Proceedings of the AAAI Interna-
tional Conference on Weblogs and Social Media.
Michael Lehrman, Cecilia Ovesdotter Alm, and Ruben
Proano. 2012. Detecting distressed vs. non-
distressed affect state in short forum texts. In Pro-
ceedings of the Workshop on Language in Social
Media (LSM 2012) at the Conference of the North
American Chapter of the Association for Compu-
tational Linguistics-Human Language Technologies,
Montreal, Canada, pages 9?18.
Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake.
2012. Learning image-derived eye movement
patterns to characterize perceptual expertise. In
CogSci, pages 1900?1905.
J John Mann, Christine Waternaux, Gretchen L Haas,
and Kevin M Malone. 1999. Toward a clinical
model of suicidal behavior in psychiatric patients.
American Journal of Psychiatry, 156(2):181?189.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Naoki Masuda, Issei Kurahashi, and Hiroko Onari.
2013. Suicide ideation of individuals in online so-
cial networks. PloS one, 8(4):e62262.
Pawel Matykiewicz, Wlodzislav Duch, and John P.
Pestian. 2009. Clustering semantic spaces of sui-
cide notes and newsgroup articles. In Proceedings of
the Workshop on BioNLP, Boulder, Colorado, pages
179?184.
Saif M Mohammad. 2012. # Emotional tweets. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 246?255.
Association for Computational Linguistics.
Matthew K Nock, Guilherme Borges, Evelyn J Bromet,
Christine B Cha, Ronald C Kessler, and Sing Lee.
2008. Suicide and suicidal behavior. Epidemiologic
Reviews, 30(1):133?154.
Matthew K Nock, Jennifer M Park, Christine T Finn,
Tara L Deliberto, Halina J Dour, and Mahzarin R
Banaji. 2010. Measuring the suicidal mind implicit
cognition predicts suicidal behavior. Psychological
Science, 21(4):511?517.
Alexander Pak and Patrick Paroubek. 2010. Twitter
as a corpus for sentiment analysis and opinion min-
ing. In Proceedings of Conference on Language Re-
sources and Evaluation.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates, 71:2001.
John P. Pestian, Pawel Matykiewicz, and Jacqueline
Grupp-Phelan. 2008. Using natural language pro-
cessing to classify suicide notes. In BioNLP 2008:
Current Trends in Biomedical Natural Language
Processing, Columbus, Ohio, pages 96?97.
Ren?e Pfitzner, Antonios Garas, and Frank Schweitzer.
2012. Emotional divergence influences information
spreading in twitter. Proceedings of the 6th Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM12), pages 2?5.
Rosalind W. Picard. 1997. Affective Computing. MIT
Press, Cambridge, MA, USA.
Megan L Ryan, Ian M Shochet, and Helen M Stallman.
2010. Universal online interventions might engage
psychologically distressed university students who
are unlikely to seek formal help. Advances in Men-
tal Health, 9(1):73?83.
Adam Sadilek, Henry A Kautz, and Vincent Silenzio.
2012. Predicting disease transmission from geo-
tagged micro-blog data. In Association for the Ad-
vancement of Articial Intelligence.
116
Adam Sadilek, Christopher Homan, Walter S. Lasecki,
Vincent Silenzio, and Henry Kautz. 2014. Mod-
eling fine-grained dynamics of mood at scale. In
WSDM 2014 Workshop on Diffusion Networks and
Cascade Analytics.
David Shaffer, Michelle Scott, Holly Wilcox, Carey
Maslow, Roger Hicks, Christopher P Lucas, Robin
Garfinkel, and Steven Greenwald. 2004. The
Columbia SuicideScreen: Validity and reliability of
a screen for youth suicide and depression. Journal of
the American Academy of Child & Adolescent Psy-
chiatry, 43(1):71?79.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2011. Sentiment in Twitter events. Journal
of the American Society for Information Science and
Technology, 62(2):406?418.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P Sheth. 2012. Harnessing Twit-
ter ?Big Data? for automatic emotion identifica-
tion. In Privacy, Security, Risk and Trust (PASSAT),
2012 International Conference on and 2012 Inter-
national Confernece on Social Computing (Social-
Com), pages 587?592. IEEE.
Kathryn Womack, Wilson McCoy, Cecilia Ovesdot-
ter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng
Shi, and Anne Haake. 2012. Disfluencies as
extra-propositional indicators of cognitive process-
ing. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 1?9. Association for Computa-
tional Linguistics.
Matt Wray, Cynthia Colen, and Bernice Pescosolido.
2011. The sociology of suicide. Annual Review of
Sociology, 37:505?528.
117
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 83?87,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Decision Style in a Clinical Reasoning Corpus
Limor Hochberg
1
Cecilia O. Alm
1
Esa M. Rantanen
1
Caroline M. DeLong
1
Anne Haake
2
1 College of Liberal Arts 2 College of Computing & Information Sciences
Rochester Institute of Technology
lxh6513|coagla|emrgsh|cmdgsh|anne.haake@rit.edu
Abstract
The dual process model (Evans, 2008)
posits two types of decision-making,
which may be ordered on a continuum
from intuitive to analytical (Hammond,
1981). This work uses a dataset of nar-
rated image-based clinical reasoning, col-
lected from physicians as they diagnosed
dermatological cases presented as images.
Two annotators with training in cognitive
psychology assigned each narrative a rat-
ing on a four-point decision scale, from in-
tuitive to analytical. This work discusses
the annotation study, and makes contribu-
tions for resource creation methodology
and analysis in the clinical domain.
1 Introduction
Physicians make numerous diagnoses daily, and
consequently clinical decision-making strate-
gies are much discussed (e.g., Norman, 2009;
Croskerry, 2003, 2009). Dual process theory pro-
poses that decision-making may be broadly cat-
egorized as intuitive or analytical (Kahneman &
Frederick, 2002; Stanovich & West, 2000). Fur-
ther, scholars argue that decision-making may be
ordered on a continuum, with intuitive and analyt-
ical at each pole (Hamm, 1988; Hammond, 1981).
Determining the decision strategies used by
physicians is of interest because certain styles may
be more appropriate for particular tasks (Ham-
mond, 1981), and better suited for expert physi-
cians rather than those in training (Norman, 2009).
Language use can provide insight into physician
decision style, as linguistic content reflects cogni-
tive processes (Pennebaker & King, 1999).
While most clinical corpora focus on patients
or conditions, physician diagnostic narratives have
been successfully annotated for conceptual units
(e.g., identifying medical morphology or a differ-
ential diagnosis), by Womack et al. (2013) and
McCoy et al. (2012). Crowley et al. (2013) cre-
ated an instructional system to detect cognitive bi-
ases in clinical decision-making, while Coderre et
al. (2003) used protocol analysis on think-aloud
diagnostic narratives, and found that features of
intuitive reasoning implied diagnostic accuracy.
In this study, speech data were collected from
physicians as they diagnosed dermatological cases
presented to them as images. Physician verbaliza-
tions were annotated for decision style on a four-
point scale from intuitive to analytical (Figure 1).
Importantly, cognitive psychologists were brought
into the loop for decision style annotation, to take
advantage of their expertise in decision theory.
Figure 1: The decision-making continuum, show-
ing the four-point rating scale. The example nar-
ratives were by two physicians for the same image
(used with permission from Logical Images, Inc.),
both correct in diagnosis. (I=Intuitive, BI=Both-
Intuitive, BA=Both-Analytical, A=Analytical).
This work describes a thorough methodology
applied in annotating a corpus of diagnostic nar-
ratives for decision style. The corpus is a unique
resource ? the first of its kind ? for studying and
modeling clinical decision style or for developing
instructional systems for training clinicians to as-
sess their reasoning processes.
This study attempts to capture empirically
decision-making constructs that are much-
83
Figure 2: Overview of annotation methodology. Conclusions from the pilot study enhanced the main
annotation study. To ensure high-quality annotation, narratives appeared in random order, and 10% (86)
of narratives were duplicated and evenly distributed in the annotation data, to later assess intra-annotator
reliability. Questionnaires were also interspersed at 5 equal intervals to study annotator strategy.
discussed theoretically. Thus, it responds to the
need for investigating subjective natural language
phenomena (Alm, 2011). The annotated corpus is
a springboard for decision research in medicine,
as well as other mission-critical domains in which
good decisions save lives, time, and money.
Subjective computational modeling is particu-
larly challenging because often, no real ?ground
truth? is available. Decision style is such a
fuzzy concept, lacking clear boundaries (Hamp-
ton, 1998), and its recognition develops in psy-
chologists over time, via exposure to knowledge
and practice in cognitive psychology. Interpreting
fuzzy decision categories also depends on mental
models which lack strong intersubjective agree-
ment. This is the nature, and challenge, of cap-
turing understandings that emerge organically.
This work?s contributions include (1) present-
ing a distinct clinical resource, (2) introducing a
robust method for fuzzy clinical annotation tasks,
(3) analyzing the annotated data comprehensively,
and (4) devising a new metric that links annotated
behavior to clinicians? decision-making profiles.
2 Corpus Description
In an experimental data-collection setting, 29
physicians (18 residents, 11 attendings) narrated
their diagnostic thought process while inspecting
30 clinical images of dermatological cases, for a
total of 868
1
narratives. Physicians described ob-
servations, differential and final diagnoses, and
confidence (out of 100%) in their final diagno-
sis. Later, narratives were assessed for correctness
(based on final diagnoses), and image cases were
evaluated for difficulty by a dermatologist.
3 Corpus Annotation of Decision Style
The corpus was annotated for decision style in a
pilot study and then a main annotation study (Fig-
1
Two physicians skipped 1 image during data collection.
ure 2).
2
Two annotators with graduate training
in cognitive psychology independently rated each
narrative on a four-point scale from intuitive to an-
alytical (Figure 1). The two middle labels reflect
the presence of both styles, with intuitive (BI) or
analytical (BA) reasoning being more prominent.
Since analytical reasoning involves detailed exam-
ination of alternatives, annotators were asked to
avoid using length as a proxy for decision style.
After the pilot, the annotators jointly dis-
cussed disagreements with one researcher. Inter-
annotator reliability, measured by linear weighted
kappa (Cohen, 1968), was 0.4 before and 0.8 af-
ter resolution; the latter score may be an upper
bound on agreement for clinical decision-making
annotation. As both annotators reported using
physician-provided confidence to judge decision
style, in subsequent annotation confidence men-
tions had been removed if they appeared after the
final diagnosis (most narratives), or, if intermixed
with diagnostic reasoning, replaced with dashes.
Finally, silent pauses
3
were coded as ellipses to
aid in the human parsing of the narratives.
4 Quantative Annotation Analysis
Table 1 shows the annotator rating distributions.
4
I BI BA A
A1 89 314 340 124
A2 149 329 262 127
Table 1: The distribution of ratings across the
4-point decision scale. I=Intuitive, BI=Both-
Intuitive, BA=Both-Analytical, A=Analytical;
A1=Annotator 1, A2=Annotator 2; N=867.
Though Annotator 1?s ratings skew slightly
more analytical than Annotator 2, a Kolmogorov-
2
Within a reasonable time frame, the annotations will be
made publicly available as part of a corpus release.
3
Above around 0.3 seconds (see L?ovgren & Doorn, 2005).
4
N = 867 after excluding a narrative that, during annota-
tion, was deemed too brief for decision style labeling.
84
Factor A1 (Avg) A1 (SD) A2 (Avg) A2 (SD)
Switching between decision styles 1.0 0.0 3.6 0.9
Timing of switch between decision styles 1.6 0.5 4.2 0.4
Silent pauses (...) 2.0 0.0 3.6 0.5
Filled pauses (e.g. uh, um) 2.0 0.7 3.6 0.5
Rel. (similarity) of final & differential diagnosis 2.8 0.4 3.2 0.8
Use of logical rules and inference 3.2 0.8 2.2 0.4
False starts (in speech) 3.4 0.9 2.4 0.9
Automatic vs. controlled processing 3.4 0.5 4.0 0.0
Holistic vs. sequential processing 3.6 0.5 4.4 0.5
No. of diagnoses in differential diagnoses 4.0 0.0 1.6 0.5
Word choice 4.0 0.7 2.6 0.5
Rel. (similarity) of final & first-mentioned diagnosis 4.0 0.0 4.0 0.0
Perceived attitude 4.0 0.7 4.0 0.0
Rel. timing of differential diagnosis in the narrative 4.2 0.8 2.8 0.8
Degree of associative (vs. linear, ordered) processing 4.2 0.4 3.8 0.4
Use of justification (e.g. X because Y) 4.2 0.4 4.0 0.0
Perceived confidence 4.4 0.5 4.2 0.4
Table 3: Annotators rated each of the listed factors as to how often they were used in annotation, on a
5-point Likert scale from for no narratives (1) to for all narratives (5). (Some factors slightly reworded.)
Smirnov test showed no significant difference be-
tween the two distributions (p = 0.77).
WK %FA %FA+ 1 N
A1 - A2 .43 50% 94% 867
A1 - A1 .64 67% 100% 86
A2 - A2 .43 50% 95% 86
Table 2: Inter- and intra-annotator reliability, mea-
sured by linear weighted kappa (WK), percent full
agreement (%FA); and full plus within 1-point
agreement (%FA+1). Intra-annotator reliability
was calculated for the narratives rated twice, and
inter-annotator reliability on the initial ratings.
As shown in Table 2, reliability was moderate to
good (Altman, 1991), and inter-annotator agree-
ment was well above chance (25%). Indeed, an-
notators were in full agreement, or agreed within
one rating on the continuum, on over 90% of nar-
ratives. This pattern reveals fuzzy category bound-
aries but sufficient regularity so as to be mea-
surable. This is in line with subjective natural
language phenomena, and may be a consequence
of imposing discrete categories on a continuum.
5
Annotator 1 had better intra-annotator reliability,
perhaps due to differences in annotation strategy.
5
Nonetheless, affect research has shown that scalar repre-
sentations are not immune to variation issues (Alm, 2009).
5 Annotator Strategy Analysis
Five questionnaires evenly spaced among the nar-
ratives asked annotators to rate how often they
used various factors in judging decision style (Ta-
ble 3). Factors were chosen based on discussion
with the annotators after the pilot, and referred to
in descriptions of decision styles in the annotator
instructions; the descriptions were based on char-
acteristics of each style in the cognitive psychol-
ogy literature (e.g., Evans, 2008). Factors with
high variability (SD columns in Table 3) reveal
changes in annotator strategy over time, and fac-
tors that may influence intra-annotator reliability.
Both annotators reported using the rel. (similar-
ity) of final & first-mentioned diagnosis, as well as
perceived attitude, perceived confidence, and use
of justification, to rate most narratives. Types of
processing were used by both sometimes; this is
important since these are central to the definitions
of decision style in decision-making theory.
Differences in strategies allow for the assess-
ment of annotators? individual preferences. Anno-
tator 1 often considered the no. of diagnoses in the
differential, and rel. timing of the differential, but
Annotator 2 rarely attended to them; the opposite
pattern occurred with respect to switching between
decision styles, and the timing of the switch.
The shared high factors reveal those consis-
tently linked to interpreting decision style, despite
85
the concept?s fuzzy boundaries. In contrast, the id-
iosyncratic high factors reveal starting points for
understanding fuzzy perception, and for further
calibrating inter-annotator reliability.
6 Narrative Case Study
Examining particular narratives is also instructive.
Of the 86 duplicated narratives with two ratings
per annotator, extreme agreement occurred for 22
cases (26%), meaning that all four ratings were ex-
actly the same.
6
Figure 3 (top) shows such a case
of intuitive reasoning: a quick decision without re-
flection or discussion of the differential. Figure
3 (middle) shows a case of analytical reasoning:
consideration of alternatives and logical inference.
Figure 3: Narratives for which annotators were in
full agreement on I (top) and A (middle) ratings,
vs. in extreme disagreement (bottom).
In the full data set (initial ratings), there were
50 cases (6%) of 2-point inter-annotator disagree-
ment and one case of 3-point inter-annotator dis-
agreement (Figure 3, bottom). This latter narra-
tive was produced by an attending (experienced
physician), 40% confident and incorrect in the fi-
nal diagnosis. Annotator 1 rated it analytical,
while Annotator 2 rated it intuitive. This is in
line with Annotator 1?s preference for analytical
ratings (Table 1). Annotator 1 may have viewed
this pattern of observation ? conclusion as logi-
cal reasoning, characteristic of analytical reason-
ing. Annotator 2 may instead have interpreted the
phrase it?s so purple it makes me think of a vas-
cular tumor...so i think [...] as intuitive, due to
the makes me think comment, indicating associa-
tive reasoning, characteristic of intuitive thinking.
This inter-annotator contrast may reflect Annota-
6
There were no cases where all four labels differed, fur-
ther emphasizing the phenomenon?s underlying regularity.
tor 1?s greater reported use of the factor logical
rules and inference (Table 3).
7 Physician Profiles of Decision Style
Annotations were also used to characterize physi-
cians? preferred decision style. A decision score
was calculated for each physician as follows:
d
p
=
1
2n
n
?
i=1
(r
A1
i
+ r
A2
i
) (1)
where p is a physician, r is a rating, n is total
images, and A1, A2 the annotators. Annotators?
initial ratings were summed ? from 1 for Intuitive
to 4 for Analytical ? for all image cases for each
physician, and divided by 2 times the number of
images, to normalize the score to a 4-point scale.
Figure 4 shows the distribution of decision scores
across residents and experienced attendings.
Residents exhibit greater variability in decision
style. While this might reflect that residents were
the majority group, it suggests that differences in
expertise are linked to decision styles; such differ-
ences hint at the potential benefits that could come
from preparing clinical trainees to self-monitor
their use of decision style. Interestingly, the over-
all distribution is skewed, with a slight preference
for analytical decision-making, and especially so
for attendings. This deserves future attention.
Figure 4: Decision score distribution by expertise.
8 Conclusion
This study exploited two layers of expertise:
physicians produced diagnostic narratives, and
trained cognitive psychologists annotated for de-
cision style. This work also highlights the impor-
tance of understanding annotator strategy, and fac-
tors influencing annotation, when fuzzy categories
are involved. Future work will examine the links
between decision style, expertise, and diagnostic
accuracy or difficulty.
86
Acknowledgements
Work supported by a CLA Faculty Dev. grant,
Xerox award, and NIH award R21 LM01002901.
Many thanks to annotators and reviewers.
This content is solely the responsibility of the
authors and does not necessarily represent the of-
ficial views of the National Institutes of Health.
References
Alm, C. O. (2009). Affect in text and speech.
Saarbr?ucken: VDM Verlag.
Alm, C. O. (2011, June). Subjective natural language
problems: Motivations, applications, characteriza-
tions, and implications. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies:
short papers-Volume 2 (pp. 107-112). Association
for Computational Linguistics.
Altman, D. (1991). Practical statistics for medical
research. London: Chapman and Hall.
Coderre, S., Mandin, H., Harasym, P. H., & Fick, G. H.
(2003). Diagnostic reasoning strategies and diagnos-
tic success. Medical Education, 37(8), 695-703.
Cohen, J. (1968). Weighted kappa: Nominal scale
agreement provision for scaled disagreement or par-
tial credit. Psychological Bulletin, 70(4), 213-220.
Crowley, R. S., Legowski, E., Medvedeva, O., Reit-
meyer, K., Tseytlin, E., Castine, M., ... & Mello-
Thoms, C. (2013). Automated detection of heuris-
tics and biases among pathologists in a computer-
based system. Advances in Health Sciences Educa-
tion, 18(3), 343-363.
Croskerry, P. (2003). The importance of cognitive er-
rors in diagnosis and strategies to minimize them.
Academic Medicine, 78(8), 775-780.
Croskerry, P. (2009). A universal model of diagnostic
reasoning. Academic Medicine, 84(8), 1022-1028.
Evans, J. (2008). Dual-processing accounts of reason-
ing, judgement and social cognition. Annual Review
of Psychology, 59, 255-278.
Hamm, R. M. (1988). Clinical intuition and clinical
analysis: Expertise and the cognitive continuum. In
J. Dowie & A.S. Elstein (Eds.), Professional judg-
ment: A reader in clinical decision making (pp. 78-
105). Cambridge, England: Cambridge University
Press.
Hammond, K. R. (1981). Principles of organization
in intuitive and analytical cognition (Report #231).
Boulder, CO: University of Colorado, Center for Re-
search on Judgment & Policy.
Hampton, J. A. (1998). Similarity-based categoriza-
tion and fuzziness of natural categories. Cognition,
65(2), 137-165.
Kahneman, D., & Frederick, S. (2002). Representa-
tiveness revisited: Attribute substitution in intuitive
judgment. In T. Gilovich, D. Griffin, & D. Kahne-
man (Eds.), Heuristics of intuitive judgment: Exten-
sions and applications (pp. 49-81). New York, NY:
Cambridge University Press.
L?ovgren, T., & Doorn, J. V. (2005). Influence of ma-
nipulation of short silent pause duration on speech
fluency. In Proceedings of Disfluency in Sponta-
neous Speech Workshop (pp. 123-126). Interna-
tional Speech Communication Association.
McCoy, W., Alm, C. O., Calvelli, C., Li, R., Pelz,
J. B., Shi, P., & Haake, A. (2012, July). Annota-
tion schemes to encode domain knowledge in med-
ical narratives. In Proceedings of the 6th Linguistic
Annotation Workshop (pp. 95-103). Association for
Computational Linguistics.
Norman, G. (2009). Dual processing and diagnostic er-
rors. Advances in Health Sciences Education, 14(1),
37-49.
Pennebaker, J. W., & King, L. A. (1999). Linguis-
tic styles: Language use as an individual difference.
Journal of Personality and Social Psychology, 77(6),
1296-1312.
Stanovich, K. E., & West, R. F. (2000). Individual
differences in reasoning: Implications for the ratio-
nality debate? Behavioral and Brain Sciences, 23,
645-665.
Womack, K., Alm, C. O., Calvelli, C., Pelz, J. B.,
Shi, P., and Haake, A. (2013, August). Using lin-
guistic analysis to characterize conceptual units of
thought in spoken medical narratives. In Proceed-
ings of Interspeech 2013 (pp. 3722-3726). Interna-
tional Speech Communication Association.
87
LAW VIII - The 8th Linguistic Annotation Workshop, pages 129?138,
Dublin, Ireland, August 23-24 2014.
Towards Automatic Annotation of Clinical Decision-Making Style
Limor Hochberg
1
Cecilia O. Alm
1
Esa M. Rantanen
1
Qi Yu
2
Caroline M. DeLong
1
Anne Haake
2
1 College of Liberal Arts 2 College of Computing & Information Sciences
Rochester Institute of Technology
lxh6513|coagla|emrgsh|qi.yu|cmdgsh|anne.haake@rit.edu
Abstract
Clinical decision-making has high-stakes outcomes for both physicians and patients, yet little
research has attempted to model and automatically annotate such decision-making. The dual
process model (Evans, 2008) posits two types of decision-making, which may be ordered on
a continuum from intuitive to analytical (Hammond, 1981). Training clinicians to recognize
decision-making style and select the most appropriate mode of reasoning for a particular context
may help reduce diagnostic error (Norman, 2009). This study makes preliminary steps towards
detection of decision style, based on an annotated dataset of image-based clinical reasoning in
which speech data were collected from physicians as they inspected images of dermatological
cases and moved towards diagnosis (Hochberg et al., 2014). A classifier was developed based on
lexical, speech, disfluency, physician demographic, cognitive, and diagnostic difficulty features.
Using random forests for binary classification of intuitive vs. analytical decision style in physi-
cians? diagnostic descriptions, the model improved on the baseline by over 30%. The introduced
computational model provides construct validity for decision styles, as well as insights into the
linguistic expression of decision-making. Eventually, such modeling may be incorporated into
instructional systems that teach clinicians to become more effective decision makers.
1 Introduction
Diagnostic accuracy is critical for both physicians and patients, but there is insufficient training on clini-
cal decision-making strategy in medical schools, towards avoiding diagnostic error (Graber et al., 2012;
Croskerry & Norman, 2008). Berner and Graber (2008) estimate that diagnostic error in medicine occurs
at a rate of 5-15%, and that two-thirds of diagnostic errors involve cognitive root causes.
The dual process model distinguishes between intuitive and analytic modes of reasoning (Kahneman
& Frederick, 2002; Evans, 1989). Use of the intuitive system, while efficient, may lead to cognitive
errors based on heuristics and biases (Graber, 2009). Croskerry (2003) distinguished over 30 such biases
and heuristics that underlie diagnostic error, including anchoring, base-rate neglect, and hindsight bias.
Hammond?s (1981) Cognitive Continuum Theory proposes that decision-making lies on a continuum
from intuitive to analytical reasoning. Intuitive reasoning is described as rapid, unconscious, moderately
accurate, and employing simultaneous use of cues and pattern recognition (Hammond, 1981). Analytical
decision-making is described as slow, conscious, task-specific, more accurate, making sequential use of
cues, and applying logical rules (Hammond, 1996). Much reasoning is quasirational: between the two
poles of purely intuitive and purely analytical decision-making (Hamm, 1988; Hammond, 1981).
Cader et al. (2005) suggested that cognitive continuum theory is appropriate for the evaluation of
decision-making in medical contexts. The current study links to another work (Hochberg et al., 2014),
where the cognitive continuum was applied to physician decision-making in dermatology. Decision style
was manually assessed in physician verbalizations during medical image inspection. Figure 1 shows the
4-point annotation scheme, ranging from intuitive to analytical; the two intermediate points on the scale
reflect the presence of both styles, with intuitive (BI) or analytical (BA) reasoning more prevalent.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
129
Figure 1: Four narratives along the intuitive-analytical decision-making continuum, for which annotators
agreed on their labels, where I=Intuitive, BI=Both-Intuitive, BA=Both-Analytical, A=Analytical. The
narratives were produced by different physicians for the same image case (left, used with permission
from Logical Images, Inc.), and all four physicians were correct in their final diagnosis. (Confidence
mentions were removed in narratives presented to annotators, to avoid any potential bias.)
This work describes computational modeling for automatic annotation of decision style using this
annotated dataset, on the basis of linguistic, speaker, and image case features.
1.1 Contributions
To date, this appears to be the first study attempting to computationally predict physician decision style.
Similar to the case of affect, automatic annotation of decision style can be characterized as a subjective
natural language processing problem (Alm, 2011). This adds special challenges to the modeling process.
Accordingly, this work details a thorough process for moving from manual to automatic annotation.
This study contributes to cognitive psychology, annotation methodology, and clinical computational
linguistic analysis. Methodologically, the study details a careful process for selecting and labeling manu-
ally annotated data for modeling in the realm of subjective natural language phenomena, thus addressing
the need for their characterization (Alm, 2011). Theoretically, acceptable annotator reliability on deci-
sion style, along with successful computational modeling, will lend construct validity to the dual process
model. From a linguistic perspective, the identification of discriminative features for intuitive and analyt-
ical reasoning provides a springboard for further studying decision-making using language as a cognitive
sensor.
Practically, prediction of decision style would also be useful for determining whether individuals are
using the appropriate style for a particular task, based on analyses linking decision style to task perfor-
mance. Importantly, detection of decision style from observable linguistic behaviors allows for objective
measurement that avoids biases present in self-report surveys (Sj?oberg, 2003; Allinson & Hayes, 1996).
130
2 Data and Manual Decision Style Annotation
The annotated corpus used in this study was introduced in Hochberg et al. (2014), which also discusses
the manual annotation scheme and annotator strategies in greater detail. For clarity, the dataset and
annotation scheme are described here briefly.
The dataset consisted of spoken narratives collected from 29 physicians as they examined 30 clinical
images of dermatological cases, for a total of 867
1
narratives. Physicians described their reasoning
process as they advanced towards a diagnosis, and they also estimated their confidence
2
in their final
diagnosis. Narratives were assessed for correctness (based on final diagnoses) and image cases were
evaluated for difficulty by a practicing dermatologist.
3
For the manual annotation of decision style, anonymized text transcripts of the narratives were pre-
sented to two annotators with graduate training in cognitive psychology.
4
Analytical reasoning considers
more alternatives in greater detail. Thus, it was expected to be associated with longer narratives, as
Figure 1 illustrates. Therefore, annotators were asked not to use length as a proxy for decision style.
Narratives were randomized to ensure high-quality annotation, and 10% of narratives were duplicated
to measure intra-annotator reliability. For analysis, primary ratings were used, and secondary ratings (on
duplicated narratives) were used to measure intra-annotator consistency. The kappa scores and proportion
agreement, detailed below, motivate the labeling and data selection process used for classification and
modeling in this work.
Figure 2 shows the distribution of annotation labels for both annotators, respectively, for the whole
dataset, on the original 4-point scale. In comparison, Figure 3 shows the annotators? distributions across
a collapsed 2-point scale of intuitive vs. analytical, where, for each annotator, narratives labeled BI were
assigned to I and those labeled BA assigned to A.
Figure 2: The distribution of ratings among the
decision-making spectrum, on a 4-point scale.
Figure 3: The distribution of ratings among the
decision-making spectrum, on a 2-point scale.
Annotator agreement was well above chance for both the 4-point (Figure 4) and 2-point (Figure 5)
scales. Notably, the annotators were in full agreement or agreed within one rating for over 90% of nar-
ratives on the original 4-point scale. This pattern of variation reveals both the fuzziness of the categories
and also that the subjective perception of decision-making style is systematic.
Annotator agreement was also assessed via linear weighted kappa scores (Cohen, 1968). As shown in
Figure 6, inter-annotator reliability was moderate, and intra-annotator reliability was moderate (Annota-
tor 2) to good (Annotator 1); see Landis and Koch (1977) and Altman (1991).
Since both proportion agreement and kappa scores were slightly higher for the 2-point scale, the
automatic annotation modeling discussed below used this binary scale. In addition, the distribution of
1
One narrative was excluded due to extreme brevity, and two physicians each skipped an image during data collection.
2
For consistency, this paper uses the term confidence, treated as interchangeable with certainty and similar synonymous
expressions used by clinicians in the medical narratives, such as sure, certain, confident, just certainty percentages, etc.
3
Some imperfections may occur in the data, e.g., in transcriptions, difficulty ratings, or annotations (or in extracted features).
4
Annotator instructions included decision style definitions, a description of the 4-point scale and example narratives. Anno-
tators were asked to focus on decision style as present in the text rather than speculate beyond it.
131
Figure 4: Inter- and intra-annotator reliability for
the 4-point scheme, by proportion agreement. The
reference line shows chance agreement (25%).
(A1=Annotator 1; A2=Annotator 2).
Figure 5: Inter- and intra-annotator reliability for
the 2-point scheme, by proportion agreement. The
reference line shows chance agreement (50%).
(A1=Annotator 1; A2=Annotator 2).
Figure 6: Annotator reliability, as measured by linear weighted kappa scores on the 2-pt and 4-pt scales.
data across binary classes was more balanced compared to the 4-point scale, as shown by the contrast
between Figures 2 and 3, further making it a suitable starting point for computational modeling.
2.1 Data Selection and Labeling for Computational Modeling
This section details the systematic method used to select data for model development. The goal of the
work was to develop a computational model that could automatically annotate narratives as intuitive
or analytical, based on lexical, speech, disfluency, physician demographic, cognitive, and diagnostic
difficulty features. The study employed a supervised learning approach, and since no real ground truth
was available, it relied on manual annotation of each narrative for decision style. However, annotators did
not always agree on the labels, as discussed above. Thus, strategies were developed to label narratives,
including in the case of disagreement (Figure 7).
The dataset used for modeling consisted of 672 narratives.
5
Annotators were in full agreement for 614
ratings on the binary scale of intuitive vs. analytical (Figure 8).
6
Next, 49 narratives were assigned a
binary label based on the center of gravity of both annotators? primary ratings (Figure 9). For example,
if a narrative was rated as Intuitive and Both-Analytical by Annotators 1 and 2, respectively, the center of
gravity was at Both-Intuitive, resulting in an Intuitive label. Finally, 9 narratives were labeled using the
annotators? secondary ratings,
7
available for 10% of narratives, to resolve annotator disagreement.
8
5
Within a reasonable time frame, the text data are expected to be made publicly available.
6
Excluding also narratives lacking confidence or correctness information.
7
Collected to measure intra-annotator reliability.
8
For example, if the primary ratings of Annotator 1 and Annotator 2 were Both-Analytical and Both-Intuitive, respectively,
but both annotators? secondary ratings were intuitive (e.g., Both-Intuitive or Intuitive), the narrative was labeled Intuitive.
132
Narratives with disagreements that could not be resolved in these ways were excluded. As perception
of decision-making style is subject to variation in human judgment, this work focused on an initial
modeling of data which represent the clearer-cut cases of decision style (rather than the disagreement
gray zone on this gradient perception continuum). From the perspective of dealing with a subjective
problem, this approach enables an approximation of ground truth, as a validation concept.
9
Figure 7: Narrative labeling pipeline. 614 narratives were labeled due to full binary agreement, and
center-of-gravity and secondary rating strategies were used to label an additional 58 narratives for which
annotators were not in agreement.
Figure 8: Demonstration of initial corpus labeling,
in which 614 narratives were labeled on the basis
of binary agreement.
Figure 9: Demonstration of center-of-gravity
strategy, used to label an additional 49 narratives.
2.2 Relationship Between Physicians? Diagnostic Correctness and Decision Style
Using the 672 narratives selected for modeling, Table 1 shows the relationship of physicians? diagnostic
correctness by decision style (intuitive vs. analytical on a binary scale).
Correct Incorrect Total
Intuitive 158 186 344
Analytical 106 222 328
Total 264 408 672
Table 1: Distribution of diagnostic correctness by decision style.
Overall, there was a slightly higher prevalence of intuitive reasoning, and there were more incorrect
than correct diagnoses.
10
Table 1 also suggests a relationship between correctness and decision-making
style, where for correct diagnoses, intuitive reasoning was more dominant. The opposite trend held
for incorrect diagnoses: analytical reasoning was more frequent. Indeed, a chi-square test revealed a
significant relationship between correctness and decision style, ?
2
(1, N = 672) = 13.05, p < 0.01.
This pattern is in line with claims that intuitive reasoning is linked to better performance when much
information is to be processed; mechanisms of intuitive reasoning and pattern recognition allow individ-
uals to overcome the limitations of their working memory (Evans, 2008). However, others have linked
intuitive reasoning to decreased diagnostic accuracy, as intuitive reasoning may be prey to inappropriate
9
Modeling of fuzzier, hard to label data, is left to future work. One possible approach is to learn the labels by using a
k-nearest neighbor classifier, which identifies the most similar narratives and uses their labels to make the prediction.
10
Contributing factors to the proportion of incorrect diagnoses might include case difficulty levels in the experimental sce-
nario, and that physicians did not have access to additional information, such as patient history or follow-up tests.
133
heuristics and biases (Croskerry, 2003). Viewed from the perspective of cognitive continuum theory, the
higher prevalence of incorrect diagnoses may be due to the use of decision styles that were not suited to
the task demands of the particular case (Hammond, 1981). Finally, it might be the case that diagnostic
difficulty was a moderating variable, where physicians preferred intuitive reasoning for less challenging
cases, and analytical reasoning for more difficult cases.
3 Methods
A model was developed for the binary prediction case (intuitive vs. analytical), since the 2-point rating
scheme had slightly higher annotator agreement (see Section 2). Model development and analysis were
performed using the WEKA data mining software package (Hall et al., 2009). The dataset was split into
80% development and 20% final test sets (Table 2).
11
Parameter tuning was performed using 10-fold
cross-validation on the best features in the development set.
12
80% Development Set 20% Final Test Set
Intuitive 276 (51%) 68 (51%)
Analytical 263 (49%) 65 (49%)
Total 539 133
Table 2: Class label statistics.
3.1 Features
Three feature types were derived from the spoken narratives to study the linguistic link to decision-
making style: lexical (37), speech (13), and disfluency (3) features. Three other feature types relevant to
decision-making were demographic (2), cognitive (2), and difficulty (2) features (Table 3).
Type Feature Description / Examples
Lexical
exclusion but, without
inclusion both, with
insight think, know
tentative maybe, perhaps
cause because, therefore
cognitive process know, whether
. . .
Speech
speech length number of tokens
pitch min, max, mean, st. dev., time of min/max
intensity min, max, mean, st. dev., time of min/max
Disfluency
silent pauses number of
fillers like, blah
nonfluencies uh, um
Demographic
gender male, female
status resident, attending
Cognitive
confidence percentage
correctness binary
Difficulty
expert rating ordinal ranking
% correctness/image percentage
Table 3: Six feature types. The listed lexical features are a sub-sample of the total set.
Relevant lexical features were extracted with the Linguistic Inquiry and Word Count (LIWC) software,
which calculates the relative frequency of syntactic and semantic classes in text samples based on val-
11
This split rests on the assumption that physicians may share common styles. Thus, the testing data will represent different
physicians, but the styles themselves have been captured by the training data so that they can be correctly classified; the same
rationale can be applied to image cases. To further investigate the phenomenon and identify the degree of inter- and intra-
individual variation in decision style, future work could experiment with holding out particular images and physicians.
12
In Section 4.1, parameters were tuned for each case of feature combinations in a similar way.
134
idated, researched dictionaries (Tausczik & Pennebaker, 2010). Disfluency features were silent pauses,
and the frequency of fillers and nonfluencies as computed by LIWC. Speech features are in Table 3.
Besides linguistic features, three additional groups of features were included, with an eye towards
application. Demographic features were gender and professional status, while cognitive features were
physician confidence in diagnosis and correctness of the final diagnosis. Difficulty features consisted
of an expert-assigned rank of diagnostic case difficulty, and the percent of correct diagnoses given by
physicians for each image, calculated on the development data only. In an instructional system, a trainee
could input a demographic profile, and the system could also collect performance data over time, while
also taking into account stored information on case difficulty when available. This information could
then be used in modeling of decision style in spoken or written diagnostic narratives.
3.2 Feature Selection
WEKA?s CfsSubsetEval, an attribute evaluator, was used for feature selection,
13
using 10-fold cross-
validation on the development set only. Features selected by the evaluator in at least 5 of 10 folds were
considered best features. The best features from the entire feature set were: 2nd person pronouns, con-
junctions, cognitive process, insight, cause, bio, and time words, plus silent pauses, speech length, time of
min. pitch, standard deviation of pitch, time of min. intensity, and difficulty: percent correctness/image.
Feature selection, using the same attribute evaluator, was also performed on only the lexical fea-
tures, which could be a starting point for analysis of decision-making style in text-only data. The best
lexical features
14
included conjunctions, cause, cognitive process, inclusion, exclusion, and perception
words. These lexical items seem associated with careful examination and reasoning, which might be
more present in analytical decision-making and less present in intuitive decision-making. Some cate-
gories, especially inclusion (e.g., with, and), exclusion (e.g., but, either, unless), and cause words (e.g.,
affect, cause, depend, therefore), seem particularly good representatives of logical reasoning and justifi-
cation, a key feature of analytical reasoning. But as shown in the next section, when available, speech
and disfluency information is useful, and potentially more so than some lexical features.
15
4 Results and Discussion
Table 4 lists the results for the Random Forest (Breiman, 2001) and Logistic Regression (Cox, 1972)
classifiers on the best features (as selected from all features) on the final test set, after training on the
development set. These results suggest that decision style can be quantified and classified on a binary
scale; the percent error reduction (compared to baseline performance) for both classifiers is substantial.
Classifier %Acc %ER Pr Re
Random Forest 88 76 88 88
Logistic Regression 84 67 84 84
Majority Class Baseline 51 ? ? ?
Table 4: Performance on final test set; reduction in error is calculated relative to majority class baseline.
Precision and recall are macro-averages of the two classes.
4.1 Feature Combination Exploration
A study of feature combinations was performed on the final test set with Random Forest (Table 5) to
explore the contribution of each feature type towards automatic annotation. The best performance was
achieved after applying feature selection on all features. Lexical and disfluency features were useful for
determining decision style, and the best linguistic features (chosen with feature selection) were slightly
more useful. These latter feature types improve on the performance achieved when considering only
13
With BestFirst search method.
14
Best lexical features were: function words, singular pronouns, prepositions, conjunctions, quantifiers, and cognitive pro-
cess, cause, discrepancy, tentative, inclusion, exclusion, perception, see, bio, motion, time, and assent words.
15
Feature selection was also performed only on the linguistic (lexical, speech, and disfluency) features as a group. The best
features of these types were: second personal pronouns, conjunctions, cognitive process, insight, cause, bio, and time words;
silent pauses; and speech length, time of minimum pitch, standard deviation of pitch, and time of minimum intensity. They
could represent a starting for point for analyzing speech data not enhanced by additional speaker and task information.
135
speech length and silent pauses, which were apparent characteristics to the human annotators and among
the best features (see Section 3.2.).
Demographic features improved somewhat over the baseline, indicating an association between gen-
der, professional status, and decision-making, and adding cognitive features increased performance. Im-
portantly, overall these findings hint at linguistic markers as key indicators of decision style.
Features Accuracy
All* 88
All 85
(Lexical + Speech + Disfluency)* 86
Lexical + Speech + Disfluency 84
Lexical + Disfluency 84
Only speech length and silent pauses 81
Disfluency 79
Lexical 77
Demographic + Cognitive 68
Demographic 64
Majority Class Baseline 51
Table 5: Performance on final test set. Star (*) indicates the use of feature selection (see Section 3.2.)
4.2 Limitations
In this study, doctors diagnosed solely on the basis of visual information (e.g., without tests or follow-
up), so their speech may reflect only part of the clinical reasoning process. In addition, most decision
style ratings on the 4-point scale were in the distribution center (Figure 2), so the binary labels used in
the study only partially reflect purely intuitive or purely analytical reasoning. However, since clinician
reasoning in the current dataset can be reliably measured by human and computational classification,
linguistic features of decision style must be present. Finally, the LIWC software used for lexical features
matches surface strings rather than senses; future work might operate on the sense rather than token level.
5 Related Work
Lauri et al. (2001) asked nurses in five countries to rate statements representative of intuitive or analytical
decision-making on a 5-point scale. They found that reasoning varies with context and that styles in the
middle of the cognitive continuum predominate. In this work, annotation ratings were prevalent in the
middle of the spectrum. Thus, both studies endorse that most decision-making occurs in the central part
of the continuum (Hamm, 1988; Hammond, 1981). Womack et al. (2012) proposed that silent pauses in
physician narration may indicate cognitive processing. Here, silent pauses were also important, perhaps
because analytical decision-making may recruit more cognitive resources than intuitive decision-making.
6 Conclusion
This work suggests that decision style is revealed in language use, in line with claims that linguistic
data reflect speakers? cognitive processes (Pennebaker & King, 1999; Tausczik & Pennebaker, 2010).
Theoretically, the study adds validity to the dual process and cognitive continuum theories. Methodolog-
ically, it articulates a method of transitioning from manual to automatic annotation of fuzzy semantic
phenomena, including label adjudication and data selection for computational modeling. Future work
may investigate modeling of the 4-point decision scale, as well as whether particular variables, such as
difficulty or expertise, mediate the relationship between diagnostic correctness and decision style.
Practically, automatic detection of decision style is useful for both clinical educational systems and
mission-critical environments. Clinical instructional systems can assess whether trainees are using the
appropriate style for a particular task (Hammond, 1981), and they can help users determine and attend to
their own decision styles, towards improving diagnostic skill (Norman, 2009). Finally, in mission-critical
environments, linguistic markers of decision-making style may be used to determine the optimal modes
of reasoning for a particular task in high-stakes human factors domains.
136
Acknowledgements
This work was supported by a COLA Faculty Development grant, Xerox award, and NIH award R21
LM01002901. Many thanks to the annotators and reviewers. This content is solely the responsibility of
the authors and does not necessarily represent the official views of the National Institutes of Health.
References
Allinson, C. W., & Hayes, J. (1996). The cognitive style index: A measure of intuition-analysis for organizational
research. Journal of Management Studies, 33(1), 119-135.
Alm, C. O. (2011, June). Subjective natural language problems: Motivations, applications, characterizations, and
implications. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies: Short papers-Volume 2 (pp. 107-112). Association for Computational Linguistics.
Altman, D. (1991). Practical statistics for medical research. London: Chapman and Hall.
Berner, E. S., & Graber, M. L. (2008). Overconfidence as a cause of diagnostic error in medicine. American
Journal of Medicine, 121, S2-S23.
Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.
Cader, R., Campbell, S., & Watson, D. (2005). Cognitive continuum theory in nursing decision-making. Journal
of Advanced Nursing, 49(4), 397-405.
Cohen, J. (1968). Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.
Psychological Bulletin, 70(4), 213-220.
Cox, D. R. (1972). Regression models and life tables. Journal of the Royal Statistical Society, Series B, 34(2),
187-220.
Croskerry, P. (2003). The importance of cognitive errors in diagnosis and strategies to minimize them. Academic
Medicine, 78, 775-780.
Croskerry, P., & Norman, G. (2008). Overconfidence in clinical decision making. The American Journal of
Medicine, 121(5), S24-S29.
Evans, J. (1989). Bias in human reasoning: Causes and consequences. Hillsdale, NJ: Erlbaum.
Evans, J. (2008). Dual-processing accounts of reasoning, judgment and social cognition. Annual Review of Psy-
chology, 59, 255-278.
Graber, M. (2009). Educational strategies to reduce diagnostic error: Can you teach this stuff? Advances in Health
Sciences Education, 14, 63-69.
Graber, M. L., Kissam, S., Payne, V. L., Meyer, A. N., Sorensen, A., Lenfestey, N., ... & Singh, H. (2012).
Cognitive interventions to reduce diagnostic error: A narrative review. BMJ Quality & Safety, 2(7), 535-557.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009). The WEKA data mining
software: An update. ACM SIGKDD Explorations Newsletter, 11(1), 10-18.
Hamm, R. M. (1988). Clinical intuition and clinical analysis: Expertise and the cognitive continuum. In J. Dowie
& A.S. Elstein (Eds.), Professional judgment: A reader in clinical decision making (pp. 78-105). Cambridge,
England: Cambridge University Press.
Hammond, K. R. (1981). Principles of organization in intuitive and analytical cognition (Report #231). Boulder,
CO: University of Colorado, Center for Research on Judgment & Policy.
Hammond, K. R. (1996). Human judgement and social policy: Irreducible uncertainty, inevitable error, unavoid-
able injustice. New York, NY: Oxford University Press.
Hochberg, L., Alm, C. O., Rantanen, E. M., DeLong, C.M., & Haake, A. (2014). Decision style in a clinical
reasoning corpus. In Proceedings of the BioNLP Workshop (pp. 83-87). Baltimore, MD: Association for Com-
putational Linguistics.
Kahneman, D., & Frederick, S. (2002). Representativeness revisited: Attribute substitution in intuitive judgment.
In T. Gilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics of intuitive judgment: Extensions and applications
(pp. 49-81). New York, NY: Cambridge University Press.
137
Lauri, S., Salanter?a, S., Chalmers, K., Ekman, S. L., Kim, H. S., K?appeli, S., & MacLeod, M. (2001). An
exploratory study of clinical decision-making in five countries. Journal of Nursing Scholarship, 33(1), 83-90.
Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics,
33(1), 159-174.
Norman, G. (2009). Dual processing and diagnostic errors. Advances in Health Sciences Education, 14(1), 37-49.
Pennebaker, J. W., & King, L. A. (1999). Linguistic styles: Language use as an individual difference. Journal of
Personality and Social Psychology, 77(6), 1296-1312.
Sj?oberg, L. (2003). Intuitive vs. analytical decision making: Which is preferred? Scandinavian Journal of Man-
agement, 19(1), 17-29.
Tausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text
analysis methods. Journal of Language and Social Psychology, 29(1), 24-54.
Womack, K., McCoy, W., Alm, C. O., Calvelli, C., Pelz, J. B., Shi, P., & Haake, A. (2012, July). Disfluencies
as extra-propositional indicators of cognitive processing. Proceedings of the Workshop on Extra-Propositional
Aspects of Meaning in Computational Linguistics (pp. 1-9). Association for Computational Linguistics.
138
