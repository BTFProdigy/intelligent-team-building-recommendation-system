Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1728?1739, Dublin, Ireland, August 23-29 2014.
Towards Semantic Validation of a Derivational Lexicon
Britta D. Zeller
?
Sebastian Pad
?
o
?
Jan
?
Snajder
?
?
Heidelberg University, Institut f?ur Computerlinguistik
69120 Heidelberg, Germany
?
Stuttgart University, Institut f?ur maschinelle Sprachverarbeitung
70569 Stuttgart, Germany
?
University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
zeller@cl.uni-heidelberg.de pado@ims.uni-stuttgart.de jan.snajder@fer.hr
Abstract
Derivationally related lemmas like friend
N
? friendly
A
? friendship
N
are derived from a common
stem. Frequently, their meanings are also systematically related. However, there are also many
examples of derivationally related lemma pairs whose meanings differ substantially, e.g., object
N
? objective
N
. Most broad-coverage derivational lexicons do not reflect this distinction, mixing up
semantically related and unrelated word pairs.
In this paper, we investigate strategies to recover the above distinction by recognizing semantically
related lemma pairs, a process we call semantic validation. We make two main contributions:
First, we perform a detailed data analysis on the basis of a large German derivational lexicon. It
reveals two promising sources of information (distributional semantics and structural information
about derivational rules), but also systematic problems with these sources. Second, we develop
a classification model for the task that reflects the noisy nature of the data. It achieves an
improvement of 13.6% in precision and 5.8% in F1-score over a strong majority class baseline.
Our experiments confirm that both information sources contribute to semantic validation, and that
they are complementary enough that the best results are obtained from a combined model.
1 Introduction
Morphological processing forms the first step of virtually all linguistic processing toolchains in natural
language processing (NLP) and precedes other analyses such as part of speech tagging, parsing, or
named entity recognition. There are three major types of morphological processes: (a) Inflection modifies
word forms according to the grammatical context; (b) derivation constructs new words from individual
existing words, typically through affixation; (c) composition combines multiple words into new lexical
items. Computational treatment of morphology is often restricted to normalization, such as lemmatization
(covering inflection only) or stemming (covering inflection and derivation heuristically, Porter (1980)).
An important reason is that English is morphologically a relatively simple language. Composition is
not marked morphologically (zoo gate) and an important derivational pattern is zero derivation where the
input and output terms are identical surface forms (a fish / to fish). Thus, lemmatization or stemming go a
long way towards treating the aspects of English morphology relevant for NLP. The situation is different
for languages with a complex morphology that calls for explicit treatment. In fact, recent years have seen
a growing body of computational work in particular on derivation, which is a very productive process of
word formation in Slavic languages but also in languages more closely related to English, like German
(
?
Stekauer and Lieber, 2005).
Derivation comprises a large number of distinct patterns, many of which cross part of speech boundaries
(nominalization, verbalization, adjectivization), but some of which do not (gender indicators like master /
mistress, approximations like red / reddish). A simple way to conceptualize derivation is that it partitions a
language?s vocabulary into derivational families of derivationally related lemmas (cf. Zeller et al. (2013),
Gaussier (1999)). In WordNet, this type of information has been included to some extent by so-called
?morpho-semantic? relations (Fellbaum et al., 2009), and the approach has been applied to languages other
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1728
lachen Lacher l?acherlich
sfx ?er?V N
Append suffix ?er? to the stem of
the verb to obtain a noun
try uml &
sfx ?lich?N
A
Try to turn the noun?s vowels into umlauts, then
append suffix ?lich? to obtain an adjective
to laugh laugh laughable
Figure 1: (Part of) a derivational family from DERIVBASE including derivational rules
than English (Bilgin et al., 2004; Pala and Hlav?a?ckov?a, 2007). Another source of derivational information
are stand-alone derivational lexicons such as CatVar (Habash and Dorr, 2003) for English, DERIVBASE
(Zeller et al., 2013) for German, or the multilingual CELEX (Baayen et al., 1996).
Recent work has demonstrated that NLP can benefit from derivational knowledge. Shnarch et al. (2011)
employ derivational knowledge in recognizing English textual entailment to better gauge the semantic
similarity of text and hypothesis. Pad?o et al. (2013) improve the prediction of German semantic similarity
judgments for lemma pairs by backing off to derivational families for infrequent lemmas. Luong et al.
(2013) and Lazaridou et al. (2013) improve distributional semantic representations.
Note that all of these applications make use of derivational knowledge to address various semantic tasks,
working on the assumption that derivationally related words, as represented in derivational lexicons, are
strongly semantically related. This assumption is not completely warranted, though. The development of
wide-coverage derivational lexicons is generally driven by morphological information, using for example
finite-state technology (Karttunen and Beesley, 2005) to characterize known derivational patterns in terms
of string transformations. Even though there is a strong correlation in derivation between morphology
and semantics, it is not perfect. The absence of (synchronic) semantic relatedness can have a number of
reasons, including accidental instantiation of derivational patterns (corn ? corner) and diachronic meaning
drift (dog (animal) ? dogged (determined)). In other words, a substantial number of the lemma pairs in
those lexicons are false positives regarding the level of semantic relatedness.
Our goal in this paper is to ameliorate this situation by developing strategies for the semantic validation
of derivational lexicons, i.e., methods to determine, for lemma pairs that are derivationally related
at the morphological level, whether they are in fact semantically related. We base our study on the
German derivational lexicon DERIVBASE, and start by assessing which strategies can be used for its
semantic validation (Section 2). In Sections 3 and 4, we analyze the contributions of semantic information
(distributional semantics) as well as structural information (derivational rules). On the basis of our
observations, we train a classifier that is able to semantically validate DERIVBASE at 89.9% F
1
-score
(Section 5), significantly outperforming a majority-class baseline of 84.1%. Section 6 reviews related
work. Section 7 concludes the paper and outlines future work.
2 A Lexicon for German Derivation
2.1 DERIVBASE
DERIVBASE (Zeller et al., 2013) is a freely available derivational lexicon for German.
1
We used a
rule-based framework to define derivation rules that cover suffixation, prefixation, and zero derivation as
well as stem changes. Following the work of
?
Snajder and Dalbelo Ba?si?c (2010), derivational processes are
defined using derivational rules and higher-order string transformation functions. The only requirements
for this method are (a) a comprehensive set of lemmas and (b) knowledge about admissible derivational
rules, which can be gathered, for example, from linguistics textbooks.
Figure 1 shows a small sample from a derivational family with three lemmas and two derivational rules,
one turning a verb into the corresponding event noun (in this case a semelfactive), and one turning the
event into an adjective associated with it. Note that there are two perspectives on such a family: It can
1
http://www.cl.uni-heidelberg.de/
?
zeller/res/derivbase/
1729
?Positive? Precision Recall
DERIVBASE release class % %
1.2 (Zeller et al., 2013)
3
R and M 83.0 71.0
1.4 (our analysis) R and M 85.1 91.4
1.4 (our analysis) R only 76.7 93.8
Table 1: DERIVBASE evaluation across releases on the DERIVBASE release 1.2 P and R samples
either be seen as a set of lemmas, or as a set of (independent) lemma pairs. We will assume the latter
perspective in this paper, leaving questions of global coherence for future work.
DERIVBASE is a good example for the problems sketched in Section 1. It is defined purely on
morphological grounds, without semantic validation of derivational families. Consequently, it contains a
substantial number of words that are not semantically related.
2.2 Morphological and Semantic Relatedness in DERIVBASE
Our original evaluation of the quality of DERIVBASE in Zeller et al. (2013) was based on manually
classified samples of lemma pairs. We introduce two samples, the ?R sample?, drawn from a large
population of lemma pairs with high string similarity, in order to calculate recall, and the ?P sample?,
drawn from the DERIVBASE families, in order to compute precision. Each lemma pair was classified into
one of five categories (R: morphologically and semantically related; M: only morphologically related;
N: not related; L: lemmatization errors; C: compounds) and inter-annotator agreement was checked to
be substantial.
2
The overall best model (L123) showed 83% precision and 71% recall. However, this
evaluation is limited in two important respects. First, it refers to DERIVBASE release 1.2 from 2013.
Since then, we have extended DERIVBASE, e.g., with rules covering particle verbs, a very productive
area of German derivation. Secondly, and more seriously, the previous evaluation considered all instances
of R and M as true positives. In other words, in Zeller et al. (2013) we only evaluated the morphological
relatedness of the lemma pairs but not the semantic relatedness.
We therefore start by presenting an evaluation of DERIVBASE focusing on the R instances in Table 1,
reusing the DERIVBASE 1.2 ?P? and ?R? samples introduced in Zeller et al. (2013, see there for evaluation
details). Between DERIVBASE 1.2 and 1.4, precision increased marginally and recall substantially, due
mainly to the inclusion of rules that cover particle verbs. However, the numbers change substantially when
only R (truly semantically related pairs) are counted as true positives. Recall increases by about 2.5%, but
precision drops about 8.5%. Almost one quarter of all pairs in the lexicon are not semantically related.
A possible confounder of this analysis is that the ?P sample? was drawn on DERIVBASE 1.2 and
therefore does not include the novel items in DERIVBASE 1.4. We therefore created a novel DERIVBASE
1.4 extended sample by combining the existing ?P sample? with those pairs from the ?R sample? that are
in the coverage of a DERIVBASE rule as of DERIVBASE 1.4, resulting in 2,545 lemma pairs.
This DERIVBASE 1.4 extended sample will form the basis of all our analyses in this paper. The class
distribution in the new sample is similar, but not identical, to the old P sample, as shown in Table 2. The
relative frequency of R drops another 2%. Since this number also corresponds to the precision of the
resource, the precision of the extended sample is 74.6%.
There are almost no compound errors C, which is not surprising given the rule-based construction of
the lexicon, and only a relatively small number (about 5%) of lemmatization errors L, which fall outside
the scope of our work. In contrast, both N and M occur with substantial frequency: Each class accounts
for around 10% of the pairs. An analysis of N shows many cases of rule overgeneration: These are often
pairs of lemmas whose stems are sufficiently similar that they might be related, e.g., by stem-changing
derivation rules. Although such rules are valid in other contexts (Verkauf
N
? Verk?aufer
N
(selling ? seller)),
2
Although we believe semantic relatedness to be fundamentally a graded scale, we adopt a binary notion of it as a convenient
operational simplification that is supported by the good inter-annotator agreement for manual labeling in DERIVBASE.
3
DERIVBASE 1.2 corresponds to DERIVBASE ?L123? in (Zeller et al., 2013, p. 1207).
1730
R M N L C
Frequency 1899 265 240 131 8
Percentage overall 74.6 10.4 9.5 5.2 0.3
Percentage on dev. set 75.5 10.3 9.0 4.8 0.3
Percentage of test set 72.6 10.6 10.6 5.9 0.3
Table 2: Class distribution in our new DERIVBASE 1.4 extended sample
erroneous application leads to N cases like Blase
N
? Bl?aser
N
(bubble ? blower). Also, we find false
matches of common noun rules with named entities (Empire
N
? Empirismus
N
(Empire ? empiricism)).
In contrast, many cases of M (as sketched in Section 1) refer to different senses of the same stem. As
an example, consider beruhen
V
? unruhig
A
(to rest on ? restless), both related to Ruhe
N
(rest). In other
cases, one of the two lemmas appears to have undergone a meaning shift (Rappel
N
? rappeln
V
(craze ?
to rattle)). This is particularly prominent for particle verbs (bauen
V
? erbaulich
A
(build ? edifying)).
We divide the DERIVBASE 1.4 extended sample into a development and a test partition (70:30 ratio);
the subsequent analyses consider only the development set.
2.3 Hypotheses for Semantic Validation
The preceding analysis of DERIVBASE has established that the lexicon contains a substantial number
(around one fourth) of lemma pairs that are not semantically related. Therefore, it is in need of semantic
validation, i.e., a computational procedure that can filter out semantically unrelated words.
In this paper, we frame semantic validation as a binary classification task that classifies all lemma pairs
within one derivational family as either semantically related or unrelated. We consider this a first step
towards splitting the current, morphologically motivated, DERIVBASE families into smaller, semantically
coherent, families. We base our work on two general hypotheses about the types of information that might
be helpful in this endeavor.
Hypothesis 1. Distributional similarity indicates semantic relatedness between derivationally related
words. The instances of polysemy and meaning shift that we observe, in particular in the M class,
motivate the use of distributional similarity (Turney and Pantel, 2010) since we expect these lemma
pairs to be distributionally less related than cases of true semantic relatedness.
Hypothesis 2. Derivational rules differ in their reliability. Both the evidence from M and N indicate
that some rules are more meaning-preserving than others. We expect this to be tied to both lexical
properties of the rules (particle verbs are more likely than diminutives to radically change meaning)
as well as structural properties (more specific rules are presumably more precise than generic rules).
In the two following Sections, we will operationalize these hypotheses and analyze the development set of
the DERIVBASE 1.4 extended sample with respect to their empirical adequacy.
3 Analysis 1: Distributional Similarity for Semantic Validation
3.1 Measuring Distributional Similarity
We examine semantic similarities as predicted by simple bag-of-words semantic space models built from
the lemmatized SDeWaC (Faa? et al., 2010), a large German web corpus containing about 880 million
words. We compute vectors for all words covered in DERIVBASE using a window of ?5 words within
sentence boundaries and considering the 10k most frequent lemma-part of speech combinations of nouns,
verbs, and adjectives in SDeWaC as contexts. Distributional vectors are built from co-occurrences which
are measured with Local Mutual Information (Evert, 2005). The semantic similarity is measured by the
cosine similarity between the vectors. Despite the size of the corpus, many lemmas from DERIVBASE
occur very infrequently, and due to the inflection in German, it is important to retrieve as many occurrences
of each lemma as possible.
1731
We therefore use a very permissive two-step lemmatization scheme that starts from lemmas from
the lexicon-based TreeTagger (Schmid, 1994), which provides reliable lemmas but with relatively low
coverage, and supplements them with lemmas and parts of speech produced by the probabilistic MATE
toolkit (Bohnet, 2010) when TreeTagger abstains.
3.2 Frequency Considerations
The advantage of the string transformation-based construction of DERIVBASE is its ability to include
infrequent lemmas in the lexicon, and in fact DERIVBASE includes more than 250,000 content lemmas,
some of which occur not more than three times in SDeWaC. However, this is a potential problem when
we build distributional representations for all lemmas in DERIVBASE since it is known from the literature
that similarity predictions for infrequent lemmas are often unreliable (Bullinaria and Levy, 2007).
Our data conform to expectations in this regard ? infrequent lemmas are indeed problematic for
validating the semantic relatedness of lemma pairs. More specifically, the semantic similarity of related
lemmas (R) is systematically underestimated, because the lemma pairs from our sample are often too
infrequent to share any dimensions. Consequently, they receive a low or zero cosine even when they are
semantically strongly related. For example, each of the lemmas Drogenverkauf
N
? Drogenverk?aufer
N
(drug selling ? drug seller) has only nine lemmas as dimensions, and those are completely disjoint. This
underestimation constitutes a general trend. The model assigns cosine scores below 0.1 to 64% of the
related pairs in the development set, cosines below 0.2 to 81%, and cosines below 0.3 to 87%. Such low
scores are problematic for separating related from unrelated pairs.
Two-step lemmatization is important for the proper handling of infrequent words. Compared to
just using TreeTagger, the TreeTagger+MATE vectors for auferstehen
V
? auferstehend
A
(to resurrect ?
resurrecting) share seven more dimensions, including Jesus, Lord, myth, and suffering. Correspondingly,
the cosine value of this pair rises by 50%. Generally, the amount of zero cosines in the DERIVBASE
1.4 extended sample drops by 45% using two-step lemmatization compared to one-step TreeTagger
lemmatization.
3.3 Conceptual Considerations
In addition to the frequency considerations discussed above, we find three conceptual phenomena that
affect distributional similarity independently of the frequency aspects.
The first one is the influence of parts of speech. Derivational rules often change the part of speech of the
input lemma, and the parts of speech of its context words change as well. This decreases context overlap.
For example,
?
Ubersch?atzung
N
? ?ubersch?atzt
A
(overestimate ? overestimated) is assigned a cosine of
merely 0.09. The upper half of Table 3 shows the top ten individual and shared context words for this
pair, ranked by LMI. The context words of the noun are mainly nominal heads of genitive complements
(overestimation of possibility/force/. . . ), while the context words of the adjective comprise many adverbs
(totally, widely, . . . ). None of the shared contexts rank among of the top ten for both target lemmas. This
is even more surprising considering that German adjectives and adverbs have the same surface realization
(as opposed to English) and are more likely to form matching context words.
The second phenomenon that we identified as influencing semantic similarity is markedness (Battistella,
1996). A considerable number of derivational rules systematically produce marked terms. A striking
example is the feminine suffix ?-in? as in Entertainer
N
? Entertainerin
N
: Although the lemmas are
intuitively very similar, their cosine is as low as 0.1. The reason is that the female versions tend to be
used in contexts where the gender of the entertainer is relevant. This is illustrated in the lower half of
Table 3. The first two contexts for both words (actor, singer) stem from frequent enumerations (actor and
entertainer X) and are almost identical, but again the female versions are marked for gender. We also find
two female given names. As a result, the target lemmas receive a low distributional similarity.
The third example are cases of mild meaning shifts that were tagged by the annotators as R. These
are lemmas where the semantic relatedness is intuitively clearly recognizable but may be accompanied
by pretty substantial changes in the distribution of contexts. Consider the semantically related pair
Absteiger
N
? absteigend
A
(descender (person) ? descending/decreasing). It achieves only a cosine of
1732
word pair (l
1
, l
2
) context(l
1
) context(l
2
) shared contexts(l
1
, l
2
)
?
Ubersch
?
atzung ?
?ubersch
?
atzt
(overestimation ?
overestimated),
cos = 0.09
eigen (own) v?ollig (totally) v?ollig (totally)
warnen (to alert) Problem (problem) M?oglichkeit (possibility)
M?oglichkeit (possibility) Gefahr (danger) Bedeutung (meaning)
f?uhren (to lead) Autor (author) Gefahr (danger)
Kraft (force) weit (widely) Einflu? (influence)
Bedeutung (meaning) total (totally) ?uberh?oht (excessive)
F?ahigkeit (ability) ernst (seriously) Macht (power)
Leistungsf?ahigkeit (performance) ?uberh?oht (excessive) gnadenlos (mercilessly)
neigen (to tend) gnadenlos (mercilessly) Kraft (force)
Einflu? (influence) Hollywood (Hollywood) h?aufig (frequent)
Entertainer ?
Entertainerin
(entertainer ? female
entertainer),
cos = 0.1
S?anger (singer) S?angerin (female singer) Schauspieler (actor)
Schauspieler (actor) Schauspielerin (actress) Musiker (musician)
Musiker (musician) Helga (female given name) Talent (talent)
Harald (male given name) Mutter (mother) bekannt (well-known)
Moderator (anchorman) ber?uhmt (famous) S?angerin (female singer)
Schmidt (surname) brillant (brilliant) beliebt (popular)
gro? (big) Lisa (female given name) gro? (big)
K?unstler (artist) K?unstlerin (female artist) ber?uhmt (famous)
Talent (talent) verstorben (deceased) Sportler (sportsman)
gut (good) Talent (talent) Schauspielerin (actress)
Table 3: Top ten individual and shared context words for
?
Ubersch?atzung
N
? ?ubersch?atzt
A
(overestimation
? overestimated) and Entertainer
N
? Entertainerin
N
. Individual context words are ranked by LMI, shared
context words by the product of their LMIs for the two target words. Shared context words that occur in
the top ten contexts for both words are marked in boldface.
0.005, because Absteiger is almost exclusively used to refer to relegated sport teams while absteigend is
used as a general verb of scalar change.
3.4 Ranking of Distributional Information
Given the results reported above, the standard distributional approach of using plain cosine scores to
measure the absolute amount of co-occurrences does not seem very promising, due to the low absolute
numbers of shared dimensions of the two lemmas. We expect other similarity measures, e.g., the Lin
measure (Lin, 1998), to perform equally poorly since they do not change the fundamental approach. Also,
although using a large corpus for semantic space construction might ameliorate the situation, we would
prefer to make improvements on the modeling side of semantic validation.
We follow the ideas of Hare et al. (2009) and Lapesa and Evert (2013) who propose to consider semantic
similarity in terms of ranks rather than absolute values. The advantage of rank-based similarity is that
it takes the density of regions in the semantic space into account. That is, a low cosine value does not
necessarily indicate low semantic relatedness ? provided that the two words are located in a ?sparse?
region. Conversely, a high cosine value can be meaningless in a densely populated region. A second
conceptual benefit of rank-based similarity is that it is directed: It is possible to distinguish the ?forward?
rank (the rank of l
1
in the neighborhood of l
2
) and the ?backward? rank (the rank of l
2
in the neighborhood
of l
1
). The previous studies found rank-based similarity to be beneficial for the prediction of priming
results. In our case, it suggests a refined version of our Hypothesis 1:
Hypothesis 1?. High rank-based distributional similarity indicates semantic relatedness between deriva-
tionally related words.
4 Analysis 2: Derivational Rules for Semantic Validation
As discussed in Section 2.3, a second source of information that should be able to complement the
problematic distributional similarity is provided by the derivational rules that are encoded in DERIVBASE
(cf. the arrows in Figure 1). Our intuition is that some rules are ?semantically stable?, meaning that they
reliably connect semantically similar lemmas, while other rules tend to cause semantic drifts. To examine
1733
this situation, we perform a qualitative analysis on all lemma pairs connected by rule paths of length one
(?simplex paths?), which are easy to analyze. Longer paths (?complex paths?) are considered below.
We find that rules indeed behave differently. For example, the ?-in? female marking rule from Section 3.3
is very reliable: every lemma pair connected by this rule is semantically related. At the other end of the
scale, there are rules that consistently lead to semantically unrelated lemmas, e.g., the ?ver-? noun-verb
prefixation: Zweifel
N
? verzweifeln
V
(doubt ? to despair). Foreign suffixes like ?-ktiv? in instruieren
V
? instruktiv
A
(to instruct ? instructive) retain semantic relatedness in most cases, but sometimes link
actually unrelated lemmas (N, C, L). For example, Objektiv
N
? Objektivismus
N
(lens ? objectivism),
is an N pair for the suffix ?-ismus?. Finally, zero derivations and very short suffixes are less reliable:
Since they easily match, they are often applied to incorrectly lemmatized words (L). For example, the
?-n? suffix, which relates nationalities with countries (Schwede
N
? Schweden
N
(Swede ? Sweden)). It
matches many wrongly lemmatized nouns due to its syncretism with the plural dative/accusative suffix -n,
as in Schweineschnitzel
N
? Schweineschnitzeln
N
(pork cutlet ? pork cutlets
dat/acc-pl
). This suggests that
rule-specific reliability is a promising feature for semantic validation. Fortunately, due to its construction,
DERIVBASE provides a rule chain for each lemma pair so that these reliabilities can be ?read off?. For
other rules, however, the variance of the individual lemma pairs that instantiate the rule is large, and the
applicability of the rule is influenced by the particular combination of rule and lemma pair. Such cases
suggest that distributional knowledge and structural rule information should be combined, a direction that
we will pursue in the next section.
On word pairs that are linked by ?complex paths?, i.e., more than one rule (lachen
V
? l?acherlich
A
in
Figure 1), our main observation in this respect is that rule paths show a clear ?weakest link? property. One
unreliable rule can be sufficient to cause a semantic drift, and only a sequence of reliable rules is likely to
link two semantically related words. We will act on this observation in the next section.
5 A Machine Learning Model for Semantic Validation
5.1 Classification
The findings of our analyses suggest that the decision to classify lemma pairs as semantically related
or unrelated can draw on a range of considerations. We therefore decided to adopt a machine learning
approach and phrase semantic validation as a binary classification task, using the analyses we performed
in Sections 3 and 4 as motivation for feature definition.
We train a classifier on the development portion of the DERIVBASE 1.4 extended sample (1,780
training instances, cf. Section 2.2). We learn a binary decision: Semantic relatedness (R) vs. non-semantic
relatedness (M, N, C, L) within derivationally related pairs. For classification, we use a nonlinear model:
Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel. Using the RBF kernel allows
us to capture the non-linear dependencies between the features.
4
We rely on LIBSVM (Chang and Lin,
2011), a well-known SVM implementation. We optimize the C and ? hyperparameters of the SVM model
using 3-fold cross-validation on the training data (i.e., the development portion of the extended sample).
5.2 Features
Our analyses motivate three feature groups comprising 35 individual features: Distributional, derivation
rule-based (?structural?), and hybrid features. Table 4 gives a list.
Distributional features. All distributional features apply to the lemma or pair level. They are calculated
from our BOW model with permissive lemmatization (Section 3.1). We use absolute and rank-based cosine
similarity (Section 3.4) as well as the number of shared contexts (computed with LMI, cf. Section 3.3)
and lemma frequency. To speed up processing, we compute the forward rank similarity for a lemma pair
(l
1
, l
2
) not on the complete vocabulary but by pairing l
1
with a random sample of 1,000 lemmas from
DERIVBASE (plus l
2
if it is not included). We do the computation analogously for the backward rank.
4
The nonlinear SVM model outperforms a linear SVM. The difference is 0.8% F-Score, statistically significant at p=0.05.
1734
Feature group Type Feature name Description
(# features) (# features)
Distributio- l Lemma frequency (2) Normalized SDeWaC corpus lemma frequencies
nal (6) p Cosine similarity Standard cosine lemma similarity
p Dimensions shared Number of shared context words
p Cos. rank similarity (2) Rank-based forward and backward similarity
Structural (25) r Rule identity (11) Indicator features for the top ten rules in the dev
set + one aggregate feature for the rest
r Rule reliability Percentage of rule applications on R pairs among
all applications of the rule in dev set
r Rule frequency rank (2) Rank-based rule frequency in DERIVBASE
r Avg. string distance (2) Avg. Levenshtein distance for all rule instances
p POS combinations (6) Indicator features for lemma POS combinations
p Path length Length of the shortest path between the lemmas
p String distance (2) Dice bigram coefficient; Levenshtein distance
Hybrid (4) r Average rank sim (2) Frequency-weighted average rank similarity of
rules on shortest path
p Rank sim deviation (2) Difference between lemma pair rank similarity
and average rule rank similarity
Table 4: Features used to characterize derivationally related lemma pairs. ?Type? indicates the level at
which each feature applies: l lemma level, p pair level, r rule level.
Structural features. The structural features encode properties of the rules and paths in DERIVBASE.
Most features apply to the level of derivation rules. This includes the identity of the rule; its reliability
(estimated as the ratio of its application on R pairs among all its applications on the dev set); its frequency
rank among all rules (as a measure of specificity)
5
; and the average Levenshtein distance between the
input and output lemmas (estimating rule complexity by measuring the amount of string modification).
For lemma pairs linked by complex paths (i.e., more than one rule, cf. Figure 1), the question arises
how the rule-level features should be computed. Following our observations on ?weakest link? behavior
in Section 4, we always combine the feature values for the individual rules adopting the most pessimistic
combination function (e.g., minimum in the case of reliability, maximum in the case of frequency rank).
Three more structural features are computed directly at the lemma pair level: their part of speech
combination (e.g., ?NV? for oxide
N
? oxidate
V
), the length of the shortest path connecting them, and the
Levenshtein and Dice string distances between the two lemmas.
Hybrid features. Hybrid features combine rule-based and distributional information to avoid their
respective shortcomings. We work with two hybrid features, one at rule level and one at pair level. The
rule-level feature models the reliability of the rule. It is the average rank similarity for each rule (computed
as a log frequency-weighted average over rule instances). This feature is a counterpart to rule reliability
that is unsupervised in that it does not require class labels. We compute it by randomly drawing 200
lemma pairs for each rule from DERIVBASE (less if the rule has fewer instances). The pair-level feature
is the difference between the rule?s average rank similarity and the rank similarity for the current pair. It
measures the rank of a pair relative to the rule?s ?baseline? rank and indicates how similar and dissimilar
lemma pairs are compared to the rule average. In parallel to the structural features, values for complex
rule paths are computed by minimum. Since the rank similarity is directional, we compute both hybrid
features in two variants, one for each direction.
6
5
We compute this feature once only on simplex paths and once on all instances of the rule in DERIVBASE, trading reliability
against noise.
6
We also tested hybrid features based on raw cosine; however, this yielded worse results than the rank-based hybrid features.
1735
Validation method Precision Recall F
1
Accuracy
Majority baseline 72.6 100 84.1 72.6
Classifier, only ?cosine similarity? feature 72.6 100 84.1 72.6
Classifier only ?similarity rank? feature 80.3 90.3 85.0 76.8
Classifier, only ?rule identity? feature 73.7 99.5 84.6 73.8
Classifier, hybrid group 80.4 95.3 87.2 79.7
Classifier, distributional group 80.5 96.6 87.8 80.5
Classifier, structural group 82.7 93.1 87.6 80.9
Classifier, hybrid + distributional groups 82.6 93.3 87.6 80.9
Classifier, hybrid + structural groups 84.9 93.7 89.1 83.4
Classifier, distributional + structural groups 85.3 94.6 89.7 84.3
Classifier, all features 86.2 93.9 89.9 84.7
Table 5: Accuracy, precision, recall, and F
1
on the test portion of the DERIVBASE 1.4 extended sample.
5.3 Results and Discussion
We applied the trained classifier to the test portion of the DERIVBASE 1.4 extended sample (cf. Section 2.2).
Table 5 summarizes precision, recall, and F
1
-score of the classifier for various combinations of features
and feature groups. Recall that since our motivation is semantic validation, i.e., the removal of false
positives, we are in particular interested in improving the precision of our predictions. We test significance
of F
1
differences among models with bootstrap resampling (Efron and Tibshirani, 1993).
Our baseline is the majority class in the sample, R. Due to the sample?s skewed class distribution (cf.
Table 2), the frequency baseline is quite high (precision 72.6, F
1
-score 84.1). We next consider the three
most prominent individual features: Distributional similarity measured as cosine, distributional similarity
measured as similarity rank, and rule identity. As expected from our analyses, the cosine similarity on
its own is not reliable; in fact, it performs at baseline level. The rank-based similarity already leads to a
considerable gain (precision +7.7%), but only a slight F
1
-score increase of 0.9% that is not statistically
significant at p=0.05. These results provide good empirical evidence for Hypothesis 1? (Section 3.4)
and underscore that 1? is a more accurate statement than Hypothesis 1 (Section 2.3). On the structural
side, rule identity alone improves the precision by 1.1%, with an F
1
-score increase in 0.5% (again not
significant).
We now proceed to complete feature groups, all of which perform at least 3% F
1
-score better than the
baseline, proving that the features within these groups are complementary. The hybrid feature group is the
worst among the three. The distributional feature group is able to improve only slightly over the individual
rank-based similarity feature in precision (80.5 vs. 80.3), but gains 6.3% in recall. This is sufficient for
a significant improvement in F
1
(+3.7%, significant at p=0.01). The structural feature group performs
surprisingly well, given that these features are very simple and most are computed only on the relatively
small training set. It yields by far the highest precision (82.7), and its F
1
-score is only slightly lower than
the one of the distributional group (87.6 vs. 87.8). We take this as further evidence for the usefulness of
structural information, as expressed by Hypothesis 2 (cf. Section 2.3).
Ultimately, all three feature groups turn out to be complementary. We obtain an improvement in
F
1
-score for two out of the three feature group combinations, and a clear improvement in precision in all
cases. Finally, the best overall result is shown by the combination of all three feature groups. It attains an
F
1
-score of 89.9, an improvement of 5.8% over the baseline and 2.1% over the best feature group (both
differences significant at p=0.01). Crucially, this model gains over 13% in precision while losing only 6%
of recall compared to the baseline. This corresponds to a reduction of false positives in the sample by
about half (from 27% to 14%) while the true positives were reduced only by 5% (from 73% to 68%).
Table 6 shows a breakdown of the predictions by the best model in terms of the five gold standard classes
1736
R M N L C total
Gold annotation 554 81 81 45 2 763
Classified as R 520 36 16 29 2 603
Classified as not R 34 45 65 16 0 160
Table 6: Predictions on the test set of the all features Classifier per annotation class.
(R, M, N, L, C). Ignoring compounds (C), of which there are too few cases to analyze, we first find that
the classifier achieves a high R recall. It is also very good in filtering out unrelated cases (N), of which it
discards around 80%. The model recognizes morphologically but not semantically related word pairs (M)
fairly well and manages to remove more than half of these. It has the hardest time with lemmatization
errors (L), of which only about 35% were removed. However, this is not surprising: Lemmatization errors
do not form a coherent category that would be easy to retrieve with the kinds of features that we have
developed. We believe that such errors should be handled in an earlier stage, i.e., during preprocessing.
6 Related Work
Given that many derivational lexicons were only developed in recent years, we are only aware of one study
(Jacquemin, 2010) that semantically validates the output of an existing derivational lexicon (Gaussier,
1999) to apply it to Question Answering. In contrast to our study, it requires elaborate dictionary
information to look up which derivations are permitted for a specific lemma, as well as word sense
disambiguation to determine the meaning of ambiguous words in context. Other related work comes from
two areas: unsupervised morphology induction and semantic clustering.
Unsupervised morphology induction is concerned with the automatic identification of morphological
relations (cf. Hammarstr?om and Borin (2011) for an overview). Most approaches in this area do not differ-
entiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception)
and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000;
Baroni et al., 2002) take distributional information into account.
Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional
information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational
properties in their feature set to learn Catalan adjective classes. However, the input to such studies is
almost always a set of words from the same part of speech with no prior morphological constraints, while
our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent,
and exhibit systematical variation in parts of speech. To our knowledge, this challenging situation has not
been addressed in previous studies.
Recent work has also considered the opposite problem, namely using derivational morphology for
improving distributional similarity predictions. Luong et al. (2013) use recursive neural networks to learn
representations of morphologically complex words and demonstrate the usefulness of their approach
on word similarity tasks across different datasets. Similarly, Lazaridou et al. (2013) improve the word
representations of derivationally related words by composing vector space representations of stems and
derivational suffixes.
7 Conclusions
Almost all existing derivational lexicons do not distinguish between only morphologically related words
on one hand and words that are both morphologically and semantically related words on the other hand.
In this paper, we have addressed the task of recovering this distinction and called it semantic validation.
We have used DERIVBASE, a German derivation lexicon, as the basis of our investigation.
We have made two contributions: (a) providing a detailed analysis of the types of information available
for this task (distributional similarity as well as structural information about derivation rules) and the prob-
lems associated with each information type; and (b) training a machine learning classifier on linguistically
1737
motivated features. The classifier, although not perfect, can substantially improve the precision of the
word pairs in DERIVBASE and thus help to filter the derivational families in the lexicon. We are making
this semantic validation information available in the DERIVBASE lexicon by attaching a probability for
the class R to each lemma pair (see footnote 1 for the DERIVBASE URL).
The approach that we have described should transfer straightforwardly to other derivational lexicons
and other languages on the conceptual level. The practical requirements are an appropriate corpus (for the
distributional features) and derivational rule information (for the structural features).
There are two clear directions for future work. First, we plan to broaden our attention from word pairs
to clusters and use the relatedness probabilities to cluster the derivational families in DERIVBASE into
semantically coherent subfamilies. Second, we will demonstrate the impact of semantic validation on
applications of derivational knowledge such as derivation-driven smoothing of distributional models (Pad?o
et al., 2013).
Acknowledgments. We gratefully acknowledge partial funding by the European Commission (project
EXCITEMENT (FP7 ICT-287923), first and second authors) as well as the Croatian Science Foundation
(project 02.03/162: ?Derivational Semantic Models for Information Retrieval?, third author). We thank
the reviewers for their valuable feedback.
References
Harald R. Baayen, Richard Piepenbrock, and Leon Gulikers. 1996. The CELEX Lexical Database. Release 2.
LDC96L14. Linguistic Data Consortium, University of Pennsylvania, Philadelphia, Pennsylvania.
Marco Baroni, Johannes Matiasek, and Harald Trost. 2002. Unsupervised Discovery of Morphologically Related
Words Based on Orthographic and Semantic Similarity. Computing Research Repository, cs.CL/0205006.
Edwin L. Battistella. 1996. The Logic of Markedness. Oxford University Press.
Orhan Bilgin, Ozlem C?etino?glu, and Kemal Oflazer. 2004. Morphosemantic relations in and across Wordnets. In
Proceedings of the Global Wordnet Conference, pages 60?66, Brno, Czech Republic.
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the
International Conference on Computational Linguistics, pages 89?97, Beijing, China.
Gemma Boleda, Sabine Schulte im Walde, and Toni Badia. 2012. Modeling Regular Polysemy: A Study on the
Semantic Classification of Catalan Adjectives. Computational Linguistics, 38(3):575?616.
John A. Bullinaria and Joe P. Levy. 2007. Extracting Semantic Representations from Word Co-occurrence Statis-
tics: A Computational Study. Behavior Research Methods, 39(3):510?526.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems Technology, 2(3):27:1?27:27.
Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York.
Stefan Evert. 2005. The Statistics of Word Cooccurrences Word Pairs and Collocations. Ph.D. thesis, University
of Stuttgart.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010. Design and Application of a Gold Standard for Morpholog-
ical Analysis: SMOR in Validation. In Proceedings of the Conference on Language Resources and Evaluation,
pages 803?810, Valletta, Malta.
Christiane Fellbaum, Anne Osherson, and Peter Clark. 2009. Putting semantics into WordNet?s ?morphosemantic?
links. In Proceedings of Human Language Technology. Challenges of the Information Society, pages 350?358,
Pozna?n, Poland.
?
Eric Gaussier. 1999. Unsupervised learning of derivational morphology from inflectional lexicons. In ACL
Workshop Proceedings on Unsupervised Learning in Natural Language Processing, pages 24?30, College Park,
Maryland.
Nizar Habash and Bonnie Dorr. 2003. A categorial variation database for English. In Proceedings of the North
American Association for Computational Linguistics, pages 96?102, Edmonton, Canada.
1738
Harald Hammarstr?om and Lars Borin. 2011. Unsupervised Learning of Morphology. Computational Linguistics,
37(2):309?350.
Mary Hare, Michael Jones, Caroline Thomson, Sarah Kelly, and Ken McRae. 2009. Activating Event Knowledge.
Cognition, 111(2):151?167.
Sabine Schulte im Walde. 2006. Experiments on the Automatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159?194.
Bernard Jacquemin. 2010. A derivational rephrasing experiment for question answering. In Proceedings of the
Conference on Language Resources and Evaluation, pages 2380?2387, Valletta, Malta.
Lauri Karttunen and Kenneth R. Beesley. 2005. Twenty-five Years of Finite-state Morphology. In Inquiries into
Words, Constraints and Contexts. Festschrift for Kimmo Koskenniemi on his 60th Birthday, pages 71?83. CSLI
Publications, Stanford, California.
Gabriella Lapesa and Stefan Evert. 2013. Evaluating neighbor rank and distance measures as predictors of se-
mantic priming. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages
66?74, Sofia, Bulgaria.
Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni. 2013. Compositional-ly derived rep-
resentations of morphologically complex words in distributional semantics. In Proceedings of the Association
for Computational Linguistics, pages 1517?1526, Sofia, Bulgaria.
Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the International Confer-
ence on Machine Learning, ICML, pages 296?304, San Francisco, California.
Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceedings of the Conference on Natural Language Learning, pages
104?113, Sofia, Bulgaria.
Sebastian Pad?o, Jan
?
Snajder, and Britta Zeller. 2013. Derivational smoothing for syntactic distributional semantics.
In Proceedings of the Association for Computational Linguistics, pages 731?735, Sofia, Bulgaria.
Karel Pala and Dana Hlav?a?ckov?a. 2007. Derivational relations in Czech wordnet. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing: Information Extraction and Enabling Technologies, pages
75?81, Prague, Czech Republic.
Martin Porter. 1980. An Algorithm for Suffix Stripping. Program, 14(3):130?137.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Interna-
tional Conference on Natural Language Processing, Manchester, UK.
Patrick Schone and Daniel Jurafsky. 2000. Knowledge-free induction of morphology using latent semantic analy-
sis. In Proceedings of the Conference on Natural Language Learning, pages 67?72. Lisbon, Portugal.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011. A probabilistic modeling framework for lexical entailment.
In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 558?563, Portland, Oregon.
Jan
?
Snajder and Bojana Dalbelo Ba?si?c. 2010. A computational model of Croatian derivational morphology. In
Proceedings of the International Conference on Formal Approaches to South Slavic and Balkan Languages,
pages 109?118, Dubrovnik, Croatia.
Pavol
?
Stekauer and Rochelle Lieber, editors. 2005. Handbook of Word-Formation, volume 64 of Studies in Natural
Language and Linguistic Theory. Springer.
Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics.
Journal of Artificial Intelligence Research, 37(1):141?188.
Britta Zeller, Jan
?
Snajder, and Sebastian Pad?o. 2013. DErivBase: Inducing and evaluating a derivational mor-
phology resource for German. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics, pages 1201?1211, Sofia, Bulgaria.
1739
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540?549,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
What Substitutes Tell Us ?
Analysis of an ?All-Words? Lexical Substitution Corpus
Gerhard Kremer
Institute for Computational Linguistics
University of Heidelberg, Germany
kremer@cl.uni-heidelberg.de
Katrin Erk
Dept. of Linguistics
University of Texas, Austin, U.S.A.
katrin.erk@utexas.edu
Sebastian Pad?
Institute for Natural Language Processing
University of Stuttgart, Germany
pado@ims.uni-stuttgart.de
Stefan Thater
Dept. of Computational Linguistics
Saarland University, Saarbr?cken, Germany
stth@coli.uni-sb.de
Abstract
We present the first large-scale English ?all-
words lexical substitution? corpus. The
size of the corpus provides a rich resource
for investigations into word meaning. We
investigate the nature of lexical substitute
sets, comparing them to WordNet synsets.
We find them to be consistent with, but
more fine-grained than, synsets. We also
identify significant differences to results
for paraphrase ranking in context reported
for the SEMEVAL lexical substitution data.
This highlights the influence of corpus con-
struction approaches on evaluation results.
1 Introduction
Many, if not most, words have multiple meanings;
for example, the word ?bank? has a financial and
a geographical sense. One common approach to
deal with this lexical ambiguity is supervised word
sense disambiguation, or WSD (McCarthy, 2008;
Navigli, 2009), which frames the task as a lemma-
level classification problem, to be solved by train-
ing classifiers on samples of lemma instances that
are labelled with their correct senses.
This approach has its problems, however. First,
it assumes a complete and consistent set of labels.
WordNet, used in the majority of studies, does
cover several 10,000 lemmas, but has been criti-
cised for both its coverage and granularity. Second,
WSD requires annotation for each sense and lemma,
leading to an ?annotation bottleneck?. A number
of technical solutions have been suggested regard-
ing the second problem (Ando and Zhang, 2005;
Navigli and Ponzetto, 2012), but not for the first.
In 2009, McCarthy and Navigli address both
problems by proposing a fundamentally different
approach, called Lexical Substitution (McCarthy
and Navigli, 2009) which avoids capturing a word?s
meaning by a single label. Instead, annotators are
asked to list, for each instance of a word, one or
more alternative words or phrases to be substituted
for the target in this particular context. This setup
provides a number of benefits over WSD. It al-
lows characterising word meaning without using
an ontology and can be obtained easily from native
speakers through crowdsourcing. Work on mod-
elling Lexical Substitution data has also assumed a
different focus from WSD. It tends to see the predic-
tion of substitutes along the lines of compositional
lexical semantics, concentrating on explaining how
word meaning is modulated in context (Mitchell
and Lapata, 2010).
There are, however, important shortcomings of
the work in the Lexical Substitution paradigm. All
existing datasets (McCarthy and Navigli, 2009;
Sinha and Mihalcea, 2014; Biemann, 2013; Mc-
Carthy et al., 2013) are either comparatively small,
are ?lexical sample? datasets, or both. ?Lexical
sample? datasets consist of sample sentences for
each target word drawn from large corpora, with
just one target word substituted in each sentence. In
WSD, ?lexical sample? datasets contrast with ?all-
words? annotation, in which all content words in a
text are annotated for sense (Palmer et al., 2001).
540
In this paper, we present the first large ?all-
words? Lexical Substitution dataset for English. It
provides substitutions for more than 30,000 words
of running text from two domains of MASC (Ide et
al., 2008; Ide et al., 2010), a subset of the Ameri-
can National Corpus (http://www.anc.org)
that is freely available and has (partial) manual
annotation. The main advantage of the all-words
setting is that it provides a realistic frequency distri-
bution of target words and their senses. We use this
to empirically investigate (a) the nature of lexical
substitution and (b) the nature of the corpus, seen
through the lens of word meaning in context.
2 Related Work
2.1 Lexical Substitution: Data
The original ?English Lexical Substitution? dataset
(McCarthy and Navigli, 2009) comprises 200 target
content words (balanced numbers of nouns, verbs,
adjectives and adverbs). Targets were explicitly se-
lected to exhibit interesting ambiguities. For each
target, 10 sentences were chosen (mostly at ran-
dom, but in part by hand) from the English Internet
Corpus (Sharoff, 2006) and presented to 5 anno-
tators to collect substitutes. Its total size is 2,000
target instances. Sinha and Mihalcea (2014) pro-
duced a small pilot dataset (500 target instances) for
all-words substitution, asking three annotators to
substitute all content words in presented sentences.
Biemann (2013) first investigated the use of
crowdsourcing, developing a three-task bootstrap-
ping design to control for noise. His study covers
over 50,000 instances, but these correspond only to
397 targets, all of which are high-frequency nouns.
Biemann clusters the resulting substitutes into word
senses. McCarthy et al. (2013) applied lexical sub-
stitution in a cross-lingual setting, annotating 130
of the original McCarthy and Navigli targets with
Spanish substitutions (i. e., translations).
2.2 Lexical Substitution: Models
The LexSub task at SEMEVAL 2007 (McCarthy
and Navigli, 2009) required systems to both de-
termine substitution candidates and choose con-
textual substitutions in each case. Erk and Pad?
(2008) treated the gold substitution candidates as
given and focused on the context-specific ranking
of those candidates. In this form, the task has been
addressed through three types of (mostly unsuper-
vised) approaches. The first group computes a sin-
gle type representation and modifies it according
to sentence context (Erk and Pad?, 2008; Thater et
al., 2010; Thater et al., 2011; Van de Cruys et al.,
2011). The second group of approaches clusters
instance representations (Reisinger and Mooney,
2010; Dinu and Lapata, 2010; Erk and Pad?, 2010;
O?S?aghdha and Korhonen, 2011). The third op-
tion is to use a language model (Moon and Erk,
2013). Recently, supervised models have emerged
(Biemann 2013; Szarvas et al., 2013a,b).
3 COINCO ? The MASC All-Words
Lexical Substitution Corpus
1
Compared to, e. g., WSD, there still is little gold-
annotated data for lexical substitution. With the
exception of the dataset created by Biemann (2013),
all existing lexical substitution datasets are fairly
small, covering at most several thousand instances
and few targets which are manually selected. We
aim to fill this gap, providing a dataset that mirrors
the actual corpus distribution of targets in sentence
context and is sufficiently large to enable a detailed,
lexically specific analysis of substitution patterns.
3.1 Source Corpus Choice
For annotation, we chose a subset of the ?Manually
Annotated Sub-Corpus? MASC (Ide et al., 2008;
Ide et al., 2010) which is ?equally distributed across
19 genres, with manually produced or validated
annotations for several layers of linguistic phenom-
ena?, created with the purpose of being ?free of
usage and redistribution restrictions?. We chose
this corpus because (a) our analyses can profit from
the preexisting annotations and (b) we can release
our annotations as part of MASC.
Since we could not annotate the complete MASC,
we selected (complete) text documents from two
prominent genres: news (18,942 tokens) and fiction
(16,605 tokens). These two genres are both rele-
vant for NLP and provide long, coherent documents
that are appropriate for all-words annotation. We
used the MASC part-of-speech annotation to iden-
tify all content words (verbs, nouns, adjectives, and
adverbs), which resulted in a total of over 15,000
targets for annotation. This method differs from
Navigli and McCarthy?s (2009) in two crucial re-
spects: we annotate all instances of each target, and
include all targets regardless of frequency or level
of lexical ambiguity. We believe that our corpus is
considerably more representative of running text.
1
Available as XML-formatted corpus ?Concepts in Con-
text? (COINCO) from http://goo.gl/5C0jBH. Also
scheduled for release as part of MASC.
541
3.2 Crowdsourcing
We used the Amazon Mechanical Turk (AMT) plat-
form to obtain substitutes by crowdsourcing. Inter-
annotator variability and quality issues due to non-
expert annotators are well-known difficulties (see,
e. g., Fossati et al. (2013)). Our design choices
were shaped by ?best practices in AMT?, including
Mason and Suri (2012) and Biemann (2013).
Defining HITs. An AMT task consists of Human
Intelligence Tasks (HITs), each of which is sup-
posed to represent a minimal, self-contained task.
In our case, potential HITs were annotations of
(all target words in) one sentence, or just one tar-
get word. The two main advantages of annotating
a complete sentence at a time are (a) less over-
head, because the sentence has only to be read
once; (b) higher reliability, since all words within a
sentence will be annotated by the same person.
Unfortunately, presenting individual sentences
as HITs also means that all sentences pay the same
amount irrespective of their length. Since long sen-
tences require more effort, they are likely to receive
less attention. We therefore decided to generally
present two random target words per HIT, and one
word in the case of ?leftover? singleton targets.
In the HITs, AMT workers (?turkers?) saw the
highlighted target word in context. Since one sen-
tence was often insufficient to understand the target
fully, we also showed the preceding and the follow-
ing sentence. The task description asked turkers to
provide (preferably single-word) substitutes for the
target that ?would not change the meaning?. They
were explicitly allowed to use a ?more general term?
in case a substitute was hard to find (e. g., dog for
the target dachshund, cf. basic level effects: Rosch
et al. (1976)). Turkers were encouraged to produce
as many replacements as possible (up to 5). If they
could not find a substitute, they had to check one of
the following radio buttons: ?proper name?, ?part
of a fixed expression?, ?no replacement possible?,
?other problem (with description)?.
Improving Reliability. Another major problem
is reliability. Ideally, the complete dataset should
be annotated by the same group of annotators, but
turkers tend to work only on a few HITs before
switching to other AMT jobs. Following an idea
of Biemann and Nygaard (2010), we introduced a
two-tier system of jobs aimed at boosting turker
loyalty. A tier of ?open tasks? served to identify
reliable turkers by manually checking their given
substitutes for plausibility. Such turkers were then
invited to the second, ?closed task? tier, with a
higher payment. In both tiers, bonus payments
were offered to those completing full HIT sets.
For each target, we asked 6 turkers to provide
substitutions. In total, 847 turkers participated suc-
cessfully. In the open tasks, 839 turkers submitted
12,158 HITs (an average of 14.5 HITs). In the
closed tasks, 25 turkers submitted 42,827 HITs (an
average of 1,713 HITs), indicating the substantial
success of our turker retention scheme.
Cost. In the open task, each HIT was paid for
with $ 0.03, in the closed task the wage was $ 0.05
per HIT. The bonus payment for completing a HIT
set amounted to $ 2 ($ 1) in the open (closed) tasks.
The average cost for annotations was $ 0.22 for one
target word instance and $ 0.02 for one substitute.
The total cost with fees was ~$ 3,400.
3.3 COINCO: Corpus and Paraset Statistics
We POS-tagged and lemmatised targets and substi-
tutes in sentence context with TreeTagger (Schmid,
1994). We manually lemmatised unknown words.
Our annotated dataset comprises a total of 167,336
responses by turkers for 15,629 target instances in
2,474 sentences (7,117 nouns, 4,617 verbs, 2,470
adjectives, and 1,425 adverbs). As outlined above,
targets are roughly balanced across the two gen-
res (news: 8,030 instances in 984 sentences; fic-
tion: 7,599 instances in 1,490 sentences). There are
3,874 unique target lemmas; 1,963 of these occur
more than once. On this subset, there is a mean of
6.99 instances per target lemma. To our knowledge,
our corpus is the largest lexical substitution dataset
in terms of lemma coverage.
Each target instance is associated with a paraset
(i. e., the set of substitutions or paraphrases pro-
duced for a target in its context) with an average
size of 10.71. Turkers produced an average of
1.68 substitutions per target instance.
2
Despite
our instructions to provide single-word substitutes,
11,337 substitutions contain more than one word.
3.4 Inter-Annotator Agreement
McCarthy and Navigli (2009) introduced two inter-
annotator agreement (IAA) measures for their
dataset. The first one is pairwise agreement (PA),
2
Note that a small portion of the corpus was annotated by
more than 6 annotators.
542
dataset # targets PA mode-% PA
m
MN09 1,703 27.7 73.9 50.7
SM13 550 15.5 N/A N/A
COINCO (complete) 15,400 19.3 70.9 44.7
COINCO (subset) 2,828 24.6 76.4 50.9
Table 1: Pairwise turker agreement (mode-%: per-
centage of target instances with a mode)
measuring the overlap of produced substitutions:
PA =
?
t?T
?
?s
t
,s
?
t
? ?C
t
|s
t
? s
?
t
|
|s
t
? s
?
t
|
?
1
|C
t
| ? |T |
where t is a target in our target set T , s
t
is the
paraset provided by one turker for t, and C
t
is the
set comprising all pairs of turker-specific parasets
for t. Only targets with non-empty parasets (i. e.,
not marked by turkers as a problematic target) from
at least two turkers are included. The second one
is mode agreement (PA
m
), the agreement of an-
notators? parasets with the mode (the unique most
frequent substitute) for all targets where one exists:
PA
m
=
?
t?T
m
?
s
t
?S
t
[m ? s
t
] ?
1
|s
t
| ? |T
m
|
where T
m
is the set of all targets with some mode
m and S
t
is the set of all parasets for target t. The
Iverson bracket notation [m ? s
t
] denotes 1 if
mode m is included in s
t
(otherwise 0).
Table 1 compares our dataset to the results by
McCarthy and Navigli (2009, MN09) and Sinha
and Mihalcea (2014, SM13). The scores for
our complete dataset (row 3) are lower than Mc-
Carthy and Navigli?s both for PA (?8 %) and PA
m
(?6 %), but higher than Sinha and Mihalcea?s, who
also note the apparent drop in agreement.
3
We believe that this is a result of differences in
the setup rather than an indicator of low quality:
Note that PA will tend to decrease both in the face
of more annotators and of more substitutes. Both
of these factors are present in our setup. To test this
interpretation, we extracted a subset of our data that
is comparable to McCarthy and Navigli?s regard-
ing these factors. It comprises all target instances
where (a) exactly 6 turkers gave responses (9,521
targets), and (b) every turker produced between one
and three substitutes (5,734 targets). The results for
this subset (row 4) are much more similar to those
of McCarthy and Navigli: the pairwise agreement
3
Please see McCarthy and Navigli (2009) for a possible
explanation of the generally low IAA numbers in this field.
relation all verb noun adj adv
syn 9.4 12.5 7.7 8.0 10.4
direct-hyper 6.6 9.3 7.6 N/A N/A
direct-hypo 7.5 11.6 8.0 N/A N/A
trans-hyper 3.2 2.8 4.7 N/A N/A
trans-hypo 3.0 3.7 3.8 N/A N/A
wn-other 68.9 60.7 66.5 88.5 85.4
not-in-wn 2.1 0.9 2.2 3.4 4.2
Table 2: Target?substitute relations in percentages,
overall (all) and by POS. Note: WordNet contains
no hypo-/hypernyms for adjectives and adverbs.
differs only by 3 %, and the mode agreement is
almost identical. We take these figures as indica-
tion that crowdsourcing can serve as a sufficiently
reliable way to create substitution data; note that
Sinha and Mihalcea?s annotation was carried out
?traditionally? by three annotators.
Investigating IAA numbers by target POS and by
genre, we found only small differences (? 2.6 %)
among the various subsets, and no patterns.
4 Characterising Lexical Substitutions
This section examines the collected lexical substi-
tutions, both quantitatively and qualitatively. We
explore three questions: (a) What lexical relations
hold between targets and their substitutes? (b) Do
parasets resemble word senses? (c) How similar
are the parasets that correspond to the same word
sense of a target? These questions have not been
addressed before, and we would argue that they
could not be addressed before, because previous
corpora were either too small or were sampled in a
way that was not conducive to this analysis.
We use WordNet (Fellbaum, 1998), release 3.1,
as a source for both lexical relations and word
senses. WordNet is the de facto standard in NLP
and is used for both WSD and broader investiga-
tions of word meaning (Navigli and Ponzetto, 2012;
Erk and McCarthy, 2009). Multi-word substitutes
are excluded from all analyses.
4
4.1 Relating Targets and Substitutes
We first look at the most canonical lexical relations
between a target and its substitutes. Table 2 lists the
percentage of substitutes that are synonyms (syn),
direct/transitive (direct-/trans-) hypernyms (hyper)
4
All automatic lexical substitution approaches, including
Section 5, omit multi-word expressions. Also, they can be
expected to have WordNet coverage and normalisation issues,
which would constitute a source of noise for this analysis.
543
sentence substitutes
Now, how can I help the elegantly mannered friend of
my Nepthys and his surprising young charge ?
dependent, person, task, lass, prot?g?, effort, companion
The distinctive whuffle of pleasure rippled through the
betas on the bridge, and Rakal let loose a small growl,
as if to caution his charges against false hope.
dependent, command, accusation, private, companion, follower,
subordinate, prisoner, teammate, ward, junior, underling, enemy,
group, crew, squad, troop, team, kid
Table 3: Context effects below the sense level: target noun ?charge? (wn-other shown in italics)
and hyponyms (hypo) of the target. If a substitute
had multiple relations to the target, the shortest path
from any of its senses to any sense of the target
was chosen. The table also lists the percentage of
substitutes that are elsewhere in WordNet but not
related to the target (wn-other) and substitutes that
are not covered by WordNet (not-in-wn).
We make three main observations. First, Word-
Net shows very high coverage throughout ? there
are very few not-in-wn substitutes. Second, the per-
centages of synonyms, hypernyms and hyponyms
are relatively similar (even though the annotation
guidelines encouraged the annotation of hyponyms
over hypernyms), but relatively small. Finally, and
most surprisingly, the vast majority of substitutes
across all parts of speech are wn-other.
A full analysis of wn-other is beyond the cur-
rent paper. But a manual analysis of wn-other
substitutes for 10 lemmas
5
showed that most of
them were context-specific substitutes that can dif-
fer even when the sense of the target is the same.
This is illustrated in Table 3, which features two
occurrences of the noun ?charge? in the sense of
?person committed to your care?. But because of
the sentence context, the first occurrence got sub-
stitutes like ?prot?g??, while the second one was
paraphrased by words like ?underling?. We also
see evidence of annotator error (e. g., ?command?
and ?accusation? in the second sentence).
6
Dis-
counting such instances still leaves a prominent
role for correct wn-other cases.
But are these indeed contextual modulation ef-
fects below the sense level, or are parasets funda-
mentally different from word senses? We perform
two quantitative analyses to explore this question.
4.2 Comparing Parasets to Synsets
To what extent do parasets follow the boundaries
of WordNet senses? To address this question, we
5
We used the nouns business, charge, place, way and the
verbs call, feel, keep, leave, show, stand.
6
A manual analysis of the same 10 lemmas showed only
38 out of 1,398 (0.027) of the substitutes to be erroneous.
paraset?sense mapping class verb noun adj adv
mappable 90.3 73.5 33.0 49.6
uniquely mappable 63.1 57.5 24.3 41.3
Table 4: Ratios of (uniquely) mappable parasets
establish a mapping between parasets and synsets.
Since gold standard word senses in MASC are lim-
ited to high-frequency lemmas and cover only a
small part of our data, we create a heuristic map-
ping that assigns each paraset to that synset of its
target with which it has the largest intersection. We
use extended WordNet synsets that include direct
hypo- and hypernyms to achieve better matches
with parasets. We call a paraset uniquely mappable
if it has a unique best WordNet match, and map-
pable if one or more best matches exist. Table 4
shows that most parasets are mappable for nouns
and verbs, but not for adjectives or adverbs.
We now focus on mappable parasets for nouns
and verbs. To ensure that this does not lead to a
confounding bias, we performed a small manual
study on the 10 noun and verb targets mentioned
above (247 parasets). We found 25 non-mappable
parasets, which were due to several roughly equally
important reasons: gaps in WordNet, multi-word
expressions, metaphor, problems of sense granular-
ity, and annotator error. We also found 66 parasets
with multiple best matches. The two dominant
sources were target occurrences that evoked more
than one sense and WordNet synset pairs with very
close meanings. We conclude that excluding non-
mappable parasets does not invalidate our analysis.
To test whether parasets tend to map to a single
synset, we use a cluster purity test that compares
a set of clusters C to a set of gold standard classes
C
?
. Purity measures the accuracy of each cluster
with respect to its best matching gold class:
purity(C,C
?
) =
1
N
K
?
k=1
max
k
?
|C
k
? C
?
k
?
|
where N is the total number of data points, K is the
544
measure verbs nouns
cluster purity (%) 75.1 81.2
common core size within sense 1.84 2.21
common core size across senses 0.39 0.41
paraset size 6.89 6.29
Table 5: Comparing uniquely mappable parasets to
senses: overlap with best WordNet match as cluster
purity (top), and intersection size of parasets with
and without the same WordNet match (bottom)
number of clusters, and C
?
k
?
is the gold class that
has the largest overlap with cluster C
k
. In our case,
C is the set of mappable parasets
7
, C
?
the set of
extended WordNet synsets, and we only consider
substitutes that occur in one of the target?s extended
synsets (these are the data points). This makes the
current analysis complementary to the relational
analysis in Table 2.
8
The result, listed in the first row of Table 5,
shows that parasets for both verbs and nouns have
a high purity, that is, substitutes tend to focus on a
single sense. This can be interpreted as saying that
annotators tend to agree on the general sense of a
target. Roughly 20?25 % of substitutes, however,
tend to stem from a synset of the target that is not
the best WordNet match. This result comes with
the caveat that it only applies to substitutes that
are synonyms or direct hypo- and hypernyms of
the target. So in the next section, we perform an
analysis that also includes wn-other substitutes.
4.3 Similarity Between Same-Sense Parasets
We now use the WordNet mappings from the pre-
vious section to ask how (dis-)similar parasets are
that represent the same word sense. We also try to
identify the major sources for dissimilarity.
We quantify paraset similarity as the common
core, that is, the intersection of all parasets for
the same target that map onto the same extended
WordNet synset. Surprisingly, the common core
is mostly non-empty (in 85.6 % of all cases), and
contains on average around two elements, as the
second row in Table 5 shows. For this analysis, we
only use uniquely mappable parasets. In relation
to the average paraset size (see row 4), this means
that one quarter to one third of the substitutes are
7
For non-uniquely mappable parasets, the purity is the
same for all best-matching synsets.
8
Including wn-other substitutes would obscure whether
low purity means substitutes from a mixture of senses (which
we are currently interested in) or simply a large number of
wn-other substitutes (which we have explored above).
set elements
synset \ core feel, perceive, comprehend
synset ? core sense
core \ synset notice
non-core substitutes detect, recall, perceive, experi-
ence, note, realize, discern
Table 6: Target feel.v.03: synset and common core
shared among all instances of the same target?sense
combination. In contrast, the common core for
all parasets of targets that map onto two or more
synsets contains only around 0.4 substitutes (see
row 3) ? that is, it is empty more often than not.
At the same time, if about one quarter to one
third of the substitutes are shared, this means that
there are more non-shared than shared substitutes
even for same-sense parasets. Some of these cases
result from small samples: Even 6 annotators can-
not always exhaust all possible substitutes. For
example, the phrase ?I?m starting to see more busi-
ness transactions? occurs twice in the corpus. The
two parasets for ?business? share the same best
WordNet sense match, but they have only 3 shared
and 7 non-shared substitutes. This is even though
the substitutes are all valid and apply to both in-
stances. Other cases are instances of the context
sensitivity of the Lexical Substitution task as dis-
cussed above. Table 6 illustrates on an example
how the common core of a target sense relates to
the corresponding synset; note the many context-
specific substitutes outside the common core.
5 Ranking Paraphrases
While there are several studies on modelling lexi-
cal substitutes, almost all reported results use Mc-
Carthy and Navigli?s SEMEVAL 2007 dataset. We
now compare the results of three recent computa-
tional models on COINCO (our work) and on the
SEMEVAL 2007 dataset to highlight similarities
and differences between the two datasets.
Models. We consider the paraphrase ranking
models of Erk and Pad? (2008, EP08), Thater et
al. (2010, TFP10) and Thater et al. (2011, TFP11).
These models have been analysed by Dinu et al.
(2012) as instances of the same general framework
and have been shown to deliver state-of-the-art per-
formance on the SEMEVAL 2007 dataset, with best
results for Thater et al. (2011).
The three models share the idea to represent the
meaning of a target word in a specific context by
545
corpus syntactically structured syntactically filtered bag of words random
TFP11 TFP10 EP08 TFP11/EP08 TFP10 TFP11/EP08 TFP10
COINCO
context 47.8 46.0 47.4 47.4 41.9 46.2 40.8
33.0
baseline 46.2 44.6 46.2 45.8 38.8 44.7 37.5
SEMEVAL 2007
context 52.5 48.6 49.4 50.1 44.7 48.0 42.6
30.0
baseline 43.7 42.7 43.7 44.4 38.0 42.7 35.8
COINCO Subset
context 40.3 37.7 39.0 39.2 34.1 37.7 32.5
23.7
baseline 36.7 35.7 36.7 36.4 30.6 35.4 28.0
Table 7: Corpus comparison in terms of paraphrase ranking quality (GAP percentage). SEMEVAL results
from Thater et al. (2011). ?Context?: full models, ?baseline?: uncontextualised target-substitute similarity.
modifying the target?s basic meaning vector with
information from the vectors of the words in the
target?s direct syntactic context. For instance, the
vector of ?coach? in the phrase ?the coach derailed?
is obtained by modifying the basic vector represen-
tation of ?coach? through the vector of ?derail?, so
that the resulting contextualised vector reflects the
train car sense of ?coach?.
We replicate the setup of Thater et al. (2011)
to make our numbers directly comparable. We
consider three versions of each model: (a) syntacti-
cally structured models use vectors which record
co-occurrences based on dependency triples, ex-
plicitly recording syntactic role information within
the vectors; (b) syntactically filtered models also
use dependency-based co-occurrence information,
but the syntactic role is not explicitly represented in
the vector representations; (c) bag-of-words mod-
els use a window of ? 5 words. All co-occur-
rence counts are extracted from the English Giga-
word corpus (http://catalog.ldc.upenn.
edu/LDC2003T05), analysed with Stanford de-
pendencies (de Marneffe et al., 2006).
We apply the models to our dataset as follows:
We first collect all substitutes for all occurrences of
a target word in the corpus. The task of our models
for each target instance is then to rank the candi-
dates so that the actual substitutes are ranked higher
than the rest. We rank candidates according to the
cosine similarity between the contextualised vec-
tor of the target and the vectors of the candidates.
Like most previous approaches, we compare the
resulting ranked list with the gold standard annota-
tion (the paraset of the target instance), using gen-
eralised average precision (Kishida, 2005, GAP),
and using substitution frequency as weights. GAP
scores range between 0 and 1; a score of 1 indicates
a perfect ranking in which all correct substitutes
precede all incorrect ones, and correct high-weight
substitutes precede low-weight substitutes.
Results. The upper part of Table 7 shows results
for our COINCO corpus and the previous stan-
dard dataset, SEMEVAL 2007. ?Context? refers to
the full models, and ?baseline? to global, context-
unaware ranking based on the semantic similarity
between target and substitute. Baselines are model-
specific since they re-use the models? vector repre-
sentations. Note that EP08 and TFP11 are identical
unless syntactically structured vectors are used, and
their baselines are identical.
The behaviour of the baselines on the two cor-
pora is quite similar: random baselines have GAPs
around 0.3, and uncontextualised baselines have
GAPs between 0.35 and 0.46. The order of the
models is also highly parallel: the syntactically
structured TFP11 is the best model, followed by
its syntactically filtered version and syntactically
structured EP08. All differences between these
models are significant (p< 0.01) for both corpora,
as computed with bootstrap resampling (Efron and
Tibshirani, 1993). That is, the model ranking on
SEMEVAL is replicated on COINCO.
There are also substantial differences between
the two corpora, though. Most notably, all models
perform substantially worse on COINCO. This
is true in absolute terms (we observe a loss of 2?
5 % GAP) but even more dramatic expressed as the
gain over the uninformed baselines (almost 9 % for
TFP11 on SEMEVAL but only 1.2 % on COINCO).
All differences between COINCO and SEMEVAL
are again significant (p< 0.01).
We see three major possible reasons for these
differences: variations in (a) the annotation setup
(crowdsourcing, multiple substitutes); (b) the sense
distribution; (c) frequency and POS distributions
between the two corpora. We focus on (c) since it
can be manipulated most easily. SEMEVAL con-
tains exactly 10 instances for all targets, while CO-
INCO reflects the Zipf distribution of ?natural? cor-
pora, with many targets occurring only once. Such
546
corpora are easier to model in terms of absolute
performance, because the paraphrase lists for rare
targets contain less false positives for each instance.
For hapax legomena, the set of substitution candi-
dates is identical to the gold standard, and the only
way to receive a GAP score lower than 1 for such
targets is to rank low-weight substitutes ahead of
high-weight substitutes. Not surprisingly, the mean
GAP score of the syntactically structured TFP11
for hapax legomena is 0.863. At the same time,
such corpora make it harder for full models to out-
perform uncontextualised baselines; the best model
(TFP11) only outperforms the baseline by 1.6 %.
To neutralise this structural bias, we created
?SEMEVAL-like? subsets of COINCO (collectively
referred to as the COINCO Subset) by extracting
all COINCO targets with at least 10 instances (141
nouns, 101 verbs, 50 adjectives, 36 adverbs) and
building 5 random samples by drawing 10 instances
for each target. These samples match SEMEVAL in
the frequency distribution of its targets. To account
for the unequal distribution of POS in the samples,
we compute GAP scores for each POS separately
and calculate these GAP scores? average.
The results for the various models on the CO-
INCO Subset in the bottom part of Table 7 show
that the differences between COINCO and SE-
MEVAL are not primarily due to the differences
in target frequencies and POS distribution ? the
COINCO Subset is actually more different to SE-
MEVAL than the complete COINCO. Strikingly,
the COINCO Subset is very difficult, with a ran-
dom baseline of 24 % and model performances be-
low 37 % (baselines) and up to 40 % (full models),
which indicates that the set of substitutes in CO-
INCO is more varied than in SEMEVAL as an effect
of the annotation setup. Encouragingly, the margin
between full models and baselines is larger than on
the complete COINCO and generally amounts to
2?4 % (3.6 % for TFP11). That is, the full models
are more useful on the COINCO corpus than they
appeared at first glance; however, their effect still
remains much smaller than on SEMEVAL.
6 Conclusion
This paper describes COINCO, the first large-scale
?all-words? lexical substitution corpus for English.
It was constructed through crowdsourcing on the
basis of MASC, a corpus of American English.
The corpus has two major advantages over previ-
ous lexical substitution corpora. First, it covers con-
tiguous documents rather than selected instances.
We believe that analyses on our corpus generalise
better to the application domain of lexical substitu-
tion models, namely random unseen text. In fact,
we find substantial differences between the perfor-
mances of paraphrase ranking models for COINCO
and the original SEMEVAL 2007 LexSub dataset:
the margin of informed methods over the baselines
are much smaller, even when controlling for target
frequencies and POS distribution. We attribute this
divergence at least in part to the partially manual se-
lection strategy of SEMEVAL 2007 (cf. Section 2.1)
which favours a more uniform distribution across
senses, while our whole-document annotation faces
the ?natural? distribution skewed towards predom-
inant senses. This favours the non-contextualised
baseline models, consistent with our observations.
At the very least, our findings demonstrate the sen-
sitivity of evaluation results on corpus properties.
The second benefit of our corpus is that its size
enables more detailed analyses of lexical substi-
tution data than previously possible. We are able
to investigate the nature of the paraset, i. e., the
set of lexical substitutes given for one target in-
stance, finding that lexical substitution sets corre-
spond fairly well to WordNet sense distinctions
(parasets for the same synset show high similarity,
while those for different senses do not). In addition,
however, we observe a striking degree of context-
dependent variation below the sense level: the ma-
jority of lexical substitutions picks up fine-grained,
situation-specific meaning components that do not
qualify as sense distinctions in WordNet.
Avenues for future work include a more detailed
analysis of the substitution data to uncover genre-
and domain-specific patterns and the development
of lexical substitution models that take advantage
of the all-words substitutes for global optimisation.
Acknowledgements
We are grateful to Jan Pawellek for implementing
the AMT task, extracting MASC data, and preparing
HITs. Furthermore, we thank Georgiana Dinu for
her support with the word meaning models.
References
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multiple
tasks and unlabeled data. Journal of Machine Learn-
ing Research, 6:1817?1853.
547
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing WordNet. In Proceedings of the 5th Global
WordNet conference, Mumbai, India.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Language Resources and Evaluation, 47(1):97?122.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454, Genoa, Italy.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP, pages 1162?1172, Cambridge, MA.
Georgiana Dinu, Stefan Thater, and S?ren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL, pages 611?615,
Montr?al, Canada.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of EMNLP, pages
440?449, Singapore.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk and Sebastian Pad?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92?97, Uppsala, Sweden.
Christiane Fellbaum, editor. 1998. WordNet: An
electronic lexical database. MIT Press, Cambridge,
MA.
Marco Fossati, Claudio Giuliano, and Sara Tonelli.
2013. Outsourcing FrameNet to the crowd. In Pro-
ceedings of ACL, pages 742?747, Sofia, Bulgaria.
Nancy Ide, Collin F. Baker, Christiane Fellbaum,
Charles Fillmore, and Rebecca Passonneau. 2008.
MASC: The manually annotated sub-corpus of
American English. In Proceedings of LREC, pages
2455?2461, Marrakech, Morocco.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. In Proceedings of ACL, pages 68?73, Upp-
sala, Sweden.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evalua-
tion indicator for information retrieval experiments.
Technical Report NII-2005-014E, Japanese National
Institute of Informatics.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on Amazon?s Mechanical Turk.
Behavior Research Methods, 44(1):1?23.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Diana McCarthy, Ravi Sinha, and Rada Mihalcea.
2013. The cross-lingual lexical substitution task.
Language Resources and Evaluation, 47(3):607?
638.
Diana McCarthy. 2008. Word sense disambiguation.
In Linguistics and Language Compass. Blackwell.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Taesun Moon and Katrin Erk. 2013. An inference-
based model of word meaning in context as a para-
phrase distribution. ACM Transactions on Intelli-
gent Systems and Technology, 4(3).
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:1?69.
Diarmuid O?S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of EMNLP, pages 1047?1057,
Edinburgh, UK.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In Pro-
ceedings of the SENSEVAL-2 workshop, pages 21?
24, Toulouse, France.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceeding of NAACL, pages 109?117, Los
Angeles, CA.
Eleanor Rosch, Carolyn B. Mervis, Wayne D. Gray,
David M. Johnson, and Penny Boyes-Braem. 1976.
Basic objects in natural categories. Cognitive Psy-
chology, 8(3):382?439.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
NEMLAP, pages 44?49, Manchester, UK.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal
of Corpus Linguistics, 11(4):435?462.
Ravi Sinha and Rada Mihalcea. 2014. Explorations
in lexical sample and all-words lexical substitution.
Natural Language Engineering, 20(1):99?129.
548
Gy?rgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013a. Supervised all-words lexical substitution
using delexicalized features. In Proceedings of
NAACL-HLT, pages 1131?1141, Atlanta, GA.
Gy?rgy Szarvas, R?bert Busa-Fekete, and Eyke H?ller-
meier. 2013b. Learning to rank lexical substitutions.
In Proceedings of EMLNP, pages 1926?1932, Seat-
tle, WA.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of IJCNLP, pages
1134?1143, Chiang Mai, Thailand.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of EMNLP, pages
1012?1022, Edinburgh, Scotland.
549
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226?230,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Crowdsourcing Annotation of Non-Local Semantic Roles
Parvin Sadat Feizabadi
Institut f?ur Computerlinguistik
Heidelberg University
69120 Heidelberg, Germany
feizabadi@cl.uni-heidelberg.de
Sebastian Pad
?
o
Institut f?ur Maschinelle Sprachverarbeitung
Stuttgart University
70569 Stuttgart, Germany
pado@ims.uni-stuttgart.de
Abstract
This paper reports on a study of crowd-
sourcing the annotation of non-local (or
implicit) frame-semantic roles, i.e., roles
that are realized in the previous discourse
context. We describe two annotation se-
tups (marking and gap filling) and find that
gap filling works considerably better, attain-
ing an acceptable quality relatively cheaply.
The produced data is available for research
purposes.
1 Introduction
In the last years, crowdsourcing, e.g., using Ama-
zon?s Mechanical Turk platform, has been used to
collect data for a range of NLP tasks, e.g., MT eval-
uation (Callison-Burch, 2009), sentiment analysis
(Mellebeek et al., 2010), and student answer rat-
ing (Heilman and Smith, 2010). Frame-semantic
role annotation (FSRA) is a task that requires more
linguistic expertise than most data collection tasks
realized with crowdsourcing; nevertheless it is also
a crucial prerequisite for high-performance frame-
semantic role labeling (SRL) systems (Das et al.,
2014). Thus, there are some studies that have in-
vestigated FSRA as a crowdsourcing task. It can be
separated into two parts: First, choosing the frame
evoked by a given predicate in a sentence; second,
assigning the semantic roles associated with the
chosen frame. Hong and Baker (2011) have re-
cently addressed the first step, experimenting with
various ways of presenting the task. Fossati et
al. (2013) have considered both steps and opera-
tionalized them separately and jointly, finding the
best results when a single annotation task is pre-
sented to turkers (due to the interdependence of the
two steps) and when the semantic role description
are simplified. Both studies conclude that crowd-
sourcing can produce usable results for FSRA but
requires careful design. Our study extends these
previous studies to the phenomenon of implicit
(non-locally realized) semantic roles where anno-
tators are presented with a target sentence in para-
graph context, and have to decide for every role
whether it is realized in the target sentence, else-
where in the paragraph, or not at all. Our results
shows that implicit roles can be annotated as well
as locally realized roles in a crowdsourcing setup,
again provided that good design choices are taken.
2 Implicit Semantic Roles
Implicit or non-locally realized semantic roles oc-
cur when arguments of a predicate are understood
although not expressed in its direct syntactic neigh-
borhood. FrameNet (Fillmore et al., 2003) dis-
tinguishes between indefinite non-instantiations
(INIs), which are interpreted generically; definite
non-instantiations (DNIs), which can often be iden-
tified with expressions from the previous context;
and constructional non-instantiations (CNI), e.g.,
passives. For instance, in the following example,
the GOAL of the predicate ?reached? is realized
locally, the SOURCE is a non-locally realized DNI,
and the PATH is an INI and not realized at all.
(1) Phileas Fogg, having shut the door of
[
SOURCE
his house] at half-past eleven, and
having put his right foot before his left
five hundred and seventy-five times, and
his left foot before his right five hundred
and seventy-six times, reached [
GOAL
the
Reform Club].
Implicit roles play an important role in discourse
comprehension and coherence (Burchardt et al.,
2005) and have found increasing attention over the
226
last years. The development was kickstarted by the
creation of a corpus of non-local frame-semantic
roles for the SemEval 2010 Task 10 (Ruppenhofer
et al., 2010), which still serves as a de facto stan-
dard. A number of systems perform SRL for non-
local roles (Chen et al., 2010; Silberer and Frank,
2012; Laparra and Rigau, 2013), but the obtained
results are still far from satisfactory, with the best
reported F-Score at 0.19. The main reason is data
sparsity: Due to the small size of the dataset (just
438 sentences), every predicate occurs only a small
number of times. Crowdsourcing can be an attrac-
tive strategy to acquire more annotations.
3 Experimental Setup
3.1 Domain
Our emphasis is on evaluating the annotation of
implicit roles. We reduce complexity by limiting
the number of frames and roles like earlier studies
(Hong and Baker, 2011; Fossati et al., 2013). We
focus on verbs from the MOTION and POSITION
frames, which realize a common set of location
roles (PLACE OF EVENT, SOURCE, GOAL, PATH).
This makes the task more uniform and allows us to
skip frame annotation. Information about spatial
relations, provided by such verbs, can be useful
for many NLP tasks which reason about spatial
information, e.g. systems generating textual de-
scriptions from visual data, robot navigation tasks,
and geographical information systems or GIS (Ko-
rdjamshidi et al., 2012).
3.2 Corpus
We chose the novel ?Around the World in Eighty
Days? by Jules Verne, annotating the ten most fre-
quent predicates meeting the conditions described
above for annotation (reach, arrive, descend, rush,
follow, approach, send, cross, escape, pass). A
post-hoc analysis later showed that each instance of
these predicates has on average 0.67 implicit roles
identifiable in previous context, which underlines
the relevance of annotating such cases. Metaphori-
cal uses were discarded before annotation, which
left an average 38.4 instances for each predicate.
4 Annotation and Agreement
We decided to present target sentences with three
sentences of previous context, as a compromise be-
tween reading overhead and coverage of non-local
roles: For nominalizations, the three previous sen-
tences cover over 85% of all non-local roles (Ger-
Source Goal Path Place
Exact Match 0.35 0.44 0.48 0.24
Overlap 0.35 0.46 0.52 0.27
Table 1: Raw agreement among annotators in the
?marking? task
ber and Chai, 2012). An example and the detailed
description of the task were provided to the an-
notators through external links. We experimented
with two alternatives: annotation as a marking task
and as a gap filling task (explained below). Each
HIT was annotated by five turkers who were asked
to annotate both local and non-local roles, since
identification of local roles is necessary for reliable
tagging of non-local roles.
4.1 Marking Task
Our rationale was to make the task as comprehen-
sible as possible for non-experts. In each HIT, the
target predicate in its context was shown in bold-
face and the annotators were asked to answer four
questions about ?the event in bold?: (a) where does
the event take place?; (b) what is its starting point?;
(c) what is its end point?; (d) which path is used?
For every question, turkers were asked to either
mark a text span (shown in a non-editable field
below the question) or click a button labeled ?not
found in the text?. The goals of this setup were (a)
to minimize annotation effort, and (b) to make the
task as layman-compatible as possible, following
Fossati et al.?s (2013) observation that linguistic
definitions can harm results.
After annotating some instances, we computed
raw inter-annotator agreement (IAA). Table 1
shows IAA among turkers in two conditions (aver-
age pairwise Exact Match and word-based Overlap)
overall annotations for the first 49 instances.
1
The
overall IAA is 37.9% (Exact Match) and 40.1%
(Overlap). We found these results to be too low to
continue this approach. The low results for Overlap
indicate that the problems cannot be due mainly to
differences in the marked spans. Indeed, an analy-
sis showed that the main reason was that annotators
were often confused by the presence of multiple
predicates in the paragraph. Consequently, many
answers marked roles pertaining not to the bolded
target predicate but to other predicates, such as (2).
(2) Leaving Bombay, it passes through Sal-
1
Kappa is not applicable since we have a large number of
disjoint annotators.
227
Source Goal Path Place
Exact Match 0.46 0.46 0.56 0.30
Overlap 0.50 0.54 0.58 0.38
Table 2: Raw agreement among annotators in the
?gap filling? task
cette, crossing to the continent opposite
Tannah, goes over the chain of the West-
ern Ghauts, [. . . ] and, descending south-
eastward by Burdivan and the French town
of Chandernagor, has its terminus at Cal-
cutta.
Annotators would be expected to annotate the
continent opposite Tannah as the goal of crossing,
but some annotated Calcutta, the final destination
of the chain of motion events described.
4.2 Gap Filling Task
Seeing that the marking task did not constrain the
interpretation of the turkers sufficiently, we moved
to a second setup, gap filling, with the aim of fo-
cussing the turkers? attention to a single predicate
rather than the complete set of predicates present
in the text shown. In this task, the annotators were
asked to complete the sentence by filling in the
blanks in two sentences:
1. [Agent] [Event+ed] from . . . to . . .
through . . . path.
2. The whole event took place in/at . . .
The first sentence corresponds to annotations of
the SOURCE, GOAL, and PATH roles; the second
one of the PLACE role. The rationale is that the
presence of the predicate in the sentence focuses
the turkers? attention on the predicate?s actual roles.
Annotators could leave gaps empty (in the case of
unrealized roles), and we asked them to remain as
close to the original material as possible, that is,
avoid paraphrases. Perfect copying is not always
possible, due to grammatical constraints.
Table 2 shows the IAA for this design. We see
that even though the gap filling introduced a new
source of variability (namely, the need for annota-
tors to copy text), the IAA improves considerably,
by up to 11% in Exact Match and 15% in Over-
lap. The new overall IAAs are 44.7% (+6.8%) and
50.2% (+10.1%), respectively. Overall, the num-
bers are still fairly low. However, note that these
IAA numbers among turkers are a lower bound for
the agreement between a ?canonical? version of
the turkers? annotation (see Section 5) and an ideal
gold standard. Additionally, a data analysis showed
that in the gap filling setup, many of the disagree-
ments are more well-behaved: unsurprisingly, they
are often cases where annotators disagree on the ex-
act range of the string to fill into the gap. Consider
the following example:
(3) Skillful detectives have been sent to all the
principal ports of America and the Conti-
nent, and he?ll be a clever fellow if he slips
through their fingers.?
Arguably, experts would annotate all the prin-
cipal ports of America and the Continent as the
GOAL role of sent. Turkers however annotated dif-
ferent spans, including all the principal ports of
America, ports, as well as the ?correct? span. The
lowest IAA is found for the place role. While it is
possible that our setup which required turkers to
consider a second sentence to annotate place con-
tributes to the overall difficulty, our data analysis
indicates that the main problem is the more vague
nature of PLACE compared to the other roles which
made it more difficult for annotators to tag consis-
tently. Consider Example (1): the PLACE could
be, among other things, the City, London, England,
etc. The large number of locations in the novel is a
compounding factor. We found that for some pred-
icates (e.g. arrive, reach), many turkers attempted
to resolve the ambiguity by (erroneously) annotat-
ing the same text as both GOAL and PLACE, which
runs counter to the FrameNet guidelines.
5 Canonicalization
We still need to compute a ?canonical? annotation
that combines the five turker?s annotations. First,
we need to decide whether a role should be realized
or left unrealized (i.e., INI, CNI, or DNI but not in
the presented context). Second, we need to decide
on a span for realized roles. Canonicalization in
crowdsourcing often assumes a majority principle,
accepting the analysis proposed by most turkers.
We found it necessary to be more flexible. Regard-
ing realization, a manual analysis of a few instances
showed that cases of two turker annotations with
non-empty overlap could be accepted as non-local
roles. That is, turkers frequently miss non-local
roles, but if two out of five annotate an overlapping
span with the same role, this is reasonable evidence.
Regarding the role?s span, we used the consensus
228
Source Goal Path Place
Exact Match 0.72 0.67 0.82 0.50
Overlap 0.72 0.69 0.82 0.54
Table 3: Raw agreement between canonical crowd-
sourcing annotation and expert annotation by role
Local Non-Local Unrealized
Exact Match 0.66 0.66 0.69
Overlap 0.69 0.70 0.69
Table 4: Raw agreement between canonical anno-
tation and expert annotation by realization status
span if it existed, and the maximal (union) span oth-
erwise, given that some turkers filled the gaps just
with head words and not complete constituents. To
test the quality of the canonical annotation, one of
the authors had previously annotated 100 random
instances that were also presented to the turkers.
We consider the result to be an expert annotation
approximating a gold standard and use it to judge
the quality of the canonical turker annotations. The
results are shown in Table 3.
The overall raw agreement numbers are 67.80%
(Exact Match) and 69.34% (Overlap). As we had
hoped, the agreement between the canonical crowd-
sourcing annotation and the expert annotation is
again substantially higher than the IAA among turk-
ers. Again, we see the highest numbers for path
(the most specific role) and the lowest numbers for
place (the least specific role).
To assess whether the number obtained in table
3 are sensitive to realization status (explicit, im-
plicit or unrealized), we broke down the agreement
numbers by realization status. Somewhat to our
(positive) surprise, the results in Table 4 indicate
that non-locally realized roles are annotated ablut
as reliably as locally realized ones. Except for the
ill-defined PLACE role, our reliability is compara-
ble to Fossati et al. (2013). Given the more difficult
nature of the task (annotators are given more con-
text and have to make a more difficult decision),
we consider this a promising result.
6 Final Dataset and Cost
The final dataset consists of 384 predicate in-
stances.
2
With four roles per predicate, a total
of 1536 roles could have been realized. We found
2
It can be downloaded for research purposes
from http://www.cl.uni-heidelberg.de/
?
feizabadi/res.mhtml
that more than half (60%) of the roles remained
unrealized even in context. 23% of the roles were
realized locally, and 17% non-locally. The distri-
bution over locally realized, non-locally realized,
and unrealized roles varies considerably among the
four roles that we consider. GOAL has the high-
est percentage of realized roles overall (unrealized
only for 34% of all predicate instances), and at the
same time the highest ratio of locally realized roles
(48% locally realized, 18% non-locally). This cor-
responds well to FrameNet?s predictions about our
chosen predicates which realize the Goal role gen-
erally as the direct object (reach) or an obligatory
prepositional phrase (arrive). In contrast, SOURCE
is realized only for 36% of all instances, and then
predominantly non-locally (24% non-local vs. 12%
local). This shows once more that a substantial part
of predicate-argument structure must be recovered
from previous discourse context.
On average, each HIT page was annotated in 1
minute and 48 seconds, which means 27 seconds
per each role and a total of 60 hours for the whole
annotation. We paid 0.15 USD for each HIT. Since
the number of roles in all HITs was fixed to four
(source, goal, path and place), each role cost 0.04
USD, which corresponds to about USD 0.19 for
every canonical role annotation. This is about twice
the amount paid by Fossati et al. and reflects the
increased effort inherent in a task that involves
discourse context.
7 Conclusion
This paper presented a study on crowdsourcing the
annotation of non-local semantic roles in discourse
context, comparing a marking and a gap filling
setup. We found that gap filling is the more reliable
choice since the repetition of the predicate helps
focusing the turkers? attention on the roles at hand
rather than understanding of the global text. Thus,
the semantic role-based crowdsourcing approach of
Fossati et al. (2013) appears to be generalizable to
the area of non-locally realized roles, provided that
the task is defined suitably. Our results also support
Fossati et al.?s observation that reliable annotations
can be obtained without providing definitions of
semantic roles. However, we also find large differ-
ences among semantic roles. Some (like PATH) can
be annotated reliably and should be usable to train
or improve SRL systems. Others (like PLACE) are
defined so vaguely that it is unclear how usable
their annotations are.
229
References
Aljoscha Burchardt, Anette Frank, and Manfred Pinkal.
2005. Building text meaning representations from
contextually related frames ? a case study. In Pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 66?77, Tilburg, Nether-
lands.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 286?295, Singapore.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 264?267, Uppsala, Sweden.
Dipanjan Das, Desai Chen, Andr?e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics. To appear.
Charles J Fillmore, Christopher R Johnson, and Miriam
R L Petruck. 2003. Background to FrameNet. Inter-
national Journal of Lexicography, 16(3):235?250.
Marco Fossati, Claudio Giuliano, Sara Tonelli, and
Fondazione Bruno Kessler. 2013. Outsourcing
FrameNet to the Crowd. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 742?747, Sofia, Bulgaria.
Matthew Gerber and Joyce Y Chai. 2012. Semantic
role labeling of implicit arguments for nominal pred-
icates. Computational Linguistics, 38(4):755?798.
Michael Heilman and Noah A Smith. 2010. Rat-
ing computer-generated questions with Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 35?40, Los An-
geles, CA.
Jisup Hong and Collin F. Baker. 2011. How good is
the crowd at ?real? WSD? In Proceedings of the
5th Linguistic Annotation Workshop, pages 30?37,
Portland, Oregon, USA.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012. Semeval-2012 task 3: Spa-
tial role labeling. In Proceedings of the First Joint
Conference on Lexical and Computational Seman-
tics, pages 365?373, Montr?eal, Canada.
Egoitz Laparra and German Rigau. 2013. Sources of
evidence for implicit argument resolution. In Pro-
ceedings of the 10th International Conference on
Computational Semantics (IWCS 2013) ? Long Pa-
pers, pages 155?166, Potsdam, Germany.
Bart Mellebeek, Francesc Benavent, Jens Grivolla,
Joan Codina, Marta R Costa-Jussa, and Rafael
Banchs. 2010. Opinion mining of spanish cus-
tomer comments with non-expert annotations on me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 114?
121, Los Angeles, CA.
Josef Ruppenhofer, Caroline Sporleder, R. Morante,
Collin Baker, and Martha Palmer. 2010. Semeval-
2010 task 10: Linking events and their participants
in discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 45?50, Up-
psala, Sweden.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task.
In Proceedings of SEM 2012: The First Joint Con-
ference on Lexical and Computational Semantics,
pages 1?10, Montreal, Canada.
230
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 43?48,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Excitement Open Platform for Textual Inferences
Bernardo Magnini
?
, Roberto Zanoli
?
, Ido Dagan
?
, Kathrin Eichler
?
, G?unter Neumann
?
,
Tae-Gil Noh
?
, Sebastian Pado
?
, Asher Stern
?
, Omer Levy
?
?
FBK (magnini|zanoli@fbk.eu)
?
Heidelberg, Stuttgart Univ. (pado|noh@cl.uni-heidelberg.de)
?
DFKI (neumann|eichler@dfki.de)
?
Bar Ilan University (dagan|sterna3|omerlevy@cs.biu.ac.il)
Abstract
This paper presents the Excitement Open
Platform (EOP), a generic architecture and
a comprehensive implementation for tex-
tual inference in multiple languages. The
platform includes state-of-art algorithms,
a large number of knowledge resources,
and facilities for experimenting and test-
ing innovative approaches. The EOP is
distributed as an open source software.
1 Introduction
In the last decade textual entailment (Dagan et al.,
2009) has been a very active topic in Computa-
tional Linguistics, providing a unifying framework
for textual inference. Several evaluation exercises
have been organized around Recognizing Textual
Entailment (RTE) challenges and many method-
ologies, algorithms and knowledge resources have
been proposed to address the task. However, re-
search in textual entailment is still fragmented and
there is no unifying algorithmic framework nor
software architecture.
In this paper, we present the Excitement Open
Platform (EOP), a generic architecture and a com-
prehensive implementation for multilingual textual
inference which we make available to the scien-
tific and technological communities. To a large
extent, the idea is to follow the successful experi-
ence of the Moses open source platform (Koehn et
al., 2007) in Machine Translation, which has made
a substantial impact on research in that field. The
EOP is the result of a two-year coordinated work
under the international project EXCITEMENT.
1
A
consortium of four academic partners has defined
the EOP architectural specifications, implemented
the functional interfaces of the EOP components,
imported existing entailment engines into the EOP
1
http://www.excitement-project.eu
and finally designed and implemented a rich envi-
ronment to support open source distribution.
The goal of the platform is to provide function-
ality for the automatic identification of entailment
relations among texts. The EOP is based on a modu-
lar architecture with a particular focus on language-
independent algorithms. It allows developers and
users to combine linguistic pipelines, entailment al-
gorithms and linguistic resources within and across
languages with as little effort as possible. For ex-
ample, different entailment decision approaches
can share the same resources and the same sub-
components in the platform. A classification-based
algorithm can use the distance component of an
edit-distance based entailment decision approach,
and two different approaches can use the same set
of knowledge resources. Moreover, the platform
has various multilingual components for languages
like English, German and Italian. The result is an
ideal software environment for experimenting and
testing innovative approaches for textual inferences.
The EOP is distributed as an open source software
2
and its use is open both to users interested in using
inference in applications and to developers willing
to extend the current functionalities.
The paper is structured as follows. Section 2
presents the platform architecture, highlighting
how the EOP component-based approach favors
interoperability. Section 3 provides a picture of
the current population of the EOP in terms of both
entailment algorithms and knowledge resources.
Section 4 introduces expected use cases of the plat-
form. Finally, Section 5 presents the main features
of the open source package.
2 Architecture
The EOP platform takes as input two text portions,
the first called the Text (abbreviated with T), the
second called the Hypothesis (abbreviated with H).
2
http://hltfbk.github.io/
Excitement-Open-Platform/
43
Linguis'c)Analysis)Pipeline)(LAP))
Entailment)Core)(EC))
Entailment)Decision))Algorithm)(EDA))
Dynamic)and)Sta'c)Components)(Algorithms)and)Knowledge))
Linguis'c)Analysis)Components)
Decision)
1)
Raw)Data)
Figure 1: EOP architecture
The output is an entailment judgement, either ?En-
tailment? if T entails H, or ?NonEntailment? if the
relation does not hold. A confidence score for the
decision is also returned in both cases.
The EOP architecture (Pad?o et al., 2014) is based
on the concept of modularization with pluggable
and replaceable components to enable extension
and customization. The overall structure is shown
in Figure 1 and consists of two main parts. The
Linguistic Analysis Pipeline (LAP) is a series of
linguistic annotation components. The Entailment
Core (EC) performs the actual entailment recog-
nition. This separation ensures that (a) the com-
ponents in the EC only rely on linguistic analysis
in well-defined ways and (b) the LAP and EC can
be run independently of each other. Configuration
files are the principal means of configuring the EOP.
In the rest of this section we first provide an intro-
duction to the LAP, then we move to the EC and
finally describe the configuration files.
2.1 Linguistic Analysis Pipeline (LAP)
The Linguistic Analysis Pipeline is a collection of
annotation components for Natural Language Pro-
cessing (NLP) based on the Apache UIMA frame-
work.
3
Annotations range from tokenization to
part of speech tagging, chunking, Named Entity
Recognition and parsing. The adoption of UIMA
enables interoperability among components (e.g.,
substitution of one parser by another one) while
ensuring language independence. Input and output
of the components are represented in an extended
version of the DKPro type system based on UIMA
3
http://uima.apache.org/
Common Analysis Structure (CAS) (Gurevych et
al., 2007; Noh and Pad?o, 2013).
2.2 Entailment Core (EC)
The Entailment Core performs the actual entail-
ment recognition based on the preprocessed text
made by the Linguistic Analysis Pipeline. It con-
sists of one or more Entailment Decision Algo-
rithms (EDAs) and zero or more subordinate com-
ponents. An EDA takes an entailment decision
(i.e., ?entailment? or ?no entailment?) while com-
ponents provide static and dynamic information for
the EDA.
Entailment Decision Algorithms are at the top
level in the EC. They compute an entailment deci-
sion for a given Text/Hypothesis (T/H) pair, and
can use components that provide standardized al-
gorithms or knowledge resources. The EOP ships
with several EDAs (cf. Section 3).
Scoring Components accept a Text/Hypothesis
pair as an input, and return a vector of scores.
Their output can be used directly to build minimal
classifier-based EDAs forming complete RTE sys-
tems. An extended version of these components are
the Distance Components that can produce normal-
ized and unnormalized distance/similarity values
in addition to the score vector.
Annotation Components can be used to add dif-
ferent annotations to the Text/Hypothesis pairs. An
example of such a type of component is one that
produces word or phrase alignments between the
Text and the Hypothesis.
Lexical Knowledge Components describe se-
mantic relationships between words. In the
EOP, this knowledge is represented as directed
rules made up of two word?POS pairs, where
the LHS (left-hand side) entails the RHS (right-
hand side), e.g., (shooting star,Noun) =?
(meteorite,Noun). Lexical Knowledge Compo-
nents provide an interface that allows for (a) listing
all RHS for a given LHS; (b) listing all LHS for
a given RHS; and (c) checking for an entailment
relation for a given LHS?RHS pair. The interface
also wraps all major lexical knowledge sources cur-
rently used in RTE research, including manually
constructed ontologies like WordNet, and encyclo-
pedic resources like Wikipedia.
Syntactic Knowledge Components capture en-
tailment relationships between syntactic and
44
lexical-syntactic expressions. We represent such
relationships by entailment rules that link (option-
ally lexicalized) dependency tree fragments that
can contain variables as nodes. For example, the
rule fall of X =? X falls, or X sells Y to Z =?
Z buys Y from X express general paraphrasing pat-
terns at the predicate-argument level that cannot be
captured by purely lexical rules. Formally, each
syntactic rule consists of two dependency tree frag-
ments plus a mapping from the variables of the
LHS tree to the variables of the RHS tree.
4
2.3 Configuration Files
The EC components can be combined into actual
inference engines through configuration files which
contain information to build a complete inference
engine. A configuration file completely describes
an experiment. For example, it specifies the re-
sources that the selected EDA has to use and the
data set to be analysed. The LAP needed for data
set preprocessing is another parameter that can be
configured too. The platform ships with a set of
predefined configuration files accompanied by sup-
porting documentation.
3 Entailment Algorithms and Resources
This section provides a description of the Entail-
ment Algorithms and Knowledge Resources that
are distributed with the EOP.
3.1 Entailment Algorithms
The current version of the EOP platform ships with
three EDAs corresponding to three different ap-
proaches to RTE: an EDA based on transformations
between T and H, an EDA based on edit distance
algorithms, and a classification based EDA using
features extracted from T and H.
Transformation-based EDA applies a sequence
of transformations on T with the goal of making
it identical to H. If each transformation preserves
(fully or partially) the meaning of the original text,
then it can be concluded that the modified text
(which is actually the Hypothesis) can be inferred
from the original one. Consider the following sim-
ple example where the text is ?The boy was located
by the police? and the Hypothesis is ?The child
was found by the police?. Two transformations for
?boy? ? ?child? and ?located? ? ?found? do the
job.
4
Variables of the LHS may also map to null, when material
of the LHS must be present but is deleted in the inference step.
In the EOP we include a transformation based
inference system that adopts the knowledge based
transformations of Bar-Haim et al. (2007), while in-
corporating a probabilistic model to estimate trans-
formation confidences. In addition, it includes a
search algorithm which finds an optimal sequence
of transformations for any given T/H pair (Stern et
al., 2012).
Edit distance EDA involves using algorithms
casting textual entailment as the problem of map-
ping the whole content of T into the content of H.
Mappings are performed as sequences of editing
operations (i.e., insertion, deletion and substitu-
tion) on text portions needed to transform T into H,
where each edit operation has a cost associated with
it. The underlying intuition is that the probability
of an entailment relation between T and H is related
to the distance between them; see Kouylekov and
Magnini (2005) for a comprehensive experimental
study.
Classification based EDA uses a Maximum En-
tropy classifier to combine the outcomes of sev-
eral scoring functions and to learn a classification
model for recognizing entailment. The scoring
functions extract a number of features at various
linguistic levels (bag-of-words, syntactic dependen-
cies, semantic dependencies, named entities). The
approach was thoroughly described in Wang and
Neumann (2007).
3.2 Knowledge Resources
As described in Section 2.2, knowledge resources
are crucial to recognize cases where T and H use
different textual expressions (words, phrases) while
preserving entailment. The EOP platform includes
a wide range of knowledge resources, including lex-
ical and syntactic resources, where some of them
are grabbed from manual resources, like dictionar-
ies, while others are learned automatically. Many
EOP resources are inherited from pre-existing RTE
systems migrated into the EOP platform, but now
use the same interfaces, which makes them acces-
sible in a uniform fashion.
There are about two dozen lexical (e.g. word-
nets) and syntactic resources for three languages
(i.e. English, Italian and German). However,
since there is still a clear predominance of En-
glish resources, the platform includes lexical and
syntactic knowledge mining tools to bootstrap re-
sources from corpora, both for other languages and
45
EDA Accuracy / F1
Transformation-based English RTE-3 67.13%
Transformation-based English RTE-6 49.55%
Edit-Distance English RTE-3 64.38%
Edit-Distance German RTE-3 59.88%
Edit-Distance Italian RTE-3 63.50%
Classification-based English RTE-3 65.25%
Classification-based German RTE-3 63.75%
Median of RTE-3 (English) submissions 61.75%
Median of RTE-6 (English) submissions 33.72%
Table 1: EDAs results
for specific domains. Particularly, the EOP plat-
form includes a language independent tool to build
Wikipedia resources (Shnarch et al., 2009), as well
as a language-independent framework for building
distributional similarity resources like DIRT (Lin
and Pantel, 2002) and Lin similarity(Lin, 1998).
3.3 EOP Evaluation
Results for the three EDAs included in the EOP
platform are reported in Table 1. Each line rep-
resents an EDA, the language and the dataset
on which the EDA was evaluated. For brevity,
we omit here the knowledge resources used for
each EDA, even though knowledge configuration
clearly affects performance. The evaluations were
performed on RTE-3 dataset (Giampiccolo et al.,
2007), where the goal is to maximize accuracy. We
(manually) translated it to German and Italian for
evaluations: in both cases the results fix a refer-
ence for the two languages. The two new datasets
for German and English are available both as part
of the EOP distribution and independently
5
. The
transformation-based EDA was also evaluated on
RTE-6 dataset (Bentivogli et al., 2010), in which
the goal is to maximize the F1 measure.
The results of the included EDAs are higher than
median values of participated systems in RTE-3,
and they are competing with state-of-the-arts in
RTE-6 results. To the best of our knowledge, the
results of the EDAs as provided in the platform are
the highest among those available as open source
systems for the community.
4 Use Cases
We see four primary use cases for the EOP. Their
requirements were reflected in our design choices.
Use Case 1: Applied Textual Entailment. This
category covers users who are not interested in the
5
http://www.excitement-project.eu/
index.php/results
details of RTE but who are interested in an NLP
task in which textual entailment can take over part
of or all of the semantic processing, such as Ques-
tion Answering or Intelligent Tutoring. Such users
require a system that is as easy to deploy as possi-
ble, which motivates our offer of the EOP platform
as a library. They also require a system that pro-
vides good quality at a reasonable efficiency as
well as guidance as to the best choice of parame-
ters. The latter point is realized through our results
archive in the official EOP Wiki on the EOP site.
Use Case 2: Textual Entailment Development.
This category covers researchers who are interested
in Recognizing Textual Entailment itself, for exam-
ple with the goal of developing novel algorithms
for detecting entailment. In contrast to the first
category, this group need to look ?under the hood?
of the EOP platform and access the source code of
the EOP. For this reason, we have spent substantial
effort to provide the code in a well-structured and
well-documented form.
A subclass of this group is formed by researchers
who want to set up a RTE infrastructure for lan-
guages in which it does not yet exist (that is, al-
most all languages). The requirements of this class
of users comprises clearly specified procedures to
replace the Linguistic Analysis Pipeline, which are
covered in our documentation, and simple methods
to acquire knowledge resources for these languages
(assuming that the EDAs themselves are largely
language-independent). These are provided by the
language-independent knowledge acquisition tools
which we offer alongside the platform (cf. Section
3.2).
Use Case 3: Lexical Semantics Evaluation. A
third category consists of researchers whose pri-
mary interest is in (lexical) semantics.
As long as their scientific results can be phrased
in terms of semantic similarities or inference rules,
the EOP platform can be used as a simple and stan-
dardized workbench for these results that indicates
the impact that the semantic knowledge under con-
sideration has on deciding textual entailment. The
main requirement for this user group is the simple
integration of new knowledge resources into the
EOP platform. This is catered for through the defi-
nition of the generic knowledge component inter-
faces (cf. Section 2.2) and detailed documentation
on how to implement these interfaces.
46
Use Case 4: Educational Use. The fourth and
final use case is as an educational tool to support
academic courses and projects on Recognizing Tex-
tual Entailment and inference more generally. This
use case calls, in common with the others, for easy
usability and flexibility. Specifically for this use
case, we have also developed a series of tutorials
aimed at acquainting new users with the EOP plat-
form through a series of increasingly complexity
exercises that cover all areas of the EOP. We are
also posting proposals for projects to extend the
EOP on the EOP Wiki.
5 EOP Distribution
The EOP infrastructure follows state-of-the-art soft-
ware engineering standards to support both users
and developers with a flexible, scalable and easy to
use software environment. In addition to communi-
cation channels, like the mailing list and the issue
tracking system, the EOP infrastructure comprises
the following set of facilities.
Version Control System: We use GitHub,
6
a
web-based hosting service for code and documen-
tation storage, development, and issue tracking.
Web Site: The GitHub Automatic Page Genera-
tor was used to build the EOP web site and Wiki,
containing a general introduction to the software
platform, the terms of its license, mailing lists to
contact the EOP members and links to the code
releases.
Documentation: Both user and developer docu-
mentation is available from Wiki pages; the pages
are written with the GitHub Wiki Editor and hosted
on the GitHub repository. The documentation in-
cludes a Quick Start guide to start using the EOP
platform right away, and a detailed step by step
tutorial.
Results Archive: As a new feature for commu-
nity building, EOP users can, and are encouraged
to, share their results: the platform configuration
files used to produce results as well as contact infor-
mation can be saved and archived into a dedicated
page on the EOP GitHub repository. That allows
other EOP users to replicate experiments under
the same condition and/or avoid doing experiments
that have already been done.
6
https://github.com/
Build Automation Tool: The EOP has been de-
veloped as a Maven
7
multi-modules project, with
all modules sharing the same Maven standard struc-
ture, making it easier to find files in the project once
one is used to Maven.
Maven Artifacts Repository: Using a Maven
repository has a twofold goal: (i) to serve as an
internal private repository of all software libraries
used within the project (libraries are binary files
and should not be stored under version control sys-
tems, which are intended to be used with text files);
(ii) to make the produced EOP Maven artifacts
available (i.e., for users who want to use the EOP
as a library in their own code). We use Artifactory
8
repository manager to store produced artifacts.
Continuous Integration: The EOP uses Jenk-
ins
9
for Continuous Integration, a software develop-
ment practice where developers of a team integrate
their work frequently (e.g., daily).
Code Quality Tool: Ensuring the quality of the
produced software is one of the most important
aspects of software engineering. The EOP uses
tools like PMD
10
that can automatically be run
during development to help the developers check
the quality of their software.
5.1 Project Repository
The EOP Java source code is hosted on the EOP
Github repository and managed using Git. The
repository consists of three main branches: the
release branch contains the code that is supposed to
be in a production-ready state, whereas the master
branch contains the code to be incorporated into the
next release. When the source code in the master
branch reaches a stable point and is ready to be
released, all of the changes are merged back into
release. Finally, the gh-pages branch contains the
web site pages.
5.2 Licensing
The software of the platform is released under the
terms of General Public License (GPL) version
3.
11
The platform contains both components and
resources designed by the EOP developers, as well
as others that are well known and freely available
7
http://maven.apache.org/
8
http://www.jfrog.com/
9
http://jenkins-ci.org/
10
http://pmd.sourceforge.net
11
http://www.gnu.org/licenses/gpl.html
47
in the NLP research community. Additional com-
ponents and resources whose license is not compat-
ible with the EOP license have to be downloaded
and installed separately by the user.
6 Conclusion
This paper has presented the main characteristics
of Excitement Open Platform platform, a rich envi-
ronment for experimenting and evaluating textual
entailment systems. On the software side, the EOP
is a complex endeavor to integrate tools and re-
sources in Computational Linguistics, including
pipelines for three languages, three pre-existing
entailment engines, and about two dozens of lex-
ical and syntactic resources. The EOP assumes a
clear and modular separation between linguistic
annotations, entailment algorithms and knowledge
resources which are used by the algorithms. A
relevant benefit of the architectural design is that
a high level of interoperability is reached, provid-
ing a stimulating environment for new research in
textual inferences.
The EOP platform has been already tested in sev-
eral pilot research projects and educational courses,
and it is currently distributed as open source soft-
ware under the GPL-3 license. To the best of our
knowledge, the entailment systems and their con-
figurations provided in the platform are the best
systems available as open source for the commu-
nity. As for the future, we are planning several
initiatives for the promotion of the platform in the
research community, as well as its active experi-
mentation in real application scenarios.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages 871?
876, Vancouver, BC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Chal-
lenge. In Proceedings of TAC, Gaithersburg, MD.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Journal of Natural
Language Engineering, 15(4):i?xvii.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, Prague, Czech Repub-
lic.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture (UIMA@GSCL 2007), T?ubingen, Ger-
many.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL demo session, pages 177?
180, Prague, Czech Republic.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the First PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 17?20, Southampton, UK.
Dekang Lin and Patrick Pantel. 2002. Discovery of
Inference Rules for Question Answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL/COLING,
pages 768?774, Montr?eal, Canada.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using
UIMA to structure an open platform for textual en-
tailment. In Proceedings of the 3rd Workshop on
Unstructured Information Management Architecture
(UIMA@GSCL 2013).
Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realiza-
tion of a modular architecture for textual entailment.
Journal of Natural Language Engineering. doi:
10.1017/S1351324913000351.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP, pages 450?458, Sin-
gapore.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL, pages 283?291,
Jeju Island, South Korea.
Rui Wang and G?unter Neumann. 2007. Recogniz-
ing textual entailment using a subsequence kernel
method. In Proceedings of AAAI, pages 937?945,
Vancouver, BC.
48
