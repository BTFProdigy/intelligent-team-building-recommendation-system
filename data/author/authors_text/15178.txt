Proceedings of NAACL-HLT 2013, pages 211?220,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Discriminative Joint Modeling of Lexical Variation and Acoustic Confusion
for Automated Narrative Retelling Assessment
Maider Lehr?, Izhak Shafran?, Emily Prud?hommeaux? and Brian Roark?
?Center for Spoken Language Understanding, Oregon Health & Science University
?Center for Language Sciences, University of Rochester
{maiderlehr,zakshafran,emilpx,roarkbr}@gmail.com
Abstract
Automatically assessing the fidelity of a
retelling to the original narrative ? a task of
growing clinical importance ? is challenging,
given extensive paraphrasing during retelling
along with cascading automatic speech recog-
nition (ASR) errors. We present a word tag-
ging approach using conditional random fields
(CRFs) that allows a diversity of features
to be considered during inference, including
some capturing acoustic confusions encoded
in word confusion networks. We evaluate the
approach under several scenarios, including
both supervised and unsupervised training, the
latter achieved by training on the output of
a baseline automatic word-alignment model.
We also adapt the ASR models to the domain,
and evaluate the impact of error rate on per-
formance. We find strong robustness to ASR
errors, even using just the 1-best system out-
put. A hybrid approach making use of both au-
tomatic alignment and CRFs trained tagging
models achieves the best performance, yield-
ing strong improvements over using either ap-
proach alone.
1 Introduction
Narrative production tasks are an essential compo-
nent of many standard neuropsychological test bat-
teries. For example, narration of a wordless pic-
ture book is part of the Autism Diagnostic Obser-
vation Schedule (ADOS) (Lord et al, 2002) and
retelling of previously narrated stories is part of both
the Developmental Neuropsychological Assessment
(NEPSY) (Korkman et al, 1998) and the Wech-
sler Logical Memory (WLM) test (Wechsler, 1997).
Such tests also arise in reading comprehension, sec-
ond language learning and other computer-based tu-
toring systems (Xie et al, 2012; Zhang et al, 2008).
The accuracy of automated scoring of a narrative
retelling depends on correctly identifying which of
the source narrative?s propositions or events (what
we will call ?story elements?) have been included
in the retelling. Speakers may choose to relate
these elements using diverse words or phrases, and
an automated method of identifying these elements
needs to model the permissible variants and para-
phrasings. In previous work (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011), we developed models based on
automatic word-alignment methods, as described
briefly in Section 3. Such alignments are learned
in an unsupervised manner from a parallel corpus of
manual or ASR transcripts of retellings and the orig-
inal source narrative, much as in machine translation
training.
Relying on manual transcripts to train the align-
ment models limits the ability of these methods to
handle ASR errors. By instead training on ASR
transcripts, these methods can automatically capture
some regularities of lexical variants and their com-
mon realizations by the recognizer. Additionally, ev-
idence of acoustic confusability is available in word
lattice output from the recognizer, which can be ex-
ploited to yield more robust automatic scoring, par-
ticularly in high error-rate scenarios.
In this paper, we present and evaluate the use of
word tagging models for this task, in contrast to
just using automatic (unsupervised) word-alignment
methods. The approach is general enough to al-
211
low tagging of word confusion networks derived
from lattices, thus allowing us to explore the utility
of such representations to achieve robustness. We
present results under a range of experimental condi-
tions, including: variously adapting the ASR mod-
els to the domain; using maximum entropy models
rather than CRFs; differing tagsets (BIO versus IO);
and with varying degrees of supervision. Finally,
we demonstrate improved utility in terms of using
the automatic scores to classify elderly individuals
as having Mild Cognitive Impairment. Ultimately
we find that hybrid approaches, making use of both
word-alignment and tagging models, yield strong
improvements over either used independently.
2 Wechsler Logical Memory (WLM) task
The Wechsler Logical Memory (WLM) task (Wech-
sler, 1997), a widely used subtest of a battery of neu-
ropsychological tests used to assess memory func-
tion in adults, has been shown to be a good indicator
of Mild Cognitive Impairment (MCI) (Storandt and
Hill, 1989; Petersen et al, 1999; Wang and Zhou,
2002; Nordlund et al, 2005), the stage of cogni-
tive decline that is often a precursor to dementia of
the Alzheimer?s type. In the WLM, the subject lis-
tens to the examiner read a brief narrative and then
retells the narrative twice: immediately upon hear-
ing it and after about 20 minutes. The examiner
grades the subject?s response by counting how many
of the story elements the subject recalled.
An excerpt of the text read by the clinician while
administering the WLM task is shown in Figure 1.
The story elements in the text are delineated using
slashes, 25 elements in all. An example retelling
is shown in Figure 2 to illustrate how the retellings
are scored. The clinical evaluation guidelines spec-
ify what lexical substitutions, if any, are allowed
for each element. Some elements, such as cafeteria
and Thompson, must be recalled verbatim. In other
cases, subjects are given credit for variants, such as
Annie for Anna, or paraphrasing of concepts such as
sympathetic for touched by the woman?s story. The
example retelling received a score of 12, with one
point for each of the recalled story elements: Anna,
Boston, employed, as a cook, and robbed of, she had
four, small children, reported, station, touched by
the woman?s story, took up a collection and for her.
Anna / Thompson / of South / Boston / em-
ployed / as a cook / in a school / cafeteria /
reported / at the police / station / that she had
been held up / on State Street / the night be-
fore / and robbed / . . . / police / touched by the
woman?s story / took up a collection / for her.
Figure 1: Reference text and the set of story elements.
Ann Taylor worked in Boston as a cook. And
she was robbed of sixty-seven dollars. Is
that right? And she had four children and
reported at the some kind of station. The fel-
low sympathetic and made a collection for her
so that she can feed the children.
Figure 2: An example retelling with 12 recalled story elements.
3 Unsupervised generative automated
scoring with word alignment
In previous work (Lehr et al, 2012; Prud?hommeaux
and Roark, 2012; Prud?hommeaux and Roark,
2011), we developed a pipeline for automatically
scoring narrative retellings for the WLM task. The
utterances corresponding to a retelling were rec-
ognized using an ASR system. The story ele-
ments were identified from the 1-best ASR transcript
using word alignments produced by the Berkeley
aligner (Liang et al, 2006), an EM-based word
alignment package developed to align parallel texts
for machine translation. The word alignment model
was estimated in an unsupervised manner from a
parallel corpus consisting of source narrative and
manual transcripts of retellings from a small set of
training subjects, and from a pairwise parallel cor-
pus of manual retelling transcripts.
During inference or test, the ASR transcripts of
the retellings were aligned using the estimated align-
ment model to the source narrative text. If a word
in the retelling was mapped by the alignment model
to a content word in the source narrative, the ele-
ment associated with that content word was counted
as correctly recalled in that retelling. Recall that
the models were trained on unsupervised data so the
aligned words may not always be permissible vari-
ants of the target elements. To alleviate such extra-
neous as well as unaligned words, the alignments
below a threshold of posterior probability are dis-
carded while decoding.
212
4 Supervised discriminative automated
scoring with log-linear models
In this work, we frame the task of detecting story
elements as a tagging task. Thus, our problem re-
duces to assigning a tag to each word position in the
retelling, the tag indicating the story element that the
word is associated with. In its simplest form, we
have 26 tags: one for each of the 25 story elements
indicating the word is ?in? that element (e.g., I15);
and one for ?outside? of any story element (?O?). By
tagging word positions, we are framing the problem
in a general enough way to allow tagging of word
confusion networks (Mangu et al, 2000), which en-
code word confusions that may provide additional
robustness, particularly in high word-error rate sce-
narios. We make use of log-linear models, which
have been used for tagging confusion networks (Ku-
rata et al, 2012), and which allow very flexible fea-
ture vector definition and discriminative optimiza-
tion.
The model allows us to experiment with three
types of inputs as illustrated in the Figure 3 ? the
manual transcript, the 1-best ASR transcript, and the
word confusion network. To create supervised train-
ing data, we force-align ASR transcripts to manual
transcripts and transfer manually annotated story el-
ement tags from the reference transcripts to word po-
sitions in the confusion network or 1-best ASR out-
put using the word-level time marks. Our unsuper-
vised training scenario instead derives story element
tags from a baseline word-alignment based model.
Figure 3: Feature vectors at each word position includes lexi-
cal variants and acoustic confusions.
Markov order 0 Markov order 1
(MaxEnt) (CRF)
Context yi yi?1yi
independent (CI) yixi yi?1yixi
Context yixi?1 yi?1yixi?1
dependent (CD) yixi+1 yi?1yixi+1
Table 1: Feature templates either using or not using neighbor-
ing tag yi?1 (MaxEnt vs. CRF); and for using or not using
neighboring words xi?1, xi+1 (CI vs. CD).
4.1 Features
Given a sequence of word positions x = x1 . . . xn,
the tagger assigns a sequence of labels y = y1 . . . yn
from a tag lexicon. For each word xi in the se-
quence, we can define features in the log-linear
model based on word and tag identities. Table 1
presents several sets of features, defined over words
and tags at various positions relative to the current
word xi and tag yi and compound features are de-
noted as concatenated symbols.
Features that rely only on the current tag yi are
used in a Markov order 0 model, i.e., one for which
each tag is labeled independently. A maximum en-
tropy classifier (see Section 4.2) is used with these
feature sets. Features that include prior tags en-
code dependencies between adjacent tags, and are
used within conditional random fields models (see
Section 4.3). To examine the utility of surrounding
words xi?1 and xi+1, we distinguish between mod-
els trained with context independent features (just
xi) and context dependent features. Note that mod-
els including context dependent feature sets also in-
clude the context independent features, and Markov
order 1 models also include Markov order 0 features.
Two other details about our use of the feature tem-
plates are worth noting. First, when tagging confu-
sion networks, each word in the network at position
i results in a feature instance. Thus, if there are five
confusable words at position i, then there will be
five different xi values being used to instantiate the
features in Table 1. Second, following Kurata et al
(2012), we multiply the feature counts for the con-
text dependent features by a weight to control their
influence on the model. In this paper, the scaling
weight of the context-dependent features was 0.3.
We investigate two different tagsets for this task,
as presented in Table 2. The simpler tagset (IO) sim-
ply identifies words that are in a story element; the
213
Tagging anna rent was due
IO-tags I1 I19 I19 I19
BIO-tags B1 B19 I19 I19
Table 2: Two possible tagsets for labeling.
larger tagset (BIO) differentiates among positions in
a story element chunk. The latter tagset is only of
utility for models with Markov order greater than
zero, and hence are only used with CRF models.
4.2 MaxEnt-based multiclass classifier
Our baseline model is a Maximum Entropy (Max-
Ent) classifier where each position i from the
retelling x gets assigned one of the IO output tags
yi corresponding to the set of 25 story elements and
a null (?O?) symbol. The output tag is modeled as
the conditional probability p(yi | xi) given the word
xi at position i in the retelling.
p(yi | xi) =
exp
(
d?
k=1
?k?k(xi, yi)
)
Z(xi)
where Z(xi) is a normalization factor. The feature
functions ?(xi, yi) are the Markov order 0 features
as defined as in the previous section. The parame-
ters ? ? <d are estimated by optimizing the above
conditional probability, with L2 regularization. We
use the MALLET Toolkit (McCallum, 2002) with
default regularization parameters.
4.3 CRF-based sequence labeling model
The MaxEnt models assign a tag to each position
from the input retelling independently. However,
there are a few reasons why reframing the task as
a sequence modeling problem may improve tagging
performance. First, some of the story elements are
multiword sequences, such as she had been held up
or on State Street. Second, even if a retelling orders
recalled elements differently than the original narra-
tive, there is a tendency for story elements to occur
in certain orders.
The parameters of the CRF model, ? ? <d are
estimated by optimizing the following conditional
probability:
P (y | x) =
exp
(
d?
k=1
?k?k(x, y)
)
Z(x)
where ?(x, y) aggregates features across the entire
sequence, and Z(x) is a global normalization con-
stant over the sequence, rather than local for a partic-
ular position as with MaxEnt. Features for the CRF
model are Markov order 1 features, and as with the
MaxEnt training, we use default (L2) regularization
parameters within the MALLET toolkit.
5 Combining tagging and alignment
This paper contrasts a discriminatively trained tag-
ging approach with an unsupervised alignment-
based approach, but there are several ways in which
the two approaches can be combined. First, the
alignment model is unsupervised and can provide
its output as training data to the tagging approach,
resulting in an unsupervised discriminative model.
Second, the alignment model can provide features to
the log-linear tagging model in the supervised condi-
tion. We explore both methods of combination here.
5.1 Unsupervised discriminative tagger
The tagging task based on log-linear models pro-
vides an appropriate framework to easily incorpo-
rate diverse features and discriminatively estimate
the parameters of the model. However, this ap-
proach requires supervised tagged training data, in
this case manual labels indicating the correspon-
dence of phrases in the retellings with story elements
in the original narrative. These manual annotations
are used to derive sequences of story element tags
labeling the words of the retelling. Manually la-
beling the retellings is costly, and the scoring (thus
labeling) scheme is very specific to the test being
analyzed. To avoid manual labeling and provide a
general framework that can easily be adopted in any
retelling based assessment task, we experiment here
with an unsupervised discriminative approach.
In this unsupervised approach, the labeled train-
ing data required by the log-linear model is provided
by the automatic word alignments trained without
supervision. The resulting tag sequences replace the
manual tag sequences used in the standard super-
vised approach.
5.2 Word-alignment derived features
When training discriminative models it is a common
practice to incorporate into the feature space the out-
put from a generative model, since it is a good esti-
214
mator. Here we augment the feature space of the
log-linear models with the tags generated by the au-
tomatic word alignments. In addition to the features
defined in Section 4.1, we include new features that
match predicted labels zi from the word-alignment
model with possible labels in the tagger yi. Our fea-
tures include the current tagger label with (1) the
current predicted word-alignment label; (2) the pre-
vious predicted label; and (3) the next predicted la-
bel. Thus, the new features were yizi, yizi?1 and
yizi+1.
6 Experimental evaluations
Corpus: Our models were trained on immediate and
delayed retellings from 144 subjects with a mean
age of 85.4, of whom 36 were clinically diagnosed
with MCI (training set). We evaluated our models
on a set of retellings from 70 non-overlapping sub-
jects with a mean age of 88.5, half of whom had
received a diagnosis of MCI (test set). In contrast
to the unsupervised word-alignment based method,
the method outlined here required manual story el-
ement labels of the retellings. The training and
test sets from this paper are therefore different from
the sets used in previous work (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011), and the results are not directly
comparable.
The recordings were sometimes made in an infor-
mal setting, such as the subject?s home or a senior
center. For this reason, there are often extraneous
noises in the recordings such as music, footsteps,
and clocks striking the hour. Although this presents
a challenge for ASR, part of the goal of our work
is to demonstrate the robustness of our methods to
noisy audio.
6.1 Automatic transcription
The baseline ASR system used in the current work
is a Broadcast News system which is modeled af-
ter Kingsbury et al (2011). Briefly, the acoustics
of speech are modeled by 4000 clustered allophone
states defined over a pentaphone context, where
states are represented by Gaussian mixture models
with a total of 150K mixture components. The ob-
servation vectors consist of PLP features, stacked
from 10 neighboring frames and projected to a 50-
1-best oracle oracle
System (%) WCN(%) lat(%)
Baseline 47.2 39.7 27.7
AM adaptation 38.2 35.5 21.2
LM adaptation 28.3 30.7 19.9
AM+LM adaptation 25.6 26.5 16.5
Table 3: Improvement in ASR word error-rate by adapting the
Broadcast News models to the domain of narrative retelling.
dimension space using linear discriminant analysis
(LDA). The acoustic models were trained on 430
hours of transcribed speech from Broadcast News
corpus (LDC97S44, LDC98S71). The language
model is defined over an 84K vocabulary and con-
sists of about 1.8M, 1M and and 331K bigrams, tri-
grams and 4-grams, estimated from standard Broad-
cast news corpus. The decoding is performed in sev-
eral stages using successively refined acoustic mod-
els ? a context-dependent model, a vocal-tract nor-
malized model, a speaker-adapted maximum likeli-
hood linear regression (MLLR) model, and finally
a discriminatively trained model with the boosted
MMI criteria (Povey et al, 2008). The system gives
a word error rate of 13.1% on the 2004 Rich Tran-
scription benchmark by NIST (Fiscus et al, 2007),
which is comparable to state-of-the-art for equiva-
lent amounts of acoustic training data. On the WLM
corpus, the recognition word error rate was signifi-
cantly higher at 47.2% due to a mismatch in domain
and the skewed demographics (age) of the speakers.
We improved the performance of the above
Broadcast News models by adapting to the domain
of the WLM retellings. The acoustic models were
adapted using standard MLLR, where linear trans-
forms were estimated in an unsupervised manner
to maximize the likelihood over the transcripts of
the retellings. The transcripts were generated from
the baseline system after the final stage of decod-
ing with the discriminative model. The language
models were adapted by interpolating the in-domain
model (weight=0.7) with the out-of-domain model.
The gains from these adaptations are reported in
the Table 3. As expected, we find substantial gains
from both acoustic model (AM) and language model
(LM) adaptation. Furthermore, we find benefit in
employing them simultaneously. We also include
the oracle word error rate (WER) of the WCNs and
lattices for each ASR configuration.
215
One thing to note is that the oracle WER of the
WCNs is worse than the 1-best WER when adapting
the language models. We speculate that this is due
to bias introduced by the language model adapted
to the story retellings, resulting in word candidates
in the bins that are not truly acoustically confusable
candidates. This is one potential reason for the lack
of utility of WCNs in low WER conditions.
6.2 Evaluating retelling scoring
We analyzed the performance of the retelling scor-
ing methods under five different input conditions for
producing transcripts: (1) the out-of-domain Broad-
cast News recognizer with no adaptation; (2) do-
main adapted acoustic model; (3) domain adapted
language model; (4) domain adapted acoustic and
language models; and (5) manual (reference) tran-
scripts. Each story element is automatically labeled
by the systems as either having been recalled or not,
and this is compared with manual scores to derive an
F-score accuracy, by calculating precision and recall
of recalled story elements. Derived word alignments
or tag sequences are converted to binary story ele-
ment indicators by simply setting the element to 1
if any open-class word is tagged for (or aligned to)
that story element.
6.2.1 Word alignment based scoring
We evaluate the word alignment approach only on
1-best ASR transcripts and manual transcripts, not
WCNs. The first row of Table 4 reports the story ele-
ment F-scores for a range of ASR adaptation scenar-
ios. The performance of the model improves signifi-
cantly as the WER reduces with adaptation. With the
fully adapted ASR the F-score improves more than
13%, and it is only 3.4% worse than with the man-
ual transcripts. The alignments produced in each of
these scenarios are used as training data in the unsu-
pervised condition evaluated below.
6.2.2 Log-linear based automated scoring
Context-independent features Table 4 summa-
rizes the performance of the log-linear models us-
ing context independent features (CI) in supervised
(section 4), unsupervised (section 5.1) and hybrid
(section 5.2) training scenarios for different inputs
(reference transcript, ASR 1-best, and word confu-
sion network ASR output) and four different ASR
configurations.
The results show a few clear trends. Both in
the supervised and unsupervised training scenarios
the CRF model provides substantial improvements
over the MaxEnt classifier. The F-scores obtained
in the unsupervised training scenario are slightly
worse than with supervision, though they are compa-
rable to supervised results and an improvement over
just using the word alignment approach, particularly
in high WER scenarios. The hybrid training sce-
nario ? supervised learning with word alignment de-
rived features ? leads to reduced differences between
MaxEnt and CRF training compared to the other two
training scenarios. In fact, in high WER scenarios,
the MaxEnt slightly outperforms the CRF.
As expected the best performance is obtained with
manual transcripts and the worst with 1-best tran-
scripts generated by the out-of-domain ASR with
relatively high word error rate. For this ASR con-
figuration, using WCNs provide some gain, though
the gain is insignificant for the hybrid approach. In
the hybrid approach, the output labels of the word
alignment are already good indicators of the output
tag and incorporating the confusable words from the
Table 4: Story element F-score achieved by baseline word-alignment model and log-linear models (MaxEnt and CRF) using
context independent features (CI) under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 71.9 77.3 84.3 85.4 N/A 88.8
Supervised MaxEnt-CI 76.0 81.7 84.6 85.6 78.9 83.4 84.0 84.7 86.4
CRF-CI 80.3 87.3 89.7 91.4 83.7 88.8 88.2 90.8 94.4
Unsupervised MaxEnt-CI 72.1 79.3 82.7 84.2 77.5 81.2 83.4 83.2 84.8
CRF-CI 79.4 85.4 86.8 88.0 81.2 85.8 86.2 87.2 90.5
Hybrid MaxEnt-CI 88.1 89.4 89.2 89.6 87.6 89.2 88.8 89.5 91.8
CRF-CI 87.0 90.9 91.5 92.1 87.4 91.5 90.1 92.4 94.6
216
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Supervised MaxEnt-CD 80.1 87.3 90.0 91.1 83.5 88.6 88.2 90.3 93.3
CRF-CD-IO 80.6 88.0 89.9 91.2 84.2 89.6 88.8 90.5 94.7
CRF-CD-BIO 81.1 87.9 90.6 91.7 84.5 89.5 88.8 90.8 94.7
Un- MaxEnt-CD 77.1 83.1 86.5 89.0 80.2 85.0 86.2 87.6 90.7
supervised CRF-CD-IO 79.1 85.3 87.1 88.3 81.0 85.9 86.4 87.5 90.3
CRF-CD-BIO 79.1 85.6 87.2 88.4 81.3 85.9 86.2 87.3 90.6
Hybrid MaxEnt-CD 88.4 90.2 90.7 91.6 88.6 90.5 90.4 91.4 93.5
CRF-CD-IO 87.9 91.3 91.6 92.5 88.3 91.7 90.7 92.1 94.8
CRF-BIO 87.8 91.9 91.8 93.0 88.7 92.0 90.7 92.3 94.7
Table 5: Story element F-score achieved by log-linear models (MaxEnt and CRF) when adding context dependent features (CD)
and BIO tags for the CRF models, under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
WCN into the feature vector apparently mainly adds
noise.
When the transcripts are generated with the
adapted models, the word confidence score of the 1-
best is higher and the WCN bins have fewer acous-
tically confusable words. Still, the WCN input is
helpful in the AM-adapted ASR system. When
the transcripts are generated with LM adapted mod-
els, the performance is better with 1-best than with
WCNs. As mentioned earlier, adapting the lan-
guage models may introduce a bias due to the rel-
atively low LM perplexity for this domain. In the
lowest WER scenarios, the best performing systems
achieve over 90% F-score, within two percent of the
performance achieved with manual transcripts.
Context-dependent features Exercising the flex-
ibility of log-linear models, we investigated the im-
pact of using context-dependent (CD) features in-
stead of the CI features used in the previous exper-
iments. Our CD features take into account the two
immediately neighboring word positions. As men-
tioned earlier, following Kurata et al (2012), the
counts from the neighboring word positions were
weighted (? = 0.3) to avoid data sparsity. This re-
duces the sensitivity of the model to time alignment
errors between the tag and feature vector sequences
without increasing the dimensions. In Table 5, we
report the F-scores for the different ASR configu-
rations, inputs, and log-linear models with context
dependent features, using the standard IO tagset as
in Table 4.
Although there are some exceptions, adding con-
text information from the input features improves
the performance of the models. In particular, the
MaxEnt models benefit from incorporating this ex-
tra information. The MaxEnt models improve their
performance substantially for all three training sce-
narios, while the gains for the CRF models are more
modest, especially for the unsupervised approach
where the performance degrades or does not change
much, since some context information is already
captured by the Markov order 1 features.
BIO tagset As detailed in Section 4.1, story el-
ements sometimes span multiple words, so for the
CRF models we investigated two different schemes
for tagging, following typical practice in named en-
tity extraction (Ratinov and Roth, 2009) and syn-
tactic chunking (Sha and Pereira, 2003). The BIO
tagging scheme makes the distinction between the
tokens from the story elements that are in the be-
ginning from the ones that are not. The O tag is
assigned to the tokens that do not belong to any of
the story elements. The IO tagging uses a single tag
for the tokens that fall in the same story element,
which is the approach we have followed so far. In
addition to presenting results using context depen-
dent features, Table 5 presents results with the BIO
tagset.
For the supervised and hybrid approaches, the
BIO tagging provides insignificant but consistent
gains for most of the scenarios. The unsupervised
approach provides mixed results. This may be due to
the way in which the word alignment model scores
the retellings. It tags only those words from the
retelling that are aligned with a content word in the
source narrative, which may result in the loss of the
217
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 0.65 0.67 0.74 0.76 N/A 0.79
Supervised MaxEnt-CD 0.65 0.73 0.76 0.77 0.70 0.73 0.77 0.77 0.81
CRF-CD-BIO 0.69 0.76 0.77 0.76 0.73 0.76 0.77 0.78 0.82
Un- MaxEnt-CD 0.65 0.72 0.75 0.76 0.70 0.75 0.75 0.76 0.80
supervised CRF-CD-BIO 0.74 0.75 0.78 0.78 0.71 0.74 0.77 0.76 0.81
Hybrid MaxEnt-CD 0.72 0.76 0.77 0.78 0.74 0.76 0.77 0.77 0.82
CRF-CD-BIO 0.72 0.76 0.78 0.78 0.76 0.77 0.78 0.79 0.81
Table 6: Classification performance (AUC) for the baseline word-alignment model and the best performing log-linear models of
both types (MaxEnt and CRF) under 3 different scenarios with 3 types of input and 4 types of ASR models.
structure of some multiwords story elements that we
are trying to capture with the BIO scheme.
6.3 Evaluating MCI classification
Each of the individuals producing retellings in our
corpus underwent a battery of neuropsychological
tests, and were assigned a Clinical Dementia Rating
(CDR) (Morris, 1993), which is a composite score
derived from measures of cognitive function in six
domains, including memory. Importantly, it is as-
signed independently of the Wechsler Logical Mem-
ory test we are analyzing in this paper, which allows
us to evaluate the utility of our WLM analyses in
an unbiased manner. MCI is defined as a CDR of
0.5 (Ritchie and Touchon, 2000), and subjects in this
study have either a CDR of 0 (no impairment) or 0.5
(MCI).
In previous work, we found that the features
extracted from the retellings are useful in dis-
tinguishing subjects with MCI from neurotyp-
ical age-matched controls (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011). From each retellings, we extract
Boolean features for each story element, for a total
of 50 features for classification. Each feature indi-
cates whether the retelling contained that story ele-
ment.
In this paper, we carry out similar classification
experiments to investigate the impact of using log-
linear models on the extraction of features for classi-
fication. We build a support vector machine (SVM)
using the LibSVM (Chang and Lin, 2011) exten-
sion to the WEKA data mining Java API (Hall et al,
2009). This allows recollection of different elements
to be weighted differently. This is unlike the manual
scoring of WLM based on clinical guidelines where
all elements are weighted equally irrespective of the
difficulty. The SVM was trained on manually ex-
tracted story element feature vectors. We compared
the performance of the MCI classification for three
types of input and four ASR configurations under
the supervised, unsupervised, and hybrid scenarios.
For each scenario we chose the best scoring system
from among the automated systems reported in Ta-
bles 4 and 5. Classification results, evaluated as area
under the curve (AUC), are reported in Table 6, both
for the log-linear trained tagging models and for the
baseline word-alignment based method. For refer-
ence (not shown in the table), the SVM classifier
performed at 0.83 when features values are manu-
ally populated.
The results show that the AUC improves steadily
as the quality of the transcription is improved, go-
ing from the baseline system to the adapted mod-
els. This is consistent with the improvements seen in
the F-score for detecting story elements. The differ-
ent approaches for detecting the story elements from
the transcriptions did not ultimately show significant
differences in MCI classification results. Overall,
the best classification values are given by the hy-
brid approach, which performs slightly better than
the other two approaches. The best AUC in the
hybrid scenario (0.79, very close to the AUC=0.81
achieved with manual transcripts) is obtained with
a CRF trained with WCNs from the fully adapted
ASR model and with context dependent features and
BIO tags.
Comparing WCN versus 1-best as inputs, using
WCN as input improves classification performance
when the 1-best transcripts are poor, as in the case
of out-of-domain ASR. The adapted recognizer im-
proves the performance of the 1-best significantly
making it unnecessary to resort to WCN as inputs.
Comparing the MaxEnt model with CRF model
218
for extracting story elements, we see that the average
F-scores for the MaxEnt models trained on CD fea-
tures are nearly as good as and sometimes slightly
better than those produced using the CRF models.
The CRF extracted story elements, however, tend to
yield classifiers that perform slightly better, espe-
cially in the unsupervised approach with 1-best in-
puts.
7 Summary and discussion
This paper examines the task of automatically scor-
ing narrative retellings in terms of their fidelity to
the original narrative content, using discriminatively
trained log-linear tagging models. Fully automatic
scoring must account for both lexical variation and
acoustic confusion from ASR errors. Lexical vari-
ation ? due to extensive paraphrasing on the part
of the individuals retelling the narrative ? can be
modeled effectively using word-alignment models
such as those employed in machine translation sys-
tems (Lehr et al, 2012; Prud?hommeaux and Roark,
2011). This paper focuses on an alternative ap-
proach, where both lexical variation and ASR con-
fusions are modeled using log-linear models. In ad-
dition to very flexible feature definitions, the log-
linear models bring the advantage of a discrimina-
tive model to the task. We see improvements in
story element F-score using these models over unsu-
pervised word-alignment models. Further, the fea-
ture definition flexibility allows us to incorporate the
unsupervised word-alignment labels into these mod-
els, resulting either in fully unsupervised approaches
that perform competitively with the supervised mod-
els or in hybrid (supervised) approaches that provide
the best performing systems in this study.
Our tagging models are able to process word con-
fusion networks as inputs and thus improve perfor-
mance over using 1-best ASR transcripts in scenar-
ios where the speech recognition error rate is high.
These improvements carry through to the MCI clas-
sification task, making use of features computed
from the automatic scoring of narrative retelling.
One advantage of the word-alignment model is
that such approaches do not require manual anno-
tation of the story elements, which is more labor in-
tensive than typical manual transcription of speech.
Thus, the word-alignment model can exploit large
numbers of retellings in an unsupervised manner
when trained on ASR transcripts of the retellings.
Controlled experiments here with relatively limited
training sets demonstrate that semi-supervised ap-
proaches on larger untranscribed sets are likely to
be successful.
Finally, experiments with different amounts of
ASR adaptation show that both acoustic and lan-
guage model adaptations in this domain are effec-
tive, yielding scenarios that are competitive with
manual transcription both for detecting story ele-
ments as well as for subsequent classification. With
full model adaptation to the domain, the 1-best
transcripts improved significantly, and their perfor-
mance was found to be at par with WCNs.
In future work, we would like to investigate two
questions left open by these results. First, word-
alignment models can be extended to process ASR
lattices or word confusion networks as part of the
unsupervised alignment learning algorithm, and in-
corporated into our approach. Second, the con-
textual features can be refined (e.g., concatenated
features instead of smoothed features) when large
amounts of training data is available.
It is noteworthy to mention that the lexical vari-
ants and paraphrasing learned from the data using
automated method may be useful in refining the clin-
ical guidelines for scoring (e.g., allowing additional
lexical variants and paraphrasings, or assigning un-
equal credits for different story elements to reflect
the difficulty of recollecting them) or to create the
guidelines for new languages or stories.
Acknowledgments
This research was supported in part by NIH awards
5K25AG033723-02 and P30 AG024978-05 and
NSF awards 1027834, 0958585, 0905095, 0964102
and 0826654. Any opinions, findings, conclusions
or recommendations expressed in this publication
are those of the authors and do not reflect the views
of the NIH or NSF. We thank Brian Kingsbury and
IBM for the use of their ASR software tools; Jeffrey
Kaye and Diane Howeison for their valuable input;
and the clinicians at the Oregon Center for Aging
and Technology for their care in collecting the data.
219
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(27):1?
27.
Jonathan Fiscus, John Garofolo, Audrey Le, Alvin Mar-
tin, greg Sanders, Mark Przybocki, and David Pallett.
2007. 2004 spring nist rich transcription (rt-04s)
evaluation data. http://www.ldc.upenn.edu/
Catalog/catalogEntry.jsp?catalogId=
LDC2007S12.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Brian Kingsbury, Hagen Soltau, George Saon,
Stephen M. Chu, Hong-Kwang Kuo, Lidia Mangu,
Suman V. Ravuri, Nelson Morgan, and Adam Janin.
2011. The IBM 2009 GALE Arabic speech tran-
scription system. In Proceedings of ICASSP, pages
4672?4675.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Gakuto Kurata, Nobuyasu Itoh, Masafumi Nishimura,
Abhinav Sethy, and Bhuvana Ramabhadran. 2012.
Leveraging word confusion networks for named entity
modeling and detection from conversational telephone
speech. Speech Communication, 54(3):491?502.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran, and
Brian Roark. 2012. Fully automated neuropsycho-
logical assessment for detecting mild cognitive impair-
ment. In Interspeech.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
John Morris. 1993. The clinical dementia rating
(CDR): Current version and scoring rules. Neurology,
43:2412?2414.
A. Nordlund, S. Rolstad, P. Hellstrom, M. Sjogren,
S. Hansen, and A. Wallin. 2005. The Goteborg MCI
study: Mild cognitive impairment is a heterogeneous
condition. Journal of Neurology, Neurosurgery and
Psychiatry, 76(11):1485?1490.
Ronald Petersen, Glenn Smith, Stephen Waring, Robert
Ivnik, Eric Tangalos, and Emre Kokmen. 1999. Mild
cognitive impairment: Clinical characterizations and
outcomes. Archives of Neurology, 56:303?308.
Daniel Povey, Dimitri Kanevsky, Brian Kingsbury,
Bhuvana Ramabhadran, George Saon, and Karthik
Visweswariah. 2008. Boosted mmi for model and fea-
ture space discriminative training. In Proceedings of
ICASSP.
Emily Prud?hommeaux and Brian Roark. 2011. Align-
ment of spoken narratives for automated neuropsycho-
logical assessment. In Proceedings of ASRU.
Emily Prud?hommeaux and Brian Roark. 2012. Graph-
based alignment of narratives for automated neuropsy-
chological assessment. In Proceedings of the NAACL
2012 Workshop on Biomedical Natural Language Pro-
cessing (BioNLP).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
EMNLP.
Karen Ritchie and Jacques Touchon. 2000. Mild cogni-
tive impairment: Conceptual basis and current noso-
logical status. Lancet, 355:225?228.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL.
Martha Storandt and Robert Hill. 1989. Very mild senile
dementia of the Alzheimer?s type: II Psychometric test
performance. Archives of Neurology, 46:383?386.
Qing-Song Wang and Jiang-Ning Zhou. 2002. Retrieval
and encoding of episodic memory in normal aging and
patients with mild cognitive impairment. Brain Re-
search, 924:113?115.
David Wechsler. 1997. Wechsler Memory Scale - Third
Edition. The Psychological Corporation, San Antonio.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of HLT-NAACL.
Xiaonan Zhang, Xiaonan Zhang, Jack Mostow, Jack
Mostow, Nell Duke, Christina Trotochaud, Joseph Va-
leri, and Al Corbett. 2008. Mining free-form spoken
responses to tutor prompts. In Proceedings of the First
International Conference on Educational Data Min-
ing, pages 234?241.
220
Proceedings of NAACL-HLT 2013, pages 709?714,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distributional semantic models for the evaluation of disordered language
Masoud Rouhizadeh?, Emily Prud?hommeaux?, Brian Roark?, Jan van Santen?
?Center for Spoken Language Understanding, Oregon Health & Science University
?Center for Language Sciences, University of Rochester
{rouhizad,vansantj}@ohsu.edu, {emilypx,roarkbr}@gmail.com
Abstract
Atypical semantic and pragmatic expression is
frequently reported in the language of children
with autism. Although this atypicality often
manifests itself in the use of unusual or un-
expected words and phrases, the rate of use
of such unexpected words is rarely directly
measured or quantified. In this paper, we
use distributional semantic models to automat-
ically identify unexpected words in narrative
retellings by children with autism. The classi-
fication of unexpected words is sufficiently ac-
curate to distinguish the retellings of children
with autism from those with typical develop-
ment. These techniques demonstrate the po-
tential of applying automated language anal-
ysis techniques to clinically elicited language
data for diagnostic purposes.
1 Introduction
Autism spectrum disorder (ASD) is a neurodevelop-
mental disorder characterized by impaired commu-
nication and social behavior. Although the symp-
toms of ASD are numerous and varied, atypical
and idiosyncratic language has been one of the
core symptoms observed in verbal individuals with
autism since Kanner first assigned a name to the
disorder (Kanner, 1943). Atypical language cur-
rently serves as a diagnostic criterion in many of the
most widely used diagnostic instruments for ASD
(Lord et al, 2002; Rutter et al, 2003), and the phe-
nomenon is especially marked in the areas of seman-
tics and pragmatics (Tager-Flusberg, 2001; Volden
and Lord, 1991).
Because structured language assessment tools are
not always sensitive to the particular atypical seman-
tic and pragmatic expression associated with ASD,
measures of atypical language are often drawn from
spontaneous language samples. Expert manual an-
notation and analysis of spontaneous language in
young people with ASD has revealed that children
and young adults with autism include significantly
more bizarre and irrelevant content (Loveland et al,
1990; Losh and Capps, 2003) in their narratives and
more abrupt topic changes (Lam et al, 2012) in
their conversations than their language-matched typ-
ically developing peers. Most normed clinical in-
struments for analyzing children?s spontaneous lan-
guage, however, focus on syntactic measures and
developmental milestones related to the acquisition
of vocabulary and syntactic structures. Measures of
semantic and pragmatic atypicality in spontaneous
language are rarely directly measured. Instead, the
degree of language atypicality is often determined
via subjective parental reports (e.g., asking a par-
ent whether their child has ever used odd phrases
(Rutter et al, 2003)) or general impressions dur-
ing clinical examination (e.g., rating the child?s de-
gree of ?stereotyped or idiosyncratic use of words or
phrases? on a four-point scale (Lord et al, 2002)).
This has led to a lack of reliable and objective infor-
mation about the frequency of atypical language use
and its precise nature in ASD.
In this study, we attempt to automatically detect
instances of contextually atypical language in spon-
taneous speech at the lexical level in order to quan-
tify its prevalence in the ASD population. We first
determine manually the off-topic, surprising, or in-
709
appropriate words in a set of narrative retellings
elicited in a clinical setting from children with ASD
and typical development. We then apply word rank-
ing methods and distributional semantic modeling to
these narrative retellings in order to automatically
identify these unexpected words. The results indi-
cate not only that children with ASD do in fact pro-
duce more semantically unexpected and inappropri-
ate words in their narratives than typically develop-
ing children but also that our automated methods
for identifying these words are accurate enough to
serve as an adequate substitute for manual annota-
tion. Although unexpected off-topic word use is just
one example of the atypical language observed in
ASD, the work presented here highlights the poten-
tial of computational language evaluation and analy-
sis methods for improving our understanding of the
linguistic deficits associated with ASD.
2 Data
Participants in this study included 37 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was diag-
nosed via clinical consensus according to the DSM-
IV-TR criteria (American Psychiatric Association,
2000) and the established threshold scores on two
diagnostic instruments: the Autism Diagnostic Ob-
servation Schedule (ADOS) (Lord et al, 2002), a
semi-structured series of activities designed to allow
an examiner to observe behaviors associated with
autism; and the Social Communication Question-
naire (SCQ) (Rutter et al, 2003), a parental ques-
tionnaire. None of the children in this study met
the criteria for a language impairment, and there
were no significant between-group differences in
age (mean=6.4) or full-scale IQ (mean=114).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Korkman
et al, 1998), a large and comprehensive battery of
tasks that test neurocognitive functioning in chil-
dren. The NEPSY Narrative Memory (NNM) sub-
test is a narrative retelling test in which the subject
listens to a brief narrative, excerpts of which are
shown in Figure 1, and then must retell the narra-
tive to the examiner. The NNM was administered
to each participant in the study, and each partici-
pant?s retelling was recorded and transcribed. Us-
ing Amazon?s Mechanical Turk, we also collected
a large corpus of retellings from neurotypical adults,
who listened to a recording of the story and provided
written retellings. We describe how this corpus was
used in Section 3, below.
Two annotators, blind to the diagnosis of the ex-
perimental subjects, identified every word in each
retelling transcript that was unexpected or inappro-
priate given the larger context of the story. For in-
stance, in the sentence T-rex could smell things, both
T-rex and smell were marked as unexpected, since
there is no mention of either concept in the story. In
a seemingly more appropriate sentence, the boy sat
up off the bridge, the word bridge is considered un-
expected since the boy is trapped up in a tree rather
than on a bridge.
3 Methods
We start with the expectation that different retellings
of the same source narrative will share a common
vocabulary and semantic space. The presence of
words outside of this vocabulary or semantic space
in a retelling may indicate that the speaker has
strayed from the topic of the story. Our approach for
automatically identifying these unexpected words
relies on the ranking of words according to the
strength of their association with the target topic of
the corpus. The word association scores used in the
Figure 1: Excerpts from the NNM narrative.
Jim was a boy whose best friend was Pepper. Pepper was a big black dog. [...] Near Jim?s house was a
very tall oak tree with branches so high that he couldn?t reach them. Jim always wanted to climb that tree,
so one day he took a ladder from home and carried it to the oak tree. He climbed up [...] When he started
to get down, his foot slipped, his shoe fell off, and the ladder fell to the ground. [...] Pepper sat below the
tree and barked. Suddenly Pepper took Jim?s shoe in his mouth and ran away. [...] Pepper took the shoe to
Anna, Jim?s sister. He barked and barked. Finally, Anna understood that Jim was in trouble. She followed
Pepper to the tree where Jim was stuck. Anna put the ladder up and rescued Jim.
710
ranking are informed by the frequency of a word
in the child?s retelling relative to the frequency of
that word in other retellings in the larger corpus of
retellings. These association measures are similar
to those developed for the information retrieval task
of topic modeling, in which the goal is to identify
topic-specific words ? i.e., words that appear fre-
quently in only a subset of documents ? in order
to cluster together documents about a similar topic.
Details about how these scores are calculated and in-
terpreted are provided in the following sections.
The pipeline for determining the set of unusual
words in each retelling begins by calculating word
association scores, described below, for each word
in each retelling and ranking the words according to
these scores. A threshold over these scores is de-
termined for each child using leave-one-out cross
validation in order to select a set of potentially un-
expected words. This set of potential unexpected
words is then filtered using two external resources
that allow us to eliminate words that were not used
in other retellings but are likely to be semantically
related to topic of the narrative. This final set of
words is evaluated against the set of manually iden-
tified words in order determine the accuracy of our
unexpected word classification.
3.1 Word association measures
Before calculating the word association measures,
we tokenize, downcase, and stem (Porter, 1980) the
transcripts and remove all punctuation. We then use
two association measures to score each word in each
child?s retelling: tf-idf, the term frequency-inverse
document frequency measure (Salton and Buckley,
1988), and the log odds ratio (van Rijsbergen et al,
1981). We use the following formulation to calcu-
late tf-idf for each child?s retelling i and each word
in that retelling j, where cij is the count of word j
in retelling i; fj is the number of retellings from the
full corpus of child and adult retellings containing
that word j; and D is the total number of retellings
in the full corpus (Manning et al, 2008):
tf-idfij =
{
(1 + log cij) log Dfj if cij ? 1
0 otherwise
The log odds ratio, another association measure
used in information retrieval and extraction tasks, is
the ratio between the odds of a particular word, j,
appearing in a child?s retelling, i, as estimated us-
ing its relative frequency in that retelling, and the
odds of that word appearing in all other retellings,
again estimated using its relative frequency in all
other retellings. Letting the probability of a word
appearing in a retelling be p1 and the probability of
that word appearing in all other retellings be p2, we
can express the odds ratio as follows:
odds ratio =
odds(p1)
odds(p2)
=
p1/(1? p1)
p2/(1? p2)
A large tf-idf or log odds score indicates that the
word j is very specific to the retelling i, which in
turn suggests that the word might be unexpected or
inappropriate in the larger context of the NNM nar-
rative. Thus we expect that the words with higher as-
sociation measure scores are likely to be the words
that were manually identified as unexpected in the
context of the NNM narrative.
3.2 Application of word association measures
As previously mentioned, both of these word associ-
ation measures are used in information retrieval (IR)
to cluster together documents about a similar target
topic. In IR, words that appear only in a subset of
documents from a large and varied corpus of docu-
ments will have high word association scores, and
the documents containing those words will likely be
focused on the same topic. In our task, however,
we have a single cluster of documents focused on
a single topic: the NNM narrative. Topic-specific
words ought to occur much more frequently than
other words. As a result, words with high tf-idf and
log odds scores are likely to be those unrelated to
the topic of the NNM story. If a child veers away
from the topic of the NNM story and uses words that
do not occur frequently in the retellings produced
by neurotypical speakers, his retellings will contain
more words with high word association scores. We
predict that this set of high-scoring words is likely to
overlap significantly with the set of words identified
by the manual annotators as unexpected or off-topic.
Applying these word association scoring ap-
proaches to each word in each child?s retelling yields
a list of words from each retelling ranked in order of
decreasing tf-idf or log odds score. We use cross-
validation to determine, for each measure, the op-
711
erating point that maximizes the unexpected word
identification accuracy in terms of F-measure. For
each child, the threshold is found using the data from
all of the other children. This threshold is then ap-
plied to the ranked word list of the held-out child.
All words above this threshold are potential unex-
pected words, while all words below this threshold
are considered to be expected and appropriate in the
context of the NNM narrative. Table 1 shows the
recall, precision, and F-measure using the two word
association measures discussed here. We see that
these two techniques result in high recall at the ex-
pense of precision. The next stage in the pipeline is
therefore to use external resources to eliminate any
semantically appropriate words from the set of po-
tentially unexpected or inappropriate words gener-
ated via thresholding on the tf-idf or log odds score.
3.3 Filtering with external resources
Recall that the corpus of retellings used to gener-
ate the word association measures described above,
is very small. It is therefore quite possible that a
child may have used an entirely appropriate word
that by chance was never used by another child or
one of the neurotypical adults. One way of increas-
ing the lexical coverage of the corpus of retellings
is through semantic expansion using an external re-
source. For each word in the set of potential un-
expected words, we located the WordNet synset for
that word (Fellbaum, 1998). If any of the WordNet
synonyms of the potentially unexpected word was
present in the source narrative or in one of the adult
retellings, that word was removed from the set of
unexpected words.
In the final step, we used the CHILDES corpus
of transcripts of children?s conversational speech
(MacWhinney, 2000) to generate topic estimates for
each remaining potentially unexpected word. For
each of these words, we located every utterance in
the CHILDES corpus containing that potentially un-
expected word. We then measured the association
of that word with every other open-class word that
appeared in an utterance with that word using the
log likelihood ratio (Dunning, 1993). The 20 words
from the CHILDES corpus with the highest log like-
lihood ratio (i.e., the words most strongly associ-
ated with the potentially unexpected word), were as-
sumed to collectively represent a particular topic. If
more than two of the words in the vector of words
representing this topic were also present in the NNM
source narrative or the adult retellings, the word that
generated that topic was eliminated from the set of
unexpected words.
We note that the optimized threshold described
in Section 3.2, above, is determined after filtering.
There is therefore potentially a different threshold
for each condition tested, and hence we do not nec-
essarily expect precision to increase and recall to
decrease after filtering. Rather, since the threshold
is selected in order to optimize F-measure, we ex-
pect that if the filtering is effective, F-measure will
increase with each additional filtering condition ap-
plied.
4 Results
We evaluated the performance of our two word rank-
ing techniques, both individually and combined by
taking either the maximum of the two measures or
the sum, against the set of manually annotations de-
scribed in Section 2. In addition, we report the re-
sults of applying these word ranking techniques in
combination with the two filtering techniques. We
compare these results with a simple baseline method
in which every word used in a retelling that is never
used in another retelling is considered to be unex-
pected. Table 1 shows the precision, accuracy, and
F-measure of these approaches. We see that all of
the more sophisticated unexpected word identifica-
tion approaches outperform the baseline by a wide
margin, and that tf-idf and log odds perform compa-
rably under the condition without filtering and both
filtering conditions. Filtering improves F-measure
under both word ranking schemes, and combining
the two measures results in further improvements
under both filtering conditions. Although apply-
ing topic-estimate filtering yields the highest preci-
sion, the simple WordNet-based approach results in
the highest F-measure and a reasonable balance be-
tween precision and recall.
Recall that the purpose of identifying these un-
expected words was to determine whether children
with ASD produce unexpected and inappropriate
words at a higher rate than children with typical de-
velopment. This appears to be true in our manu-
ally annotated data. On average, 7.5% of the words
712
Unexpected word identification method P R F1
Baseline 46.3 74.0 57.0
TF-IDF 72.1 79.5 75.6
Log-odds 70.5 79.5 74.7
Sum(TF-IDF, Log-odds) 72.2 83.3 77.4
Max(TF-IDF, Log-odds) 69.9 83.3 76.0
TF-IDF+WordNet 83.8 80.5 82.1
Log-odds+WordNet 82.1 83.1 82.6
Sum(TF-IDF, Log-odds)+WordNet 84.2 83.1 83.7
Max(TF-IDF, Log-odds)+WordNet 83.3 84.4 83.9
TF-IDF+WordNet+topic 85.7 77.9 81.7
Log-odds+WordNet+topic 83.8 80.5 82.1
Sum(TF-IDF, Log-odds)+WordNet+topic 86.1 80.5 83.2
Max(TF-IDF, Log-odds)+WordNet+topic 85.1 81.8 83.4
Table 1: Accuracy of unexpected word identification.
types produced by children with ASD were marked
as unexpected, while only 2.5% of words produced
by children with TD were marked as unexpected, a
significant difference (p < 0.01, using a one-tailed
t-test). This significant between-group difference
in rate of unexpected word use holds even when
using the automated methods of unexpected word
identification, with the best performing unexpected
word identification method estimating a mean of
6.6% in the ASD group and 2.5% in the TD group
(p < 0.01).
5 Conclusions and future work
The automated methods presented here for rank-
ing and filtering words according to their distribu-
tions in different corpora, which are adapted from
techniques originally developed for topic modeling
in the context of information retrieval and extrac-
tion tasks, demonstrate the utility of automated ap-
proaches for the analysis of semantics and pragmat-
ics. We were able to use these methods to iden-
tify unexpected or inappropriate words with high
enough accuracy to replicate the patterns of unex-
pected word use manually observed in our two di-
agnostic groups. This work underscores the poten-
tial of automated techniques for improving our un-
derstanding of the prevalence and diagnostic utility
of linguistic features associated with ASD and other
communication and language disorders.
In future work, we plan to use a development set
to determine the optimal number of topical words
to select during the topic estimate filtering stage of
the pipeline in order to maintain improvements in
precision without a loss in recall. We would also
like to investigate using part-of-speech, word sense,
and parse information to improve our approaches
for both semantic expansion and topic estimation.
Although the rate of unexpected word use alone is
unlikely to provide sufficient power to classify the
two diagnostic groups investigated here, we expect
that it can serve as one feature in an array of fea-
tures that capture the broad range of semantic and
pragmatic atypicalities observed in the spoken lan-
guage of children with autism. Finally, we plan to
apply these same methods to identify the confabula-
tions and topic shifts often observed in the narrative
retellings of the elderly with neurodegenerative con-
ditions.
Acknowledgments
This work was supported in part by NSF
Grant #BCS-0826654, and NIH NIDCD grant
#1R01DC012033-01. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
713
Christian Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic language
deficits in children with high-functioning autism: De-
picting the phenotype using the pragmatic rating scale.
Research in Autism Spectrum Disorders, 6:792797.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability
in high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental Dis-
orders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tunali.
1990. Narrative story telling in autism and down?s
syndrome. British Journal of Developmental Psychol-
ogy, 8(1):9?23.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval. Cambridge University Press.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Helen Tager-Flusberg. 2001. Understanding the lan-
guage and communicative impairments in autism. In-
ternational Review of Research in Mental Retardation,
23:185?205.
C.J. van Rijsbergen, D.J. Harper, and M.F. Porter. 1981.
The selection of good search terms. Information Pro-
cessing and Management, 17(2):77?91.
Joanne Volden and Catherine Lord. 1991. Neologisms
and idiosyncratic language in autistic speakers. Jour-
nal of Autism and Developmental Disorders, 21:109?
130.
714
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 88?96,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Classification of atypical language in autism
Emily T. Prud?hommeaux, Brian Roark, Lois M. Black, and Jan van Santen
Center for Spoken Language Understanding
Oregon Health & Science University
20000 NW Walker Rd., Beaverton, Oregon 97006
{emily,roark,lmblack,vansanten}@cslu.ogi.edu
Abstract
Atypical or idiosyncratic language is a char-
acteristic of autism spectrum disorder (ASD).
In this paper, we discuss previous work iden-
tifying language errors associated with atyp-
ical language in ASD and describe a proce-
dure for reproducing those results. We de-
scribe our data set, which consists of tran-
scribed data from a widely used clinical di-
agnostic instrument (the ADOS) for children
with autism, children with developmental lan-
guage disorder, and typically developing chil-
dren. We then present methods for automati-
cally extracting lexical and syntactic features
from transcripts of children?s speech to 1)
identify certain syntactic and semantic errors
that have previously been found to distinguish
ASD language from that of children with typ-
ical development; and 2) perform diagnostic
classification. Our classifiers achieve results
well above chance, demonstrating the poten-
tial for using NLP techniques to enhance neu-
rodevelopmental diagnosis and atypical lan-
guage analysis. We expect further improve-
ment with additional data, features, and clas-
sification techniques.
1 Introduction
Atypical language and communication have been as-
sociated with autism spectrum disorder (ASD) since
Kanner (1943) first gave the name autism to the dis-
order. The Autism Diagnostic Observation Sched-
ule (ADOS) (Lord et al, 2002) and other widely
used diagnostic instruments include unusual word
use as a diagnostic criterion. The broad and con-
flicting definitions used in diagnostic instruments for
ASD, however, can lead to difficulty distinguishing
the language peculiarities associated with autism.
The most recent and the most systematic study of
unusual word use in ASD (Volden and Lord, 1991)
found that certain types of atypical word use were
significantly more prevalent in ASD speech than
in the speech of children with typical development
(TD). Although the results provided interesting in-
formation about unusual language in ASD, the pro-
cess of coding these types of errors was laborious
and required substantial linguistic and clinical ex-
pertise.
In this paper, we first use our own data to repro-
duce a subset of the results reported in Volden and
Lord (1991). We then present a method of automat-
ically identifying the types of errors associated with
ASD using spoken language features and machine
learning techniques. These same features are then
used to differentiate subjects with ASD or a devel-
opmental language disorder (DLD) from those with
TD. Although these linguistic features yield strong
classification results, they also reveal a number of
obstacles to distinguishing language characteristics
associated with autism from those associated with
language impairment.
2 Previous Work
Since it was first recognized as a neurodevelop-
mental disorder, autism has been associated with
language described variously as: ?seemingly non-
sensical and irrelevant?, ?peculiar and out of place
in ordinary conversation? (Kanner, 1946); ?stereo-
typed?, ?metaphorical?, ?inappropriate? (Bartak et
al., 1975); and characterized by ?a lack of ease in
88
the use of words? (Rutter, 1965) and ?the use of
standard, familiar words or phrases in idiosyncratic
but meaningful way? (Volden and Lord, 1991). The
three most common instruments used in ASD diag-
nosis ? the Autism Diagnostic Observation Sched-
ule (ADOS) (Lord et al, 2002), the Autism Di-
agnostic Interview-Revised (ADI-R) (Lord et al,
1994), and the Social Communication Questionnaire
(SCQ) (Rutter et al, 2003) ? make reference to
these language particularities in their scoring algo-
rithms. Unfortunately, the guidelines for identify-
ing this unusual language are often vague (SCQ:
?odd?, ADI-R: ?idiosyncratic?, ADOS: ?unusual?)
and sometimes contradictory (ADOS: ?appropriate?
vs. ADI-R: ?inappropriate?; ADOS: ?phrases...they
could not have heard? vs. SCQ: ?phrases that he/she
has heard other people use?).
In what is one of the only studies focused specif-
ically on unusual word use in ASD, Volden and
Lord (1991) transcribed two 10-minute speech sam-
ples from the ADOS for 20 school-aged, high-
functioning children with autism and 20 with typi-
cal development. Utterances containing non-English
words or the unusual use of a word or phrase were
flagged by student workers and then categorized by
the authors into one of three classes according to the
type of error:
? Developmental syntax error: a violation of a
syntactic rule normally acquired in early child-
hood, such as the use of object pronoun in sub-
ject position or an overextension of a regular
morphological rule, e.g., What does cows do?
? Non-developmental syntax error: a syntactic
error not commonly observed in the speech of
children acquiring language, e.g., But in the car
it?s some.
? Semantic error: a syntactically intact sentence
with an odd or unexpected word given the con-
text and intended meaning, e.g., They?re siding
the table.
The authors found that high-functioning chil-
dren with ASD produced significantly more non-
developmental and semantic errors than children
with typical development. The number of develop-
mental syntax errors was not significantly different
between these two groups.
Although there has been virtually no previous
work on automated analysis of unannotated tran-
scripts of the speech of children with ASD, auto-
matically extracted language features have shown
promise in the identification of other neurological
disorders such as language impairment and cogni-
tive impairment. Gabani et al (2009) used part-of-
speech language models to derive perplexity scores
for transcripts of the speech of children with and
without language impairment. These scores offered
significant diagnostic power, achieving an F1 mea-
sure of roughly 70% when used within an support
vector machine (SVM) for classification. Roark et
al. (in press) extracted a much larger set of lan-
guage complexity features derived from syntactic
parse trees from transcripts of narratives produced
by elderly subjects for the diagnosis of mild cogni-
tive impairment. Selecting a subset of these features
for classification with an SVM yielded accuracy, as
measured by the area under the receiver operating
characteristic curve, of 0.73.
Language models have also been applied to the
task of error identification, but primarily in writ-
ing samples of ESL learners. Gamon et al (2008)
used word-based language models to detect and
correct common ESL errors, while Leacock and
Chodorow (2003) used part-of-speech bigram lan-
guage models to identify potentially ungrammatical
two-word sequences in ESL essays. Although these
tasks differ in a number of ways from our tasks, they
demonstrate the utility of using both word and part-
of-speech language models for error detection.
3 Data Collection
3.1 Subjects
Our first objective was to gather data in order repro-
duce the results reported in Volden and Lord (1991).
As shown in Table 1, the participants in our study
were 50 children ages 4 to 8 with a performance
IQ greater than 80 and a diagnosis of either typical
Diagnosis Count Age (s.d.) IQ (s.d.)
TD 17 6.24 (1.38) 125.7 (11.63)
ASD 20 6.38 (1.25) 108.9 (16.41)
DLD 13 7.01 (1.10) 100.6 (10.95)
Table 1: Count, mean age and IQ by subject group.
89
development (TD, n=17), autism spectrum disorder
(ASD, n=20), or developmental language disorder
(DLD, n=13).
Developmental language disorder (DLD), also
sometimes known as specific language impairment
(SLI), is generally defined as the delayed or im-
paired acquisition of language without accompany-
ing comparable delays or deficits in hearing, cogni-
tion, and socio-emotional development (McCauley,
2001). The language impairments that characterize
DLD are not related to articulation or ?speech im-
pediments? but rather are associated with more pro-
found problems producing and often comprehend-
ing language in terms of its pragmatics, syntax, se-
mantics, and phonology. The DSM-IV-TR (Ameri-
can Psychiatric Association, 2000) includes neither
DLD nor SLI as a disorder, but for the purposes
of this work, DLD corresponds to the DSM?s des-
ignations Expressive Language Disorder and Mixed
Expressive-Receptive Language Disorder.
For this study, a subject received a diagnosis of
DLD if he or she met one of two commonly used
criteria: 1) The Tomblin Epi-SLI criteria (Tomblin,
et al, 1996), in which diagnosis of language im-
pairment is indicated when scores in two out of five
domains (vocabulary, grammar, narrative, receptive,
and expressive) are greater than 1.25 standard devia-
tions below the mean; and 2) The CELF-Preschool-
2/CELF-4 criteria, in which diagnosis of language
impairment is indicated when one out of three index
scores and one out of three spontaneous language
scores are more than one standard deviation below
the mean.
A diagnosis of ASD required a previous medi-
cal, educational, or clinical diagnosis of ASD, which
was then confirmed by our team of clinicians ac-
cording to the criteria of the DSM-IV-TR (Ameri-
can Psychiatric Association, 2000), the revised al-
gorithm of the ADOS (Lord et al, 2002), and the
SCQ parental interview (Rutter et al, 2003). Fifteen
of the 20 ASD subjects participating in this study
also met at least one of the above described criteria
for DLD.
3.2 Data Preparation
The ADOS (Lord et al, 2002), a semi-structured se-
ries of activities designed to reveal behaviors asso-
ciated with autism, was administered to all 50 sub-
jects. Five of the ADOS activities that require sig-
nificant amounts spontaneous speech (Make-Believe
Play, Joint Interactive Play, Description of a Pic-
ture, Telling a Story From a Book, and Conversa-
tion and Reporting) were then transcribed at the ut-
terance level for all 50 speakers. All utterances from
the transcripts longer than four words (11,244) were
presented to individuals blind to the purposes of the
study, who were asked to flag any sentence with
atypical or unusual word use. Those sentences were
then classified by the authors as having no errors or
one of the three error types described in Volden and
Lord. Examples from our data are given in Table 2.
3.3 Reproducing Previous Results
In order to compare our results to those reported in
Volden and Lord, we calculated the rates of the three
types of errors for each subject, as shown in Ta-
ble 2. With a two-sample (TD v. ASD) t-test, the
rates of nondevelopmental and semantic errors were
significantly higher in the ASD group than in the
TD group, while there was no significant difference
in developmental errors between the two groups.
These results reflect the same trends observed in
Volden and Lord, in which the raw counts of both
developmental and semantic errors were higher in
the ASD group.
Using ANOVA for significance testing over all
three diagnostic groups, we found that the rate of
developmental errors was significantly higher in the
DLD group than in the other groups. The difference
in semantic error rate between TD and ASD using
the t-test was preserved, but the difference in nonde-
velopmental error rate was lost when comparing all
three diagnostic groups with ANOVA, as shown in
Figure 1.
Error Example
Dev.
I have a games.
The baby drinked it.
The frogs was watching TV.
Nondev.
He locked him all of out.
Would you like to be fall down?
He got so the ball went each way.
Sem.
Something makes my eyes poke.
It smells like it?s falling on your head.
All the fish are leaving in the air.
Table 2: Examples of error types.
90
00.02
0.04
0.06
0.08
Dev. Nondev. Sem.
TD
ASD
DLD
*
*
*
Figure 1: Error rates by diagnostic group (*p <0.05).
The process of manually identifying sentences
with atypical or unusual language was relatively
painless, but determining the specific error types is
subjective and time-consuming, and requires a great
deal of expertise. In addition, although we do ob-
serve significant differences between groups, it is
not clear whether the differences are sufficient for
diagnostic classification or discrimination.
We now propose automatically extracting from
the transcripts various measures of linguistic likeli-
hood, complexity, and surprisal that have the poten-
tial to objectively capture qualities that differentiate
1) the three types of errors described above, and 2)
the three diagnostic groups discussed above. In the
next three sections, we will discuss the various lin-
guistic features we extract; methods for using these
features to classify each sentence according to its er-
ror type for the purpose of automatic error-detection;
and methods for using these features, calculated for
each subject, for diagnostic classification.
4 Features
N-gram cross entropy. Following previous work
in both error detection (Gamon et al, 2008; Leacock
and Chodorow, 2003) and neurodevelopmental di-
agnostic classification (Gabani et al, 2009), we be-
gin with simple bigram language model features. A
bigram language model provides information about
the likelihood of a given item (e.g., a word or part
of speech) in a sentence given the previous item in
that sentence. We suspect that some of the types
of unusual language investigated here, in particular
those seen in the syntactic errors shown in Table 2,
are characterized by unlikely words (drinked) and
word or part-of-speech sequences (a games, all of
out) and hence might be distinguished by language
model-based scores.
We build a word-level bigram language model and
a part-of-speech level bigram language model from
the Switchboard (Godfrey et al, 1992) corpus. We
then automatically generate part-of-speech tags for
each sentence (where the tags were derived from
the best scoring output of the full syntactic parser
mentioned below), and then apply the two models
to each sentence. For each sentence, we calculate
its cross entropy and perplexity. For a word string
w1 . . . wn of length n, the cross entropy H is
H(w1 . . . wn) = ?
1
n
log P(w1 . . . wn) (1)
where P(w1 . . . wn) is calculated as the product of
the n-gram probabilities of each word in the string.
The corresponding measure can be calculated for the
POS-tag sequence, based on an n-gram model of
tags. Perplexity is simply 2H .
While we would prefer to use a corpus that is
closer to the child language that we are attempting
to model, we found the conversational style of the
Switchboard corpus to be the most effective large
corpus that we had at our disposal for this study.
As the size of our small corpus grows, we intend to
make use of the text to assist with model building,
but for this study, we used all out-of-domain data
for n-gram language models and parsing models.
Using Switchboard also allowed us to use the same
corpus to train both n-gram and parsing models.
Surprisal-based features. Surprisal, or the unex-
pectedness of a word or syntactic category in a given
context, is often used as a psycholinguistic mea-
sure of sentence-processing difficulty (Hale, 2001;
Boston et al, 2008). Although surprisal is usually
discussed in the context of cognitive load for lan-
guage processing, we hoped that it might also cap-
ture some of the language characteristics of the se-
mantic errors like those in Table 2, which often con-
tain common words used in surprising ways, and
the nondevelopmental syntax errors, which often in-
clude strings of function words presented in an order
that would be difficult to anticipate.
To derive surprisal-based features, each sentence
is parsed using the Roark (2001) incremental
top-down parser relying on a model built again on
91
the Switchboard corpus. The incremental output of
the parser shows the surprisal for each word, as well
as other scores, as presented in Roark et al (2009).
For each sentence, we collected the mean surprisal
(equivalent to the cross entropy given the model);
the mean syntactic surprisal; and the mean lexical
surprisal. The lexical and syntactic surprisal are a
decomposition of the total surprisal into that portion
due to probability mass associated with building
non-terminal structure (syntactic surprisal) and that
portion due to probability mass associated with
building terminal lexical items in the tree (lexical
surprisal). We refer the reader to that paper for
further details.
Other linguistic complexity measures The non-
developmental syntax errors in Table 2 are charac-
terized by their ill-formed syntactic structure. Fol-
lowing Roark et al (in press), in which the authors
explored the relationship between linguistic struc-
tural complexity and cognitive decline, and Sagae
(2005), in which the authors used automatic syntac-
tic annotation to assess syntactic development, we
also investigated the following measures of linguis-
tic complexity: words per clause, tree nodes per
word, dependency length per word, and Ygnve and
Frazier scores per word. Each of these scores can
be calculated from a provided syntactic parse tree,
and to generate these we made use of the Charniak
parser (Charniak, 2000), also trained on the Switch-
board treebank.
Briefly, words per clause is the total number of
words divided by the total number of clauses; and
tree nodes per word is the total number of nodes
in the parse tree divided by the number of words.
The dependency length for a word is the distance (in
word tokens) between that word and its governor,
as determined through standard head-percolation
methods from the output of the Charniak parser. We
calculate the mean of this length over all words in
the utterance. The Yngve score of a word is the
size of the stack of a shift-reduce parser after that
word; and the Frazier score essentially counts how
many intermediate nodes exist in the tree between
the word and its lowest ancestor that is either the
root or has a left sibling in the tree. We calculate
the mean of both of these scores over the utterance.
We refer the reader to the above cited paper for more
details on these measures.
As noted in Roark et al (in press), some of these
measures are influenced by particular characteristics
of the Penn Treebank style trees ? e.g., flat noun
phrases, etc. ? and measures vary in the degree to
which they capture divergence from typical struc-
tures. Some (including Yngve) are sensitive to the
breadth of trees (e.g., flat productions with many
children); others (including Frazier) are sensitive to
depth of trees. This variability is a key reason for
including multiple, complementary features, such as
both Frazier and Yngve scores, to capture more sub-
tle syntactic characteristics than would be available
from any of these measures alone.
Although we were not able to measure parsing ac-
curacy on our data set and how it might affect the re-
liability of these features, Roark et al (in press) did
investigate this very issue. They found that all of the
above described syntactic measures, when they were
derived from automatically generated parse trees,
correlated very highly (greater than 0.9) with those
measures when they were derived from manually
generated parse trees. For the moment, we assume
that the same principle holds true for our data set,
though we do intend both to verify this assump-
tion and to supplement our parsing models with data
from child speech. Based on manual inspection of
parser output, the current parsing model does seem
to be recovering largely valid structures.
5 Error Classification
The values for 8 of the 12 features were significantly
different over the three error classes, as measured
by one-way ANOVA: words per clause, Yngve, de-
pendency, word cross-entropy all significant at p <
0.001; Frazier, nodes per word at p < 0.01; overall
surprisal and lexical surprisal at p < 0.05. We built
classification and regression trees (CART) using the
Weka data mining software (Hall et al, 2009) us-
ing all of the 12 features described above to predict
which error each sentence contained, and we report
the accuracy, weighted F measure, and area under
the receiver operating characteristic curve (AUC).
Including all 12 features in the CART using 10-
fold cross validation resulted in an AUC of 0.68,
while using only those features with significant
between-group differences yielded an AUC of 0.65.
92
Classifier Acc. F1 AUC
Baseline 1 41% 0.24 0.5
Baseline 2 33% 0.32 0.5
All features 53% 0.53 0.68
Feature subset 49% 0.49 0.65
Table 3: Error-type classification results.
These are both substantial improvements over a
baseline with an unbalanced corpus in which the
most frequent class is chosen for all input items
(Baseline 1) or a baseline with a balanced corpus in
which class is chosen at random (Baseline 2), which
both have an AUC of 0.5. The results for each of
these classifiers, provided in Table 3, show potential
for automating the identification of error type.
6 Diagnostic Classification
In Section 3, we found a number of significant dif-
ferences in error type production rates across our
three diagnostic groups. Individual rates of error
production, however, provide almost no classifica-
tion power within a CART (AUC = 0.51). Perhaps
the phenomena being observed in ASD and DLD
language are related to subtle language features that
are less easily identified than simply the membership
of a sentence in one of these three error categories.
Given the ability of our language features to dis-
criminate error types moderately well, as shown in
Section 5, we decided to extract these same 12 fea-
tures from every sentence longer than 4 words from
the entire transcript for each of the subjects. We
then took the mean of each feature over all of the
sentences for each speaker. These per-speaker fea-
ture vectors were used for diagnostic classification
within a CART.
We first performed classification over the three di-
agnostic groups using the full set of 12 features de-
scribed in Section 4. This results in only modest
gains in performance over the baseline that uses er-
ror rates as the only features. We then used ANOVA
to determine which of the 12 features differed sig-
nificantly across the three groups. Only four fea-
tures were found to be significantly different across
the three groups (words per clause, Yngve, depen-
dency, word cross entropy), and none of them dif-
ferent significantly between the ASD group and the
DLD group. As expected, classification did not im-
Features Acc. F1 AUC
Error rates 33% 0.32 0.51
All features 42% 0.38 0.59
Feature subset 40% 0.37 0.6
Table 4: All subjects: Diagnostic classification results.
prove with this feature subset, as reported in Table 4.
Recall that 15 of the 20 ASD subjects also met at
least one criterion for a developmental language dis-
order. Perhaps the language peculiarities we observe
in our subjects with ASD are related in part to lan-
guage characteristics of DLD rather than ASD. We
now attempt to tease apart these two sources of un-
usual language by investigating three separate clas-
sification tasks: TD vs. ASD, TD vs. DLD, and
ASD vs. DLD.
6.1 TD vs. ASD
We perform classification of the TD and ASD sub-
jects with three feature sets: 1) per-subject error
rates; 2) all 12 features described in Section 4; and
3) the subset of significantly different features. We
found that 7 of the 12 features explored in Section 4
differed significantly between the TD group and the
ASD group: words per clause, Yngve, dependency,
word cross-entropy, overall surprisal, syntactic sur-
prisal, and lexical surprisal. Classification results are
shown in Table 5. We see that using the automati-
cally derived linguistic features improves classifica-
tion substantially over the baseline using per-subject
error rates, particularly when we use the feature sub-
set. Note that the best classification accuracy results
are comparable to those reported in related work on
language impairment and mild cognitive impairment
described in Section 2.
6.2 TD vs. DLD
We perform classification of TD and DLD subjects
with the same three feature sets used for the TD
vs. ASD classification. We found that 6 of the 12
Features Acc. F1 AUC
Error rates 62% 0.62 0.56
All features 62% 0.62 0.65
Feature subset 68% 0.67 0.72
Table 5: TD vs. ASD: Diagnostic classification results.
93
Features Acc. F1 AUC
Error rates 67% 0.67 0.72
All features 80% 0.79 0.75
Feature subset 77% 0.75 0.66
Table 6: TD vs. DLD: Diagnostic classification results.
features explored in Section 4 different significantly
between the TD group and the ASD group: words
per clause, Yngve, dependency, word cross-entropy,
overall surprisal, and lexical surprisal. Note that this
is a subset of the features that differed between the
TD group and ASD group. Classification results are
shown in Table 6. Interestingly, using per-subject er-
ror rates for classification of TD and DLD subjects
was quite robust. Using all of the features improved
classification somewhat, while using only a subset
resulted in degraded performance. We see that the
discriminative power of these features is superior to
that reported in earlier work using LM-based fea-
tures for classification of specific language impair-
ment (Gabani et al, 2009).
6.3 ASD vs. DLD
Finally, we perform classification of the ASD and
DLD subjects using only the first two features
sets, since there were no features found to be even
marginally significantly different between these two
groups. Classification results, which are dismal for
both feature sets, are shown in Table 7.
6.4 Discussion
It seems quite clear that the error rates, feature val-
ues, and classification performance are all being in-
fluenced by the fact that a majority of the ASD sub-
jects also meet at least one criterion for a develop-
mental language disorder. Neither error rates nor
feature values could discriminate between the ASD
and DLD group. Nevertheless we see that our ASD
group and DLD group do not follow the same pat-
terns in their error production or language feature
scores. Clearly there are differences in the language
Features Acc. F1 AUC
Error rates 55% 0.52 0.48
All features 58% 0.44 0.40
Table 7: ASD vs. DLD: Diagnostic classification results.
patterns of the two groups that are not being cap-
tured with any of the methods discussed here.
We also observe that the error rates them-
selves, while sometimes significantly different
across groups as originally observed in Volden and
Lord, do not perform well as diagnostic features
for ASD in our framework. Volden and Lord did
not attempt classification in their study, so it is not
known whether the authors would have encountered
the same problem. There are, however, a number
of possible explanations for a discrepancy between
our results and theirs. First, our data was gath-
ered from pre-school and young school-aged chil-
dren, while the Volden and Lord subjects were gen-
erally teenagers and young adults. The way in which
their spoken language samples were elicited allowed
Volden and Lord to use raw error counts rather than
error rates. There may also have been important dif-
ferences in the way we carried out the manual er-
ror identification process, despite our best efforts to
replicate their procedure. Further development of
our classification methods and additional data col-
lection are needed to determine the utility of error
type identification for diagnostic purposes.
7 Future Work
Although our classifiers using automatically ex-
tracted features were generally robust, we expect
that including additional classification techniques,
subjects (especially ASD subjects without DLD),
and features will further improve our results. In
particular, we would like to explore semantic and
lexical features that are less dependent on linear or-
der and syntactic structure, such as Resnik similarity
and features derived using latent semantic analysis.
We also plan to expand the training input for
the language model and parser to include children?s
speech. The Switchboard corpus is conversational
speech, but it may fail to adequately model many lin-
guistic features characteristic of small children. The
CHILDES database of children?s speech, although
it is not large enough to be used on its own for our
analysis and would require significant manual syn-
tactic annotation, might provide enough data for us
to adapt our models to the child language domain.
Finally, we would like to investigate how infor-
mative the error types are and whether they can be
94
reliably coded by multiple judges. When we exam-
ined the output of our error-type classifier, we no-
ticed that many of the misclassified examples could
be construed, upon closer inspection, as belonging
to multiple error classes. The sentence He?s flying
in a lily-pond, for instance, could contain a devel-
opmental error (i.e., the child has not yet acquired
the correct meaning of in) or a semantic error (i.e.,
the child is using the word flying instead of swim-
ming). Without knowing the context in which the
sentence was uttered, it is not possible to determine
the type of error through any manual or automatic
means. The seemingly large number of misclassifi-
cations of sentences like this indicates the need for
further investigation of the existing coding proce-
dure and in-depth classification error analysis.
8 Conclusions
Our method of automatically identifying error type
shows promise as a supplement to, or substitute for,
the time-consuming and subjective manual coding
process described in Volden and Lord (Volden and
Lord, 1991). However, the superior performance of
our automatically extracted language features sug-
gests that perhaps it may not be the errors them-
selves that characterize the speech of children with
ASD and DLD but rather a preference for certain
structures and word sequences that sometimes mani-
fest themselves as clear language errors. Such varia-
tions in complexity and likelihood might be too sub-
tle for humans to reliably observe.
In summary, the methods explored in this paper
show potential for improving diagnostic discrimina-
tion between typically developing children and those
with these neurodevelopmental disorders. Further
research is required, however, in finding the most re-
liable markers that can be derived from such spoken
language samples.
Acknowledgments
This work was supported in part by NSF Grant
#BCS-0826654; an Innovative Technology for
Autism grant from Autism Speaks; and NIH NIDCD
grant #1R01DC007129-01. Any opinions, findings,
conclusions or recommendations expressed in this
publication are those of the authors and do not nec-
essarily reflect the views of the NSF, Autism Speaks,
or the NIH. Thanks to Meg Mitchell and Cheryl
Greene for their assistance with this project.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC, 4th edition.
Laurence Bartak, Michael Rutter, and Anthony Cox.
1975. A comparative study of infantile autism
and specific developmental receptive language disor-
der. I. The children. British Journal of Psychiatry,
126:27145.
Mariss Ferrara Boston, John Hale, Reinhold Kliegl, and
Shravan Vasishth. 2008. Surprising parser actions
and reading difficulty. In Proceedings of ACL-08:HLT,
Short Papers.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Conference of the
North American Chapter of the ACL, pages 132?139.
Keyur Gabani, Melissa Sherman, Thamar Solorio, and
Yang Liu. 2009. A corpus-based approach for the
prediction of language impairment in monolingual en-
glish and spanish-english bilingual children. In Pro-
ceedings of NAACL-HLT, pages 46?55.
Michael Gamon, Jianfeng Gao, Chris Brockett, and
Re Klementiev. 2008. Using contextual speller tech-
niques and language modeling for ESL error correc-
tion. In Proceedings of IJCNLP.
John J. Godfrey, Edward Holliman, and Jane McDaniel.
1992. SWITCHBOARD: telephone speech corpus for
research and development. In Proceedings of ICASSP,
volume 1, pages 517?520.
John T. Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the 2nd
meeting of NAACL.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
Leo Kanner. 1946. Irrelevant and metaphorical lan-
guage. American Journal of Psychiatry, 103:242?246.
Claudia Leacock and Martin Chodorow. 2003. Auto-
mated grammatical error detection. In M.D. Shermis
and J. Burstein, editors, Automated essay scoring: A
cross-disciplinary perspective. Lawrence Erlbaum As-
sociates, Inc., Hillsdale, NJ.
Catherine Lord, Michael Rutter, and Anne LeCouteur.
1994. Autism diagnostic interview-revised: A revised
95
version of a diagnostic interview for caregivers of in-
dividuals with possible pervasive developmental disor-
ders. Journal of Autism and Developmental Disorders,
24:659?685.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Rebecca McCauley. 2001. Assessment of language dis-
orders in children. Lawrence Erlbaum Associates,
Mahwah, NJ.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In Pro-
ceedings of EMNLP, pages 324?333.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristina Hollingshead, and Jeffrey Kaye. in press.
Spoken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Audio,
Speech and Language Processing.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
Michael Rutter. 1965. Speech disorders in a series of
autistic children. In A. Franklin, editor, Children with
communication problems, pages 39?47. Pitman.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the ACL.
Joanne Volden and Catherine Lord. 1991. Neologisms
and idiosyncratic language in autistic speakers. Jour-
nal of Autism and Developmental Disorders, 21:109?
130.
96
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 1?10,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Graph-based alignment of narratives for automated neurological assessment
Emily T. Prud?hommeaux and Brian Roark
Center for Spoken Language Understanding
Oregon Health & Science University
{emilypx,roarkbr}@gmail.com
Abstract
Narrative recall tasks are widely used in neu-
ropsychological evaluation protocols in or-
der to detect symptoms of disorders such
as autism, language impairment, and demen-
tia. In this paper, we propose a graph-based
method commonly used in information re-
trieval to improve word-level alignments in
order to align a source narrative to narra-
tive retellings elicited in a clinical setting.
From these alignments, we automatically ex-
tract narrative recall scores which can then be
used for diagnostic screening. The signifi-
cant reduction in alignment error rate (AER)
afforded by the graph-based method results
in improved automatic scoring and diagnos-
tic classification. The approach described here
is general enough to be applied to almost any
narrative recall scenario, and the reductions in
AER achieved in this work attest to the po-
tential utility of this graph-based method for
enhancing multilingual word alignment and
alignment of comparable corpora for more
standard NLP tasks.
1 Introduction
Much of the work in biomedical natural language
processing has focused on mining information from
electronic health records, clinical notes, and medical
literature, but NLP is also very well suited for ana-
lyzing patient language data, in terms of both con-
tent and linguistic features, for neurological eval-
uation. NLP-driven analysis of clinical language
data has been used to assess language development
(Sagae et al, 2005), language impairment (Gabani
et al, 2009) and cognitive status (Roark et al, 2007;
Roark et al, 2011). These approaches rely on the ex-
traction of syntactic features from spoken language
transcripts in order to identify characteristics of lan-
guage use associated with a particular disorder. In
this paper, rather than focusing on linguistic fea-
tures, we instead propose an NLP-based method for
automating the standard manual method for scoring
the Wechsler Logical Memory (WLM) subtest of the
Wechsler Memory Scale (Wechsler, 1997) with the
eventual goal of developing a screening tool for Mild
Cognitive Impairment (MCI), the earliest observable
precursor to dementia. During standard administra-
tion of the WLM, the examiner reads a brief narra-
tive to the subject, who then retells the story to the
examiner, once immediately upon hearing the story
and a second time after a 30-minute delay. The ex-
aminer scores the retelling in real time by counting
the number of recalled story elements, each of which
corresponds to a word or short phrase in the source
narrative. Our method for automatically extracting
the score from a retelling relies on an alignment be-
tween substrings in the retelling and substrings in
the original narrative. The scores thus extracted can
then be used for diagnostic classification.
Previous approaches to alignment-based narra-
tive analysis (Prud?hommeaux and Roark, 2011a;
Prud?hommeaux and Roark, 2011b) have relied ex-
clusively on modified versions of standard word
alignment algorithms typically applied to large bilin-
gual parallel corpora for building machine transla-
tion models (Liang et al, 2006; Och et al, 2000).
Scores extracted from the alignments produced us-
ing these algorithms achieved fairly high classifi-
1
cation accuracy, but the somewhat weak alignment
quality limited performance. In this paper, we com-
pare these word alignment approaches to a new ap-
proach that uses traditionally-derived word align-
ments between retellings as the input for graph-
based exploration of the alignment space in order to
improve alignment accuracy. Using both earlier ap-
proaches and our novel method for word alignment,
we then evaluate the accuracy of automated scoring
and diagnostic classification for MCI.
Although the alignment error rates for our data
might be considered high in the context of building
phrase tables for machine translation, the alignments
produced using the graph-based method are remark-
ably accurate given the small size of our training
corpus. In addition, these more accurate alignments
lead to gains in scoring accuracy and to classification
performance approaching that of manually derived
scores. This method for word alignment and score
extraction is general enough to be easily adapted
to other tests used in neuropsychological evalua-
tion, including not only those related to narrative re-
call, such as the NEPSY Narrative Memory subtest
(Korkman et al, 1998) but also picture description
tasks, such as the Cookie Theft picture description
task of the Boston Diagnostic Aphasia Examination
(Goodglass et al, 2001) or the Renfrew Bus Story
(Glasgow and Cowley, 1994). In addition, this tech-
nique has the potential to improve word alignment
for more general NLP tasks that rely on small cor-
pora, such as multilingual word alignment or word
alignment of comparable corpora.
2 Background
The act of retelling or producing a narrative taps into
a wide array of cognitive functions, not only mem-
ory but also language comprehension, language pro-
duction, executive function, and theory of mind. The
inability to coherently produce or recall a narrative
is therefore associated with many different cogni-
tive and developmental disorders, including demen-
tia, autism (Tager-Flusberg, 1995), and language im-
pairment (Dodwell and Bavin, 2008; Botting, 2002).
Narrative tasks are widely used in neuropsycholog-
ical assessment, and many commonly used instru-
ments and diagnostic protocols include a task in-
volving narrative recall or production (Korkman et
al., 1998; Wechsler, 1997; Lord et al, 2002).
In this paper, we focus on evaluating narrative re-
call within the context of Mild Cognitive Impair-
ment (MCI), the earliest clinically significant pre-
cursor of dementia. The cognitive and memory
problems associated with MCI do not necessarily
interfere with daily living activities (Ritchie and
Touchon, 2000) and can therefore be difficult to
diagnose using standard dementia screening tools,
such as the Mini-Mental State Exam (Folstein et al,
1975). A definitive diagnosis of MCI requires an
extensive interview with the patient and a family
member or caregiver. Because of the effort required
for diagnosis and the insensitivity of the standard
screening tools, MCI frequently goes undiagnosed,
delaying the introduction of appropriate treatment
and remediation. Early and unobtrusive detection
will become increasingly important as the elderly
population grows and as research advances in delay-
ing and potentially stopping the progression of MCI
into moderate and severe dementia.
Narrative recall tasks, such as the test used in re-
search presented here, the Wechsler Logical Mem-
ory subtest (WLM), are often used in conjunction
with other cognitive measures in attempts to identify
MCI and dementia. Multiple studies have demon-
strated a significant difference in performance on the
WLM between subjects with MCI and typically ag-
ing controls, particularly in combination with tests
of verbal fluency and memory (Storandt and Hill,
1989; Peterson et al, 1999; Nordlund et al, 2005).
The WLM can also serve as a cognitive indicator of
physiological characteristics associated with symp-
tomatic Alzheimers disease, even in the absence of
previously reported dementia (Schmitt et al, 2000;
Bennett et al, 2006).
Some previous work on automated analysis of the
WLM has focused on using the retellings as a source
of linguistic data for extracting syntactic and pho-
netic features that can distinguish subjects with MCI
from typically aging controls (Roark et al, 2011).
There has been some work on automating scoring
of other narrative recall tasks using unigram overlap
(Hakkani-Tur et al, 2010), but Dunn et al (2002)
are among the only researchers to apply automated
methods to scoring the WLM for the purpose of
identifying dementia, using latent semantic analysis
to measure the semantic distance between a retelling
2
Dx n Age Education
MCI 72 88.7 14.9 yr
Non-MCI 163 87.3 15.1 yr
Table 1: Subject demographic data.
and the source narrative. Although scoring automa-
tion is not typically used in a clinical setting, the
objectivity offered by automated measures is par-
ticularly important for tests like the WLM, which
are often administered by practitioners working in a
community setting and serving a diverse population.
Researchers working on NLP tasks such as para-
phrase extraction (Barzilay and McKeown, 2001),
word-sense disambiguation (Diab and Resnik,
2002), and bilingual lexicon induction (Sahlgren and
Karlgren, 2005), often rely on aligned parallel or
comparable corpora. Recasting the automated scor-
ing of a neuropsychological test as another NLP task
involving the analysis of parallel texts, however, is a
relatively new idea. We hope that the methods pre-
sented here will both highlight the flexibility of tech-
niques originally developed for standard NLP tasks
and attract attention to the wide variety of biomed-
ical data sources and potential clinical applications
for these techniques.
3 Data
3.1 Subjects
The data examined in this study was collected from
participants in a longitudinal study on brain aging
at the Layton Aging and Alzheimers Disease Cen-
ter at the Oregon Health and Science University
(OHSU), including 72 subjects with MCI and 163
typically aging seniors roughly matched for age and
years of education. Table 1 shows the mean age
and mean years of education for the two diagnos-
tic groups. There were no significant between-group
differences in either measure.
Following (Shankle et al, 2005), we assign a di-
agnosis of MCI according to the Clinical Dementia
Rating (CDR) (Morris, 1993). A CDR of 0.5 corre-
sponds to MCI (Ritchie and Touchon, 2000), while
a CDR of zero indicates the absence of MCI or any
dementia. The CDR is measured via the Neurobe-
havioral Cognitive Status Examination (Kiernan et
al., 1987) and a semi-structured interview with the
patient and a family member or caregiver that allows
the examiner to assess the subject in several key ar-
eas of cognitive function, such as memory, orienta-
tion, problem solving, and personal care. The CDR
has high inter-annotator reliability (Morris, 1993)
when conducted by trained experts. It is crucial to
note that the calculation of CDR is completely inde-
pendent of the neuropsychological test investigated
in this paper, the Wechsler Logical Memory subtest
of the Wechsler Memory Scale. We refer readers to
the above cited papers for a further details.
3.2 Wechsler Logical Memory Test
The Wechsler Logical Memory subtest (WLM) is
part of the Wechsler Memory Scale (Wechsler,
1997), a diagnostic instrument used to assess mem-
ory and cognition in adults. In the WLM, the subject
listens to the examiner read a brief narrative, shown
in Figure 1. The subject then retells the narrative to
the examiner twice: once immediately upon hearing
it (Logical Memory I, LM-I) and again after a 30-
minute delay (Logical Memory II, LM-II). The nar-
rative is divided into 25 story elements. In Figure 1,
the boundaries between story elements are denoted
by slashes. The examiner notes in real time which
story elements the subject uses. The score that is re-
ported under standard administration of the task is
a summary score, which is simply the raw number
of story elements recalled. Story elements do not
need to be recalled verbatim or in the correct tempo-
ral order. The published scoring guidelines describe
the permissible substitutions for each story element.
The first story element, Anna, can be replaced in the
retelling with Annie or Ann, while the 16th story
element, fifty-six dollars, can be replaced with any
number of dollars between fifty and sixty.
An example LM-I retelling is shown in Figure 2.
According to the published scoring guidelines, this
retelling receives a score of 12, since it contains the
following 12 elements: Anna, employed, Boston, as
a cook, was robbed of, she had four, small children,
reported, station, touched by the woman?s story,
took up a collection, and for her.
3.3 Word alignment data
The Wechsler Logical Memory immediate and de-
layed retellings for all of the 235 experimental sub-
jects were transcribed at the word level. We sup-
3
Anna / Thompson / of South / Boston / em-
ployed / as a cook / in a school / cafeteria /
reported / at the police / station / that she had
been held up / on State Street / the night be-
fore / and robbed of / fifty-six dollars. / She
had four / small children / the rent was due /
and they hadn?t eaten / for two days. / The po-
lice / touched by the woman?s story / took up
a collection / for her.
Figure 1: Text of WLM narrative segmented into 25 story
elements.
Ann Taylor worked in Boston as a cook. And
she was robbed of sixty-seven dollars. Is that
right? And she had four children and reported
at the some kind of station. The fellow was
sympathetic and made a collection for her so
that she can feed the children.
Figure 2: Sample retelling of the Wechsler narrative.
plemented the data collected from our experimental
subjects with transcriptions of retellings from 26 ad-
ditional individuals whose diagnosis had not been
confirmed at the time of publication or who did
not meet the eligibility criteria for this study. Par-
tial words, punctuation, and pause-fillers were ex-
cluded from all transcriptions used for this study.
The retellings were manually scored according to
published guidelines. In addition, we manually pro-
duced word-level alignments between each retelling
and the source narrative presented in Figure 1.
Word alignment for phrase-based machine trans-
lation typically takes as input a sentence-aligned
parallel corpus or bi-text, in which a sentence on
one side of the corpus is a translation of the sen-
tence in that same position on the other side of the
corpus. Since we are interested in learning how to
align words in the source narrative to words in the
retellings, our primary parallel corpus must consist
of source narrative text on one side and retelling
text on the other. Because the retellings contain
omissions, reorderings, and embellishments, we are
obliged to consider the full text of the source narra-
tive and of each retelling to be a ?sentence? in the
parallel corpus.
We compiled three parallel corpora to be used for
the word alignment experiments:
? Corpus 1: A roughly 500-line source-to-
retelling corpus consisting of the source narra-
tive on one side and each retelling on the other.
? Corpus 2: A roughly 250,000-line pairwise
retelling-to-retelling corpus, consisting of ev-
ery possible pairwise combination of retellings.
? Corpus 3: A roughly 900-line word identity
corpus, consisting of every word that appears
in every retelling and the source narrative.
The explicit parallel alignments of word identities
that compose Corpus 3 are included in order to en-
courage the alignment of a word in a retelling to that
same word in the source, if it exists.
The word alignment techniques that we use are
entirely unsupervised. Therefore, as in the case
with most experiments involving word alignment,
we build a model for the data we wish to evalu-
ate using that same data. We do, however, use the
retellings from the 26 individuals who were not ex-
perimental subjects as a development set for tuning
the various parameters of our system, which is de-
scribed below.
4 Word Alignment
4.1 Baseline alignment
We begin by building two word alignment models
using the Berkeley aligner (Liang et al, 2006), a
state-of-the-art word alignment package that relies
on IBM mixture models 1 and 2 (Brown et al, 1993)
and an HMM. We chose to use the Berkeley aligner,
rather than the more widely used Giza++ alignment
package, for this task because its joint training and
posterior decoding algorithms yield lower alignment
error rates on most data sets and because it offers
functionality for testing an existing model on new
data and for outputting posterior probabilities. The
smaller of our two Berkeley-generated models is
trained on Corpus 1 (the source-to-retelling parallel
corpus described above) and ten copies of Corpus
3 (the word identity corpus). The larger model is
trained on Corpus 1, Corpus 2 (the pairwise retelling
corpus), and 100 copies of Corpus 3. Both models
are then tested on the 470 retellings from our 235 ex-
perimental subjects. In addition, we use both mod-
els to align every retelling to every other retelling so
that we will have all pairwise alignments available
for use in the graph-based model.
4
Figure 3: Depiction of word graph.
The first two rows of Table 2 show the preci-
sion, recall, F-measure, and alignment error rate
(AER) (Och and Ney, 2003) for these two Berkeley
aligner models. We note that although AER for the
larger model is lower, the time required to train the
model is significantly larger. The alignments gen-
erated by the Berkeley aligner serve not only as a
baseline for comparison but also as a springboard
for the novel graph-based method of alignment we
will now discuss.
4.2 Graph-based refinement
Graph-based methods, in which paths or random
walks are traced through an interconnected graph of
nodes in order to learn more about the nodes them-
selves, have been used for various NLP tasks in in-
formation extraction and retrieval, including web-
page ranking (PageRank (Page et al, 1999)) and ex-
tractive summarization (LexRank (Erkan and Radev,
2004; Otterbacher et al, 2009)). In the PageRank al-
gorithm, the nodes of the graph are web pages and
the edges connecting the nodes are the hyperlinks
leading from those pages to other pages. The nodes
in the LexRank algorithm are sentences in a docu-
ment and the edges are the similarity scores between
those sentences. The likelihood of a random walk
through the graph starting at a particular node and
ending at another node provides information about
the relationship between those two nodes and the im-
portance of the starting node.
In the case of our graph-based method for word
alignment, each node represents a word in one of the
retellings or in the source narrative. The edges are
Figure 4: Changes in AER as ? increases.
the normalized posterior-weighted alignments that
the Berkeley aligner proposes between each word
and (1) words in the source narrative, and (2) words
in the other retellings, as depicted in Figure 3. Start-
ing at a particular node (i.e., a word in one of the
retellings), our algorithm can either walk from that
node to another node in the graph or to a word in
the source narrative. At each step in the walk, there
is a set probability ? that determines the likelihood
of transitioning to another retelling word versus a
word in the source narrative. When transitioning to
a retelling word, the destination word is chosen ac-
cording to the posterior probability assigned by the
Berkeley aligner to that alignment. When the walk
arrives at a source narrative word, that word is the
new proposed alignment for the starting word.
For each word in each retelling, we perform 1000
of these random walks, thereby generating a distri-
bution for each retelling word over all of the words
in the source narrative. The new alignment for the
word is the source word with the highest frequency
in that distribution.
We build two graphs on which to carry out these
random walks: one graph is built using the align-
ments generated by the smaller Berkeley alignment
model, and the other is built from the alignments
generated by the larger Berkeley alignment model.
Alignments with posterior probabilities of 0.5 or
greater are included as edges within the graph, since
this is the default posterior threshold used by the
Berkeley aligner. The value of ?, the probability of
walking to a retelling word node rather than a source
word, is tuned to the development set of retellings,
5
Model P R F AER
Berkeley-Small 72.1 79.6 75.6 24.5
Berkeley-Large 78.6 80.5 79.5 20.5
Graph-Small 77.9 81.2 79.5 20.6
Graph-Large 85.4 76.9 81.0 18.9
Table 2: Aligner performance comparison.
discussed in Section 3.3. Figure 4 shows how AER
varies according to the value of ? for the two graph-
based approaches.
Each of these four alignment models produces,
for each retelling, a set of word pairs containing one
word from the original narrative and one word from
the retelling. The manual gold alignments for the
235 experimental subjects were evaluated against
the alignments produced by each of the four models.
Table 2 shows the accuracy of word alignment us-
ing these two graph-based models in terms of preci-
sion, accuracy, F-measure, and alignment error rate,
alongside the same measures for the two Berkeley
models. We see that each of the graph-based models
outperforms the Berkeley model of the same size.
The performance of the small graph-based model is
especially remarkable since it an AER comparable
to the large Berkeley model while requiring signif-
icantly fewer computing resources. The difference
in processing time between the two approaches was
especially remarkable: the graph-based model com-
pleted in only a few minutes, while the large Berke-
ley model required 14 hours of training.
Figures 5 and 6 show the results of aligning
the retelling presented in Figure 2 using the small
Berkeley model and the large graph-based model,
respectively. Comparing these two alignments, we
see that the latter model yields more precise align-
ments with very little loss of recall, as is borne out
by the overall statistics shown in Table 2.
5 Scoring
The published scoring guidelines for the WLM spec-
ify the source words that compose each story ele-
ment. Figure 7 displays the source narrative with
the element IDs (A? Y ) and word IDs (1? 65) ex-
plicitly labeled. Element Q, for instance, consists of
the words 39 and 40, small children. Using this in-
formation, we extract scores from the alignments as
follows: for each word in the original narrative, if
[A anna1] [B thompson2] [C of3 south4]
[D boston5] [E employed6] [F as7 a8
cook9] [G in10 a11 school12] [H cafeteria13]
[I reported14] [J at15 the16 police17] [K
station18] [L that19 she20 had21 been22 held23
up24] [M on25 state26 street27] [N the28
night29 before30] [O and31 robbed32 of33] [P
fifty-six34 dollars35] [Q she36 had37 four38]
[R small39 children40] [S the41 rent42 was43
due44] [T and45 they46 had47 n?t48 eaten49]
[U for50 two51 days52] [V the53 police54] [W
touched55 by56 the57 woman?s58 story59] [X
took60 up61 a62 collection63] [Y for64 her65]
Figure 7: Text of Wechsler Logical Memory narrative
with story-element labeled bracketing and word IDs.
anna(1) : A
thompson(2) : B
employed(6) : E
boston(5) : D
cook(9) : F
robbed(32) : O
fifty-six(34) : P
four(38) : Q
children(40) : R
reported(14) : I
station(18) : K
took(60) : X
collection(63) : X
for(64) : Y
her(65) : Y
Figure 8: Source content words from the alignment in
Figure 6 with corresponding story element IDs.
that word is aligned to a word in the retelling, the
story element that it is associated with is considered
to be recalled. Figure 8 shows the story elements
extracted from the word alignments in Figure 6.
When we convert alignments to scores in this way,
any alignment can be mapped to an element, even an
alignment between function words such as the and
of, which would be unlikely to indicate that the story
element had been recalled. To avoid such scoring er-
rors, we disregard any word-alignment pair contain-
ing a source function word. The two exceptions to
this rule are the final two words, for her, which are
not content words but together make a single story
element.
The element-level scores induced from the four
word alignments for all 235 experimental sub-
jects were evaluated against the manual per-element
scores. We report the precision, recall, and f-
measure for all four alignment models in Table 3. In
addition, report Cohen?s kappa as a measure of reli-
ability between our automated scores and the man-
ually assigned scores. We see that as AER im-
proves, scoring accuracy also improves, with the
large graph-based model outperforming all other
models in terms of precision, f-measure, and inter-
6
ann(1) : anna(1)
worked(3) : employed(6)
in(4) : in(10)
boston(5) : boston(5)
as(6) : as(7)
a(7) : a(8)
cook(8) : cook(9)
and(9) : and(31)
robbed(12) : robbed(32)
of(13) : of(33)
dollars(15) : dollars(35)
is(16) : was(43)
that(17) : that(19)
and(19) : and(45)
she(20) : she(36)
had(21) : had(37)
four(22) : four(38)
children(23) : children(40)
reported(25) : reported(14)
at(26) : at(15)
the(27) : the(16)
some(28) : police(17)
station(31) : station(18)
made(37) : up(61)
made(37) : took(60)
a(38) : a(62)
collection(39) : collection(63)
for(40) : for(64)
her(41) : her(65)
so(42) : woman?s(58)
she(44) : she(20)
Figure 5: Word alignment generated by the small Berkeley alignment model with retelling words italicized.
ann(1) : anna(1)
taylor(2) : thompson(2)
worked(3) : employed(6)
in(4) : in(10)
boston(5) : boston(5)
as(6) : as(7)
a(7) : a(8)
cook(8) : cook(9)
robbed(12) : robbed(32)
of(13) : of(33)
sixty-seven(14) : fifty-six(34)
dollars(15) : dollars(35)
she(20) : she(36)
had(21) : had(37)
four(22) : four(38)
children(23) : children(40)
reported(25) : reported(14)
at(26) : at(15)
the(27) : the(16)
station(31) : station(18)
made(37) : took(60)
a(38) : a(62)
collection(39) : collection(63)
for(40) : for(64)
her(41) : her(65)
Figure 6: Word alignment generated by the large graph-based model with retelling words italicized.
Model P R F ?
Berkeley-Small 87.2 88.9 88.0 76.1
Berkeley-Large 86.8 90.7 88.7 77.1
Graph-Small 84.7 93.6 88.9 76.9
Graph-Big 88.8 89.3 89.1 78.3
Table 3: Scoring accuracy results.
rater reliability. The scoring accuracy levels re-
ported here are comparable to the levels of inter-rater
agreement typically reported for the WLM, and re-
liability between our automated scores and the man-
ual scores, as measured by Cohen?s kappa, is well
within the ranges reported in the literature (Johnson
et al, 2003). As will be shown in the following sec-
tion, scoring accuracy is very important for achiev-
ing high diagnostic classification accuracy, which is
the ultimate goal of this work.
6 Diagnostic Classification
As discussed in Section 2, poor performance on the
Wechsler Logical Memory test is associated with
Mild Cognitive Impairment. We now use the scores
we have extracted from the word alignments as fea-
tures with a support vector machine (SVM) to per-
form diagnostic classification for distinguishing sub-
jects with MCI from those without. For each of the
235 experimental subjects, we generate 2 summary
scores: one for the immediate retelling and one for
the delayed retelling. The summary score ranges
from 0, indicating that no elements were recalled,
to 25, indicating that all elements were recalled. In
addition to the summary score, we also provide the
SVM with a vector of 50 per-element scores: for
each of the 25 element in each of the two retellings
per subject, there is a vector element with the value
of 0 if the element was not recalled, or 1 if the el-
ement was recalled. Since previous work has indi-
cated that certain elements may be more powerful in
their ability to predict the presence of MCI, we ex-
pect that giving the SVM these per-elements scores
may improve classification performance. To train
and test our classifiers, we use the WEKA API (Hall
et al, 2009) and LibSVM (Chang and Lin, 2011),
with a second-order polynomial kernel and default
parameter settings.
We evaluate the performance of the SVMs us-
ing a leave-pair-out validation scheme (Cortes et al,
2007; Pahikkala et al, 2008). In the leave-pair-out
technique, every pairing between a negative exam-
ple and a positive example is tested using a classi-
fier trained on all of the remaining examples. The
resulting pairs of scores can be used to calculate
the area under the receiver operating characteristic
(ROC) curve (Egan, 1975), which is a plot of the
false positive rate of a classifier against its true pos-
itive rate. The area under this curve (AUC) has a
7
Model Summ. (s.d.) Elem. (s.d.)
Manual Scores 73.3 (3.76) 81.3 (3.32)
Berkeley-Small 73.7 (3.74) 77.9 (3.52)
Berkeley-Big 75.1 (3.67) 79.2 (3.45)
Graph-Small 74.2 (3.71) 78.9 (3.47)
Graph-Big 74.8 (3.69) 78.6 (3.49)
Table 4: Classification accuracy results (AUC).
value of 0.5 when the classifier performs at chance
and a value 1.0 when perfect classification accuracy
is achieved.
Table 4 shows the classification results for the
scores derived from the four alignment models along
with the classification results using the examiner-
assigned manual scores. It appears that, in all cases,
the per-element scores are more effective than the
summary scores in classifying the two diagnostic
groups. In addition, we see that our automated
scores have classificatory power comparable to that
of the manual gold scores, and that as scoring ac-
curacy increases from the small Berkeley model to
the graph-based models and bigger models, classifi-
cation accuracy improves. This suggests both that
accurate scores are crucial for accurate classifica-
tion and that pursuing even further improvements in
word alignment is likely to result in improved di-
agnostic differentiation. We note that although the
large Berkeley model achieved the highest classi-
fication accuracy, this very slight margin of differ-
ence may not justify its significantly greater compu-
tational requirements.
7 Conclusions and Future Work
The work presented here demonstrates the utility
of adapting techniques drawn from a diverse set of
NLP research areas to tasks in biomedicine. In par-
ticular, the approach we describe for automatically
analyzing clinically elicited language data shows
promise as part of a pipeline for a screening tool for
Mild Cognitive Impairment. Our novel graph-based
approach to word alignment resulted in large reduc-
tions in alignment error rate. These reductions in er-
ror rate in turn led to human-level scoring accuracy
and improved diagnostic classification.
As we have mentioned, the methods outlined here
are general enough to be used for other episodic
recall and description scenarios. Although the re-
sults are quite robust, several enhancements and im-
provements should be made before we apply the sys-
tem to other tasks. First, although we were able to
achieve decent word alignment accuracy, especially
with our graph-based approach, many alignment er-
rors remain. As shown in Figure 4, the graph-based
alignment technique could potentially result in an
AER of as low as 11%. We expect that our deci-
sion to select as a new alignment the most frequent
source word over the distribution of source words at
the end of 1000 walks could be improved, since it
does not allow for one-to-many mappings. In addi-
tion, it would be worthwhile to experiment with sev-
eral posterior thresholds, both during the decoding
step of the Berkeley aligner and in the graph edges.
In order to produce a viable clinical screening
tool, it is crucial that we incorporate speech recogni-
tion in the pipeline. Our very preliminary investiga-
tion into using ASR to generate transcripts for align-
ment seems promising and surprisingly robust to the
problems that might be expected when working with
noisy audio. In our future work, we also plan to ex-
amine longitudinal data for individual subjects to see
whether our techniques can detect subtle differences
in recall and coherence between a recent retelling
and a series of earlier baseline retellings. Since the
metric commonly used to quantify the progression
of dementia, the Clinical Dementia Rating, relies on
observed changes in cognitive function over time,
longitudinal analysis of performance on the Wech-
sler Logical Memory task may be the most promis-
ing application for our research.
References
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceeding of ACL.
D.A. Bennett, J.A. Schneider, Z. Arvanitakis, J.F. Kelly,
N.T. Aggarwal, R.C. Shah, and R.S. Wilson. 2006.
Neuropathology of older persons without cognitive
impairment from two community-based studies. Neu-
rology, 66:1837?844.
Nicola Botting. 2002. Narrative as a tool for the assess-
ment of linguistic and pragmatic impairments. Child
Language Teaching and Therapy, 18(1).
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
8
tical machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(27):1?
27.
Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.
2007. An alternative ranking problem for search en-
gines. In Proceedings of WEA2007, LNCS 4525,
pages 1?21. Springer-Verlag.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
Kristy Dodwell and Edith L. Bavin. 2008. Children
with specific language impair ment: an investigation of
their narratives and memory. International Journal of
Language and Communication Disorders, 43(2):201?
218.
John C. Dunn, Osvaldo P. Almeida, Lee Barclay, Anna
Waterreus, and Leon Flicker. 2002. Latent seman-
tic analysis: A new method to measure prose recall.
Journal of Clinical and Experimental Neuropsychol-
ogy, 24(1):26?35.
James Egan. 1975. Signal Detection Theory and ROC
Analysis. Academic Press.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
M. Folstein, S. Folstein, and P. McHugh. 1975. Mini-
mental state - a practical method for grading the cog-
nitive state of patients for the clinician. Journal of Psy-
chiatric Research, 12:189?198.
Keyur Gabani, Melissa Sherman, Thamar Solorio, and
Yang Liu. 2009. A corpus-based approach for the
prediction of language impairment in monolingual En-
glish and Spanish-English bilingual children. In Pro-
ceedings of NAACL-HLT, pages 46?55.
Cheryl Glasgow and Judy Cowley. 1994. Renfrew
Bus Story test - North American Edition. Centreville
School.
H Goodglass, E Kaplan, and B Barresi. 2001. Boston
Diagnostic Aphasia Examination. 3rd ed. Pro-Ed.
Dilek Hakkani-Tur, Dimitra Vergyri, and Gokhan Tur.
2010. Speech-based automated cognitive status as-
sessment. In Proceedings of Interspeech, pages 258?
261.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
David K. Johnson, Martha Storandt, and David A. Balota.
2003. Discourse analysis of logical memory recall in
normal aging and in dementia of the alzheimer type.
Neuropsychology, 17(1):82?92.
R.J. Kiernan, J. Mueller, J.W. Langston, and C. Van
Dyke. 1987. The neurobehavioral cognitive sta-
tus examination, a brief but differentiated approach to
cognitive assessment. Annals of Internal Medicine,
107:481?485.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT NAACL.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services.
John Morris. 1993. The clinical dementia rating
(CDR): Current version and scoring rules. Neurology,
43:2412?2414.
A Nordlund, S Rolstad, P Hellstrom, M Sjogren,
S Hansen, and A Wallin. 2005. The goteborg mci
study: mild cognitive impairment is a heterogeneous
condition. Journal of Neurology, Neurosurgery and
Psychiatry, 76:1485?1490.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, , and Hermann
Ney. 2000. Improved alignment models for statisti-
cal machine translation. In Proceedings of ACL, pages
440?447.
Jahna Otterbacher, Gu?nes Erkan, and Dragomir R. Radev.
2009. Biased lexrank: Passage retrieval using random
walks with question-based priors. Inf. Process. Man-
age., 45(1):42?54.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number =
SIDL-WP-1999-0120.
Tapio Pahikkala, Antti Airola, Jorma Boberg, and Tapio
Salakoski. 2008. Exact and efficient leave-pair-out
cross-validation for ranking RLS. In Proceedings of
AKRR 2008, pages 1?8.
Ronald Peterson, Glenn Smith, Stephen Waring, Robert
Ivnik, Eric Tangalos, and Emre Kokmen. 1999. Mild
cognitive impairment: Clinical characterizations and
outcomes. Archives of Neurology, 56:303?308.
Emily T. Prud?hommeaux and Brian Roark. 2011a.
Alignment of spoken narratives for automated neu-
ropsychological assessment. In Proceedings of ASRU.
Emily T. Prud?hommeaux and Brian Roark. 2011b. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Proceedings of Interspeech.
Karen Ritchie and Jacques Touchon. 2000. Mild cogni-
tive impairment: Conceptual basis and current noso-
logical status. Lancet, 355:225?228.
9
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for de-
tecting mild cognitive impairment. In Proceedings of
the ACL 2007 Workshop on Biomedical Natural Lan-
guage Processing (BioNLP), pages 1?8.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristina Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Audio,
Speech and Language Processing, 19(7):2081?2090.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of ACL, pages 197?
204.
Magnus Sahlgren and Jussi Karlgren. 2005. Automatic
bilingual lexicon acquisition using random indexing
of parallel corpora. Natural Language Engineering,
11(3).
F.A. Schmitt, D.G. Davis, D.R. Wekstein, C.D. Smith,
J.W. Ashford, and W.R. Markesbery. 2000. Preclini-
cal ad revisited: Neuropathology of cognitively normal
older adults. Neurology, 55:370?376.
William R. Shankle, A. Kimball Romney, Junko Hara,
Dennis Fortier, Malcolm B. Dick, James M. Chen,
Timothy Chan, and Xijiang Sun. 2005. Methods
to improve the detection of mild cognitive impair-
ment. Proceedings of the National Academy of Sci-
ences, 102(13):4919?4924.
Martha Storandt and Robert Hill. 1989. Very mild senile
dementia of the alzheimers type: Ii psychometric test
performance. Archives of Neurology, 46:383?386.
Helen Tager-Flusberg. 1995. Once upon a ribbit: Stories
narrated by autistic children. British journal of devel-
opmental psychology, 13(1):45?59.
David Wechsler. 1997. Wechsler Memory Scale - Third
Edition Manual. The Psychological Corporation.
10
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 46?50,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Detecting linguistic idiosyncratic interests in autism
using distributional semantic models
Masoud Rouhizadeh
?
, Emily Prud?hommeaux
?
, Jan van Santen
?
, Richard Sproat
?
?
Center for Spoken Language Understanding, Oregon Health & Science University
?
Center for Language Sciences, University of Rochester
?
Google, Inc.
{rouhizad,vansantj}@ohsu.edu, emilypx@gmail.com, rws@xoba.com
Abstract
Children with autism spectrum disorder
often exhibit idiosyncratic patterns of be-
haviors and interests. In this paper, we fo-
cus on measuring the presence of idiosyn-
cratic interests at the linguistic level in
children with autism using distributional
semantic models. We model the semantic
space of children?s narratives by calculat-
ing pairwise word overlap, and we com-
pare the overlap found within and across
diagnostic groups. We find that the words
used by children with typical development
tend to be used by other children with typ-
ical development, while the words used
by children with autism overlap less with
those used by children with typical devel-
opment and even less with those used by
other children with autism. These findings
suggest that children with autism are veer-
ing not only away from the topic of the
target narrative but also in idiosyncratic
semantic directions potentially defined by
their individual topics of interest.
1 Introduction
Autism spectrum disorder (ASD) is a neurode-
velopmental disorder characterized by impaired
communication and social behavior. One of the
core deficits associated with ASD is an intense
preoccupation with a restricted set of interests
(American Psychiatric Association, 2000; Amer-
ican Psychiatric Association, 2013), which can of-
ten be observed in an individual?s tendency to per-
severate on specific, idiosyncratic topics of con-
versation. Because this symptom is explicitly
mentioned among the diagnostic criteria for ASD
used in the DSM-IV and DSM-5, many diagnos-
tic instruments (Lord et al., 2002; Rutter et al.,
2003) require a qualitative assessment of this phe-
nomenon. Instances of perseveration on a partic-
ular topic in the spontaneous spoken language of
children with ASD, however, are not typically ex-
plicitly counted in a clinical setting, making com-
parisons with typically developing children diffi-
cult to quantify.
Expert manual analysis of conversations and
narratives of individuals with ASD has shown that
children and teenagers with autism include signif-
icantly more bizarre and irrelevant content in their
narratives (Loveland et al., 1990; Losh and Capps,
2003) and introduce more abrupt topic changes in
their conversations (Lam et al., 2012) than their
typically developing peers. Automatic detection
of poor topic maintenance has also been explored
using techniques originally developed for infor-
mation extraction (Rouhizadeh et al., 2013). There
has been little work, however, in annotating the
precise direction of the departure from a target
topic. Thus, it is not clear whether children with
ASD are instigating similar topic changes or pur-
suing idiosyncratic directions in their narratives
and conversations consistent with their restricted
interests.
In this paper, we attempt to automatically iden-
tify topic changes and idiosyncratic interests ex-
pressed in the language of children with ASD
by measuring the semantic similarity of narrative
retellings produced by children with and without
ASD. We first use word overlap measures to cal-
culate the semantic similarity between every pos-
sible pair of narratives. We then build three pair-
wise comparison matrices: one comparing pairs of
typically developing (TD) children; one compar-
ing pairs of children with ASD; and a third com-
46
paring pairs consisting of one child with ASD and
one child with TD. We calculate the significance
of the differences between the pairs in the three
matrices using the Monte Carlo method to shuffle
the diagnosis label of each child.
We find that TD children share the greatest
word overlap with one another, while children
with ASD have significantly less word overlap
with TD children and even less word overlap with
other ASD children. These results indicate that
TD children tend to adhere to the target topic in
the narrative retellings, while children with ASD
often stray from the target topic. Furthermore,
the fact that the word choices of an individual
child with ASD seem not to resemble the word
choices of other children with ASD suggests that
when a child with ASD chooses to abandon the
target topic, he or she does so in an idiosyncratic
way. Although these results are only indirect in-
dications of the presence of restricted interests,
the work presented here highlights the potential of
computational language analysis methods for im-
proving our understanding of the social and lin-
guistic deficits associated with the disorder.
2 Data
Participants in this study included 39 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was di-
agnosed via clinical consensus according to the
DSM-IV-TR criteria (American Psychiatric Asso-
ciation, 2000) and the established threshold scores
on two diagnostic instruments: the Autism Di-
agnostic Observation Schedule (ADOS) (Lord et
al., 2002), a semi-structured series of activities de-
signed to allow an examiner to observe behaviors
associated with autism; and the Social Communi-
cation Questionnaire (SCQ) (Rutter et al., 2003),
a parental questionnaire. None of the children
in this study met the criteria for a language im-
pairment, and there were no significant between-
group differences in age (mean=6.3) or full-scale
IQ (mean=115.5).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Kork-
man et al., 1998), a large and comprehensive bat-
tery of tasks that test neurocognitive functioning in
children. The NEPSY Narrative Memory (NNM)
subtest is a narrative retelling test in which the sub-
ject listens to a brief narrative about a boy and his
dog and then must retell the narrative to the ex-
aminer. Under standard administration, the NNM
free recall score is calculated by counting how
many from a set of 17 story elements were used
in a retelling. Following the free recall portion of
the test is the cued recall task, in which the ex-
aminer then asks the subject to provide answers to
questions about all of the story elements that were
omitted in the retelling.
The NNM was administered to each participant
in the study, and each participant?s retelling was
recorded and transcribed. The responses for the
cued recall portion of the subtest were not in-
cluded in this work presented here. There was no
significant difference between the two diagnostic
groups in the standard NNM free recall score.
3 Methods
We expect that two different retellings of the same
source will lie in the same lexico-semantic space.
As a result, they should include high percentage
of overlapping words. When a pair of retellings
has a low word overlap measure, it could be that
one or both retellings include intrusions from un-
related topics. An alternative explanation is that
the subjects recalled a non-overlapping set of story
elements or simply a small set of story elements.
However, since we did not find any significant dif-
ference between the TD and ASD groups in the
standard narrative recall score, we infer that a low
percentage of word overlap indicates a difference
in topic between the two retellings.
3.1 Word overlap measures
In order to calculate the similarity between a pair
of narratives i and j, we use type and token over-
lap measures based on the Jaccard similarity coef-
ficient. Token similarity is defined as the size of
intersection of the words (i.e., the actual number
of tokens in common) in narratives i and j relative
to the size of the union of the words in the two
narratives (i.e., summing over all tokens in both
narratives, the maximum number of instances of
that token in either narrative). Type similarity is
defined as the size of intersection of the types (i.e.,
unique words) in narratives i and j relative to the
size of the union of the types in the two narratives.
For instance, for the following set of words i and
j:
i = {a, b, c, d, c}
j = {a, c, e, c, a, a},
the token intersection is equal to {a, c, c} and
47
Group Means
TD.TD TD.ASD ASD.ASD
Type Overlap .23 .17 .13
Token Overlap .19 .14 .11
Table 3: Word overlap pairwise group means
the token union is {a, a, a, c, c, b, e, d}. The token
overlap similarity between the two sets i and j is
therefore 3/8. The type intersection of i and j is
equal to {a, c} and the type union is {a, c, b, e, d},
yielding a type overlap similarity of 2/5.
3.2 Pairwise similarity matrix
We next build a similarity matrix for the type and
token overlap measures, comparing every possi-
ble pair of children. Every child in the TD and
ASD groups is compared to the children in his own
group (TD.TD and ASD.ASD), as well as the chil-
dren in the other group (TD.ASD). The pairwise
similarity matrix is diagonally symmetrical, and
we thus consider only the top right section of the
matrix above the diagonal in our analysis.
3.3 Monte Carlo permutation
Since we may not have enough information to
make an assumption that the pairwise similarity
measures of all children are from a particular dis-
tribution, we utilize a non-parametric procedure,
the Monte Carlo permutation approach, which is
widely used in non-standard significance testing
situations.
Given the three sub-matrices in the similarity
matrix described above (TD.TD, TD.ASD, and
ASD.ASD), we first calculate for each pair of sub-
matrices (e.g., TD.TD vs ASD.ASD) three statis-
tics that compare all cells in one submatrix with
the cells in other submatrices: the difference be-
tween the means, t-statistics (using the Welch
Two Sample t-test), and w-statistics (using the
Wilcoxon rank sum test). We label these observed
values observed-mean, observed-t, and observed-
w. We next take a large random sample with re-
placement from all possible permutations of the
data by shuffling the diagnosis labels of the chil-
dren 1000 times, and then calculate each of the
three above statistics for each shuffle. Finally, we
determine the number of times the observed values
exceed the values generated by the 1000 shuffles.
4 Results
The comparison of the group means of each of
the three sub-matrices described in Section 3.2
show that TD children have the greatest overlap
with each other; children with ASD have less
word overlap with TD children than TD children
have with one another and even less word over-
lap with other ASD children. The group means
of both type and token overlap are summarized
in Table 3. In addition, examples of overlapping
and non-overlapping terms between the groups are
provided in Tables 1 and 2 respectively.
The level plot of the pairwise token overlap
is shown in figure 1. We see that the TD.TD
sub-matrix has the lightest color, indicating higher
overlap, followed by TD.ASD. The ASD.ASD
submatrix has the darkest color, indicating low
word overlap.
In the next step, we determine the significance
of the group mean differences. As described in
Section 3.3, using the Monte Carlo permutation to
test the significance of the following comparisons:
TD.TD vs ASD.ASD, TD.TD vs TD.ASD, and
TD.ASD vs ASD.ASD. The results of these signif-
Group Top 10 overlapping words
TD.TD shoe, tree, climb, ladder, fall, Pepper, Jim, dog, sister, branch
TD.ASD shoe, tree, Jim, climb, dog, ladder, Pepper, fall, branch, sister
ASD.ASD shoe, tree, Jim, dog, climb, Pepper, ladder, branch, boy, run
Table 1: Top 10 overlapping words between the groups
Group Examples of non-overlapping words
TD.TD coconut, couch, jew, lie, picture, spike, stuff, t-rex, tight, watch
TD.ASD arm, bottom, cousin, doctor, eat, fruit, giant, meat, push, sense
ASD.ASD bite, bridge, crunch, donut, gadget, lizard, microphone, sell, table, vision
Table 2: Examples of non-overlapping words between the groups
48
??
??
??
Figure 1: Level plot of the pairwise token overlap
(lighter colors indicate higher overlap)
icance tests are summarized in table 4, and in all
cases the differences are significant at p < 0.05.
5 Conclusions and future work
The methods presented for comparing the lexical
choices made by children with and without ASD
while generating a narrative retelling demonstrate
the utility of language analysis for revealing diag-
nostically interesting information. The low rates
of word overlap between retellings produced by
children with ASD and those produced by typi-
cally developing children suggest that the children
with ASD are having difficulty maintaining the
target topic. Furthermore, the low overlap between
pairs of children with ASD suggests that children
with ASD are not straying from the topic in sim-
ilar ways but are instead exploring topics that are
of idiosyncratic interest.
These findings can be potentially used for
diagnostic purposes in combinations of other
applications of speech and language process-
ing for automated narrative retelling assessment
(Lehr et al., 2013), detection of off-topic words
(Rouhizadeh et al., 2013), and pragmatic deficits
(Prud?hommeaux and Rouhizadeh, 2012). From a
clinical standpoint, diagnostic measures utilizing
these methods for automated evaluation of disor-
dered language could be very useful in diagnosis
and planning interventions.
One major focus of our future work will be to
manually annotate the narrative retellings used in
this study to determine the frequency of topic de-
partures and the nature of these departures. Given
the vocabulary differences seen here, we expect
to find not only that children with ASD are aban-
doning the topic of the source narrative more fre-
quently than children with typical development
but also that the topics they choose to pursue are
related to their own individual specific interests.
A second area we hope to explore is the use
of external resources, such as WordNet, to ex-
pand the set of terms used to calculate word over-
lap. It is perfectly reasonable to expect that people
will use synonyms and paraphrases in their narra-
tive retellings. It is therefore possible that chil-
dren with autism are discussing the appropriate
topic but choosing unusual words within that topic
space in their retellings, which could be consis-
tent with the type of atypical language often ob-
served in children with ASD. By considering se-
mantic overlap rather than simple word overlap,
we may be able to distinguish instances of atypical
language from true examples of poor topic main-
tenance.
Third, we are also interested in applying the
analysis described above to a set of retellings from
seniors with and without mild cognitive impair-
ment, a frequent precursor to dementia. Like chil-
dren with ASD, seniors with dementia are also
more likely to include irrelevant information in
overlap statistic
p-values
TD.TD vs ASD.ASD TD.TD vs TD.ASD TD.ASD vs ASD.ASD
Type Overlap
Means .004 .042 .008
t.test .009 .012 .008
Wilcoxon test .004 .002 .002
Token Overlap
Means .012 .034 .028
t.test .014 .022 .022
Wilcoxon test .012 .002 .002
Table 4: Monte Carlo significance test results
49
their narrative retellings. These intrusions, how-
ever, are often informed by real-world knowledge,
and thus may not result in a decrease in measures
of word overlap with narratives produced by unim-
paired individuals.
Finally, we plan to apply our methods to the out-
put of an automatic speech recognition (ASR) sys-
tem rather than manual transcripts. Although the
ASR output is likely to contain many errors, the
fact that our methods focus on content words may
make them robust to the sorts of function word
recognition errors typically produced by ASR sys-
tems.
Acknowledgments
This work was supported in part by NSF grant
#BCS-0826654, and NIH NIDCD grants #R01-
DC007129 and #1R01DC012033-01. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of
the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washing-
ton, DC.
American Psychiatric Association. 2013. Diagnostic
and statistical manual of mental disorders (5th ed.).
American Psychiatric Publishing, Washington, DC.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological as-
sessment. The Psychological Corporation, San An-
tonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic lan-
guage deficits in children with high-functioning
autism: Depicting the phenotype using the prag-
matic rating scale. Research in Autism Spectrum
Disorders, 6(2):792?797.
Maider Lehr, Izhak Shafran, Emily Prud?hommeaux,
and Brian Roark. 2013. Discriminative joint model-
ing of lexical variation and acoustic confusion for
automated narrative retelling assessment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability in
high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental
Disorders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tu-
nali. 1990. Narrative story telling in autism and
down?s syndrome. British Journal of Developmen-
tal Psychology, 8(1):9?23.
Emily Prud?hommeaux and Masoud Rouhizadeh.
2012. Automatic detection of pragmatic deficits
in children with autism. In Proceedings of the
3rd Workshop on Child, Computer and Interaction
(WOCCI).
Masoud Rouhizadeh, Emily Prud?hommeaux, Brian
Roark, and Jan van Santen. 2013. Distributional
semantic models for the evaluation of disordered
language. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
50
