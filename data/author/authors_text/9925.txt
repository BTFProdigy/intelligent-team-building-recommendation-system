Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219?228,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Feature-Rich Translation by Quasi-Synchronous Lattice Parsing
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We present a machine translation frame-
work that can incorporate arbitrary fea-
tures of both input and output sentences.
The core of the approach is a novel de-
coder based on lattice parsing with quasi-
synchronous grammar (Smith and Eis-
ner, 2006), a syntactic formalism that
does not require source and target trees
to be isomorphic. Using generic approx-
imate dynamic programming techniques,
this decoder can handle ?non-local? fea-
tures. Similar approximate inference tech-
niques support efficient parameter esti-
mation with hidden variables. We use
the decoder to conduct controlled exper-
iments on a German-to-English transla-
tion task, to compare lexical phrase, syn-
tax, and combined models, and to mea-
sure effects of various restrictions on non-
isomorphism.
1 Introduction
We have seen rapid recent progress in machine
translation through the use of rich features and the
development of improved decoding algorithms,
often based on grammatical formalisms.
1
If we
view MT as a machine learning problem, features
and formalisms imply structural independence as-
sumptions, which are in turn exploited by efficient
inference algorithms, including decoders (Koehn
et al, 2003; Yamada and Knight, 2001). Hence a
tension is visible in the many recent research ef-
forts aiming to decode with ?non-local? features
(Chiang, 2007; Huang and Chiang, 2007).
Lopez (2009) recently argued for a separation
between features/formalisms (and the indepen-
1
Informally, features are ?parts? of a parallel sentence pair
and/or their mutual derivation structure (trees, alignments,
etc.). Features are often implied by a choice of formalism.
dence assumptions they imply) from inference al-
gorithms in MT; this separation is widely appreci-
ated in machine learning. Here we take first steps
toward such a ?universal? decoder, making the fol-
lowing contributions:
Arbitrary feature model (?2): We define a sin-
gle, direct log-linear translation model (Papineni
et al, 1997; Och and Ney, 2002) that encodes most
popular MT features and can be used to encode
any features on source and target sentences, de-
pendency trees, and alignments. The trees are op-
tional and can be easily removed, allowing sim-
ulation of ?string-to-tree,? ?tree-to-string,? ?tree-
to-tree,? and ?phrase-based? models, among many
others. We follow the widespread use of log-linear
modeling for direct translation modeling; the nov-
elty is in the use of richer feature sets than have
been previously used in a single model.
Decoding as QG parsing (?3?4): We present a
novel decoder based on lattice parsing with quasi-
synchronous grammar (QG; Smith and Eisner,
2006).
2
Further, we exploit generic approximate
inference techniques to incorporate arbitrary ?non-
local? features in the dynamic programming algo-
rithm (Chiang, 2007; Gimpel and Smith, 2009).
Parameter estimation (?5): We exploit simi-
lar approximate inference methods in regularized
pseudolikelihood estimation (Besag, 1975) with
hidden variables to discriminatively and efficiently
train our model. Because we start with inference
(the key subroutine in training), many other learn-
ing algorithms are possible.
Experimental platform (?6): The flexibility
of our model/decoder permits carefully controlled
experiments. We compare lexical phrase and de-
pendency syntax features, as well as a novel com-
2
To date, QG has been used for word alignment (Smith
and Eisner, 2006), adaptation and projection in parsing
(Smith and Eisner, 2009), and various monolingual recog-
nition and scoring tasks (Wang et al, 2007; Das and Smith,
2009); this paper represents its first application to MT.
219
?, T source and target language vocabularies, respectively
Trans : ? ? {NULL} ? 2
T
function mapping each source word to target words to which it may translate
s = ?s
0
, . . . , s
n
? ? ?
n
source language sentence (s
0
is the NULL word)
t = ?t
1
, . . . , t
m
? ? T
m
target language sentence, translation of s
?
s
: {1, . . . , n} ? {0, . . . , n} dependency tree of s, where ?
s
(i) is the index of the parent of s
i
(0 is the root, $)
?
t
: {1, . . . ,m} ? {0, . . . ,m} dependency tree of t, where ?
t
(i) is the index of the parent of t
i
(0 is the root, $)
a : {1, . . . ,m} ? 2
{1,...,n}
alignments from words in t to words in s; ? denotes alignment to NULL
? parameters of the model
g
trans
(s,a, t) lexical translation features (?2.1):
f
lex
(s, t) word-to-word translation features for translating s as t
f
phr
(s
j
i
, t
`
k
) phrase-to-phrase translation features for translating s
j
i
as t
`
k
g
lm
(t) language model features (?2.2):
f
N
(t
j
j?N+1
) N -gram probabilities
g
syn
(t, ?
t
) target syntactic features (?2.3):
f
att
(t, j, t
?
, k) syntactic features for attaching target word t
?
at position k to target word t at position j
f
val
(t, j, I) syntactic valence features with word t at position j having children I ? {1, . . . ,m}
g
reor
(s, ?
s
,a, t, ?
t
) reordering features (?2.4):
f
dist
(i, j) distortion features for a source word at position i aligned to a target word at position j
g
tree
2
(?
s
,a, ?
t
) tree-to-tree syntactic features (?3):
f
qg
(i, i
?
, j, k) configuration features for source pair s
i
/s
i
?
being aligned to target pair t
j
/t
k
g
cov
(a) coverage features (?4.2)
f
scov
(a), f
zth
(a), f
sunc
(a) counters for ?covering? each s word each time, the zth time, and leaving it ?uncovered?
Table 1: Key notation. Feature factorings are elaborated in Tab. 2.
bination of the two. We quantify the effects
of our approximate inference. We explore the
effects of various ways of restricting syntactic
non-isomorphism between source and target trees
through the QG. We do not report state-of-the-art
performance, but these experiments reveal inter-
esting trends that will inform continued research.
2 Model
(Table 1 explains notation.) Given a sentence s
and its parse tree ?
s
, we formulate the translation
problem as finding the target sentence t
?
(along
with its parse tree ?
?
t
and alignment a
?
to the
source tree) such that
3
?t
?
, ?
?
t
,a
?
? = argmax
?t,?
t
,a?
p(t, ?
t
,a | s, ?
s
) (1)
In order to include overlapping features and permit
hidden variables during training, we use a single
globally-normalized conditional log-linear model.
That is, p(t, ?
t
,a | s, ?
s
) =
exp{?
>
g(s, ?
s
,a, t, ?
t
)}
?
a
?
,t
?
,?
?
t
exp{?
>
g(s, ?
s
,a
?
, t
?
, ?
?
t
)}
(2)
where the g are arbitrary feature functions and the
? are feature weights. If one or both parse trees or
the word alignments are unavailable, they can be
ignored or marginalized out as hidden variables.
In a log-linear model over structured objects,
the choice of feature functions g has a huge effect
3
We assume in this work that s is parsed. In principle, we
might include source-side parsing as part of decoding.
on the feasibility of inference, including decoding.
Typically these feature functions are chosen to fac-
tor into local parts of the overall structure. We
next define some key features used in current MT
systems, explaining how they factor. We will use
subscripts on g to denote different groups of fea-
tures, which may depend on subsets of the struc-
tures t, ?
t
, a, s, and ?
s
. When these features fac-
tor into parts, we will use f to denote the factored
vectors, so that if x is an object that breaks into
parts {x
i
}
i
, then g(x) =
?
i
f(x
i
).
4
2.1 Lexical Translations
Classical lexical translation features depend on s
and t and the alignment a between them. The sim-
plest are word-to-word features, estimated as the
conditional probabilities p(t | s) and p(s | t) for
s ? ? and t ? T. Phrase-to-phrase features gen-
eralize these, estimated as p(t
?
| s
?
) and p(s
?
| t
?
)
where s
?
(respectively, t
?
) is a substring of s (t).
A major difference between the phrase features
used in this work and those used elsewhere is
that we do not assume that phrases segment into
disjoint parts of the source and target sentences
4
There are two conventional definitions of feature func-
tions. One is to let the range of these functions be conditional
probability estimates (Och and Ney, 2002). These estimates
are usually heuristic and inconsistent (Koehn et al, 2003).
An alternative is to instantiate features for different structural
patterns (Liang et al, 2006; Blunsom et al, 2008). This offers
more expressive power but may require much more training
data to avoid overfitting. For this reason, and to keep training
fast, we opt for the former convention, though our decoder
can handle both, and the factorings we describe are agnostic
about this choice.
220
(Koehn et al, 2003); they can overlap.
5
Addi-
tionally, since phrase features can be any func-
tion of words and alignments, we permit features
that consider phrase pairs in which a target word
outside the target phrase aligns to a source word
inside the source phrase, as well as phrase pairs
with gaps (Chiang, 2005; Ittycheriah and Roukos,
2007).
Lexical translation features factor as in Eq. 3
(Tab. 2). We score all phrase pairs in a sentence
pair that pair a target phrase with the smallest
source phrase that contains all of the alignments in
the target phrase; if
?
k:i?k?j
a(k) = ?, no phrase
feature fires for t
j
i
.
2.2 N -gram Language Model
N -gram language models have become standard
in machine translation systems. For bigrams and
trigrams (used in this paper), the factoring is in
Eq. 4 (Tab. 2).
2.3 Target Syntax
There have been many features proposed that con-
sider source- and target-language syntax during
translation. Syntax-based MT systems often use
features on grammar rules, frequently maximum
likelihood estimates of conditional probabilities in
a probabilistic grammar, but other syntactic fea-
tures are possible. For example, Quirk et al
(2005) use features involving phrases and source-
side dependency trees and Mi et al (2008) use
features from a forest of parses of the source sen-
tence. There is also substantial work in the use
of target-side syntax (Galley et al, 2006; Marcu
et al, 2006; Shen et al, 2008). In addition, re-
searchers have recently added syntactic features to
phrase-based and hierarchical phrase-based mod-
els (Gimpel and Smith, 2008; Haque et al, 2009;
Chiang et al, 2008).
In this work, we focus on syntactic features of
target-side dependency trees, ?
t
, along with the
words t. These include attachment features that
relate a word to its syntactic parent, and valence
features. They factor as in Eq. 5 (Tab. 2). Features
that consider only target-side syntax and words
without considering s can be seen as ?syntactic
language model? features (Shen et al, 2008).
5
Segmentation might be modeled as a hidden variable in
future work.
g
trans
(s,a, t) =
P
m
j=1
P
i?a(j)
f
lex
(s
i
, t
j
) (3)
+
P
i,j:1?i<j?m
f
phr
(s
last(i,j)
first(i,j)
, t
j
i
)
g
lm
(t) =
P
N?{2,3}
P
m+1
j=1
f
N
(t
j
j?N+1
) (4)
g
syn
(t, ?
t
) =
P
m
j=1
f
att
(t
j
, j, t
?
t
(j)
, ?
t
(j))
+f
val
(t
j
, j, ?
?1
t
(j)) (5)
g
reor
(s, ?
s
,a, t, ?
t
) =
P
m
j=1
P
i?a(j)
f
dist
(i, j) (6)
g
tree
2
(?
s
,a, ?
t
) =
m
X
j=1
f
qg
(a(j),a(?
t
(j)), j, ?
t
(j)) (7)
Table 2: Factoring of global feature collections g into
f . x
j
i
denotes ?x
i
, . . . x
j
? in sequence x = ?x
1
, . . .?.
first(i, j) = min
k:i?k?j
(min(a(k))) and last(i, j) =
max
k:i?k?j
(max(a(k))).
2.4 Reordering
Reordering features take many forms in MT. In
phrase-based systems, reordering is accomplished
both within phrase pairs (local reordering) as
well as through distance-based distortion mod-
els (Koehn et al, 2003) and lexicalized reorder-
ing models (Koehn et al, 2007). In syntax-based
systems, reordering is typically parameterized by
grammar rules. For generality we permit these
features to ?see? all structures and denote them
g
reor
(s, ?
s
,a, t, ?
t
). Eq. 6 (Tab. 2) shows a factor-
ing of reordering features based on absolute posi-
tions of aligned words.
We turn next to the ?backbone? model for our
decoder; the formalism and the properties of its
decoding algorithm will inspire two additional sets
of features.
3 Quasi-Synchronous Grammars
A quasi-synchronous dependency grammar
(QDG; Smith and Eisner, 2006) specifies a
conditional model p(t, ?
t
,a | s, ?
s
). Given a
source sentence s and its parse ?
s
, a QDG induces
a probabilistic monolingual dependency grammar
over sentences ?inspired? by the source sentence
and tree. We denote this grammar by G
s,?
s
; its
(weighted) language is the set of translations of s.
Each word generated by G
s,?
s
is annotated with
a ?sense,? which consists of zero or more words
from s. The senses imply an alignment (a) be-
tween words in t and words in s, or equivalently,
between nodes in ?
t
and nodes in ?
s
. In principle,
any portion of ?
t
may align to any portion of ?
s
,
but in practice we often make restrictions on the
alignments to simplify computation. Smith and
Eisner, for example, restricted |a(j)| for all words
221
tj
to be at most one, so that each target word
aligned to at most one source word, which we also
do here.
6
Which translations are possible depends heav-
ily on the configurations that the QDG permits.
Formally, for a parent-child pair ?t
?
t
(j)
, t
j
? in ?
t
,
we consider the relationship between a(?
t
(j)) and
a(j), the source-side words to which t
?
t
(j)
and
t
j
align. If, for example, we require that, for
all j, a(?
t
(j)) = ?
s
(a(j)) or a(j) = 0, and
that the root of ?
t
must align to the root of ?
s
or to NULL, then strict isomorphism must hold
between ?
s
and ?
t
, and we have implemented a
synchronous CF dependency grammar (Alshawi
et al, 2000; Ding and Palmer, 2005). Smith and
Eisner (2006) grouped all possible configurations
into eight classes and explored the effects of per-
mitting different sets of classes in word align-
ment. (?a(?
t
(j)) = ?
s
(a(j))? corresponds to
their ?parent-child? configuration; see Fig. 3 in
Smith and Eisner (2006) for illustrations of the
rest.) More generally, we can define features on
tree pairs that factor into these local configura-
tions, as shown in Eq. 7 (Tab. 2).
Note that the QDG instantiates the model in
Eq. 2. Of the features discussed in ?2, f
lex
, f
att
,
f
val
, and f
dist
can be easily incorporated into the
QDG as described while respecting the indepen-
dence assumptions implied by the configuration
features. The others (f
phr
, f
2
, and f
3
) are non-
local, or involve parts of the structure that, from
the QDG?s perspective, are conditionally indepen-
dent given intervening material. Note that ?non-
locality? is relative to a choice of formalism; in ?2
we did not commit to any formalism, so it is only
now that we can describe phrase and N -gram fea-
tures as non-local. Non-local features will present
a challenge for decoding and training (?4.3).
4 Decoding
Given a sentence s and its parse ?
s
, at decoding
time we seek the target sentence t
?
, the target tree
?
?
t
, and the alignments a
?
that are most probable,
as defined in Eq. 1.
7
(In ?5 we will consider k-
best and all-translations variations on this prob-
6
I.e., from here on, a : {1, . . . ,m} ? {0, . . . , n} where
0 denotes alignment to NULL.
7
Arguably, we seek argmax
t
p(t | s), marginalizing out
everything else. Approximate solutions have been proposed
for that problem in several settings (Blunsom and Osborne,
2008; Sun and Tsujii, 2009); we leave their combination with
our approach to future work.
lem.) As usual, the normalization constant is not
required for decoding; it suffices to solve:
?t
?
, ?
?
t
,a
?
? = argmax
?t,?
t
,a?
?
>
g(s, ?
s
,a, t, ?
t
) (8)
For a QDG model, the decoding problem has
not been addressed before. It equates to finding the
most probable derivation under the s/?
s
-specific
grammar G
s,?
s
. We solve this by lattice parsing,
assuming that an upper bound on m (the length
of t) is known. The advantage offered by this
approach (like most other grammar-based trans-
lation approaches) is that decoding becomes dy-
namic programming (DP), a technique that is both
widely understood in NLP and for which practical,
efficient, generic techniques exist. A major advan-
tage of DP is that, with small modifications, sum-
ming over structures is also possible with ?inside?
DP algorithms. We will exploit this in training
(?5). Efficient summing opens up many possibili-
ties for training ?, such as likelihood and pseudo-
likelihood, and provides principled ways to handle
hidden variables during learning.
4.1 Translation as Monolingual Parsing
We decode by performing lattice parsing on a lat-
tice encoding the set of possible translations. The
lattice is a weighted ?sausage? lattice that permits
sentences up to some maximum length `; ` is de-
rived from the source sentence length. Let the
states be numbered 0 to `; states from b?`c to `
are final states (for some ? ? (0, 1)). For every
position between consecutive states j ? 1 and j
(0 < j ? `), and for every word s
i
in s, and
for every word t ? Trans(s
i
), we instantiate an
arc annotated with t and i. The weight of such an
arc is exp{?
>
f}, where f is the sum of feature
functions that fire when s
i
translates as t in target
position j (e.g., f
lex
(s
i
, t) and f
dist
(i, j)).
Given the lattice and G
s,?
s
, lattice parsing
is a straightforward generalization of standard
context-free dependency parsing DP algorithms
(Eisner, 1997). This decoder accounts for f
lex
,
f
att
, f
val
, f
dist
, and f
qg
as local features.
Figure 1 gives an example, showing a German
sentence and dependency tree from an automatic
parser, an English reference, and a lattice repre-
senting possible translations. In each bundle, the
arcs are listed in decreasing order according to
weight and for clarity only the first five are shown.
The output of the decoder consists of lattice arcs
222
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can you deliver it by tomorrow morning ?
can     you     deliver  it    by     tomorrow morning ?
CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
...
es:it
morgen:tomorrow
liefern:deliver
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
konnten:could konnten:could
es:it
sie:you
konnten:might
...
konnten:couldn
... ... ... ...
sie:let
sie:you
sie:them
es:it sie:you
konnten:could?bersetzen:
translate
?bersetzen:
translate
?bersetzen:
translated
?bersetzen:
translate
?bersetzen:
translated
?:?
konnten:could
es:it
es:it
?:?
es:it
?:?
NULL:to
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
...
fr?h:morning
morgen:tomorrow
morgen:morning
liefern:deliver
vorbei:by
$
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
Source:          $  konnten  sie  es  ?bersetzen  ?
Reference:         could  you  translate  it  ?
Decoder output:
Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out)
and thicker blue arcs forming a dependency tree over them.
selected at each position and a dependency tree
over them.
4.2 Source-Side Coverage Features
Most MT decoders enforce a notion of ?coverage?
of the source sentence during translation: all parts
of s should be aligned to some part of t (alignment
to NULL incurs an explicit cost). Phrase-based sys-
tems such as Moses (Koehn et al, 2007) explic-
itly search for the highest-scoring string in which
all source words are translated. Systems based
on synchronous grammars proceed by parsing the
source sentence with the synchronous grammar,
ensuring that every phrase and word has an ana-
logue in ?
t
(or a deliberate choice is made by the
decoder to translate it to NULL). In such sys-
tems, we do not need to use features to implement
source-side coverage, as it is assumed as a hard
constraint always respected by the decoder.
Our QDG decoder has no way to enforce cov-
erage; it does not track any kind of state in ?
s
apart from a single recently aligned word. This
is a problem with other direct translation models,
such as IBM model 1 used as a direct model rather
than a channel model (Brown et al, 1993). This
sacrifice is the result of our choice to use a condi-
tional model (?2).
The solution is to introduce a set of coverage
features g
cov
(a). Here, these include:
? A counter for the number of times each source
word is covered: f
scov
(a) =
?
n
i=1
|a
?1
(i)|.
? Features that fire once when a source word is
covered the zth time (z ? {2, 3, 4}) and fire
again all subsequent times it is covered; these
are denoted f
2nd
, f
3rd
, and f
4th
.
? A counter of uncovered source words:
f
sunc
(a) =
?
n
i=1
?(|a
?1
(i)|, 0).
Of these, only f
scov
is local.
4.3 Non-Local Features
The lattice QDG parsing decoder incorporates
many of the features we have discussed, but not
all of them. Phrase lexicon features f
phr
, lan-
guage model features f
N
for N > 1, and most
coverage features are non-local with respect to our
QDG. Recently Chiang (2007) introduced ?cube
pruning? as an approximate decoding method that
extends a DP decoder with the ability to incorpo-
rate features that break the Markovian indepen-
dence assumptions DP exploits. Techniques like
cube pruning can be used to include the non-local
features in our decoder.
8
5 Training
Training requires us to learn values for the param-
eters ? in Eq. 2. Given T training examples of the
form ?t
(i)
, ?
(i)
t
, s
(i)
, ?
(i)
s
?, for i = 1, ..., T , max-
imum likelihood estimation for this model con-
sists of solving Eq. 9 (Tab. 3).
9
Note that the
8
A full discussion is omitted for space, but in fact we use
?cube decoding,? a slightly less approximate, slightly more
expensive method that is more closely related to the approxi-
mate inference methods we use for training, discussed in ?5.
9
In practice, we regularize by including a term ?c???
2
2
.
223
LL(?) =
T
X
i=1
log p(t
(i)
, ?
(i)
t
| s
(i)
, ?
(i)
s
) =
T
X
i=1
log
P
a
exp{?
>
g(s
(i)
, ?
(i)
s
,a, t
(i)
, ?
(i)
t
)}
P
t,?
t
,a
exp{?
>
g(s
(i)
, ?
(i)
s
,a, t, ?
t
)}
=
T
X
i=1
log
?numerator?
?denominator?
(9)
PL(?) =
T
X
i=1
log
?
X
a
p(t
(i)
,a | ?
(i)
t
, s
(i)
, ?
(i)
s
)
?
+
T
X
i=1
log
?
X
a
p(?
(i)
t
,a | t
(i)
, s
(i)
, ?
(i)
s
)
?
(10)
?denominator? of
term 1 in Eq. 10
=
n
X
i=0
X
t
?
?Trans(s
i
)
S(?
?1
t
(0), i, t
?
) ? exp
n
?
>
`
f
lex
(s
i
, t
?
) + f
att
($, 0, t
?
, k) + f
qg
(0, i, 0, k)
?
o
(11)
S(j, i, t) =
Y
k??
?1
t
(j)
n
X
i
?
=0
X
t
?
?Trans(s
i
?
)
S(k, i
?
, t
?
) ? exp
?
?
>
?
f
lex
(s
i
?
, t
?
) + f
att
(t, j, t
?
, k)+
f
val
(t, j, ?
?1
t
(j)) + f
qg
(i, i
?
, j, k)
?ff
(12)
S(j, i, t) = exp
n
?
>
`
f
val
(t, j, ?
?1
t
(j))
?
o
if ?
?1
t
(j) = ? (13)
Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. ?. Eqs. 11?13: Recursive DP
equations for summing over t and a.
alignments are treated as a hidden variable to be
marginalized out.
10
Optimization problems of this
form are by now widely known in NLP (Koo and
Collins, 2005), and have recently been used for
machine translation as well (Blunsom et al, 2008).
Such problems are typically solved using varia-
tions of gradient ascent; in our experiments, we
will use an online method called stochastic gra-
dient ascent (SGA). This requires us to calculate
the function?s gradient (vector of first derivatives)
with respect to ?.
11
Computing the numerator in Eq. 9 involves
summing over all possible alignments; with QDG
and a hard bound of 1 on |a(j)| for all j, a fast
?inside? DP solution is known (Smith and Eisner,
2006; Wang et al, 2007). It runs in O(mn
2
) time
and O(mn) space.
Computing the denominator in Eq. 9 requires
summing over all word sequences and depen-
dency trees for the target language sentence and
all word alignments between the sentences. With
a maximum length imposed, this is tractable us-
ing the ?inside? version of the maximizing DP al-
gorithm of Sec. 4, but it is prohibitively expen-
sive. We therefore optimize pseudo-likelihood in-
stead, making the following approximation (Be-
10
Alignments could be supplied by automatic word align-
ment algorithms. We chose to leave them hidden so that we
could make the best use of our parsed training data when con-
figuration constraints are imposed, since it is not always pos-
sible to reconcile automatic word alignments with automatic
parses.
11
When the function?s value is computed by ?inside? DP,
the corresponding ?outside? algorithm can be used to obtain
the gradient. Because outside algorithms can be automati-
cally derived from inside ones, we discuss only inside algo-
rithms in this paper; see Eisner et al (2005).
sag, 1975):
p(t, ?
t
| s, ?
s
) ? p(t | ?
t
, s, ?
s
) ? p(?
t
| t, s, ?
s
)
Plugging this into Eq. 9, we arrive at Eq. 10
(Tab. 3). The two parenthesized terms in Eq. 10
each have their own numerators and denomina-
tors (not shown). The numerators are identical to
each other and to that in Eq. 9. The denominators
are much more manageable than in Eq. 9, never
requiring summation over more than two struc-
tures at a time. We must sum over target word se-
quences and word alignments (with fixed ?
t
), and
separately over target trees and word alignments
(with fixed t).
5.1 Summing over t and a
The summation over target word sequences and
alignments given fixed ?
t
bears a resemblance to
the inside algorithm, except that the tree structure
is fixed (Pereira and Schabes, 1992). Let S(j, i, t)
denote the sum of all translations rooted at posi-
tion j in ?
t
such that a(j) = i and t
j
= t.
Tab. 3 gives the equations for this DP: Eq. 11
is the quantity of interest, Eq. 12 is the recursion,
and Eq. 13 shows the base cases for leaves of ?
t
.
Letting q = max
0?i?n
|Trans(s
i
)|, this algo-
rithm runs in O(mn
2
q
2
) time and O(mnq) space.
For efficiency we place a hard upper bound on q
during training (details in ?6).
5.2 Summing over ?
t
and a
For the summation over dependency trees and
alignments given fixed t, required for p(?
t
|
t, s, ?
s
), we perform ?inside? lattice parsing with
G
s,?
s
. The technique is the summing variant of
the decoding method in ?4, except for each state j,
224
the sausage lattice only includes arcs from j?1 to
j that are labeled with the known target word t
j
.
If a is the number of arcs in the lattice, which is
O(mn), this algorithm runs in O(a
3
) time and re-
quires O(a
2
) space. Because we use a hard upper
bound on |Trans(s)| for all s ? ?, this summation
is much faster in practice than the one over words
and alignments.
5.3 Handling Non-Local Features
So far, all of our algorithms have exploited DP,
disallowing any non-local features (e.g., f
phr
, f
N
for N > 1, f
zth
, f
sunc
). We recently proposed
?cube summing,? an approximate technique that
permits the use of non-local features for inside DP
algorithms (Gimpel and Smith, 2009). Cube sum-
ming is based on a slightly less greedy variation of
cube pruning (Chiang, 2007) that maintains k-best
lists of derivations for each DP chart item. Cube
summing augments the k-best list with a residual
term that sums over remaining structures not in
the k-best list, albeit without their non-local fea-
tures. Using the machinery of cube summing, it
is straightforward to include the desired non-local
features in the summations required for pseudo-
likelihood, as well as to compute their approxi-
mate gradients.
Our approach permits an alternative to mini-
mum error-rate training (MERT; Och, 2003); it is
discriminative but handles latent structure and reg-
ularization in more principled ways. The pseudo-
likelihood calculations for a sentence pair, taken
together, are faster than (k-best) decoding, making
SGA?s inner loop faster than MERT?s inner loop.
6 Experiments
Our decoding framework allows us to perform
many experiments with the same feature rep-
resentation and inference algorithms, includ-
ing combining and comparing phrase-based and
syntax-based features and examining how isomor-
phism constraints of synchronous formalisms af-
fect translation output.
6.1 Data and Evaluation
We use the German-English portion of the Ba-
sic Travel Expression Corpus (BTEC). The cor-
pus has approximately 100K sentence pairs. We
filter sentences of length more than 15 words,
which only removes 6% of the data. We end up
with a training set of 82,299 sentences, a develop-
ment set of 934 sentences, and a test set of 500
sentences. We evaluate translation output using
case-insensitive BLEU (Papineni et al, 2001), as
provided by NIST, and METEOR (Banerjee and
Lavie, 2005), version 0.6, with Porter stemming
and WordNet synonym matching.
6.2 Features
Our base system uses features as discussed
in ?2. To obtain lexical translation features
g
trans
(s,a, t), we use the Moses pipeline (Koehn
et al, 2007). We perform word alignment us-
ing GIZA++ (Och and Ney, 2003), symmetrize
the alignments using the ?grow-diag-final-and?
heuristic, and extract phrases up to length 3. We
define f
lex
by the lexical probabilities p(t | s) and
p(s | t) estimated from the symmetrized align-
ments. After discarding phrase pairs with only
one target-side word (since we only allow a tar-
get word to align to at most one source word), we
define f
phr
by 8 features: {2, 3} target words ?
phrase conditional and ?lexical smoothing? prob-
abilities ? two conditional directions.
Bigram and trigam language model features, f
2
and f
3
, are estimated using the SRI toolkit (Stol-
cke, 2002) with modified Kneser-Ney smoothing
(Chen and Goodman, 1998).
For our target-language syntactic features g
syn
,
we use features similar to lexicalized CFG events
(Collins, 1999), specifically following the de-
pendency model of Klein and Manning (2004).
These include probabilities associated with in-
dividual attachments (f
att
) and child-generation
valence probabilities (f
val
). These probabilities
are estimated on the training corpus parsed using
the Stanford factored parser (Klein and Manning,
2003). The same probabilities are also included
using 50 hard word classes derived from the paral-
lel corpus using the GIZA++ mkcls utility (Och
and Ney, 2003). In total, there are 7 lexical and 7
word-class syntax features.
For reordering, we use a single absolute distor-
tion feature f
dist
(i, j) that returns |i?j|whenever
a(j) = i and i, j > 0. (Unlike the other feature
functions, which returned probabilities, this fea-
ture function returns a nonnegative integer.)
The tree-to-tree syntactic features g
tree
2
in our
model are binary features f
qg
that fire for particu-
lar QG configurations. We use one feature for each
of the configurations in (Smith and Eisner, 2006),
adding 7 additional features that score configura-
225
Phrase Syntactic Features:
features: +f
att
? f
val
+f
qg
(base) (target) (tree-to-tree)
(base) 0.3727 0.4458 0.4424
+f
phr
0.4682 0.4971 0.5142
Table 4: Feature set comparison (BLEU).
tions involving root words and NULL-alignments
more finely. There are 14 features in this category.
Coverage features g
cov
are as described in ?4.2.
In all, 46 feature weights are learned.
6.3 Experimental Procedure
Our model permits training the system on the full
set of parallel data, but we instead use the parallel
data to estimate feature functions and learn ? on
the development set.
12
We trained using three it-
erations of SGA over the development data with a
batch size of 1 and a fixed step size of 0.01. We
used `
2
regularization with a fixed, untuned coef-
ficient of 0.1. Cube summing used a 10-best list
for training and a 7-best list for decoding unless
otherwise specified.
To obtain the translation lexicon (Trans) we
first included the top three target words t for each
s using p(s | t) ? p(t | s) to score target words.
For any training sentence ?s, t? and t
j
for which
t
j
6?
?
n
i=1
Trans(s
i
), we added t
j
to Trans(s
i
)
for i = argmax
i
?
?I
p(s
i
?
|t
j
) ? p(t
j
|s
i
?
), where
I = {i : 0 ? i ? n ? |Trans(s
i
)| < q
i
}.
We used q
0
= 10 and q
>0
= 5, restricting
|Trans(NULL)| ? 10 and |Trans(s)| ? 5 for any
s ? ?. This made 191 of the development sen-
tences unreachable by the model, leaving 743 sen-
tences for learning ?.
During decoding, we generated lattices with all
t ? Trans(s
i
) for 0 ? i ? n, for every position.
We used ? = 0.9, causing states within 90% of the
source sentence length to be final states. Between
each pair of consecutive states, we pruned edges
that fell outside a beam of 70% of the sum of edge
weights (see ?4.1; edge weights use f
lex
, f
dist
,
and f
scov
) of all edges between those two states.
6.4 Feature Set Comparison
Our first set of experiments compares feature sets
commonly used in phrase- and syntax-based trans-
lation. In particular, we compare the effects of
combining phrase features and syntactic features.
The base model contains f
lex
, g
lm
, g
reor
, and
12
We made this choice both for similarity to standard MT
systems and a more rapid experiment cycle.
g
cov
. The results are shown in Table 4. The sec-
ond row contains scores when adding in the eight
f
phr
features. The second column shows scores
when adding the 14 target syntax features (f
att
and f
val
), and the third column adds to them the
14 additional tree-to-tree features (f
qg
). We find
large gains in BLEU by adding more features, and
find that gains obtained through phrase features
and syntactic features are partially additive, sug-
gesting that these feature sets are making comple-
mentary contributions to translation quality.
6.5 Varying k During Decoding
For models without syntactic features, we con-
strained the decoder to produce dependency trees
in which every word?s parent is immediately to its
right and ignored syntactic features while scoring
structures. This causes decoding to proceed left-
to-right in the lattice, the way phrase-based de-
coders operate. Since these models do not search
over trees, they are substantially faster during de-
coding than those that use syntactic features and
do not require any pruning of the lattice. There-
fore, we explored varying the value of k used dur-
ing k-best cube decoding; results are shown in
Fig. 2. Scores improve when we increase k up
to 10, but not much beyond, and there is still a
substantial gap (2.5 BLEU) between using phrase
features with k = 20 and using all features with
k = 5. Models without syntax perform poorly
when using a very small k, due to their reliance on
non-local language model and phrase features. By
contrast, models with syntactic features, which are
local in our decoder, perform relatively well even
with k = 1.
6.6 QG Configuration Comparison
We next compare different constraints on isomor-
phism between the source and target dependency
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0 5 10 15 20
Value of k  for Decoding
B
L
E
U
Phrase + Syntactic
Phrase
Syntactic
Neither
Figure 2: Comparison of size of k-best list for cube decoding
with various feature sets.
226
QDG Configurations BLEU METEOR
synchronous 0.4008 0.6949
+ nulls, root-any 0.4108 0.6931
+ child-parent, same node 0.4337 0.6815
+ sibling 0.4881 0.7216
+ grandparent/child 0.5015 0.7365
+ c-command 0.5156 0.7441
+ other 0.5142 0.7472
Table 5: QG configuration comparison. The name of each
configuration, following Smith and Eisner (2006), refers to
the relationship between a(?
t
(j)) and a(j) in ?
s
.
trees. To do this, we impose harsh penalties on
some QDG configurations (?3) by fixing their fea-
ture weights to ?1000. Hence they are permit-
ted only when absolutely necessary in training
and rarely in decoding.
13
Each model uses all
phrase and syntactic features; they differ only in
the sets of configurations which have fixed nega-
tive weights.
Tab. 5 shows experimental results. The
base ?synchronous? model permits parent-child
(a(?
t
(j)) = ?
s
(a(j))), any configuration where
a(j) = 0, including both words being linked to
NULL, and requires the root word in ?
t
to be linked
to the root word in ?
s
or to NULL(5 of our 14
configurations). The second row allows any con-
figuration involving NULL, including those where
t
j
aligns to a non-NULL word in s and its par-
ent aligns to NULL, and allows the root in ?
t
to
be linked to any word in ?
s
. Each subsequent
row adds additional configurations (i.e., trains its
? rather than fixing it to ?1000). In general, we
see large improvements as we permit more con-
figurations, and the largest jump occurs when we
add the ?sibling? configuration (?
s
(a(?
t
(j))) =
?
s
(a(j))). The BLEU score does not increase,
however, when we permit all configurations in the
final row of the table, and the METEOR score in-
creases only slightly. While allowing certain cate-
gories of non-isomorphism clearly seems helpful,
permitting arbitrary violations does not appear to
be necessary for this dataset.
6.7 Discussion
We note that these results are not state-of-the-
art on this dataset (on this task, Moses/MERT
achieves 0.6838 BLEU and 0.8523METEORwith
maximum phrase length 3).
14
Our aim has been to
13
In fact, the strictest ?synchronous? model used the
almost-forbidden configurations in 2% of test sentences; this
behavior disappears as configurations are legalized.
14
We believe one cause for this performance gap is the gen-
eration of the lattice and plan to address this in future work
by allowing the phrase table to inform lattice generation.
illustrate how a single model can provide a con-
trolled experimental framework for comparisons
of features, of inference methods, and of con-
straints. Our findings show that phrase features
and dependency syntax produce complementary
improvements to translation quality, that tree-to-
tree configurations (a new feature in MT) are help-
ful for translation, and that substantial gains can
be obtained by permitting certain types of non-
isomorphism. We have validated cube summing
and decoding as practical methods for approxi-
mate inference.
Our framework permits exploration of alter-
native objectives, alternative approximate infer-
ence techniques, additional hidden variables (e.g.,
Moses? phrase segmentation variable), and, of
course, additional feature representations. The
system is publicly available at www.ark.cs.
cmu.edu/Quipu.
7 Conclusion
We presented feature-rich MT using a princi-
pled probabilistic framework that separates fea-
tures from inference. Our novel decoder is based
on efficient DP-based QG lattice parsing extended
to handle ?non-local? features using generic tech-
niques that also support efficient parameter esti-
mation. Controlled experiments permitted with
this system show interesting trends in the use of
syntactic features and constraints.
Acknowledgments
We thank three anonymous EMNLP reviewers,
David Smith, and Stephan Vogel for helpful com-
ments and feedback that improved this paper. This
research was supported by NSF IIS-0836431 and
IIS-0844507, a grant from Google, and computa-
tional resources provided by Yahoo.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000.
Learning dependency translation modles as colec-
tions of finite-state head transducers. Computa-
tional Linguistics, 26(1):45?60.
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for MT evaluation with improved
correlation with human judgments. In Proc. of
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization.
J. E. Besag. 1975. Statistical analysis of non-lattice
data. The Statistician, 24:179?195.
227
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical report 10-98, Harvard University.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proc. of EMNLP.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, U. Penn.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition.
In Proc. of ACL-IJCNLP.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammar. In Proc. of ACL.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
piling Comp Ling: Practical weighted dynamic pro-
gramming and the Dyna language. In Proc. of HLT-
EMNLP.
J. Eisner. 1997. Bilexical grammars and a cubic-time
probabilistic parser. In Proc. of IWPT.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic transla-
tion models. In Proc. of COLING-ACL.
K. Gimpel and N. A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proc. of ACL-2008 Workshop on Statistical Machine
Translation.
K. Gimpel and N. A. Smith. 2009. Cube summing,
approximate inference with non-local features, and
dynamic programming without semirings. In Proc.
of EACL.
R. Haque, S. K. Naskar, Y. Ma, and A. Way. 2009.
Using supertags as source language context in SMT.
In Proc. of EAMT.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In Proc. of HLT-NAACL.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in NIPS 15.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proc. of ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
(demo session).
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. of EMNLP.
P. Liang, A. Bouchard-C?ot?e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.
A. Lopez. 2009. Translation as weighted deduction.
In Proc. of EACL.
D. Marcu, W.Wang, A. Echihabi, and K. Knight. 2006.
Statistical machine translation with syntactified tar-
get language phrases. In Proc. of EMNLP.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, and T. Ward. 1997. Feature-
based language understanding. In EUROSPEECH.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of ACL.
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside
reestimation from partially bracketed corpora. In
Proc. of ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
of ACL.
D. A. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proc. of HLT-NAACLWorkshop on
Statistical Machine Translation.
D. A. Smith and J. Eisner. 2009. Parser adaptation
and projection with quasi-synchronous features. In
Proc. of EMNLP.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Proc. of ICSLP.
X. Sun and J. Tsujii. 2009. Sequential labeling with
latent variables: An exact inference algorithm and
its efficient approximation. In Proc. of EACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for QA. In Proc. of EMNLP-CoNLL.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of ACL.
228
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 318?326,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Cube Summing, Approximate Inference with Non-Local Features,
and Dynamic Programming without Semirings
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We introduce cube summing, a technique
that permits dynamic programming algo-
rithms for summing over structures (like
the forward and inside algorithms) to be
extended with non-local features that vio-
late the classical structural independence
assumptions. It is inspired by cube prun-
ing (Chiang, 2007; Huang and Chiang,
2007) in its computation of non-local
features dynamically using scored k-best
lists, but also maintains additional resid-
ual quantities used in calculating approx-
imate marginals. When restricted to lo-
cal features, cube summing reduces to a
novel semiring (k-best+residual) that gen-
eralizes many of the semirings of Good-
man (1999). When non-local features are
included, cube summing does not reduce
to any semiring, but is compatible with
generic techniques for solving dynamic
programming equations.
1 Introduction
Probabilistic NLP researchers frequently make in-
dependence assumptions to keep inference algo-
rithms tractable. Doing so limits the features that
are available to our models, requiring features
to be structurally local. Yet many problems in
NLP?machine translation, parsing, named-entity
recognition, and others?have benefited from the
addition of non-local features that break classical
independence assumptions. Doing so has required
algorithms for approximate inference.
Recently cube pruning (Chiang, 2007; Huang
and Chiang, 2007) was proposed as a way to lever-
age existing dynamic programming algorithms
that find optimal-scoring derivations or structures
when only local features are involved. Cube prun-
ing permits approximate decoding with non-local
features, but leaves open the question of how the
feature weights or probabilities are learned. Mean-
while, some learning algorithms, like maximum
likelihood for conditional log-linear models (Laf-
ferty et al, 2001), unsupervised models (Pereira
and Schabes, 1992), and models with hidden vari-
ables (Koo and Collins, 2005; Wang et al, 2007;
Blunsom et al, 2008), require summing over the
scores of many structures to calculate marginals.
We first review the semiring-weighted logic
programming view of dynamic programming al-
gorithms (Shieber et al, 1995) and identify an in-
tuitive property of a program called proof locality
that follows from feature locality in the underlying
probability model (?2). We then provide an analy-
sis of cube pruning as an approximation to the in-
tractable problem of exact optimization over struc-
tures with non-local features and show how the
use of non-local features with k-best lists breaks
certain semiring properties (?3). The primary
contribution of this paper is a novel technique?
cube summing?for approximate summing over
discrete structures with non-local features, which
we relate to cube pruning (?4). We discuss imple-
mentation (?5) and show that cube summing be-
comes exact and expressible as a semiring when
restricted to local features; this semiring general-
izes many commonly-used semirings in dynamic
programming (?6).
2 Background
In this section, we discuss dynamic programming
algorithms as semiring-weighted logic programs.
We then review the definition of semirings and im-
portant examples. We discuss the relationship be-
tween locally-factored structure scores and proofs
in logic programs.
2.1 Dynamic Programming
Many algorithms in NLP involve dynamic pro-
gramming (e.g., the Viterbi, forward-backward,
318
probabilistic Earley?s, and minimum edit distance
algorithms). Dynamic programming (DP) in-
volves solving certain kinds of recursive equations
with shared substructure and a topological order-
ing of the variables.
Shieber et al (1995) showed a connection
between DP (specifically, as used in parsing)
and logic programming, and Goodman (1999)
augmented such logic programs with semiring
weights, giving an algebraic explanation for the
intuitive connections among classes of algorithms
with the same logical structure. For example, in
Goodman?s framework, the forward algorithm and
the Viterbi algorithm are comprised of the same
logic program with different semirings. Goodman
defined other semirings, including ones we will
use here. This formal framework was the basis
for the Dyna programming language, which per-
mits a declarative specification of the logic pro-
gram and compiles it into an efficient, agenda-
based, bottom-up procedure (Eisner et al, 2005).
For our purposes, a DP consists of a set of recur-
sive equations over a set of indexed variables. For
example, the probabilistic CKY algorithm (run on
sentence w1w2...wn) is written as
CX,i?1,i = pX?wi (1)
CX,i,k = max
Y,Z?N;j?{i+1,...,k?1}
pX?Y Z ? CY,i,j ? CZ,j,k
goal = CS,0,n
where N is the nonterminal set and S ? N is the
start symbol. Each CX,i,j variable corresponds to
the chart value (probability of the most likely sub-
tree) of an X-constituent spanning the substring
wi+1...wj . goal is a special variable of greatest in-
terest, though solving for goal correctly may (in
general, but not in this example) require solving
for all the other values. We will use the term ?in-
dex? to refer to the subscript values on variables
(X, i, j on CX,i,j).
Where convenient, we will make use of Shieber
et al?s logic programming view of dynamic pro-
gramming. In this view, each variable (e.g., CX,i,j
in Eq. 1) corresponds to the value of a ?theo-
rem,? the constants in the equations (e.g., pX?Y Z
in Eq. 1) correspond to the values of ?axioms,?
and the DP defines quantities corresponding to
weighted ?proofs? of the goal theorem (e.g., find-
ing the maximum-valued proof, or aggregating
proof values). The value of a proof is a combi-
nation of the values of the axioms it starts with.
Semirings define these values and define two op-
erators over them, called ?aggregation? (max in
Eq. 1) and ?combination? (? in Eq. 1).
Goodman and Eisner et al assumed that the val-
ues of the variables are in a semiring, and that the
equations are defined solely in terms of the two
semiring operations. We will often refer to the
?probability? of a proof, by which we mean a non-
negative R-valued score defined by the semantics
of the dynamic program variables; it may not be a
normalized probability.
2.2 Semirings
A semiring is a tuple ?A,?,?,0,1?, in which A
is a set, ? : A ? A ? A is the aggregation
operation, ? : A ? A ? A is the combina-
tion operation, 0 is the additive identity element
(?a ? A, a ? 0 = a), and 1 is the multiplica-
tive identity element (?a ? A, a ? 1 = a). A
semiring requires ? to be associative and com-
mutative, and ? to be associative and to distribute
over?. Finally, we require a?0 = 0?a = 0 for
all a ? A.1 Examples include the inside semir-
ing, ?R?0,+,?, 0, 1?, and the Viterbi semiring,
?R?0,max,?, 0, 1?. The former sums the prob-
abilities of all proofs of each theorem. The lat-
ter (used in Eq. 1) calculates the probability of the
most probable proof of each theorem. Two more
examples follow.
Viterbi proof semiring. We typically need to
recover the steps in the most probable proof in
addition to its probability. This is often done us-
ing backpointers, but can also be accomplished by
representing the most probable proof for each the-
orem in its entirety as part of the semiring value
(Goodman, 1999). For generality, we define a
proof as a string that is constructed from strings
associated with axioms, but the particular form
of a proof is problem-dependent. The ?Viterbi
proof? semiring includes the probability of the
most probable proof and the proof itself. Letting
L ? ?? be the proof language on some symbol
set ?, this semiring is defined on the set R?0 ? L
with 0 element ?0, ? and 1 element ?1, ?. For
two values ?u1, U1? and ?u2, U2?, the aggregation
operator returns ?max(u1, u2), Uargmaxi?{1,2} ui?.
1When cycles are permitted, i.e., where the value of one
variable depends on itself, infinite sums can be involved. We
must ensure that these infinite sums are well defined under
the semiring. So-called complete semirings satisfy additional
conditions to handle infinite sums, but for simplicity we will
restrict our attention to DPs that do not involve cycles.
319
Semiring A Aggregation (?) Combination (?) 0 1
inside R?0 u1 + u2 u1u2 0 1
Viterbi R?0 max(u1, u2) u1u2 0 1
Viterbi proof R?0 ? L ?max(u1, u2), Uargmaxi?{1,2} ui? ?u1u2, U1.U2? ?0, ? ?1, ?
k-best proof (R?0 ? L)?k max-k(u1 ? u2) max-k(u1 ? u2) ? {?1, ?}
Table 1: Commonly used semirings. An element in the Viterbi proof semiring is denoted ?u1, U1?, where u1 is the probability
of proof U1. The max-k function returns a sorted list of the top-k proofs from a set. The ? function performs a cross-product
on two k-best proof lists (Eq. 2).
The combination operator returns ?u1u2, U1.U2?,
where U1.U2 denotes the string concatenation of
U1 and U2.2
k-best proof semiring. The ?k-best proof?
semiring computes the values and proof strings of
the k most-probable proofs for each theorem. The
set is (R?0 ? L)?k, i.e., sequences (up to length
k) of sorted probability/proof pairs. The aggrega-
tion operator ? uses max-k, which chooses the k
highest-scoring proofs from its argument (a set of
scored proofs) and sorts them in decreasing order.
To define the combination operator ?, we require
a cross-product that pairs probabilities and proofs
from two k-best lists. We call this ?, defined on
two semiring values u = ??u1, U1?, ..., ?uk, Uk??
and v = ??v1, V1?, ..., ?vk, Vk?? by:
u ? v = {?uivj , Ui.Vj? | i, j ? {1, ..., k}} (2)
Then, u ? v = max-k(u ? v). This is similar to
the k-best semiring defined by Goodman (1999).
These semirings are summarized in Table 1.
2.3 Features and Inference
Let X be the space of inputs to our logic program,
i.e., x ? X is a set of axioms. Let L denote the
proof language and let Y ? L denote the set of
proof strings that constitute full proofs, i.e., proofs
of the special goal theorem. We assume an expo-
nential probabilistic model such that
p(y | x) ?
?M
m=1 ?
hm(x,y)
m (3)
where each ?m ? 0 is a parameter of the model
and each hm is a feature function. There is a bijec-
tion between Y and the space of discrete structures
that our model predicts.
Given such a model, DP is helpful for solving
two kinds of inference problems. The first prob-
lem, decoding, is to find the highest scoring proof
2We assume for simplicity that the best proof will never
be a tie among more than one proof. Goodman (1999) han-
dles this situation more carefully, though our version is more
likely to be used in practice for both the Viterbi proof and
k-best proof semirings.
y? ? Y for a given input x ? X:
y?(x) = argmaxy?Y
?M
m=1 ?m
hm(x,y) (4)
The second is the summing problem, which
marginalizes the proof probabilities (without nor-
malization):
s(x) =
?
y?Y
?M
m=1 ?m
hm(x,y) (5)
As defined, the feature functions hm can depend
on arbitrary parts of the input axiom set x and the
entire output proof y.
2.4 Proof and Feature Locality
An important characteristic of problems suited for
DP is that the global calculation (i.e., the value of
goal ) depend only on local factored parts. In DP
equations, this means that each equation connects
a relatively small number of indexed variables re-
lated through a relatively small number of indices.
In the logic programming formulation, it means
that each step of the proof depends only on the the-
orems being used at that step, not the full proofs
of those theorems. We call this property proof lo-
cality. In the statistical modeling view of Eq. 3,
classical DP requires that the probability model
make strong Markovian conditional independence
assumptions (e.g., in HMMs, St?1 ? St+1 | St);
in exponential families over discrete structures,
this corresponds to feature locality.
For a particular proof y of goal consisting of
t intermediate theorems, we define a set of proof
strings `i ? L for i ? {1, ..., t}, where `i corre-
sponds to the proof of the ith theorem.3 We can
break the computation of feature function hm into
a summation over terms corresponding to each `i:
hm(x, y) =
?t
i=1 fm(x, `i) (6)
This is simply a way of noting that feature func-
tions ?fire? incrementally at specific points in the
3The theorem indexing scheme might be based on a topo-
logical ordering given by the proof structure, but is not im-
portant for our purposes.
320
proof, normally at the first opportunity. Any fea-
ture function can be expressed this way. For local
features, we can go farther; we define a function
top(`) that returns the proof string corresponding
to the antecedents and consequent of the last infer-
ence step in `. Local features have the property:
hlocm (x, y) =
?t
i=1 fm(x, top(`i)) (7)
Local features only have access to the most re-
cent deductive proof step (though they may ?fire?
repeatedly in the proof), while non-local features
have access to the entire proof up to a given the-
orem. For both kinds of features, the ?f? terms
are used within the DP formulation. When tak-
ing an inference step to prove theorem i, the value
?M
m=1 ?
fm(x,`i)
m is combined into the calculation
of that theorem?s value, along with the values of
the antecedents. Note that typically only a small
number of fm are nonzero for theorem i.
When non-local hm/fm that depend on arbitrary
parts of the proof are involved, the decoding and
summing inference problems are NP-hard (they
instantiate probabilistic inference in a fully con-
nected graphical model). Sometimes, it is possible
to achieve proof locality by adding more indices to
the DP variables (for example, consider modify-
ing the bigram HMMViterbi algorithm for trigram
HMMs). This increases the number of variables
and hence computational cost. In general, it leads
to exponential-time inference in the worst case.
There have been many algorithms proposed for
approximately solving instances of these decod-
ing and summing problems with non-local fea-
tures. Some stem from work on graphical mod-
els, including loopy belief propagation (Sutton and
McCallum, 2004; Smith and Eisner, 2008), Gibbs
sampling (Finkel et al, 2005), sequential Monte
Carlo methods such as particle filtering (Levy et
al., 2008), and variational inference (Jordan et al,
1999; MacKay, 1997; Kurihara and Sato, 2006).
Also relevant are stacked learning (Cohen and
Carvalho, 2005), interpretable as approximation
of non-local feature values (Martins et al, 2008),
and M-estimation (Smith et al, 2007), which al-
lows training without inference. Several other ap-
proaches used frequently in NLP are approximate
methods for decoding only. These include beam
search (Lowerre, 1976), cube pruning, which we
discuss in ?3, integer linear programming (Roth
and Yih, 2004), in which arbitrary features can act
as constraints on y, and approximate solutions like
McDonald and Pereira (2006), in which an exact
solution to a related decoding problem is found
and then modified to fit the problem of interest.
3 Approximate Decoding
Cube pruning (Chiang, 2007; Huang and Chi-
ang, 2007) is an approximate technique for decod-
ing (Eq. 4); it is used widely in machine transla-
tion. Given proof locality, it is essentially an effi-
cient implementation of the k-best proof semiring.
Cube pruning goes farther in that it permits non-
local features to weigh in on the proof probabili-
ties, at the expense of making the k-best operation
approximate. We describe the two approximations
cube pruning makes, then propose cube decoding,
which removes the second approximation. Cube
decoding cannot be represented as a semiring; we
propose a more general algebraic structure that ac-
commodates it.
3.1 Approximations in Cube Pruning
Cube pruning is an approximate solution to the de-
coding problem (Eq. 4) in two ways.
Approximation 1: k < ?. Cube pruning uses
a finite k for the k-best lists stored in each value.
If k = ?, the algorithm performs exact decoding
with non-local features (at obviously formidable
expense in combinatorial problems).
Approximation 2: lazy computation. Cube
pruning exploits the fact that k < ? to use lazy
computation. When combining the k-best proof
lists of d theorems? values, cube pruning does not
enumerate all kd proofs, apply non-local features
to all of them, and then return the top k. Instead,
cube pruning uses a more efficient but approxi-
mate solution that only calculates the non-local
factors on O(k) proofs to obtain the approximate
top k. This trick is only approximate if non-local
features are involved.
Approximation 2 makes it impossible to formu-
late cube pruning using separate aggregation and
combination operations, as the use of lazy com-
putation causes these two operations to effectively
be performed simultaneously. To more directly
relate our summing algorithm (?4) to cube prun-
ing, we suggest a modified version of cube prun-
ing that does not use lazy computation. We call
this algorithm cube decoding. This algorithm can
be written down in terms of separate aggregation
321
and combination operations, though we will show
it is not a semiring.
3.2 Cube Decoding
We formally describe cube decoding, show that
it does not instantiate a semiring, then describe
a more general algebraic structure that it does in-
stantiate.
Consider the set G of non-local feature functions
that map X ? L ? R?0.4 Our definitions in ?2.2
for the k-best proof semiring can be expanded to
accommodate these functions within the semiring
value. Recall that values in the k-best proof semir-
ing fall inAk = (R?0?L)?k. For cube decoding,
we use a different set Acd defined as
Acd = (R?0 ? L)?k
? ?? ?
Ak
?G? {0, 1}
where the binary variable indicates whether the
value contains a k-best list (0, which we call an
?ordinary? value) or a non-local feature function
in G (1, which we call a ?function? value). We
denote a value u ? Acd by
u = ???u1, U1?, ?u2, U2?, ..., ?uk, Uk??
? ?? ?
u?
, gu, us?
where each ui ? R?0 is a probability and each
Ui ? L is a proof string.
We use ?k and ?k to denote the k-best proof
semiring?s operators, defined in ?2.2. We let g0 be
such that g0(`) is undefined for all ` ? L. For two
values u = ?u?, gu, us?,v = ?v?, gv, vs? ? Acd,
cube decoding?s aggregation operator is:
u?cd v = ?u??k v?, g0, 0? if ?us ? ?vs (8)
Under standard models, only ordinary values will
be operands of?cd, so?cd is undefined when us?
vs. We define the combination operator ?cd:
u?cd v = (9)?
?????
?????
?u??k v?, g0, 0? if ?us ? ?vs,
?max-k(exec(gv, u?)), g0, 0? if ?us ? vs,
?max-k(exec(gu, v?)), g0, 0? if us ? ?vs,
???, ?z.(gu(z)? gv(z)), 1? if us ? vs.
where exec(g, u?) executes the function g upon
each proof in the proof list u?, modifies the scores
4In our setting, gm(x, `) will most commonly be defined
as ?fm(x,`)m in the notation of ?2.3. But functions in G could
also be used to implement, e.g., hard constraints or other non-
local score factors.
in place by multiplying in the function result, and
returns the modified proof list:
g? = ?`.g(x, `)
exec(g, u?) = ??u1g?(U1), U1?, ?u2g?(U2), U2?,
..., ?ukg
?(Uk), Uk??
Here, max-k is simply used to re-sort the k-best
proof list following function evaluation.
The semiring properties fail to hold when in-
troducing non-local features in this way. In par-
ticular, ?cd is not associative when 1 < k < ?.
For example, consider the probabilistic CKY algo-
rithm as above, but using the cube decoding semir-
ing with the non-local feature functions collec-
tively known as ?NGramTree? features (Huang,
2008) that score the string of terminals and nonter-
minals along the path from word j to word j + 1
when two constituents CY,i,j and CZ,j,k are com-
bined. The semiring value associated with such
a feature is u = ???,NGramTreepi(), 1? (for a
specific path pi), and we rewrite Eq. 1 as fol-
lows (where ranges for summation are omitted for
space):
CX,i,k =
?
cd pX?Y Z ?cdCY,i,j ?cdCZ,j,k?cdu
The combination operator is not associative
since the following will give different answers:5
(pX?Y Z ?cd CY,i,j)?cd (CZ,j,k ?cd u) (10)
((pX?Y Z ?cd CY,i,j)?cd CZ,j,k)?cd u (11)
In Eq. 10, the non-local feature function is ex-
ecuted on the k-best proof list for Z, while in
Eq. 11, NGramTreepi is called on the k-best proof
list for the X constructed from Y and Z. Further-
more, neither of the above gives the desired re-
sult, since we actually wish to expand the full set
of k2 proofs of X and then apply NGramTreepi
to each of them (or a higher-dimensional ?cube?
if more operands are present) before selecting the
k-best. The binary operations above retain only
the top k proofs of X in Eq. 11 before applying
NGramTreepi to each of them. We actually would
like to redefine combination so that it can operate
on arbitrarily-sized sets of values.
We can understand cube decoding through an
algebraic structure with two operations ? and ?,
where ? need not be associative and need not dis-
tribute over?, and furthermore where? and? are
5Distributivity of combination over aggregation fails for
related reasons. We omit a full discussion due to space.
322
defined on arbitrarily many operands. We will re-
fer here to such a structure as a generalized semir-
ing.6 To define ?cd on a set of operands with N ?
ordinary operands and N function operands, we
first compute the full O(kN
?
) cross-product of the
ordinary operands, then apply each of the N func-
tions from the remaining operands in turn upon the
full N ?-dimensional ?cube,? finally calling max-k
on the result.
4 Cube Summing
We present an approximate solution to the sum-
ming problem when non-local features are in-
volved, which we call cube summing. It is an ex-
tension of cube decoding, and so we will describe
it as a generalized semiring. The key addition is to
maintain in each value, in addition to the k-best list
of proofs from Ak, a scalar corresponding to the
residual probability (possibly unnormalized) of all
proofs not among the k-best.7 The k-best proofs
are still used for dynamically computing non-local
features but the aggregation and combination op-
erations are redefined to update the residual as ap-
propriate.
We define the set Acs for cube summing as
Acs = R?0 ? (R?0 ? L)?k ? G? {0, 1}
A value u ? Acs is defined as
u = ?u0, ??u1, U1?, ?u2, U2?, ..., ?uk, Uk??
? ?? ?
u?
, gu, us?
For a proof list u?, we use ?u?? to denote the sum
of all proof scores,
?
i:?ui,Ui??u? ui.
The aggregation operator over operands
{ui}Ni=1, all such that uis = 0,
8 is defined by:
?N
i=1 ui = (12)??N
i=1 ui0 +
?
?
?Res
(?N
i=1 u?i
)?
?
? ,
max-k
(?N
i=1 u?i
)
, g0, 0
?
6Algebraic structures are typically defined with binary op-
erators only, so we were unable to find a suitable term for this
structure in the literature.
7Blunsom and Osborne (2008) described a related ap-
proach to approximate summing using the chart computed
during cube pruning, but did not keep track of the residual
terms as we do here.
8We assume that operands ui to ?cs will never be such
that uis = 1 (non-local feature functions). This is reasonable
in the widely used log-linear model setting we have adopted,
where weights ?m are factors in a proof?s product score.
where Res returns the ?residual? set of scored
proofs not in the k-best among its arguments, pos-
sibly the empty set.
For a set ofN+N ? operands {vi}Ni=1?{wj}
N ?
j=1
such that vis = 1 (non-local feature functions) and
wjs = 1 (ordinary values), the combination oper-
ator ? is shown in Eq. 13 Fig. 1. Note that the
case where N ? = 0 is not needed in this applica-
tion; an ordinary value will always be included in
combination.
In the special case of two ordinary operands
(where us = vs = 0), Eq. 13 reduces to
u? v = (14)
?u0v0 + u0 ?v??+ v0 ?u??+ ?Res(u? ? v?)? ,
max-k(u? ? v?), g0, 0?
We define 0 as ?0, ??, g0, 0?; an appropriate def-
inition for the combination identity element is less
straightforward and of little practical importance;
we leave it to future work.
If we use this generalized semiring to solve a
DP and achieve goal value of u, the approximate
sum of all proof probabilities is given by u0+?u??.
If all features are local, the approach is exact. With
non-local features, the k-best list may not contain
the k-best proofs, and the residual score, while in-
cluding all possible proofs, may not include all of
the non-local features in all of those proofs? prob-
abilities.
5 Implementation
We have so far viewed dynamic programming
algorithms in terms of their declarative speci-
fications as semiring-weighted logic programs.
Solvers have been proposed by Goodman (1999),
by Klein and Manning (2001) using a hypergraph
representation, and by Eisner et al (2005). Be-
cause Goodman?s and Eisner et al?s algorithms as-
sume semirings, adapting them for cube summing
is non-trivial.9
To generalize Goodman?s algorithm, we sug-
gest using the directed-graph data structure known
variously as an arithmetic circuit or computation
graph.10 Arithmetic circuits have recently drawn
interest in the graphical model community as a
9The bottom-up agenda algorithm in Eisner et al (2005)
might possibly be generalized so that associativity, distribu-
tivity, and binary operators are not required (John Blatz, p.c.).
10This data structure is not specific to any particular set of
operations. We have also used it successfully with the inside
semiring.
323
N?
i=1
vi ?
N ??
j=1
wj =
??
?
?
B?P(S)
?
b?B
wb0
?
c?S\B
?w?c?
?
? (13)
+ ?Res(exec(gv1 , . . . exec(gvN , w?1 ? ? ? ? ? w?N ?) . . .))? ,
max-k(exec(gv1 , . . . exec(gvN , w?1 ? ? ? ? ? w?N ?) . . .)), g0, 0
?
Figure 1: Combination operation for cube summing, where S = {1, 2, . . . , N ?} and P(S) is the power set of S excluding ?.
tool for performing probabilistic inference (Dar-
wiche, 2003). In the directed graph, there are ver-
tices corresponding to axioms (these are sinks in
the graph), ? vertices corresponding to theorems,
and ? vertices corresponding to summands in the
dynamic programming equations. Directed edges
point from each node to the nodes it depends on;
? vertices depend on? vertices, which depend on
? and axiom vertices.
Arithmetic circuits are amenable to automatic
differentiation in the reverse mode (Griewank
and Corliss, 1991), commonly used in back-
propagation algorithms. Importantly, this permits
us to calculate the exact gradient of the approx-
imate summation with respect to axiom values,
following Eisner et al (2005). This is desirable
when carrying out the optimization problems in-
volved in parameter estimation. Another differen-
tiation technique, implemented within the semir-
ing, is given by Eisner (2002).
Cube pruning is based on the k-best algorithms
of Huang and Chiang (2005), which save time
over generic semiring implementations through
lazy computation in both the aggregation and com-
bination operations. Their techniques are not as
clearly applicable here, because our goal is to sum
over all proofs instead of only finding a small sub-
set of them. If computing non-local features is a
computational bottleneck, they can be computed
only for the O(k) proofs considered when choos-
ing the best k as in cube pruning. Then, the com-
putational requirements for approximate summing
are nearly equivalent to cube pruning, but the ap-
proximation is less accurate.
6 Semirings Old and New
We now consider interesting special cases and
variations of cube summing.
6.1 The k-best+residual Semiring
When restricted to local features, cube pruning
and cube summing can be seen as proper semir-
k-best proof
(Goodman, 1999)
k-best + residual
Viterbi proof
(Goodman, 1999)
all proof
(Goodman, 1999)
Viterbi
(Viterbi, 1967)
ignore
proof
inside
(Baum et al, 1970)
ign
or
e r
es
idu
al
k = 0
k = ?k =
 1
Figure 2: Semirings generalized by k-best+residual.
ings. Cube pruning reduces to an implementation
of the k-best semiring (Goodman, 1998), and cube
summing reduces to a novel semiring we call the
k-best+residual semiring. Binary instantiations of
? and ? can be iteratively reapplied to give the
equivalent formulations in Eqs. 12 and 13. We de-
fine 0 as ?0, ??? and 1 as ?1, ?1, ??. The ? opera-
tor is easily shown to be commutative. That ? is
associative follows from associativity of max-k,
shown by Goodman (1998). Showing that ? is
associative and that ? distributes over ? are less
straightforward; proof sketches are provided in
Appendix A. The k-best+residual semiring gen-
eralizes many semirings previously introduced in
the literature; see Fig. 2.
6.2 Variations
Once we relax requirements about associativity
and distributivity and permit aggregation and com-
bination operators to operate on sets, several ex-
tensions to cube summing become possible. First,
when computing approximate summations with
non-local features, we may not always be inter-
ested in the best proofs for each item. Since the
purpose of summing is often to calculate statistics
324
under a model distribution, we may wish instead
to sample from that distribution. We can replace
the max-k function with a sample-k function that
samples k proofs from the scored list in its argu-
ment, possibly using the scores or possibly uni-
formly at random. This breaks associativity of ?.
We conjecture that this approach can be used to
simulate particle filtering for structured models.
Another variation is to vary k for different theo-
rems. This might be used to simulate beam search,
or to reserve computation for theorems closer to
goal , which have more proofs.
7 Conclusion
This paper has drawn a connection between cube
pruning, a popular technique for approximately
solving decoding problems, and the semiring-
weighted logic programming view of dynamic
programming. We have introduced a generaliza-
tion called cube summing, to be used for solv-
ing summing problems, and have argued that cube
pruning and cube summing are both semirings that
can be used generically, as long as the under-
lying probability models only include local fea-
tures. With non-local features, cube pruning and
cube summing can be used for approximate decod-
ing and summing, respectively, and although they
no longer correspond to semirings, generic algo-
rithms can still be used.
Acknowledgments
We thank three anonymous EACL reviewers, John Blatz, Pe-
dro Domingos, Jason Eisner, Joshua Goodman, and members
of the ARK group for helpful comments and feedback that
improved this paper. This research was supported by NSF
IIS-0836431 and an IBM faculty award.
A k-best+residual is a Semiring
In showing that k-best+residual is a semiring, we will restrict
our attention to the computation of the residuals. The com-
putation over proof lists is identical to that performed in the
k-best proof semiring, which was shown to be a semiring by
Goodman (1998). We sketch the proofs that ? is associative
and that ? distributes over ?; associativity of ? is straight-
forward.
For a proof list a?, ?a?? denotes the sum of proof scores,P
i:?ai,Ai??a?
ai. Note that:
?Res(a?)?+ ?max-k(a?)? = ?a?? (15)
?
?a? ? b?
?
? = ?a??
?
?b?
?
? (16)
Associativity. Given three semiring values u, v, and w, we
need to show that (u?v)?w = u?(v?w). After expand-
ing the expressions for the residuals using Eq. 14, there are
10 terms on each side, five of which are identical and cancel
out immediately. Three more cancel using Eq. 15, leaving:
LHS = ?Res(u? ? v?)? ?w??+ ?Res(max-k(u? ? v?) ? w?)?
RHS = ?u?? ?Res(v? ? w?)?+ ?Res(u? ? max-k(v? ? w?))?
If LHS = RHS, associativity holds. Using Eq. 15 again, we
can rewrite the second term in LHS to obtain
LHS = ?Res(u? ? v?)? ?w??+ ?max-k(u? ? v?) ? w??
? ?max-k(max-k(u? ? v?) ? w?)?
Using Eq. 16 and pulling out the common term ?w??, we have
LHS =(?Res(u? ? v?)?+ ?max-k(u? ? v?)?) ?w??
? ?max-k(max-k(u? ? v?) ? w?)?
= ?(u? ? v?) ? w?? ? ?max-k(max-k(u? ? v?) ? w?)?
= ?(u? ? v?) ? w?? ? ?max-k((u? ? v?) ? w?)?
The resulting expression is intuitive: the residual of (u?v)?
w is the difference between the sum of all proof scores and
the sum of the k-best. RHS can be transformed into this same
expression with a similar line of reasoning (and using asso-
ciativity of ?). Therefore, LHS = RHS and ? is associative.
Distributivity. To prove that ? distributes over ?, we must
show left-distributivity, i.e., thatu?(v?w) = (u?v)?(u?
w), and right-distributivity. We show left-distributivity here.
As above, we expand the expressions, finding 8 terms on the
LHS and 9 on the RHS. Six on each side cancel, leaving:
LHS = ?Res(v? ? w?)? ?u??+ ?Res(u? ? max-k(v? ? w?))?
RHS = ?Res(u? ? v?)?+ ?Res(u? ? w?)?
+ ?Res(max-k(u? ? v?) ?max-k(u? ? w?))?
We can rewrite LHS as:
LHS = ?Res(v? ? w?)? ?u??+ ?u? ? max-k(v? ? w?)?
? ?max-k(u? ? max-k(v? ? w?))?
= ?u?? (?Res(v? ? w?)?+ ?max-k(v? ? w?)?)
? ?max-k(u? ? max-k(v? ? w?))?
= ?u?? ?v? ? w?? ? ?max-k(u? ? (v? ? w?))?
= ?u?? ?v? ? w?? ? ?max-k((u? ? v?) ? (u? ? w?))?
where the last line follows because ? distributes over ?
(Goodman, 1998). We now work with the RHS:
RHS = ?Res(u? ? v?)?+ ?Res(u? ? w?)?
+ ?Res(max-k(u? ? v?) ?max-k(u? ? w?))?
= ?Res(u? ? v?)?+ ?Res(u? ? w?)?
+ ?max-k(u? ? v?) ?max-k(u? ? w?)?
? ?max-k(max-k(u? ? v?) ?max-k(u? ? w?))?
Since max-k(u? ? v?) and max-k(u? ? w?) are disjoint (we
assume no duplicates; i.e., two different theorems can-
not have exactly the same proof), the third term becomes
?max-k(u? ? v?)?+ ?max-k(u? ? w?)? and we have
= ?u? ? v??+ ?u? ? w??
? ?max-k(max-k(u? ? v?) ?max-k(u? ? w?))?
= ?u?? ?v??+ ?u?? ?w??
? ?max-k((u? ? v?) ? (u? ? w?))?
= ?u?? ?v? ? w?? ? ?max-k((u? ? v?) ? (u? ? w?))? .
325
References
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970.
A maximization technique occurring in the statis-
tical analysis of probabilistic functions of Markov
chains. Annals of Mathematical Statistics, 41(1).
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
W. W. Cohen and V. Carvalho. 2005. Stacked sequen-
tial learning. In Proc. of IJCAI.
A. Darwiche. 2003. A differential approach to infer-
ence in Bayesian networks. Journal of the ACM,
50(3).
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
piling Comp Ling: Practical weighted dynamic pro-
gramming and the Dyna language. In Proc. of HLT-
EMNLP.
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proc. of ACL.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005.
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In Proc.
of ACL.
J. Goodman. 1998. Parsing inside-out. Ph.D. thesis,
Harvard University.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25(4):573?605.
A. Griewank and G. Corliss. 1991. Automatic Differ-
entiation of Algorithms. SIAM.
L. Huang and D. Chiang. 2005. Better k-best parsing.
In Proc. of IWPT.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL.
L. Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul.
1999. An introduction to variational methods for
graphical models. Machine Learning, 37(2).
D. Klein and C. Manning. 2001. Parsing and hyper-
graphs. In Proc. of IWPT.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. of EMNLP.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Proc. of
ICGI.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
R. Levy, F. Reali, and T. Griffiths. 2008. Modeling the
effects of memory on human online sentence pro-
cessing with particle filters. In Advances in NIPS.
B. T. Lowerre. 1976. The Harpy Speech Recognition
System. Ph.D. thesis, Carnegie Mellon University.
D. J. C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labo-
ratory, Cambridge.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proc. of
EMNLP.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL.
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside
reestimation from partially bracketed corpora. In
Proc. of ACL, pages 128?135.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CoNLL.
S. Shieber, Y. Schabes, and F. Pereira. 1995. Principles
and implementation of deductive parsing. Journal of
Logic Programming, 24(1-2):3?36.
D. A. Smith and J. Eisner. 2008. Dependency parsing
by belief propagation. In Proc. of EMNLP.
N. A. Smith, D. L. Vail, and J. D. Lafferty. 2007. Com-
putationally efficient M-estimation of log-linear
structure models. In Proc. of ACL.
C. Sutton and A. McCallum. 2004. Collective seg-
mentation and labeling of distant entities in infor-
mation extraction. In Proc. of ICML Workshop on
Statistical Relational Learning and Its Connections
to Other Fields.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimal decoding algo-
rithm. IEEE Transactions on Information Process-
ing, 13(2).
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for QA. In Proc. of EMNLP-CoNLL.
326
Proceedings of the Third Workshop on Statistical Machine Translation, pages 9?17,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Rich Source-Side Context for Statistical Machine Translation
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We explore the augmentation of statistical ma-
chine translation models with features of the
context of each phrase to be translated. This
work extends several existing threads of re-
search in statistical MT, including the use of
context in example-based machine translation
(Carl and Way, 2003) and the incorporation of
word sense disambiguation into a translation
model (Chan et al, 2007). The context fea-
tures we consider use surrounding words and
part-of-speech tags, local syntactic structure,
and other properties of the source language
sentence to help predict each phrase?s transla-
tion. Our approach requires very little compu-
tation beyond the standard phrase extraction
algorithm and scales well to large data sce-
narios. We report significant improvements
in automatic evaluation scores for Chinese-
to-English and English-to-German translation,
and also describe our entry in the WMT-08
shared task based on this approach.
1 Introduction
Machine translation (MT) by statistical modeling of
bilingual phrases is one of the most successful ap-
proaches in the past few years. Phrase-based MT
systems are straightforward to train from parallel
corpora (Koehn et al, 2003) and, like the origi-
nal IBM models (Brown et al, 1990), benefit from
standard language models built on large monolin-
gual, target-language corpora (Brants et al, 2007).
Many of these systems perform well in competitive
evaluations and scale well to large-data situations
(NIST, 2006; Callison-Burch et al, 2007). End-to-
end phrase-based MT systems can be built entirely
from freely-available tools (Koehn et al, 2007).
We follow the approach of Koehn et al (2003),
in which we translate a source-language sentence f
into the target-language sentence e? that maximizes a
linear combination of features and weights:1
?e?, a?? = argmax
?e,a?
score(e,a,f) (1)
= argmax
?e,a?
M?
m=1
?mhm(e,a,f) (2)
where a represents the segmentation of e and f
into phrases and a correspondence between phrases,
and each hm is a R-valued feature with learned
weight ?m. The translation is typically found us-
ing beam search (Koehn et al, 2003). The weights
??1, ..., ?M ? are typically learned to directly mini-
mize a standard evaluation criterion on development
data (e.g., the BLEU score; Papineni et al, (2002))
using numerical search (Och, 2003).
Many features are used in phrase-based MT, but
nearly ubiquitous are estimates of the conditional
translation probabilities p(eji | f
`
k) and p(f
`
k | e
j
i )
for each phrase pair ?eji ,f
l
k? in the candidate sen-
tence pair.2 In this paper, we add and evaluate fea-
1In the statistical MT literature, this is often referred to as a
?log-linear model,? but since the score is normalized during nei-
ther parameter training nor decoding, and is never interpreted as
a log-probability, it is essentially a linear combination of feature
functions. Since many of the features are actually probabilities,
this linear combination is closer to a mixture model.
2We will use xji to denote the subsequence of x containing
the ith through jth elements of x, inclusive.
9
tures that condition on additional context features on
the source (f ) side:
p(eji | Phrase = f
`
k,Context = ?f
k?1
1 ,f
F
`+1, ...?)
The advantage of considering context is well-
known and exploited in the example-basedMT com-
munity (Carl and Way, 2003). Recently researchers
have begun to use source phrase context informa-
tion in statistical MT systems (Stroppa et al, 2007).
Statistical NLP researchers understand that condi-
tioning a probability model on more information is
helpful only if there are sufficient training data to ac-
curately estimate the context probabilities.3 Sparse
data are often the death of elaborate models, though
this can be remedied through careful smoothing.
In this paper we leverage the existing linear
model (Equation 2) to bring source-side context into
phrase-based MT in a way that is robust to data
sparseness. We interpret the linear model as a mix-
ture of many probability estimates based on different
context features, some of which may be very sparse.
The mixture coefficients are trained in the usual way
(?minimum error-rate training,? Och, 2003), so that
the additional context is exploited when it is useful
and ignored when it isn?t.
The paper proceeds as follows. We first review re-
lated work that enriches statistical translation mod-
els using context (?2). We then propose a set
of source-side features to be incorporated into the
translation model, including the novel use of syntac-
tic context from source-side parse trees and global
position within f (?3). We explain why analogous
target-side features pose a computational challenge
(?4). Specific modifications to the standard training
and evaluation paradigm are presented in ?5. Exper-
imental results are reported in ?6.
2 Related Work
Stroppa et al (2007) added souce-side context fea-
tures to a phrase-based translation system, including
conditional probabilities of the same form that we
use. They consider up to two words and/or POS tags
of context on either side. Because of the aforemen-
tioned data sparseness problem, they use a decision-
3An illustrative example is the debate over the use of bilex-
icalized grammar rules in statistical parsing (Gildea, 2001;
Bikel, 2004).
tree classifier that implicitly smooths relative fre-
quency estimates. The method improved over a stan-
dard phrase-based baseline trained on small amounts
of data (< 50K sentence pairs) for Italian? English
and Chinese ? English. We explore a significantly
larger space of context features, a smoothing method
that more naturally fits into the widely used, error-
driven linear model, and report a more comprehen-
sive experimental evaluation (including feature com-
parison and scaling up to very large datasets).
Recent research on the use of word-sense dis-
ambiguation in machine translation also points to-
ward our approach. For example, Vickrey et al
(2005) built classifiers inspired by those used in
word sense disambiguation to fill in blanks in
a partially-completed translation. Gime?nez and
Ma`rquez (2007) extended the work by considering
phrases and moved to full translation instead of fill-
ing in target-side blanks. They trained an SVM for
each source language phrase using local features of
the sentences in which the phrases appear. Carpuat
and Wu (2007) and Chan et al (2007) embedded
state-of-the-art word sense disambiguation modules
into statistical MT systems, achieving performance
improvements under several automatic measures for
Chinese ? English translation.
Our approach is also reminiscent of example-
based machine translation (Nagao, 1984; Somers,
1999; Carl and Way, 2003), which has for many
years emphasized use of the context in which source
phrases appear when translating them. Indeed, like
the example-based community, we do not begin with
any set of assumptions about which kinds of phrases
require additional disambiguation (cf. the applica-
tion of word-sense disambiguation, which is moti-
vated by lexical ambiguity). Our feature-rich ap-
proach is omnivorous and can exploit any linguistic
analysis of an input sentence.
3 Source-Side Context Features
Adding features to the linear model (Equation 2)
that consider more of the source sentence requires
changing the decoder very little, if at all. The reason
is that the source sentence is fully observed, so the
information to be predicted is the same as before?
the difference is that we are using more clues to
carry out the prediction.
10
We see this as an opportunity to include many
more features in phrase-based MT without increas-
ing the cost of decoding at runtime. This discussion
is reminiscent of an advantage gained by moving
from hidden Markov models to conditional random
fields for sequence labeling tasks. While the same
core algorithm is used for decoding with both mod-
els, a CRF allows inclusion of features that consider
the entire observed sequence?i.e., more of the ob-
servable context of each label to be predicted. Al-
though this same advantage was already obtained
in statistical MT through the transition from ?noisy
channel? translation models to (log-)linear models,
the customary set of features used in most phrase-
based systems does not take full advantage of the
observed data.
The standard approach to estimating the phrase
translation conditional probability features is via rel-
ative frequencies (here e and f are phrases):
p(e | f) =
count(e,f)
?
e? count(e
?,f)
Our new features all take the form p(e |
f ,f context), where e is the target language phrase,
f is the source language phrase, and f context is the
context of the source language phrase in the sentence
in which it was observed. Like the context-bare con-
ditional probabilities, we estimate probability fea-
tures using relative frequencies:
p(e | f ,f context) =
count(e,f ,f context)?
e? count(e
?,f ,f context)
Since we expect that adding conditioning vari-
ables will lead to sparser counts and therefore more
zero estimates, we compute features for many dif-
ferent types of context. To combine the many
differently-conditioned features into a single model,
we provide them as features to the linear model
(Equation 2) and use minimum error-rate training
(Och, 2003) to obtain interpolation weights ?m.
This is similar to an interpolation of backed-off es-
timates, if we imagine that all of the different con-
texts are differently-backed off estimates of the com-
plete context. The error-driven weight training ef-
fectively smooths one implicit context-rich estimate
p(e | f ,f context) so that all of the backed-off es-
timates are taken into account, including the orig-
inal p(e | f). Our approach is asymmetrical; we
have not, for example, estimated features of the form
p(f ,f context | e).
We next discuss the specific source-side context
features used in our model.
3.1 Lexical Context Features
The most obvious kind of context of a source phrase
f `k is the m-length sequence before it (f
k?1
k?m) and
the m-length sequence after it (f `+m`+1 ). We include
context features for m ? {1, 2}, padding sentences
with m special symbols at the beginning and at the
end. For each value of m, we include three features:
? p(e | f ,fk?1k?m), the left lexical context;
? p(e | f ,f `+m`+1 ), the right lexical context;
? p(e | f ,fk?1k?m,f
`+m
`+1 ), both sides.
3.2 Shallow Syntactic Features
Lexical context features, especially when m > 1,
are expected to be sparse. Representing the context
by part-of-speech (POS) tags is one way to over-
come that sparseness. We used the same set of the
lexical context features described above, but with
POS tags replacing words in the context. We also
include a feature which conditions on the POS tag
sequence of the actual phrase being translated.
3.3 Syntactic Features
If a robust parser is available for the source lan-
guage, we can include context features from parse
trees. We used the following parse tree features:
? Is the phrase (exactly) a constituent?
? What is the nonterminal label of the lowest node
in the parse tree that covers the phrase?
? What is the nonterminal label or POS of the high-
est nonterminal node that ends immediately be-
fore the phrase? Begins immediately after the
phrase?
? Is the phrase strictly to the left of the root word,
does it contain the root word, or is it strictly to
the right of the root word? (Requires a parse with
head annotations.)
We also used a feature that conditions on both fea-
tures in the third bullet point above.
11
S[support] Syntactic Features:
Not a constituent
NP covers phrase
VBP to left
PP to right
Right of root word
PP
Shallow
Lexical
Phrase
PP
NP
VP
IN     DT       NN        ,  PRP VBP     DT   NN   IN  DT      NN         IN     NN     CC    NN    , ...
NP
SBAR
NPNPNP
NP
..
...
.
...
Positional Features:
Not at start
Not at end
Second fifth of sentence
Covers 18.5% of sentence
  (quantized to 20%)
Against this background ,  we   support   the report  of  the committee   on transport and tourism , which...
In dieser Hinsicht unterst?tzen wir   den Bericht des Ausschusses   f?r Verkehr und Fremdenverkehr , in...
Syntactic
NP
PP
Figure 1: A (partial) sentence pair from the WMT-07 Europarl training corpus. Processing of the data (parsing, word
alignment) was done as discussed in ?6. The phrase pair of interest is boxed and context features are shown in dotted
shapes. The context features help determine whether the phrase should be translated as ?der Bericht des Ausschusses?
(nominative case) or ?den Bericht des Ausschusses? (accusative case). See text for details.
3.4 Positional Features
We include features based on the position of the
phrase in the source sentence, the phrase length, and
the sentence length. These features use information
from the entire source sentence, but are not syntac-
tic. For a phrase f `k in a sentence f of length n:
? Is the phrase at the start of the sentence (k = 1)?
? Is the phrase at the end of the sentence (` = n)?
? A quantization of r =
k+ `?k+12
n , the relative po-
sition in (0, 1) of the phrase?s midpoint within f .
We choose the smallest q ? {0.2, 0.4, 0.6, 0.8, 1}
such that q > r.
? A quantization of c = `?k+1n , the fraction of the
words in f that are covered by the phrase. We
choose the smallest q ? { 140 ,
1
20 .
1
10 ,
1
5 ,
1
3 , 1} such
that q > c.
An illustration of the context features is shown
in Fig. 1. Consider the phrase pair ?the report
of the committee?/?den Bericht des Ausschusses?
extracted by our English ? German baseline MT
system (described in ?6.3). The German word
?Bericht? is a masculine noun; therefore, it takes the
article ?der? in the nominative case, ?den? in the ac-
cusative case, and ?dem? in the dative case. These
three translations are indeed available in the phrase
table for ?the report of the committee? (see Table 1,
?no context? column), with relatively high entropy.
The choice between ?den? and ?der? must be made
by the language model alone.
Knowing that the phrase follows a verb, or ap-
pears to the right of the sentence?s root word, or
within the second fifth of the sentence should help.
Indeed, a probability distribution that conditions on
context features gives more peaked distributions that
give higher probability to the correct translation,
given this context, and lower probability given some
other contexts (see Table 1).
4 Why Not Target-Side Context?
While source context is straightforward to exploit
in a model, including target-side context features
breaks one of the key independence assumptions
made by phrase-based translation models: the trans-
lations of the source-side phrases are conditionally
independent of each other, given f , thereby requir-
ing new algorithms for decoding (Marino et al,
2006).
We suggest that target-side context may already
be well accounted for in current MT systems. In-
deed, language models pay attention to the local
context of phrases, as do reordering models. The re-
cent emphasis on improving these components of a
translation system (Brants et al, 2007) is likely due
in part to the widespread availability of NLP tools
for the language that is most frequently the target:
English. We will demonstrate that NLP tools (tag-
12
Shallow: 2 POS on left Syntax: of root Positional: rel. pos.
g no context ??PRP VBP? ?VBN IN? ?right left ?2nd fifth 1st fifth
den bericht des ausschusses 0.3125 1.0000 0.3333 0.5000 0.0000 0.6000 0.0000
der bericht des ausschusses 0.3125 0.0000 0.0000 0.1000 0.6667 0.2000 0.6667
dem bericht des ausschusses 0.2500 0.0000 0.6667 0.3000 0.1667 0.0000 0.1667
Table 1: Phrase table entries for ?the report of the committee? and their scores under different contexts. These are the
top three phrases in the baseline English ? German system (?no context? column). Contexts from the source sentence
in Fig. 1 (starred) predict correctly; we show also alternative contexts that give very different distributions.
gers and parsers) for the source side can be used to
improve the translation model, exploiting analysis
tools for other languages.
5 Implementation
The additional data required to compute the context
features is extracted along with the phrase pairs dur-
ing execution of the standard phrase extraction algo-
rithm, affecting phrase extraction and scoring time
by a constant factor. We avoid the need to modify
the standard phrase-based decoder to handle context
features by appending a unique identifier to each to-
ken in the sentences to be translated. Then, we pre-
compute a phrase table for the phrases in these sen-
tences according to the phrase contexts. To avoid
extremely long lists of translations of common to-
kens, we filter the generated phrase tables, remov-
ing entries for which the estimate of p(e | f) < c,
for some small c. In our experiments, we fixed
c = 0.0002. This filtering reduced time for exper-
imentation dramatically and had no apparent effect
on the translation output. We did not perform any
filtering for the baseline system.
6 Experiments
In this section we present experimental results using
our context-endowed phrase translation model with
a variety of different context features, on Chinese ?
English, German ? English, and English ? Ger-
Chinese ? English (UN)
Context features BLEU NIST METEOR
None 0.3715 7.918 0.6486
Lexical 0.4030 8.367 0.6716
Shallow 0.3807 7.981 0.6523
Lexical + Shallow 0.4030 8.403 0.6703
Syntactic 0.3823 7.992 0.6531
Positional 0.3775 7.958 0.6510
Table 2: Chinese ? English experiments: training and
testing on UN data. Boldface marks scores significantly
higher than ?None.?
man translation tasks. Dataset details are given in
Appendices A (Chinese) and B (German).
Baseline We use the Moses MT system (Koehn et
al., 2007) as a baseline and closely follow the exam-
ple training procedure given for the WMT-07 and
WMT-08 shared tasks.4 In particular, we perform
word alignment in each direction using GIZA++
(Och and Ney, 2003), apply the ?grow-diag-final-
and? heuristic for symmetrization and use a max-
imum phrase length of 7. In addition to the two
phrase translation conditionals p(e | f) and p(f |
e), we use lexical translation probabilities in each
direction, a word penalty, a phrase penalty, a length-
based reordering model, a lexicalized reordering
model, and an n-gram language model, SRILM im-
plementation (Stolcke, 2002) with modified Kneser-
Ney smoothing (Chen and Goodman, 1998). Mini-
mum error-rate (MER) training (Och, 2003) was ap-
plied to obtain weights (?m in Equation 2) for these
features. A recaser is trained on the target side of the
parallel corpus using the script provided withMoses.
All output is recased and detokenized prior to evalu-
ation.
Evaluation We evaluate translation output using
three automatic evaluation measures: BLEU (Pap-
ineni et al, 2002), NIST (Doddington, 2002), and
METEOR (Banerjee and Lavie, 2005, version 0.6).5
All measures used were the case-sensitive, corpus-
level versions. The version of BLEU used was that
provided by NIST. Significance was tested using a
paired bootstrap (Koehn, 2004) with 1000 samples
(p < 0.05).6
4http://www.statmt.org/wmt08
5METEOR details: For English, we use exact matching,
Porter stemming, and WordNet synonym matching. For Ger-
man, we use exact matching and Porter stemming. These are the
same settings that were used to evaluate systems for the WMT-
07 shared task.
6Code implementing this test for these metrics can be freely
downloaded at http://www.ark.cs.cmu.edu/MT.
13
Chinese ? English
Testing on UN Testing on News (NIST 2005)
Context features BLEU NIST METEOR BLEU NIST METEOR
Training on in-domain data only:
None 0.3715 7.918 0.6486 0.2700 7.986 0.5314
Training on all data:
None 0.3615 7.797 0.6414 0.2593 7.697 0.5200
Lexical 0.3898 8.231 0.6697 0.2522 7.852 0.5273
Shallow: ? 1 POS tag 0.3611 7.713 0.6430 0.2669 8.243 0.5526
Shallow: ? 2 POS tags 0.3657 7.808 0.6455 0.2591 7.843 0.5288
Lexical + Shallow 0.3886 8.245 0.6675 0.2628 7.881 0.5290
Syntactic 0.3717 7.899 0.6531 0.2653 8.123 0.5403
Lexical + Syntactic 0.3926 8.224 0.6636 0.2572 7.774 0.5234
Positional 0.3647 7.766 0.6469 0.2648 7.891 0.5275
All 0.3772 8.176 0.6582 0.2566 7.775 0.5225
Feature selection (see Sec. 6.4) 0.3843 8.079 0.6594 0.2730 8.059 0.5343
Table 3: Chinese ? English experiments: first row shows baseline performance when training only on in-domain
data for each task; all other rows show results when training on all data (UN and News). Left half shows results when
tuning and testing on UN test sets; right half shows results when tuning on NIST 2004 News test set and testing on
NIST 2005. For feature selection, an additional set of unseen data was used: 2000 held-out sentences from the UN
data for the left half and the NIST 2003 test set for the right half. Boldface marks scores that are significantly higher
than the first row, in-domain baseline.
6.1 Chinese ? English
For our Chinese ? English experiments, two kinds
of data were used: UN proceedings, and newswire
as used in NIST evaluations.
UN Data UN data results are reported in Ta-
ble 2. Significant improvements are obtained on all
three evaluation measures?e.g., more than 3 BLEU
points?using lexical or lexical and shallow fea-
tures. While improvements are smaller for other fea-
tures and feature combinations, performance is not
harmed by conditioning on context features. Note
that using syntactic features gave 1 BLEU point of
improvement.
News Data In News data experiments, none of our
features obtained BLEU performance statistically
distinguishable from the baseline of 0.2700 BLEU
(neither better, nor worse). The News training cor-
pus is less than half the size of the UN training cor-
pus (in words); unsurprisingly, the context features
were too sparse to be helpful. Further, newswire are
less formulaic and repetitive than UN proceedings,
so contexts do not generalize as well from training
to test data. Fortunately, our ?error-minimizing mix-
ture? approach protects the BLEU score, which the
?m are tuned to optimize.
Combined UN + News Data Our next experi-
ment used all of the available training data (> 200M
words on each side) to train the models, in-domain
?m tuning, and testing for each domain separately;
see Table 3. Without context features, training
on mixed-domain data consistently harms perfor-
mance. With contexts that include lexical features,
the mixed-domain model significantly outperforms
the in-domain baseline for UN data. These results
suggest that context features enable better use of out-
of-domain data, an important advantage for statis-
tical MT since parallel data often arise from very
different sources than those of ?real-world? transla-
tion scenarios. On News data, context features did
not give a significant advantage on the BLEU score,
though syntactic and ? 1 POS contexts did give sig-
nificant NIST and METEOR improvements over the
in-domain baseline.
6.2 German ? English
We do not report full results for this task, because
the context features neither helped nor hurt perfor-
mance significantly. We believe this is due to data
sparseness resulting from the size of the training cor-
pus (26M German words), German?s relatively rich
morphology, and the challenges of German parsing.
14
English ? German
Context features BLEU NIST METEOR
None 0.2069 6.020 0.2811
Lexical 0.2018 6.031 0.2772
Shallow 0.2017 5.911 0.2748
Syntactic 0.2077 6.049 0.2829
Positional 0.2045 5.930 0.2772
Lex. + Shal. + Syn. 0.2045 6.061 0.2817
All 0.2053 6.009 0.2797
Feature selection 0.2080 6.009 0.2807
Table 4: English ? German experiments: training
and testing on Europarl data. WMT-07 Europarl parallel
training data was used for training, dev06 was used for
tuning, devtest06 was used for feature selection and de-
velopmental testing, and test07 was used for final testing.
Boldface marks scores significantly higher than ?None.?
6.3 English ? German
English ? German results are shown in Table 4.
The baseline system here is highly competitive, hav-
ing scored higher on automatic evaluation measures
than any other system in the WMT-07 shared task
(Callison-Burch et al, 2007). Though most results
are not statistically significant, small improvements
do tend to come from syntactic context features.
Comparing with the German? English experiment,
we attribute this effect to the high accuracy of the
English parser compared to the German parser.
6.4 Feature Selection
Translation performance does not always increase
when features are added to the model. This mo-
tivates the use of feature selection algorithms to
choose a subset of features to optimize perfor-
mance. We experimented with several feature se-
lection algorithms based on information-theoretic
quantities computed among the source phrase, the
target phrase, and the context, but found that a sim-
ple forward variable selection algorithm (Guyon and
Elisseeff, 2003) worked best. In this procedure, we
start with no context features and, at each iteration,
add the single feature that results in the largest in-
crease in BLEU score on an unseen development
set after ?m tuning. The algorithm terminates if no
features are left or if none result in an increase in
BLEU. We ran this algorithm to completion for the
two Chinese? English tune/test sets (training on all
data in each case) and the English ? German task;
see Tables 3 and 4. In all cases, the algorithm fin-
ishes after ? 4 iterations.
Feature selection for Chinese ? English (UN)
first chose the lexical feature ?1 word on each side,?
then the positional feature indicating which fifth of
the sentence contains the phrase, and finally the lex-
ical feature ?1 word on right.? For News, the fea-
tures chosen were the shallow syntactic feature ?1
POS on each side,? then the positional beginning-
of-sentence feature, then the position relative to the
root (a syntactic feature). For English ? German,
the shallow syntactic feature ?2 POS on left,? then
the lexical feature ?1 word on right? were selected.
In the case where context features were most
helpful (Chinese ? English UN data), we found
feature selection to be competitive at 2.28 BLEU
points above the no-context baseline, but not the best
achieved. In the other two cases (Chinese? English
News and English ? German Europarl), our best
results were achieved using these automatically se-
lected features, and in the Chinese ? English News
case, improvements on all three scores (including
1.37 BLEU points) are significant compared to the
no-context baseline trained on the same data.
6.5 WMT-08 Shared Task: English ? German
Since we began this research before the release
of the data for the WMT-08 shared task, we per-
formed the majority of our experiments using the
data released for the WMT-07 shared task (see Ap-
pendix B). To prepare our entry for the 2008 shared
task, we trained a baseline system on the 2008 data
using a nearly identical configuration.7 Table 5 com-
pares performance of the baseline system (with no
context features) to performance with the two con-
text features chosen automatically as described in
?6.4. In addition to the devtest06 data, we report re-
sults on the 2007 and 2008 Europarl test sets. Most
improvements were statistically significant.
7 Future Work
In future work, we plan to apply more sophisticated
learning algorithms to rich-feature phrase table esti-
mation. Context features can also be used as condi-
tioning variables in other components of translation
7The only differences were the use of a larger max sentence
length threshold of 55 tokens instead of 50, and the use of the
better-performing ?englishFactored? Stanford parser model.
15
devtest06 test07 test08
System BLEU NIST METEOR BLEU NIST METEOR BLEU NIST METEOR
Baseline 0.2009 5.866 0.2719 0.2051 5.957 0.2782 0.2003 5.889 0.2720
Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772
Table 5: English ? German shared task system results using WMT-08 Europarl parallel data for training, dev06 for
tuning, and three test sets, including the final 2008 test set. The row labeled ?Context? uses the top-performing feature
set {2 POS on left, 1 word on right}. Boldface marks scores that are significantly higher than the baseline.
models, including the lexicalized reordering model
and the lexical translation model in the Moses MT
system, or hierarchical or syntactic models (Chiang,
2005). Additional linguistic analysis (e.g., morpho-
logical disambiguation, named entity recognition,
semantic role labeling) can be used to define new
context features.
8 Conclusion
We have described a straightforward, scalable
method for improving phrase translation models by
modeling features of a phrase?s source-side context.
Our method allows incorporation of features from
any kind of source-side annotation and barely affects
the decoding algorithm. Experiments show perfor-
mance rivaling or exceeding strong, state-of-the-art
baselines on standard translation tasks. Automatic
feature selection can be used to achieve performance
gains with just two or three context features. Per-
formance is strongest when large in-domain training
sets and high-accuracy NLP tools for the source lan-
guage are available.
Acknowledgments
This research was supported in part by an ARCS
award to the first author, NSF IIS-0713265, su-
percomputing resources provided by Yahoo, and a
Google grant. We thank Abhaya Agarwal, Ashish
Venugopal, and Andreas Zollmann for helpful con-
versations and Joy Zhang for his Chinese segmenter.
We also thank the anonymous reviewers for helpful
comments.
A Dataset Details (Chinese-English)
We trained on data from the NIST MT 2008 con-
strained Chinese-English track: Hong Kong
Hansards and news (LDC2004T08), Sino-
rama (LDC2005T10), FBIS (LDC2003E14),
Xinhua (LDC2002E18), and financial news
(LDC2006E26)?total 2.5M sents., 66M Chinese
words, 68M English. For news experiments, the
newswire portion of the NIST 2004 test set was used
for tuning, the full NIST 2003 test set was used for
developmental testing and feature selection, and the
NIST 2005 test set was used for testing (900-1000
sents. each). We also used the United Nations paral-
lel text (LDC2004E12), divided into training (4.7M
sents.; words: 136M Chinese, 144M English),
tuning (2K sents.), and test sets (2K sents.). We
removed sentence pairs where either side was longer
than 80 words, segmented all Chinese text automat-
ically,8 and parsed/tagged using the Stanford parser
with the pre-trained ?xinhuaPCFG? model (Klein
and Manning, 2003). Trigram language models
were trained on the English side of the parallel
corpus along with approximately 115M words from
the Xinhua section of the English Gigaword corpus
(LDC2005T12), years 1995?2000 (total 326M
words).
B Dataset Details (German-English)
For German ? English experiments, we used data
provided for the WMT-07 shared task (1.1M sents.,
26M German words, 27M English). We used dev06
for tuning, devtest06 for feature selection and de-
velopmental testing, and test07 for final testing
(2K sents. each). We removed sentence pairs
where either side was longer than 50 words and
parsed/tagged the German and English data using
the Stanford parser (Klein andManning, 2003) (with
pre-trained ?germanFactored? and ?englishPCFG?
models). 5-gram language models were trained on
the entire target side of the parallel corpus (37M
German words, 38M English).
8Available at http://projectile.is.cs.cmu.
edu/research/public/tools/segmentation/
lrsegmenter/lrSegmenter.perl.
16
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proc. of ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization.
D. M. Bikel. 2004. A distributional analysis of a lexical-
ized statistical parsing model. In Proc. of EMNLP.
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine translation.
In Proc. of EMNLP-CoNLL.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85.
C. Callison-Burch, P. Koehn, C. Fordyce, and C. Monz,
editors. 2007. Proc. of the 2nd Workshop on Statisti-
cal Machine Translation.
M. Carl and A.Way. 2003. Recent Advances in Example-
Based Machine Translation. Kluwer Academic.
M. Carpuat and D. Wu. 2007. Improving statistical ma-
chine translation using word sense disambiguation. In
Proc. of EMNLP-CoNLL.
Y. Chan, H. Ng, and D. Chiang. 2007. Word sense dis-
ambiguation improves statistical machine translation.
In Proc. of ACL.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. of HLT.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
J. Gime?nez and L. Ma`rquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proc. of the 2ndWorkshop on Statistical
Machine Translation.
I. Guyon and A. Elisseeff. 2003. An introduction to vari-
able and feature selection. Journal of Machine Learn-
ing Research, 3:1157?1182.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proc. of HLT-NAACL,
pages 127?133.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL demonstration
session.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
Jose? B. Marino, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrik Lambert, Jose? A. R. Fonollosa, and
Marta R. Costa-jussa`. 2006. N -gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
M. Nagao. 1984. A framework of a mechanical trans-
lation between Japanese and English by analogy prin-
ciple. In Proc. of the International NATO Symposium
on Artificial and Human Intelligence. Elsevier North-
Holland, Inc.
NIST. 2006. NIST 2006 machine translation evaluation
official results.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
H. Somers. 1999. Review article: Example-based ma-
chine translation. Machine Translation, 14(2).
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In Proc. of TMI.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation.
In Proc. of HLT-EMNLP.
17
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 474?485,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Quasi-Synchronous Phrase Dependency Grammars
for Machine Translation
Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon Univeristy
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We present a quasi-synchronous dependency
grammar (Smith and Eisner, 2006) for ma-
chine translation in which the leaves of the
tree are phrases rather than words as in pre-
vious work (Gimpel and Smith, 2009). This
formulation allows us to combine structural
components of phrase-based and syntax-based
MT in a single model. We describe a method
of extracting phrase dependencies from paral-
lel text using a target-side dependency parser.
For decoding, we describe a coarse-to-fine ap-
proach based on lattice dependency parsing of
phrase lattices. We demonstrate performance
improvements for Chinese-English and Urdu-
English translation over a phrase-based base-
line. We also investigate the use of unsuper-
vised dependency parsers, reporting encourag-
ing preliminary results.
1 Introduction
Two approaches currently dominate statistical ma-
chine translation (MT) research. Phrase-based mod-
els (Koehn et al, 2003) excel at capturing local
reordering phenomena and memorizing multi-word
translations. Models that employ syntax or syntax-
like representations (Chiang, 2005; Galley et al,
2006; Zollmann and Venugopal, 2006; Huang et al,
2006) handle long-distance reordering better than
phrase-based systems (Auli et al, 2009) but often re-
quire constraints on the formalism or rule extraction
method in order to achieve computational tractabil-
ity. As a result, certain instances of syntactic diver-
gence are more naturally handled by phrase-based
systems (DeNeefe et al, 2007).
In this paper we present a new way of combin-
ing the advantages of phrase-based and syntax-based
MT. We propose a model in which phrases are orga-
nized into a tree structure inspired by dependency
syntax. Instead of standard dependency trees in
which words are vertices, our trees have phrases as
vertices. We describe a simple heuristic to extract
phrase dependencies from an aligned parallel cor-
pus parsed on the target side, and use them to com-
pute target-side tree features. We define additional
string-to-tree features and, if a source-side depen-
dency parser is available, tree-to-tree features to cap-
ture properties of how phrase dependencies interact
with reordering.
To leverage standard phrase-based features along-
side our novel features, we require a formalism
that supports flexible feature combination and effi-
cient decoding. Quasi-synchronous grammar (QG)
provides this backbone (Smith and Eisner, 2006);
we describe a coarse-to-fine approach for decod-
ing within this framework, advancing substantially
over earlier QG machine translation systems (Gim-
pel and Smith, 2009). The decoder involves generat-
ing a phrase lattice (Ueffing et al, 2002) in a coarse
pass using a phrase-based model, followed by lat-
tice dependency parsing of the phrase lattice. This
approach allows us to feasibly explore the combined
search space of segmentations, phrase alignments,
and target phrase dependency trees.
Our experiments demonstrate an average im-
provement of +0.65 BLEU in Chinese-English
translation across three test sets and an improvement
of +0.75 BLEU in Urdu-English translation over
a phrase-based baseline. We also describe experi-
ments in which we replace supervised dependency
parsers with unsupervised parsers, reporting promis-
ing results: using a supervised Chinese parser and
a state-of-the-art unsupervised English parser pro-
vides our best results, giving an averaged gain of
+0.79 BLEU over the baseline. We also discuss how
our model improves translation quality and discuss
future possibilities for combining approaches to ma-
474
chine translation using our framework.
2 Related Work
We previously applied quasi-synchronous grammar
to machine translation (Gimpel and Smith, 2009),
but that system performed translation fundamentally
at the word level. Here we generalize that model to
function on phrases, enabling a tighter coupling be-
tween the phrase segmentation and syntactic struc-
tures. We also present a decoder efficient enough to
scale to large data sets and present performance im-
provements in large-scale experiments over a state-
of-the-art phrase-based baseline.
Aside from QG, there have been many efforts
to use dependency syntax in machine translation.
Quirk et al (2005) used a source-side dependency
parser and projected automatic parses across word
alignments in order to model dependency syntax on
phrase pairs. Shen et al (2008) presented an exten-
sion to Hiero (Chiang, 2005) in which rules have
target-side dependency syntax and therefore enable
the use of a dependency language model.
More recently, researchers have sought the bene-
fits of dependency syntax while preserving the ad-
vantages of phrase-based models, such as efficiency
and coverage. Galley and Manning (2009) loos-
ened standard assumptions about dependency pars-
ing so that the efficient left-to-right decoding pro-
cedure of phrase-based translation could be retained
while a dependency language model is incorporated.
Carreras and Collins (2009) presented a string-to-
dependency system that permits non-projective de-
pendency trees (thereby allowing a larger space of
translations) and use a rule extraction procedure that
includes rules for every phrase in the phrase table.
We take an additional step in this direction by
working with dependency grammars on the phrases
themselves, thereby bringing together the structural
components of phrase-based and dependency-based
MT in a single model. While others have worked
on combining rules from multiple syntax-based sys-
tems (Liu et al, 2009) or using posteriors from mul-
tiple models to score translations (DeNero et al,
2010), we are not aware of any other work that seeks
to directly integrate phrase-based and syntax-based
machine translation at the modeling level.1
1Dymetman and Cancedda (2010) present a formal analy-
3 Model
Given a sentence s and its dependency tree ?s,
we formulate the translation problem as finding the
target sentence t?, the segmentation ?? of s into
phrases, the segmentation ?? of t? into phrases, the
dependency tree ??? on the target phrases ??, and the
one-to-one phrase alignment a? such that
?t?,??,??, ???,a??= argmax
?t,?,?,??,a?
p(t,?,?, ??,a |s, ?s)
We use a linear model (Och and Ney, 2002):
p(t,?,?, ??,a | s, ?s) ?
exp{?>g(s, ?s, t,?,?, ??,a)}
where g is a vector of arbitrary feature functions on
the full set of structures and ? holds corresponding
feature weights. Table 1 summarizes our notation.
In modeling p(t,?,?, ??,a | s, ?s), we make
use of quasi-synchronous grammar (QG; Smith
and Eisner, 2006). Given a source sentence and
its parse, a QG induces a probabilistic monolingual
grammar over sentences ?inspired? by the source
sentence and tree. We denote this grammar byGs,?s ;
its (weighted) language is the set of translations of s.
Quasi-synchronous grammar makes no restric-
tions on the form of the target monolingual gram-
mar, though dependency grammars have been used
in most previous applications of QG (Wang et al,
2007; Das and Smith, 2009; Smith and Eisner,
2009), including previous work in MT (Smith and
Eisner, 2006; Gimpel and Smith, 2009). We pre-
viously presented a word-based machine translation
model based on a quasi-synchronous dependency
grammar. However, it is well-known in the MT com-
munity that translation quality is improved when
larger units are modeled. Therefore, we use a de-
pendency grammar in which the leaves are phrases
rather than words.
We define a phrase dependency grammar as a
model p(?, ??|t) over the joint space of segmen-
tations of a sentence into phrases and dependency
trees on the phrases.2 Phrase dependency grammars
sis of the problem of intersecting phrase-based and hierarchical
translation models, but do not provide experimental results.
2We restrict our attention to projective trees in this paper,
but the generalization to non-projective trees is easily made.
475
s = ?s1, . . . , sn? source language sentence
t = ?t1, . . . , tm? target language sentence, translation of s
? = ??1, . . . , ?n?? segmentation of s into phrases
?i, ?i = ?sj , . . . , sk? s.t. ?1 ? . . . ? ?n? = s
? = ??1, . . . , ?m?? segmentation of t into phrases
?i, ?i = ?tj , . . . , tk? s.t. ?1 ? . . . ? ?m? = t
?s : {1, . . . , n} ? {0, . . . , n} dependency tree on source words s, where ?s(i) is the index of
the parent of word si (0 is the root, $)
?? : {1, . . . ,m?} ? {0, . . . ,m?} dependency tree on target phrases ?, where ??(i) is the index of
the parent of phrase ?i
a : {1, . . . ,m?} ? {1, . . . , n?} one-to-one alignment from phrases in ? to phrases in ?
? = ??,?? parameters of the full model (? = phrase-based, ? = QPDG)
Table 1: Key notation.
have recently been used by Wu et al (2009) for fea-
ture extraction for opinion mining. When used for
translation modeling, they allow us to capture phe-
nomena like local reordering and idiomatic transla-
tions within each phrase as well as long-distance re-
lationships among the phrases in a sentence.
We then define a quasi-synchronous phrase
dependency grammar (QPDG) as a conditional
model p(t,?,?, ??,a | s, ?s) that induces a prob-
abilistic monolingual phrase dependency grammar
over sentences inspired by the source sentence and
(lexical) dependency tree. The source and tar-
get sentences are segmented into phrases and the
phrases are aligned in a one-to-one alignment.
We note that we actually depart here slightly from
the original definition of QG. The alignment variable
in QG links target tree nodes to source tree nodes.
However, we never commit to a source phrase de-
pendency tree, instead using a source lexical depen-
dency tree output by a dependency parser, so our
alignment variable a is a function from target tree
nodes (phrases in ?) to source phrases in ?, which
might not be source tree nodes. The features in our
model may consider a large number of source phrase
dependency trees as long as they are consistent with
?s.
4 Features
Our model contains all of the standard phrase-based
features found in systems like Moses (Koehn et al,
2007), including four phrase table probability fea-
tures, a phrase penalty feature, an n-gram language
model, a distortion cost, six lexicalized reordering
features, and a word penalty feature.
We now describe in detail the additional features
$? said : $? we should
$? said that $? has been
$? is a - us? relations
$? will be $? he said
$? it is cross - strait? relations
$? this is $? pointed out that
$? we must , and? is
the? united states the chinese? government
the? development of $? is the
the two? countries $? said ,
he? said : one - china? principle
$? he said : sino - us? relations
Table 2: Most frequent phrase dependencies with at least
2 words in one of the phrases (dependencies in which one
phrase is entirely punctuation are not shown). $ indicates
the root of the tree.
in our model that are used to score phrase depen-
dency trees. We shall refer to these as QPDG
features and will find it useful later to notation-
ally distinguish their feature weights from those of
the phrase-based model. We use ? for weights of
the standard phrase-based model features and ? for
weights of the QPDG features. We include three cat-
egories of features, differentiated by what pieces of
structure they consider.
4.1 Target Tree Features
We first include features that only consider t, ?,
and ??. These features can be categorized as ?syn-
tactic language model? features (Shen et al, 2008;
Galley and Manning, 2009), though unlike previous
work our features model both the phrase segmenta-
tion and dependency structure. Typically, these sorts
of features are probabilities estimated from a corpus
parsed using a supervised parser. However, there do
not currently exist treebanks with annotated phrase
476
,? made up 0.057
he? made up 0.021
supreme court? made up 0.014
court? made up 0.014
in september 2000? made up 0.014
in september 2000 ,? made up 0.014
made up? of 0.065
made up? . 0.029
made up? , 0.016
made up? mind to 0.01
Table 3: Most probable child phrases for the parent
phrase ?made up? for each direction, sorted by the con-
ditional probability of the child phrase given the parent
phrase and direction.
dependency trees.
Our solution is to use a standard supervised de-
pendency parser and extract phrase dependencies us-
ing bilingual information.3 We begin by obtaining
symmetrized word alignments and extracting phrase
pairs using the standard heuristic from phrase-based
MT (Koehn et al, 2003). Given the set of extracted
phrase pairs for a sentence, denote by W the set of
unique target-side phrases among them. We parse
the target sentence with a dependency parser and, for
each pair of phrases u, v ? W , we extract a phrase
dependency (along with its direction) if u and v do
not overlap and there is at least one lexical depen-
dency between a word in u and a word in v. If there
are lexical dependencies in both directions, we ex-
tract a phrase dependency only for the single longest
one. Since we use a projective dependency parser,
the longest lexical dependency between two phrases
is guaranteed to be unique. Table 2 shows a listing
of the most frequent phrase dependencies extracted
(lexical dependencies are omitted).
We note that during training we never explicitly
commit to any single phrase dependency tree for a
target sentence. Rather, we extract phrase depen-
dencies from all phrase dependency trees consis-
tent with the word alignments and the lexical de-
pendency tree. Thus we treat phrase dependency
trees analogously to phrase segmentations in stan-
dard phrase extraction.
We perform this procedure on all sentence pairs
in the parallel corpus. Given a set of extracted
3For a monolingual task, Wu et al (2009) used a shal-
low parser to convert lexical dependencies from a dependency
parser into phrase dependencies.
phrase dependencies of the form ?u, v, d?, where
u is the head phrase, v is the child phrase, and
d ? {left , right} is the direction, we then estimate
conditional probabilities p(v|u, d) using relative fre-
quency estimation. Table 3 shows the most probable
child phrases for an example parent phrase. To com-
bat data sparseness, we perform the same procedure
with each word replaced by its word cluster ID ob-
tained from Brown clustering (Brown et al, 1992).
We include a feature in the model for the sum of
the scaled log-probabilities of each attachment:
m??
i=1
max
(
0, C + log p(?i|???(i), d(i)
)
(1)
where d(i) = I[??(i)? i > 0] is the direction of the
dependency arc.
Although we use log-probabilities in this feature
function, we first add a constant C to each to ensure
they are all positive.4 The max expression protects
unseen parent-child phrase dependencies from caus-
ing the score to be negative infinity. Our motivation
is a desire for the features to be used to prefer one
derivation over another but not to rule out a deriva-
tion completely if it merely happens to contain a de-
pendency unobserved in the training data.
We also include lexical weighting features simi-
lar to those used in phrase-based MT (Koehn et al,
2003). Whenever we extract a phrase dependency,
we extract the longest lexical dependency contained
within it. For all ?parent, child, direction? lexi-
cal dependency tuples ?x, y, d?, we estimate condi-
tional probabilities plex (y|x, d) from the parsed cor-
pus using relative frequency estimation. Then, for a
phrase dependency with longest lexical dependency
?x, y, d?, we add a feature for plex (y|x, d) to the
model, using a formula similar to Eq. 1. Different
instances of a phrase dependency may have different
lexical dependencies extracted with them. We add
the lexical weight for the most frequent, breaking
ties by choosing the lexical dependency that maxi-
mizes p(y|x, d), as was also done by Koehn et al
(2003).
In all, we include 4 target tree features: one for
phrase dependencies, one for lexical dependencies,
4The reasoning here is that whenever we use a phrase de-
pendency that we have observed in the training data, we want to
boost the score of the translation. If we used log-probabilities,
each observed dependency would incur a penalty.
477
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can you deliver it by tomorrow morning ?
can     you     deliver  it    by     tomorrow morning ?
CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
...
es:it
morgen:tomorrow
liefern:deliver
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
...
fr?h:morning
morgen:tomorrow
morgen:morning
liefern:deliver
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
x y      z
a    b   c d e
x y      z
a    b   c d ex y      z
a    b   c d e
x y      z
a    b   c d e
Figure 1: String-to-tree configurations; each is associated
with a feature that counts its occurrences in a derivation.
and the same features computed from a transformed
version of the corpus in which each word is replaced
by its Brown cluster.
4.2 String-to-Tree Configurations
We consider features that count instances of reorder-
ing configurations involving phrase dependencies.
In addition to the target-side structures, these fea-
tures consider ? and a, though not s or ?s. For ex-
ample, when building a parent-child phrase depen-
dency with the child to the left, one feature value is
incremented if their aligned source-side phrases are
in the same order. This configuration is the leftmost
in Fig. 1; we include features for the other three con-
figurations there as well, for a total of 4 features in
this category.
4.3 Tree-to-Tree Configurations
We include features that consider s, ?, and ?s in ad-
dition to t, ?, and ??. We begin with features for
each of the quasi-synchronous configurations from
Smith and Eisner (2006), adapted to phrase depen-
dency grammars. That is, for a parent-child pair
???(i), i? in ??, we consider the relationship be-
tween a(??(i)) and a(i), the source-side phrases
to which ??(i) and i align. We use the follow-
ing named configurations from Smith and Eisner:
root-root, parent-child, child-parent, grandparent-
grandchild, sibling, and c-command.5 We define a
feature to count instances of each of these configu-
rations, including an additional feature for ?other?
configurations that do not fit into these categories.6
When using a QPDG, there are multiple ways
to compute tree-to-tree configuration features, since
5See Fig. 3 in Smith and Eisner (2006) for illustrations.
6We actually include two versions of each configuration fea-
ture other than ?root-root?: one for the source phrases being in
the same order as the target phrases and one for them being
swapped.
Input: sentence s, dependency parse ?s, coarse
parameters ?M , fine parameters ??,??
Output: translation t
LMERT ? GenerateLattices (s, ?M );
LFB ? FBPrune (LMERT, ?M );
?t,?,?, ??,a? ? QGDEPPARSE(LFB, ??,??);
return t;
Algorithm 1: CoarseToFineDecode
we use a phrase dependency tree for the target side,
a lexical dependency tree for the source side, and
a phrase alignment. We use the following heuristic
approach. Given a pair of source words, one with
index j in source phrase a(??(i)) and the other with
index k in source phrase a(i), we have a parent-
child configuration if ?s(k) = j; if ?s(j) = k, a
child-parent configuration is present. In order for the
grandparent-grandchild configuration to be present,
the intervening parent word must be outside both
phrases. For sibling and other c-command config-
urations, the shared parent or ancestor must also be
outside both phrases.
After obtaining a list of all configurations present
for each pair of words ?j, k?, we fire the feature for
the single configuration corresponding to the max-
imum distance |j ? k|. If no configurations are
present between any pair of words, the ?other? fea-
ture fires. Therefore, only one configuration feature
fires for each phrase dependency attachment.
Finally, we include features that consider the
dependency path distance between phrases in the
source-side dependency tree that are aligned to
parent-child pairs in ??. We include a feature that
sums, for each target phrase i, the inverse of the
minimum undirected path length between each word
in a(i) and each word in ??(a(i)). The minimum
undirected path length is defined as the number of
dependency arcs that must be crossed to travel from
one word to the other in ?s. We use one feature
for undirected path length and one other for directed
path length. If there is no (un)directed path from a
word in a(i) to a word in ??(a(i)), we use? as the
minimum length.
There are 15 features in this category, for a total
of 23 QPDG features.
478
5 Decoding
For a QPDG model, decoding consists of finding
the highest-scoring tuple ?t,?,?, ??,a? for an in-
put sentence s and its parse ?s, i.e., finding the most
probable derivation under the s/?s-specific grammar
Gs,?s . We follow Gimpel and Smith (2009) in con-
structing a lattice to representGs,?s and using lattice
parsing to search for the best derivation, but we con-
struct the lattice differently and employ a coarse-to-
fine strategy (Petrov, 2009) to speed up decoding.
It has become common in recent years for MT re-
searchers to exploit efficient data structures for en-
coding concise representations of the pruned search
space of the model, such as phrase lattices for
phrase-based MT (Ueffing et al, 2002; Macherey
et al, 2008; Tromble et al, 2008). Each edge in
a phrase lattice corresponds to a phrase pair and
each path through the lattice corresponds to a tuple
?t,?,?,a? for the input s. Decoding for a phrase
lattice consists of finding the highest-scoring path,
which is done using dynamic programming. To also
maximize over ??, we perform lattice dependency
parsing, which allows us to search over the space of
tuples ?t,?,?,a, ???. Given the lattice and Gs,?s ,
lattice parsing is a straightforward generalization of
the standard arc-factored dynamic programming al-
gorithm from Eisner (1996).
The lattice parsing algorithm requires O(E2V )
time and O(E2 + V E) space, where E is the num-
ber of edges in the lattice and V is the number of
nodes.7 Typical phrase lattices might easily contain
tens of thousands of nodes and edges, making exact
search prohibitively expensive for all but the small-
est lattices. So, we use approximate search based on
coarse-to-fine decoding. We now discuss each step
of this procedure; an outline is shown as Alg. 1.
Pass 1: Lattice Pruning After generating phrase
lattices using a phrase-based MT system, we prune
lattice edges using forward-backward pruning (Six-
tus and Ortmanns, 1999), which has also been used
in previous work using phrase lattices (Tromble et
al., 2008). This pruning method computes the max-
marginal for each lattice edge, which is the score of
the best full path that uses that edge. Max-marginals
7To prevent confusion, we use the term edge to refer to a
phrase lattice edge and arc to refer to a parent-child dependency
in the phrase dependency tree.
offer the advantage that the best path in the lattice is
preserved during pruning. For each lattice, we use
a grid search to find the most liberal threshold that
leaves fewer than 1000 edges in the resulting lattice.
As complexity is quadratic in E, forcing E to be
less than 1000 improves runtime substantially. Af-
ter pruning, the lattices still contain more than 1016
paths on average and oracle BLEU scores are typi-
cally 12-15 points higher than the model-best paths.
Pass 2: Parent Ranking Given a pruned lattice,
we then remove some candidate dependency arcs
from consideration. It is common in dependency
parsing to use a coarse model to rank the top k par-
ents for each word, and to only consider these during
parsing (Martins et al, 2009; Bergsma and Cherry,
2010). Unlike string parsing, our phrase lattices im-
pose several types of constraints on allowable arcs.
For example, each node in the phrase lattice is an-
notated with a coverage vector?a bit vector indicat-
ing which words in the source sentence have been
translated?which implies a topological ordering of
the nodes. To handle constraints like these, we first
use the Floyd-Warshall algorithm (Floyd, 1962) to
find the best score between every pair of nodes in
the lattice. This algorithm also tells us whether each
edge is reachable from each other edge, allowing
us to immediately prune dependency arcs between
edges that are unreachable from each other.
After eliminating impossible arcs, we turn to
pruning away unlikely ones. In standard (string) de-
pendency parsing, every word is assigned a parent.
In lattice parsing, however, most lattice edges will
not be assigned any parent. Certain lattice edges are
much more likely to be contained within paths, so
we allow some edges to have more candidate parent
edges than others. We introduce hyperparameters
?, ?, and ? to denote, respectively, the minimum,
maximum, and average number of parent edges to
be considered for each lattice edge (? ? ? ? ?).
We rank the full set of E2 arcs according to their
scores (using the QPDG features and their weights
?) and choose the top ?E of these arcs while en-
suring that each edge has at least ? and at most ?
potential parent edges.
This step reduces the time complexity from
O(E2V ) to O(?EV ), where ? < E. In our ex-
periments, we set ? = 300, ? = 100, and ? = 400.
479
Input: tuning set D = ?S, T ?, initial weights ?0 for
coarse model, initial weights ?0 for
additional features in fine model
Output: coarse model learned weights: ?M , fine
model learned weights: ???,???
?M ? MERT (S, T , ?0, 100, MOSES);
LMERT ? GenerateLattices (S, ?M );
LFB ? FBPrune (LMERT, ?M );
???,??? ?
MERT (LFB, T , ??M ,?0?, 200, QGDEPPARSE);return ?M , ???,???;
Algorithm 2: CoarseToFineTrain
Pass 3: Lattice Dependency Parsing After com-
pleting the coarse passes, we parse using bottom-up
dynamic programming based on the agenda algo-
rithm (Nederhof, 2003; Eisner et al, 2005). We only
consider arcs that survived the filtering in Pass 2.
We weight agenda items by the sum of their scores
and the Floyd-Warshall best path scores both from
the start node of the lattice to the beginning of the
item and the end of the item to any final node. This
heuristic helps us to favor exploration of items that
are highly likely under the phrase-based model.
If the score of the partial structure can only get
worse when combining it with other structures (e.g.,
in a PCFG), then the first time that we pop an item
of type GOAL from the agenda, we are guaranteed
to have the best parse. However, in our model, some
features are positive and others negative, making this
property no longer hold; as a result, GOAL items
may be popped out of order from the agenda. There-
fore, we use an approximation, simply popping G
GOAL items from the agenda and then stopping. The
items are sorted by their scores and the best is re-
turned by the decoder (or the k best in the case of
MERT). In our experiments, we set G = 4000.
The combined strategy yields average decoding
times in the range of 30 seconds per sentence, which
is comparable to other syntax-based MT systems.
6 Training
For tuning the coarse and fine parameters, we use
minimum error rate training (MERT; Och, 2003) in
a procedure shown as Alg. 2. We first use MERT to
train parameters for the coarse phrase-based model
used to generate phrase lattices. Then, after gener-
ating the lattices, we prune them and run MERT a
second time to tune parameters of the fine model,
which includes all phrase-based and QPDG param-
eters. The arguments to MERT are a vector of source
sentences (or lattices), a vector of target sentences,
the initial parameter values, the size of the k-best
list, and finally the decoder. We initialize ? to the
default Moses feature weights and for ? we ini-
tialize the two target phrase dependency weights to
0.004, the two lexical dependency weights to 0.001,
and the weights for all configuration features to 0.0.
Our training procedure requires two executions of
MERT, and the second typically takes more itera-
tions to converge (10 to 20 is typical) than the first
due to the use of a larger feature set and increased
possibility for search error due to the enlarged search
space.
7 Experiments
For experimental evaluation, we consider Chinese-
to-English (ZH-EN) and Urdu-to-English (UR-
EN) translation and compare our system to
Moses (Koehn et al, 2007). For ZH-EN, we
used 303k sentence pairs from the FBIS corpus
(LDC2003E14). We segmented the Chinese data
using the Stanford Chinese segmenter in ?CTB?
mode (Chang et al, 2008), giving us 7.9M Chinese
words and 9.4M English words. For UR-EN, we
used parallel data from the NIST MT08 evaluation
consisting of 1.2M Urdu words and 1.1M English
words.
We trained a baseline Moses system using de-
fault settings and features. Word alignment was
performed using GIZA++ (Och and Ney, 2003) in
both directions and the grow-diag-final-and
heuristic was used to symmetrize the alignments.
We used a max phrase length of 7 when extracting
phrases. Trigram language models were estimated
using the SRI language modeling toolkit (Stolcke,
2002) with modified Kneser-Ney smoothing (Chen
and Goodman, 1998). To estimate language models
for each language pair, we used the English side of
the parallel corpus concatenated with 200M words
of randomly-selected sentences from the Gigaword
v4 corpus (excluding the NY Times and LA Times).
We used this baseline Moses system to gener-
ate phrase lattices for our system, so our model in-
cludes all of the Moses features in addition to the
480
MT03 (tune) MT02 MT05 MT06 Average
Moses 33.84 33.35 31.81 28.82 31.33
QPDG (TT) 34.63 (+0.79) 34.10 (+0.75) 32.15 (+0.34) 29.33 (+0.51) 31.86 (+0.53)
QPDG (TT+S2T+T2T) 34.98 (+1.14) 34.26 (+0.91) 32.34 (+0.53) 29.35 (+0.53) 31.98 (+0.65)
Table 4: Chinese-English Results (% BLEU).
QPDG features described in ?4. In our experiments,
we compare our QPDG system (lattice parsing on
each lattice) to the Moses baseline (finding the best
path through each lattice). The conventional wis-
dom holds that hierarchical phrase-based transla-
tion (Chiang, 2005) performs better than phrase-
based translation for language pairs that require
large amounts of reordering, such as ZH-EN and
UR-EN. However, researchers have shown that this
performance gap diminishes when using a larger dis-
tortion limit (Zollmann et al, 2008) and may dis-
appear entirely when using a lexicalized reordering
model (Lopez, 2008; Galley and Manning, 2010).
So, we increase the Moses distortion limit from 6
(the default) to 10 and use Moses? default lexical-
ized reordering model (Koehn et al, 2005).
We parsed the Chinese text using the Stanford
parser (Levy and Manning, 2003) and the English
text using TurboParser (Martins et al, 2009). We
note that computing our features requires parsing the
target (English) side of the parallel text, but not the
source side. We only need to parse the source side
of the tuning and test sets, and the only features that
look at the source-side parse are those from ?4.3.
To obtain Brown clusters for the target tree fea-
tures in ?4.1, we used code from Liang (2005).8
We induced 100 clusters from the English side of
the parallel corpus concatenated with 10M words of
randomly-selected Gigaword sentences. Only words
that appeared at least twice in this data were con-
sidered during clustering. An additional cluster was
created for all other words; this allowed us to use
phrase dependency cluster features even for out-of-
vocabulary words. We used a max phrase length of
7 when extracting phrase dependencies to match the
max phrase length used in phrase extraction. Ap-
proximately 87M unique phrase dependencies were
extracted from the ZH-EN data and 7M from the
UR-EN data.
We tuned the weights of our model using the pro-
8http://www.cs.berkeley.edu/?pliang/
software
Dev (tune) MT09
Moses 24.21 23.56
QPDG (TT+S2T) 24.94 (+0.73) 24.31 (+0.75)
Table 5: Urdu-English Results (% BLEU).
cedure described in ?6. For ZH-EN we used MT03
for tuning and MT02, MT05, and MT06 for test-
ing. For UR-EN we used half of the documents (882
sentence pairs) from the MT08 test set for tuning
(?Dev?) and MT09 for testing. We evaluated trans-
lation output using case-insensitive IBM BLEU (Pa-
pineni et al, 2001).
7.1 Results
Results for ZH-EN and UR-EN translation are
shown in Tables 4 and 5. We show results when us-
ing only the target tree features from ?4.1 (TT), as
well as when adding the string-to-tree features from
?4.2 (S2T) and the tree-to-tree features from ?4.3
(T2T). We note that T2T features are unavailable for
UR-EN because we do not have an Urdu parser. We
find that we can achieve moderate but consistent im-
provements over the baseline Moses system, for an
average increase of 0.65 BLEU points for ZH-EN
and 0.75 for UR-EN.
Fig. 2 shows an example sentence from the MT05
test set alng with its translation output and deriva-
tions produced by Moses and our QPDG system
with the full feature set. This example shows the
kind of improvements that our system makes. In
Chinese, modifiers such as prepositional phrases and
clauses are generally placed in front of the words
they modify, frequently the opposite of English. In
addition, Chinese occasionally uses postpositions
where English uses prepositions. The Chinese sen-
tence in Fig. 2 exhibits both of these, as the preposi-
tional phrase ?after the Palestinian election? appears
before the verb ?strengthen? in the Chinese sen-
tence and ?after? appears as a postposition. Moses
(Fig. 2(a)) does not properly reorder the preposi-
tional phrase, while our system (Fig. 2(b)) properly
handles both reorderings.9 We shall discuss these
9Our system?s derivation is not perfect, in that ?in? is incor-
481
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can you deliver it by tomorrow morning ?
can     you     deliver  it    by     tomorrow morning ?
CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
...
es:it
morgen:tomorrow
liefern:deliver
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
...
fr?h:morning
morgen:tomorrow
morgen:morning
liefern:deliver
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
bush   :   palestinian   presidential election   in   the  united states will   strengthen the peace  efforts
palestine elections strengthen peace effortsafterbush will in:
united
states
bush   : the united states will   strengthen   the   peace   efforts   after   the palestinian   election$
$
bush : us set to boost peace efforts after palestinian election
bush : us to step up peace efforts after palestinian elections
bush : u.s. will enhance peace efforts after palestinian election
us to boost peace efforts after palestinian elections : bush
References
(a)
(b)
(c)
??    :    ?   ?   ?   ????   ?        ?     ??   ??   ??
??    :  ?  ?   ?   ????   ?        ?   ??   ??   ??
Figure 2: (a) Moses translation output along with ?, ?, and a. An English gloss is shown above the Chinese sentence
and above the gloss is shown the dependency parse from the Stanford parser. (b) QPDG system output with additional
structure ??. (c) reference translations.
types of improvements further in ?8.
7.2 Unsupervised Parsing
Our results thus far use supervised parsers for both
Chinese and English, but parsers are only available
for a small fraction of the languages we would like
to translate. Fortunately, unsupervised dependency
grammar induction has improved substantially in re-
cent years due to a flurry of recent research. While
attachment accuracies on standard treebank test sets
are still relatively low, it may be the case that even
though unsupervised parsers do not match treebank
annotations very well, they may perform well when
used for extrinsic applications. We believe that
syntax-based MT offers a compelling platform for
development and extrinsic evaluation of unsuper-
vised parsers.
In this paper, we use the standard dependency
model with valence (DMV; Klein and Manning,
2004). When training is initialized using the out-
put of a simpler, concave dependency model, the
rectly translated and reordered, but the system was nonetheless
able to use it to improve the fluency of the output.
DMV can approach state-of-the-art unsupervised ac-
curacy (Gimpel and Smith, 2011). For English, the
resulting parser achieves 53.1% attachment accu-
racy on Section 23 of the Penn Treebank (Marcus et
al., 1993), which approaches the 55.7% accuracy of
a recent state-of-the-art unsupervised model (Blun-
som and Cohn, 2010). The Chinese parser, ini-
tialized and trained the same way, achieves 44.4%,
which is the highest reported accuracy on the Chi-
nese Treebank (Xue et al, 2004) test set.
Most unsupervised grammar induction models
assume gold standard POS tags and sentences
stripped of punctuation. We use the Stanford tag-
ger (Toutanova et al, 2003) to obtain tags for both
English and Chinese, parse the sentences without
punctuation using the DMV, and then attach punc-
tuation tokens to the root word of the tree in a post-
processing step. For English, the predicted parents
agreed with those of TurboParser for 48.7% of the
tokens in the corpus.
We considered all four scenarios: supervised and
unsupervised English parsing paired with supervised
and unsupervised Chinese parsing. Table 6 shows
482
EN
unsupervised supervised
ZH unsupervised 31.18 (33.76) 31.86 (34.78)supervised 32.12 (34.74) 31.98 (34.98)
Moses 31.33 (33.84)
Table 6: Results when using unsupervised dependency
parsers. Cells contain averaged % BLEU on the three test
sets and % BLEU on tuning data (MT03) in parentheses.
Feature Initial Learned
Left child, same order 9.0 8.9
Left child, swap phrases 1.1 0.0
Right child, same order 7.3 7.3
Right child, swap phrases 1.6 2.3
Root-root 0.4 0.8
Parent-child 4.2 6.1
Child-parent 1.2 0.4
Grandparent-grandchild 1.0 0.2
Sibling 2.4 1.9
C-command 6.1 6.7
Other 1.5 0.9
Table 7: Average feature values across best translations
of sentences in the MT03 tuning set, both before MERT
(column 2) and after (column 3). ?Same? versions of tree-
to-tree configuration features are shown; the rarer ?swap?
features showed a similar trend.
BLEU scores averaged over the three test sets with
tuning data BLEU in parentheses. Surprisingly, we
achieve our best results when using the unsupervised
English parser in place of the supervised one (+0.79
over Moses), while keeping the Chinese parser su-
pervised. Competitive performance is also found
by using the unsupervised Chinese parser and super-
vised English parser (+0.53 over Moses).
However, when using unsupervised parsers for
both languages, performance was below that of
Moses. During tuning for this configuration, we
found that MERT struggled to find good parameter
estimates, typically converging to suboptimal solu-
tions after a small number of iterations. We believe
this is due to the large number of features (37), the
noise in the parse trees, and known instabilities of
MERT. In future work we plan to experiment with
training algorithms that are more stable and that can
handle larger numbers of features.
8 Analysis
To understand what our model learns during MER
training, we computed the feature vectors of the best
derivation for each sentence in the tuning data at
both the start and end of tuning. Table 7 shows
these feature values averaged across all tuning sen-
tences. The first four features are the configurations
from Fig. 1, in order from left to right. From these
rows, we can observe that the model learns to en-
courage swapping when generating right children
and penalize swapping for left children. In addi-
tion to objects, right children in English are often
prepositional phrases, relative clauses, or other mod-
ifiers; as we noted above, Chinese generally places
these modifiers before their heads, requiring reorder-
ing during translation. Here the model appears to be
learning this reordering behavior.
From the second set of features, we see that the
model learns to favor producing dependency trees
that are mostly isomorphic to the source tree, by fa-
voring root-root and parent-child configurations at
the expense of most others.
9 Discussion
In looking at BLEU score differences between the
two systems, the unigram precisions were typically
equal or only slightly different, while precisions for
higher-order n-grams contained the bulk of the im-
provement. This suggests that our system is not
finding substantially better translations for individ-
ual words in the input, but rather is focused on re-
ordering the existing translations. This is not sur-
prising given our choice of features, which focus on
syntactic language modeling and syntax-based re-
ordering. The obvious next step for our framework
is to include bilingual rules that include source syn-
tax (Quirk et al, 2005), target syntax (Shen et al,
2008), and syntax on both sides. Our framework al-
lows integrating together all of these and other types
of structures, with the ultimate goal of combining
the strengths of multiple approaches to translation
in a single model.
Acknowledgments
We thank Chris Dyer and the anonymous reviewers for
helpful comments that improved this paper. This research
was supported in part by the NSF through grant IIS-
0844507, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant number
W911NF-10-1-0533, and Sandia National Laboratories
(fellowship to K. Gimpel).
483
References
M. Auli, A. Lopez, H. Hoang, and P. Koehn. 2009. A
systematic analysis of translation model search spaces.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation.
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proc. of COLING.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In Proc. of EMNLP.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18.
X. Carreras and M. Collins. 2009. Non-projective pars-
ing for statistical machine translation. In Proc. of
EMNLP.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the Third Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proc. of ACL-IJCNLP.
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proc. of EMNLP-CoNLL.
J. DeNero, S. Kumar, C. Chelba, and F. J. Och. 2010.
Model combination for machine translation. In Proc.
of NAACL.
M. Dymetman and N. Cancedda. 2010. Intersecting hi-
erarchical and phrase-based models of translation. for-
mal aspects and algorithms. In Proc. of SSST-4.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
piling Comp Ling: Practical weighted dynamic pro-
gramming and the Dyna language. In Proc. of HLT-
EMNLP.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
R. W. Floyd. 1962. Algorithm 97: Shortest path. Com-
munications of the ACM, 5(6).
M. Galley and C. D. Manning. 2009. Quadratic-time
dependency parsing for machine translation. In Proc.
of ACL-IJCNLP.
M. Galley and C. D. Manning. 2010. Accurate non-
hierarchical phrase-based translation. In Proc. of
NAACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of COLING-ACL.
K. Gimpel and N. A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP.
K. Gimpel and N. A. Smith. 2011. Concavity and initial-
ization for unsupervised dependency grammar induc-
tion. Technical report, Carnegie Mellon University.
L. Huang, K. Knight, and A. Joshi. 2006. Statistical
syntax-directed translation with extended domain of
locality. In Proc. of AMTA.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Edinburgh
system description for the 2005 iwslt speech transla-
tion evaluation. In Proc. of IWSLT.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
R. Levy and C. D. Manning. 2003. Is it harder to parse
chinese, or the chinese treebank? In Proc. of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Y. Liu, H. Mi, Y. Feng, and Q. Liu. 2009. Joint de-
coding with multiple translation models. In Proc. of
ACL-IJCNLP.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proc. of ACL.
M.-J. Nederhof. 2003. Weighted deductive parsing and
knuth?s algorithm. Computational Linguistics, 29(1).
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
484
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL.
L. Shen, J. Xu, and R. Weischedel. 2008. A new string-
to-dependency machine translation algorithm with a
target dependency language model. In Proc. of ACL.
A. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal
Processing.
D. A. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proc. of HLT-NAACL Workshop on
Statistical Machine Translation.
D. A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous features. In Proc.
of EMNLP.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of HLT-NAACL.
R. Tromble, S. Kumar, F. Och, and W. Macherey. 2008.
Lattice Minimum Bayes-Risk decoding for statistical
machine translation. In EMNLP.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase
dependency parsing for opinion mining. In Proc. of
EMNLP.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1?30.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proc. of NAACL 2006 Workshop on Statistical Ma-
chine Translation.
A. Zollmann, A. Venugopal, F. J. Och, and J. Ponte.
2008. A systematic comparison of phrase-based, hi-
erarchical and syntax-augmented statistical MT. In
Proc. of COLING.
485
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1357?1367, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Word Salad: Relating Food Prices and Descriptions
Victor Chahuneau Kevin Gimpel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,kgimpel}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Lily Scherlis
Phillips Academy
Andover, MA 01810, USA
lily.scherlis@gmail.com
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We investigate the use of language in food
writing, specifically on restaurant menus and
in customer reviews. Our approach is to build
predictive models of concrete external vari-
ables, such as restaurant menu prices. We
make use of a dataset of menus and customer
reviews for thousands of restaurants in several
U.S. cities. By focusing on prediction tasks
and doing our analysis at scale, our method-
ology allows quantitative, objective measure-
ments of the words and phrases used to de-
scribe food in restaurants. We also explore
interactions in language use between menu
prices and sentiment as expressed in user re-
views.
1 Introduction
What words might a menu writer use to justify the
high price of a steak? How does describing an item
as chargrilled vs. charbroiled affect its price? When
a customer writes an unfavorable review of a restau-
rant, how is her word choice affected by the restau-
rant?s prices? In this paper, we explore questions
like these that relate restaurant menus, prices, and
customer sentiment. Our goal is to understand how
language is used in the food domain, and we di-
rect our investigation using external variables such
as restaurant menu prices.
We build on a thread of NLP research that seeks
linguistic understanding by predicting real-world
quantities from text data. Recent examples include
prediction of stock volatility (Kogan et al 2009)
and movie revenues (Joshi et al 2010). There, pre-
diction tasks were used for quantitative evaluation
and objective model comparison, while analysis of
learned models gave insight about the social process
behind the data.
We echo this pattern here as we turn our atten-
tion to language use on restaurant menus and in user
restaurant reviews. We use data from a large cor-
pus of restaurant menus and reviews crawled from
the web and formulate several prediction tasks. In
addition to predicting menu prices, we also consider
predicting sentiment along with price.
The relationship between language and senti-
ment is an active area of investigation (Pang and
Lee, 2008). Much of this research has focused on
customer-written reviews of goods and services, and
perspectives have been gained on how sentiment is
expressed in this type of informal text. In addition
to sentiment, however, other variables are reflected
in a reviewer?s choice of words, such as the price of
the item under consideration. In this paper, we take
a step toward joint modeling of multiple variables
in review text, exploring connections between price
and sentiment in restaurant reviews.
Hence this paper contributes an exploratory data
1357
analysis of language used to describe food (by its
purveyors and by its consumers). While our primary
goal is to understand the language used in our cor-
pus, our findings bear relevance to economics and
hospitality research as well. This paper is a step on
the way to the eventual goal of using linguistic anal-
ysis to understand social phenomena like sales and
consumption.
2 Related Work
There are several areas of related work scattered
throughout linguistics, NLP, hospitality research,
and economics.
Freedman and Jurafsky (2011) studied the use of
language in food advertising, specifically the words
on potato chip bags. They argued that, due to
the ubiquity of food writing across cultures, eth-
nic groups, and social classes, studying the use of
language for describing food can provide perspec-
tive on how different socioeconomic groups self-
identify using language and how they are linguisti-
cally targeted. In particular, they showed that price
affects how ?authenticity? is realized in marketing
language, a point we return to in ?5. This is an ex-
ample of how price can affect how an underlying
variable is expressed in language. Among other ex-
plorations in this paper, we consider how price inter-
acts with expression of sentiment in user reviews of
restaurants.
As mentioned above, our work is related to re-
search in predicting real-world quantities using text
data (Koppel and Shtrimberg, 2006; Ghose et al
2007; Lerman et al 2008; Kogan et al 2009; Joshi
et al 2010; Eisenstein et al 2010; Eisenstein et
al., 2011; Yogatama et al 2011). Like much of
this prior work, we aim to learn how language is
used in a specific context while building models that
achieve competitive performance on a quantitative
prediction task.
Along these lines, there is recent interest in ex-
ploring the relationship between product sales and
user-generated text, particularly online product re-
views. For example, Ghose and Ipeirotis (2011)
studied the sales impact of particular properties of
review text, such as readability, the presence of
spelling errors, and the balance between subjective
and objective statements. Archak et al(2011) had a
similar goal but decomposed user reviews into parts
describing particular aspects of the product being
reviewed (Hu and Liu, 2004). Our paper differs
from price modeling based on product reviews in
several ways. We consider a large set of weakly-
related products instead of a homogeneous selection
of a few products, and the reviews in our dataset are
not product-centered but rather describe the overall
experience of visiting a restaurant. Consequently,
menu items are not always mentioned in reviews and
rarely appear with their exact names. This makes it
difficult to directly use review features in a pricing
model for individual menu items.
Menu planning and pricing has been studied for
many years by the culinary and hospitality research
community (Kasavana and Smith, 1982; Kelly et al
1994), often including recommendations for writing
menu item descriptions (Miller and Pavesic, 1996;
McVety et al 2008). Their guidelines frequently
include example menus from successful restaurants,
but typically do not use large corpora of menus or
automated analysis, as we do here. Other work
focused more specifically on particular aspects of
the language used on menus, such as the study by
Zwicky and Zwicky (1980), who made linguistic ob-
servations through manual analysis of a corpus of
200 menus.
Relatedly, Wansink et al(2001; 2005) showed
that the way that menu items are described af-
fects customers? perceptions and purchasing behav-
ior. When menu items are described evocatively,
customers choose them more often and report higher
satisfaction with quality and value, as compared to
when they are given the same items described with
conventional names. Wansink et aldid not use a
corpus, but rather conducted a small-scale experi-
ment in a working cafeteria with customers and col-
lected surveys to analyze consumer reaction. While
our goals are related, our experimental approach is
different, as we use automated analysis of thousands
of restaurant menus and rely on a set of one mil-
lion reviews as a surrogate for observing customer
behavior.
Finally, the connection between products and
prices is also a central issue in economics. How-
ever, the stunning heterogeneity in products makes
empirical work challenging. For example, there are
over 50,000 menu items in New York that include
1358
City # Restaurants # Menu Items # Reviews
train dev. test train dev. test train dev. test
Boston 930 107 113 63,422 8,426 8,409 80,309 10,976 11,511
Chicago 804 98 100 51,480 6,633 6,939 73,251 9,582 10,965
Los Angeles 624 80 68 17,980 2,938 1,592 75,455 13,227 5,716
New York 3,965 473 499 365,518 42,315 45,728 326,801 35,529 37,795
Philadelphia 1,015 129 117 83,818 11,777 9,295 52,275 7,347 5,790
San Francisco 1,908 255 234 103,954 12,871 12,510 499,984 59,378 67,010
Washington, D.C. 773 110 121 47,188 5,957 7,224 71,179 11,852 14,129
Total 10,019 1,252 1,252 733,360 90,917 91,697 1,179,254 147,891 152,916
Table 1: Dataset statistics.
the word chicken. What is the price of chicken? This
is an important practical and daunting matter when
measuring inflation (e.g., Consumer Price Index is
measured with a precisely-defined basket of goods).
Price dispersion across goods and the variation of
the goods is an important area of industrial organi-
zation economic theory. For example, economists
are interested in models of search, add-on pricing,
and obfuscation (Baye et al 2006; Ellison, 2005).
3 Data
We crawled Allmenus.com (www.allmenus.
com) to gather menus for restaurants in seven
U.S. cities: Boston, Chicago, Los Angeles, New
York, Philadelphia, San Francisco, and Washing-
ton, D.C. Each menu includes a list of item names
with optional text descriptions and prices. Most All-
menus restaurant pages contain a link to the cor-
responding page on Yelp (www.yelp.com) with
metadata and user reviews for the restaurant, which
we also collected.
The metadata consist of many fields for each
restaurant, which can be divided into three cate-
gories: location (city, neighborhood, transit stop),
services available (take-out, delivery, wifi, parking,
etc.), and ambience (good for groups, noise level,
attire, etc.). Also, the category of food and a price
range ($ to $$$$, indicating the price of a typical
meal at the restaurant) are indicated. The user re-
views include a star rating on a scale of 1 to 5.
The distribution of prices of individual menu
items is highly skewed, with a mean of $9.22 but
a median of $6.95. On average, a restaurant has
73 items on its menu with a median price of $8.69
and 119 Yelp reviews with a median rating of 3.55
????????
0
100k
200k
300k
400k
500k
  
 
 
 
 
 
 
 
 
star rating
??????? ????????
?????????????
????????????????????????????
???Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1100?1111,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Systematic Exploration of Diversity in Machine Translation
Kevin Gimpel? Dhruv Batra? Chris Dyer? Gregory Shakhnarovich?
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
?Virginia Tech, Blacksburg, VA 24061, USA
?Carnegie Mellon University, Pittsburgh, PA 15213, USA
Corresponding author: kgimpel@ttic.edu
Abstract
This paper addresses the problem of produc-
ing a diverse set of plausible translations. We
present a simple procedure that can be used
with any statistical machine translation (MT)
system. We explore three ways of using di-
verse translations: (1) system combination,
(2) discriminative reranking with rich features,
and (3) a novel post-editing scenario in which
multiple translations are presented to users.
We find that diversity can improve perfor-
mance on these tasks, especially for sentences
that are difficult for MT.
1 Introduction
From the perspective of user interaction, the ideal
machine translator is an agent that reads documents
in one language and produces accurate, high qual-
ity translations in another. This interaction ideal
has been implicit in machine translation (MT) re-
search since the field?s inception. It is the way
we interact with commercial MT services (such as
Google Translate and Microsoft Translator), and the
way MT systems are evaluated (Bojar et al, 2013).
Unfortunately, when a real, imperfect MT system
makes an error, the user is left trying to guess what
the original sentence means.
Multiple Hypotheses. In contrast, when we look
at the way other computer systems consume out-
put from MT systems (or similarly unreliable tools),
we see a different pattern. In a pipeline setting
it is commonplace to propagate not just a single-
best output but the M -best hypotheses (Venugopal
et al, 2008). Multiple solutions are also used for
reranking (Collins, 2000; Shen and Joshi, 2003;
Collins and Koo, 2005; Charniak and Johnson,
2005), tuning (Och, 2003), minimum Bayes risk de-
coding (Kumar and Byrne, 2004), and system com-
bination (Rosti et al, 2007). When dealing with
error-prone systems, knowing about alternatives has
benefits over relying on only a single output (Finkel
et al, 2006; Dyer, 2010).
Need for Diversity. Unfortunately, M -best lists are
a poor surrogate for structured output spaces (Finkel
et al, 2006; Huang, 2008). In MT, for exam-
ple, many translations on M -best lists are extremely
similar, often differing only by a single punctua-
tion mark or minor morphological variation. Re-
cent work has explored reasoning about sets using
packed representations such as lattices and hyper-
graphs (Macherey et al, 2008; Tromble et al, 2008;
Kumar et al, 2009), or sampling translations propor-
tional to their probability (Chatterjee and Cancedda,
2010). We argue that the implicit goal behind these
techniques is to better explore the output space by
introducing diversity into the surrogate set.
Overview and Contributions. In this work, we el-
evate diversity to a first-class status and directly ad-
dress the problem of generating a set of diverse,
plausible translations. We use the recently pro-
posed technique of Batra et al (2012), which pro-
duces diverse M -best solutions from a probabilistic
model using a generic dissimilarity function ?(?, ?)
that specifies how two solutions differ. Our first con-
tribution is a family of dissimilarity functions for
MT that admit simple algorithms for generating di-
verse translations. Other contributions are empiri-
cal: we show that diverse translations can lead to
improvements for system combination and discrim-
inative reranking. We also perform a novel human
1100
post-editing evaluation in order to measure whether
diverse translations can help users make sense of
noisy MT output. We find that diverse translations
can help post-editors produce better outputs for sen-
tences that are the most difficult for MT. While we
focus on machine translation in this paper, we note
that our approach is applicable to other structure pre-
diction problems in NLP.
2 Preliminaries and Notation
Let X denote the set of all strings in a source lan-
guage. For an x ? X, let Yx denote the set of its pos-
sible translations y in the target language. MT mod-
els typically include a latent variable that captures
the derivational structure of the translation process.
Regardless of its specific form, we refer to this vari-
able as a derivation h ? Hx, where Hx is the set of
possible values of h for x. Derivations are coupled
with translations and we define Tx ? Yx ? Hx as
the set of possible ?y,h? pairs for x.
We use a linear model with a parameter vector w
and a vector ?(x,y,h) of feature functions on x, y,
and h (Och and Ney, 2002). The translation of x is
selected using a simple decision rule:
?y?, h?? = argmax
?y,h??Tx
w??(x,y,h) (1)
where we also maximize over the latent variable h
for efficiency. Translation models differ in the form
of Tx and the choice of the feature functions ?. In
this paper we focus on phrase-based (Koehn et al,
2003) and hierarchical phrase-based (Chiang, 2007)
models, which include several bilingual and mono-
lingual features, including n-gram language models.
3 Diversity in Machine Translation
We now address the task of producing a set of di-
verse high-scoring translations.
3.1 Generating Diverse Translations
We use a recently proposed technique (Batra et al,
2012) that constructs diverse lists via a greedy itera-
tive procedure as follows. Let y1 be the model-best
translation (Eq. 1). On the m-th iteration, the m-th
best (diverse) translation is obtained as ?ym,hm? =
argmax
?y,h??Tx
w??(x,y,h) +
m?1?
j=1
?j?(yj ,y) (2)
where ? is a dissimilarity function and ?j is the
weight placed on dissimilarity to previous trans-
lation j relative to the model score. Intuitively,
we seek a translation that is highly-scoring under
the model while being different (as measured by
?) from all previous translations. The ? param-
eters determine the trade-off between model score
and diversity. We refer to Eq. (2) as dissimilarity-
augmented decoding.
The objective in Eq. (2) is a Lagrangian relax-
ation for an intractable constrained objective speci-
fying a minimum dissimilarity ?min between trans-
lations in the list, i.e., ?(yj ,y) ? ?min (Batra et
al., 2012). Instead of setting the dissimilarity thresh-
old ?min , we set the weights ?j . While the formu-
lation allows for a different ?j for each previous so-
lution j, we simply use a single ? = ?j for all j.
This was also done in the experiments in (Batra et
al., 2012).
Note that if the dissimilarity function factors
across the parts of the output variables ?y,h? in the
same way as the features ?, then the same decod-
ing algorithm can be used as for Eq. (1). We discuss
design choices for ? next.
3.2 Dissimilarity Functions for MT
When designing a dissimilarity function ?(?, ?) for
MT, we want to consider variation both in individ-
ual word choice and longer-range sentence structure.
We also want a function that can be easily incorpo-
rated into extant statistical MT systems. We propose
a dissimilarity function that simply counts the num-
ber of times any n-gram is present in both transla-
tions, then negates. Letting q = n? 1:
?n(y,y?) = ?
|y|?q?
i=1
|y?|?q?
j=1
[[yi:i+q = y?j:j+q]] (3)
where [[?]] is the Iverson bracket (1 if input condition
is true, 0 otherwise) and yi:j is the subsequence of y
from word i to word j (inclusive).
Importantly, Eq. (2) can be solved with no change
to the decoding algorithm. The dissimilarity terms
can simply be incorporated as an additional lan-
guage model in ARPA format that sets the log-
probability to the negated count for each n-gram
in previous diverse translations, and sets to zero
all other n-grams? log-probabilities and back-off
weights.
1101
The advantage of this dissimilarity function is its
simplicity. It can be easily used with any transla-
tion system that uses n-gram language models with-
out any change to the decoder. Indeed, we use both
phrase-based and hierarchical phrase-based models
in our experiments below.
4 Related Work
MT researchers have recently started to con-
sider diversity in the context of system combina-
tion (Macherey and Och, 2007). Most closely-
related is work by Devlin and Matsoukas (2012),
who proposed a way to generate diverse transla-
tions by varying particular ?traits,? such as transla-
tion length, number of rules applied, etc. Their ap-
proach can be viewed as solving Eq. (2) with a richer
dissimilarity function that requires a special-purpose
decoding algorithm. We chose our n-gram dissimi-
larity function due to its simplicity and applicability
to most MT systems without requiring any change
to decoders.
Among other work, Xiao et al (2013) used bag-
ging and boosting to get diverse system outputs for
system combination and Cer et al (2013) used mul-
tiple identical systems trained jointly with an objec-
tive function that encourages the systems to generate
complementary translations.
There is also similarity between our approach and
minimum Bayes risk decoding (Kumar and Byrne,
2004), variational decoding (Li et al, 2009), and
other ?consensus? decoding algorithms (DeNero et
al., 2009). These all seek a single translation that
is most similar on average to the model?s preferred
translations. In this way, they try to capture the
model?s range of beliefs in a single translation. We
instead seek a set of translations that, when consid-
ered as a whole, similarly express the full range of
the model?s beliefs about plausible translations for
the input.
Also related is work on determinantal point pro-
cesses (DPPs; Kulesza and Taskar, 2010), an ele-
gant probabilistic model over sets of items that nat-
urally prefers diverse sets. DPPs have been ap-
plied to summarization (Kulesza and Taskar, 2011)
and discovery of topical threads in document collec-
tions (Gillenwater et al, 2012). Unfortunately, in
the structured setting, DPPs make severely restric-
tive assumptions on the scoring function, while our
framework does not.
5 Experimental Setup
We now embark on an extensive empirical evalua-
tion of the framework presented above. We begin
by analyzing our diverse sets of translations, show-
ing how they differ from standard M -best lists (Sec-
tion 6), followed by three tasks that illustrate how di-
versity can be exploited to improve translation qual-
ity: system combination (Section 7), discrimina-
tive reranking (Section 8), and a novel human post-
editing task (Section 9). In the remainder of this sec-
tion, we describe details of our experimental setup.
5.1 Language Pairs and Datasets
We use three language pairs: Arabic-to-English
(AR?EN), Chinese-to-English (ZH?EN), and
German-to-English (DE?EN). For AR?EN and
DE?EN, we used a phrase-based model (Koehn et
al., 2003) and for ZH?EN we used a hierarchical
phrase-based model (Chiang, 2007).
Each language pair has two tuning and one test
set: TUNE1 is used for tuning the baseline sys-
tems with minimum error rate training (MERT; Och,
2003), TUNE2 is used for training system combin-
ers and rerankers, and TEST is used for evaluation.
There are four references for AR?EN and ZH?EN
and one for DE?EN.
For AR?EN, we used data provided by the LDC
for the NIST evaluations, which includes 3.3M sen-
tences of UN data and 982K sentences from other
(mostly news) sources. Arabic text was prepro-
cessed using an HMM segmenter that splits attached
prepositional phrases, personal pronouns, and the
future marker (Lee et al, 2003). The common stylis-
tic sentence-initial w+ (and) clitic was removed.
The resulting corpus contained 130M Arabic tokens
and 130M English tokens. We used the NIST MT06
test set as TUNE1, a 764-sentence subset of MT05 as
TUNE2, and MT08 as TEST.
For ZH?EN, we used 303k sentence pairs from
the FBIS corpus (LDC2003E14). We segmented
the Chinese data using the Stanford Chinese seg-
menter (Chang et al, 2008) in ?CTB? mode, giving
us 7.9M Chinese tokens and 9.4M English tokens.
We used the NIST MT02 test set as TUNE1, MT05
1102
as TUNE2, and MT03 as TEST.
For DE?EN, we used data released for the
WMT2011 shared task (Callison-Burch et al, 2011).
German compound words were split using a CRF
segmenter (Dyer, 2009). We used the WMT2010
test set as TUNE1, the 2009 test set as TUNE2, and
the 2011 test set as TEST.
5.2 Baseline Systems
We used the Moses MT toolkit (Koehn et al,
2007; Hoang et al, 2009) with default settings
and features for both phrase-based and hierarchi-
cal systems. Word alignment was done using
GIZA++ (Och and Ney, 2003) in both directions,
with the grow-diag-final-and heuristic used
to symmetrize the alignments and a max phrase
length of 7 used for phrase extraction.
Language models used the target side of the paral-
lel corpus in each case augmented with 24.8M lines
(601M tokens) of randomly-selected sentences from
the Gigaword v4 corpus (excluding the NY Times
and LA Times). We used 5-gram models, estimated
using the SRI Language Modeling toolkit (Stolcke,
2002) with modified Kneser-Ney smoothing (Chen
and Goodman, 1998). The minimum count cut-off
for unigrams, bigrams, and trigrams was 1 and the
cut-off for 4-grams and 5-grams was 3. Language
model inference used KenLM (Heafield, 2011).
Uncased IBM BLEU was used for evaluation (Pa-
pineni et al, 2002). MERT was used to train the fea-
ture weights for the baseline systems on TUNE1. We
used the learned parameters to generate M -best and
diverse lists for TUNE2 and TEST to use for subse-
quent experiments.
5.3 Diverse List Generation
Generating diverse translations depends on two hy-
perparameters: the n-gram order used by the dissim-
ilarity function ?n (?3.2) and the ?j weights on the
dissimilarity terms in Eq. (2). Though our frame-
work permits different ?j for each j, we use a sin-
gle ? value for simplicity, as was also done in (Ba-
tra et al, 2012). The values of n and ? were tuned
on a 200 sentence subset of TUNE1 separately for
each language pair (which we call TUNE200), so as
to maximize the oracle BLEU score of the diverse
AR?EN ZH?EN DE?EN
1 best 50.1 36.9 21.8
20 best 54.0 40.3 24.7
200 best 57.5 43.8 27.7
1000 best 59.8 46.4 29.8
unique 20 best 56.6 44.1 26.7
unique 200 best 59.6 46.4 29.5
20 diverse 58.5 46.4 28.6
20 div ? 10 best 61.3 48.7 30.3
20 div ? 50 best 63.2 50.6 31.6
Table 1: Oracle BLEU scores on TEST for various sizes
of M -best and diverse lists. Unique lists were obtained
from 1,000-best lists and therefore may not contain the
target number of unique translations for all sentences.
lists.1 We considered n values in {2, 3, . . . , 9} and
? values in {0.005, 0.01, 0.05, 0.1}. We give details
on optimal values for these hyperparameters when
discussing particular tasks below.
Though simple, our approach is computationally
expensive as M grows because it requires decoding
M times for each sentence. So, we assumeM ? 20.
But we also extract an N -best list for each of the M
diverse translations.2 Many MT decoders, including
the phrase-based and hierarchical implementations
in Moses, permit efficient extraction of N -best lists,
so we exploit this to obtain larger lists that still ex-
hibit diversity. But we note that these N -best lists
for each diverse solution are not in themselves di-
verse; with more computational power or more effi-
cient algorithms (Devlin and Matsoukas, 2012) we
could potentially generate larger, more diverse lists.
6 Analysis of Diverse Lists
We now characterize our diverse lists by compar-
ing them to M -best lists. Table 1 shows oracle
BLEU scores on TEST for M -best lists, unique M -
best lists, and diverse lists of several sizes. To get
unique lists, we first generated 1000-best lists, then
retained only the highest-scoring derivation for each
unique translation. When comparingM -best and di-
verse lists of comparable size, the diverse lists al-
1Since BLEU does not decompose additively across seg-
ments, we chose translations for individual sentences that max-
imized BLEU+1 (Lin and Och, 2004), then computed ?oracle?
corpus BLEU of these translations.
2We did not consider n-grams from previous N -best lists
when computing the dissimilarity function, but only those from
the previous diverse translations.
1103
0 
10 
20 
30 
40 
50 
60 
70 
0-25 25-36 36-47 47-94 
%B
LEU
 
1-best BLEU bin 
20 best 20 diverse 
Figure 1: Median, min, and max BLEU+1 of 20-best
and 20-diverse lists for the ZH?EN test set, divided into
quartiles according to the BLEU+1 score of the 1-best
translation, and averaged across sentences in each quar-
tile. Heights of the bars show median and ?error bars?
indicate max and min.
ways have higher oracle BLEU. The differences are
largest when comparing 20-best lists and 20-diverse
lists, where they range from 4 to 6 BLEU points.
When generating these diverse lists, we used the
n and ? values that were tuned for each language
pair to maximize oracle BLEU on TUNE200 for the
?20 div ? 50 best? configuration. The optimal val-
ues of n were 6 for ZH?EN and AR?EN and 7 for
DE?EN.3 When instead tuning to maximize oracle
BLEU for 20-diverse lists, the optimal n stayed at
7 for DE?EN, but increased to 7 for AR?EN and 9
for ZH?EN. These values are noticeably larger than
n-gram sizes typically used in language modeling
and evaluation. They suggest that for optimal ora-
cle BLEU, translations with long-spanning amounts
of repeated material should be avoided, while short
overlapping n-grams are permitted.
Figure 1 shows other statistics on TEST for
ZH?EN. Plots for AR?EN and DE?EN are quali-
tatively similar. We divided the TEST sentences into
quartiles based on BLEU+1 of the 1-best transla-
tions from the baseline system. We computed the
median, min, and max BLEU+1 on each list and av-
eraged over the sentences in each quartile. As shown
in the plot, the ranges of 20-diverse lists subsume
those of 20-best lists, though the medians of diverse
3The optimal values of ? were 0.005 for AR?EN and 0.01
for ZH?EN and DE?EN. Since these values depend on the
scale of the weights learned by MERT, they are difficult to in-
terpret in isolation.
lists drop when the baseline system has high BLEU
score. This matches intuition: when the baseline
system is performing well, forcing it to find different
translations is likely to result in worse translations.
So we may expect diverse lists to be most helpful for
more difficult sentences, a point we return to in our
experiments below.
7 System Combination Experiments
One way to evaluate the quality of our diverse lists
is to use them in system combination, as was sim-
ilarly done by Devlin and Matsoukas (2012) and
Cer et al (2013). We use the system combination
framework of Heafield and Lavie (2010b), which
has an open-source implementation (Heafield and
Lavie, 2010a).4
We use our baseline systems (trained on TUNE1)
to generate lists for system combination on TUNE2
and TEST. We compareM -best lists, uniqueM -best
lists, and M -diverse lists, with M ? {10, 15, 20}.5
For each choice of list type and M , we trained the
system combiner on TUNE2 and tested on TEST with
the learned parameters. System combination hyper-
parameters (whether to use feature length normal-
ization; the size of the k-best lists generated by the
system combiner during tuning, k ? {300, 600})
were chosen to maximize BLEU on TUNE200. Also,
we removed the individual features from the
default feature set because they correspond to in-
dividual systems in the combination; they did not
seem appropriate for us since our hypotheses all
come from the same system.
The results are shown in Table 2. Like Devlin and
Matsoukas (2012), we see no gain from system com-
bination using M -best lists. We see some improve-
ment with unique lists, particularly for AR?EN, al-
though it is not consistent across M values. But
we see larger improvements with diverse lists for
AR?EN and ZH?EN. For these language pairs, our
4The implementation uses MERT to tune parameters, but we
found this to be time-consuming and noisy for the larger feature
sets. So we used a structured support vector machine learning
framework instead (described in Section 8), using multiple it-
erations of learning interleaved with (system combiner) N -best
list generation, and accumulating N -best lists across iterations.
5Dissimilarity hyperparameters n and ? were again chosen
to maximize oracle BLEU on TUNE200, separately for each M
and for each language pair.
1104
AR?EN ZH?EN DE?EN
10 15 20 10 15 20 10 15 20
baseline (no system combination) 50.1 36.9 21.8
M -best 50.2 50.1 50.0 36.7 36.9 37.0 21.7 21.7 21.8
unique M -best (from 1000-best list) 50.6 50.0 50.8 37.1 36.9 37.1 21.8 21.9 21.9
M -diverse 51.4 51.2 51.2 37.6 37.6 37.5 22.0 21.8 21.6
Table 2: System combination results (%BLEU on TEST). Size of lists is M ? {10, 15, 20}. Highest score in each
column is bold.
AR?EN ZH?EN DE?EN
q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4
baseline 30.1 44.1 55.1 70.0 15.2 28.9 41.0 57.5 5.3 14.4 23.7 40.9
15-best 30.1 44.6 55.5 68.8 15.9 29.2 40.5 56.8 6.0 15.0 23.6 40.0
unique 15-best 30.4 44.7 55.2 68.4 16.7 29.0 41.2 56.6 5.9 14.9 23.8 40.6
15-diverse 31.3 45.3 57.8 69.1 17.7 30.6 41.7 56.9 7.6 15.2 23.4 39.6
Table 3: System combination results (%BLEU on quartiles of TEST, M = 15). Source sentences were divided into
quartiles (numbered ?qn?) according to BLEU+1 of the 1-best translations of the baseline system. Highest score in
each column is bold.
gains are similar to those seen by Devlin and Mat-
soukas, but use our simpler dissimilarity function.6
For DE?EN, results are similar for all settings and
do not show much improvement from system com-
bination.
In Table 3, we break down the scores according
to 1-best BLEU+1 quartiles, as done in Figure 1.7
In general, we find the largest gains for the low-
BLEU translations. For the two worst BLEU quar-
tiles, we see gains of 1.2 to 2.5 BLEU points, while
the gains shrink or disappear entirely for the best
quartile. This may be a worthwhile trade-off: a
large improvement in the worst translations may be
more significant to users than a smaller degredation
on sentences that are already being translated well.
In addition, quality estimation (Specia et al, 2011;
Bach et al, 2011) could be used to automatically de-
termine the BLEU quartile for each sentence. Then
system combination of diverse translations might be
used only when the 1-best translation is predicted to
be of low quality.
8 Reranking Experiments
We now turn to discriminative reranking, which has
frequently been used to easily add rich features to
a model. It has been used for MT with varying de-
6They reported +0.8 BLEU from system combination for
AR?EN, and saw a further +0.5?0.7 from their new features.
7Quartile points are: 39, 49, 61 for AR?EN; 25, 36, and 47
for ZH?EN; and 14.5, 21.1, and 30.3 for DE?EN.
gree of success (Och et al, 2004; Shen et al, 2004;
Hildebrand and Vogel, 2008); some have attributed
its mixed results to a lack of diversity in the M -best
lists traditionally used. We propose diverse lists as a
way to address this concern.
8.1 Learning Framework
Several learning formulations have been proposed
for M -best reranking. One commonly-used ap-
proach in MT is MERT, used in the reranking ex-
periments of Och et al (2004) and Hildebrand and
Vogel (2008), among others. We experimented with
MERT and other algorithms, including pairwise
ranking optimization (Hopkins and May, 2011), but
we found best results using the approach of Yadol-
lahpour et al (2013), who used a slack-rescaled
structured support vector machine (Tsochantaridis
et al, 2005) with L2 regularization. As a sentence-
level loss, we used negated BLEU+1. We used the
1-slack cutting-plane algorithm of Joachims et al
(2009) for optimization during learning.8 A more
detailed description of the reranker is provided in the
supplementary material.
We used 5-fold cross-validation on TUNE2 to
choose the regularization parameter C from the set
{0.01, 0.1, 1, 10}. We selected the value yielding
the highest average BLEU score across the held-out
8Our implementation uses OOQP (Gertz and Wright, 2003)
to solve the quadratic program in the inner loop, which uses
HSL, a collection of Fortran codes for large-scale scientific
computation (www.hsl.rl.ac.uk).
1105
folds. This value was then used for one final round
of training on the entirety of TUNE2. Additionally,
we tuned the decision to return the parameters at
convergence or those that produced the highest train-
ing corpus BLEU score. Since we use a sentence-
level metric during training (BLEU+1) and a corpus-
level metric for final evaluation (BLEU), we found
that it was often better to return parameters that pro-
duced the highest training BLEU score.
This tuning procedure was repeated for each fea-
ture set and for each list type (M -best or diverse).
The test set was not used for any of this tuning.
8.2 Features
In addition to the features from the baseline models
(14 for phrase-based, 8 for hierarchical), we add 36
more for reranking:
Inverse Model 1 (INVMOD1): We added the ?in-
verse? versions of the three IBM Model 1 features
described in Section 2.2 of Hildebrand and Vogel
(2008). The first is the probability of the source sen-
tence given the translation under IBM Model 1, the
second replaces the
?
with a max in the first fea-
ture, and the third computes the percentage of words
whose lexical translation probability falls below a
threshold. We also include versions of the first 2
features normalized by the translation length, for a
total of 5 INVMOD1 features.
Large LM (LLM): We created a large 4-gram LM
by interpolating LMs from the WMT news data, Gi-
gaword, Europarl, and the DE?EN news commen-
tary (NC) corpus to maximize likelihood of a held-
out development set (WMT08 test set). We used the
average per-word log-probability as the single fea-
ture function in this category.
Syntactic LM (SYN): We used the syntactic treelet
language model of Pauls and Klein (2012) to com-
pute two features: the translation log probability and
the length-normalized log probability.
Finite/Non-Finite Verbs (VERB): We ran the Stan-
ford part-of-speech (POS) tagger (Toutanova et al,
2003) on each translation and added four features:
the fraction of words tagged as finite/non-finite
verbs, and the fraction of verbs that are finite/non-
finite.9
9Words tagged as MD, VBP, VBZ, and VBD were counted
Reranking AR?EN ZH?EN DE?EN
features best div best div best div
N/A (baseline) 50.1 36.9 21.8
None 50.5 50.7 37.3 37.1 21.9 21.6
+ INVMOD1 50.3 50.8 37.6 37.1 22.0 21.8
+ LLM, SYN 50.5 51.1 37.4 37.3 21.7 21.7
+ VERB, DISC 50.4 51.3 37.3 37.3 21.9 22.2
+ GOOG 50.7 51.3 36.8 37.1 21.9 22.2
+ WCLM 51.2 51.8 37.3 37.4 22.2 22.3
Table 4: Reranking results (%BLEU on TEST).
Discriminative Word/Tag LMs (DISC): For each
language pair, we generated 10,000-best lists for
TUNE1 and computed BLEU+1 for each. From
these lists, we estimated 3- and 5-gram LMs,
weighting the n-gram counts by the BLEU+1
scores.10 We repeated this procedure except using
1 minus BLEU+1 as the weight (learning a language
model of ?bad? translations). This yielded 4 fea-
tures. The procedure was then repeated using POS
tags instead of words, for 8 features in total.
Google 5-Grams (GOOG): Translations were com-
pared to the Google 5-gram corpus (LDC2006T13)
to compute: the number of 5-grams that matched,
the number of 5-grams that missed, and a set of
indicator features that fire if the fraction of 5-
grams that matched in the sentence was greater than
{0.05, 0.1, 0.2, . . . , 0.9}, for a total of 12 features.
Word Cluster LMs (WCLM): Using an imple-
mentation provided by Liang (2005), we performed
Brown clustering (Brown et al, 1992) on 900k En-
glish sentences, including the NC corpus and ran-
dom sentences from Gigaword. We clustered words
that appeared at least twice, once with 300 clus-
ters and again with 1000. We then replaced words
with their clusters in a large corpus consisting of
the WMT news data, Gigaword, and the NC data.
An additional cluster label was used for unknown
words. For each of the clusterings (300 and 1000),
we estimated 5- and 7-gram LMs with Witten-Bell
smoothing (Witten and Bell, 1991). We added 4 fea-
tures to the reranker, one for the log-probability of
the translation under each of the word cluster LMs.
as finite verbs, and VB, VBG, and VBN were non-finite verbs.
10Before estimating LMs, we projected the sentence weights
so that the min and max per source sentence were 0 and 1.
1106
List type
Features
None All
20 best 50.3 50.6
100 best 50.6 50.8
200 best 50.4 51.2
1000 best 50.5 51.2
unique 20 best 50.5 51.2
unique 100 best 50.6 51.2
unique 200 best 50.4 51.3
20 diverse 50.5 51.1
20 div ? 5 best 50.6 51.4
20 div ? 10 best 50.7 51.3
20 div ? 50 best 50.7 51.8
Table 5: List comparison for AR?EN reranking.
8.3 Results
Our results are shown in Table 4. We report results
using the baseline system alone (labeled ?N/A (base-
line)?), and reranking standard M -best lists and our
diverse lists. For diverse lists, we use the ?20 div ?
50 best? lists described in Section 5.3, with the tuned
dissimilarity hyperparameters reported in Section 6.
In the reranking settings, we also report results with-
out adding any additional features (the row labeled
?None?).11
The remaining rows add features. For AR?EN,
we see the largest gains, both over the baseline as
well as differences betweenM -best lists and diverse
lists. When using all features, we achieve a gain
of 0.6 BLEU over M -best reranking and 1.7 BLEU
points over the baseline system. The difference of
0.6 BLEU is consistent across feature subsets. We
found the WCLM features to give the largest in-
dividual improvement, with the remaining feature
sets each contributing a small amount. For Chinese
and German, the gains and individual differences are
smaller. Nonetheless, diverse lists appear to be more
robust for these language pairs as features are added.
In Table 5, we compare several sizes and types of
lists for AR?EN reranking both with no additional
features and with the full set. We see that using 20-
diverse lists nearly matches the performance of 200-
best lists. Also, retaining 50-best lists for each di-
verse solution improves BLEU by 0.7.
11Though such results have not always been reported in prior
work on reranking, we generally found them to improve over
the baseline, presumably because seeing more data improves
generalization ability.
Train
best div
Te
st best 51.2 51.7
div 50.5 51.8
Table 6: Comparing M -best and diverse lists for train-
ing/testing (AR?EN, all features).
Thus far, when training the reranker on M -best
lists, we tested it on M -best lists, and similarly for
diverse lists. Table 6 shows what happens with the
other two pairings for AR?EN with the full feature
set. When training on diverse lists, we see very lit-
tle difference in BLEU whether testing on M -best
or diverse lists. This has a practical benefit: we can
use (computationally-expensive) diverse lists during
offline training and then use fast M -best lists at test
time. When training on M -best lists and testing
on diverse lists, we see a substantial drop (51.2 vs
50.5). The reranker may be overfitting to the limited
scope of translations present in typical M -best lists,
thereby hindering its ability to correctly rank diverse
lists at test time. These results suggest that part of
the benefit of using diverse lists comes from seeing
a larger portion of the output space during training.
9 Human Post-Editing Experiments
We wanted to determine whether diverse translations
could be helpful to users struggling to understand
the output of an imperfect MT system. We con-
sider a post-editing task in which users are presented
with translation output without the source sentence,
and are asked to improve it. This setting has been
studied; e.g., Koehn (2010) presented evidence that
monolingual speakers could often produce improved
translations for this task, occasionally reaching the
level of an expert translator.
Here, we use a novel variation of this task in
which multiple translations are shown to editors. We
compare the use of entries from an M -best list and
entries from a diverse list. Again, the original source
sentence is not provided. Our goal is to determine
whether multiple, diverse translations can help users
to more accurately guess the meaning of the original
sentence than entries from a standard M -best list. If
so, commercial MT systems might permit users to
request additional diverse translations for those sen-
tences whose model-best translations are difficult to
understand.
1107
9.1 Translation List Post-Editing
We use Amazon Mechanical Turk (MTurk) for this
experiment. Workers are shown 3 outputs from an
MT system. They are not shown the original sen-
tence, nor are they shown a reference. Based on
the 3 imperfect translations, they are asked to write
a single fluent English translation that best cap-
tures the understood meaning. Half of the time, the
worker is shown 3 entries from an M -best list, and
the other half of the time 3 entries from a diverse
list. We then compare the outputs produced under
the two conditions. The goal is to measure whether
workers are able to produce translations that are
closer in meaning to the (unseen) references when
shown diverse translations. We refer to this task as
the EDITING task.
To evaluate the outputs, we use a second task in
which users are shown a reference translation along
with two outputs from the first task: one created
from M -best lists and one from diverse lists. Work-
ers in this task are asked to choose which translation
is a better match to the reference in terms of mean-
ing, or they can indicate that the translations are of
the same quality. We refer to this second task as the
EVAL task.
9.2 Dissimilarity Functions
To generate diverse lists for the EDITING task, we
use the same dissimilarity function as in reranking,
but we tune the hyperparameters n and ? differently.
Since our expectation here is that workers may com-
bine information from multiple translations to pro-
duce a superior output, we are interested in the cov-
erage of the translations in the diverse list, rather
than the oracle BLEU score.
We designed a metric based on coverage of entire
lists of translations. It is similar to BLEU+1, except
(1) it uses n-gram recalls instead of n-gram preci-
sions, (2) there is no brevity penalty term, and (3) it
compares a list to a set of references and any trans-
lation in the list can contribute a match of an n-gram
in any reference. Like BLEU, counts are clipped
based on those in the references. We maximized
this metric over diverse lists of length 5, for n ?
{2, 3, . . . , 9} and ? ? {0.005, 0.01, 0.05, 0.1, 0.2}.
The optimal values for AR?EN were n = 4 and
? = 0.1, while for ZH?EN they were n = 4 and
? = 0.2. These n values are smaller than for rerank-
ing, and the ? values are larger. This suggests that,
when maximizing coverage of a small diverse list,
more dissimilarity is desired among the translations.
9.3 Detailed Procedure
We focused on AR?EN and ZH?EN for this study.
We sampled 200 sentences from their test sets, cho-
sen from among those whose reference translation
was between 5 and 25 words. We generated a unique
5-best list for each sentence using our baseline sys-
tem (described in Section 5.2) and also generated a
diverse list of length 5 using the dissimilarity func-
tion ? with hyperparameters tuned using the proce-
dure from the previous section. We untokenized and
truecased the translations. We dropped non-ASCII
characters because we feared they would confuse
our workers. As a result, workers must contend with
missing words in the output, often proper nouns.
Given the 2 lists for each sentence, we sampled
two integers i, j ? {2, 3, 4, 5} without replacement.
The indices i and j indicate two entries from the
lists. We took translations 1, i, and j from the 5-best
list and created an EDITING task from them. We did
the same using entries 1, i, and j from the diverse
list. We repeated this process 3 times for each sen-
tence, obtaining 3? 2 = 6 tasks for each, giving us
a total of 1,200 EDITING tasks per language pair.
The outputs of the EDITING tasks were evaluated
with EVAL tasks. For each sentence, we had 3 post-
edited outputs generated using entries in 5-best lists
and 3 post-edited outputs from diverse lists. We cre-
ated EVAL tasks for all 9 output pairs, for all 200
sentences per language pair. We additionally gave
each task to three MTurk workers. This gave us
10,800 evaluation judgments for the EVAL task.
9.4 Results
Figure 2 shows the quartile breakdown for judg-
ments collected from the EVAL task. The Y axis
represents the percentage of judgments for which
best/diverse outputs were preferred; the missing per-
centage for each bin is accounted for by ?same?
judgments.
We observe an interesting phenomenon. Overall,
there is a slight preference for the post-edited out-
puts of M -best entries (?best?) over those from di-
verse translations (?div?); this preference is clearest
1108
0?34 34?46 46?62 62?9420
25
30
35
40
45 Arabic?English
% C
hos
en
0?23 23?36 36?50 50?94
20
30
40
BLEU Bin
% C
hos
en
Chinese?English
 
 best
div
Figure 2: Percentages in which post-edited output given
M -best entries (?best?) was preferred by human eval-
uators as compared to post-edited output given diverse
translations (?div?), broken down by the BLEU+1 score
of the 1-best translation for the sentences. When the base-
line system is doing poorly, diversity helps post-editors to
produce better translations.
when the baseline system?s 1-best translation had a
high BLEU score. However, we see this trend re-
versed for sentences in which the baseline system?s
1-best translation had a low BLEU score. In general,
when the BLEU score of the baseline system is be-
low 35, it is preferable to give diverse translations to
users for post-editing. But when the baseline system
does very well, diverse translations do not contribute
anything, and in fact hurt because they may distract
users from the high-quality (and typically very sim-
ilar) translations from the 5-best lists.
Estimation of the quality of the output (?confi-
dence estimation?) has recently gained interest in
the MT community (Specia et al, 2011; Bach et
al., 2011; Callison-Burch et al, 2012; Bojar et al,
2013), including specifically for post-editing (Tat-
sumi, 2009; Specia, 2011; Koponen, 2012). Future
work could investigate whether such automatic con-
fidence estimation could be used to identify situa-
tions in which diverse translations can be helpful for
aiding user understanding.
10 Future Work
Our dissimilarity function captures diversity in the
particular phrases used by an MT system, but for
certain applications we may prefer other types of di-
versity. Defining the dissimilarity function on POS
tags or word clusters would help us to capture stylis-
tic patterns in sentence structure, as would targeting
syntactic structures in syntax-based translation.
A weakness of our approach is its computational
expense; by contrast, the method of Devlin and Mat-
soukas (2012) obtains diverse translations more ef-
ficiently by extracting them from a single decoding
of an input sentence (albeit with a wide beam). We
expect their ideas to be directly applicable to our set-
ting in order to get diverse solutions more cheaply.
We also plan to explore methods of explicitly target-
ing multiple, diverse solutions as part of the search
algorithm.
Finally, M -best lists are currently used to ap-
proximate structured spaces for many areas of MT,
including tuning (Och, 2003), minimum Bayes
risk decoding (Kumar and Byrne, 2004), and
pipelines (Venugopal et al, 2008). Future work
could replace M -best lists with diverse lists in these
and related tasks, whether for MT or other areas of
structured NLP.
Acknowledgments
We thank the anonymous reviewers as well as Colin
Cherry, Kenneth Heafield, Silja Hildebrand, Fei
Huang, Dan Klein, Adam Pauls, and Bing Xiang.
DB was partially supported by the National Science
Foundation under Grant No. 1353694.
References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness:
A method for measuring machine translation confi-
dence. In Proc. of ACL.
D. Batra, P. Yadollahpour, A. Guzman-Rivera, and
G. Shakhnarovich. 2012. Diverse M-best solutions
in Markov random fields. In Proc. of ECCV.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proc. of WMT.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram mod-
1109
els of natural language. Computational Linguistics,
18.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 Workshop on Statistical
Machine Translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 Work-
shop on Statistical Machine Translation. In Proc. of
WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2013. Positive
diversity tuning for machine translation system com-
bination. In Proc. of WMT.
P. Chang, M. Galley, and C. D. Manning. 2008. Opti-
mizing Chinese word segmentation for machine trans-
lation performance. In Proc. of WMT.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. of ACL.
S. Chatterjee and N. Cancedda. 2010. Minimum error
rate training by sampling the translation lattice. In
Proc. of EMNLP.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1).
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
J. DeNero, D. Chiang, and K. Knight. 2009. Fast con-
sensus decoding over translation forests. In Proc. of
ACL.
J. Devlin and S. Matsoukas. 2012. Trait-based hypoth-
esis selection for machine translation. In Proc. of
NAACL.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proc. of HLT-
NAACL.
C. Dyer. 2010. A Formal Model of Ambiguity and its Ap-
plications in Machine Translation. Ph.D. thesis, Uni-
versity of Maryland.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
E. M. Gertz and S. J. Wright. 2003. Object-oriented soft-
ware for quadratic programming. ACM Transactions
on Mathematical Software, 29(1).
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Discov-
ering diverse and salient threads in document collec-
tions. In Proc. of EMNLP.
K. Heafield and A. Lavie. 2010a. Combining machine
translation output with open source: The Carnegie
Mellon multi-engine machine translation scheme. The
Prague Bulletin of Mathematical Linguistics, 93.
K. Heafield and A. Lavie. 2010b. Voting on n-grams for
machine translation system combination. In Proc. of
AMTA.
K. Heafield. 2011. Kenlm: Faster and smaller language
model queries. In Proc. of WMT.
A. Hildebrand and S. Vogel. 2008. Combination of
machine translation systems via hypothesis selection
from combined n-best lists. In Proc. of AMTA.
H. Hoang, P. Koehn, and A. Lopez. 2009. A Uni-
fied Framework for Phrase-Based, Hierarchical, and
Syntax-Based Statistical Machine Translation. In
Proc. of IWSLT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
T. Joachims, T. Finley, and C. Yu. 2009. Cutting-
plane training of structural SVMs. Machine Learning,
77(1).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
P. Koehn. 2010. Enabling monolingual translators: Post-
editing vs. options. In Proc. of NAACL.
M. Koponen. 2012. Comparing human perceptions of
post-editing effort with post-editing operations. In
Proc. of WMT.
A. Kulesza and B. Taskar. 2010. Structured determinan-
tal point processes. In Proc. of NIPS.
A. Kulesza and B. Taskar. 2011. Learning determinantal
point processes. In Proc. of UAI.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
1110
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.
2003. Language model based Arabic word segmenta-
tion. In Proc. of ACL.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
C. Lin and F. J. Och. 2004. Orange: a method for evalu-
ating automatic evaluation metrics for machine trans-
lation. In Proc. of COLING.
W. Macherey and F. J. Och. 2007. An empirical study
on computing consensus translations from multiple
machine translation systems. In Proc. of EMNLP-
CoNLL.
W. Macherey, F. J. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In Proc. of EMNLP.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
A. Pauls and D. Klein. 2012. Large-scale syntactic lan-
guage modeling with treelets. In Proc. of ACL.
A.-V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas,
R. Schwartz, and B. Dorr. 2007. Combining outputs
from multiple machine translation systems. In HLT-
NAACL.
L. Shen and A. K. Joshi. 2003. An SVM-based voting
algorithm with application to parse reranking. In Proc.
of CoNLL.
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proc. of HLT-
NAACL.
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2011.
Predicting machine translation adequacy. In Proc. of
MT Summit XIII.
L. Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proc. of
EAMT.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
M. Tatsumi. 2009. Correlation between automatic evalu-
ation metric scores, post-editing speed, and some other
factors. In Proc. of MT Summit XII.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of HLT-NAACL.
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008.
Lattice Minimum Bayes-Risk decoding for statistical
machine translation. In Proc. of EMNLP.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. JMLR, 6.
A. Venugopal, A. Zollmann, N.A. Smith, and S. Vogel.
2008. Wider pipelines: N-best alignments and parses
in MT training. In Proc. of AMTA.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4).
T. Xiao, J. Zhu, and T. Liu. 2013. Bagging and boosting
statistical machine translation systems. Artif. Intell.,
195.
P. Yadollahpour, D. Batra, and G. Shakhnarovich. 2013.
Discriminative re-ranking of diverse segmentations. In
Proc. of CVPR.
1111
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1329?1341,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Weakly-Supervised Learning with
Cost-Augmented Contrastive Estimation
Kevin Gimpel Mohit Bansal
Toyota Technological Institute at Chicago, IL 60637, USA
{kgimpel,mbansal}@ttic.edu
Abstract
We generalize contrastive estimation in
two ways that permit adding more knowl-
edge to unsupervised learning. The first
allows the modeler to specify not only the
set of corrupted inputs for each observa-
tion, but also how bad each one is. The
second allows specifying structural prefer-
ences on the latent variable used to explain
the observations. They require setting ad-
ditional hyperparameters, which can be
problematic in unsupervised learning, so
we investigate new methods for unsuper-
vised model selection and system com-
bination. We instantiate these ideas for
part-of-speech induction without tag dic-
tionaries, improving over contrastive esti-
mation as well as strong benchmarks from
the PASCAL 2012 shared task.
1 Introduction
Unsupervised NLP aims to discover useful struc-
ture in unannotated text. This structure might
be part-of-speech (POS) tag sequences (Merialdo,
1994), morphological segmentation (Creutz and
Lagus, 2005), or syntactic structure (Klein and
Manning, 2004), among others. Unsupervised
systems typically improve when researchers incor-
porate knowledge to bias learning to capture char-
acteristics of the desired structure.
1
There are many successful examples of adding
knowledge to improve learning without labeled
examples, including: sparsity in POS tag distri-
butions (Johnson, 2007; Ravi and Knight, 2009;
Ganchev et al., 2010), short attachments for
dependency parsing (Smith and Eisner, 2006),
1
We note that doing so strains the definition of the term
unsupervised. Hence we will use the term weakly-supervised
to refer to methods that do not explicitly train on labeled ex-
amples for the task of interest, but do use some form of task-
specific knowledge.
agreement of word alignment models (Liang et
al., 2006), power law effects in lexical distribu-
tions (Blunsom and Cohn, 2010; Blunsom and
Cohn, 2011), multilingual constraints (Smith and
Eisner, 2009; Ganchev et al., 2009; Snyder et al.,
2009; Das and Petrov, 2011), and orthographic
cues (Spitkovsky et al., 2010c; Spitkovsky et al.,
2011b), inter alia.
Contrastive estimation (CE; Smith and Eisner,
2005) is a general approach to weakly-supervised
learning with a particular way of incorporating
knowledge. CE increases the likelihood of the ob-
servations at the expense of those in a particular
neighborhood of each observation. The neighbor-
hood typically contains corrupted versions of the
observations. The latent structure is marginalized
out for both the observations and their corruptions;
the intent is to learn latent structure that helps to
explain why the observation was generated rather
than any of the corrupted alternatives.
In this paper, we present a new objective func-
tion for weakly-supervised learning that general-
izes CE by including two types of cost functions,
one on observations and one on output structures.
The first (?4) allows us to specify not only the set
of corrupted observations, but also how bad each
corruption was. We use n-gram language models
to measure the severity of each corruption.
The second (?5) allows us to specify prefer-
ences on desired output structures, regardless of
the input sentence. For POS tagging, we attempt
to learn language-independent tag frequencies by
computing counts from treebanks for 11 languages
not used in our POS induction experiments. For
example, we encourage tag sequences that contain
adjacent nouns and penalize those that contain ad-
jacent adpositions.
We consider several unsupervised ways to set
hyperparameters for these cost functions (?7), in-
cluding the recently-proposed log-likelihood esti-
mator of Bengio et al. (2013). We also circumvent
1329
hyperparameter selection via system combination,
developing a novel voting scheme for POS induc-
tion that aligns tag identifiers across runs.
We evaluate our approach, which we call cost-
augmented contrastive estimation (CCE), on
POS induction without tag dictionaries for five
languages from the PASCAL shared task (Gelling
et al., 2012). We find that CCE improves over both
standard CE as well as strong baselines from the
shared task. In particular, our final average accu-
racies are better than all entries in the shared task
that use the same number of tags.
2 Related Work
Weakly-supervised techniques can be roughly cat-
egorized in terms of whether they influence the
model, the learning procedure, or explicitly target
the output structure. Examples abound in NLP;
we focus on those that have been applied to POS
tagging.
There have been many efforts at biasing
models, including features (Smith and Eisner,
2005a; Berg-Kirkpatrick et al., 2010), sparse
priors (Johnson, 2007; Goldwater and Griffiths,
2007; Toutanova and Johnson, 2007), sparsity
in tag transition distributions (Ravi and Knight,
2009), small models via minimum description
length criteria (Vaswani et al., 2010; Poon et al.,
2009), a one-tag-per-type constraint (Blunsom and
Cohn, 2011), and power law effects via Bayesian
nonparametrics (Van Gael et al., 2009; Blunsom
and Cohn, 2010; Blunsom and Cohn, 2011).
We focus below on efforts that induce bias into
the learning (?2.1) or more directly in the output
structure (?2.2), as they are more closely related
to our contributions in this paper.
2.1 Biasing Learning
Some unsupervised methods do not change the
model or attempt to impose structural bias; rather,
they change the learning. This may involve op-
timizing a different objective function for the
same model, e.g., by switching from soft to hard
EM (Spitkovsky et al., 2010b). Or it may in-
volve changing the objective during learning via
annealing (Smith and Eisner, 2004) or more gen-
eral multi-objective techniques (Spitkovsky et al.,
2011a; Spitkovsky et al., 2013).
Other learning modifications relate to automatic
data selection, e.g., choosing examples for genera-
tive learning (Spitkovsky et al., 2010a) or automat-
ically generating negative examples for discrimi-
native unsupervised learning (Li et al., 2010; Xiao
et al., 2011).
CE does both, automatically generating nega-
tive examples and changing the objective function
to include them. Our observation cost function al-
ters CE?s objective function, sharpening the effec-
tive distribution of the negative examples.
2.2 Structural Bias
Our output cost function is used to directly spec-
ify preferences on desired output structures. Sev-
eral others have had similar aims. For dependency
grammar induction, Smith and Eisner (2006) fa-
vored short attachments using a fixed-weight fea-
ture whose weight was optionally annealed during
learning. Their bias could be implemented as an
output cost function in our framework.
Posterior regularization (PR; Ganchev et al.,
2010) is a general framework for declaratively
specifying preferences on model outputs. Naseem
et al. (2010) proposed universal syntactic rules for
unsupervised dependency parsing and used them
in a PR regime; we use analogous universal tag
sequences in our cost function.
Our output cost is similar to posterior regular-
ization. The difference is that we specify pref-
erences via an arbitrary cost function on output
structures, while PR uses expectation constraints
on posteriors of the model. We compare to the PR
tag induction system of Grac?a et al. (2011) in our
experiments, improving over it in several settings.
2.3 Exploiting Resources
Much of the work mentioned above also benefits
from leveraging existing resources. These may be
curated or crowdsourced resources like the Wik-
tionary (Li et al., 2012), or traditional annotated
treebanks for languages other than those under in-
vestigation (Cohen et al., 2011). In this paper, we
use tag statistics from treebanks for 11 languages
to impose our structural bias for a different set of
languages used in our POS induction experiments.
Substantial recent work has improved many
NLP tasks by leveraging multilingual or paral-
lel text (Cohen and Smith, 2009; Snyder et al.,
2009; Wang and Manning, 2014), including un-
supervised POS tagging (Naseem et al., 2009; Das
and Petrov, 2011; T?ackstr?om et al., 2013; Ganchev
and Das, 2013). This sort of multilingual guidance
could also be captured by particular output cost
functions, though we leave this to future work.
1330
3 Unsupervised Structure Learning
We consider a structured unsupervised learning
setting. We use X to denote our set of possible
structured inputs, and for a particular x ? X,
we use Y(x) to denote the set of valid structured
outputs for x. We are given a dataset of inputs
{x
(i)
}
N
i=1
. To map inputs to outputs, we start by
building a model of the joint probability distribu-
tion p?(x,y). We use a log-linear parameteriza-
tion with feature vector f and weight vector ?:
p?(x,y) =
exp
{
?
>
f(x,y)
}
?
x??X,y??Y(x?) exp
{
?
>
f(x
?
,y
?
)
}
where the sum in the denominator ranges over all
possible inputs and all valid outputs for them.
In this paper, we consider ways of learning the
parameters ?. Given ?, at test time we output a y
for a new x using, e.g., Viterbi or minimum Bayes
risk decoding; we use the latter in this paper.
3.1 EM and Contrastive Estimation
We start by reviewing two ways of choosing
?. The expectation-maximization algorithm (EM;
Dempster et al., 1977) finds a local optimum of
the marginal (log-)likelihood of the observations
{x
(i)
}
N
i=1
. The marginal log-likelihood is a sum
over all x
(i)
of the gain function ?
EM
(x
(i)
):
?
EM
(x
(i)
) = log
?
y?Y(x(i))
p?(x
(i)
,y)
= log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)
}
? log
?
x??X,y??Y(x?)
exp
{
?
>
f(x
?
,y
?
)
}
? ?? ?
Z(?)
The difficulty is the final term, logZ(?), which
requires summing over all possible inputs and
all valid outputs for them. This summation is
typically intractable for structured problems, and
may even diverge. For this reason, EM is typi-
cally only used to train log-linear model weights
when Z(?) = 1, e.g., for hidden Markov models,
probabilistic context-free grammars, and models
composed of locally-normalized log-linear mod-
els (Berg-Kirkpatrick et al., 2010), among others.
There have been efforts at approximating the
summation over elements of X, whether by limit-
ing sequence length (Haghighi and Klein, 2006),
only summing over observations in the training
data (Riezler, 1999), restricting the observation
space based on the task (Dyer et al., 2011), or us-
ing Gibbs sampling to obtain an unbiased sample
of the full space (Della Pietra et al., 1997; Rosen-
feld, 1997).
Contrastive estimation (CE) addresses this chal-
lenge by using a neighborhood function N : X?
2
X
that generates a set of inputs that are ?corrup-
tions? of an input x; N(x) always includes x. Us-
ing shorthand N
i
for N(x
(i)
), CE corresponds to
maximizing the sum over inputs x
(i)
of the gain
?
CE
(x
(i)
)= log Pr(x
(i)
| N
i
)
= log
?
y?Y(x(i)) p?(x
(i)
,y)
?
x??N
i
?
y??Y(x?) p?(x
?
,y
?
)
= log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)
}
?
log
?
x??N
i
?
y??Y(x?)
exp
{
?
>
f(x
?
,y
?
)
}
Two logZ(?) terms cancel out, leaving the sum-
mation over input/output pairs in the neighbor-
hood instead of the full summation over pairs.
Two desiderata govern the choice of N. One is
to make the summation over its elements computa-
tionally tractable. If N(x) = X for all x ? X, we
obtain EM, so a smaller neighborhood typically
must be used in practice. The second considera-
tion is to target learning for the task of interest. For
POS tagging and dependency parsing, Smith and
Eisner (2005a, 2005b) used neighborhood func-
tions that corrupted the observations in systematic
ways, e.g., their TRANS1 neighborhood contains
the original sentence along with those that result
from transposing a single pair of adjacent words.
The intent was to force the learner to explain why
the given sentences were observed at the expense
of the corrupted sentences.
Next we present our modifications to con-
trastive estimation. Both can be viewed as adding
specialized cost functions that penalize some part
of the structured input/output pair.
4 Modeling Corruption Costs
While CE allows us to specify a set of corrupted
x for each x
(i)
via the neighborhood function N,
it says nothing about how bad each corruption is.
The same type of corruption might be harmful in
one context and not harmful in another.
This fact was suggested as the reason why cer-
tain neighborhoods did not work as well for POS
1331
tagging as others (Smith and Eisner, 2005a). One
poorly-performing neighborhood consisted of sen-
tences in which a single word of the original
was deleted. Deleting a single word in a sen-
tence might not harm grammaticality. By contrast,
neighborhoods that transpose adjacent words led
to better results. These kinds of corruptions are ex-
pected to be more frequently harmful, at least for
languages with relatively rigid word order. How-
ever, there may still be certain transpositions that
are benign, at least for grammaticality.
To address this, we introduce an observation
cost function ? : X ? X ? R
?0
that indicates
how much two observations differ. Using ?, we
define the following gain function ?
CCE
1
(x
(i)
) =
log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)
}
?
log
?
x??N
i
?
y??Y(x?)
exp
{
?
>
f(x
?
,y
?
) + ?(x
(i)
,x
?
)
}
The function ? inflates the score of neighbor-
hood entries with larger differences from the ob-
served x
(i)
. This gain function is inspired by ideas
from structured large-margin learning (Taskar et
al., 2003; Tsochantaridis et al., 2005), specifi-
cally softmax-margin (Povey et al., 2008; Gimpel
and Smith, 2010). Softmax-margin extends con-
ditional likelihood by allowing the user to specify
a cost function to give partial credit for structures
that are partially correct. Conditional likelihood,
by contrast, treats all incorrect structures equally.
While softmax-margin uses a cost function to
specify how two output structures differ, our gain
function ?
CCE
1
uses a cost function ? to specify
how two inputs differ. But the motivations are sim-
ilar: since poor structures have their scores artifi-
cially inflated by ?, learning pays more attention
to them, choosing weights that penalize them more
than the lower-cost structures.
4.1 Observation Cost Functions
What types of cost functions should we consider?
For efficient inference, we want to ensure that
? decomposes additively across parts of the cor-
rupted input x
?
in the same way as the features; we
assume unigram and bigram features in this paper.
In addition, the choice of the observation cost
function ? is tied to the choice of neighborhood
function. In our experiments, we use neighbor-
hoods that change the order of words in the obser-
vation but not the set of words. Our first cost func-
tion simply counts the number of novel bigrams
introduced when corrupting the original:
?
I
(x
(i)
,x) = ?
|x|+1
?
j=1
I
[
x
j?1
x
j
/? 2grams(x
(i)
)
]
where x
j
is the jth word of sentence x, x
0
is
the start-of-sentence marker, x
|x|+1 is the end-of-
sentence marker, 2grams(x) returns the set of bi-
grams in x, I[] returns 1 if its argument is true and
0 otherwise, and ? is a constant to be tuned. We
call this cost function MATCH. Only x
(i)
(which
is always contained in N
i
) is guaranteed to have
cost 0. In the TRANS1 neighborhood, corrupted
sequences will be penalized more if their transpo-
sitions occur in the middle of the sentence rather
than at the beginning or end.
We also consider a version that weights the in-
dicator by the negative log probability of the novel
bigram: ?
LM
(x
(i)
,x) =
?
|x|+1
?
j=1
?log P(x
j
|x
j?1
)I
[
x
j?1
x
j
/? 2grams(x
(i)
)
]
where P(x
j
|x
j?1
) is obtained from a bigram lan-
guage model. Among novel bigrams in the cor-
ruption x, if the second word is highly surprising
conditioned on the first, the bigram will incur high
cost. We refer to ?
LM
(x
(i)
,x) as MATLM.
5 Expressing Structural Preferences
Our second modification to CE allows us to spec-
ify structural preferences for outputs y. We first
note that there exist objective functions for su-
pervised structure prediction that never require
computing the feature vector for the true output
y
(i)
. Examples include Bayes risk (Kaiser et al.,
2000; Povey and Woodland, 2002) and structured
ramp loss (Do et al., 2008). These two objec-
tives do, however, need to compute a cost func-
tion cost(y
(i)
,y), which requires the true output
y
(i)
. We start with the following form of struc-
tured ramp loss from Gimpel and Smith (2012),
transformed here to a gain function:
max
y?Y(x(i))
(
?
>
f(x
(i)
,y)? cost(y
(i)
,y)
)
?
max
y??Y(x(i))
(
?
>
f(x
(i)
,y
?
) + cost(y
(i)
,y
?
)
)
(1)
Maximizing this gain function for supervised
learning corresponds to increasing the model score
1332
of outputs that have both high model score (?
>
f )
and low cost, while decreasing the model score of
outputs with high model score and high cost.
For unsupervised learning, we do not have y
(i)
,
so we simply drop y
(i)
from the cost function. The
result is an output cost function pi : Y ? R
?0
which captures our a priori knowledge about de-
sired output structures. The value of pi(y) should
be large for outputs y that are far from the ideal.
In this paper, we consider POS induction and use
intrinsic evaluation; however, in a real-world sce-
nario, the output cost function could use signals
derived from the downstream task in which the
tags are being used.
Given pi, we convert each max to a log
?
exp in
Eq. 1 and introduce the contrastive neighborhood
into the second term, defining our new gain func-
tion ?
CCE
2
(x
(i)
) =
log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)? pi(y)
}
?
log
?
x??N
i
?
y??Y(x?)
exp
{
?
>
f(x
?
,y
?
) + pi(y
?
)
}
Gimpel (2012) found that using such ?softened?
versions of the ramp losses worked better than the
original versions (e.g., Eq. 1) when training ma-
chine translation systems.
5.1 Output Cost Functions
The output cost pi should capture our desider-
ata about y for the task of interest. We con-
sider universal POS tag subsequences analogous
to the universal syntactic rules of Naseem et al.
(2010). In doing so, we use the universal tags of
Petrov et al. (2012): NOUN, VERB, ADJ (ad-
jective), ADV (adverb), PRON (pronoun), DET
(determiner), ADP (pre/postposition), NUM (nu-
meral), CONJ (conjunction), PRT (particle), ?.?
(punctuation), and X (other).
We aimed for a set of rules that would be ro-
bust across languages. So, we used treebanks for
11 languages from the CoNLL 2006/2007 shared
tasks (Buchholz and Marsi, 2006; Nivre et al.,
2007) other than those used in our POS induc-
tion experiments. In particular, we used Arabic,
Bulgarian, Catalan, Czech, English, Spanish, Ger-
man, Hungarian, Italian, Japanese, and Turkish.
We replicated shorter treebanks a sufficient num-
ber of times until they were a similar size as the
largest treebank. Then we counted gold POS tag
unigrams and bigrams from the concatenation.
tag unigram count cost
X 50783 3.83
NUM 174613 2.59
PRT 179131 2.57
ADV 330210 1.96
CONJ 436649 1.68
PRON 461880 1.62
DET 615284 1.33
ADJ 694685 1.21
ADP 906922 0.95
VERB 1018989 0.83
. 1042662 0.81
NOUN 2337234 0
tag bigram count cost
DET PRT 109 84.41
DET CONJ 518 68.82
NUM ADV 1587 57.63
NOUN NOUN 409828 2.09
DET NOUN 454980 1.04
NOUN . 504897 0
Table 1: Counts and costs for universal tags based
on treebanks for 11 languages not used in POS in-
duction experiments.
Where #(y) is the count of tag y in the treebank
concatenation, the cost of y is
u(y) = log
(
max
y
?
#(y
?
)
#(y)
)
and, where #(?y
1
, y
2
?) is the count of tag bigram
?y
1
, y
2
?, the cost of ?y
1
, y
2
? is
u(?y
1
, y
2
?) = 10?log
(
max
?y
?
1
,y
?
2
?
#(?y
?
1
, y
?
2
?)
#(?y
1
, y
2
?)
)
We use a multiplier of 10 in order to exaggerate
count differences among bigrams, which gener-
ally are closer together than unigram counts. In
Table 1, we show counts and costs for all tag uni-
grams and selected tag bigrams.
2
Given these costs for individual tag unigrams
and bigrams, we use the following pi function,
which we call UNIV:
pi(y) = ?
|y|+1
?
j=1
u(y
j
) + u(?y
j?1
, y
j
?)
where ? is a constant to be tuned and y
j
is the
jth tag of y. We define y
0
to be the beginning-
of-sentence marker and y
|y|+1 to be the end-of-
sentence marker (which has unigram cost 0).
Many POS induction systems use one-tag-
per-type constraints (Blunsom and Cohn, 2011;
Gelling et al., 2012), which often lead to higher
2
The complete tag bigram list is provided in the supple-
mentary material.
1333
max
?
N?
i=1
log
?
y?Y(x(i))
exp
{
?>f(x(i),y)
}
? log
?
x??N
i
?
y??Y(x?)
exp
{
?>f(x?,y?)
}
(2)
max
?
N?
i=1
log
?
y?Y(x(i))
exp
{
?>f(x(i),y)? pi(y)
}
? log
?
x??N
i
?
y??Y(x?)
exp
{
?>f(x?,y?) + ?(x(i),x?) + pi(y?)
}
(3)
Figure 1: Contrastive estimation (Eq. 2) and cost-augmented contrastive estimation (Eq. 3). L2 regular-
ization terms (
C
2
?
|?|
j=1
?
2
j
) are not shown here but were used in our experiments.
accuracies even though the gold standard is not
constrained in this way. This constraint can be en-
coded as an output cost function, though it would
require approximate inference (Poon et al., 2009).
6 Cost-Augmented CE
We extended the objective function underlying
CE by defining two new types of cost functions,
one on observations (?4) and one on outputs (?5).
We combine them into a single objective, which
we call cost-augmented contrastive estimation
(CCE), shown as Eq. 3 in Figure 1.
If the cost functions ? and pi factor in the same
way as the features f , then it is straightforward
to implement CCE atop an existing CE implemen-
tation. The additional terms in the cost functions
can be implemented as features with fixed weights
(albeit where the weight differs depending on the
context).
7 Model Selection
Our modifications give increased flexibility, but
require setting new hyperparameters. In addition
to the choice of the cost functions, each has a
weight: ? for ? and ? for pi. We need ways to
set these weights that do not require labeled data.
Smith and Eisner (2005a) chose the hyperpa-
rameter values that yielded the best CE objec-
tive on held-out development data. We use their
strategy, though we experiment with two others as
well.
3
In particular, we estimate held-out data log-
likelihood via the method of Bengio et al. (2013)
and also consider ways of combining outputs from
multiple models.
7.1 Estimating Held-Out Log-Likelihood
Bengio et al. (2013) recently proposed ways to
efficiently estimate held-out data log-likelihood
3
When using their strategy for CCE, we compute the CE
criterion only, omitting the costs. We do so because the
weights of the cost terms can have a large impact on the mag-
nitude of the objective, making it difficult to do a fair com-
parison of models with different cost weights.
for generative models. They showed empirically
that a simple, biased version of their conserva-
tive sampling-based log-likelihood (CSL) estima-
tor can be useful for model selection.
The biased CSL requires a Markov chain on the
variables in the model (i.e., x and y) as well as
the ability to compute p?(x|y). It generates con-
secutive samples of y from a Markov chain ini-
tialized at each x in a development set D, with
S Markov chains run for each x. We compute
and sum p?(x|y
j
) for each sampled y
j
, then sum
over all x in D. The result is a biased estimate for
the log-likelihood of D. Bengio et al. showed that
these biased estimates could give the same model
ranking as unbiased estimates, though more effi-
ciently. They also showed that taking the single,
initial sample from the S Markov chains resulted
in the same model ranking as using many samples
from each chain. We follow suit here.
Our Markov chain is a blocked Gibbs sam-
pler in which we alternate between sampling from
p?(y|x) and p?(x|y). Since we only use a sin-
gle sample from each Markov chain and initialize
each chain to x, this simply amounts to drawing S
samples from p?(y|x). To sample from p?(y|x),
we use the exact algorithm obtained by running
the backward algorithm and then performing left-
to-right sampling of tags using the local features
and requisite backward terms to define the local
tag distributions.
We then compute p?(x|y) for each sampled y.
If there are no features in f that look at more than
one word (which is the case with the features used
in our experiments), then this probability factors:
p?(x|y) =
?
|y|
k=1
p?(xk|yk)
This is easily computable assuming that we have
normalization constants Z(y) cached for each tag
y. To compute each Z(y), we sum over all words
observed in the training data (replacing some with
a special UNK token; see below). We can then
compute likelihoods for individual words and mul-
1334
tiply them across the words in the sentence to com-
pute p?(x|y).
To summarize, we get a log-likelihood estimate
for development setD = {x
(i)
}
|D|
i=1
by sampling S
times from p?(y|x
(i)
) for each x
(i)
, getting sam-
ples {{y
(i),j
}
S
j=1
}
|D|
i=1
, then we compute
?
|D|
i=1
?
S
j=1
log p?(x
(i)
|y
(i),j
)
We used values of S ? {1, 10, 100}, finding that
the ranking of models was consistent across S val-
ues. We used S = 10 in all results reported below.
We note that this estimator was originally pre-
sented for generative models, and that (C)CE is
not a generative training criterion. It seeks to max-
imize the conditional probability of an observation
given its neighborhood. Nonetheless, when imple-
menting our log-likelihood estimator, we treat the
model as a generative model, computing the Z(y)
constants by summing over all words in the vocab-
ulary.
7.2 System Combination
We can avoid choosing a single model by com-
bining the outputs of multiple models via system
combination. We decode test data by using poste-
rior decoding. To combine the outputs of multiple
models, we find the max-posterior tag under each
model, then choose the highest vote-getter, break-
ing ties arbitrarily.
However, when doing POS induction without a
tag dictionary, the tags are simply unique identi-
fiers and may not have consistent meaning across
runs. To address this, we propose a novel voting
scheme that is inspired by the widely-used 1-to-1
accuracy metric for POS induction (Haghighi and
Klein, 2006). This metric maps system tags to
gold tags to maximize accuracy with the constraint
that each gold tag is mapped to at most once. The
optimal mapping can be found by solving a maxi-
mum weighted bipartite matching problem.
We adapt this idea to map tags between two sys-
tems, rather than between system tags and gold
tags. Given k systems that we want to combine,
we choose one to be the backbone and map the re-
maining k ? 1 systems? outputs to the backbone.
4
After mapping each system?s output to the back-
bone system, we perform simple majority voting
among all k systems. To choose the backbone, we
4
We use the LEMON C++ toolkit (Dezs et al., 2011) to
solve the maximum weighted bipartite matching problems.
consider each of the k systems in turn as back-
bone and maximize the sum of the weights of the
weighted bipartite matching solutions found. This
is a heuristic that attempts to choose a backbone
that is similar to all other systems. We found
that highly-weighted matchings often led to high
POS tagging accuracy metrics. We call this vot-
ing scheme ALIGN. To see the benefit of ALIGN,
we also compare to a simple scheme (NA
?
IVE) that
performs majority voting without any tag map-
ping.
8 Experiments
Task and Datasets We consider POS induction
without tag dictionaries using five freely-available
datasets from the PASCAL shared task (Gelling
et al., 2012).
5
These include Danish (DA), using
the Copenhagen Dependency Treebank v2 (Buch-
Kromann et al., 2007); Dutch (NL), using the
Alpino treebank (Bouma et al., 2001); Por-
tuguese (PT), using the Floresta Sint?a(c)tica tree-
bank (Afonso et al., 2002); Slovene (SL), us-
ing the jos500k treebank (Erjavec et al., 2010);
and Swedish (SV), using the Talbanken tree-
bank (Nivre et al., 2006). We use their provided
training, development, and test sets.
Evaluation We fix the number of tags in our
models to 12, which matches the number of uni-
versal tags from Petrov et al. (2012). We use
both many-to-1 (M-1) and 1-to-1 (1-1) accuracy
as our evaluation metrics, using the universal tags
for the gold standard (which was done for the of-
ficial evaluation for the shared task).
6
We note
that our pi function assigns identities to tags (e.g.,
tag 1 is assumed to be NOUN), so we could use
actual tagging accuracy when training with the pi
cost function. But we use M-1 and 1-1 accuracy
to enable easier comparison both among different
settings and to prior work.
Baselines From the shared task, we compare
to all entries that used 12 tags. These include
5
http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure/SharedTask
6
It is common to use a greedy algorithm to com-
pute 1-to-1 accuracy, e.g., as in the shared task scor-
ing script (http://www.dcs.shef.ac.uk/
?
tcohn/
wils/eval.tar.gz), though the optimal mapping can
be computed efficiently via the maximum weighted bipartite
matching algorithm, as stated above. We use the shared task
scorer for all results here for ease of comparison. When we
instead evaluate using the optimal mapping, we find that ac-
curacies are usually only slightly higher than those found by
the greedy algorithm.
1335
BROWN clusters (Brown et al., 1992), clusters ob-
tained using the mkcls tool (Och, 1995), and the
featurized HMM with sparsity constraints trained
using posterior regularization (PR), described by
Grac?a et al. (2011). The PR system achieved the
highest average 1-1 accuracy in the shared task.
We restrict our attention to systems that use 12
tags because the M-1 and 1-1 metrics are highly
dependent upon the number of hypothesized tags.
In general, using more tags leads to higher M-1
and lower 1-1 (Gelling et al., 2012). By keep-
ing the number of tags fixed, we hope to provide a
cleaner comparison among approaches.
We compare to two other baselines: an HMM
trained with 500 iterations of EM and an HMM
trained with 100 iterations of stepwise EM (Liang
and Klein, 2009). We used random initialization
as done by Liang and Klein: we set each param-
eter in each multinomial to exp{1 + c}, where
c ? U [0, 1], then normalized to get probability
distributions. For stepwise EM, we used mini-
batch size 3 and stepsize reduction power 0.7.
For all models we trained, including both base-
lines and CCE, we used only the training data
during training and used the unannotated devel-
opment data for certain model selection criteria.
No labels were used except for final evaluation on
the test data. Therefore, we need a way to handle
unknown words in test data. When running EM
and stepwise EM, while reading in the final 10%
of sentences in the training set, we replace novel
words with the special token UNK. We then re-
place unknown words in test data with UNK.
8.1 CCE Setup
Features We use standard indicator features on
tag-tag transitions and tag-word emissions, the
spelling features from Smith and Eisner (2005a),
and additional emission features based on Brown
clusters. The latter features are simply indicators
for tag-cluster pairs?analogous to tag-word emis-
sions in which the word is replaced by its Brown
cluster identifier. We run Brown clustering (Liang,
2005) on the POS training data for each language,
once with 12 clusters and once with 40, then add
tag-cluster emission features for each clustering
and one more for their conjunction.
7
7
To handle unknown words: for words that only appear
in the final 10% of training sentences, we replace them with
UNK when firing their tag-word emission features. We use
special Brown cluster identifiers reserved for UNK. But we
still use all spelling features derived from the actual word
Learning We solve Eq. 2 and Eq. 3 by running
LBFGS until convergence on the training data, up
to 100 iterations. We tag the test data with mini-
mum Bayes risk decoding and evaluate.
We use two neighborhood functions:
? TRANS1: the original sentence along with all
sentences that result from doing a single trans-
position of adjacent words.
? SHUFF10: the original sentence along with 10
random permutations of it.
We use L2 regularization, adding
C
2
?
|?|
j=1
?
2
j
to
the objectives shown in Figure 1. We use a fixed
(untuned) C = 0.0001 for all experiments re-
ported below.
8
We initialize each CE model by
sampling weights from N(0, 1).
Cost Functions The cost functions ? and pi
have constants ? and ? which balance their con-
tributions relative to the model score and must be
tuned. We consider the ways proposed in Sec-
tion 7, namely tuning based on the contrastive es-
timation criterion computed on development data
(CE), the log-likelihood estimate on development
data with S = 10 (LL), and our two system com-
bination algorithms: na??ve voting (NA
?
IVE) and
aligned voting (ALIGN), both of which use as in-
put the 4 system outputs whose hyperparameters
led to the highest values for the CE criterion on
development data.
We used ? ? {3 ? 10
?4
, 10
?3
, 3 ?
10
?3
, 0.01, 0.03, 0.1, 0.3} and ? ? {3 ?
10
?6
, 10
?5
, 3 ? 10
?5
, 10
?4
, 3 ? 10
?4
}. Setting
? = ? = 0 gives us CE, which we also compare
to. When using both MATLM and UNIV simul-
taneously, we first choose the best two ? values
by the LL criterion and the best two ? values by
the CE criterion when using only those individual
costs. This gives us 4 pairs of values; we run ex-
periments with these pairs and choose the pair to
report using each of the model selection criteria.
For system combination, we use the 4 system out-
puts resulting from these 4 pairs.
For training bigram language models for the
MATLM cost, we use the language?s POS train-
ing data concatenated with its portion of the Eu-
roparl v7 corpus (Koehn, 2005) and the text of its
type. For unknown words at test time, we use the UNK emis-
sion feature, the Brown cluster features with the special UNK
cluster identifiers, and the word?s actual spelling features.
8
In subsequent experiments we tried C ? {0.01, 0.001}
for the baseline CE setting and found minimal differences.
1336
neigh-
cost
mod. DA NL PT SL SV avg
borhood sel. M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1
SHUFF10
none N/A 45.0 38.0 55.1 45.7 54.2 38.0 54.7 45.7 47.4 31.3 51.3 39.7
MATCH
CE 48.9 31.5 56.5 46.4 54.2 37.7 55.9 46.8 48.9 33.8 52.9 39.2
LL 49.9 34.4 56.5 46.4 54.1 38.9 57.2 48.9 48.9 33.8 53.3 40.5
MATLM
CE 49.1 34.3 59.6 50.4 53.6 37.1 55.0 46.2 48.8 33.1 53.2 40.2
LL 50.2 40.0 59.6 50.4 53.1 36.0 58.0 48.4 48.8 33.1 53.9 41.6
TRANS1
none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2
MATCH
CE 58.5 42.5 66.3 53.3 70.6 43.3 59.1 45.6 59.3 54.2 62.7 47.8
LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6
MATLM
CE 59.4 43.5 63.8 50.1 70.2 43.0 58.5 46.1 59.2 54.8 62.2 47.5
LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9
Table 2: Results for observation cost functions. The CE baseline corresponds to rows where cost=?none?.
Other rows are CCE. Best score for each column and each neighborhood is bold.
Wikipedia. The word counts for the Wikipedias
used range from 18M for Slovene to 1.9B for
Dutch. We used modified Kneser-Ney smoothing
as implemented by SRILM (Stolcke, 2002).
8.2 Results
We present two sets of results. First we compare
our MATCH and MATLM observation cost func-
tions for our two neighborhoods and two ways of
doing model selection. Then we do a broader com-
parison, comparing both types of costs and their
combination to our full set of baselines.
Observation Cost Functions In Table 2, we
show results for observation cost functions. We
note that the TRANS1 neighborhood works much
better than the SHUFF10 neighborhood, but we
find that using cost functions can close the gap in
certain cases, particularly for Dutch and Slovene
for which the SHUFF10 MATLM scores approach
or exceed the TRANS1 scores without a cost.
Since the SHUFF10 neighborhood exhibits
more diversity than TRANS1, we expect to see
larger gains from using observation cost functions.
We do in fact see larger gains in M-1, e.g., average
improvements are 1.6-2.6 for SHUFF10 and 0.4-
1.3 for TRANS1, though 1-1 gains are closer.
For TRANS1, while MATCH does reach a
slightly higher average M-1 than MATLM, the lat-
ter does much better in 1-1 (49.9 vs. 47.6 when
using LL for model selection). For SHUFF10,
MATLM consistently does better than MATCH.
Nonetheless, we suspect MATCH works as well as
it does because it at least differentiates the obser-
vation (which is always part of the neighborhood)
from the corruptions.
We find that the LL model selection criterion
consistently works better than the CE criterion for
model selection. When using LL model selection
and fixing the neighborhood, all average scores are
better than their CE baselines. For M-1, the aver-
age improvement is 1.0 to 2.6 points, and for 1-1
the average improvement ranges from 0.4 to 2.7.
We find the best overall performance when us-
ing MATLM with LL model selection with the
TRANS1 neighborhood, and we report this setting
in our subsequent experiments.
Output Cost Function Table 3 shows results
when using our UNIV output cost function, as well
as our full set of baselines. All (C)CE experiments
used the TRANS1 neighborhood.
We find that our contrastive estimation baseline
(cost=?none?) has a higher average M-1 (61.8)
than all results from the shared task, but its average
1-1 accuracy is lower than that reached by poste-
rior regularization, the best system in the shared
task according to 1-1. Using an observation cost
function increases both M-1 and 1-1: MATLM
yields an average 1-1 of 49.9, nearing the 50.1 of
PR while exceeding it in M-1 by nearly 2 points.
When using the UNIV cost function, we see
some variation in performance across model selec-
tion criteria, but we find improvements in both M-
1 and 1-1 accuracy under most settings. When do-
ing model selection via ALIGN voting, we roughly
match the average 1-1 of PR, and when using the
CE criterion, we beat it by 1 point on average (51.3
vs. 50.1).
Combined Costs When using the UNIV cost,
we find that model selection via CE works bet-
ter than LL. So for the combined costs, we took
the two best MATLM weights (? values) accord-
ing to LL and the two best UNIV weights (? val-
ues) according to CE and ran combined cost ex-
periments (MATCHLM+UNIV) with the four pairs
of hyperparameters. Then from among these four,
1337
system
DA NL PT SL SV avg
M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1
HMM, EM 42.5 28.1 53.0 40.6 59.4 33.7 50.3 34.7 49.3 33.9 50.9 34.2
HMM, stepwise EM 51.7 38.2 61.6 45.2 66.5 46.7 53.6 35.7 55.3 39.6 57.7 41.1
BROWN 47.1 39.2 57.3 43.1 67.6 51.6 58.3 42.3 57.6 51.3 57.6 45.5
mkcls 53.1 44.2 63.0 54.1 68.1 46.3 50.4 40.6 57.3 43.6 58.4 45.8
posterior regularization 53.8 45.6 57.6 45.4 74.4 56.1 60.0 48.5 58.8 54.9 60.9 50.1
contrastive estimation
cost model sel.
none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2
MATCH LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6
MATLM LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9
UNIV
CE 59.7 45.6 60.6 51.1 70.0 62.7 60.9 44.1 57.1 52.8 61.7 51.3
LL 59.5 42.2 62.1 56.3 70.7 43.1 60.9 44.1 57.1 52.8 62.1 47.7
NA
?
IVE 59.2 45.6 62.2 52.8 72.7 52.7 60.0 43.8 56.2 53.0 62.2 49.6
ALIGN 61.6 47.3 63.7 54.5 74.4 53.1 59.7 42.1 56.6 53.2 63.2 50.0
MATLM CE 59.8 45.7 60.4 48.4 70.0 62.8 52.9 45.0 59.4 54.9 60.5 51.4
+
LL 59.3 42.5 61.9 56.2 70.8 43.1 59.3 41.9 60.0 55.1 62.3 47.8
NA
?
IVE 58.5 44.4 64.9 60.3 65.4 52.1 55.5 45.9 59.0 54.4 60.6 51.4
UNIV ALIGN 61.1 45.4 66.2 60.9 75.8 49.8 59.5 48.2 59.0 54.4 64.3 51.7
Table 3: Unsupervised POS tagging accuracies for five languages, showing results for three systems from
the PASCAL shared task as well as three other baselines (EM, stepwise EM, and contrastive estimation).
All (C)CE results use the TRANS1 neighborhood. The best score in each column is bold.
we again chose results by CE, LL, and both voting
schemes.
The results are shown in the lower part of Ta-
ble 3. We find different trends in M-1 and 1-
1 depending on whether we use CE or LL for
model selection, which may be due to our lim-
ited hyperparameter search stemming from com-
putational constraints. However, by comparing
NA
?
IVE to ALIGN, we see a consistent benefit
from aligning tags before voting, leading to our
highest average accuracies. In particular, using
MATCHLM+UNIV and ALIGN, we improve over
CE by 2.5 in M-1 and 4.5 in 1-1, also improving
over the best results from the shared task.
9 Conclusion
We have shown how to modify contrastive estima-
tion to use additional sources of knowledge, both
in terms of observation and output cost functions.
We adapted a recently-proposed technique for es-
timating the log-likelihood of held-out data, find-
ing it to be effective as a model selection criterion
when using observation cost functions. We im-
proved tagging accuracy by using weak supervi-
sion in the form of universal tag frequencies. We
proposed a system combination method for POS
induction systems that consistently performs bet-
ter than na??ve voting and circumvents hyperpa-
rameter selection. We reported results on par with
or exceeding the best systems from the PASCAL
2012 shared task.
Contrastive estimation has been shown effective
for numerous NLP tasks, including dependency
grammar induction (Smith and Eisner, 2005b),
bilingual part-of-speech induction (Chen et al.,
2011), morphological segmentation (Poon et al.,
2009), and machine translation (Xiao et al., 2011).
The hope is that our contributions can benefit these
and other applications of weakly-supervised learn-
ing.
Acknowledgments
We thank the anonymous reviewers for their in-
sightful comments and Waleed Ammar, Chris
Dyer, David McAllester, Sasha Rush, Nathan
Schneider, Noah Smith, and John Wieting for
helpful discussions.
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta sint?a(c)tica: a treebank for Portuguese. In
Proc. of LREC.
Y. Bengio, L. Yao, and K. Cho. 2013. Bounding
the test log-likelihood of generative models. arXiv
preprint arXiv:1311.6184.
T. Berg-Kirkpatrick, A. Bouchard-C?ot?e, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induc-
tion of tree substitution grammars for dependency
parsing. In Proc. of EMNLP.
1338
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech
induction. In Proc. of ACL.
G. Bouma, G. Van Noord, and R. Malouf. 2001.
Alpino: Wide-coverage computational analysis of
Dutch. Language and Computers, 37(1).
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram
models of natural language. Computational Lin-
guistics, 18(4).
M. Buch-Kromann, J. Wedekind, and J. Elming.
2007. The Copenhagen Danish-English dependency
treebank v. 2.0. code.google.com/p/copenhagen-
dependency-treebank.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
D. Chen, C. Dyer, S. B. Cohen, and N. A. Smith. 2011.
Unsupervised bilingual POS tagging with Markov
random fields. In Proc. of the First Workshop on
Unsupervised Learning in NLP.
S. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of EMNLP.
M. Creutz and K. Lagus. 2005. Unsupervised mor-
pheme segmentation and morphology induction from
text corpora using Morfessor 1.0. Helsinki Univer-
sity of Technology.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Trans.
Pattern Anal. Mach. Intell., 19(4).
A. Dempster, N. Laird, and D. Rubin. 1977. Maxi-
mum likelihood estimation from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society B, 39:1?38.
B. Dezs, A. J?uttner, and P. Kov?acs. 2011. LEMON - an
open source C++ graph template library. Electron.
Notes Theor. Comput. Sci., 264(5).
C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola.
2008. Tighter bounds for structured estimation. In
Advances in NIPS.
C. Dyer, J. H. Clark, A. Lavie, and N. A. Smith. 2011.
Unsupervised word alignment with arbitrary fea-
tures. In Proc. of ACL.
T. Erjavec, D. Fiser, S. Krek, and N. Ledinek. 2010.
The JOS linguistically tagged corpus of Slovene. In
Proc. of LREC.
K. Ganchev and D. Das. 2013. Cross-lingual discrim-
inative learning of sequence models with posterior
regularization. In Proc. of EMNLP.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. of ACL.
K. Ganchev, J. V. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11.
D. Gelling, T. Cohn, P. Blunsom, and J. V. Grac?a.
2012. The PASCAL challenge on grammar induc-
tion. In Proc. of NAACL-HLT Workshop on the In-
duction of Linguistic Structure.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost func-
tions. In Proc. of NAACL.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Proc.
of NAACL.
K. Gimpel. 2012. Discriminative Feature-Rich Mod-
eling for Syntax-Based Machine Translation. Ph.D.
thesis, Carnegie Mellon University.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. V. Grac?a, K. Ganchev, L. Coheur, F. Pereira, and
B. Taskar. 2011. Controlling complexity in part-
of-speech induction. J. Artif. Int. Res., 41(2).
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proc. of HLT-
NAACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss
function for the overall risk criterion based discrimi-
native training of HMM models. In Proc. of ICSLP.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proc. of MT Summit.
Z. Li, Z. Wang, S. Khudanpur, and J. Eisner. 2010.
Unsupervised discriminative language model train-
ing for machine translation using simulated confu-
sion sets. In Proc. of COLING.
S. Li, J. V. Grac?a, and B. Taskar. 2012. Wiki-ly super-
vised part-of-speech tagging. In Proc. of EMNLP.
1339
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT-NAACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute
of Technology.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2).
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. JAIR, 36.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010. Using universal linguistic knowledge to guide
grammar induction. In Proc. of EMNLP.
J. Nivre, J. Nilsson, and J. Hall. 2006. Talbanken05: A
Swedish treebank with phrase structure and depen-
dency annotation. In Proc. of LREC.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of CoNLL.
F. J. Och. 1995. Maximum-likelihood-sch?atzung
von wortkategorien mit verfahren der kombina-
torischen optimierung. Bachelor?s thesis (Studien-
arbeit), Friedrich-Alexander-Universit?at Erlangen-
N?urnburg, Germany.
S. Petrov, D. Das, and R. McDonald. 2012. A univer-
sal part-of-speech tagset. In Proc. of LREC.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsuper-
vised morphological segmentation with log-linear
models. In Proc. of HLT: NAACL.
D. Povey and P. C. Woodland. 2002. Minimum
phone error and I-smoothing for improved discrima-
tive training. In Proc. of ICASSP.
D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhad-
ran, G. Saon, and K. Visweswariah. 2008. Boosted
MMI for model and feature space discriminative
training. In Proc. of ICASSP.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proc. of
ACL.
S. Riezler. 1999. Probabilistic Constraint Logic Pro-
gramming. Ph.D. thesis, Universit?at T?ubingen.
R. Rosenfeld. 1997. A whole sentence maximum en-
tropy language model. In Proc. of ASRU.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In
Proc. of ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. of ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsuper-
vised grammar induction using contrastive estima-
tion. In Proc. of IJCAI Workshop on Grammatical
Inference Applications.
N. A. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. of COLING-ACL.
D. A. Smith and J. Eisner. 2009. Parser adaptation
and projection with quasi-synchronous features. In
Proc. of EMNLP.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From Baby Steps to Leapfrog: How ?Less is More?
in unsupervised dependency parsing. In Proc. of
NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010b. Viterbi training improves unsu-
pervised dependency parsing. In Proc. of CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010c.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In Proc. of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a.
Lateen EM: Unsupervised training with multiple ob-
jectives, applied to dependency grammar induction.
In Proc. of EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b.
Punctuation: Making a point in unsupervised depen-
dency parsing. In Proc. of CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013.
Breaking out of local optima with count transforms
and model recombination: A study in grammar in-
duction. In Proc. of EMNLP.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Proc. of ICSLP.
O. T?ackstr?om, D. Das, S. Petrov, R. McDonald, and
J. Nivre. 2013. Token and type constraints for cross-
lingual part-of-speech tagging. Transactions of the
Association for Computational Linguistics, 1.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Advances in NIPS 16.
K. Toutanova and M. Johnson. 2007. A Bayesian
LDA-based model for semi-supervised part-of-
speech tagging. In Advances in NIPS.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6.
1340
J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009.
The infinite HMM for unsupervised POS tagging.
In Proc. of EMNLP.
A. Vaswani, A. Pauls, and D. Chiang. 2010. Efficient
optimization of an MDL-inspired objective function
for unsupervised part-of-speech tagging. In Proc. of
ACL.
M. Wang and C. D. Manning. 2014. Cross-lingual
projected expectation regularization for weakly su-
pervised learning. Transactions of the Association
for Computational Linguistics, 2.
X. Xiao, Y. Liu, Q. Liu, and S. Lin. 2011. Fast gen-
eration of translation forest for large-scale SMT dis-
criminative training. In Proc. of EMNLP.
1341
Phrase Dependency Machine
Translation with Quasi-Synchronous
Tree-to-Tree Features
Kevin Gimpel?
Toyota Technological Institute
at Chicago
Noah A. Smith??
Carnegie Mellon University
Recent research has shown clear improvement in translation quality by exploiting linguistic
syntax for either the source or target language. However, when using syntax for both languages
(?tree-to-tree? translation), there is evidence that syntactic divergence can hamper the extraction
of useful rules (Ding and Palmer 2005). Smith and Eisner (2006) introduced quasi-synchronous
grammar, a formalism that treats non-isomorphic structure softly using features rather than
hard constraints. Although a natural fit for translation modeling, its flexibility has proved
challenging for building real-world systems. In this article, we present a tree-to-tree machine
translation system inspired by quasi-synchronous grammar. The core of our approach is a new
model that combines phrases and dependency syntax, integrating the advantages of phrase-based
and syntax-based translation. We report statistically significant improvements over a phrase-
based baseline on five of seven test sets across four language pairs. We also present encouraging
preliminary results on the use of unsupervised dependency parsing for syntax-based machine
translation.
1. Introduction
Building translation systems for many language pairs requires addressing a wide range
of translation divergence phenomena. Several researchers have studied divergence be-
tween languages in corpora and found it to be considerable, even for closely related
languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; S?gaard
and Kuhn 2009). To address this, many have incorporated linguistic syntax into trans-
lation model design. The statistical natural language processing (NLP) community
has developed automatic parsers that can produce syntactic analyses for sentences in
several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al.
? Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu.
?? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nasmith@cs.cmu.edu.
Submission received: 10 November 2012; revised submission received: 12 May 2013; accepted for publication:
23 June 2013.
doi:10.1162/COLI a 00175
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
2007). The availability of these parsers, and gains in their accuracy, triggered research
interest in syntax-based statistical machine translation (Yamada and Knight 2001).
Syntax-based translation models are diverse, using different grammatical for-
malisms and features. Some use a parse tree for the source sentence (?tree-to-string?),
others produce a parse when generating the target sentence (?string-to-tree?), and
others combine both (?tree-to-tree?). We focus on the final category in this article.
Tree-to-tree translation has proved to be a difficult modeling problem, as initial at-
tempts at it underperformed systems that used no syntax at all (Cowan, Kuc?erova?,
and Collins 2006; Ambati and Lavie 2008; Liu, Lu?, and Liu 2009). Subsequent re-
search showed that substantial performance gains can be achieved if hard constraints?
specifically, isomorphism between a source sentence?s parse and the parse of its
translation?are relaxed (Liu, Lu?, and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong
2011; Hanneman and Lavie 2011). This suggests that constraints must be handled
with care.
Yet the classic approach to tree-to-tree translation imposes hard constraints through
the use of synchronous grammars developed for programming language compilation
(Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously:
one in the source language and one in the target language. A single derivation is
used for both strings, which limits the divergence phenomena that can be captured.
As a result, researchers have developed synchronous grammars with larger rules
that, rule-internally, capture more phenomena, typically at increased computational
expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005).
We take a different approach. We take inspiration from a family of formalisms
called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous
grammar, QG assumes the entire input sentence and some syntactic parse of it are
provided and fixed. QG then defines a monolingual grammar whose language is a
set of translations inspired by the input sentence and tree. The productions in this
monolingual grammar generate a piece of the translation?s tree and align it to a piece of
the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible between
the two trees. A weighted QG uses feature functions to softly penalize or encourage
particular types of syntactic divergence.
In this article, we present a statistical tree-to-tree machine translation system in-
spired by quasi-synchronous grammar. We exploit the flexibility of QG to develop a
new syntactic translation model that seeks to combine the benefits of both phrase-based
and syntax-based translation. Our model organizes phrases into a tree structure inspired
by dependency syntax (Tesnie`re 1959). Instead of standard dependency trees in which
words are vertices, our trees have phrases as vertices. The result captures phenomena
like local reordering and idiomatic translations within phrases, as well as long-distance
relationships among the phrases in a sentence. We use the term phrase dependency tree
when referring to this type of dependency tree; phrase dependencies have also been
used by Wu et al. (2009) for opinion mining and previously for machine translation by
Hunter and Resnik (2010). Because we combine phrase dependencies with features from
quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase
dependency (QPD) translation model.
Our tree-to-tree approach requires parsers for both the source and target languages.
For two of the language pairs we consider (Chinese?English and German?English),
treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank;
Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical
parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and
Xing 2009). We also want to apply our model to languages that do not have tree-
350
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The
NLP community has developed a range of statistical algorithms for building unsuper-
vised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010;
Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require
only raw, unannotated text in the language of interest, making them ideal for use in
translation.
Unsupervised shallow syntactic analysis has been used successfully for translation
modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech
tags could be used to label the hierarchical translation rules of Chiang (2005) to match
the performance of a system that uses supervised full syntactic parses. We take addi-
tional steps in this direction, leveraging state-of-the-art unsupervised models for full
syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and
Smith 2012a) to obtain improvements in translation quality. We find that replacing a
supervised parser for Chinese with an unsupervised one has no effect on performance,
and using an unsupervised English parser only hurts slightly. We use unsupervised
parsing to apply our full model to Urdu?English and English?Malagasy translation,
reporting statistically significant improvements over our baselines. These initial results
offer promise for researchers to apply syntactic translation models to the thousands of
languages for which we do not have manually annotated corpora, and naturally suggest
future research directions.
The rest of this article is laid out as follows. In Section 2, we discuss quasi-
synchronous grammar and dependency syntax and motivate our modeling choices.
We present our translation model in Section 3, describe how we extract rules in Sec-
tion 4, and list our feature functions in Section 5. Decoding algorithms are given in
Section 6. We present experiments measuring our system?s performance on translation
tasks involving four language pairs and several test sets in Section 7. We find statistically
significant improvements over a strong phrase-based baseline on five out of seven test
sets across four language pairs. We also perform a human evaluation to study how our
system improves translation quality. This article is a significantly expanded version of
Gimpel and Smith (2011), containing additional features, a new decoding algorithm,
and a more thorough experimental evaluation. It presents key material from Gimpel
(2012), to which readers seeking further details are referred.
2. Background and Motivation
We begin by laying groundwork for the rest of the article. We define notation in
Section 2.1. Section 2.2 discusses how synchronous and quasi-synchronous grammar
handle syntactic divergence. In Section 2.3, we introduce dependency syntax and review
prior work that has used it for machine translation. Section 2.4 presents two examples
of syntactic divergence that motivate the model we develop in Section 3.
2.1 Notation
We use boldface for vectors and we denote individual elements in vectors using sub-
scripts; for example, the source and target sentences are denoted x = ?x1, . . . , xn? and
y = ?y1, . . . , ym?. We denote sequences of elements in vectors using subscripts and su-
perscripts; for example, the sequence from source word i to source word j (inclusive)
is denoted xji, and therefore x
i
i = ?xi?. We denote the set containing the first k positive
integers as [k]. This notation is summarized in Table 1.
351
Computational Linguistics Volume 40, Number 2
Table 1
Notation used in this article.
i, j, k, l integers
x,y vectors
xi entry i in vector x
xji sequence from entry i to entry j (inclusive) in vector x
[i] the set containing the first i positive integers
|x| length of vector x
2.2 Synchronous and Quasi-Synchronous Grammars
To model syntactic transformations, researchers have developed powerful grammat-
ical formalisms, many of which are variations of synchronous grammars. The most
widely used is synchronous context-free grammar (Wu 1997; Gildea 2003; Chiang 2005;
Melamed 2003), an extension of context-free grammar to a bilingual setting where two
strings are generated simultaneously with a single derivation. Synchronous context-free
grammars are computationally attractive but researchers have shown that they cannot
handle certain phenomena in manually aligned parallel data (Wellington, Waxmonsky,
and Melamed 2006; S?gaard and Kuhn 2009). Figure 1 shows two such examples of
word alignment patterns in German?English data. These patterns were called ?cross-
serial discontinuous translation units? (CDTUs) by S?gaard and Kuhn (2009). CDTUs
cannot even be handled by the more sophisticated synchronous formalisms given by
Eisner (2003) and Ding and Palmer (2005). CDTUs can be handled by synchronous
tree adjoining grammar (STAG; Shieber and Schabes 1990), but STAG comes with sub-
stantially heftier computational requirements. Furthermore, S?gaard and Kuhn (2009)
found examples in parallel data that even STAG cannot handle.
Smith and Eisner (2006) noted that these limitations of synchronous grammars
result from an emphasis on generating the two strings. However, for many real-world
applications, such as translation, one of the sentences is provided. The model only needs
to score translations of the given source sentence, not provide a generative account for
sentence pairs. Smith and Eisner proposed an alternative to synchronous grammar?
quasi-synchronous grammar (QG)?that exploits this fact for increased flexibility in
translation modeling. A QG assumes the source sentence and a parse are given and
scores possible translations of the source sentence along with their parses. That is, a
quasi-synchronous grammar is a monolingual grammar that derives strings in the target
language. The strings? derivations are scored using feature functions on an alignment
from nodes in the target tree to nodes in the source tree. The quasi-synchronous depen-
dency grammars of Smith and Eisner (2006) and Gimpel and Smith (2009b) can generate
the translations in Figure 1, as can phrase-based models like Moses (Koehn et al. 2007)
and the phrase dependency model we present in Section 3.
wir  durchleben  keine  wiederholung  des  jahres  1938  . 
          we  are  not  living  a  replay  of  1938  . 
wir  wollen  keinen  .
we  do  not  want  one  . 
Figure 1
Examples of word alignment patterns in German?English that require the increased expressive
power of synchronous tree adjoining grammar.
352
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Quasi-synchronous grammar, like synchronous grammar, can in principle be in-
stantiated for a wide range of formalisms. Dependency syntax (which we discuss in
Section 2.3) has been used in most previous applications of QG, including word align-
ment (Smith and Eisner 2006) and machine translation (Gimpel and Smith 2009b). Aside
from translation, QG has been used for a variety of applications involving relationships
among sentences, including question answering (Wang, Smith, and Mitamura 2007),
paraphrase identification (Das and Smith 2009), parser projection and adaptation (Smith
and Eisner 2009), title generation (Woodsend, Feng, and Lapata 2010), sentence sim-
plification (Woodsend and Lapata 2011), information retrieval (Park, Croft, and Smith
2011), and supervised parsing from multiple treebanks with different annotation
conventions (Li, Liu, and Che 2012).
2.3 Dependency Syntax and Machine Translation
Many syntactic theories have been applied to translation modeling, but we focus in
this article on dependency syntax (Tesnie`re 1959). Dependency syntax is a lightweight
formalism that builds trees consisting of a set of directed arcs from words to their
syntactic heads (also called ?parents?). Examples of dependency trees are shown in
Figure 2. Each word has exactly one parent, and $ is a special ?wall? symbol that is
located at position 0 in the sentence and acts as parent to words that have no other
parent in the sentence. Formally, a dependency tree on an m-word sentence y is a
function ?y : {1, . . . , m} ? {0, . . . , m} where ?y (i) is the index of the parent of word
yi. If ?y (i) = 0, we say word yi is a root of the tree. The function ?y is not permitted
to have cycles. We restrict our attention to projective dependency trees in this article.
Projective dependency trees are informally defined as having no crossing arcs when all
dependencies are drawn on one side of the sentence. See Ku?bler, McDonald, and Nivre
(2009) for formal definitions of these terms.
Researchers have shown that dependency trees are better preserved when pro-
jecting across word alignments than phrase structure trees (Fox 2002). This makes
dependency syntax appealing for translation modeling, but to date there are not many
tree-to-tree translation models that use dependency syntax on both sides. One exception
is the system of Ding and Palmer (2005), who used a synchronous tree substitution
grammar designed for dependency syntax, capturing non-isomorphic structure within
rules using elementary trees. Another is the system of Riezler and Maxwell III (2006),
who used lexical-functional dependency trees on both sides and also include phrase
translation rules. Relatedly, Quirk, Menezes, and Cherry (2005) used a source-side
dependency parser and projected automatic parses across word alignments in order
to model dependency syntax on phrase pairs.
$  konnten  sie  es  ?bersetzen  ?
$  could  you  translate  it  ?
Figure 2
Examples of dependency trees with word alignment. Arrows are drawn from children to
parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a
special ?wall? symbol that serves as the parent of all root words in the tree (i.e., those with
no other parent).
353
Computational Linguistics Volume 40, Number 2
But most who have used dependency syntax have done so either on the source side
in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or
the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and
Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al.
2010). Others have added features derived from source dependency parses to phrase-
based or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao,
Koehn, and Birch 2011).
2.4 Motivating Examples
Although Fox (2002) found that dependencies are more often preserved across hand-
aligned bitext than constituents, there are still several concerns when using dependency
syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for
small data sets and few language pairs, so in practice we must deal with the noise in
automatic word aligners and parsers. Second, not all dependencies are preserved in
hand-aligned data, so we would need to be able to handle non-isomorphic structure
even if we did have perfect tools. The model we present in Section 3 avoids isomor-
phism constraints from synchronous grammar and encourages dependency preserva-
tion across languages by using dependencies on phrases?flat multi-word units?rather
than words.
To motivate these choices, we now give two frequently occurring examples of de-
pendency tree-to-tree divergence in German?English data.1 We consider the German?
English parallel corpus used in our experiments (and described in Appendix A). We
parsed the English side using TurboParser (Martins et al. 2010), a state-of-the-art depen-
dency parser. TurboParser was trained on the Penn Treebank (Marcus, Santorini, and
Marcinkiewicz 1993) converted to dependencies using the Yamada-Matsumoto head
rules (Yamada and Matsumoto 2003). We parsed the German side using the factored
model in the Stanford parser (Rafferty and Manning 2008), which is trained from
the NEGRA phrase-structure treebank (Skut et al. 1997). The Stanford parser?s source
code defines a set of head rules for converting the phrase-structure parse output to
dependencies.2
The first example is shown in Figure 3. The bold words illustrate a ?sibling? rela-
tionship, meaning that the source words aligned to the parent and child in the English
sentence have the same parent on the German side. Many sibling configurations appear
when the English dependency is DET?N within a PP. By convention, the NEGRA
treebank uses flat structures for PPs like ?P DET N? rather than using a separate NP
for DET N. When the parser converts this to a dependency tree, the DET and N are made
children of the P. In English dependency parsing, due to the Penn Treebank conventions,
the DET is made a child of the N, which is a child of the P. There are many other instances
like this one that frequently lie within PPs, like the?us and recent?years. However, if
we tokenized the us as a phrase and also den usa, then both would be children of the
preposition, and the dependency would be preserved.
1 The study that uncovered these examples is detailed in Gimpel (2012). It gives evidence of frequent
non-isomorphic dependency structure between German and English with automatic word aligners
and parsers.
2 These rules use comparable conventions to the Yamada-Matsumoto head rules for English (modulo
the differences in languages and tag/label sets): finite verbs are sentence roots, adpositions are heads
of adpositional phrases, nouns are heads of noun phrases, and so forth.
354
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
$  auch  die mietm?rkte  in den  usa  sind  flexibler
$  rental  markets  are  also  more  flexible  in  the  us
Figure 3
Example of a sentence pair containing a frequently-observed ?sibling? relationship in
German?English data: in the the?us dependency, the aligned German words are siblings
in the source dependency tree. This occurs due to differences in treebank and head rule
conventions between the two data sets. The German parser produces flat PPs with little
internal structure, so when the dependency tree is generated, each word in the PP attaches
to the P, the head of the phrase.
The second example is shown in Figure 4, which gives an example of a
?grandparent-grandchild? relationship. In the English dependency until?recently, the
aligned source words are in a grandparent relationship in the source sentence?s depen-
dency tree. We note, however, that if vor kurzem is tokenized as a phrase, then we might
let the entire phrase be the child of bis, preserving the dependency across languages.
By considering phrasal structure and dependencies among phrases, we can reduce
some of the syntactic divergence in real-world data. The model we develop in the next
section is based on this idea.
3. Model
In the previous section we noted two examples in which flattening dependency tree
structure into ?phrasal dependencies? could improve dependency preservation be-
tween German and English. This idea is compatible with the well-known principle that
translation quality is improved when larger units are modeled within translation rules.
For example, improvements were found by moving from word-based models to so-
called phrase-based translation models. Modern phrase-based translation systems are
typified by the Moses system (Koehn et al. 2007), based on the approach presented by
Koehn, Och, and Marcu (2003). Phrase-based models excel at capturing local reordering
phenomena and memorizing multi-word translations.
$  bis vor kurzem hielten sich beide seiten an diesen stillschweigenden vertrag
$  until recently , both sides adhered to this tacit contract
Figure 4
Example of a sentence pair containing a frequently-observed ?grandparent-grandchild?
relationship in German?English data: the English parent and child words in the until?recently
dependency are aligned to German words in a grandparent-grandchild relationship.
355
Computational Linguistics Volume 40, Number 2
On the other hand, models that use rules employing syntax (Yamada and Knight
2001) or syntax-like representations (Chiang 2005) handle long-distance reordering
better than phrase-based systems (Birch, Blunsom, and Osborne 2009), and therefore
perform better for certain language pairs (Zollmann et al. 2008). In order to better handle
syntactic divergence and obtain the benefits of these two types of models, we use rules
that combine phrases and syntax. In particular, our rules use dependencies between
phrases rather than words; we call them phrase dependencies. When adding in source
syntax, we eschew the constraints of synchronous grammar in favor of the feature-based
approach of quasi-synchronous grammar. So we call our model a quasi-synchronous
phrase dependency (QPD) translation model.
In Section 3.1, we define phrase dependency trees and in Section 3.2 we present
our model. We discuss rule extraction in Section 4 and define the feature functions in
the model in Section 5. Decoding is discussed in Section 6 and an empirical evaluation
is given in Section 7. Key definitions used throughout this section and the remaining
sections are listed in Table 2.
3.1 Phrase Dependencies
In Section 2.3 we defined dependency trees. Now we provide an analogous definition
for phrase dependency trees. We first define a segmentation of a sentence into phrases.
Given a sentence y, where m = |y|, we define a phrase? as a word sequence ykj , for j and
Table 2
Key definitions for our model.
x = ?x1, . . . , xn? source language sentence
y = ?y1, . . . , ym? target language sentence, translation of x
pi = xkj source-sentence phrase: subsequence of words in the source
sentence x, i.e., 1 ? j ? k ? n; the number of words in pi is |pi|
? = ykj target-sentence phrase: subsequence of words in the target
sentence y, i.e., 1 ? j ? k ? m
pi = ?pi1, . . . ,pin?? segmentation of x into phrases such that for i ? [n
?], pii = xkj is
a source-sentence phrase and pi1 ? . . . ? pin? = x
? = ??1, . . . ,?n?? segmentation of y into phrases such that for i ? [n
?], ?i = ykj is
a target-sentence phrase and ?1 ? . . . ??n? = y
b : {1, . . . , n?} ? {1, . . . , n?} one-to-one alignment (bijection) from phrases in ? to phrases
in pi; for all i ? [n?], if b(i) = j, then pij is a subsequence of x and
?i is a subsequence of y
?x : {1, . . . , n} ? {0, . . . , n} dependency tree on source words x, where ?x (i) is the index
of the parent of word xi (0 is the wall symbol $)
?? : {1, . . . , n?} ? {0, . . . , n?} dependency tree on target phrases ?, where ??(i) is the index
of the parent of phrase ?i (0 is the wall symbol $)
h = ?h?,h??? vector of feature functions; h? holds the Moses feature func-
tions and h?? holds the QPD feature functions
? = ???,???? vector of feature weights for h
356
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
k such that 1 ? j ? k ? m. The number of words in phrase ? is denoted |?|. We define a
phrase segmentation of y as ? = ??1, . . . ,?n?? such that for i ? [n?], ?i = ykj is a phrase
and ?1 ? . . . ??n? = y, where ? denotes string concatenation.
Given a phrase segmentation ?, we define a phrase dependency tree as a function
?? : [n?]? {0} ? [n?] where ??(i) is the index of the parent of phrase?i. If ??(i) = 0, we
say phrase ?i is the root of the phrase dependency tree; we require there to be exactly
one root phrase. As with dependency trees, ?? cannot have cycles.3 To distinguish
phrase dependency trees from the ordinary dependency trees defined in Section 2.3,
we will sometimes refer to the latter as ?lexical dependency trees.?
Phrase dependency trees have also been used by Wu et al. (2009) to extract features
for opinion mining and a similar formalism was used previously for machine translation
by Hunter and Resnik (2010). Phrase dependencies allow us to capture phenomena like
local reordering and idiomatic translations within each phrase as well as longer-distance
relationships among the phrases in a sentence.
3.2 Quasi-Synchronous Phrase Dependency Translation
Let X denote the set of all strings in a source language and, for a particular x ? X , let Yx
denote the set of its possible translations (correct and incorrect) in the target language.
Given a sentence x and its lexical dependency tree ?x , we formulate the translation
problem as finding the target sentence y?, the phrase segmentation pi? of x, the phrase
segmentation ?? of y?, the phrase dependency tree ??? on the target phrases ?
?, and the
one-to-one phrase alignment b? such that
?y?,pi?,??, ???,b
?? = argmax
?y,pi,?,??,b?
? ? h(x, ?x ,y,pi,?, ??,b) (1)
where h is a vector of feature functions and ? is a vector of feature weights. The source-
language dependency parse ?x is optional and can be omitted if no source dependency
parser is available. If ?x is provided, we include tree-to-tree configurational features
from QG, which are described in Section 5.3. Hence we call the model defined in
Equation (1) a quasi-synchronous phrase dependency (QPD) translation model.
Our model extends the phrase-based translation model of Koehn, Och, and Marcu
(2003). The phrase segmentation variables ? and the one-to-one phrase alignment b :
[n?]? [n?] are taken directly from phrase-based translation. For all i ? [n?], if b(i) = j,
then pij is a subvector of x and ?i is a subvector of y. If ?x is not given and the features
ignore ??, then the remaining variables (x, y, pi, ?, and b) are defined in the same way
as in phrase-based models.
Computational tractability requires that the feature functions h decompose across
?parts? of the output structures in the model. The feature functions that look only at
the phrase-based variables (x, y, pi, ?, and b) are identical to the features used in the
Moses phrase-based system (Koehn et al. 2007), so they decompose in the same way
as in Moses.4 For clarity, we partition the features and weights into two parts, namely,
? = ???,???? and h = ?h?,h???, where ?? are the weights for the phrase-based features h?
3 Further, we restrict our attention to projective phrase dependency trees in this article. We conjecture that
non-projective trees may be a better fit for translation modeling (Carreras and Collins 2009; Galley and
Manning 2009), particularly for certain language pairs, but we leave their exploration for future work.
4 A more detailed discussion of how the Moses features decompose can be found in Gimpel (2012).
357
Computational Linguistics Volume 40, Number 2
and ??? are the weights for the QPD features h??. So we rewrite the right-hand side of
Equation (1) as the following:
argmax
?y,pi,?,??,b?
?? ? h?(x,y,pi,?,b) + ??? ? h??(x, ?x ,y,pi,?, ??,b) (2)
Furthermore, we assume an additive decomposition across individual phrase depen-
dencies in the phrase dependency tree ??, allowing us to rewrite Equation (2) as
argmax
?y,pi,?,??,b?
?? ? h?(x,y,pi,?,b)
+
n??
i=1
??? ? f (x, ?x , i, ??(i),?i,???(i),b(i),b(??(i)),pib(i),pib(??(i))) (3)
where we introduce new notation f to represent the feature vector that operates on a
single phrase dependency at a time in the ?arc-factored? decomposition of h??. Each
feature in f can look at the entirety of x and ?x because they are inputs, but can only
look at a single target-side phrase dependency ??i,???(i)? at a time (along with their
aligned source phrases pib(i) and pib(??(i)) and the indices).
Example. Figure 5 shows an example. The inputs to the model are a segmented Chi-
nese sentence and its lexical dependency tree. We used the Stanford Chinese word
segmenter (Chang, Galley, and Manning 2008) to segment the Chinese data and the
Stanford parser (Levy and Manning 2003) to get Chinese dependency trees. The outputs
references: annan to hold talks with us , russia and eu over situation in middle east
annan will discuss middle east situation with u.s. , russia and european union
annan to discuss mideast situation with us , russia and eu
annan to meeting the us , russia and eu to discuss middle east crisis
annan  will  hold talks  with  the united states  , russia and  the european union  to discuss  the middle east  situation$
$
Figure 5
Example output of our model for Chinese?English translation. The word-segmented Chinese
sentence and dependency tree are inputs. Our model?s outputs include the English translation,
phrase segmentations for each sentence (a box surrounds each phrase), a one-to-one alignment
between the English and Chinese phrases, and a projective dependency tree on the English
phrases. Note that the Chinese dependency tree is on words whereas the English dependency
tree is on phrases.
358
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
of the model include a segmentation of the Chinese sentence into phrases, the English
translation, its segmentation into phrases, a projective dependency tree on the English
phrases, and a one-to-one alignment between the English phrases and Chinese phrases.
Four reference translations are also shown. In this example, the model correctly moved
the phrase hold talks and also noted its connection to to discuss by making the latter a
phrasal dependent.
4. Rule Extraction
In typical statistical machine translation (SMT) models, the space of allowable trans-
lations is constrained by a set of rules. Informally, a rule consumes part of the input
text and emits text in the output language. Building an SMT system typically requires
collecting a massive set of rules from parallel text, a process called rule extraction.
For phrase-based translation, these rules are phrase pairs and the translation space
is constrained by the phrase pairs in the phrase table.5 In our model, even though
we have additional structure (i.e., the phrase dependency tree ??), we do not want
to enforce any additional constraints on the search space. That is, the space of valid
translations is still constrained solely by a standard phrase table. We allow ?? to be
any projective phrase dependency tree on ?, so the structure of ?? merely affects how
translations are scored, not what translations are permitted. We made this decision
because we did not want to reduce the coverage of phrase-based models, which is one
of their strengths. Rather, we wanted to better score their translations.6
So, even though our phrase dependency rules do not consume parts of the input,
we still speak in terms of ?rule extraction? because our procedure is similar to rule
extraction in other systems and we define feature functions on our rules in a standard
way. In particular, we use the extracted rule instances to compute relative frequency
estimates for many of the features presented in Section 5.
The rest of this section is organized as follows. In Section 4.1 we describe how we
extract rules that only look at target-side words and syntactic structure. In Section 4.2
we extract rules that also look at the source sentence, but not its syntax. (Although our
system uses unlexicalized features based on source-side syntax, they do not derive from
rules; we turn to features in Section 5). This lets us avoid the computational expense of
parsing the source side of the parallel training corpus.
4.1 Target-Tree Rules
We first extract rules that only consider the target side: y, ?, and ??. These rules can be
used as the basis for ?dependency language model? features (Shen, Xu, and Weischedel
2008; Galley and Manning 2009; Zhang 2009), though unlike previous work, our features
model both the phrase segmentation and dependency structure. Typically, these sorts
of features are relative frequencies from a corpus parsed using a supervised parser.
However, there do not currently exist treebanks with annotated phrase dependency
5 It is common to add ?identity? phrase pairs for unknown words to allow them to pass through
untranslated.
6 This strategy can also lead to limitations. Because we do not expand the search space beyond what is
licensed by the phrase table, we are limited by the ability of the underlying phrase-based model to
provide us with a good search space.
359
Computational Linguistics Volume 40, Number 2
trees. Our solution is to use a standard lexical dependency parser and extract phrase
dependencies using bilingual information.7 Essentially, we combine phrases from the
standard phrase extraction pipeline with selected lexical dependencies from the output
of a dependency parser.
We first give an overview of our approach and then describe it more formally. We
begin by obtaining word alignments and extracting phrase pairs using the standard
heuristic approach of Koehn, Och, and Marcu (2003). We then parse the target sentence
with a projective dependency parser to obtain a projective dependency tree ?y for a
sentence y. Note that ?y is a tree on words, not phrases (cf. ??). For each pair of target-
side phrases in the phrase pairs from phrase extraction, we extract a phrase dependency
(along with its direction) if the phrases do not overlap and there is at least one lexical
dependency between them. If there is only a dependency in one direction, we extract
a single phrase dependency with that direction. If there are lexical dependencies in
both directions, we extract a phrase dependency only for the single longest lexical
dependency, and in its direction. Because we use a projective dependency parser, the
longest lexical dependency between two phrases is guaranteed to be unique. If a phrase
contains a root word in ?y , we extract a phrase dependency with the wall symbol as
its head.
We now present the procedure more formally. Given word-aligned sentence pairs,
we extract phrase pairs that are p-consistent with (i.e., do not violate) the word align-
ments. Let R denote a relation between the two sets [n] and [m], where n = |x| and
m = |y|. If a pair (i, j) belongs to R for some i ? [n] and j ? [m], then we say that xi is
aligned to yj. We define new notation R here instead of using b because R allows many-
to-many word alignments, which are typically used for phrase extraction.8 A phrase
pair ?xji,y
l
k? is p-consistent with R if, for all u such that i ? u ? j, and all v such that (u, v)
belongs to R, it is the case that k ? v ? l. So far this is identical to the phrase extraction
pipeline used in Moses.
Given word alignments R and a dependency tree ?y on y, we extract (target-side)
phrase dependencies. We say a phrase dependency ?yji,y
l
k?with y
l
k as the parent phrase
is d-consistent with ?y and R if:
1. ?xj
?
i? ,x
l?
k? such that ?x
j?
i? ,y
j
i? and ?x
l?
k? ,y
l
k? are p-consistent with R
2. yji and y
l
k do not overlap: (1 ? i ? j < k ? l ? m) ? (1 ? k ? l < i ? j ? m)
3. the longest lexical dependency from yji to y
l
k is longer than the longest from
ylk to y
j
i: maxu:i?u?j,k??y (u)?l
|?y (u)? u| > max
v:k?v?l,i??y (v)?j
|?y (v)? v|
The final condition also implies that there is a lexical dependency from a word in yji to a
word in ylk: ?u, i ? u ? j, such that k ? ?y (u) ? l.
7 Other ways of getting phrase dependencies are possible. For example, for a monolingual task, Wu et al.
(2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase
dependencies.
8 Many-to-many word alignments can be obtained from certain alignment models or, more frequently, by
using heuristics to combine alignments from one-to-many and many-to-one alignments (Koehn, Och, and
Marcu 2003).
360
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
We also need to extract root phrase dependencies. We say a root phrase dependency
?yji, $? is d-consistent with ?y and R if:
1. ?xj
?
i? such that ?x
j?
i? ,y
j
i? is p-consistent with R
2. ?u, i ? u ? j, such that ?y (u) = 0
We extract all phrase dependencies that are d-consistent with the word alignments
and target-side lexical dependency trees. We note that while extracting phrase depen-
dencies we never explicitly commit to any single phrase dependency tree for a target
sentence. Rather, we extract phrase dependencies from all phrase dependency trees
compatible with the word alignments and the lexical dependency tree. Thus we treat
phrase dependency trees analogously to phrase segmentations in phrase extraction.
When actually extracting phrase dependencies, we record additional information
from the sentence pairs in which we found them. Specifically, for d-consistent phrase
dependencies ?yji,y
l
k? (where y
l
k is the parent), we extract tuples of the following form:
?yji,y
l
k, yu? , y?y (u? ), I
[
j < k
]
? (4)
where I [P] is the indicator function that returns 1 if P evaluates to true and 0 otherwise.
The index u? is chosen to make ?yu? , y?y (u? )? the longest lexical dependency within the
phrase dependency:
u? = argmax
u:i?u?j,k??y (u)?l
|?y (u)? u| (5)
This lexical dependency is recorded for use in back-off features, analogous to the lexical
weighting in phrase-based models. The fifth field in Equation (4) holds the direction of
the phrase dependency, which is also the direction of the longest lexical dependency.
Root phrase dependencies use k = l = 0 in the parent phrase and designate $ as y0. The
direction of root phrase dependencies is inconsequential and can remain as I
[
j < k
]
.
4.1.1 Examples. What do typical phrase dependencies look like? Tables 3 and 4
show some of the most frequent examples of root phrases and parent-child phrase
dependencies extracted by this technique on our German?English (DE?EN) corpus.
The English side of the parallel corpus was parsed using TurboParser (Martins et al.
2010). Naturally, there are many phrase dependencies with a single word in each
phrase, but because these are very similar to lists of frequent lexical dependencies in a
parsed corpus, we have only shown dependencies with phrases containing more than
one word.
Root phrases (Table 3) frequently contain a subject along with a verb (it is, i would
like, etc.), though the lexical root is typically a verb or auxiliary. These are examples of
how we can get syntactic information for phrases that typically would not correspond
to constituents in phrase structure trees.
Table 4 shows frequent phrase dependencies from the same corpus; because this
corpus is mostly European Parliamentary proceedings, certain formulaic and domain-
specific phrases appear with large counts. When phrases attach to each other, they
typically behave like their heads. For example, in the phrase dependency of the?union,
the word union is the child phrase because of the is behaving like of . There is likely also a
361
Computational Linguistics Volume 40, Number 2
Table 3
Top 60 most frequent root phrases in DE?EN data with at least two words, shown with their
counts. Shown in bold are the actual root words in the lexical dependency trees from which these
phrases were extracted; these are extracted along with the phrases and used for back-off features.
35,265 it is 6,210 i think 4,843 would be 2,918 thank you
13,751 this is 6,115 is that 4,289 we will 2,816 it will
12,763 is a 6,105 is not 4,262 i believe that 2,788 is to
11,831 we have 6,019 , it is 4,018 is also 2,737 it is a
11,551 would like 5,975 believe that 3,910 that is why 2,736 it has
11,245 we must 5,818 will be 3,838 i would like to 2,730 they are
11,243 is the 5,706 we need 3,775 would like to 2,611 we can
11,015 i would like 5,628 there are 3,505 hope that 2,580 i think that
10,008 there is 5,495 should like 3,427 is an 2,551 i will
8,983 i am 5,453 i should like 3,239 , i would like 2,483 does not
8,019 we are 5,227 i hope 3,130 i hope that 2,482 debate is
7,722 that is 5,150 , is 3,101 need to 2,445 i can
6,883 i would 5,110 we should 3,059 it was 2,438 want to
6,443 i have 5,010 has been 3,021 have been 2,416 must be
6,328 i believe 4,917 do not 2,937 think that 2,405 this is a
dependency from the to union whenever the longer phrase dependency is extracted, but
due to our practice of following the longest lexical dependency in deciding the direction,
of?union is favored over the?union.
We note that even though these phrase dependencies only contain words from the
target language (English), the presence and counts of the phrase dependencies will
Table 4
Most frequent phrase dependencies in DE?EN data, shown with their counts and attachment
directions. Child phrases point to their parents. To focus on interesting phrase dependencies,
we only show those in which one phrase has at least two tokens and neither phrase is entirely
punctuation. The words forming the longest lexical dependency in each extracted phrase
dependency are shown in bold; these are used for back-off features.
30,064 mr? president , 4,582 i believe? that
19,931 the? european union 4,516 , which? is
18,819 the european? union 4,347 that? will be
12,318 i? would like 4,297 the fact? that
11,990 the?member states 4,289 it is? important
8,169 it is? that 4,232 one? of the
7,779 the? european parliament 4,215 of the? commission
7,762 madam? president , 3,932 it is? not
7,448 the european? parliament 3,793 i? would like to
6,897 of the? union 3,761 in the? union
6,196 mr? president , i 3,752 in?member states
6,188 i? should like 3,673 president? ladies and gentlemen ,
6,087 that the? is 3,673 is? that the
5,478 i? believe that 3,667 president ,? ladies and gentlemen ,
5,283 of the? european union 3,602 i hope? that
5,268 that? and that 3,531 we? need to
4,956 of the european? union 3,495 the? fact that
4,902 , and? is 3,494 that the? commission
4,798 the? united states 3,462 i? do not
4,607 ) mr? president , 3,446 , the? commission
4,592 , it? is 3,421 that the? will
362
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
depend on the source language through the word alignments. For example, when of
the union is expressed in German, the preposition will often be dropped and the definite
article chosen to express genitive case. In our corpus, the most common translation of
the English union is the German noun union, which is feminine. The genitive feminine
definite article is der and, indeed, we find in the phrase table that the translation of of
the union with highest probability is der union.9 Thus the dominance of the phrase depen-
dency of the?union (6,897 occurrences) as compared with of?the union (142 occurrences)
is caused by the German translation.
4.1.2 Word Clusters. When trying to compute feature functions for dependencies between
long phrases, we expect to face problems of data sparseness. Long phrases do not occur
very often, so pairs of long phrases will occur less often still. One way to address this is
to also extract rules that use part-of-speech (POS) tags in place of words. However, since
words can have multiple POS tags, we would then need to infer POS tags for the words
in order to determine which rule is applicable. So we instead use hard word clusters,
which provide a deterministic mapping from words to cluster identifiers. Furthermore,
certain types of hard word clusters, such as Brown clusters (Brown et al. 1992), have
been shown to correspond well to POS tag categories (Christodoulopoulos, Goldwater,
and Steedman 2010). We chose Brown clusters for this reason.
Brown clustering uses a bigram hidden Markov model (HMM) in which states
are hard cluster labels and observations are words. The emission distributions are
constrained such that each word has a nonzero emission probability from at most
one cluster label. Clusters can be obtained efficiently through a greedy algorithm that
approximately maximizes the HMM?s log-likelihood by alternately proposing new
clusters and merging existing ones. This procedure actually produces a hierarchical
clustering, but we discard the hierarchy information and simply use unique IDs for
each cluster. The number of clusters is specified as an input to the algorithm; we used
100 clusters for all experiments in this article. Additional details on cluster generation
for our data sets are provided in Appendix B.
Given Brown clusters, we extract tuples like those above in which we replace each
word by its Brown cluster ID:
?clust(yji), clust(y
l
k), clust(yu? ), clust(y?y (u? )), I
[
j < k
]
? (6)
where clust() is a function that takes a sequence of words and replaces each by
its Brown cluster ID. The index u? is defined as in Equation (5). Examples of fre-
quent Brown cluster phrase dependencies, including root dependencies, are shown in
Table 5.
4.2 String-to-Tree Rules
Our simplest probability features use the information in these tuples, but we also extract
tuples with more information to support richer features. In particular, we record aligned
9 The phrase table probability of the German der union given the English of the union is 0.64. The next
most-probable German phrase is der europa?ischen union, with probability 0.03.
363
Computational Linguistics Volume 40, Number 2
Table 5
Most frequent Brown cluster phrase dependencies extracted from DE?EN data, shown with
their counts. As in Table 4, we only show those in which one phrase has at least two tokens and
neither phrase is entirely punctuation. Each cluster is shown as a set of words large enough to
cover 95% of the token counts in the cluster, up to a maximum of four words. It is characteristic
of Brown clustering that very frequent tokens (e.g., function words) often receive their own
clusters.
47,137 {mr, mrs, madam, mr.}? {president, president-in-office, van, barroso} ,
35,656 $? it is
29,775 the? {time, way, right, question} of
28,199 the? {european, soviet} {union, parliament, globalisation}
27,373 $? i {say, believe, think, know}
26,480 the? {state, development, group, security} of
26,388 the {european, soviet}? {union, parliament, globalisation}
24,726 {one, part, number, behalf}? of the
22,536 of the? {people, countries, members, citizens}
21,449 {state, development, group, security} {and, or}? {state, development, group, security}
21,007 $? {we, they} {should, must, cannot, shall}
20,933 {state, development, group, security}? {and, or} {state, development, group, security}
20,919 the? {one, part, number, behalf} of
20,897 of the? {report, committee, issue, agreement}
20,081 the? {economic, political, international, national} {policy, years, rights, market}
19,209 the? {report, committee, issue, agreement} of
18,535 {people, countries, members, citizens}? of the
18,523 $? {say, believe, think, know} that
18,510 {time, way, right, question}? of the
18,232 the? {member, united} {states, nations}
18,157 {one, part, number, behalf} of? {people, countries, members, citizens}
17,950 {people, countries, members, citizens} {and, or}? {people, countries, members, citizens}
17,643 {state, development, group, security}? of the
17,539 the? {people, countries, members, citizens} of
17,457 the? {economic, political, international, national} {state, development, group, security}
16,608 to {take, make, see, help}? {people, countries, members, citizens}
16,163 the {time, way, right, question}? of
15,517 the? {economic, political, international, national} {people, countries, members, citizens}
15,292 in the? {report, committee, issue, agreement}
15,257 a? {new, good, u.s., common} {report, committee, issue, agreement}
15,223 the {state, development, group, security}? of
15,217 {people, countries, members, citizens}? {and, or} {people, countries, members, citizens}
15,214 it is? {important, clear, necessary, concerned}
14,977 i? {say, believe, think, know} that
14,697 $? is {important, clear, necessary, concerned}
14,582 i {say, believe, think, know}? that
14,399 {should, must, cannot, shall}? be {made, taken, put, set}
14,146 $? this is
14,089 a {new, good, u.s., common}? {report, committee, issue, agreement}
14,047 {europe, china, today, women}? {and, or} {europe, china, today, women}
13,599 {made, taken, put, set}? {by, from, into, between} the
13,190 the? {new, good, u.s., common} {report, committee, issue, agreement}
13,089 the? {new, good, u.s., common} {people, countries, members, citizens}
13,035 $? {we, they} have
13,034 {economic, political, international, national}? {and, or} {economic, political, international,. . .}
13,013 $? is a
12,713 $? {need, want, needs, wish} to
12,399 $? i {say, believe, think, know} that
12,387 the? {time, way, right, question} of the
12,319 i?would {like, according, relating}
12,217 in the? {eu, world, government, country}
12,125 the? {economic, political, international, national} {report, committee, issue, agreement}
11,979 of? {economic, political, international, national} {state, development, group, security}
11,955 the {report, committee, issue, agreement}? of
11,838 $? {we, they} {are, were}
11,551 $?would {like, according, relating}
11,537 the? {people, countries, members, citizens} of the
364
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
source phrases and details about reordering and the presence of gaps between phrases.
That is, for d-consistent phrase dependencies ?yji,y
l
k?, we extract tuples
?yji,y
l
k,x
j?
i? ,x
l?
k? ,
I
[
j < k
]
,
I
[
I
[
j < k)
]
= I
[
j? < k?
]]
,
I
[
(j + 1 = k) ? (l + 1 = i)
]
,
I
[
(j? + 1 = k?) ? (l? + 1 = i?)
]
? (7)
for all i?, j?, k?, and l? such that the phrase pairs ?xj
?
i? ,y
j
i? and ?x
l?
k? ,y
l
k? are p-consistent
with R, and such that xj
?
i? does not overlap with x
l?
k? .
10 Again, I [P] is the indicator
function that returns 1 if P evaluates to true and 0 otherwise. That is, we include the
two target phrases, their aligned source phrases, the direction of the target attachment,
the orientation between the source and target phrases (whether the two target phrases
are in the same order as their aligned source phrases or swapped), whether a gap is
present between the two target phrases, and finally whether a gap is present between
the two source phrases. When ylk = $, all of the additional fields are irrelevant except
the aligned source phrase xj
?
i? .
We now note some examples of the phenomena that we can model with these richer
tuples. A common cause of reordering in German-to-English translation relates to verbs.
Figure 6 shows two examples of frequently extracted phrase dependencies that model
verb movement. Figure 6(a) gives an example of how German reorders the finite verb
to the end of a dependent clause, whereas English keeps it next to the subject. The
extracted rule, shown below the sentence pair, only applies when intervening words
appear on the German side and no intervening words appear on the English side. This
is indicated by the presence (absence) of an ellipsis on the German (English) side of the
rule.
Figure 6(b) shows an example of how German moves an infinitive (danken, ?to
thank?) to the end of an independent clause when a modal verb (mo?chte, ?would like?)
is present. The ellipses on both sides indicate that other words must be present between
both the source and target phrase pairs. We note that this rule says nothing about what
fills the gap. In particular, the gap-filling material does not have to be translationally
equivalent, and indeed in the given sentence pair it is not. As opposed to rules in
hierarchical phrase-based models (Chiang 2005), which typically specify translationally
equivalent substructures, this rule simply models the reordering and long-distance
movement of the infinitive. Much prior work has found phrase pairs with gaps to
be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley
and Manning 2010), and we extract tuples as in Equation (7) so that we can model
such structures, even though we do not directly model gap-filling like hierarchical
models and other models based on synchronous context-free grammar (Zollmann and
Venugopal 2006, inter alia).
10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from
the previous section, which are extracted even when the source phrases overlap.
365
Computational Linguistics Volume 40, Number 2
ich meine deshalb  , dass es  eine frage der geeigneten methodik  ist  .
i think  that it   is  consequently a question of the appropriate methodologies .
dass es   ...
(a)
(b)
ist
that it  is  
abschlie?end m?chte ich herrn langen herzlich  f?r seinen  bericht  danken  ,...
finally , mr president , i would like  to thank  mr langen warmly  for his  report ,...
f?r seinen    ... danken
to thank     ...  for his  
Figure 6
Examples of illustrative sentence pairs and frequently extracted rules that model verb
movement between German and English. An ellipsis indicates that there must be material
between the two phrases for the rule to apply. (a) Example of movement of the finite verb
to the end of a dependent clause. (b) Example of movement of an infinitive to the end of an
independent clause following a modal verb (mo?chte, ?would like?). Discussion of the features
used to score these string-to-tree rules is given in Section 5.2.
The tuples described here are used to compute all of the lexicalized phrase depen-
dency features in our model. We extract each tuple with a count of 1 each time it is
observed, aggregate the counts across all sentence pairs in the parallel corpus, and use
the counts to compute the statistical features we present in the next section. We also
have structural features that consider string-to-tree and tree-to-tree configurations, but
these do not require any rule extraction. In the next section we describe the full set of
features in our model.
5. Features
Our model extends the phrase-based translation model of Moses (Koehn et al. 2007), so
we include all of its features in our model. These include four phrase table probability
features, a phrase penalty feature, an n-gram language model, a distortion cost, six
lexicalized reordering features, and a word penalty feature. These features are contained
in h? in Equation (3), reproduced here:
argmax
?y,pi,?,??,b?
?? ? h?(x,y,pi,?,b)
+
n??
i=1
??? ? f (x, ?x , i, ??(i),?i,???(i),b(i),b(??(i)),pib(i),pib(??(i))) (8)
366
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
We now describe in detail the additional features f that are used to score phrase de-
pendency trees. Each operates on a single phrase dependency and takes the arguments
?x, ?x , c, d,?c,?d, c?, d?,pic? ,pid??, which are, in order, the source sentence (x), the source
dependency tree (?x), the target child phrase index (c), the target parent phrase index
(d), the target child phrase (?c), the target parent phrase (?d), the index of the source
phrase aligned to the target child (c?), the index of the source phrase aligned to the
target parent (d?), the child-aligned source phrase (pic? ), and the parent-aligned source
phrase (pid? ).
Like the phrase probability features in Moses, many of our feature functions are
conditional probabilities computed using relative frequency estimation given the full
collection of extracted tuples. That is, for a tuple ??,??, the conditional probability of
field ? given field ? is estimated as
p?(? | ?) =
#{??,??}
?
?? #{??,?
??}
(9)
where #{??,??} denotes the count of the tuple ??,?? in the multiset of extracted tuples.
We use the notation p? in the following to indicate that relative frequency estimates are
being used.11
5.1 Target-Tree Features
We first include features that only consider the target-side words and phrase depen-
dency tree; these are computed based on the rules extracted in Section 4.1. The first
feature is the sum of the scaled log-probabilities of each phrase dependency attachment
in ??:
fpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? ) = max
(
0, C + log p?(?c | ?d, dir(c, d))
)
(10)
where dir(c, d) is defined
dir(c, d) =
?
??
??
root if d = 0
left if d > c
right otherwise
(11)
and returns the direction of the attachment for head index d and child index c, that is,
the direction in which the child resides; root indicates that phrase c is the root.
Although we use log-probabilities in this feature function, we add a constant C,
chosen to ensure the feature value is never negative. The reasoning here is that when-
ever we use a phrase dependency that we have observed in the training data, we
want to boost the score of the translation. If we used log-probabilities, each observed
dependency would incur a penalty. The max expression prevents unseen parent-child
phrase dependencies from causing the score to be negative infinity. Our motivation is
a desire for the features to prefer one derivation over another but not to rule out a
derivation completely if it merely happens to contain an unseen phrase dependency.
11 Note that, as is standard across many SMT models, all frequencies here are counts of extraction events.
They are not counts of derivation or translation events, since many competing rules may be extracted from
each training instance.
367
Computational Linguistics Volume 40, Number 2
Because we will use this same practice for all other probability features, we intro-
duce some shorthand for simplicity of presentation. We first redefine this feature:
fpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? ) =
max
(
0, Cpdep + log gpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? )
)
(12)
where
gpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? ) = p?(?c | ?d, dir(c, d)) (13)
In what follows, we will restrict our attention to defining the g-style functions for
probability features, and assume that there is always a corresponding f that has the same
subscript and takes the same inputs, as in Equation (12). Furthermore, when presenting
the remaining features, we will suppress the arguments of each for clarity; all take the
same arguments as fpdep and gpdep.
We will assume C is chosen appropriately for each g based on the minimum log-
probability for the feature. For example,
Cpdep = 0.01? min
?,??,r
log p?(? | ??, r) (14)
that is, the minimum log-probability is found, negated, and a small positive value
(0.01) is added to ensure the feature is greater than zero. This ensures that, if a phrase
dependency has been seen, its contribution is at least 0.01.
To counteract data sparseness, we include other features that are less specific than
gpdep. First, we include a version of this feature with words replaced by Brown clusters:
gpdep
clust
= p?(clust(?c) | clust(?d), dir(c, d)) (15)
We also include lexical weighting features similar to those used in phrase-based ma-
chine translation (Koehn, Och, and Marcu 2003). These use the longest lexical depen-
dencies extracted during rule extraction. First, for all ?child, parent, direction? lexical
dependency tuples ?y, y?, r? in the parsed target side of the parallel corpus, we estimate
conditional probabilities p?lex(y | y?, r) using relative frequency estimation.
Then, assuming the given phrase dependency ??c,?d? has longest child-parent
lexical dependency ?y, y?? for direction dir(c, d), we include the feature:
gldep = p?lex(y | y
?, dir(c, d)) (16)
We include an analogous feature with words replaced by Brown clusters. Different
instances of a phrase dependency may have different lexical dependencies extracted
with them. We only use the lexical weight for the most frequent, breaking ties by
choosing the lexical dependency that maximizes p?lex(y | y?, r), as was done similarly by
Koehn, Och, and Marcu (2003).
So far we described four features that consider y, ?, and ??: one for phrase de-
pendencies, one for lexical dependencies, and the same two features computed on
a transformed version of the corpus in which each word is replaced by its Brown
cluster ID.
368
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
5.2 String-to-Tree Features
We next discuss features that consider properties of the source sentence x, its phrase
segmentation pi, and the phrase alignment b, in addition to y,?, and ??. However, these
features still do not depend on the source tree ?x , so they can be included even when a
parser for the source language is not available. We will discuss features that use ?x in
Section 5.3.
These features are similar to the previously defined gpdep, but condition on addi-
tional pieces of structure. All features condition on direction. The first pair of features
condition on the source phrase (pic? ) aligned to the child phrase (?c) in the target phrase
dependency (??c,?d?):
gpdep
child
= p?(?c | ?d, dir(c, d),pic? ) (17)
gpdep
child
clust
= p?(?c | clust(?d), dir(c, d),pic? ) (18)
In the second feature, we condition on word clusters for the parent phrase ?d, but on
words in the aligned source phrase pic? . Because Brown clusters often correspond to
syntactic clusters, even at times resembling part-of-speech tags (Christodoulopoulos,
Goldwater, and Steedman 2010), it did not seem logical to model translation probabili-
ties between source- and target-language word clusters. This is why we did not include
a feature like the above with word clusters for ?c and pic? . Our use of these clusters is a
simple kind of backoff or smoothing that allows some sharing across specific phrases,
since statistics on phrase pairs are expected to be sparse.
The next set of features includes those that condition on the orientation between
the source- and target-side phrases. The ori function returns the orientation of the
aligned source phrases in a target phrase dependency attachment, namely, whether the
aligned source phrases are in the same order as the target phrases (?same?) or if they
are in the opposite order (?swap?):
ori(c, d, c?, d?) =
?
??
??
root if d = 0
same if dir(c, d) = dir(c?, d?)
swap otherwise
(19)
Given this definition of ori, we define the following features that condition on orienta-
tion (in addition to other fields):
gpdep
orient
= p?(?c | ?d, dir(c, d), ori(c, d, c
?, d?)) (20)
gpdep
orient
clust
= p?(clust(?c) | clust(?d), dir(c, d), ori(c, d, c
?, d?)) (21)
gpdep
child
orient
= p?(?c | ?d, dir(c, d),pic? , ori(c, d, c
?, d?)) (22)
gpdep
child
orient
clust
= p?(?c | clust(?d), dir(c, d),pic? , ori(c, d, c
?, d?)) (23)
369
Computational Linguistics Volume 40, Number 2
where the last two features condition on the aligned child phrase pic? in addition to the
direction and orientation.
We next give features that condition on the presence of gaps between the child and
parent target phrases and gaps between the aligned phrases on the source side. The
gap(c, d) function indicates whether there is a gap between the phrases indexed by c
and d:
gap(c, d) =
?
??
??
root if d = 0
yes if |d? c| ? 1
no otherwise
(24)
Given this gap function, we define the following features:
gpdep
orient
gap
= p?(?c | ?d, dir(c, d), ori(c, d, c
?, d?), gap(c, d), gap(c?, d?)) (25)
gpdep
orient
gap
clust
= p?(clust(?c) | clust(?d), dir(c, d), ori(c, d, c
?, d?), gap(c, d), gap(c?, d?)) (26)
All the features mentioned so far have the child phrase on the left-hand side of the
conditioning bar. We now present features that have both the child and parent phrases
on the left-hand side:
gpdep
pc
= p?(?c,?d, dir(c, d) | pic? ,pid? ) (27)
gpdep
pc
orient
= p?(?c,?d, dir(c, d) | pic? ,pid? , ori(c, d, c
?, d?)) (28)
gpdep
pc
orient
gap
= p?(?c,?d, dir(c, d), gap(c, d) | pic? ,pid? , ori(c, d, c
?, d?), gap(c?, d?)) (29)
These last features score larger rules composed of two phrase pairs from the phrase
table. Including direction, orientation, and gaps enables us to model longer-distance re-
orderings; we showed some examples of such frequently extracted phrase dependencies
in Section 4.2.
In all, we introduced 11 features in this section, giving us a total of 15 so far. For
the feature ablation experiments in Section 7, we will partition these features into two
parts: We refer to the six features with subscript clust as CLUST and the other nine
as WORD.
5.2.1 String-to-Tree Configurations (CFG). We now present features that count instances of
local reordering configurations involving phrase dependencies. We refer to the features
described in this section and the next section as CFG. These features consider the target
segmentation ?, the target phrase dependency tree ??, and the phrase alignment b,
but not the target words y or the source words x, segmentation pi, or dependency
tree ?x .
370
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Our first set of features only looks at configurations involving direction and orien-
tation. The first feature value is incremented if the child is to the left and the aligned
source-side phrases are in the same order:
fleft
same
= I
[
dir(c, d) = left ? ori(c, d, c?, d?) = same
]
(30)
Another feature fires if the aligned source phrases are in the opposite order:
fleft
swap
= I
[
dir(c, d) = left ? ori(c, d, c?, d?) = swap
]
(31)
Analogous features are used when the child is to the right of the parent:
fright
same
= I
[
dir(c, d) = right ? ori(c, d, c?, d?) = same
]
(32)
fright
swap
= I
[
dir(c, d) = right ? ori(c, d, c?, d?) = swap
]
(33)
These four configuration features are shown in order in the leftmost column in
Figure 7. They are agnostic as to the presence of gaps between the two target phrases
and between the two source phrases. We include 16 features that add gap information
to these four coarse configurations, as shown in the remainder of the table. Four gap
configurations are possible, constructed from one binary variable indicating the pres-
ence or absence of a source gap paired with a binary variable indicating the presence or
absence of a target gap. We replicate the four coarse features for each gap configuration,
giving us a total of 20 string-to-tree configuration features, all shown in Figure 7.
?
?
no gaps source gap target gap source andtarget gaps
coarse configurations
(only direction
and orientation)
xi xj xk xi xj xk xlxi xj xk xlxi xj xk
xlxi xj xk xlxi xj xk
xlxi xj xk xlxi xj xk
xlxi xj xk xlxi xj xk
xi xj xk
xi xj xk xi xj xk
xi xj xk xi xj xk
xi xj xk xi xj xk
?xi xj xk
?xi xj xk
?xi xj xk
yk?yi? yj? yk?yi? yj? yk?yi? yj? yl? yk?yi? yj? yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yk?yi? yj? yk?yi? yj?
yk?yi? yj? yk?yi? yj?
yk?yi? yj? yk?yi? yj?
? yk?yi? yj?
? yk?yi? yj?
? yk?yi? yj?
Figure 7
String-to-tree configurations; each is associated with a feature that counts its occurrences in
a derivation.
371
Computational Linguistics Volume 40, Number 2
5.2.2 Dependency Length Features. Related to the string-to-tree configurations are features
that score source- and target-side lengths (i.e., number of words crossed) of target-side
phrase dependencies. These lengths can also be useful for hard constraints to speed up
inference; we return to this in Section 6. These features and constraints are similar to
those used in vine grammar (Eisner and Smith 2005).
We first include a feature that counts the number of source-side words between the
aligned source phrases in each attachment in ??. Letting pic? = x
j?
i? and pid? = x
l?
k? :
fsrc
vine
= I
[
dir(c?, d?) = left
] (
k? ? (j? + 1)
)
+ I
[
dir(c?, d?) = right
] (
i? ? (l? + 1)
)
(34)
Although this feature requires the segmentation of the source sentence in order to
determine the number of source words crossed, the actual identities of those words are
not needed, so the feature does not depend on x. We would expect this feature?s weight
to be negative for most language pairs, encouraging closeness in the source sentence of
phrases aligned to each phrase dependency in the target.
We would like to use a similar feature for target-side dependency lengths, for
example, where ?c = y
j
i and ?d = x
l
k:
I [dir(c, d) = left]
(
k? (j + 1)
)
+ I [dir(c, d) = right] (i? (l + 1)) (35)
However, such a feature could require looking at the entire phrase segmentation being
generated to score a single phrase dependency (e.g., if ??(1) = n?). Using this feature
would prevent us from being able to use dynamic programming for decoding (we
discuss our approach to decoding in Section 6). Instead, we use a feature that considers
bounds on the number of target words crossed by each phrase dependency. In particular,
the feature sums the maximum number of target words that could be crossed by a
particular phrase dependency. We will discuss how this feature is computed when we
discuss decoding in Section 6.
We use CFG to refer to the set containing the 20 string-to-tree configuration features
and the 2 string-to-tree dependency length features. Adding these 22 features to the 15
from Sections 5.1 and 5.2 gives us 37 QPD features so far.
5.3 Tree-to-Tree Features (TREETOTREE)
The last two sets of features consider the source-side dependency tree ?x in addition
to x, pi, b, y, ?, and ??. These are the only features that use source and target syntax
simultaneously. We use TREETOTREE to refer to these features.
5.3.1 Quasi-Synchronous Tree-to-Tree Configurations. We begin with features based on
the quasi-synchronous configurations from Smith and Eisner (2006), shown for lexical
dependency trees in Figure 8. For a child-parent dependency on the target side, these
configurations consider the relationship between the aligned source words. For exam-
ple, if the aligned source words form a child-parent dependency in the source tree, then
we have a ?parent-child? configuration. There is also an ?other? category for those that
do not fit any of the named categories.
However, for our model we need to score configurations involving phrase depen-
dencies. That is, for a child-parent phrase dependency ??c,?d? in ??, we consider the
relationship between pic? and pid? , the source-side phrases to which ?c and ?d align.
372
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
xi   ...   xj
yk   ...   yl
parent-child
xh  ...  xi   ...  xj
yk   ...   yl
grandparent-grandchild c-command
xh  ...  xi   ...  xj
yk   ...   yl
siblings
xg  ...  xh   ...  xi  ...  xj
yk   ...   yl
$   ...   xi
$   ...   yk
root-root
xi   ...   xj
yk   ...   yl
child-parent
xi
yk   ...   yl
same-node
Figure 8
Quasi-synchronous tree-to-tree configurations from Smith and Eisner (2006). There are
additional configurations involving NULL alignments and an ?other? category for those that do
not fit into any of the named categories.
There are several options for computing configuration features for our model, since we
use a phrase dependency tree for the target sentence, a lexical dependency tree for the
source sentence, and a phrase alignment.
We use a heuristic approach. First we find the full set of configurations that are
present between any word in one source phrase and any word in the other source
phrase. That is, given a pair of source words, one with index j in source phrase d?
and the other with index k in source phrase c?, we have a parent-child configura-
tion if ?x (k) = j; if ?x (j) = k, a child-parent configuration is present. In order for the
grandparent-grandchild configuration to be present, the intervening parent word must
be outside both phrases. For sibling configurations, the shared parent must also be
outside both phrases. In lieu of standard (non-sibling) c-command relationships, we
define a modified c-command category as follows. We first find the highest ancestors of
words j and k that are still in their respective phrases. Of these two ancestors, if neither
is an ancestor of the other and if they are not siblings, then the ?c-command? feature
fires.
After obtaining a list of all configurations present for each pair of words ?j, k?, we fire
the feature for the single configuration corresponding to the maximum distance |j? k|.
If no configurations are present between any pair of words, the ?other? feature fires.
Therefore, only one configuration feature fires for each extracted phrase dependency
attachment.
For the six configurations other than ?root-root,? we actually include multiple
instances of each configuration feature: one set includes direction (6? 2 = 12 features),
another set includes orientation (12 features), and the final set includes both source- and
target-side gap information (24 features). There are therefore 49 features in this category
(including the single ?root-root? feature).
5.3.2 Tree-to-Tree Dependency Path Length Features. Finally, we include features that con-
sider the dependency path length between the source phrases aligned to the target
phrases in each phrase dependency. The features in Section 5.2.2 considered distance
along the source sentence (the number of words crossed). Now we add features that
consider distance along the source tree (the number of lexical dependency arcs crossed).
We expect the learned weights for these features to encourage short dependency path
lengths on the source side.
373
Computational Linguistics Volume 40, Number 2
We first include a feature that sums, for each target phrase i, the inverse of the
minimum undirected path length between each word in pic? = x
j?
i? and each word in
pid? = xl
?
k? :
fundir
path
=
j??
j=i?
l??
k=k?
1
minUndirPathLen(x, ?x , j, k)
(36)
where minUndirPathLen(x, ?x , j, k) returns the shortest undirected dependency path
length from xj to xk in ?x . The shortest undirected path length is defined as the number
of dependency arcs that must be crossed to travel from one word to the other along the
arcs in ?x .
Assuming an analogous function minDirPathLen(x, ?x , j, k) that computes the mini-
mum directed dependency path length, we also include the following feature:
fdir
path
=
j??
j=i?
l??
k=k?
1
minDirPathLen(x, ?x , j, k)
(37)
If there is no directed path from xj to xk, minDirPathLen returns?.
Adding these two features gives us a total of 88 QPD features. Along with the 14
phrase-based features there are a total of 102 features in our model.
6. Decoding
For our model, decoding consists of solving Equation (1)?that is, finding the highest-
scoring tuple ?y,pi,?, ??,b? for an input sentence x and its parse ?x . This is a challenging
search problem, because it is at least as hard as the search problem for phrase-based
models, which is intractable (Koehn, Och, and Marcu 2003). Because of this we use a
coarse-to-fine strategy for decoding (Charniak and Johnson 2005; Petrov 2009). Coarse-
to-fine inference is a general term for procedures that make two (or more) passes over
the search space, pruning the space with each pass. Typically, feature complexity is
increased in each pass, as richer features can often be computed more easily in the
smaller search space.
One simple coarse-to-fine procedure for our model would start by generating a
k-best list of derivations using a phrase-based decoder. This ?coarse model? would
account for all of the phrase-based features. Then we could parse each derivation to
incorporate the QPD features and rerank the k-best list with the modified scores; this is
the ?fine model.? The advantage of this approach is its simplicity, but other research has
shown that k-best lists for structured prediction tend to have very little diversity (Huang
2008), and we expect even less diversity in cases like machine translation where latent
variables are almost always present. Instead, we generate a phrase lattice (Ueffing, Och,
and Ney 2002) in a coarse pass and perform lattice dependency parsing as the fine pass.
The remainder of this section is laid out as follows. We begin by reviewing phrase
lattices in Section 6.1. In Section 6.2 we present our basic lattice dependency parsing
algorithm. We give three ways to speed it up in Section 6.3; one enables a more judicious
search without affecting the search space, and the other two prune the search space
in different ways. In Section 6.4, we discuss how decoding affects learning of the
feature weights ?, and we describe the structured support vector machine reranking
formulation from Yadollahpour, Batra, and Shakhnarovich (2013) that we use. We close
374
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
konnten / could
konnten / could
konnten sie / could you
sie / you
sie / you
es ?bersetzen / translate it
sie es   ?bersetzen / you translate it
?bersetzen /
translate
?bersetzen /
translate
es / it
es / it
es / it
? / ?
? / ?
... you
... could
... could
source:  konnten sie es ?bersetzen ?
reference:  could you translate it ?
Figure 9
Example phrase lattice for the source sentence shown. Each node contains an n-gram history for
computing n-gram language model features and a coverage vector representing the source
words that have been translated so far. For clarity, the n-gram history (n = 2) and coverage
vector are only shown for three nodes.
in Section 6.5 with a brief discussion of how this decoder differs from earlier versions
published in Gimpel and Smith (2009b, 2011).
6.1 Phrase Lattices
The most common decoding strategy for phrase-based models is to use beam
search (Koehn, Och, and Marcu 2003). The search is performed by choosing phrase
pairs from the phrase table and applying them to translate source phrases into the target
language. Coverage vectors are maintained during decoding to track which words have
been translated so far. They are used to enforce the constraint that each source word
appear in exactly one phrase pair.
It is often convenient to build a packed representation of the (pruned) search space
explored during decoding. For phrase-based models, this representation takes the form
of a phrase lattice (Ueffing, Och, and Ney 2002), a finite-state acceptor in which each
path corresponds to a derivation. Figure 9 shows an example. The source sentence and
a reference translation are shown at the top of the figure. Each path from the start node
on the left to a final node corresponds to a complete output in the model?s output space.
Each lattice edge corresponds to a phrase pair used in the output. All paths leading to a
given node in the lattice must agree in the set of source words that have been translated
thus far. So, every node in the lattice is annotated with the coverage vector of all paths
that end there. This is shown for three of the nodes in the figure.
The lattice is constructed such that all features in the model are locally computable
on individual lattice edges. To make n-gram language model features local, all paths
leading to a given node must end in the same n? 1 words.12 In the example, there
are two nodes with equivalent coverage vectors that are separated because they end in
12 In practice, this state replication can be reduced by exploiting sparsity in the language model (Li and
Khudanpur 2008).
375
Computational Linguistics Volume 40, Number 2
different words (you vs. could). Decoders like Moses can output phrase lattices like these;
the lattice simply encodes the paths explored during the beam search.
6.2 Lattice Dependency Parsing
Each path in a phrase lattice corresponds to a tuple ?y,pi,?,b? for the input x. To also
maximize over ??, we perform lattice dependency parsing, which allows us to search
over the space of tuples ?y,pi,?,b, ???. Lattice parsing jointly maximizes over paths
through a lattice and parse structures on those paths.
Because we use an arc-factored phrase dependency model (Equation (3)), the lattice
dependency parsing algorithm we use is a straightforward generalization of the arc-
factored dynamic programming algorithm from Eisner (1996). The algorithm is shown
in Figure 10. It is shown as a set of recursive equations in which shapes are used in
place of function names and shape indices are used in place of function arguments.
The equations ground out in functions edgeScore and arcScore that score individual
lattice edges and phrase dependency arcs, respectively.13 A semiring-generic format is
used; for decoding, the semiring ?plus? operator (?) would be defined as max and the
semiring ?times? operator (?) would be defined as +. The entry point when executing
the algorithm is to build GOAL, which in turn requires building the other structures.
We use a simple top?down implementation with memoization. Our style of spec-
ifying dynamic programming algorithms is similar to weighted deduction, but ad-
ditionally specifies indices and ranges of iteration, which are useful for a top?down
implementation. Top?down dynamic programming avoids the overhead of maintaining
a priority queue that is required by bottom?up agenda algorithms (Nederhof 2003;
Eisner, Goldlust, and Smith 2005).
The disadvantage of top?down dynamic programming is that wasted work can be
done; structures can be built that are never used in any full parse. This problem appears
when parsing with context-free grammars, and so the CKY algorithm works bottom?
up, starting with the smallest constituents and incrementally building larger ones. This
is because context-free grammars may contain rules with only non-terminals. Top?
down execution may consider the application of such rules in sequence, producing long
derivations of non-terminals that never ?ground out? in any symbols in the string. A
dependency model, on the other hand, always works directly on words when building
items, so a top?down implementation can avoid wasted effort.
However, this situation changes with lattice dependency parsing. It is possible for
a top?down lattice dependency parser to consider some dependencies that are never
used in a full parse. We address this issue in the next section.
6.3 Computational Complexity and Speeding Up Decoding
The lattice parsing algorithm requires O(E2V) time and O(E2 + VE) space, where E
is the number of edges in the lattice and V is the number of nodes. Typical phrase
lattices might easily contain tens of thousands of nodes and edges, making exact search
prohibitively expensive for all but the smallest lattices. So we use three techniques to
speed up decoding: (1) avoiding construction of items that are inconsequential (i.e.,
13 To prevent confusion, we use the term edge to refer to a phrase lattice edge and arc to refer to a
dependency attachment in a dependency tree.
376
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Figure 10
Lattice dependency parsing using an arc-factored dependency model. Lone indices like p and
i denote nodes in the lattice, and an ordered pair like (i, j) denotes the lattice edge from node i
to node j. START is the single start node in the lattice and FINAL is a set of final nodes. We use
edgeScore(i, j) to denote the model score of crossing lattice edge (i, j), which only includes
the phrase-based features h?. We use arcScore((i, j), (l, m)) to denote the score of building the
dependency arc from lattice edge (i, j) to its parent (l, m); arcScore only includes the QPD
features h??.
that could never be contained in a full parse), (2) pruning the lattices, and (3) limiting
the maximum length of a phrase dependency.
6.3.1 Avoiding Construction of Inconsequential Items. By design, our phrase lattices impose
several types of natural constraints on allowable dependency arcs. For example, each
node in the phrase lattice is annotated with a coverage vector?a bit vector indicating
377
Computational Linguistics Volume 40, Number 2
which words in the source sentence have been translated?which implies a topological
ordering of the nodes. Once a word in the source sentence has been covered (i.e.,
translated), it cannot be uncovered later. This can tell us whether certain nodes are
unreachable from other nodes. For example, for a three-word source sentence, there
cannot exist a directed path from a node with coverage vector ?0, 1, 0? to a node with
coverage vector ?0, 0, 1?. However, there may or may not be a path from a node with
vector ?0, 1, 0? to one with ?0, 1, 1?.
Generally, we need an efficient way to determine, for any two nodes in the lattice,
whether there exists a path from one to the other. If there is no path, we can avoid
wasting time figuring out the best way to build items that would end at the two nodes.
To discover this, we use an all-pairs shortest paths algorithm to find the score of the best
path between each pair of nodes in the lattice. The algorithm also tells us whether each
edge is reachable from each other edge, allowing us to avoid drawing dependencies
that will never ground out in a lattice path. We use the Floyd-Warshall algorithm (Floyd
1962). This adds some initial overhead to decoding, but in preliminary experiments we
found that it saves more time than it costs. We actually run a modified version of the
algorithm that computes the length (in words) of the longest path between any two
nodes. If the maximum length between two nodes is?, the nodes are unreachable from
each other. Before we build an item in the algorithm in Figure 10, we check reachability
of the item endpoints and only proceed if one can reach the other.
We modified the algorithm to output maximum lengths because we use the max-
imum lengths to compute the target-side vine grammar features and constraints, as
mentioned in Section 5.2.2. In particular we use a feature ftgt
vine
that is a target-side
analog to fsrc
vine
but using the Floyd-Warshall maximum path lengths in place of the actual
lengths.
6.3.2 Lattice Pruning. To reduce phrase lattice sizes, we prune lattice edges using
forward?backward pruning (Sixtus and Ortmanns 1999), which has also been used by
Tromble et al. (2008). This pruning method computes the max-marginal for each lattice
edge, which is the score of the best full path that uses that edge, then prunes edges
whose max-marginal is below a certain fraction of the best path score in the lattice. Max-
marginals have been used for other coarse-to-fine learning frameworks (Weiss, Sapp,
and Taskar 2010) and offer the advantage that the best path in the lattice is preserved
during pruning.
We only use the score contribution from the phrase-based features when computing
these max-marginals. For each lattice, we use a grid search to find the most liberal
threshold that leaves fewer than 2,000 edges in the resulting lattice. As complexity is
quadratic in E, forcing E to be less than 2,000 improves runtime substantially. After
pruning, the lattices contain more than 1016 paths on average and oracle BLEU scores
are typically 10?15 points higher than the model-best paths.
6.3.3 Maximum Dependency Lengths. We can easily adapt our vine grammar features
to function as hard constraints on allowable dependency trees, as originally done by
Eisner and Smith (2005) for monolingual dependency parsing. We use two simple
constraints on the maximum length of a phrase dependency used during translation.
One constrains the number of source words that are crossed from one aligned source
phrase to the other aligned source phrase by the phrase dependency. The other con-
strains the maximum number of target-side words crossed by any path from one
target phrase to the other target phrase in a phrase dependency. During translation,
we never build items that would require using dependency arcs that violate these
378
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
constraints. In Section 7 we discuss the values we used in our primary experiments
and also compare translation quality and decoding speed for several values of these
hyperparameters.
6.4 Interaction with Learning
The use of a coarse-to-fine decoding procedure affects how we learn the parameters
of our model. We use two separate versions of the phrase-based feature weights: one
for lattice generation and one for lattice dependency parsing. This is common with
coarse-to-fine strategies?separate instances of coarser parameters are required for each
subsequent pass. We first learn parameters for the coarse phrase-based model used
to generate phrase lattices. Then, after generating the lattices, we prune them (Sec-
tion 6.3.2) and use a second round of tuning to learn parameters of the fine model, which
includes all phrase-based and QPD feature weights. We initialized the phrase-based
feature weights using the default Moses weights. For the QPD features, we initialized
the phrase dependency probability feature weights to 0.002 and the weights for all other
features to 0.
For tuning, we need the k-best outputs, for which efficient dynamic programming
algorithms are available. We use Algorithm 3 from Huang and Chiang (2005), which
lazily finds the k best derivations efficiently. In preliminary testing, we found that the
k-best lists tended to be dominated by repeated translations with different derivations,
so we used the technique presented by Huang, Knight, and Joshi (2006), which finds a
unique k-best list, returning the highest-scoring derivation for each of k unique transla-
tions. This modification requires the maintenance of additional data structures to store
all of the previously found string yields for each item built during parsing. This incurs
additional overhead but allows us to obtain a far more diverse k-best list given a fixed
time and memory budget.
For the first round of tuning, we use RAMPION (Gimpel and Smith 2012b), which
performs competitively with minimum error rate training (Och 2003) but is more stable.
For training the fine model, however, we found that RAMPION did not lead to substan-
tial improvements over the output of the coarse phrase-based model alone. We found
better performance by using a fine learner designed for the k-best reranking setting, in
particular the structured support vector machine reranker described by Yadollahpour,
Batra, and Shakhnarovich (2013). Though we are doing lattice reranking rather than
k-best reranking, the learning problem for our fine model is similar to that for k-best
reranking in that the decoder is exact (i.e., there is no pruning that could lead to different
patterns of search error as the parameters change). That is, phrase lattice generation and
pruning (described in Section 6.3.2) only depend on the coarse phrase-based feature
weights and the maximum dependency length constraints (described in Section 6.3.3);
they do not depend on the fine model parameters.
We now briefly describe how we learn parameters for the fine model via lattice
reranking. For simplicity, we will only write the source sentence x and its translation
y when describing the reranker and omit the additional input and output variables
?x ,pi,?, ??, and b, but they are always present and used for computing features. We
assume a tuning set with N source sentences: {xi}Ni=1. Let Y
R
i be the set of reference
translations for source sentence xi. Let Yi = {y
(1)
i . . .y
(k)
i } denote the set of k candidate
translations (outputs of our lattice dependency parsing decoder) for xi. Let y?i denote
the highest-quality translation in the set, that is, y?i = argminy?Yi `(Y
R
i ,y), where `(Y
R
i ,y)
is the negated BLEU+1 score (Lin and Och 2004) of y evaluated against references YRi .
379
Computational Linguistics Volume 40, Number 2
We use the following cost function for sentence i and candidate translation y:
L(YRi ,y) = `(Y
R
i ,y)? `(Y
R
i ,y
?
i ) (38)
that is, the negated BLEU+1 score of translation yi relative to that of the best translation
(y?i ) in the set.
Yadollahpour, Batra, and Shakhnarovich (2013) formulate the reranking learning
problem as an L2-regularized slack-rescaled structured support vector machine (SSVM;
Tsochantaridis et al. 2005). The feature weights ? for the fine model are learned by
solving the following quadratic program:
min
?,?i
||?||22 + ?
?
i?[N]
?i (39a)
s.t. ?>
(
h(xi,y
?
i )? h(xi,y)
)
? 1?
?i
L(YRi ,y)
(39b)
?i ? 0, ?y ? Yi \ y
?
i , (39c)
In Equation (39b), the violation in the margin ?i is scaled by the cost of the translation.
Thus if in addition to y?i there are other good solutions in the set, the margin for such
solutions will not be tightly enforced. On the other hand, the margin between y?i and
bad solutions will be very strictly enforced. Equation (39) is solved via the 1-slack
cutting-plane algorithm of Joachims, Finley, and Yu (2009).14 During the execution of
the cutting-plane algorithm, we compute the tuning set BLEU score with all param-
eter vector values that are considered. At convergence we return the parameters that
led to the highest tuning BLEU score. This helps to bridge the discrepancy between
our use of sentence-level BLEU+1 in the loss function and corpus BLEU for final
evaluation.
We alternate between generating k-best lists using our lattice parser and solving
Equation (39) on the fixed lists, each time pooling all previous iterations? lists. We repeat
until the parameters do not change, up to a maximum of 15 iterations. We used k-best
lists of size 150 and a fixed, untuned value of ? = 0.1 for all experiments.
6.5 Comparison to Earlier Work
The decoder described above represents some advances over those presented in earlier
papers. Our original decoder was designed for a lexical dependency model; we used
lattice dependency parsing on lattices in which each edge contained a single source-
target word pair (Gimpel and Smith 2009b). Inference was approximated using cube
decoding (Gimpel and Smith 2009a), an algorithm that incorporates non-local features
in a way similar to cube pruning (Chiang 2007). After developing our QPD model,
we moved to phrase lattices but still approximated inference using an agenda algo-
rithm (Nederhof 2003; Eisner, Goldlust, and Smith 2005) with pre-pruning of depen-
dency edges in a coarse pass (Gimpel and Smith 2011).
14 We used OOQP (Gertz and Wright 2003) to solve the quadratic program in the inner loop, which uses
HSL, a collection of Fortran codes for large-scale scientific computation (www.hsl.rl.ac.uk).
380
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
All decoders used lattice dependency parsing, but our current decoder uses an exact
algorithm once two simple approximations are made: the pruning of the lattice and the
use of maximum dependency length constraints. Hyperparameters control the severity
of these two approximations and the use of an exact parsing algorithm allows us to
measure their effects on runtime and accuracy.
7. Experiments and Analysis
We now present experimental results using our QPD model. Because our model extends
phrase-based translation models with features on source- and target-side syntactic
structures, we can conduct experiments that simulate phrase-based, string-to-tree, and
tree-to-tree translation, merely by specifying which feature sets to include. This suggests
an additional benefit of using a quasi-synchronous approach for machine translation. By
using features rather than constraints, we can simulate a range of translation systems
in a single framework, allowing clean experimental comparisons among modeling
strategies and combining strengths of diverse approaches.
We describe our experimental setup in Section 7.1 and present our main results in
Section 7.2. We measure the impact of using unsupervised parsing in Section 7.2.1 and
include feature ablation experiments in Section 7.2.2. We present the results of a manual
evaluation in Section 7.3 and give examples. We conclude in Section 7.4 with a runtime
analysis of our decoder and show the impact of decoding constraints on speed and
translation quality.
7.1 Experimental Setup
In this section we describe details common to the experiments reported in this sec-
tion. Details about decoding and learning were described in Section 6. Full details
about language pairs, data sets, and baseline systems are given in Appendix A and
Appendix B. We repeat important details here. We use case-insensitive IBM BLEU
(Papineni et al. 2002) for evaluation. To measure significance, we use a paired bootstrap
(Koehn 2004) with 100,000 samples (p ? 0.05).
7.1.1 Language Pairs. We consider German?English (DE?EN), Chinese?English
(ZH?EN), Urdu?English (UR?EN), and English?Malagasy (EN?MG) translation.
These four languages exhibit a range of syntactic divergence from English. They also
vary in the availability of resources like parallel data, monolingual target-language data,
and treebanks. It is standard practice to evaluate unsupervised parsers on languages
that do actually have treebanks, which are used for evaluation. We consider this case
as well, comparing supervised parsers for English and Chinese to our unsupervised
parsers, but we also want to evaluate our ability to exploit unsupervised parsing for
languages that have small or nonexistent treebanks, hence our inclusion of Urdu and
Malagasy.
7.1.2 Baselines. We compare our model to several baselines:
r Moses, RAMPION, S = 200: This is a standard Moses phrase-based system,
trained with RAMPION. The Moses default stack size S of 200 was used
during tuning and testing. This is the result one would obtain with an
off-the-shelf Moses phrase-based system on these data sets (and trained
using RAMPION).
381
Computational Linguistics Volume 40, Number 2
r Moses, RAMPION, S = 500: This baseline trains a model in the same way as
the previous using S = 200, but then uses a larger stack size (S = 500)
when decoding the test sets. This larger stack size was used for generating
phrase lattices for lattice reranking, so it provides a more appropriate
baseline for comparing to our model.
r Moses, SSVM reranking: Using phrase lattices generated with the
preceding configuration, this baseline uses the SSVM reranker from
Section 6.4 on the phrase lattices with only the Moses phrase-based features,
that is, without any QPD features. This baseline helps to separate out the
gains achieved through SSVM reranking and the addition of QPD features.
r Hiero, RAMPION: This is a standard hierarchical phrase-based
system (Chiang 2007), as implemented in the Moses toolkit and trained
using RAMPION.
We see the three Moses systems as our primary baselines because Moses was used
to generate phrase lattices for our system. Our model adds new syntactic structures and
features to Moses, but because our decoder use Moses? phrase lattices, our approach
can be viewed as rescoring Moses? search space. There are pros and cons to this choice.
It lets us build on a strong baseline rather than building a system from scratch. Also, by
comparing the third baseline (?Moses, SSVM reranking?) to our model, we are able to
cleanly measure the contribution of our QPD features. However, Hiero has been shown
to perform better than phrase-based systems for certain language pairs (Chiang 2007;
Zollmann et al. 2008; Birch, Blunsom, and Osborne 2009), and in these cases Hiero
proves to be a strong baseline for our model to beat as well. We note that our QPD
features could also be used to rescore Hiero?s search space to potentially yield further
improvements, but we leave this to future work.
7.1.3 Parsers. Our full QPD model requires parsers for both source and target languages.
For each language pair, the target-language parser is only used to parse the target side
of the parallel corpus and the source-language parser is only used to parse the source
side of the tuning and test sets.
We have access to supervised parsers for Chinese, German, and English, which we
used for our experiments. In particular, we used the Stanford parser (Levy and Manning
2003; Rafferty and Manning 2008) for Chinese and German and TurboParser (Martins
et al. 2010) for English (see Appendix A for details). The Stanford parser is fundamen-
tally a phrase-structure parser and generates dependency trees via head rules, but we
chose it for our experiments for its ease of use and compatibility with the tokenization
we used, particularly the Chinese segmentation which we obtained from the Stanford
Chinese segmenter.15
For Urdu and Malagasy, we turn to unsupervised parsing. To measure the impact
of using unsupervised parsers, we also performed experiments in which we replaced
supervised parsers for Chinese and English with unsupervised counterparts. We now
describe how we trained unsupervised parsers for these four languages.
15 More dependency parsers have been made available by the research community since we began this
research and would be natural choices for further experimentation, such as ParZu (Sennrich et al. 2009)
for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and
DuDuPlus (Chen et al. 2012) for Chinese.
382
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
The most common approach to unsupervised parsing is to train models on sen-
tences from treebanks (without using the annotated trees, of course) along with their
gold standard POS tags. This practice must be changed if we wish to use unsupervised
parsing for machine translation, because we do not have gold standard POS tags for
our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using
automatic POS tags for dependency grammar induction can work as well as or better
than gold standard POS tags. For syntax-based translation, Zollmann and Vogel (2011)
showed that unsupervised tags could work as well as those from a supervised POS
tagger.
For Urdu and Malagasy, we use fully unsupervised POS tagging, using the ap-
proach from Berg-Kirkpatrick et al. (2010) with 40 tags. We use the ?direct gradient?
version optimized by L-BFGS (Liu and Nocedal 1989). For Chinese and English, we
use the gold standard POS tags from their respective treebanks for training the parser,
then use the Stanford POS tagger (Toutanova et al. 2003) to tag the parallel data, tuning,
and test sets. As our dependency parsing model, we use the dependency model with
valence (Klein and Manning 2004) initialized with a convex initializer (Gimpel and
Smith 2012a). The training procedure is described in Gimpel (2012). Our Chinese and
English unsupervised parsers are roughly 30 percentage points worse than supervised
parsers in dependency attachment accuracy on standard treebank test sets.
We also compared the supervised and unsupervised parsers to a uniform-at-random
parser. Well-known algorithms exist for sampling derivations under a context-free
grammar for a sentence (Johnson, Griffiths, and Goldwater 2007). These algorithms can
be used to sample projective dependency trees by representing a projective dependency
grammar using a context-free grammar (Smith 2006; Johnson 2007). We used cdec (Dyer
et al. 2010) to sample projective dependency trees uniformly at random for each
sentence.16
We only compared the random parser for source-side parsing. Swapping parsers for
the target language requires parsing the target side of the parallel corpus, rerunning
rule extraction and feature computation with the new parses, and finally re-tuning to
learn new feature weights. By contrast, changing the source-side parser only requires
re-parsing the source side of the tuning and test sets and re-tuning.
7.2 Results
We now present our main results, shown in Tables 6?9. We see that enlarging the
search space results in gains in BLEU, as Moses with stack size 500 typically out-
performs Moses with stack size 200. For DE?EN (Table 6), SSVM reranking im-
proves performance even without adding any more features, pushing the numbers
close to that of Hiero; and adding our QPD features does not provide any additional
improvement.
For the other language pairs, however, we do see significant gains over the Moses
baselines. For ZH?EN (Table 7), we see an average gain of 0.5 BLEU over the best
Moses baseline when using target syntactic features (TGTTREE), and a total average gain
of 0.7 BLEU with the full QPD model (TGTTREE + TREETOTREE). The QPD numbers
still lag behind the Hiero results on average, but are statistically indistinguishable from
Hiero on two of the three test sets. Our QPD features are able to mostly close the
16 We thank Chris Dyer for implementing this feature in cdec for us.
383
Computational Linguistics Volume 40, Number 2
Table 6
%BLEU on tune and test sets for DE?EN translation, comparing the baselines to our QPD model
with target syntactic features (TGTTREE) and then also with source syntax (+ TREETOTREE).
Here, merely using the additional round of tuning with the SSVM reranker improves the BLEU
score to 19.9, which is statistically indistinguishable from the two QPD feature sets. Differences
between Hiero and the three 19.9 numbers are at the border of statistical significance; the first
two are statistically indistinguishable from Hiero but the third is different at p = 0.04.
German?English
model notes tune test
Moses RAMPION, S = 200 16.2 19.0
RAMPION, S = 500 16.2 19.2
SSVM reranking 16.9 19.9
QPD TGTTREE 17.2 19.9
TGTTREE + TREETOTREE 17.1 19.9
Hiero RAMPION 17.1 20.1
Table 7
%BLEU on tune and test sets for ZH?EN translation, showing the contribution of feature sets in
our QPD model. Both QPD models are significantly better than the best Moses numbers on test
sets 1 and 2, but not on test set 3. The full QPD model is significantly better than the version with
only TGTTREE features on test set 1 but statistically indistinguishable on the other two test sets.
Hiero is significantly better than the full QPD model on test set 2 but not on the other two.
Chinese?English
model notes tune test 1 test 2 test 3 test avg.
Moses RAMPION, S = 200 36.0 35.5 34.3 31.3 33.7
RAMPION, S = 500 36.2 36.1 34.6 31.8 34.2
SSVM reranking 36.3 36.1 34.6 31.8 34.2
QPD TGTTREE 37.1 36.8 35.3 32.0 34.7
TGTTREE + TREETOTREE 37.3 37.2 35.5 31.9 34.9
Hiero RAMPION 37.3 37.4 36.1 32.1 35.2
performance gap between Moses and Hiero, suggesting that the Moses search space
(and even our heavily pruned Moses phrase lattices) has the potential for significant
improvements when using the right features.
Results for UR?EN translation are shown in Table 8. Here we only have a super-
vised parser for English, so the TREETOTREE features are incorporated using our unsu-
pervised Urdu parser. All QPD results are significantly better than all Moses baseline
results, but there is no significant difference between the two QPD feature sets. This
may be due to our use of unsupervised parsing; perhaps the Urdu parses are too noisy
for us to see any benefit from the TREETOTREE features. In Section 7.2.1 we measure the
impact of using unsupervised parsing for ZH?EN translation. Hiero still significantly
outperforms the QPD model, although we have halfway closed the gap between Moses
and Hiero.
384
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table 8
%BLEU on tune and test sets for UR?EN translation, using our unsupervised Urdu parser to
incorporate source syntactic features. The two QPD rows are statistically indistinguishable on
both test sets. Both are significantly better than all Moses results, but Hiero is significantly better
than all others.
Urdu?English
model notes tune test 1 test 2 test avg.
Moses RAMPION, S = 200 24.6 24.6 24.5 24.5
RAMPION, S = 500 24.7 24.8 24.9 24.8
SSVM reranking 24.9 24.4 24.7 24.6
QPD TGTTREE 25.8 25.4 25.5 25.4
TGTTREE + (unsup.) TREETOTREE 25.8 25.4 25.6 25.5
Hiero RAMPION 25.7 26.4 26.7 26.6
Table 9
%BLEU on tune and test sets for EN?MG translation, using a supervised English parser and an
unsupervised Malagasy parser. The 15.6 BLEU reached by the full QPD model is statistically
significantly better than all other results on the test set. All other test set numbers are statistically
indistinguishable.
English?Malagasy
model notes tune test
Moses RAMPION, S = 200 17.6 15.1
RAMPION, S = 500 17.8 15.1
SSVM reranking 17.8 15.1
QPD (unsup.) TGTTREE 17.6 15.2
(unsup.) TGTTREE + TREETOTREE 17.9 15.6
Hiero RAMPION 17.4 15.0
For EN?MG translation (Table 9), we see significant gains in BLEU over both
Moses and Hiero when using the full QPD model.17 We used unsupervised parsing
to incorporate TGTTREE features but we only see a statistically significant improvement
when we add TREETOTREE features, which use a supervised English parser.
7.2.1 Impact of Unsupervised Parsing. Table 10 shows results when comparing parsers
for ZH?EN translation. We pair supervised and unsupervised parsers for English and
Chinese. The final row shows the Moses BLEU scores for comparison.
17 For the EN?MG experiments, we modified our initialization procedure for the QPD feature weights.
When using the same initialization as the other language pairs (setting QPD probability feature weights
to 0.002 and all other QPD weights to 0), we found that SSVM reranking did not find any higher BLEU
score in the initial k-best lists than the 1-best translations for all sentences. So we multiplied the initial
QPD weights by 10 in an effort to inject more diversity in the initial k-best lists.
385
Computational Linguistics Volume 40, Number 2
Table 10
%BLEU on tune and test sets when comparing parsers for ZH?EN translation. QPD uses all
features, including TGTTREE and TREETOTREE. The table first pairs supervised English parsing
with supervised, unsupervised, and random Chinese parsing, then pairs unsupervised English
parsing with supervised and unsupervised Chinese parsing. ? = significantly better than
sup/sup, ? = significantly worse than sup/sup.
EN parser ZH parser tune test 1 test 2 test 3 avg. test
%BLEU %BLEU %BLEU %BLEU %BLEU
Q
P
D sup.
sup. 37.3 37.2 35.5 31.9 34.9
unsup. 37.2 37.0 35.8? 31.8 34.9
random 37.1 36.5? 35.2 31.6? 34.4
unsup. sup. 37.2 37.1 35.3 31.7
? 34.7
unsup. 37.2 36.8? 35.3 31.5? 34.5
Moses, RAMPION, S = 500 36.2 36.1? 34.6? 31.8 34.2
When using supervised English parsing, we find that using our unsupervised
Chinese parser in place of the Stanford parser leads to the same average test set BLEU
score. When instead using random Chinese parses, we see a significant drop on two of
the three test sets and an average decrease of 0.5 BLEU. When pairing unsupervised
English parsing with supervised Chinese parsing, we see an average drop of just 0.2
BLEU compared to the fully supervised case. When both parsers are unsupervised,
BLEU scores drop further but are still above the best Moses baseline on average.
One idea that we have not explored is to parse our parallel corpus using each
parser (unsupervised and supervised), then extract rules consistent with any of the
parses. This might give us some of the benefits of forest-based rule extraction, which has
frequently been shown to improve translation quality (Liu et al. 2007; Mi, Huang, and
Liu 2008; Mi and Huang 2008). Similarly, because we train systems for several language
pairs, we could pool the rules extracted from all parallel corpora for computing target-
syntactic features. For example, adding the English phrase dependency rules from the
DE?EN corpus could improve performance of our ZH?EN and UR?EN systems.
Moving beyond translation, we could use the pool of extracted rules from all systems
(and using all parsers) to build monolingual phrase dependency parsers for use in other
applications (Wu et al. 2009).
7.2.2 Feature Ablation. We performed feature ablation experiments for UR?EN transla-
tion, shown in Table 11. Starting with TGTTREE features, which consist of word (WORD),
cluster (CLUST), and configuration (CFG) feature sets, we alternately removed each of
the three. We find only a small (and statistically insignificant) drop in BLEU when
omitting word features, but a larger drop when omitting word cluster features. This
may be due to the small size of our training data for UR?EN (approximately 1 million
words of parallel text). With limited training data, it is not surprising that unlexicalized
features like the cluster and configuration features would show a stronger effect than
the lexicalized features.
7.3 Human Evaluation
We focused on UR?EN and ZH?EN translation for our manual evaluation, as these
language pairs showed the largest gains in BLEU when using our QPD model. We
386
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table 11
Feature ablation experiments for UR?EN translation with string-to-tree features, showing the
drop in BLEU when separately removing word (WORD), cluster (CLUST), and configuration
(CFG) feature sets. ? = significantly worse than TGTTREE. Removing word features causes no
significant difference. Removing cluster features results in a significant difference on both test
sets, and removing configuration features results in a significant difference on test 2 only.
Urdu?English
model notes tune test 1 test 2 test avg. (?)
Moses SSVM reranking 24.9 24.4 24.7 24.6
QPD TGTTREE = WORD + CLUST + CFG 25.8 25.4 25.5 25.4
TGTTREE ? WORD 25.6 25.0 25.5 25.2 (?0.2)
TGTTREE ? CLUST 25.4 24.8? 24.9? 24.9 (?0.5)
TGTTREE ? CFG 25.1 25.1 25.0? 25.0 (?0.4)
began by performing a human evaluation using Amazon Mechanical Turk (MTurk)
in order to validate the BLEU differences against human preference judgments and to
identify translations that were consistently judged better under each model for follow-
up manual evaluation.
7.3.1 Procedure. We first removed sentences with unknown words, as we feared they
would only confuse judges.18 We then randomly selected 500 sentences from UR?EN
test 2 and 500 from the concatenation of ZH?EN test 1 and test 2. For each of the 1,000
sentences, we chose a single reference translation from among the four references to
show to judges.19 All text was detokenized. Judges were shown the reference transla-
tion, the translation from the Moses system with SSVM reranking, and the translation
from our QPD system with the full feature set. We randomized the order in which the
two machine translations were presented. Judges were asked to select which translation
was closer in meaning to the reference; alternatively, they could indicate that they were
of the same quality. We obtained judgments like these from three judges for each of the
1,000 sentences.
7.3.2 Results and Analysis. Table 12 shows the results of our MTurk experiments. If a
sentence was judged to be translated better by one system more often than the other,
it was counted as a victory for that system. The QPD translations for 40?43% of the
sentences were preferred over Moses, but for 28?33% of the sentences, the reverse was
true.
We can use these judgments to study when and how our system improves over
Moses, and also when Moses still performs better. For a follow-up manual evaluation,
we looked at ZH?EN sentences for which all three judges selected either Moses or
the QPD model; these should be the clearest examples of success for each system. In
18 Although this filtering step may introduce bias, we confirmed that the system differences in BLEU were
similar whether looking at sentences with unknown words, those without unknown words, or all
sentences.
19 For UR?EN test 2 and ZH?EN test 2, we chose the first reference set from the four provided. For
ZH?EN test 1, we instead chose the second reference set because its average length was closer to the
average across the four reference sets.
387
Computational Linguistics Volume 40, Number 2
Table 12
Results of human evaluation performed via Amazon Mechanical Turk. The percentages
represent the portion of sentences for which one system had more preference judgments
than the other system. If a sentence had an equal number of judgments for the two systems,
it was counted in the final row (?neither preferred?).
% of sentences preferred
ZH?EN UR?EN
Moses, SSVM reranking 33.4% 28.6%
QPD, TGTTREE + TREETOTREE 40.6% 42.8%
neither preferred 26.0% 28.6%
looking at these sentences, we attempted to categorize the primary reasons why all three
judges would have preferred one system?s output over the other. We began with two
broad categories of improvement: word choice and word order. We divided word choice
improvements into two subcategories: those involving verbs and those involving words
other than verbs. The reason we made this distinction is because some differences in
non-verb translation are not as crucial for understanding a sentence as differences in
verb translation or word order. Anecdotally, we observed that when one sentence has
a better verb translation and the other has a better preposition translation, judges tend
to prefer the translation with the better verb. We noted some sentences that fit multiple
categories, but in our analysis we chose a single category that we deemed to be the most
important factor in the judges? decisions.
Of the 26 sentences for which Moses output was preferred unanimously, we agreed
with the consensus on 25 and found that 19 of these improved due to better word
choice, most frequently (13 out of 19) for words other than verbs. Only 6 of the 25
were determined to be preferred due to word order. The top section of Table 13 shows
representative examples when Moses? translations were unanimously preferred. Moses
handles prepositions and other function words better than the QPD model in these
examples. This may occur due to the reliance of phrase-based systems upon strong
n-gram language models to ensure local fluency. The QPD model uses all of Moses?
features, including the same n-gram language model, but it adds many other features
that score longer-distance word order and may be overwhelming the n-gram model in
certain cases.
For the 44 sentences for which QPD output was unanimously preferred, we agreed
with the judges on 42. Of these, we found that 15 had improved word order, 14 had
improvements in verb word choice, and 13 had improved word choice for non-verbs.
So the QPD model?s improvements were due to word order on 36% of unanimous
sentences, compared with Moses? 24%, suggesting that the QPD model?s strength is
in improving word order. The lower section of Table 13 shows representative examples.
Consider the final example in the table. The Moses translation has better local fluency,
but mixes words across clauses, confusing the meaning. The QPD translation has two
local disfluencies (?because law-abiding citizen hopes to? and ?don?t need to fear will
attack?), but has no erroneous word reordering across clause boundaries.
7.4 Decoding Speed
So far we have reported BLEU scores for various feature sets and parsers, but we
have not discussed decoding speed. BLEU improvements may not be worth substantial
388
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
reductions in translation speed. In this section we report decoding speeds and BLEU
scores for UR?EN translation as pruning thresholds are varied. Our lattice dependency
parsing decoding algorithm is exact, but two pruning stages precede lattice parsing, as
discussed in Section 6.3: (1) pruning the phrase lattices based on the phrase-based model
Table 13
Examples from manual evaluation of ZH?EN output, selected from sentences with unanimous
preference judgments for Moses (top) and our QPD model (bottom). R = reference; M = Moses,
SSVM reranking; Q = QPD, TGTTREE + TREETOTREE.
Moses preferred:
R the purpose of cheney?s trip is to win supports of middle east countries in case of a u.s.
attack on iraq.
M cheney?s trip is to lobby the middle east countries to support the us attack on iraq.
Q cheney?s trip was to lobby the middle east countries support us attacks on iraq.
R he said, "after this conference, our position is clear to all."
M he said: "this time after the talks, our position is well known."
Q he said: "this round of talks, our position is well known."
R ukraine condemned us for interfering its internal affairs.
M ukraine condemned the united states for interfering in its internal affairs.
Q ukraine condemned the united states of interfering in its internal affairs.
R former serbian interior minister, vlajko stojiljkovic, attempts to commit suicide.
M yugoslavia, a former serbian interior minister tried to commit suicide.
Q interior minister tried to commit suicide before the serbian republic of yugoslavia.
R arafat died in a french hospital of an unknown illness on november 11.
M arafat died in a french hospital on nov. 11, due to an unidentified disease.
Q arafat died on nov. 11 in a french hospital, an unidentified disease.
QPD preferred:
R bush: nato must forge a new relationship with russia
M bush said that the establishment of a new type of relationship between russia and nato
must
Q bush says nato must build new relations with russia
R that would portend increased flows of glacial ice from antarctica and a rise in global sea
levels, endangering low-lying land areas.
M this bodes well for the antarctica ice flow will increase, the global sea level will rise in the
lowlands, will cause harm to the region.
Q this indicates that the antarctica?s ice flow will increase, global sea level will rise, and will
cause harm to the lowlands region.
R this is the first sandstorm in north korea since spring this year.
M this is the first since the spring of this year the sandstorms.
Q this is the first sandstorm of the korean experience since spring this year.
R "we really have to tackle the problem of street-crime. law-abiding citizens want to feel safe
when they walk on the street. they want a peaceful life and untroubled by attack," he said.
M he said: "we really should rectify the order because of law-abiding citizens, in the hope of
peace and security on the streets and on the living, not to be afraid of attacks."
Q he said: "we really should rectify the order in the streets, because law-abiding citizen hopes
to secure a safe life on the streets, and don?t need to fear will attack."
389
Computational Linguistics Volume 40, Number 2
scores, and (2) pruning the search space deterministically based on source- and target-
side limits on dependency lengths. In this section, we measure the impact of the latter
type of pruning only.20
We vary maximum dependency lengths and we report BLEU scores and decoding
speeds. We find that we can set these limits to be relatively strict and get similar BLEU
scores in less time. In all previous experiments, we used a source-side limit ?x of 15
and a target-side limit ?y of 20. That is, all target-side phrase dependencies may cover
a maximum of 20 words in the target sentence, and the number of words between the
aligned source phrases can be at most 15. We often use a larger value of ?y because
it is constraining an upper bound on the number of words crossed in the translation,
whereas?x constraints the exact number of source words crossed by a dependency (see
Section 6.3.3 for details).
For timing experiments, we ran a single decoding thread on a Sun Fire X2200 M2 x64
server with two 2.6-GHz dual-core CPUs. Decoding during tuning is time-consuming,
because we generate unique 150-best lists for each iteration, so we only use two max
dependency length settings for tuning. But given trained models, finding the 1-best
output on the test data is much faster. So we experimented with more pruning settings
for decoding. Table 14 shows our results. The upper table reminds us of the baseline
BLEU scores. The lower table shows what happens when we train with two pruning
settings: (?x = 10,?y = 15) and (?x = 15,?y = 20), and test with many others.
The times reported only include the time required for running the Floyd-Warshall
algorithm on the lattice and performing lattice dependency parsing. We use the Moses
decoder for lattice generation; this typically takes only slightly longer than ordinary
decoding, which is generally in the range of a couple seconds per sentence, depending
on how the phrase table and language model are accessed. The average time required
to run the Floyd-Warshall algorithm on the lattices is approximately 0.8 seconds per
sentence, so it begins to dominate the total time as the pruning thresholds go below
(5, 5). The earlier numbers in this section used (?x = 15,?y = 20) for both tuning and
testing, which causes test-time decoding to take approximately 6 seconds per sentence,
as shown in the table. We can see that we can use stricter constraints during test-
time decoding only (e.g., (?x = 5,?y = 10)) and speed this up by a factor of 3 while
only losing 0.1 BLEU. The only severe drops in BLEU appear when using thresholds
below (5, 5).
8. Conclusion and Future Work
We presented a new approach to machine translation that combines phrases, depen-
dency syntax, and quasi-synchronous tree-to-tree relationships. We introduced several
categories of features for dependency-based translation, including string-to-tree and
tree-to-tree features. We proposed lattice dependency parsing to solve the decoding
problem and presented ways to speed up the search and prune the search space. We
presented experimental results on seven test sets across four language pairs, finding
statistically significant improvements over strong phrase-based baselines on five of the
seven. Manual inspection reveals improvement in the translation of verbs, an important
component in preserving the meaning of the source text. We showed that unsupervised
20 Ideally we could also measure the impact of pruning the phrase lattices to various sizes, but this would
require the time-consuming process of filtering our phrase dependency tables for each lattice size, so we
have not yet tested the effect of this pruning systematically.
390
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table 14
%BLEU on tune and test sets for UR?EN translation, comparing several settings for maximum
dependency lengths in the decoder (?x is for the source side and?y is for the target side). The
upper table shows Moses BLEU scores for comparison. The lower table compares two max
dependency length settings during tuning, and several for decoding on the test sets, showing
both BLEU scores and average decoding times per sentence. See text for discussion.
tune
%BLEU
test 1
%BLEU
test 2
%BLEU
avg. test
%BLEU
Moses, SSVM reranking 24.9 24.4 24.7 24.6
tune
(?x ,?y )
tune
%BLEU
test
(?x ,?y )
test 1
%BLEU
test 2
%BLEU
avg. test
%BLEU
time
(sec./sent.)
Q
P
D
(10, 15) 25.9
(3, 3) 21.7 22.3 22.0 1.11
(3, 5) 23.2 23.5 23.3 1.28
(5, 5) 24.9 24.7 24.8 1.41
(5, 10) 25.2 25.0 25.1 2.09
(10, 10) 25.4 25.3 25.3 3.01
(10, 15) 25.3 25.4 25.4 4.00
(15, 20) 25.5 25.5 25.5 6.15
(20, 20) 25.6 25.5 25.6 7.18
(20, 25) 25.5 25.5 25.5 7.83
(15, 20) 25.8
(3, 3) 22.2 22.8 22.5 1.11
(3, 5) 23.2 24.0 23.6 1.28
(5, 5) 25.2 25.2 25.2 1.41
(5, 10) 25.3 25.4 25.4 2.08
(10, 10) 25.2 25.5 25.4 3.01
(10, 15) 25.4 25.6 25.5 4.02
(15, 20) 25.4 25.6 25.5 6.07
(20, 20) 25.4 25.6 25.5 7.16
(20, 25) 25.4 25.6 25.5 7.96
dependency parsing can be used effectively within a tree-to-tree translation system,
enabling the use of our system for low-resource languages like Urdu and Malagasy. This
result offers promise for researchers to apply syntactic translation models to languages
for which we do not have manually annotated corpora.
There are many directions for future work. Unsupervised learning of syntax can be
improved if parallel text is available and we have a parser for one of the languages: The
parallel text can be word-aligned and the annotations can be projected across the word
alignments (Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentoswki 2001). The
projected parses can be improved by applying manually written rules (Hwa et al. 2005)
or modeling the noisy projection process (Ganchev, Gillenwater, and Taskar 2009; Smith
and Eisner 2009). If we do not have parsers for either language, grammar induction
models have been developed to exploit parallel text without using any annotations
on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also
available for grammar induction using treebanks in different languages that are not
built on parallel data (Cohen, Das, and Smith 2011).
Researchers have recently begun to target learning of parsers specifically for ap-
plications like machine translation. Hall et al. (2011) developed a framework to train
supervised parsers for use in particular applications by optimizing arbitrary evaluation
metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering
391
Computational Linguistics Volume 40, Number 2
in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised
learning of syntactic structure in parallel text to target reordering phenomena.
In addition, we may not need full monolingual syntactic parses to obtain the
benefits of syntax-based translation modeling. Indeed, the widely used hierarchical
phrase-based model of Chiang (2005) induces a synchronous grammar from parallel
text without any linguistic annotations. Zollmann and Vogel (2011) and Zollmann (2011)
showed that using a supervised POS tagger to label these synchronous rules can im-
prove performance up to the level of a model that uses a supervised full syntactic parser.
They further showed that unsupervised POS taggers could be effectively used in place
of supervised taggers. These results suggest that it may be fruitful to explore the use of
simpler annotation tools such as POS taggers, whether supervised or unsupervised, in
order to apply syntax-based translation to new language pairs.
Appendix A. Language Pairs
We consider four language pairs in this article, two for which large amounts of par-
allel data are available and two involving low-resource languages. The large-data
language pairs we consider are Chinese?English (ZH?EN) and German?English
(DE?EN). The two low-resource language pairs are Urdu?English (UR?EN) and
English?Malagasy (EN?MG).
For all language pairs, English text was parsed using TurboParser version 0.1
(Martins et al. 2010). We used a second-order model with sibling and grandparent
features that was trained to maximize conditional log-likelihood.
The following sections describe the procedures used to prepare the data for each
language pair. The line and token counts are summarized in Tables A.1?A.3.
Chinese?English. For ZH?EN, we used 303k sentence pairs from the FBIS corpus
(LDC2003E14). We segmented the Chinese data using the Stanford Chinese seg-
menter (Chang, Galley, and Manning 2008) in ?CTB? mode, giving us 7.9M Chinese
Table A.1
Statistics of data used for rule extraction and feature computation.
lines source tokens target tokens
ZH?EN 302,996 7,984,637 9,350,506
DE?EN 1,010,466 23,154,642 24,044,528
UR?EN 165,159 1,169,367 1,083,807
EN?MG 83,670 1,500,922 1,686,022
Table A.2
Statistics of data used for tuning. The numbers of target tokens are averages across four
reference translations for ZH?EN and UR?EN, rounded to the nearest token.
lines source tokens target tokens
ZH?EN 919 24,152 28,870
DE?EN 1,300 29,791 31,318
UR?EN 882 18,004 16,606
EN?MG 1,359 28,408 32,682
392
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table A.3
Test data statistics. The numbers of target tokens are averages across four reference translations
for ZH?EN and UR?EN, rounded to the nearest token.
test 1 test 2 test 3
lines source target lines source target lines source targettokens tokens tokens tokens tokens tokens
ZH?EN 878 22,708 26,877 1,082 29,956 35,227 1,664 38,787 48,169
DE?EN 2,525 62,699 65,595 N/A N/A
UR?EN 883 21,659 19,575 1,792 42,082 39,889 N/A
EN?MG 1,133 24,362 28,301 N/A N/A
tokens and 9.4M English tokens. For tuning and testing, we used MT03 (?tune?), MT02
(?test 1?), MT05 (?test 2?), and MT06 (?test 3?). The Chinese text was parsed using the
Stanford parser (Levy and Manning 2003).
German?English. We started with the Europarl corpus provided for the WMT12 shared
task. We tokenized both sides, filtered sentences with more than 50 words, and down-
cased the text. We then discarded every other sentence, beginning with the second,
leaving half of the corpus remaining. We did this to speed our experiment cycle. The
corpus still has about 850k sentence pairs. We did the same processing with the news
commentary corpus, but did not discard half of the sentences. There were about 150k
news commentary sentences, giving us a total of about 1M lines of DE?EN parallel
training data. For tuning, we used the first 1,300 sentences from the 2008 2,051-sentence
test set (?tune?). For testing, we used the 2009 test set (?test 1?). The tuning/test sets are
from the newswire domain. The German text was parsed using the factored model in
the Stanford parser (Rafferty and Manning 2008).
Urdu?English. For UR?EN, we used parallel data from the NIST MT08 evaluation.
Although there are 165,159 lines of parallel data, there are many dictionary and
otherwise short entries, so it is close to an order of magnitude smaller than ZH?EN.
We used half of the documents (882 sentences) from the MT08 test set for tuning
(?tune?). We used the remaining half for one test set (?test 1?) and MT09 as a second
test set (?test 2?). The Urdu text was parsed using an unsupervised dependency parser
as described in Section 7.1.3.
English?Malagasy. For EN?MG translation, we used data obtained from the Global
Voices weblogging community (http://globalvoicesonline.org), prepared by Victor
Chahuneau.21 We used release 12.06 along with its recommended training, development
(tuning), and test set. Like Urdu, the Malagasy text was parsed using an unsupervised
dependency parser as described in Section 7.1.3.
Appendix B. Experimental Details
Appendix A contains details about the data sets used in our experiments. Other experi-
mental details are given here.
21 The data are publicly available at http://www.ark.cs.cmu.edu/global-voices/.
393
Computational Linguistics Volume 40, Number 2
Translation Models. For phrase-based models, we used the Moses machine translation
toolkit (Koehn et al. 2007). We mostly used default settings and features, includ-
ing the default lexicalized reordering model. Word alignment was performed using
GIZA++ (Och and Ney 2003) in both directions, the grow-diag-final-and heuristic
was used to symmetrize the alignments, and a max phrase length of 7 was used for
phrase extraction. The only exception to the defaults was setting the distortion limit to
10 in all experiments.
Language Models. Language models were trained using the target side of the parallel
corpus in each case augmented with 24,760,743 lines (601,052,087 tokens) of randomly
selected sentences from the Gigaword v4 corpus (excluding the New York Times and Los
Angeles Times). The minimum count cutoff for unigrams, bigrams, and trigrams was one
and the cutoff for fourgrams and fivegrams was three. Language models were estimated
using the SRI Language Modeling toolkit (Stolcke 2002) with modified Kneser-Ney
smoothing (Chen and Goodman 1998). Language model inference was performed using
KenLM (Heafield 2011) within Moses.
For EN?MG, we estimated a 5-gram language model using only the target side
of the parallel corpus, which contained 89,107 lines with 2,031,814 tokens. We did not
use any additional Malagasy data for estimating the EN?MG language models in
order to explore a scenario in which target-language text is limited or expensive to
obtain.
Word Clustering. Brown clusters (Brown et al. 1992) were generated using code provided
by Liang (2005). For each language pair, 100 word clusters were generated for the target
language. The implementation allows the use of a token count cutoff, which causes the
algorithm to only cluster words appearing more times than the cutoff. When the clusters
are used, all words with counts below the cutoff are assigned a special ?unknown word?
cluster. So in practice, if a clustering with 100 clusters is generated, there are 101 clusters
used when the clusters are applied.
For ZH?EN, DE?EN, and UR?EN, the target side of the parallel data was used
along with 412,000 lines of randomly selected Gigaword data comprising 10,001,839
words. This data was a subset of the Gigaword data used for language modeling. The
count cutoff was 2. For EN?MG, only the target side of the parallel corpus was used.
The count cutoff was 1. In all cases, the data was tokenized and downcased prior to
cluster generation.
Acknowledgments
We thank the anonymous reviewers as well
as Dhruv Batra, Jaime Carbonell, David
Chiang, Shay Cohen, Dipanjan Das, Chris
Dyer, Jason Eisner, Alon Lavie, Andre?
Martins, Greg Shakhnarovich, David Smith,
Stephan Vogel, and Eric Xing. This research
was supported in part by the National
Science Foundation through grant
IIS-0844507, the U.S. Army Research
Laboratory and the U.S. Army Research
Office under contract/grant number
W911NF-10-1-0533, and Sandia National
Laboratories (fellowship to K. Gimpel).
References
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Computer and System
Sciences, 3(1):37?56.
Ambati, V. and A. Lavie. 2008. Improving
syntax driven translation models
by re-structuring divergent and
non-isomorphic parse tree structures. In
Proceedings of the Eighth Conference of the
Association for Machine Translation in the
Americas, pages 235?244, Waikiki, HI.
Berg-Kirkpatrick, T., A. Bouchard-Co?te?,
J. DeNero, and D. Klein. 2010. Painless
394
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
unsupervised learning with features.
In Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL), pages 582?590,
Los Angeles, CA.
Birch, A., P. Blunsom, and M. Osborne. 2009.
A quantitative analysis of reordering
phenomena. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 197?205, Athens.
Blunsom, P. and T. Cohn. 2010. Unsupervised
induction of tree substitution grammars
for dependency parsing. In Proceedings
of the 2010 Conference on Empirical
Methods in Natural Language Processing,
pages 1,204?1,213, Cambridge, MA.
Bohnet, B. 2010. Top accuracy and
fast dependency parsing is not a
contradiction. In Proceedings of the 23rd
International Conference on Computational
Linguistics (Coling 2010), pages 89?97,
Beijing.
Brown, P. F., P. V. deSouza, R. L. Mercer,
V. J. Della Pietra, and J. C. Lai. 1992.
Class-based n-gram models of natural
language. Computational Linguistics,
18(4):467?479.
Buchholz, S. and E. Marsi. 2006. CoNLL-X
shared task on multilingual dependency
parsing. In Proceedings of the Tenth
Conference on Computational Natural
Language Learning (CoNLL-X),
pages 149?164, New York City.
Carreras, X. and M. Collins. 2009.
Non-projective parsing for statistical
machine translation. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing,
pages 200?209, Singapore.
Chang, P., M. Galley, and C. Manning. 2008.
Optimizing Chinese word segmentation
for machine translation performance.
In Proceedings of the Third Workshop
on Statistical Machine Translation,
pages 224?232, Columbus, OH.
Charniak, E. and M. Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, MI.
Chen, S. and J. Goodman. 1998. An empirical
study of smoothing techniques for
language modeling. Technical report 10-98,
Harvard University.
Chen, W., J. Kazama, K. Uchimoto, and
K. Torisawa. 2012. Exploiting subtrees in
auto-parsed data to improve dependency
parsing. Computational Intelligence,
28(3):426?451.
Chiang, D. 2005. A hierarchical phrase-based
model for statistical machine translation.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL?05), pages 263?270, Ann Arbor, MI.
Chiang, D. 2007. Hierarchical phrase-based
translation. Computational Linguistics,
33(2):201?228.
Chiang, D. 2010. Learning to translate with
source and target syntax. In Proceedings
of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 1,443?1,452, Uppsala.
Christodoulopoulos, C., S. Goldwater, and
M. Steedman. 2010. Two decades of
unsupervised POS induction: How far
have we come? In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 575?584,
Cambridge, MA.
Cohen, S. B. 2011. Computational Learning of
Probabilistic Grammars in the Unsupervised
Setting. Ph.D. thesis, Carnegie Mellon
University, Pittsburgh, PA.
Cohen, S. B., D. Das, and N. A. Smith. 2011.
Unsupervised structure prediction with
non-parallel multilingual guidance.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 50?61, Edinburgh.
Cowan, B., I. Kuc?erova?, and M. Collins. 2006.
A discriminative model for tree-to-tree
translation. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 232?241,
Sydney.
Crego, J. M. and F. Yvon. 2009. Gappy
translation units under left-to-right SMT
decoding. In Proceedings of the Meeting of the
European Association for Machine Translation
(EAMT), pages 66?73, Barcelona.
Das, D. and N. A. Smith. 2009. Paraphrase
identification as probabilistic
quasi-synchronous recognition.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 468?476, Suntec.
DeNero, J. and J. Uszkoreit. 2011. Inducing
sentence structure from parallel corpora
for reordering. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 193?203,
Edinburgh.
Ding, Y. and M. Palmer. 2005. Machine
translation using probabilistic
395
Computational Linguistics Volume 40, Number 2
synchronous dependency insertion
grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 541?548,
Ann Arbor, MI.
Dorr, B. J. 1994. Machine translation
divergences: a formal description and
proposed solution. Computational
Linguistics, 20(4):597?633.
Dyer, C., A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan,
V. Eidelman, and P. Resnik. 2010. cdec:
A decoder, alignment, and learning
framework for finite-state and context-free
translation models. In Proceedings of the
ACL 2010 System Demonstrations,
pages 7?12, Uppsala.
Eisner, J. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
Conference on Computational Linguistics
(COLING-96), pages 340?345, Copenhagen.
Eisner, J. 2003. Learning non-isomorphic tree
mappings for machine translation.
In Proceedings of ACL, pages 205?208,
Sapporo.
Eisner, J., E. Goldlust, and N. A. Smith. 2005.
Compiling Comp Ling: Practical weighted
dynamic programming and the Dyna
language. In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 281?290, Vancouver.
Eisner, J. and N. A. Smith. 2005. Parsing with
soft and hard constraints on dependency
length. In Proceedings of IWPT,
pages 30?41, Vancouver.
Floyd, R. W. 1962. Algorithm 97: Shortest
path. Communications of the ACM, 5(6):345.
Fox, H. J. 2002. Phrasal cohesion and
statistical machine translation.
In Proceedings of the 2002 Conference
on Empirical Methods in Natural
Language Processing, pages 304?311,
Philadelphia, PA.
Galley, M. and C. D. Manning. 2009.
Quadratic-time dependency parsing for
machine translation. In Proceedings of the
Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 773?781, Suntec.
Galley, M. and C. D. Manning. 2010.
Accurate non-hierarchical phrase-based
translation. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 966?974, Los Angeles, CA.
Ganchev, K., J. Gillenwater, and B. Taskar.
2009. Dependency grammar induction via
bitext projection constraints. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 369?377,
Suntec.
Gao, Y., P. Koehn, and A. Birch. 2011. Soft
dependency constraints for reordering in
hierarchical phrase-based translation.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 857?868, Edinburgh.
Gertz, E. M. and S. J. Wright. 2003.
Object-oriented software for quadratic
programming. ACM Transactions on
Mathematical Software, 29(1):58?81.
Gildea, D. 2003. Loosely tree-based
alignment for machine translation.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics, pages 80?87, Sapporo.
Gimpel, K. 2012. Discriminative Feature-Rich
Modeling for Syntax-Based Machine
Translation. Ph.D. thesis, Carnegie
Mellon University, Pittsburgh, PA.
Gimpel, K. and N. A. Smith. 2008. Rich
source-side context for statistical machine
translation. In Proceedings of the Third
Workshop on Statistical Machine Translation,
pages 9?17, Columbus, OH.
Gimpel, K. and N. A. Smith. 2009a. Cube
summing, approximate inference with
non-local features, and dynamic
programming without semirings. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009),
pages 318?326, Athens.
Gimpel, K. and N. A. Smith. 2009b.
Feature-rich translation by
quasi-synchronous lattice parsing. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 219?228, Singapore.
Gimpel, K. and N. A. Smith. 2011.
Quasi-synchronous phrase dependency
grammars for machine translation. In
Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 474?485, Edinburgh.
Gimpel, K. and N. A. Smith. 2012a.
Concavity and initialization for
unsupervised dependency parsing.
In Proceedings of the 2012 Conference of the
North American Chapter of the Association
for Computational Linguistics: Human
Language Technologies, pages 577?581,
Montre?al.
396
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Gimpel, K. and N. A. Smith. 2012b.
Structured ramp loss minimization for
machine translation. In Proceedings of the
2012 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies,
pages 221?231, Montre?al.
Hall, K., R. McDonald, J. Katz-Brown, and
M. Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple
objectives. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 1,489?1,499,
Edinburgh.
Hanneman, G. and A. Lavie. 2011.
Automatic category label coarsening for
syntax-based machine translation. In
Proceedings of Fifth Workshop on Syntax,
Semantics and Structure in Statistical
Translation, pages 98?106, Portland, OR.
Heafield, K. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of
the Sixth Workshop on Statistical Machine
Translation, pages 187?197, Edinburgh.
Huang, L. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL-08: HLT,
pages 586?594, Columbus, OH.
Huang, L. and D. Chiang. 2005. Better
k-best parsing. In Proceedings of the Ninth
International Workshop on Parsing
Technology, pages 53?64, Vancouver.
Huang, L., K. Knight, and A. Joshi. 2006.
Statistical syntax-directed translation with
extended domain of locality. In Proceedings
of the Association for Machine Translation in
the Americas, pages 66?73, Cambridge, MA.
Hunter, T. and P. Resnik. 2010. Exploiting
syntactic relationships in a phrase-based
decoder: An exploration. Machine
Translation, 24(2):123?140.
Hwa, R., P. Resnik, A. Weinberg, C. Cabezas,
and O. Kolak. 2005. Bootstrapping parsers
via syntactic projection across parallel
texts. Journal of Natural Language
Engineering, 11(3):311?25.
Joachims, T., T. Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural
SVMs. Machine Learning, 77(1):27?59.
Johnson, M. 2007. Transforming projective
bilexical dependency grammars into
efficiently-parsable CFGs with unfold-fold.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 168?175, Prague.
Johnson, M., T. Griffiths, and S. Goldwater.
2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human
Language Technologies 2007: The Conference
of the North American Chapter of the
Association for Computational Linguistics;
Proceedings of the Main Conference,
pages 139?146, Rochester, NY.
Katz-Brown, J., S. Petrov, R. McDonald,
F. Och, D. Talbot, H. Ichikawa, M. Seno,
and H. Kazawa. 2011. Training a parser
for machine translation reordering.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 183?192, Edinburgh.
Klein, D. and C. D. Manning. 2002. A
generative constituent-context model
for improved grammar induction.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 128?135, Philadelphia, PA.
Klein, D. and C. D. Manning. 2003. Fast exact
inference with a factored model for natural
language parsing. In Advances in Neural
Information Processing Systems 15 (NIPS),
pages 3?10, Vancouver.
Klein, D. and C. D. Manning. 2004.
Corpus-based induction of syntactic
structure: Models of dependency and
constituency. In Proceedings of the 42nd
Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume,
pages 478?485, Barcelona.
Koehn, P. 2004. Statistical significance tests
for machine translation evaluation.
In Proceedings of EMNLP 2004,
pages 388?395, Barcelona.
Koehn, P., H. Hoang, A. Birch,
C. Callison-Burch, M. Federico,
N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague.
Koehn, P., F. J. Och, and D. Marcu. 2003.
Statistical phrase-based translation.
In Proceedings of HLT-NAACL,
pages 48?54, Edmonton.
Ku?bler, S., R. McDonald, and J. Nivre. 2009.
Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan
& Claypool.
Kuhn, J. 2004. Experiments in parallel-text
based grammar induction. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main
Volume, pages 470?477, Barcelona.
Levy, R. and C. D. Manning. 2003. Is it harder
to parse Chinese, or the Chinese treebank?
In Proceedings of the 41st Annual Meeting of
397
Computational Linguistics Volume 40, Number 2
the Association for Computational Linguistics,
pages 439?446, Sapporo.
Li, Z. and S. Khudanpur. 2008. A scalable
decoder for parsing-based machine
translation with equivalent language
model state maintenance. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax
and Structure in Statistical Translation
(SSST-2), pages 10?18, Columbus, OH.
Li, Z., T. Liu, and W. Che. 2012. Exploiting
multiple treebanks for parsing with
quasi-synchronous grammars. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics
(Volume 1: Long Papers), pages 675?684,
Jeju Island.
Liang, P. 2005. Semi-supervised learning
for natural language. Master?s thesis,
Massachusetts Institute of Technology,
Cambridge, MA.
Lin, C. and F. J. Och. 2004. Orange: A method
for evaluating automatic evaluation
metrics for machine translation. In
Proceedings of Coling 2004, pages 501?507,
Geneva.
Lin, D. 2004. A path-based transfer model for
machine translation. In Proceedings of
Coling 2004, pages 625?630, Geneva.
Liu, D. C. and J. Nocedal. 1989. On the
limited memory BFGS method for large
scale optimization. Mathematical
Programming, 45:503?528.
Liu, Y., Y. Huang, Q. Liu, and S. Lin. 2007.
Forest-to-string statistical translation rules.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 704?711, Prague.
Liu, Y., Y. Lu?, and Q. Liu. 2009. Improving
tree-to-tree translation with packed forests.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 558?566, Suntec.
Marcus, M. P., B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19:313?330.
Martins, A. F. T., N. A. Smith, and E. P. Xing.
2009. Concise integer linear programming
formulations for dependency parsing.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 342?350, Suntec.
Martins, A. F. T., N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M. A. T. Figueiredo.
2010. Turbo parsers: Dependency parsing
by approximate variational inference.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing, pages 34?44, Cambridge, MA.
Melamed, I. D. 2003. Multitext grammars
and synchronous parsers. In Proceedings of
the 2003 Conference of the North American
Chapter of the Association for Computational
Linguistics on Human Language Technology -
Volume 1, pages 79?86, Edmonton.
Mi, H. and L. Huang. 2008. Forest-based
translation rule extraction. In Proceedings
of the 2008 Conference on Empirical
Methods in Natural Language Processing,
pages 206?214, Honolulu, HI.
Mi, H., L. Huang, and Q. Liu. 2008.
Forest-based translation. In Proceedings
of ACL-08: HLT, pages 192?199,
Columbus, OH.
Naseem, T., H. Chen, R. Barzilay, and
M. Johnson. 2010. Using universal
linguistic knowledge to guide grammar
induction. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,234?1,244,
Cambridge, MA.
Nederhof, M.-J. 2003. Weighted deductive
parsing and Knuth?s algorithm.
Computational Linguistics, 29(1):135?143.
Nivre, J., J. Hall, S. Ku?bler, R. McDonald,
J. Nilsson, S. Riedel, and D. Yuret.
2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings
of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932,
Prague.
Och, F. J. 2003. Minimum error rate training
in statistical machine translation.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics, pages 160?167, Sapporo.
Och, F. J. and H. Ney. 2003. A systematic
comparison of various statistical
alignment models. Computational
Linguistics, 29(1):19?51.
Papineni, K., S. Roukos, T. Ward, and
W. J. Zhu. 2002. BLEU: A method for
automatic evaluation of machine
translation. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Park, J. H., W. B. Croft, and D. A. Smith.
2011. A quasi-synchronous dependence
model for information retrieval. In
Proceedings of the 20th ACM International
Conference on Information and Knowledge
Management, pages 17?26, Glasgow.
398
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Petrov, S. 2009. Coarse-to-Fine Natural
Language Processing. Ph.D. thesis,
University of California at Berkeley.
Quirk, C., A. Menezes, and C. Cherry.
2005. Dependency treelet translation:
Syntactically informed phrasal SMT.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 271?279,
Ann Arbor, MI.
Rafferty, A. N. and C. D. Manning. 2008.
Parsing three German treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German, pages 40?46, Columbus, OH.
Riezler, S. and J. T. Maxwell III. 2006.
Grammatical machine translation.
In Proceedings of the Human Language
Technology Conference of the NAACL,
Main Conference, pages 248?255,
New York City.
Seeker, W. and J. Kuhn. 2012. Making ellipses
explicit in dependency conversion for a
German treebank. In Proceedings of the
Eight International Conference on Language
Resources and Evaluation (LREC?12),
pages 3,132?3,139, Istanbul.
Sennrich, R., G. Schneider, M. Volk, and
M. Warin. 2009. A new hybrid dependency
parser for German. In Proceedings of the
German Society for Computational Linguistics
and Language Technology (GSCL),
pages 115?124, Potsdam.
Shen, L., J. Xu, and R. Weischedel. 2008.
A new string-to-dependency machine
translation algorithm with a target
dependency language model.
In Proceedings of ACL-08: HLT,
pages 577?585, Columbus, OH.
Shieber, S. M. and Y. Schabes. 1990.
Synchronous tree-adjoining grammars.
In Proceedings of the 13th International
Conference on Computational Linguistics,
pages 253?258, Helsinki.
Simard, M., N. Cancedda, B. Cavestro,
M. Dymetman, E. Gaussier, C. Goutte,
K. Yamada, P. Langlais, and A. Mauser. 2005.
Translating with non-contiguous phrases.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 755?762, Vancouver.
Sixtus, A. and S. Ortmanns. 1999.
High quality word graphs using
forward-backward pruning. In Proceedings
of the IEEE International Conference
Acoustics, Speech, and Signal Processing,
1999 - Volume 02, pages 593?596,
Phoenix, AZ.
Skut, W., B. Krenn, T. Brants, and
H. Uszkoreit. 1997. An annotation
scheme for free word order languages.
In Proceedings of the Fifth Conference on
Applied Natural Language Processing,
pages 88?95, Washington, DC.
Smith, D. A. and J. Eisner. 2006.
Quasi-synchronous grammars:
Alignment by soft projection of syntactic
dependencies. In Proceedings of the
Workshop on Statistical Machine Translation,
pages 23?30, New York City.
Smith, D. A. and J. Eisner. 2009. Parser
adaptation and projection with
quasi-synchronous grammar features.
In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 822?831, Singapore.
Smith, N. A. 2006. Novel Estimation Methods
for Unsupervised Discovery of Latent
Structure in Natural Language Text.
Ph.D. thesis, Johns Hopkins University,
Baltimore, MD.
Snyder, B., T. Naseem, and R. Barzilay. 2009.
Unsupervised multilingual grammar
induction. In Proceedings of the Joint
Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 73?81, Suntec.
S?gaard, A. and J. Kuhn. 2009. Empirical
lower bounds on alignment error rates
in syntax-based machine translation.
In Proceedings of the Third Workshop on
Syntax and Structure in Statistical
Translation (SSST-3) at NAACL HLT 2009,
pages 19?27, Boulder, CO.
Spitkovsky, V. I., H. Alshawi, A. X. Chang,
and D. Jurafsky. 2011. Unsupervised
dependency parsing without gold
part-of-speech tags. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 1,281?1,290, Edinburgh.
Spitkovsky, V. I., H. Alshawi, and D. Jurafsky.
2010. From baby steps to leapfrog:
How ?less is More? in unsupervised
dependency parsing. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 751?759, Los Angeles, CA.
Stolcke, A. 2002. SRILM?An extensible
language modeling toolkit. In Proceedings
of the 7th International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Su, J., Y. Liu, H. Mi, H. Zhao, Y. Lu?, and
Q. Liu. 2010. Dependency-based
399
Computational Linguistics Volume 40, Number 2
bracketing transduction grammar for
statistical machine translation. In Coling
2010: Posters, pages 1,185?1,193, Beijing.
Tesnie`re, L. 1959. E?le?ment de Syntaxe
Structurale. Klincksieck.
Toutanova, K., D. Klein, C. D. Manning, and
Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the
North American Chapter of the Association
for Computational Linguistics on Human
Language Technology - Volume 1,
pages 173?180, Edmonton.
Tromble, R., S. Kumar, F. Och, and
W. Macherey. 2008. Lattice minimum
Bayes-risk decoding for statistical machine
translation. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 620?629,
Honolulu, HI.
Tsochantaridis, I., T. Joachims, T. Hofmann,
and Y. Altun. 2005. Large margin methods
for structured and interdependent output
variables. Journal of Machine Learning
Research, 6:1453?1484.
Tu, Z., Y. Liu, Y. Hwang, Q. Liu, and S. Lin.
2010. Dependency forest for statistical
machine translation. In Proceedings
of the 23rd International Conference on
Computational Linguistics (Coling 2010),
pages 1,092?1,100, Beijing.
Ueffing, N., F. J. Och, and H. Ney. 2002.
Generation of word graphs in statistical
machine translation. In Proceedings of the
ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10,
pages 156?163, Philadelphia, PA.
Wang, M., N. A. Smith, and T. Mitamura.
2007. What is the Jeopardy model? A
quasi-synchronous grammar for QA.
In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 22?32, Prague.
Weiss, D., B. Sapp, and B. Taskar. 2010.
Sidestepping intractable inference
with structured ensemble cascades.
In Advances in Neural Information
Processing Systems (NIPS),
pages 2,415?2,423, Vancouver.
Wellington, B., S. Waxmonsky, and
I. D. Melamed. 2006. Empirical lower
bounds on the complexity of translational
equivalence. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 977?984, Sydney.
Woodsend, K., Y. Feng, and M. Lapata. 2010.
Title generation with quasi-synchronous
grammar. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 513?523,
Cambridge, MA.
Woodsend, K. and M. Lapata. 2011.
Learning to simplify sentences with
quasi-synchronous grammar and integer
programming. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 409?420,
Edinburgh.
Wu, D. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
Wu, Y., Q. Zhang, X. Huang, and L. Wu. 2009.
Phrase dependency parsing for opinion
mining. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 1,533?1,541, Singapore.
Xie, J., H. Mi, and Q. Liu. 2011. A novel
dependency-to-string model for statistical
machine translation. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 216?226, Edinburgh.
Xiong, D., Q. Liu, and S. Lin. 2007.
A dependency treelet string
correspondence model for statistical
machine translation. In Proceedings of the
Second Workshop on Statistical Machine
Translation, pages 40?47, Prague.
Yadollahpour, P., D. Batra, and
G. Shakhnarovich. 2013. Discriminative
re-ranking of diverse segmentations.
In Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 1,923?1,930, Portland, OR.
Yamada, H. and Y. Matsumoto. 2003.
Statistical dependency analysis with
support vector machines. In Proceedings
of IWPT, pages 195?206, Nancy, France.
Yamada, K. and K. Knight. 2001.
A syntax-based statistical translation
model. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 523?530, Toulouse.
Yarowsky, D. and G. Ngai. 2001. Inducing
multilingual POS taggers and NP
bracketers via robust projection across
aligned corpora. In Proceedings of the
Second Meeting of the North American
Chapter of the Association for Computational
Linguistics on Language Technologies,
pages 1?8, Pittsburgh, PA.
Yarowsky, D., G. Ngai, and R. Wicentoswki.
2001. Inducing multilingual text analysis
400
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
tools via robust projection across
aligned corpora. In Proceedings of the First
International Conference on Human
Language Technology Research, pages 1?8,
San Diego, CA.
Zhang, J., F. Zhai, and C. Zong. 2011.
Augmenting string-to-tree translation
models with fuzzy use of source-side
syntax. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language
Processing, pages 204?215, Edinburgh.
Zhang, Y. 2009. Structured Language Models
for Statistical Machine Translation. Ph.D.
thesis, Carnegie Mellon University,
Pittsburgh, PA.
Zollmann, A. 2011. Learning
Multiple-Nonterminal Synchronous
Grammars for Statistical Machine Translation.
Ph.D. thesis, Carnegie Mellon University,
Pittsburgh, PA.
Zollmann, A. and A. Venugopal. 2006.
Syntax augmented machine translation
via chart parsing. In Proceedings on the
Workshop on Statistical Machine Translation,
pages 138?141, New York City.
Zollmann, A., A. Venugopal, F. J. Och, and
J. Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and
syntax-augmented statistical MT.
In Proceedings of the 22nd International
Conference on Computational Linguistics
(Coling 2008), pages 1,145?1,152,
Manchester.
Zollmann, A. and S. Vogel. 2011.
A word-class approach to labeling
PSCFG rules for machine translation.
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies, pages 1?11,
Portland, OR.
401
402
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 293?296,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Movie Reviews and Revenues: An Experiment in Text Regression?
Mahesh Joshi Dipanjan Das Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{maheshj,dipanjan,kgimpel,nasmith}@cs.cmu.edu
Abstract
We consider the problem of predicting a
movie?s opening weekend revenue. Previous
work on this problem has used metadata about
a movie?e.g., its genre, MPAA rating, and
cast?with very limited work making use of
text about the movie. In this paper, we use
the text of film critics? reviews from several
sources to predict opening weekend revenue.
We describe a new dataset pairing movie re-
views with metadata and revenue data, and
show that review text can substitute for meta-
data, and even improve over it, for prediction.
1 Introduction
Predicting gross revenue for movies is a problem
that has been studied in economics, marketing,
statistics, and forecasting. Apart from the economic
value of such predictions, we view the forecasting
problem as an application of NLP. In this paper, we
use the text of critics? reviews to predict opening
weekend revenue. We also consider metadata for
each movie that has been shown to be successful for
similar prediction tasks in previous work.
There is a large body of prior work aimed at pre-
dicting gross revenue of movies (Simonoff and Spar-
row, 2000; Sharda and Delen, 2006; inter alia). Cer-
tain information is used in nearly all prior work on
these tasks, such as the movie?s genre, MPAA rating,
running time, release date, the number of screens on
which the movie debuted, and the presence of partic-
ular actors or actresses in the cast. Most prior text-
based work has used automatic text analysis tools,
deriving a small number of aggregate statistics. For
example, Mishne and Glance (2006) applied sen-
timent analysis techniques to pre-release and post-
release blog posts about movies and showed higher
?We appreciate reviewer feedback and technical advice
from Brendan O?Connor. This work was supported by NSF IIS-
0803482, NSF IIS-0844507, and DARPA NBCH-1080004.
correlation between actual revenue and sentiment-
based metrics, as compared to mention counts of the
movie. (They did not frame the task as a revenue
prediction problem.) Zhang and Skiena (2009) used
a news aggregation system to identify entities and
obtain domain-specific sentiment for each entity in
several domains. They used the aggregate sentiment
scores and mention counts of each movie in news
articles as predictors.
While there has been substantial prior work on
using critics? reviews, to our knowledge all of this
work has used polarity of the review or the number
of stars given to it by a critic, rather than the review
text directly (Terry et al, 2005).
Our task is related to sentiment analysis (Pang et
al., 2002) on movie reviews. The key difference is
that our goal is to predict a future real-valued quan-
tity, restricting us from using any post-release text
data such as user reviews. Further, the most im-
portant clues about revenue may have little to do
with whether the reviewer liked the movie, but rather
what the reviewer found worth mentioning. This pa-
per is more in the tradition of Ghose et al (2007) and
Kogan et al (2009), who used text regression to di-
rectly quantify review ?value? and make predictions
about future financial variables, respectively.
Our aim in using the full text is to identify partic-
ular words and phrases that predict the movie-going
tendencies of the public. We can also perform syn-
tactic and semantic analysis on the text to identify
richer constructions that are good predictors. Fur-
thermore, since we consider multiple reviews for
each movie, we can compare these features across
reviews to observe how they differ both in frequency
and predictive performance across different media
outlets and individual critics.
In this paper, we use linear regression from text
and non-text (meta) features to directly predict gross
revenue aggregated over the opening weekend, and
the same averaged per screen.
293
Domain train dev test total
Austin Chronicle 306 94 62 462
Boston Globe 461 154 116 731
LA Times 610 2 13 625
Entertainment Weekly 644 208 187 1039
New York Times 878 273 224 1375
Variety 927 297 230 1454
Village Voice 953 245 198 1396
# movies 1147 317 254 1718
Table 1: Total number of reviews from each domain for
the training, development and test sets.
2 Data
We gathered data for movies released in 2005?2009.
For these movies, we obtained metadata and a list
of hyperlinks to movie reviews by crawling Meta-
Critic (www.metacritic.com). The metadata
include the name of the movie, its production house,
the set of genres it belongs to, the scriptwriter(s),
the director(s), the country of origin, the primary
actors and actresses starring in the movie, the re-
lease date, its MPAA rating, and its running time.
From The Numbers (www.the-numbers.com),
we retrieved each movie?s production budget, open-
ing weekend gross revenue, and the number of
screens on which it played during its opening week-
end. Only movies found on both MetaCritic and The
Numbers were included.
Next we chose seven review websites that most
frequently appeared in the review lists for movies at
Metacritic, and obtained the text of the reviews by
scraping the raw HTML. The sites chosen were the
Austin Chronicle, the Boston Globe, the LA Times,
Entertainment Weekly, the New York Times, Vari-
ety, and the Village Voice. We only chose those
reviews that appeared on or before the release date
of the movie (to ensure that revenue information is
not present in the review), arriving at a set of 1718
movies with at least one review. We partitioned this
set of movies temporally into training (2005?2007),
development (2008) and test (2009) sets. Not all
movies had reviews at all sites (see Table 1).
3 Predictive Task
We consider two response variables, both in
U.S. dollars: the total revenue generated by a movie
during its release weekend, and the per screen rev-
enue during the release weekend. We evaluate these
predictions using (1) mean absolute error (MAE) in
U.S. dollars and (2) Pearson?s correlation between
the actual and predicted revenue.
We use linear regression to directly predict the
opening weekend gross earnings, denoted y, based
on features x extracted from the movie metadata
and/or the text of the reviews. That is, given an input
feature vector x ? Rp, we predict an output y? ? R
using a linear model: y? = ?0 + x>?. To learn val-
ues for the parameters ? = ??0,??, the standard
approach is to minimize the sum of squared errors
for a training set containing n pairs ?xi, yi? where
xi ? Rp and yi ? R for 1 ? i ? n:
?? = argmin
?=(?0,?)
1
2n
n?
i=1
(
yi ? (?0 + x>i ?)
)2
+?P (?)
A penalty term P (?) is included in the objective for
regularization. Classical solutions use an `2 or `1
norm, known respectively as ridge and lasso regres-
sion. Introduced recently is a mixture of the two,
called the elastic net (Zou and Hastie, 2005):
P (?) =
?p
j=1
(
1
2(1? ?)?
2
j + ?|?j |
)
where ? ? (0, 1) determines the trade-off be-
tween `1 and `2 regularization. For our experi-
ments we used the elastic net and specifically the
glmnet package which contains an implementa-
tion of an efficient coordinate ascent procedure for
training (Friedman et al, 2008).
We tune the ? and ? parameters on our develop-
ment set and select the model with the ??, ?? com-
bination that yields minimum MAE on the develop-
ment set.
4 Experiments
We compare predictors based on metadata, predic-
tors based on text, and predictors that use both kinds
of information. Results for two simple baselines of
predicting the training set mean and median are re-
ported in Table 2 (Pearson?s correlation is undefined
since the standard deviation is zero).
4.1 Metadata Features
We considered seven types of metadata features, and
evaluated their performance by adding them to our
pool of features in the following order: whether the
294
film is of U.S. origin, running time (in minutes), the
logarithm of its budget, # opening screens, genre
(e.g., Action, Comedy) and MPAA rating (e.g., G,
PG, PG-13), whether the movie opened on a holiday
weekend or in summer months, total count as well as
of presence of individual Oscar-winning actors and
directors and high-grossing actors. For the first task
of predicting the total opening weekend revenue of
a movie, the best-performing feature set in terms of
MAE turned out to be all the features. However, for
the second task of predicting the per screen revenue,
addition of the last feature subset consisting of infor-
mation related to the actors and directors hurt perfor-
mance (MAE increased). Therefore, for the second
task, the best performing set contained only the first
six types of metadata features.
4.2 Text Features
We extract three types of text features (described be-
low). We only included feature instances that oc-
curred in at least five different movies? reviews. We
stem and downcase individual word components in
all our features.
I. n-grams. We considered unigrams, bigrams, and
trigrams. A 25-word stoplist was used; bigrams
and trigrams were only filtered if all words were
stopwords.
II. Part-of-speech n-grams. As with words, we
added unigrams, bigrams, and trigrams. Tags
were obtained from the Stanford part-of-speech
tagger (Toutanova and Manning, 2000).
III. Dependency relations. We used the Stanford
parser (Klein and Manning, 2003) to parse the
critic reviews and extract syntactic dependen-
cies. The dependency relation features consist
of just the relation part of a dependency triple
?relation, head word, modifier word?.
We consider three ways to combine the collec-
tion of reviews for a given movie. The first (???)
simply concatenates all of a movie?s reviews into
a single document before extracting features. The
second (?+?) conjoins each feature with the source
site (e.g., New York Times) from whose review it was
extracted. A third version (denoted ?B?) combines
both the site-agnostic and site-specific features.
Features Site
Total Per Screen
MAE MAE
($M) r ($K) r
Predict mean 11.672 ? 6.862 ?
Predict median 10.521 ? 6.642 ?
m
et
a
Best 5.983 0.722 6.540 0.272
te
xt
I
? 8.013 0.743 6.509 0.222
+ 7.722 0.781 6.071 0.466
see Tab. 3 B 7.627 0.793 6.060 0.411
I ? II
? 8.060 0.743 6.542 0.233
+ 7.420 0.761 6.240 0.398
B 7.447 0.778 6.299 0.363
I ? III
? 8.005 0.744 6.505 0.223
+ 7.721 0.785 6.013 0.473
B 7.595 0.796 ?6.010 0.421
m
et
a
?
te
xt
I
? 5.921 0.819 6.509 0.222
+ 5.757 0.810 6.063 0.470
B 5.750 0.819 6.052 0.414
I ? II
? 5.952 0.818 6.542 0.233
+ 5.752 0.800 6.230 0.400
B 5.740 0.819 6.276 0.358
I ? III
? 5.921 0.819 6.505 0.223
+ 5.738 0.812 6.003 0.477
B 5.750 0.819 ?5.998 0.423
Table 2: Test-set performance for various models, mea-
sured using mean absolute error (MAE) and Pearson?s
correlation (r), for two prediction tasks. Within a column,
boldface shows the best result among ?text? and ?meta ?
text? settings. ?Significantly better than the meta baseline
with p < 0.01, using the Wilcoxon signed rank test.
4.3 Results
Table 2 shows our results for both prediction tasks.
For the total first-weekend revenue prediction task,
metadata features baseline result (r2 = 0.521) is
comparable to that reported by Simonoff and Spar-
row (2000) on a similar task of movie gross predic-
tion (r2 = 0.446). Features from critics? reviews
by themselves improve correlation on both predic-
tion tasks, however improvement in MAE is only
observed for the per screen revenue prediction task.
A combination of the meta and text features
achieves the best performance both in terms of MAE
and r. While the text-only models have some high
negative weight features, the combined models do
not have any negatively weighted features and only
a very few metadata features. That is, the text is able
to substitute for the other metadata features.
Among the different types of text-based features
that we tried, lexical n-grams proved to be a strong
baseline to beat. None of the ?I ? ?? feature sets are
significantly better than n-grams alone, but adding
295
the dependency relation features (set III) to the n-
grams does improve the performance enough to
make it significantly better than the metadata-only
baseline for per screen revenue prediction.
Salient Text Features: Table 3 lists some of the
highly weighted features, which we have catego-
rized manually. The features are from the text-only
model annotated in Table 2 (total, not per screen).
The feature weights can be directly interpreted as
U.S. dollars contributed to the predicted value y? by
each occurrence of the feature. Sentiment-related
features are not as prominent as might be expected,
and their overall proportion in the set of features
with non-zero weights is quite small (estimated in
preliminary trials at less than 15%). Phrases that
refer to metadata are the more highly weighted
and frequent ones. Consistent with previous re-
search, we found some positively-oriented sentiment
features to be predictive. Some other prominent
features not listed in the table correspond to spe-
cial effects (?Boston Globe: of the art?, ?and cgi?),
particular movie franchises (?shrek movies?, ?Vari-
ety: chronicle of?, ?voldemort?), hype/expectations
(?blockbuster?, ?anticipation?), film festival (?Vari-
ety: canne? with negative weight) and time of re-
lease (?summer movie?).
5 Conclusion
We conclude that text features from pre-release re-
views can substitute for and improve over a strong
metadata-based first-weekend movie revenue pre-
diction. The dataset used in this paper has been
made available for research at http://www.
ark.cs.cmu.edu/movie$-data.
References
J. Friedman, T. Hastie, and R. Tibshirani. 2008. Regular-
ized paths for generalized linear models via coordinate
descent. Technical report, Stanford University.
A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007.
Opinion mining using econometrics: A case study on
reputation systems. In Proc. of ACL.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of NAACL, pages 272?280.
Feature Weight ($M)
ra
ti
ng
pg +0.085
New York Times: adult -0.236
New York Times: rate r -0.364
se
qu
el
s this series +13.925
LA Times: the franchise +5.112
Variety: the sequel +4.224
pe
op
le Boston Globe: will smith +2.560
Variety: brittany +1.128
? producer brian +0.486
ge
nr
e
Variety: testosterone +1.945
Ent. Weekly: comedy for +1.143
Variety: a horror +0.595
documentary -0.037
independent -0.127
se
nt
im
en
t Boston Globe: best parts of +1.462
Boston Globe: smart enough +1.449
LA Times: a good thing +1.117
shame $ -0.098
bogeyman -0.689
pl
ot
Variety: torso +9.054
vehicle in +5.827
superhero $ +2.020
Table 3: Highly weighted features categorized manu-
ally. ? and $ denote sentence boundaries. ?brittany?
frequently refers to Brittany Snow and Brittany Murphy.
?? producer brian? refers to producer Brian Grazer (The
Da Vinci Code, among others).
G. Mishne and N. Glance. 2006. Predicting movie sales
from blogger sentiment. In AAAI Spring Symposium
on Computational Approaches to Analysing Weblogs.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79?86.
R. Sharda and D. Delen. 2006. Predicting box office suc-
cess of motion pictures with neural networks. Expert
Systems with Applications, 30(2):243?254.
J. S. Simonoff and I. R. Sparrow. 2000. Predicting movie
grosses: Winners and losers, blockbusters and sleep-
ers. Chance, 13(3):15?24.
N. Terry, M. Butler, and D. De?Armond. 2005. The de-
terminants of domestic box office performance in the
motion picture industry. Southwestern Economic Re-
view, 32:137?148.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63?70.
W. Zhang and S. Skiena. 2009. Improving movie gross
prediction through news analysis. In Web Intelligence,
pages 301?304.
H. Zou and T. Hastie. 2005. Regularization and variable
selection via the elastic net. Journal Of The Royal Sta-
tistical Society Series B, 67(5):768?768.
296
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733?736,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Softmax-Margin CRFs: Training Log-Linear Models with Cost Functions
Kevin Gimpel Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We describe a method of incorporating task-
specific cost functions into standard condi-
tional log-likelihood (CLL) training of linear
structured prediction models. Recently intro-
duced in the speech recognition community,
we describe the method generally for struc-
tured models, highlight connections to CLL
and max-margin learning for structured pre-
diction (Taskar et al, 2003), and show that
the method optimizes a bound on risk. The
approach is simple, efficient, and easy to im-
plement, requiring very little change to an
existing CLL implementation. We present
experimental results comparing with several
commonly-used methods for training struc-
tured predictors for named-entity recognition.
1 Introduction
Conditional random fields (CRFs; Lafferty et al
2001) and other conditional log-linear models
(Berger et al, 1996) achieve strong performance
for many NLP problems, but the conditional log-
likelihood (CLL) criterion optimized when training
these models cannot take a task-specific cost func-
tion into account.
In this paper, we describe a simple approach
for training conditional log-linear models with cost
functions. We show how the method relates to other
methods and how it provides a bound on risk. We
apply the method to train a discriminative model
for named-entity recognition, showing a statistically
significant improvement over CLL.
2 Structured Log-Linear Models
Let X denote a structured input space and, for a par-
ticular x ? X, let Y(x) denote a structured output
space for x. The size of Y(x) is often exponential
in x, which differentiates structured prediction from
multiclass classification. For named-entity recogni-
tion, for example, x might be a sentence and Y(x)
the set of all possible named-entity labelings for the
sentence. Given an x ? X and a y ? Y(x), we use a
conditional log-linear model for p?(y|x):
p?(y|x) =
exp{?>f(x, y)}
?
y??Y(x) exp{?
>f(x, y?)}
(1)
where f(x, y) is a feature vector representation of
x and y and ? is a parameter vector containing one
component for each feature.
2.1 Training Criteria
Many criteria exist for training the weights ?. We
next review three choices in detail. For the follow-
ing, we assume a training set consisting of n exam-
ples {?x(i), y(i)?}ni=1. Some criteria will make use of
a task-specific cost function that measures the extent
to which a structure y differs from the true structure
y(i), denoted by cost(y(i), y).
2.1.1 Conditional Log-Likelihood
The learning problem for maximizing conditional
log-likelihood is shown in Eq. 3 in Fig. 1 (we trans-
form it into a minimization problem for easier com-
parison). This criterion is commonly used when a
probabilistic interpretation of the model is desired.
2.1.2 Max-Margin
An alternative approach to training structured lin-
ear classifiers is based on maximum-margin Markov
networks (Taskar et al, 2003). The basic idea is
to choose weights such that the linear score of each
?x(i), y(i)? is better than ?x(i), y? for all alternatives
y ? Y(x(i)) \ {y(i)}, with a larger margin for those
y with higher cost. The ?margin rescaling? form of
this training criterion is shown in Eq. 4. Note that
the cost function is incorporated into the criterion.
2.1.3 Risk
Risk is defined as the expected value of the cost
with respect to the conditional distribution p?(y|x);
733
on training data:
?n
i=1
?
y?Y(x(i)) p?(y|x
(i))cost(y(i), y) (2)
With a log-linear model, learning then requires solv-
ing the problem shown in Eq. 5. Unlike the previous
two criteria, risk is typically non-convex.
Risk minimization first appeared in the speech
recognition community (Kaiser et al, 2000; Povey
and Woodland, 2002). In NLP, Smith and Eis-
ner (2006) minimized risk using k-best lists to de-
fine the distribution over output structures. Li and
Eisner (2009) introduced a novel semiring for min-
imizing risk using dynamic programming; Xiong et
al. (2009) minimized risk in a CRF.
2.1.4 Other Criteria
Many other criteria have been proposed to at-
tempt to tailor training conditions to match task-
specific evaluation metrics. These include the aver-
age per-label marginal likelihood for sequence label-
ing (Kakade et al, 2002), minimum error-rate train-
ing for machine translation (Och, 2003), F1 for lo-
gistic regression classifiers (Jansche, 2005), and a
wide range of possible metrics for sequence label-
ing and segmentation tasks (Suzuki et al, 2006).
3 Softmax-Margin
The softmax-margin objective is shown as Eq. 6 and
is a generalization of that used by Povey et al (2008)
and similar to that used by Sha and Saul (2006).
The simple intuition is the same as the intuition
in max-margin learning: high-cost outputs for x(i)
should be penalized more heavily. Another view
says that we replace the probabilistic score inside
the exp function of CLL with the ?cost-augmented?
score from max-margin. A third view says that we
replace the ?hard? maximum of max-margin with
the ?softmax? (log
?
exp) from CLL; hence we use
the name ?softmax-margin.? Like CLL and max-
margin, the objective is convex; a proof is provided
in Gimpel and Smith (2010).
3.1 Relation to Other Objectives
We next show how the softmax-margin criterion
(Eq. 6) bounds the risk criterion (Eq. 5). We first
define some additional notation:
E(i)[F ] =
?
y?Y(x(i)) p?(y | x
(i))F (y)
for some function F : Y(x(i)) ? R. First note that
the softmax-margin objective (Eq. 6) is equal to:
(Eq. 3) +
?n
i=1 logE(i)[exp cost(y
(i), ?)] (7)
The first term must be nonnegative. Taking each part
of the second term, and using Jensen?s inequality,
logE(i)[e
cost(y(i),?)] ? E(i)[log e
cost(y(i),?)]
= E(i)[cost(y
(i), ?)]
which is exactly Eq. 5. Softmax-margin is also an
upper bound on the CLL criterion because, assum-
ing cost is nonnegative, logE[exp cost] ? 0. Fur-
thermore, softmax-margin is a differentiable upper
bound on max-margin, because the softmax function
is a differentiable upper bound on the max function.
We note that it may also be interest-
ing to consider minimizing the function
?n
i=1 logE(i)[exp cost(y
(i), ?)], since it is an
upper bound on risk but requires less computation
for computing the gradient.1 We call this objec-
tive the Jensen risk bound and include it in our
experimental comparison below.
3.2 Implementation
Most methods for training structured models with
cost functions require the cost function to decom-
pose across the pieces of the structure in the same
way as the features, such as the standard methods
for maximizing margin and minimizing risk (Taskar
et al, 2003; Li and Eisner, 2009). If the same con-
ditions hold, softmax-margin training can be im-
plemented atop standard CRF training simply by
adding additional ?features? to encode the local
cost components, only when computing the partition
function during training.2 The weights of these ?cost
features? are not learned.
4 Experiments
We consider the problem of named-entity recog-
nition (NER) and use the English data from the
CoNLL 2003 shared task (Tjong Kim Sang and De
Meulder, 2003). The data consist of news articles
1Space does not permit a full discussion; see Gimpel and
Smith (2010) for details.
2Since cost(y(i), y(i)) = 0 by definition, these ?features?
will never fire for the numerator and can be ignored.
734
CLL: min
?
n?
i=1
??>f(x(i), y(i)) + log
?
y?Y(x(i))
exp{?>f(x(i), y)} (3)
Max-Margin: min
?
n?
i=1
??>f(x(i), y(i)) + max
y?Y(x(i))
(
?>f(x(i), y) + cost(y(i), y)
)
(4)
Risk: min
?
n?
i=1
?
y?Y(x(i))
cost(y(i), y)
exp{?>f(x(i), y)}
?
y??Y(x(i)) exp{?
>f(x(i), y?)}
(5)
Softmax-Margin: min
?
n?
i=1
??>f(x(i), y(i)) + log
?
y?Y(x(i))
exp{?>f(x(i), y) + cost(y(i), y)} (6)
Figure 1: Objective functions for training linear models. Regularization terms (e.g., C
?d
j=1 ?
2
j ) are not shown here.
annotated with four entity types: person, location,
organization, and miscellaneous. Our experiments
focus on comparing training objectives for struc-
tured sequential models for this task. For all objec-
tives, we use the same standard set of feature tem-
plates, following Kazama and Torisawa (2007) with
additional token shape like those in Collins (2002b)
and simple gazetteer features. A feature was in-
cluded if it occurred at least once in training data
(total 1,312,255 features).
The task is evaluated using the F1 score, which
is the harmonic mean of precision and recall (com-
puted at the level of entire entities). Since this metric
is computed from corpus-level precision and recall,
it is not easily decomposable into features used in
standard chain CRFs. For simplicity, we only con-
sider Hamming cost in this paper; experiments with
other cost functions more targeted to NER are pre-
sented in Gimpel and Smith (2010).
4.1 Baselines
We compared softmax-margin to several baselines:
the structured perceptron (Collins, 2002a), 1-best
MIRA with cost-augmented inference (Crammer et
al., 2006), CLL, max-margin, risk, and our Jensen
risk bound (JRB) introduced above.
We used L2 regularization, experimenting with
several coefficients for each method. For CLL,
softmax-margin, max-margin, and MIRA, we used
regularization coefficients C ? {0.01, 0.1, 1}. Risk
has not always been used with regularization, as reg-
ularization does not have as clear a probabilistic in-
terpretation with risk as it does with CLL; so, for
risk and JRB we only used C ? {0.0, 0.01}. In
addition, since these two objectives are non-convex,
we initialized with the output of the best-performing
CLL model on dev data (which was the CLL model
with C = 0.01).3 All methods except CLL and the
perceptron make use of a cost function, for which
we used Hamming cost. We experimented with dif-
ferent fixed multipliers m for the cost function, for
m ? {1, 5, 10, 20}.
The hyperparameters C and m were tuned on the
development data and the best-performing combina-
tion was used to label the test data. We also tuned
the decision to average parameters across all train-
ing iterations; this has generally been found to help
the perceptron and MIRA, but in our experiments
had mixed results for the other methods.
We ran 100 iterations through the training data for
each method. For CLL, softmax-margin, risk, and
JRB, we used stochastic gradient ascent with a fixed
step size of 0.01. For max-margin, we used stochas-
tic subgradient ascent (Ratliff et al, 2006) also with
a fixed step size of 0.01.4 For the perceptron and
MIRA, we used their built-in step size formulas.
4.2 Results
Table 1 shows our results. On test data, softmax-
margin is statistically indistinguishable from MIRA,
risk, and JRB, but performs significantly better
than CLL, max-margin, and the perceptron (p <
0.03, paired bootstrap with 10,000 samples; Koehn,
3When using initialization of all ones for risk and JRB, re-
sults were several points below the results here, and with all
zeroes, learning failed, resulting in 0.0 F-measure on dev data.
Thus, risk and JRB appear sensitive to model initialization.
4In preliminary experiments, we tried other fixed and de-
creasing step sizes for (sub)gradient ascent and found that a
fixed step of 0.01 consistently performed well across training
objectives, so we used it for all settings for simplicity.
735
Method Dev. Test (C, m, avg.?)
Perceptron 90.48 83.98 (Y)
MIRA 91.13 85.72 (0.01, 20, Y)
CLL 90.79 85.46 (0.01, N)
Max-Margin 91.17 85.28 (0.01, 1, Y)
Risk 91.14 85.59 (0.01, 10, N)
JRB 91.05 85.65 (0.01, 1, N)
Softmax-Margin 91.30 85.84 (0.01, 5, N)
Table 1: Results on development and test sets, along with
hyperparameter values chosen using development set.
2004). It may be surprising that an improvement
of 0.38 in F1 could be significant, but this indicates
that the improvements are not limited to certain cate-
gories of phenomena in a small number of sentences
but rather appear throughout the majority of the test
set. The Jensen risk bound performs comparably to
risk, and takes roughly half as long to train.
5 Discussion
The softmax-margin approach offers (1) a convex
objective, (2) the ability to incorporate task-specific
cost functions, and (3) a probabilistic interpretation
(which supports, e.g., hidden-variable learning and
computation of posteriors). In contrast, max-margin
training and MIRA do not provide (3); risk and
JRB do not provide (1); and CLL does not support
(2). Furthermore, softmax-margin training improves
over standard CLL training of CRFs, is straightfor-
ward to implement, and requires the same amount of
computation as CLL.
We have also presented the Jensen risk bound,
which is easier to implement and faster to train than
risk, yet gives comparable performance. The pri-
mary limitation of all these approaches, including
softmax-margin, is that they only support cost func-
tions that factor in the same way as the features of
the model. Future work might exploit approximate
inference for more expressive cost functions.
Acknowledgments
We thank the reviewers, John Lafferty, and Andre? Martins
for helpful comments and feedback on this work. This
research was supported by NSF grant IIS-0844507.
References
A. Berger, V. J. Della Pietra, and S. A. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
M. Collins. 2002a. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. of EMNLP.
M. Collins. 2002b. Ranking algorithms for named-entity ex-
traction: Boosting and the voted perceptron. In Proc. of
ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
Journal of Machine Learning Research, 7:551?585.
K. Gimpel and N. A. Smith. 2010. Softmax-margin training
for structured log-linear models. Technical report, Carnegie
Mellon University.
M. Jansche. 2005. Maximum expected F -measure training of
logistic regression models. In Proc. of HLT-EMNLP.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss function
for the overall risk criterion based discriminative training of
HMM models. In Proc. of ICSLP.
S. Kakade, Y. W. Teh, and S. Roweis. 2002. An alternate ob-
jective function for Markovian fields. In Proc. of ICML.
J. Kazama and K. Torisawa. 2007. A new perceptron algorithm
for sequence labeling with non-local features. In Proc. of
EMNLP-CoNLL.
P. Koehn. 2004. Statistical significance tests for machine trans-
lation evaluation. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
Z. Li and J. Eisner. 2009. First- and second-order expecta-
tion semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP.
F. J. Och. 2003. Minimum error rate training for statistical
machine translation. In Proc. of ACL.
D. Povey and P. C. Woodland. 2002. Minimum phone error and
I-smoothing for improved discrimative training. In Proc. of
ICASSP.
D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran,
G. Saon, and K. Visweswariah. 2008. Boosted MMI for
model and feature space discriminative training. In Proc. of
ICASSP.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006. Subgradient
methods for maximum margin structured learning. In ICML
Workshop on Learning in Structured Output Spaces.
F. Sha and L. K. Saul. 2006. Large margin hidden Markov
models for automatic speech recognition. In Proc. of NIPS.
D. A. Smith and J. Eisner. 2006. Minimum risk annealing for
training log-linear models. In Proc. of COLING-ACL.
J. Suzuki, E. McDermott, and H. Isozaki. 2006. Training con-
ditional random fields with multivariate evaluation measures.
In Proc. of COLING-ACL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in NIPS 16.
E. F. Tjong Kim Sang and F. DeMeulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent named
entity recognition. In Proc. of CoNLL.
Y. Xiong, J. Zhu, H. Huang, and H. Xu. 2009. Minimum tag
error for discriminative training of conditional random fields.
Information Sciences, 179(1-2):169?179.
736
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 221?231,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Structured Ramp Loss Minimization for Machine Translation
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
This paper seeks to close the gap between
training algorithms used in statistical machine
translation and machine learning, specifically
the framework of empirical risk minimization.
We review well-known algorithms, arguing
that they do not optimize the loss functions
they are assumed to optimize when applied to
machine translation. Instead, most have im-
plicit connections to particular forms of ramp
loss. We propose to minimize ramp loss di-
rectly and present a training algorithm that is
easy to implement and that performs compa-
rably to others. Most notably, our structured
ramp loss minimization algorithm, RAMPION,
is less sensitive to initialization and random
seeds than standard approaches.
1 Introduction
Every statistical MT system relies on a training al-
gorithm to fit the parameters of a scoring function to
examples from parallel text. Well-known examples
include MERT (Och, 2003), MIRA (Chiang et al,
2008), and PRO (Hopkins and May, 2011). While
such procedures can be analyzed as machine learn-
ing algorithms?e.g., in the general framework of
empirical risk minimization (Vapnik, 1998)?their
procedural specifications have made this difficult.
From a practical perspective, such algorithms are of-
ten complex, difficult to replicate, and sensitive to
initialization, random seeds, and other hyperparam-
eters.
In this paper, we consider training algorithms that
are first specified declaratively, as loss functions to
be minimized. We relate well-known training algo-
rithms for MT to particular loss functions. We show
that a family of structured ramp loss functions (Do
et al, 2008) is useful for this analysis. For example,
McAllester and Keshet (2011) recently suggested
that, while Chiang et al (2008, 2009) described their
algorithm as ?MIRA? (Crammer et al, 2006), in fact
it targets a kind of ramp loss. We note here other ex-
amples: Liang et al (2006) described their algorithm
as a variant of the perceptron (Collins, 2002), which
has a unique loss function, but the loss actually opti-
mized is closer to a particular ramp loss (that differs
from the one targeted by Chiang et al). Och and
Ney (2002) sought to optimize log loss (likelihood
in a probabilistic model; Lafferty et al, 2001) but
actually optimized a version of the soft ramp loss.
Why isn?t the application of ML to MT more
straightforward? We note two key reasons: (i) ML
generally assumes that the correct output can always
be scored by a model, but in MT the reference trans-
lation is often unreachable, due to a model?s limited
expressive power or search error, requiring the use
of ?surrogate? references; (ii) MT models nearly al-
ways include latent derivation variables, leading to
non-convex losses that have generally received little
attention in ML. In this paper, we discuss how these
two have caused a disconnect between the loss func-
tion minimized by an algorithm in ML and the loss
minimized when it is adapted for MT.
From a practical perspective, our framework leads
to a simple training algorithm for structured ramp
loss based on general optimization techniques. Our
algorithm is simple to implement and, being a batch
algorithm like MERT and PRO, can easily be inte-
221
grated with any decoder. Our experiments show that
our algorithm, which we call RAMPION, performs
comparably to MERT and PRO, is less sensitive to
randomization and initialization conditions, and is
robust in large-feature scenarios.
2 Notation and Background
Let X denote the set of all strings in a source lan-
guage and, for a particular x ? X, let Y(x) denote
the set of its possible translations (correct and incor-
rect) in the target language. In typical models for
machine translation, a hidden variable is assumed
to be constructed during the translation process.1
Regardless of its specific form, we will refer to it as
a derivation and denote it h ? H(x), where H(x)
is the set of possible values of h for the input x.
Derivations will always be coupled with translations
and therefore we define the set T(x) ? Y(x)?H(x)
of valid output pairs ?y,h? for x.
To model translation, we use a linear model pa-
rameterized by a parameter vector ? ? ?. Given a
vector f(x,y,h) of feature functions on x, y, and
h, and assuming ? contains a component for each
feature function, output pairs ?y,h? for a given in-
put x are selected using a simple argmax decision
rule: ?y?,h?? = argmax
?y,h??T(x)
?>f(x,y,h)
? ?? ?
score(x,y,h;?)
.
The training problem for machine translation cor-
responds to choosing ?. There are many ways to do
this, and we will describe each in terms of a partic-
ular loss function loss : XN ? YN ?? ? R that
maps an input corpus, its reference translations, and
the model parameters to a real value indicating the
quality of the parameters. Risk minimization cor-
responds to choosing
argmin??? Ep(X,Y ) [loss (X,Y ,?)] (1)
where p(X,Y ) is the (unknown) true joint distri-
bution over corpora. We note that the loss function
depends on the entire corpus, while the decoder op-
erates independently on one sentence at a time. This
is done to fit the standard assumptions in MT sys-
tems: the evaluation metric (e.g., BLEU) depends on
1For phrase-based MT, a segmentation of the source
and target sentences into phrases and an alignment between
them (Koehn et al, 2003). For hierarchical phrase-based MT, a
derivation under a synchronous CFG (Chiang, 2005).
the entire corpus and does not decompose linearly,
while the model score does. Since in practice we do
not know p(X,Y ), but we do have access to an ac-
tual corpus pair ?X?, Y? ?, where X? = {x(i)}Ni=1 and
Y? = {y(i)}Ni=1, we instead consider regularized
empirical risk minimization:
argmin??? loss(X?, Y? ,?) +R(?) (2)
where R(?) is the regularization function used to
mitigate overfitting. The regularization function is
frequently a squared norm of the parameter vector,
such as the `1 or `2 norm, but many other choices
are possible. In this paper, we use `2.
Models are evaluated using a task-specific notion
of error, here encoded as a cost function, cost :
YN ? YN ? R?0, such that the worse a translation
is, the higher its cost. The cost function will typi-
cally make use of an automatic evaluation metric for
machine translation; e.g., cost might be 1 minus the
BLEU score (Papineni et al, 2001).2
We note that our analysis in this paper is appli-
cable for understanding the loss function being op-
timized given a fixed set of k-best lists.3 However,
most training procedures periodically invoke the de-
coder to generate new k-best lists, which are then
typically merged with those from previous training
iterations. It is an open question how this practice
affects the loss function being optimized by the pro-
cedure as a whole.
Example 1: MERT. The most commonly-used
training algorithm for machine translation is mini-
mum error rate training, which seeks to directly
minimize the cost of the predictions on the training
data. This idea has been used in the pattern recogni-
tion and speech recognition communities (Duda and
Hart, 1973; Juang et al, 1997); its first application
to MT was by Och (2003). The loss function takes
the following form: losscost
(
X?, Y? ,?
)
=
cost
?
?Y? ,
{
argmax
?y,h??T(x(i))
score(x(i),y,h;?)
}N
i=1
?
?
(3)
2We will abuse notation and allow cost to operate on both
sets of sentences as well as individual sentences. For nota-
tional convenience we also let cost accept hidden variables but
assume that the hidden variables do not affect the value; i.e.,
cost(?y,h?, ?y?,h??) = cost(y, ?y?,h??) = cost(y,y?).
3Cherry and Foster (2012) have concurrently performed a
similar analysis.
222
MERT directly minimizes the corpus-level cost
function of the best outputs from the decoder with-
out any regularization (i.e., R(?) = 0).4 The loss is
non-convex and not differentiable for cost functions
like BLEU, so Och (2003) developed a coordinate
ascent procedure with a specialized line search.
MERT avoids the need to compute feature vec-
tors for the references (?1(i)) and allows corpus-
level metrics like BLEU to be easily incorporated.
However, the complexity of the loss and the diffi-
culty of the search lead to instabilities during learn-
ing. Remedies have been suggested, typically in-
volving additional search directions and experiment
replicates (Cer et al, 2008; Moore and Quirk, 2008;
Foster and Kuhn, 2009; Clark et al, 2011). But de-
spite these improvements, MERT is ineffectual for
training weights for large numbers of features; in
addition to anecdotal evidence from the MT com-
munity, Hopkins and May (2011) illustrated with
synthetic data experiments that MERT struggles in-
creasingly to find the optimal solution as the number
of parameters grows.
Example 2: Probabilistic Models. By exponenti-
ating and normalizing score(x,y,h;?), we obtain
a conditional log-linear model, which is useful for
training criteria with probabilistic interpretations:
p?(y,h|x) = 1Z(x,?) exp{score(x,y,h;?)} (4)
The log loss then defines losslog(X?, Y? ,?) =
?
?N
i=1 log p?(y
(i) | x(i)).
Example 3: Bayes Risk. The term ?risk? as used
above should not be confused with the Bayes risk
framework, which uses a probability distribution
(Eq. 4) and a cost function to define a loss:
lossB risk =
?N
i=1 Ep?(y,h|x(i))[cost(y
(i),y)] (5)
The use of this loss is often simply called ?risk
minimization? in the speech and MT communities.
Bayes risk is non-convex, whether or not latent vari-
ables are present. Like MERT, it naturally avoids
the need to compute features for y(i) and uses a
cost function, making it appealing for MT. Bayes
risk minimization first appeared in the speech recog-
nition community (Kaiser et al, 2000; Povey and
4However, Cer et al (2008) and Macherey et al (2008)
achieved a sort of regularization by altering MERT?s line search.
Woodland, 2002) and more recently has been ap-
plied to MT (Smith and Eisner, 2006; Zens et al,
2007; Li and Eisner, 2009).
3 Training Methods for MT
In this section we consider other ML-inspired ap-
proaches to MT training, situating each in the frame-
work from ?2: ramp, perceptron, hinge, and ?soft?
losses. Each of the first three kinds of losses can be
understood as a way of selecting, for each x(i), two
candidate translation/derivation pairs: ?y?,h?? and
?y?,h??. During training, the loss function can be
improved by increasing the score of the former and
decreasing the score of the latter, through manipu-
lation of the parameters ?. Figure 1 gives a general
visualization of some of the key output pairs that are
considered for these roles. Learning alters the score
function, or, in the figure, moves points horizontally
so that scores approximate negated costs.
3.1 Structured Ramp Loss Minimization
The structured ramp loss (Do et al, 2008) is a
non-convex loss function with certain attractive the-
oretical properties. It is an upper bound on losscost
(Eq. 3) and is a tighter bound than other loss func-
tions (Collobert et al, 2006). Ramp loss has been
shown to be statistically consistent in the sense
that, in the limit of infinite training data, mini-
mizing structured ramp loss reaches the minimum
value of losscost that is achievable with a linear
model (McAllester and Keshet, 2011). This is true
whether or not latent variables are present.
Consistency in this sense is not a common prop-
erty of loss functions; commonly-used convex loss
functions such as the perceptron, hinge, and log
losses (discussed below) are not consistent, because
they are all sensitive to outliers or otherwise noisy
training examples. Ramp loss is better at dealing
with outliers in the training data (Collobert et al,
2006).
There are three forms of latent structured ramp
loss: Eq. 6?8 (Fig. 2). Ramp losses are appealing for
MT because they do not require computing the fea-
ture vector of y(i) (?1(i)). The first form, Eq. 6, sets
?y?,h?? to be the current model prediction (?y?, h??
in Fig. 1) and ?y?,h?? to be an output that is both
favored by the model and has high cost. Such an
223
yy
^
y*
^?*^y*
score
- cost
score
- c
os
t
- cost
score
- cost y*
y
score
- cost y*
y^
score
- cost y*
y^
cost diminished
?+
y-
?y?,h?? = argmin
?y,h??T (x(i))
cost(y(i),y)
?y,h? = argmax
?y,h??T (x(i))
score(x(i),y,h;?)? cost(y(i),y)
?y?,h?? = argmax
?y,h??T (x(i))
score(x(i),y,h;?) + cost(y(i),y) score
- c
os
t
?y?, h?? = argmax
?y,h??T (x(i))
score(x(i),y,h;?)
score
- c
os
t ?y
?,h?? = argmin
?y,h??T (x(i))
cost(y(i),y)
?y,h? = argmax
Ty,h???(x(i))
score(x(i),y,h;?)? cost(y(i),y)
?y?,h?? = argmax
?y,h??T (x(i))
score(x(i),y,h;?) + cost(y(i),y)
?=y, =h? ; argmat
?y,h??T (x(i))
score(x(i),y,h+?)
score
- c
os
t
argmin
?y,h??K(x(i))
cost(y(i),y)
argmin
?y,h??K(x(i))
cost(y(i),y)
Figure 1: Hypothetical output space of a translation model for an input sentence x(i). Each point corresponds to a
single translation/derivation output pair. Horizontal ?bands? are caused by output pairs with the same translation (and
hence the same cost) but different derivations. The left plot shows the entire output space and the right plot highlights
outputs in the k-best list. Choosing the output with the lowest cost in the k-best list is similar to finding ?y+,h+?.
output is shown as ?y?,h?? in Fig. 1; finding y?
is often called cost-augmented decoding, which is
also used to define hinge loss (?3.3).
The second form, Eq. 7, penalizes the model
prediction (?y?,h?? = ?y?, h??) and favors an out-
put pair that has both high model score and low
cost; this is the converse of cost-augmented decod-
ing and therefore we call it cost-diminished decod-
ing; ?y?,h?? = ?y+,h+? in Fig. 1. The third form,
Eq. 8, sets ?y?,h?? = ?y+,h+? and ?y?,h?? =
?y?,h??. This loss underlies RAMPION. It is sim-
ilar to the loss optimized by the MIRA-inspired al-
gorithm used by Chiang et al (2008, 2009).
Optimization The ramp losses are continuous but
non-convex and non-differentiable, so gradient-
based optimization methods are not available.5 For-
tunately, Eq. 8 can be optimized by using a concave-
convex procedure (CCCP; Yuille and Rangarajan,
2002). CCCP is a batch optimization algorithm for
any function that is the the sum of a concave and a
convex function. The idea is to approximate the sum
as the convex term plus a tangent line to the con-
cave function at the current parameter values; the
resulting sum is convex and can be optimized with
(sub)gradient methods.
5For non-differentiable, continuous, convex functions, sub-
gradient-based methods are available, such as stochastic sub-
gradient descent (SSD), and it is tempting to apply them here.
However, non-convex functions are not everywhere subdiffer-
entiable and so a straightforward application of SSD may en-
counter problems in practice.
With our loss functions, CCCP first imputes the
outputs in the concave terms in each loss (i.e., solves
the negated max expressions) for the entire training
set and then uses an optimization procedure to op-
timize the loss with the imputed values fixed. Any
convex optimization procedure can be used once the
negated max terms are solved; we use stochastic
subgradient descent (SSD) but MIRA could be eas-
ily used instead.
The CCCP algorithm we use for optimizing
lossramp 3, which we call RAMPION, is shown as
Alg. 1. Similar algorithms can easily be derived for
the other ramp losses. The first step done on each
iteration is to generate k-best lists for the full tun-
ing set (line 3). We then run CCCP on the k-best
lists for T ? iterations (lines 4?15). This involves first
finding the translation to update towards for all sen-
tences in the tuning set (lines 5?7), then making pa-
rameter updates in an online fashion with T ?? epochs
of stochastic subgradient descent (lines 8?14). The
subgradient update for the `2 regularization term is
done in line 11 and then for the loss in line 12.6
Unlike prior work that targeted similar loss func-
tions (Watanabe et al, 2007; Chiang et al, 2008;
Chiang et al, 2009), we do not use a fully online al-
gorithm such as MIRA in an outer loop because we
are not aware of an online learning algorithm with
theoretical guarantees for non-differentiable, non-
convex loss functions like the ramp losses. CCCP
6`2 regularization done here regularizes toward ?0, not 0.
224
lossramp 1 =
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)) + max
?y,h??Ti
(scorei(y,h;?) + costi(y)) (6)
lossramp 2 =
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)? costi(y)) + max
?y,h??Ti
(scorei(y,h;?)) (7)
lossramp 3 =
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)? costi(y)) + max
?y,h??Ti
(scorei(y,h;?) + costi(y)) (8)
lossperc =
N?
i=1
? max
h:?y(i),h??Ti
scorei(y(i),h;?) + max
?y,h??Ti
scorei(y,h;?) (9)
lossperc kbest =
n?
i=1
?score
(
x(i), argmin
?y,h??Ki
(costi(y)) ;?
)
+ max
?y,h??Ti
scorei(y,h;?) (10)
?
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)? ?icosti(y)) + max
?y,h??Ti
scorei(y,h;?) (11)
Figure 2: Formulae mentioned in text for latent-variable loss functions. Each loss is actually a function loss(X?, Y? ,?);
we suppress the arguments for clarity. ?Ti? is shorthand for ?T(x(i)).? ?Ki? is shorthand for the k-best list for x(i).
?costi(?)? is shorthand for ?cost(y(i), ?).? ?scorei(?)? is shorthand for ?score(x(i), ?).? As noted in ?3.4, any operator
of the form maxs?S can be replaced by log
?
s?S exp, known as softmax, giving many additional loss functions.
is fundamentally a batch optimization algorithm and
has been used for solving many non-convex learn-
ing problems, such as latent structured SVMs (Yu
and Joachims, 2009).
3.2 Structured Perceptron
The stuctured perceptron algorithm (Collins, 2002)
was considered by Liang et al (2006) as an alterna-
tive to MERT. It requires only a decoder and comes
with some attractive guarantees, at least for mod-
els without latent variables. Liang et al modified
the perceptron in several ways for use in MT. The
first was to generalize it to handle latent variables.
The second change relates to the need to compute
the feature vector for the reference translation y(i),
which may be unreachable (?1(i)). To address this,
researchers have proposed the use of surrogates that
are both favored by the current model parameters
and similar to the reference. Och and Ney (2002)
were the first to do so, using the translation on a
k-best list with the highest evaluation metric score
as y?. This practice was followed by Liang et al
(2006) and others with success (Arun and Koehn,
2007; Watanabe et al, 2007).7
Perceptron Loss Though typically described and
7Liang et al (2006) also tried a variant that updated directly
to the reference when it is reachable (?bold updating?), but they
and others found that Och and Ney?s strategy worked better.
analyzed procedurally, it is straightforward to show
that Collins? perceptron (without latent variables)
equates to SSD with fixed step size 1 on loss:
N?
i=1
?score(x(i),y(i);?)+ max
y?Y(x(i))
score(x(i),y;?)
(12)
This loss is convex but ignores cost functions.
In our notation, y? = y(i) and y? =
argmaxy?Y(x(i)) score(x
(i),y;?).
Adaptation for MT We chart the transformations
from Eq. 12 toward the loss Liang et al?s algorithm
actually optimized. First, generalize to latent vari-
ables; see Eq. 9 (Fig. 2), sacrificing convexity. Sec-
ond, to cope with unreachable references, use a k-
best surrogate as shown in Eq. 10 (Fig. 2), where
Ki ? T(x(i))k is a set containing the k best out-
put pairs for x(i). Now the loss only depends on
y(i) through the cost function. (Even without hid-
den variables, this loss can only be convex when the
k-best list is fixed, keeping y? unchanged across it-
erations. Updating the k-best lists makes y? depend
on ?, resulting in a non-convex loss.)
It appears that Eq. 10 (Fig. 2) is the loss that
Liang et al (2006) sought to optimize, using SSD. In
light of footnote 5 and the non-convexity of Eq. 10
(Fig. 2), we have no theoretical guarantee that such
an algorithm will find a (local) optimum.
225
Input: inputs {x(i)}Ni=1, references {y
(i)}Ni=1, init.
weights ?0, k-best list size k, step size ?, `2
reg. coeff. C, # iters T , # CCCP iters T ?, #
SSD iters T ??
Output: learned weights: ?
? ? ?0;1
for iter ? 1 to T do2
{Ki}Ni=1 ? Decode({x
(i)}Ni=1,?, k);3
for iter ? ? 1 to T ? do4
for i? 1 to N do5
?y+i ,h
+
i ? ?6
argmax?y,h??Ki scorei(y,h;?)? costi(y);
end7
for iter ?? ? 1 to T ?? do8
for i? 1 to N do9
?y?,h?? ?10
argmax?y,h??Ki scorei(y,h;?) + costi(y);
? ?= ?C
(
???0
N
)
;11
? += ?
(
f(x(i),y+i ,h
+
i )? f(x
(i),y?,h?)
)
;12
end13
end14
end15
end16
return ?;17
Algorithm 1: RAMPION.
We note that Eq. 10 is similar to Eq. 11 (Fig. 2),
where each ? is used to trade off between model and
cost. Fig. 1 illustrates the similarity by showing that
the min-cost output on a k-best list resides in a simi-
lar region of the output space as ?y+,h+? computed
from the full output space. While it is not the case
that we can always choose ?i so as to make the two
losses equivalent, they are similar in that they up-
date towards some y? with high model score and
low cost. Eq. 11 corresponds to Eq. 7 (Fig. 2), the
second form of the latent structured ramp loss.
Thus, one way to understand Liang et al?s algo-
rithm is as a form of structured ramp loss. However,
another interpretation is given by McAllester et al
(2010), who showed that procedures like that used
by Liang et al approach direct cost minimization in
the limiting case.
3.3 Large-Margin Methods
A related family of approaches for training MT mod-
els involves the margin-infused relaxed algorithm
(MIRA; Crammer et al, 2006), an online large-
margin training algorithm. It has recently shown
success for MT, particularly when training models
with large feature sets (Watanabe et al, 2007; Chi-
ang et al, 2008; Chiang et al, 2009). In order to
apply it to MT, Watanabe et al and Chiang et al
made modifications similar to those made by Liang
et al for perceptron training, namely the extension
to latent variables and the use of a surrogate refer-
ence with high model score and low cost.
Hinge Loss It can be shown that 1-best MIRA corre-
sponds to dual coordinate ascent for the structured
hinge loss when using `2 regularization (Martins et
al., 2010). The structured hinge is the loss underly-
ing maximum-margin Markov networks (Taskar et
al., 2003): setting y? = y(i) and:
y? = argmax
y?Y(x(i))
(
score(x(i),y;?) + cost(y(i),y)
)
(13)
Unlike the perceptron losses, which penalize the
highest-scoring outputs, hinge loss penalizes an out-
put that is both favored by the model and has high
cost. Such an output is shown as ?y?,h?? in Fig. 1;
the structured hinge loss focuses on pushing such
outputs to the left. As mentioned in ?3.1, finding y?
is often called cost-augmented decoding.
Structured hinge loss is convex, can incorporate
a cost function, and can be optimized with several
algorithms, including SSD (Ratliff et al, 2006).
Adaptation for MT While prior work has used
MIRA-like algorithms for training machine transla-
tion systems, the proposed algorithms did not actu-
ally optimize the structured hinge loss, for similar
reasons to those mentioned above for the perceptron:
latent variables and surrogate references. Incorpo-
rating latent variables in the hinge loss results in
the latent structured hinge loss (Yu and Joachims,
2009). Like the latent perceptron, this loss is non-
convex and inappropriate for MT because it requires
computing the feature vector for y(i). By using a
surrogate instead of y(i), the actual loss optimized
becomes closer to Eq. 8 (Fig. 2), the third form of
the latent structured ramp loss.
Watanabe et al (2007) and Arun and Koehn
(2007) used k-best oracles like Liang et al, but Chi-
ang et al (2008, 2009) used a different approach, ex-
plicitly defining the surrogate as ?y+,h+? in Fig. 1.
While the method of Chiang et al showed impres-
226
sive performance improvements, its implementation
is non-trivial, involving a complex cost function and
a parallel architecture, and it has not yet been em-
braced by the MT community. Indeed, the com-
plexity of Chiang et als algorithm was one of the
reasons cited for the development of PRO (Hopkins
and May, 2011). In this paper, we have sought to
isolate the loss functions used in prior work like that
by Chiang et al and identify simple, generic opti-
mization procedures for optimizing them. We offer
RAMPION as an alternative to Chiang et als MIRA
that is simpler to implement and achieves empirical
success in experiments (?4).
3.4 Likelihood and Softened Losses
We can derive new loss functions from the above
by converting any ?max? operator to a ?softmax?
(log
?
exp, where the set of elements under the
summation is the same as under the max). For exam-
ple, the softmax version of the perceptron loss is the
well-known log loss (?2, Ex. 2), the loss underlying
the conditional likelihood training criterion which
is frequently used when a probabilistic interpreta-
tion of the learned model is desired, as in conditional
random fields (Lafferty et al, 2001).
Och and Ney (2002) popularized the use of log-
linear models for MT and initially sought to opti-
mize log loss, but by using the min-cost transla-
tion on a k-best list as their surrogate, we argue that
their loss was closer to the soft ramp loss obtained
by softening the second max in lossramp 2 in Eq. 7
(Fig. 2). The same is true for others who aimed to
optimize log loss for MT (Smith and Eisner, 2006;
Zens et al, 2007; Cer, 2011).
The softmax version of the latent variable percep-
tron loss, Eq. 9 (Fig. 2), is the latent log loss inher-
ent in latent-variable CRFs (Quattoni et al, 2004).
Blunsom et al (2008) and Blunsom and Osborne
(2008) actually did optimize latent log loss for MT,
discarding training examples for which y(i) was un-
reachable by the model.
Finally, we note that ?softening? the ramp loss
in Eq. 6 (Fig. 2) results in the Jensen risk
bound from Gimpel and Smith (2010), which is
a computationally-attractive upper bound on the
Bayes risk.
4 Experiments
The goal of our experiments is to compare RAM-
PION (Alg. 1) to state-of-the-art methods for train-
ing MT systems. RAMPION minimizes lossramp 3,
which we found in preliminary experiments to work
better than other loss functions tested.8
System and Datasets We use the Moses phrase-
based MT system (Koehn et al, 2007) and consider
Urdu?English (UR?EN), Chinese?English
(ZH?EN) translation, and Arabic?English
(AR?EN) translation.9 We trained a Moses system
using default settings and features, except for
setting the distortion limit to 10. Word alignment
was performed using GIZA++ (Och and Ney, 2003)
in both directions, the grow-diag-final-and
heuristic was used to symmetrize the alignments,
and a max phrase length of 7 was used for phrase
extraction. We estimated 5-gram language models
using the SRI toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing (Chen and Goodman, 1998).
For each language pair, we used the English side
of the parallel text and 600M words of randomly-
selected sentences from the Gigaword v4 corpus
(excluding NYT and LAT).
For UR?EN, we used parallel data from the
NIST MT08 evaluation consisting of 1.2M Urdu
words and 1.1M English words. We used half of
the documents (882 sentences) from the MT08 test
set for tuning. We used the remaining half for
one test set (?MT08??) and MT09 as our other test
set. For ZH?EN, we used 303k sentence pairs
from the FBIS corpus (LDC2003E14). We seg-
mented the Chinese data using the Stanford Chi-
nese segmenter (Chang et al, 2008) in ?CTB? mode,
giving us 7.9M Chinese words and 9.4M English
words. We used MT03 for tuning and used MT02
and MT05 for testing.
For AR?EN, we used data provided by the LDC
8We only present full results using lossramp 3. We found
that minimizing lossramp 1 did poorly, resulting in single-digit
BLEU scores, and that lossramp 2 reached high BLEU scores on
the tuning data but failed to generalize well. Softened versions
of the ramp losses performed comparably to lossramp 3 but were
slightly worse on both tuning and held-out data.
9We found similar trends for other language pairs and sys-
tems, including Hiero (Chiang, 2005). A forthcoming report
will present these results, as well as experiments with additional
loss functions, in detail.
227
for the NIST evaluations, including 3.29M sentence
pairs of UN data and 982k sentence pairs of non-
UN data. The Arabic data was preprocessed using
an HMM segmenter that splits off attached prepo-
sitional phrases, personal pronouns, and the future
marker (Lee et al, 2003). The common stylistic
sentence-initial wa# (and ...) was removed from the
training and test data. The resulting corpus con-
tained 130M Arabic tokens and 130M English to-
kens. We used MT06 for tuning and three test sets:
MT05, the MT08 newswire test set (?MT08 NW?),
and the MT08 weblog test set (?MT08 WB?).
For all languages we evaluated translation output
using case-insensitive IBM BLEU (Papineni et al,
2001).
Training Algorithms Our baselines are MERT and
PRO as implemented in the Moses toolkit.10 PRO
uses the hyperparameter settings from Hopkins and
May (2011), including k-best lists of size 1500 and
25 training iterations.11 MERT uses k-best lists of
size 100 and was run to convergence. For both
MERT and PRO, previous iterations? k-best lists
were merged in.
For RAMPION, we used T = 20, T ? = 10,
T ?? = 5, k = 500, ? = 0.0001, and C = 1.
Our cost function is ?(1 ? BLEU+1(y,y?)) where
BLEU+1(y,y?) returns the BLEU+1 score (Lin and
Och, 2004) for reference y and hypothesis y?. We
used ? = 10. We used these same hyperparameter
values for all experiments reported here and found
them to perform well across other language pairs
and systems.12
4.1 Results
Table 1 shows our results. MERT and PRO were run
3 times with differing random seeds and averages
10The PRO algorithm samples pairs of translations from k-
best lists on each iteration and trains a binary classifier to rank
pairs according to the cost function. The loss function under-
lying PRO depends on the choice of binary classifier and also
on the sampling strategy. We leave an analysis of PRO?s loss
function to future work.
11Hopkins and May used 30 iterations, but showed that train-
ing had converged by 25.
12We found performance to be better when using a smaller
value of T ?; we suspect that using small T ? guards against over-
fitting to any particular set of k-best lists. We also found the
value of ? to affect performance, although ? ? {1, 5, 10} all
worked well. Performance was generally insensitive to C. We
fixed ? = 0.0001 early on and did little tuning to it.
35 36
35
36
M
T
02
 B
L
E
U
Tune BLEU
35 36
34
35
M
T
05
 B
L
E
U
Tune BLEU
MERT
PRO
Rampion
Figure 3: ZH?EN training runs. The cluster of PRO
points to the left corresponds to one of the random initial
models; MERT and RAMPION were able to recover while
PRO was not.
and standard deviations are shown. The three al-
gorithms perform very similarly on the whole, with
certain algorithms performing better on certain lan-
guages. MERT shows larger variation across ran-
dom seeds, as reported by many others in the com-
munity. On average across all language pairs and
test sets, RAMPION leads to slightly higher BLEU
scores.
4.2 Sensitivity Analysis
We now measure the sensitivity of these training
methods to different initializers and to randomness
in the algorithms. RAMPION is deterministic, but
MERT uses random starting points and search di-
rections and PRO uses random sampling to choose
pairs for training its binary classifier.
For initial models, we used the default parame-
ters in Moses as well as two randomly-generated
models.13 We ran RAMPION once with each of the
three initial models, and MERT and PRO three times
with each. This allows us to compare variance due
to initializers as well as due to the nondeterminism
in each algorithm. Fig. 3 plots the results. While
PRO exhibits a small variance for a given initializer,
as also reported by Hopkins and May (2011), it had
13The default weights are 0.3 for reordering features, 0.2 for
phrase table features, 0.5 for the language model, and -1 for the
word penalty. We generated each random model by sampling
each feature weight from aN(?, ?2) with ? equal to the default
weight for that feature and ? = |?/2|.
228
Method
UR?EN ZH?EN AR?EN
avg
MT08? MT09 MT02 MT05 MT05 MT08 NW MT08 WB
MERT 24.5 (0.1) 24.6 (0.0) 35.7 (0.3) 34.2 (0.2) 55.0 (0.7) 49.8 (0.3) 32.6 (0.2) 36.6
PRO 24.2 (0.1) 24.2 (0.1) 36.3 (0.1) 34.5 (0.0) 55.6 (0.1) 49.6 (0.0) 31.7 (0.0) 36.6
RAMPION 24.5 24.6 36.4 34.7 55.5 49.8 32.1 36.8
Table 1: %BLEU on several test sets for UR?EN, ZH?EN, and AR?EN translation. Algorithms with randomization
(MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by
standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).
The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT
and PRO and 7 for RAMPION.
Method
UR?EN ZH?EN
Tune MT08? MT09 Tune MT02 MT05
PRO 29.4 22.3 23.0 40.9 35.7 33.6
RAMPION 27.8 24.2 24.6 38.8 36.2 34.3
Table 2: %BLEU with large feature sets.
trouble recovering from one of the random initializ-
ers. Therefore, while the within-initializer variance
for PRO tended to be smaller than that of MERT,
PRO?s overall range was larger. RAMPION found
very similar weights regardless of ?0.
4.3 Adding Features
Finally, we compare RAMPION and PRO with an ex-
tended feature set; MERT is excluded as it fails in
such settings (Hopkins and May, 2011).
We added count features for common monolin-
gual and bilingual lexical patterns from the parallel
corpus: the 1k most common bilingual word pairs
from phrase extraction, 200 top unigrams, 1k top bi-
grams, 1k top trigrams, and 4k top trigger pairs ex-
tracted with the method of Rosenfeld (1996), ranked
by mutual information. We integrated the features
with our training procedure by using Moses to gen-
erate lattices instead of k-best lists. We used cube
pruning (Chiang, 2007) to incorporate the additional
(potentially non-local) features while extracting k-
best lists from the lattices to pass to the training al-
gorithms.14
Results are shown in Table 2. We find that PRO
finds much higher BLEU scores on the tuning data
but fails to generalize, leading to poor performance
on the held-out test sets. We suspect that incorporat-
ing regularization into training the binary classifier
within PRO may mitigate this overfitting. RAMPION
is more stable by contrast. This is a challenging
learning task, as lexical features are prone to over-
14In cube pruning, each node?s local n-best list had n = 100.
fitting with a small tuning set. Hopkins and May
(2011) similarly found little gain on test data when
using extended feature sets in phrase-based transla-
tion for these two language pairs.
Results for AR?EN translation were similar and
are omitted for space; these and additional experi-
ments will be included in a forthcoming report.
5 Conclusion
We have framed MT training as empirical risk min-
imization and clarified loss functions that were op-
timized by well-known procedures. We have pro-
posed directly optimizing the structured ramp loss
implicit in prior work with a novel algorithm?
RAMPION?which performs comparably to state-
of-the-art training algorithms and is empirically
more stable. Our source code, which integrates
easily with Moses, is available at www.ark.cs.
cmu.edu/MT.
Acknowledgments
We thank Colin Cherry, Chris Dyer, Joseph Keshet,
David McAllester, and members of the ARK research
group for helpful comments that improved this paper.
This research was supported in part by the NSF through
CAREER grant IIS-1054319, the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533, and Sandia
National Laboratories (fellowship to K. Gimpel).
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL.
229
D. Cer, D. Jurafsky, and C. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of ACL-2008 Workshop on Statistical Machine
Translation.
D. Cer. 2011. Parameterizing Phrase Based Statisti-
cal Machine Translation Models: An Analytic Study.
Ph.D. thesis, Stanford University.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of ACL-2008 Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proc. of NAACL.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang, W. Wang, and K. Knight. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL-HLT.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011.
Better hypothesis testing for statistical machine trans-
lation: Controlling for optimizer instability. In Proc.
of ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
R. Collobert, F. Sinz, J. Weston, and L. Bottou. 2006.
Trading convexity for scalability. In ICML.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola.
2008. Tighter bounds for structured estimation. In
Proc. of NIPS.
R. O. Duda and P. E. Hart. 1973. Pattern classification
and scene analysis. John Wiley, New York.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. In Proc. of Fourth Workshop on Statisti-
cal Machine Translation.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In Proc. of NAACL.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
B. H. Juang, W. Chou, and C. H. Lee. 1997. Minimum
classification error rate methods for speech recogni-
tion. Speech and Audio Processing, IEEE Transac-
tions on, 5(3):257?265, may.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss
function for the overall risk criterion based discrimina-
tive training of hmm models. In Proc. of ICSLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.
2003. Language model based Arabic word segmenta-
tion. In Proc. of ACL.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. of EMNLP.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proc. of Coling.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In EMNLP.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M A. T. Figueiredo. 2010. Learn-
ing structured classifiers with dual coordinate descent.
Technical report, Carnegie Mellon University.
D. McAllester and J. Keshet. 2011. Generalization
bounds and consistency for latent structural probit and
ramp loss. In Proc. of NIPS.
D. McAllester, T. Hazan, and J. Keshet. 2010. Direct
loss minimization for structured prediction. In Proc.
of NIPS.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of Coling.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
230
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
D. Povey and P. C. Woodland. 2002. Minimum phone
error and I-smoothing for improved discrimative train-
ing. In Proc. of ICASSP.
A. Quattoni, M. Collins, and T. Darrell. 2004. Condi-
tional random fields for object recognition. In NIPS
17.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006.
Subgradient methods for maximum margin structured
learning. In ICML Workshop on Learning in Struc-
tured Output Spaces.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10(3).
D. A. Smith and J. Eisner. 2006. Minimum risk an-
nealing for training log-linear models. In Proc. of
COLING-ACL.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in NIPS 16.
V. Vapnik. 1998. Statistical learning theory. Wiley.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP-CoNLL.
C. J. Yu and T. Joachims. 2009. Learning structural
SVMs with latent variables. In Proc. of ICML.
A. L. Yuille and Anand Rangarajan. 2002. The concave-
convex procedure (CCCP). In Proc. of NIPS. MIT
Press.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP.
231
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 577?581,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Concavity and Initialization for Unsupervised Dependency Parsing
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We investigate models for unsupervised learn-
ing with concave log-likelihood functions. We
begin with the most well-known example,
IBM Model 1 for word alignment (Brown
et al, 1993) and analyze its properties, dis-
cussing why other models for unsupervised
learning are so seldom concave. We then
present concave models for dependency gram-
mar induction and validate them experimen-
tally. We find our concave models to be effec-
tive initializers for the dependency model of
Klein and Manning (2004) and show that we
can encode linguistic knowledge in them for
improved performance.
1 Introduction
In NLP, unsupervised learning typically implies op-
timization of a ?bumpy? objective function riddled
with local maxima. However, one exception is IBM
Model 1 (Brown et al, 1993) for word alignment,
which is the only model commonly used for unsu-
pervised learning in NLP that has a concave log-
likelihood function.1 For other models, such as
those used in unsupervised part-of-speech tagging
and grammar induction, and indeed for more sophis-
ticated word alignment models, the log-likelihood
function maximized by EM is non-concave. As a
result, researchers are obligated to consider initial-
ization in addition to model design (Klein and Man-
ning, 2004; Goldberg et al, 2008).
For example, consider the dependency grammar
induction results shown in Table 1 when training the
1It is not strictly concave (Toutanova and Galley, 2011).
widely used dependency model with valence (DMV;
Klein and Manning, 2004). Using uniform distri-
butions for initialization (UNIF) results in an accu-
racy of 17.6% on the test set, well below the base-
line of attaching each word to its right neighbor
(ATTACHRIGHT, 31.7%). Furthermore, when using
a set of 50 random initializers (RAND), the standard
deviation of the accuracy is an alarming 8.3%.
In light of this sensitivity to initialization, it is
compelling to consider unsupervised models with
concave log-likelihood functions, which may pro-
vide stable, data-supported initializers for more
complex models. In this paper, we explore the issues
involved with such an expedition and elucidate the
limitations of such models for unsupervised NLP.
We then present simple concave models for depen-
dency grammar induction that are easy to implement
and offer efficient optimization. We also show how
linguistic knowledge can be encoded without sacri-
ficing concavity. Using our models to initialize the
DMV, we find that they lead to an improvement in
average accuracy across 18 languages.
2 IBM Model 1 and Concavity
IBM Model 1 is a conditional model of a target-
language sentence e of length m and an alignment
a given a source-language sentence f of length l.
The generation of m is assumed to occur with some
(inconsequential) uniform probability . The align-
ment vector a, a hidden variable, has an entry for
each element of e that contains the index in f of
the aligned word. These entries are used to define
which translation parameters t(ej | faj ) are active.
Model 1 assumes that the probability of the ith ele-
577
ment in a, denoted a(i | j, l,m), is simply a uni-
form distribution over all l source words plus the
null word. These assumptions result in the follow-
ing log-likelihood for a sentence pair ?f , e? under
Model 1 (marginalizing a):
log p(e | f) = log (l+1)m+
?m
j=1 log
?l
i=0 t(ej | fi)
(1)
The only parameters to be learned in the model are
t = {t(e | f)}e,f . Since a parameter is concave in
itself, the sum of concave functions is concave, and
the log of a concave function is concave, Eq. 1 is
concave in t (Brown et al, 1993).
IBM Model 2 involves a slight change to Model
1 in which the probability of a word link depends
on the word positions. However, this change renders
it no longer concave. Consider the log-likelihood
function for Model 2:
log +
?m
j=1 log
?l
i=0 t(ej | fi) ?a(i | j, l,m) (2)
Eq. 2 is not concave in the parameters t(ej | fi) and
a(i | j, l,m) because a product is neither convex nor
concave in its vector of operands. This can be shown
by computing the Hessian matrix of f(x, y) = xy
and showing that it is indefinite.
In general, concavity is lost when the log-
likelihood function contains a product of model pa-
rameters enclosed within a log
?
. If the sum is not
present, the log can be used to separate the prod-
uct of parameters, making the function concave. It
can also be shown that a ?featurized? version (Berg-
Kirkpatrick et al, 2010) of Model 1 is not con-
cave. More generally, any non-concave function en-
closed within log
?
will cause the log-likelihood
function to be non-concave, though there are few
other non-concave functions with a probabilistic se-
mantics than those just discussed.
3 Concave, Unsupervised Models
Nearly every other model used for unsupervised
learning in NLP has a non-concave log-likelihood
function. We now proceed to describe the conditions
necessary to develop concave models for two tasks.
3.1 Part-of-Speech Tagging
Consider a standard first-order hidden Markov
model for POS tagging. Letting y denote the tag
sequence for a sentence e with m tokens, the single-
example log-likelihood is:
log
?
y p(stop | ym)
?m
j=1 p(yj | yj?1) ? p(ej | yj)
(3)
where y0 is a designated ?start? symbol. Unlike IBM
Models 1 and 2, we cannot reverse the order of the
summation and product here because the transition
parameters p(yj | yj?1) cause each tag decision to
affect its neighbors. Therefore, Eq. 3 is non-concave
due to the presence of a product within a log
?
.
However, if the tag transition probabilities p(yj |
yj?1) are all constants and also do not depend on
the previous tag yj?1, then we can rewrite Eq. 3 as
the following concave log-likelihood function (using
C(y) to denote a constant function of tag y, e.g., a
fixed tag prior distribution):
logC(stop) + log
?m
j=1
?
yj
C(yj) ? p(ej | yj)
Lacking any transition modeling power, this model
appears weak for POS tagging. However, we note
that we can add additional conditioning information
to the p(ej | yj) distributions and retain concavity,
such as nearby words and tag dictionary informa-
tion. We speculate that such a model might learn
useful patterns about local contexts and provide an
initializer for unsupervised part-of-speech tagging.
3.2 Dependency Grammar Induction
To develop dependency grammar induction models,
we begin with a version of Model 1 in which a sen-
tence e is generated from a copy of itself (denoted
e?): log p(e | e?)
= log (m+1)m +
?m
j=1 log
?m
i=0,i 6=j c(ej | e
?
i) (4)
If a word ej is ?aligned? to e?0, ej is a root. This
is a simple child-generation model with no tree con-
straint. In order to preserve concavity, we are forbid-
den from conditioning on other parent-child assign-
ments or including any sort of larger constraints.
However, we can condition the child distributions
on additional information about e? since it is fully
observed. This conditioning information may in-
clude the direction of the edge, its distance, and
any properties about the words in the sentence. We
found that conditioning on direction improved per-
formance: we rewrite the c distributions as c(ej |
e?i, sign(j ? i)) and denote this model by CCV1.
578
We note that we can also include constraints in the
sum over possible parents and still preserve concav-
ity. Naseem et al (2010) found that adding parent-
child constraints to a grammar induction system can
improve performance dramatically. We employ one
simple rule: roots are likely to be verbs.2 We mod-
ify CCV1 to restrict the summation over parents to
exclude e?0 if the child word is not a verb.
3 We only
employ this restriction during EM learning for sen-
tences containing at least one verb. For sentences
without verbs, we allow all words to be the root. We
denote this model by CCV2.
In related work, Brody (2010) also developed
grammar induction models based on the IBM word
alignment models. However, while our goal is to
develop concave models, Brody employed Bayesian
nonparametrics in his version of Model 1, which
makes the model non-concave.
4 Experiments
We ran experiments to determine how well our con-
cave grammar induction models CCV1 and CCV2 can
perform on their own and when used as initializers
for the DMV (Klein and Manning, 2004). The DMV
is a generative model of POS tag sequences and pro-
jective dependency trees over them. It is the foun-
dation of most state-of-the-art unsupervised gram-
mar induction models (several of which are listed in
Tab. 1). The model includes multinomial distribu-
tions for generating each POS tag given its parent
and the direction of generation: where ei is the par-
ent POS tag and ej the child tag, these distributions
take the form c(ej | ei, sign(j ? i)), analogous to
the distributions used in our concave models. The
DMV also has multinomial distributions for decid-
ing whether to stop or continue generating children
in each direction considering whether any children
have already been generated in that direction.
The majority of researchers use the original ini-
tializer from Klein and Manning (2004), denoted
here K&M. K&M is a deterministic harmonic initial-
izer that sets parent-child token affinities inversely
2This is similar to the rule used by Marec?ek and Z?abokrtsky?
(2011) with empirical success.
3As verbs, we take all tags that map to V in the universal tag
mappings from Petrov et al (2012). Thus, to apply this con-
straint to a new language, one would have to produce a similar
tag mapping or identify verb tags through manual inspection.
Train ? 10 Train ? 20
Test Test
Model Init. ?10 ?? ?10 ??
ATTRIGHT N/A 38.4 31.7 38.4 31.7
CCV1 UNIF 31.4 25.6 31.0 23.7
CCV2 UNIF 43.1 28.6 43.9 27.1
UNIF 21.3 17.6 21.3 16.4
RAND? 41.0 31.8 - -
DMV K&M 44.1 32.9 51.9 37.8
CCV1 45.3 30.9 53.9 36.7
CCV2 54.3 43.0 64.3 53.1
Shared LN K&M 61.3 41.4
L-EVG RAND? 68.8 -
Feature DMV K&M 63.0 -
LexTSG-DMV K&M 67.7 55.7
Posterior Reg. K&M 64.3 53.3
Punc/UTags K&M? - 59.1?
Table 1: English attachment accuracies on Section 23, for
short sentences (?10 words) and all (??). We include
selected results on this same test set: Shared LN = Cohen
and Smith (2009), L-EVG = Headden III et al (2009),
Feature DMV = Berg-Kirkpatrick et al (2010), LexTSG-
DMV = Blunsom and Cohn (2010), Posterior Reg. =
Gillenwater et al (2010), Punc/UTags = Spitkovsky et
al. (2011a). K&M? is from Spitkovsky et al (2011b).
?Accuracies are averages over 50 random initializers;
? = 10.9 for test sentences ? 10 and 8.3 for all. ?Used
many random initializers with unsupervised run selec-
tion. ?Used staged training with sentences ? 45 words.
proportional to their distances, then normalizes to
obtain probability distributions. K&M is often de-
scribed as corresponding to an initial E step for an
unspecified model that favors short attachments.
Procedure We run EM for our concave models for
100 iterations. We evaluate the learned models di-
rectly as parsers on the test data and also use them
to initialize the DMV. When using them directly as
parsers, we use dynamic programming to ensure that
a valid tree is recovered. When using the concave
models as initializers for the DMV, we copy the c
parameters over directly since they appear in both
models. We do not have the stop/continue parame-
ters in our concave models, so we simply initialize
them uniformly for the DMV. We train each DMV
for 200 iterations and use minimum Bayes risk de-
coding with the final model on the test data. We use
several initializers for training the DMV, including
the uniform initializer (UNIF), K&M, and our trained
concave models CCV1 and CCV2.
579
Init. eu bg ca zh cs da nl en de el hu
UNIF 24/21 32/26 27/29 44/40 32/30 24/19 21/21 21/18 31/24 37/32 23/18
K&M 32/26 48/40 24/25 38/33 31/29 34/23 39/33 44/33 47/37 50/41 23/20
CCV1 22/21 34/27 44/51 46/45 33/31 19/14 24/24 45/31 46/31 51/45 32/28
CCV2 26/25 34/26 29/35 46/44 50/40 29/18 50/43 54/43 49/33 50/45 60/46
it ja pt sl es sv tr avg. accuracy avg. log-likelihood
UNIF 31/24 35/30 49/36 20/20 29/24 26/22 33/30 29.8 / 25.7 -15.05
K&M 32/24 39/31 44/28 33/27 19/11 46/33 39/36 36.7 / 29.4 -14.84
CCV1 34/25 42/27 50/38 30/25 41/33 45/33 37/29 37.5 / 30.9 -14.93
CCV2 55/48 49/31 50/38 22/21 57/50 46/32 31/22 43.7 / 35.5 -14.45
Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences ? 10
words and second is for all sentences. For training, sentences ? 10 words from each treebank were used. In order,
languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian,
Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish.
Data We use data prepared for the CoNLL
2006/07 shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007).4 We follow standard practice
in removing punctuation and using short sentences
(? 10 or ? 20 words) for training. For all experi-
ments, we train on separate data from that used for
testing and use gold POS tags for both training and
testing. We report accuracy on (i) test set sentences
?10 words and (ii) all sentences from the test set.
Results Results for English are shown in Tab. 1.
We train on ?2?21 and test on ?23 in the Penn Tree-
bank. The constraint on sentence roots helps a great
deal, as CCV2 by itself is competitive with the DMV
when testing on short sentences. The true benefit of
the concave models, however, appears when using
them as initializers. The DMV initialized with CCV2
achieves a substantial improvement over all others.
When training on sentences of length ? 20 words
(bold), the performance even rivals that of several
more sophisticated models shown in the table, de-
spite only using the DMV with a different initializer.
Tab. 2 shows results for 18 languages. On av-
erage, CCV2 performs best and CCV1 does at least
as well as K&M. This shows that a simple, concave
model can be as effective as a state-of-the-art hand-
designed initializer (K&M), and that concave mod-
els can encode linguistic knowledge to further im-
prove performance.
4In some cases, we did not use official CoNLL test sets but
instead took the training data and reserved the first 80% of the
sentences for training, the next 10% for development, and the
final 10% as our test set; dataset details are omitted for space
but are the same as those given by Cohen (2011).
Average log-likelihoods (micro-averaged across
sentences) achieved by EM training are shown in the
final column of Tab. 2. CCV2 leads to substantially-
higher likelihoods than the other initializers, sug-
gesting that the verb-root constraint is helping EM
to find better local optima.5
5 Discussion
Staged training has been shown to help unsupervised
learning in the past, from early work in grammar in-
duction (Lari and Young, 1990) and word alignment
(Brown et al, 1993) to more recent work in depen-
dency grammar induction (Spitkovsky et al, 2010).
While we do not yet offer a generic procedure for
extracting a concave approximation from any model
for unsupervised learning, our results contribute evi-
dence in favor of the general methodology of staged
training in unsupervised learning, and provide a sim-
ple and powerful initialization method for depen-
dency grammar induction.
Acknowledgments
We thank Shay Cohen, Dipanjan Das, Val Spitkovsky,
and members of the ARK research group for helpful com-
ments that improved this paper. This research was sup-
ported in part by the NSF through grant IIS-0915187, the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-
1-0533, and Sandia National Laboratories (fellowship to
K. Gimpel).
5However, while CCV1 leads to a higher average accuracy
than K&M, the latter reaches slightly higher likelihood, sug-
gesting that the success of the concave initializers is only par-
tially due to reaching high training likelihood.
580
References
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In Proc. of EMNLP.
S. Brody. 2010. It depends on the translation: Unsu-
pervised dependency parsing via word alignment. In
Proc. of EMNLP.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
S. Cohen and N. A. Smith. 2009. Shared logistic normal
distributions for soft parameter tying in unsupervised
grammar induction. In Proc. of NAACL.
S. Cohen. 2011. Computational Learning of Probabilis-
tic Grammars in the Unsupervised Setting. Ph.D. the-
sis, Carnegie Mellon University.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, , and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Journal of Machine Learning
Research.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proc. of ACL.
W. Headden III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35?56.
D. Marec?ek and Z. Z?abokrtsky?. 2011. Gibbs sampling
with treeness constraint in unsupervised dependency
parsing. In Proc. of Workshop on Robust Unsuper-
vised and Semisupervised Methods in Natural Lan-
guage Processing.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proc. of EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
CoNLL.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From Baby Steps to Leapfrog: How ?Less is More?
in unsupervised dependency parsing. In Proc. of
NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, A. X. Chang, and D. Juraf-
sky. 2011a. Unsupervised dependency parsing with-
out gold part-of-speech tags. In Proc. of EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b.
Punctuation: Making a point in unsupervised depen-
dency parsing. In Proc. of CoNLL.
K. Toutanova and M. Galley. 2011. Why initialization
matters for IBM Model 1: Multiple optima and non-
strict convexity. In Proc. of ACL.
581
Proceedings of NAACL-HLT 2013, pages 380?390,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Part-of-Speech Tagging for Online Conversational Text
with Word Clusters
Olutobi Owoputi? Brendan O?Connor? Chris Dyer?
Kevin Gimpel? Nathan Schneider? Noah A. Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
Corresponding author: brenocon@cs.cmu.edu
Abstract
We consider the problem of part-of-speech
tagging for informal, online conversational
text. We systematically evaluate the use of
large-scale unsupervised word clustering
and new lexical features to improve tagging
accuracy. With these features, our system
achieves state-of-the-art tagging results on
both Twitter and IRC POS tagging tasks;
Twitter tagging is improved from 90% to 93%
accuracy (more than 3% absolute). Quali-
tative analysis of these word clusters yields
insights about NLP and linguistic phenomena
in this genre. Additionally, we contribute the
first POS annotation guidelines for such text
and release a new dataset of English language
tweets annotated using these guidelines.
Tagging software, annotation guidelines, and
large-scale word clusters are available at:
http://www.ark.cs.cmu.edu/TweetNLP
This paper describes release 0.3 of the ?CMU
Twitter Part-of-Speech Tagger? and annotated
data.
1 Introduction
Online conversational text, typified by microblogs,
chat, and text messages,1 is a challenge for natu-
ral language processing. Unlike the highly edited
genres that conventional NLP tools have been de-
veloped for, conversational text contains many non-
standard lexical items and syntactic patterns. These
are the result of unintentional errors, dialectal varia-
tion, conversational ellipsis, topic diversity, and cre-
ative use of language and orthography (Eisenstein,
2013). An example is shown in Fig. 1. As a re-
sult of this widespread variation, standard model-
ing assumptions that depend on lexical, syntactic,
and orthographic regularity are inappropriate. There
1Also referred to as computer-mediated communication.
ikr
!
smh
G
he
O
asked
V
fir
P
yo
D
last
A
name
N
so
P
he
O
can
V
add
V
u
O
on
P
fb
?
lololol
!
Figure 1: Automatically tagged tweet showing nonstan-
dard orthography, capitalization, and abbreviation. Ignor-
ing the interjections and abbreviations, it glosses as He
asked for your last name so he can add you on Facebook.
The tagset is defined in Appendix A. Refer to Fig. 2 for
word clusters corresponding to some of these words.
is preliminary work on social media part-of-speech
(POS) tagging (Gimpel et al, 2011), named entity
recognition (Ritter et al, 2011; Liu et al, 2011), and
parsing (Foster et al, 2011), but accuracy rates are
still significantly lower than traditional well-edited
genres like newswire. Even web text parsing, which
is a comparatively easier genre than social media,
lags behind newspaper text (Petrov and McDonald,
2012), as does speech transcript parsing (McClosky
et al, 2010).
To tackle the challenge of novel words and con-
structions, we create a new Twitter part-of-speech
tagger?building on previous work by Gimpel et
al. (2011)?that includes new large-scale distribu-
tional features. This leads to state-of-the-art results
in POS tagging for both Twitter and Internet Relay
Chat (IRC) text. We also annotated a new dataset of
tweets with POS tags, improved the annotations in
the previous dataset from Gimpel et al, and devel-
oped annotation guidelines for manual POS tagging
of tweets. We release all of these resources to the
research community:
? an open-source part-of-speech tagger for online
conversational text (?2);
? unsupervised Twitter word clusters (?3);
? an improved emoticon detector for conversational
text (?4);
380
? POS annotation guidelines (?5.1); and
? a new dataset of 547 manually POS-annotated
tweets (?5).
2 MEMM Tagger
Our tagging model is a first-order maximum en-
tropy Markov model (MEMM), a discriminative se-
quence model for which training and decoding are
extremely efficient (Ratnaparkhi, 1996; McCallum
et al, 2000).2 The probability of a tag yt is condi-
tioned on the input sequence x and the tag to its left
yt?1, and is parameterized by a multiclass logistic
regression:
p(yt = k | yt?1,x, t;?) ?
exp
(
?(trans)yt?1,k +
?
j ?
(obs)
j,k fj(x, t)
)
We use transition features for every pair of labels,
and extract base observation features from token t
and neighboring tokens, and conjoin them against
all K = 25 possible outputs in our coarse tagset
(Appendix A). Our feature sets will be discussed
below in detail.
Decoding. For experiments reported in this paper,
we use the O(|x|K2) Viterbi algorithm for predic-
tion; K is the number of tags. This exactly max-
imizes p(y | x), but the MEMM also naturally al-
lows a fasterO(|x|K) left-to-right greedy decoding:
for t = 1 . . . |x|:
y?t ? argmaxk p(yt = k | y?t?1,x, t;?)
which we find is 3 times faster and yields similar ac-
curacy as Viterbi (an insignificant accuracy decrease
of less than 0.1% absolute on the DAILY547 test set
discussed below). Speed is paramount for social me-
dia analysis applications?which often require the
processing of millions to billions of messages?so
we make greedy decoding the default in the released
software.
2Although when compared to CRFs, MEMMs theoretically
suffer from the ?label bias? problem (Lafferty et al, 2001), our
system substantially outperforms the CRF-based taggers of pre-
vious work; and when comparing to Gimpel et al system with
similar feature sets, we observed little difference in accuracy.
This is consistent with conventional wisdom that the quality
of lexical features is much more important than the paramet-
ric form of the sequence model, at least in our setting: part-of-
speech tagging with a small labeled training set.
This greedy tagger runs at 800 tweets/sec. (10,000
tokens/sec.) on a single CPU core, about 40 times
faster than Gimpel et al?s system. The tokenizer by
itself (?4) runs at 3,500 tweets/sec.3
Training and regularization. During training,
the MEMM log-likelihood for a tagged tweet ?x,y?
is the sum over the observed token tags yt, each con-
ditional on the tweet being tagged and the observed
previous tag (with a start symbol before the first to-
ken in x),
`(x,y,?) =
?|x|
t=1 log p(yt | yt?1,x, t;?).
We optimize the parameters ? with OWL-QN, an
L1-capable variant of L-BFGS (Andrew and Gao,
2007; Liu and Nocedal, 1989) to minimize the regu-
larized objective
argmin
?
? 1N
?
?x,y? `(x,y,?) +R(?)
where N is the number of tokens in the corpus and
the sum ranges over all tagged tweets ?x,y? in the
training data. We use elastic net regularization (Zou
and Hastie, 2005), which is a linear combination of
L1 and L2 penalties; here j indexes over all features:
R(?) = ?1
?
j |?j |+
1
2?2
?
j ?
2
j
Using even a very small L1 penalty eliminates many
irrelevant or noisy features.
3 Unsupervised Word Clusters
Our POS tagger can make use of any number of pos-
sibly overlapping features. While we have only a
small amount of hand-labeled data for training, we
also have access to billions of tokens of unlabeled
conversational text from the web. Previous work has
shown that unlabeled text can be used to induce un-
supervised word clusters which can improve the per-
formance of many supervised NLP tasks (Koo et al,
2008; Turian et al, 2010; T?ckstr?m et al, 2012, in-
ter alia). We use a similar approach here to improve
tagging performance for online conversational text.
We also make our induced clusters publicly avail-
able in the hope that they will be useful for other
NLP tasks in this genre.
3Runtimes observed on an Intel Core i5 2.4 GHz laptop.
381
Binary path Top words (by frequency)
A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol
A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
A3 111010100100 yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
A4 111010100101 yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
A5 11101011011100 smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
B 011101011 u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo yue juu dya youz yyou
C 11100101111001 w fo fa fr fro ov fer fir whit abou aft serie fore fah fuh w/her w/that fron isn agains
D 111101011000 facebook fb itunes myspace skype ebay tumblr bbm flickr aim msn netflix pandora
E1 0011001 tryna gon finna bouta trynna boutta gne fina gonn tryina fenna qone trynaa qon
E2 0011000 gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
F 0110110111 soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
G1 11101011001010 ;) :p :-) xd ;-) ;d (; :3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-d
G2 11101011001011 :) (: =) :)) :] :?) =] ^_^ :))) ^.^ [: ;)) ((: ^__^ (= ^-^ :))))
G3 1110101100111 :( :/ -_- -.- :-( :?( d: :| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /: :(( >_< =[ :[ #fml
G4 111010110001 <3 xoxo <33 xo <333 #love s2 <URL-twitition.com> #neversaynever <3333
Figure 2: Example word clusters (HMM classes): we list the most probable words, starting with the most probable, in
descending order. Boldfaced words appear in the example tweet (Figure 1). The binary strings are root-to-leaf paths
through the binary cluster tree. For example usage, see e.g. search.twitter.com, bing.com/social and
urbandictionary.com.
3.1 Clustering Method
We obtained hierarchical word clusters via Brown
clustering (Brown et al, 1992) on a large set of
unlabeled tweets.4 The algorithm partitions words
into a base set of 1,000 clusters, and induces a hi-
erarchy among those 1,000 clusters with a series of
greedy agglomerative merges that heuristically opti-
mize the likelihood of a hidden Markov model with a
one-class-per-lexical-type constraint. Not only does
Brown clustering produce effective features for dis-
criminative models, but its variants are better unsu-
pervised POS taggers than some models developed
nearly 20 years later; see comparisons in Blunsom
and Cohn (2011). The algorithm is attractive for our
purposes since it scales to large amounts of data.
When training on tweets drawn from a single
day, we observed time-specific biases (e.g., nu-
merical dates appearing in the same cluster as the
word tonight), so we assembled our unlabeled data
from a random sample of 100,000 tweets per day
from September 10, 2008 to August 14, 2012,
and filtered out non-English tweets (about 60% of
the sample) using langid.py (Lui and Baldwin,
2012).5 Each tweet was processed with our to-
4As implemented by Liang (2005), v. 1.3: https://
github.com/percyliang/brown-cluster
5https://github.com/saffsd/langid.py
kenizer and lowercased. We normalized all at-
mentions to ?@MENTION? and URLs/email ad-
dresses to their domains (e.g. http://bit.ly/
dP8rR8 ? ?URL-bit.ly?). In an effort to reduce
spam, we removed duplicated tweet texts (this also
removes retweets) before word clustering. This
normalization and cleaning resulted in 56 million
unique tweets (847 million tokens). We set the
clustering software?s count threshold to only cluster
words appearing 40 or more times, yielding 216,856
word types, which took 42 hours to cluster on a sin-
gle CPU.
3.2 Cluster Examples
Fig. 2 shows example clusters. Some of the chal-
lenging words in the example tweet (Fig. 1) are high-
lighted. The term lololol (an extension of lol for
?laughing out loud?) is grouped with a large number
of laughter acronyms (A1: ?laughing my (fucking)
ass off,? ?cracking the fuck up?). Since expressions
of laughter are so prevalent on Twitter, the algorithm
creates another laughter cluster (A1?s sibling A2),
that tends to have onomatopoeic, non-acronym vari-
ants (e.g., haha). The acronym ikr (?I know, right??)
is grouped with expressive variations of ?yes? and
?no? (A4). Note that A1?A4 are grouped in a fairly
specific subtree; and indeed, in this message ikr and
382
lololol are both tagged as interjections.
smh (?shaking my head,? indicating disapproval)
seems related, though is always tagged in the an-
notated data as a miscellaneous abbreviation (G);
the difference between acronyms that are interjec-
tions versus other acronyms may be complicated.
Here, smh is in a related but distinct subtree from the
above expressions (A5); its usage in this example
is slightly different from its more common usage,
which it shares with the other words in its cluster:
message-ending expressions of commentary or emo-
tional reaction, sometimes as a metacomment on the
author?s message; e.g., Maybe you could get a guy
to date you if you actually respected yourself #smh
or There is really NO reason why other girls should
send my boyfriend a goodmorning text #justsaying.
We observe many variants of categories tradition-
ally considered closed-class, including pronouns (B:
u = ?you?) and prepositions (C: fir = ?for?).
There is also evidence of grammatical categories
specific to conversational genres of English; clusters
E1?E2 demonstrate variations of single-word con-
tractions for ?going to? and ?trying to,? some of
which have more complicated semantics.6
Finally, the HMM learns about orthographic vari-
ants, even though it treats all words as opaque sym-
bols; cluster F consists almost entirely of variants
of ?so,? their frequencies monotonically decreasing
in the number of vowel repetitions?a phenomenon
called ?expressive lengthening? or ?affective length-
ening? (Brody and Diakopoulos, 2011; Schnoebe-
len, 2012). This suggests a future direction to jointly
model class sequence and orthographic informa-
tion (Clark, 2003; Smith and Eisner, 2005; Blunsom
and Cohn, 2011).
We have built an HTML viewer to browse these
and numerous other interesting examples.7
3.3 Emoticons and Emoji
We use the term emoticon to mean a face or icon
constructed with traditional alphabetic or punctua-
6One coauthor, a native speaker of the Texan English dialect,
notes ?finna? (short for ?fixing to?, cluster E1) may be an im-
mediate future auxiliary, indicating an immediate future tense
that is present in many languages (though not in standard En-
glish). To illustrate: ?She finna go? approximately means ?She
will go,? but sooner, in the sense of ?She is about to go.?
7http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
tion symbols, and emoji to mean symbols rendered
in software as small pictures, in line with the text.
Since our tokenizer is careful to preserve emoti-
cons and other symbols (see ?4), they are clustered
just like other words. Similar emoticons are clus-
tered together (G1?G4), including separate clusters
of happy [[ :) =) ?_? ]], sad/disappointed [[ :/ :(
-_- </3 ]], love [[ ?xoxo ?.? ]] and winking [[
;) (?_-) ]] emoticons. The clusters are not per-
fectly aligned with our POS annotation guidelines;
for example, the ?sad? emoticon cluster included
emotion-bearing terms that our guidelines define as
non-emoticons, such as #ugh, #tear, and #fml (?fuck
my life?), though these seem potentially useful for
sentiment analysis.
One difficult task is classifying different types
of symbols in tweets: our annotation guidelines
differentiate between emoticons, punctuation, and
garbage (apparently non-meaningful symbols or to-
kenization errors). Several Unicode character ranges
are reserved for emoji-style symbols (including the
three Unicode hearts in G4); however, depending
on the user?s software, characters in these ranges
might be rendered differently or not at all. We
have found instances where the clustering algo-
rithm groups proprietary iOS emoji symbols along
with normal emoticons; for example, the character
U+E056, which is interpreted on iOS as a smiling
face, is in the same G2 cluster as smiley face emoti-
cons. The symbol U+E12F, which represents a pic-
ture of a bag of money, is grouped with the words
cash and money.
3.4 Cluster-Based Features
Since Brown clusters are hierarchical in a binary
tree, each word is associated with a tree path rep-
resented as a bitstring with length ? 16; we use pre-
fixes of the bitstring as features (for all prefix lengths
? {2, 4, 6, . . . , 16}). This allows sharing of statisti-
cal strength between similar clusters. Using prefix
features of hierarchical clusters in this way was sim-
ilarly found to be effective for named-entity recog-
nition (Turian et al, 2010) and Twitter POS tag-
ging (Ritter et al, 2011).
When checking to see if a word is associated with
a cluster, the tagger first normalizes the word using
the same techniques as described in ?3.1, then cre-
ates a priority list of fuzzy match transformations
383
of the word by removing repeated punctuation and
repeated characters. If the normalized word is not
in a cluster, the tagger considers the fuzzy matches.
Although only about 3% of the tokens in the devel-
opment set (?6) did not appear in a clustering, this
method resulted in a relative error decrease of 18%
among such word tokens.
3.5 Other Lexical Features
Besides unsupervised word clusters, there are two
other sets of features that contain generalized lexi-
cal class information. We use the tag dictionary fea-
ture from Gimpel et al, which adds a feature for
a word?s most frequent part-of-speech tag.8 This
can be viewed as a feature-based domain adaptation
method, since it gives lexical type-level information
for standard English words, which the model learns
to map between PTB tags to the desired output tags.
Second, since the lack of consistent capitalization
conventions on Twitter makes it especially difficult
to recognize names?Gimpel et al and Foster et
al. (2011) found relatively low accuracy on proper
nouns?we added a token-level name list feature,
which fires on (non-function) words from names
from several sources: Freebase lists of celebrities
and video games (Google, 2012), the Moby Words
list of US Locations,9 and lists of male, female, fam-
ily, and proper names from Mark Kantrowitz?s name
corpus.10
4 Tokenization and Emoticon Detection
Word segmentation on Twitter is challenging due
to the lack of orthographic conventions; in partic-
ular, punctuation, emoticons, URLs, and other sym-
bols may have no whitespace separation from textual
8Frequencies came from the Wall Street Journal and Brown
corpus sections of the Penn Treebank. If a word has multiple
PTB tags, each tag is a feature with value for the frequency rank;
e.g. for three different tags in the PTB, this feature gives a value
of 1 for the most frequent tag, 2/3 for the second, etc. Coarse
versions of the PTB tags are used (Petrov et al, 2011). While
88% of words in the dictionary have only one tag, using rank
information seemed to give a small but consistent gain over only
using the most common tag, or using binary features conjoined
with rank as in Gimpel et al
9http://icon.shef.ac.uk/Moby/mwords.html
10http://www.cs.cmu.edu/afs/cs/project/
ai-repository/ai/areas/nlp/corpora/names/
0.html
words (e.g. no:-d,yes should parse as four tokens),
and internally may contain alphanumeric symbols
that could be mistaken for words: a naive split(/[^a-
zA-Z0-9]+/) tokenizer thinks the words ?p? and ?d?
are among the top 100 most common words on Twit-
ter, due to misanalysis of :p and :d. Traditional Penn
Treebank?style tokenizers are hardly better, often
breaking a string of punctuation characters into a
single token per character.
We rewrote twokenize (O?Connor et al,
2010), a rule-based tokenizer, emoticon, and URL
detector, for use in the tagger. Emoticons are es-
pecially challenging, since they are open-class and
productive. We revise O?Connor et al?s regular ex-
pression grammar that describes possible emoticons,
adding a grammar of horizontal emoticons (e.g. -_-),
known as ?Eastern-style,?11 though we observe high
usage in English-speaking Twitter (Fig. 2, G2?G3).
We also add a number of other improvements to the
patterns. Because this system was used as prepro-
cessing for the word clustering experiment in ?3, we
were able to infer the emoticon clusters in Fig. 2.
Furthermore, whether a token matches the emoticon
pattern is also used as a feature in the tagger (?2).
URL recognition is also difficult, since the http://
is often dropped, resulting in protocol-less URLs
like about.me. We add recognition patterns for these
by using a list of top-level and country domains.
5 Annotated Dataset
Gimpel et al (2011) provided a dataset of POS-
tagged tweets consisting almost entirely of tweets
sampled from one particular day (October 27,
2010). We were concerned about overfitting to time-
specific phenomena; for example, a substantial frac-
tion of the messages are about a basketball game
happening that day.
We created a new test set of 547 tweets for eval-
uation. The test set consists of one random English
tweet from every day between January 1, 2011 and
June 30, 2012. In order for a tweet to be considered
English, it had to contain at least one English word
other than a URL, emoticon, or at-mention. We no-
ticed biases in the outputs of langid.py, so we
instead selected these messages completely manu-
11http://en.wikipedia.org/wiki/List_of_
emoticons
384
ally (going through a random sample of one day?s
messages until an English message was found).
5.1 Annotation Methodology
Gimpel et al provided a tagset for Twitter (shown in
Appendix A), which we used unmodified. The orig-
inal annotation guidelines were not published, but in
this work we recorded the rules governing tagging
decisions and made further revisions while annotat-
ing the new data.12 Some of our guidelines reiter-
ate or modify rules made by Penn Treebank annota-
tors, while others treat specific phenomena found on
Twitter (refer to the next section).
Our tweets were annotated by two annotators who
attempted to match the choices made in Gimpel et
al.?s dataset. The annotators also consulted the POS
annotations in the Penn Treebank (Marcus et al,
1993) as an additional reference. Differences were
reconciled by a third annotator in discussion with all
annotators.13 During this process, an inconsistency
was found in Gimpel et al?s data, which we cor-
rected (concerning the tagging of this/that, a change
to 100 labels, 0.4%). The new version of Gimpel et
al.?s data (called OCT27), as well as the newer mes-
sages (called DAILY547), are both included in our
data release.
5.2 Compounds in Penn Treebank vs. Twitter
Ritter et al (2011) annotated tweets using an aug-
mented version of the PTB tagset and presumably
followed the PTB annotation guidelines. We wrote
new guidelines because the PTB conventions are in-
appropriate for Twitter in several ways, as shown in
the design of Gimpel et al?s tagset. Importantly,
?compound? tags (e.g., nominal+verbal and nomi-
nal+possessive) are used because tokenization is dif-
ficult or seemingly impossible for the nonstandard
word forms that are commonplace in conversational
text.
For example, the PTB tokenization splits contrac-
tions containing apostrophes: I?m? I/PRP ?m/VBP.
But conversational text often contains variants that
resist a single PTB tag (like im), or even chal-
lenge traditional English grammatical categories
12The annotation guidelines are available online at
http://www.ark.cs.cmu.edu/TweetNLP/
13Annotators are coauthors of this paper.
(like imma or umma, which both mean ?I am go-
ing to?). One strategy would be to analyze these
forms into a PTB-style tokenization, as discussed in
Forsyth (2007), who proposes to analyze doncha as
do/VBP ncha/PRP, but notes it would be difficult.
We think this is impossible to handle in the rule-
based framework used by English tokenizers, given
the huge (and possibly growing) number of large
compounds like imma, gonna, w/that, etc. These
are not rare: the word clustering algorithm discov-
ers hundreds of such words as statistically coherent
classes (e.g. clusters E1 and E2 in Fig. 2); and the
word imma is the 962nd most common word in our
unlabeled corpus, more frequent than cat or near.
We do not attempt to do Twitter ?normalization?
into traditional written English (Han and Baldwin,
2011), which we view as a lossy translation task. In
fact, many of Twitter?s unique linguistic phenomena
are due not only to its informal nature, but also a set
of authors that heavily skews towards younger ages
and minorities, with heavy usage of dialects that are
different than the standard American English most
often seen in NLP datasets (Eisenstein, 2013; Eisen-
stein et al, 2011). For example, we suspect that
imma may implicate tense and aspect markers from
African-American Vernacular English.14 Trying to
impose PTB-style tokenization on Twitter is linguis-
tically inappropriate: should the lexico-syntactic be-
havior of casual conversational chatter by young mi-
norities be straightjacketed into the stylistic conven-
tions of the 1980s Wall Street Journal? Instead, we
would like to directly analyze the syntax of online
conversational text on its own terms.
Thus, we choose to leave these word forms un-
tokenized and use compound tags, viewing com-
positional multiword analysis as challenging fu-
ture work.15 We believe that our strategy is suf-
ficient for many applications, such as chunking or
named entity recognition; many applications such
as sentiment analysis (Turney, 2002; Pang and Lee,
2008, ?4.2.3), open information extraction (Carl-
son et al, 2010; Fader et al, 2011), and informa-
tion retrieval (Allan and Raghavan, 2002) use POS
14See ?Tense and aspect? examples in http:
//en.wikipedia.org/wiki/African_American_
Vernacular_English
15For example, wtf has compositional behavior in ?Wtf just
happened???, but only debatably so in ?Huh wtf?.
385
#Msg. #Tok. Tagset Dates
OCT27 1,827 26,594 App. A Oct 27-28, 2010
DAILY547 547 7,707 App. A Jan 2011?Jun 2012
NPSCHAT 10,578 44,997 PTB-like Oct?Nov 2006
(w/o sys. msg.) 7,935 37,081
RITTERTW 789 15,185 PTB-like unknown
Table 1: Annotated datasets: number of messages, to-
kens, tagset, and date range. More information in ?5,
?6.3, and ?6.2.
patterns that seem quite compatible with our ap-
proach. More complex downstream processing like
parsing is an interesting challenge, since contraction
parsing on traditional text is probably a benefit to
current parsers. We believe that any PTB-trained
tool requires substantial retraining and adaptation
for Twitter due to the huge genre and stylistic differ-
ences (Foster et al, 2011); thus tokenization conven-
tions are a relatively minor concern. Our simple-to-
annotate conventions make it easier to produce new
training data.
6 Experiments
We are primarily concerned with performance on
our annotated datasets described in ?5 (OCT27,
DAILY547), though for comparison to previous
work we also test on other corpora (RITTERTW in
?6.2, NPSCHAT in ?6.3). The annotated datasets
are listed in Table 1.
6.1 Main Experiments
We use OCT27 to refer to the entire dataset de-
scribed in Gimpel et al; it is split into train-
ing, development, and test portions (OCT27TRAIN,
OCT27DEV, OCT27TEST). We use DAILY547 as
an additional test set. Neither OCT27TEST nor
DAILY547 were extensively evaluated against until
final ablation testing when writing this paper.
The total number of features is 3.7 million, all
of which are used under pure L2 regularization; but
only 60,000 are selected by elastic net regularization
with (?1, ?2) = (0.25, 2), which achieves nearly the
same (but no better) accuracy as pure L2,16 and we
use it for all experiments. We observed that it was
16We conducted a grid search for the regularizer values on
part of DAILY547, and many regularizer values give the best or
nearly the best results. We suspect a different setup would have
yielded similar results.
l
l
l
l l l l
1e+03 1e+05 1e+07
75
80
85
90
Number of Unlabeled Tweets
Ta
gg
ing
 Ac
cu
rac
y
l
l
l l
l l l
1e+03 1e+05 1e+07
0.6
0
0.6
5
0.7
0
Number of Unlabeled Tweets
To
ke
n 
Co
ve
ra
ge
Figure 3: OCT27 development set accuracy using only
clusters as features.
Model In dict. Out of dict.
Full 93.4 85.0
No clusters 92.0 (?1.4) 79.3 (?5.7)
Total tokens 4,808 1,394
Table 3: DAILY547 accuracies (%) for tokens in and out
of a traditional dictionary, for models reported in rows 1
and 3 of Table 2.
possible to get radically smaller models with only
a slight degradation in performance: (4, 0.06) has
0.5% worse accuracy but uses only 1,632 features, a
small enough number to browse through manually.
First, we evaluate on the new test set, training on
all of OCT27. Due to DAILY547?s statistical repre-
sentativeness, we believe this gives the best view of
the tagger?s accuracy on English Twitter text. The
full tagger attains 93.2% accuracy (final row of Ta-
ble 2).
To facilitate comparisons with previous work, we
ran a series of experiments training only on OCT27?s
training and development sets, then report test re-
sults on both OCT27TEST and all of DAILY547,
shown in Table 2. Our tagger achieves substantially
higher accuracy than Gimpel et al (2011).17
Feature ablation. A number of ablation tests in-
dicate the word clusters are a very strong source of
lexical knowledge. When dropping the tag dictio-
naries and name lists, the word clusters maintain
most of the accuracy (row 2). If we drop the clus-
ters and rely only on tag dictionaries and namelists,
accuracy decreases significantly (row 3). In fact,
we can remove all observation features except for
word clusters?no word features, orthographic fea-
17These numbers differ slightly from those reported by Gim-
pel et al, due to the corrections we made to the OCT27 data,
noted in Section 5.1. We retrained and evaluated their tagger
(version 0.2) on our corrected dataset.
386
Feature set OCT27TEST DAILY547 NPSCHATTEST
All features 91.60 92.80 91.19 1
with clusters; without tagdicts, namelists 91.15 92.38 90.66 2
without clusters; with tagdicts, namelists 89.81 90.81 90.00 3
only clusters (and transitions) 89.50 90.54 89.55 4
without clusters, tagdicts, namelists 86.86 88.30 88.26 5
Gimpel et al (2011) version 0.2 88.89 89.17 6
Inter-annotator agreement (Gimpel et al, 2011) 92.2 7
Model trained on all OCT27 93.2 8
Table 2: Tagging accuracies (%) in ablation experiments. OCT27TEST and DAILY547 95% confidence intervals are
roughly ?0.7%. Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.
tures, affix n-grams, capitalization, emoticon pat-
terns, etc.?and the accuracy is in fact still better
than the previous work (row 4).18
We also wanted to know whether to keep the tag
dictionary and name list features, but the splits re-
ported in Fig. 2 did not show statistically signifi-
cant differences; so to better discriminate between
ablations, we created a lopsided train/test split of
all data with a much larger test portion (26,974 to-
kens), having greater statistical power (tighter con-
fidence intervals of ? 0.3%).19 The full system got
90.8% while the no?tag dictionary, no-namelists ab-
lation had 90.0%, a statistically significant differ-
ence. Therefore we retain these features.
Compared to the tagger in Gimpel et al, most of
our feature changes are in the new lexical features
described in ?3.5.20 We do not reuse the other lex-
ical features from the previous work, including a
phonetic normalizer (Metaphone), a name list con-
sisting of words that are frequently capitalized, and
distributional features trained on a much smaller un-
labeled corpus; they are all worse than our new
lexical features described here. (We did include,
however, a variant of the tag dictionary feature that
uses phonetic normalization for lookup; it seemed to
yield a small improvement.)
18Furthermore, when evaluating the clusters as unsupervised
(hard) POS tags, we obtain a many-to-one accuracy of 89.2%
on DAILY547. Before computing this, we lowercased the text
to match the clusters and removed tokens tagged as URLs and
at-mentions.
19Reported confidence intervals in this paper are 95% bino-
mial normal approximation intervals for the proportion of cor-
rectly tagged tokens: ?1.96
?
p(1? p)/ntokens . 1/
?
n.
20Details on the exact feature set are available in a technical
report (Owoputi et al, 2012), also available on the website.
Non-traditional words. The word clusters are es-
pecially helpful with words that do not appear in tra-
ditional dictionaries. We constructed a dictionary
by lowercasing the union of the ispell ?American?,
?British?, and ?English? dictionaries, plus the stan-
dard Unix words file from Webster?s Second Inter-
national dictionary, totalling 260,985 word types.
After excluding tokens defined by the gold stan-
dard as punctuation, URLs, at-mentions, or emoti-
cons,21 22% of DAILY547?s tokens do not appear in
this dictionary. Without clusters, they are very dif-
ficult to classify (only 79.2% accuracy), but adding
clusters generates a 5.7 point improvement?much
larger than the effect on in-dictionary tokens (Ta-
ble 3).
Varying the amount of unlabeled data. A tagger
that only uses word clusters achieves an accuracy of
88.6% on the OCT27 development set.22 We created
several clusterings with different numbers of unla-
beled tweets, keeping the number of clusters con-
stant at 800. As shown in Fig. 3, there was initially
a logarithmic relationship between number of tweets
and accuracy, but accuracy (and lexical coverage)
levels out after 750,000 tweets. We use the largest
clustering (56 million tweets and 1,000 clusters) as
the default for the released tagger.
6.2 Evaluation on RITTERTW
Ritter et al (2011) annotated a corpus of 787
tweets23 with a single annotator, using the PTB
21We retain hashtags since by our guidelines a #-prefixed to-
ken is ambiguous between a hashtag and a normal word, e.g. #1
or going #home.
22The only observation features are the word clusters of a
token and its immediate neighbors.
23https://github.com/aritter/twitter_nlp/
blob/master/data/annotated/pos.txt
387
Tagger Accuracy
This work 90.0 ? 0.5
Ritter et al (2011), basic CRF tagger 85.3
Ritter et al (2011), trained on more data 88.3
Table 4: Accuracy comparison on Ritter et al?s Twitter
POS corpus (?6.2).
Tagger Accuracy
This work 93.4 ? 0.3
Forsyth (2007) 90.8
Table 5: Accuracy comparison on Forsyth?s NPSCHAT
IRC POS corpus (?6.3).
tagset plus several Twitter-specific tags, referred
to in Table 1 as RITTERTW. Linguistic concerns
notwithstanding (?5.2), for a controlled comparison,
we train and test our system on this data with the
same 4-fold cross-validation setup they used, attain-
ing 90.0% (?0.5%) accuracy. Ritter et al?s CRF-
based tagger had 85.3% accuracy, and their best tag-
ger, trained on a concatenation of PTB, IRC, and
Twitter, achieved 88.3% (Table 4).
6.3 IRC: Evaluation on NPSCHAT
IRC is another medium of online conversational
text, with similar emoticons, misspellings, abbrevi-
ations and acronyms as Twitter data. We evaluate
our tagger on the NPS Chat Corpus (Forsyth and
Martell, 2007),24 a PTB-part-of-speech annotated
dataset of Internet Relay Chat (IRC) room messages
from 2006.
First, we compare to a tagger in the same setup as
experiments on this data in Forsyth (2007), training
on 90% of the data and testing on 10%; we average
results across 10-fold cross-validation.25 The full
tagger model achieved 93.4% (?0.3%) accuracy,
significantly improving over the best result they re-
port, 90.8% accuracy with a tagger trained on a mix
of several POS-annotated corpora.
We also perform the ablation experiments on this
corpus, with a slightly different experimental setup:
we first filter out system messages then split data
24Release 1.0: http://faculty.nps.edu/
cmartell/NPSChat.htm
25Forsyth actually used 30 different 90/10 random splits; we
prefer cross-validation because the same test data is never re-
peated, thus allowing straightforward confidence estimation of
accuracy from the number of tokens (via binomial sample vari-
ance, footnote 19). In all cases, the models are trained on the
same amount of data (90%).
into 5,067 training and 2,868 test messages. Results
show a similar pattern as the Twitter data (see final
column of Table 2). Thus the Twitter word clusters
are also useful for language in the medium of text
chat rooms; we suspect these clusters will be appli-
cable for deeper syntactic and semantic analysis in
other online conversational text mediums, such as
text messages and instant messages.
7 Conclusion
We have constructed a state-of-the-art part-of-
speech tagger for the online conversational text
genres of Twitter and IRC, and have publicly re-
leased our new evaluation data, annotation guide-
lines, open-source tagger, and word clusters at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgements
This research was supported in part by the National Sci-
ence Foundation (IIS-0915187 and IIS-1054319).
A Part-of-Speech Tagset
N common noun
O pronoun (personal/WH; not possessive)
^ proper noun
S nominal + possessive
Z proper noun + possessive
V verb including copula, auxiliaries
L nominal + verbal (e.g. i?m), verbal + nominal (let?s)
M proper noun + verbal
A adjective
R adverb
! interjection
D determiner
P pre- or postposition, or subordinating conjunction
& coordinating conjunction
T verb particle
X existential there, predeterminers
Y X + verbal
# hashtag (indicates topic/category for tweet)
@ at-mention (indicates a user as a recipient of a tweet)
~ discourse marker, indications of continuation across
multiple tweets
U URL or email address
E emoticon
$ numeral
, punctuation
G other abbreviations, foreign words, possessive endings,
symbols, garbage
Table 6: POS tagset from Gimpel et al (2011) used in this
paper, and described further in the released annotation
guidelines.
388
References
J. Allan and H. Raghavan. 2002. Using part-of-speech
patterns to reduce query ambiguity. In Proc. of SIGIR.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech in-
duction. In Proc. of ACL.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proc. of EMNLP.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18(4).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hr-
uschka Jr, and T. M. Mitchell. 2010. Toward an archi-
tecture for never-ending language learning. In Proc. of
AAAI.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proc. of EACL.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
J. Eisenstein. 2013. What to do about bad language on
the internet. In Proc. of NAACL.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proc. of
EMNLP.
E. N. Forsyth and C. H. Martell. 2007. Lexical and dis-
course analysis of online chat dialog. In Proc. of ICSC.
E. N. Forsyth. 2007. Improving automated lexical and
discourse analysis of online chat dialog. Master?s the-
sis, Naval Postgraduate School.
J. Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan,
J. Nivre, D. Hogan, and J. van Genabith. 2011. #hard-
toparse: POS tagging and parsing the Twitterverse. In
Proc. of AAAI-11 Workshop on Analysing Microtext.
K. Gimpel, N. Schneider, B. O?Connor, D. Das, D. Mills,
J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan,
and N. A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proc. of ACL.
Google. 2012. Freebase data dumps. http://
download.freebase.com/datadumps/.
B. Han and T. Baldwin. 2011. Lexical normalisation of
short text messages: Makn sens a #twitter. In Proc. of
ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical programming, 45(1).
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recogniz-
ing named entities in tweets. In Proc. of ACL.
M. Lui and T. Baldwin. 2012. langid.py: An off-the-
shelf language identification tool. In Proc. of ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2).
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Proc. of
NAACL.
B. O?Connor, M. Krieger, and D. Ahn. 2010.
TweetMotif: exploratory search and topic summariza-
tion for Twitter. In Proc. of AAAI Conference on We-
blogs and Social Media.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel, and
N. Schneider. 2012. Part-of-speech tagging for Twit-
ter: Word clusters and other advances. Technical Re-
port CMU-ML-12-107, Carnegie Mellon University.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
S. Petrov and R. McDonald. 2012. Overview of the 2012
shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimental
study. In Proc. of EMNLP.
T. Schnoebelen. 2012. Do you smile with your nose?
Stylistic variation in Twitter emoticons. University of
Pennsylvania Working Papers in Linguistics, 18(2):14.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
O. T?ckstr?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proc. of NAACL.
389
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proc. of ACL.
P. D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
67(2):301?320.
390
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809?815,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Tailoring Continuous Word Representations for Dependency Parsing
Mohit Bansal Kevin Gimpel Karen Livescu
Toyota Technological Institute at Chicago, IL 60637, USA
{mbansal,kgimpel,klivescu}@ttic.edu
Abstract
Word representations have proven useful
for many NLP tasks, e.g., Brown clusters
as features in dependency parsing (Koo et
al., 2008). In this paper, we investigate the
use of continuous word representations as
features for dependency parsing. We com-
pare several popular embeddings to Brown
clusters, via multiple types of features, in
both news and web domains. We find that
all embeddings yield significant parsing
gains, including some recent ones that can
be trained in a fraction of the time of oth-
ers. Explicitly tailoring the representations
for the task leads to further improvements.
Moreover, an ensemble of all representa-
tions achieves the best results, suggesting
their complementarity.
1 Introduction
Word representations derived from unlabeled text
have proven useful for many NLP tasks, e.g., part-
of-speech (POS) tagging (Huang et al, 2014),
named entity recognition (Miller et al, 2004),
chunking (Turian et al, 2010), and syntactic
parsing (Koo et al, 2008; Finkel et al, 2008;
T?ackstr?om et al, 2012). Most word representa-
tions fall into one of two categories. Discrete rep-
resentations consist of memberships in a (possibly
hierarchical) hard clustering of words, e.g., via k-
means or the Brown et al (1992) algorithm. Con-
tinuous representations (or distributed representa-
tions or embeddings) consist of low-dimensional,
real-valued vectors for each word, typically in-
duced via neural language models (Bengio et al,
2003; Mnih and Hinton, 2007) or spectral meth-
ods (Deerwester et al, 1990; Dhillon et al, 2011).
Koo et al (2008) found improvement on in-
domain dependency parsing using features based
on discrete Brown clusters. In this paper, we ex-
periment with parsing features derived from con-
tinuous representations. We find that simple at-
tempts based on discretization of individual word
vector dimensions do not improve parsing. We
see gains only after first performing a hierarchi-
cal clustering of the continuous word vectors and
then using features based on the hierarchy.
We compare several types of continuous rep-
resentations, including those made available by
other researchers (Turian et al, 2010; Collobert et
al., 2011; Huang et al, 2012), and embeddings we
have trained using the approach of Mikolov et al
(2013a), which is orders of magnitude faster than
the others. The representations exhibit different
characteristics, which we demonstrate using both
intrinsic metrics and extrinsic parsing evaluation.
We report significant improvements over our base-
line on both the Penn Treebank (PTB; Marcus et
al., 1993) and the English Web treebank (Petrov
and McDonald, 2012).
While all embeddings yield some parsing im-
provements, we find larger gains by tailoring them
to capture similarity in terms of context within
syntactic parses. To this end, we use two sim-
ple modifications to the models of Mikolov et al
(2013a): a smaller context window, and condition-
ing on syntactic context (dependency links and la-
bels). Interestingly, the Brown clusters of Koo et
al. (2008) prove to be difficult to beat, but we find
that our syntactic tailoring can lead to embeddings
that match the parsing performance of Brown (on
all test sets) in a fraction of the training time. Fi-
nally, a simple parser ensemble on all the represen-
tations achieves the best results, suggesting their
complementarity for dependency parsing.
2 Continuous Word Representations
There are many ways to train continuous represen-
tations; in this paper, we are primarily interested
in neural language models (Bengio et al, 2003),
which use neural networks and local context to
learn word vectors. Several researchers have
made their trained representations publicly avail-
809
Representation Source Corpus Types, Tokens V D Time
BROWN Koo et al (2008) BLLIP 317K, 43M 316,710 ? 2.5 days
?
SENNA Collobert et al (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months
?
TURIAN Turian et al (2010) RCV1 269K, 37M 268,810 50 few weeks
?
HUANG Huang et al (2012) Wikipedia 8.3M, 1.8B 100,232 50 ?
CBOW, SKIP, SKIP
DEP
Mikolov et al (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.
?
Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous
representations require an additional 4 hours to run hierarchical clustering to generate features (?3.2). RCV1 = Reuters Corpus,
Volume 1. ? = time reported by authors. ? = run by us on a 3.50 GHz desktop, using a single thread.
able, which we use directly in our experiments.
In particular, we use the SENNA embeddings of
Collobert et al (2011); the scaled TURIAN em-
beddings (C&W) of Turian et al (2010); and the
HUANG global-context, single-prototype embed-
dings of Huang et al (2012). We also use the
BROWN clusters trained by Koo et al (2008). De-
tails are given in Table 1.
Below, we describe embeddings that we train
ourselves (?2.1), aiming to make them more useful
for parsing via smaller context windows (?2.1.1)
and conditioning on syntactic context (?2.1.2). We
then compare the representations using two intrin-
sic metrics (?2.2).
2.1 Syntactically-tailored Representations
We train word embeddings using the continu-
ous bag-of-words (CBOW) and skip-gram (SKIP)
models described in Mikolov et al (2013a;
2013b) as implemented in the open-source toolkit
word2vec. These models avoid hidden layers
in the neural network and hence can be trained
in only minutes, compared to days or even weeks
for the others, as shown in Table 1.
1
We adapt
these embeddings to be more useful for depen-
dency parsing in two ways, described next.
2.1.1 Smaller Context Windows
The CBOW model learns vectors to predict a
word given its set of surrounding context words
in a window of size w. The SKIP model learns
embeddings to predict each individual surround-
ing word given one particular word, using an anal-
ogous window size w. We find that w affects
the embeddings substantially: with large w, words
group with others that are topically-related; with
small w, grouped words tend to share the same
POS tag. We discuss this further in the intrinsic
evaluation presented in ?2.2.
1
We train both models on BLLIP (LDC2000T43) with
PTB removed, the same corpus used by Koo et al (2008) to
train their BROWN clusters. We created a special vector for
unknown words by averaging the vectors for the 50K least
frequent words; we did not use this vector for the SKIP
DEP
(?2.1.2) setting because it performs slightly better without it.
2.1.2 Syntactic Context
We expect embeddings to help dependency pars-
ing the most when words that have similar parents
and children are close in the embedding space. To
target this type of similarity, we train the SKIP
model on dependency context instead of the linear
context in raw text. When ordinarily training SKIP
embeddings, words v
?
are drawn from the neigh-
borhood of a target word v, and the sum of log-
probabilities of each v
?
given v is maximized. We
propose to instead choose v
?
from the set contain-
ing the grandparent, parent, and children words of
v in an automatic dependency parse.
A simple way to implement this idea is to train
the original SKIP model on a corpus of depen-
dency links and labels. For this, we parse the
BLLIP corpus (minus PTB) using our baseline de-
pendency parser, then build a corpus in which each
line contains a single child word c, its parent word
p, its grandparent g, and the dependency label ` of
the ?c, p? link:
?`
<L>
g
<G>
p c `
<L>
?,
that is, both the dependency label and grandparent
word are subscripted with a special token to avoid
collision with words.
2
We train the SKIP model on
this corpus of tuples with window size w = 1, de-
noting the result SKIP
DEP
. Note that this approach
needs a parsed corpus, but there also already ex-
ist such resources (Napoles et al, 2012; Goldberg
and Orwant, 2013).
2.2 Intrinsic Evaluation of Representations
Short of running end-to-end parsing experiments,
how can we choose which representations to use
for parsing tasks? Several methods have been pro-
posed for intrinsic evaluation of word representa-
2
We use a subscript on g so that it will be treated dif-
ferently from c when considering the context of p. We re-
moved all g
<G>
from the vocabulary after training. We also
tried adding information about POS tags. This increases M-1
(?2.2), but harms parsing performance, likely because the em-
beddings become too tag-like. Similar ideas have been used
for clustering (Sagae and Gordon, 2009; Haffari et al, 2011;
Grave et al, 2013), semantic space models (Pad?o and Lapata,
2007), and topic modeling (Boyd-Graber and Blei, 2008).
810
Representation SIM M-1
BROWN ? 89.3
SENNA 49.8 85.2
TURIAN 29.5 87.2
HUANG 62.6 78.1
CBOW, w = 2 34.7 84.8
SKIP, w = 1 37.8 86.6
SKIP, w = 2 43.1 85.8
SKIP, w = 5 44.4 81.1
SKIP, w = 10 44.6 71.5
SKIP
DEP
34.6 88.3
Table 2: Intrinsic evaluation of representations. SIM column
has Spearman?s ?? 100 for 353-pair word similarity dataset.
M-1 is our unsupervised POS tagging metric. For BROWN,
M-1 is simply many-to-one accuracy of the clusters. Best
score in each column is bold.
tions; we discuss two here:
Word similarity (SIM): One widely-used evalu-
ation compares distances in the continuous space
to human judgments of word similarity using the
353-pair dataset of Finkelstein et al (2002). We
compute cosine similarity between the two vectors
in each word pair, then order the word pairs by
similarity and compute Spearman?s rank correla-
tion coefficient (?) with the gold similarities. Em-
beddings with high ? capture similarity in terms of
paraphrase and topical relationships.
Clustering-based tagging accuracy (M-1): In-
tuitively, we expect embeddings to help parsing
the most if they can tell us when two words are
similar syntactically. To this end, we use a met-
ric based on unsupervised evaluation of POS tag-
gers. We perform clustering and map each cluster
to one POS tag so as to maximize tagging accu-
racy, where multiple clusters can map to the same
tag. We cluster vectors corresponding to the to-
kens in PTB WSJ sections 00-21.
3
Table 2 shows these metrics for representations
used in this paper. The BROWN clusters have
the highest M-1, indicating high cluster purity in
terms of POS tags. The HUANG embeddings have
the highest SIM score but low M-1, presumably
because they were trained with global context,
making them more tuned to capture topical sim-
ilarity. We compare several values for the win-
dow size (w) used when training the SKIP embed-
dings, finding that smallw leads to higher M-1 and
lower SIM. Table 3 shows examples of clusters
obtained by clustering SKIP embeddings of w = 1
versus w = 10, and we see that the former cor-
respond closely to POS tags, while the latter are
3
For clustering, we use k-means with k = 1000 and ini-
tialize by placing centroids on the 1000 most-frequent words.
w Example clusters
1 [Mr., Mrs., Ms., Prof., ...], [Jeffrey, Dan, Robert,
Peter, ...], [Johnson, Collins, Schmidt, Freedman,
...], [Portugal, Iran, Cuba, Ecuador, ...], [CST, 4:30,
9-10:30, CDT, ...], [his, your, her, its, ...], [truly,
wildly, politically, financially, ...]
10 [takeoff, altitude, airport, carry-on, airplane, flown,
landings, ...], [health-insurance, clinic, physician,
doctor, medical, health-care, ...], [financing, equity,
investors, firms, stock, fund, market, ...]
Table 3: Example clusters for SKIP embeddings with win-
dow size w = 1 (syntactic) and w = 10 (topical).
much more topically-coherent and contain mixed
POS tags.
4
For parsing experiments, we choose
w = 2 for CBOW and w = 1 for SKIP. Finally,
our SKIP
DEP
embeddings, trained with syntactic
context and w = 1 (?2.1.2), achieve the highest
M-1 of all continuous representations. In ?4, we
will relate these intrinsic metrics to extrinsic pars-
ing performance.
3 Dependency Parsing Features
We now discuss the features that we add to our
baseline dependency parser (second-order MST-
Parser; McDonald and Pereira, 2006) based on
discrete and continuous representations.
3.1 Brown Cluster Features
We start by replicating the features of Koo et al
(2008) using their BROWN clusters; each word is
represented by a 0-1 bit string indicating the path
from the root to the leaf in the binary merge tree.
We follow Koo et al in adding cluster versions of
the first- and second-order features in MSTParser,
using bit string prefixes of the head, argument,
sibling, intermediate words, etc., to augment or
replace the POS and lexical identity information.
We tried various sets of prefix lengths on the devel-
opment set and found the best setting to use pre-
fixes of length 4, 6, 8, and 12.
5
3.2 Continuous Representation Features
We tried two kinds of indicator features:
Bucket features: For both parent and child vec-
tors in a potential dependency, we fire one indi-
cator feature per dimension of each embedding
4
A similar effect, when changing distributional context
window sizes, was found by Lin and Wu (2009).
5
See Koo et al (2008) for the exact feature templates.
They used the full string in place of the length-12 prefixes,
but that setting worked slightly worse for us. Note that the
baseline parser used by Koo et al (2008) is different from the
second-order MSTParser that we use here; their parser allows
grandparent interactions in addition to the sibling interactions
in ours. We use their clusters, available at http://people.
csail.mit.edu/maestro/papers/bllip-clusters.gz.
811
vector, where the feature consists of the dimen-
sion index d and a bucketed version of the embed-
ding value in that dimension, i.e., bucket
k
(E
vd
)
for word index v and dimension d, where E is the
V ?D embedding matrix.
6
We also tried standard
conjunction variants of this feature consisting of
the bucket values of both the head and argument
along with their POS-tag or word information, and
the attachment distance and direction.
7
Cluster bit string features: To take into account
all dimensions simultaneously, we perform ag-
glomerative hierarchical clustering of the embed-
ding vectors. We use Ward?s minimum variance
algorithm (Ward, 1963) for cluster distance and
the Euclidean metric for vector distance (via MAT-
LAB?s linkage function with {method=ward,
metric=euclidean}). Next, we fire features on the
hierarchical clustering bit strings using templates
identical to those for BROWN, except that we use
longer prefixes as our clustering hierarchies tend
to be deeper.
8
4 Parsing Experiments
Setup: We use the publicly-available MST-
Parser for all experiments, specifically its second-
order projective model.
9
We remove all fea-
tures that occur only once in the training data.
For WSJ parsing, we use the standard train(02-
21)/dev(22)/test(23) split and apply the NP brack-
eting patch by Vadas and Curran (2007). For
Web parsing, we still train on WSJ 02-21, but
test on the five Web domains (answers, email,
newsgroup, reviews, and weblog) of the ?English
Web Treebank? (LDC2012T13), splitting each do-
main in half (in original order) for the develop-
ment and test sets.
10
For both treebanks, we con-
vert from constituent to dependency format us-
ing pennconverter (Johansson and Nugues,
2007), and generate POS tags using the MXPOST
tagger (Ratnaparkhi, 1996). To evaluate, we use
6
Our bucketing function bucket
k
(x) converts the real
value x to its closest multiple of k. We choose a k value
of around 1/5th of the embedding?s absolute range.
7
We initially experimented directly with real-valued fea-
tures (instead of bucketed indicator features) and similar con-
junction variants, but these did not perform well.
8
We use prefixes of length 4, 6, 8, 12, 16, 20, and full-
length, again tuned on the development set.
9
We use the recommended MSTParser settings: training-
k:5 iters:10 loss-type:nopunc decode-type:proj
10
Our setup is different from SANCL 2012 (Petrov and
McDonald, 2012) because the exact splits and test data were
only available to participants.
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA (Buckets) 92.64 92.04
SENNA (Bit strings) 92.88 92.30
HUANG (Buckets) 92.44 91.86
HUANG (Bit strings) 92.55 92.36
CBOW (Buckets) 92.57 91.93
CBOW (Bit strings) 93.06 92.53
Table 4: Bucket vs. bit string features (UAS on WSJ).
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA 92.88 92.30
TURIAN 92.84 92.26
HUANG 92.55 92.36
CBOW 93.06 92.53
SKIP 92.94 92.29
SKIP
DEP
93.33 92.69
Ensemble Results
ALL ? BROWN 93.46 92.90
ALL 93.54 92.98
Table 5: Full results with bit string features (UAS on WSJ).
unlabeled attachment score (UAS).
11
We report
statistical significance (p < 0.01, 100K sam-
ples) using the bootstrap test (Efron and Tibshi-
rani, 1994).
Comparing bucket and bit string features: In
Table 4, we find that bucket features based on in-
dividual embedding dimensions do not lead to im-
provements in test accuracy, while bit string fea-
tures generally do. This is likely because indi-
vidual embedding dimensions rarely correspond to
interpretable or useful distinctions among words,
whereas the hierarchical bit strings take into ac-
count all dimensions of the representations simul-
taneously. Their prefixes also naturally define fea-
tures at multiple levels of granularity.
WSJ results: Table 5 shows our main WSJ
results. Although BROWN yields one of the
highest individual gains, we also achieve statis-
tically significant gains over the baseline from
all embeddings. The CBOW embeddings per-
form as well as BROWN (i.e., no statistically
significant difference) but are orders of magni-
tude faster to train. Finally, the syntactically-
trained SKIP
DEP
embeddings are statistically indis-
tinguishable from BROWN and CBOW, and sig-
nificantly better than all other embeddings. This
suggests that targeting the similarity captured by
syntactic context is useful for dependency parsing.
11
We find similar improvements under labeled attachment
score (LAS). We ignore punctuation : , ? ? . in our evalua-
tion (Yamada and Matsumoto, 2003; McDonald et al, 2005).
812
System ans eml nwg rev blog Avg
Baseline 82.6 81.2 84.3 83.8 85.5 83.5
BROWN 83.4 81.7 85.2 84.5 86.1 84.2
SENNA 83.7 81.9 85.0 85.0 86.0 84.3
TURIAN 83.0 81.5 85.0 84.1 85.7 83.9
HUANG 83.1 81.8 85.1 84.7 85.9 84.1
CBOW 82.9 81.3 85.2 83.9 85.8 83.8
SKIP 83.1 81.1 84.7 84.1 85.4 83.7
SKIP
DEP
83.3 81.5 85.2 84.3 86.0 84.1
Ensemble Results
ALL?BR 83.9 82.2 85.9 85.0 86.6 84.7
ALL 84.2 82.3 85.9 85.1 86.8 84.9
Table 6: Main UAS test results on Web treebanks. Here,
ans=answers, eml=email, nwg=newsgroup, rev=reviews,
blog=weblog, BR=BROWN, Avg=Macro-average.
Web results: Table 6 shows our main Web re-
sults.
12
Here, we see that the SENNA, BROWN,
and SKIP
DEP
embeddings perform the best on av-
erage (and are statistically indistinguishable, ex-
cept SENNA vs. SKIP
DEP
on the reviews domain).
They yield statistically significant UAS improve-
ments over the baseline across all domains, except
weblog for SENNA (narrowly misses significance,
p=0.014) and email for SKIP
DEP
.
13
Ensemble results: When analyzing errors, we
see differences among the representations, e.g.,
BROWN does better at attaching proper nouns,
prepositions, and conjunctions, while CBOW
does better on plural common nouns and adverbs.
This suggests that the representations might be
complementary and could benefit from combina-
tion. To test this, we use a simple ensemble parser
that chooses the highest voted parent for each ar-
gument.
14
As shown in the last two rows of Ta-
bles 5 and 6, this leads to substantial gains. The
?ALL ? BROWN? ensemble combines votes from
all non-BROWN continuous representations, and
the ?ALL? ensemble also includes BROWN.
Characteristics of representations: We now re-
late the intrinsic metrics from ?2.2 to parsing
performance. The clearest correlation appears
when comparing variations of a single model,
e.g., for SKIP, the WSJ dev accuracies are 93.33
(SKIP
DEP
), 92.94 (w = 1), 92.86 (w = 5), and
92.70 (w = 10), which matches the M-1 score or-
der and is the reverse of the SIM score order.
12
We report individual domain results and macro-average
over domains. We do not tune any features/parameters on
Web dev sets; we only show the test results for brevity.
13
Note that SENNA and HUANG are trained on Wikipedia
which may explain why they work better on Web parsing as
compared to WSJ parsing.
14
This does not guarantee a valid tree. Combining features
from representations will allow training to weigh them appro-
priately and also guarantee a tree.
5 Related Work
In addition to work mentioned above, relevant
work that uses discrete representations exists for
POS tagging (Ritter et al, 2011; Owoputi et
al., 2013), named entity recognition (Ratinov
and Roth, 2009), supersense tagging (Grave et
al., 2013), grammar induction (Spitkovsky et al,
2011), constituency parsing (Finkel et al, 2008),
and dependency parsing (Tratz and Hovy, 2011).
Continuous representations in NLP have been
evaluated for their ability to capture syntactic and
semantic word similarity (Huang et al, 2012;
Mikolov et al, 2013a; Mikolov et al, 2013b) and
used for tasks like semantic role labeling, part-
of-speech tagging, NER, chunking, and sentiment
classification (Turian et al, 2010; Collobert et al,
2011; Dhillon et al, 2012; Al-Rfou? et al, 2013).
For dependency parsing, Hisamoto et al (2013)
also used embedding features, but there are several
differences between their work and ours. First,
they use only one set of pre-trained embeddings
(TURIAN) while we compare several and also train
our own, tailored to the task. Second, their em-
bedding features are simpler than ours, only us-
ing flat (non-hierarchical) cluster IDs and binary
strings obtained via sign quantization (1[x > 0])
of the vectors. They also compare to a first-order
baseline and only evaluate on the Web treebanks.
Concurrently, Andreas and Klein (2014) inves-
tigate the use of embeddings in constituent pars-
ing. There are several differences: we work on de-
pendency parsing, use clustering-based features,
and tailor our embeddings to dependency-style
syntax; their work additionally studies vocabulary
expansion and relating in-vocabulary words via
embeddings.
6 Conclusion
We showed that parsing features based on hierar-
chical bit strings work better than those based on
discretized individual embedding values. While
the Brown clusters prove to be well-suited to pars-
ing, we are able to match their performance with
our SKIP
DEP
embeddings that train much faster.
Finally, we found the various representations to
be complementary, enabling a simple ensemble
to perform best. Our SKIP
DEP
embeddings and
bit strings are available at ttic.edu/bansal/
data/syntacticEmbeddings.zip.
813
References
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of CoNLL.
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In Pro-
ceedings of ACL.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155, March.
Jordan L. Boyd-Graber and David M. Blei. 2008. Syn-
tactic topic models. In Proceedings of NIPS.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Paramveer Dhillon, Dean P. Foster, and Lyle H. Ungar.
2011. Multi-view learning of word embeddings via
CCA. In Proceedings of NIPS.
Paramveer Dhillon, Jordan Rodu, Dean P. Foster, and
Lyle H. Ungar. 2012. Two Step CCA: A new spec-
tral method for estimating vector models of words.
In Proceedings of ICML.
Bradley Efron and Robert J. Tibshirani. 1994. An in-
troduction to the bootstrap, volume 57. CRC press.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (* SEM),
volume 1, pages 241?247.
Edouard Grave, Guillaume Obozinski, and Francis
Bach. 2013. Hidden markov tree models for se-
mantic class induction. In Proceedings of CoNLL.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL.
Sorami Hisamoto, Kevin Duh, and Yuji Matsumoto.
2013. An empirical investigation of word repre-
sentations for parsing the web. In Proceedings of
ANLP.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL.
Fei Huang, Arun Ahuja, Doug Downey, Yi Yang,
Yuhong Guo, and Alexander Yates. 2014. Learning
representations for weakly supervised natural lan-
guage processing tasks. Computational Linguistics,
40(1).
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In 16th Nordic Conference of Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19:313?330.
Ryan T. McDonald and Fernando C. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL.
Ryan T. McDonald, Koby Crammer, and Fernando C.
Pereira. 2005. Spanning tree methods for discrim-
inative training of dependency parsers. Technical
Report MS-CIS-05-11, University of Pennsylvania.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NIPS.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
814
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, AKBC-WEKEX ?12, pages 95?100,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of EMNLP.
Kenji Sagae and Andrew S. Gordon. 2009. Cluster-
ing words by syntactic similarity improves depen-
dency parsing of predicate-argument structures. In
Proceedings of the 11th International Conference on
Parsing Technologies.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of EMNLP.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL.
Joe H. Ward. 1963. Hierarchical grouping to optimize
an objective function. Journal of the American sta-
tistical association, 58(301):236?244.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of International Conference
on Parsing Technologies.
815
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 213?222,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Distributed Asynchronous Online Learning
for Natural Language Processing
Kevin Gimpel Dipanjan Das Noah A. Smith
Language Technologies Institute
Carnegie Mellon Univeristy
Pittsburgh, PA 15213, USA
{kgimpel,dipanjan,nasmith}@cs.cmu.edu
Abstract
Recent speed-ups for training large-scale
models like those found in statistical NLP
exploit distributed computing (either on
multicore or ?cloud? architectures) and
rapidly converging online learning algo-
rithms. Here we aim to combine the two.
We focus on distributed, ?mini-batch?
learners that make frequent updates asyn-
chronously (Nedic et al, 2001; Langford
et al, 2009). We generalize existing asyn-
chronous algorithms and experiment ex-
tensively with structured prediction prob-
lems from NLP, including discriminative,
unsupervised, and non-convex learning
scenarios. Our results show asynchronous
learning can provide substantial speed-
ups compared to distributed and single-
processor mini-batch algorithms with no
signs of error arising from the approximate
nature of the technique.
1 Introduction
Modern statistical NLP models are notoriously
expensive to train, requiring the use of general-
purpose or specialized numerical optimization al-
gorithms (e.g., gradient and coordinate ascent al-
gorithms and variations on them like L-BFGS and
EM) that iterate over training data many times.
Two developments have led to major improve-
ments in training time for NLP models:
? online learning algorithms (LeCun et al, 1998;
Crammer and Singer, 2003; Liang and Klein,
2009), which update the parameters of a model
more frequently, processing only one or a small
number of training examples, called a ?mini-
batch,? between updates; and
? distributed computing, which divides training
data among multiple CPUs for faster processing
between updates (e.g., Clark and Curran, 2004).
Online algorithms offer fast convergence rates
and scalability to large datasets, but distributed
computing is a more natural fit for algorithms that
require a lot of computation?e.g., processing a
large batch of training examples?to be done be-
tween updates. Typically, distributed online learn-
ing has been done in a synchronous setting, mean-
ing that a mini-batch of data is divided among
multiple CPUs, and the model is updated when
they have all completed processing (Finkel et al,
2008). Each mini-batch is processed only after the
previous one has completed.
Synchronous frameworks are appealing in that
they simulate the same algorithms that work on
a single processor, but they have the drawback
that the benefits of parallelism are only obtainable
within one mini-batch iteration. Moreover, empir-
ical evaluations suggest that online methods only
converge faster than batch algorithms when using
very small mini-batches (Liang and Klein, 2009).
In this case, synchronous parallelization will not
offer much benefit.
In this paper, we focus our attention on asyn-
chronous algorithms that generalize those pre-
sented by Nedic et al (2001) and Langford et al
(2009). In these algorithms, multiple mini-batches
are processed simultaneously, each using poten-
tially different and typically stale parameters. The
key advantage of an asynchronous framework is
that it allows processors to remain in near-constant
use, preventing them from wasting cycles wait-
ing for other processors to complete their por-
tion of the current mini-batch. In this way, asyn-
chronous algorithms allow more frequent parame-
ter updates, which speeds convergence.
Our contributions are as follows:
? We describe a framework for distributed asyn-
chronous optimization (?5) similar to those de-
scribed by Nedic et al (2001) and Langford et
al. (2009), but permitting mini-batch learning.
The prior work contains convergence results for
asynchronous online stochastic gradient descent
213
for convex functions (discussed in brief in ?5.2).
? We report experiments on three structured NLP
tasks, including one problem that matches
the conditions for convergence (named entity
recognition; NER) and two that depart from the-
oretical foundations, namely the use of asyn-
chronous stepwise EM (Sato and Ishii, 2000;
Cappe? and Moulines, 2009; Liang and Klein,
2009) for both convex and non-convex opti-
mization.
? We directly compare asynchronous algorithms
with multiprocessor synchronous mini-batch al-
gorithms (e.g., Finkel et al, 2008) and tradi-
tional batch algorithms.
? We experiment with adding artificial delays to
simulate the effects of network or hardware traf-
fic that could cause updates to be made with ex-
tremely stale parameters.
? Our experimental settings include both indi-
vidual 4-processor machines as well as large
clusters of commodity machines implementing
the MapReduce programming model (Dean and
Ghemawat, 2004). We also explore effects of
mini-batch size.
Our main conclusion is that, when small mini-
batches work well, asynchronous algorithms of-
fer substantial speed-ups without introducing er-
ror. When large mini-batches work best, asyn-
chronous learning does not hurt.
2 Optimization Setting
We consider the problem of optimizing a function
f : Rd ? R with respect to its argument, denoted
? = ??1, ?2, . . . , ?d?. We assume that f is a sum
of n convex functions (hence f is also convex):1
f(?) =
?n
i=1 fi(?) (1)
We initially focus our attention on functions that
can be optimized using gradient or subgradient
methods. Log-likelihood for a probabilistic model
with fully observed training data (e.g., conditional
random fields; Lafferty et al, 2001) is one exam-
ple that frequently arises in NLP, where the fi(?)
each correspond to an individual training exam-
ple and the ? are log-linear feature weights. An-
other example is large-margin learning for struc-
tured prediction (Taskar et al, 2005; Tsochan-
1We use ?convex? to mean convex-up when minimizing
and convex-down, or concave, when maximizing.
taridis et al, 2005), which can be solved by sub-
gradient methods (Ratliff et al, 2006).
For concreteness, we discuss the architecture
in terms of gradient-based optimization, using the
following gradient descent update rule (for mini-
mization problems):2
?(t+1) ? ?(t) ? ?(t)g(?(t)) (2)
where ?(t) is the parameter vector on the tth iter-
ation, ?(t) is the step size on the tth iteration, and
g : Rd ? Rd is the vector function of first deriva-
tives of f with respect to ?:
g(?) =
?
?f
??1
(?), ?f??2 (?), . . . ,
?f
??d
(?)
?
(3)
We are interested in optimizing such functions
using distributed computing, by which we mean to
include any system containing multiple processors
that can communicate in order to perform a single
task. The set of processors can range from two
cores on a single machine to a MapReduce cluster
of thousands of machines.
Note our assumption that the computation re-
quired to optimize f with respect to ? is, essen-
tially, the gradient vector g(?(t)), which serves
as the descent direction. The key to distribut-
ing this computation is the fact that g(?(t)) =
?n
i=1 gi(?
(t)), where gi(?) denotes the gradient
of fi(?) with respect to ?. We now discuss several
ways to go about distributing such a problem, cul-
minating in the asynchronous mini-batch setting.
3 Distributed Batch Optimization
Given p processors plus a master processor, the
most straightforward way to optimize f is to par-
tition the fi so that for each i ? {1, 2, . . . , n},
gi is computed on exactly one ?slave? processor.
Let Ij denote the subset of examples assigned to
the jth slave processor (
?p
j=1 Ij = {1, . . . , n}
and j 6= j? ? Ij ? Ij? = ?). Processor j re-
ceives the examples in Ij along with the neces-
sary portions of ?(t) for calculating gIj (?
(t)) =
?
i?Ij
gi(?
(t)). The result of this calculation is
returned to the master processor, which calculates
g(?(t)) =
?
j gIj (?
(t)) and executes Eq. 2 (or
something more sophisticated that uses the same
information) to obtain a new parameter vector.
It is natural to divide the data so that each pro-
cessor is assigned approximately n/p of the train-
ing examples. Because of variance in the expense
2We use the term ?gradient? for simplicity, but subgradi-
ents are sufficient throughout.
214
of calculating the different gi, and because of un-
predictable variation among different processors?
speed (e.g., variation among nodes in a cluster,
or in demands made by other users), there can be
variation in the observed runtime of different pro-
cessors on their respective subsamples. Each it-
eration of calculating g will take as long as the
longest-running among the processors, whatever
the cause of that processor?s slowness. In comput-
ing environments where the load on processors is
beyond the control of the NLP researcher, this can
be a major bottleneck.
Nonetheless, this simple approach is widely
used in practice; approaches in which the gradient
computation is distributed via MapReduce have
recently been described in machine learning and
NLP (Chu et al, 2006; Dyer et al, 2008; Wolfe et
al., 2008). Mann et al (2009) compare this frame-
work to one in which each processor maintains a
separate parameter vector which is updated inde-
pendently of the others. At the end of learning, the
parameter vectors are averaged or a vote is taken
during prediction. A similar parameter-averaging
approach was taken by Chiang et al (2008) when
parallelizing MIRA (Crammer et al, 2006). In
this paper, we restrict our attention to distributed
frameworks which maintain and update a single
copy of the parameters ?. The use of multiple
parameter vectors is essentially orthogonal to the
framework we discuss here and we leave the inte-
gration of the two ideas for future exploration.
4 Distributed Synchronous Mini-Batch
Optimization
Distributed computing can speed up batch algo-
rithms, but we would like to transfer the well-
known speed-ups offered by online and mini-batch
algorithms to the distributed setting as well. The
simplest way to implement mini-batch stochastic
gradient descent (SGD) in a distributed computing
environment is to divide each mini-batch (rather
than the entire batch) among the processors that
are available and to update the parameters once the
gradient from the mini-batch has been computed.
Finkel et al (2008) used this approach to speed
up training of a log-linear model for parsing. The
interaction between the master processor and the
distributed computing environment is nearly iden-
tical to the distributed batch optimization scenario.
Where M (t) is the set of indices in the mini-batch
processed on iteration t, the update is:
?(t+1) ? ?(t) ? ?(t)
?
i?M(t) gi(?
(t)) (4)
The distributed synchronous framework can
provide speed-ups over a single-processor imple-
mentation of SGD, but inevitably some processors
will end up waiting for others to finish processing.
This is the same bottleneck faced by the batch ver-
sion in ?3. While the time for each mini-batch is
shorter than the time for a full batch, mini-batch
algorithms make far more updates and some pro-
cessor cycles will be wasted in computing each
one. Also, more mini-batches imply that more
time will be lost due to per-mini-batch overhead
(e.g., waiting for synchronization locks in shared-
memory systems, or sending data and ? to the pro-
cessors in systems without shared memory).
5 Distributed Asynchronous Mini-Batch
Optimization
An asynchronous framework may use multiple
processors more efficiently and minimize idle time
(Nedic et al, 2001; Langford et al, 2009). In this
setting, the master sends ? and a mini-batchMk to
each slave k. Once slave k finishes processing its
mini-batch and returns gMk(?), the master imme-
diately updates ? and sends a new mini-batch and
the new ? to the now-available slave k. As a result,
slaves stay occupied and never need to wait on oth-
ers to finish. However, nearly all gradient com-
ponents are computed using slightly stale parame-
ters that do not take into account the most recent
updates. Nedic et al (2001) proved that conver-
gence is still guaranteed under certain conditions,
and Langford et al (2009) obtained convergence
rate results. We describe these results in more de-
tail in ?5.2.
The update takes the following form:
?(t+1) ? ?(t) ? ?(t)
?
i?M(?(t)) gi(?
(?(t))) (5)
where ?(t) ? t is the start time of the mini-batch
used for the tth update. Since we started pro-
cessing the mini-batch at time ?(t) (using param-
eters ?(?(t))), we denote the mini-batch M (?(t)). If
?(t) = t, then Eq. 5 is identical to Eq. 4. That is,
t? ?(t) captures the ?staleness? of the parameters
used to compute the gradient for the tth update.
Asynchronous frameworks do introduce error
into the training procedure, but it is frequently
the case in NLP problems that only a small frac-
tion of parameters is needed for each mini-batch
215
Input: number of examples n, mini-batch size m,
random seed r
?` ? ?;
seedRandomNumberGenerator (r);
while converged (?) = false do
g ? 0;
for j ? 1 to m do
k ? Uniform({1, . . . , n});
g ? g + gk(?`);
end
acquireLock (?);
? ? updateParams (?, g);
?` ? ?;
releaseLock (?);
end
Algorithm 1: Procedure followed by each thread for multi-
core asynchronous mini-batch optimization. ? is the single
copy of the parameters shared by all threads. The conver-
gence criterion is left unspecified here.
of training examples. For example, for simple
word alignment models like IBM Model 1 (Brown
et al, 1993), only parameters corresponding to
words appearing in the particular subsample of
sentence pairs are needed. The error introduced
when making asynchronous updates should intu-
itively be less severe in these cases, where dif-
ferent mini-batches use small and mostly non-
overlapping subsets of ?.
5.1 Implementation
The algorithm sketched above is general enough
to be suitable for any distributed system, but when
using a system with shared memory (e.g., a single
multiprocessor machine) a more efficient imple-
mentation is possible. In particular, we can avoid
the master/slave architecture and simply start p
threads that each compute and execute updates in-
dependently, with a synchronization lock on ?. In
our single-machine experiments below, we use Al-
gorithm 1 for each thread. A different random seed
(r) is passed to each thread so that they do not all
process the same sequence of examples. At com-
pletion, the result is contained in ?.
5.2 Convergence Results
We now briefly summarize convergence results
from Nedic et al (2001) and Langford et al
(2009), which rely on the following assumptions:
(i) The function f is convex. (ii) The gradients
gi are bounded, i.e., there exists C > 0 such that
?gi(?
(t))? ? C. (iii) ? (unknown) D > 0 such
that t ? ?(t) < D. (iv) The stepsizes ?(t) satisfy
certain standard conditions.
In addition, Nedic et al require that all func-
tion components are used with the same asymp-
totic frequency (as t ? ?). Their results are
strongest when choosing function components in
each mini-batch using a ?cyclic? rule: select func-
tion fi for the kth time only after all functions have
been selected k ? 1 times. For a fixed step size
?, the sequence of function values f(?(t)) con-
verges to a region of the optimum that depends
on ?, the maximum norm of any gradient vector,
and the maximum delay for any mini-batch. For
a decreasing step size, convergence is guaranteed
to the optimum. When choosing components uni-
formly at random, convergence to the optimum is
again guaranteed using a decreasing step size for-
mula, but with slightly more stringent conditions
on the step size.
Langford et al (2009) present convergence rates
via regret bounds, which are linear in D. The con-
vergence rate of asynchronous stochastic gradient
descent is O(
?
TD), where T is the total number
of updates made. In addition to the situation in
which function components are chosen uniformly
at random, Langford et al provide results for sev-
eral other scenarios, including the case in which an
adversary supplies the training examples in what-
ever ordering he chooses.
Below we experiment with optimization of both
convex and non-convex functions, using fixed step
sizes and decreasing step size formulas, and con-
sider several values of D. Even when exploring
regions of the experimental space that are not yet
supported by theoretical results, asynchronous al-
gorithms perform well empirically in all settings.
5.3 Gradients and Expectation-Maximization
The theory applies when using first-order methods
to optimize convex functions. Though the function
it is optimizing is not usually convex, the EM algo-
rithm can be understood as a hillclimber that trans-
forms the gradient to keep ? feasible; it can also
be understood as a coordinate ascent algorithm.
Either way, the calculations during the E-step re-
semble g(?). Several online or mini-batch vari-
ants of the EM algorithm have been proposed, for
example incremental EM (Neal and Hinton, 1998)
and online EM (Sato and Ishii, 2000; Cappe? and
Moulines, 2009), and we follow Liang and Klein
(2009) in referring to this latter algorithm as step-
wise EM. Our experiments with asynchronous
minibatch updates include a case where the log-
likelihood f is convex and one where it is not.
216
task data n # params. eval. method convex?
?6.1 named entity
recognition (CRF;
Lafferty et al, 2001)
CoNLL 2003 English
(Tjong Kim Sang and De
Meulder, 2003)
14,987
sents.
1.3M F1 SGD yes
?6.2 word alignment (Model
1, both directions;
Brown et al, 1993)
NAACL 2003 parallel text
workshop (Mihalcea and
Pedersen, 2003)
300K
pairs
14.2M ?2
(E?F +
F?E)
AER EM yes
S6.3 unsupervised POS
(bigram HMM)
Penn Treebank ?1?21
(Marcus et al, 1993)
41,825
sents.
2,043,226 (Johnson,
2007)
EM no
Table 1: Our experiments consider three tasks.
0 2 4 6 8 10 12
84
86
88
90
Wall clock time (hours)
F1
 
 
Asynchronous (4 processors)Synchronous (4 processors)Single?processor
Figure 1: NER: Synchronous
mini-batch SGD converges faster
in F1 than the single-processor
version, and the asynchronous
version converges faster still. All
curves use a mini-batch size of 4.
6 Experiments
We performed experiments to measure speed-ups
obtainable through distributed online optimiza-
tion. Since we will be considering different opti-
mization algorithms and computing environments,
we will primarily be interested in the wall-clock
time required to obtain particular levels of perfor-
mance on metrics appropriate to each task. We
consider three tasks, detailed in Table 1.
For experiments on a single node, we used a
64-bit machine with two 2.6GHz dual-core CPUs
(i.e., 4 processors in all) with a total of 8GB of
RAM. This was a dedicated machine that was not
available for any other jobs. We also conducted
experiments using a cluster architecture running
Hadoop 0.20 (an implementation of MapReduce),
consisting of 400 machines, each having 2 quad-
core 1.86GHz CPUs with a total of 6GB of RAM.
6.1 Named Entity Recognition
Our NER CRF used a standard set of features, fol-
lowing Kazama and Torisawa (2007), along with
token shape features like those in Collins (2002)
and simple gazetteer features; a feature was in-
cluded if and only it occurred at least once in train-
ing data (total 1.3M).We used a diagonal Gaussian
prior with a variance of 1.0 for each weight.
We compared SGD on a single processor to dis-
tributed synchronous SGD and distributed asyn-
chronous SGD. For all experiments, we used a
fixed step size of 0.01 and chose each training ex-
ample for each mini-batch uniformly at random
from the full data set.3 We report performance by
3In preliminary experiments, we experimented with vari-
0 2 4 6 8 10
86
88
90
F1
 
 
Synchronous (4 processors)Synchronous (2 processors)Single?processor
0 2 4 6 8 10
86
88
90
Wall clock time (hours)
F1
 
 
Asynchronous (4 processors)
Asynchronous (2 processors)Single?processor
Figure 2: NER: (Top) Synchronous optimization improves
very little when moving from 2 to 4 processors due to the
need for load-balancing, leaving some processors idle for
stretches of time. (Bottom) Asynchronous optimization does
not require load balancing and therefore improves when mov-
ing from 2 to 4 processors because each processor is in near-
constant use. All curves use a mini-batch size of 4 and the
?Single-processor? curve is identical in the two plots.
plotting test-set accuracy against wall-time over
12 hours.4
Comparing Synchronous and Asynchronous
Algorithms Figure 1 shows our primary result
for the NER experiments. When using all four
available processors, the asynchronous algorithm
converges faster than the other two algorithms. Er-
ror due to stale parameters during gradient com-
putation does not appear to cause any more varia-
ous fixed step sizes and decreasing step size schedules, and
found a fixed step size to work best for all settings.
4Decoding was performed offline (so as not to affect mea-
surments) with models sampled every ten minutes.
217
tion in performance than experienced by the syn-
chronous mini-batch algorithm. Note that the dis-
tributed synchronous algorithm and the single-
processor algorithm make identical sequences of
parameter updates; the only difference is the
amount of time between each update. Since we
save models every ten minutes and not every ith
update, the curves have different shapes. The se-
quence of updates for the asynchronous algorithm,
on the other hand, actually depends on the vagaries
of the computational environment. Nonetheless,
the asynchronous algorithm using 4 processors has
nearly converged after only 2 hours, while the
single-processor algorithm requires 10?12 hours
to reach the same F1.
Varying the Number of Processors Figure 2
shows the improvement in convergence time by
using 4 vs. 2 processors for the synchronous (top)
and asynchronous (bottom) algorithms. The ad-
ditional two processors help the asynchronous al-
gorithm more than the synchronous one. This
highlights the key advantage of asynchronous al-
gorithms: it is easier to keep all processors in
constant use. Synchronous algorithms might be
improved through load-balancing; in our experi-
ments here, we simply assigned m/p examples to
each processor, wherem is the mini-batch size and
p is the number of processors. When m = p, as in
the 4-processor curve in the upper plot of Figure 2,
we assign a single example to each processor; this
is optimal in the sense that no other scheduling
strategy will process the mini-batch faster. There-
fore, the fact that the 2-processor and 4-processor
curves are so close suggests that the extra two pro-
cessors are not being fully exploited, indicating
that the optimal load balancing strategy for a small
mini-batch still leaves processors under-used due
to the synchronous nature of the updates.
The only bottleneck in the asynchronous algo-
rithm is the synchronization lock during updating,
required since there is only one copy of ?. For
CRFs with a few million weights, the update is
typically much faster than processing a mini-batch
of examples; furthermore, when using small mini-
batches, the update vector is typically sparse.5 For
all experimental results presented thus far, we used
a mini-batch size of 4. We experimented with ad-
5In a standard implementation, the sparsity of the update
will be nullified by regularization, but to improve efficiency
in practice the regularization penalty can be accumulated and
applied less frequently than every update.
0 2 4 6 8 10 1285
86
87
88
89
90
91
Wall clock time (hours)
F1
 
 
Asynchronous, no delay
Asynchronous, ? = 5
Single?processor, no delay
Asynchronous, ? = 10
Asynchronous, ? = 20
Figure 3: NER: Convergence curves when a delay is incurred
with probability 0.25 after each mini-batch is processed. The
delay durations (in seconds) are sampled fromN(?, (?/5)2),
for several means ?. Each mini-batch (size = 4) takes less
than a second to process, so if the delay is substantially longer
than the time required to process a mini-batch, the single-
node version converges faster. While curves with ? = 10 and
20 appear less smooth than the others, they are still heading
steadily toward convergence.
ditional mini-batch sizes of 1 and 8, but there was
very little difference in the resulting curves.
Artificial Delays We experimented with adding
artificial delays to the algorithm to explore how
much overhead would be tolerable before paral-
lelized computation becomes irrelevant. Figure 3
shows results when each processor sleeps with
0.25 probability for a duration of time between
computing the gradient on its mini-batch of data
and updating the parameters. The delay length is
chosen from a normal distribution with the means
(in seconds) shown and ? = ?/5 (truncated at
zero). Since only one quarter of the mini-batches
have an artificial delay, increasing ? increases the
average parameter ?staleness?, letting us see how
the asynchronous algorithm fares with extremely
stale parameters.
The average time required to compute the gradi-
ent for a mini-batch of 4 is 0.62 seconds. When the
average delay is 1.25 seconds (? = 5), twice the
average time for a mini-batch, the asynchronous
algorithm still converges faster than the single-
node algorithm. In addition, even with substan-
tial delays of 5?10 times the processing time for a
mini-batch, the asynchronous algorithm does not
fail but proceeds steadily toward convergence.
The practicality of using the asynchronous algo-
rithm depends on the average duration for a mini-
batch and the amount of expected additional over-
head. We attempted to run these experiments on
218
AER Time (h:m)
Single machine:
Asynch. stepwise EM 0.274 1:58
Synch. stepwise EM (4 proc.) 0.274 2:08
Synch. stepwise EM (1 proc.) 0.272 6:57
Batch EM 0.276 2:15
MapReduce:
Asynch. stepwise EM 0.281 5:41
Synch. stepwise EM 0.273 27:03
Batch EM 0.276 8:35
Table 2: Alignment error rates and wall time after 20 itera-
tions of EM for various settings. See text for details.
a large MapReduce cluster, but the overhead re-
quired for each MapReduce job was too large to
make this viable (30?60 seconds).
6.2 Word Alignment
We trained IBM Model 1 in both directions. To
align test data, we symmetrized both directional
Viterbi alignments using the ?grow-diag-final?
heuristic (Koehn et al, 2003). We evaluated our
models using alignment error rate (AER).
Experiments on a Single Machine We fol-
lowed Liang and Klein (2009) in using syn-
chronous (mini-batch) stepwise EM on a single
processor for this task. We used the same learning
rate formula (?(t) = (t+2)?q, with 0.5 < q ? 1).
We also used asynchronous stepwise EM by using
the same update rule, but gathered sufficient statis-
tics on 4 processors of a single machine in paral-
lel, analogous to our asynchronous method from
?5. Whenever a processor was done gathering the
expected counts for its mini-batch, it updated the
sufficient statistics vector and began work on the
next mini-batch.
We used the sparse update described by Liang
and Klein, which allows each thread to make
additive updates to the parameter vector and
to separately-maintained normalization constants
without needing to renormalize after each update.
When probabilities are needed during inference,
normalizers are divided out on-the-fly as needed.
We made 10 passes of asynchronous stepwise
EM to measure its sensitivity to q and the mini-
batch size m, using different values of these
hyperparameters (q ? {0.5, 0.7, 1.0}; m ?
{5000, 10000, 50000}), and selected values that
maximized log-likelihood (q = 0.7, m = 10000).
Experiments on MapReduce We implemented
the three techniques in a MapReduce framework.
We implemented batch EM on MapReduce by
converting each EM iteration into two MapRe-
duce jobs: one for the E-step and one for the M-
step.6 For the E-step, we divided our data into
24 map tasks, and computed expected counts for
the source-target parameters at each mapper. Next,
we summed up the expected counts in one reduce
task. For the M-step, we took the output from
the E-step, and in one reduce task, normalized
each source-target parameter by the total count for
the source word.7 To gather sufficient statistics
for synchronous stepwise EM, we used 6 mappers
and one reducer for a mini-batch of size 10000.
For the asynchronous version, we ran four parallel
asynchronous mini-batches, the sufficient statis-
tics being gathered using MapReduce again for
each mini-batch with 6 map tasks and one reducer.
Results Figure 4 shows log-likelihood for the
English?French direction during the first 80 min-
utes of optimization. Similar trends were observed
for the French?English direction as well as for
convergence in AER. Table 2 shows the AER at
the end of 20 iterations of EM for the same set-
tings.8 It takes around two hours to finish 20 iter-
ations of batch EM on a single machine, while it
takes more than 8 hours to do so on MapReduce.
This is because of the extra overhead of transfer-
ring ? from a master gateway machine to mappers,
from mappers to reducers, and from reducers back
to the master. Synchronous and asynchronous EM
suffer as well.
From Figure 4, we see that synchronous and
asynchronous stepwise EM converge at the same
rate when each is given 4 processors. The main
difference between this task and NER is the size
of the mini-batch used, so we experimented with
several values for the mini-batch size m. Fig-
ure 5 shows the results. As m decreases, a larger
fraction of time is spent updating parameters; this
slows observed convergence time even when us-
ing the sparse update rule. It can be seen that,
though synchronous and asynchronous stepwise
EM converge at the same rate with a large mini-
batch size (m = 10000), asynchronous stepwise
6The M-step could have been performed without MapRe-
duce by storing all the parameters in memory, but memory
restrictions on the gateway node of our cluster prevented this.
7For the reducer in the M-step, the source served as the
key, and the target appended by the parameter?s expected
count served as the value.
8Note that for wall time comparison, we sample models
every five minutes. The time taken to write these models
ranges from 30 seconds to a minute, thus artificially elon-
gating the total time for all iterations.
219
10 20 30 40 50 60 70 80
?40
?35
?30
?25
?20
Log
?Li
keli
hoo
d
 
 
Asynch. Stepwise EM (4 processors)
Synch. Stepwise EM (4 processors)
Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
10 20 30 40 50 60 70 80
?40
?35
?30
?25
?20
Wall clock time (minutes)
Log
?Li
keli
hoo
d
 
 
Asynch. Stepwise EM (MapReduce)
Synch. Stepwise EM (MapReduce)
Batch EM (MapReduce)
Figure 4: English?French log-likelihood vs. wall clock time
in minutes on both a single machine (top) and on a large
MapReduce cluster (bottom), shown on separate plots for
clarity, though axis scales are identical. We show runs of
each setting for the first 80 minutes, although EM was run
for 20 passes through the data in all cases (Table 2). Fastest
convergence is obtained by synchronous and asynchronous
stepwise EM using 4 processors on a single node. While the
algorithms converge more slowly on MapReduce due to over-
head, the asynchronous algorithm converges the fastest. We
observed similar trends for the French?English direction.
EM converges faster as m decreases. With large
mini-batches, load-balancing becomes less impor-
tant as there will be less variation in per-mini-
batch observed runtime. These results suggest that
asynchronous mini-batch algorithms will be most
useful for learning problems in which small mini-
batches work best. Fortunately, however, we do
not see any problems stemming from approxima-
tion errors due to the use of asynchronous updates.
6.3 Unsupervised POS Tagging
Our unsupervised POS experiments use the same
task and approach of Liang and Klein (2009) and
so we fix hyperparameters for stepwise EM based
on their findings (learning rate ?(t) = (t+2)?0.7).
The asynchronous algorithm uses the same learn-
ing rate formula as the single-processor algorithm.
There is only a single t that is maintained and gets
incremented whenever any thread updates the pa-
rameters. Liang and Klein used a mini-batch size
of 3, but we instead use a mini-batch size of 4 to
better suit our 4-processor synchronous and asyn-
chronous architectures.
Like NER, we present results for unsupervised
tagging experiments on a single machine only, i.e.,
not using a MapReduce cluster. For tasks like POS
tagging that have been shown to work best with
small mini-batches (Liang and Klein, 2009), we
10 20 30 40 50 60 70 80
?35
?30
?25
?20
Wall clock time (minutes)
Log
?L
ike
liho
od
 
 
Asynch. (m = 10,000)
Synch. (m = 10,000)
Asynch. (m = 1,000)
Synch. (m = 1,000)
Asynch. (m = 100)
Synch. (m = 100)
Figure 5: English?French log-likelihood vs. wall clock time
in minutes for stepwise EM with 4 processors for various
mini-batch sizes (m). The benefits of asynchronous updat-
ing increase as m decreases.
did not conduct experiments with MapReduce due
to high overhead per mini-batch.
For initialization, we followed Liang and Klein
by initializing each parameter as ?i ? e1+ai ,
ai ? Uniform([0, 1]). We generated 5 random
models using this procedure and used each to ini-
tialize each algorithm. We additionally used 2
random seeds for choosing the ordering of exam-
ples,9 resulting in a total of 10 runs for each al-
gorithm. We ran each for six hours, saving mod-
els every five minutes. After training completed,
using each model we decoded the entire training
data using posterior decoding and computed the
log-likelihood. The results for 5 initial models and
two example orderings are shown in Figure 6. We
evaluated tagging performance using many-to-1
accuracy, which is obtained by mapping the HMM
states to gold standard POS tags so as to maximize
accuracy, where multiple states can be mapped to
the same tag. This is the metric used by Liang and
Klein (2009) and Johnson (2007), who report fig-
ures comparable to ours. The asynchronous algo-
rithm converges much faster than the single-node
algorithm, allowing a tagger to be trained from
the Penn Treebank in less than two hours using
a single machine. Furthermore, the 4-processor
synchronous algorithm improves only marginally
9We ensured that the examples processed in the sequence
of mini-batches were identical for the 1-processor and 4-
processor versions of synchronous stepwise EM, but the
asynchronous algorithm requires a different seed for each
processor and, furthermore, the actual order of examples pro-
cessed depends on wall times and cannot be controlled for.
Nonetheless, we paired a distinct set of seeds for the asyn-
chronous algorithm with each of the two seeds used for the
synchronous algorithms.
220
0 1 2 3 4 5 6
?7.5
?7
?6.5
?6 x 10
6
Log
?Li
keli
hoo
d
0 1 2 3 4 5 6
50
55
60
65
Wall clock time (hours)
Acc
urac
y (%
)
 
 
Asynch. Stepwise EM (4 processors)Synch. Stepwise EM (4 processors)Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
0 1 2 3 4 5 6
?7.5
?7
?6.5
?6 x 10
6
Log
?Li
keli
hoo
d
0 1 2 3 4 5 6
50
55
60
65
Wall clock time (hours)
Acc
urac
y (%
)
 
 
Asynch. Stepwise EM (4 processors)Synch. Stepwise EM (4 processors)
Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
Figure 6: POS: Asynchronous stepwise EM converges faster in log-likelihood and accuracy than the synchronous versions.
Curves are shown for each of 5 random initial models. One example ordering random seed is shown on the left, another on the
right. The accuracy curves for batch EM do not appear because the highest accuracy reached is only 40.7% after six hours.
over the 1-processor baseline.
The accuracy of the asynchronous curves of-
ten decreases slightly after peaking. We can sur-
mise from the log-likelihood plot that the drop
in accuracy is not due to the optimization be-
ing led astray, but probably rather due to the
complex relationship between likelihood and task-
specific evaluation metrics in unsupervised learn-
ing (Merialdo, 1994). In fact, when we exam-
ined the results of synchronous stepwise EM be-
tween 6 and 12 hours of execution, we found sim-
ilar drops in accuracy as likelihood continued to
improve. From Figure 6, we conclude that the
asynchronous algorithm has no harmful effect on
learned model?s accuracy beyond the choice to op-
timize log-likelihood.
While there are currently no theoretical conver-
gence results for asynchronous optimization algo-
rithms for non-convex functions, our results are
encouraging for the prospects of establishing con-
vergence results for this setting.
7 Discussion
Our best results were obtained by exploiting mul-
tiple processors on a single machine, while exper-
iments using a MapReduce cluster were plagued
by communication and framework overhead.
Since Moore?s Law predicts a continual in-
crease in the number of cores available on a sin-
gle machine but not necessarily an increase in the
speed of those cores, we believe that algorithms
that can effectively exploit multiple processors on
a single machine will be increasingly useful. Even
today, applications in NLP involving rich-feature
structured prediction, such as parsing and transla-
tion, typically use a large portion of memory for
storing pre-computed data structures, such as lex-
icons, feature name mappings, and feature caches.
Frequently these are large enough to prevent the
multiple cores on a single machine from being
used for multiple experiments, leaving some pro-
cessors unused. However, using multiple threads
in a single program allows these large data struc-
tures to be shared and allows the threads to make
use of the additional processors.
We found the overhead incurred by the MapRe-
duce programming model, as implemented in
Hadoop 0.20, to be substantial. Nonetheless,
we found that asynchronously running multiple
MapReduce calls at the same time, rather than
pooling all processors into a single MapReduce
call, improves observed convergence with negli-
gible effects on performance.
8 Conclusion
We have presented experiments using an asyn-
chronous framework for distributed mini-batch
optimization that show comparable performance
of trained models in significantly less time than
traditional techniques. Such algorithms keep pro-
cessors in constant use and relieve the programmer
from having to implement load-balancing schemes
for each new problem encountered. We expect
asynchronous learning algorithms to be broadly
applicable to training NLP models.
Acknowledgments The authors thank Qin Gao, Garth Gib-
son, Andre? Martins, Brendan O?Connor, Stephan Vogel, and
the reviewers for insightful comments. This work was sup-
ported by awards from IBM, Google, computing resources
from Yahoo, and NSF grants 0836431 and 0844507.
221
References
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
O. Cappe? and E. Moulines. 2009. Online EM algo-
rithm for latent data models. Journal of the Royal
Statistics Society: Series B (Statistical Methodol-
ogy), 71.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proc. of EMNLP.
C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and
K. Olukotun. 2006. Map-Reduce for machine learn-
ing on multicore. In NIPS.
S. Clark and J.R. Curran. 2004. Log-linear models for
wide-coverage CCG parsing. In Proc. of EMNLP.
M. Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In Proc. of ACL.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Sixth
Symposium on Operating System Design and Imple-
mentation.
C. Dyer, A. Cordova, A. Mont, and J. Lin. 2008. Fast,
easy, and cheap: Construction of statistical machine
translation models with MapReduce. In Proc. of the
Third Workshop on Statistical Machine Translation.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In Proc. of ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
J. Kazama and K. Torisawa. 2007. A new perceptron
algorithm for sequence labeling with non-local fea-
tures. In Proc. of EMNLP-CoNLL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. Langford, A. J. Smola, and M. Zinkevich. 2009.
Slow learners are fast. In NIPS.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278?2324.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL-HLT.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models.
In NIPS.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?172.
R. Mihalcea and T. Pedersen. 2003. An evaluation
exercise for word alignment. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond.
R. Neal and G. E. Hinton. 1998. A view of the EM al-
gorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models.
A. Nedic, D. P. Bertsekas, and V. S. Borkar. 2001.
Distributed asynchronous incremental subgradient
methods. In Proc. of the March 2000 Haifa Work-
shop: Inherently Parallel Algorithms in Feasibility
and Optimization and Their Applications.
N. Ratliff, J. Bagnell, and M. Zinkevich. 2006. Sub-
gradient methods for maximum margin structured
learning. In ICML Workshop on Learning in Struc-
tured Outputs Spaces.
M. Sato and S. Ishii. 2000. On-line EM algorithm for
the normalized Gaussian network. Neural Compu-
tation, 12(2).
B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin.
2005. Learning structured prediction models: A
large margin approach. In Proc. of ICML.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
J. Wolfe, A. Haghighi, and D. Klein. 2008. Fully
distributed EM for very large datasets. In Proc. of
ICML.
222
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337?343,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The CMU-ARK German-English Translation System
Chris Dyer Kevin Gimpel Jonathan H. Clark Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{cdyer,kgimpel,jhclark,nasmith}@cs.cmu.edu
Abstract
This paper describes the German-English
translation system developed by the ARK re-
search group at Carnegie Mellon University
for the Sixth Workshop on Machine Trans-
lation (WMT11). We present the results of
several modeling and training improvements
to our core hierarchical phrase-based trans-
lation system, including: feature engineering
to improve modeling of the derivation struc-
ture of translations; better handing of OOVs;
and using development set translations into
other languages to create additional pseudo-
references for training.
1 Introduction
We describe the German-English translation system
submitted to the shared translation task in the Sixth
Workshop on Machine Translation (WMT11) by the
ARK research group at Carnegie Mellon Univer-
sity.1 The core translation system is a hierarchical
phrase-based machine translation system (Chiang,
2007) that has been extended in several ways de-
scribed in this paper.
Some of our innovations focus on modeling.
Since German and English word orders can diverge
considerably, particularly in non-matrix clauses,
we focused on feature engineering to improve the
modeling of long-distance relationships, which are
poorly captured in standard hierarchical phrase-
based translation models. To do so, we devel-
oped features that assess the goodness of the source
1http://www.ark.cs.cmu.edu
language parse tree under the translation grammar
(rather than of a ?linguistic? grammar). To train the
feature weights, we made use of a novel two-phase
training algorithm that incorporates a probabilistic
training objective and standard minimum error train-
ing (Och, 2003). These segmentation features were
supplemented with a 7-gram class-based language
model, which more directly models long-distance
relationships. Together, these features provide a
modest improvement over the baseline and suggest
interesting directions for future work. While our
work on parse modeling was involved and required
substantial changes to the training pipeline, some
other modeling enhancements were quite simple: for
example, improving how out-of-vocabulary words
are handled. We propose a very simple change, and
show that it provides a small, consistent gain.
On the training side, we had two improvements
over our baseline system. First, we were inspired
by the work of Madnani (2010), who showed that
when training to optimize BLEU (Papineni et al,
2002), overfitting is reduced by supplementing a sin-
gle human-generated reference translation with ad-
ditional computer-generated references. We gener-
ated supplementary pseudo-references for our de-
velopment set (which is translated into many lan-
guages, but once) by using MT output from a sec-
ondary Spanish-English translation system. Second,
following Foster and Kuhn (2009), we used a sec-
ondary development set to select from among many
optimization runs, which further improved general-
ization.
We largely sought techniques that did not require
language-specific resources (e.g., treebanks, POS
337
annotations, morphological analyzers). An excep-
tion is a compound segmentation model used for
preprocessing that was trained on a corpus of man-
ually segmented German. Aside from this, no fur-
ther manually annotated data was used, and we sus-
pect many of the improvements described here can
be had in other language pairs. Despite avoiding
language-specific resources and using only the train-
ing data provided by the workshop, an extensive
manual evaluation determined that the outputs pro-
duced were of significantly higher quality than both
statistical and rule-based systems that made use of
language-specific resources (Callison-Burch et al,
2011).
2 Baseline system and data
Our translation system is based on a hierarchical
phrase-based translation model (Chiang, 2007), as
implemented in the cdec decoder (Dyer et al,
2010). Since German is a language that makes
productive use of ?closed? compounds (compound
words written as a single orthographic token), we
use a CRF segmentation model of to evaluate the
probability of all possible segmentations, encoding
the most probable ones compactly in a lattice (Dyer,
2009). For the purposes of grammar induction, the
single most probable segmentation of each word in
the source side of the parallel training data under the
model was inferred.
The parallel data were aligned using the
Giza++ implementation of IBM Model 4 run
in both directions and then symmetrized using
the grow-diag-final-and heuristic (Och and
Ney, 2002; Brown et al, 1993; Koehn et al, 2003).
The aligned corpus was encoded as a suffix array
(Lopez, 2008) and lattice-specific grammars (con-
taining just the rules that are capable of matching
spans in the input lattice) were extracted for each
sentence in the test and development sets, using the
heuristics recommended by Chiang (2007).
A 4-gram modified Kneser-Ney language model
(Chen and Goodman, 1996) was constructed using
the SRI language modeling toolkit (Stolcke, 2002)
from the English side of the parallel text, the mono-
lingual English data, and the English version 4 Giga-
word corpus (Parker et al, 2009). Since there were
many duplicate segments in the training data (much
of which was crawled from the web), duplicate seg-
ments and segments longer than 100 words were re-
moved. Inference was carried out using the language
modeling library described by Heafield (2011).
The newstest-2009 set (with the 500 longest
segments removed) was used for development,2 and
newstest-2010 was used as a development test
set. Results in this paper are reported on the dev-
test set using uncased BLEU4 with a single refer-
ence translation. Minimum error rate training (Och,
2003) was used to optimize the parameters of the
system to maximize BLEU on the development data,
and inference was performed over a pruned hyper-
graph representation of the translation hypothesis
space (Kumar et al, 2009).
For the experiments reported in this paper, Viterbi
(max-derivation) decoding was used. The system
submitted for manual evaluation used segment-level
MBR decoding with 1 ? BLEU as the loss function,
approximated over a 500-best list for each sentence.
This reliably results in a small but consistent im-
provement in translation quality, but is much more
time consuming to compute (Kumar and Byrne,
2004).
3 Source parse structure modeling
Improving phrase-based translation systems is chal-
lenging in part because our intuitions about what
makes a ?good? phrase or translation derivation are
often poor. For example, restricting phrases and
rules to be consistent with syntactic constituents
consistently harms performance (Chiang, 2007; Gal-
ley et al, 2006; Koehn et al, 2003), although our
intuitions might suggest this is a reasonable thing
to do. On the other hand, it has been shown that
incorporating syntactic information in the form of
features can lead to improved performance (Chiang,
2010; Gimpel and Smith, 2009; Marton and Resnik,
2008). Syntactic features that are computed by as-
sessing the overlap of the translation parse with a
linguistic parse can be understood to improve trans-
lation because they lead to a better model of what a
?correct? parse of the source sentence is under the
translation grammar.
Like the ?soft syntactic features? used in pre-
2Removing long segments substantially reduces training
time and does not appear to negatively affect performance.
338
vious work (Marton and Resnik, 2008; Chiang et
al., 2008), we propose features to assess the tree
structure induced during translation. However, un-
like that work, we do not rely on linguistic source
parses, but instead only make use of features that
are directly computable from the source sentence
and the parse structure being considered in the de-
coder. In particular, we take inspiration from the
model of Klein and Manning (2002), which mod-
els constituency in terms of the contexts that rule
productions occur in. Additionally, we make use of
salient aspects of the spans being dominated by a
nonterminal, such as the words at the beginning and
end of the span, and the length of the span. Impor-
tantly, the features do not rely on the target words
being predicted, but only look at the structure of the
translation derivation. As such, they can be under-
stood as monolingual parse features.3
Table 1 lists the feature templates that were used.
Template Description
CTX:fi?1, fj context bigram
CTX:fi?1, fj , x context bigram + NT
CTX:fi?1, fj , x, (j ? i) context bigram + NT + len
LU:fi?1 left unigram
LB:fi?1, fi left bigram (overlapping)
RU:fj right unigram
RB:fj?1, fj right bigram (overlapping)
Table 1: Context feature templates for features extracted
from every translation rule used; i and j indicate hypothe-
sized constituent span, x is its nonterminal category label
(in our grammar, X or S), and fk is the kth word of the
source sentence, with f<1 = ?s? and f>|f| = ?/s?. If a
word fk is not among the 1000 most frequent words in
the training corpus, it is replaced by a special unknown
token. The SMALLCAPS prefixes prevent accidental fea-
ture collisions.
3.1 Two-phase discriminative learning
The parse features just introduced are numerous and
sparse, which means that MERT can not be used
to infer their weights. Instead, we require a learn-
ing algorithm that can cope with millions of fea-
tures and avoid overfitting, perhaps by eliminating
most of the features and keeping only the most valu-
able (which would also keep the model compact).
3Similar features have been proposed for use in discrimina-
tive monolingual parsing models (Taskar et al, 2004).
Furthermore, we would like to be able to still tar-
get the BLEU measure of translation quality during
learning. While large-scale discriminative training
for machine translation is a widely studied problem
(Hopkins and May, 2011; Li and Eisner, 2009; De-
vlin, 2009; Blunsom et al, 2008; Watanabe et al,
2007; Arun and Koehn, 2007; Liang et al, 2006), no
tractable algorithm exists for learning a large num-
ber of feature weights while directly optimizing a
corpus-level metric like BLEU. Rather than resorting
to a decomposable approximation, we have explored
a new two-phase training algorithm in development
of this system.
The two-phase algorithm works as follows. In
phase 1, we use a non-BLEU objective to train a
translation model that includes the large feature set.
Then, we use this model to compute a small num-
ber of coarse ?summary features,? which summa-
rize the ?opinion? of the first model about a trans-
lation hypothesis in a low dimensional space. Then,
in the second training pass, MERT is used to deter-
mine how much weight to give these summary fea-
tures together with the other standard coarse trans-
lation features. At test time, translation becomes a
multi-step process as well. The hypothesis space is
first scored using the phase-1 model, then summary
features are computed, then the hypothesis space is
rescored with the phase-2 model. As long as the fea-
tures used factor with the edges in the translation
space (which ours do), this can be carried out in lin-
ear time in the size of the translation forest.
3.1.1 Phase 1 training
For the first model, which includes the sparse parse
features, we learn weights in order to optimize pe-
nalized conditional log likelihood (Blunsom et al,
2008). We are specifically interested in modeling
an unobserved variable (i.e., the parse tree underly-
ing a translation derivation), this objective is quite
natural, since probabilistic models offer a principled
account of unobserved data. Furthermore, because
our features factor according to edges in the trans-
lation forest (they are ?stateless? in standard MT
terminology), there are efficient dynamic program-
ming algorithms that can be used to exactly compute
the expected values of the features (Lari and Young,
1990), which are necessary for computing the gradi-
ents used in optimization.
339
We are therefore optimizing the following objec-
tive, given a set T of parallel training sentences:
L = ?R(?)?
?
?f,e??T
log
?
d
p?(e,d | f)
where p?(e,d | f) =
exp ?>h(f, e,d)
Z(f)
,
where d is a variable representing the unobserved
synchronous parses giving rise to the pair of sen-
tences ?f, e?, and where R(?) is a penalty that favors
less complex models. Since we not only want to pre-
vent over fitting but also want a small model, we use
R(?) =
?
k |?k|, the `1 norm, which forces many
parameters to be exactly 0.
Although L is not convex in ? (on account of the
latent derivation variable), we make use of an on-
line stochastic gradient descent algorithm that im-
poses an `1 penalty on the objective (Tsuruoka et
al., 2009). Online algorithms are often effective for
non-convex objectives (Liang and Klein, 2009).
We selected 12,500 sentences randomly from the
news-commentary portion of the training data to use
to train the latent variable model. Using the stan-
dard rule extraction heuristics (Chiang, 2007), 9,967
of the sentence pairs could be derived.4 In addition
to the parse features describe above, the standard
phrase features (relative frequency and lexical trans-
lation probabilities), and a rule count feature were
included. Training was run for 48 hours on a sin-
gle machine, which resulted in 8 passes through the
training data, instantiating over 8M unique features.
The regularization strength ? was chosen so that ap-
proximately 10, 000 (of the 8M) features would be
non-zero.5
3.1.2 Summary features
As outlined above, the phase 1 model will be incor-
porated into the final translation model using a low
dimensional ?summary? of its opinion. Because we
are using a probabilistic model, posterior probabili-
ties (given the source sentence f) under the parsing
4When optimizing conditional log likeligood, it is necessary
to be able to exactly derive the training pair. See Blunsom et al
(2008) for more information.
5Ideally, ? would have been tuned to optimize held-out like-
lihood or BLEU; however, the evaluation deadline prevented us
from doing this.
model are easily defined and straightforward to com-
pute with dynamic programming. We made use of
four summary features: the posterior log probability
log p?(e,d|f); for every rule r ? d, the probability of
its span being a constituent under the parse model;
the probabilities that some span starts at the r?s start-
ing index, or that some rule ends at r?s ending index.
Once these summary features have been com-
puted, the sparse features are discarded, and the
summary features are reweighted using coefficients
learned by MERT, together with the standard MT
features (language model, word penalty, etc.). This
provides a small improvement over our already very
strong baseline, as the first two rows in Table 2 show.
Condition BLEU
baseline 25.0
+ parse features 25.2
+ parse features + 7-gram LM 25.4
Table 2: Additional features designed to improve model
of long-range reordering.
3.2 7-gram class-based LM
The parsing features above were intended to im-
prove long range reordering quality. To further sup-
port the modeling of larger spans, we incorporated
a 7-gram class-based language model. Automatic
word clusters are attractive because they can be
learned for any language without supervised data,
and, unlike part-of-speech annotations, each word
is in only a single class, which simplifies inference.
We performed Brown clustering (Brown et al, 1992)
on 900k sentences from our language modeling data
(including the news commentary corpus and a sub-
set of Gigaword). We obtained 1,000 clusters us-
ing an implementation provided by Liang (2005),6
as Turian et al (2010) found that relatively large
numbers clusters gave better performance for infor-
mation extraction tasks. We then replaced words
with their clusters in our language modeling data
and built a 7-gram LM with Witten-Bell smoothing
(Witten and Bell, 1991).7 The last two rows of Ta-
6http://www.cs.berkeley.edu/?pliang/
software
7The distributional assumptions made by the more com-
monly used Kneser-Ney estimator do not hold in the word-
340
ble 2 shows that in conjunction with the source parse
features, a slight improvement comes from includ-
ing the 7-gram LM.
4 Non-translating tokens
When two languages share a common alphabet (as
German and English largely do), it is often appro-
priate to leave some tokens untranslated when trans-
lating. Named entities, numbers, and graphical el-
ements such as emoticons are a few common ex-
amples of such ?non-translating? elements. To en-
sure that such elements are well-modeled, we aug-
ment our translation grammar so that every token
in the input can translate as itself and add a feature
that counts the number of times such self-translation
rules are used in a translation hypothesis. This is in
contrast to the behavior of most other decoders, such
as Moses, which only permit a token to translate as
itself if it is learned from the training data, or if there
is no translation in the phrase table at all.
Since many non-translating tokens are out-of-
vocabulary (OOV) in the target LM, we also add
a feature that fires each time the LM encounters a
word that is OOV.8 This behavior be understood as
discriminatively learning the unknown word penalty
that is part of the LM. Again, this is in contrast to
the behavior of other decoders, which typically add
a fixed (and very large) cost to the LM feature for
every OOV. Our multi-feature parameterization per-
mits the training algorithm to decide that, e.g., some
OOVs are acceptable if they occur in a ?good? con-
text rather than forcing the decoder to avoid them
at all costs. Table 3 shows that always providing
a non-translating translation option together with a
discriminative learned OOV feature improves the
quality of German-English translation.9
Condition BLEU
?OOV (baseline) 24.6
+OOV and non-translating rules 25.0
Table 3: Effect of discriminatively learned penalties for
OOV words.
classified corpus.
8When multiple LMs are used, there is an extra OOV feature
for each LM.
9Both systems were trained using the human+ES-EN refer-
ence set described below (?5).
5 Computer-generated references
Madnani (2010) shows that models learned by op-
timizing BLEU are liable to overfit if only a sin-
gle reference is used, but that this overfitting can
be mitigated by supplementing the single reference
with supplemental computer-generated references
produced by paraphrasing the human reference us-
ing a whole-sentence statistical paraphrase system.
These computer-generated paraphrases are just used
to compute ?better? BLEU scores, but not directly as
examples of target translations.
Although we did not have access to a paraphrase
generator, we took advantage of the fact that our de-
velopment set (newstest-2009) was translated
into several languages other than English. By trans-
lating these back into English, we hypothesized we
would get suitable pseudo-references that could be
used in place of computer-generated paraphrases.
Table 4 shows the results obtained on our held-out
test set simply by altering the reference translations
used to score the development data. These systems
all contain the OOV features described above.
Condition BLEU
1 human 24.7
1 human + ES-EN 25.0
1 human + FR-EN 24.0
1 human + ES-EN + FR-EN 24.2
Table 4: Effect of different sets of reference translations
used during tuning.
While the effect is somewhat smaller than Mad-
nani (2010) reports using a sentential paraphraser,
the extremely simple technique of adding the output
of a Spanish-English (ES-EN) system was found to
consistently improve the quality of the translations
of the held-out data. However, a comparable effect
was not found when using references generated from
a French-English (FR-EN) translation system, indi-
cating that the utility of this technique must be as-
sessed empirically and depends on several factors.
6 Case restoration
Our translation system generates lowercased out-
put, so we must restore case as a post-processing
step. We do so using a probabilistic transducer as
implemented in SRILM?s disambig tool. Each
341
lowercase token in the input can be mapped to a
cased variant that was observed in the target lan-
guage training data. Ambiguities are resolved us-
ing a language model that predicts true-cased sen-
tences.10 We used the same data sources to con-
struct this model as were used above. During devel-
opment, it was observed that many named entities
that did not require translation required some case
change, from simple uppercasing of the first letter,
to more idiosyncratic casings (e.g., iPod). To ensure
that these were properly restored, even when they
did not occur in the target language training data, we
supplement the true-cased LM training data and case
transducer training data with the German source test
set.
Condition BLEU (Cased)
English-only 24.1
English+test-set 24.3
Table 5: Effect of supplementing recasing model training
data with the test set source.
7 Model selection
Minimum error rate training (Och, 2003) is a
stochastic optimization algorithm that typically finds
a different weight vector each time it is run. Foster
and Kuhn (2009) showed that while the variance on
the development set objective may be narrow, the
held-out test set variance is typically much greater,
but that a secondary development set can be used to
select a system that will have better generalization.
We therefore replicated MERT 6 times and selected
the output that performed best on NEWSTEST-2010.
Since we had no additional blind test set, we can-
not measure what the impact is. However, the BLEU
scores we selected on varied from 25.4 to 26.1.
8 Summary
We have presented a summary of the enhancements
made to a hierarchical phrase-based translation sys-
tem for the WMT11 shared translation task. Some
of our results are still preliminary (the source parse
10The model used is p(y | x)p(y). While this model is some-
what unusual (the conditional probability is backwards from a
noisy channel model), it is a standard and effective technique
for case restoration.
model), but a number of changes we made were
quite simple (OOV handling, using MT output to
provide additional references for training) and also
led to improved results.
Acknowledgments
This research was supported in part by the NSF through
grant IIS-0844507, the U. S. Army Research Laboratory
and the U. S. Army Research Office under contract/grant
number W911NF-10-1-0533, and Sandia National Labo-
ratories (fellowship to K. Gimpel). We thank the anony-
mous reviewers for their thorough feedback.
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18:467?479.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of the Sixth Workshop
on Statistical Machine Translation.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL, pages 310?318.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. EMNLP, pages 224?233.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
D. Chiang. 2010. Learning to translate with source and
target syntax. In Proc. of ACL, pages 1443?1452.
J. Devlin. 2009. Lexical features for statistical machine
translation. Master?s thesis, University of Maryland.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
342
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of ACL, pages 961?968.
K. Gimpel and N. A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP, pages 219?228.
K. Heafield. 2011. KenLM: Faster and smaller language
model queries. In Proc. of the Sixth Workshop on Sta-
tistical Machine Translation.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of ACL, pages 128?135.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. of EMNLP,
pages 40?51.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
N. Madnani. 2010. The Circle of Meaning: From Trans-
lation to Paraphrasing and Back. Ph.D. thesis, De-
partment of Computer Science, University of Mary-
land College Park.
Y. Marton and P. Resnik. 2008. Soft syntactic constraints
for hierarchical phrased-based translation. In Proc. of
ACL, pages 1003?1011, Columbus, Ohio.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2009. English gigaword fourth edition.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Intl. Conf. on Spoken Language Pro-
cessing.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proc. of ACL, pages 384?394.
T. Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Trans. Informa-
tion Theory, 37(4).
343
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 512?522,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Generative Models of Monolingual and Bilingual Gappy Patterns
Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
A growing body of machine translation re-
search aims to exploit lexical patterns (e.g., n-
grams and phrase pairs) with gaps (Simard et
al., 2005; Chiang, 2005; Xiong et al, 2011).
Typically, these ?gappy patterns? are discov-
ered using heuristics based on word align-
ments or local statistics such as mutual infor-
mation. In this paper, we develop generative
models of monolingual and parallel text that
build sentences using gappy patterns of arbi-
trary length and with arbitrarily many gaps.
We exploit Bayesian nonparametrics and col-
lapsed Gibbs sampling to discover salient pat-
terns in a corpus. We evaluate the patterns
qualitatively and also add them as features to
an MT system, reporting promising prelimi-
nary results.
1 Introduction
Beginning with the success of phrase-based transla-
tion models (Koehn et al, 2003), a trend arose of
modeling larger and increasingly complex structural
units in translation. One thread of work has focused
on the use of lexical patterns with gaps. Simard et
al. (2005) proposed using phrase pairs with gaps in a
phrase-based translation model, providing a heuris-
tic method to extract gappy phrase pairs from word-
aligned parallel corpora. The widely-used hierarchi-
cal phrase-based translation framework was intro-
duced by Chiang (2005) and also relies on a simple
heuristic for phrase pair extraction. On the mono-
lingual side, researchers have taken inspiration from
trigger-based language modeling for speech recog-
nition (Rosenfeld, 1996). Recently Xiong et al
(2011) used monolingual trigger pairs to improve
handling of long-distance dependencies in machine
translation output.
All of this previous work used heuristics or local
statistical tests to extract patterns from corpora. In
this paper, we present probabilistic models that gen-
erate text using gappy patterns of arbitrary length
and with arbitrarily-many gaps. We exploit non-
parametric priors and use Bayesian inference to dis-
cover the most salient gappy patterns in monolin-
gual and parallel text. We first inspect these pat-
terns manually and discuss the categories of phe-
nomena that they capture. We also add them as
features in a discriminatively-trained phrase-based
MT system, using standard techniques to train their
weights (Arun and Koehn, 2007; Watanabe et al,
2007) and incorporate them during decoding (Chi-
ang, 2007). We present experiments for Spanish-
English and Chinese-English translation, reporting
encouraging preliminary results.
2 Related Work
There is a rich history of trigger-based language
modeling in the speech recognition community, typ-
ically involving the use of statistical tests to discover
useful trigger-word pairs (Rosenfeld, 1996; Jelinek,
1997). Xiong et al (2011) used Rosenfeld?s mutual
information procedure to discover trigger pairs and
added a single feature to a phrase-based MT system
that scores new words based on all potential trig-
gers from previous parts of the derivation. We are
not aware of prior work that uses generative model-
ing and Bayesian nonparametrics to discover these
same types of patterns automatically; doing so al-
lows us to discover larger patterns with more words
and gaps if they are warranted by the data.
In addition to the gappy phrase-based (Simard et
al., 2005) and hierarchical phrase-based (Chiang,
2005) models mentioned earlier, other researchers
have explored the use of bilingual gappy structures
for machine translation. Crego and Yvon (2009) and
512
?(  ) = .
?(  ) = baltic states
it provides either too little or too much .
it 's neither particularly complicated nor novel .
nato must either say " yes " or " no " to the baltic states .
good scientific ideas formulated in bad english either die or get repackaged .
nato must either say " yes " or " no " to the baltic states .
?(  ) = either __ or
???pi????????(  ) = either __ or
?(  ) = to the
?(  ) = " __ " __ " __ "?(  ) = must
?(  ) = yes __ no
?(  ) = say?(  ) = nato
Figure 1: A sentence from the news commentary cor-
pus, along with color assignments for the words and the
pi function for each color.
Galley and Manning (2010) proposed ways of incor-
porating phrase pairs with gaps into standard left-to-
right decoding algorithms familiar to phrase-based
and N -gram-based MT; both used heuristics to ex-
tract phrase pairs. Bansal et al (2011) presented a
model and training procedure for word alignment
that uses phrase pairs with gaps. They use a semi-
Markov model with an enlarged dynamic program-
ming state in order to represent alignment between
gappy phrases. Their model permits up to one gap
per phrase while our models permit an arbitrary
number.
3 Monolingual Pattern Models
We first present a model that generates a sentence as
a set of lexical items that we will refer to as gappy
patterns, or simply patterns. A pattern is defined as
a sequence containing elements of two types: words
and gaps. All patterns must obey the regular expres-
sion w+( w+)*, where w is a word and is a gap.
That is, patterns must begin and end with words and
may not contain consecutive gaps.
We assume that we have an n-word sentence
w1:n.1 We represent patterns in a sentence by as-
sociating each word with a color. To do so, we in-
troduce a vector of color assignment variables c1:n,
with one for each word. We represent a color Cj as
a set in terms of the ci variables: Cj = {i : ci = j}.
Each color corresponds to a pattern that is obtained
by concatenating its words from left to right in the
sentence, inserting gaps when necessary. We denote
the pattern for a color Cj by pi(Cj); Figure 1 shows
examples of the correspondence between colors and
patterns.
The generative story for a single sentence follows:
1We use boldface lowercase letters to denote vectors (e.g.,
f ), denote entry i as fi, and denote the range from i to j as
f i:j .
1. Sample the number of words: n ? Poisson(?)
2. Sample the number of unique colors in the sen-
tence given n: m ? Uniform(1, n)
3. For each word index i = 1 . . . n, sample the color
of word i: ci ? Uniform(1,m). If any of the m
colors has no words, repeat this step.
4. For each color j = 1 . . .m, sample from a
multinomial distribution over patterns: wCj ?
Mult(?). If the words wCj are not consistent
with the color assignments, i.e., wrong number of
words or gaps, gaps not in the correct locations,
repeat this step.
Thus, the probability of generating number of words
n, words w1:n, color assignments c1:n, and number
of colors m is
p(w1:n, c1:n,m | ?, ?)
=
1
Z
(
?n
n!
e??
)(
1
n
)(
1
m
)n m?
j=1
p?(pi(Cj))
(1)
where Z is a normalization constant required by the
potential repetition of sampling in the final two steps
of the generative story. Without Z, the model would
be deficient as we would waste probability mass on
internally inconsistent color assignments.
The core of the model is a single multinomial
distribution p?(?) over patterns. We use a Dirich-
let process (DP) prior for this multinomial so that
we can model an unbounded set of patterns: ? ?
DP(?, P0), where ? is the concentration parameter
and P0 is the base distribution. The base distribution
includes a Poisson(?) over the number of words in
the pattern, a uniform distribution (over word types
in the vocabulary) for each word, a uniform distri-
bution over the number of gaps given the number of
words, and a uniform distribution over the arrange-
ment of gaps given the numbers of gaps and words.2
Inference We use collapsed Gibbs sampling
for inference. Our goal is to obtain samples
from the posterior distribution p({c(i),m(i)}Si=1 |
{w(i)}Si=1, ?, ?), where S is the total number of sen-
tences in the corpus and ? is marginalized out.3
2The number of ways of arranging y gaps among x words is
?(x? 1) choose y?.
3Since we assume the words are given, ? is irrelevant.
513
During each iteration of Gibbs sampling, we pro-
ceed through the corpus and sample a new value for
each ci variable conditioned on the values of all oth-
ers in the corpus. Them variables are determined by
the ci variables and therefore do not need to be sam-
pled directly. When sampling ci, we first remove
ci from the corpus (and its color if the color only
contained i). Where the remaining colors in the sen-
tence are numbered from 1 to m, there are m + 1
possibilities for ci: m for each of the existing colors
and one for choosing a new color.
Since choosing a new color corresponds to creat-
ing a new instance of the pattern pi({i}), the proba-
bility of choosing a new color m+ 1 is proportional
to
#pi({i}) + ?P0(pi({i}))
# + ?
(2)
where #pi is the count of pattern pi in the rest of the
sentence and all other sentences in the corpus, and
# is the total count of all patterns in this same set.
The probability of choosing the existing color j (for
1 ? j ? m) is proportional to
#pi(Cj?{i}) + ?P0(pi(Cj ? {i}))
#pi(Cj) + ?P0(pi(Cj))
(3)
where the denominator encodes the fact that the
move will cause an instance of the pattern for the
color Cj to be removed from the corpus as the new
pattern for Cj ? {i} is added.
We note that, even though these two types of
moves will result in different numbers of colors (m)
in the sentence, we do not have to include a term for
this in the sampler because we use a uniform dis-
tribution for m and therefore all (valid) numbers of
colors have the same probability. The normalization
constant Z in Equation 1 does not affect inference
because our sampler is designed to only consider
valid (i.e., internally consistent) settings for the c(i)
and m(i) variables.
This model makes few assumptions, using uni-
form distributions whenever possible. This simpli-
fies inference and causes the resulting lexicon to be
influenced primarily by the ?rich-get-richer? effect
of the DP prior. Despite its simplicity, we will show
later that this model discovers patterns that capture
a variety of linguistic phenomena.
?(  ) = .
?(  ) = baltic states
it provides either too little or too much .
it 's neither particularly complicated nor novel .
nato must either say " yes " or " no " to the baltic states .
good scientific ideas formulated in bad english either die or get repackaged .
nato must either say " yes " or " no " to the baltic states .
la otan tiene que decir " s? " o " no " a los pa?ses b?lticos .
?(  ) = either      or
???pi????????(  ) = either __ or
?(  ) = to the
?(  ) =
"      "
?(  ) = must
?(  ) = yes __ no
?(  ) = say?(  ) = nato
?(  ) = 
nato
otan ?(  ) =
to the
13-12 15-13 16-15
o " " a
Figure 2: A Spanish-English sentence pair with the in-
tersection of automatic word alignments in each direc-
tion. Some source words accept the colors of target words
aligned to them while others (light gray) do not. Bilingual
patterns for a few colors are shown.
4 Bilingual Pattern Models
We now present a generative model for a sentence
pair that will enable us to discover bilingual pat-
terns. In this section we present one example of ex-
tending th previous model to be bilingu l, but we
note that many other extensions are possible; indeed,
flexibility is one of the key advantages of working
within the framework of probabilistic modeling.
We assume that we are given sentence pairs and
one-to-one word alignments. That is, in addition to
an n-word target sentencew1:n, we assume we have
an n?-word source sentence w?1:n? and word align-
ments a1:n? where ai = j iff w?i is aligned to wj and
ai = 0 if w?i is aligned to null.
To model bilingual patterns, we distinguish
source colors from target colors. A target-language
word can only be colored with a target color, but
a source word can be colored with either a source
color or with the target color of the target word it
is aligned to (if any). We have m target colors as
before and now add m? source colors. We intro-
duce additional random variables in the form of a
binary vector g of length n? that indicates, for each
source word, whether or not it accepts the color of
its aligned target word. We introduce an additional
parameter ? for the probability that a source word
will accept the color of its aligned word. We fix its
value to 0.5 and do not learn it during inference. Fig-
ure 2 shows an example Spanish-English sentence
pair with automatic word alignments and color as-
signments. The bilingual patterns for a few target
colors are shown.
The generative story for a sentence pair follows:
1. Sample the numbers of words in the source and
target sentences: n?, n ? Poisson(?)
514
2. Sample the numbers of source and target col-
ors given n?, n: m? ? Uniform(1, n?),m ?
Uniform(1, n)
3. Sample the alignment vector from any distribu-
tion that ensures links are 1-to-1:4 a1:n? ? p(a)
4. For each target word index i = 1 . . . n, sample
the color of target word i from a uniform distribu-
tion over all target colors: ci ? Uniform(1,m).
While any of the m colors has no words, repeat
this step.
5. For each source word index i = 1 . . . n?:
1. Decide whether to use a source color or to use
the target color of the aligned target word: gi ?
p?(gi | ai)
2. If gi = 1, set c?i = cai ; otherwise, sample a
source color: c?i ? Uniform(1,m
?)
6. If any source color has no words, repeat Step 5.
7. For each source color j = 1 . . .m?:
1. Sample from a multinomial over source pat-
terns: wC?j ? Mult(?
?). While the words wC?j
are not consistent with the color assignments,
repeat this step.
8. For each target color j = 1 . . .m:
1. Sample from a multinomial over bilingual pat-
terns: wCj ? Mult(?). While the words wCj
are not consistent with the color assignments,
repeat this step.
The distribution p?(gi | ai) is defined below:
p?(gi = 1 | ai 6= ?1) = ?
p?(gi = 1 | ai = ?1) = 0
where ? determines how frequently source tokens
will be added to target patterns.
The probability of generating target words w1:n,
source words w?1:n? , alignments a1:n? , target color
assignments c1:n, source color assignments c?1:n? ,
color propagation variables g1:n? , number of target
4Since we assume alignments are provided during inference,
it does not matter what distribution is used, so long as only 1-
to-1 links are permitted.
colors m, and number of source colors m? is
1
Z
p(n)p(n?)p(m | n)p(m? | n?)p(a1:n?)
?
(
n?
i=1
p(ci | m)
)
?
(
n??
i=1
p?(gi | ai)p(c
?
i | m
?)I[gi==0]
)
?
?
?
m??
j=1
p??(pi(C
?
j))
?
?
?
?
m?
j=1
p?(pi(Cj))
?
?
where Z again serves as a normalization constant to
prevent the model from leaking probability mass on
internally inconsistent configurations.
There are now two multinomial distributions over
patterns with parameter vectors ? and ??. They both
use DP priors with identical concentration param-
eters ? and differing base distributions P0 and P ?0.
The base distribution for source patterns, P ?0, takes
the same form as the base distribution for the model
described in ?3.
For target patterns with aligned source words, P0
generates the target part of the pattern like the base
distribution in ?3 and then generates the number
of aligned source words to each target word with
a Poisson(1) distribution; the number of aligned
source words can only be 0 or 1 when all word links
are 1-to-1. If it is 1, the base distribution generates
the aligned source word by sampling uniformly from
among all source types.
While there are connections between this model
and work on performing translation using phrase
pairs with gaps, the patterns we discover are not
guaranteed to be bilingual translation units. Rather,
they typically contain additional target-side words
that have no explicit correlate on the source side.
They can be used to assist an existing translation
model by helping to choose the best phrase trans-
lation for each source phrase. To define a genera-
tive model for phrase pairs with gaps, changes would
have to be made to the bilingual model we presented.
Inference As before, we use collapsed Gibbs sam-
pling for inference. Our goal is to obtain sam-
ples from the posterior p({?c, c?, g,m,m??(i)}Si=1 |
{?w,w?,a?(i)}Si=1).
515
We go through each sentence pair and sample new
color assignment variables for each word. For an
aligned word pair (w?i, wj), we sample a new value
for the tuple (gi, c?i, cj). The possible values for
cj include all target colors, including a new target
color. The possible values for gi are 0, in which case
c?i can be any of the source colors, including a new
source color, and 1, for which c?i must be cj . For an
unaligned target word wj , cj can be any target color,
including a new one, and for an unaligned source
word w?i, c
?
i can be any source color, including a new
one. The full equations for sampling can be easily
derived using the equations from ?3.
5 Evaluation
We conducted evaluation to determine (1) what
types of phenomena are captured by the most prob-
able patterns discovered by our models, and (2)
whether including the patterns as features can im-
prove translation quality.
5.1 Qualitative Evaluation
5.1.1 Monolingual Model
Since inference is computationally expensive,
we used the 126K-sentence English news com-
mentary corpus provided for the WMT shared
tasks (Callison-Burch et al, 2010). We ran Gibbs
sampling for 600 iterations through the data, dis-
carding the first 300 samples for burn-in and com-
puting statistics of the patterns using the remaining
300 samples. Each iteration took approximately 3
minutes on a single 2.2GHz CPU. When looking pri-
marily at the most frequent patterns, we found that
this list did not vary much when only using half of
the data instead. We set ? = 3 and ? = 100; we
found these hyperparameters to have only minor ef-
fects on the results.
Since many frequent patterns include the period
(.), we found it useful to constrain the model to treat
this token differently: we modify the base distribu-
tion so that it assigns zero probability to patterns
that contain a period along with other words and we
force each occurrence of a period to be alone in its
own pattern during initialization. We do not need to
change the inference procedure at all; with the mod-
ified base distribution and with no patterns including
a period with other words, the probability of creat-
" " as as " " " "
? ? the of in why ?
( ) the is , the of
the of not only but from to
, , , it is that the between and
the ( ) of " " such as ,
both and not , but either or
the of and in , in but is
more than the of , " " the
- - what ? has been
, " " between and in , ,
the " " the of ?s an of
Table 1: Top-ranked gappy patterns from samples accord-
ing to p(pi); patterns without gaps are omitted. The spe-
cial string ? ? represents a gap that can be filled by any
nonempty sequence of words.
ing a new illegal pattern during inference is always
zero (Eq. 3).
We also perform inference on a transformed ver-
sion of the corpus in which every word is replaced
with its hard word class obtained from Brown clus-
tering (Brown et al, 1992). One property of Brown
clusters is that each function word effectively re-
ceives its own class, as each ends up in a cluster in
which it occupies ?95% of the token counts of all
types in the cluster. We call clusters that satisfy this
property singleton clusters.
To obtain Brown clusters for the source and tar-
get languages, we used code from Liang (2005).5
We used the data from the news commentary cor-
pus along with the first 500K sentences of the addi-
tional monolingual newswire data also provided for
the WMT shared tasks. We used 300 clusters, ig-
noring words that appeared only once in this corpus.
We did not use the hierarchical information from the
clusters but merely converted each cluster name into
a unique integer, using one additional integer for un-
known words.
We used the same values for ? and ? as above
but ran Gibbs sampling for 1,300 iterations, again
using the last 300 for collecting statistics on pat-
terns. Judging by the number of color assignments
changed on each iteration, the sampler takes longer
to converge when run on word clusters than on
words. As above, we constrain the singleton word
cluster corresponding to the period to be alone dur-
ing both initialization and inference.
5http://www.cs.berkeley.edu/?pliang/
software
516
academy sciences regulators supervisors
beijing shanghai sine non
booms busts stalin mao
council advisers treasury secretary geithner
dominicans haitian sooner later
flemish walloons first foremost
gref program played role
heat droughts down road
humanitarian displaced freedom expression
karnofsky hassenfeld at disposal
kazakhstan kyrgyzstan take granted
portugal greece - -
Table 2: Gappy patterns with highest conditional proba-
bility p(pi|w(pi)).
? ? whether or france germany
( ) around world he his
- - has been allow to
both and how ? for first time
not only but the ( ) china india
" " on basis what do
more than less than we our
either or on other hand over past
why ? at level prevent from
neither nor it is that in way
what ? not , but one another
rule law play role political economic
Table 3: Top-ranked gappy patterns according to
p(pi)p(pi|w(pi)).
Pattern Ranking Statistics Several choices exist
for ranking patterns. The simplest is to take the pat-
tern count from the posterior samples, averaged over
all sampling iterations after burn-in. We refer to this
criterion as the marginal probability:
p(pi) =
#pi
#
where #pi is the average count of the pattern across
the posterior samples and # is the count of all pat-
terns. The top-ranked gappy patterns under this cri-
terion are shown in Table 1. While many of these
patterns match our intuitions, there are also sev-
eral that are highly-ranked simply because their con-
stituent words are frequent.
Alternatively, we can rank patterns by the con-
ditional probability of the pattern given the words
that comprise it:
p(pi|w(pi)) =
#pi
#w(pi)
where w(pi) returns the sequence of words in the
pattern pi and #w(pi) is the number of occurrences
of this sequence of words in the corpus that are com-
patible with pattern pi. The ranking of patterns under
this criterion is shown in Table 2. This method fa-
vors precision but also causes very rare patterns to
be highly ranked.
To address this, we also consider a product-of-
experts model by simply multiplying together the
two probabilities, resulting in the ranking shown in
Table 3. This ranking is similar to that in Table 1
but penalizes patterns that are only ranked highly be-
cause they consist of common words. Table 4 shows
a manual grouping of these highly-ranked patterns
into several categories. We show both lexical and
Brown cluster patterns.6
It is common in both types of patterns to find
long-distance dependencies involving punctuation
near the top of the ranking. Among agreement pat-
terns, the lexical model finds relationships between
pronouns and their associated possessive adjectives
while the cluster model finds more general patterns
involving classes of nouns. Cluster patterns are more
likely to capture topicality within a sentence, while
the finer granularity of the lexical model is required
to identify constructions like those shown (verbs
triggering particular prepositions).
There are also many probable patterns without
gaps, shown at the bottom of Table 4. From these
patterns we can see that our models can also be used
to find collocations, but we note that these are dis-
covered in the context of the gappy patterns. That
is, due to the use of latent variables in our models
(the color assignments), there is a natural trading-off
effect whereby the gappy patterns encourage partic-
ular non-gappy patterns to be used, and vice versa.
5.1.2 Bilingual Model
We use the news commentary corpus for each lan-
guage and take the intersection of GIZA++ (Och
and Ney, 2003) word alignments in each direction,
thereby ensuring that they are 1-to-1 alignments. We
ran Gibbs sampling for 300 iterations, averaging pat-
tern counts from the last 200. We set ? = 100,
? = 3, and ? = 0.5. We ran the model in 3 con-
ditions: source words, target words; source clusters,
target clusters; and source clusters, target words. We
6We filter Brown cluster patterns in which every cluster is
a singleton, since these patterns are typically already accounted
for in the lexical patterns.
517
Rank Gappy Lexical Patterns Rank Gappy Brown Cluster Patterns
P
un
ct
ua
ti
on
1 -- -- 2 {what, why, whom, whatever} {?, !}
2 ( ) 6 {--, -, ?} {--, -, ?}
6 " " 28 {according, compared, subscribe, thanks, referring} to ,
9 why ? 178 {?, -, ?} {even, especially, particularly, mostly, mainly} {?, -, ?}
63 according to , 239 {obama, bush, clinton, mccain, brown} " "
A
gr
ee
m
en
t
26 he his 8 {people, things, americans, journalists, europeans} their
31 we our 12 we {our, my}
46 his his 21 {children, women, others, men, students} their
86 china its 23 {china, europe, america, russia, iran} ?s its
90 his he 43 {obama, bush, clinton, mccain, brown} his
99 you your 46 {our, my} {our, my}
136 leaders their 149 {people, things, americans, journalists, europeans} they
140 we ourselves 172 {president, bill, sen., king, senator} {obama, bush, clinton, mccain, brown} his
165 these are 180 {all, both, either} {countries, companies, banks, groups, issues}
C
on
ne
ct
iv
es
4 both and 5 {more, less} {more, less}
5 not only but 9 if , {will, would, could, should, might}
8 either or 19 {deal, plan, vote, decision, talks} {against, between, involving} and
10 neither nor 40 a {against, between, involving} and
13 whether or 45 {better, different, further, higher, lower} than
19 less than 50 {much, far, slightly, significantly, substantially} than
23 not , but 56 {yet, instead, perhaps, thus, neither} but
54 if then 68 not {only, necessarily} {also, hardly}
109 between and 98 as {much, far, slightly, significantly, substantially} as
192 relationship between and 131 is {more, less} than
To
pi
ca
li
ty
25 france germany 1 ?UNK? ?UNK?
29 china india 15 {china, europe, . . .} ?s {system, crisis, program, recession, situation}
36 political economic 30 {health, security, defense, safety, intelligence} {health, . . .}
43 rich poor 47 {china, europe, . . .} {china, europe, . . .} {china, europe, . . .}
50 oil gas 62 {power, growth, interest, development} {10, 1, 20, 30, 2} {percent, %, p.m., a.m.}
62 billions dollars 72 in {iraq, washington, london, 2008, 2009} {iraq, washington, london, 2008, 2009}
96 economic social 73 the {end, cost, head, rules, average} of {prices, markets, services, problems, costs}
106 the us europe 113 {china, europe, . . .} ?s {economy, election, elections, population, investigation}
181 public private 119 {prices, markets, . . .} {oil, energy, tax, food, investment} {oil, energy, . . .}
P
re
po
si
ti
on
s
14 around world 14 for {first, second, third, final, whole} {time, period, term, class, avenue}
18 on basis 17 in {last, next, 20th} {year, week, month, season, summer}
38 at time 51 at {end, cost, head, rules, average} of
42 in region 71 at {group, rate, leader, level, manager}
80 in manner 112 for {times, points, games, goals, reasons}
85 at expense 126 {over, around, across, behind, above} {country, company, region, nation, virus}
112 during period 190 {one, none} of {best, top, largest, main, biggest}
C
on
st
ru
ct
io
ns 33 prevent from
84 enable to
114 provide for
123 impose on
177 turn into
Non-Gappy Lexical Patterns Non-Gappy Brown Cluster Patterns
as well their own as {well, soon, quickly, seriously, slowly} as {rather, please} than
the united states prime minister the united {states, nations, airlines} {don, didn, doesn, isn, wasn} ?t
have been climate change {president, bill, sen., king, senator} {mr., mr, john, david, michael} {obama, bush, clinton, . . .}
rather than the bush administration {order, plans, needs, efforts, failed} to {make, take, give, keep, provide}
based on developing countries {will, would, could, should, might} not be {can, ?ll} be
Table 4: Gappy patterns manually divided into categories of long-distance dependencies. Patterns were ranked ac-
cording to p(pi)p(pi|w(pi)) and manually selected from the top 300 to exemplify categories. Lower pane shows top
ranked non-gappy patterns. Clusters are shown as enough words to cover 95% of the token counts of the cluster, up to
a maximum of 5.
again ensured that the period and its word class re-
mained isolated in their own patterns for each con-
dition. We note that no source-side word order in-
formation is contained within these bilingual pat-
terns; aligned source words can be in any order in
the source sentence and the pattern will still match.
The most probable patterns included many mono-
lingual source-only and target-only patterns that are
similar to those shown in Table 4. There were also
many phrase pairs with gaps like those that are com-
518
monly extracted by heuristics (Galley and Manning,
2010). Additionally we noted examples of source
words triggering more target-side information than
merely one word. There were several examples of
patterns that encouraged inclusion of the subject in
English when translating from Spanish, as Spanish
often drops the subject when it is clear from context,
e.g., ?we are(estamos)?. Also, one probable pattern
for German-English was ?the of the(des)? (des is
aligned to the final the). The German determiner
des is in the genitive case, so this pattern helps to
encourage its object to also be in the genitive case
when translated.
5.2 Quantitative Evaluation
We consider the Spanish-to-English (ES?EN)
translation task from the ACL-2010 Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2010). We trained a Moses system (Koehn et al,
2007) following the baseline training instructions for
the shared task.7 In particular, we performed word
alignment in each direction using GIZA++ (Och and
Ney, 2003), used the ?grow-diag-final-and? heuristic
for symmetrization, and extracted phrase pairs up to
a maximum length of seven. After filtering sentence
pairs with one sentence longer than 50 words, we
ended up with 1.45M sentence pairs of Europarl data
and 91K sentence pairs of news commentary data.
Language models (N = 5) were estimated using the
SRI language modeling toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Language models were trained on the
target side of the parallel corpus as well as the first 5
million additional sentences from the extra English
monolingual newswire data provided for the shared
tasks. We used news-test2008 for tuning and
news-test2009 for testing.
We also consider Chinese-English (ZH?EN) and
followed a similar training procedure as above. We
used 303K sentence pairs from the FBIS corpus
(LDC2003E14) and segmented the Chinese data
using the Stanford Chinese segmenter in ?CTB?
mode (Chang et al, 2008), giving us 7.9M Chi-
nese words and 9.4M English words. A trigram lan-
guage model was estimated using modified Kneser-
Ney smoothing from the English side of the parallel
7www.statmt.org/wmt10/baseline.html.
corpus concatenated with 200M words of randomly-
selected sentences from the Gigaword v4 corpus (ex-
cluding the NY Times and LA Times). We used
NIST MT03 for tuning and NIST MT05 for test-
ing. For evaluation, we used case-insensitive IBM
BLEU (Papineni et al, 2001).
5.2.1 Training and Decoding
Unlike n-gram language models, our models have
latent structure (the color assignments), making it
difficult to compute the probability of a translation
during decoding. We leave this problem for future
work and instead simply add a feature for each of
the most probable patterns discovered by our mod-
els. Each feature counts the number of occurrences
of its pattern in the translation.
We wish to add thousands of features to our
model, but the standard training algorithm ? mini-
mum error rate training (MERT; Och, 2003) ? can-
not handle large numbers of features. So, we lever-
age recent work on feature-rich training for MT us-
ing online discriminative learning algorithms. Our
training procedure is shown as Algorithm 1. We
find it convenient to notationally distinguish feature
weights for the standard Moses features (?) from
weights for our pattern features (?). We use h(e)
to denote the feature vector for translation e. The
function Bi(t) returns the sentence BLEU score for
translation t given reference ei (i.e., treating the sen-
tence pair as a corpus).8
MERT is run to convergence on the tuning set to
obtain weights for the standard Moses features (line
1). Phrase lattices (Ueffing et al, 2002) are gen-
erated for all source sentences in the tuning set us-
ing the trained weights ?M (line 2). The lattices
are used within a modified version of the margin-
infused relaxed algorithm (MIRA; Crammer et al,
2006) for structured max-margin learning (lines 5-
15). A k-best list is extracted from the current lattice
(line 7), then the translations on the k-best list with
the highest and lowest sentence-level BLEU scores
are found (lines 8 and 9). The step size is then com-
puted using the standard MIRA formula (lines 10-
11) and the update is made (line 12). The returned
weights are averaged over all updates.
This training procedure is inspired by several
8When computing sentence BLEU, we smooth by replacing
precisions of 0.0 with 0.01.
519
Input: input sentences F = {fi}Ni=1, references
E = {ei}Ni=1, initial weights ?0, size of
k-best list k, MIRA max step size C, num.
iterations T
Output: learned weights: ?M , ??
?,???
?M ? MERT (F , E, ?0);1
{`i}Ni=1 ? generateLattices (F , ?M );2
?? ?M ; ? ? 0;3
???, ??? ? ??,??;4
for iter ? 1 to T do5
for i? 1 to N do6
{tj}kj=1 ? Decode(`i, ??,??);7
e+ ? argmax1?j?k Bi(tj);8
e? ? argmin1?j?k Bi(tj);9
?? max(0, ??,??> [h(e?)? h(e+)]10
+Bi(e+)?Bi(e?));
? ? min(C, ?
?h(e+)?h(e?)?2 );11
? ? ? + ? [h(e+)? h(e?)];12
???, ??? ? ???, ???+ ??,??;13
end14
end15
???,??? ? ???, ??? ? 1T?N+1 ;16
return ?M , ??
?,???;17
Algorithm 1: Train
others that have been shown to be effective for
MT (Liang et al, 2006; Arun and Koehn, 2007;
Watanabe et al, 2007; Chiang et al, 2008). Though
not shown in the algorithm, in practice we store the
BLEU-best translation on each k-best list from all
previous iterations and use it as e+ if it has a higher
BLEU score than any on the k-best list on the cur-
rent iteration.
At decoding time, we follow a procedure similar
to training: we generate lattices for each source sen-
tence using Moses with its standard set of features
and using weights ?M . We rescore the lattices us-
ing ?? and use cube pruning (Chiang, 2007; Huang
and Chiang, 2007) to incorporate the gappy pattern
features with weights ??. Cube pruning is necessary
because the pattern features may match anywhere in
the translation; thus they are non-local in the phrase
lattice and require approximate inference.
5.3 Training Algorithm Comparison
Before adding pattern features, we evaluate our
training algorithm by comparing it to MERT us-
ing the same standard Moses features. As the ini-
ES?EN ZH?EN
MERT 25.64 32.47
Alg. 1 25.85 32.33
Table 5: Comparing MERT to our training procedure. All
numbers are %BLEU.
tial weights ?0, we used the default Moses feature
weights. We used k = 100, C = 0.0001, and
T = 15. For the n-best list size used during cube
pruning during both training and decoding, we used
n = 100. There are several Moses parameters that
affect the scope of the search during decoding and
therefore the size of the phrase lattices. We used
default values for these except for the stack size pa-
rameter, for which we used 100. The resulting lat-
tices encode up to 1050 derivations for ES?EN and
1065 derivations for ZH?EN.
Table 5 shows test set %BLEU for each language
pair and training algorithm. Our procedure per-
forms comparably to MERT. Therefore we use it as
our baseline for subsequent experiments since it can
handle a large number of feature weights; this al-
lows us to observe the contribution of the additional
gappy pattern features more clearly.
5.4 Feature Preparation
We chose monolingual and bilingual pattern features
using the posterior samples obtained via the infer-
ence procedures described above. We ranked pat-
terns using the product-of-experts formula, removed
patterns consisting of only a single token, and added
the top 10K patterns from the lexical model and the
top 15K patterns from the Brown cluster model. For
simplicity of implementation, we skipped over pat-
terns with 3 or more gaps and patterns with 2 gaps
and more than 3 total words; this procedure skipped
fewer than 1% of the top patterns. For results with
bilingual pattern features, we added 15K pattern fea-
tures (5K word-word, 5K cluster-cluster, and 5K
cluster-word).
5.5 Results
The first set of results is shown in Table 6. The
first row is the same as in Table 5, the second
row adds monolingual pattern features, the third
adds bilingual pattern features, and the final row in-
cludes both sets. While gains are modest overall,
520
ES?EN ZH?EN
Baseline 25.85 32.33
MONOPATS 25.84 32.81
BIPATS 25.92 32.68
MONOPATS + BIPATS 25.59 32.80
Table 6: Adding gappy pattern features. All numbers are
%BLEU.
Ranking %BLEU
Baseline N/A 32.33
MONOPATS p(pi) 32.65
MONOPATS p(pi|w(pi)) 32.53
MONOPATS p(pi)p(pi|w(pi)) 32.81
BIPATS p(pi) 32.68
MONOPATS + BIPATS p(pi) 32.78
MONOPATS + BIPATS p(pi)p(pi|w(pi)) 32.80
Table 7: Comparing ways of ranking patterns from pos-
terior samples. Scores are on MT05 for ZH?EN transla-
tion.
the pattern features show an encouraging improve-
ment of 0.48 BLEU for ZH?EN. This is similar
to the improvement reported by Xiong et al (2011)
(+0.4 BLEU when adding their trigger pair language
model). While bilingual patterns give an improve-
ment of 0.35 BLEU, using both monolingual and
bilingual features in the same model does not pro-
vide additional improvement over monolingual fea-
tures alone.
For ES?EN, the pattern features have only small
effects on BLEU; we suspect that the decreased
BLEU score for the full feature set is due to over-
fitting. It is unclear why the results differ for the two
language pairs. One possibility is the use of only
a single reference translation when tuning and test-
ing with ES?EN while four references were used
for ZH?EN. Another possibility is that our pattern
features are correcting some of the mid- to long-
range reorderings that are known to be problem-
atic for phrase-based modeling of ZH?EN transla-
tion. ES?EN exhibits less long-range reordering
and therefore may not benefit as much from our pat-
terns.
Table 7 shows additional ZH?EN results when
varying the method of ranking patterns. When us-
ing both sets of features, the ?Ranking? column
contains the criterion for ranking monolingual pat-
terns; bilingual patterns are always ranked using
said that the however , the agence france presse
?s , ?s us iraq reported the
of million , likely said that and
added " - - rate percent
the {media, school, university, election, bank}
{made, established, given, taken, reached}
{said, stressed, stated, indicated, noted} that in
{meeting, report, conference, reports} {1, july, june, march, april}
{news, press, spokesman, reporter} {meeting, . . .} {1, july, . . .}
{news, press, spokesman, reporter} {1, july, june, march, april}
the {enterprises, companies, students, customers, others}
{enterprises, companies, students, customers, others}
{japan, russia, europe, 2003, 2004} {us, japanese, russian, u.s.}
Table 8: Selected features from the 15 most highly-
weighted lexical and cluster pattern features in the best
ZH?EN model.
p(pi). The results show that ranking monolingual
patterns using the product-of-experts method results
in the highest BLEU scores, validating our intu-
itions from observing Tables 1-3. Table 8 shows the
most highly-weighted pattern features for the best
ZH?EN model.
6 Conclusion
We have presented generative models for monolin-
gual and bilingual gappy patterns. A qualitative
analysis shows that the models discover patterns
that match our intuitions in capturing linguistic phe-
nomena. Our experimental results show promise
for the ability of these patterns to improve trans-
lation for certain language pairs. A key advan-
tage of generative models is the ability to rapidly
develop and experiment with variations, especially
when using Gibbs sampling for inference. In order
to encourage modifications and extensions to these
models we have made our source code available at
www.ark.cs.cmu.edu/MT.
Acknowledgments
The authors thank Chris Dyer, Qin Gao, Alon Lavie,
Nathan Schneider, Stephan Vogel, and the anonymous
reviewers for helpful comments. This research was sup-
ported in part by the NSF through grant IIS-0844507, the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-
1-0533, and Sandia National Laboratories (fellowship to
K. Gimpel).
521
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
M. Bansal, C. Quirk, and R. Moore. 2011. Gappy
phrasal alignment by agreement. In Proc. of ACL.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram mod-
els of natural language. Computational Linguistics,
18.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 joint workshop on statistical machine translation
and metrics for machine translation. In Proc. of the
5th Workshop on Statistical Machine Translation.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the Third Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. M. Crego and F. Yvon. 2009. Gappy translation units
under left-to-right SMT decoding. In Proc. of EAMT.
M. Galley and C. D. Manning. 2010. Accurate non-
hierarchical phrase-based translation. In Proc. of
NAACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
F. Jelinek. 1997. Statistical methods for speech recogni-
tion. MIT Press.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10(3).
M. Simard, N. Cancedda, B. Cavestro, M. Dymetman,
E?. Gaussier, C. Goutte, K. Yamada, P. Langlais, and
A. Mauser. 2005. Translating with non-contiguous
phrases. In Proc. of HLT-EMNLP.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP-CoNLL.
D. Xiong, M. Zhang, and H. Li. 2011. Enhancing lan-
guage models in statistical machine translation with
backward N-grams and mutual information triggers.
In Proc. of ACL.
522
