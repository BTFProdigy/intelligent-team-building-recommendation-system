Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 771?778, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Word-Sense Disambiguation for Machine Translation
David Vickrey Luke Biewald Marc Teyssier Daphne Koller
Department of Computer Science
Stanford University
Stanford, CA 94305-9010
{dvickrey,lukeb,teyssier,koller}@cs.stanford.edu
Abstract
In word sense disambiguation, a system attempts to
determine the sense of a word from contextual fea-
tures. Major barriers to building a high-performing
word sense disambiguation system include the dif-
ficulty of labeling data for this task and of pre-
dicting fine-grained sense distinctions. These is-
sues stem partly from the fact that the task is be-
ing treated in isolation from possible uses of au-
tomatically disambiguated data. In this paper, we
consider the related task of word translation, where
we wish to determine the correct translation of a
word from context. We can use parallel language
corpora as a large supply of partially labeled data
for this task. We present algorithms for solving the
word translation problem and demonstrate a signif-
icant improvement over a baseline system. We then
show that the word-translation system can be used
to improve performance on a simplified machine-
translation task and can effectively and accurately
prune the set of candidate translations for a word.
1 Introduction
The problem of distinguishing between multiple
possible senses of a word is an important subtask in
many NLP applications. However, despite its con-
ceptual simplicity, and its obvious formulation as a
standard classification problem, achieving high lev-
els of performance on this task has been a remark-
ably elusive goal.
In its standard formulation, the disambiguation
task is specified via an ontology defining the dif-
ferent senses of ambiguous words. In the Sense-
val competition, for example, WordNet (Fellbaum,
1998) is used to define this ontology. However, on-
tologies such as WordNet are not ideally suited to
the task of word-sense disambiguation. In many
cases, WordNet is overly ?specific?, defining senses
which are very similar and hard to distinguish. For
example, there are seven definitions of ?respect?
as a noun (including closely related senses such as
?an attitude of admiration or esteem? and ?a feel-
ing of friendship and esteem?); there are even more
when the verb definitions are included as well. Such
closely related senses pose a challenge both for auto-
matic disambiguation and hand labeling. Moreover,
the use of a very fine-grained set of senses, most of
which are quite rare in practice, makes it very diffi-
cult to obtain sufficient amounts of training data.
These issues are clearly reflected in the perfor-
mance of current word-sense disambiguation sys-
tems. When given a large amount of training data
for a particular word with reasonably clear sense
distinctions, existing systems perform fairly well.
However, for the ?all-words? task, where all am-
biguous words from a test corpus must be disam-
biguated, it has so far proved difficult to perform sig-
nificantly better than the baseline heuristic of choos-
ing the most common sense for each word.1
In this paper, we address a different formulation
of the word-sense disambiguation task. Rather than
considering this task on its own, we consider a task
of disambiguating words for the purpose of some
larger goal. Perhaps the most direct and compelling
application of a word-sense disambiguator is to ma-
chine translation. If we knew the correct seman-
tic meaning of each word in the source language,
we could more accurately determine the appropriate
words in the target language. Importantly, for this
application, subtle shades of meaning will often be
irrelevant in choosing the most appropriate words in
the target language, as closely related senses of a
single word in one language are often encoded by a
single word in another. In the context of this larger
goal, we can focus only on sense distinctions that a
human would consider when choosing the transla-
tion of a word in the source language.
We therefore consider the task of word-sense dis-
ambiguation for the purpose of machine translation.
Rather than predicting the sense of a particular word
a, we predict the possible translations of a into the
1See, for example, results of Senseval-3, available at
http://www.senseval.org/senseval3
771
target language. We both train and evaluate the sys-
tem on this task. This formulation of the word-sense
disambiguation task, which we refer to as word
translation, has multiple advantages. First, a very
large amount of ?partially-labeled? data is available
for this task in the form of bilingual corpora (which
exist for a wide range of languages). Second, the
?labeling? of these corpora (that is, translation from
one language to another), is a task at which humans
are quite proficient and which does not generally re-
quire the labeler (translator) to make difficult dis-
tinctions between fine shades of meaning.
In the remainder of this paper, we first discuss
how training data for this task can be acquired au-
tomatically from bilingual corpora. We apply a
standard learning algorithm for word-sense disam-
biguation to the word translation task, with several
modifications which proved useful for this task.We
present the results of our algorithm on word trans-
lation, showing that it significantly improves perfor-
mance on this task. We also consider two simple
methods for incorporating word translation into ma-
chine translation. First, we can use the output of
our model to help a translation model choose better
words; since general translation is a very noisy pro-
cess, we present results on a simplified translation
task. Second, we show that the output of our model
can be used to prune candidate word sets for trans-
lation; this could be used to significantly speed up
current translation systems.
2 Machine Translation
In machine translation, we wish to translate a sen-
tence s in our source language into t in our target
language. The standard approach to statistical ma-
chine translation uses the source-channel model ,
argmaxtP (t|s) = argmaxtP (t)P (s|t),
where P (t) is the language model for the target lan-
guage, and P (s|t) is an alignment model from the
target language to the source language. Together
they define a generative model for the source/target
pair (s, t): first t is generated according to the lan-
guage model P (t); then s is generated from t ac-
cording to P (s|t).2
Typically, strong independence assumptions are
then made about the distribution P (s|t). For ex-
ample, in the IBM Models (Brown et al, 1993),
each word ti independently generates 0, 1, or more
2Note that we refer to t as the target sentence, even though in
the source-channel model, t is the source sentence which goes
through the channel model P (s|t) to produce the observed sen-
tence s.
words in the source language. Thus, the words gen-
erated by ti are independent of the words generated
by tj for each j 6= i. This means that correla-
tions between words in the source sentence are not
captured by P (s|t), and so the context we will use
in our word translation models to predict ti given
si is not available to a system making these inde-
pendence assumptions. In this type of system, se-
mantic and syntactic relationships between words
are only modeled in the target language; most or
all of the semantic and syntactic information con-
tained in the source sentence is ignored. The lan-
guage model P (t) does introduce some context-
dependencies, but the standard n-gram model used
in machine translation is too weak to provide a rea-
sonable solution to the strong independence assump-
tions made by the alignment model.
3 Task Formulation
We define the word translation task as finding, for
an individual word a in the source language S , the
correct translation, either a word or phrase, in the
target language T . Clearly, there are cases where
a is part of a multi-word phrase that needs to be
translated as a unit. Our approach could be extended
by preprocessing the data in S to find phrases, and
then executing the entire algorithm treating phrases
as atomic units. We do not explore this extension in
this paper, instead focusing on the word-to-phrase
translation problem.
As we discussed, a key advantage of the word
translation vs. word sense disambiguation is the
availability of large amounts of training data. This
data is in the form of bilingual corpora, such as
the European Parliament proceedings3 . Such doc-
uments provide many training instances, where a
word in one language is translated into another.
However, the data is only partially labeled in that
we are not given a word-to-word alignment between
the two languages, and thus we do not know what
every word in the source language S translates to in
the target language T . While sentence-to-sentence
alignment is a fairly easy task, word-to-word align-
ment is considerably more difficult. To obtain word-
to-word alignments, we used GIZA++4, an imple-
mentation of the IBM Models (specifically, we used
the output of IBM Model 4). We did not perform
stemming on either language, so as to preserve suf-
fix information for our word translation system and
the machine translation language model.
Let DS be the set of sentences in the source lan-
3Available at http://www.isi.edu/ koehn/
4Available at http://www.isi.edu/ och/GIZA++.html
772
French (frequency) Translation
monte?e(51) going up
le`ve(10), lever(17) standing up
hausse(58), augmenter(37), increase(number)
augmentation(150)
interviens(53) to rise to speak
naissance(21), source(10) to be created, arise
souleve?(10) raising an issue
Table 1: Aligned translations for ?rise? occurring at
least 10 times in the corpus
guage and DT the set of target language sentences.
The alignment algorithm can be run in either di-
rection. When run in the S ? T direction, the al-
gorithm aligns each word in t to at most one word
in s. Consider some source sentence s that contains
the word a, and let Ua,s?t = b1, . . . , bk be the set
of words that align to a in the aligned sentence t. In
general, we can consider Ua = {Ua,s?t}s?Da to be
the candidate set of translations for a in T , where
Da is the set of source language sentences contain-
ing a. However, this definition is quite noisy: a word
bi might have been aligned with a arbitrarily; or, bi
might be a word that itself corresponds to a multi-
word translation in S . Thus, we also align the sen-
tences in the T ? S direction, and require that each
bi in the phrase aligns either with a or with nothing.
As this process is still fairly noisy, we only consider
a word or phrase b ? Ua to be a candidate translation
for a if it occurs some minimum number of times in
the data.
For example, Table 1 shows a possible candidate
set for the English word ?rise?, with French as the
target language. Note that this set can contain not
only target words corresponding to different mean-
ings of ?rise? (the rows in the table) but also words
which correspond to different grammatical forms in
the target language corresponding to different parts
of speech, verb tenses, etc. So, disambiguation in
this case is both over senses and grammatical forms.
The final result of our processing of the corpus is,
for each source word a, a set of target words/phrases
Ua; and a set of sentences Da where, in each sen-
tence, a is aligned to some b ? Ua. For any sen-
tence s ? Da, aligned to some target sentence t,
let ua,s ? Ua be the word or phrase in t aligned
with a. We can now treat this set of sentences as
a fully-labeled corpus, which can be split into a set
used for learning the word-translation model and a
test set used for evaluating its performance.
We note, however, that there is a limitation to us-
ing accuracy on the test set for evaluating the perfor-
mance of the algorithm. A source word a in a given
context may have two equally good, interchangeable
translations into the target language. Our evaluation
metric only rewards the algorithm for selecting the
target word/phrase that happened to be used in the
actual translation. Thus, accuracies measured us-
ing this metric may be artificially low. This is a
common problem with evaluating machine transla-
tion systems.
Another issue is that we take as ground truth the
alignments produced by GIZA++. This has two im-
plications: first, our training data may be noisy since
some alignments may be incorrect; and second, our
test data may not be completely accurate. As men-
tioned above, we only consider possible translations
which occur some minimum number of times; this
removes many of the mistakes made by GIZA++.
Even if the test set is not 100% reliable, though, im-
provement over baseline performance is indicative
of the potential of a method.
4 Word Translation Algorithms
The word translation task and the word-sense dis-
ambiguation task have the same form: each word a
is associated with a set of possible labels Ua; given
a sentence s containing word a, we must determine
which of the possible labels in Ua to assign to a in
the context s. The only difference in the two tasks is
the set Ua: for word translation it is the set of pos-
sible translations of a, while for word sense disam-
biguation it is the set of possible senses of a in some
ontology. Thus, we may use any word sense disam-
biguation algorithm as a word translation algorithm
by appropriately defining the senses (assuming that
the WSD algorithm does not assume that a particular
ontology is used to choose the senses).
Our main focus in this paper is to show that ma-
chine learning techniques are effective for the word
translation task, and to demonstrate that we can use
the output of our word translation system to im-
prove performance on two machine-translation re-
lated tasks. We will therefore restrict our atten-
tion to a relatively simple model, logistic regres-
sion (Minka, 2000). There are several motivations
for using this discriminative, probabilistic model.
First, it is known both theoretically and empirically
(e.g., (Ng and Jordan, 2002)) that discriminative
models achieve higher accuracies than generative
models if enough data is available. For the tradi-
tional word-sense disambiguation task, data must be
hand-labeled, and is therefore often too scarce to al-
low for discriminative training. In our setting, how-
ever, training data is acquired automatically from
bilingual corpora, which are widely available and
quite large. Thus, discriminative training is a viable
option for the word translation problem. A second
773
consideration is that, to effectively incorporate our
system into a statistical machine translation system,
we would like to produce not just a single prediction,
but a list of confidence-rated possibilities. The op-
timization procedure of logistic regression attempts
to produce a distribution over possible translations
which accurately represents the confidence of the
model for each translation. By contrast, a classical
Naive Bayes model often assigns very low proba-
bilities to all but the most likely translation. Other
word-sense disambiguation models may not produce
confidence measures at all.
Features. Our word translation model for a word
a in a sentence s = w1, . . . , wk is based on features
constructed from the word and its context within the
sentence. Our basic logistic regression model uses
the following features, which correspond to the fea-
ture space for a standard Naive Bayes model:
? the part of speech of a (generated using the
Brill tagger)5;
? a binary ?occurs? variable for each word which
is 1 if that word is in a fixed context centered
at a (cr words to the right and cl words to the
left), and 0 otherwise.
We also consider an extension to this model, where
instead of the fixed context features above, we use:
? for each direction d ? {l, r} and each possi-
ble context size cd ? {1, ..., Cd}, an ?occurs?
variable for each word.
This is a true generalization of the previous con-
text features, since it contains features for all pos-
sible context sizes, not just one particular fixed size.
This feature set is equivalent to having one feature
for each word in each context position, except that
it will have a different prior over parameters under
standard L2 regularization. This feature set alows
our model to distinguish between very local (often
syntactic) features and somewhat longer range fea-
tures whose exact position is not as important.
Let ?a,s be the set of features for word a to be
translated, with sentence context s (the description
of the model does not depend on the particular fea-
ture set selected).
Model. The logistic regression model encodes the
conditional distribution (P (ua,s = b | a, s) : b ?
Ua). Such a model is parameterized by a set of vec-
tors ?ab , one for each word a and each possible target
b ? Ua, where each vector contains a weight ?ab,j for
each feature ?a,sj . We can now define our conditional
distribution:
5Available at http://www.cs.jhu.edu/ brill/
P?a(b | a, s) =
1
Za,s
e?ab?a,s
with partition function Za,s =
?
b??Ua exp(?ab??a,s).
Training. We train the logistic regression model to
maximize the conditional likelihood of the observed
labels given the features in our training set. Thus,
our goal in training the model for a is to maximize
?
s?Da
P?a(ua,s | a, s).
We maximize this objective by maximizing its log-
arithm (the log-conditional-likelihood) using conju-
gate gradient ascent (Shewchuk, 1994).
One important consideration when training using
maximum likelihood is regularization of the param-
eters. In the case of logistic regression, the most
common type of regularization is L2 regularization;
we then maximize
?
b,j
exp
(
?
(?ab,j)2
2?2
)
?
s?Da
P?a(ua,s | a, s).
This penalizes the likelihood for the distance of each
parameter ?ab,j from 0; it corresponds to a Gaussian
prior on each parameter with variance ?2.
5 Word Translation Results
For our word translation experiments we used the
European Parliament proceedings corpus, which
contains approximately 27 million words in each of
English and French (as well as a number of other
languages). We tested on a set of 1859 ambigu-
ous words ? specifically, all ambiguous words con-
tained in the first document of the corpus. For each
of these words, we found all instances of the word in
the corpus and split these instances into training and
test sets.
We tested four different models. The first, Base-
line, always chooses the most common translation
for the word; the second, Baseline with Part of
Speech, uses tagger-generated parts of speech to
choose the most common translation for the ob-
served word/part-of-speech pair. The third model,
Simple Logistic, is the logistic regression model
with the simpler feature set, a context window of a
fixed size. We selected the window size by eval-
uating accuracy for a variety of window sizes on
20 of the 1859 ambiguous words using a random
train-test split. The window size which performed
best on average extended one word to the left and
774
Model Macro Micro
Baseline 0.511 0.526
Baseline with Part of Speech 0.519 0.532
Simple Logistic 0.581 0.605
Logistic 0.596 0.620
Table 2: Average Word Translation Accuracy
two words to the right (larger windows generally re-
sulted in overfitting). The fourth model, Logistic, is
the logistic regression model with overlapping con-
text windows; the maximum window size for this
model was four words to the left and four words to
the right. We selected the standard deviation ?2 for
the logistic models by trying different values on the
same small subset of the ambiguous words. For the
Simple Logistic model, the best value was ?2 = 1;
for the Logistic model, it was 0.35.
Table 2 shows results of these four models. The
first column is macro-averaged over the 1859 words,
that is, the accuracy for each word counts equally
towards the average. The second column shows the
micro-averaged accuracy, where each test example
counts equally. We will focus on the micro-averaged
results, since they correspond to overall accuracy.
The less accurate of our two models, Simple Lo-
gistic, improves around 8% over the simple baseline
and 7% over the part-of-speech baseline on aver-
age. Our more complex logistic model, which is able
to handle larger context sizes without significantly
overfitting, improves accuracy by another 1.5%.
There was a great deal of variance from word
to word in the performance of our models relative
to baseline. For a few words, we achieved very
large increases in accuracy. For instance, the noun
?agenda? showed a 31.2% increase over both base-
lines. Similarly, the word ?rise? (either a noun
or a verb) had part-of-speech baseline accuracy of
27.9%. Our model increased the accuracy to 57.0%.
It is worth repeating that accuracies on this task
are artificially low since in many cases a single word
can be translated to many different words with the
same meaning. At the same time, accuracies are ar-
tificially inflated by the fact that we only consider
examples where we can find an aligned word in
the French corpus, so translations where a word is
dropped or translated as part of a compound word
are not counted.
One disadvantage of the EuroParl corpus is that it
is not ?balanced? in terms of semantic content. It is
not clear how this affects our results.
6 Blank-Filling Task
One of the most difficult parts of machine translation
is decoding ? finding the most likely translation ac-
cording to some probability model. The difficulty
arises from the enormous number of possible trans-
lated sentences. Existing decoders generally use ei-
ther highly pruned search or greedy heuristic search.
In either case, the quality of a translation can vary
greatly from sentence to sentence. This variation
is much higher than the improvement in ?seman-
tic? accuracy our model is attempting to achieve.
Moreover, currently available decoders do not pro-
vide a natural way to incorporate the results of a
word translation system. For example, Carpuat and
Wu (2005) obtain negative results for two methods
of incorporating the output of a word-sense disam-
biguation system into a machine translation system.
Thus, we instead used our word translation model
for a simplified translation problem. We prepared a
dataset as follows: for each occurrence of an am-
biguous words in an English sentence in the first
document of the Europarl corpus, we tried to de-
termine what the correct translation for that word
was in the corresponding French sentence. If we
found one and exactly one possible translation for
that word in the French sentence, we replaced that
word with a ?blank?, and linked the English word
to that blank. The final result was a set of 655 sen-
tences with a total of 3018 blanks.
For example, the following English-French sen-
tence pair contains the two ambiguous words ad-
dress and issue and one possible translation for each,
examiner and question:
? Therefore, the commission should address the
issue once and for all.
? Par conse?quent, la commission devra enfin ex-
aminer cette question particulie`re.
We replace the translations of the ambiguous words
with blanks; we would like a decoder to replace the
blanks with the correct translations:
? Par conse?quent, la commission devra enfin [ad-
dress] cette [issue] particulie`re.
An advantage of this task is that, for a given distri-
bution P (t|s), we can easily write a decoder which
exhaustively searches the entire solution space for
the best answer (provided that there are not too many
blanks and that P (t|s) is sufficiently ?local? with re-
spect to t). Thus, we can be sure that it is the prob-
ability model, and not the decoder, which is deter-
mining the quality of the output. Also, we have re-
moved most or all syntactic variability from the task,
775
Model ?lm ?ga ?da ?wt Acc
Language Model only 1 0 0 0 0.749
Source-Channel 1 1 0 0 0.821
LM + GA + DA 1 0.6? 0.6? 0 0.833
LM + GA + DA + WT 1 0.6? 0? 1.2? 0.846
Table 3: Blank-filling results. Weights marked with
* have been optimized.
allowing us to better gauge whether we are choosing
semantically correct translations.
Let (ai, bi) be the pairs of words corresponding to
the blanks in sentence t. Then the alignment model
decomposes as a product of terms over these pairs,
e.g. P (s|t) ? ?(ai,bi) P (ai|bi). Analogously, we
extend the word translation model as Pwt(t|s) ?
?
(ai,bi) Pwt(bi|s, ai).
The source-channel model can be used directly
to solve the blank filling task; the language model
makes use of the French words surrounding each
blank, while the alignment model guesses the ap-
propriate translation based on the aligned English
word. As we have mentioned, this model does not
take full advantage of the context in the English sen-
tence. Thus, we hope that incorporating the word
translation model into the decoder will improve per-
formance on this task.
Conversely, simply using the word translation
model alone for the blank-filling task would not take
advantage of the available French context. There
are four probability distributions we might consider
using: the language model Plm(t); the ?genera-
tive? alignment model Pga(s|t), which we calcu-
late using the training samples from the previous
section; the analogous ?discriminative? alignment
model Pda(t|s), which corresponds to the Base-
line system we compared to on the word translation
task; and our overlapping context logistic model,
Pwt(t|s), which also goes in the ?discriminative? di-
rection, but uses the context features in the source
language for determining the distribution over each
word?s possible translations.
We combine these models by simply taking a log-
linear combination:
log P (t|s) ? ?lm logPlm(t) + ?ga log Pga(s|t)
+ ?da logPda(t|s) + ?wt logPwt(t|s).
The case of ?lm = ?ga = 1 and ?da = ?wt = 0 re-
duces to the source-channel model; other settings in-
corporate discriminative models to varying degrees.
We evaluated this combined translation model on
the blank-filling task for various settings of the mix-
ture coefficients ?. For our language model we used
0 0.5 1 1.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Generative Coefficient
W
or
d 
Tr
an
sla
tio
n 
Co
ef
fic
ie
nt
0.77
0.79
0.81
0.83
0.83
0.84
0.84
0.845
0.8
45
Figure 1: Accuracy on blank-filling task with ?lm = 1 and
?disc = 0 as a function of ?gen and ?wt.
the CMU-Cambridge toolkit.6 The word translation
model for each ambiguous word was trained on all
documents except the first.
Table 3 shows results for several sets of weights.
A * denotes entries which we optimized (see be-
low); other entries were fixed. For example, the third
model was obtained by fixing the coefficient of the
language model to 1 and the word-translation to 0,
and optimizing the weights for the generative and
discriminative alignment models.
The language model alone is able to achieve rea-
sonable results; adding the alignment models im-
proves performance further. By adding the word-
translation model, we are able to improve perfor-
mance by approximately 2.5% over the source-
channel model, a relative error reduction of 14%,
and 1.3% over the optimized model using the
language model and generative and discriminative
alignment models, a relative error reduction of 7.8%.
We chose optimal coefficients for the combined
probability models by exhaustively trying all possi-
ble settings of the weights, at a resolution of 0.1,
evaluating accuracy for each one on the test set. Fig-
ure 1 shows the performance on the blank-filling
task as a function of the weights of the generative
alignment model and the word-translation model
(the optimum value of the discriminative alignment
model P (t|s) is always 0 when we include the
word-translation model). As we can see, the per-
formance of this model is robust with respect to
the exact value of the coefficients. The ?obvious?
setting of 1.0 for the generative model and 1.0 for
the word translation model performs nearly as well
6Available at http://mi.eng.cam.ac.uk/ prc14/toolkit.html.
776
as the optimized setting. In the optimal region,
the word-translation model receives twice as much
weight as the generative alignment model, indicat-
ing that word-translation model is more informative
than the generative alignment model. Incorporating
the discriminative alignment model into the source-
channel model also improves performance, but not
nearly as much as using the word-translation model.
An alternate way to optimize weights over trans-
lation features is described in Och and Ney (2002).
They consider a number of translation features, in-
cluding the language model and generative and dis-
criminative alignment models.
7 Search Space Pruning
As we have mentioned, one of the main difficulties
in translation is that there are an enormous number
of possible translations to consider. Decoding al-
gorithms must therefore use some kind of search-
space pruning in order to be efficient. A key part
of pruning the search space is deciding on the set
of words to consider in possible translations (Ger-
mann et al, 2001). One standard method is to con-
sider only target words which have high probabil-
ity according to the discriminative alignment model.
But we have already shown that the word translation
model achieves much better performance on word
translation than this baseline model; thus, we would
expect the word translation model to improve accu-
racy when used to pick sets of candidate translations.
Given a probability distribution over possible
translations of a word, P (b|a, s), there are several
ways to choose a reduced set of possible transla-
tions. Two commonly used methods are to only
consider the top n scoring words from this distribu-
tion (best-n); and to only consider words b such that
P (b|a, s) is above some fixed threshold (cut-off ).
We use the same data set as for the blank-filling
task. We evaluate the accuracy of a pruning strategy
by evaluating whether the correct translation is in
the candidate set selected by the pruning strategy.
To compare results for different pruning strategies,
we plot performance as a function of average size
of the candidate translation set. Figure 2 shows the
accuracy vs. average candidate set size for the word-
translation model, discriminative alignment model,
and generative alignment model.
The generative alignment model has the worst
performance of the three. This is not surprising as it
does not take into account the prior probability of the
target word P (b). More interestingly, we see that the
word-translation model outperforms the discrimina-
tive translation model by a significant amount. For
0 2 4 6 8 10 12
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Average number of possible translations
Ac
cu
ra
cy
Figure 2: Accuracy of best-n strategy (dotted lines) and cut-
off strategy (solid lines). o = generative alignment, + = discrim-
inative alignment, * = word translation.
instance, to achieve 95% recall (that is, for 95% of
the ambiguous words, we retain the correct transla-
tion), we only need candidate sets of average size 4.2
for the cut-off strategy using the word-translation
model, whereas for the same strategy on the discrim-
inative alignment model we require an average set
size of 6.7 words.
As the size of the solution space grows exponen-
tially with the size of the candidate sets, the word-
translation model could potentially greatly reduce
the search space while maintaining good accuracy.
It would be interesting to use similar techniques to
learn null fertility (i.e., when a word a has no trans-
lation in the target sentence t).
8 Related Work
Berger et al (1996) apply maximum entropy meth-
ods (equivalent to logistic regression) to, among
other tasks, the word-translation task. However, no
quantitative results are presented. In this paper we
demonstrate that the method can improve perfor-
mance on a large data set and show how it might
be used to improve machine translation.
Diab and Resnik (2002) suggest using large bilin-
gual corpora to improve performance on word sense
disambiguation. The main idea is that knowing a
French word may help determine the meaning of the
corresponding English word. They apply this intu-
ition to the Senseval word disambiguation task by
running off-the-shelf translators to produce transla-
tions which they then use for disambiguation.
Ng et al (2003) address word sense disambigua-
tion by manually annotating WordNet senses with
their translation in the target language (Chinese),
and then automatically extracting labeled examples
for word sense disambiguation by applying the IBM
777
Models to a bilingual corpus. They achieve compa-
rable results to training on hand-labeled examples.
Koehn and Knight (2003) focus on the task of
noun-phrase translation. They improve performance
on the noun-phrase translation task, and show that
they can use this to improve full translations. A key
difference is that, in predicting noun-phrase trans-
lations, they do not consider the context of nouns.
They present results which indicate that humans can
accurately translate noun phrases without looking
at the surrounding context. However, as we have
demonstrated in this paper, context can be very use-
ful for a (sub-human-level) machine translator.
A similar argument applies to phrase-based trans-
lation methods (e.g., Koehn et al (2003)). While
phrase-based systems do take into account context
within phrases, they are not able to use context
across phrase boundaries. This is especially impor-
tant when ambiguous words do not occur as part of
a phrase ? verbs in particular often appear alone.
9 Conclusions
In this paper, we focus on the word-translation prob-
lem. By viewing word-sense disambiguation in the
context of a larger task, we were able to obtain large
amounts of training data and directly evaluate the
usefulness of our system for a real-world task. Our
results improve over a baseline which is difficult to
outperform in the word sense disambiguation task.
The word translation model could be improved in
a variety of ways, drawing upon the large body of
work on word-sense disambiguation. In particular,
there are many types of context features which could
be used to improve word translation performance,
but which are not available to standard machine-
translation systems. Also, the model could be ex-
tended to handle phrases.
To evaluate word translation in the context of a
machine translation task, we introduce the novel
blank-filling task, which decouples the impact of
word translation from a variety of other factors, such
as syntactic correctness. For this task, increased
word-translation accuracy leads to improved ma-
chine translation. We also show that the word trans-
lation model is effective at choosing sets of candi-
date translations, suggesting that a word translation
component would be immediately useful to current
machine translations systems.
There are several ways in which the results of
word translation could be integrated into a full trans-
lation system. Most naturally, the word translation
model can be used directly to modify the score of
different translations. Alternatively, a decoder can
produce several candidate translations, which can be
reranked using the word translation model. Unfortu-
nately, we were unable to try these approaches, due
to the lack of an appropriate publicly available de-
coder. Carpuat and Wu (2005) recently observed
that simpler integration approaches, such as forcing
the machine translation system to use the word trans-
lation model?s first choice, do not improve transla-
tion results. Together, these results suggest that one
should incorporate the results of word translation in
a ?soft? way, allowing the word translation, align-
ment, and language models to work together to pro-
duce coherent translations. Given an appropriate de-
coder, trying such a unified approach is straightfor-
ward, and would provide insight about the value of
word translation.
References
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1).
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2).
M. Carpuat and D. Wu. 2005. Word sense disambigua-
tion vs. statistical machine translation. Proc. ACL.
M. Diab and P. Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. Proc.
ACL.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. Proc. ACL.
P. Koehn and K. Knight. 2003. Feature-rich statistical
translation of noun phrases. Proc. ACL.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. HLT/NAACL.
T. Minka. 2000. Algorithms for
maximum-likelihood logistic regression.
http://lib.stat.cmu.edu/ minka/papers/logreg.html.
A. Ng and M. Jordan. 2002. On discriminative vs. gen-
erative classifiers: A comparison of logistic regression
and naive bayes. Proc. NIPS.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting
parallel texts for word sense disambiguation: An em-
pirical study. Proc. ACL.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. Proc. ACL.
J. Shewchuk. 1994. An introduction to the conjugate gra-
dient method without the agonizing pain. http://www-
2.cs.cmu.edu/ jrs/jrspapers.html.
778
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60?63
Manchester, August 2008
A Small-Vocabulary Shared Task for Medical Speech Translation
Manny Rayner1, Pierrette Bouillon1, Glenn Flores2, Farzad Ehsani3
Marianne Starlander1, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald5
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.ch
Marianne.Starlander@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USA
farzad@fluentialinc.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
5 Dolores Labs
lukeab@gmail.com
Abstract
We outline a possible small-vocabulary
shared task for the emerging medical
speech translation community. Data would
consist of about 2000 recorded and tran-
scribed utterances collected during an eval-
uation of an English ? Spanish version
of the Open Source MedSLT system; the
vocabulary covered consisted of about 450
words in English, and 250 in Spanish. The
key problem in defining the task is to agree
on a scoring system which is acceptable
both to medical professionals and to the
speech and language community. We sug-
gest a framework for defining and admin-
istering a scoring system of this kind.
1 Introduction
In computer science research, a ?shared task? is a
competition between interested teams, where the
goal is to achieve as good performance as possible
on a well-defined problem that everyone agrees to
work on. The shared task has three main compo-
nents: training data, test data, and an evaluation
metric. Both test and training data are divided
up into sets of items, which are to be processed.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The evaluation metric defines a score for each pro-
cessed item. Competitors are first given the train-
ing data, which they use to construct and/or train
their systems. They are then evaluated on the test
data, which they have not previously seen.
In many areas of speech and language process-
ing, agreement on a shared task has been a major
step forward. Often, it has in effect created a new
subfield, since it allows objective comparison of
results between different groups. For example, it
is very common at speech conference to have spe-
cial sessions devoted to recognition within a par-
ticular shared task database. In fact, a conference
without at least a couple of such sessions would
be an anomaly. A recent success story in language
processing is the Recognizing Textual Entailment
(RTE) task1. Since its inception in 2004, this has
become extremely popular; the yearly RTE work-
shop now attracts around 40 submissions, and error
rates on the task have more than halved.
Automatic medical speech translation would
clearly benefit from a shared task. As was made
apparent at the initial 2006 workshop in New
York2, nearly every group has both a unique ar-
chitecture and a unique set of data, essentially
making comparisons impossible. In this note, we
will suggest an initial small-vocabulary medical
1http://www.pascal-network.org/
Challenges/RTE/
2http://www.issco.unige.ch/pub/
SLT workshop proceedings book.pdf
60
shared task. The aspect of the task that is hard-
est to define is the evaluation metric, since there
unfortunately appears to be considerable tension
between the preferences of medical professionals
and speech system implementers. Medical profes-
sionals would prefer to carry out a ?deep? evalu-
ation, in terms of possible clinical consequences
following from a mistranslation. System evalua-
tors will on the other hand prefer an evaluation
method that can be carried out quickly, enabling
frequent evaluations of evolving systems. The plan
we will sketch out is intended to be a compromise
between these two opposing positions.
The rest of the note is organised as follows.
Section 2 describes the data we propose to use,
and Section 3 discusses our approach to evaluation
metrics. Section 4 concludes.
2 Data
The data we would use in the task is for the English
? Spanish language pair, and was collected us-
ing two different versions of the MedSLT system3.
In each case, the scenario imagines an English-
speaking doctor conducting a verbal examination
of a Spanish-speaking patient, who was assumed
to be have visited the doctor because they were
displaying symptoms which included a sore throat.
The doctor?s task was to use the translation sys-
tem to determine the likely reason for the patient?s
symptoms.
The two versions of the system differed in
terms of the linguistic coverage offered. The
more restricted version supported a minimal range
of English questions (vocabulary size, about 200
words), and only allowed the patient to respond
using short phrases (vocabulary size, 100 words).
Thus for example the doctor could ask ?How long
have you had a sore throat??, and the patient would
respond Hace dos d??as (?for two days?). The
less restricted version supported a broader range
of doctor questions (vocabulary size, about 450
words), and allowed the patient to respond using
both short phrases and complete sentences (vocab-
ulary size, about 225 words). Thus in response
to ?How long have you had a sore throat??, the
patient could say either Hace dos d??as (?for two
days?) or Tengo dolor en la garganta hace dos d??as
(?I have had a sore throat for two days?).
Data was collected in 64 sessions, carried out
3http://www.issco.unige.ch/projects/
medslt/
over two days in February 2008 at the University
of Texas Medical Center, Dallas. In each session,
the part of the ?doctor? was played by a real physi-
cian, and the part of the ?patient? by a Spanish-
speaking interpreter. This resulted in 1005 En-
glish utterances, and 967 Spanish utterances. All
speech data is available in SPHERE-headed form,
and totals about 90 MB. A master file, organised in
spreadsheet form, lists metadata for each recorded
file. This includes a transcription, a possible valid
translation (verified by a bilingual translator), IDs
for the ?doctor?, the ?patient?, the session and the
system version, and the preceding context. Con-
text is primarily required for short answers, and
consists of the most recent preceding doctor ques-
tion.
3 Evaluation metrics
The job of the evaluation component in the shared
task is to assign a score to each translated utter-
ance. Our basic model will be the usual one for
shared tasks in speech and language. Each pro-
cessed utterance will be assigned to a category;
each category will be associated with a specified
score; the score for a complete testset will the sum
of the scores for all of its utterances. We thus have
three sub-problems: deciding what the categories
are, deciding how to assign a category to a pro-
cessing utterance, and deciding what scores to as-
sociate with each category.
3.1 Defining categories
If the system attempts to translate an utterance,
there are a priori three things that can happen:
it can produce a correct translation, an incorrect
translation, or no translation. Medical speech
translation is a safety-critical problem; a mistrans-
lation may have serious consequences, up to and
including the death of the patient. This implies
that the negative score for an incorrect translation
should be high in comparison to the positive score
for a correct translation. So a naive scoring func-
tion might be ?1 point for a correct translation, 0
points for no translation, ?1000 points for an in-
correct translation.?
However, since the high negative score for a
mistranslation is justified by the possible serious
consequences, not all mistranslations are equal;
some are much more likely than others to result in
clinical consequences. For example, consider the
possible consequences of two different mistrans-
61
lations of the Spanish sentence La penicilina me
da alergias. Ideally, we would like the system to
translate this as ?I am allergic to penicillin?. If it
instead says ?I am allergic to the penicillin?, the
translation is slightly imperfect, but it is hard to see
any important misunderstanding arising as a result.
In contrast, the translation ?I am not allergic to
penicillin?, which might be produced as the result
of a mistake in speech recognition, could have very
serious consequences indeed. (Note in passing that
both errors are single-word insertions). Another
type of result is a nonsensical translation, perhaps
due to an internal system error. For instance, sup-
pose the translation of our sample sentence were
?The allergy penicillin does me?. In this case, it
is not clear what will happen. Most users will
probably dismiss the output as meaningless; a few
might be tempted to try and decipher it, with un-
predictable results.
Examples like these show that it is important for
the scoring metric to differentiate between differ-
ent classes of mistranslations, with the differentia-
tion based on possible clinical consequences of the
error. For similar reasons, it is important to think
about the clinical consequences when the system
produces correct translations, or fails to produce
a translation. For example, when the system cor-
rectly translates ?Hello? as Buenas d??as, there are
not likely to be any clinical consequences, so it is
reasonable to reward it with a lower score than the
one assigned to a clinically contentful utterance.
When no translation is produced, it also seems cor-
rect to distinguish the case where the user was able
recover by a suitably rephrasing the utterance from
the one where they simply gave up. For example,
if the system failed to translate ?How long has this
cough been troubling you??, but correctly handled
the simpler formulation ?How long have you had a
cough??, we would give this a small positive score,
rather than a simple zero.
Summarising, we propose to classify transla-
tions into the following seven categories:
1. Perfect translation, useful clinical conse-
quences.
2. Perfect translation, no useful clinical conse-
quences.
3. Imperfect translation, but not dangerous in
terms of clinical consequences.
4. Imperfect translation, potentially dangerous.
5. Nonsense.
6. No translation produced, but later rephrased
in a way the system handled adequately.
7. No translation produced, but not rephrased in
a way the system handled adequately.
3.2 Assigning utterances to categories
At the moment, medical professionals will only
accept the validity of category assignments made
by trained physicians. In the worst case, it is
clearly true that a layman, even one who has re-
ceived some training, will not be able to determine
whether or not a mistranslation has clinical signif-
icance.
Physician time is, however, a scarce and valu-
able resource, and, as usual, typical case and worst
case may be very different. Particularly for routine
testing during system development, it is clearly not
possible to rely on expert physician assessments.
We consequently suggest a compromise strategy.
We will first carry out an evaluation using medical
experts, in order to establish a gold standard. We
will then repeat this evaluation using non-experts,
and determine how large the differential is in prac-
tice.
We initially intend to experiment with two dif-
ferent groups of non-experts. At Geneva Uni-
versity, we will use students from the School of
Translation. These students will be selected for
competence in English and Spanish, and will re-
ceive a few hours of training on determination of
clinical significance in translation, using guide-
lines developed in collaboration with Glenn Flores
and his colleagues at the UT Southwestern Medi-
cal Center, Texas. Given that the corpus material
is simple and sterotypical, we think that this ap-
proach should yield a useful approximation to ex-
pert judgements.
Although translation students are far cheaper
than doctors, they are still quite expensive, and
evaluation turn-around will be slow. For these rea-
sons, we also propose to investigate the idea of per-
forming evaluations using Amazon?s Mechanical
Turk4. This will be done by Dolores Labs, a new
startup specialising in Turk-based crowdsourcing.
3.3 Scores for categories
We have not yet agreed on exact scores for the
different categories, and this is something that is
4http://www.mturk.com/mturk/welcome
62
probably best decided after mutual discussion at
the workshop. Some basic principles will be evi-
dent from the preceding discussion. The scale will
be normalised so that failure to produce a trans-
lation is counted as zero; potentially dangerous
mistranslations will be associated with a negative
score large in comparison to the positive score for
a useful correct translation. Inability to communi-
cate can certainly be dangerous (this is the point of
having a translation system in the first place), but
mistakenly believing that one has communicated
is usually much worse. As Mark Twain put it: ?It
ain?t what you don?t know that gets you into trou-
ble. It?s what you know for sure that just ain?t so?.
3.4 Discarding uncertain responses
Given that both speech recognition and machine
translation are uncertain technologies, a high
penalty for mistranslations means that systems
which attempt to translate everything may eas-
ily end up with an average negative score - in
other words, they would score worse than a system
which did nothing! For the shared task to be in-
teresting, we must address this problem, and in the
doctor to patient direction there is a natural way
to do so. Since the doctor can reasonably be as-
sumed to be a trained professional who has had
time to learn to operate the system, we can say that
he has the option of aborting any translation where
the machine does not appear to have understood
correctly.
We thus relativise the task with respect to a ?fil-
ter?: for each utterance, we produce both a transla-
tion in the target language, and a ?reference trans-
lation? in the source language, which in some way
gives information about what the machine has un-
derstood. The simplest way to produce this ?ref-
erence translation? is to show the words produced
by speech recognition. When scoring, we evaluate
both translations, and ignore all examples where
the reference translation is evaluated as incorrect.
To go back to the ?penicillin? example, suppose
that Spanish source-language speech recognition
has incorrectly recognised La penicilina me da
alergias as La penicilina no me da alergias. Even
if this produces the seriously incorrect translation
?I am not allergic to penicillin?, we can score it
as a zero rather than a negative, on the grounds
that the speech recognition result already shows
the Spanish-speaking doctor that something has
gone wrong before any translation has happened.
The reference translation may also be produced in
a more elaborate way; a common approach is to
translate back from the target language result into
the source language.
Although the ?filtered? version of the medical
speech translation task makes good sense in the
doctor to patient direction, it is less clear how
meaningful it is in the patient to doctor direction.
Most patients will not have used the system before,
and may be distressed or in pain. It is consequently
less reasonable to expect them to be able to pay at-
tention to the reference translation when using the
system.
4 Summary and conclusions
The preceding notes are intended to form a frame-
work which will serve as a basis for discussion at
the workshop. As already indicated, the key chal-
lenge here is to arrive at metrics which are ac-
ceptable to both the medical and the speech and
language community. This will certainly require
more negotiation. We are however encouraged by
the fact that the proposal, as presented here, has
been developed jointly by representatives of both
communities, and that we appear to be fairly near
agreement. Another important parameter which
we have intentionally left blank is the duration of
the task; we think it will be more productive to de-
termine this based on the schedules of interested
parties.
Realistically, the initial definition of the metric
can hardly be more than a rough guess. Experi-
mentation during the course of the shared task will
probably show that some adjustment will be desir-
able, in order to make it conform more closely to
the requirements of the medical community. If we
do this, we will, in the interests of fairness, score
competing systems using all versions of the metric.
63
