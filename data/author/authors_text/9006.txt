BioNLP 2007: Biological, translational, and clinical language processing, pages 41?48,
Prague, June 2007. c?2007 Association for Computational Linguistics
Combining Multiple Evidence for Gene Symbol Disambiguation  
Hua Xu 
Dept. of Biomedical Informatics, 
Columbia University 
622 W 168th St. NY, USA 
hux7002@dbmi.columbia.edu 
Jung-Wei Fan 
Dept. of Biomedical Infor-
matics, Columbia University 
622 W 168th St. NY, USA 
fan@dbmi.columbia.edu
Carol Friedman 
Dept. of Biomedical Informatics, 
Columbia University 
622 W 168th St. NY, USA 
friedman@dbmi.columbia.edu
   
Abstract 
Gene names and symbols are important 
biomedical entities, but are highly 
ambiguous. This ambiguity affects the 
performance of both information extraction 
and information retrieval systems in the 
biomedical domain. Existing knowledge 
sources contain different types of 
information about genes and could be used 
to disambiguate gene symbols. In this 
paper, we applied an information retrieval 
(IR) based method for human gene symbol 
disambiguation and studied different 
methods to combine various types of 
information from available knowledge 
sources. Results showed that a combination 
of evidence usually improved performance. 
The combination method using coefficients 
obtained from a logistic regression model 
reached the highest precision of 92.2% on a 
testing set of ambiguous human gene 
symbols.         
1 Introduction 
In the past decade, biomedical discoveries and 
publications have increased exponentially due to 
high-throughput technologies such as automated 
genomic sequencing, and therefore, it is impossible 
for researchers to keep up-to-date with the most 
recent knowledge by manually reading the litera-
ture. Therefore, automated text mining tools, such 
as information retrieval and information extraction 
systems, have received great amounts of interest 
(Erhardt et al, 2006; Krallinger and Valencia, 
2005). Biomedical entity recognition is a  first cru-
cial step for text mining tools in this domain, but is 
a very challenging task, partially due to the ambi-
guity (one name referring to different entities) of 
names in the biomedical field.  
Genes are among the most important biological 
entities for understanding biological functions and 
processes, but gene names and symbols are highly 
ambiguous. Chen et al (2005) obtained gene in-
formation from 21 organisms and found that ambi-
guities within species, across species, with English 
words and with medical terms were 5.02%, 
13.43%, 1.10%, 2.99%, respectively, when both 
official gene symbols and aliases were considered. 
When mining MEDLINE abstracts, they found that 
85.1% of mouse genes in the articles were am-
biguous with other gene names. Recently, Fundel 
and Zimmer (2006) studied gene/protein nomen-
clature in 5 public databases. Their results showed 
that the ambiguity problem was not trivial. The 
degree of ambiguity also varied among different 
organisms. Unlike other abbreviations in the litera-
ture, which usually are accompanied by their cor-
responding long forms, many gene symbols occur 
alone without any mention of their long forms. Ac-
cording to Schuemie et al (2004), only 30% of 
gene symbols in abstracts and 18% in full text 
were accompanied by their corresponding full 
names, which makes the task of gene symbol nor-
malization much harder.  
Gene symbol disambiguation (GSD) is a par-
ticular case of word sense disambiguation (WSD), 
which has been extensively studied in the domain 
of general English. One type of method for WSD 
uses established knowledge bases, such as a ma-
chine readable dictionary (Lesk, 1986; Harley and 
Glennon, 1997). Another type of WSD method 
uses supervised machine learning (ML) technolo-
41
gies (Bruce and Wiebe, 1994; Lee and Ng, 2002; 
Liu et al, 2002).  
In the biomedical domain, there are many gene 
related knowledge sources, such as Entrez Gene 
(Maglott et al, 2005), developed at NCBI (Na-
tional Center for Biotechnology Information), 
which have been used for gene symbol disam-
biguation. Podowski et al (2004) used MEDLINE 
references in the LocusLink and SwissProt data-
bases to build Bayesian classifiers for GSD. A 
validation on MEDLINE documents for a set of 66 
human genes showed most accuracies were greater 
than 90% if there was enough training data (more 
than 20 abstracts for each gene sense). 
More recently, information retrieval (IR) based 
approaches have been applied to resolve gene am-
biguity using existing knowledge sources. Typi-
cally, a profile vector for each gene sense is built 
from available knowledge source(s) and a context 
vector is derived from the context where the am-
biguous gene occurs. Then similarities between the 
context vector and candidate gene profile vectors 
are calculated, and the gene corresponding to the 
gene profile vector that has the highest similarity 
score to the context vector is selected as the correct 
sense. Schijvenaars et al (2005) reported on an IR-
based method for human GSD. It utilized informa-
tion from either Online Mendelian Inheritance in 
Man (OMIM) annotation or MEDLINE abstracts.  
The system achieved an accuracy rate of 92.7% on 
an automatically generated testing set when five 
abstracts were used for the gene profile. Xu et al 
(2007) studied the performance of an IR-based ap-
proach for GSD for mouse, fly and yeast organ-
isms when different types of information from dif-
ferent knowledge sources were used. They also 
used a simple method to combine different types of 
information and reported that a highest precision of 
93.9% was reached for a testing set of mouse genes 
using multiple types of information.  
In the field of IR, it has been shown that com-
bining heterogeneous evidence improves retrieval 
effectiveness. Studies on combining multiple rep-
resentations of document content (Katzer et al, 
1982), combining results from different queries 
(Xu and Croft, 1996), different ranking algorithms 
(Lee, 1995), and different search systems (Lee, 
1997) have shown improved performance of re-
trieval systems. Different methods have also been 
developed to combine different evidence for IR 
tasks. The inference-network-based framework, 
developed by Turtle and Croft (1991), was able to 
combine different document representations and 
retrieval algorithms into an overall estimate of the 
probability of relevance. Fox et al (1988) extended 
the vector space model to use sub-vectors to de-
scribe different representations derived from 
documents. An overall similarity between a docu-
ment and a query is defined as a weighted linear 
combination of similarities of sub-vectors. A linear 
regression analysis was used to determine the 
value of the coefficients. 
 Though previous related efforts (Schijvenaars et 
al., 2005, Xu et al, 2007) have explored the use of 
multiple types of information from different 
knowledge sources, none have focused on devel-
opment of formal methods for combining multiple 
evidence for the GSD problem to optimize per-
formance of an IR-based method. In this study, we 
adapted various IR-based combination models spe-
cifically for the GSD problem. Our motivation for 
this work is that there are diverse knowledge 
sources containing different types of information 
about genes, and the amount of such information is 
continuously increasing. A primary source contain-
ing gene information is MEDLINE articles, which 
could be linked to specific genes through annota-
tion databases. For example, Entrez Gene contains 
an annotated file called ?gene2pubmed?, which 
lists the PMIDs (PubMed ID) of articles associated 
with a particular gene. From related MEDLINE 
articles, words and different ontological concepts 
can be obtained and then be used as information 
associated with a gene. However they could be 
noisy, because one article could mention multiple 
genes. Another type of source contains summa-
rized annotation of genes, which are more specific 
to certain aspects of genes. For example, Entrez 
Gene contains a file called ?gene2go?. This file 
lists genes and their associated Gene Ontology 
(GO) (Ashburner et al, 2000) codes, which include 
concepts related to biological processes, molecular 
functions, and cellular components of genes. 
Therefore, methods that are able to efficiently 
combine the different types of information from 
the different sources are important to explore for 
the purpose of improving performance of GSD 
systems. In this paper, we describe various models 
for combining different types of information from 
MEDLINE abstracts for IR-based GSD systems. 
We also evaluated the combination models using 
two data sets containing ambiguous human genes. 
42
  
 
Figure 1 Overview of an IR combination-based gene symbol disambiguation approach using different 
types of information.
2 Methods 
In this paper, we extend the IR vector space model 
to be capable of combining different types of gene 
related information in a flexible manner, thus im-
proving the performance of an IR-based GSD sys-
tem. Figure 1 shows an overview of the IR combi-
nation-based approach. We generated three differ-
ent sub-vectors for the context and three for the 
profile, so that each sub-vector corresponded to a 
different type of information. The similarity scores 
between context and profile were measured for 
each type of sub-vector and then combined to gen-
erate the overall similarity scores to determine the 
correct sense. We explored five different combina-
tion methods using two testing sets. 
2.1 Knowledge Sources and Available Infor-
mation 
The ?gene2pubmed? file in Entrez Gene was 
downloaded in January 2006. A profile was then 
built for each gene using information derived from 
the related articles. We used the following three 
types of information: 1) Words in the related 
MEDLINE articles (title and abstract). This is the 
simplest type of information about a gene. General 
English stop words were removed and all other 
words were stemmed using the Porter stemming 
algorithm (Porter, 1980). 2) UMLS (Unified 
Medical Language System) (Bodenreider 2004) 
CUIs (Concept Unique Identifier), which were 
obtained from titles and abstracts of MEDLINE 
articles using an NLP system called MetaMap 
(Aronson 2001). 3) MeSH (Medical Subject 
Headings) terms, which are manually annotated by 
curators based on full-text articles at the National 
Library of Medicine (NLM) of the United States. 
2.2 Document Set and Testing Sets 
Using the ?gene2pubmed? file, we downloaded the 
MEDLINE abstracts that were known to be related 
to human genes. Articles associated with more than 
25 genes (as determined by our observation) were 
excluded, since they mostly discussed high-
throughput technologies and provided less valuable 
information for GSD. This excluded 168 articles 
and yielded a collection of 116,929 abstracts, 
which were used to generate gene profiles and one 
of the test sets. Two test sets were obtained for 
evaluating the combination methods: testing set 1 
was based on the ?gene2pubmed? file, and testing 
set 2 was based on the BioCreAtIvE II evaluation. 
  Testing set 1 was automatically generated from 
the 116,929 abstracts, using the following 3 steps:  
1) Identifying ambiguous gene symbols in the 
abstracts. This involved processing the entire col-
lection of abstracts using an NLP system called 
BioMedLEE (Biomedical Language Extracting and 
Encoding System) (Lussier et al 2006), which was 
shown to identify gene names/symbols with high 
precision when used in conjunction with GO anno-
tations. When an ambiguous gene was identified in 
an article, the candidate gene identifiers (GeneID 
from Entrez Gene) were listed by the NLP system, 
but not disambiguated. For each ambiguous gene 
that was detected, a pair was created consisting of 
the PMID of the article and the gene symbol, so 
that each pair would be considered a possible test-
ing sample. Repeated gene symbols in the same 
article were ignored, because we assumed only one 
sense per gene symbol in the same article. Using 
this method, 69,111 PMID and ambiguous human 
gene symbol pairs were identified from the above 
collection of abstracts. 
43
2) Tagging the correct sense of the ambiguous 
gene symbols. The list of candidate PMID/gene 
symbol pairs generated from the articles was then 
compared with the list of gene identifiers known to 
be associated with the articles based on 
?gene2pubmed?. If one of the candidate gene 
senses matched, that gene sense was assumed to be 
the correct sense. Then the PMID/gene-symbol 
pair was tagged with that sense and set aside as a 
testing sample. We identified a pool of 12,289 test-
ing samples, along with the corresponding tagged 
senses. 
3) Selecting testing set 1. We randomly selected 
2,000 testing samples from the above pool to form 
testing set 1. 
Testing set 2 was derived using the training and 
evaluation sets of the BioCreAtIvE II Gene Nor-
malization (GN) task (Morgan 2007). The Bio-
CreAtIvE II GN task involved mapping human 
gene mentions in MEDLINE abstracts to gene 
identifiers (Entrez Gene ID), which is a broader 
task than the GSD task. However, these abstracts 
were useful for creating a testing set for GSD, be-
cause whenever a gene mention mapped to more 
than one identifier, disambiguation was required. 
Therefore, it was possible to derive a list of am-
biguous gene symbols based on data that was pro-
vided by BioCreAtIvE. We combined both manu-
ally annotated training (281 abstracts) and evalua-
tion (262 abstracts) sets provided by BioCreAtIvE. 
Using the same process as described in step 1 of 
testing set 1, we processed the abstracts and identi-
fied 217 occurrences of ambiguous gene symbols 
from the combined set. Following a similar proce-
dure as was used for step 2 in the testing set 1 (ex-
cept that the reference standard in this case was the 
manually annotated results obtained from Bio-
CreAtIvE instead of ?gene2pubmed?), we obtained 
124 PMID/gene-symbol pairs with the correspond-
ing tagged senses, which formed testing set 2. 
Because one article may contain multiple am-
biguous gene symbols, a total of 2,048 PMIDs 
were obtained from both testing sets 1 and 2. Arti-
cles with those PMIDs were excluded from the 
collection of 116,929 abstracts. We used the re-
maining document set to generate gene profiles, 
which were used for both testing sets. 
2.3 Profile and Context Vectors 
For each gene in ?gene2pubmed? file, we created a 
profile. It consisted of three sub-vectors containing 
word, CUI, or MeSH, respectively, using the in-
formation derived from the related MEDLINE ab-
stracts. Similarly, a context vector was also formed 
for each testing sample, using three sub-vectors 
containing word, CUI, or MeSH, which were de-
rived from the abstract whose PMID was stated in 
the testing sample.  The tf-idf weighting schema 
(Salton and Buckley, 1988) was used to assign 
weights to index terms in the profile and context 
sub-vectors. Given a document d, the Term Fre-
quency (tf) of term t is defined as the frequency of 
t occurring in d. The Inverse Document Frequency 
(idf) of term t is defined as the logarithm of the 
number of all documents in the collection divided 
by the number of documents containing the term t. 
Then term t in document d is weighted as tf*idf. 
2.4 Similarity Measurement 
The similarity score between the same type of con-
text and profile sub-vectors were measured as co-
sine similarity of two vectors. The cosine similarity 
between two vectors a and b is defined as the inner 
product of a and b, normalized by the length of 
two vectors. See the formula below: 
Sim(a,b) = cosine ? = 
ba
ba ?
  where  
22
2
2
1 ... naaaa +++=    22221 ... nbbbb +++=   
 
We built three basic classifiers that used only 
one type of sub-vector: word, CUI, or MeSH, re-
spectively, recorded three individual similarity 
scores of each sub-vector for each candidate gene 
of all testing samples. We implemented five meth-
ods to combine similarity scores from each basic 
classifier, which are described as follows: 
1) CombMax - Each individual similarity score 
from a basic classifier was normalized by di-
viding the sum of similarity scores of all 
candidate genes for that basic classifier. 
Then the decision made by the classifier with 
the highest normalized score was selected as 
the final decision of the combined method. 
2) CombSum - Each individual similarity score 
from a basic classifier was normalized by di-
viding the maximum similarity score of all 
candidate genes for that basic classifier. The 
overall similarity score of a candidate gene 
was considered to be the sum of the normal-
ized similarity scores from all three basic 
classifiers for that gene. The candidate gene 
44
with the highest overall similarity was se-
lected as the correct sense. 
3) CombSumVote - The overall similarity score 
was considered as the similarity score from 
CombSum, multiplied by the number of basic 
classifiers that voted for that gene as the cor-
rect sense.  
4) CombLR - The overall similarity score was 
defined as a predicted probability (P) of be-
ing the correct sense, given the coefficients 
obtained from a logistic regression model 
and similarity scores from all three basic 
classifiers for that gene. The relation be-
tween dependent variable (probability of be-
ing the correct sense) and independent vari-
ables (similarity scores from individual basic 
classifiers) of the logistic regression model is 
shown below, where Cs (Cword, Ccui, Cmesh and 
C) are the coefficients, and SIMs (SIMword, 
SIMcui, SIMmesh) are the individual similarity 
scores from the basic classifiers. To obtain 
the model, we divided 2,000 testing samples 
into a training set and a testing set, as de-
scribed in section 2.5. For samples in the 
training set, the correct gene senses were la-
beled as ?1? and incorrect gene senses were 
labeled as ?0?. Then logistic regression was 
applied, taking the binary labels as the value 
of the dependent variable and the similarities 
from the basic classifiers as the independent 
variables. In testing, coefficients obtained 
from training were used to predict each can-
didate gene?s probability of being the correct 
sense for a given ambiguous symbol. 
 
CSIMmeshCmeshSIMcuiCcuiSIMwordCword
CSIMmeshCmeshSIMcuiCcuiSIMwordCword
e
e
P +++
+++
+= ***
***
1
 
 
5) CombRank ? Instead of using the similarity 
scores, we ranked the similarity scores and 
used the rank to determine the combined 
output. Following a procedure called Borda 
count (Black, 1958), the top predicted gene 
sense was given a ranking score of N-1, the 
second top was given N-2, and so on, where 
N is the total number of candidate senses. 
After each sense was ranked for each basic 
classifier, the combined ranking score of a 
candidate gene was determined by the sum 
of ranking scores from all three basic classi-
fiers.  The sense with the highest combined 
ranking score was selected as the correct 
sense. 
2.5 Experiments and Evaluation 
In this study, we measured both precision and cov-
erage of IR-based GSD approaches. Precision was 
defined as the ratio between the number of cor-
rectly disambiguated samples and the number of 
total testing samples for which the disambiguation 
method yielded a decision. When a candidate gene 
had an empty profile or different candidate gene 
profiles had the same similarity scores (e.g. zero 
score) with a particular context vector, the disam-
biguation method was not able to make a decision. 
Therefore, we also reported on coverage, which 
was defined as the number of testing samples that 
could be disambiguated using the profile-based 
method over the total number of testing samples. 
We evaluated precision and coverage of different 
combined methods for gene symbol disambigua-
tion on both testing sets. 
Results of three basic classifiers that used a sin-
gle type of information were reported as well. We 
also defined a baseline method. It used the major-
ity sense of an ambiguous gene symbol as the cor-
rect sense. The majority sense is defined as the 
gene sense which was associated with the most 
MEDLINE articles based on the ?gene2pubmed? 
file.  
To evaluate the CombLR, we used 10-fold cross 
validation. We divided the sense-tagged testing set 
into 10 equal partitions, which resulted in 200 test-
ing samples for each partition. When one partition 
was used for testing, the remaining nine partitions 
were combined and used for training, which also 
involved deriving coefficients for each round. To 
make other combination methods comparable with 
CombLR, we tested the performance of other com-
bination methods on the same partitions as well. 
Therefore, we had 10 measurements for each com-
bination method. Mean precision and mean cover-
age were reported for those 10 measurements. For 
testing set 2, we did not test the CombLR method 
because the set was too small to train a regression 
model. 
We used Friedman?s Test (Friedman, 1937) fol-
lowed by Dunn?s Test (Dunn, 1964), which are 
non-parametric tests, to assess whether there were 
significant differences in terms of median precision 
among the different single or combined methods. 
45
3 Results 
Results of different combination methods for test-
ing set 1 are shown in Table 1, which contains the 
mean precision and coverage for 10-fold cross 
validation, as well as the standard errors in paren-
theses. All IR-based gene symbol disambiguation 
approaches showed large improvements when 
compared to the baseline method. All of the com-
bination methods showed improved performance 
when compared to results from any run that used a 
single type of information. Among the five differ-
ent combination methods, CombLR achieved the 
highest mean precision of 0.922 for testing set 1. 
CombSum, which is a simple combination method, 
also had a good mean precision of 0.920 on testing 
set 1. The third Column of Table 1 shows that cov-
erage was in a range of 0.936-0.938. 
 
Table 1. Results on testing set 1. 
 
Table 2. Results on testing set 2. 
 
We performed Friedman?s test followed by 
Dunn?s test on each single run: word, CUI or 
MeSH, with all combination runs respectively. 
Friedman tests showed that differences of median 
precisions among the different methods were sta-
tistically significant at ?=0.05.  Dunn tests showed 
that combination runs CombSum, CombSumVote, 
CombLR, and CombRank were statistically signifi-
cantly better than single runs using word or CUI. 
For single run using MeSH, combination runs 
CombLR and CombSum were statistically signifi-
cantly better. 
The results of different runs on testing set 2 are 
shown in Table 2. Most combined methods, except 
CombRank, showed improved precision. The high-
est precision of 0.906 was reached when using 
CombSum and CombMax methods. Note that the 
logistic regression method was not applicable. The 
coverage for testing set 2 was 0.944 for all of the 
methods. 
4 Discussion 
4.1 Why Combine? 
As stated in Croft (2002), a Bayesian probabilistic 
framework could provide the theoretical justifica-
tion for evidence combination. Additional evidence 
with smaller errors can reduce the effect of large 
errors from one piece of evidence and lower the 
average error.  
The idea behind CombMax was to use the single 
classifier that had the most confidence, but it did 
not seem to improve performance very much be-
cause it ignored evidence from the other two basic 
classifiers. The CombSum was a simple combina-
tion method, but with reasonable performance, 
which was also observed by other studies for the 
IR task (Fox and Shaw, 1994).  CombSumVote was 
a variant of CombSum. It favors the candidate 
genes selected by more basic classifiers. In Lee 
(1997), a similar implementation of CombSumVote 
(named ?CombMNZ?) also achieved better per-
formance in the IR task. CombLR, the combination 
method trained on a logistic regression model, 
achieved the best performance in this study. It used 
a set of coefficients derived from the training data 
when combining the similarities from individual 
basic classifiers. Therefore, it could be considered 
as a more complicated linear combination model 
than CombSum. In situations where training data is 
not available, CombSum or CombSumVote would 
be a good choice. CombRank did not perform as 
well as methods that used similarity scores, proba-
bly due to the loss of subtle probability information 
in the similarity scores. We explored ranking be-
cause it was independent of the weighting schema 
and could be valuable if it performed well. 
Run Precision Coverage 
Baseline 0.707 (0.032) 0.992 (0.005) 
Word 0.882 (0.023)  0.937 (0.017) 
CUI 0.887 (0.022) 0.938 (0.017) 
MeSH 0.900 (0.021) 0.936 (0.017) 
CombMax 0.909 (0.020) 0.938 (0.017) 
CombSum 0.920 (0.019) 0.937 (0.017) 
CombSumVote 0.917(0.019) 0.938 (0.017) 
CombLR 0.922 (0.019) 0.938 (0.017) 
CombRank 0.918 (0.020) 0.938 (0.017) 
Run Precision Coverage 
Baseline 0.593 0.991 
Word 0.872 0.944 
CUI 0.897 0.944 
MeSH 0.863 0.944 
CombMax 0.906 0.944 
CombSum 0.906 0.944 
CombSumVote 0.897 0.944 
CombRank 0.889 0.944 
46
The typical scenario where combination should 
help is when a classifier based on one type of in-
formation made a wrong prediction, but the 
other(s), based on different types of information, 
made the correct predictions. In those cases, the 
overall prediction may be correct when an appro-
priate combination method applies. For example, 
an ambiguous gene symbol PDK1 (in the article 
with PMID 10856237), which has two possible 
gene senses (?GeneID:5163 pyruvate dehydro-
genase kinase, isoenzyme 1? and ?GeneID:5170 3-
phosphoinositide dependent protein kinase-1?), 
was incorrectly predicted as ?GeneID: 5163? when 
only ?word? was used. But the classifiers using 
?CUI? and ?MeSH? predicted it correctly. When 
the CombSum method was used to combine the 
similarity scores from all three classifiers, the cor-
rect sense ?GeneID: 5170? was selected. When all 
three classifiers were incorrect in predicting a test-
ing sample, generally none of the combination 
methods would help in making the final decision 
correct. Therefore, there is an upper bound on the 
performance of the combined system. In our case, 
we detected that all three classifiers made incorrect 
predictions for 65 testing samples of the 2,000 
samples. Therefore, the upper bound would be 
1,935/2,000=96.7%. 
The methods for combining different types of 
information from biomedical knowledge sources 
described in this study, though targeted to the GSD 
problem, could be also applicable to other text 
mining tasks that are based on similarity measure-
ment, such as text categorization, clustering, and 
the IR task in the biomedical domain.  
4.2 Coverage of the Methods 
The IR-based gene symbol disambiguation method 
described in this paper aims to resolve intra-
species gene ambiguity. We focused on ambiguous 
gene symbols within the human species and used 
articles known to be associated with human genes. 
Fundel and Zimmer (2006) reported that the degree 
of ambiguity of the human gene symbols from En-
trez Gene was 3.16%?3.32%, which is substantial. 
However, this is only part of the gene ambiguity 
problem.  
Based on the ?gene_info? file downloaded in 
January 2006 from Entrez Gene, there were a total 
of 32,852 human genes. Based on the 
?gene2pubmed? file, 24,170 (73.4%) out of 32,852 
human genes have at least one associated MED-
LINE article, which indicates that profiles could be 
generated for at least 73.4% of human genes. On 
average, there are 9.02 MEDLINE articles associ-
ated with a particular human gene. Coverage re-
ported in this study was relatively high because the 
testing samples were selected from annotated arti-
cles as listed in ?gene2pubmed?, and not randomly 
from the collection of all MEDLINE abstracts. 
4.3 Evaluation Issues 
It would be interesting to compare our work with 
other related work, but that would require use of 
the same testing set. For example, it is not straight-
forward to compare our precision result (92.2%) 
with that (92.7%) reported by Schijvenaars et al 
(2005), because they used a testing set that was 
generated by removing ambiguous genes with less 
than 6 associated articles for each of their senses, 
and they did not report on coverage. The data set 
from the BioCreAtIvE II GN task therefore is a 
valuable testing set that enables evaluation and 
comparison of other gene symbol disambiguation 
methods. From the BioCreAtIvE abstracts, we 
identified 217 occurrences of ambiguous gene 
symbols, but only 124 were annotated in the Bio-
CreAtIvE data set. There are a few possible expla-
nations for this. First, the version of the Entrez 
Gene database used by the NLP system was not the 
most recent one, so some new genes were not 
listed as possible candidate senses. The second is-
sue is related to gene families or genes/proteins 
with multiple sub-units. According to the 
?gene_info? file, the gene symbol ?IL-1? is a syno-
nym for both ?GeneID: 3552 interleukin 1, alpha? 
and ?GeneID: 3553 interleukin 1, beta?. Therefore, 
the NLP system identified it as an ambiguous gene 
symbol.  When annotators in the BioCreAtIvE II 
task saw a gene family name that was not clearly 
mapped to a specific gene identifier in Entrez 
Gene, they may not have added it to the mapped 
list. In Morgan et al (2007), it was suggested that 
mapping gene family mentions might be appropri-
ate for those entities. Testing set 2 was a small set 
and results from that set might not be statistically 
meaningful, but it is useful for comparing with 
others working on the same data set. 
In this paper, we focused on the study of im-
provements in precision of the gene symbol dis-
ambiguation system. When combining information 
from different knowledge sources, coverage may 
47
also be increased by benefiting from the cross-
coverage of different knowledge sources.  
5 Conclusion and Future Work 
We applied an IR-based approach for human gene 
symbol disambiguation, focusing on a study of 
different methods for combining various types of 
information from available knowledge sources. 
Results showed that combination of multiple 
evidence usually improved the performance of 
gene symbol disambiguation. The combination 
method using coefficients obtained from a logistic 
regression model reached the highest precision of 
92.2% on an automatically generated testing set of 
ambiguous human gene symbols. On a testing set 
derived from BioCreAtIvE II GN task, the combi-
nation method that performed summation of indi-
vidual similarities reached the highest precision of 
90.6%. However, the regression-based method 
could not be used, because the testing sample was 
small.  
In the future, we will add information that is 
specifically related to genes, such as GO codes, 
into the combination model. Meanwhile, we will 
also study the performance gain in terms of 
coverage by integrating different knowledge 
sources.    
Acknowledgements 
This work was supported in part by Grants R01 
LM7659 and R01 LM8635 from the National Li-
brary of Medicine, and Grant NSF-IIS-0430743 
from the National Science Foundation. We would 
like to thank Alexander Morgan for providing the 
evaluation set from the BioCreAtIvE II GN task.  
References 
Aronson, A. R. 2001. Proc. AMIA. Symp., 17-21. 
Ashburner, M. et al 2000. Nat Genet, 25, 25-29. 
Black, D. 1958. Cambridge University Press. 
Bodenreider, O. 2004. Nucleic Acids Research, 2004, 
32, D267-D270.
Bruce, R. and Wiebe, J. 1994. Proceedings of ACL 
1994, 139-146. 
Chen, L., Liu, H. and Friedman, C. 2005. Bioinformat-
ics, 21, 248-256. 
Croft, W. 2002. Advances in Information Retrieval. 
Springer Netherlands, Chapter 1, 1-36 
Dunn, O. J. 1964. Technometrics, 6, 241-252. 
Erhardt, R.A., Schneider, R. and Blaschke, C. 2006. 
Drug Discov. Today, 11, 315-325. 
Fox, E., Nunn, G., and Lee, W. 1988. Proceedings of 
the 11th ACM SIGIR Conference on Research and 
Development in Information Retrieval, 291?308. 
Fox, E. and Shaw, J. 1994. Proceedings TREC-2, 243?
252. 
Friedman, M. 1937. Journal of the American Statistical 
Association, 32, 675-701. 
Fundel, K. and Zimmer, R. 2006. BMC. Bioinformatics., 
7: 372. 
Harley, A. and Glennon, D. 1997. Proc. SIGLEX Work-
shop "Tagging Text With Lexical Semantics", 74-78. 
Katzer, J., McGill, M., Tessier, J., Frakes,W., and 
DasGupta, P. 1998. Information Technology: Re-
search and Development, 1(4):261?274. 
Krallinger, M. and Valencia, A. 2005. Genome Biol., 6, 
224. 
Lee, J. 1995. Proceedings of the 18th ACMSIGIR Con-
ference on Research and Development in Information 
Retrieval, 180?188. 
Lee, J. 1997. Proceedings of the 20th ACM SIGIR Con-
ference on Research and Development in Information 
Retrieval, 267?276. 
Lee, Y. K. and Ng, H. T. 2002. Proc EMNLP 2002, 41-
48. 
Lesk, M. 1986. 1986 SIGDOC Conference, 24-26. 
Liu, H., Johnson, S. B. and Friedman, C. 2002. J. Am. 
Med. Inform. Assoc., 9, 621-636. 
Lussier, Y., Borlawsky, T., Rappaport, D., Liu, Y., 
Friedman, C. 2006. Pac. Symp. Biocomput., 11, 64-
75. 
Maglott D, Ostell J, Pruitt KD, Tatusova T. 2005. Nu-
cleic Acids Res., 3, D54-D58. 
Morgan, A., Wellner, B., Colombe, J. B., Arens, R., 
Colosimo, M. E., Hirschman L.  2007. Pacific Sym-
posium on Biocomputing 12:281-291. 
Podowski, R.M., Cleary, J.G., Goncharoff, N.T., 
Amoutzias, G., Hayes W.S. 2004. Proc IEEE Com-
put Syst Bioinform Conf, 2004, 415-24. 
Porter,M.F. 1980. Program, 14, 130-137. 
Salton, G. and Buckley, C. 1988. Information 
Processing & Management, 24, 513-523. 
Schijvenaars, B.JA. et al 2005. BMC. Bioinformatics., 
6:149. 
Schuemie, M.J. et al 2004. Bioinformatics, 20, 2597-
2604. 
Turtle, H. and Croft, W. 1991. ACM Transactions on 
Information Systems, 9(3):187?222. 
Xu, H., Fan, J. W., Hripcsak, G., Mendon?a A. E., Mar-
katou, M., Friedman, C. 2007. Bioinformatics, doi: 
10.1093/bioinformatics/btm056 
Xu, J. and Croft,W. 1996. Proceedings of the 19th ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, pages 4?11. 
 
48
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1272?1280,
Beijing, August 2010
Grouping Product Features Using Semi-Supervised Learning
with Soft-Constraints* 
Zhongwu Zhai?, Bing Liu?, Hua Xu? and Peifa Jia?
?State Key Lab of Intelligent Tech. & Sys. 
Tsinghua National Lab for Info. Sci. and Tech. 
Dept. of Comp. Sci. & Tech., Tsinghua Univ. 
zhaizhongwu@gmail.com
?Dept. of Comp. Sci.
University of Illinois at Chicago 
liub@cs.uic.edu
Abstract
In opinion mining of product reviews, one of-
ten wants to produce a summary of opinions 
based on product features/attributes. Howev-
er, for the same feature, people can express it 
with different words and phrases. To produce 
a meaningful summary, these words and 
phrases, which are domain synonyms, need to 
be grouped under the same feature group. 
This paper proposes a constrained semi-
supervised learning method to solve the prob-
lem. Experimental results using reviews from 
five different domains show that the proposed 
method is competent for the task. It outper-
forms the original EM and the state-of-the-art 
existing methods by a large margin. 
1 Introduction*
One form of opinion mining in product reviews 
is to produce a feature-based summary (Hu and 
Liu, 2004a; Liu, 2010). In this model, product 
features are first identified, and positive and neg-
ative opinions on them are aggregated to produce 
a summary on the features. Features of a product 
are attributes, components and other aspects of 
the product, e.g., ?picture quality?, ?battery life? 
and ?zoom? of a digital camera. 
In reviews (or any writings), people often use 
different words and phrases to describe the same 
product feature. For example, ?picture? and 
?photo? refer to the same feature for cameras. 
Grouping such synonyms is critical for effective 
opinion summary. Although WorldNet and other 
*Supported by National Natural Science Foundation of Chi-
na (Grant No: 60875073). 
    This work was done when the first author was visiting 
Bing Liu?s group at the University of Illinois at Chicago.  
thesaurus dictionaries can help to some extent, 
they are far from sufficient due to a few reasons. 
First, many words and phrases that are not syn-
onyms in a dictionary may refer to the same fea-
ture in an application domain. For example, ?ap-
pearance? and ?design? are not synonymous, but 
they can indicate the same feature, design.
Second, many synonyms are domain dependent. 
For example, ?movie? and ?picture? are syn-
onyms in movie reviews, but they are not syn-
onyms in camera reviews as ?picture? is more 
likely to be synonymous to ?photo? while ?mov-
ie? to ?video?. Third, determining which expres-
sions indicate the same feature can be dependent 
on the user?s application need. For example, in 
car reviews, internal design and external design 
can be regarded as two separate features, but can 
also be regarded as one feature, called ?design?, 
based to the level of details that the user needs to 
study. In camera reviews, one may want to study 
battery as a whole (one feature), or as more than 
one feature, e.g., battery weight, and battery life. 
Due to this reason, in applications the user needs 
to be involved in synonym grouping.  
Before going further, let us introduce two con-
cepts, feature group and feature expression. Fea-
ture group (or feature for short) is the name of a 
feature (given by the user), while a feature ex-
pression of a feature is a word or phrase that ac-
tually appears in a review to indicate the feature. 
For example, a feature group could be named 
?picture quality?, but there are many possible 
expressions indicating the feature, e.g., ?picture?, 
?photo?, ?image?, and even the ?picture quality? 
itself. All the feature expressions in a feature 
group signify the same feature.  
Grouping feature expressions manually into 
suitable groups is time consuming as there are 
1272
often hundreds of feature expressions. This paper 
helps the user to perform the task more efficient-
ly. To focus our research, we assume that feature 
expressions have been discovered from a review 
corpus by an existing system such as those in 
(Hu and Liu, 2004b; Popescu and Etzioni, 2005; 
Kim and Hovy, 2006; Kobayashi et al, 2007; 
Mei et al, 2007; Stoyanov and Cardie, 2008; Jin
et al, 2009; Ku et al, 2009). 
To reflect the user needs, he/she can manually 
label a small number of seeds for each feature 
group. The feature groups are also provided by 
the user based on his/her application needs. The 
system then assigns the rest of the feature ex-
pressions to suitable groups. To the best of our 
knowledge, this problem has not been studied in 
opinion mining (Pang and Lee, 2008).  
The problem can be formulated as semi-
supervised learning. The small set of seeds la-
beled by the user is the labeled data, and the rest 
of the discovered feature expressions are the un-
labeled data. This is the transductive setting 
(Joachims, 1999) because the unlabeled set is 
used in learning and also in testing since our ob-
jective is to assign unlabeled expressions to the 
right feature groups.  
Any semi-supervised learning method can be 
applied to tackle the problem. In this work, we 
use the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). Specifically, we 
use the na?ve Bayesian EM formulation in 
(Nigam et al, 2000), which runs a Bayesian clas-
sifier iteratively on the labeled and unlabeled 
data until the probabilities for the unlabeled data 
converge. When the algorithm ends, each unla-
beled example is assigned a posterior probability 
of belonging to each group.  
However, we can do better since the EM algo-
rithm only achieves local optimal. What local 
optimal it achieves depends on the initialization, 
i.e., the initial seeds. We show that some prior 
knowledge can help provide a better initialization, 
and consequently generate better grouping results. 
Thus, we propose to create another set of data 
extracted from the unlabeled set based on two 
pieces of natural language knowledge: 
1. Feature expressions sharing some common 
words are likely to belong to the same group, 
e.g., ?battery life? and ?battery power?. 
2. Feature expressions that are synonyms in a 
dictionary are likely to belong to the same 
group, e.g., ?movie? and ?picture?.  
We call these two pieces of prior knowledge soft 
constraints because they constrain the feature 
expressions to be in the same feature group. The 
constraints are soft (rather than hard) as they can 
be relaxed in the learning process. This relaxa-
tion is important because the above two con-
straints can result in wrong groupings. The EM 
algorithm is allowed to re-assign them to other 
groups in the learning process.  
We call the proposed framework constrained 
semi-supervised learning. Since we use EM and 
soft constraints, we call the proposed method SC-
EM. Clearly, the problem can also be attempted 
using some other techniques, e.g., topic modeling 
(e.g, LDA (Blei et al, 2003)), or clustering using 
distributional similarity (Pereira et al, 1993; Lin, 
1998; Chen et al, 2006; Sahami and Heilman, 
2006). However, our results show that these me-
thods do not perform as well. 
The input to the proposed algorithm consists 
of: a set of reviews R, and a set of discovered 
feature expressions F from R (using an existing 
algorithm). The user labels a small set of feature 
expressions, i.e., assigning them to the user-
specified feature groups. The system then assigns 
the rest of the discovered features to the feature 
groups. EM is run using the distributional (or 
surrounding words) contexts of feature expres-
sions in review set R to build a na?ve Bayesian 
classifier in each iteration.  
Our evaluation was conducted using reviews 
from 5 different domains (insurance, mattress, 
vacuum, car and home-theater). The results show 
that the proposed method outperforms different 
variations of the topic modeling method LDA, k-
means clustering, and the recent unsupervised 
feature grouping method mLSA.  
In summary, this paper makes three main con-
tributions:
1. It proposes a new sub-problem of opinion 
mining, i.e., grouping feature expressions in 
the context of semi-supervised learning. Al-
though there are existing methods for solving 
the problem based on unsupervised learning, 
we argue that for practical use some form of 
supervision from the user is necessary to let 
the system know what the user wants.  
2. An EM formulation is used to solve the prob-
lem. We augment EM with two soft con-
straints. These constraints help guide EM to 
1273
produce better solutions. We note that these 
constraints can be relaxed in the process to 
correct the imperfection of the constraints.  
3. It is shown experimentally the new method 
outperforms the main existing state-of-the-art 
methods that can be applied to the task.  
2 Related Work 
This work is mainly related to existing research 
on synonyms grouping, which clusters words and 
phrases based on some form of similarity.  
The methods for measuring word similarity 
can be classified into two main types (Agirre et 
al., 2009): those relying on pre-existing know-
ledge resources (e.g., thesauri, or taxonomies) 
(Yang and Powers, 2005; Alvarez and Lim, 2007; 
Hughes and Ramage, 2007), and those based on 
distributional properties (Pereira et al, 1993; 
Lin, 1998; Chen et al, 2006; Sahami and 
Heilman, 2006; Pantel et al, 2009).   
In the category that relies on existing know-
ledge sources, the work of Carenini et al (2005) 
is most related to ours. The authors proposed a 
method to map feature expressions to a given 
domain feature taxonomy, using several similari-
ty metrics on WordNet. This work does not use 
the word distribution information, which is its 
main weakness because many expressions of the 
same feature are not synonyms in WordNet as 
they are domain/application dependent. Dictiona-
ries do not contain domain specific knowledge, 
for which a domain corpus is needed.
Another related work is distributional similari-
ty, i.e., words with similar meaning tend to ap-
pear in similar contexts (Harris, 1968). As such, 
it fetches the surrounding words as context for 
each term. Similarity measures such as Cosine,
Jaccard, Dice, etc (Lee, 1999), can be employed 
to compute the similarities between the seeds and 
other feature expressions. To suit our need, we 
tested the k-means clustering with distributional 
similarity. However, it does not perform as well 
as the proposed method.  
Recent work also applied topic modeling (e.g., 
LDA) to solve the problem. Guo et al (2009) 
proposed a multilevel latent semantic association 
technique (called mLSA) to group product feature 
expressions, which runs LDA twice. However, 
mLSA is an unsupervised approach. For our eval-
uation, we still implemented the method and 
compared it with our SC-EM method.  
Our work is also related to constrained cluster-
ing (Wagstaff et al, 2001), which uses two forms 
of constraints, must-link and cannot-link. Must-
links state that some data points must be in the 
same cluster, and cannot-links state that some 
data points cannot be in the same cluster. In 
(Andrzejewski et al, 2009), the two constraints 
are added to LDA, called DF-LDA. We show 
that both these methods do not perform as well as 
our semi-supervised learning method SC-EM.
3 The Proposed Algorithm 
Since our problem can be formulated as semi-
supervised learning, we briefly describe the set-
ting in our context. Given a set C of classes (our 
feature groups), we use L to denote the small set 
of labeled examples (labeled feature expressions 
or seeds), and U the set of unlabeled examples 
(unlabeled feature expressions). A classifier is 
built using L and U to classify every example in 
U to a class. Several existing algorithms can be 
applied. In this work, we use EM as it is efficient 
and it allows prior knowledge to be used easily. 
Below, we first introduce the EM algorithm that 
we use, and then present our augmented EM. The 
constraints and their conflict handling are dis-
cussed in Section 4.  
3.1 Semi-Supervised Learning Using EM 
EM is a popular iterative algorithm for maximum 
likelihood estimation in problems with missing 
data. In our case, the group memberships of the 
unlabeled expressions are considered missing 
because they come without group labels.  
We use the EM algorithm based on na?ve 
Bayesian classification (Nigam et al, 2000). Al-
though it is involved to derive, using it is simple. 
First, a classifier f is learned using only the la-
beled data L (Equations 1 and 2). Then, f is ap-
plied to assign a probabilistic label to each unla-
beled example in U (see Equation 3). Next, a 
new classifier f is learned using both L and the 
newly probabilistically labeled unlabeled exam-
ples in UPL, again using Equations 1 and 2. These 
last two steps iterate until convergence. 
We now explain the notations in the Equations. 
Given a set of training documents D, each docu-
ment di in D is considered as an ordered list of 
words. ????? denotes the k
th word in di, where 
each word is from the vocabulary V={w1, w2,?,
w|V|}. C={c1, c2,?, c|C|} is the set of pre-defined 
1274
classes or groups. Nti is the number of times the 
word wt occurs in document di.
For our problem, the surrounding words con-
texts of the labeled seeds form L, while the sur-
rounding words of the non-seed feature expres-
sions form U. When EM converges, the classifi-
cation labels of the unlabeled feature expressions 
give us the final grouping. Surrounding words 
contexts will be discussed in Section 5. 
3.2 Proposed Soft-Constrained EM 
Although EM can be directly applied to deal with 
our problem, we can do better. As we discussed 
earlier, EM only achieves local optimal based on 
the initialization, i.e., the labeled examples or 
seeds. We show that natural languages con-
straints can be used to provide a better initializa-
tion, i.e., to add more seeds that are likely to be 
correct, called soft-labeled examples or soft seeds 
(SL). Soft-labeled examples are handled diffe-
rently from the original labeled examples in L.
With the soft seeds, we have the proposed soft-
constrained EM (called SC-EM). 
Compared with the original EM, SC-EM has 
two main differences:
y Soft constraints are applied to L and U to pro-
duce a set SL of soft-labeled examples (or soft 
seeds) to initialize EM in addition to L. SL is 
thus a subset of U. The training set size is in-
creased, which helps produce better results as 
our experimental results show.  
y In the first iteration of EM, soft-labeled ex-
amples SL are treated in the same way as the 
labeled examples in L. Thus both SL and L are 
used as labeled examples to learn the initial 
classifier f0. However, in the subsequent itera-
tions, SL is treated in the same way as any ex-
amples in U. That is, the classifier fx from 
each iteration x (including f0) will predict U.
After that, a new classifier is built using both 
L and UPL (which is U with probabilistic la-
1 Laplace smoothing is used to prevent zero probabilities for 
infrequently occurring words. 
bels). Clearly, this implies that the class labels 
of the examples in SL are allowed to change. 
That is also why we call SL the soft-labeled 
set in contrast to the hard-labeled set L, i.e., 
the examples in L will not change labels in 
EM. The reason that SL is allowed to change 
labels/classes is because the constraints can 
make mistakes. EM may be able to correct 
some of the mistakes. 
The detailed algorithm is given in Figure 1. The 
constraints are discussed in Section 4. 
4 Generating SL Using Constraints 
As mentioned earlier, two forms of constraints 
are used to induce the soft-labeled set SL. For 
easy reference, we reproduce them here:  
1. Feature expressions sharing some common 
words are likely to belong to the same group. 
2. Feature expressions that are synonyms in a 
dictionary are likely to belong to one group.  
According to the number of words, feature ex-
pressions can be categorized into single-word 
expressions and phrase expressions. They are 
handled differently. The detailed algorithm is 
given in Figure 2. In the algorithm, L is the la-
beled set and U is the unlabeled set. L, in fact, 
consists of a set of sets, L = {L1, L2, ?, L|L|}. 
Each Li contains a set of labeled examples (fea-
ture expressions) of the ith class (feature group). 
Similarly, the output set SL (the soft-labeled set) 
also consists of a set of sets, i.e., SL = {SL1,
SL2, ?, SL|L|}. Each SLi is a set of soft-labeled 
examples (feature expressions) of the ith class 
???? ??? ?
? ? ? ???? ??????
???
???
??? ? ? ? ???? ??????
???
???
???
???
(11)
? ??? ?
? ? ? ? ??????
???
???
??? ? ???
(21)
? ?????? ?
? ???? ??????? ???
????
???
? ????
???
??? ? ??????????
????
???
(3)
Input:
- Labeled examples L
- Unlabeled examples U
1 Extract SL from U using constraints (Section 4); 
2 Learn an initial na?ve Bayesian classifier f0 using L
? SL and Equations 1 and 2; 
3 repeat
4 // E-Step 
5 for each example di in U (including SL) do
6 Using the current classifier fx to compute 
P(cj|di) using Equation 3. 
7 end
8 // M-Step 
9 Learn a new na?ve Bayesian classifier fx from L
and U by computing P(wt|cj) and P(cj) using 
Equations 1 and 2. 
10 until the classifier parameters stabilize 
Output: the classifier fx from the last iteration.
 Figure 1. The proposed SC-EM algorithm  
1275
(feature group). Thus Li and SLi correspond to 
each other as they represent the original labeled 
examples and the newly soft-labeled examples of 
the ith class (or feature group) respectively.  
The algorithm basically compares each fea-
ture expression u in U (line 1) with each feature 
expression e (line 4) in every labeled subset Li
(line 2) based on the above two constraints. If 
any of the constraints is satisfied (lines 5-17), it 
means that u is likely to belong to Li (or the i
th
class or feature group), and it is added to SLi.
There are conflict situations that need to be re-
solved. That is, u may satisfy a constraint of 
more than one labeled sub-set Li. For example, if 
u is a single word, it may be synonyms of feature 
expressions from more than one feature groups. 
The question is which group it is likely to belong. 
Further, u may be synonyms of a few single-
word feature expressions in Li. Clearly, u being a 
synonym of more than one word in Li is better 
than it is only the synonym of one word in Li.
Similar problems also occur when u is an ele-
ment of a feature expression phrase e.
To match u and e, there are a few possibilities. 
If both u and e are single words (lines 5-6), the 
algorithm checks if they are synonyms (line 7). 
The score in line 8 is discussed below. When one 
of u and e is a phrase, or both of them are phrases, 
we see whether they have shared words. Again, 
conflict situations can happen with multiple 
classes (feature groups) as discussed above. Note 
that in these cases, we do not use the synonym 
constraint, which does not help in our test.  
Given these complex cases, we need to decide 
which class that u should be assigned to or 
should not be assigned to any class (as it does not 
meet any constraint). We use a score to record 
the level of satisfaction. Once u is compared with 
each e in every class, the accumulated score is 
used to determine which class Li has the strong-
est association with u. The class j with the high-
est score is assigned to u. In other words, u is 
added to SLj. Regarding the score value, syn-
onyms gets the score of 1 (line 8), and intersec-
tion (shared words) gets the score equal to the 
size of the intersection (lines 10-17). 
5 Distributional Context Extraction 
To apply the proposed algorithm, a document di
needs to be prepared for each feature expression 
ei for na?ve Bayesian learning. di is formed by 
aggregating the distributional context of each 
sentence sij in our corpus that contains the ex-
pression ei. The context of a sentence is the sur-
rounding words of ei in a text window of [-t, t], 
including the words in ei. Given a relevant cor-
pus R, the document di for each feature expres-
sion ei in L (or U) is generated using the algo-
rithm in Figure 3. Stopwords are removed. 
1 for each feature expression ei in L (or U) do
2       Si ? all sentences containing ei in R;
3       for each sentence sij ? Si do
4            dij ? words in a window of [-t, t] on the left 
and right (including the words in ei);
5       di ? words from all dij, j = 1, 2, ?, |Si|; 
          // duplicates are kept as it is not union
Figure 3. Distributional context extraction 
For example, a feature expression from L (or 
U) is ei = ?screen? and there are two sentences in 
our corpus R that contain ?screen?
si1 = ?The LCD screen gives clear picture?.
si2 = ?The picture on the screen is blur?
We use the window size of [-3, 3]. Sentence si1,
gives us di1 = <LCD, screen, give, clear, picture> 
as a bag of words. ?the? and ?is? are removed as 
stopwords. si2 gives us di2 = <picture, screen, 
blur>. ?on?, ?the? and ?is? are removed as stop-
words. Finally, we obtain the document di for 
feature expression ei as a bag of words: 
di = <LCD, screen, give, clear, picture,
picture, screen, blur> 
6 Empirical Evaluation 
This section evaluates the SC-EM algorithm and 
compares it with the main existing methods that 
can be applied to solve the problem.   
1  for each feature expression u ? U do
2 for each feature group Li ? L do
3 score(Li) ? 0; 
4 for each feature expression e ? Li do
5 if u is a single word expression then
6 if e is a single word expression then
7 if u and e are synonyms then
8 score(Li) ? score(Li) + 1; 
9 else if w ? e then  // e is a phrase 
10 score(Li) ? score(Li) + 1 
11 else  // u is a phrase 
12 if e is a single word expression then
13 if e ? u then  // u is a phrase 
14 score(Li) ? score(Li) + 1 
15 else
16 s ? e ? u;
17 score(Li) ? score(Li) + |s|
18 u is added to SLj s.t. ???????? ?????????
Figure 2. Generating the soft-labeled set SL
1276
6.1 Review Data Sets and Gold Standards 
To demonstrate the generality of the proposed 
method, experiments were conducted using re-
views from five domains: Hometheater, Insur-
ance, Mattress, Car and Vacuum. All the data 
sets and the gold standard feature expressions 
and groups were from a company that provides 
opinion mining services. The details of the data 
sets and the gold standards are given in Table 1.  
Hometheater Insurance Mattress Car Vacuum
#Sentences 6355 12446 12107 9731 8785
#Reviews 587 2802 933 1486 551
#Feature  
expressions 237 148 333 317 266 
#Feature 
groups 15 8 15 16 28 
Table 1. Data sets and gold standards 
6.2 Evaluation Measures 
Since SC-EM is based on semi-supervised learn-
ing, we can use classification accuracy to eva-
luate it. We can also see it as clustering with ini-
tial seeds. Thus we also use clustering evaluation 
methods. Given gold standards, two popular 
clustering evaluation measures are Entropy and 
Purity (Liu, 2006). As accuracy is fairly standard, 
we will not discuss it further. Below, we briefly 
describe entropy and purity. 
Given a data set DS, its gold partition is G =
{??,?,??,?,??}, where k is the known number 
of clusters. The groups partition DS into k dis-
joint subsets, DS1,?, DSi, ?, DSk.
Entropy: For each resulting cluster, we can 
measure its entropy using Equation 4, where 
Pi(??) is the proportion of ?? data points in DSi.
The total entropy of the clustering (considering 
all clusters) is calculated by Equation 5. 
????????? ??? ? ?? ??????????????
?
???
(4)
???????????? ??
?? ???
????
????????? ???
?
???
(5)
Purity: Purity measures the extent that a clus-
ter contains only data from one gold-partition. 
Each cluster?s purity is computed by Equation 6, 
and the total purity of the whole clustering is 
computed with Equation 7. 
???????? ??? ? ???? ????? (6)
??????????? ??
?? ???
????
???????? ???
?
???
 (7)
In testing, the unlabeled set U is also our test 
set. This is justified because our purpose is to 
assign unlabeled data to appropriate groups.  
6.3 Baseline Methods and Settings 
The proposed SC-EM method is compared with 
a set of existing methods, which can be catego-
rized into unsupervised and semi-supervised me-
thods. We list the unsupervised methods first.  
LDA: LDA is a popular topic modeling me-
thod (see Section 2). Given a set of documents, it 
outputs groups of terms of different topics. In our 
case, each feature expression is a term, and the 
documents refer to the distributional contexts of 
each feature expressions (see Section 5).  
mLSA: This is a state-of-the-art unsupervised 
method for solving the problem. It is based on 
LDA, and has been discussed in related work. 
Kmeans: This is the k-means clustering me-
thod (MacQueen, 1966) based on distributional 
similarity with cosine as the similarity measure. 
In the semi-supervised category, the methods 
are further classified into un-constrained, hard-
constrained, and soft-constrained methods. 
For the un-constrained subclass (no con-
straints are used), we have the following: 
LDA(L, H): This method is based on LDA,
but the labeled examples L are used as seeds for 
each group/topic. All examples in L will always 
stay in the same topic. We call this hard initiali-
zation (H). L is handled similarly below. 
DF-LDA(L, H). DF-LDA is the LDA method 
(Andrzejewski et al, 2009) that takes must-links 
and cannot-links. Our L set can be expressed as a 
combination of must-links and cannot-links. Un-
fortunately, only must-links can be used because 
the number of cannot-links is huge and crashes 
the system. For example, for the car data, the 
number of cannot-links is 194,400 for 10% la-
beled data (see Section 6.4) and for 20% it is 
466,560,000. DF-LDA also has a parameter ?
controlling the link strength, which is set very 
high (=1000) to reflect the hard initialization. We 
did not use DF-LDA in the unsupervised subclass 
above as without constraints it reduces to LDA.
Kmeans(L, H): This method is based on 
Kmeans, but the clusters of the labeled seeds are 
fixed at the initiation and remain unchanged. 
EM(L, H): This is the original EM for semi-
supervised learning. Only the labeled examples 
are used as the initial seeds.  
For the hard-constrained (H) subclass (our 
1277
two constraints are applied and cannot be vi-
olated), we have the following methods (LC is L
plus SL produced by the constraints (C): 
Rand(LC, H): This is an important baseline. It 
shows whether the constraints alone are suffi-
cient to produce good results. That is, the final 
result is the expanded seeds SL plus the rest of U
assigned randomly to different groups.
LDA(LC, H): It is similar to LDA(L,H), but 
both the initial seeds L and the expanded seeds 
SL are considered as labeled examples. They also 
stay in the same topics/groups in the process. 
Note that although SL is called a set of soft-
labeled examples (seeds) in the proposed algo-
rithm, they are treated as hard-labeled examples 
here just for experimental comparison.  
DF-LDA(LC, H): This is DF-LDA with both 
L and SL expressed as must-links. Again, a large 
? (= 1000) is used to make sure that must-links 
for L and SL will not be violated.  
Kmeans(LC,H): It is similar to Kmeans(L,H), 
but both L and SL stay in their assigned clusters.  
EM(LC, H): It is similar to SC-EM, but SL is 
added to the labeled set L, and their classes are 
not allowed to change in the EM iterations.  
For the soft-constrained (S) subclass, our two 
constraints can be violated. Initially, both the 
initial seeds L and the expanded seeds SL are 
considered as labeled data, but subsequently, on-
ly L is taken as the labeled data (i.e., staying in 
the same classes). The algorithm will re-estimate 
the label of each feature expression in SL. This 
subclass has the following methods: 
LDA(LC, S): This is in contrast to LDA(LC, 
H). It allows the SL set to change topics/groups. 
Kmeans(LC, S): This is in contrast to 
Kmeans(LC, H).
A soft DF-LDA is not included here because 
different ? values give different results, and they 
are generally worse than DF-LDA(LC, H).
For all LDA based methods, the topic model-
ing parameters were set to their default values. 
The number of iteration is 1000. We used the 
LDA in MALLET2, and modified it to suit differ-
ent LDA-based methods except DF-LDA, which 
was downloaded from its authors? website3. We 
implemented mLSA, Kmeans and changed EM4
to take soft seeds. For all Kmeans based methods, 
the distance function is the cosine similarity. 
2 http://mallet.cs.umass.edu/ 
3 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html 
4 http://alias-i.com/lingpipe/ 
6.4 Evaluation Results 
We now compare the results of SC-EM and the 
14 baseline methods. To see the effects of differ-
ent numbers of labeled examples (seeds), we ex-
perimented with 10%, 20%, 30%, 40%, and 50% 
of the feature expressions from the gold standard 
data as the labeled set L, and the rest as the unla-
beled set U. All labeled data were selected ran-
domly. For each setting, we run the algorithms 
30 times and report the average results. Due to 
space limitations, we can only show the detailed 
purity (Pur), entropy (Ent) and accuracy (Acc) 
results for 30% as the labeled data (70% as unla-
beled) in Table 2. For the other proportions of 
labeled data, we summarize them in Table 3. 
Each result in Table 3 is thus the average of the 5 
data sets. All the results were obtained from the 
unlabeled set U, which was our test set. For en-
tropy, the smaller the value is the better, but for 
purity and accuracy, the larger the better. For 
these experiments, we used the window size t = 5. 
Section 6.5 studies the effects of window sizes.  
Tables 2 and 3 clearly show that the proposed 
algorithm (SC-EM) outperforms all 14 baseline 
methods by a large margin on every dataset. In 
detail, we observe the following:  
? LDA, mLSA and Kmeans with no seeds (la-
beled data) perform the worst. Seeds help to 
improve the results, which is intuitive. With-
out seeds, DF-LDA is the same as LDA.
? LDA based methods seems to be the weakest. 
Kmeans based methods are slightly better, but 
EM based methods are the best. This clearly 
indicates that classification (EM) performs 
better than clustering. Comparing DF-LDA
and Kmeans, their results are similar.  
? For LDA, and Kmeans, hard-constrained me-
thods (i.e., LDA(L, H), and Kmeans(L, H))
perform better than soft-constrained methods 
(i.e., LDA(LC, S) and Kmeans(LC, S)). This 
indicates that soft-constrained versions may 
change some correctly constrained expres-
sions into wrong groups. However, for the 
EM based methods, the soft-constrained me-
thod (SC-EM) performs markedly better than 
the hard-constrained version (EM(LC, H)). 
This indicates that Bayesian classifier used in 
EM can take advantage of the soft constraints 
and correct some wrong assignments made by 
constraints. Much weaker results of Rand(LC,
H) than SC-EM in different settings show that 
1278
constraints alone (i.e., synonyms and sharing 
of words) are far from sufficient. EM can im-
prove it considerably.  
? Comparing EM based methods, we can see 
that soft seeds in SL make a big difference for 
all data sets. SC-EM is clearly the best.  
? As the number of labeled examples increases 
(from 10% to 50%), the results improve for 
every method (except those for DF-LDA,
which does not change much).  
6.5 Varying the Context Window Size 
We varied the text window size t from 1 to 10 to 
see how it impacts on the performance of SC-EM.
The results are given in Figure 4 (they are aver-
ages of the 5 datasets). Again for purity and ac-
curacy, the greater the value the better, while for 
entropy it is the opposite. It is clear that the win-
dow sizes of 2~6 produce similar good results. 
All evaluations reported above used t = 5. 
7 Conclusion
This paper proposed the task of feature grouping 
in a semi-supervised setting. It argued that some 
form of supervision is needed for the problem 
because its solution depends on the user applica-
tion needs. The paper then proposed to use the 
EM algorithm to solve the problem, which was 
improved by considering two soft constraints. 
Empirical evaluations using 5 real-life data sets 
show that the proposed method is superior to 14 
baselines. In our future work, we will focus on 
further improving the accuracy.  
Methods 
Hometheater Insurance Mattress Car Vacuum 
Acc Pur Ent Acc Pur Ent Acc Pur Ent Acc Pur Ent Acc Pur Ent 
LDA 0.06 0.31 2.54 0.11 0.36 2.24 0.05 0.32 2.57 0.06 0.37 2.39 0.03 0.36 2.09
mLSA 0.06 0.31 2.53 0.14 0.38 2.19 0.06 0.34 2.55 0.09 0.37 2.40 0.03 0.37 2.11
Kmeans 0.21 0.42 2.14 0.25 0.45 1.90 0.15 0.39 2.32 0.25 0.44 2.16 0.24 0.47 1.78
LDA(L, H) 0.10 0.32 2.50 0.16 0.37 2.22 0.10 0.34 2.57 0.19 0.39 2.36 0.10 0.39 2.09
DF-LDA(L, H) 0.27 0.37 2.32 0.25 0.41 2.00 0.19 0.39 2.35 0.28 0.45 2.15 0.31 0.40 1.98
Kmeans(L, H) 0.20 0.42 2.12 0.25 0.43 1.92 0.17 0.42 2.26 0.27 0.48 2.04 0.20 0.48 1.76
EM(L, H) 0.48 0.50 1.93 0.50 0.53 1.69 0.52 0.56 1.87 0.56 0.58 1.80 0.49 0.52 1.79
Rand(CL, H) 0.41 0.46 2.07 0.40 0.46 1.94 0.40 0.47 2.07 0.34 0.41 2.31 0.39 0.52 1.59
LDA(CL, H) 0.44 0.50 1.96 0.42 0.48 1.89 0.42 0.49 1.97 0.44 0.52 1.87 0.43 0.55 1.48
DF-LDA(CL, H) 0.35 0.49 1.86 0.33 0.49 1.71 0.23 0.39 2.26 0.34 0.51 1.88 0.37 0.52 1.58
Kmeans(CL, H) 0.49 0.55 1.70 0.48 0.55 1.62 0.44 0.51 1.91 0.47 0.54 1.80 0.44 0.58 1.42
EM(CL, H) 0.59 0.60 1.62 0.58 0.60 1.46 0.56 0.59 1.74 0.62 0.64 1.54 0.55 0.60 1.44
LDA(CL, S) 0.24 0.35 2.44 0.27 0.40 2.14 0.23 0.37 2.44 0.27 0.41 2.33 0.23 0.41 2.01
Kmeans(CL, S) 0.33 0.46 2.04 0.34 0.45 1.90 0.25 0.43 2.20 0.29 0.47 2.07 0.37 0.50 1.68
SC-EM 0.67 0.68 1.30 0.66 0.68 1.18 0.68 0.70 1.27 0.70 0.71 1.24 0.67 0.68 1.18
Table 2. Comparison results (L = 30% of the gold standard data) 
Methods 
Acc Pur Ent
10% 20% 30% 40% 50% 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%
LDA 0.07 0.07 0.06 0.06 0.08 0.33 0.33 0.34 0.35 0.38 2.50 2.44 2.37 2.28 2.11
mLSA 0.07 0.07 0.08 0.07 0.07 0.34 0.35 0.35 0.37 0.38 2.48 2.42 2.36 2.26 2.12
Kmeans 0.22 0.23 0.22 0.22 0.22 0.42 0.43 0.44 0.44 0.46 2.16 2.11 2.06 1.98 1.86
LDA(L, H) 0.10 0.10 0.13 0.14 0.15 0.34 0.34 0.36 0.37 0.39 2.48 2.43 2.35 2.25 2.11
DF-LDA(L, H) 0.23 0.25 0.26 0.27 0.30 0.41 0.40 0.41 0.41 0.44 2.23 2.23 2.16 2.10 1.94
Kmeans(L, H) 0.13 0.16 0.22 0.24 0.28 0.42 0.43 0.45 0.45 0.48 2.15 2.11 2.02 1.95 1.79
EM(L, H) 0.35 0.44 0.51 0.55 0.58 0.43 0.49 0.54 0.57 0.61 2.22 1.99 1.81 1.65 1.49
Rand(CL, H) 0.28 0.35 0.39 0.42 0.45 0.39 0.43 0.47 0.50 0.54 2.33 2.15 2.00 1.82 1.63
LDA(CL, H) 0.31 0.38 0.43 0.46 0.49 0.43 0.47 0.51 0.54 0.58 2.16 1.99 1.83 1.69 1.49
DF-LDA(CL, H) 0.32 0.33 0.33 0.34 0.36 0.49 0.50 0.48 0.48 0.48 1.90 1.85 1.86 1.83 1.82
Kmeans(CL, H) 0.33 0.41 0.46 0.49 0.52 0.47 0.51 0.55 0.57 0.61 1.98 1.82 1.69 1.56 1.42
EM(CL, H) 0.44 0.54 0.58 0.61 0.64 0.49 0.57 0.61 0.64 0.67 1.98 1.72 1.56 1.40 1.25
LDA(CL, S) 0.17 0.21 0.25 0.30 0.34 0.34 0.36 0.39 0.42 0.46 2.47 2.37 2.27 2.09 1.87
Kmeans(CL, S) 0.23 0.28 0.32 0.36 0.42 0.43 0.44 0.46 0.48 0.51 2.15 2.08 1.98 1.86 1.70
SC-EM 0.45 0.58 0.68 0.75 0.81 0.50 0.61 0.69 0.76 0.82 1.95 1.56 1.24 0.94 0.69
Table 3. Influence of the seeds? proportion (which reflects the size of the labeled set L)
Figure 4. Influence of context window size 
1.0
1.1
1.2
1.3
1.4
1.5
62%
64%
66%
68%
70%
1 2 3 4 5 6 7 8 9 10
E
n
tr
op
y
P
u
ri
ty
/A
cc
u
ra
cy
Window Size t
SC-EM
Purity
Accuracy
Entropy
1279
References 
Agirre E., E. Alfonseca, K. Hall, J. Kravalova, M. Pa 
ca and A. Soroa 2009. A study on similarity and 
relatedness using distributional and WordNet-
based approaches. Proceedings of ACL. 
Alvarez M. and S. Lim 2007. A Graph Modeling of 
Semantic Similarity between Words. Proceeding of 
the Conference on Semantic Computing. 
Andrzejewski D., X. Zhu and M. Craven 2009. 
Incorporating domain knowledge into topic 
modeling via Dirichlet forest priors. Proceedings 
of ICML. 
Blei D., A. Y. Ng and M. I. Jordan 2003. "Latent 
Dirichlet Allocation." JMLR 3: 993-1022. 
Carenini G., R. Ng and E. Zwart 2005. Extracting 
knowledge from evaluative text. Proceedings of 
International Conference on Knowledge Capture. 
Chen H., M. Lin and Y. Wei 2006. Novel association 
measures using web search with double checking.
Proceedings of ACL. 
Dempster A., N. Laird and D. Rubin 1977. "Maximum
likelihood from incomplete data via the EM 
algorithm." Journal of the Royal Statistical Society 
39(1): 1-38. 
Guo H., H. Zhu, Z. Guo, X. Zhang and Z. Su 2009. 
Product feature categorization with multilevel 
latent semantic association. Proc. of CIKM. 
Harris Z. S. 1968. Mathematical structures of 
language. New York, Interscience Publishers. 
Hu M. and B. Liu 2004a. Mining and summarizing 
customer reviews. Proceedings of SIGKDD. 
Hu M. and B. Liu 2004b. Mining Opinion Features in 
Customer Reviews. Proceedings of AAAI. 
Hughes T. and D. Ramage 2007. Lexical semantic 
relatedness with random graph walks. EMNLP. 
Jin W., H. Ho and R. Srihari 2009. OpinionMiner: a 
novel machine learning system for web opinion 
mining and extraction. Proceedings of KDD. 
Joachims T. 1999. Transductive inference for text 
classification using support vector machines.
Proceedings of ICML. 
Kim S. and E. Hovy 2006. Extracting opinions, 
opinion holders, and topics expressed in online 
news media text. Proceedings of EMNLP. 
Kobayashi N., K. Inui and Y. Matsumoto 2007. 
Extracting aspect-evaluation and aspect-of 
relations in opinion mining. Proceedings of 
EMNLP.
Ku L., H. Ho and H. Chen 2009. "Opinion mining and 
relationship discovery using CopeOpi opinion 
analysis system." Journal of the American Society 
for Information Science and Technology 60(7): 
1486-1503. 
Lee L. 1999. Measures of distributional similarity,
Proceedings of ACL. 
Lin D. 1998. Automatic retrieval and clustering of 
similar words, Proceedings of ACL. 
Liu B. 2006. Web data mining; Exploring hyperlinks, 
contents, and usage data, Springer. 
Liu B. 2010. Sentiment Analysis and Subjectivity.
Handbook of Natural Language Processing N.
Indurkhya and F. J. Damerau. 
MacQueen J. 1966. Some methods for classification 
and analysis of multivariate observations. Proc. of 
Symposium on Mathematical Statistics and 
Probability. 
Mei Q., X. Ling, M. Wondra, H. Su and C. Zhai 2007. 
Topic sentiment mixture: modeling facets and 
opinions in weblogs. Proceedings of WWW. 
Nigam K., A. McCallum, S. Thrun and T. Mitchell 
2000. "Text classification from labeled and 
unlabeled documents using EM." Machine 
Learning 39(2). 
Pang B. and L. Lee 2008. "Opinion mining and 
sentiment analysis." Foundations and Trends in 
Information Retrieval 2(1-2): 1-135. 
Pantel P., E. Crestan, A. Borkovsky, A. Popescu and 
V. Vyas 2009. Web-scale distributional similarity 
and entity set expansion. EMNLP. 
Pereira F., N. Tishby and L. Lee 1993. Distributional 
clustering of English words. Proceedings of ACL. 
Popescu A.-M. and O. Etzioni 2005. Extracting 
Product Features and Opinions from Reviews.
EMNLP.
Sahami M. and T. Heilman 2006. A web-based kernel 
function for measuring the similarity of short text 
snippets. Proceedings of WWW. 
Stoyanov V. and C. Cardie 2008. Topic identification 
for fine-grained opinion analysis. COLING. 
Wagstaff K., C. Cardie, S. Rogers and S. Schroedl 
2001. Constrained k-means clustering with 
background knowledge. In Proceedings of ICML. 
Yang D. and D. Powers 2005. Measuring semantic 
similarity in the taxonomy of WordNet,
Proceedings of the Australasian conference on 
Computer Science. 
1280
Coling 2010: Poster Volume, pages 259?266,
Beijing, August 2010
Abstract
Due to the lack of annotated data sets, 
there are few studies on machine learning 
based approaches to extract named enti-
ties (NEs) in clinical text. The 2009 i2b2 
NLP challenge is a task to extract six 
types of medication related NEs, includ-
ing medication names, dosage, mode, 
frequency, duration, and reason from 
hospital discharge summaries. Several 
machine learning based systems have 
been developed and showed good per-
formance in the challenge. Those systems 
often involve two steps: 1) recognition of 
medication related entities; and 2) deter-
mination of the relation between a medi-
cation name and its modifiers (e.g., do-
sage). A few machine learning algo-
rithms including Conditional Random 
Field (CRF) and Maximum Entropy have 
been applied to the Named Entity Recog-
nition (NER) task at the first step. In this 
study, we developed a Support Vector 
Machine (SVM) based method to recog-
nize medication related entities. In addi-
tion, we systematically investigated vari-
ous types of features for NER in clinical 
text. Evaluation on 268 manually anno-
tated discharge summaries from i2b2 
challenge showed that the SVM-based 
NER system achieved the best F-score of 
90.05% (93.20% Precision, 87.12% Re-
call), when semantic features generated 
from a rule-based system were included.
1 Introduction
Named Entity Recognition (NER) is an impor-
tant step in natural language processing (NLP). It 
has many applications in general language do-
main such as identifying person names, locations, 
and organizations. NER is crucial for biomedical 
literature mining as well (Hirschman, Morgan, & 
Yeh, 2002; Krauthammer & Nenadic, 2004) and 
many studies have focused on biomedical entities, 
such as gene/protein names. There are mainly 
two types of approaches to identify biomedical 
entities: rule-based and machine learning based 
approaches. While rule-based approaches use 
existing biomedical knowledge/resources, ma-
chine learning (ML) based approaches rely much 
on annotated training data. The advantage of 
rule-based approaches is that they usually can 
achieve stable performance across different data 
sets due to the verified resources, while machine 
learning approaches often report better results 
when the training data are good enough. In order 
to harness the advantages of both approaches, the 
combination of them, called the hybrid approach, 
has often been used as well. CRF and SVM are 
two common machine learning algorithms that 
have been widely used in biomedical NER 
(Takeuchi & Collier, 2003; Kazama, Makino, 
Ohta, & Tsujii, 2002; Yamamoto, Kudo, 
Konagaya, & Matsumoto, 2003; Torii, Hu, Wu, 
& Liu, 2009; Li, Savova, & Kipper-Schuler, 
2008). Some studies reported better results using 
CRF (Li, Savova, & Kipper-Schuler, 2008),
while others showed that the SVM was better 
(Tsochantaridis, Joachims, & Hofmann, 2005) in 
NER. Keerthi & Sundararajan (Keerthi & Sunda-
rarajan, 2007) conducted some experiments and 
demonstrated that CRF and SVM were quite 
close in performance, when identical feature 
functions were used.
2 Background
There has been large ongoing effort on 
processing clinical text in Electronic Medical 
Records (EMRs). Many clinical NLP systems 
Recognizing Medication related Entities in Hospital Discharge 
Summaries using Support Vector Machine
Son Doan and Hua Xu
Department of Biomedical Informatics
School of Medicine, Vanderbilt University
Son.Doan@Vanderbilt.edu, Hua.Xu@Vanderbilt.edu
259
have been developed, including MedLEE
(Friedman, Alderson, Austin, Cimino, & John-
son, 1994), SymTex (Haug et al, 1997), Meta-
Map (Aronson, 2001). Most of those systems 
recognize clinical named entities such as diseas-
es, medications, and labs, using rule-based me-
thods such as lexicon lookup, mainly because of 
two reasons: 1) there are very rich knowledge 
bases and vocabularies of clinical entities, such 
as the Unified Medical Language System 
(UMLS) (Lindberg, Humphreys, & McCray, 
1993), which includes over 100 controlled bio-
medical vocabularies, such as RxNorm, 
SNOMED,  and ICD-9-CM; 2) very few anno-
tated data sets of clinical text are available for 
machine learning based approaches.
Medication is one of the most important types 
of information in clinical text. Several studies 
have worked on extracting drug names from clin-
ical notes. Evans et al (Evans, Brownlow, Hersh, 
& Campbell, 1996) showed that drug and dosage 
phrases in discharge summaries could be identi-
fied by the CLARIT system with an accuracy of 
80%. Chhieng et al (Chhieng, Day, Gordon, & 
Hicks, 2007) reported a precision of 83% when 
using a string matching method to identify drug 
names in clinical records. Levin et al (Levin, 
Krol, Doshi, & Reich, 2007) developed an effec-
tive rule-based system to extract drug names 
from anesthesia records and map to RxNorm 
concepts with 92.2% sensitivity and 95.7% spe-
cificity. Sirohi and Peissig (Sirohi & Peissig, 
2005) studied the effect of lexicon sources on 
drug extraction. Recently, Xu et al (Xu et al, 
2010) developed a rule-based system for medica-
tion information extraction, called MedEx, and 
reported F-scores over 90% on extracting drug 
names, dose, route, and frequency from dis-
charge summaries.
Starting 2007, Informatics for Integrating Bi-
ology and the Bedside (i2b2), an NIH-funded 
National Center for Biomedical Computing 
(NCBC) based at Partners Healthcare System in 
Boston, organized a series of shared tasks of
NLP in clinical text. The 2009 i2b2 NLP chal-
lenge was to extract medication names, as well as 
their corresponding signature information includ-
ing dosage, mode, frequency, duration, and rea-
son from de-identified hospital discharge sum-
maries (Uz?ner, Solti, & Cadag, 2009). At the 
beginning of the challenge, a training set of 696 
notes were provided by the organizers. Among 
them, 17 notes were annotated by the i2b2 orga-
nizers, based on an annotation guideline (see Ta-
ble 1 for examples of medication information in 
the guideline), and the rest were un-annotated 
notes. Participating teams would develop their 
systems based on the training set, and they were 
Class # Example Description
Medication 12773 ?Lasix?, ?Caltrate plus D?, ?fluoci-
nonide 0.5% cream?, ?TYLENOL 
( ACETAMINOPHEN )?
Prescription substances, biological 
substances, over-the-counter drugs, 
excluding diet, allergy, lab/test, alco-
hol.
Dosage 4791 ?1 TAB?, ?One tablet?, ?0.4 mg? 
?0.5 m.g.?, ?100 MG?, ?100 mg x 2
tablets?
The amount of a single medication 
used in each administration.
Mode 3552 ?Orally?, ?Intravenous?, ?Topical?, 
?Sublingual?
Describes the method for administer-
ing the medication.
Frequency 4342 ?Prn?, ?As needed?, ?Three times a 
day as needed?, ?As needed three 
times a day?, ?x3 before meal?, ?x3 
a day after meal as needed?
Terms, phrases, or abbreviations that 
describe how often each dose of the 
medication should be taken.
Duration 597 ?x10 days?, ?10-day course?, ?For 
ten days?, ?For a month?, ?During 
spring break?, ?Until the symptom 
disappears?, ?As long as needed?
Expressions that indicate for how 
long the medication is to be adminis-
tered.
Reason 1534 ?Dizziness?, ?Dizzy?, ?Fever?, ?Di-
abetes?, ?frequent PVCs?, ?rare an-
gina?
The medical reason for which the 
medication is stated to be given.
Table 1.Number of classes and descriptions with examples in i2b2 2009 dataset.
260
allowed to annotate additional notes in the train-
ing set. The test data set included 547clinical 
notes, from which 251 notes were randomly 
picked by the organizers. Those 251 notes were 
then annotated by participating teams, as well as 
the organizers, and they served as the gold stan-
dard for evaluating the performance of systems 
submitted by participating teams. An example of 
original text and annotated text were shown in 
Figure 1.
The results of systems submitted by the partic-
ipating teams were presented at the i2b2 work-
shop and short papers describing each system 
were available at i2b2 web site with protected 
passwords. Among top 10 systems which 
achieved the best performance, there were 6 rule-
based, 2 machine learning based, and 2 hybrid 
systems. The best system, which used a machine 
learning based approach, reached the highest F-
score of 85.7% (Patrick & Li, 2009). The second 
best system, which was a rule-based system us-
ing the existing MedEx tool, reported an F-score 
of 82.1% (Doan, Bastarache L., Klimkowski S., 
Denny J.C., & Xu, 2009). The difference be-
tween those two systems was statistically signifi-
cant. However, this finding was not very surpris-
ing, as the machine learning based system uti-
lized additional 147 annotated notes by the par-
ticipating team, while the rule-based system 
mainly used 17 annotated training data to cus-
tomize the system. 
Interestingly, two machine learning systems in 
the top ten systems achieved very different per-
formance, one (Patrick et al, 2009) achieved an 
F-score of 85.7%, ranked the first; while another 
(Li et al, 2009) achieved an F-score of 76.4%, 
ranked the 10th on the final evaluation. Both sys-
tems used CRF for NER, on the equivalent num-
ber of training data (145 and 147 notes respec-
tively). The large difference in F-score of those 
two systems could be due to: the quality of train-
ing set, and feature sets using for classification. 
More recently, i2b2 organizers also reported a 
Maximum Entropy (ME) based approach for the 
2009 challenge (Halgrim, Xia, Solti, Cadag, & 
Uzuner, 2010). Using the same annotated data set 
as in (Patrick et al, 2009), they reported an F-
score of 84.1%, when combined features such as 
unigram, word bigrams/trigrams, and label of 
previous words were used. These results indi-
cated the importance of feature sets used in ma-
chine learning algorithms in this task. 
For supervised machine learning based sys-
tems in the i2b2 challenge, the task was usually 
divided into two steps: 1) NER of six medication 
related findings; and 2) determination of the rela-
tion between detected medication names and 
other entities. It is obvious that NER is the first 
crucial step and it affects the performance of the 
whole system. However, short papers presented 
at the i2b2 workshop did not show much detailed 
evaluation on NER components in machine 
learning based systems.  The variation in perfor-
mance of different machine learning based sys-
tems also motivated us to further investigate the 
effect of different types of features on recogniz-
Figure. 1. An example of the i2b2 data, ?m? is for MED NAME, ?do? is for DOSE, ?mo? is for 
MODE, ?f? is for FREQ, ?du? is for DURATION, ?r? is for REASON, ?ln? is for ?list/narrative.?
# Line Original text
70
..
74
75
DISCHARGE MEDICATION: 
?
Additionally, Percocet 1-2 tablets p.o. q 4 prn, Colace 100 mg
p.o.
b.i.d. , insulin NPH 10 units subcu b.i.d. , sliding scale insulin?
Annotated text:
m="colace" 74:10 74:10||do="100 mg" 74:11 74:12||mo="p.o." 74:13 74:13||f="b.i.d." 75:0 
75:0||du="nm" ||r="nm"||ln="list"
m="percocet" 74:2 74:2||do="1-2 tablets" 74:3 74:4||mo="p.o." 74:5 74:5||f="q 4 prn" 74:6 
74:8||du="nm"||r="nm"||ln="list"
261
ing medication related entities.  
In this study, we developed an SVM-based 
NER system for recognizing medication related 
entities, which is a sub-task of the i2b2 chal-
lenge. We systematically investigated the effects 
of typical local contextual features that have been 
reported in many biomedical NER studies. Our 
studies provided some valuable insights to NER 
tasks of medical entities in clinical text.
3 Methods
A total of 268 annotated discharge summaries 
(17 from training set and 251 from test set) from 
i2b2 challenge were used in this study. This an-
notated corpus contains 9,689 sentences, 326,474 
words, and 27,589 entities. Annotated notes were 
converted into a BIO format and different types 
of feature sets were used in an SVM classifier for 
NER. Performance of the NER system was eva-
luated using precision, recall, and F-score, based 
on 10-fold cross validation.  
3.1 Preprocessing
The annotated corpus was converted into a BIO 
format (see an example in Figure 2).  Specifically, 
it assigned each word into a class as follows: B
means beginning of an entity, I means inside an 
entity, and O means outside of an entity. As we 
have six types of entities, we have six different B 
classes and six different I classes. For example, 
for medication names, we define the B class as
?B-m?, and the I class as ?I-m?.  Therefore, we 
had total 13 possible classes to each word 
(including O class). 
DISCHARGE MEDICATION: 
O O
Additionally, Percocet 1-2 Tablets
O B-m B-do I-do
p.o. Q 4 prn,
B-mo B-f I-f I-f
Figure 2. An example of the BIO representation 
of annotated clinical text (Where m as medica-
tion, do as dose, mo as mode, and f as frequency).
After preprocessing, the NER problem now 
can be considered as a classification problem, 
which is to assigns one of the 13 class labels to 
each word. 
3.2 SVM
Support Vector Machine (SVM) is a machine 
learning method that is widely used in many 
NLP tasks such as chunking, POS, and NER. 
Essentially, it constructs a binary classifier using 
labeled training samples. Given a set of training 
samples, the SVM training phrase tries to find 
the optimal hyperplane, which maximizes the 
distance of training sample nearest to it (called 
support vectors). SVM takes an input as a vector 
and maps it into a feature space using a kernel 
function. 
In this paper we used TinySVM1 along with 
Yamcha2
3.3 Features sets
developed at NAIST (Kudo & Matsu-
moto, 2000; Kudo & Matsumoto, 2001). We 
used a polynomial kernel function with the de-
gree of kernel as 2, context window as +/-2, and 
the strategy for multiple classification as pair-
wise (one-against-one). Pairwise strategy means 
it will build K(K-1)/2 binary classifiers in which 
K is the number of classes (in this case K=13). 
Each binary classifier will determine whether the 
sample should be classified as one of the two 
classes. Each binary classifier has one vote and 
the final output is the class with the maximum 
votes. These parameters were used in many bio-
medical NER tasks such as (Takeuchi & Collier, 
2003; Kazama et al, 2002; Yamamoto et al, 
2003).
In this study, we investigated different types of 
features for the SVM-based NER system for me-
dication related entities, including 1) words; 2) 
Part-of-Speech (POS) tags; 3) morphological 
clues; 4) orthographies of words; 5)  previous 
history features; 6) semantic tags determined by 
MedEx, a rule based medication extraction sys-
tem. Details of those features are described be-
low: 
x Words features: Words only. We referred it 
as a baseline method in this study.
x POS features: Part-of-Speech tags of words. 
To obtain POS information, we used a POS 
tagger in the NLTK package3
1 Available at 
http://chasen.org/~taku/software/TinySVM/
.
2 Available at 
http://chasen.org/~taku/software/YamCha/
3 www.nltk.org
262
x Morphologic features: suffix/prefix of up to 
3 characters within a word. 
x Orthographic features: information about if a 
word contains capital letters, digits, special 
characters etc. We used orthographic features 
described in (Collier, Nobata, & Tsujii, 
2000) and modified some as for medication 
information such as ?digit and percent?. We 
had totally 21 labels for orthographic fea-
tures.
x Previous history features: Class assignments 
of preceding words, by the NER system it-
self.
x Semantic tag features: semantic categories of 
words. Typical NER systems use dictionary 
lookup methods to determine semantic cate-
gories of a word (e.g., gene names in a dic-
tionary). In this study, we used MedEx, the 
best rule-based medication extraction system 
in the i2b2 challenge, to assign medication 
specific categories into words.
MedEx was originally developed at Vanderbilt 
University, for extracting medication information 
from clinical text (Xu et al, 2010). MedEx labels 
medication related entities with a pre-defined 
semantic categories, which has overlap with the 
six entities defined in the i2b2 challenge, but not 
exactly same. For example, MedEx breaks the 
phrase ?fluocinonide 0.5% cream? into drug 
name: ?fluocinonide?, strength: ?0.5%?, and
form: ?cream?; while i2b2 labels the whole 
phrase as a medication name. There are a total of 
11 pre-defined semantic categories which are 
listed in (Xu et al, 2010c). When the Vanderbilt 
team applied MedEx to the i2b2 challenge, they 
customized and extended MedEx to label medi-
cation related entities as required by i2b2. Those 
customizations included:
- Customized Rules to combine entities recog-
nized by MedEx into i2b2 entities, such as 
combine drug name: ?fluocinonide?,
strength: ?0.5%?, and form: ?cream? into 
one medication name ?fluocinonide 0.5% 
cream?.
- A new Section Tagger to filter some drug 
names in sections such as ?allergy? and
?labs?.
- A new Spell Checker to check whether a 
word can be a misspelled drug names. 
In a summary, the MedEx system will produce 
two sets of semantic tags: 1) initial tags that are 
identified by the original MedEx system; 2) final 
tags that are identified by the customized MedEx 
system for the i2b2 challenge. The initial tagger 
will be equivalent to some simple dictionary look 
up methods used in many NER systems. The fi-
nal tagger is a more advanced method that inte-
grates other level of information such as sections 
and spellings. The outputs of initial tag include 
11 pre-defined semantic tags in MedEx, and out-
puts of final tags consist of 6 types of NEs as in 
the i2b2 requirements. Therefore, it is interesting 
to us to study effects of both types of tags from 
MedEx in this study. These semantic tags were 
also converted into the BIO format when they 
were used as features.
4 Results and Discussions
In this study, we measured Precision, Recall, and 
Features Pre Rec F-score
Words (Baseline) 87.09 77.05 81.76
Words + History 90.34 78.17 83.81
Words + History + Morphology 91.72 81.08 86.06
Words + History + Morphology + POS 91.81 81.06 86.10
Words + History + Morphology + POS + Orthographies 91.78 81.29 86.22
Words + Semantic Tags (Original MedEx) 90.15 83.17 86.51
Words + Semantic Tags (Customized MedEx) 92.38 86.73 89.47
Words + History + Morphology + POS + Orthographies + Semantic Tags 
(Original MedEx) 91.43 84.2 87.66
Words + History + Morphology + POS + Orthographies + Semantic Tags 
(Customized MedEx) 93.2 87.12 90.05
Table 2. Performance of the SVM-based NER system for different feature combinations.
263
F-score using the CoNLL evaluation script4
Table 2 shows the precision, recall, and F-
score of the SVM-based NER system for all six 
types of entities, when different combinations of 
feature sets were used. Among them, the best F-
score of 90.05% was achieved, when all feature 
sets were used. A number of interesting findings 
can be concluded from those results. First, the 
contribution of different types of features to the 
system?s performance varies. For example, the 
?previous history feature? and the ?morphology 
feature? improved the performance substantially 
(F-score from 81.76% to 83.83%, and from 
83.81% to 86.06% respectively). These findings 
were consistent with previous reported results on 
protein/gene NER (Kazama et al, 2002; Takeu-
chi and Collier, 2003; Yamamoto et al, 2003). 
However, ?POS? and ?orthographic? features 
contributed very little, not as much as in pro-
tein/gene names recognition tasks.  This could be 
related to the differences between gene/protein 
phrases and medication phrases ? more ortho-
graphic clues are observed in gene/protein 
names. Second, the ?semantic tags? features 
alone, even just using the original tagger in Me-
dEx, improved the performance dramatically 
(from 81.76% to 86.51% or 89.47%). This indi-
. Pre-
cision is the ratio between the number of correct-
ly identified NE chunks by the system and the 
total number of NE chunks found by the system; 
Recall is the ratio between the number of correct-
ly identified NE chunks by the system and the 
total number of NE chunks in the gold standard.
Experiments were run in a Linux machine with 
16GB RAM and 8 cores of Intel Xeon 2.0GHz
processor. The performance of different types of 
feature sets was evaluated using 10-fold cross-
validation. 
4 Available at 
http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.t
xt
cates that the knowledge bases in the biomedical 
domain are crucial to biomedical NER. Third, the 
customized final semantic tagger in MedEx had 
much better performance than the original tagger, 
which indicated that advanced semantic tagging 
methods that integrate other levels of linguistic 
information (e.g., sections) were more useful 
than simple dictionary lookup methods.
Table 3 shows the precision, recall, and F-
score for each type of entity, from the MedEx 
alone, and the baseline and the best runs of the 
SVM-based NER system. As we can see, the best 
SVM-based NER system that combines all types 
of features (including inputs from MedEx) was 
much better than the MedEx system alone 
(90.05% vs. 85.86%). This suggested that the 
combination of rule-based systems with machine
learning approaches could yield the most opti-
mized performance in biomedical NER tasks.
Among six types of medication entities, we 
noticed that four types of entities (medication 
names, dosage, mode, and frequency) got very 
high F-scores (over 92%); while two others (du-
ration and reason) had low F-scores (up to 50%). 
This finding was consistent with results from 
i2b2 challenge. Duration and reason are more 
difficult to identify because they do not have 
well-formed patterns and few knowledge bases 
exist for duration and reasons.
This study only focused on the first step of the 
i2b2 medication extraction challenge ? NER. Our 
next plan is to work on the second step of deter-
mining relations between medication names and 
other entities, thus allowing us to compare our 
results with those reported in the i2b2 challenge. 
In addition, we will also evaluate and compare 
the performance of other ML algorithms such as 
CRF and ME on the same NER task.  
Entity MedEx only SVM (Baseline) SVM (Best)
Pre Rec F-score Pre Rec F-score Pre Rec F-score
ALL 87.85 83.97 85.86 87.09 77.05 81.76 93.2 87.12 90.05
Medication 87.25 90.21 88.71 88.38 75.03 81.16 93.3 91.35 92.31
Dosage 92.79 83.94 88.14 89.43 83.65 86.41 94.38 90.99 92.65
Mode 95.86 90.06 92.87 96.18 93.30 94.70 97.12 93.8 95.41
Frequency 92.67 89.00 90.80 90.33 87.60 88.94 95.88 93.04 94.43
Duration 42.65 40.15 41.36 24.16 19.62 21.45 65.18 40.16 49.57
Reason 54.23 36.72 43.79 48.40 25.51 33.30 69.21 37.39 48.4
Table 3. Comparison between a rule based system and the SVM based system.
264
5 Conclusions
In this study, we developed an SVM-based NER 
system for medication related entities. We sys-
tematically investigated different types of fea-
tures and our results showed that by combining
semantic features from a rule-based system, the 
ML-based NER system could achieve the best F-
score of 90.05% in recognizing medication re-
lated entities, using the i2b2 annotated data set. 
The experiments also showed that optimized 
usage of external knowledge bases were crucial 
to high performance ML based NER systems for 
medical entities such as drug names.
Acknowledgements
Authors would like to thank i2b2 organizers for 
organizing the 2009 i2b2 challenge and provid-
ing dataset for research studies. This study was in 
part supported by NCI grant R01CA141307-01.
References:
Aronson, A. R. (2001). Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap 
program. Proc AMIA Symp., 17-21.
Chhieng, D., Day, T., Gordon, G., & Hicks, J. (2007). 
Use of natural language programming to extract 
medication from unstructured electronic medical 
records. AMIA.Annu.Symp.Proc., 908.
Collier, N., Nobata, C., & Tsujii, J. (2000). Extracting 
the names of genes and gene products with a hidden 
Markov model. Proc.of the 18th Conf.on Computa-
tional linguistics., 1, 201-207.
Doan, S., Bastarache L., Klimkowski S., Denny J.C., 
& Xu, H. (2009). Vanderbilt's System for Medica-
tion Extraction. Proc of 2009 i2b2 workshop..
Evans, D. A., Brownlow, N. D., Hersh, W. R., & 
Campbell, E. M. (1996). Automating concept iden-
tification in the electronic medical record: an expe-
riment in extracting dosage information. 
Proc.AMIA.Annu.Fall.Symp., 388-392.
Friedman, C., Alderson, P. O., Austin, J. H., Cimino, 
J. J., & Johnson, S. B. (1994). A general natural-
language text processor for clinical radiology. 
J.Am.Med.Inform.Assoc., 1, 161-174.
Halgrim, S., Xia, F., Solti, I., Cadag, E., & Uzuner, O. 
(2010). Statistical Extraction of Medication Infor-
mation from Clinical Records. AMIA Summit on 
Translational Bioinformatics, 10-12.
Haug, P. J., Christensen, L., Gundersen, M., Clemons, 
B., Koehler, S., & Bauer, K. (1997). A natural lan-
guage parsing system for encoding admitting diag-
noses. Proc AMIA Annu.Fall.Symp., 814-818.
Hirschman, L., Morgan, A. A., & Yeh, A. S. (2002). 
Rutabaga by any other name: extracting biological 
names. J.Biomed.Inform., 35, 247-259.
Kazama, J., Makino, T., Ohta, Y., & Tsujii, T. (2002). 
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition. Proceedings of the 
ACL-02 Workshop on Natural Language 
Processing in the Biomedical Domain, 1-8.
Keerthi, S. & Sundararajan, S. (2007). CRF versus 
SVM-struct for sequence labeling. Yahoo Research 
Technical Report.
Krauthammer, M. & Nenadic, G. (2004). Term identi-
fication in the biomedical literature. 
J.Biomed.Inform., 37, 512-526.
Kudo, T. & Matsumoto, Y. (2000). Use of Support 
Vector Learning for Chunk Identification. Proc.of 
CoNLL-2000.
Kudo, T. & Matsumoto, Y. (2001). Chunking with 
Support Vector Machines. Proc.of NAACL 2001.
Levin, M. A., Krol, M., Doshi, A. M., & Reich, D. L. 
(2007). Extraction and mapping of drug names from 
free text to a standardized nomenclature. 
AMIA.Annu.Symp.Proc., 438-442.
Li, D., Savova, G., & Kipper-Schuler, K. (2008). 
Conditional random fields and support vector ma-
chines for disorder named entity recognition in clin-
ical texts. Proceedings of the workshop on current 
trends in biomedical natural language processing 
(BioNLP'08), 94-95.
Li, Z., Cao, Y., Antieau, L., Agarwal, S., Zhang, Z., & 
Yu, H. (2009). A Hybrid Approach to Extracting 
Medication Information from Medical Discharge 
Summaries. Proc of 2009 i2b2 workshop..
Lindberg, D. A., Humphreys, B. L., & McCray, A. T. 
(1993). The Unified Medical Language System. 
Methods Inf.Med., 32, 281-291.
265
Patrick, J. & Li, M. (2009). A Cascade Approach to 
Extract Medication Event (i2b2 challenge 2009). 
Proc of 2009 i2b2 workshop..
Sirohi, E. & Peissig, P. (2005). Study of effect of drug 
lexicons on medication extraction from electronic 
medical records. Pac.Symp.Biocomput., 308-318.
Takeuchi, K. & Collier, N. (2003). Bio-medical entity 
extraction using Support Vector Machines. Pro-
ceedings of the ACL 2003 workshop on Natural 
language processing in biomedicine, 57-64.
Torii, M., Hu, Z., Wu, C. H., & Liu, H. (2009). Bio-
Tagger-GM: a gene/protein name recognition sys-
tem. J.Am.Med.Inform.Assoc., 16, 247-255.
Tsochantaridis, I., Joachims, T., & Hofmann, T. 
(2005). Large margin methods for structured and in-
terdependent output variables. Journal of Machine 
Learning Research, 6, 1453-1484.
Uz?ner, O., Solti, I., & Cadag, E. (2009). The third 
2009 i2b2 challenge. 
In https://www.i2b2.org/NLP/Medication/.
Xu, H., Stenner, S. P., Doan, S., Johnson, K. B., 
Waitman, L. R., & Denny, J. C. (2010). MedEx: a 
medication information extraction system for clini-
cal narratives. J.Am.Med.Inform.Assoc., 17, 19-24.
Yamamoto, K., Kudo, T., Konagaya, A., & Matsumo-
to, Y. (2003). Protein name tagging for biomedical 
annotation in text. Proceedings of ACL 2003 Work-
shop on Natural Language Processing inBiomedi-
cine,2003, 13, 65-72.
266
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 903?907,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Implicit Feature Detection via a Constrained Topic Model and SVM
Wei Wang ?, Hua Xu ? and Xiaoqiu Huang ?
?State Key Laboratory of Intelligent Technology and Systems,
Tsinghua National Laboratory for Information Science and Technology,
Department of Computer Science and Technology, Tsinghua University,
Beijing 100084, China
?Beijing University of Posts and Telecommunications, Beijing 100876, China
ww880412@gmail.com, xuhua@tsinghua.edu.cn, alexalexhxqhxq@gmail.com
Abstract
Implicit feature detection, also known as im-
plicit feature identification, is an essential as-
pect of feature-specific opinion mining but
previous works have often ignored it. We
think, based on the explicit sentences, sever-
al Support Vector Machine (SVM) classifier-
s can be established to do this task. Never-
theless, we believe it is possible to do bet-
ter by using a constrained topic model instead
of traditional attribute selection methods. Ex-
periments show that this method outperforms
the traditional attribute selection methods by
a large margin and the detection task can be
completed better.
1 Introduction
Feature-specific opinion mining has been well de-
fined by Ding and Liu(2008). Example 1 is a cell
phone review in which two features are mentioned.
Example 1 This cell phone is fashion in appear-
ance, and it is also very cheap.
If a feature appears in a review directly, it is called
an explicit feature. If a feature is only implied, it is
called an implicit feature. In Example 1, appearance
is an explicit feature while price is an implicit fea-
ture, which is implied by cheap. Furthermore, an ex-
plicit sentence is defined as a sentence containing at
least one explicit feature, and an implicit sentence is
the sentence only containing implicit features. Thus,
the first sentence is an explicit sentence, while the
second is an implicit one.
This paper proposes an approach for implicit fea-
ture detection based on SVM and Topic Model(TM).
The Topic Model, which incorporated into con-
straints based on the pre-defined product feature,
is established to extract the training attributes for
SVM. In the end, several SVM classifiers are con-
structed to train the selected attributes and utilized
to detect the implicit features.
2 Related Work
The definition of implicit feature comes from Liu
et al (2005)?s work. Su et al (2006) used Point-
wise Mutual Information (PMI) based semantic as-
sociation analysis to identify implicit features, but
no quantitative experimental results were provided.
Hai et al (2011) used co-occurrence association rule
mining to identify implicit features. However, they
only dealt with opinion words and neglected the
facts. Therefore, in this paper, both the opinions and
facts will be taken into account.
Blei et al (2003) proposed the original LDA us-
ing EM estimation. Griffiths and Steyvers (2004)
applied Gibbs sampling to estimate LDA?s parame-
ters. Since the inception of these works, many vari-
ations have been proposed. For example, LDA has
previously been used to construct attributes for clas-
sification; it often acts to reduce data dimension(Blei
and Jordan, 2003; Fei-Fei and Perona, 2005; Quel-
has et al, 2005). Here, we modify LDA and adopt it
to select the training attributes for SVM.
3 Model Design
3.1 Introduction to LDA
We briefly introduce LDA, following the notation
of Griffiths(Griffiths and Steyvers, 2004). Given D
903
documents expressed over W unique words and T
topics, LDA outputs the document-topic distribution
? and topic-word distribution ?, both of which can
be obtained with Gibbs Sampling. For this scheme,
the core process is the topic updating for each word
in each document according to Equation 1.
P (zi = j|z?i,w, ?, ?) =
(
n(wi)?i,j + ?
?W
w? n
(w?)
?i,j +W?
)(
n(di)?i,j + ?
?T
j n
(di)
?i,j + T?
)
(1)
where zi = j represents the assignment of the ith
word in a document to topic j, z?i represents all
the topic assignments excluding the ith word. n(w
?)
j
is the number of instances of word w? assigned to
topic j and n(di)j is the number of words from doc-
ument di assigned to topic j, the ?i notation sig-
nifies that the counts are taken omitting the value
of zi. Furthermore, ? and ? are hyper-parameters
for the document-topic and topic-word Dirichlet dis-
tributions, respectively. After N iterations of Gibbs
sampling for all words in all documents, the distri-
bution ? and ? are finally estimated using Equations
2 and 3.
?(wi)j =
n(wi)j + ?
?W
w? n
(w?)
j +W?
(2)
?(di)j =
n(di)j + ?
?T
j n
(di)
j + T?
(3)
3.2 Framework
Algorithm 1 summarizes the main steps. When a
specific product and the reviews are provided, the
explicit sentences and corresponding features are
extracted(Line 1) by word segmentation, part-of-
speech(POS) tagging and synonyms feature cluster-
ing. Then the prior knowledge are drawn from the
explicit sentences automatically and integrated in-
to the constrained topic model((Line 3 - Line 5).
The word clusters are chosen as the training at-
tributes(Line 6). Finally, several SVM classifier-
s are generated and applied to detect implicit fea-
tures(Line 7 - Line 12).
Algorithm 1 Implicit Feature Detection
1: ES ? extract explicit sentence set
2: NES ? non-explicit sentence set
3: CS ? constraint set from ES
4: CPK ? correlation prior knowledge from ES
5: ETM?ConstrainedTopicModel(T ,ES,CS,CPK)
6: TA? select training attributes from ETM
7: for each fi in feature clusters do
8: TDi ? GenerateTrainingData(TAi,ES)
9: Ci? BuildClassificationModelBySVM(TDi)
10: PRi? positive result of Classify(Ci,NES)
11: the feature of sentence in PRi ? fi
12: end for
3.3 Prior Knowledge Extraction and
Incorporation
It is obvious that the pre-existing knowledge can as-
sist to produce better and more significant clusters.
In our work, we use a constrained topic model to s-
elect attributes for each product features. Each topic
is first pre-defined a product feature. Then two type-
s of prior knowledge, which are derived from the
pre-defined product features, are extracted automat-
ically and incorporated: must-link/cannot-link and
correlation prior knowledge.
3.3.1 Must-link and Cannot-link
Must-link: It specifies that two data instances
must be in the same cluster. Here is the must-link
from an observation: as ?cheap? to ?price?, some
words must be associated with a feature. In order
to mine these words, we compute the co-occurrence
degree by frequency*PMI(f,w), whose formula is as
following: Pf&w ? log2
Pf&w
PfPw , where P is the proba-
bility of subscript occurrence in explicit sentences,
f is the feature, w is the word, and f&w means
the co-occurrence of f and w. A higher value of
frequency*PMI signifies that w often indicates f .
For a feature fi, the top five words and fi consti-
tute must-links. For example, the co-occurrence of
?price? and ?cheap? is very high, then the must-link
between ?price? and ?cheap? can be identified.
Cannot-link: It specifies that two data instances
cannot be in the same cluster. If a word and a fea-
ture never co-occur in our corpus, we assume them
to form a cannot-link. For example, the word low-
cost has never co-occurred with the product feature
screen, so they constitute a cannot-link in our cor-
904
pus.
In this paper, the pre-defined process, must-link,
and cannot-link are derived from Andrzejewski and
Zhu (2009)?s work, all must-links and cannot-links
are incorporated our constrained topic model. We
multiply an indicator function ?(wi, zj), which rep-
resents a hard constraint, to the Equation 1 as the
final probability for topic updating (see Equation 4).
P (zi = j|z?i,w, ?, ?) =
?(wi, zj)(
n(wi)?i,j + ?
?W
w? n
(w?)
?i,j +W?
)(
n(di)?i,j + ?
?T
j n
(di)
?i,j + T?
)
(4)
As illustrated by Equations 1 and 4, ?(wi, zj),
which represents intervention or help from pre-
existing knowledge of must-links and cannot-links,
plays a key role in this study. In the topic updating
for each word in each document, we assume that the
current word is wi and its linked feature topic set is
Z(wi), then for the current topic zj , ?(wi, zj) is cal-
culated as follows:
1. If wi is constrained by must-links and the
linked feature belongs to Z(wi), ?(wi, zj |zj ?
Z(wi)) = 1 and ?(wi, zj |zj /? Z(wi)) = 0.
2. If wi is constrained by cannot-links and the
linked feature belongs to Z(wi), ?(wi, zj |zj ?
Z(wi)) = 0 and ?(wi, zj |zj /? Z(wi)) = 1.
3. In other cases, ?(wi, zj |j = 1, . . . , T ) = 1.
3.3.2 Correlation Prior Knowledge
In view of the explicit product feature of each top-
ic, the association of the word and the feature to
topic-word distribution should be taken into accoun-
t. Therefore, Equation 2 is revised as the following:
?(wi)j =
(1 + Cwi,j)(n
(wi)
j ) + ?
?W
w?(1 + Cw?,j)(n
(w?)
j ) +W?
(5)
where Cw?,j reflects the correlation of w? with the
topic j, which is centered on the product feature fzj .
The basic idea is to determine the association of w?
and fzj , if they have the high relevance,Cw?,j should
be set as a positive number. Otherwise, if we can
determine w? and fzj are irrelevant, Cw?,j should be
set as a positive number. In this paper, we attempt
to using PMI or dependency relation to judge the
relevance. For word w? and feature fzj :
1. Dependency relation judgement: If w? as par-
ent node in the syntax tree mainly co-occurs
with fzj ,Cw?,j will be set positive. Ifw? mainly
co-occurs with several features including fzj ,
Cw?,j will be set negative. Otherwise, Cw?,j
will be set 0.
2. PMI judgement: If w? mainly co-occurs with
fzj and PMI(w?, fzj ) is greater than the giv-
en value, Cw?,j will be set positive. Otherwise,
Cw?,j will be set negative.
3.4 Attribute Selection
Some words, such as ?good?, can modify sever-
al product features and should be removed. In the
result of run once, if a word appears in the topics
which relates to different features, it is defined as a
conflicting word. If a term is thought to describe
several features or indicate no features, it is defined
as a noise word .
When each topic has been pre-allocated, we run
the explicit topic model 100 times. If a word turns
into a conflicting word Tcw times(Tcw is set to 20),
we assume that it is a noise word. Then the noise
word collection is obtained and applied to filter the
explicit sentences. Actually, here 100 is just an esti-
mated number. And for Tcw, when it is between 15
and 25, the result is same, and when it exceeds 25,
the result does not change a lot. The most important
part to filter noise words is the correlation compu-
tation. So the experiment can work well with only
estimated parameters.
Next, By integrating pre-existing knowledge, the
explicit topic model, which runs Titer times, sever-
s as attribute selection for SVM. In every result for
each topic cluster, we remove the least four prob-
able of word groups and merge the results by the
pre-defined product feature. For a feature, if a word
appears in its topic words more than Titer ? tratio
times, it is selected as one of the training attributes
for the feature. In the end, if an attribute associates
with different features, it is deleted.
905
0 10 20 30 40 50 60 70 80 90 100Attribute Factor Number
 ChiSquare GainRatio InfoGain
(a) SVM based on traditional attribute
selection method
0.0 0.1 0.2 0.3 0.4 0.5t ratio
 TM TM+must TM+cannot TM+must+cannot TM+syntactic TM+must+cannot+syntactic TM+PMI TM+must+cannot+PMI TM+correlation knowledge(PMI+syntactic) TM+must+cannot+correlation knowledge
(b) our constrained topic model by
different tratio (Titer = 20)
0 10 20 30 40 50T iter
 TM TM+must TM+cannot TM+must+cannot TM+syntactic TM+must+cannot+syntactic TM+PMI TM+must+cannot+PMI TM+correlation knowledge(PMI+syntactic) TM+must+cannot+correlation knowledge
(c) our constrained topic model by
different Titer (tratio = 0.1)
Figure 1: Performance of different cases
3.5 Implicit Feature Detection via SVM
After completing attribute selection, vector space
model(VSM) is applied to the selected attributes on
the explicit sentences. For each feature fi, a SVM
classifierCi is adopted. In train-set, the positive cas-
es are the explicit sentences of fi, and the negative
cases are the other explicit sentences. For a non-
explicit sentence, if the classification result of Ci is
positive, it is an implicit sentence which implies fi.
4 Evaluation of Experimental Results
4.1 Data Sets
There has no standard data set yet, we crawled the
experiment data, which included reviews about a
cellphone, from a famous Chinese shopping web-
site1. The data contains 14218 sentences. The fea-
ture of each sentence was manually annotated by
two research assistants. A handful of sentences
which were annotated inconsistently were deleted.
Table 1 depicts the data set which is evaluated. Other
features were ignored because of their rare appear-
ance.
Here are some explanations: (1)The sentences
containing several explicit features were not added
to the train-set. (2) A tiny number of sentences con-
tain both explicit and implicit features, and they can
only be regarded as explicit sentences. (3) The train-
ing set contains 3140 explicit sentences, the test set
contains 7043 non-explicit sentences and more than
5500 sentences have no feature. (4) According to
the ratio among the explicit sentences(6:1:2:3:1:2),
it is reasonable that the most suitable number of top-
ics should be 14. For example, the ratio of the prod-
1http://www.360buy.com/
Table 1: Experiment data
Features Explicit Implicit Total
screen 1165 244 1409
quality 199 83 282
battery 456 205 661
price 627 561 1188
appearance 224 167 391
software 469 129 598
uct feature screen is 6, so we can assign the feature
to topic 0,1,2,3,4,5. In our experiment, the perfor-
mance of algorithm 1 is evaluated using F-measure.
(5) Although the size of dataset is limited, out pro-
posed is based on the constraint-based topic model,
which has been widely used in different NLP field-
s. So, our approach can generalize well in different
datasets. Of course, more high quality data will be
collected to do the experiment in the future.
4.2 Experimental Results
Figure 1a depicts the performance of using tradi-
tional attribute selection methods on SVM. Using
?2 test on SVM can achieve the best performance,
which is about 66.7%. In our constrained topic
model, we use different Titer and tratio. We con-
ducted experiments by incorporating different types
prior knowledge. From Figure 1b and 1c, we con-
clude that: (1)All these methods perform much bet-
ter than the traditional feature selection methods, the
improvements are more than 6%. (2)The reason for
the little improvement of must-links is that the top-
ic clusters have already obtained these linked word-
906
s. (3)All the pre-existing knowledge performs best
and shows 3% improvement over non prior knowl-
edge. (4)Different types of prior knowledge have
different impact on the stabilities of different pa-
rameters. (5)As we have expected, by combing al-
l prior knowledge, the best performance can reach
77.78%. Furthermore, as tratio or Titer changes,
our constrained topic model incorporating all prior
knowledge look like very stable.
5 Conclusions
In this paper, we adopt a constrained topic model
incorporating prior knowledge to select attribute for
SVM classifiers to detect implicit features. Exper-
iments show this method outperforms the attribute
feature selection methods and detect implicit fea-
tures better.
6 Acknowledgments
This work is supported by National Natural Science
Foundation of China (Grant No: 61175110) and Na-
tional Basic Research Program of China (973 Pro-
gram, Grant No: 2012CB316305).
References
David Andrzejewski and Xiaojin Zhu. 2009. Laten-
t dirichlet alocation with topic-in-set knowledge. In
Proceedings of the NAACL HLT 2009 Workshop on
Semi-Supervised Learning for Natural Language Pro-
cessing, pages 43?48. Association for Computational
Linguistics.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of the 26th annual international
ACM SIGIR conference on Research and development
in informaion retrieval, pages 127?134. ACM.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Laten-
t dirichlet alocation. the Journal of machine Learning
research, 3:993?1022.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the international conference on Web
search and web data mining, WSDM ?08, pages 231?
240, New York, NY, USA. ACM.
L. Fei-Fei and P. Perona. 2005. A bayesian hierarchical
model for learning natural scene categories. In Com-
puter Vision and Pattern Recognition, 2005. CVPR
2005. IEEE Computer Society Conference on, vol-
ume 2, pages 524?531. IEEE.
T.L. Griffiths and M. Steyvers. 2004. Finding scientif-
ic topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228?5235.
Z. Hai, K. Chang, and J. Kim. 2011. Implicit feature
identification via co-occurrence association rule min-
ing. Computational Linguistics and Intelligent Text
Processing, pages 393?404.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer:
analyzing and comparing opinions on the web. In Pro-
ceedings of the 14th international conference on World
Wide Web, pages 342?351. ACM.
P. Quelhas, F. Monay, J.M. Odobez, D. Gatica-Perez,
T. Tuytelaars, and L. Van Gool. 2005. Modeling
scenes with local descriptors and latent aspects. In
Computer Vision, 2005. ICCV 2005. Tenth IEEE In-
ternational Conference on, volume 1, pages 883?890.
IEEE.
Q. Su, K. Xiang, H. Wang, B. Sun, and S. Yu. 2006. Us-
ing pointwise mutual information to identify implicit
features in customer reviews. Computer Processing of
Oriental Languages. Beyond the Orient: The Research
Challenges Ahead, pages 22?30.
907
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 802?806,
Dublin, Ireland, August 23-24, 2014.
UTH_CCB: A Report for SemEval 2014 ? Task 7 Analysis of Clinical Text  
   Yaoyun Zhang1   Jingqi Wang1   Buzhou Tang2   Yonghui Wu1   Min Jiang1              Yukun Chen3  Hua Xu1* 1University of Texas School of Biomedical Informatics at Houston Houston, TX, 77030, USA 
2Harbin Institute of Technology Shenzhen Graduate School Shenzhen, 518055, China 
3Vanderbilt University Department of Biomedical Informatics Nashville, TN, 37240, USA {Yaoyun.Zhang, Yonghui.Wu, Min.Jiang, Hua.Xu} @uth.tmc.edu tangbuzhou@gmail.com yukun.chen@Vanderbilt.Edu      Abstract This work describes the participation of the University of Texas Health Science Center at Houston (UTHealth) team on the SemEval 2014 ? Task 7 analysis of clinical text challenge. The task consisted of two subtasks: (1) disorder entity recognition,  recognizing mentions of disorder concepts; (2) disorder entity encoding, mapping each mention to a unique Concept Unique Identifier (CUI) defined in Unified Medical Language System (UMLS). We developed three ensemble learning approaches for recognizing disorder entities and a Vector Space Model based method for encoding. Our approaches achieved top rank in both subtasks, with the best F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.  1 Introduction  In recent years, clinical natural language processing (NLP) has received great attention for its critical role in unlocking information embedded in clinical documents. Leveraging such information can facilitate the secondary1 use of electronic health record (EHR) data to                                                      This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details:http://creativecommons.org/licenses/by/4.0/ 
promote clinical and translational research.  Clinical entity recognition, which recognizes mentions of clinically relevant concepts (e.g., disorders, procedures, drugs etc.) in narratives,   and clinical entity encoding, which maps the recognized entities to concepts in standard vocabularies (e.g., UMLS CUI (Bodenreider, 2004)), are among the fundamental tasks in clinical NLP research. Many systems have been developed to extract clinical concepts from various types of clinical notes in last two decades, ranging from early symbolic NLP systems heavily dependent on domain knowledge to machine learning algorithm based systems driven by increasingly available annotated clinical corpora. The representative systems include MedLEE (Friedman et al., 1994), MetaMap (Aronson and Lang, 2010), KnowledgeMap (Denny et al., 2003), cTAKES (Savova et al., 2010), etc. Clinical NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) have promoted research using machine learning algorithms to recognize clinical entities (Uzuner et al., 2010; Uzuner et al., 2011).  Unlike the previous i2b2 challenges, the ShARe/CLEF challenge of clinical disorder extraction and encoding held in 2013 took the initiative to recognize disjoint entities, in addition to entities made up of consecutive words (Chapman et al., 2013). ShARe/CLEF challenge also required encoding of the disorder entities to Systematized Nomenclature Of Medicine Clinical Terms (SNOMED-CT) (using UMLS CUIs).  
802
In this paper, we describe our system for Task 7 of SemEval 2014, which followed the requirements of 2013 ShARe/CLEF challenge. Our system employed ensemble learning based approaches for disorder entity recognition and a Vector Space Model (VSM) based method for mapping extracted entities to CUIs of SNOMED-CT concepts. Our system was top-ranked among all participating teams according to evaluation by the organizer. 2 Method Our end-to-end system for Task 7 of SemEval 2014 consists of two components: disorder entity recognition and encoding. The raw clinical notes first went through the pre-processing modules for rule-based sentence boundary detection and tokenization. Extracted features were then used to train two machine learning algorithm-based entity recognition models, Conditional Random Fields (CRFs) (Lafferty et al., 2001) and Structural Support Vector Machines (SSVMs) (Tsochantaridis et al., 2005), respectively. These two models were ensembled with MetaMap, a symbolic biomedical NLP system, by three different approaches. Recognized entities were mapped to SNOWMED-CT CUIs in the encoding component. Detailed information of the components are presented in the following sections.  2.1 Dataset The training and test sets of 2013 ShARe/CLEF challenge were used as the training and development sets respectively for system development in SemEval 2014 Task 7. The training set consists of 199 notes and the development set has 99 notes, both of which were collected from four types of clinical notes including discharge summaries (DIS), radiology reports (RAD), and ECG/ECHO reports. Based on a pre-defined guideline, disorder entities were annotated for each note and then mapped to UMLS CUIs of SNOMED-CT concepts. Disorder entities not found in SNOMED-CT were marked as ?CUI-less?. The training set contained 5811 disorder entities which were mapped to 1007 unique CUIs or CUI-less. The development set contained 5340 disorder entities mapped to 795 CUIs or CUI-less. The test set contained 133 notes, all of which were discharge summaries. As the gold-standard annotation of the test set is not released by the organizer, the detailed annotation information of the test set is 
not available. Table 1 shows the total counts of notes, entities and CUIs in the three datasets.   Dataset Type Note Entity CUI CUI-less Train ALL 199 5816 4177 1639 ECHO 42 828 662 166 RAD 42 555 392 163 DIS 61 3589 2646 943 ECG 54 193 103 90 Dev ALL 99 5340 3619 1721 ECHO 12 338 241 97 RAD 12 162 126 36 DIS 75 4840 3252 1588 ECG 0 0 0  Test ALL 133 - -  DIS 133 - -   Table 1. Statistics of the dataset.  2.2 Disorder entity recognition The disorder entity recognition component consists of two modules: 1) the machine learning (e.g., CRF and SSVM) based named entity recognition (NER) module and 2) the   ensemble learning module. For the challenge of this year, we mainly focused on the second ensemble learning module. Machine learning based NER Module. This module was built based on our previous challenge participation in the 2013 ShARe/CLEF challenge (Tang et al., 2013). Annotated data were typically converted into a BIO format in machine learning-based NER systems. Each word was assigned one of the three labels: B for beginning of an entity, I for inside an entity, and O for outside of an entity. A unique challenge of this task is the high frequency (>10%) of disjoint disorders. For example, in the sentence ?the left atrium is not moderately dilated?, the discontinuous phrase ?left atrium?dilated? is defined as a disjoint disorder. Such entities could not be directly represented using the traditional BIO approach. Therefore, in addition to traditional BIO tags used for labeling words in the consecutive disorder entities, two sets of tags were created for disjoint entities: (1) D{B, I} was used to label disjoint entity words that are not shared by multiple concepts; and (2) H{B, I} was used to label head words that belonged to more than two disjoint concepts. Ultimately, we assigned one of the seven labels {B, I, O, DB, DI, HB, HI} to each word. A few simple rules were then defined to convert labeled words to entities (Tang et al., 2013).  
803
We exploited two state-of-the-art machine learning algorithms for disorder entity recognition, namely CRF (Lafferty et al., 2001) and SSVM (Tsochantaridis et al., 2005). CRFsuite and SVMhmm were used to implement CRF and SSVM respectively. For features, we used bag-of-word, part-of-speech from Stanford tagger, type of notes, section information, word representation from Brown clustering (Brown et al., 1992), random indexing (Lund and Burgess, 1996) and semantic categories of words based on UMLS lookup, MetaMap, and cTAKES outputs. More detailed information of this module can be found in our paper for 2013 ShARe/CLEF challenge (Tang et al., 2013).  One thing to note is that for word representation features like Brown clustering and random indexing, we only use the combination of traning and development and test datasets for feature extraction. The non-annotated corpus provided by the SemEval organizers was not employed currently. We do plan to pre-generate word clusters and random indexing using the provided corpus in the near future. Ensemble Learning Module. Three approaches were employed to consolidate the CRF-model, SSVM-model and the MetaMap outputs, namely machine learning classifier based ensemble (ensembleML), majortiy voting based ensemble (ensembleMV) and direct merging of the entity recognition results from the three models (ensembleDM). In the ensembleML approach, a binary classifier was trained to determine if the entities recognized by the CRF-model, SSVM-model and MetaMap were true positives. A new set of features were then extracted for each candidate entity, that included the specific models recognizing the entity, the entity itself, n-gram and word shape features of the first/last word of the entity. A sliding window based feature was extracted to check whether there was any recognized entity within 20 characters before the first and after the last word. Some features extracted from the first module were also employed. We used the open source toolkit Liblinear (Fan et al., 2008), to build the binary classifier for ensembleML.  2.3 Disorder Entity Encoding We developed a Vector Space Model (VSM) based approach to find the most suitable CUI for a given disorder entity. The disorder entity was 
used as query and all the UMLS terms were treated as documents. We used the cosine-similarity score to rank the candidate terms. For post-processing, if the top-ranked CUI was not a disorder CUI, it was replaced with ?CUI-less?.  ?CUI-less? was also assigned to entities without any retrieved candidate CUI. 2.4 Experiments and Evaluation Our system was developed and trained using the enlarged training set by merging the 199 notes in the training set and the 99 notes in the development set. All parameters of CRF, SSVM and Liblinear were optimized by 10-fold cross-validation on the enlarged training dataset. The performance of disorder entity recognition was evaluated by precision, recall and F-measure, which were measured in both ?strict? and ?re-laxed? modes. The ?strict? mode was defined as follows: a concept is correctly recognized if and only if it can be matched exactly to a disorder mention in the gold standard, and the ?relaxed? mode means that a disorder mention is correctly recognized if it overlaps with any disorder men-tion in the gold standard. For entity encoding, all participating systems were evaluated using accu-racy, in ?strict? and ?relaxed? modes, as defined in (Suominen et al., 2013). 3 Results Table 2 and Table 3 show the best performance of our systems in the SemEval 2014 Task 7 as reported by the organizers, where ?P?, ?R?, ?F? denote precision, recall and F-measure respectively. For disorder entity recognition, the ensembleML based system outperformed the other two ensemble approaches, achieving the best F-measure of 0.813 under ?strict? criterion and was ranked first in the challenge. For encoding, our system achieved an accuracy of 0.741 by ensembleDM under ?strict? criterion and was again ranked first in the challenge.   Strict Relaxed P R F P R F ensembleML 84.3 78.6 81.3 93.6 86.6 90.0 Table 2. The disorder recognition performance of our system for the SemEval 2014 task 7 (%).   Accuracy Strict Relaxed ensembleDM 0.741 0.873 Table 3. The SNOMED encoding performance of our system for the SemEval 2014 task 7.  
804
4 Discussion In this study, we developed an ensemble learning-based approach to recognize disorder entities and a vector space model-based method to encode disorders to UMLS CUIs.  Our system was top-ranked among all participating teams. However, there are still expectations for further improvement.  For disorder entity recognition, directly merging the entity recognition results of the three models (ensembleDM) achieved the highest encoding accuracy of 0.741. This shows the great potential of performance enhancement by combining different models. However, the precision of ensembleDM was much lower than the current machine learning-based ensemble approach ensembleML. ensembleML improved the precision to 84.3%, with the lowest recall of 78.6% among the three ensemble approaches. Further investigations for balancing and enhancing both precision and recall simultaneously by combining different models will be pursued in the follow-up studies. For encoding, when a disorder entity can be labelled with multiple CUIs in different contexts, a more effective disambiguation model could be exploited. Further, query expansion techniques may be helpful and worth investigating. The above methods should be potentially helpful to address the problems caused by synonyms or spelling variants.  5 Conclusion We developed a clinical disorder recognition and encoding system that consists of a ensemble learning-based approach to recognize disorder entities and a vector space model-based method to encode the identified disorders to UMLS CUIs of SNOMED-CT concepts. The performance of our system was top-ranked in the SemEval 2014 Task 7, indicating that our approaches are promising. However, further improvements are needed in order to enhance performance on concept extraction and encoding in clinical text. Acknowledgments This study is supported in part by grants from NLM R01LM010681, NCI 1R01CA141307, NIGMS 1R01GM102282 and CPRIT R1307 (H.X).    
Reference Aronson, A. R., & Lang, F.-M. (2010). An overview of MetaMap: historical perspective and recent advances. Journal of the American Medical Informatics Association: JAMIA, 17(3), 229?236.  Bodenreider, O. (2004). The unified medical language system (UMLS): integrating biomedical terminology. Nucleic Acids Research, 32(suppl 1), 267?270. Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-Based n-gram Models of Natural Language. Computational Linguistics, 18, 467?479. Denny, J. C., Irani, P. R., Wehbe, F. H., Smithers, J. D., & Spickard, A. (2003). The KnowledgeMap Project: Development of a Concept-Based Medical School Curriculum Database. AMIA Annual Symposium Proceedings, 2003, 195?199. Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9, 1871?1874. Friedman, C., Alderson, P. O., Austin, J. H., Cimino, J. J., & Johnson, S. B. (1994). A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1(2), 161?174. Lafferty, J., McCallum, A., & Pereira, F. C. N. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Departmental Papers (CIS).  Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, & Computers, 28(2), 203?208.  Savova, G. K., Masanz, J. J., Ogren, P. V., Zheng, J., Sohn, S., Kipper-Schuler, K. C., & Chute, C. G. (2010). Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association: JAMIA, 17(5), 507?513.  Suominen, H., Salanter?, S., Velupillai, S., Chapman, W. W., Savova, G., & Elhadad, N. (2013). Overview of the ShARe/CLEF eHealth Evaluation Lab 2013. Information Access Evaluation. Multilinguality, Multimodality, and Visualization, 2013, 212?231. Tang, B., Cao, H., Wu, Y., Jiang, M., & Xu, H. (2013). Recognizing and Encoding Discorder Concepts in Clinical Text using Machine Learning and Vector Space Model. Workshop of ShARe/CLEF eHealth Evaluation Lab 2013.  
805
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6, 1453?1484. Uzuner, ?., Solti, I., & Cadag, E. (2010). Extracting medication information from clinical text. Journal of the American Medical Informatics Association, 17(5), 514?518.  Uzuner, ?., South, B. R., Shen, S., & DuVall, S. L. (2011). 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association: JAMIA, 18(5), 552?556.    
806
Soochow University: Description and Analysis of the Chinese 
Word Sense Induction System for CLP2010 
Hua Xu   Bing Liu   Longhua Qian?   Guodong Zhou 
Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
Email: 
{20094227034,20084227065055,qianlonghua,gdzhou}@suda.edu.cn
 
                                                 
? Corresponding author 
Abstract 
Recent studies on word sense induction 
(WSI) mainly concentrate on European 
languages, Chinese word sense induction 
is becoming popular as it presents a new 
challenge to WSI. In this paper, we 
propose a feature-based approach using 
the spectral clustering algorithm to this 
problem. We also compare various 
clustering algorithms and similarity 
metrics. Experimental results show that 
our system achieves promising 
performance in F-score. 
1 Introduction 
Word sense induction (WSI) is an open problem 
of natural language processing (NLP), which 
governs the process of automatic discovery of 
the possible senses of a word. WSI is similar to 
word sense disambiguation (WSD) both in 
methods employed and in problem encountered. 
In the procedure of WSD, the senses are as-
sumed to be known and the task focuses on 
choosing the correct one for an ambiguous word 
in a context. The main difference between them 
is that the task of WSD generally requires large-
scale manually annotated lexical resources while 
WSI does not. As WSI doesn?t rely on the 
manually annotated corpus, it has become one of 
the most important topics in current NLP re-
search (Pantel and Lin, 2002; Neill, 2002; Rapp, 
2003). Typically, the input to a WSI algorithm is 
a target word to be disambiguated. The task of 
WSI is to distinguish which target words share 
the same meaning when they appear in different 
contexts. Such result can be at the very least 
used as empirically grounded suggestions for 
lexicographers or as input for WSD algorithm. 
Other possible uses include automatic thesaurus 
or ontology construction, machine translation or 
information retrieval. Compared with European 
languages, the study of WSI in Chinese is scarce. 
Furthermore, as Chinese has its special writing 
style and Chinese word senses have their own 
characteristics, the methods that work well in 
English may not perform effectively in Chinese 
and the usefulness of WSI in real-world applica-
tions has yet to be tested and proved. 
The core idea behind word sense induction is 
that contextual information provides important 
cues regarding a word?s meaning. The idea dates 
back to (at least) Firth (1957) (?You shall know 
a word by the company it keeps?), and under-
lies most WSD and lexicon acquisition work to 
date. For example, when the adverb phrase oc-
curring prior to the ambiguous word????, 
then the target word is more likely to be a verb 
and the meaning of which is ?to hold something?; 
Otherwise, if an adjective phrase locates in the 
same position, then it probably means ?confi-
dence? in English. Thus, the words surrounds 
the target word are main contributor to sense 
induction. 
The bake off task 4 on WSI in the first CIPS-
SIGHAN Joint Conference on Chinese Lan-
guage Processing (CLP2010) is intended to 
promote the exchange of ideas among partici-
pants and improve the performance of Chinese 
WSI systems. Generally, our WSI system also 
adopts a clustering algorithm to group the con-
texts of a target word. Differently, after generat-
ing feature vectors of words, we compute a simi-
larity matrix with each cell denoting the similar-
ity between two contexts. Furthermore, the set of 
similarity values of a context with other contexts 
is viewed as another kind of feature vector, 
which we refer to as similarity vector. Both fea-
ture vectors and similarity vectors can be sepa-
rately used as the input to clustering algorithms. 
Experimental results show our system achieves 
good performances on the development dataset 
as well as on the final test dataset provided by 
the CLP2010. 
2 System Description 
This section sequentially describes the architec-
ture of our WSI system and its main components. 
2.1 System Architecture 
Figure 1 shows the architecture of our WSI 
system. The first step is to preprocess the raw 
dataset for feature extraction. After that, we 
extract ?bag of words? from the sentence 
containing a target word (feature extraction) and 
transform them into high-dimension vectors 
(feature vector generation). Then, similarities of 
every two vectors could be computed based on 
the feature vectors (similarity measurement). the 
similarities of an instance can be viewed as 
another vector?similarity vector. Both feature 
vectors and similarity vectors can be served as 
the input for clustering algorithms. Finally, we 
perform three clustering algorithms, namely, k-
means, HAC and spectral clustering.  
Dataset
Preprocess
Feature
Extraction
Vector
Generation
Similarity
Measurement
Similarity
As VectorClustering
WSI
Results
 
Figure 1  Architecture of our Chinese 
WSI system 
2.2 Feature Engineering 
In the task of WSI, the target words with their 
topical context are first transformed into multi-
dimensional vectors with various features, and 
then applying clustering algorithm to detect the 
relevance of each other. 
Corpus Preprocessing 
For each raw file, we first extract each sentence 
embedded in the tag <instance>, including 
the <head> and </head> tags which are used 
to identify the ambiguous word. Then, we put all 
the sentences related to one target word into a 
file, ordered by their instance IDs. The next step 
is word segmentation, which segments each sen-
tence into a sequence of Chinese words and is 
unique for Chinese WSI. Here, we use the soft-
ware from Hylanda1 since it is ready to use and 
considered an efficient word segmentation tool. 
Finally, since we retain the <head> tag in the 
sentence, the <head> and </head> tags are 
usually separated after word segmentation, thus 
we have to restore them in order to correctly lo-
cate the target word during the process of feature 
extraction. 
Feature Extraction 
After word segmentation, for a context of a par-
ticular word, we extract all the words around it 
in the sentence and build a feature vector based 
on a ?bag-of-words? Boolean model. ?Bag-of-
words? means that we don?t consider the order 
of words. Meanwhile, in the Boolean model, 
each word in the context is used to generate a 
feature. This feature will be set to 1 if the word 
appears in the context or 0 if it does not. Finally, 
we get a number of feature vectors, each of them 
corresponds to an instance of the target word. 
One problem with this feature-based method is 
that, since the size of word set may be huge, the 
dimension is also very high, which might lead to 
data sparsity problem.  
Similarity measurement 
One commonly used metric for similarity meas-
urement is cosine similarity, which measures the 
angle between two feature vectors in a high-
dimensional space. Formally, the cosine similar-
ity can be computed as follows: 
cos ,ine similarity ?< > = ?
x yx y
x y
 
where ,x y are two vectors in the vector space 
and x , y are the lengths of  ,x y  respectively. 
                                                 
1 http://www.hylanda.com/
Some clustering algorithms takes feature vec-
tors as the input and use cosine similarity as the 
similarity measurement between two vectors. 
This may lead to performance degradation due 
to data sparsity in feature vectors. To avoid this 
problem, we compute the similarities of every 
two vectors and generate an  similarity 
matrix, where  is the number of all the in-
stances containing the ambiguous word. Gener-
ally, is usually much smaller than the dimen-
sion size and may alleviate the data sparsity 
problem. Moreover, we view every row of this 
matrix (i.e., an ordered set of similarities of an 
instance with other instances) as another kind of 
feature vector. In other words, each instance it-
self is regarded as a feature, and the similarity 
with this instance reflects the weight of the fea-
ture. We call this vector similarity vector, which 
we believe will more properly represent the in-
stance and achieve promising performance. 
*N N
N
N
2.3 Clustering Algorithm 
Clustering is a very popular technique which 
aims to partition a dataset into such subgroups 
that samples in the same group share more simi-
larities than those from different groups. Our 
system explores various cluster algorithms for 
Chinese WSI, including K-means, hierarchical 
agglomerative clustering (HAC), and spectral 
clustering (SC). 
K-means (KM) 
K-means is a very popular method for general 
clustering used to automatically partition a data 
set into k groups. K-means works by assigning 
multidimensional vectors to one of K clusters, 
where is given as a priori. The aim of the al-
gorithm is to minimize the variance of the vec-
tors assigned to each cluster.  
K
K-means proceeds by selecting k  initial clus-
ter centers and then iteratively refining them as 
follows: 
(1) Choose cluster centers to coincide with 
k randomly-chosen patterns or k  ran-
domly defined points. 
k
(2) Assign each pattern to the closest cluster 
center. 
(3) Recompute the cluster centers using the 
current cluster memberships. 
(4) If a convergence criterion is not met, go 
to step 2. 
Hierarchical Agglomerative Clustering (HAC) 
Different from K-means, hierarchical clustering 
creates a hierarchy of clusters which can be 
represented in a tree structure called a 
dendrogram. The root of the tree consists of a 
single cluster containing all objects, and the 
leaves correspond to individual object.  
Typically, hierarchical agglomerative 
clustering (HAC) starts at the leaves and 
successively merges two clusters together as 
long as they have the shortest distance among all 
the pair-wise distances between any two clusters.  
Given a specified number of clusters, the key 
problem is to determine where to cut the hierar-
chical tree into clusters. In this paper, we gener-
ate the final flat cluster structures greedily by 
maximizing the equal distribution of instances 
among different clusters. 
Spectral Clustering (SC) 
Spectral clustering refers to a class of techniques 
which rely on the eigen-structure of a similarity 
matrix to partition points into disjoint clusters 
with points in the same cluster having high simi-
larity and points in different clusters having low 
similarity.  
Compared to the ?traditional algorithms? such 
as K-means or single linkage, spectral clustering 
has many fundamental advantages. Results ob-
tained by spectral clustering often outperform 
the traditional approaches, spectral clustering is 
very simple to implement and can be solved ef-
ficiently by standard linear algebra methods. 
3 System Evaluation 
This section reports the evaluation dataset and 
system performance for our feature-based Chi-
nese WSI system. 
3.1  Dataset and Evaluation Metrics 
We use the CLP2010 bake off task 4 sample 
dataset as our development dataset. There are 
2500 examples containing 50 target words and 
each word has 50 sentences with different mean-
ings. The exact meanings of the target words are 
blind, only the number of the meanings is pro-
vided in the data. We compute the system per-
formance with the sample dataset because it con-
tains the answers of each candidate meaning. 
The test dataset provided by the CLP2010 is 
similar to the sample dataset. It contains 100 
target words and 5000 instances in total. How-
ever, it doesn?t provide the answers. 
The F-score measurement is the same as Zhao 
and Karypis (2005). Given a particular 
class rL of size and a particular cluster  of 
size , suppose  in the cluster  belong to
rn iS
in irn iS rL , 
then the value of this class and cluster is de-
fined to be 
F
2 ( , ) ( ,( , )
( , ) ( , )
r i r i
r i
r i r i
)R L S P L SF L S
R L S P L S
? ?= +  
( , ) /r i ir rR L S n n=  
( , ) /r i ir iP L S n n=  
where ( , )r iR L S is the recall value and  
is the precision value. The F-score of class 
( , )r iP L S
rL is 
the maximum value and F-score value follow: F
( ) max ( , )
ir S r i
F score L F L S? =  
1
( )
c
r
r
r
nF score F score L
n=
? = ??  
where  is the total number of classes and n  is 
the total size. 
c
3.2 Experiment Results 
Table 1 reports the F-score of our feature-based 
Chinese WSI for different feature sets with 
various window sizes using K-means clustering. 
Since there are different results for each run of 
K-means clustering algorithm, we perform 20 
trials and compute their average as the final 
results. The columns denote different window 
size n, that is, the n words before and after the 
target word are extracted as features. Particularly, 
the size of infinity (?) means that all the words 
in the sentence except the target word are 
considered. The rows represent various 
combinations of feature sets and similarity 
measurements, currently, four of which are 
considered as follows: 
F-All: all the words are considered as features 
and from them feature vectors are constructed. 
F-Stop: the top 150 most frequently occurring 
words in the total ?word bags? of the corpus are 
regarded as stop words and thus removed from 
the feature set. Feature vectors are then formed 
from these words. 
S-All: the feature set and the feature vector 
are the same as those of F-All, but instead the 
similarity vector is used for clustering (c.f. Sec-
tion 2.2). 
S-Stop: the feature set and the feature vector 
are the same as those of F-Stop, but instead the 
similarity vector is used for clustering. 
Table 1 Experimental results for differ-
ent feature sets with different window sizes us-
ing K-means clustering 
 
This table shows that S-Stop achieves the best 
performance of 0.7320 in F-score. This suggests 
that for K-means clustering, Chinese WSI can 
benefit much from removing stop words and 
adopting similarity vector. It also shows that: 
Feature/ 
Similarity 3 7 10 ? 
F-All 0.5949 0.6199 0.6320 0.6575
F-Stop 0.6384 0.6500 0.6493 0.6428
S-All 0.5856 0.6044 0.6186 0.6843
S-Stop 0.6532 0.6696 0.6804 0.7320
z As the window size increases, the perform-
ance is almost consistently enhanced. This 
indicates that all the words in the sentence 
more or less help disambiguate the target 
word. 
z Removing stop words consistently improves 
the F-score for both similarity metrics. This 
means some high frequent words do not help 
discriminate the meaning of the target words, 
and further work on feature selection is thus 
encouraged. 
z Similarity vector consistently outperforms 
feature vector for stop-removed features, but 
not so for all-words features. This may be 
due to the fact that, when the window size is 
limited, the influence of frequently occur-
ring stop words is relatively high, thus the 
similarity vector misrepresent the context of 
the target word. On the contrary, when stop 
words are removed or the context is wide, 
the similarity vector can better reflect the 
target word?s context, leading to better per-
formance. 
In order to intuitively explain why the simi-
larity vector is more discriminative than the fea-
ture vector, we take two sentences containing 
the Chinese word ???? (hold, grasp) as an ex-
ample (Figure 2). These two sentences have few 
common words, so clustering via feature vectors 
puts them into different classes. However, since 
the similarities of these two feature vectors with 
other feature vectors are much similar, cluster-
ing via similarity vectors group them into the 
same class.  
 
Figure 2  An example from the dataset 
 
According to the conclusion of the above ex-
periments, it is better to include all the words 
except stop words in the sentence as the features 
in the subsequent experiment. Table 2 lists the 
results using various clustering algorithms with 
this same experimental setting. It shows that the 
spectral clustering algorithm achieves the best 
performance of 0.7692 in F-score for Chinese 
WSI using the S-All setup. Additionally, there 
are some interesting findings: 
mi-
 of 
han 
ing 
this 
nto 
lus-
lly 
er-
re. 
ex-
der 
ers the density information, therefore S-All 
will not significantly improve the perform-
ance. 
 
Feature/ 
Similarity 
KM HAC SC 
F-All 0.6428 0.6280 0.7686 
S-All 0.7320 0.6332 0.7692 
Table 2 Experiments results using dif-
ferent clustering algorithms 
<lexelt item="??" snum="4"> 
<instance id="0012"> 
???????????????????
?????????????????
<head>??</head>???????????
?? 
 </instance>  
<instance id="0015">  
???????????????????
???????????????<head>?
?</head>???????????????
??????????????????? 
</instance>  
</lexelt> 
3.3 Final System Performance 
For the CLP2010 task 4 test dataset which con-
tains 100 target words and 5000 instances in to-
tal, we first extract all the words except stop 
words in a sentence containing the target word, 
then produce the feature vector for each context 
and generate the similarity matrix, finally we 
perform the spectral cluster algorithm. Probably 
because the distribution of the target word in the 
test dataset is different from that in the develop-
ment dataset, the F-score of our system on the 
test dataset is 0.7108, about 0.05 units lower 
than that we got on the sample dataset. 
4 Conclusions and Future Work 
In our Chinese WSI system, we extract all the 
words except stop words in the sentence, con-
struct feature vectors and similarity vectors, and 
apply the spectral clustering algorithm to this 
problem. Experimental results show that our 
simple and efficient system achieve a promising 
result. Moreover, we also compare various clus-
tering algorithms and similarity metrics. We find 
that although the spectral clustering algorithm 
outperforms other clustering algorithms, the K-
means clustering with similarity vectors can also 
achieve comparable results. 
For future work, we will incorporate more 
linguistic features, such as base chunking, parse 
tree feature as well as dependency information 
into our system to further improve the perform-
ance. 
Acknowledgement 
This research is supported by Project 60873150, 
60970056 and 90920004 under the National 
Natural Science Foundation of China. We would z Although SC performs best, KM with si
larity vectors achieves comparable results
0.7320 units in F-score, slightly lower t
that of SC. 
z HAC performs worst among all cluster
algorithms. An observation reveals that 
algorithm always groups the instances i
highly skewed clusters, i.e., one or two c
ters are extremely large while others usua
have only one instance in each cluster. 
z It is surprising that S-All slightly outp
forms F-All by only 0.0006 units in F-sco
The truth is that, as discussed in the first 
periment, KM using F-All doesn?t consi
instance density while S-All does. On the 
contrary, SC identifies the eign-structure in 
the instance space and thus already consid-
also like to thank other contributors in the NLP 
lab at Soochow University. 
References 
Jain A, Murty M. 1999.Flynn P. Data clustering : A 
Review [J]. ACM Computing Surveys,1999,31 
(3) :2642323 
F. Bach and M. Jordan.2004. Learning spectral clus-
tering. In Proc. of NIPS-16. MIT Press, 2004. 
Samuel Brody and Mirella Lapata. 2009. Bayesian 
word sense induction. In Proceedings of the 12th 
Conference of the European Chapter of the ACL 
(EACL 2009), pages 103?111. 
Neill, D. B. 2002. Fully Automatic Word Sense In-
duction by Semantic Clustering. Cambridge Uni-
versity, Master?s Thesis, M.Phil. in Computer 
Speech. 
Agirre, E. and Soroa, A. 2007.  Semeval-2007 task 02: 
Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations:7-12 
Ioannis P. Klapaftis and Suresh Manandhar. 2008. 
Word sense induction using graphs of collocations. 
In Proceedings of the 18th European Conference 
On Artificial Intelligence (ECAI-2008), Patras, 
Greece, July. IOS Press. 
Kannan, R., Vempala, S and Vetta, A. 2004. On clus-
terings: Good, bad and spectral. J. ACM, 51(3), 
497?515. 
Reinhard Rapp.2004. A practical solution to the 
problem of automatic word sense induction. Pro-
ceedings of the ACL 2004 on Interactive poster 
and demonstration sessions, p.26-es, July 21-26, 
2004, Barcelona, Spain 
Bordag, S. 2006. Word sense induction: Triplet-based 
clustering and automatic evaluation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics (EACL, Trento, Italy). 137--144. 
Ying Zhao, and George Karypis.2005. Hierarchical 
Clustering Algorithms for Document Datasets. Da-
ta Mining and Knowledge Discovery, 10, 141?168. 
 
