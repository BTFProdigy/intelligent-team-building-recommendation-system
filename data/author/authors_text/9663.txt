Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 371?379,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Global Models of Document Structure Using Latent Permutations
Harr Chen, S.R.K. Branavan, Regina Barzilay, David R. Karger
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{harr, branavan, regina, karger}@csail.mit.edu
Abstract
We present a novel Bayesian topic model for
learning discourse-level document structure.
Our model leverages insights from discourse
theory to constrain latent topic assignments in
a way that reflects the underlying organiza-
tion of document topics. We propose a global
model in which both topic selection and order-
ing are biased to be similar across a collection
of related documents. We show that this space
of orderings can be elegantly represented us-
ing a distribution over permutations called the
generalized Mallows model. Our structure-
aware approach substantially outperforms al-
ternative approaches for cross-document com-
parison and single-document segmentation.1
1 Introduction
In this paper, we introduce a novel latent topic model
for the unsupervised learning of document structure.
Traditional topic models assume that topics are ran-
domly spread throughout a document, or that the
succession of topics in a document is Markovian.
In contrast, our approach takes advantage of two
important discourse-level properties of text in de-
termining topic assignments: first, that each docu-
ment follows a progression of nonrecurring coher-
ent topics (Halliday and Hasan, 1976); and sec-
ond, that documents from the same domain tend
to present similar topics, in similar orders (Wray,
2002). We show that a topic model incorporat-
ing these long-range dependencies outperforms al-
1Code, data, and annotations used in this work are available
at http://groups.csail.mit.edu/rbg/code/mallows/
ternative approaches for segmentation and cross-
document comparison.
For example, consider a collection of encyclope-
dia articles about cities. The first constraint captures
the notion that a single topic, such as Architecture,
is expressed in a contiguous block within the docu-
ment, rather than spread over disconnected sections.
The second constraint reflects our intuition that all
of these related articles will generally mention some
major topics associated with cities, such as History
and Culture, and will often exhibit similar topic or-
derings, such as placing History before Culture.
We present a Bayesian latent topic model over re-
lated documents that encodes these discourse con-
straints by positing a single distribution over a doc-
ument?s entire topic structure. This global view on
ordering is able to elegantly encode discourse-level
properties that would be difficult to represent using
local dependencies, such as those induced by hid-
den Markov models. Our model enforces that the
same topic does not appear in disconnected portions
of the topic sequence. Furthermore, our approach
biases toward selecting sequences with similar topic
ordering, by modeling a distribution over the space
of topic permutations.
Learning this ordering distribution is a key tech-
nical challenge in our proposed approach. For this
purpose, we employ the generalized Mallows model,
a permutation distribution that concentrates proba-
bility mass on a small set of similar permutations.
It directly captures the intuition of the second con-
straint, and uses a small parameter set to control how
likely individual topics are to be reordered.
We evaluate our model on two challenging
371
document-level tasks. In the alignment task, we aim
to discover paragraphs across different documents
that share the same topic. We also consider the seg-
mentation task, where the goal is to partition each
document into a sequence of topically coherent seg-
ments. We find that our structure modeling approach
substantially outperforms state-of-the-art baselines
for both tasks. Furthermore, we demonstrate the im-
portance of explicitly modeling a distribution over
topic permutations; our model yields significantly
better results than variants that either use a fixed or-
dering, or are order-agnostic.
2 Related Work
Topic and ContentModels Our work is grounded
in topic modeling approaches, which posit that la-
tent state variables control the generation of words.
In earlier topic modeling work such as latent Dirich-
let alocation (LDA) (Blei et al, 2003; Griffiths and
Steyvers, 2004), documents are treated as bags of
words, where each word receives a separate topic
assignment; the topic assignments are auxiliary vari-
ables to the main task of language modeling.
More recent work has attempted to adapt the con-
cepts of topic modeling to more sophisticated repre-
sentations than a bag of words; they use these rep-
resentations to impose stronger constraints on topic
assignments (Griffiths et al, 2005; Wallach, 2006;
Purver et al, 2006; Gruber et al, 2007). These
approaches, however, generally model Markovian
topic or state transitions, which only capture lo-
cal dependencies between adjacent words or blocks
within a document. For instance, content mod-
els (Barzilay and Lee, 2004; Elsner et al, 2007)
are implemented as HMMs, where the states cor-
respond to topics of domain-specific information,
and transitions reflect pairwise ordering prefer-
ences. Even approaches that break text into con-
tiguous chunks (Titov and McDonald, 2008) as-
sign topics based on local context. While these
locally constrained models can implicitly reflect
some discourse-level constraints, they cannot cap-
ture long-range dependencies without an explosion
of the parameter space. In contrast, our model cap-
tures the entire sequence of topics using a compact
representation. As a result, we can explicitly and
tractably model global discourse-level constraints.
Modeling Ordering Constraints Sentence order-
ing has been extensively studied in the context of
probabilistic text modeling for summarization and
generation (Barzilay et al, 2002; Lapata, 2003;
Karamanis et al, 2004). The emphasis of that body
of work is on learning ordering constraints from
data, with the goal of reordering new text from the
same domain. Our emphasis, however, is on ap-
plications where ordering is already observed, and
how that ordering can improve text analysis. From
the methodological side, that body of prior work is
largely driven by local pairwise constraints, while
we aim to encode global constraints.
3 Problem Formulation
Our document structure learning problem can be for-
malized as follows. We are given a corpus of D
related documents. Each document expresses some
subset of a common set of K topics. We assign a
single topic to each paragraph,2 incorporating the
notion that paragraphs are internally topically con-
sistent (Halliday and Hasan, 1976). To capture the
discourse constraint on topic progression described
in Section 1, we require that topic assignments be
contiguous within each document.3 Furthermore,
we assume that the underlying topic sequences ex-
hibit similarity across documents. Our goal is to re-
cover a topic assignment for each paragraph in the
corpus, subject to these constraints.
Our formulation shares some similarity with the
standard LDA setup, in that a common set of topics
is assigned across a collection of documents. How-
ever, in LDA each word?s topic assignment is con-
ditionally independent, following the bag of words
view of documents. In contrast, our constraints on
how topics are assigned let us connect word distri-
butional patterns to document-level topic structure.
4 Model
We propose a generative Bayesian model that ex-
plains how a corpus of D documents, given as se-
quences of paragraphs, can be produced from a set
of hidden topic variables. Topic assignments to each
2Note that our analysis applies equally to other levels of tex-
tual granularity, such as sentences.
3That is, if paragraphs i and j are assigned the same topic,
every paragraph between them must have that topic.
372
paragraph, ranging from 1 to K, are the model?s
final output, implicitly grouping topically similar
paragraphs. At a high level, the process first selects
the bag of topics to be expressed in the document,
and how they are ordered; these topics then deter-
mine the selection of words for each paragraph.
For each document dwithNd paragraphs, we sep-
arately generate a bag of topics td and a topic order-
ing pid. The unordered bag of topics, which contains
Nd elements, expresses how many paragraphs of the
document are assigned to each of theK topics. Note
that some topics may not appear at all. Variable td
is constructed by taking Nd samples from a distri-
bution over topics ? , a multinomial representing the
probability of each topic being expressed. Sharing
? between documents captures the intuition that cer-
tain topics are more likely across the entire corpus.
The topic ordering variable pid is a permutation
over the numbers 1 through K that defines the order
in which topics appear in the document. We draw pid
from the generalized Mallows model, a distribution
over permutations that we explain in Section 4.1. As
we will see, this particular distribution biases the
permutation selection to be close to a single cen-
troid, reflecting the discourse constraint of prefer-
ring similar topic structures across documents.
Together, a document?s bag of topics td and or-
dering pid determine the topic assignment zd,p for
each of its paragraphs. For example, in a corpus
with K = 4, a seven-paragraph document d with
td = {1, 1, 1, 1, 2, 4, 4} and pid = (2 4 3 1) would
induce the topic sequence zd = (2 4 4 1 1 1 1). The
induced topic sequence zd can never assign the same
topic to two unconnected portions of a document,
thus satisfying the constraint of topic contiguity.
As with LDA, we assume that each topic k is as-
sociated with a language model ?k. The words of a
paragraph assigned to topic k are then drawn from
that topic?s language model ?k.
Before turning to a more formal discussion of the
generative process, we first provide background on
the permutation model for topic ordering.
4.1 The Generalized Mallows Model
A central challenge of the approach we take is mod-
eling the distribution over possible topic permuta-
tions. For this purpose we use the generalized Mal-
lows model (GMM) (Fligner and Verducci, 1986;
Lebanon and Lafferty, 2002; Meila? et al, 2007),
which exhibits two appealing properties in the con-
text of this task. First, the model concentrates proba-
bility mass on some ?canonical? ordering and small
perturbations of that ordering. This characteris-
tic matches our constraint that documents from the
same domain exhibit structural similarity. Second,
its parameter set scales linearly with the permuta-
tion length, making it sufficiently constrained and
tractable for inference. In general, this distribution
could potentially be applied to other NLP applica-
tions where ordering is important.
Permutation Representation Typically, permuta-
tions are represented directly as an ordered sequence
of elements. The GMM utilizes an alternative rep-
resentation defined as a vector (v1, . . . , vK?1) of in-
version counts with respect to the identity permuta-
tion (1, . . . ,K). Term vj counts the number of times
a value greater than j appears before j in the permu-
tation.4 For instance, given the standard-form per-
mutation (3 1 5 2 4), v2 = 2 because 3 and 5 appear
before 2; the entire inversion count vector would be
(1 2 0 1). Every vector of inversion counts uniquely
identifies a single permutation.
The Distribution The GMM assigns proba-
bility mass according to the distance of a
given permutation from the identity permutation
{1, . . . ,K}, based on K ? 1 real-valued parameters
(?1, . . . ?K?1).5 Using the inversion count represen-
tation of a permutation, the GMM?s probability mass
function is expressed as an independent product of
probabilities for each vj :
GMM(v | ?) = e
??j ?jvj
?(?)
=
n?1?
j=1
e??jvj
?j(?j) , (1)
where ?j(?j) is a normalization factor with value:
?j(?j) = 1? e
?(K?j+1)?j
1? e??j .
4The sum of a vector of inversion counts is simply that per-
mutation?s Kendall?s ? distance to the identity permutation.
5In our work we take the identity permutation to be the fixed
centroid, which is a parameter in the full GMM. As we explain
later, our model is not hampered by this apparent restriction.
373
Due to the exponential form of the distribution, re-
quiring that ?j > 0 constrains the GMM to assign
highest probability mass to each vj being zero, cor-
responding to the identity permutation. A higher
value for ?j assigns more probability mass to vj be-
ing close to zero, biasing j to have fewer inversions.
The GMM elegantly captures our earlier require-
ment for a probability distribution that concentrates
mass around a global ordering, and uses few param-
eters to do so. Because the topic numbers in our
task are completely symmetric and not linked to any
extrinsic observations, fixing the identity permuta-
tion to be that global ordering does not sacrifice any
representational power. Another major benefit of
the GMM is its membership in the exponential fam-
ily of distributions; this means that it is particularly
amenable to a Bayesian representation, as it admits
a natural conjugate prior:
GMM0(?j | vj,0, ?0) ? e(??jvj,0?log?j(?j))?0 . (2)
Intuitively, this prior states that over ?0 prior trials,
the total number of inversions was ?0vj,0. This dis-
tribution can be easily updated with the observed vj
to derive a posterior distribution.6
4.2 Formal Generative Process
We now fully specify the details of our model. We
observe a corpus of D documents, each an ordered
sequence of paragraphs, and a specification of a
number of topics K. Each paragraph is represented
as a bag of words. The model induces a set of hid-
den variables that probabilistically explain how the
words of the corpus were produced. Our final de-
sired output is the distributions over the paragraphs?
hidden topic assignment variables. In the following,
variables subscripted with 0 are fixed prior hyperpa-
rameters.
1. For each topic k, draw a language model ?k ?
Dirichlet(?0). As with LDA, these are topic-
specific word distributions.
2. Draw a topic distribution ? ? Dirichlet(?0),
which expresses how likely each topic is to ap-
pear regardless of position.
6Because each vj has a different range, it is inconvenient
to set the prior hyperparameters vj,0 directly. In our work, we
instead fix the mode of the prior distribution to a value ?0, which
works out to setting vj,0 = 1exp(?0)?1 ? K?j+1exp((K?j+1)?0)?1 .
3. Draw the topic ordering distribution parame-
ters ?j ? GMM0(?0, ?0) for j = 1 to K ? 1.
These parameters control how rapidly probabil-
ity mass decays for having more inversions for
each topic. A separate ?j for every topic allows
us to learn that some topics are more likely to
be reordered than others.
4. For each document d with Nd paragraphs:
(a) Draw a bag of topics td by sampling Nd
times from Multinomial(?).
(b) Draw a topic ordering pid by sampling a
vector of inversion counts vd ? GMM(?).
(c) Compute the vector of topic assignments
zd for document d?s paragraphs, by sorting
td according to pid.7
(d) For each paragraph p in document d:
i. Sample each word wd,p,j according to
the language model of p: wd,p,j ?
Multinomial(?zd,p).
5 Inference
The variables that we aim to infer are the topic as-
signments z of each paragraph, which are deter-
mined by the bag of topics t and ordering pi for each
document. Thus, our goal is to estimate the marginal
distributions of t and pi given the document text.
We accomplish this inference task through Gibbs
sampling (Bishop, 2006). A Gibbs sampler builds
a Markov chain over the hidden variable state space
whose stationary distribution is the actual posterior
of the joint distribution. Each new sample is drawn
from the distribution of a single variable conditioned
on previous samples of the other variables. We can
?collapse? the sampler by integrating over some of
the hidden variables in the model, in effect reducing
the state space of the Markov chain. Collapsed sam-
pling has been previously demonstrated to be effec-
tive for LDA and its variants (Griffiths and Steyvers,
2004; Porteous et al, 2008; Titov and McDonald,
2008). Our sampler integrates over all but three sets
7Multiple permutations can contribute to the probability of a
single document?s topic assignments zd, if there are topics that
do not appear in td. As a result, our current formulation is bi-
ased toward assignments with fewer topics per document. In
practice, we do not find this to negatively impact model perfor-
mance.
374
of hidden variables: bags of topics t, orderings pi,
and permutation inversion parameters ?. After a
burn-in period, we treat the last samples of t and
pi as a draw from the true posterior.
Document Probability As a preliminary step,
consider how to calculate the probability of a single
document?s words wd given the document?s para-
graph topic assignments zd, and other documents
and their topic assignments. Note that this proba-
bility is decomposable into a product of probabil-
ities over individual paragraphs, where paragraphs
with different topics have conditionally independent
word probabilities. Let w?d and z?d indicate the
words and topic assignments to documents other
than d, and W be the vocabulary size. The proba-
bility of the words in d is then:
P (wd | z,w?d, ?0)
=
K?
k=1
?
?k
P (wd | zd, ?k)P (?k | z,w?d, ?0)d?k
=
K?
k=1
DCM({wd,i : zd,i = k}
| {w?d,i : z?d,i = k}, ?0), (3)
where DCM(?) refers to the Dirichlet compound
multinomial distribution, the result of integrat-
ing over multinomial parameters with a Dirichlet
prior (Bernardo and Smith, 2000). For a Dirichlet
prior with parameters ? = (?1, . . . , ?W ), the DCM
assigns the following probability to a series of ob-
servations x = {x1, . . . , xn}:
DCM(x | ?) = ?(
?
j ?j)?
j ?(?j)
W?
i=1
?(N(x, i) + ?i)
?(|x|+?j ?j)
,
where N(x, i) refers to the number of times word
i appears in x. Here, ?(?) is the Gamma function,
a generalization of the factorial for real numbers.
Some algebra shows that the DCM?s posterior prob-
ability density function conditioned on a series of
observations y = {y1, . . . , yn} can be computed by
updating each ?i with counts of how often word i
appears in y:
DCM(x | y, ?)
= DCM(x | ?1 +N(y, 1), . . . , ?W +N(y,W )).
(4)
Equation 3 and 4 will be used again to compute the
conditional distributions of the hidden variables.
We now turn to a discussion of how each individ-
ual random variable is resampled.
Bag of Topics First we consider how to resample
td,i, the ith topic draw for document d conditioned
on all other parameters being fixed (note this is not
the topic of the ith paragraph, as we reorder topics
using pid):
P (td,i = t | . . .)
? P (td,i = t | t?(d,i), ?0)P (wd | td, pid,w?d, z?d, ?0)
?
N(t?(d,i), t) + ?0
|t?(d,i)|+K?0 P (wd | z,w?d, ?0),
where td is updated to reflect td,i = t, and zd is de-
terministically computed by mapping td and pid to
actual paragraph topic assignments. The first step
reflects an application of Bayes rule to factor out the
term for wd. In the second step, the first term arises
out of the DCM, by updating the parameters ?0 with
observations t?(d,i) as in equation 4 and dropping
constants. The document probability term is com-
puted using equation 3. The new td,i is selected
by sampling from this probability computed over all
possible topic assignments.
Ordering The parameterization of a permutation
pi as a series of inversion values vj reveals a natural
way to decompose the search space for Gibbs sam-
pling. For a single ordering, each vj can be sampled
independently, according to:
P (vj = v | . . .)
? P (vj = v | ?j)P (wd | td, pid,w?d, z?d, ?0)
= GMMj(v | ?j)P (wd | zd,w?d, z?d, ?0),
where pid is updated to reflect vj = v, and zd is com-
puted according to td and pid. The first term refers
to the jth multiplicand of equation 1; the second is
computed using equation 3. Term vj is sampled ac-
cording to the resulting probabilities.
GMM Parameters For each j = 1 to K ? 1, we
resample ?j from its posterior distribution:
P (?j | . . .)
= GMM0
(
?j
????
?
i vj,i + vj,0?0
N + ?0 , N + ?0
)
,
375
where GMM0 is evaluated according to equation 2.
The normalization constant of this distribution is un-
known, meaning that we cannot directly compute
and invert the cumulative distribution function to
sample from this distribution. However, the distri-
bution itself is univariate and unimodal, so we can
expect that an MCMC technique such as slice sam-
pling (Neal, 2003) should perform well. In practice,
the MATLAB black-box slice sampler provides a ro-
bust draw from this distribution.
6 Experimental Setup
Data Sets We evaluate our model on two data sets
drawn from the English Wikipedia. The first set
is 100 articles about large cities, with topics such
as History, Culture, and Demographics. The sec-
ond is 118 articles about chemical elements in the
periodic table, including topics such as Biological
Role, Occurrence, and Isotopes. Within each cor-
pus, articles often exhibit similar section orderings,
but many have idiosyncratic inversions. This struc-
tural variability arises out of the collaborative nature
of Wikipedia, which allows articles to evolve inde-
pendently. Corpus statistics are summarized below.
Corpus Docs Paragraphs Vocab Words
Cities 100 6,670 41,978 492,402
Elements 118 2,810 18,008 191,762
In each data set, the articles? noisy section head-
ings induce a reference structure to compare against.
This reference structure assumes that two para-
graphs are aligned if and only if their section head-
ings are identical, and that section boundaries pro-
vide the correct segmentation of each document.
These headings are only used for evaluation, and are
not provided to any of the systems.
Using the section headings to build the reference
structure can be problematic, as the same topic may
be referred to using different titles across different
documents, and sections may be divided at differing
levels of granularity. Thus, for the Cities data set, we
manually annotated each article?s paragraphs with a
consistent set of section headings, providing us an
additional reference structure to evaluate against. In
this clean section headings set, we found approxi-
mately 18 topics that were expressed in more than
one document.
Tasks and Metrics We study performance on the
tasks of alignment and segmentation. In the former
task, we measure whether paragraphs identified to
be the same topic by our model have the same sec-
tion headings, and vice versa. First, we identify the
?closest? topic to each section heading, by finding
the topic that is most commonly assigned to para-
graphs under that section heading. We compute the
proportion of paragraphs where the model?s topic as-
signment matches the section heading?s topic, giv-
ing us a recall score. High recall indicates that
paragraphs of the same section headings are always
being assigned to the same topic. Conversely, we
can find the closest section heading to each topic,
by finding the section heading that is most com-
mon for the paragraphs assigned to a single topic.
We then compute the proportion of paragraphs from
that topic whose section heading is the same as the
reference heading for that topic, yielding a preci-
sion score. High precision means that paragraphs
assigned to a single topic usually correspond to the
same section heading. The harmonic mean of recall
and precision is the summary F-score.
Statistical significance in this setup is measured
with approximate randomization (Noreen, 1989), a
nonparametric test that can be directly applied to
nonlinear metrics such as F-score. This test has been
used in prior evaluations for information extraction
and machine translation (Chinchor, 1995; Riezler
and Maxwell, 2005).
For the second task, we take the boundaries at
which topics change within a document to be a
segmentation of that document. We evaluate us-
ing the standard penalty metrics Pk and WindowD-
iff (Beeferman et al, 1999; Pevzner and Hearst,
2002). Both pass a sliding window over the doc-
uments and compute the probability of the words
at the ends of the windows being improperly seg-
mented with respect to each other. WindowDiff re-
quires that the number of segmentation boundaries
between the endpoints be correct as well.8
Our model takes a parameter K which controls
the upper bound on the number of latent topics. Note
that our algorithm can select fewer thanK topics for
each document, soK does not determine the number
8Statistical significance testing is not standardized and usu-
ally not reported for the segmentation task, so we omit these
tests in our results.
376
of segments in each document. We report results
using both K = 10 and 20 (recall that the cleanly
annotated Cities data set had 18 topics).
Baselines andModel Variants We consider base-
lines from the literature that perform either align-
ment or segmentation. For the first task, we
compare against the hidden topic Markov model
(HTMM) (Gruber et al, 2007), which represents
topic transitions between adjacent paragraphs in a
Markovian fashion, similar to the approach taken in
content modeling work. Note that HTMM can only
capture local constraints, so it would allow topics to
recur noncontiguously throughout a document.
We also compare against the structure-agnostic
approach of clustering the paragraphs using the
CLUTO toolkit,9 which uses repeated bisection to
maximize a cosine similarity-based objective.
For the segmentation task, we compare to
BayesSeg (Eisenstein and Barzilay, 2008),10
a Bayesian topic-based segmentation model
that outperforms previous segmentation ap-
proaches (Utiyama and Isahara, 2001; Galley et al,
2003; Purver et al, 2006; Malioutov and Barzilay,
2006). BayesSeg enforces the topic contiguity
constraint that motivated our model. We provide
this baseline with the benefit of knowing the correct
number of segments for each document, which is
not provided to our system. Note that BayesSeg
processes each document individually, so it cannot
capture structural relatedness across documents.
To investigate the importance of our ordering
model, we consider two variants of our model that
alternately relax and tighten ordering constraints. In
the constrained model, we require all documents to
follow the same canonical ordering of topics. This
is equivalent to forcing the topic permutation distri-
bution to give all its probability to one ordering, and
can be implemented by fixing all inversion counts v
to zero during inference. At the other extreme, we
consider the uniform model, which assumes a uni-
form distribution over all topic permutations instead
of biasing toward a small related set. In our im-
plementation, this can be simulated by forcing the
9http://glaros.dtc.umn.edu/gkhome/views/cluto/
10We do not evaluate on the corpora used in their work, since
our model relies on content similarity across documents in the
corpus.
GMM parameters ? to always be zero. Both variants
still enforce topic contiguity, and allow segments
across documents to be aligned by topic assignment.
Evaluation Procedures For each evaluation of
our model and its variants, we run the Gibbs sampler
from five random seed states, and take the 10,000th
iteration of each chain as a sample. Results shown
are the average over these five samples. All Dirich-
let prior hyperparameters are set to 0.1, encouraging
sparse distributions. For the GMM, we set the prior
decay parameter ?0 to 1, and the sample size prior
?0 to be 0.1 times the number of documents.
For the baselines, we use implementations pub-
licly released by their authors. We set HTMM?s pri-
ors according to values recommended in the authors?
original work. For BayesSeg, we use its built-in hy-
perparameter re-estimation mechanism.
7 Results
Alignment Table 1 presents the results of the
alignment evaluation. In every case, the best per-
formance is achieved using our full model, by a sta-
tistically significant and usually substantial margin.
In both domains, the baseline clustering method
performs competitively, indicating that word cues
alone are a good indicator of topic. While the sim-
pler variations of our model achieve reasonable per-
formance, adding the richer GMM distribution con-
sistently yields superior results.
Across each of our evaluations, HTMM greatly
underperforms the other approaches. Manual ex-
amination of the actual topic assignments reveals
that HTMM often selects the same topic for discon-
nected paragraphs of the same document, violating
the topic contiguity constraint, and demonstrating
the importance of modeling global constraints for
document structure tasks.
We also compare performance measured on the
manually annotated section headings against the ac-
tual noisy headings. The ranking of methods by per-
formance remains mostly unchanged between these
two evaluations, indicating that the noisy headings
are sufficient for gaining insight into the compara-
tive performance of the different approaches.
Segmentation Table 2 presents the segmentation
experiment results. On both data sets, our model
377
Cities: clean headings Cities: noisy headings Elements: noisy headings
Recall Prec F-score Recall Prec F-score Recall Prec F-score
K
=
10
Clustering 0.578 0.439 ? 0.499 0.611 0.331 ? 0.429 0.524 0.361 ? 0.428
HTMM 0.446 0.232 ? 0.305 0.480 0.183 ? 0.265 0.430 0.190 ? 0.264
Constrained 0.579 0.471 ? 0.520 0.667 0.382 ? 0.485 0.603 0.408 ? 0.487
Uniform 0.520 0.440 ? 0.477 0.599 0.343 ? 0.436 0.591 0.403 ? 0.479
Our model 0.639 0.509 0.566 0.705 0.399 0.510 0.685 0.460 0.551
K
=
20
Clustering 0.486 0.541 ? 0.512 0.527 0.414 ? 0.464 0.477 0.402 ? 0.436
HTMM 0.260 0.217 ? 0.237 0.304 0.187 ? 0.232 0.248 0.243 ? 0.246
Constrained 0.458 0.519 ? 0.486 0.553 0.415 ? 0.474 0.510 0.421 ? 0.461
Uniform 0.499 0.551 ? 0.524 0.571 0.423 ? 0.486 0.550 0.479  0.512
Our model 0.578 0.636 0.606 0.648 0.489 0.557 0.569 0.498 0.531
Table 1: Comparison of the alignments produced by our model and a series of baselines and model variations, for both
10 and 20 topics, evaluated against clean and noisy sets of section headings. Higher scores are better. Within the same
K, the methods which our model significantly outperforms are indicated with ? for p < 0.001 and  for p < 0.01.
Cities: clean headings Cities: noisy headings Elements: noisy headings
Pk WD # Segs Pk WD # Segs Pk WD # Segs
BayesSeg 0.321 0.376 ? 12.3 0.317 0.376 ? 13.2 0.279 0.316 ? 7.7
K
=
10 Constrained 0.260 0.281 7.7 0.267 0.288 7.7 0.227 0.244 5.4Uniform 0.268 0.300 8.8 0.273 0.304 8.8 0.226 0.250 6.6
Our model 0.253 0.283 9.0 0.257 0.286 9.0 0.201 0.226 6.7
K
=
20 Constrained 0.274 0.314 10.9 0.274 0.313 10.9 0.231 0.257 6.6Uniform 0.234 0.294 14.0 0.234 0.290 14.0 0.209 0.248 8.7
Our model 0.221 0.278 14.2 0.222 0.278 14.2 0.203 0.243 8.6
Table 2: Comparison of the segmentations produced by our model and a series of baselines and model variations, for
both 10 and 20 topics, evaluated against clean and noisy sets of section headings. Lower scores are better. ?BayesSeg
is given the true number of segments, so its segments count reflects the reference structure?s segmentation.
outperforms the BayesSeg baseline by a substantial
margin regardless of K. This result provides strong
evidence that learning connected topic models over
related documents leads to improved segmentation
performance. In effect, our model can take advan-
tage of shared structure across related documents.
In all but one case, the best performance is ob-
tained by the full version of our model. This result
indicates that enforcing discourse-motivated struc-
tural constraints allows for better segmentation in-
duction. Encoding global discourse-level constraints
leads to better language models, resulting in more
accurate predictions of segment boundaries.
8 Conclusions
In this paper, we have shown how an unsupervised
topic-based approach can capture document struc-
ture. Our resulting model constrains topic assign-
ments in a way that requires global modeling of en-
tire topic sequences. We showed that the generalized
Mallows model is a theoretically and empirically ap-
pealing way of capturing the ordering component
of this topic sequence. Our results demonstrate the
importance of augmenting statistical models of text
analysis with structural constraints motivated by dis-
course theory.
Acknowledgments
The authors acknowledge the funding support of
NSF CAREER grant IIS-0448168, the NSF Grad-
uate Fellowship, the Office of Naval Research,
Quanta, Nokia, and the Microsoft Faculty Fellow-
ship. We thank the members of the NLP group at
MIT and numerous others who offered suggestions
and comments on this work. We are especially grate-
ful to Marina Meila? for introducing us to the Mal-
lows model. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
378
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL/HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Ar-
tificial Intelligence Research, 17:35?55.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34:177?210.
Jose? M. Bernardo and Adrian F.M. Smith. 2000.
Bayesian Theory. Wiley Series in Probability and
Statistics.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Nancy Chinchor. 1995. Statistical significance of MUC-
6 results. In Proceedings of the 6th Conference on
Message Understanding.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL/HLT.
M.A. Fligner and J.S. Verducci. 1986. Distance based
ranking models. Journal of the Royal Statistical Soci-
ety, Series B, 48(3):359?369.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse segmen-
tation of multi-party conversation. In Proceedings of
ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in NIPS.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Proceedings of AIS-
TATS.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence for text structuring using
a reliably annotated corpus. In Proceedings of ACL.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL.
Guy Lebanon and John Lafferty. 2002. Cranking: com-
bining rankings using conditional probability models
on permutations. In Proceedings of ICML.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL.
Marina Meila?, Kapil Phadnis, Arthur Patterson, and Jeff
Bilmes. 2007. Consensus ranking under the exponen-
tial model. In Proceedings of UAI.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Eric W. Noreen. 1989. Computer Intensive Methods for
Testing Hypotheses. An Introduction. Wiley.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent dirichlet alo-
cation. In Proceedings of SIGKDD.
Matthew Purver, Konrad Ko?rding, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of ACL/COLING.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL.
Hanna M. Wallach. 2006. Topic modeling: beyond bag
of words. In Proceedings of ICML.
Alison Wray. 2002. Formulaic Language and the Lexi-
con. Cambridge University Press, Cambridge.
379
Proceedings of ACL-08: HLT, pages 263?271,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Document-Level Semantic Properties from Free-text Annotations
S.R.K. Branavan Harr Chen Jacob Eisenstein Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, harr, jacobe, regina}@csail.mit.edu
Abstract
This paper demonstrates a new method for
leveraging free-text annotations to infer se-
mantic properties of documents. Free-text an-
notations are becoming increasingly abundant,
due to the recent dramatic growth in semi-
structured, user-generated online content. An
example of such content is product reviews,
which are often annotated by their authors
with pros/cons keyphrases such as ?a real bar-
gain? or ?good value.? To exploit such noisy
annotations, we simultaneously find a hid-
den paraphrase structure of the keyphrases, a
model of the document texts, and the underly-
ing semantic properties that link the two. This
allows us to predict properties of unannotated
documents. Our approach is implemented as
a hierarchical Bayesian model with joint in-
ference, which increases the robustness of the
keyphrase clustering and encourages the doc-
ument model to correlate with semantically
meaningful properties. We perform several
evaluations of our model, and find that it sub-
stantially outperforms alternative approaches.
1 Introduction
A central problem in language understanding is
transforming raw text into structured representa-
tions. Learning-based approaches have dramatically
increased the scope and robustness of this type of
automatic language processing, but they are typi-
cally dependent on large expert-annotated datasets,
which are costly to produce. In this paper, we show
how novice-generated free-text annotations avail-
able online can be leveraged to automatically infer
document-level semantic properties.
With the rapid increase of online content cre-
ated by end users, noisy free-text annotations have
pros/cons: great nutritional value
... combines it all: an amazing product, quick and
friendly service, cleanliness, great nutrition ...
pros/cons: a bit pricey, healthy
... is an awesome place to go if you are health con-
scious. They have some really great low calorie dishes
and they publish the calories and fat grams per serving.
Figure 1: Excerpts from online restaurant reviews with
pros/cons phrase lists. Both reviews discuss healthiness,
but use different keyphrases.
become widely available (Vickery and Wunsch-
Vincent, 2007; Sterling, 2005). For example, con-
sider reviews of consumer products and services.
Often, such reviews are annotated with keyphrase
lists of pros and cons. We would like to use these
keyphrase lists as training labels, so that the proper-
ties of unannotated reviews can be predicted. Hav-
ing such a system would facilitate structured access
and summarization of this data. However, novice-
generated keyphrase annotations are incomplete de-
scriptions of their corresponding review texts. Fur-
thermore, they lack consistency: the same under-
lying property may be expressed in many ways,
e.g., ?healthy? and ?great nutritional value? (see Fig-
ure 1). To take advantage of such noisy labels, a sys-
tem must both uncover their hidden clustering into
properties, and learn to predict these properties from
review text.
This paper presents a model that addresses both
problems simultaneously. We assume that both the
document text and the selection of keyphrases are
governed by the underlying hidden properties of the
document. Each property indexes a language model,
thus allowing documents that incorporate the same
263
property to share similar features. In addition, each
keyphrase is associated with a property; keyphrases
that are associated with the same property should
have similar distributional and surface features.
We link these two ideas in a joint hierarchical
Bayesian model. Keyphrases are clustered based
on their distributional and lexical properties, and a
hidden topic model is applied to the document text.
Crucially, the keyphrase clusters and document top-
ics are linked, and inference is performed jointly.
This increases the robustness of the keyphrase clus-
tering, and ensures that the inferred hidden topics
are indicative of salient semantic properties.
Our model is broadly applicable to many scenar-
ios where documents are annotated in a noisy man-
ner. In this work, we apply our method to a col-
lection of reviews in two categories: restaurants and
cell phones. The training data consists of review text
and the associated pros/cons lists. We then evaluate
the ability of our model to predict review properties
when the pros/cons list is hidden. Across a variety
of evaluation scenarios, our algorithm consistently
outperforms alternative strategies by a wide margin.
2 Related Work
Review Analysis Our approach relates to previous
work on property extraction from reviews (Popescu
et al, 2005; Hu and Liu, 2004; Kim and Hovy,
2006). These methods extract lists of phrases, which
are analogous to the keyphrases we use as input
to our algorithm. However, our approach is dis-
tinguished in two ways: first, we are able to pre-
dict keyphrases beyond those that appear verbatim
in the text. Second, our approach learns the rela-
tionships between keyphrases, allowing us to draw
direct comparisons between reviews.
Bayesian Topic Modeling One aspect of our
model views properties as distributions over words
in the document. This approach is inspired by meth-
ods in the topic modeling literature, such as Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), where
topics are treated as hidden variables that govern the
distribution of words in a text. Our algorithm ex-
tends this notion by biasing the induced hidden top-
ics toward a clustering of known keyphrases. Tying
these two information sources together enhances the
robustness of the hidden topics, thereby increasing
the chance that the induced structure corresponds to
semantically meaningful properties.
Recent work has examined coupling topic mod-
els with explicit supervision (Blei and McAuliffe,
2007; Titov and McDonald, 2008). However, such
approaches assume that the documents are labeled
within a predefined annotation structure, e.g., the
properties of food, ambiance, and service for restau-
rants. In contrast, we address free-text annotations
created by end users, without known semantic prop-
erties. Rather than requiring a predefined annotation
structure, our model infers one from the data.
3 Problem Formulation
We formulate our problem as follows. We assume
a dataset composed of documents with associated
keyphrases. Each document may be marked with
multiple keyphrases that express unseen semantic
properties. Across the entire collection, several
keyphrases may express the same property. The
keyphrases are also incomplete ? review texts of-
ten express properties that are not mentioned in their
keyphrases. At training time, our model has access
to both text and keyphrases; at test time, the goal is
to predict the properties supported by a previously
unseen document. We can then use this property list
to generate an appropriate set of keyphrases.
4 Model Description
Our approach leverages both keyphrase clustering
and distributional analysis of the text in a joint, hi-
erarchical Bayesian model. Keyphrases are drawn
from a set of clusters; words in the documents are
drawn from language models indexed by a set of
topics, where the topics correspond to the keyphrase
clusters. Crucially, we bias the assignment of hid-
den topics in the text to be similar to the topics rep-
resented by the keyphrases of the document, but we
permit some words to be drawn from other topics
not represented by the keyphrases. This flexibility in
the coupling allows the model to learn effectively in
the presence of incomplete keyphrase annotations,
while still encouraging the keyphrase clustering to
cohere with the topics supported by the text.
We train the model on documents annotated with
keyphrases. During training, we learn a hidden
topic model from the text; each topic is also asso-
264
? ? keyphrase cluster model
x ? keyphrase cluster assignment
s ? keyphrase similarity values
h ? document keyphrases
? ? document keyphrase topics
? ? probability of selecting ? instead of ?
c ? selects between ? and ? for word topics
? ? document topic model
z ? word topic assignment
? ? language models of each topic
w ? document words
? ? Dirichlet(?0)
x? ? Multinomial(?)
s?,?? ?
{
Beta(?=) if x? = x??
Beta(?6=) otherwise
?d = [?d,1 . . . ?d,K ]T
where
?d,k ?
{
1 if x? = k for any l ? hd
0 otherwise
? ? Beta(?0)
cd,n ? Bernoulli(?)
?d ? Dirichlet(?0)
zd,n ?
{
Multinomial(?d) if cd,n = 1
Multinomial(?d) otherwise
?k ? Dirichlet(?0)
wd,n ? Multinomial(?zd,n)
Figure 2: The plate diagram for our model. Shaded circles denote observed variables, and squares denote hyper
parameters. The dotted arrows indicate that ? is constructed deterministically from x and h.
ciated with a cluster of keyphrases. At test time,
we are presented with documents that do not con-
tain keyphrase annotations. The hidden topic model
of the review text is used to determine the proper-
ties that a document as a whole supports. For each
property, we compute the proportion of the docu-
ment?s words assigned to it. Properties with propor-
tions above a set threshold (tuned on a development
set) are predicted as being supported.
4.1 Keyphrase Clustering
One of our goals is to cluster the keyphrases, such
that each cluster corresponds to a well-defined prop-
erty. We represent each distinct keyphrase as a vec-
tor of similarity scores computed over the set of
observed keyphrases; these scores are represented
by s in Figure 2, the plate diagram of our model.1
Modeling the similarity matrix rather than the sur-
1We assume that similarity scores are conditionally inde-
pendent given the keyphrase clustering, though the scores are
in fact related. Such simplifying assumptions have been previ-
ously used with success in NLP (e.g., Toutanova and Johnson,
2007), though a more theoretically sound treatment of the sim-
ilarity matrix is an area for future research.
face forms allows arbitrary comparisons between
keyphrases, e.g., permitting the use of both lexical
and distributional information. The lexical com-
parison is based on the cosine similarity between
the keyphrase words. The distributional similar-
ity is quantified in terms of the co-occurrence of
keyphrases across review texts. Our model is inher-
ently capable of using any arbitrary source of simi-
larity information; for a discussion of similarity met-
rics, see Lin (1998).
4.2 Document-level Distributional Analysis
Our analysis of the document text is based on proba-
bilistic topic models such as LDA (Blei et al, 2003).
In the LDA framework, each word is generated from
a language model that is indexed by the word?s topic
assignment. Thus, rather than identifying a single
topic for a document, LDA identifies a distribution
over topics.
Our word model operates similarly, identifying a
topic for each word, written as z in Figure 2. To
tie these topics to the keyphrases, we deterministi-
cally construct a document-specific topic distribu-
265
tion from the clusters represented by the document?s
keyphrases ? this is ? in the figure. ? assigns equal
probability to all topics that are represented in the
keyphrases, and a small smoothing probability to
other topics.
As noted above, properties may be expressed in
the text even when no related keyphrase appears. For
this reason, we also construct a document-specific
topic distribution ?. The auxiliary variable c indi-
cates whether a given word?s topic is drawn from
the set of keyphrase clusters, or from this topic dis-
tribution.
4.3 Generative Process
In this section, we describe the underlying genera-
tive process more formally.
First we consider the set of all keyphrases ob-
served across the entire corpus, of which there are
L. We draw a multinomial distribution ? over the K
keyphrase clusters from a symmetric Dirichlet prior
?0. Then for the ?th keyphrase, a cluster assign-
ment x? is drawn from the multinomial ?. Finally,
the similarity matrix s ? [0, 1]L?L is constructed.
Each entry s?,?? is drawn independently, depending
on the cluster assignments x? and x?? . Specifically,
s?,?? is drawn from a Beta distribution with parame-
ters ?= if x? = x?? and ?6= otherwise. The parame-
ters ?= linearly bias s?,?? towards one (Beta(?=) ?
Beta(2, 1)), and the parameters ?6= linearly bias s?,??
towards zero (Beta(?6=) ? Beta(1, 2)).
Next, the words in each of the D documents
are generated. Document d has Nd words; zd,n is
the topic for word wd,n. These latent topics are
drawn either from the set of clusters represented by
the document?s keyphrases, or from the document?s
topic model ?d. We deterministically construct a
document-specific keyphrase topic model ?d, based
on the keyphrase cluster assignments x and the ob-
served keyphrases hd. The multinomial ?d assigns
equal probability to each topic that is represented by
a phrase in hd, and a small probability to other top-
ics.
As noted earlier, a document?s text may support
properties that are not mentioned in its observed
keyphrases. For that reason, we draw a document
topic multinomial ?d from a symmetric Dirichlet
prior ?0. The binary auxiliary variable cd,n deter-
mines whether the word?s topic is drawn from the
keyphrase model ?d or the document topic model
?d. cd,n is drawn from a weighted coin flip, with
probability ?; ? is drawn from a Beta distribution
with prior ?0. We have zd,n ? ?d if cd,n = 1,
and zd,n ? ?d otherwise. Finally, the word wd,n
is drawn from the multinomial ?zd,n , where zd,n in-
dexes a topic-specific language model. Each of the
K language models ?k is drawn from a symmetric
Dirichlet prior ?0.
5 Posterior Sampling
Ultimately, we need to compute the model?s poste-
rior distribution given the training data. Doing so
analytically is intractable due to the complexity of
the model, but sampling-based techniques can be
used to estimate the posterior. We employ Gibbs
sampling, previously used in NLP by Finkel et al
(2005) and Goldwater et al (2006), among others.
This technique repeatedly samples from the condi-
tional distributions of each hidden variable, eventu-
ally converging on a Markov chain whose stationary
distribution is the posterior distribution of the hid-
den variables in the model (Gelman et al, 2004).
We now present sampling equations for each of the
hidden variables in Figure 2.
The prior over keyphrase clusters ? is sampled
based on hyperprior ?0 and keyphrase cluster as-
signments x. We write p(? | . . .) to mean the prob-
ability conditioned on all the other variables.
p(? | . . .) ? p(? | ?0)p(x | ?),
= p(? | ?0)
L
?
?
p(x? | ?)
= Dir(?;?0)
L
?
?
Mul(x?;?)
= Dir(?;??),
where ??i = ?0 + count(x? = i). This update rule
is due to the conjugacy of the multinomial to the
Dirichlet distribution. The first line follows from
Bayes? rule, and the second line from the conditional
independence of each keyphrase assignment x? from
the others, given ?.
?d and ?k are resampled in a similar manner:
p(?d | . . .) ? Dir(?d;??d),
p(?k | . . .) ? Dir(?k; ??k),
266
p(x? | . . .) ? p(x? | ?)p(s | x?,x??, ?)p(z | ?, ?, c)
? p(x? | ?)
?
?
?
?? 6=?
p(s?,?? | x?, x?? , ?)
?
?
?
?
D
?
d
?
cd,n=1
p(zd,n | ?d)
?
?
= Mul(x?;?)
?
?
?
?? 6=?
Beta(s?,?? ;?x?,x?? )
?
?
?
?
D
?
d
?
cd,n=1
Mul(zd,n; ?d)
?
?
Figure 3: The resampling equation for the keyphrase cluster assignments.
where ??d,i = ?0 + count(zd,n = i ? cd,n = 0)
and ??k,i = ?0 +
?
d count(wd,n = i ? zd,n = k). In
building the counts for ??d,i, we consider only cases
in which cd,n = 0, indicating that the topic zd,n is
indeed drawn from the document topic model ?d.
Similarly, when building the counts for ??k, we con-
sider only cases in which the word wd,n is drawn
from topic k.
To resample ?, we employ the conjugacy of the
Beta prior to the Bernoulli observation likelihoods,
adding counts of c to the prior ?0.
p(? | . . .) ? Beta(?;??),
where ?? = ?0 +
[ ?
d count(cd,n = 1)
?
d count(cd,n = 0)
]
.
The keyphrase cluster assignments are repre-
sented by x, whose sampling distribution depends
on ?, s, and z, via ?. The equation is shown in Fig-
ure 3. The first term is the prior on x?. The second
term encodes the dependence of the similarity ma-
trix s on the cluster assignments; with slight abuse of
notation, we write ?x?,x?? to denote ?= if x? = x?? ,
and ?6= otherwise. The third term is the dependence
of the word topics zd,n on the topic distribution ?d.
We compute the final result of Figure 3 for each pos-
sible setting of x?, and then sample from the normal-
ized multinomial.
The word topics z are sampled according to
keyphrase topic distribution ?d, document topic dis-
tribution ?d, words w, and auxiliary variables c:
p(zd,n | . . .)
? p(zd,n | ?d, ?d, cd,n)p(wd,n | zd,n, ?)
=
{
Mul(zd,n; ?d)Mul(wd,n; ?zd,n) if cd,n = 1,
Mul(zd,n;?d)Mul(wd,n; ?zd,n) otherwise.
As with x?, each zd,n is sampled by computing
the conditional likelihood of each possible setting
within a constant of proportionality, and then sam-
pling from the normalized multinomial.
Finally, we sample each auxiliary variable cd,n,
which indicates whether the hidden topic zd,n is
drawn from ?d or ?d. The conditional probability
for cd,n depends on its prior ? and the hidden topic
assignments zd,n:
p(cd,n | . . .)
? p(cd,n | ?)p(zd,n | ?d, ?d, cd,n)
=
{
Bern(cd,n;?)Mul(zd,n; ?d) if cd,n = 1,
Bern(cd,n;?)Mul(zd,n;?d) otherwise.
We compute the likelihood of cd,n = 0 and cd,n = 1
within a constant of proportionality, and then sample
from the normalized Bernoulli distribution.
6 Experimental Setup
Data Sets We evaluate our system on reviews from
two categories, restaurants and cell phones. These
reviews were downloaded from the popular Epin-
ions2 website. Users of this website evaluate prod-
ucts by providing both a textual description of their
opinion, as well as concise lists of keyphrases (pros
and cons) summarizing the review. The statistics of
this dataset are provided in Table 1. For each of
the categories, we randomly selected 50%, 15%, and
35% of the documents as training, development, and
test sets, respectively.
Manual analysis of this data reveals that authors
often omit properties mentioned in the text from
the list of keyphrases. To obtain a complete gold
2http://www.epinions.com/
267
Restaurants Cell Phones
# of reviews 3883 1112
Avg. review length 916.9 1056.9
Avg. keyphrases / review 3.42 4.91
Table 1: Statistics of the reviews dataset by category.
standard, we hand-annotated a subset of the reviews
from the restaurant category. The annotation effort
focused on eight commonly mentioned properties,
such as those underlying the keyphrases ?pleasant
atmosphere? and ?attentive staff.? Two raters anno-
tated 160 reviews, 30 of which were annotated by
both. Cohen?s kappa, a measure of interrater agree-
ment ranging from zero to one, was 0.78 for this sub-
set, indicating high agreement (Cohen, 1960).
Each review was annotated with 2.56 properties
on average. Each manually-annotated property cor-
responded to an average of 19.1 keyphrases in the
restaurant data, and 6.7 keyphrases in the cell phone
data. This supports our intuition that a single se-
mantic property may be expressed using a variety of
different keyphrases.
Training Our model needs to be provided with the
number of clusters K . We setK large enough for the
model to learn effectively on the development set.
For the restaurant data ? where the gold standard
identified eight semantic properties ? we set K to
20, allowing the model to account for keyphrases not
included in the eight most common properties. For
the cell phones category, we set K to 30.
To improve the model?s convergence rate, we per-
form two initialization steps for the Gibbs sampler.
First, sampling is done only on the keyphrase clus-
tering component of the model, ignoring document
text. Second, we fix this clustering and sample the
remaining model parameters. These two steps are
run for 5,000 iterations each. The full joint model
is then sampled for 100,000 iterations. Inspection
of the parameter estimates confirms model conver-
gence. On a 2GHz dual-core desktop machine, a
multi-threaded C++ implementation of model train-
ing takes about two hours for each dataset.
Inference The final point estimate used for test-
ing is an average (for continuous variables) or a
mode (for discrete variables) over the last 1,000
Gibbs sampling iterations. Averaging is a heuris-
tic that is applicable in our case because our sam-
ple histograms are unimodal and exhibit low skew.
The model usually works equally well using single-
sample estimates, but is more prone to estimation
noise.
As previously mentioned, we convert word topic
assignments to document properties by examining
the proportion of words supporting each property. A
threshold for this proportion is set for each property
via the development set.
Evaluation Our first evaluation examines the ac-
curacy of our model and the baselines by compar-
ing their output against the keyphrases provided by
the review authors. More specifically, the model
first predicts the properties supported by a given re-
view. We then test whether the original authors?
keyphrases are contained in the clusters associated
with these properties.
As noted above, the authors? keyphrases are of-
ten incomplete. To perform a noise-free compari-
son, we based our second evaluation on the man-
ually constructed gold standard for the restaurant
category. We took the most commonly observed
keyphrase from each of the eight annotated proper-
ties, and tested whether they are supported by the
model based on the document text.
In both types of evaluation, we measure the
model?s performance using precision, recall, and F-
score. These are computed in the standard manner,
based on the model?s keyphrase predictions com-
pared against the corresponding references. The
sign test was used for statistical significance test-
ing (De Groot and Schervish, 2001).
Baselines To the best of our knowledge, this task
not been previously addressed in the literature. We
therefore consider five baselines that allow us to ex-
plore the properties of this task and our model.
Random: Each keyphrase is supported by a doc-
ument with probability of one half. This baseline?s
results are computed (in expectation) rather than ac-
tually run. This method is expected to have a recall
of 0.5, because in expectation it will select half of
the correct keyphrases. Its precision is the propor-
tion of supported keyphrases in the test set.
Phrase in text: A keyphrase is supported by a doc-
ument if it appears verbatim in the text. Because of
this narrow requirement, precision should be high
whereas recall will be low.
268
Restaurants Restaurants Cell Phones
gold standard annotation free-text annotation free-text annotation
Recall Prec. F-score Recall Prec. F-score Recall Prec. F-score
Random 0.500 0.300 ? 0.375 0.500 0.500 ? 0.500 0.500 0.489 ? 0.494
Phrase in text 0.048 0.500 ? 0.087 0.078 0.909 ? 0.144 0.171 0.529 ? 0.259
Cluster in text 0.223 0.534 0.314 0.517 0.640 ? 0.572 0.829 0.547 0.659
Phrase classifier 0.028 0.636 ? 0.053 0.068 0.963 ? 0.126 0.029 0.600 ? 0.055
Cluster classifier 0.113 0.622 ? 0.192 0.255 0.907 ? 0.398 0.210 0.759 0.328
Our model 0.625 0.416 0.500 0.901 0.652 0.757 0.886 0.585 0.705
Our model + gold clusters 0.582 0.398 0.472 0.795 0.627 ? 0.701 0.886 0.520 ? 0.655
Table 2: Comparison of the property predictions made by our model and the baselines in the two categories as evaluated
against the gold and free-text annotations. Results for our model using the fixed, manually-created gold clusterings are
also shown. The methods against which our model has significantly better results on the sign test are indicated with a
? for p <= 0.05, and ? for p <= 0.1.
Cluster in text: A keyphrase is supported by a
document if it or any of its paraphrases appears in
the text. Paraphrasing is based on our model?s clus-
tering of the keyphrases. The use of paraphrasing
information enhances recall at the potential cost of
precision, depending on the quality of the clustering.
Phrase classifier: Discriminative classifiers are
trained for each keyphrase. Positive examples are
documents that are labeled with the keyphrase;
all other documents are negative examples. A
keyphrase is supported by a document if that
keyphrase?s classifier returns positive.
Cluster classifier: Discriminative classifiers are
trained for each cluster of keyphrases, using our
model?s clustering. Positive examples are docu-
ments that are labeled with any keyphrase from the
cluster; all other documents are negative examples.
All keyphrases of a cluster are supported by a docu-
ment if that cluster?s classifier returns positive.
Phrase classifier and cluster classifier employ
maximum entropy classifiers, trained on the same
features as our model, i.e., word counts. The former
is high-precision/low-recall, because for any partic-
ular keyphrase, its synonymous keyphrases would
be considered negative examples. The latter broad-
ens the positive examples, which should improve re-
call. We used Zhang Le?s MaxEnt toolkit3 to build
these classifiers.
3http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
7 Results
Comparative performance Table 2 presents the
results of the evaluation scenarios described above.
Our model outperforms every baseline by a wide
margin in all evaluations.
The absolute performance of the automatic meth-
ods indicates the difficulty of the task. For instance,
evaluation against gold standard annotations shows
that the random baseline outperforms all of the other
baselines. We observe similar disappointing results
for the non-random baselines against the free-text
annotations. The precision and recall characteristics
of the baselines match our previously described ex-
pectations.
The poor performance of the discriminative mod-
els seems surprising at first. However, these re-
sults can be explained by the degree of noise in
the training data, specifically, the aforementioned
sparsity of free-text annotations. As previously de-
scribed, our technique allows document text topics
to stochastically derive from either the keyphrases or
a background distribution ? this allows our model
to learn effectively from incomplete annotations. In
fact, when we force all text topics to derive from
keyphrase clusters in our model, its performance de-
grades to the level of the classifiers or worse, with
an F-score of 0.390 in the restaurant category and
0.171 in the cell phone category.
Impact of paraphrasing As previously ob-
served in entailment research (Dagan et al, 2006),
paraphrasing information contributes greatly to im-
proved performance on semantic inference. This is
269
Figure 4: Sample keyphrase clusters that our model infers
in the cell phone category.
confirmed by the dramatic difference in results be-
tween the cluster in text and phrase in text baselines.
Therefore it is important to quantify the quality of
automatically computed paraphrases, such as those
illustrated in Figure 4.
Restaurants Cell Phones
Keyphrase similarity only 0.931 0.759
Joint training 0.966 0.876
Table 3: Rand Index scores of our model?s clusters, using
only keyphrase similarity vs. using keyphrases and text
jointly. Comparison of cluster quality is against the gold
standard.
One way to assess clustering quality is to com-
pare it against a ?gold standard? clustering, as con-
structed in Section 6. For this purpose, we use the
Rand Index (Rand, 1971), a measure of cluster sim-
ilarity. This measure varies from zero to one; higher
scores are better. Table 3 shows the Rand Indices
for our model?s clustering, as well as the clustering
obtained by using only keyphrase similarity. These
scores confirm that joint inference produces better
clusters than using only keyphrases.
Another way of assessing cluster quality is to con-
sider the impact of using the gold standard clustering
instead of our model?s clustering. As shown in the
last two lines of Table 2, using the gold clustering
yields results worse than using the model clustering.
This indicates that for the purposes of our task, the
model clustering is of sufficient quality.
8 Conclusions and Future Work
In this paper, we have shown how free-text anno-
tations provided by novice users can be leveraged
as a training set for document-level semantic infer-
ence. The resulting hierarchical Bayesian model
overcomes the lack of consistency in such anno-
tations by inducing a hidden structure of seman-
tic properties, which correspond both to clusters of
keyphrases and hidden topic models in the text. Our
system successfully extracts semantic properties of
unannotated restaurant and cell phone reviews, em-
pirically validating our approach.
Our present model makes strong assumptions
about the independence of similarity scores. We be-
lieve this could be avoided by modeling the genera-
tion of the entire similarity matrix jointly. We have
also assumed that the properties themselves are un-
structured, but they are in fact related in interest-
ing ways. For example, it would be desirable to
model antonyms explicitly, e.g., no restaurant review
should be simultaneously labeled as having good
and bad food. The correlated topic model (Blei and
Lafferty, 2006) is one way to account for relation-
ships between hidden topics; more structured repre-
sentations, such as hierarchies, may also be consid-
ered.
Finally, the core idea of using free-text as a
source of training labels has wide applicability, and
has the potential to enable sophisticated content
search and analysis. For example, online blog en-
tries are often tagged with short keyphrases. Our
technique could be used to standardize these tags,
and assign keyphrases to untagged blogs. The no-
tion of free-text annotations is also very broad ?
we are currently exploring the applicability of this
model to Wikipedia articles, using section titles as
keyphrases, to build standard article schemas.
Acknowledgments
The authors acknowledge the support of the NSF,
Quanta Computer, the U.S. Office of Naval Re-
search, and DARPA. Thanks to Michael Collins,
Dina Katabi, Kristian Kersting, Terry Koo, Brian
Milch, Tahira Naseem, Dan Roy, Benjamin Snyder,
Luke Zettlemoyer, and the anonymous reviewers for
helpful comments and suggestions. Any opinions,
findings, and conclusions or recommendations ex-
pressed above are those of the authors and do not
necessarily reflect the views of the NSF.
270
References
David M. Blei and John D. Lafferty. 2006. Correlated
topic models. In Advances in NIPS, pages 147?154.
David M. Blei and Jon McAuliffe. 2007. Supervised
topic models. In Advances in NIPS.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. Lecture Notes in Computer Science,
3944:177?190.
Morris H. De Groot and Mark J. Schervish. 2001. Prob-
ability and Statistics. Addison Wesley.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by Gibbs sampling. In
Proceedings of the ACL, pages 363?370.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, 2nd edi-
tion.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of ACL, pages
673?680.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of SIGKDD,
pages 168?177.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING/ACL, pages 483?490.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML, pages 296?304.
Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.
2005. OPINE: Extracting product features and opin-
ions from reviews. In Proceedings of HLT/EMNLP,
pages 339?346.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850, December.
Bruce Sterling. 2005. Order out of chaos: What is the
best way to tag, bag, and sort data? Give it to the
unorganized masses. http://www.wired.com/
wired/archive/13.04/view.html?pg=4.
Accessed April 21, 2008.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the ACL.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Advances in NIPS.
Graham Vickery and Sacha Wunsch-Vincent. 2007. Par-
ticipative Web and User-Created Content: Web 2.0,
Wikis and Social Networking. OECD Publishing.
271
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82?90,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Reinforcement Learning for Mapping Instructions to Actions
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, harr, lsz, regina}@csail.mit.edu
Abstract
In this paper, we present a reinforce-
ment learning approach for mapping nat-
ural language instructions to sequences of
executable actions. We assume access to
a reward function that defines the qual-
ity of the executed actions. During train-
ing, the learner repeatedly constructs ac-
tion sequences for a set of documents, ex-
ecutes those actions, and observes the re-
sulting reward. We use a policy gradient
algorithm to estimate the parameters of a
log-linear model for action selection. We
apply our method to interpret instructions
in two domains ? Windows troubleshoot-
ing guides and game tutorials. Our results
demonstrate that this method can rival su-
pervised learning techniques while requir-
ing few or no annotated training exam-
ples.1
1 Introduction
The problem of interpreting instructions written
in natural language has been widely studied since
the early days of artificial intelligence (Winograd,
1972; Di Eugenio, 1992). Mapping instructions to
a sequence of executable actions would enable the
automation of tasks that currently require human
participation. Examples include configuring soft-
ware based on how-to guides and operating simu-
lators using instruction manuals. In this paper, we
present a reinforcement learning framework for in-
ducing mappings from text to actions without the
need for annotated training examples.
For concreteness, consider instructions from a
Windows troubleshooting guide on deleting tem-
porary folders, shown in Figure 1. We aim to map
1Code, data, and annotations used in this work are avail-
able at http://groups.csail.mit.edu/rbg/code/rl/
Figure 1: A Windows troubleshooting article de-
scribing how to remove the ?msdownld.tmp? tem-
porary folder.
this text to the corresponding low-level commands
and parameters. For example, properly interpret-
ing the third instruction requires clicking on a tab,
finding the appropriate option in a tree control, and
clearing its associated checkbox.
In this and many other applications, the valid-
ity of a mapping can be verified by executing the
induced actions in the corresponding environment
and observing their effects. For instance, in the
example above we can assess whether the goal
described in the instructions is achieved, i.e., the
folder is deleted. The key idea of our approach
is to leverage the validation process as the main
source of supervision to guide learning. This form
of supervision allows us to learn interpretations
of natural language instructions when standard su-
pervised techniques are not applicable, due to the
lack of human-created annotations.
Reinforcement learning is a natural framework
for building models using validation from an envi-
ronment (Sutton and Barto, 1998). We assume that
supervision is provided in the form of a reward
function that defines the quality of executed ac-
tions. During training, the learner repeatedly con-
structs action sequences for a set of given docu-
ments, executes those actions, and observes the re-
sulting reward. The learner?s goal is to estimate a
82
policy ? a distribution over actions given instruc-
tion text and environment state ? that maximizes
future expected reward. Our policy is modeled in a
log-linear fashion, allowing us to incorporate fea-
tures of both the instruction text and the environ-
ment. We employ a policy gradient algorithm to
estimate the parameters of this model.
We evaluate our method on two distinct applica-
tions: Windows troubleshooting guides and puz-
zle game tutorials. The key findings of our ex-
periments are twofold. First, models trained only
with simple reward signals achieve surprisingly
high results, coming within 11% of a fully su-
pervised method in the Windows domain. Sec-
ond, augmenting unlabeled documents with even
a small fraction of annotated examples greatly re-
duces this performance gap, to within 4% in that
domain. These results indicate the power of learn-
ing from this new form of automated supervision.
2 Related Work
Grounded Language Acquisition Our work
fits into a broader class of approaches that aim to
learn language from a situated context (Mooney,
2008a; Mooney, 2008b; Fleischman and Roy,
2005; Yu and Ballard, 2004; Siskind, 2001; Oates,
2001). Instances of such approaches include
work on inferring the meaning of words from
video data (Roy and Pentland, 2002; Barnard and
Forsyth, 2001), and interpreting the commentary
of a simulated soccer game (Chen and Mooney,
2008). Most of these approaches assume some
form of parallel data, and learn perceptual co-
occurrence patterns. In contrast, our emphasis
is on learning language by proactively interacting
with an external environment.
Reinforcement Learning for Language Pro-
cessing Reinforcement learning has been previ-
ously applied to the problem of dialogue manage-
ment (Scheffler and Young, 2002; Roy et al, 2000;
Litman et al, 2000; Singh et al, 1999). These
systems converse with a human user by taking ac-
tions that emit natural language utterances. The
reinforcement learning state space encodes infor-
mation about the goals of the user and what they
say at each time step. The learning problem is to
find an optimal policy that maps states to actions,
through a trial-and-error process of repeated inter-
action with the user.
Reinforcement learning is applied very differ-
ently in dialogue systems compared to our setup.
In some respects, our task is more easily amenable
to reinforcement learning. For instance, we are not
interacting with a human user, so the cost of inter-
action is lower. However, while the state space can
be designed to be relatively small in the dialogue
management task, our state space is determined by
the underlying environment and is typically quite
large. We address this complexity by developing
a policy gradient algorithm that learns efficiently
while exploring a small subset of the states.
3 Problem Formulation
Our task is to learn a mapping between documents
and the sequence of actions they express. Figure 2
shows how one example sentence is mapped to
three actions.
Mapping Text to Actions As input, we are
given a document d, comprising a sequence of sen-
tences (u1, . . . , u`), where each ui is a sequence
of words. Our goal is to map d to a sequence of
actions ~a = (a0, . . . , an?1). Actions are predicted
and executed sequentially.2
An action a = (c,R,W ?) encompasses a com-
mand c, the command?s parameters R, and the
words W ? specifying c and R. Elements of R re-
fer to objects available in the environment state, as
described below. Some parameters can also refer
to words in document d. Additionally, to account
for words that do not describe any actions, c can
be a null command.
The Environment The environment state E
specifies the set of objects available for interac-
tion, and their properties. In Figure 2, E is shown
on the right. The environment state E changes
in response to the execution of command c with
parameters R according to a transition distribu-
tion p(E ?|E , c, R). This distribution is a priori un-
known to the learner. As we will see in Section 5,
our approach avoids having to directly estimate
this distribution.
State To predict actions sequentially, we need to
track the state of the document-to-actions map-
ping over time. A mapping state s is a tuple
(E , d, j,W ), where E refers to the current environ-
ment state; j is the index of the sentence currently
being interpreted in document d; and W contains
words that were mapped by previous actions for
2That is, action ai is executed before ai+1 is predicted.
83
Figure 2: A three-step mapping from an instruction sentence to a sequence of actions in Windows 2000.
For each step, the figure shows the words selected by the action, along with the corresponding system
command and its parameters. The words of W ? are underlined, and the words of W are highlighted in
grey.
the same sentence. The mapping state s is ob-
served after each action.
The initial mapping state s0 for document d is
(Ed, d, 0, ?); Ed is the unique starting environment
state for d. Performing action a in state s =
(E , d, j,W ) leads to a new state s? according to
distribution p(s?|s, a), defined as follows: E tran-
sitions according to p(E ?|E , c, R), W is updated
with a?s selected words, and j is incremented if
all words of the sentence have been mapped. For
the applications we consider in this work, environ-
ment state transitions, and consequently mapping
state transitions, are deterministic.
Training During training, we are provided with
a set D of documents, the ability to sample from
the transition distribution, and a reward function
r(h). Here, h = (s0, a0, . . . , sn?1, an?1, sn) is
a history of states and actions visited while in-
terpreting one document. r(h) outputs a real-
valued score that correlates with correct action
selection.3 We consider both immediate reward,
which is available after each action, and delayed
reward, which does not provide feedback until the
last action. For example, task completion is a de-
layed reward that produces a positive value after
the final action only if the task was completed suc-
cessfully. We will also demonstrate how manu-
ally annotated action sequences can be incorpo-
rated into the reward.
3In most reinforcement learning problems, the reward
function is defined over state-action pairs, as r(s, a) ? in this
case, r(h) =
P
t r(st, at), and our formulation becomes a
standard finite-horizon Markov decision process. Policy gra-
dient approaches allow us to learn using the more general
case of history-based reward.
The goal of training is to estimate parameters ?
of the action selection distribution p(a|s, ?), called
the policy. Since the reward correlates with ac-
tion sequence correctness, the ? that maximizes
expected reward will yield the best actions.
4 A Log-Linear Model for Actions
Our goal is to predict a sequence of actions. We
construct this sequence by repeatedly choosing an
action given the current mapping state, and apply-
ing that action to advance to a new state.
Given a state s = (E , d, j,W ), the space of pos-
sible next actions is defined by enumerating sub-
spans of unused words in the current sentence (i.e.,
subspans of the jth sentence of d not in W ), and
the possible commands and parameters in envi-
ronment state E .4 We model the policy distribu-
tion p(a|s; ?) over this action space in a log-linear
fashion (Della Pietra et al, 1997; Lafferty et al,
2001), giving us the flexibility to incorporate a di-
verse range of features. Under this representation,
the policy distribution is:
p(a|s; ?) =
e???(s,a)
?
a?
e???(s,a
?)
, (1)
where ?(s, a) ? Rn is an n-dimensional feature
representation. During test, actions are selected
according to the mode of this distribution.
4For parameters that refer to words, the space of possible
values is defined by the unused words in the current sentence.
84
5 Reinforcement Learning
During training, our goal is to find the optimal pol-
icy p(a|s; ?). Since reward correlates with correct
action selection, a natural objective is to maximize
expected future reward ? that is, the reward we
expect while acting according to that policy from
state s. Formally, we maximize the value function:
V?(s) = Ep(h|?) [r(h)] , (2)
where the history h is the sequence of states and
actions encountered while interpreting a single
document d ? D. This expectation is averaged
over all documents in D. The distribution p(h|?)
returns the probability of seeing history h when
starting from state s and acting according to a pol-
icy with parameters ?. This distribution can be de-
composed into a product over time steps:
p(h|?) =
n?1?
t=0
p(at|st; ?)p(st+1|st, at). (3)
5.1 A Policy Gradient Algorithm
Our reinforcement learning problem is to find the
parameters ? that maximize V? from equation 2.
Although there is no closed form solution, policy
gradient algorithms (Sutton et al, 2000) estimate
the parameters ? by performing stochastic gradi-
ent ascent. The gradient of V? is approximated by
interacting with the environment, and the resulting
reward is used to update the estimate of ?. Policy
gradient algorithms optimize a non-convex objec-
tive and are only guaranteed to find a local opti-
mum. However, as we will see, they scale to large
state spaces and can perform well in practice.
To find the parameters ? that maximize the ob-
jective, we first compute the derivative of V?. Ex-
panding according to the product rule, we have:
?
??
V?(s) = Ep(h|?)
[
r(h)
?
t
?
??
log p(at|st; ?)
]
,
(4)
where the inner sum is over all time steps t in
the current history h. Expanding the inner partial
derivative we observe that:
?
??
log p(a|s; ?) = ?(s, a)?
?
a?
?(s, a?)p(a?|s; ?),
(5)
which is the derivative of a log-linear distribution.
Equation 5 is easy to compute directly. How-
ever, the complete derivative of V? in equation 4
Input: A document set D,
Feature representation ?,
Reward function r(h),
Number of iterations T
Initialization: Set ? to small random values.
for i = 1 . . . T do1
foreach d ? D do2
Sample history h ? p(h|?) where3
h = (s0, a0, . . . , an?1, sn) as follows:
3a for t = 0 . . . n? 1 do
3b Sample action at ? p(a|st; ?)
3c Execute at on state st: st+1 ? p(s|st, at)
end
??
P
t
`
?(st, at)?
P
a? ?(st, a
?)p(a?|st; ?)
?
4
? ? ? + r(h)?5
end
end
Output: Estimate of parameters ?
Algorithm 1: A policy gradient algorithm.
is intractable, because computing the expectation
would require summing over all possible histo-
ries. Instead, policy gradient algorithms employ
stochastic gradient ascent by computing a noisy
estimate of the expectation using just a subset of
the histories. Specifically, we draw samples from
p(h|?) by acting in the target environment, and
use these samples to approximate the expectation
in equation 4. In practice, it is often sufficient to
sample a single history h for this approximation.
Algorithm 1 details the complete policy gradi-
ent algorithm. It performs T iterations over the
set of documents D. Step 3 samples a history that
maps each document to actions. This is done by
repeatedly selecting actions according to the cur-
rent policy, and updating the state by executing the
selected actions. Steps 4 and 5 compute the empir-
ical gradient and update the parameters ?.
In many domains, interacting with the environ-
ment is expensive. Therefore, we use two tech-
niques that allow us to take maximum advantage
of each environment interaction. First, a his-
tory h = (s0, a0, . . . , sn) contains subsequences
(si, ai, . . . sn) for i = 1 to n ? 1, each with its
own reward value given by the environment as a
side effect of executing h. We apply the update
from equation 5 for each subsequence. Second,
for a sampled history h, we can propose alterna-
tive histories h? that result in the same commands
and parameters with different word spans. We can
again apply equation 5 for each h?, weighted by its
probability under the current policy, p(h
?|?)
p(h|?) .
85
The algorithm we have presented belongs to
a family of policy gradient algorithms that have
been successfully used for complex tasks such as
robot control (Ng et al, 2003). Our formulation is
unique in how it represents natural language in the
reinforcement learning framework.
5.2 Reward Functions and ML Estimation
We can design a range of reward functions to guide
learning, depending on the availability of anno-
tated data and environment feedback. Consider the
case when every training document d ? D is an-
notated with its correct sequence of actions, and
state transitions are deterministic. Given these ex-
amples, it is straightforward to construct a reward
function that connects policy gradient to maxi-
mum likelihood. Specifically, define a reward
function r(h) that returns one when h matches the
annotation for the document being analyzed, and
zero otherwise. Policy gradient performs stochas-
tic gradient ascent on the objective from equa-
tion 2, performing one update per document. For
document d, this objective becomes:
Ep(h|?)[r(h)] =
?
h
r(h)p(h|?) = p(hd|?),
where hd is the history corresponding to the an-
notated action sequence. Thus, with this reward
policy gradient is equivalent to stochastic gradient
ascent with a maximum likelihood objective.
At the other extreme, when annotations are
completely unavailable, learning is still possi-
ble given informative feedback from the environ-
ment. Crucially, this feedback only needs to cor-
relate with action sequence quality. We detail
environment-based reward functions in the next
section. As our results will show, reward func-
tions built using this kind of feedback can provide
strong guidance for learning. We will also con-
sider reward functions that combine annotated su-
pervision with environment feedback.
6 Applying the Model
We study two applications of our model: follow-
ing instructions to perform software tasks, and
solving a puzzle game using tutorial guides.
6.1 Microsoft Windows Help and Support
On its Help and Support website,5 Microsoft pub-
lishes a number of articles describing how to per-
5support.microsoft.com
Notation
o Parameter referring to an environment object
L Set of object class names (e.g. ?button?)
V Vocabulary
Features onW and object o
Test if o is visible in s
Test if o has input focus
Test if o is in the foreground
Test if o was previously interacted with
Test if o came into existence since last action
Min. edit distance between w ?W and object labels in s
Features on words inW , command c, and object o
?c? ? C, w ? V : test if c? = c and w ?W
?c? ? C, l ? L: test if c? = c and l is the class of o
Table 1: Example features in the Windows do-
main. All features are binary, except for the nor-
malized edit distance which is real-valued.
form tasks and troubleshoot problems in the Win-
dows operating systems. Examples of such tasks
include installing patches and changing security
settings. Figure 1 shows one such article.
Our goal is to automatically execute these sup-
port articles in the Windows 2000 environment.
Here, the environment state is the set of visi-
ble user interface (UI) objects, and object prop-
erties such as label, location, and parent window.
Possible commands include left-click, right-click,
double-click, and type-into, all of which take a UI
object as a parameter; type-into additionally re-
quires a parameter for the input text.
Table 1 lists some of the features we use for this
domain. These features capture various aspects of
the action under consideration, the current Win-
dows UI state, and the input instructions. For ex-
ample, one lexical feature measures the similar-
ity of a word in the sentence to the UI labels of
objects in the environment. Environment-specific
features, such as whether an object is currently in
focus, are useful when selecting the object to ma-
nipulate. In total, there are 4,438 features.
Reward Function Environment feedback can
be used as a reward function in this domain. An
obvious reward would be task completion (e.g.,
whether the stated computer problem was fixed).
Unfortunately, verifying task completion is a chal-
lenging system issue in its own right.
Instead, we rely on a noisy method of check-
ing whether execution can proceed from one sen-
tence to the next: at least one word in each sen-
tence has to correspond to an object in the envi-
86
Figure 3: Crossblock puzzle with tutorial. For this
level, four squares in a row or column must be re-
moved at once. The first move specified by the
tutorial is greyed in the puzzle.
ronment.6 For instance, in the sentence from Fig-
ure 2 the word ?Run? matches the Run... menu
item. If no words in a sentence match a current
environment object, then one of the previous sen-
tences was analyzed incorrectly. In this case, we
assign the history a reward of -1. This reward is
not guaranteed to penalize all incorrect histories,
because there may be false positive matches be-
tween the sentence and the environment. When
at least one word matches, we assign a positive
reward that linearly increases with the percentage
of words assigned to non-null commands, and lin-
early decreases with the number of output actions.
This reward signal encourages analyses that inter-
pret al of the words without producing spurious
actions.
6.2 Crossblock: A Puzzle Game
Our second application is to a puzzle game called
Crossblock, available online as a Flash game.7
Each of 50 puzzles is played on a grid, where some
grid positions are filled with squares. The object
of the game is to clear the grid by drawing vertical
or horizontal line segments that remove groups of
squares. Each segment must exactly cross a spe-
cific number of squares, ranging from two to seven
depending on the puzzle. Humans players have
found this game challenging and engaging enough
to warrant posting textual tutorials.8 A sample
puzzle and tutorial are shown in Figure 3.
The environment is defined by the state of the
grid. The only command is clear, which takes a
parameter specifying the orientation (row or col-
umn) and grid location of the line segment to be
6We assume that a word maps to an environment object if
the edit distance between the word and the object?s name is
below a threshold value.
7hexaditidom.deviantart.com/art/Crossblock-108669149
8www.jayisgames.com/archives/2009/01/crossblock.php
removed. The challenge in this domain is to seg-
ment the text into the phrases describing each ac-
tion, and then correctly identify the line segments
from references such as ?the bottom four from the
second column from the left.?
For this domain, we use two sets of binary fea-
tures on state-action pairs (s, a). First, for each
vocabulary word w, we define a feature that is one
if w is the last word of a?s consumed words W ?.
These features help identify the proper text seg-
mentation points between actions. Second, we in-
troduce features for pairs of vocabulary word w
and attributes of action a, e.g., the line orientation
and grid locations of the squares that a would re-
move. This set of features enables us to match
words (e.g., ?row?) with objects in the environ-
ment (e.g., a move that removes a horizontal series
of squares). In total, there are 8,094 features.
Reward Function For Crossblock it is easy to
directly verify task completion, which we use as
the basis of our reward function. The reward r(h)
is -1 if h ends in a state where the puzzle cannot
be completed. For solved puzzles, the reward is
a positive value proportional to the percentage of
words assigned to non-null commands.
7 Experimental Setup
Datasets For the Windows domain, our dataset
consists of 128 documents, divided into 70 for
training, 18 for development, and 40 for test. In
the puzzle game domain, we use 50 tutorials,
divided into 40 for training and 10 for test.9
Statistics for the datasets are shown below.
Windows Puzzle
Total # of documents 128 50
Total # of words 5562 994
Vocabulary size 610 46
Avg. words per sentence 9.93 19.88
Avg. sentences per document 4.38 1.00
Avg. actions per document 10.37 5.86
The data exhibits certain qualities that make
for a challenging learning problem. For instance,
there are a surprising variety of linguistic con-
structs ? as Figure 4 shows, in the Windows do-
main even a simple command is expressed in at
least six different ways.
9For Crossblock, because the number of puzzles is lim-
ited, we did not hold out a separate development set, and re-
port averaged results over five training/test splits.
87
Figure 4: Variations of ?click internet options on
the tools menu? present in the Windows corpus.
Experimental Framework To apply our algo-
rithm to the Windows domain, we use the Win32
application programming interface to simulate hu-
man interactions with the user interface, and to
gather environment state information. The operat-
ing system environment is hosted within a virtual
machine,10 allowing us to rapidly save and reset
system state snapshots. For the puzzle game do-
main, we replicated the game with an implemen-
tation that facilitates automatic play.
As is commonly done in reinforcement learn-
ing, we use a softmax temperature parameter to
smooth the policy distribution (Sutton and Barto,
1998), set to 0.1 in our experiments. For Windows,
the development set is used to select the best pa-
rameters. For Crossblock, we choose the parame-
ters that produce the highest reward during train-
ing. During evaluation, we use these parameters
to predict mappings for the test documents.
Evaluation Metrics For evaluation, we com-
pare the results to manually constructed sequences
of actions. We measure the number of correct ac-
tions, sentences, and documents. An action is cor-
rect if it matches the annotations in terms of com-
mand and parameters. A sentence is correct if all
of its actions are correctly identified, and analo-
gously for documents.11 Statistical significance is
measured with the sign test.
Additionally, we compute a word alignment
score to investigate the extent to which the input
text is used to construct correct analyses. This
score measures the percentage of words that are
aligned to the corresponding annotated actions in
correctly analyzed documents.
Baselines We consider the following baselines
to characterize the performance of our approach.
10VMware Workstation, available at www.vmware.com
11In these tasks, each action depends on the correct execu-
tion of all previous actions, so a single error can render the
remainder of that document?s mapping incorrect. In addition,
due to variability in document lengths, overall action accu-
racy is not guaranteed to be higher than document accuracy.
? Full Supervision Sequence prediction prob-
lems like ours are typically addressed us-
ing supervised techniques. We measure how
a standard supervised approach would per-
form on this task by using a reward signal
based on manual annotations of output ac-
tion sequences, as defined in Section 5.2. As
shown there, policy gradient with this re-
ward is equivalent to stochastic gradient as-
cent with a maximum likelihood objective.
? Partial Supervision We consider the case
when only a subset of training documents is
annotated, and environment reward is used
for the remainder. Our method seamlessly
combines these two kinds of rewards.
? Random and Majority (Windows) We con-
sider two na??ve baselines. Both scan through
each sentence from left to right. A com-
mand c is executed on the object whose name
is encountered first in the sentence. This
command c is either selected randomly, or
set to the majority command, which is left-
click. This procedure is repeated until no
more words match environment objects.
? Random (Puzzle) We consider a baseline
that randomly selects among the actions that
are valid in the current game state.12
8 Results
Table 2 presents evaluation results on the test sets.
There are several indicators of the difficulty of this
task. The random and majority baselines? poor
performance in both domains indicates that na??ve
approaches are inadequate for these tasks. The
performance of the fully supervised approach pro-
vides further evidence that the task is challenging.
This difficulty can be attributed in part to the large
branching factor of possible actions at each step ?
on average, there are 27.14 choices per action in
the Windows domain, and 9.78 in the Crossblock
domain.
In both domains, the learners relying only
on environment reward perform well. Although
the fully supervised approach performs the best,
adding just a few annotated training examples
to the environment-based learner significantly re-
duces the performance gap.
12Since action selection is among objects, there is no natu-
ral majority baseline for the puzzle.
88
Windows Puzzle
Action Sent. Doc. Word Action Doc. Word
Random baseline 0.128 0.101 0.000 ?? 0.081 0.111 ??
Majority baseline 0.287 0.197 0.100 ?? ?? ?? ??
Environment reward ? 0.647 ? 0.590 ? 0.375 0.819 ? 0.428 ? 0.453 0.686
Partial supervision  0.723 ? 0.702 0.475 0.989 0.575 ? 0.523 0.850
Full supervision  0.756 0.714 0.525 0.991 0.632 0.630 0.869
Table 2: Performance on the test set with different reward signals and baselines. Our evaluation measures
the proportion of correct actions, sentences, and documents. We also report the percentage of correct
word alignments for the successfully completed documents. Note the puzzle domain has only single-
sentence documents, so its sentence and document scores are identical. The partial supervision line
refers to 20 out of 70 annotated training documents for Windows, and 10 out of 40 for the puzzle. Each
result marked with ? or  is a statistically significant improvement over the result immediately above it;
? indicates p < 0.01 and  indicates p < 0.05.
Figure 5: Comparison of two training scenarios where training is done using a subset of annotated
documents, with and without environment reward for the remaining unannotated documents.
Figure 5 shows the overall tradeoff between an-
notation effort and system performance for the two
domains. The ability to make this tradeoff is one
of the advantages of our approach. The figure also
shows that augmenting annotated documents with
additional environment-reward documents invari-
ably improves performance.
The word alignment results from Table 2 in-
dicate that the learners are mapping the correct
words to actions for documents that are success-
fully completed. For example, the models that per-
form best in the Windows domain achieve nearly
perfect word alignment scores.
To further assess the contribution of the instruc-
tion text, we train a variant of our model without
access to text features. This is possible in the game
domain, where all of the puzzles share a single
goal state that is independent of the instructions.
This variant solves 34% of the puzzles, suggest-
ing that access to the instructions significantly im-
proves performance.
9 Conclusions
In this paper, we presented a reinforcement learn-
ing approach for inducing a mapping between in-
structions and actions. This approach is able to use
environment-based rewards, such as task comple-
tion, to learn to analyze text. We showed that hav-
ing access to a suitable reward function can signif-
icantly reduce the need for annotations.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0835445,
grant IIS-0835652, and a Graduate Research Fel-
lowship) and the ONR. Thanks to Michael Collins,
Amir Globerson, Tommi Jaakkola, Leslie Pack
Kaelbling, Dina Katabi, Martin Rinard, and mem-
bers of the MIT NLP group for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
89
References
Kobus Barnard and David A. Forsyth. 2001. Learning
the semantics of words and pictures. In Proceedings
of ICCV.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: a test of grounded language acqui-
sition. In Proceedings of ICML.
Stephen Della Pietra, Vincent J. Della Pietra, and
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Trans. Pattern Anal. Mach. Intell.,
19(4):380?393.
Barbara Di Eugenio. 1992. Understanding natural lan-
guage instructions: the case of purpose clauses. In
Proceedings of ACL.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated language learning. In Proceed-
ings of CoNLL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Diane J. Litman, Michael S. Kearns, Satinder Singh,
and Marilyn A. Walker. 2000. Automatic optimiza-
tion of dialogue management. In Proceedings of
COLING.
Raymond J. Mooney. 2008a. Learning language
from its perceptual context. In Proceedings of
ECML/PKDD.
Raymond J. Mooney. 2008b. Learning to connect lan-
guage and perception. In Proceedings of AAAI.
Andrew Y. Ng, H. Jin Kim, Michael I. Jordan, and
Shankar Sastry. 2003. Autonomous helicopter flight
via reinforcement learning. In Advances in NIPS.
James Timothy Oates. 2001. Grounding knowledge
in sensors: Unsupervised learning for language and
planning. Ph.D. thesis, University of Massachusetts
Amherst.
Deb K. Roy and Alex P. Pentland. 2002. Learn-
ing words from sights and sounds: a computational
model. Cognitive Science 26, pages 113?146.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of ACL.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT.
Satinder P. Singh, Michael J. Kearns, Diane J. Litman,
and Marilyn A. Walker. 1999. Reinforcement learn-
ing for spoken dialogue systems. In Advances in
NIPS.
Jeffrey Mark Siskind. 2001. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. J. Artif. Intell. Res. (JAIR),
15:31?90.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. The MIT
Press.
Richard S. Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 2000. Policy gradient meth-
ods for reinforcement learning with function approx-
imation. In Advances in NIPS.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press.
Chen Yu and Dana H. Ballard. 2004. On the integra-
tion of grounding language and learning objects. In
Proceedings of AAAI.
90
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234?1244,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Using Universal Linguistic Knowledge to Guide Grammar Induction
Tahira Naseem, Harr Chen, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{tahira, harr, regina} @csail.mit.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
We present an approach to grammar induc-
tion that utilizes syntactic universals to im-
prove dependency parsing across a range of
languages. Our method uses a single set
of manually-specified language-independent
rules that identify syntactic dependencies be-
tween pairs of syntactic categories that com-
monly occur across languages. During infer-
ence of the probabilistic model, we use pos-
terior expectation constraints to require that a
minimum proportion of the dependencies we
infer be instances of these rules. We also auto-
matically refine the syntactic categories given
in our coarsely tagged input. Across six lan-
guages our approach outperforms state-of-the-
art unsupervised methods by a significant mar-
gin.1
1 Introduction
Despite surface differences, human languages ex-
hibit striking similarities in many fundamental as-
pects of syntactic structure. These structural corre-
spondences, referred to as syntactic universals, have
been extensively studied in linguistics (Baker, 2001;
Carnie, 2002; White, 2003; Newmeyer, 2005) and
underlie many approaches in multilingual parsing.
In fact, much recent work has demonstrated that
learning cross-lingual correspondences from cor-
pus data greatly reduces the ambiguity inherent in
syntactic analysis (Kuhn, 2004; Burkett and Klein,
2008; Cohen and Smith, 2009a; Snyder et al, 2009;
Berg-Kirkpatrick and Klein, 2010).
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/dependency/
Root ? Auxiliary Noun ? Adjective
Root ? Verb Noun ? Article
Verb ? Noun Noun ? Noun
Verb ? Pronoun Noun ? Numeral
Verb ? Adverb Preposition ? Noun
Verb ? Verb Adjective ? Adverb
Auxiliary ? Verb
Table 1: The manually-specified universal dependency
rules used in our experiments. These rules specify head-
dependent relationships between coarse (i.e., unsplit)
syntactic categories. An explanation of the ruleset is pro-
vided in Section 5.
In this paper, we present an alternative gram-
mar induction approach that exploits these struc-
tural correspondences by declaratively encoding a
small set of universal dependency rules. As input
to the model, we assume a corpus annotated with
coarse syntactic categories (i.e., high-level part-of-
speech tags) and a set of universal rules defined over
these categories, such as those in Table 1. These
rules incorporate the definitional properties of syn-
tactic categories in terms of their interdependencies
and thus are universal across languages. They can
potentially help disambiguate structural ambiguities
that are difficult to learn from data alone ? for
example, our rules prefer analyses in which verbs
are dependents of auxiliaries, even though analyz-
ing auxiliaries as dependents of verbs is also consis-
tent with the data. Leveraging these universal rules
has the potential to improve parsing performance
for a large number of human languages; this is par-
ticularly relevant to the processing of low-resource
1234
languages. Furthermore, these universal rules are
compact and well-understood, making them easy to
manually construct.
In addition to these universal dependencies, each
specific language typically possesses its own id-
iosyncratic set of dependencies. We address this
challenge by requiring the universal constraints to
only hold in expectation rather than absolutely, i.e.,
we permit a certain number of violations of the con-
straints.
We formulate a generative Bayesian model that
explains the observed data while accounting for
declarative linguistic rules during inference. These
rules are used as expectation constraints on the
posterior distribution over dependency structures.
This approach is based on the posterior regular-
ization technique (Grac?a et al, 2009), which we
apply to a variational inference algorithm for our
parsing model. Our model can also optionally re-
fine common high-level syntactic categories into
per-language categories by inducing a clustering of
words using Dirichlet Processes (Ferguson, 1973).
Since the universals guide induction toward linguis-
tically plausible structures, automatic refinement be-
comes feasible even in the absence of manually an-
notated syntactic trees.
We test the effectiveness of our grammar induc-
tion model on six Indo-European languages from
three language groups: English, Danish, Portuguese,
Slovene, Spanish, and Swedish. Though these lan-
guages share a high-level Indo-European ancestry,
they cover a diverse range of syntactic phenomenon.
Our results demonstrate that universal rules greatly
improve the accuracy of dependency parsing across
all of these languages, outperforming current state-
of-the-art unsupervised grammar induction meth-
ods (Headden III et al, 2009; Berg-Kirkpatrick and
Klein, 2010).
2 Related Work
Learning with Linguistic Constraints Our work
is situated within a broader class of unsupervised ap-
proaches that employ declarative knowledge to im-
prove learning of linguistic structure (Haghighi and
Klein, 2006; Chang et al, 2007; Grac?a et al, 2007;
Cohen and Smith, 2009b; Druck et al, 2009; Liang
et al, 2009a). The way we apply constraints is clos-
est to the latter two approaches of posterior regular-
ization and generalized expectation criteria.
In the posterior regularization framework, con-
straints are expressed in the form of expectations on
posteriors (Grac?a et al, 2007; Ganchev et al, 2009;
Grac?a et al, 2009; Ganchev et al, 2010). This de-
sign enables the model to reflect constraints that are
difficult to encode via the model structure or as pri-
ors on its parameters. In their approach, parame-
ters are estimated using a modified EM algorithm,
where the E-step minimizes the KL-divergence be-
tween the model posterior and the set of distributions
that satisfies the constraints. Our approach also ex-
presses constraints as expectations on the posterior;
we utilize the machinery of their framework within
a variational inference algorithm with a mean field
approximation.
Generalized expectation criteria, another tech-
nique for declaratively specifying expectation con-
straints, has previously been successfully applied to
the task of dependency parsing (Druck et al, 2009).
This objective expresses constraints in the form of
preferences over model expectations. The objective
is penalized by the square distance between model
expectations and the prespecified values of the ex-
pectation. This approach yields significant gains
compared to a fully unsupervised counterpart. The
constraints they studied are corpus- and language-
specific. Our work demonstrates that a small set of
language-independent universals can also serve as
effective constraints. Furthermore, we find that our
method outperforms the generalized expectation ap-
proach using corpus-specific constraints.
Learning to Refine Syntactic Categories Recent
research has demonstrated the usefulness of auto-
matically refining the granularity of syntactic cat-
egories. While most of the existing approaches
are implemented in the supervised setting (Finkel
et al, 2007; Petrov and Klein, 2007), Liang et al
(2007) propose a non-parametric Bayesian model
that learns the granularity of PCFG categories in
an unsupervised fashion. For each non-terminal
grammar symbol, the model posits a Hierarchical
Dirichlet Process over its refinements (subsymbols)
to automatically learn the granularity of syntactic
categories. As with their work, we also use non-
parametric priors for category refinement and em-
1235
ploy variational methods for inference. However,
our goal is to apply category refinement to depen-
dency parsing, rather than to PCFGs, requiring a
substantially different model formulation. While
Liang et al (2007) demonstrated empirical gains on
a synthetic corpus, our experiments focus on unsu-
pervised category refinement on real language data.
Universal Rules in NLP Despite the recent surge
of interest in multilingual learning (Kuhn, 2004; Co-
hen and Smith, 2009a; Snyder et al, 2009; Berg-
Kirkpatrick and Klein, 2010), there is surprisingly
little computational work on linguistic universals.
On the acquisition side, Daume? III and Campbell
(2007) proposed a computational technique for dis-
covering universal implications in typological fea-
tures. More closely related to our work is the posi-
tion paper by Bender (2009), which advocates the
use of manually-encoded cross-lingual generaliza-
tions for the development of NLP systems. She ar-
gues that a system employing such knowledge could
be easily adapted to a particular language by spe-
cializing this high level knowledge based on the ty-
pological features of the language. We also argue
that cross-language universals are beneficial for au-
tomatic language processing; however, our focus is
on learning language-specific adaptations of these
rules from data.
3 Model
The central hypothesis of this work is that unsu-
pervised dependency grammar induction can be im-
proved using universal linguistic knowledge. To-
ward this end our approach is comprised of two
components: a probabilistic model that explains
how sentences are generated from latent dependency
structures and a technique for incorporating declar-
ative rules into the inference process.
We first describe the generative story in this sec-
tion before turning to how constraints are applied
during inference in Section 4. Our model takes as
input (i.e., as observed) a set of sentences where
each word is annotated with a coarse part-of-speech
tag. Table 2 provides a detailed technical descrip-
tion of our model?s generative process, and Figure 1
presents a model diagram.
For each observed coarse symbol s:
1. Draw top-level infinite multinomial over
subsymbols ?s ? GEM(?).
2. For each subsymbol z of symbol s:
(a) Draw word emission multinomial
?sz ? Dir(?0).
(b) For each context value c:
i. Draw child symbol generation
multinomial ?szc ? Dir(?0).
ii. For each child symbol s?:
A. Draw second-level infinite
multinomial over subsymbols
pis?szc ? DP(?, ?s?).
For each tree node i generated in context c by
parent symbol s? and parent subsymbol z?:
1. Draw coarse symbol si ? Mult(?s?z?).
2. Draw subsymbol zi ? Mult(pisis?z?c).
3. Draw word xi ? Mult(?sizi).
Table 2: The generative process for model parameters
and parses. In the above GEM, DP, Dir, and Mult refer
respectively to the stick breaking distribution, Dirichlet
process, Dirichlet distribution, and multinomial distribu-
tion.
Generating Symbols and Words We describe
how a single node of the tree is generated before
discussing how the entire tree structure is formed.
Each node of the dependency tree is comprised of
three random variables: an observed coarse symbol
s, a hidden refined subsymbol z, and an observed
word x. In the following let the parent of the cur-
rent node have symbol s? and subsymbol z?; the root
node is generated from separate root-specific distri-
butions. Subsymbol refinement is an optional com-
ponent of the full model and can be omitted by de-
terministically equating s and z. As we explain at
the end of this section, without this aspect the gener-
ative story closely resembles the classic dependency
model with valence (DMV) of Klein and Manning
(2004).
First we draw symbol s from a finite multinomial
1236
s - coarse symbol (observed)
z - refined subsymbol
x - word (observed)
?szc - distr over child coarse symbols for
each parent s and z and context c
?s - top-level distr over subsymbols for s
piss?z?c - distr over subsymbols for each s,
parent s? and z?, and context c
?sz - distr over words for s and z
Figure 1: Graphical representation of the model and a summary of the notation. There is a copy of the outer plate for
each distinct symbol in the observed coarse tags. Here, node 3 is shown to be the parent of nodes 1 and 2. Shaded
variables are observed, square variables are hyperparameters. The elongated oval around s and z represents the two
variables jointly. For clarity the diagram omits some arrows from ? to each s, pi to each z, and ? to each x.
distribution with parameters ?s?z?c. As the indices
indicate, we have one such set of multinomial pa-
rameters for every combination of parent symbol
s? and subsymbol z? along with a context c. Here
the context of the current node can take one of six
values corresponding to every combination of di-
rection (left or right) and valence (first, second, or
third or higher child) with respect to its parent. The
prior (base distribution) for each ?s?z?c is a symmet-
ric Dirichlet with hyperparameter ?0.
Next we draw the refined syntactic category sub-
symbol z from an infinite multinomial with parame-
ters piss?z?c. Here the selection of pi is indexed by the
current node?s coarse symbol s, the symbol s? and
subsymbol z? of the parent node, and the context c
of the current node.
For each unique coarse symbol s we tie together
the distributions piss?z?c for all possible parent and
context combinations (i.e., s?, z?, and c) using a Hi-
erarchical Dirichlet Process (HDP). Specifically, for
a single s, each distribution piss?z?c over subsymbols
is drawn from a DP with concentration parameter
? and base distribution ?s over subsymbols. This
base distribution ?s is itself drawn from a GEM prior
with concentration parameter ?. By formulating the
generation of z as an HDP, we can share parame-
ters for a single coarse symbol?s subsymbol distribu-
tion while allowing for individual variability based
on node parent and context. Note that parameters
are not shared across different coarse symbols, pre-
serving the distinctions expressed via the coarse tag
annotations.
Finally, we generate the word x from a finite
multinomial with parameters ?sz , where s and z are
the symbol and subsymbol of the current node. The
? distributions are drawn from a symmetric Dirich-
let prior.
Generating the Tree Structure We now consider
how the structure of the tree arises. We follow
an approach similar to the widely-referenced DMV
model (Klein and Manning, 2004), which forms
the basis of the current state-of-the-art unsuper-
vised grammar induction model (Headden III et al,
2009). After a node is drawn we generate children
on each side until we produce a designated STOP
symbol. We encode more detailed valence informa-
tion than Klein and Manning (2004) and condition
child generation on parent valence. Specifically, af-
ter drawing a node we first decide whether to pro-
ceed to generate a child or to stop conditioned on the
parent symbol and subsymbol and the current con-
text (direction and valence). If we decide to gener-
ate a child we follow the previously described pro-
cess for constructing a node. We can combine the
stopping decision with the generation of the child
symbol by including a distinguished STOP symbol
as a possible outcome in distribution ?.
No-Split Model Variant In the absence of sub-
symbol refinement (i.e., when subsymbol z is set to
be identical to coarse symbol s), our model simpli-
fies in some respects. In particular, the HDP gener-
1237
ation of z is obviated and word x is drawn from a
word distribution ?s indexed solely by coarse sym-
bol s. The resulting simplified model closely resem-
bles DMV (Klein and Manning, 2004), except that it
1) explicitly generate words x rather than only part-
of-speech tags s, 2) encodes richer context and va-
lence information, and 3) imposes a Dirichlet prior
on the symbol distribution ?.
4 Inference with Constraints
We now describe how to augment our generative
model of dependency structure with constraints de-
rived from linguistic knowledge. Incorporating arbi-
trary linguistic rules directly in the generative story
is challenging as it requires careful tuning of either
the model structure or priors for each constraint. In-
stead, following the approach of Grac?a et al (2007),
we constrain the posterior to satisfy the rules in ex-
pectation during inference. This effectively biases
the inference toward linguistically plausible settings.
In standard variational inference, an intractable
true posterior is approximated by a distribution from
a tractable set (Bishop, 2006). This tractable set typ-
ically makes stronger independence assumptions be-
tween model parameters than the model itself. To in-
corporate the constraints, we further restrict the set
to only include distributions that satisfy the specified
expectation constraints over hidden variables.
In general, for some given model, let ? denote
the entire set of model parameters and z and x de-
note the hidden structure and observations respec-
tively. We are interested in estimating the posterior
p(?, z | x). Variational inference transforms this
problem into an optimization problem where we try
to find a distribution q(?, z) from a restricted set Q
that minimizes the KL-divergence between q(?, z)
and p(?, z | x):
KL(q(?, z) ? p(?, z | x))
=
?
q(?, z) log
q(?, z)
p(?, z, x)
d?dz + log p(x).
Rearranging the above yields:
log p(x) = KL(q(?, z) ? p(?, z | x)) + F ,
where F is defined as
F ?
?
q(?, z) log
p(?, z, x)
q(?, z)
d?dz. (1)
Thus F is a lower bound on likelihood. Maximizing
this lower bound is equivalent to minimizing the KL-
divergence between p(?, z | x) and q(?, z). To make
this maximization tractable we make a mean field
assumption that q belongs to a set Q of distributions
that factorize as follows:
q(?, z) = q(?)q(z).
We further constrain q to be from the subset of Q
that satisfies the expectation constraintEq[f(z)] ? b
where f is a deterministically computable function
of the hidden structures. In our model, for exam-
ple, f counts the dependency edges that are an in-
stance of one of the declaratively specified depen-
dency rules, while b is the proportion of the total
dependencies that we expect should fulfill this con-
straint.2
With the mean field factorization and the expec-
tation constraints in place, solving the maximization
of F in (1) separately for each factor yields the fol-
lowing updates:
q(?) = argmin
q(?)
KL
(
q(?) ? q?(?)
)
, (2)
q(z) = argmin
q(z)
KL
(
q(z) ? q?(z)
)
s.t. Eq(z)[f(z)] ? b, (3)
where
q?(?) ? expEq(z)[log p(?, z, x)], (4)
q?(z) ? expEq(?)[log p(?, z, x)]. (5)
We can solve (2) by setting q(?) to q?(?) ? since
q(z) is held fixed while updating q(?), the expecta-
tion function of the constraint remains constant dur-
ing this update. As shown by Grac?a et al (2007), the
update in (3) is a constrained optimization problem
and can be solved by performing gradient search on
its dual:
argmin
?
?>b + log
?
z
q?(z) exp(??>f(z)) (6)
For a fixed value of ? the optimal q(z) ?
q?(z) exp(??>f(z)). By updating q(?) and q(z)
as in (2) and (3) we are effectively maximizing the
lower bound F .
2Constraints of the form Eq[f(z)] ? b are easily imposed
by negating f(z) and b.
1238
4.1 Variational Updates
We now derive the specific variational updates for
our dependency induction model. First we assume
the following mean-field factorization of our varia-
tional distribution:
q(?, ?, pi, ?, z)
= q(z) ?
?
s?
q(?s?) ?
T?
z?=1
q(?s?z?)?
?
c
q(?s?z?c) ?
?
s
q(piss?z?c), (7)
where s? varies over the set of unique symbols in the
observed tags, z? denotes subsymbols for each sym-
bol, c varies over context values comprising a pair
of direction (left or right) and valence (first, second,
or third or higher) values, and s corresponds to child
symbols.
We restrict q(?s?z?c) and q(?s?z?) to be Dirichlet
distributions and q(z) to be multinomial. As with
prior work (Liang et al, 2009b), we assume a de-
generate q(?) ? ???(?) for tractability reasons, i.e.,
all mass is concentrated on some single ??. We also
assume that the top level stick-breaking distribution
is truncated at T , i.e., q(?) assigns zero probability
to integers greater than T . Because of the truncation
of ?, we can approximate q(piss?z?c) with an asym-
metric finite dimensional Dirichlet.
The factors are updated one at a time holding all
other factors fixed. The variational update for q(pi)
is given by:
q(piss?z?c) = Dir
(
piss?z?c;?? + Eq(z)[Css?z?c(z)]
)
,
where term Eq(z)[Css?z?c(z)] is the expected count
w.r.t. q(z) of child symbol s and subsymbol z in
context c when generated by parent symbol s? and
subsymbol z?.
Similarly, the updates for q(?) and q(?) are given
by:
q(?s?z?c) = Dir
(
?s?z?c; ?0 + Eq(z)[Cs?z?c(s)]
)
,
q(?s?z?) = Dir
(
?s?z? ;?0 + Eq(z)[Cs?z?(x)]
)
,
where Cs?z?c(s) is the count of child symbol s being
generated by the parent symbol s? and subsymbol z?
in context c and Cs?z?x is the count of word x being
generated by symbol s? and subsymbol z?.
The only factor affected by the expectation con-
straints is q(z). Recall from the previous section that
the update for q(z) is performed via gradient search
on the dual of a constrained minimization problem
of the form:
q(z) = argmin
q(z)
KL(q(z) ? q?(z)).
Thus we first compute the update for q?(z):
q?(z) ?
N?
n=1
len(n)?
j=1
(expEq(?)[log ?snjznj (xnj)]
? expEq(?)[log ?sh(nj)zh(nj)cnj (snj)]
? expEq(pi)[log pisnjsh(nj)zh(nj)cnj (znj)]),
where N is the total number of sentences, len(n)
is the length of sentence n, and index h(nj) refers
to the head of the jth node of sentence n. Given
this q?(z) a gradient search is performed using (6) to
find the optimal ? and thus the primal solution for
updating q(z).
Finally, we update the degenerate factor q(?s)
with the projected gradient search algorithm used
by Liang et al (2009b).
5 Linguistic Constraints
Universal Dependency Rules We compile a set of
13 universal dependency rules consistent with vari-
ous linguistic accounts (Carnie, 2002; Newmeyer,
2005), shown in Table 1. These rules are defined
over coarse part-of-speech tags: Noun, Verb, Adjec-
tive, Adverb, Pronoun, Article, Auxiliary, Preposi-
tion, Numeral and Conjunction. Each rule specifies
a part-of-speech for the head and argument but does
not provide ordering information.
We require that a minimum proportion of the pos-
terior dependencies be instances of these rules in ex-
pectation. In contrast to prior work on rule-driven
dependency induction (Druck et al, 2009), where
each rule has a separately specified expectation, we
only set a single minimum expectation for the pro-
portion of all dependencies that must match one of
the rules. This setup is more relevant for learn-
ing with universals since individual rule frequencies
vary greatly between languages.
1239
1. Identify non-recursive NPs:
? All nouns, pronouns and possessive
marker are part of an NP.
? All adjectives, conjunctions and deter-
miners immediately preceding an NP
are part of the NP.
2. The first verb or modal in the sentence is the
headword.
3. All words in an NP are headed by the last
word in the NP.
4. The last word in an NP is headed by the
word immediately before the NP if it is a
preposition, otherwise it is headed by the
headword of the sentence if the NP is be-
fore the headword, else it is headed by the
word preceding the NP.
5. For the first word set its head to be the head-
word of the sentence. For each other word
set its headword to be the previous word.
Table 3: English-specific dependency rules.
English-specific Dependency Rules For English,
we also consider a small set of hand-crafted depen-
dency rules designed by Michael Collins3 for deter-
ministic parsing, shown in Table 3. Unlike the uni-
versals from Table 1, these rules alone are enough to
construct a full dependency tree. Thus they allow us
to judge whether the model is able to improve upon
a human-engineered deterministic parser. Moreover,
with this dataset we can assess the additional benefit
of using rules tailored to an individual language as
opposed to universal rules.
6 Experimental Setup
Datasets and Evaluation We test the effective-
ness of our grammar induction approach on English,
Danish, Portuguese, Slovene, Spanish, and Swedish.
For English we use the Penn Treebank (Marcus et
al., 1993), transformed from CFG parses into depen-
3Personal communication.
dencies with the Collins head finding rules (Collins,
1999); for the other languages we use data from the
2006 CoNLL-X Shared Task (Buchholz and Marsi,
2006). Each dataset provides manually annotated
part-of-speech tags that are used for both training
and testing. For comparison purposes with previ-
ous work, we limit the cross-lingual experiments to
sentences of length 10 or less (not counting punc-
tuation). For English, we also explore sentences of
length up to 20.
The final output metric is directed dependency ac-
curacy. This is computed based on the Viterbi parses
produced using the final unnormalized variational
distribution q(z) over dependency structures.
Hyperparameters and Training Regimes Un-
less otherwise stated, in experiments with rule-based
constraints the expected proportion of dependencies
that must satisfy those constraints is set to 0.8. This
threshold value was chosen based on minimal tun-
ing on a single language and ruleset (English with
universal rules) and carried over to each other ex-
perimental condition. A more detailed discussion of
the threshold?s empirical impact is presented in Sec-
tion 7.1.
Variational approximations to the HDP are trun-
cated at 10. All hyperparameter values are fixed to 1
except ? which is fixed to 10.
We also conduct a set of No-Split experiments to
evaluate the importance of syntactic refinement; in
these experiments each coarse symbol corresponds
to only one refined symbol. This is easily effected
during inference by setting the HDP variational ap-
proximation truncation level to one.
For each experiment we run 50 iterations of vari-
ational updates; for each iteration we perform five
steps of gradient search to compute the update for
the variational distribution q(z) over dependency
structures.
7 Results
In the following section we present our primary
cross-lingual results using universal rules (Sec-
tion 7.1) before performing a more in-depth analysis
of model properties such as sensitivity to ruleset se-
lection and inference stability (Section 7.2).
1240
DMV PGI No-Split HDP-DEP
English 47.1 62.3 71.5 71.9 (0.3)
Danish 33.5 41.6 48.8 51.9 (1.6)
Portuguese 38.5 63.0 54.0 71.5 (0.5)
Slovene 38.5 48.4 50.6 50.9 (5.5)
Spanish 28.0 58.4 64.8 67.2 (0.4)
Swedish 45.3 58.3 63.3 62.1 (0.5)
Table 4: Directed dependency accuracy using our model
with universal dependency rules (No-Split and HDP-
DEP), compared to DMV (Klein andManning, 2004) and
PGI (Berg-Kirkpatrick and Klein, 2010). The DMV re-
sults are taken from Berg-Kirkpatrick and Klein (2010).
Bold numbers indicate the best result for each language.
For the full model, the standard deviation in performance
over five runs is indicated in parentheses.
7.1 Main Cross-Lingual Results
Table 4 shows the performance of both our full
model (HDP-DEP) and its No-Split version using
universal dependency rules across six languages.
We also provide the performance of two baselines?
the dependency model with valence (DMV) (Klein
and Manning, 2004) and the phylogenetic grammar
induction (PGI) model (Berg-Kirkpatrick and Klein,
2010).
HDP-DEP outperforms both DMV and PGI
across all six languages. Against DMV we achieve
an average absolute improvement of 24.1%. This
improvement is expected given that DMV does not
have access to the additional information provided
through the universal rules. PGI is more relevant
as a point of comparison, since it is able to lever-
age multilingual data to learn information similar to
what we have declaratively specified using universal
rules. Specifically, PGI reduces induction ambigu-
ity by connecting language-specific parameters via
phylogenetic priors. We find, however, that we out-
perform PGI by an average margin of 7.2%, demon-
strating the benefits of explicit rule specification.
An additional point of comparison is the lexi-
calized unsupervised parser of Headden III et al
(2009), which yields the current state-of-the-art un-
supervised accuracy on English at 68.8%. Our
method also outperforms this approach, without em-
ploying lexicalization and sophisticated smoothing
as they do. This result suggests that combining the
complementary strengths of their approach and ours
English
Rule Excluded Acc Loss Gold Freq
Preposition ? Noun 61.0 10.9 5.1
Verb ? Noun 61.4 10.5 14.8
Noun ? Noun 64.4 7.5 10.7
Noun ? Article 64.7 7.2 8.5
Spanish
Rule Excluded Acc Loss Gold Freq
Preposition ? Noun 53.4 13.8 8.2
Verb ? Noun 61.9 5.4 12.9
Noun ? Noun 62.6 4.7 2.0
Root ? Verb 65.4 1.8 12.3
Table 5: Ablation experiment results for universal depen-
dency rules on English and Spanish. For each rule, we
evaluate the model using the ruleset excluding that rule,
and list the most significant rules for each language. The
second last column is the absolute loss in performance
compared to the setting where all rules are available. The
last column shows the percentage of the gold dependen-
cies that satisfy the rule.
can yield further performance improvements.
Table 4 also shows the No-Split results where syn-
tactic categories are not refined. We find that such
refinement usually proves to be beneficial, yielding
an average performance gain of 3.7%. However, we
note that the impact of incorporating splitting varies
significantly across languages. Further understand-
ing of this connection is an area of future research.
Finally, we note that our model exhibits low vari-
ance for most languages. This result attests to how
the expectation constraints consistently guide infer-
ence toward high-accuracy areas of the search space.
Ablation Analysis Our next experiment seeks to
understand the relative importance of the various
universal rules from Table 1. We study how accu-
racy is affected when each of the rules is removed
one at a time for English and Spanish. Table 5 lists
the rules with the greatest impact on performance
when removed. We note the high overlap between
the most significant rules for English and Spanish.
We also observe that the relationship between
a rule?s frequency and its importance for high ac-
curacy is not straightforward. For example, the
?Preposition ? Noun? rule, whose removal de-
grades accuracy the most for both English and Span-
1241
50?
55?
60?
65?
70?
75?
Gold? 70? 75? 80? 85? 90?
Ac
cu
rac
y?
Constraints?Threshold?
Average? English?
Figure 2: Accuracy of our model with different threshold
settings, on English only and averaged over all languages.
?Gold? refers to the setting where each language?s thresh-
old is set independently to the proportion of gold depen-
dencies satisfying the rules ? for English this proportion
is 70%, while the average proportion across languages is
63%.
ish, is not the most frequent rule in either language.
This result suggests that some rules are harder to
learn than others regardless of their frequency, so
their presence in the specified ruleset yields stronger
performance gains.
Varying the Constraint Threshold In our main
experiments we require that at least 80% of the ex-
pected dependencies satisfy the rule constraints. We
arrived at this threshold by tuning on the basis of En-
glish only. As shown in Figure 2, for English a broad
band of threshold values from 75% to 90% yields re-
sults within 2.5% of each other, with a slight peak at
80%.
To further study the sensitivity of our method to
how the threshold is set, we perform post hoc ex-
periments with other threshold values on each of the
other languages. As Figure 2 also shows, on average
a value of 80% is optimal across languages, though
again accuracy is stable within 2.5% between thresh-
olds of 75% to 90%. These results demonstrate that
a single threshold is broadly applicable across lan-
guages.
Interestingly, setting the threshold value indepen-
dently for each language to its ?true? proportion
based on the gold dependencies (denoted as the
?Gold? case in Figure 2) does not achieve optimal
Length
? 10 ? 20
Universal Dependency Rules
1 HDP-DEP 71.9 50.4
No Rules (Random Init)
2 HDP-DEP 24.9 24.4
3 Headden III et al (2009) 68.8 -
English-Specific Parsing Rules
4 Deterministic (rules only) 70.0 62.6
5 HDP-DEP 73.8 66.1
Druck et al (2009) Rules
6 Druck et al (2009) 61.3 -
7 HDP-DEP 64.9 42.2
Table 6: Directed accuracy of our model (HDP-DEP) on
sentences of length 10 or less and 20 or less from WSJ
with different rulesets and with no rules, along with vari-
ous baselines from the literature. Entries in this table are
numbered for ease of reference in the text.
performance. Thus, knowledge of the true language-
specific rule proportions is not necessary for high
accuracy.
7.2 Analysis of Model Properties
We perform a set of additional experiments on En-
glish to gain further insight into HDP-DEP?s behav-
ior. Our choice of language is motivated by the
fact that a wide range of prior parsing algorithms
were developed for and tested exclusively on En-
glish. The experiments below demonstrate that 1)
universal rules alone are powerful, but language-
and dataset-tailored rules can further improve per-
formance; 2) our model learns jointly from the
rules and data, outperforming a rules-only deter-
ministic parser; 3) the way we incorporate posterior
constraints outperforms the generalized expectation
constraint framework; and 4) our model exhibits low
variance when seeded with different initializations.
These results are summarized in Table 6 and dis-
cussed in detail below; line numbers refer to entries
in Table 6. Each run of HDP-DEP below is with
syntactic refinement enabled.
Impact of Rules Selection We compare the per-
formance of HDP-DEP using the universal rules ver-
sus a set of rules designed for deterministically pars-
ing the Penn Treebank (see Section 5 for details).
1242
As lines 1 and 5 of Table 6 show, language-specific
rules yield better performance. For sentences of
length 10 or less, the difference between the two
rulesets is a relatively small 1.9%; for longer sen-
tences, however, the difference is a substantially
larger 15.7%. This is likely because longer sen-
tences tend to be more complex and thus exhibit
more language-idiosyncratic dependencies. Such
dependencies can be better captured by the refined
language-specific rules.
We also test model performance when no linguis-
tic rules are available, i.e., performing unconstrained
variational inference. The model performs substan-
tially worse (line 2), confirming that syntactic cat-
egory refinement in a fully unsupervised setup is
challenging.
Learning Beyond Provided Rules Since HDP-
DEP is provided with linguistic rules, a legitimate
question is whether it improves upon what the rules
encode, especially when the rules are complete and
language-specific. We can answer this question by
comparing the performance of our model seeded
with the English-specific rules against a determin-
istic parser that implements the same rules. Lines
4 and 5 of Table 6 demonstrate that the model out-
performs a rules-only deterministic parser by 3.8%
for sentences of length 10 or less and by 3.5% for
sentences of length 20 or less.
Comparison with Alternative Semi-supervised
Parser The dependency parser based on the gen-
eralized expectation criteria (Druck et al, 2009) is
the closest to our reported work in terms of tech-
nique. To compare the two, we run HDP-DEP using
the 20 rules given by Druck et al (2009). Our model
achieves an accuracy of 64.9% (line 7) compared to
61.3% (line 6) reported in their work. Note that we
do not rely on rule-specific expectation information
as they do, instead requiring only a single expecta-
tion constraint parameter.4
Model Stability It is commonly acknowledged
in the literature that unsupervised grammar induc-
tion methods exhibit sensitivity to initialization.
As in the previous section, we find that the pres-
4As explained in Section 5, having a single expectation pa-
rameter is motivated by our focus on parsing with universal
rules.
ence of linguistic rules greatly reduces this sensitiv-
ity: for HDP-DEP, the standard deviation over five
randomly initialized runs with the English-specific
rules is 1.5%, compared to 4.5% for the parser de-
veloped by Headden III et al (2009) and 8.0% for
DMV (Klein and Manning, 2004).
8 Conclusions
In this paper we demonstrated that syntactic uni-
versals encoded as declarative constraints improve
grammar induction. We formulated a generative
model for dependency structure that models syntac-
tic category refinement and biases inference to co-
here with the provided constraints. Our experiments
showed that encoding a compact, well-accepted set
of language-independent constraints significantly
improves accuracy on multiple languages compared
to the current state-of-the-art in unsupervised pars-
ing.
While our present work has yielded substantial
gains over previous unsupervised methods, a large
gap still remains between our method and fully su-
pervised techniques. In future work we intend to
study ways to bridge this gap by 1) incorporat-
ing more sophisticated linguistically-driven gram-
mar rulesets to guide induction, 2) lexicalizing the
model, and 3) combining our constraint-based ap-
proach with richer unsupervised models (e.g., Head-
den III et al (2009)) to benefit from their comple-
mentary strengths.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0904684,
and a Graduate Research Fellowship). We are es-
pecially grateful to Michael Collins for inspiring us
toward this line of inquiry and providing determin-
istic rules for English parsing. Thanks to Taylor
Berg-Kirkpatrick, Sabine Iatridou, Ramesh Sridha-
ran, and members of the MIT NLP group for their
suggestions and comments. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors, and do not necessar-
ily reflect the views of the funding organizations.
1243
References
Mark C. Baker. 2001. The Atoms of Language: The
Mind?s Hidden Rules of Grammar. Basic Books.
Emily M. Bender. 2009. Linguistically na??ve != lan-
guage independent: Why NLP needs linguistic typol-
ogy. In Proceedings of the EACL 2009 Workshop
on the Interaction between Linguistics and Compu-
tational Linguistics: Virtuous, Vicious or Vacuous?,
pages 26?32.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic grammar induction. In Proceedings of ACL,
pages 1288?1297.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Information Science and Statis-
tics. Springer.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Andrew Carnie. 2002. Syntax: A Generative Introduc-
tion (Introducing Linguistics). Blackwell Publishing.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of ACL, pages 280?
287.
Shay B. Cohen and Noah A. Smith. 2009a. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL/HLT, pages 74?82.
Shay B. Cohen and Noah A. Smith. 2009b. Variational
inference for grammar induction with prior knowl-
edge. In Proceedings of ACL/IJCNLP 2009 Confer-
ence Short Papers, pages 1?4.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Hal Daume? III and Lyle Campbell. 2007. A bayesian
model for discovering typological implications. In
Proceedings of ACL, pages 65?72.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Pro-
ceedings of ACL/IJCNLP, pages 360?368.
Thomas S. Ferguson. 1973. A bayesian analysis of
some nonparametric problems. Annals of Statistics,
1(2):209?230.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL, pages 272?279.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL/IJCNLP,
pages 369?377.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:2001?2049.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs. parameter sparsity in la-
tent variable models. In Advances in NIPS, pages 664?
672.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS, pages 569?576.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of ACL, pages
881?888.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of NAACL/HLT, pages 101?109.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 478?485.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of ACL, pages
470?477.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of EMNLP/CoNLL, pages
688?697.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009a.
Learning from measurements in exponential families.
In Proceedings of ICML, pages 641?648.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009b.
Probabilistic grammars and hierarchical Dirichlet pro-
cesses. The Handbook of Applied Bayesian Analysis.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Frederick J. Newmeyer. 2005. Possible and Probable
Languages: A Generative Perspective on Linguistic
Typology. Oxford University Press.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceeding of
AAAI, pages 1663?1666.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of ACL/IJCNLP, pages 73?81.
Lydia White. 2003. Second Language Acquisition and
Universal Grammar. Cambridge University Press.
1244
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 530?540,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
In-domain Relation Discovery with Meta-constraints
via Posterior Regularization
Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{harr, eob, tahira, regina} @csail.mit.edu
Abstract
We present a novel approach to discovering re-
lations and their instantiations from a collec-
tion of documents in a single domain. Our
approach learns relation types by exploiting
meta-constraints that characterize the general
qualities of a good relation in any domain.
These constraints state that instances of a
single relation should exhibit regularities at
multiple levels of linguistic structure, includ-
ing lexicography, syntax, and document-level
context. We capture these regularities via the
structure of our probabilistic model as well
as a set of declaratively-specified constraints
enforced during posterior inference. Across
two domains our approach successfully recov-
ers hidden relation structure, comparable to
or outperforming previous state-of-the-art ap-
proaches. Furthermore, we find that a small
set of constraints is applicable across the do-
mains, and that using domain-specific con-
straints can further improve performance. 1
1 Introduction
In this paper, we introduce a novel approach for the
unsupervised learning of relations and their instan-
tiations from a set of in-domain documents. Given
a collection of news articles about earthquakes, for
example, our method discovers relations such as the
earthquake?s location and resulting damage, and ex-
tracts phrases representing the relations? instantia-
tions. Clusters of similar in-domain documents are
1The source code for this work is available at:
http://groups.csail.mit.edu/rbg/code/relation extraction/
A strong earthquake rocked the Philippine island of Min-
doro early Tuesday, [destroying]ind [some homes]arg ...
A strong earthquake hit the China-Burma border early
Wednesday ... The official Xinhua News Agency said
[some houses]arg were [damaged]ind ...
A strong earthquake with a preliminary magnitude of 6.6
shook northwestern Greece on Saturday, ... [destroying]ind
[hundreds of old houses]arg ...
Figure 1: Excerpts from newswire articles about earth-
quakes. The indicator and argument words for the dam-
age relation are highlighted.
increasingly available in forms such as Wikipedia ar-
ticle categories, financial reports, and biographies.
In contrast to previous work, our approach learns
from domain-independent meta-constraints on rela-
tion expression, rather than supervision specific to
particular relations and their instances. In particular,
we leverage the linguistic intuition that documents
in a single domain exhibit regularities in how they
express their relations. These regularities occur both
in the relations? lexical and syntactic realizations as
well as at the level of document structure. For in-
stance, consider the damage relation excerpted from
earthquake articles in Figure 1. Lexically, we ob-
serve similar words in the instances and their con-
texts, such as ?destroying? and ?houses.? Syntacti-
cally, in two instances the relation instantiation is the
dependency child of the word ?destroying.? On the
discourse level, these instances appear toward the
beginning of their respective documents. In general,
valid relations in many domains are characterized by
these coherence properties.
We capture these regularities using a Bayesian
model where the underlying relations are repre-
530
sented as latent variables. The model takes as in-
put a constituent-parsed corpus and explains how the
constituents arise from the latent variables. Each re-
lation instantiation is encoded by the variables as
a relation-evoking indicator word (e.g., ?destroy-
ing?) and corresponding argument constituent (e.g.,
?some homes?).2 Our approach capitalizes on rela-
tion regularity in two ways. First, the model?s gen-
erative process encourages coherence in the local
features and placement of relation instances. Sec-
ond, we apply posterior regularization (Grac?a et
al., 2007) during inference to enforce higher-level
declarative constraints, such as requiring indicators
and arguments to be syntactically linked.
We evaluate our approach on two domains pre-
viously studied for high-level document structure
analysis, news articles about earthquakes and finan-
cial markets. Our results demonstrate that we can
successfully identify domain-relevant relations. We
also study the importance and effectiveness of the
declaratively-specified constraints. In particular, we
find that a small set of declarative constraints are
effective across domains, while additional domain-
specific constraints yield further benefits.
2 Related Work
Extraction with Reduced Supervision Recent
research in information extraction has taken large
steps toward reducing the need for labeled data. Ex-
amples include using bootstrapping to amplify small
seed sets of example outputs (Agichtein and Gra-
vano, 2000; Yangarber et al, 2000; Bunescu and
Mooney, 2007; Zhu et al, 2009), leveraging ex-
isting databases that overlap with the text (Mintz
et al, 2009; Yao et al, 2010), and learning gen-
eral domain-independent knowledge bases by ex-
ploiting redundancies in large web and news cor-
pora (Hasegawa et al, 2004; Shinyama and Sekine,
2006; Banko et al, 2007; Yates and Etzioni, 2009).
Our approach is distinct in both the supervision
and data we operate over. First, in contrast to boot-
strapping and database matching approaches, we
learn from meta-qualities, such as low variability in
syntactic patterns, that characterize a good relation.
2We do not use the word ?argument? in the syntactic sense?
a relation?s argument may or may not be the syntactic depen-
dency argument of its indicator.
We hypothesize that these properties hold across re-
lations in different domains. Second, in contrast to
work that builds general relation databases from het-
erogeneous corpora, our focus is on learning the re-
lations salient in a single domain. Our setup is more
germane to specialized domains expressing informa-
tion not broadly available on the web.
Earlier work in unsupervised information extrac-
tion has also leveraged meta-knowledge indepen-
dent of specific relation types, such as declaratively-
specified syntactic patterns (Riloff, 1996), frequent
dependency subtree patterns (Sudo et al, 2003), and
automatic clusterings of syntactic patterns (Lin and
Pantel, 2001; Zhang et al, 2005) and contexts (Chen
et al, 2005; Rosenfeld and Feldman, 2007). Our ap-
proach incorporates a broader range of constraints
and balances constraints with underlying patterns
learned from the data, thereby requiring more so-
phisticated machinery for modeling and inference.
Extraction with Constraints Previous work has
recognized the appeal of applying declarative con-
straints to extraction. In a supervised setting, Roth
and Yih (2004) induce relations by using linear pro-
gramming to impose global declarative constraints
on the output from a set of classifiers trained on lo-
cal features. Chang et al (2007) propose an objec-
tive function for semi-supervised extraction that bal-
ances likelihood of labeled instances and constraint
violation on unlabeled instances. Recent work has
also explored how certain kinds of supervision can
be formulated as constraints on model posteriors.
Such constraints are not declarative, but instead
based on annotations of words? majority relation la-
bels (Mann and McCallum, 2008) and pre-existing
databases with the desired output schema (Bellare
and McCallum, 2009). In contrast to previous work,
our approach explores a different class of constraints
that does not rely on supervision that is specific to
particular relation types and their instances.
3 Model
Our work performs in-domain relation discovery by
leveraging regularities in relation expression at the
lexical, syntactic, and discourse levels. These regu-
larities are captured via two components: a proba-
bilistic model that explains how documents are gen-
erated from latent relation variables and a technique
531
? ? ? ?
is_verb 0 1 0
earthquake 1 0 0
hit 0 1 0
? ? ? ?
has_proper 0 0 1
has_number 0 0 0
depth 1 3 2
Figure 2: Words w and constituents x of syntactic parses
are represented with indicator features ?i and argument
features ?a respectively. A single relation instantiation is
a pair of indicator w and argument x; we filter w to be
nouns and verbs and x to be noun phrases and adjectives.
for biasing inference to adhere to declaratively-
specified constraints on relation expression. This
section describes the generative process, while Sec-
tions 4 and 5 discuss declarative constraints.
3.1 Problem Formulation
Our input is a corpus of constituent-parsed docu-
ments and a number K of relation types. The output
is K clusters of semantically related relation instan-
tiations. We represent these instantiations as a pair
of indicator word and argument sequence from the
same sentence. The indicator?s role is to anchor a
relation and identify its type. We only allow nouns
or verbs to be indicators. For instance, in the earth-
quake domain a likely indicator for damage would
be ?destroyed.? The argument is the actual rela-
tion value, e.g., ?some homes,? and corresponds to
a noun phrase or adjective.3
Along with the document parse trees, we utilize
a set of features ?i(w) and ?a(x) describing each
potential indicator word w and argument constituent
x, respectively. An example feature representation
is shown in Figure 2. These features can encode
words, part-of-speech tags, context, and so on. Indi-
cator and argument feature definitions need not be
the same (e.g., has number is important for argu-
3In this paper we focus on unary relations; binary relations
can be modeled with extensions of the hidden variables and con-
straints.
ments but irrelevant for indicators).4
3.2 Generative Process
Our model associates each relation type k with a set
of feature distributions ?k and a location distribution
?k. Each instantiation?s indicator and argument, and
its position within a document, are drawn from these
distributions. By sharing distributions within each
relation, the model places high probability mass on
clusters of instantiations that are coherent in features
and position. Furthermore, we allow at most one in-
stantiation per document and relation, so as to target
relations that are relevant to the entire document.
There are three steps to the generative process.
First, we draw feature and location distributions for
each relation. Second, an instantiation is selected
for every pair of document d and relation k. Third,
the indicator features of each word and argument
features of each constituent are generated based on
the relation parameters and instantiations. Figure 3
presents a reference for the generative process.
Generating Relation Parameters Each relation k
is associated with four feature distribution param-
eter vectors: ?ik for indicator words, ?
bi
k for non-
indicator words, ?ak for argument constituents, and
?bak for non-argument constituents. Each of these is
a set of multinomial parameters per feature drawn
from a symmetric Dirichlet prior. A likely indica-
tor word should have features that are highly proba-
ble according to ?ik, and likewise for arguments and
?ak. Parameters ?
bi
k and ?
ba
k represent background dis-
tributions for non-relation words and constituents,
similar in spirit to other uses of background distri-
butions that filter out irrelevant words (Che, 2006).5
By drawing each instance from these distributions,
we encourage the relation to be coherent in local lex-
ical and syntactic properties.
Each relation type k is also associated with a pa-
rameter vector ?k over document segments drawn
from a symmetric Dirichlet prior. Documents are
divided into L equal-length segments; ?k states how
likely relation k is for each segment, with one null
outcome for the relation not occurring in the doc-
ument. Because ?k is shared within a relation, its
4We consider only categorical features here, though the ex-
tension to continuous or ordinal features is straightforward.
5We use separate background distributions for each relation
to make inference more tractable.
532
For each relation type k:
? For each indicator feature ?i draw feature distri-
butions ?ik,?i , ?
bi
k,?i ? Dir(?0)
? For each argument feature ?a draw feature dis-
tributions ?ak,?a , ?
ba
k,?a ? Dir(?0)
? Draw location distribution ?k ? Dir(?0)
For each relation type k and document d:
? Select document segment sd,k ? Mult(?k)
? Select sentence zd,k uniformly from segment
sd,k, and indicator id,k and argument ad,k uni-
formly from sentence zd,k
For each word w in every document d:
? Draw each indicator feature ?i(w) ?
Mult
(
1
Z
?K
k=1 ?k,?i
)
, where ?k,?i is ?
i
k,?i
if id,k = w and ?bik,?i otherwise
For each constituent x in every document d:
? Draw each argument feature ?a(x) ?
Mult
(
1
Z
?K
k=1 ?k,?a
)
, where ?k,?a is ?ak,?a
if ad,k = x and ?bak,?a otherwise
Figure 3: The generative process for model parameters
and features. In the above Dir and Mult refer respectively
to the Dirichlet distribution and multinomial distribution.
Fixed hyperparameters are subscripted with zero.
instances will tend to occur in the same relative po-
sitions across documents. The model can learn, for
example, that a particular relation typically occurs in
the first quarter of a document (if L = 4).
Generating Relation Instantiations For every rela-
tion type k and document d, we first choose which
portion of the document (if any) contains the instan-
tiation by drawing a document segment sd,k from
?k. Our model only draws one instantiation per pair
of k and d, so each discovered instantiation within a
document is a separate relation. We then choose the
specific sentence zd,k uniformly from within the seg-
ment, and the indicator word id,k and argument con-
stituent ad,k uniformly from within that sentence.
Generating Text Finally, we draw the feature val-
ues. We make a Na??ve Bayes assumption between
features, drawing each independently conditioned
on relation structure. For a word w, we want all re-
lations to be able to influence its generation. Toward
this end, we compute the element-wise product of
feature parameters across relations k = 1, . . . ,K,
using indicator parameters ?ik if relation k selected
w as an indicator word (if id,k = w) and background
parameters ?bik otherwise. The result is then normal-
ized to form a valid multinomial that produces word
w?s features. Constituents are drawn similarly from
every relations? argument distributions.
4 Inference with Constraints
The model presented above leverages relation reg-
ularities in local features and document placement.
However, it is unable to specify global syntactic
preferences about relation expression, such as indi-
cators and arguments being in the same clause. An-
other issue with this model is that different relations
could overlap in their indicators and arguments.6
To overcome these obstacles, we apply declara-
tive constraints by imposing inequality constraints
on expectations of the posterior during inference
using posterior regularization (Grac?a et al, 2007).
In this section we present the technical details
of the approach; Section 5 explains the specific
linguistically-motivated constraints we consider.
4.1 Inference with Posterior Regularization
We first review how posterior regularization impacts
the variational inference procedure in general. Let
?, z, and x denote the parameters, hidden struc-
ture, and observations of an arbitrary model. We
are interested in estimating the posterior distribution
p(?, z | x) by finding a distribution q(?, z) ? Q that
is minimal in KL-divergence to the true posterior:
KL(q(?, z) ? p(?, z | x))
=
?
q(?, z) log
q(?, z)
p(?, z, x)
d?dz + log p(x). (1)
For tractability, variational inference typically
makes a mean-field assumption that restricts the set
Q to distributions where ? and z are independent,
i.e., q(?, z) = q(?)q(z). We then optimize equa-
tion 1 by coordinate-wise descent on q(?) and q(z).
To incorporate constraints into inference, we fur-
ther restrict Q to distributions that satisfy a given
6In fact, a true maximum a posteriori estimate of the model
parameters would find the same most salient relation over and
over again for every k, rather than finding K different relations.
533
set of inequality constraints, each of the form
Eq[f(z)] ? b. Here, f(z) is a deterministic func-
tion of z and b is a user-specified threshold. Inequal-
ities in the opposite direction simply require negat-
ing f(z) and b. For example, we could apply a syn-
tactic constraint of the form Eq[f(z)] ? b, where
f(z) counts the number of indicator/argument pairs
that are syntactically connected in a pre-specified
manner (e.g., the indicator and argument modify the
same verb), and b is a fixed threshold.
Given a set C of constraints with functions fc(z)
and thresholds bc, the updates for q(?) and q(z) from
equation 1 are as follows:
q(?) = argmin
q(?)
KL
(
q(?) ? q?(?)
)
, (2)
where q?(?) ? expEq(z)[log p(?, z, x)], and
q(z) = argmin
q(z)
KL
(
q(z) ? q?(z)
)
s.t. Eq(z)[fc(z)] ? bc, ?c ? C, (3)
where q?(z) ? expEq(?)[log p(?, z, x)]. Equation 2
is not affected by the posterior constraints and is up-
dated by setting q(?) to q?(?). We solve equation 3
in its dual form (Grac?a et al, 2007):
argmin
?
?
c?C
?cbc + log
?
z
q?(z)e?
P
c?C ?cfc(z)
s.t. ?c ? 0, ?c ? C. (4)
With the box constraints of equation 4, a numerical
optimization procedure such as L-BFGS-B (Byrd
et al, 1995) can be used to find optimal dual pa-
rameters ??. The original q(z) is then updated to
q?(z) exp
(
?
?
c?C ?
?
cfc(z)
)
and renormalized.
4.2 Updates for our Model
Our model uses this mean-field factorization:
q(?, ?, z, a, i)
=
K?
k=1
q(?k; ??k)q(?
i
k; ??
i
k)q(?
bi
k ; ??
bi
k )q(?
a
k; ??
a
k)q(?
ba
k ; ??
ba
k )
?
?
d
q(zd,k, ad,k, id,k; c?d,k) (5)
In the above, ?? and ?? are Dirichlet distribution pa-
rameters, and c? are multinomial parameters. Note
that we do not factorize the distribution of z, i, and
a for a single document and relation, instead repre-
senting their joint distribution with a single set of
variational parameters c?. This is tractable because a
single relation occurs only once per document, re-
ducing the joint search space of z, i, and a. The
factors in equation 5 are updated one at a time while
holding the other factors fixed.
Updating ?? Due to the Na??ve Bayes assumption
between features, each feature?s q(?) distributions
can be updated separately. However, the product
between feature parameters of different relations in-
troduces a nonconjugacy in the model, precluding
a closed form update. Instead we numerically opti-
mize equation 1 with respect to each ??, similarly to
previous work (Boyd-Graber and Blei, 2008). For
instance, ??ik,? of relation k and feature ? is updated
by finding the gradient of equation 1 with respect to
??ik,? and applying L-BFGS. Parameters ??
bi, ??a, and
??ba are updated analogously.
Updating ?? This update follows the standard
closed form for Dirichlet parameters:
??k,` = ?0 + Eq(z,a,i)[C`(z, a, i)], (6)
whereC` counts the number of times z falls into seg-
ment ` of a document.
Updating c? Parameters c? are updated by first com-
puting an unconstrained update q?(z, a, i; c??):
c??d,k,(z,a,i) ? exp
?
?Eq(?k)[log p(z, a, i | ?k)]
+ Eq(?ik)[log p(i | ?
i
k)] +
?
w 6=i
Eq(?bik )[log p(w | ?
bi
k )]
+ Eq(?ak)[log p(a | ?
a
k)] +
?
x 6=a
Eq(?bak )[log p(x | ?
ba
k )]
?
?
We then perform the minimization on the dual in
equation 4 under the provided constraints to derive a
final update to the constrained c?.
Simplifying Approximation The update for ?? re-
quires numerical optimization due to the nonconju-
gacy introduced by the point-wise product in fea-
ture generation. If instead we have every relation
type separately generate a copy of the corpus, the ??
534
Quantity f(z, a, i) ? or ? b
Syntax ?k Counts i, a of relation k that match a pattern (see text) ? 0.8D
Prevalence ?k Counts instantiations of relation k ? 0.8D
Separation (ind) ?w Counts times w selected as i ? 2
Separation (arg) ?w Counts times w selected as part of a ? 1
Table 1: Each constraint takes the form Eq[f(z, a, i)] ? b or Eq[f(z, a, i)] ? b; D denotes the number of corpus
documents, ?k means one constraint per relation type, and ?w means one constraint per token in the corpus.
updates becomes closed-form expressions similar to
equation 6. This approximation yields similar pa-
rameter estimates as the true updates while vastly
improving speed, so we use it in our experiments.
5 Declarative Constraints
We now have the machinery to incorporate a va-
riety of declarative constraints during inference.
The classes of domain-independent constraints we
study are summarized in Table 1. For the propor-
tion constraints we arbitrarily select a threshold of
80% without any tuning, in the spirit of building a
domain-independent approach.
Syntax As previous work has observed, most rela-
tions are expressed using a limited number of com-
mon syntactic patterns (Riloff, 1996; Banko and Et-
zioni, 2008). Our syntactic constraint captures this
insight by requiring that a certain proportion of the
induced instantiations for each relation match one of
these syntactic patterns:
? The indicator is a verb and the argument?s
headword is either the child or grandchild of
the indicator word in the dependency tree.
? The indicator is a noun and the argument is a
modifier or complement.
? The indicator is a noun in a verb?s subject and
the argument is in the corresponding object.
Prevalence For a relation to be domain-relevant, it
should occur in numerous documents across the cor-
pus, so we institute a constraint on the number of
times a relation is instantiated. Note that the effect
of this constraint could also be achieved by tuning
the prior probability of a relation not occurring in a
document. However, this prior would need to be ad-
justed every time the number of documents or fea-
ture selection changes; using a constraint is an ap-
pealing alternative that is portable across domains.
Separation The separation constraint encourages
diversity in the discovered relation types by restrict-
ing the number of times a single word can serve as
either an indicator or part of the argument of a re-
lation instance. Specifically, we require that every
token of the corpus occurs at most once as a word
in a relation?s argument in expectation. On the other
hand, a single word can sometimes be evocative of
multiple relations (e.g., ?occurred? signals both date
and time in ?occurred on Friday at 3pm?). Thus, we
allow each word to serve as an indicator more than
once, arbitrarily fixing the limit at two.
6 Experimental Setup
Datasets and Metrics We evaluate on two datasets,
financial market reports and newswire articles about
earthquakes, previously used in work on high-level
content analysis (Barzilay and Lee, 2004; Lap-
ata, 2006). Finance articles chronicle daily mar-
ket movements of currencies and stock indexes, and
earthquake articles document specific earthquakes.
Constituent parses are obtained automatically us-
ing the Stanford parser (Klein and Manning, 2003)
and then converted to dependency parses using the
PennConvertor tool (Johansson and Nugues, 2007).
We manually annotated relations for both corpora,
selecting relation types that occurred frequently in
each domain. We found 15 types for finance and
9 for earthquake. Corpus statistics are summarized
below, and example relation types are shown in Ta-
ble 2.
Docs Sent/Doc Tok/Doc Vocab
Finance 100 12.1 262.9 2918
Earthquake 200 9.3 210.3 3155
In our task, annotation conventions for desired
output relations can greatly impact token-level per-
formance, and the model cannot learn to fit a par-
ticular convention by looking at example data. For
example, earthquakes times are frequently reported
in both local and GMT, and either may be arbitrar-
ily chosen as correct. Moreover, the baseline we
535
F
in
an
ce Bond 104.58 yen, 98.37 yen
Dollar Change up 0.52 yen, down 0.01 yen
Tokyo Index Change down 5.38 points or 0.41 percent, up 0.16 points, insignificant in percentage terms
E
ar
th
qu
ak
e Damage about 10000 homes, some buildings, no information
Epicenter
Patuca about 185 miles (300 kilometers) south of Quito, 110 kilometers (65 miles)
from shore under the surface of the Flores sea in the Indonesian archipelago
Magnitude 5.7, 6, magnitude-4
Table 2: Example relation types identified in the finance and earthquake datasets with example instance arguments.
compare against produces lambda calculus formulas
rather than spans of text as output, so a token-level
comparison requires transforming its output.
For these reasons, we evaluate on both sentence-
level and token-level precision, recall, and F-score.
Precision is measured by mapping every induced re-
lation cluster to its closest gold relation and comput-
ing the proportion of predicted sentences or words
that are correct. Conversely, for recall we map ev-
ery gold relation to its closest predicted relation and
find the proportion of gold sentences or words that
are predicted. This mapping technique is based on
the many-to-one scheme used for evaluating unsu-
pervised part-of-speech induction (Johnson, 2007).
Note that sentence-level scores are always at least as
high as token-level scores, since it is possible to se-
lect a sentence correctly but none of its true relation
tokens while the opposite is not possible.
Domain-specific Constraints On top of the cross-
domain constraints from Section 5, we study
whether imposing basic domain-specific constraints
can be beneficial. The finance dataset is heav-
ily quantitative, so we consider applying a single
domain-specific constraint stating that most rela-
tion arguments should include a number. Likewise,
earthquake articles are typically written with a ma-
jority of the relevant information toward the begin-
ning of the document, so its domain-specific con-
straint is that most relations should occur in the
first two sentences of a document. Note that these
domain-specific constraints are not specific to in-
dividual relations or instances, but rather encode a
preference across all relation types. In both cases,
we again use an 80% threshold without tuning.
Features For indicators, we use the word, part of
speech, and word stem. For arguments, we use the
word, syntactic constituent label, the head word of
the parent constituent, and the dependency label of
the argument to its parent.
Baselines We compare against three alternative un-
supervised approaches. Note that the first two only
identify relation-bearing sentences, not the specific
words that participate in the relation.
Clustering (CLUTO): A straightforward way of
identifying sentences bearing the same relation is
to simply cluster them. We implement a cluster-
ing baseline using the CLUTO toolkit with word and
part-of-speech features. As with our model, we set
the number of clusters K to the true number of rela-
tion types.
Mallows Topic Model (MTM): Another technique
for grouping similar sentences is the Mallows-based
topic model of Chen et al (2009). The datasets we
consider here exhibit high-level regularities in con-
tent organization, so we expect that a topic model
with global constraints could identify plausible clus-
ters of relation-bearing sentences. Again, K is set to
the true number of relation types.
Unsupervised Semantic Parsing (USP): Our fi-
nal unsupervised comparison is to USP, an unsuper-
vised deep semantic parser introduced by Poon and
Domingos (2009). USP induces a lambda calculus
representation of an entire corpus and was shown to
be competitive with open information extraction ap-
proaches (Lin and Pantel, 2001; Banko et al, 2007).
We give USP the required Stanford dependency for-
mat as input (de Marneffe and Manning, 2008). We
find that the results are sensitive to the cluster granu-
larity prior, so we tune this parameter and report the
best-performing runs.
We recognize that USP targets a different out-
put representation than ours: a hierarchical semantic
structure over the entirety of a dependency-parsed
text. In contrast, we focus on discovering a limited
numberK of domain-relevant relations expressed as
constituent phrases. Despite these differences, both
536
methods ultimately aim to capture domain-specific
relations expressed with varying verbalizations, and
both operate over in-domain input corpora supple-
mented with syntactic information. For these rea-
sons, USP provides a clear and valuable point of
comparison. For this comparison, we transform
USP?s lambda calculus formulas to relation spans as
follows. First, we group lambda forms by a combi-
nation of core form, argument form, and the parent?s
core form.7 We then filter to the K relations that
appear in the most documents. For token-level eval-
uation we take the dependency tree fragment corre-
sponding to the lambda form. For example, in the
sentence ?a strong earthquake rocked the Philippines
island of Mindoro early Tuesday,? USP learns that
the word ?Tuesday? has a core form corresponding
to words {Tuesday, Wednesday, Saturday}, a parent
form corresponding to words {shook, rock, hit, jolt},
and an argument form of TMOD; all phrases with
this same combination are grouped as a relation.
Training Regimes and Hyperparameters For each
run of our model we perform three random restarts
to convergence and select the posterior with lowest
final free energy. We fix K to the true number of
annotated relation types for both our model and USP
and L (the number of document segments) to five.
Dirichlet hyperparameters are set to 0.1.
7 Results
Table 3?s first two sections present the results of our
main evaluation. For earthquake, the far more diffi-
cult domain, our base model with only the domain-
independent constraints strongly outperforms all
three baselines across both metrics. For finance,
the CLUTO and USP baselines achieve performance
comparable to or slightly better than our base model.
Our approach, however, has the advantage of provid-
ing a formalism for seamlessly incorporating addi-
tional arbitrary domain-specific constraints. When
we add such constraints (denoted as model+DSC),
we achieve consistently higher performance than all
baselines across both datasets and metrics, demon-
strating that this approach provides a simple and ef-
fective framework for injecting domain knowledge
into relation discovery.
7This grouping mechanism yields better results than only
grouping by core form.
The first two baselines correspond to a setup
where the number of sentence clusters K is set to
the true number of relation types. This has the effect
of lowering precision because each sentence must be
assigned a cluster. To mitigate this impact, we exper-
imented with using K+N clusters, with N ranging
from 1 to 30. In each case, we then keep only the K
largest clusters. For the earthquake dataset, increas-
ing N improves performance until some point, after
which performance degrades. However, the best F-
Score corresponding to the optimal number of clus-
ters is 42.2, still far below our model?s 66.0 F-score.
For the finance domain, increasing the number of
clusters hurts performance.
Our results show a large gap in F-score between
the sentence and token-level evaluations for both the
USP baseline and our model. A qualitative analysis
of the results indicates that our model often picks up
on regularities that are difficult to distinguish with-
out relation-specific supervision. For earthquake, a
location may be annotated as ?the Philippine island
of Mindoro? while we predict just the word ?Min-
doro.? For finance, an index change can be anno-
tated as ?30 points, or 0.8 percent,? while our model
identifies ?30 points? and ?0.8 percent? as separate
relations. In practice, these outputs are all plausi-
ble discoveries, and a practitioner desiring specific
outputs could impose additional constraints to guide
relation discovery toward them.
The Impact of Constraints To understand the im-
pact of the declarative constraints, we perform an
ablation analysis on the constraint sets. We con-
sider removing the constraints on syntactic patterns
(no-syn) and the constraints disallowing relations to
overlap (no-sep) from the full domain-independent
model.8 We also try a version with hard syntac-
tic constraints (hard-syn), which requires that every
extraction match one of the three syntactic patterns
specified by the syntactic constraint.
Table 3?s bottom section presents the results of
this evaluation. The model?s performance degrades
when either of the two constraint sets are removed,
demonstrating that the constraints are in fact benefi-
cial for relation discovery. Additionally, in the hard-
syn case, performance drops dramatically for finance
8Prevalence constraints are always enforced, as otherwise
the prior on not instantiating a relation would need to be tuned.
537
Finance Earthquake
Sentence-level Token-level Sentence-level Token-level
Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Model 82.1 59.7 69.2 42.2 23.9 30.5 54.2 68.1 60.4 20.2 16.8 18.3
Model+DSC 87.3 81.6 84.4 51.8 30.0 38.0 66.4 65.6 66.0 22.6 23.1 22.8
CLUTO 56.3 92.7 70.0 ? ? ? 19.8 58.0 29.5 ? ? ?
MTM 40.4 99.3 57.5 ? ? ? 18.6 74.6 29.7 ? ? ?
USP 91.3 66.1 76.7 28.5 32.6 30.4 61.2 43.5 50.8 9.9 32.3 15.1
No-sep 97.8 35.4 52.0 86.1 8.7 15.9 42.2 21.9 28.8 16.1 4.6 7.1
No-syn 83.3 46.1 59.3 20.8 9.9 13.4 53.8 60.9 57.1 14.0 13.8 13.9
Hard-syn 47.7 39.0 42.9 11.6 7.0 8.7 55.0 66.2 60.1 20.1 17.3 18.6
Table 3: Top section: our model, with and without domain-specific constraints (DSC). Middle section: The three
baselines. Bottom section: ablation analysis of constraint sets for our model. For all scores, higher is better.
while remaining almost unchanged for earthquake.
This suggests that formulating constraints as soft in-
equalities on posterior expectations gives our model
the flexibility to accommodate both the underlying
signal in the data and the declarative constraints.
Comparison against Supervised CRF Our final
set of experiments compares a semi-supervised ver-
sion of our model against a conditional random field
(CRF) model. The CRF model was trained using
the same features as our model?s argument features.
To incorporate training examples in our model, we
simply treat annotated relation instances as observed
variables. For both the baselines and our model,
we experiment with using up to 10 annotated docu-
ments. At each of those levels of supervision, we av-
erage results over 10 randomly drawn training sets.
At the sentence level, our model compares very
favorably to the supervised CRF. For finance, it takes
at least 10 annotated documents (corresponding to
roughly 130 annotated relation instances) for the
CRF to match the semi-supervised model?s perfor-
mance. For earthquake, using even 10 annotated
documents (about 71 relation instances) is not suf-
ficient to match our model?s performance.
At the token level, the supervised CRF base-
line is far more competitive. Using a single la-
beled document (13 relation instances) yields su-
perior performance to either of our model variants
for finance, while four labeled documents (29 re-
lation instances) do the same for earthquake. This
result is not surprising?our model makes strong
domain-independent assumptions about how under-
lying patterns of regularities in the text connect to
relation expression. Without domain-specific super-
vision such assumptions are necessary, but they can
prevent the model from fully utilizing available la-
beled instances. Moreover, being able to annotate
even a single document requires a broad understand-
ing of every relation type germane to the domain,
which can be infeasible when there are many unfa-
miliar, complex domains to process.
In light of our strong sentence-level performance,
this suggests a possible human-assisted application:
use our model to identify promising relation-bearing
sentences in a new domain, then have a human an-
notate those sentences for use by a supervised ap-
proach to achieve optimal token-level extraction.
8 Conclusions
This paper has presented a constraint-based ap-
proach to in-domain relation discovery. We have
shown that a generative model augmented with
declarative constraints on the model posterior can
successfully identify domain-relevant relations and
their instantiations. Furthermore, we found that a
single set of constraints can be used across divergent
domains, and that tailoring constraints specific to a
domain can yield further performance benefits.
Acknowledgements
The authors gratefully acknowledge the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0172. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the view of the DARPA, AFRL, or
the US government. Thanks also to Hoifung Poon
and the members of the MIT NLP group for their
suggestions and comments.
538
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of DL.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
HLT/NAACL.
Kedar Bellare and Andrew McCallum. 2009. Gen-
eralized expectation criteria for bootstrapping extrac-
tors using record-text alignment. In Proceedings of
EMNLP.
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic
topic models. In Advances in NIPS.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In Proceedings of ACL.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou
Zhu. 1995. A limited memory algorithm for bound
constrained optimization. SIAM Journal on Scientific
Computing, 16(5):1190?1208.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of ACL.
2006. Modeling general and specific aspects of docu-
ments with a probabilistic topic model. In Advances
in NIPS.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and Zheng-
Yu Niu. 2005. Automatic relation extraction with
model order selection and discriminative label identi-
fication. In Proceedings of IJCNLP.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using la-
tent permutations. Journal of Artificial Intelligence
Research, 36:129?163.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-framework and Cross-domain Parser Evalu-
ation.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of NODALIDA.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):471?484.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
SIGKDD.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Proceedings of ACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of ACL/IJCNLP.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of EMNLP.
Ellen Riloff. 1996. Automatically generating extraction
patterns from untagged texts. In Proceedings of AAAI.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for unsupervised relation identification. In Pro-
ceedings of CIKM.
Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of HLT/NAACL.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-
ceedings of ACL.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition of
domain knowledge for information extraction. In Pro-
ceedings of COLING.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Cross-document relation extraction without la-
belled data. In Proceedings of EMNLP.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP.
539
Jun Zhu, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW.
540
