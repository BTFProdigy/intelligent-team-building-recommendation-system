Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7?12,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Biases in Predicting the Human Language Model
Alex B. Fine
University of Illinois at Urbana-Champaign
abfine@illinois.edu
Austin F. Frank
Riot Games
aufrank@riotgames.com
T. Florian Jaeger
University of Rochester
fjaeger@bcs.rochester.edu
Benjamin Van Durme
Johns Hopkins University
vandurme@cs.jhu.edu
Abstract
We consider the prediction of three hu-
man behavioral measures ? lexical deci-
sion, word naming, and picture naming ?
through the lens of domain bias in lan-
guage modeling. Contrasting the predic-
tive ability of statistics derived from 6 dif-
ferent corpora, we find intuitive results
showing that, e.g., a British corpus over-
predicts the speed with which an Amer-
ican will react to the words ward and
duke, and that the Google n-grams over-
predicts familiarity with technology terms.
This study aims to provoke increased con-
sideration of the human language model
by NLP practitioners: biases are not lim-
ited to differences between corpora (i.e.
?train? vs. ?test?); they can exist as well
between corpora and the intended user of
the resultant technology.
1 Introduction
Computational linguists build statistical language
models for aiding in natural language processing
(NLP) tasks. Computational psycholinguists build
such models to aid in their study of human lan-
guage processing. Errors in NLP are measured
with tools like precision and recall, while errors in
psycholinguistics are defined as failures to model
a target phenomenon.
In the current study, we exploit errors of the lat-
ter variety?failure of a language model to predict
human performance?to investigate bias across
several frequently used corpora in computational
linguistics. The human data is revealing because
it trades on the fact that human language process-
ing is probability-sensitive: language processing
reflects implicit knowledge of probabilities com-
puted over linguistic units (e.g., words). For ex-
ample, the amount of time required to read a word
varies as a function of how predictable that word is
(McDonald and Shillcock, 2003). Thus, failure of
a language model to predict human performance
reveals a mismatch between the language model
and the human language model, i.e., bias.
Psycholinguists have known for some time that
the ability of a corpus to explain behavior depends
on properties of the corpus and the subjects (cf.
Balota et al (2004)). We extend that line of work
by directly analyzing and quantifying this bias,
and by linking the results to methodological con-
cerns in both NLP and psycholinguistics.
Specifically, we predict human data from
three widely used psycholinguistic experimental
paradigms?lexical decision, word naming, and
picture naming?using unigram frequency esti-
mates from Google n-grams (Brants and Franz,
2006), Switchboard (Godfrey et al, 1992), spoken
and written English portions of CELEX (Baayen
et al, 1995), and spoken and written portions
of the British National Corpus (BNC Consor-
tium, 2007). While we find comparable overall
fits of the behavioral data from all corpora un-
der consideration, our analyses also reveal spe-
cific domain biases. For example, Google n-
grams overestimates the ease with which humans
will process words related to the web (tech, code,
search, site), while the Switchboard corpus?a
collection of informal telephone conversations be-
tween strangers?overestimates how quickly hu-
mans will react to colloquialisms (heck, darn) and
backchannels (wow, right).
7
Figure 1: Pairwise correlations between log frequency es-
timates from each corpus. Histograms show distribution over
frequency values from each corpus. Lower left panels give
Pearson (top) and Spearman (bottom) correlation coefficients
and associated p-values for each pair. Upper right panels plot
correlations
2 Fitting Behavioral Data
2.1 Data
Pairwise Pearson correlation coefficients for log
frequency were computed for all corpora under
consideration. Significant correlations were found
between log frequency estimates for all pairs (Fig-
ure 1). Intuitive biases are apparent in the corre-
lations, e.g.: BNCw correlates heavily with BNCs
(0.91), but less with SWBD (0.79), while BNCs
correlates more with SWBD (0.84).
1
Corpus Size (tokens)
Google n-grams (web release) ? 1 trillion
British National Corpus (written, BNCw) ? 90 million
British National Corpus (spoken, BNCs) ? 10 million
CELEX (written, CELEXw) ? 16.6 million
CELEX (spoken, CELEXs) ? 1.3 million
Switchboard (Penn Treebank subset 3) ? 800,000
Table 1: Summary of the corpora under consideration.
2.2 Approach
We ask whether domain biases manifest as sys-
tematic errors in predicting human behavior. Log
unigram frequency estimates were derived from
each corpus and used to predict reaction times
(RTs) from three experiments employing lexical
1
BNCw and BNCs are both British, while BNCs and
SWBD are both spoken.
decision (time required by subjects to correctly
identify a string of letters as a word of English
(Balota et al, 1999)); word naming (time required
to read aloud a visually presented word (Spieler
and Balota, 1997); (Balota and Spieler, 1998));
and picture naming (time required to say a pic-
ture?s name (Bates et al, 2003)). Previous work
has shown that more frequent words lead to faster
RTs. These three measures provide a strong test
for the biases present in these corpora, as they
span written and spoken lexical comprehension
and production.
To compare the predictive strength of log fre-
quency estimates from each corpus, we fit mixed
effects regression models to the data from each
experiment. As controls, all models included (1)
mean log bigram frequency for each word, (2)
word category (noun, verb, etc.), (3) log mor-
phological family size (number of inflectional and
derivational morphological family members), (4)
number of synonyms, and (5) the first principal
component of a host of orthographic and phono-
logical features capturing neighborhood effects
(type and token counts of orthographic and phono-
logical neighbors as well as forward and backward
inconsistent words; (Baayen et al, 2006)). Mod-
els of lexical decision and word naming included
random intercepts of participant age to adjust for
differences in mean RTs between old (mean age
= 72) vs. young (mean age = 23) subjects, given
differences between younger vs. older adults? pro-
cessing speed (cf. (Ramscar et al, 2014)). (All
participants in the picture naming study were col-
lege students.)
2.3 Results
For each of the six panels corresponding to fre-
quency estimates from a corpus A, Figure 2 gives
the ?
2
value resulting from the log-likelihood ra-
tio of (1) a model containing A and an estimate
from one of the five remaining corpora (given on
the x axis) and (2) a model containing just the cor-
pus indicated on the x axis. Thus, for each panel,
each bar in Figure 2 shows the explanatory power
of estimates from the corpus given at the top of the
panel after controlling for estimates from each of
the other corpora.
Model fits reveal intuitive, previously undocu-
mented biases in the ability of each corpus to pre-
dict human data. For example, corpora of British
English tend to explain relatively little after con-
8
trolling for other British corpora in modeling lexi-
cal decision RTs (yellow). Similarly, Switchboard
provides relatively little explanatory power over
the other corpora in predicting picture naming
RTs (blue bars), possibly because highly image-
able nouns and verbs frequent in everyday interac-
tions are underrepresented in telephone conversa-
tions between people with no common visual ex-
perience. In other words, idiosyncratic facts about
the topics, dialects, etc. represented in each cor-
pus lead to systematic patterns in how well each
corpus can predict human data relative to the oth-
ers. In some cases, the predictive value of one
corpus after controlling for another?apparently
for reasons related to genre, dialect?can be quite
large (cf. the ?
2
difference between a model with
both Google and Switchboard frequency estimates
compared to one with only Switchboard [top right
yellow bar]).
In addition to comparing the overall predictive
power of the corpora, we examined the words
for which behavioral predictions derived from the
corpora deviated most from the observed behav-
ior (word frequencies strongly over- or under-
estimated by each corpora). First, in Table 2 we
give the ten words with the greatest relative differ-
ence in frequency for each corpus pair. For exam-
ple, fife is deemed more frequent according to the
BNC than to Google.
2
These results suggest that particular corpora
may be genre-biased in systematic ways. For in-
stance, Google appears to be biased towards termi-
nology dealing with adult material and technology.
Similarly, BNCw is biased, relative to Google, to-
wards Britishisms. For these words in the BNC
and Google, we examined errors in predicted lexi-
cal decision times. Figure 3 plots errors in the lin-
ear model?s prediction of RTs for older (top) and
younger (bottom) subjects.
The figure shows a positive correlation between
how large the difference is between the lexical de-
cision RT predicted by the model and the actu-
ally observed RT, and how over-estimated the log
frequency of that word is in the BNC relative to
Google (left panel) or in Google relative to the
BNC (right panel). The left panel shows that BNC
produces a much greater estimate of the log fre-
2
Surprisingly, fife was determined to be one of the words
with the largest frequency asymmetry between Switchboard
and the Google n-grams corpus. This was a result of lower-
casing all of the words in in the analyses, and the fact that
Barney Fife was mentioned several times in the BNC.
quency of the word lee relative to Google, which
leads the model to predict a lower RT for this word
than is observed (i.e., the error is positive; though
note that the error is less severe for older relative to
younger subjects). By contrast, the asymmetry be-
tween the two corpora in the estimated frequency
of sir is less severe, so the observed RT deviates
less from the predicted RT. In the right panel, we
see that Google assigns a much greater estimate
of log frequency to the word tech than the BNC,
which leads a model predicting RTs from Google-
derived frequency estimates to predict a far lower
RT for this word than observed.
3 Discussion
Researchers in computational linguistics often as-
sume that more data is always better than less
data (Banko and Brill, 2001). This is true in-
sofar as larger corpora allow computational lin-
guists to generate less noisy estimates of the av-
erage language experience of the users of compu-
tational linguistics applications. However, corpus
size does not necessarily eliminate certain types of
biases in estimates of human linguistic experience,
as demonstrated in Figure 3.
Our analyses reveal that 6 commonly used cor-
pora fail to reflect the human language model in
various ways related to dialect, modality, and other
properties of each corpus. Our results point to
a type of bias in commonly used language mod-
els that has been previously overlooked. This bias
may limit the effectiveness of NLP algorithms in-
tended to generalize to a linguistic domains whose
statistical properties are generated by humans.
For psycholinguists these results support an im-
portant methodological point: while each corpus
presents systematic biases in how well it predicts
human behavior, all six corpora are, on the whole,
of comparable predictive value and, specifically,
the results suggest that the web performs as well
as traditional instruments in predicting behavior.
This has two implications for psycholinguistic re-
search. First, as argued by researchers such as
Lew (2009), given the size of the Web compared to
other corpora, research focusing on low-frequency
linguistic events?or requiring knowledge of the
distributional characteristics of varied contexts?
is now more tractable. Second, the viability of
the web in predicting behavior opens up possibil-
ities for computational psycholinguistic research
in languages for which no corpora exist (i.e., most
9
CELEX written BNC written Google
CELEX spoken BNC spoken Switchboard
0
40
80
120
0
10
20
30
40
0
10
20
30
0
10
20
30
0
10
20
30
40
0
5
10
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
Comparison
? ?2
task
lexical decision
picture naming
word naming
Pairwise model comparisons
Figure 2: Results of log likelihood ratio model comparisons. Large values indicate that the reference predictor (panel title)
explained a large amount of variance over and above the predictor given on the x-axis.
Google and BNC written
Standardized difference score
Err
or 
in l
ine
ar 
mo
de
l
cent dame
doleduke
fife
glen
god
gulf hall
hank
king
lee
lord march
nick
prime
prince
sir ward
cent damedoleduke
fife
glen
god
gulf
hallhank king
lee
lord march
nick
primeprince
sir ward
ass
bin
bug
butt cartchat click
code
darn
den
dialdikefileflip gayheckhop hunklink log
mail
map page
pee
prep
print
quote
ranchscript
search
self
sex
site
skipslotstore
suck
tag
tech
teens
thread
tiretoe
twain
webwhiz
wow
zip
ass
bin
bugbutt
cartchat click
codedarndendial
dike
file
flip
gay
heck
hop hunk
link log mailmap page
peepr p
print quoteranchscript
search
selfsex
site
skip
slot
store sucktag
tech
teens
thread
tire
toe
twain web
whizwow
zip
-0.1
0.0
0.1
0.2
0.3
0.4
-0.1
0.0
0.1
0.2
0.3
0.4
old
young
-3.5 -3.0 -2.5 2.5 3.0 3.5 4.0 4.5 5.0 5.5
Google < BNC written Google > BNC written
goog.f
-4
-2
0
2
Figure 3: Errors in the linear model predicting lexical decision RTs from log frequency are plotted against the standardized
difference in log frequency in the Google n-grams corpus versus the written portion of the BNC. Top and bottom panels show
errors for older and younger subjects, respectively. The left panel plots words with much greater frequency in the written
portion of the BNC relative to Google; the right panel plots words occurring more frequently in Google. Errors in the linear
model are plotted against the standardized difference in log frequency across the corpora, and word color encodes the degree to
which each word is more (red) or less (blue) frequent in Google. That the fit line in each graph is above 0 in the y-axis means
that on average these biased words in each domain are being over-predicted, i.e., the corpus frequencies suggest humans will
react (sometimes much) faster than they actually did in the lab.
10
Greater Lesser Top-10
google bnc.s web, ass, gay, tire, text, tool, code, woe, site, zip
google bnc.w ass, teens, tech, gay, bug, suck, site, cart, log, search
google celex.s teens, cart, gay, zip, mail, bin, tech, click, pee, site
google celex.w web, full, gay, bin, mail, zip, site, sake, ass, log
google swbd gay, thread, text, search, site, link, teens, seek, post, sex
bnc.w google fife, lord, duke, march, dole, god, cent, nick, dame, draught
bnc.w bnc.s pact, corps, foe, tract, hike, ridge, dine, crest, aide, whim
bnc.w celex.s staff, nick, full, waist, ham, lap, knit, sheer, bail, march
bnc.w celex.w staff, lord, last, nick, fair, glen, low, march, should, west
bnc.w swbd rose, prince, seek, cent, text, clause, keen, breach, soul, rise
celex.s google art, yes, pound, spoke, think, mean, say, thing, go, drove
celex.s bnc.s art, hike, pact, howl, ski, corps, peer, spoke, jazz, are
celex.s bnc.w art, yes, dike, think, thing, sort, mean, write, pound, lot
celex.s celex.w yes, sort, thank, think, jazz, heck, tape, well, fife, get
celex.s swbd art, cell, rose, spoke, aim, seek, shall, seed, text, knight
celex.w google art, plod, pound, shake, spoke, dine, howl, sit, say, draught
celex.w bnc.s hunch, stare, strife, hike, woe, aide, rout, yell, glaze, flee
celex.w bnc.w dike, whiz, dine, shake, grind, jerk, whoop, say, are, cram
celex.w celex.s wrist, pill, lawn, clutch, stare, spray, jar, shark, plead, horn
celex.w swbd art, rose, seek, aim, rise, burst, seed, cheek, grin, lip
swbd google mow, kind, lot, think, fife, corps, right, cook, sort, do
swbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, nap
swbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid
swbd celex.s wow, sauce, mall, deck, full, spray, flute, rib, guy, bunch
swbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fair
Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife,
lord, and duke, in the BNC are far greater than in Google).
languages). This furthers the arguments of the ?the
web as corpus? community (Kilgarriff and Grefen-
stette, 2003) with respect to psycholinguistics.
Finally, combining multiple sources of fre-
quency estimates is one way researchers may be
able to reduce the prediction bias from any sin-
gle corpus. This relates to work in automatically
building domain specific corpora (e.g., Moore and
Lewis (2010), Axelrod et al (2011), Daum?e III
and Jagarlamudi (2011), Wang et al (2014), Gao
et al (2002), and Lin et al (1997)). Those efforts
focus on building representative document collec-
tions for a target domain, usually based on a seed
set of initial documents. Our results prompt the
question: can one use human behavior as the tar-
get in the construction of such a corpus? Con-
cretely, can we build corpora by optimizing an ob-
jective measure that minimizes error in predicting
human reaction times? Prior work in building bal-
anced corpora used either rough estimates of the
ratio of genre styles a normal human is exposed to
daily (e.g., the Brown corpus (Kucera and Fran-
cis, 1967)), or simply sampled text evenly across
genres (e.g., COCA: the Corpus of Contemporary
American English (Davies, 2009)). Just as lan-
guage models have been used to predict reading
grade-level of documents (Collins-Thompson and
Callan, 2004), human language models could be
used to predict the appropriateness of a document
for inclusion in an ?automatically balanced? cor-
pus.
4 Conclusion
We have shown intuitive, domain-specific biases
in the prediction of human behavioral measures
via corpora of various genres. While some psy-
cholinguists have previously acknowledged that
different corpora carry different predictive power,
this is the first work to our knowledge to system-
atically document these biases across a range of
corpora, and to relate these predictive errors to do-
main bias, a pressing issue in the NLP community.
With these results in hand, future work may now
consider the automatic construction of a ?prop-
erly? balanced text collection, such as originally
desired by the creators of the Brown corpus.
Acknowledgments
The authors wish to thank three anonymous ACL
reviewers for helpful feedback. This research
was supported by a DARPA award (FA8750-13-2-
0017) and NSF grant IIS-0916599 to BVD, NSF
IIS-1150028 CAREER Award and Alfred P. Sloan
Fellowship to TFJ, and an NSF Graduate Research
Fellowship to ABF.
11
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 11).
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2). Linguis-
tic Data Consortium, Philadelphia.
R. H. Baayen, L. F. Feldman, and R. Schreuder.
2006. Morphological influences on the recognition
of monosyllabic monomorphemic words. Journal of
Memory and Language, 53:496?512.
D. A. Balota and D. H. Spieler. 1998. The utility of
item-level analyses in model evaluation: A reply to
Seidenberg & Plaut (1998). Psychological Science.
D. A. Balota, M. J. Cortese, and M. Pilotti. 1999. Item-
level analyses of lexical decision performance: Re-
sults from a mega-study. In Abstracts of the 40th An-
nual Meeting of the Psychonomics Society, page 44.
D. Balota, M. Cortese, S. Sergent-Marshall, D. Spieler,
and M. Yap. 2004. Visual word recognition for
single-syllable words. Journal of Experimental Psy-
chology:General, (133):283316.
M. Banko and E. Brill. 2001. Mitigating the paucity of
data problem. Human Language Technology.
E. Bates, S. D?Amico, T. Jacobsen, A. Szkely, E. An-
donova, A. Devescovi, D. Herron, CC Lu, T. Pech-
mann, C. Plh, N. Wicha, K. Federmeier, I. Gerd-
jikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer,
K. Kohnert, T. Mehotcheva, A. Orozco-Figueroa,
A. Tzeng, and O. Tzeng. 2003. Timed picture nam-
ing in seven languages. Psychonomic Bulletin & Re-
view, 10(2):344?380.
BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Ox-
ford University Computing Services on behalf of the
BNC Consortium.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium (LDC).
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In HLT-NAACL, pages 193?200.
H. Daum?e III and J. Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT 11).
M. Davies. 2009. The 385+ million word corpus of
contemporary american english (19902008+): De-
sign, architecture, and linguistic insights. Inter-
national Journal of Corpus Linguistics, 14(2):159?
190.
J. Gao, J. Goodman, M. Li, and K. F. Lee. 2002. To-
ward a unified approach to statistical language mod-
eling for chinese. In Proceedings of the ACM Trans-
actions on Asian Language Information Processing
(TALIP 02).
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone Speech Corpus for
Research and Development. In Proceedings of
ICASSP-92, pages 517?520.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333?348.
H. Kucera and W.N. Francis. 1967. Computational
analysis of present-day american english. provi-
dence, ri: Brown university press.
R. Lew, 2009. Contemporary Corpus Linguistics,
chapter The Web as corpus versus traditional cor-
pora: Their relative utility for linguists and language
learners, pages 289?300. London/New York: Con-
tinuum.
S. C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, and
L. S. Lee. 1997. Chinese language model adapta-
tion based on document classification and multiple
domain-specific language models. In Proceedings
of the 5th European Conference on Speech Commu-
nication and Technology.
S.A. McDonald and R.C. Shillcock. 2003. Eye
movements reveal the on-line computation of lexical
probabilities during reading. Psychological science,
14(6):648?52, November.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 10).
M. Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H.
Baayen. 2014. The myth of cognitive decline: non-
linear dynamics of lifelong learning. Topics in Cog-
nitive Science, 32:5?42.
D. H. Spieler and D. A. Balota. 1997. Bringing com-
putational models of word naming down to the item
level. 6:411?416.
L. Wang, D.F. Wong, L.S. Chao, Y. Lu, and J. Xing.
2014. A systematic comparison of data selection
criteria for smt domain adaptation. The Scientific
World Journal.
12
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 18?26,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Is there syntactic adaptation in language comprehension?
Alex B. Fine, Ting Qian, T. Florian Jaeger and Robert A. Jacobs
Department of Brain and Cognitive Sciences
University of Rochester
Rochester, NY, USA
{afine, tqian, fjaeger, robbie}@bcs.rochester.edu
Abstract
In this paper we investigate the manner in
which the human language comprehension
system adapts to shifts in probability dis-
tributions over syntactic structures, given
experimentally controlled experience with
those structures. We replicate a classic
reading experiment, and present a model
of the behavioral data that implements a
form of Bayesian belief update over the
course of the experiment.
1 Introduction
One of the central insights to emerge from ex-
perimental psycholinguistics over the last half
century is that human language comprehension
and production are probability-sensitive. Dur-
ing language comprehension, language users ex-
ploit probabilistic information in the linguistic sig-
nal to make inferences about the speaker?s most
likely intended message. In syntactic compre-
hension specifically, comprehenders exploit statis-
tical information about lexical and syntactic co-
occurrence statistics. For instance, (1) is temporar-
ily ambiguous at the noun phrase the study, since
the NP can be parsed as either the direct object
(DO) of the verb acknowledge or as the subject
NP of a sentence complement (SC).
(1) The reviewers acknowledged the study...
? DO: ... in the journal.
? SC: ... had been revolutionary.
The ambiguity in the SC continuation is resolved
at had been, which rules out the direct object in-
terpretation of the study. Reading times at had
been?the so-called point of disambiguation?are
correlated with a variety of lexical-syntactic prob-
abilities. For instance, if the probability of a SC
is low, given the verb, subjects are garden-pathed
and will display longer reading times at had been.
Conversely, if the probability of a SC is high, the
material at the point of disambiguation is rela-
tively unsurprising (i.e. conveys less information),
and reading times will be short. Readers are also
sensitive to the probability of the post-verbal NP
occurring as the direct object of the verb. This is
often discussed in terms of plausibility?in (1), the
study is a plausible direct object of acknowledge
(relative to, say, the window), which will also con-
tribute to longer reading times in the event of a SC
continuation (Garnsey et al, 1997).
Thus, humans make pervasive use of proba-
bilistic cues in the linguistic signal. A question
that has received very little attention, however, is
how language users maintain or update their rep-
resentations of the probability distributions rele-
vant to language use, given new evidence?a phe-
nomenon we will call adaptation. That is, while
we know that language users have access to lin-
guistic statistics, we know little about the dynam-
ics of this knowledge in language users: is the
probabilistic information relevant to comprehen-
sion derived from experience during a critical pe-
riod of language acquisition, or do comprehenders
update their knowledge on the basis of experience
throughout adulthood? A priori, both scenarios
seem plausible?given the sheer number of cues
relevant to comprehension, it would be advanta-
geous to limit the resources devoted to acquiring
this knowledge; on the other hand, any learner?s
linguistic experience is bound to be incomplete, so
the ability to adapt to novel distributional patterns
in the linguistic input may prove to be equally use-
ful. The goal of this paper is to explore this is-
sue and to take an initial step toward providing a
computational framework for characterizing adap-
tation in language processing.
1.1 Adaptation in Sentence Comprehension
Both over time and across situations, humans are
exposed to linguistic evidence that, in principle,
18
ought to lead to shifts in our representations of the
relevant probability distributions. An efficient lan-
guage processing system is expected to take new
evidence into account so that behavior (decisions
during online production, predictions about up-
coming words, etc.) will be guided by accurate es-
timates of these probability distributions. At least
at the level of phonetic perception and produc-
tion, there is evidence that language users quickly
adapt to the statistical characteristics of the am-
bient language. For instance, over the course of
a single interaction, the speech of two interlocu-
tors becomes more acoustically similar, a phe-
nomenon known as spontaneous phonetic imita-
tion (Goldinger, 1998). Perhaps even more strik-
ingly, Clayards et al (2008) demonstrated that,
given a relatively small number of tokens, compre-
henders shift the degree to which they rely on an
acoustic cue as the variance of that cue changes,
reflecting adaptation to the distributions of proba-
bilistic cues in speech perception.
At the level of syntactic processing, belief up-
date/adaptation has only recently been addressed
(Wells et al, 2009; Snider and Jaeger, in prep). In
this study, we examine adaptation at the level of
syntactic comprehension. We provide a computa-
tional model of short- to medium-term adaptation
to local shifts in the statistics of the input. While
the Bayesian model presented can account for the
behavioral data, the quality of the model depends
on how control variables are treated. We discuss
the theoretical and methodological implications of
this result.
Section 2 describes the behavioral experiment,
a slight modification of the classic reading experi-
ment reported in Garnsey et al (1997). The study
reported in section 3 replicates the basic findings
of (Garnsey et al, 1997). In sections 4 and 5
we outline a Bayesian model of syntactic adapta-
tion, in which distributions over syntactic struc-
tures are updated at each trial based on the ev-
idence in that trial, and discuss the relationship
between the model results and control variables.
Section 6 concludes.
2 Behavioral Experiment
2.1 Participants
Forty-six members of the university community
participated in a self-paced reading study for pay-
ment. All were native speakers of English with
normal or corrected to normal vision, based on
self-report.
2.2 Materials
Subjects read a total of 98 sentences, of which 36
were critical items containing DO/SC ambiguities,
as in (1). These 36 sentences comprise a subset of
those used in Garnsey et al (1997). The stim-
uli were manipulated along two dimensions: first,
verbs were chosen such that the conditional prob-
ability of a SC, given the verb, varied. In Garnsey
et al (1997), this conditional probability was es-
timated from a norming study, in which subjects
completed sentence fragments containing DO/SC
verbs (e.g. the lawyer acknowledged...). We adopt
standard psycholinguistic terminology and refer
to this conditional probability as SC-bias. The
verbs used in the critical sentences in Garnsey et
al. (1997) were selected to span a wide range of
SC-bias values, from .01 to .9. Each sentence con-
tained a different DO/SC verb. In addition to SC-
bias, half of the sentences presented to each sub-
ject included the complementizer that, as in (2).
(2) The reviewers acknowledged that the
study had been revolutionary.
Sentences with a complementizer were included
as an unambiguous baseline (Garnsey et al 1997).
The presence of a complementizer was counter-
balanced, such that each subject saw half of the
sentences with a complementizer and all sen-
tences occurred with and without a complemen-
tizer equally often across subjects. All of the criti-
cal sentences contained a SC continuation. The 36
critical items were interleaved with 72 fillers that
included simple transitives and intransitives.
2.3 Procedure
Subjects read critical and filler sentences in a self-
paced moving window display (Just et al, 1982),
presented using the Linger experimental presen-
tation software (Rohde, 2005). Sentences were
presented in a noncumulative word-by-word self-
paced moving window. At the beginning of each
trial, the sentence appeared on the screen with all
non-space characters replaced by a dash. Using
their dominant hands, subjects pressed the space
bar to view each consecutive word in the sen-
tence. Durations between space bar presses were
recorded. At each press of the space bar, the
currently-viewed word reverted to dashes as the
next word was converted to letters. A yes/no com-
19
prehension question followed all experimental and
filler sentences.
2.4 Analysis
In keeping with standard procedure, we used
length-corrected residual per-word reading times
as our dependent measure. Following Garnsey et
al. (1997), we define the point of disambiguation
in the critical sentences as the two words follow-
ing the post-verbal NP (e.g. had been in (1) and
(2)). All analyses reported here were conducted on
residual reading times at this region. For a given
subject, residual reading times more than two stan-
dard deviations from that subject?s mean residual
reading time were excluded.
3 Study 1
Residual reading times at the point of disambigua-
tion were fit to a linear mixed effects regression
model. This model included the full factorial de-
sign (i.e. all main effects and all interactions) of
logged SC-bias (taken from the norming study re-
ported in Garnsey et al 1997) and complemen-
tizer presence. Additionally, the model included
random intercepts of subject and item. This was
the maximum random effect structure justified by
the data, based on comparison against more com-
plex models.1 All predictors in the model were
centered at zero in order to reduce collinearity.
P-values reported in all subsequent models were
calculated using MCMC sampling (where N =
10,000).
3.1 Results
This model replicated the findings reported by
Garnsey et al (1997). There was a significant
main effect of complementizer presence (? =
?3.2, t = ?2.5, p < .05)?reading times at
the point of disambiguation were lower when
the complementizer was present. Additionally,
there was a significant two-way interaction be-
tween complementizer presence and logged SC-
bias (? = 3.0, t = 2.5, p < .05)?SC-bias has a
stronger negative correlation with reading times in
the disambiguating region when the complemen-
tizer is absent, as expected. Additionally, Gar-
nsey et al (1997) found a main effect of SC-bias.
For us, this main effect did not reach significance
1For a detailed description of the procedure used,
see http://hlplab.wordpress.com/2009/05/14/random-effect-
should-i-stay-or-should-i-go/
(? = ?1.2, t = ?1.11, p = .5), possibly owing to
the fact that we tested a much smaller sample than
Garnsey et al (1997) (51 compared to 82 partici-
pants).
4 Study 2: Bayesian Syntactic
Adaptation
Reading times at the point of disambiguation in
these stimuli reflect, among other things, sub-
jects? estimates of the conditional probability
p(SC|verb) (Garnsey et al 1997), which we have
been calling SC-bias. Thus, we model the task fac-
ing subjects in this experiment as one of Bayesian
inference, where subjects are, when reading a sen-
tence containing the verb vi, inferring a posterior
probability P(SC|vi), i.e. the probability that a
sentence complement clause will follow a verb vi.
According to Bayes rule, we have:
p(SC|vi) =
p(vi|SC)p(SC)
p(vi)
(1)
In Equation (1), we use the relative frequency
of vi (estimated from the British National Corpus)
as the estimate for p(vi). The first term in the nu-
merator, p(vi|SC), is the likelihood, which we es-
timate by using the relative frequency of vi among
all verbs that can take a sentence complement as
their argument. These values are taken from the
corpus study by Roland et al (2007). Roland et al
(2007) report, among other things, the number of
times a SC occurs as the argument of roughly 200
English verbs. These values are reported across a
number of corpora. We use the values from the
BNC to compute p(vi|SC).
The prior probability of a sentence complement
clause, p(SC), is the estimate of interest in this
study. We hypothesize that, under the assumptions
of the current model, subjects update their esti-
mate for p(SC) based on the evidence presented
in each trial. As a result, the posterior probability
varies from trial to trial, not only because the verb
used in each stimulus is different, but also because
the belief about the probability of a sentence com-
plement is being updated based on the evidence in
each trial. We employ the beta-binomial model to
simulate this updating process, as described next.
4.1 Belief Update
We adopt an online training paradigm involving
an ideal observer learning from observations. Af-
ter observing a sentence containing a DO/SC verb,
20
we predict that subjects will update both the likeli-
hood p(vi|SC) for that verb, as well as the proba-
bility p(SC). Because each verb occurs only once
for a given subject, the effect of updating the first
quantity is impossible to measure in the current ex-
perimental paradigm. We therefore focus on mod-
eling how subjects update their belief of p(SC)
from trial to trial.
We make the simplifying assumption that the
only possible argument that DO/SC verbs can take
is either a direct object or a sentence complement
clause. Further, subjects are assumed to have an
initial belief about how probable a sentence com-
plement is, on a scale of 0 to 1. Let ? denote
this probability estimate, and p(?) the strength of
this estimate. From the perspective of an ideal
observer, p(?) will go up for ? > 0.5 when a
DO/SC verb is presented with a sentence comple-
ment as its argument. This framework assumes
that subjects do not compute ? by merely relying
on frequency (otherwise, ? will be simply the ra-
tio between SC and DO structures in a block of
trials), but they have a distribution P (?), where
each possible estimate of ? is associated with a
probability indicating the confidence on that es-
timate. In order to make our results comparable
to existing models, however, we use the expected
value of P (?) in each iteration of training as point
estimates. Therefore, for one subject, we have
36 estimated ?? values, each corresponding to the
changed belief after seeing a sentence containing
SC in an experiment of 36 trials. Because none
of the filler items included DO/SC verbs, we as-
sume that filler trials have no effect on subjects?
estimates of P (?).
Since all stimuli in our experiment have the SC
structure, the general expectation is the distribu-
tion P (?) will shift towards the end where ? = 1.
Our belief update model tries to capture the shape
of this shift during the course of the experiment.
Using Bayesian inference, we can describe the up-
dating process as the following, where ?i repre-
sents a particular belief of the value ?.
p(? = ?i|obs.) =
p(obs.|? = ?i)p(? = ?i)
p(obs.)
= p(obs.|? = ?i)p(? = ?i)? 1
0
p(obs.|?)p(?) d?
(2)
This posterior probability is hypothesized to re-
flect how likely a subject would consider the prob-
ability of SC to be ?i after being exposed to one
experimental item. We discretized ? to 100 evenly
spaced ?i values, ranging from 0 to 1. Thus, the
denominator can be calculated by marginalizing
over the 100 ?i values. The two terms in the nu-
merator in Equation (2) are estimated in the fol-
lowing manner.
Likelihood function p(obs.|? = ?i) is modeled
by a binomial distribution, where the parameters
are ?i (the probability of observing a SC clause)
and 1 ? ?i (the probability of observing a direct
object), and where the outcome is the experimen-
tal item presented to the subject. Therefore:
p(obs.|? = ?i) =
(nsc + ndo)!
nsc!ndo!
?nsci (1? ?i)ndo
(3)
In the current experiment, ndo is always 0 since
all stimuli contain the SC argument. In addition,
between-trial reading time differences are mod-
elled at one item a step for each subject so that nsc
is always 1 in each trial. It is in theory possible to
set nsc to other numbers.
The prior In online training, the posterior of the
previous iteration is used as the prior for the cur-
rent one. Nevertheless, the prior p(? = ?i) for
the very first iteration of training needs to be es-
timated. Here we assume a beta distribution with
parameters ? and ?. The probability of the prior
then is:
p(? = ?i) =
???1i (1? ?i)??1
B(?,?)
Intuitively, ? and ? capture the number of times
subjects have observed the SC and DO outcomes,
respectively, before the experiment. In the context
of our research, this model assumes that subjects?
beliefs about p(SC) and p(DO) are based on ??1
observations of SC and ? ? 1 observations of DO
prior to the experiment.
The values of the parameters of the beta distri-
bution were obtained by searching through the pa-
rameter space with an objective function based on
the Bayesian information criterion (BIC) score of
a regression model containing the log of the pos-
terior computed using the updated prior p(SC),
complementizer presence, and the two-way inter-
action. The BIC (Schwarz, 1978) is a measure
of model quality that weighs the models empirical
coverage against its parsimony (BIC = 2ln(L)+
21
k ? ln(n), where k is the number of parameters in
the model, n the number of data points, and L is
the models data likelihood). Smaller BIC indicate
better models. The ? and ? values yielding the
lowest BIC score are used.
In estimating ? and ?, we considered all pairs of
non-negative integers such that both values were
below 1000. The values of ? and ? used here were
1 and 177, respectively. These values do not im-
ply that subjects have seen only 1 SC and 177 DOs
prior to the experiment, but that only this many ob-
servations inform subjects? prior beliefs about this
distribution. The relationship between the choice
of the parameters of the beta distribution, ? and
?, and the BIC of the model used in the parameter
estimation is shown in Figure 1.
Beta
Alpha
BIC
Figure 1: The relationship between the BIC of the
model used in the parameter estimation step and
values of ? and ? in the beta distribution
Because we model subjects? estimates of
p(SC|vi) in terms of Bayesian inference, with a
continuously updated prior, p(SC), the value of
p(SC|vi) depends, in our model, on both verb-
specific statistics (i.e. the likelihood p(vi|SC) and
the probability of the verb p(vi)) and the point in
the experiment at which the trial containing that
verb is encountered. We can visualize this rela-
tionship in Figure 2, which shows the values given
by the model of p(SC|vi) for four particular dif-
ferent verbs, depending on the point in the experi-
ment at which the verb is seen.
The approach we take is hence fundamentally
Presentation Order
Pos
terio
r p(S
C|v)
0.2
0.4
0.6
0.8
0 10 20 30
confide deny
know
0 10 20 30
0.2
0.4
0.6
0.8
print
Figure 2: The relationship, for four of the verbs,
between the value of p(SC|vi) given by the model
as a function of when in the experiment vi is en-
countered
different from the approach commonly taken in
psycholinguistics, which is to use static estimates
of quantities such as p(SC|vi) derived from cor-
pora or norming studies.
4.2 Analysis
To test whether the model-derived values of
p(SC|vi) are a good fit for the behavioral data,
we fit residual reading times at the point of dis-
ambiguation using linear mixed effects regression.
The model included main effects of p(SC|vi)?as
given by the model just described?and comple-
mentizer presence, as well as the two-way inter-
action between these two predictors. Additionally,
there were random intercepts of subject and item.
p(SC|vi) was logged and centered at zero.
4.3 Results
There was a highly significant main effect of
the posterior probability p(SC|vi) yielded by the
beta-binomial model (? = ?40, t = ?21.2, p <
.001), as well as a main effect of complemen-
tizer presence (?4.5, t = ?3.7, p < .001).
The two-way interaction between complementizer
presence and the posterior probability from the
beta-binomial model did not reach significance
(? = 0.5, t = .5, p > .05). The reason is likely
that, in the analysis presented for Study 1, we can
interpret the interaction as indicating that when
22
SC-bias is high, the complementizer has less of
an effect; in our model, the posterior probabil-
ity p(SC|vi) is both generally higher and has less
variance than the same quantity when based on
corpus- or norming study estimates, since the prior
probability p(SC) is continuously increasing over
the course of the experiment. This would have the
effect of eliminating or at least obscuring the in-
teraction with complementizer presence.
The posterior p(SC|vi) has a much stronger
negative correlation with residual reading times
than the measure of SC-bias used in Study 1 (? =
?40 as opposed to ? = ?1.2).
4.4 Discussion
So far, we have replicated a classic finding in the
sentence processing literature (Study 1), provided
evidence that subjects? estimates of the conditional
probability p(SC|vi) change based on evidence
throughout the experiment, and that this process
is captured well by a model which implements a
form of incremental Bayesian belief update. We
take this as evidence that the language comprehen-
sion system is adaptive, in the sense that language
users continually update their estimates of proba-
bility distributions over syntactic structures.
5 Syntactic Adaptation vs. Motor
Adaptation
The results of the model presented in section 4
are amenable to (at least) two explanations. We
have hypothesized that, given exposure to new ev-
idence about probability distributions over syn-
tactic structures in English, subjects update their
beliefs about these probability distributions, re-
flected in reading times?a phenomenon we refer
to as syntactic adaptation. An alternative explana-
tion, however, is one that appeals to motor adap-
tation, rather than syntactic adaptation. Specifi-
cally, it could be that subjects are simply adapt-
ing to the task?rather than to changes in syntactic
distributions?as the experiment proceeds, lead-
ing to faster reading times.
We expect the effect of motor adaptation to
be captured by presentation order, or the point
in the experiment at which subjects encounter a
given stimulus. In particular, we predict a neg-
ative correlation between presentation order and
reading times. Unfortunately, in the current ex-
periment, presentation order and p(SC|vi) derived
from the Beta-binomial model are positively cor-
related (r = .6)?the latter increases with increas-
ing presentation order, since participants only see
SC continuations. The results we observed above
could hence also be due to an effect of presentation
order.
The expected shape of a possible effect of task
adaptation is not obvious. That is, it is not clear
whether the relationship between presentation or-
der and reading times will be linear. On the one
hand, linearity would be the default assumption
prior to theoretical considerations about the dis-
tributional properties of presentation order. On
the other hand, presentation order is a lower-
bounded variable, which often are distributed ap-
proximately log-normally. Additionally, it is pos-
sible that there may be a floor effect: participants
may get used to having to press the space bar to ad-
vance to the next word and may quickly get faster
at that procedure until RTs converge against the
minimal time it takes to program the motor move-
ment to press the space bar. Such an effect would
likely lead to an approximately log-linear effect of
presentation order.
We test for an effect of motor adaptation by ex-
amining the effect of presentation order on read-
ing times, comparing the effect of linear and log-
transformed presentation order.
5.1 Controlling for Presentation Order in the
Beta-binomial model
We test for separate effects of syntactic adaptation
and motor adaptation by conducting stepwise re-
gressions with two models containing the full fac-
torial design of the Beta-binomial posterior, com-
plementizer presence, and, for the first model, a
linear effect of presentation order and, for the
second model, log-transformed presentation order.
We conducted stepwise regressions using back-
ward elimination, starting with all predictors and
removing non-significant predictors (i.e. p > .1),
one at a time, until all non-significant predictors
are deleted.
For both the model including a linear effect
of presentation order and a model including log-
transformed presentation order, the final mod-
els resulting from the stepwise regression proce-
dure included only main effects of complemen-
tizer presence and log presentation order. These
models are summarized in Figure 1, which in-
cludes coefficient-based tests for significance of
each of the predictors (i.e. whether the coefficient
23
is significantly different from zero) as well as ?2-
based tests for significance (i.e. the difference be-
tween a model with that predictor and one with-
out). Comparing the two resulting models based
on the Bayesian Information Criterion, the model
containing log-transformed presentation order is a
better model than one with a linear effect of pre-
sentation order (BIClog = 37467; BICnon?log =
37510).
Pres. order untransformed
Coef. and ?2-based tests
Predictor ? p ?2 p
Comp. pres. ?4.3 < .05 4.9 < .05
Pres. order ?.7 < .001 28.2 < .001
Pres. order log-transformed
Coef. and ?2-based tests
Predictor ? p ?2 p
Comp. pres. ?4.3 < .05 4.8 < .05
Pres. order ?33.8 < .001 29.4 < .001
Table 1: Coefficient- and ?2-based tests for sig-
nificance of model resulting from stepwise regres-
sion
In sum, the beta-binomial derived posterior ap-
pears to have no predictive power after presenta-
tion order is controlled for. This result does not
depend on how presentation order is treated (i.e.
log-transformed or not).
5.2 The interaction between SC-bias and
presentation order
The results from the previous section suggest that
the Beta-binomial derived posterior carries no pre-
dictive power after presentation order is controlled
for. Is there any evidence at all for syntactic adap-
tation (as opposed to motor, or task, adaptation)?
To attempt to answer this, we analyzed the read-
ing data using the model reported in section 3,
with an additional main effect of presentation or-
der, as well as the interactions between presenta-
tion order and the other predictors in the model.
An overall decrease in reading times due to mo-
tor adaptation should surface as a main effect of
presentation order, as mentioned; syntactic adap-
tation, however, is predicted to show up as a two-
way interaction between SC-bias and presentation
order?since subjects only see SC continuations,
subjects should expect this outcome to become
more and more probable over the course of the ex-
periment, causing the correlation between SC-bias
and reading times to become weaker (thus we pre-
dict the interaction to have a positive coefficient).
To test for such an interaction, we performed
a stepwise regressions with two models contain-
ing the full factorial design of SC-bias, comple-
mentizer presence, and, for the first model, a lin-
ear effect of presentation order and, for the second
model, log-transformed presentation order. The
stepwise regression procedure here was identical
to the one reported in the previous section.
For both models, the remaining predictors were
main effects of presentation order, complemen-
tizer presence, and SC-bias, as well as a two-way
interaction between SC-bias and complementizer
presence and a two-way interaction between SC-
bias and presentation order. The results of these
models are given in Table 2.
Pres. order untransformed
Coef. and ?2-based tests
Predictor ? p ?2 p
SC-bias ?.4 = .8 11.5 < .001
Comp. pres. ?4.4 < .001 18.1 < .001
Pres. order ?.9 < .001 420.9 < .001
SC-bias:Comp. 2.6 < .05 5.3 < .05
SC-bias:Pres. Order .1 < .05 6.2 < .05
Pres. order log-transformed
Coef. and ?2-based tests
Predictor ? p ?2 p
SC-bias ?1.4 = .5 8.9 < .05
Comp. pres. ?4.6 < .001 19.3 < .001
Pres. order ?42.4 < .001 461.2 < .001
SC-bias:Comp. 2.6 < .05 5.2 < .05
SC-bias:Pres. Order 3.5 = .06 3.4 = .06
Table 2: Coefficient- and ?2-based tests for sig-
nificance of model resulting from stepwise regres-
sion
The main findings reported in Study 1 (i.e. a
main effect of complementizer presence and a
two-way interaction between SC-bias and com-
plementizer presence) are replicated here, and do
not depend on whether presentation order is log-
transformed. However, the interaction between
SC-bias and presentation order is less reliable
when presentation order is log-transformed, reach-
ing only marginal significance. In short, an ad-
equate account of the data requires reference to
both motor adaptation (in the form of a main effect
of presentation order, log-transformed) and syn-
tactic adaptation.
If subjects are improving at the task, and the
effect of presentation order represents a kind of
adaptation to the task of self-paced reading, we
would expect to find a main effect of presenta-
tion order on reading times at all regions. This
24
is the case?a strong negative correlation between
presentation order and reading times holds across
all regions. Evidence that the observed interac-
tion is due to syntactic belief update comes from
the fact that the interaction between SC-bias and
presentation order, unlike the main effect of pre-
sentation order, is limited to the disambiguating
region of the sentence. We performed the regres-
sion reported above on residual reading times at
the main verb (e.g. acknowledge), ambiguous (e.g.
the study), and disambiguating (e.g. had been) re-
gions. These analyses revealed, as expected, main
effects of presentation order across all regions. At
the verb and ambiguous regions, however, presen-
tation order did not interact with SC-bias.
Region ? p? value
Main effect of pres. order
Verb ?.95 < .001
Ambig. region ?.9 < .001
Disambig. region ?.9 < .001
Pres. order X SC-bias interaction
Verb .09 = .24
Ambig. region .04 = .37
Disambig. region .1 < .05
Table 3: Main effect of presentation order and in-
teraction of presentation order with SC-bias at dif-
ferent regions in the critical sentences
This finding provides initial evidence that sub-
jects adapt their linguistic expectations to the evi-
dence observed throughout the experiment. How-
ever, the interaction between presentation order
and SC-bias in this analysis is amenable to an al-
ternative interpretation: interactions between pre-
sentation order and other variables could emerge
if subjects? reaction times reach some minimum
value over the course of the experiment, causing
any other variable to become less strongly corre-
lated with the dependent measure as reaction times
approach that minimum value. Thus this interac-
tion could be an artefact of a floor effect.
To test the possibility that the SC-bias-
presentation order interaction is the result of a
floor effect, we compared the 1st, 5th, and 10th
fastest percentiles of residual reading times across
all regions. As shown in Figure 3, faster reading
times are observed at each quantile in at least one
other region. In other words, reading times in the
disambiguating region do not seem to be bounded
by motor demands associated with the task. We
hence tentatively conclude that the interaction be-
tween SC-bias and log-transformed presentation
order is not the result of a floor effect, although
this issue deserves further attention.
Figure 3: Minimum and upper boundary of 1st,
5th, and 10th percentile values of residual reading
times across all sentence regions
6 Conclusion
We hypothesized that the language comprehension
system rapidly adapts to shifts in the probability
distributions over syntactic structures on the ba-
sis of experience with those structures. To in-
vestigate this phenomenon, we modelled reading
times from a self-paced reading experiment us-
ing a Bayesian model of incremental belief up-
date. While an initial test of the Beta-binomial
model was encouraging, the predictions of the
Beta-binomial model are highly correlated with
presentation order in the current data set. This
means that it is hard to distinguish between adap-
tation to the task of self-paced reading and syntac-
tic adaptation. Indeed, model comparison suggests
that the Bayesian model does not explain a signif-
icant amount of the variance in reading times once
motor adaptation (as captured by stimulus presen-
tation order) is accounted for. In a secondary anal-
ysis, we did, however, find preliminary evidence
of syntactic adaptation. That is, while the Beta-
binomial model does not seem to capture syntac-
tic belief update adequately, there is evidence that
comprehenders continuously update their syntac-
tic distributions.
25
Teasing apart the effects of motor adaptation
and linguistic adaptation will require experimen-
tal designs in which these two factors are not as
highly correlated as in the present study. Ongoing
work addresses this issue.
Acknowledgements
The authors wish to thank Neal Snider and mem-
bers of the Human Language Processing lab, as
well as three anonymous ACL reviewers for help-
ful discussion and feedback. We are also very
grateful to Jeremy Ferris for help in collecting the
data reported here. This work was supported by
the University of Rochesters Provost Award for
Multidisciplinary Research and NSF grant BCS-
0845059 to TFJ.
References
John Anderson. 1990. The adaptive character of
thought. Lawrence Erlbaum.
Bock and Griffin. 2000. The persistence of structural
priming: Transient activation or implicit learning?
Journal of Experimental Psychology, 129(2):177?
192.
Chang, Dell, and Bock. 2006. Becoming syntactic.
Psychological Review, 113(2):234?272.
Clayards, Tanenhaus, Aslin, and Jacobs. 2008. Per-
ception of speech reflects optimal use of probabilis-
tic cues. Cognition, 108:804?809.
Garnsey, Pearlmutter, Myers, and Lotocky. 1997.
The contributions of verb bias and plausibility to
the comprehension of temporarily ambiguous sen-
tences. Journal of Memory and Language, (37):58?
93.
S.D. Goldinger. 1998. Echoes of echoes? an episodic
theory of lexical access. Psychological Review,
(105):251?279.
Florian Jaeger. in press. Redundancy and reduc-
tion: speakers manage syntactic information density.
Cognitive Psychology.
Just, Carpenter, and Woolley. 1982. Paradigms and
processes in reading comprehension. Journal of Ex-
perimental Psychology: General, 111:228?238.
Rohde. 2005. Linger experiment presentation soft-
ware. http://tedlab.mit.edu/ dr/Linger/.
Schwarz. 1978. Estimating the dimension of a model.
Annals of Statistics, 6:461?464.
Herbert Simon, 1987. 77K New Palgrave Dictionary
of Economics, chapter Bounded Rationality, pages
266?268. Macmillan, London.
Neal Snider and Florian Jaeger. in prep.
Thothathiri and Snedeker. 2008. Give and take:
Syntactic priming during language comprehension.
Cognition, 108:51?68.
Wells, Christiansen, Race, Acheson, and MacDonald.
2009. Experience and sentence comprehension:
Statistical learning and relative clause comprehen-
sion. Cognitive Psychology, 58:250?271.
26
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 45?53,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Close = Relevant? The Role of Context in Efficient Language Production
Ting Qian and T. Florian Jaeger
Department of Brain and Cognitive Sciences
University of Rochester
Rochester, NY 14627 United States
{tqian,fjaeger}@bcs.rochester.edu
Abstract
We formally derive a mathematical model
for evaluating the effect of context rele-
vance in language production. The model
is based on the principle that distant con-
textual cues tend to gradually lose their
relevance for predicting upcoming linguis-
tic signals. We evaluate our model against
a hypothesis of efficient communication
(Genzel and Charniak?s Constant Entropy
Rate hypothesis). We show that the devel-
opment of entropy throughout discourses
is described significantly better by a model
with cue relevance decay than by previ-
ous models that do not consider context ef-
fects.
1 Introduction
In this paper, we present a study on the effect
of context relevance decay on the entropy of lin-
guistic signals in natural discourses. Context rele-
vance decay refers to the phenomenon that contex-
tual cues that are distant from an upcoming event
(e.g. production of a new linguistic signal) are less
likely to be relevant to the event, as discourse con-
tents that are close to one another are likely to be
semantically related. One can also view the words
and sentences in a discourse as time steps, where
distant context becomes less relevant simply due
to normal forgetting over time (e.g. activation de-
cay in memory). The present study investigates
how this decaying property of discourse context
might affect the development of entropy of lin-
guistic signals in discourses. We first introduce
the background on efficient language production
and then propose our hypothesis.
1.1 Background on Efficient Language
Production
The metaphor ?communication channel?, bor-
rowed from Shannon?s information theory (Shan-
non, 1948), can be conceived of as an abstract en-
tity that defines the constraints of language com-
munication (e.g. ambient noise, distortions in ar-
ticulation). For error free communication to occur,
the ensemble of messages that a speaker may utter
must be encoded in a system of signals whose en-
tropy is under the capacity of the communication
channel. Entropy of these signals, in this context,
correlates with the average number of upcoming
messages that the speaker can choose from for a
particular signal (e.g. a word to be spoken) given
preceding discourse context. In other words, if
the average number of choices given any linguis-
tic signal exceeds the channel capacity, it cannot
be guaranteed that the receiver can correctly infer
the originally intended message. Such transmis-
sion errors will reduce the efficiency of language
communication.
Keeping the entropy of linguistic signals be-
low the channel capacity alone is not efficient, for
one can devise a code where each signal corre-
sponds to a distinct message. With a unique choice
per signal, this encoding achieves an entropy of
zero at the cost of requiring a look-up table that
is too large to be possible (cf. Zipf (1935), who
makes a similar argument for meaning and form).
In fact, the most efficient code requires language
users to encode messages into signals of the en-
tropy bounded by the capacity of the channel. One
implication of this efficient encoding is that over
time, the entropy of the signals is constant. One
of the first studies to investigate such constancy
is Genzel and Charniak (2002), in which the au-
thors proposed the Constant Entropy Rate (CER)
hypothesis: in written text, the entropy per sig-
nal symbol is constant across sentence positions in
discourses. That is, if we view sentence positions
as a measure of time steps, then the entropy per
word at each step should be the same in order to
achieve efficient communication (word is selected
as the unit of signal, although it does not have to
45
be case; cf. Qian and Jaeger (2009)).
The difficulty in testing this direct prediction is
computationally specifying the code used by hu-
man speakers to obtain a context-sensitive esti-
mate of the entropy per word. An ngram model
overestimates the entropy of upcoming messages
by relying on only the preceding n-1 words within
a sentence, while in reality the upcoming message
is also constrained by extra-sentential context that
accumulates within a discourse. The more extra-
sentential context that the ngram model ignores,
the higher estimate for entropy will be. Hence,
the CER hypothesis indirectly predicts that the
entropy of signals, as estimated by ngrams, will
increase across sentence positions. While some
studies have found the predicted positive correla-
tion between sentence position and the per-word
entropy of signals estimated by ngrams, most of
them assumed the correlation to be linear (Genzel
and Charniak, 2002; Genzel and Charniak, 2003;
Keller, 2004; Piantadosi and Gibson, 2008). How-
ever, in previous work, we found that a log-linear
regression model was a better fit for empirical data
than a simple linear regression model based on
data of 12 languages (Qian and Jaeger, under re-
view). Why this would be case remained a puzzle.
Our research question is closely related to this
indirect prediction of the Constant Entropy Rate
hypothesis. Intuitively, the number of possible
messages that a speaker can choose from for an
upcoming signal in a discourse is often restricted
by the presence of discourse context. Contex-
tual cues in the preceding discourse can make the
upcoming content more predictable and thus ef-
fectively reduces signal entropy. As previously
mentioned, however, different contextual cues, de-
pending on how long ago they were provided, have
various degrees of effectiveness in reducing sig-
nal entropy. Thus we ask the question whether
the decay of context relevance could explain the
sublinear relation between entropy and discourse
progress that has been observed in previous stud-
ies.
We formally derive two nonlinear models for
testing our Relevance Decay Hypothesis (intro-
duced next). In addition to the constant entropy as-
sumption in CER, our model assumed that the rel-
evance of early sentences in the discourse system-
atically decays as a function of discourse progress.
Our models provide the best fit to the distribution
of entropy of signals, suggesting the availability
of discourse context can affect the planning of the
rest of a discourse.
1.2 Relevance Decay Hypothesis
We hypothesize the sublinear relation between the
entropy of signals, when estimated out of dis-
course context (hereafter, out-of-context entropy
of signals) using an ngram model, and sentence
position (Piantadosi and Gibson, 2008; Qian and
Jaeger, under review) is due to the role of dis-
course context (hereafter, context). Consider the
following example. Assume that context at the kth
sentence position comes from the 1 . . . k ? 1 sen-
tences in the past. If k is large enough, context
from the early sentences 1 . . . i (i  k) is essen-
tially no longer relevant. Rather, the nearby k ? i
sentences are contributing most of the discourse
context. As a result, the constraint on the entropy
of signals at sentence position k is mostly due to
the nearby window of k ? i sentences. Then if we
look ahead to the (k + 1)th sentence position and
follow the same steps of reasoning, context at that
point also mostly comes from the nearby window
of k? i sentences (i.e. (k+ 1)? (i+ 1) = k? i).
Hence, for later sentence positions, the difference
in available context is minimal. Consequently,
their out-of-context entropy of signals increases
at a very small rate. On the other hand, when k
is fairly small, to the extent that the k ? i win-
dow covers the entire preceding discourse, all of
the 1 . . . k ? 1 sentences are contributing relevant
context. As k increases, the number of preced-
ing sentences increases, which results in a more
significant change in relevant context, but the rel-
evance of each individual sentence decreases with
its distance to k, which results in a sublinear pat-
tern of relevant context with respect to sentence
position overall. As we will show, the relation of
out-of-context entropy of signals to sentence posi-
tion follows from the relation of relevant context
to sentence position, exhibiting a sublinear form
as well.
The problem of interest here is to specify how
quickly the relevance of a preceding sentence de-
cays as a function of its distance to a target sen-
tence position k. We experimented with two forms
of decay functions ? power law decay and expo-
nential decay. It has been established that many
types of human behaviors can be well described by
the power function (Wixted and Ebbesen, 1991),
so we mainly focus on building a model under the
46
Language Training Data Test Data
in words in sentences in words in sentences per position
Danish 154,514 5,640 8,048 270 18
Dutch 50,309 3,255 2,105 90 6
English 597,698 23,295 31,276 1155 77
French 229,461 9,300 11,371 435 29
Italian 97,198 4,245 4,524 225 15
Mandarin Chinese 145,127 4,875 4,310 150 10
Norwegian 89,724 4,125 2,973 150 10
Portuguese 170,342 5,340 9,044 240 16
Russian 398,786 18,075 20,668 930 62
Spanish (Latin-American) 1,363,560 41,160 67,870 2,070 138
Spanish (European) 255,366 7,485 8,653 240 16
Swedish 266,348 11,535 13,369 555 37
Table 1: Number of words and sentences in the training and test data for each of the twelve languages.
The last column gives the number of sentences at each sentence position (which is identical to the number
of documents contained in the corpora).
power law, and examine if the model under the ex-
ponential law yields any difference. Under the as-
sumptions of true entropy rate is constant across
sentences, we predict that our models will bet-
ter characterize the changes in estimated entropy
of signals than general regression models that are
blind to the role of context.
2 Methods
2.1 Data
We used the Reuters Corpus Volume 1 and 2
(Lewis et al, 2004). The corpus contains about
810,000 English news articles and over 487,000
news articles in thirteen languages. Because of in-
consistent annotation, we excluded the data from
three languages, Chinese, German, and Japanese.
For Chinese, we substituted the Treebank Cor-
pus (Xue et al, 2005) for the Reuters data, leav-
ing us with twelve languages: Danish, Dutch,
English, French, Italian, Mandarin Chinese, Nor-
wegian, Portuguese, Russian, European Spanish,
Latin-American Spanish, and Swedish. In order
to estimate out-of-context entropy per word (i.e.
per signal symbol) for each sentence position, ar-
ticles were divided into a training set (95% of all
stories) for training language models and a test set
(the remaining 5%) for analysis (see Table 1 for
details). Out-of-context entropy per word was es-
timated by computing the average log probability
of sentences at that position, normalized by their
lengths in words (i.e. for an individual sentence
token s, the term to be averaged is ? log p(s)length(s) bits per
word). Standard trigram language models were
used to compute these probabilities (Clarkson and
Rosenfeld, 1997). The majority of the 12 lan-
guages belong to the Indo-European family, while
Mandarin Chinese is a Sino-Tibetan language.
2.2 Modeling Relevance Decay of Context
Formally, we define the relevance of context in the
same unit as entropy of signals ? bits per word.
Let r0 denote the entropy of signals that efficiently
encode the ensemble of messages a speaker can
choose from for any sentence position, a constant
under the assumption of CER. According to Infor-
mation Theory, r0 is equivalent to the uncertainty
associated with any sentence position if context is
considered. Thus, in error free communication,
linguistic signals presented at the kth sentence po-
sition are said to have resolved the uncertainty at
k and therefore are r0-bit relevant at the kth sen-
tence position. Then, at the (k+i)th sentence posi-
tion, these linguistic signals have become context
by definition and their relevance has decayed to
some r bits. Our models start from defining the
value of r as a function of the distance between
context and a target sentence position.
2.2.1 Power-law Decay Model
If the relevance of a cue q (e.g. a preceding sen-
tence), which is originally r0-bit relevant at po-
sition kq, decays at the rate following the power
function, its remaining relevance at target sentence
position k is:
relevancepow(k, q) = r0(k ? kq + 1)
?? (1)
In Equation (1), k > kq and ? is the decay rate.
This means at position k, the relevance of the cue
47
from the (k?1)th sentence is r0?2??-bit relevant;
the relevance of the cue from the (k?2)th sentence
is r0 ? 3??-bit relevant, and so on. As a result, the
relevance of discourse-specific context at position
k is the marginalization of all cues up to qk?1:
contextpow(k) = r0
?
qi?{q1...qk?1}
(k ? kqi + 1)
?? (2)
The general trend predicted by Equation (2)
is that discourse-specific context increases more
rapidly at the beginning of a discourse and much
more slowly towards the end due to the relevance
decay of distant cues. Rewriting Equation (2) in
a closed-form formula so that a model can be fit-
ted to data is not a trivial task without knowing the
rate ?, but the paradox is that ? has to be estimated
from the data. As a workaround, we approximated
the value of Equation (2) by computing a definite
integral of Equation (1), where ?i is a shorthand
for k ? kq + 1:
contextpow(k) ?
? k
1
r0?i
??d?i
= r0(
k1?? ? 1
1? ?
) (3)
Equation (3) uses an integral to approximate the
sum of a series defined as a function. The result
is usually acceptable as long as ? is greater than
1 so that the series defined by Equation 1 is con-
vergent (this assumption is empirically supported;
see Figure 5). Note that Equation (3) produces
the desirable effect that upon encountering the
first sentence of a discourse, no discourse-specific
contextual cues are available to the speaker (i.e.
context(1) = 0).
Now that we know the maximum relevance of
context at sentence position k, we can predict the
amount of out-of-context entropy of signals r(k)
based on the idea of uncertainty again. There are
new linguistic signals that are r0-bit relevant in
context at any sentence position. In addition, we
now know context(k) bits of relevant context are
also available. Thus, the sum of r0 and context(k)
defines the maximum amount of out-of-context
uncertainty that can be resolved at sentence posi-
tion k. Therefore, the out-of-context entropy of
signals at k is at most:
rpow(k) = context(k) + r0 (4)
= r0
k1?? ? 1
1? ?
+ r0
Whether speakers will utilize all available con-
text as predicted by Equation (4) is another de-
bate. Here we adopt the view that speakers are
maximally efficient in that they do make use of
all available context. Thus, we make the predic-
tion that out-of-context entropy of signals, as ob-
served empirically from data, can be described by
this model. Figure 1 shows the behavior of this
function with various parameter sets.
2 4 6 8 10 12 14
5
6
7
8
9
10
11
12
Sentence Position
Mod
el?P
redic
ted E
ntro
py p
er W
ord
r0 = 5.5,? = 2r0 = 5.5,? = 2.2r0 = 5,? = 2r0 = 5,? = 2.2
Figure 1: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is a power function.
2.2.2 Exponential Decay Model
The second model assumes the relevance of con-
text decays exponentially. Following the same no-
tations as before, the relevance of a cue q at posi-
tion k is:
relevanceexp(k, q) = r0e
??(k?kq) (5)
The major difference between the power func-
tion and the exponential one is that the relevance
of a contextual cue drops more slowly in the expo-
nential case (Anderson, 1995). The relevance of
all discourse-specific context for a speaker at k is:
contextexp(k) = r0
k?1?
i=1
e??i (6)
48
Equation (6) is the sum of a geometric progres-
sion series. We can write Equation (6) in a closed-
form:
contextexp(k) =
r0
e? ? 1
(1? e?(k?1)?) (7)
As a result, the out-of-context entropy of signals
is:
rexp(k) =
r0
e? ? 1
(1? e?(k?1)?) + r0 (8)
Figure 2 schematically shows the behavior of
this function. One can notice this function con-
verges against a ceiling more quickly than the
power function. Thus, this model makes a slightly
different prediction from the power law model.
2 4 6 8 10 12 14
5
6
7
8
9
10
11
12
Sentence Position
Mod
el?P
redic
ted E
ntro
py p
er W
ord
r0 = 5.5,? = 0.6r0 = 5.5,? = 0.8r0 = 5,? = 0.6r0 = 5,? = 0.8
Figure 2: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is an exponential func-
tion.
2.3 Nonlinear Regression Analysis
To test whether the proposed models (i.e. Equa-
tions 4 and 8) better characterize the data, we
built nonlinear regression models with document-
specific random effects, where the out-of-context
entropy of signals, rij , is regressed on sentence
position, kj . Based on the power law model, we
have
rij = (?1+b1i)
kj1??2 ? 1
1? ?2
+(?1+b1i)+ij (9)
where ?1 corresponds to r0, the theoretical con-
stant entropy of signals under an ideal encod-
ing. b1i represents the document-specific devia-
tions from the overall mean. ?2 corresponds to ?,
the mean rate at which the relevance of a past cue
decays, which is unfortunately not considered for
random effects for the practical purpose of making
computation feasible in the current work. Finally,
ij represents the errors independently distributed
as N (0, ?2), orthogonal to document specific de-
viations.
For the exponential model, the nonlinear model
is the following (symbols have the same interpre-
tations as in Equation 9):
rij =
(?1 + b1i)
e?2 ? 1
(1?e?(kj?1)?2)+(?1+b1i)+ij
(10)
Fitting data with the above nonlinear models
requires starting estimates for fixed-effect coeffi-
cients (i.e. ?1s and ?2s). Unfortunately, there are
no principled methods for selecting these values.
We heuristically selected 6 for ?1 and 2 for ?2 as
starting values for the power law model, and 4 and
0.5 as starting values for the exponential model.
3 Results
We examined the quality of the models and the pa-
rameters in the models: r0, the within-context en-
tropy rate, and ?, the rate of context decay.
3.1 Model Quality Comparison
The CER hypothesis indirectly predicts that out-
of-context entropy of signals of sentence positions
(bits per word) should increase throughout a dis-
course. The two models go one step further to
predict specific sublinear increase patterns, based
on the speaker?s considerations of the relevance of
past contextual cues. We compared the quality of
models in terms of Bayesian Information Criterion
(BIC) within languages. A lower BIC score indi-
cates a better fit. As shown by Figure 3, we find
our models best explain the data in 9 out of the 12
languages, reporting lower BIC scores than both
the linear and log-linear models as reported in our
previous work (Qian and Jaeger, under review).
For Danish, English and Italian, although neither
of our models produced a better score than the log-
linear model, the relative difference is small: 0.54
on average (comparing to BIC scores on the order
of 102 to 103).
49
Danish Dutch English French Italian Mandarin Norwegian Portuguese Russian E.Spanish L.Spanish Swedish
Power Law Exponential Loglinear Linear
?
20
?
10
0
10
20
Figure 3: Our models yield superior BIC scores in most languages. The y-axis shows the differences
between BIC scores of individual models for a language and mean BIC of the models for that language
(E.Spanish = European Spanish; L.Spanish = Latin-American Spanish).
Specifically, in terms of BIC scores, the power-
law model is better than the linear model (t(11) =
?3.98, p < 0.01), and the log-linear model
(t(11) = ?3.10, p < 0.05). The exponen-
tial model is also better than the linear model
(t(11) = ?3.98, p < 0.01), and the log-linear
model (t(11) = ?3.18, p < 0.01). The power-
law model and the exponential model are not sig-
nificantly different from each other (t(11) = 0.5,
p > 0.5).
3.2 Interpretation of Parameters
Constant Entropy of Signals r0. Both models
are constructed in such a way that the first param-
eter r0, in theory, corresponds to the theoretical
within-context entropy of signals of sentence po-
sitions. This parameter refers to how many bits per
word are needed to encode the ensemble of mes-
sages at a sentence position when context is taken
into account. The CER hypothesis directly pre-
dicts that this rate should be constant throughout
a discourse. Although we are unable to test this
prediction directly, it is nevertheless interesting to
compare whether these two independently devel-
oped models yield the same estimates for this pa-
rameter in each language.
Figure 4 shows encouraging results. Not only
the estimates made by the power model are well
correlated with those by the exponential model,
but also the slope of this correlation is equal to 1
(t(10) = 1.01, p < 0.0001). Since there are no
reasons a priori to suspect that these two models
l
l
4.0 4.5 5.0 5.5 6.0 6.5
4.0
4.5
5.0
5.5
6.0
6.5
Estimated Within?context Entropy per Word (Exponential Model)
Estim
ated
 With
in?c
onte
xt En
tropy
 per 
Word
 (Pow
er?
law M
odel
)
l
l
DanishDutchEnglishFrenchItalianMandarinNorwegianPortugueseRussianE. SpanishL. SpanishSwedish
Figure 4: Estimates of r0 correlate between both
models with a slope of 1.
would give the same estimates, this is a first step to
confirming the entropy per word in sentence pro-
duction is indeed a tractable constant throughout
discourses.
Among all languages, r0 has a mean of 5.0
bits in both models, and a variance of 0.46 in
the power-law model and 0.48 in the exponential
model, both remarkably small. The similarity in
r0 between languages may lead one to speculate
whether the amount of uncertainty per word in dis-
courses is largely the same regardless of the actual
language used by the speakers. On the other hand,
50
the differences in r0 may reveal the specific prop-
erties of different languages. Meanwhile, precau-
tions need to be taken in interpreting those esti-
mates given that the corpora are of different sizes,
and the ngram model is simplistic in nature.
Decay Rate ?. The second parameter ? corre-
sponds to the rate of relevance decay in both mod-
els. Since the base relevance r0 varies between
languages, ? can be more intuitively interpreted as
to indicate the percentage of the original relevance
of a contextual cue still remains in n positions. In
the power-law model, for example, the context in-
formation from a previous sentence in Danish, on
average, is only 11.6% (2?3.10 = 0.116) as rele-
vant. Hence, the relevance of a contextual cue de-
creases rather quickly for Danish. Table 2 shows
this is in fact the general picture for all languages
we tested.
Language Relevance of Context in Discourse (%)
1 pos. before 2 pos. before 3 pos. before
Danish 11.6 3.3 1.4
Dutch 10.4 2.8 1.1
English 0.1 0.0 0.0
French 8.5 2.0 0.7
Italian 10.2 2.7 1.0
Mandarin 7.7 1.7 0.6
Norwegian 18.9 7.1 3.6
Portuguese 5.5 1.0 0.3
Russian 12.7 3.8 1.6
E. Spanish 0.8 0.0 0.0
L. Spanish 2.7 0.3 0.1
Swedish 5.8 1.1 0.3
Table 2: In the power model, relevance of a con-
textual cue decays rather quickly for each lan-
guage.
The picture of ? looks a little different in the
exponential model. The relevance percentage on
average is significantly higher, which confirms an
earlier point that the power function decreases
more quickly than the exponential function. Table
3 shows a summary for the 12 languages.
One may note that the decay rate varies greatly
between languages under the prediction of both
models. However, these number are only approxi-
mations since the entropy estimated by the ngram
language model is far from psychological real-
ity. Furthermore, it is unlikely that speakers of
one language would exhibit the same decay rate
of context relevance in their production, let alne
speakers of different languages, who may be sub-
ject to language-specific constraints during pro-
Language Relevance of Context in Discourse (%)
1 pos. before 2 pos. before 3 pos. before
Danish 30.1 9.1 2.7
Dutch 28.7 8.2 2.4
English 9.6 0.9 0.1
French 26.7 7.1 1.9
Italian 28.7 8.2 2.4
Mandarin 25.7 6.6 1.7
Norwegian 42.3 17.9 7.6
Portuguese 22.5 5.1 1.1
Russian 34.6 12.0 4.2
E. Spanish 14.2 2.0 0.3
L. Spanish 18.6 3.5 0.6
Swedish 23.7 5.6 1.3
Table 3: In the exponential model, relevance of a
contextual cue decays more slowly.
duction. Therefore, the variation in estimates of
? seems reasonable.
Correlation between r0 and ?. Interestingly, r0
and ? are highly correlated (r2 = 0.39, p < 0.05
in the power model, Figure 5; r2 = 0.47, p < 0.01
in the exponential model, Figure 6): a high rel-
evance decay rate tends to be coupled with high
within-context entropy of signals. This unan-
ticipated observation is in fact compatible with
the account of efficient language production: a
high within-context entropy of signals indicates
the base relevance of a contextual cue (i.e. r0)
is high. It is then useful for its relevance to de-
cay more quickly to allow the speaker to inte-
grate context from other cues. Otherwise, the to-
tal amount of relevant context may presumably
overload working memory. However, our cur-
rent results come from only cross-linguistic sam-
ples. Cross-validation in within-language samples
is needed for confirming this hypothesis.
3.3 The Bigger Picture
Having obtained the estimates for r0 and ?, we are
now in a position to examine how out-of-context
entropy of signals increases as a function of sen-
tence positions, given the estimates of these two
parameters. As shown in Figure 7, the predictions
from both models are qualitative similar except
that 1) when the decay rate in the power-law model
is low, out-of-context entropy of signals converges
more slowly than in the exponential model (Figure
7, right panel); 2) when the decay rate in the power
model is high, it almost converges as quickly as
the exponential model, and only minor differences
exist in their predictions (Figure 7, left panel).
51
ll
4.0 4.5 5.0 5.5 6.0
4
6
8
10
Within?Context Entropy per Word
Rele
van
ce 
Dec
ay R
ate
l
l
DanishDutchEnglishFrenchItalianMandarin
NorwegianPortugueseRussianE. SpanishL. SpanishSwedish
Figure 5: The rate of relevance decay is corre-
lated with within-context entropy of signals in the
power-law model.
l
l
4.5 5.0 5.5 6.0
1.0
1.5
2.0
Within?Context Entropy per Word
Rele
van
ce 
Dec
ay R
ate
l
l
DanishDutchEnglishFrenchItalianMandarin
NorwegianPortugueseRussianE. SpanishL. SpanishSwedish
Figure 6: The rate of relevance decay is correlated
with within-context entropy of signals in the expo-
nential model.
Because of the nonlinearity in our models, it
is not possible to report the results in an intuitive
manner as in ?an increase in sentence position cor-
responds to an increase ofX bits of out-of-context
entropy per word?. Instead, we can analytically
solve for the derivative of the predicted out-of-
context entropy of signals with respect to sentence
position (Equation 4 and 8). This gives us:
rpower(k)
? = r0k
?? (11)
for the power-law model, showing the rate of in-
crease in predicted out-of-context entropy of sig-
nals is a monotonically decreasing power function,
and
l
l
l l
l
l
l
l
l
l l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
2 4 6 8 12
4
5
6
7
8
Sentence Position
Out
?of?
con
text
 En
trop
y pe
r W
ord 
in D
utch
Power:? = 3.27Exp:? = 1.25 l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l l l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l l l
l
l
ll
l
l
l
l
l
l
l
l
l
2 4 6 8 12
4
5
6
7
8
Sentence Position
Out
?of?
con
text
 En
trop
y pe
r W
ord 
in N
orw
egia
n
Power:? = 2.4Exp:? = 0.86
Figure 7: Predicted out-of-context entropy of sig-
nals by the power-law model (solid) and the expo-
nential model (dashed) in Dutch and Norwegian,
with the actual distributions plotted on the back-
ground.
rexp(k)
? =
r0?
e? ? 1
(e?(k?1)?) (12)
for the exponential model, showing the rate of in-
crease is a monotonically decreasing exponential
function. These mathematical properties indeed
match our observations in Figure 7.
4 Discussion and Future Work
The models introduced in this paper try to answer
this question: if the relevance of a contextual cue
for predicting an upcoming linguistic signal de-
cays over the course of a discourse, how much un-
certainty (entropy) is associated with each individ-
ual sentence position? We have shown under that
models that incorporate (power law or exponen-
tial) cue relevance decay in most cases describe
the relation of out-of-context entropy of signals
to sentence position are better accounted for than
previously suggested models.
We are continuing to investigate along this line.
Specifically, we are interested in finding the role of
semantic memory in affecting the relevance decay
of context. To test that, we plan to implement a
probabilistic topic model, in which topic continu-
ity between a preceding sentence and an upcom-
ing sentence is quantitatively measured. Thus, the
decay of contextual cues can be based on the esti-
52
mated semantic relatedness between sentences, in
addition to the abstract notion of rate as used in
this paper.
Finally, our relevance decay model can be ap-
plied to the domain of language processing as
well. For instance, the distance between a con-
textual cue and the target word may affect how
quickly a comprehender can process the informa-
tion conveyed by the word. We plan to address
these question in future work.
5 Conclusion
We have presented a new approach for examin-
ing the distribution of entropy of linguistic sig-
nals in discourses, showing that not only the out-
of-context entropy of signals increases sublinearly
with sentence position, but also the sublinear trend
is better explained by our nonlinear models than
by log-linear models of previous work. Our mod-
els are built on the assumption that the relevance
of a contextual cue for predicting a linguistic sig-
nal in the future decays with its distance to the tar-
get, and predict the relation of out-of-context en-
tropy of signals to sentence position in discourses.
These results indirectly lend support to the hypoth-
esis that speakers maintain a constant entropy of
signals across sentence positions in a discourse.
Acknowledgements
We wish to thank Meredith Brown, Alex Fine and
three anonymous reviewers for their helpful com-
ments on this paper. This work was supported by
NSF grant BCS-0845059 to TFJ.
References
John R. Anderson. 1995. Learning and Memory: An
integrated approach. John Wiley & Sons.
Philip R. Clarkson and Roni Rosenfeld. 1997. Sta-
tistical language modeling using the cmu-cambridge
toolkit. In Proceedings of ESCA Eurospeech.
Dimitry Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In ACL, pages 199?206.
Dimitry Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function
of the sentence number. in. In EMNLP, pages 65?
72.
Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In EMNLP, pages 317?324.
D. D. Lewis, Y. Yang, T. Rose, and F Li. 2004. Rcv1:
A new benchmark collection for text categorization
research. J Mach Learn Res, 5:361?397.
Steve Piantadosi and Edwards Gibson. 2008. Uniform
information density in discourse: a cross-corpus
analysis of syntactic and lexical predictability. In
CUNY.
Ting Qian and T. Florian Jaeger. 2009. Evidence
for efficient language production in chinese. In
CogSci09, pages 851?856.
Ting Qian and T. Florian Jaeger. under review. En-
tropy profiles in language: A cross-linguistic inves-
tigation.
C. E. Shannon. 1948. A mathematical theory of com-
munications. Bell Labs Tech J, 27(4):623?656.
J. T. Wixted and E. B. Ebbesen. 1991. On the form of
forgetting. Psychological Science, 2:409?415.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Nat Lang
Eng, 11:207?238.
G. K. Zipf. 1935. Psycho-Biology of Languages.
Houghton-Mifflin.
53
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 10?19,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
A Bayesian belief updating model of phonetic recalibration and selective
adaptation
Dave Kleinschmidt1 and T. Florian Jaeger1,2
Departments of 1Brain and Cognitive Sciences and 2Computer Science
University of Rochester
Rochester, NY, USA
{dkleinschmidt,fjaeger}@bcs.rochester.edu
Abstract
The mapping from phonetic categories to
acoustic cue values is highly flexible, and
adapts rapidly in response to exposure. There
is currently, however, no theoretical frame-
work which captures the range of this adap-
tation. We develop a novel approach to mod-
eling phonetic adaptation via a belief-updating
model, and demonstrate that this model natu-
rally unifies two adaptation phenomena tradi-
tionally considered to be distinct.
1 Introduction
In order to understand speech, people map a contin-
uous, acoustic signal onto discrete, linguistic cate-
gories, such as words. Despite a long history of re-
search, no invariant mapping from acoustic features
to underlying linguistic units has yet been found.
Some of this lack of invariance is due to random
factors, such as errors in production and percep-
tion, but much is due to systematic factors, such as
differences between speakers, dialects/accents, and
speech conditions.
The human speech perception system appears to
deal with the lack of invariance in two ways: by stor-
ing separate, speaker-, group-, or context-specific
representations of the same categories (Goldinger,
1998), and by rapidly adapting phonetic categories
to acoustic input. Even though a person?s inven-
tory of native language phonetic categories is gen-
erally fixed from an early age (Werker and Tees,
1984), the mapping between these categories and
their acoustic realizations is flexible. Listeners adapt
rapidly to foreign-accented speech (Bradlow and
Bent, 2008) and acoustically distorted speech (Davis
et al, 2005), showing increased comprehension af-
ter little exposure. Such adaptation results in tem-
porary and perhaps speaker-specific changes in pho-
netic categorization (Norris et al, 2003; Vroomen et
al., 2007; Kraljic and Samuel, 2007).
To our knowledge, there is no theoretical frame-
work which explains the range and specific pat-
terns of adaptation of phonetic categories. In this
paper, we propose a novel framework for under-
standing phonetic category adaptation?rational be-
lief updating?and develop a computational model
within this framework which straightforwardly ex-
plains two types of phonetic category adaptation
which are traditionally considered to be separate.
While phonetic category adaptation has not thus
far been described in this way, it nevertheless shows
many hallmarks of rational inference under uncer-
tainty (Jacobs and Kruschke, 2010). When there is
another possible explanation for strange pronunci-
ations (e.g. the speaker has a pen in her mouth),
listeners do not show any adaptation (Kraljic et
al., 2008). Listeners are more willing to gener-
alize features of a foreign accent to new talkers
if they were exposed to multiple talkers initially,
rather than a single talker (Bradlow and Bent, 2008).
Listeners also show rational patterns of generaliza-
tions of perceptual learning for specific phonetic
contrasts, generalizing to new speakers only when
the adapted phonetic categories of the old and new
speakers share similar acoustic cue values (Kraljic
and Samuel, 2007).
While it is not conclusive, the available evidence
suggests that listeners update their beliefs about pho-
10
1100 1680F2 at closure (Hz)
/aba/ /ada/
Pr
op
or
tio
n 
'ba
' re
sp
on
se
s
Pr
op
or
tio
n 
/b/
 re
sp
on
se
s
/aba/ /ada/
0
1
Pr
op
or
tio
n 
'ba
' re
sp
on
se
s
/aba/ /ada/
Figure 1: Left: approximate distribution of acoustic cue values for /aba/ and /ada/ stimuli from Vroomen et al
(2007). Right: exposure to acoustically ambiguous /aba/ tokens results in recalibration of the /aba/ category, with
the classification boundary shifting towards /ada/ (center-right), while exposure to unambiguous /aba/ tokens results
in selective adaptation of the /aba/ category, where the classification boundary shifts towards /aba/ (far right).
netic categories based on experience in a rational
way. We propose that Bayesian belief updating
can provide a principled computational framework
for understanding rapid adaptation of phonetic cate-
gories as optimal inference under uncertainty. Such
a framework has the appeal of being successfully ap-
plied in other domains (Brenner et al, 2000; Fine
et al, 2010). In addition, rational models have also
been used within the domain of speech perception to
model acquisition of phonetic categories (Vallabha
et al, 2007; Feldman et al, 2009a; McMurray et al,
2009), the perceptual magnet effect (Feldman et al,
2009b), and how various cues to the same phonetic
contrast can be combined (Toscano and McMurray,
2010).
2 The Phenomena: Perceptual
recalibration and selective adaptation
The flexibility of phonetic categories has been
demonstrated through studies which manipulate the
distribution of acoustic cues associated with a par-
ticular category. These studies take advantage of
the natural variability of acoustic cues. Take, for
example, the consonants /b/ and /d/. These two
consonants can be distinguished largely on the ba-
sis of the trajectory of the second formant before
and after closure (Iskarous et al, 2010). Like all
acoustic-phonetic cues, there is natural variability in
the F2 locus for productions of each category (de-
picted schematically in Figure 1, left). Listeners re-
act to subtle changes in the distributions of acous-
tic cues, and adjust their phonetic categories for a
variety of contrasts and manipulations (Kraljic and
Samuel, 2006). In this paper, we model the effects
of the two most common types of manipulation stud-
ied thus far, which produce opposite changes in pho-
netic classification.
The first of these is repeated exposure to acousti-
cally ambiguous tokens, which results in a change in
classification termed ?perceptual learning? (Norris
et al, 2003) or ?perceptual recalibration? (Bertelson
et al, 2003) in which the initially-ambiguous token
becomes an accepted example of one phonetic cate-
gory. Such ambiguous cue values are not uncommon
because of the natural variability in normal speech.
It is thus possible to generate a synthetic production
/?/ which is acoustically intermediate between /b/
and /d/, and which is phonetically ambiguous in the
absence of other cues but nevertheless sounds like a
plausible production. When paired with another cue
which implies /b/, subjects reliably classify /?/ as
/b/. Disambiguating information could be provided
by a video of a talker producing /b/ (Vroomen et al,
2007), or a word such as a?out, where a /b/ has been
replaced with /?/ (Norris et al, 2003). When /?/ is
repeatedly paired in this way with information bias-
ing a /b/ interpretation, subjects begin to interpret
/?/ as /b/ in general, classifying more items on a
/b/-to-/d/ continuum as /b/ (Figure 1, center-right,
red curve).
A second manipulation is repeated exposure to
the same, acoustically unambiguous token. Re-
peated exposure to /b/ causes ?selective adaptation?
of this category, where listeners are less likely to
11
classify items as /b/, indicated by a shift in the /b/-
/d/ classification boundary towards /b/ (Figure 1,
far-right).
Traditionally, recalibration and selective adapta-
tion have been analyzed as separate processes,
driven by separate underlying mechanisms
(Vroomen et al, 2004), since they arise under
different circumstances and produce opposite
effects on classification. They also show different
time courses. Vroomen et al (2007) found that,
on the one hand, strong recalibration effects occur
after just a few exposures to ambiguous tokens, but
fade with further exposure (Figure 3, upper curve).
On the other, selective adaptation is present after a
few exposures to unambiguous tokens, but grows
steadily stronger with further exposure (Figure 3,
lower curve).
We will show that these two superficially differ-
ent adaptation phenomena are actually closely re-
lated, and will provide a unified account by appeal-
ing to principles of Bayesian belief updating. These
principles are used to construct two models. The
first, a unimodal model, treats phonetic categories
as distributions over acoustic cue dimensions. The
second, a multimodal model, treats phonetic cate-
gories as distributions over phonetic cue dimensions,
which integrate information from both audio and vi-
sual cues. Both models capture the general effect
directions of selective adaptation and recalibration,
but only the multimodal model captures their dis-
tinct time courses.
The next section provides a high-level descrip-
tions of these models, and how they might describe
the selective adaptation and recalibration data of
Vroomen et al (2007). Section 4 describes this data
and the methods used to collect it in more details.
Section 5 describes the general modeling frame-
work, how it was fit to the data, and the results, and
Section 6 describes the multimodal model and its fit
to the data.
3 Phonetic category adaptation via belief
updating
In our proposed framework, the listener?s classifica-
tion behavior can be viewed as arising from their be-
liefs about the distribution of acoustic cues for each
phonetic category. Specifically, as we will develop
ll
l ll
l
l
l l
ll
l
ll l
l
l
ll
l
l
l
l
l l
l
l
l l
l
ll
l
l
l
l l
l
l l
l
l
l ll
ll
l
l
ll l
ll
l
l l
ll
l l
ll
l l
l
l
l
ll
l
l l
l
ll
l
l
ll
l
l
l
l
ll
l
l
l
l
llll
ll ll
l
l
l
l
l
l ll l ll
l
l
l
l
ll
ll
ll
l
l l
l
ll
l
l
l
l
l
l
l l
l
lll
l
l
l
l
l
lll
l
ll l
lll
l
ll
l
l
lll
ll l
ll
l
ll
l
l
l l
l
l
l
l ll ll ll
l
l
l
l
ll l
l
l
ll
l
Figure 2: An incremental belief-updating model for pho-
netic recalibration and selective adaptation. These distri-
butions correspond to the classification functions in Fig-
ure 1. Left: ambiguous stimuli labeled as /b/ cause a
shift of the /b/ category towards those stimuli. Right:
repeated unambiguous stimuli correspond to a narrower
distribution than expected.
more rigorously below, the probability of classify-
ing a given token x (which is the value of either an
acoustic cue or a multimodal, phonetic cue) as /b/
is proportional to the relative likelihood of the cue
value x arising from /b/ (relative to the overall like-
lihood of observing tokens like x, regardless of cat-
egory). Thus, changes in the listener?s beliefs about
the distribution of cue values of category /b/ will re-
sult in changes in their willingness to classify tokens
as /b/.
A belief-updating model accounts for recalibra-
tion and selective adaptation in the following way.
When, on the one hand, a listener encounters many
tokens that they consider to be /b/ but which are all
acoustically intermediate between /b/ and /d/, they
will change their beliefs about the distribution of
/b/, shifting it to better align with these ambiguous
cue values (Figure 2, left). This results in increased
categorization of items on a /b/-to-/d/ continuum
as /b/, since the range on the continuum over which
the likelihood associated with /b/ is higher than that
of /d/ is extended.
On the other hand, when a listener encounters
repeated, tightly-clustered and highly prototypical
/b/ productions, they update their beliefs about the
distribution of /b/ to reflect that /b/ productions
are more precise than they previously believed (Fig-
ure 2, right). They consequently assign lower likeli-
hood to intermediate, ambiguous cue values for /b/,
causing them to classify fewer /b/-/d/ continuum
items as /b/.
Modeling the time course of selective adaptation
12
Exposures
/b/?
/d/ 
diff
er
en
ce
 sc
or
e
?0.5
0.0
0.5
0 50 100 150 200 250
Acoustic stimulus
ambiguous
unambiguous
Figure 3: The results of Vroomen et al (2007), show-
ing the build-up time course of selective adaptation (as a
function of unambiguous exposure trials) and recalibra-
tion (as a function of ambiguous exposure trials).
is straightforward: the more observations are made,
the narrower the distribution becomes, and the more
the classification boundary shifts towards the adapt-
ing category. However, modeling the time course
of recalibration, as measured by Vroomen et al
(2007), is more complicated. Recalibration comes
on quickly, but fades gradually with many expo-
sures (Figure 3). As discussed below in Section 5.3,
the unimodal model cannot account for this pattern,
because it consideres the acoustically-similar expo-
sure and test stimuli the same. The multimodal
model, by integrating audio and visual cues to form
the adapting percept, dissociates the adapting stim-
ulus from the test stimuli and does not suffer from
this problem. It is thus in principle capable of re-
producing the empirical time course of recalibration
observed by Vroomen et al (2007). In practice, this
model does indeed provide a good qualitative fit to
human data, as discussed in Section 6.
4 Behavioral data: Vroomen et al (2007)
Vroomen et al (2007) investigated the time course
of adaptation to audio-visual speech stimuli. In each
block, subjects were repeatedly exposed to a sin-
gle type of stimulus. The visual stimulus was either
/aba/ or /ada/, and the audio stimulus was either an
unambiguous match of the visual stimulus or was an
ambiguous production. Throughout exposure, sub-
jects were tested with unimodal acoustic test stimuli
in order to measure the effect of exposure thus far.
?j ?j
xi ci
N
M
?j ? Normal(?0j ,?)
?j ? Gamma(?,?)
ci ? Categorical(pi)
xi ? Normal(?ci ,?ci)
Figure 1: Graphical model for MOG observations with independent priors on
component parameters. Categories are indexed by j and observations are in-
dexed by i.
?j ?j
xi ci
N
M
?j ? Normal(?0j ,??j)
?j ? Gamma(?,?)
ci ? Categorical(pi)
xi ? Normal(?ci ,?ci)
Figure 2: Graphical model for MOG with Normal-Gamma prior on component
parameters. Categories are indexed by j = 1 . . .N and observations are indexed
by i = 1 . . .M .
1
Figure 4: Graphical model for the mixture of Gaussians
with n rmal-gamma prior model. See text for descrip-
tion.
The overall effect of exposure to unambiguous stim-
uli was computed by comparing classification be-
tween unambiguous-/b/ and unambiguous-/d/ ex-
posure, and likewise for the effect of exposure to
ambiguous stimuli.
The acoustic stimuli used in exposure and test
were drawn from a nine-item continuum (denoted
x = 1, . . . , 9) from /aba/ to /ada/, formed by ma-
nipulating the second formant frequency before and
after the stop consonant (Vroomen et al, 2004). The
most /aba/-like item x = 1 was synthesized us-
ing the formant values from a normal /aba/ pro-
duction, and the most /ada/-like item x = 9 was
derived from an /ada/ production. The maximally
ambiguous item was determined for each subject via
a labeling function (percent-/aba/ classification for
each token) derived from pre-test classification data
(98 trials from across the entire continuum). All
subjects? maximally ambiguous tokens were one of
x = 4, 5 or 6.
Each exposure block consisted of 256 repetitions
of the bimodal exposure stimulus. After 1, 2, 4, 8,
16, 32, 64, 128, and 256 exposure trials subjects
completed a test block, of six classification trials.
They were asked to classify as /aba/ or /ada/ the
three most ambiguous stimuli from the continuum
(the most ambiguous stimulus and the two neigh-
boring stimuli) twice each. For each ambiguity con-
dition, the aggregate effect of exposure across cat-
egories was a difference score, calculated by sub-
tracting the percent /aba/-classification after /d/-
exposure from the percent after /b/-exposure. This
/b/-/d/ difference score, as a function of cumulative
exposure trials, is plotted in Figure 3.
13
5 The unimodal model
We implemented an incremental belief-updating
model using a mixture of Gaussians as the underly-
ing model of phonetic categories (Figure 4), where
each phonetic category j = 1 . . .M corresponds to
a normal distribution over percepts x with mean ?j
and precision (inverse-variance) ?j (e.g. Figure 1,
left).
p(xi | ci) = N (?ci , ?ci) (1)
The listener?s beliefs about phonetic categories
are captured by additionally assigning probability
distributions to the means ?j and precisions ?j
of each phonetic category. The prior distribution
p(?j , ?j) represents the listener?s beliefs before ex-
posure to the experimental stimuli, and the posterior
p(?j , ?j |X) captures the listener?s beliefs after ex-
posure to stimuli X from category j. These two dis-
tributions are related via Bayes? Rule:
p(?j , ?j |X) ? p(X |?j , ?j)p(?j , ?j) (2)
In order to quantitatively evaluate such a model,
the form of the prior distributions needs to be spec-
ified. A natural prior to use in this case is known as
a Normal-Gamma prior.1 This prior factorizes the
joint prior into
p(?j , ?j) = p(?j |?j)p(?j)
p(?j |?j) = N (?0j , ??j)
p(?j) = G(?, ?)
where N (?0j , ??j) is a Normal distribution with
mean ?0j and precision ??j , and G(?, ?) is a Gamma
distribution with shape ? and rate ? (Figure 4).
5.1 Identifying individual subjects? prior
beliefs
In order to pick the most ambiguous token for each
subject, Vroomen et al (2007) collected calibration
data from their subjects, which consisted of 98 two-
alternative forced choice trials on acoustic tokens
spanning the entire /aba/-to-/ada/ continuum. As
1It is natural in that the Normal-Gamma distribution is the
conjugate prior for a Gaussian distribution where there is some
uncertainty about both the mean and the precision. Using the
conjugate prior ensures that the posterior distribution has the
same form as the prior.
revealed by this pre-test data, each subject?s pho-
netic categories are different, and so we chose to es-
timate the prior beliefs about the nature of the expo-
sure categories on a subject-by-subject basis. We fit
each subject?s classification function using logistic
regression. The logistic function is closely related
to the distribution over category labels given obser-
vations in a mixture of Gaussians model. Specifi-
cally, when there are only two categories (as in our
case), the probability that an observation at x will be
labeled c1 is2
p(c1 |x) =
p(x | c1)p(c1)
p(x | c1)p(c1) + p(x | c2)p(c2)
(3)
Further assuming that the categories have equal pre-
cision ? and equal prior probability p(c1) = p(c2) =
0.53, this reduces to a logistic function of the form
p(c1 |x) = (1 + exp(?gx+ b))?1, where
g = (?1 ? ?2)? and b = (?21 ? ?22)?
Even when b and g can be estimated from the sub-
ject?s pre-test data, one additional degree of freedom
needs to be fixed, and we chose to fix the distance
between the means, ?1??2. Given these values, the
values for (?1 + ?2)/2 (the middle of the subject?s
continuum) and ? can be calculated using
?1 + ?2
2 =
b
g and ? =
g
?1 ? ?2
(4)
We chose to use ?1 ? ?2 = 8, the length of the
acoustic continuum, which stretches from x = 1
(derived from a natural /aba/) to x = 9 (from a nat-
ural /ada/). This is roughly equivalent to assuming
that all subjects would accept these tokens as good
productions of /aba/ and /ada/, which indeed they
do (Vroomen et al, 2004).
So far, we have accounted for the expected val-
ues of category means and precisions. The strength
of these prior beliefs, however, has yet to be speci-
fied, and unfortunately there is no way to estimate
this based on the pre-test data of Vroomen et al
(2007). The model parameters corresponding to the
2Here we are abusing notation a bit by using c1 as a short-
hand for c = 1.
3This assumption is not strictly necessary, but for this pre-
liminary model we chose to make it in order to keep the model
as simple as possible.
14
subject?s confidence in their prior beliefs are ? and
? for the means and variances, respectively. Given
the specific form of the prior we use here, these two
parameters are closely related to the number of ob-
servations that are required to modify the subject?s
belief about a phonetic category (Murphy, 2007).
5.2 Model fitting
In order to evaluate the performance of this model
relative to human subjects, four simulations were
run per subject, corresponding to the four condi-
tions used by Vroomen et al (2007): ambiguous /d/
and /b/, and unambiguous /d/ and /b/. For each
subject, the hyper-parameters (?0j , ?, ?, ?) were set
according to the methods described above: values
were chosen for the free parameters ? and ?, and ?
and ?0j were set based on the subject?s pre-test data.
To model the effect of n exposure trials in a given
condition, the stimuli used by Vroomen et al (2007)
were input into the model in the following way. For
ambiguous blocks, the observations X were n repe-
titions of that subject?s most ambiguous token, and
for unambiguous blocks they were n repetitions of
the x = 1 for /b/ or x = 9 for /d/. For /b/ ex-
posure blocks, the category labels C were set to 1,
and for /d/ they were set to 2, corresponding to the
disambiguating effect of the visual cues.
For each subject, condition, and number of expo-
sures, the posterior distribution over category means
and precisions p(?j , ?j |X,C) was sampled using
numerical MCMC techniques.4
To compare the simulation results with the test
data of Vroomen et al (2007), it was neces-
sary to find the classification function, p(ctest =
1 |xtest, X), which is the probability that acoustic
test stimulus xtest will be categorized as /b/ (ctest =
1) given the training dataX . Based on (3), it suffices
to find the predictive distributions
p(xtest | ctest = 1, X)
=
??
p(xtest |?1, ?1)p(?1, ?1 |X)d?1d?1
and, analogously, p(xtest | ctest = 2, X). These in-
4Specifically, 1 000 samples for each parameter were ob-
tained after burn-in using JAGS, an open-source implemen-
tation of the BUGS language for Gibbs sampling of graph-
ical models: https://sourceforge.net/projects/
mcmc-jags
/b/?
/d/ 
diff
er
en
ce
 sc
or
e
?0.5
0.0
0.5
0 50 100 150 200 250
Exposures
/b/?
/d/ 
diff
er
en
ce
 sc
or
e
?0.5
0.0
0.5
0 50 100 150 200 250
Figure 5: Overall fit of the acoustic-only (top, R2 =
0.14) and bimodal model (bottom R2 = 0.67). Solid
lines correspond to the best fit averaged over subjects, and
dashed lines correspond to empirical difference scores,
with shaded regions corresponding to the 95% confidence
interval on the empirical subject means.
tegrals can be approximated numerically, by averag-
ing over the individual likelihoods corresponding to
each individual pair of means and variances drawn
from the posterior p(?j , ?j |X).
Once this labeling function is obtained, the de-
pendent measure used by Vroomen et al (2007)?
average percentage categorized as /b/?can be
calculated, by averaging the value of p(ctest =
1 |xtest, X) for the test stimuli xtest used by
Vroomen et al (2007). These were the subject?s
maximally ambiguous stimulus (x = 4, 5 or 6, de-
pending on the subject), and its two neighbors on the
continuum. The difference score used by Vroomen
et al (2007) was computed by subtracting the aver-
age probability of /b/ classification after /b/ (c = 1)
exposure from the probability of /b/ classification
after /d/ (c = 2) exposure. The best fitting con-
fidence parameters ? and ? were those which mini-
mized mean squared error between the empirical and
model difference scores.
15
ll
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l
ll
l
lll
ll
l l
l ll
l
l
ll
l
l
l
l
l
l
l
l l
ll
l
l
l
l l
l
l
l
l
ll
l
l
l
ll
l
l
l
ll ll
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l
ll
lll
l
l
l
l
l
ll
l
l
l
l
ll
l
l
l
l l
l l
l
ll ll
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l ll l
l l
l
l
l
l
l
l
ll
l
l
l
ll
l
l l
l l
l
l
lll
l
ll
l
l
l l
l
l
l
l l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l ll
l l
l
l
l
l
l
l l
l
l
l
l
l
l
l ll
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
ll
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l l
l
ll
ll
l
l
l l
ll l
l
l
l
l
l
l
l
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
l l
l
l
l
l
l l l
l
ll
l l
l
l
l
ll
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
ll
l ll
l
Pr
op
or
tio
n 
'ba
' re
sp
on
se
s
/aba/ /ada/
0
1
Pr
op
or
tio
n 
/b/
 re
sp
on
se
s
Figure 6: When audio and visual cues are integrated before categorization, a small number of ambiguous tokens still
produces a shift in the category mean, and thus recalibration (left, bright red). However, a large number of ambiguous
tokens produces both a shift of the category mean and an increase in precision (center-right, dark blue). If the audio-
visual percept is located away from the maximally ambiguous middle region of the continuum, this can result in an
extinction of the initial recalibration effect with increasing exposure (far right).
5.3 Results
Figure 5, top panel shows the results of the unimodal
model. While this model clearly captures the direc-
tion of the effects caused by ambiguous and unam-
biguous exposure, it fails to account for a significant
qualitative feature of the human data: the rise and
then fall of the recalibration effect (red line).
The reason for this is that the audio component
of the audio-visual exposure stimuli is identical to
the maximally ambiguous (audio-only) test stimu-
lus. Under this model, the probability with which a
stimulus is classified as /b/ is proportional to the
likelihood assigned to that cue value by category
/b/, relative to the total likelihood assigned by /b/
and /d/. In addition, under rational belief updating
the likelihood assigned to the exposure stimulus? cue
value will always increase with more exposure. In
the unimodal model the cue dimension is only au-
ditory (with the visual information in the exposure
stimuli only being used to assign category labels),
and so to the unimodal model the ambiguous expo-
sure stimuli and the ambiguous test stimuli are ex-
actly the same. Thus, the probability that the test
stimuli will be categorized as the exposure category
increases monotonically with further exposure.
6 The multimodal model
The unimodal model assumes that the cue dimen-
sions which phonetic categories are defined over
are acoustic, incorporating information from other
modalities only indirectly. This assumption is al-
most certainly wrong, based on work on audio-
visual speech, which shows strong and pervasive
cross-modal interactions (McGurk and MacDonald,
1976; Bejjanki et al, 2011). Indeed, Bertelson et al
(2003) report strong effects of the visual cue used
by Vroomen et al (2007): subjects were at chance
in discriminating acoustically ambiguous versus un-
ambiguous bimodal tokens when the visual cue
matched.
The multimodal model replaces the acoustic per-
cept x in the unimodal model with a phonetic per-
cept which integrates information from audio and
visual cues. Under reasonably general assumptions,
information from auditory and visual cues to the
same phonetic dimension can be optimally com-
bined by a simple weighted sum x = waxa +wvxv,
where the weights wa and wv sum to 1 and are pro-
portional to the reliability of the auditory and visual
cues (Ernst and Banks, 2002; Knill and Saunders,
2003; Jacobs, 2002; Toscano and McMurray, 2010).
Such optimal linear cue-combination can be in-
corporated into our model in an approximate way
by replacing x with a weighted sum of the con-
tinuum values for the auditory and visual tokens
x = wxa + (1 ? w)xv. In the unambiguous con-
ditions, there is no mismatch between these values
(xa, xv = 1 for /aba/ trials and 9 for /ada/ tri-
als), and behavior is the same. In the ambiguous tri-
als, however, the combination of visual and auditory
cues creates a McGurk illusion, and pulls the ob-
served stimulus?now located on a phonetic /aba/-
/ada/ continuum rather than an acoustic one?away
16
Exposures
/b/?
/d/ 
diff
er
en
ce
 sc
or
e
?1.0
?0.5
0.0
0.5
1.0
?1.0
?0.5
0.0
0.5
1.0
?1.0
?0.5
0.0
0.5
1.0
 1
10
20
50 150 250
 2
11
21
50 150 250
 3
12
25
50 150 250
 4
13
26
50 150 250
 5
14
27
50 150 250
 6
15
28
50 150 250
 7
16
29
50 150 250
 8
17
30
50 150 250
 9
19
31
50 150 250
Figure 7: Best model fit for each individual subject. Dashed lines are empirical difference scores (shaded regions are
95% confidence intervals) and solid lines are the best-fitting model for that subject. Mean R2 = 0.57, SE= 0.04.
from the maximally ambiguous test stimuli, which
are still located at the middle of the continuum, be-
ing audio-only. This allows recalibration to dom-
inate early, as the mean of the adapted category
moves towards the adapting percept, but be reversed
later, as the precision increases with further expo-
sure percepts, all tightly clustered around the new,
intermediate mean (Figure 6).
To be optimal, w must be the relative reliability
(precision) of audio cues relative to visual cues, but
in this preliminary model it is treated as a free pa-
rameter, between 0 and 1, and fit to each subject?s
test data individually, in the same way as the confi-
dence parameters ? and ?.
The best fitting models? predictions are shown av-
eraged across subjects in Figure 5 (bottom panel).
Unlike the unimodal model, the multimodal model
clearly captures the initial rise and later fall of recal-
ibration for ambiguous stimuli, and captures a fair
amount of the variation between subjects (Figure 7).
7 Discussion
The Bayesian belief updating model developed in
this paper, which takes into account cross-modal cue
integration, provides a good qualitative fit to both
the overall direction and detailed time-course of two
very different types of adaptation of phonetic cat-
egories, recalibration and selective adaptation, as
studied by Vroomen et al (2007). This constitutes
a first step towards a novel theoretical framework
for understanding the flexibility that characterizes
the mapping between phonetic categories to acoustic
(and other) cues. There is a large number of models
which adhere to the basic principles outlined here,
and we have investigated only two of the simplest
ones in order to show that, firstly, selective adapta-
tion and recalibration can be considered the product
of the same underlying inferential process, and sec-
ondly, this process likely occurs at the level of mul-
timodal phonetic percepts.
One of the most striking findings from this work,
which space precludes discussing in depth, is that
all subjects? data is fit best when the strength of the
prior beliefs is quite low, corresponding to a few
hundred or thousand prior examples, which is many
orders of magnitude less than the number of /b/s
and /d/s a normal adult has encountered in their life.
Why should this number be so low? The answer
lies in the fact that phonetic adaptation is often ex-
tremely specific, at the level of a single speaker or
situation. In the future, we plan to model these pat-
terns of specificity and generalization (Kraljic and
17
Samuel, 2007; Kraljic and Samuel, 2006) via hier-
archical extensions of the current model, with con-
nected mixtures of Gaussians for phonetic categories
that vary in predictable ways between groups of
speakers.
Besides being a principled, mathematical frame-
work, Bayesian belief updating and the broader
framework of rational inference under uncertainty
also provides a good framework for understanding
how and why multiple cues are combined in pho-
netic categorization (Toscano and McMurray, 2010;
Jacobs, 2002). Finally, this approach is similar in
spirit and in its mathematical formalisms to models
which treat the acquisition of phonetic categories as
statistical inference, where the number of categories
needs to be inferred, as well as the means and preci-
sions of those categories (Vallabha et al, 2007; Feld-
man et al, 2009a). It is also similar to recent work
on syntactic adaptation (Fine et al, 2010), and thus
constitutes a central part of an emerging paradigm
for understanding language as inference and learn-
ing under uncertain conditions.
Acknowledgements
We would like to thank Jean Vroomen for gener-
ously making the raw data from Vroomen et al
(2007) available.
This work was partially funded by NSF Grant
BCS-0844472 and an Alfred P. Sloan Fellowship to
TFJ.
References
Vikranth Rao Bejjanki, Meghan A Clayards, David C
Knill, and Richard N Aslin. 2011. Cue Integra-
tion in Categorical Tasks : Insights from Audio-Visual
Speech Perception. PLoS ONE, in press.
Paul Bertelson, Jean Vroomen, and Be?atrice de Gelder.
2003. Visual recalibration of auditory speech identifi-
cation: a McGurk aftereffect. Psychological Science,
14(6):592?597, November.
Ann R Bradlow and Tessa Bent. 2008. Perceptual adap-
tation to non-native speech. Cognition, 106(2):707?
29, February.
Naama Brenner, William Bialek, and Rob de Ruyter Van
Steveninck. 2000. Adaptive Rescaling Maximizes
Information Transmission. Neuron, 26(3):695?702,
June.
Matthew H Davis, Ingrid S Johnsrude, Alexis Hervais-
Adelman, Karen Taylor, and Carolyn McGettigan.
2005. Lexical information drives perceptual learning
of distorted speech: evidence from the comprehension
of noise-vocoded sentences. Journal of experimental
psychology. General, 134(2):222?41, May.
Marc O Ernst and Martin S Banks. 2002. Humans in-
tegrate visual and haptic information in a statistically
optimal fashion. Nature, 415(6870):429?33.
Naomi H Feldman, Thomas L Griffiths, and James L
Morgan. 2009a. Learning phonetic categories by
learning a lexicon. Proceedings of the 31st Annual
Conference of the Cognitive Science Society, pages
2208?2213.
Naomi H Feldman, Thomas L Griffiths, and James L
Morgan. 2009b. The influence of categories on per-
ception: explaining the perceptual magnet effect as
optimal statistical inference. Psychological review,
116(4):752?82, October.
Alex B Fine, Ting Qian, T Florian Jaeger, and Robert A
Jacobs. 2010. Is there syntactic adaptation in lan-
guage comprehension? In ACL Workshop on Cog-
nitive Modeling and Computational Linguistics, pages
18?26.
Stephen D Goldinger. 1998. Echoes of echoes? An
episodic theory of lexical access. Psychological re-
view, 105(2):251?79, April.
Khalil Iskarous, Carol A Fowler, and D H Whalen. 2010.
Locus equations are an acoustic expression of articu-
lator synergy. The Journal of the Acoustical Society of
America, 128(4):2021?32, October.
Robert A Jacobs and John K Kruschke. 2010. Bayesian
learning theory applied to human cognition. Wiley In-
terdisciplinary Reviews: Cognitive Science, pages n/a?
n/a, May.
Robert A Jacobs. 2002. What determines visual cue re-
liability? Trends in cognitive sciences, 6(8):345?350,
August.
David C Knill and Jeffrey A Saunders. 2003. Do hu-
mans optimally integrate stereo and texture informa-
tion for judgments of surface slant? Vision Research,
43(24):2539?2558, November.
Tanya Kraljic and Arthur G Samuel. 2006. Generaliza-
tion in perceptual learning for speech. Psychonomic
bulletin & review, 13(2):262?8, April.
Tanya Kraljic and Arthur G Samuel. 2007. Perceptual
adjustments to multiple speakers. Journal of Memory
and Language, 56(1):1?15, January.
Tanya Kraljic, Arthur G Samuel, and Susan E Brennan.
2008. First impressions and last resorts: how listeners
adjust to speaker variability. Psychological science : a
journal of the American Psychological Society / APS,
19(4):332?8, April.
Harry McGurk and John MacDonald. 1976. Hearing lips
and seeing voices. Nature, 264(5588):746?748.
18
Bob McMurray, Richard N Aslin, and Joseph C Toscano.
2009. Statistical learning of phonetic categories: in-
sights from a computational approach. Developmental
Science, 12(3):369?78, April.
Kevin P Murphy. 2007. Conjugate Bayesian analysis of
the Gaussian distribution. Technical report, University
of British Columbia.
Dennis Norris, James M McQueen, and Anne Cutler.
2003. Perceptual learning in speech. Cognitive Psy-
chology, 47(2):204?238, September.
Joseph C Toscano and Bob McMurray. 2010. Cue in-
tegration with categories: Weighting acoustic cues in
speech using unsupervised learning and distributional
statistics. Cognitive science, 34(3):434?464, April.
Gautam K Vallabha, James L McClelland, Ferran Pons,
Janet F Werker, and Shigeaki Amano. 2007. Unsuper-
vised learning of vowel categories from infant-directed
speech. Proceedings of the National Academy of Sci-
ences of the United States of America, 104(33):13273?
8, August.
Jean Vroomen, Sabine van Linden, Mirjam Keetels,
Be?atrice de Gelder, and Paul Bertelson. 2004. Se-
lective adaptation and recalibration of auditory speech
by lipread information: dissipation. Speech Commu-
nication, 44(1-4):55?61, October.
Jean Vroomen, Sabine van Linden, Be?atrice de Gelder,
and Paul Bertelson. 2007. Visual recalibration and
selective adaptation in auditory-visual speech percep-
tion: Contrasting build-up courses. Neuropsychologia,
45(3):572?7, February.
Janet F Werker and Richard C Tees. 1984. Cross-
language speech perception: Evidence for perceptual
reorganization during the first year of life. Infant Be-
havior and Development, 7(1):49?63, January.
19
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 10?18,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
Investigating the role of entropy in sentence processing
Tal Linzen
Department of Linguistics
New York University
linzen@nyu.edu
T. Florian Jaeger
Brain and Cognitive Sciences
University of Rochester
fjaeger@bcs.rochester.edu
Abstract
We outline four ways in which uncertainty
might affect comprehension difficulty in
human sentence processing. These four
hypotheses motivate a self-paced reading
experiment, in which we used verb sub-
categorization distributions to manipulate
the uncertainty over the next step in the
syntactic derivation (single step entropy)
and the surprisal of the verb?s comple-
ment. We additionally estimate word-
by-word surprisal and total entropy over
parses of the sentence using a probabilistic
context-free grammar (PCFG). Surprisal
and total entropy, but not single step en-
tropy, were significant predictors of read-
ing times in different parts of the sen-
tence. This suggests that a complete model
of sentence processing should incorporate
both entropy and surprisal.
1 Introduction
Predictable linguistic elements are processed
faster than unpredictable ones. Specifically, pro-
cessing load on an element A in context C is lin-
early correlated with its surprisal, ? log
2
P (A|C)
(Smith and Levy, 2013). This suggests that read-
ers maintain expectations as to the upcoming ele-
ments: likely elements are accessed or constructed
in advance of being read. While there is substan-
tial amount of work on the effect of predictability
on processing difficulty, the role (if any) of the dis-
tribution over expectations is less well understood.
Surprisal predicts that the distribution over
competing predicted elements should not affect
reading times: if the conditional probability of a
word A is P (A|C), reading times on the word
will be proportional to ? log
2
P (A|C), regardless
of whether the remaining probability mass is dis-
tributed among two or a hundred options.
The entropy reduction hypothesis (Hale,
2003; Hale, 2006), on the other hand, accords
a central role to the distribution over predicted
parses. According to this hypothesis, an incom-
ing element is costly to process when it entails a
change from a state of high uncertainty (e.g., mul-
tiple equiprobable parses) to a state of low uncer-
tainty (e.g., one where a single parse is much more
likely than the others). Uncertainty is quantified
as the entropy of the distribution over complete
parses of the sentence; that is, if A
i
is the set of
all possible parses of the sentence after word w
i
,
then the uncertainty following w
i
is given by
H
w
i
= ?
?
a?A
i
P (a) log
2
P (a) (1)
Processing load in this hypothesis is propor-
tional to the entropy reduction caused by w
n
:
1
ER(w
n
) = max{H
w
n?1
?H
w
n
, 0} (2)
A third hypothesis, which we term the com-
petition hypothesis, predicts that higher compe-
tition among potential outcomes should result in
increased processing load at the point at which
the competing parses are still valid (McRae et al.,
1998; Tabor and Tanenhaus, 1999). This contrasts
with the entropy reduction hypothesis, according
to which processing cost arises when competition
is resolved. Intuitively, the two hypotheses make
inversely correlated predictions: on average, there
will be less competition following words that re-
duce entropy. A recent study found that reading
times on w
i
correlated positively with entropy fol-
lowing w
i
, providing support for this hypothesis
(Roark et al., 2009).
The fourth hypothesis we consider, which we
term the commitment hypothesis, is derived from
1
No processing load is predicted for words that increase
uncertainty.
10
start
a
b
c
d
e
f
0.5
0.5
0.25
0.25
0.25
0.25
0.25
0.75
Figure 1: Example language. Output strings are indicated
inside the nodes, and transition probabilities are indicated on
the edges. For example, the probability of the sentence bf is
0.5? 0.75.
the event-related potential (ERP) literature on con-
textual constraint. Studies in this tradition have
compared the responses to a low-predictability
word across two types of context: high-constraint
contexts, in which there is a strong expectation for
a (different) word, and low-constraint ones, which
are not strongly predictive of any individual word.
There is increasing evidence for an ERP compo-
nent that responds to violations of a strong pre-
diction (Federmeier, 2007; Van Petten and Luka,
2012). This component can be interpreted as re-
flecting disproportional commitment to high prob-
ability predictions at the expense of lower proba-
bility ones, a more extreme version of the proposal
that low-probability parses are pruned in the pres-
ence of a high-probability parse (Jurafsky, 1996).
Surprisal is therefore expected to have a larger ef-
fect in high constraint contexts, in which entropy
was low before the word being read. Commitment
to a high probability prediction may also result in
increased processing load at the point at which the
commitment is made.
We illustrate these four hypotheses using the
simple language sketched in Figure 1. Consider
the predictions made by the four hypotheses for
the sentences ae and be. Surprisal predicts no dif-
ference in reading times between these sentences,
since the conditional probabilities of the words in
the two sentences are identical (0.5 and 0.25 re-
spectively).
The competition hypothesis predicts increased
reading times on the first word in ae compared to
be, because the entropy following a is higher than
the entropy following b (2 bits compared to 0.71).
Since all sentences in the language are two word
long, entropy goes down to 0 after the second word
in both sentences. This hypothesis therefore does
not predict a reading time difference on the second
word e.
Moving on to the entropy reduction hypothesis,
five of the six possible sentences in the language
have probability 0.5? 0.25, and the sixth one (bf )
has probability 0.5? 0.75. The full entropy of the
grammar is therefore 2.4 bits. The first word re-
duces entropy in both ae and be (to 2 and 0.71 bits
respectively), but entropy reduction is higher when
the first word is b. The entropy reduction hypoth-
esis therefore predicts longer reading times on the
first word in be than in ae. Conversely, since en-
tropy goes down to 0 in both cases, but from 2 bits
in ae compared to 0.71 bits in be, this hypothesis
predicts longer reading times on e in ae than in be.
Finally, the commitment hypothesis predicts
that after b the reader will become committed to
the prediction that the second word will be f . This
will lead to longer reading times on e in be than
in ae, despite the fact that its conditional proba-
bility is identical in both cases. If commitment to
a prediction entails additional work, this hypothe-
sis predicts longer reading times on the first word
when it is b.
This paper presents an reading time study that
aims to test these hypotheses. Empirical tests
of computational theories of sentence processing
have employed either reading time corpora (Dem-
berg and Keller, 2008) or controlled experimen-
tal materials (Yun et al., 2010). The current paper
adopts the latter approach, trading off a decrease
in lexical and syntactic heterogeneity for increased
control. This paper is divided into two parts. Sec-
tion 2 describes a reading time experiment, which
tested the predictions of the surprisal, competi-
tion and commitment hypotheses, as applied to the
entropy over the next single step in the syntactic
derivation.
2
We then calculate the total entropy
(up to an unbounded number of derivation steps)
at each word using a PCFG; Section 3 describes
how this grammar was constructed, overviews the
predictions that it yielded in light of the four hy-
potheses, and evaluates these predictions on the re-
sults of the reading time experiment.
2
We do not test the predictions of the entropy reduction
hypothesis in this part of the paper, since that theory explicitly
only applies to total rather than single-step entropy.
11
2 Reading time experiment
2.1 Design
To keep syntactic structure constant while ma-
nipulating surprisal and entropy over the next
derivation step, we took advantage of the fact
that verbs vary in the probability distribution
of their syntactic complements (subcategorization
frames). Several studies have demonstrated that
readers are sensitive to subcategorization probabil-
ities (Trueswell et al., 1993; Garnsey et al., 1997).
The structure of the experimental materials is
shown in Table 1. In a 2x2x2 factorial design, we
crossed the surprisal of a sentential complement
(SC) given the verb, the entropy of the verb?s sub-
categorization distribution, and the presence or ab-
sence of the complementizer that. When the com-
plementizer is absent, the region the island is am-
biguous between a direct object and an embedded
subject.
Surprisal theory predicts an effect of SC sur-
prisal on the disambiguating region in ambiguous
sentences (sentences without that), as obtained in
previous studies (Garnsey et al., 1997), and an ef-
fect of SC surprisal on the complementizer that
in unambiguous sentences. Reading times should
not differ at the verb: in the minimal context we
used (the men), the surprisal of the verb should
be closely approximated by its lexical frequency,
which was matched across conditions.
The competition hypothesis predicts a positive
main effect of subcategorization frame entropy
(subcategorization frame entropy) at the verb:
higher uncertainty over the syntactic category of
the complement should result in slower reading
times.
The commitment hypothesis predicts that the
effect of surprisal in the disambiguating region
should be amplified when subcategorization frame
entropy is low, since the readers will have commit-
ted to the competing high probability frame. If the
commitment step in itself incurs a processing cost,
there should be a negative main effect of subcate-
gorization frame entropy at the verb.
This experimental design varies the entropy
over the single next derivation step: it assumes
that the parser only predicts the identity of the sub-
categorization frame, but not its internal structure.
Since the predictions of the entropy reduction hy-
pothesis crucially depend on predicting the inter-
nal structure as well, we defer the discussion of
that hypothesis until Section 3.
The men discovered (that) the island
mat. subj. verb that emb. subj.
had been invaded by the enemy.
emb. verb complex rest
Table 1: Structure of experimental materials (mat. = matrix,
emb. = embedded, subj. = subject).
2.2 Methods
2.2.1 Participants
128 participants were recruited through Amazon
Mechanical Turk and were paid $1.75 for their
participation.
2.2.2 Materials
32 verbs were selected from the Gahl et al. (2004)
subcategorization frequency database, in 4 con-
ditions: high vs. low SC surprisal and high vs.
low subcategorization frame entropy (see Table 2).
Verbs were matched across conditions for length
in characters and for frequency in SUBTLEX-US
corpus (Brysbaert and New, 2009). A sentence
was created for each verb, following the structure
in Table 1. Each sentence had two versions: one
with the complementizer that after the verb and
one without it. The matrix subjects were mini-
mally informative two-word NPs (e.g. the men).
Following the complementizer (or the verb, if the
complementizer was omitted) was a definite NP
(the island), which was always a plausible direct
object of the matrix verb.
The embedded verb complex region consisted
of three words: two auxiliary verbs (had been) or
an auxiliary verb and negation (would not), fol-
lowed by a past participle form (invaded). Each
of the function words appeared the same num-
ber of times in each condition. The embedded
verb complex was followed by three more words.
The nouns and verbs in the embedded clause were
matched for frequency and length across condi-
tions.
In addition to the target sentences, the exper-
iment contained 64 filler sentences, with various
complex syntactic structures.
2.2.3 Procedure
The sentences were presented word by word in a
self-paced moving window paradigm. The partic-
ipants were presented with a Y/N comprehension
question after each trial. The participants did not
12
NP Inf PP SC SC s. SFE
forget 0.55 0.14 0.2 0.09 3.46 1.7
hear 0.72 0 0.17 0.11 3.22 1.12
claim 0.36 0.12 0 0.45 1.15 1.71
sense 0.61 0 0.02 0.34 1.55 1.18
Table 2: A example verb from each of the four conditions.
On the left, probabilities of complement types: noun phrase
(NP), infinitive (Inf), prepositional phrase (PP), sentential
complement (SC); on the right, SC surprisal and subcatego-
rization frame entropy.
receive feedback on their responses. The experi-
ment was conducted online using a Flash applica-
tion written by Harry Tily (now at Nuance Com-
munications).
2.2.4 Statistical analysis
Subjects were excluded if their answer accuracy
was lower than 75% (two subjects), or if their
mean reading time (RT) differed by more than
2.5 standard deviations from the overall mean RT
across subjects (two subjects). The results re-
ported in what follows are based on the remaining
124 subjects (97%).
We followed standard preprocessing procedure.
Individual words were excluded if their raw RT
was less than 100 ms or more than 2000 ms, or if
the log-transformed RT was more than 3 standard
deviations away from the participant?s mean. Log
RTs were length-corrected by taking the residuals
of a mixed-effects model (Bates et al., 2012) that
had log RT as the response variable, word length
as a fixed effect, and a by-subject intercept and
slope.
The length-corrected reading times were re-
gressed against the predictors of interest, sepa-
rately for each region. We used a maximal random
effect structure. All p values for fixed effects were
calculated using model comparison with a simpler
model with the same random effect structure that
did not contain that fixed effect.
2.3 Results
Reading times on the matrix subject (the men) or
matrix verb (discovered) did not vary significantly
across conditions.
The embedded subject the island was read faster
in unambiguous sentences (p < 0.001). Read-
ing times on this region were longer when SC sur-
prisal was high (p = 0.04). Models fitted to am-
biguous and unambiguous sentences separately re-
vealed that the simple effect of SC surprisal on the
embedded subject was significant for unambigu-
ous sentences (p = 0.02) but not for ambiguous
sentences (p = 0.46), though the interaction be-
tween SC surprisal and ambiguity did not reach
significance (p = 0.22).
The embedded verb complex (had been in-
vaded) was read faster in unambiguous than in am-
biguous sentences (p < 0.001). Reading times
in this region were longer overall in the high SC
surprisal condition (p = 0.03). As expected, this
effect interacted with the presence of that (p =
0.01): the simple effect of SC surprisal was not
significant in unambiguous sentences (p = 0.28),
but was highly significant in ambiguous ones (p =
0.007). We did not find an interaction between SC
surprisal and subcategorization frame entropy (of
the sort predicted by the commitment hypothesis).
Subcategorization frame entropy did not have a
significant effect in any of the regions of the sen-
tence. It was only strictly predicted to have an ef-
fect on the matrix verb: longer reading times ac-
cording to the competition hypothesis, and (possi-
bly) shorter reading times according to the com-
mitment hypothesis. The absence of an subcat-
egorization frame entropy effect provides weak
support for the predictions of surprisal theory, ac-
cording to which entropy should not affect reading
times.
3 Deriving predictions from a PCFG
3.1 Calculating entropy
As mentioned above, the entropy of the next
derivation step following the current word (which
we term single-step entropy) is calculated as fol-
lows. If a
i
is a nonterminal, ?
i
is the set of rules
rewriting a
i
, and p
r
is the application probability
of rule r, then the single-step entropy of a
i
is given
by
h(a
i
) = ?
?
r??
i
p
r
log
2
p
r
(3)
discover
NP (14 bits)
SC (50 bits)
0.5
0.5
Figure 3: Entropy calculation example: the single step en-
tropy after discover is 1 bit; the overall entropy is 1 + 0.5 ?
14 + 0.5? 50 = 33 bits.
13
Ambiguous Unambiguous
?0.10
?0.05
0.00
0.05
Th
e
m
en
dis
co
ve
re
d the
isla
nd ha
d
be
en
inv
ad
ed by the Th
e
m
en
dis
co
ve
re
d
tha
t
the
isla
nd ha
d
be
en
inv
ad
ed by the
Sentential complement
surprisal
Low
High
Subcategorization
entropy
Low
High
Figure 2: Results of the self-paced reading experiment
The entropy of all derivations starting with a
i
(which we term total entropy) is then given by the
following recurrence:
H(a
i
) = h(a
i
) +
?
r??
i
p
r
k
r
?
j=1
H(a
r,j
) (4)
where a
r,1
, . . . , a
r,k
r
are the nonterminals on
the right-hand side of r. This recurrence has
a closed form solution (Wetherell, 1980; Hale,
2006). The expectation matrix A is a square ma-
trix with N rows and columns, where N is the set
of nonterminals. Each element A
ij
indicates the
expected number of times nonterminal a
j
will oc-
cur when a
i
is rewritten using exactly one rule of
the grammar. If h = (h
1
, . . . , h
N
) is the vector of
all single-step entropy values for the N nontermi-
nal types in the grammar, andH = (H
1
, . . . ,H
N
)
is the vector of all total entropy values, then the
closed form solution for the recurrence is given by
H = (I ?A)
?1
h (5)
where I is the identity matrix. The entropy af-
ter the first n words of the sentence, H
w
n
, can be
calculated by applying Equation 5 to the grammar
formed by intersecting the original grammar with
the prefix w
1
, . . . , w
n
(i.e., considering only the
parses that are compatible with the words encoun-
tered so far) (Hale, 2006).
Two points are worth noting about these equa-
tions. First, Equation 5 shows that calculating the
entropy of a PCFG requires inverting the matrix
I ? A, which is the size of the number of non-
terminal symbols in the grammar. This makes it
impractical to use a lexicalized grammar, as advo-
cated by Roark et al. (2009), since those grammars
have a very large number of nonterminal types.
Second, Equation 4 shows that the entropy of a
nonterminal is the sum of its single-step entropy
and a weighted average of entropy of the nonter-
minals it derives. In the context of subcategoriza-
tion decisions, the number of possible subcatego-
rization frames is small, and the single-step en-
tropy is on the order of magnitude of 1 or 2 bits.
The entropy of a typical complement, on the other
hand, is much higher (consider all of the possible
internal structures that an SC could have). This
means that the total entropy H after processing
the verb is dominated by the entropy of its po-
tential complements rather than the verb?s single-
step entropy h (see Figure 3 for an illustration). A
lookahead of a single word (as used in Roark et
al. (2009)) may therefore be only weakly related
to total entropy.
3.2 Constructing the grammar
We used a PCFG induced from the Penn Treebank
(Marcus et al., 1993). As mentioned above, the
grammar was mostly unlexicalized; however, in
order for the predictions to depend on the identity
of the verb, the grammar had to contain lexically
specific rules for each verb. We discuss these rules
at end of this section.
The Penn Treebank tag set is often expanded
by adding to each node?s tag an annotation of the
14
node?s parent, e.g., marking an NP whose parent
is a VP as NP VP (Klein and Manning, 2003).
While systematic parent annotation would have in-
creased the size of the grammar dramatically, we
did take the following minimal steps to improve
parsing accuracy. First, the word that is tagged
in the Penn Treebank as a preposition (IN) when
it occurs as a subordinating conjunction. This re-
sulted in SCs being erroneously parsed as preposi-
tional phrases. To deal with this issue, we replaced
the generic IN with IN[that] whenever it referred
to that.
Second, the parser assigned high probability
parses to reduced relative clauses in implausible
contexts. We made sure that cases that should not
be reduced relative clauses were not parsed as such
by splitting the VP category into sub-categories
based on the leftmost child of the VP (since only
VP[VPN] should be able to be a reduced rela-
tive), and by splitting SBAR into SBAR[overt]
when the SBAR had an overt complementizer and
SBAR[none] when it did not.
Following standard practice, we removed gram-
matical role information and filler-gap annota-
tions, e.g., NP-SUBJ-2 was treated as NP. To re-
duce the number of rules in the grammar as much
as possible, we removed punctuation and the silent
element NONE (used to mark gaps, silent comple-
mentizers, etc.), rules that occurred less than 100
times (out of the total 1320490 nonterminal pro-
ductions), and rules that had a probability of less
than 0.01. These steps resulted in the removal of
13%, 14% and 10% rule tokens respectively. We
then applied horizontal Markovization (Klein and
Manning, 2003).
Finally, we added lexically specific rules to
capture the verbs? subcategorization preferences,
based on the Gahl et al. (2004) subcategorization
database. The probability of frame f
j
following
verb v
i
was calculated as:
P (VP[VBD]? v
i
f
j
) =
1
2
P (v
i
)P (f
j
|v
i
)
?
i
P (v
i
)
(6)
In other words, half of the probability mass of
production rules deriving VP[VBD] (VP headed
by past tense verbs) was taken away from the un-
lexicalized rules and assigned to the verb-specific
rules. The same procedure was performed for
VP[VBN] (VP headed by a past participle, with
the exception of the verbs forgot and wrote, which
are not ambiguous between the past and past par-
ticiple forms. The total probability of all rules de-
riving VP as a specific verb (e.g., discovered) was
estimated as the corpus frequency of that verb di-
vided by the total corpus frequency of all 32 verbs
used in the experiment, yielding a normalized es-
timate of the relative frequency of that verb.
3.3 Surprisal, entropy and entropy reduction
profiles
Word-by-word surprisal, entropy and entropy re-
duction values for each item were derived from the
equations in Section 3.1 using the Cornell Con-
ditional Probability Calculator (provided by John
Hale). Figure 4 shows the predictions averaged by
the conditions of the factorial design. Surprisal on
the verb is always high because this is the only part
of the grammar that encodes lexical identity; sur-
prisal on the verb therefore conflates lexical and
syntactic surprisal. Surprisal values on all other
words are low, with the exception of the point
at which the reader gets the information that the
verb?s complement is an SC: the embedded verb
complex in ambiguous sentences, and the comple-
mentizer in unambiguous sentence.
The entropy profile is dominated by the fact that
SCs have much higher internal entropy than NPs.
As a consequence, entropy after the verb is higher
whenever an SC is a more likely subcategorization
frame. The entropy after high subcategorization
frame entropy verbs is higher than that after low
subcategorization frame entropy verbs, though the
difference is small in comparison to the effect of
SC surprisal. In ambiguous sentences, entropy re-
mains higher for low SC surprisal verbs through-
out the ambiguous region. Somewhat counterin-
tuitively, entropy increases when the parse is dis-
ambiguated in favor of an SC. This is again a
consequence of the higher internal entropy of a
SC: the entropy of the ambiguity between SC and
NP is dwarfed by the internal entropy of a SC.
The entropy profile for unambiguous sentences
is straightforward: it increases sharply when the
reader finds out that the complement is a SC, then
decreases gradually as more details are revealed
about the internal structure of the SC.
The reading time predictions made by the en-
tropy reduction hypothesis are therefore very dif-
ferent than those made by surprisal theory. On
the verb, the entropy reduction hypothesis predicts
that high SC surprisal verbs will be read more
15
Ambiguous Unambiguous
l
l
l l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
ll
ll
l
ll
l
l
l l
l
l
l
l
l
l
l
l l
l l
l
l
l
ll
0
10
10
20
0
10
20
Surprisal
Entropy
Ent. R
eduction
m
en
dis
co
ve
re
d the
isla
nd ha
d
be
en
inv
ad
ed
m
en
dis
co
ve
re
d
tha
t
the
isla
nd ha
d
be
en
inv
ad
ed
Sentential complement
surprisal
Low
High
Subcategorization
entropy
l
l
Low
High
Figure 4: Parser-derived surprisal, entropy and entropy reduction estimates for the stimuli in our experiments, averaged within
each condition of the factorial design (first word of sentence and rest region excluded).
slowly than low SC surprisal verbs, whereas sur-
prisal predicts no difference. On the disambiguat-
ing region in ambiguous sentences, the entropy re-
duction hypothesis predicts no reading time dif-
ferences at all, since an increase in entropy is
not predicted to affect reading times. In fact, en-
tropy reduction on the word had is positive only in
unambiguous sentences, so the entropy reduction
hypothesis predicts a slowdown in unambiguous
compared to ambiguous sentences.
3.4 Evaluation on reading times
We tested whether reading times could be pre-
dicted by the word-by-word estimates derived
from the PCFG. Since total entropy, entropy re-
duction and surprisal values did not line up with
the factorial design, we used continuous regres-
sion instead, again using lme4 with a maximal ran-
dom effects structure. We only analyzed words
for which the predictions depended on the prop-
erties of the verb (as Figure 4 shows, this is only
the case for a minority of the words). As outcome
variables, we considered both reading times on the
word w
i
, and a spillover variable computed as the
sum of the reading times on w
i
and the next word
w
i+1
. The predictors were standardized (sepa-
rately for each word) to facilitate effect compar-
ison.
Parser-derived entropy reduction values varied
the most on the main verb. Since the word follow-
ing the verb differs between the ambiguous and
unambiguous conditions, we added a categorical
control variable for sentence ambiguity. In the
resulting model, lower entropy (or equivalently,
higher entropy reduction values), caused an in-
crease in reading times (no spillover:
?
? = 0.014,
p = 0.05; one word spillover:
?
? = 0.022, p =
0.04). Our design does not enable us to determine
whether the effect of entropy on the verb is due to
entropy reduction or simply entropy. The commit-
ment hypothesis is therefore equally supported by
this pattern as is the entropy reduction hypothesis.
The only other word on which entropy reduc-
tion values varied across verbs was the first word
the of the ambiguous region. Neither entropy re-
duction nor surprisal were significant predictors of
reading times on this word.
There was also some variation across verbs in
entropy (though not entropy reduction) on the sec-
ond word of the embedded subject (island) in am-
biguous sentences; however, entropy was not a
significant predictor of reading times on that word.
In general, entropy is much higher in the embed-
16
ded subject region in unambiguous than ambigu-
ous sentences, since it is already known that the
complement is an SC, and the entropy of an SC
is higher. Yet as mentioned above, reading times
on the embedded subject were higher when it was
ambiguous (p < 0.001).
Finally, PCFG-based surprisal was a significant
predictor of reading times on the disambiguating
word in ambiguous sentences (no spillover: n.s.;
one word spillover:
?
? = 0.037, p = 0.02; two-
word spillover:
?
? = 0.058, p = 0.001). In con-
trast with simple SC surprisal (see Section 2.2.4),
PCFG-based surprisal was not a significant predic-
tor of reading times on the complementizer that in
unambiguous sentences.
4 Discussion
We presented four hypotheses as to the role of en-
tropy in syntactic processing, and evaluated them
on the results of a reading time study. We did not
find significant effects of subcategorization frame
entropy, which is the entropy over the next deriva-
tion step following the verb. Entropy over com-
plete derivations, on the other hand, was a signifi-
cant predictor of reading time on the verb. The ef-
fect went in the direction predicted by the entropy
reduction and commitment hypotheses, and oppo-
site to that predicted by the competition hypothe-
sis: reading times were higher when post-verb en-
tropy was lower.
Reading times on the embedded subject in am-
biguous sentences were increased compared to un-
ambiguous sentences. This can be seen as sup-
porting the competition hypothesis: the SC and
NP parses both need to be maintained, which in-
creases processing cost. Yet the parser predic-
tions showed that total entropy on the embedded
subject was higher in unambiguous than ambigu-
ous sentences, since the probability of the high-
entropy sentential complement is 1 in unambigu-
ous sentences. In this case, then, total entropy,
which entails searching enormous amounts of pre-
dicted structure, may not be the right measure, and
single-step (or n-step) entropy may be a better pre-
dictor.
In related work, Frank (2013) tested a version of
the entropy reduction hypothesis whereby entropy
reduction was not bounded by 0 (was allowed to
take negative values). A Simple Recurrent Net-
work was used to predict the next four words in
the sentence; the uncertainty following the current
word was estimated as the entropy of this quadri-
gram distribution. Higher (modified) entropy re-
duction resulted in increased reading times. These
results are not directly comparable to the present
results, however. Frank (2013) tested a theory that
takes into account both positive and negative en-
tropy changes. In addition, a four-word lookahead
may not capture the dramatic difference in internal
entropy between SCs and NPs, which is responsi-
ble for the differential reading times predicted on
the matrix. This caveat applies even more strongly
to the one-word lookahead in Roark et al. (2009).
In contrast with much previous work, we cal-
culated total entropy using a realistic PCFG ac-
quired from a Treebank corpus. In future work,
this method can be used to investigate the ef-
fect of entropy in a naturalistic reading time cor-
pus. It will be important to explore the extent to
which the reading time predictions derived from
the grammar are affected by representational de-
cisions (e.g., the parent annotations we used in
Section 3.2). This applies in particular to entropy,
which is sensitive to the distribution over syntactic
parses active at the word; surprisal depends only
the conditional probability assigned to the word
by the grammar, irrespective of the number and
distribution over the parses that predict the current
word, and is therefore somewhat less sensitive to
representational assumptions.
5 Conclusion
This paper described four hypotheses regarding
the role of uncertainty in sentence processing. A
reading time study replicated a known effect of
surprisal, and found a previously undocumented
effect of entropy. Entropy predicted reading times
only when it was calculated over complete deriva-
tions of the sentence, and not when it was calcu-
lated over the single next derivation step. Our re-
sults suggest that a full theory of sentence process-
ing would need to take both surprisal and uncer-
tainty into account.
Acknowledgments
We thank Alec Marantz for discussion and An-
drew Watts for technical assistance. This work
was supported by an Alfred P. Sloan Fellowship
to T. Florian Jaeger.
17
References
D. Bates, M. Maechler, and B. Bolker, 2012. lme4:
Linear mixed-effects models using S4 classes. R
package version 0.999999-0.
M. Brysbaert and B. New. 2009. Moving beyond
Ku?cera and Francis: A critical evaluation of cur-
rent word frequency norms and the introduction of
a new and improved word frequency measure for
American English. Behavior Research Methods,
41(4):977?990.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
K. D. Federmeier. 2007. Thinking ahead: The role
and roots of prediction in language comprehension.
Psychophysiology, 44(4):491?505.
S. L. Frank. 2013. Uncertainty reduction as a measure
of cognitive load in sentence comprehension. Topics
in Cognitive Science, 5(3):475?494.
S. Gahl, D. Jurafsky, and D. Roland. 2004. Verb
subcategorization frequencies: American English
corpus data, methodological studies, and cross-
corpus comparisons. Behavior Research Methods,
36(3):432?443.
S. Garnsey, N. Pearlmutter, E. Myers, and M. Lotocky.
1997. The contributions of verb bias and plausi-
bility to the comprehension of temporarily ambigu-
ous sentences. Journal of Memory and Language,
37(1):58?93.
J. Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101?123.
J. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643?672.
D. Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20(2):137?194.
D. Klein and C. D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423?430. Association
for Computational Linguistics.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
K. McRae, M. Spivey-Knowlton, and M. Tanenhaus.
1998. Modeling the influence of thematic fit (and
other constraints) in on-line sentence comprehen-
sion. Journal of Memory and Language, 38(3):283?
312.
B. Roark, A. Bachrach, C. Cardenas, and C. Pallier.
2009. Deriving lexical and syntactic expectation-
based measures for psycholinguistic modeling via
incremental top-down parsing. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1-Volume 1,
pages 324?333. Association for Computational Lin-
guistics.
N. J. Smith and R. Levy. 2013. The effect of word
predictability on reading time is logarithmic. Cog-
nition, 128(3):302?319.
W. Tabor and M. K. Tanenhaus. 1999. Dynamical
models of sentence processing. Cognitive Science,
23(4):491?515.
J. Trueswell, M. Tanenhaus, and C. Kello. 1993. Verb-
specific constraints in sentence processing: Sep-
arating effects of lexical preference from garden-
paths. Journal of Experimental Psychology: Learn-
ing, Memory, and Cognition, 19(3):528?553.
C. Van Petten and B. Luka. 2012. Prediction during
language comprehension: Benefits, costs, and ERP
components. International Journal of Psychophysi-
ology, 83(2):176?190.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. ACM Computing
Surveys (CSUR), 12(4):361?379.
J. Yun, J. Whitman, and J. Hale. 2010. Subject-object
asymmetries in Korean sentence comprehension. In
Proceedings of the 32nd Annual Meeting of the Cog-
nitive Science Society.
18
