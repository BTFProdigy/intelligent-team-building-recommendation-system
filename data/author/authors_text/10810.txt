Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582?590,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Labeled Class Instances using Graph
Random Walks
Partha Pratim Talukdar?
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Joseph Reisinger?
University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Deepak Ravichandran
Google Inc.
Mountain View, CA 94043
deepakr@google.com
Rahul Bhagat?
USC Information Sciences Institute
Marina Del Rey, CA 90292
rahul@isi.edu
Fernando Pereira
Google Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
We present a graph-based semi-supervised la-
bel propagation algorithm for acquiring open-
domain labeled classes and their instances
from a combination of unstructured and struc-
tured text sources. This acquisition method
significantly improves coverage compared to
a previous set of labeled classes and instances
derived from free text, while achieving com-
parable precision.
1 Introduction
1.1 Motivation
Users of large document collections can readily ac-
quire information about the instances, classes, and
relationships described in the documents. Such rela-
tions play an important role in both natural language
understanding andWeb search, as illustrated by their
prominence in both Web documents and among the
search queries submitted most frequently by Web
users (Jansen et al, 2000). These observations moti-
vate our work on algorithms to extract instance-class
information from Web documents.
While work on named-entity recognition tradi-
tionally focuses on the acquisition and identifica-
tion of instances within a small set of coarse-grained
classes, the distribution of instances within query
logs indicates that Web search users are interested
in a wider range of more fine-grained classes. De-
pending on prior knowledge, personal interests and
immediate needs, users submit for example medi-
cal queries about the symptoms of leptospirosis or
?Contributions made during internships at Google.
the treatment of monkeypox, both of which are in-
stances of zoonotic diseases, or the risks and benefits
of surgical procedures such as PRK and angioplasty.
Other users may be more interested in African coun-
tries such as Uganda and Angola, or active volca-
noes like Etna and Kilauea. Note that zoonotic dis-
eases, surgical procedures, African countries and
active volcanoes serve as useful class labels that cap-
ture the semantics of the associated sets of class in-
stances. Such interest in a wide variety of specific
domains highlights the utility of constructing large
collections of fine-grained classes.
Comprehensive and accurate class-instance in-
formation is useful not only in search but also
in a variety of other text processing tasks includ-
ing co-reference resolution (McCarthy and Lehn-
ert, 1995), named entity recognition (Stevenson and
Gaizauskas, 2000) and seed-based information ex-
traction (Riloff and Jones, 1999).
1.2 Contributions
We study the acquisition of open-domain, labeled
classes and their instances from both structured
and unstructured textual data sources by combin-
ing and ranking individual extractions in a princi-
pled way with the Adsorption label-propagation al-
gorithm (Baluja et al, 2008), reviewed in Section 3
below.
A collection of labeled classes acquired from
text (Van Durme and Pas?ca, 2008) is extended in two
ways:
1. Class label coverage is increased by identify-
ing additional class labels (such as public agen-
cies and governmental agencies) for existing
582
instances such as Office of War Information),
2. The overall instance coverage is increased by
extracting additional instances (such as Addi-
son Wesley and Zebra Books) for existing class
labels (book publishers).
The WebTables database constructed by Cafarella
et al (2008) is used as the source of additional
instances. Evaluations on gold-standard labeled
classes and instances from existing linguistic re-
sources (Fellbaum, 1998) indicate coverage im-
provements relative to that of Van Durme and Pas?ca
(2008), while retaining similar precision levels.
2 First Phase Extractors
To show Adsorption?s ability to uniformly combine
extractions from multiple sources and methods, we
apply it to: 1) high-precision open-domain extrac-
tions from free Web text (Van Durme and Pas?ca,
2008), and 2) high-recall extractions from WebTa-
bles, a large database of HTML tables mined from
the Web (Cafarella et al, 2008). These two meth-
ods were chosen to be representative of two broad
classes of extraction sources: free text and structured
Web documents.
2.1 Extraction from Free Text
Van Durme and Pas?ca (2008) produce an open-
domain set of instance clusters C ? C that parti-
tions a given set of instances I using distributional
similarity (Lin and Pantel, 2002), and labels using
is-a patterns (Hearst, 1992). By filtering the class
labels using distributional similarity, a large number
of high-precision labeled clusters are extracted. The
algorithm proceeds iteratively: at each step, all clus-
ters are tested for label coherence and all coherent
labels are tested for high cluster specificity. Label
L is coherent if it is shared by at least J% of the
instances in cluster C, and it is specific if the total
number of other clusters C ? ? C, C ? 6= C containing
instances with label L is less thanK. When a cluster
is found to match these criteria, it is removed from
C and added to an output set. The procedure termi-
nates when no new clusters can be removed from C.
Table 1 shows a few randomly chosen classes and
representative instances obtained by this procedure.
2.2 Extraction from Structured Text
To expand the instance sets extracted from free
text, we use a table-based extraction method that
mines structured Web data in the form of HTML
tables. A significant fraction of the HTML ta-
bles in Web pages is assumed to contain coherent
lists of instances suitable for extraction. Identifying
such tables from scratch is hard, but seed instance
lists can be used to identify potentially coherent ta-
ble columns. In this paper we use the WebTables
database of around 154 million tables as our struc-
tured data source (Cafarella et al, 2008).
We employ a simple ranking scheme for candi-
date instances in the WebTables corpus T . Each ta-
ble T ? T consists of one or more columns. Each
column g ? T consists of a set of candidate in-
stances i ? g corresponding to row elements. We
define the set of unique seed matches in g relative to
semantic class C ? C as
MC(g)
def
= {i ? I(C) : i ? g}
where I(C) denotes the set of instances in seed class
C. For each column g, we define its ?-unique class
coverage, that is, the set of classes that have at least
? unique seeds in g,
Q(g;?)
def
= {C ? C : |MC(g)| ? ?}.
Using M and Q we define a method for scoring
columns relative to each class. Intuitively, such a
score should take into account not only the number
of matches from class C, but also the total num-
ber of classes that contribute to Q and their relative
overlap. Towards this end, we introduce the scoring
function
score(C, g;?)
def
= |MC(g)|
? ?? ?
seed matches
?
class coherence
? ?? ?
|MC(g)|
|
?
C??Q(g;?) I(C
?)|
which is the simplest scoring function combining
the number of seed matches with the coherence of
the table column. Coherence is a critical notion
in WebTables extraction, as some tables contain in-
stances across many diverse seed classes, contribut-
ing to extraction noise. The class coherence intro-
duced here also takes into account class overlap; that
583
Class Size Examples of Instances
Book Publishers 70 crown publishing, kluwer academic, prentice hall, puffin
Federal Agencies 161 catsa, dhs, dod, ex-im bank, fsis, iema, mema, nipc, nmfs, tdh, usdot
Mammals 956 armadillo, elephant shrews, long-tailed weasel, river otter, weddell seals, wild goat
NFL Players 180 aikman, deion sanders, fred taylor, jamal lewis, raghib ismail, troy vincent
Scientific Journals 265 biometrika, european economic review, nature genetics, neuroscience
Social Issues 210 gender inequality, lack of education, substandard housing, welfare dependency
Writers 5089 bronte sisters, hemingway, kipling, proust, torquato tasso, ungaretti, yeats
Table 1: A sample of the open-domain classes and associated instances from (Van Durme and Pas?ca, 2008).
is, a column containing many semantically similar
classes is penalized less than one containing diverse
classes.1 Finally, an extracted instance i is assigned
a score relative to class C equal to the sum of all its
column scores,
score(i, C;?)
def
=
1
ZC
?
g?T,T?T
score(C, g;?)
where ZC is a normalizing constant set to the max-
imum score of any instance in class C. This scor-
ing function assigns high rank to instances that oc-
cur frequently in columns with many seed matches
and high class specificity.
The ranked list of extracted instances is post-
filtered by removing all instances that occur in less
than d unique Internet domains.
3 Graph-Based Extraction
To combine the extractions from both free and struc-
tured text, we need a representation capable of en-
coding efficiently all the available information. We
chose a graph representation for the following rea-
sons:
? Graphs can represent complicated relationships
between classes and instances. For example,
an ambiguous instance such as Michael Jor-
dan could belong to the class of both Profes-
sors and NBA players. Similarly, an instance
may belong to multiple nodes in the hierarchy
of classes. For example, Blue Whales could be-
long to both classes Vertebrates and Mammals,
because Mammals are a subset of Vertebrates.
1Note that this scoring function does not take into account
class containment: if all seeds are both wind Instruments and
instruments, then the column should assign higher score to the
more specific class.
? Extractions frommultiple sources, such asWeb
queries, Web tables, and text patterns can be
represented in a single graph.
? Graphs make explicit the potential paths of in-
formation propagation that are implicit in the
more common local heuristics used for weakly-
supervised information extraction. For exam-
ple, if we know that the instance Bill Clinton
belongs to both classes President and Politician
then this should be treated as evidence that the
class of President and Politician are related.
Each instance-class pair (i, C) extracted in the
first phase (Section 2) is represented as a weighted
edge in a graph G = (V,E,W ), where V is the set
of nodes, E is the set of edges and W : E ? R+
is the weight function which assigns positive weight
to each edge. In particular, for each (i, C,w) triple
from the set of base extractions, i and C are added
to V and (i, C) is added to E, 2 with W (i, C) = w.
The weight w represents the total score of all extrac-
tions with that instance and class. Figure 1 illustrates
a portion of a sample graph. This simple graph rep-
resentation could be refined with additional types of
nodes and edges, as we discuss in Section 7.
In what follows, all nodes are treated in the same
way, regardless of whether they represent instances
or classes. In particular, all nodes can be assigned
class labels. For an instance node, that means that
the instance is hypothesized to belong to the class;
for a class node, that means that the node?s class is
hypothesized to be semantically similar to the label?s
class (Section 5).
We now formulate the task of assigning labels to
nodes as graph label propagation. We are given a
2In practice, we use two directed edges, from i to C and
from C to i, both with weight w.
584
bob dylan
musician
0.95
johnny cash
0.87
singer
0.73
billy joel
0.82
0.75
Figure 1: Section of a graph used as input into Adsorp-
tion. Though the nodes do not have any type associated
with them, for readability, instance nodes are marked in
pink while class nodes are shown in green.
set of instances I and a set of classes C represented
as nodes in the graph, with connecting edges as de-
scribed above. We annotate a few instance nodes
with labels drawn from C. That is, classes are used
both as nodes in the graph and as labels for nodes.
There is no necessary alignment between a class
node and any of the (class) labels, as the final labels
will be assigned by the Adsorption algorithm.
The Adsorption label propagation algo-
rithm (Baluja et al, 2008) is now applied to
the given graph. Adsorption is a general framework
for label propagation, consisting of a few nodes
annotated with labels and a rich graph structure
containing the universe of all labeled and unlabeled
nodes. Adsorption proceeds to label all nodes
based on the graph structure, ultimately producing a
probability distribution over labels for each node.
More specifically, Adsorption works on a graph
G = (V,E,W ) and computes for each node v a la-
bel distribution Lv that represents which labels are
more or less appropriate for that node. Several in-
terpretations of Adsorption-type algorithms have ap-
peared in various fields (Azran, 2007; Zhu et al,
2003; Szummer and Jaakkola, 2002; Indyk and Ma-
tousek, 2004). For details, the reader is referred to
(Baluja et al, 2008). We use two interpretations
here:
Adsorption through Random Walks: Let Gr =
(V,Er,Wr) be the edge-reversed version of the
original graph G = (V,E,W ) where (a, b) ?
Er iff (b, a) ? E; and Wr(a, b) = W (b, a).
Now, choose a node of interest q ? V . To es-
timate Lq for q, we perform a random walk on
Gr starting from q to generate values for a ran-
dom label variable L. After reaching a node v
during the walk, we have three choices:
1. With probability pcontv , continue the ran-
dom walk to a neighbor of v.
2. With probability pabndv , abandon the ran-
dom walk. This abandonment proba-
bility makes the random walk stay rela-
tively close to its source when the graph
has high-degree nodes. When the ran-
dom walk passes through such a node,
it is likely that further transitions will be
into regions of the graph unrelated to the
source. The abandonment probability mit-
igates that effect.
3. With probability pinjv , stop the random
walk and emit a label L from Iv.
Lq is set to the expectation of all labels L emit-
ted from random walks initiated from node q.
Adsorption through Averaging: For this interpre-
tation we make some changes to the original
graph structure and label set. We extend the la-
bel distributions Lv to assign a probability not
only to each label in C but also to the dummy
label ?, which represents lack of information
about the actual label(s). We represent the ini-
tial knowledge we have about some node labels
in an augmented graph G? = (V ?, E?,W ?) as
follows. For each v ? V , we define an ini-
tial distribution Iv = L?, where L? is the
dummy distribution with L?(?) = 1, repre-
senting lack of label information for v. In addi-
tion, let Vs ? V be the set of nodes for which
we have some actual label knowledge, and let
V ? = V ? {v? : v ? Vs}, E? = E ? {(v?, v) :
v ? Vs}, and W ?(v?, v) = 1 for v ? Vs,
W ?(u, v) = W (u, v) for u, v ? V . Finally,
let Iv? (seed labels) specify the knowledge about
possible labels for v ? Vs. Less formally, the
v? nodes in G? serve to inject into the graph the
prior label distributions for each v ? Vs.
The algorithm proceeds as follows: For each
node use a fixed-point computation to find label
585
distributions that are weighted averages of the
label distributions for all their neighbors. This
causes the non-dummy initial distribution of Vs
nodes to be propagated across the graph.
Baluja et al (2008) show that those two views are
equivalent. Algorithm 1 combines the two views:
instead of a random walk, for each node v, it itera-
tively computes the weighted average of label distri-
butions from neighboring nodes, and then uses the
random walk probabilities to estimate a new label
distribution for v.
For the experiments reported in Section 4, we
used the following heuristics from Baluja et al
(2008) to set the random walk probabilities:
? Let cv =
log ?
log(? + expH(v)) where H(v) =
?
?
u puv ? log(puv) with puv =
W (u,v)
P
u
? W (u
? ,v)
.
H(v) can be interpreted as the entropy of v?s
neighborhood. Thus, cv is lower if v has many
neighbors. We set ? = 2.
? jv = (1 ? cv) ?
?
H(v) if Iv 6= L> and 0
otherwise.
? Then let
zv = max(cv + jv, 1)
pcontv = cv/zv
pinjv = jv/zv
pabndv = 1? p
cont
v ? p
abnd
v
Thus, abandonment occurs only when the con-
tinuation and injection probabilities are low
enough.
The algorithm is run until convergence which is
achieved when the label distribution on each node
ceases to change within some tolerance value. Alter-
natively, the algorithm can be run for a fixed number
of iterations which is what we used in practice3.
Finally, since Adsorption is memoryless, it eas-
ily scales to tens of millions of nodes with dense
edges and can be easily parallelized, as described
by Baluja et al (2008).
3The number of iterations was set to 10 in the experiments
reported in this paper.
Algorithm 1 Adsorption Algorithm.
Input: G? = (V
?
, E
?
,W ?), Iv (?v ? V ?).
Output: Distributions {Lv : v ? V }.
1: Lv = Iv ?v ? V
?
2:
3: repeat
4: Nv =
?
u W (u, v)
5: Dv = 1Nv
?
u W (u, v)Lu ?v ? V
?
6: for all v ? V
?
do
7: Lv = pcontv ?Dv +p
inj
v ? Iv +pabndv ?L
>
8: end for
9: until convergence
4 Experiments
4.1 Data
As mentioned in Section 3, one of the benefits of
using Adsorption is that we can combine extrac-
tions by different methods from diverse sources into
a single framework. To demonstrate this capabil-
ity, we combine extractions from free-text patterns
and from Web tables. To the best of our knowl-
edge, this is one of the first attempts in the area of
minimally-supervised extraction algorithms where
unstructured and structured text are used in a prin-
cipled way within a single system.
Open-domain (instance, class) pairs were ex-
tracted by applying the method described by Van
Durme and Pas?ca (2008) on a corpus of over 100M
English web documents. A total of 924K (instance,
class) pairs were extracted, containing 263K unique
instances in 9081 classes. We refer to this dataset as
A8.
Using A8, an additional 74M unique (in-
stance,class) pairs are extracted from a random 10%
of the WebTables data, using the method outlined in
Section 2.2. For maximum coverage we set ? = 2
and d = 2, resulting in a large, but somewhat noisy
collection. We refer to this data set as WT.
4.2 Graph Creation
We applied the graph construction scheme described
in Section 3 on the A8 and WT data combined, re-
sulting in a graph with 1.4M nodes and 75M edges.
Since extractions in A8 are not scored, weight of all
586
Seed Class Seed Instances
Book Publishers millbrook press, academic press, springer verlag, chronicle books, shambhala publications
Federal Agencies dod, nsf, office of war information, tsa, fema
Mammals african wild dog, hyaena, hippopotamus, sperm whale, tiger
NFL Players ike hilliard, isaac bruce, torry holt, jon kitna, jamal lewis
Scientific Journals american journal of roentgenology, pnas, journal of bacteriology, american economic review,
ibm systems journal
Table 2: Classes and seeds used to initialize Adsorption.
edges originating from A8 were set at 14. This graph
is used in all subsequent experiments.
5 Evaluation
We evaluated the Adsorption algorithm under two
experimental settings. First, we evaluate Adsorp-
tion?s extraction precision on (instance, class) pairs
obtained by Adsorption but not present in A8 (Sec-
tion 5.1). This measures whether Adsorption can
add to the A8 extractions at fairly high precision.
Second, we measured Adsorption?s ability to assign
labels to a fixed set of gold instances drawn from
various classes (Section 5.2).
Book Publishers Federal Agencies NFL Players Scientific Journals Mammals20
40
60
80
100
 
 Adsorption A8
Book
Publishers
Federal
Agencies
NFL
Players
Scientific
Journals
Mammals
A8 Adsorption
Figure 2: Precision at 100 comparisons for A8 and Ad-
sorption.
5.1 Instance Precision
First we manually evaluated precision across five
randomly selected classes from A8: Book Publish-
ers, Federal Agencies, NFL Players, Scientific Jour-
nals and Mammals. For each class, 5 seed in-
stances were chosen manually to initialize Adsorp-
tion. These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the
4A8 extractions are assumed to be high-precision and hence
we assign them the highest possible weight.
resulting ranked extractions were manually evalu-
ated.
Since the A8 system does not produce ranked lists
of instances, we chose 100 random instances from
the A8 results to compare to the top 100 instances
produced by Adsorption. Each of the resulting 500
instance-class pairs (i, C) was presented to two hu-
man evaluators, who were asked to evaluate whether
the relation ?i is a C? was correct or incorrect. The
user was also presented with Web search link to ver-
ify the results against actual documents. Results
from these experiments are presented in Figure 2
and Table 4. The results in Figure 2 show that the
A8 system has higher precision than the Adsorption
system. This is not surprising since the A8 system is
tuned for high precision. When considering individ-
ual evaluation classes, changes in precision scores
between the A8 system and the Adsorption system
vary from a small increase from 87% to 89% for the
class Book Publishers, to a significant decrease from
52% to 34% for the class Federal Agencies, with a
decrease of 10% as an average over the 5 evaluation
classes.
Class Precision at 100
(non-A8 extractions)
Book Publishers 87.36
Federal Agencies 29.89
NFL Players 94.95
Scientific Journals 90.82
Mammal Species 84.27
Table 4: Precision of top 100 Adsorption extractions (for
five classes) which were not present in A8.
Table 4 shows the precision of the Adsorption sys-
tem for instances not extracted by the A8 system.
587
Seed Class Non-Seed Class Labels Discovered by Adsorption
Book Publishers small presses, journal publishers, educational publishers, academic publishers,
commercial publishers
Federal Agencies public agencies, governmental agencies, modulation schemes, private sources,
technical societies
NFL Players sports figures, football greats, football players, backs, quarterbacks
Scientific Journals prestigious journals, peer-reviewed journals, refereed journals, scholarly journals,
academic journals
Mammal Species marine mammal species, whale species, larger mammals, common animals, sea mammals
Table 3: Top class labels ranked by their similarity to a given seed class in Adsorption.
Seed Class Sample of Top Ranked Instances Discovered by Adsorption
Book Publishers small night shade books, house of anansi press, highwater books,
distributed art publishers, copper canyon press
NFL Players tony gonzales, thabiti davis, taylor stubblefield, ron dixon, rodney hannah
Scientific Journals journal of physics, nature structural and molecular biology,
sciences sociales et sante?, kidney and blood pressure research,
american journal of physiology?cell physiology
Table 5: Random examples of top ranked extractions (for three classes) found by Adsorption which were not present
in A8.
Such an evaluation is important as one of the main
motivations of the current work is to increase cov-
erage (recall) of existing high-precision extractors
without significantly affecting precision. Results in
Table 4 show that Adsorption is indeed able to ex-
traction with high precision (in 4 out of 5 cases)
new instance-class pairs which were not extracted
by the original high-precision extraction set (in this
case A8). Examples of a few such pairs are shown
in Table 5. This is promising as almost all state-
of-the-art extraction methods are high-precision and
low-recall. The proposed method shows a way to
overcome that limitation.
As noted in Section 3, Adsorption ignores node
type and hence the final ranked extraction may also
contain classes along with instances. Thus, in ad-
dition to finding new instances for classes, it also
finds additional class labels similar to the seed class
labels with which Adsorption was run, at no extra
cost. Some of the top ranked class labels extracted
by Adsorption for the corresponding seed class la-
bels are shown in Table 3. To the best of our knowl-
edge, there are no other systems which perform both
tasks simultaneously.
5.2 Class Label Recall
Next we evaluated each extraction method on its rel-
ative ability to assign labels to class instances. For
each test instance, the five most probably class la-
bels are collected using each method and the Mean
Reciprocal Rank (MRR) is computed relative to a
gold standard target set. This target set, WN-gold,
consists of the 38 classes in Wordnet containing 100
or more instances.
In order to extract meaningful output from Ad-
sorption, it is provided with a number of labeled seed
instances (1, 5, 10 or 25) from each of the 38 test
classes. Regardless of the actual number of seeds
used as input, all 25 seed instances from each class
are removed from the output set from all methods,
in order to ensure fair comparison.
The results from this evaluation are summarized
in Table 6; AD x refers to the adsorption run with x
seed instances. Overall, Adsorption exhibits higher
MRR than either of the baseline methods, with MRR
increasing as the amount of supervision is increased.
Due to its high coverage, WT assigns labels to
a larger number of the instance in WN-gold than
any other method. However, the average rank of
the correct class assignment is lower, resulting is
588
MRR MRR # found
Method (full) (found only)
A8 0.16 0.47 2718
WT 0.15 0.21 5747
AD 1 0.26 0.45 4687
AD 5 0.29 0.48 4687
AD 10 0.30 0.51 4687
AD 25 0.32 0.55 4687
Table 6: Mean-Reciprocal Rank scores of instance class
labels over 38 Wordnet classes (WN-gold). MRR (full)
refers to evaluation across the entire gold instance set.
MRR (found only) computes MRR only on recalled in-
stances.
lower MRR scores compared to Adsorption. This
result highlights Adsorption?s ability to effectively
combine high-precision, low-recall (A8) extractions
with low-precision, high-recall extractions (WT) in
a manner that improves both precision and coverage.
6 Related Work
Graph based algorithms for minimally supervised
information extraction methods have recently been
proposed. For example, Wang and Cohen (2007)
use a random walk on a graph built from entities and
relations extracted from semi-structured text. Our
work differs both conceptually, in terms of its focus
on open-domain extraction, as well as methodologi-
cally, as we incorporate both unstructured and struc-
tured text. The re-ranking algorithm of Bellare et al
(2007) also constructs a graph whose nodes are in-
stances and attributes, as opposed to instances and
classes here. Adsorption can be seen as a general-
ization of the method proposed in that paper.
7 Conclusion
The field of open-domain information extraction has
been driven by the growth of Web-accessible data.
We have staggering amounts of data from various
structured and unstructured sources such as general
Web text, online encyclopedias, query logs, web ta-
bles, or link anchor texts. Any proposed algorithm
to extract information needs to harness several data
sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards
that goal. In doing so, we achieved the following:
1. Improved coverage relative to a high accuracy
instance-class extraction system while main-
taining adequate precision.
2. Combined information from two different
sources: free text and web tables.
3. Demonstrated a graph-based label propagation
algorithm that given as little as five seeds per
class achieved good results on a graph with
more than a million nodes and 70 million
edges.
In this paper, we started off with a simple graph.
For future work, we plan to proceed along the fol-
lowing lines:
1. Encode richer relationships between nodes,
for example instance-instance associations and
other types of nodes.
2. Combine information from more data sources
to answer the question of whether more data or
diverse sources are more effective in increasing
precision and coverage.
3. Apply similar ideas to other information extrac-
tion tasks such as relation extraction.
Acknowledgments
We would like to thank D. Sivakumar for useful dis-
cussions and the anonymous reviewers for helpful
comments.
References
A. Azran. 2007. The rendezvous algorithm: multiclass
semi-supervised learning with markov random walks.
Proceedings of the 24th international conference on
Machine learning, pages 49?56.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random
walks through the view graph.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. NIPS 2007Workshop
on Machine Learning for Web Search.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. Webtables: Exploring the power of tables on the
web. VLDB.
589
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
P. Indyk and J. Matousek. 2004. Low-distortion embed-
dings of finite metric spaces. Handbook of Discrete
and Computational Geometry.
B. Jansen, A. Spink, and T. Saracevic. 2000. Real life,
real users, and real needs: a study and analysis of user
queries on the Web. Information Processing and Man-
agement, 36(2):207?227.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
K. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence (IJCAI-95), pages 1050?1055, Montreal, Que-
bec.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-
derived name lists for named entity recognition. In
Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), Seattle, Wash-
ington.
M. Szummer and T. Jaakkola. 2002. Partially labeled
classification with markov random walks. Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2002 NIPS Conference.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. Twenty-Third AAAI Conference on Artificial In-
telligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using theWeb. Data
Mining, 2007. ICDM 2007. Seventh IEEE Interna-
tional Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
590
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 620?628,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Latent Variable Models of Concept-Attribute Attachment
Joseph Reisinger?
Department of Computer Sciences
The University of Texas at Austin
Austin, Texas 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
This paper presents a set of Bayesian
methods for automatically extending the
WORDNET ontology with new concepts
and annotating existing concepts with
generic property fields, or attributes. We
base our approach on Latent Dirichlet Al-
location and evaluate along two dimen-
sions: (1) the precision of the ranked
lists of attributes, and (2) the quality of
the attribute assignments to WORDNET
concepts. In all cases we find that the
principled LDA-based approaches outper-
form previously proposed heuristic meth-
ods, greatly improving the specificity of
attributes at each concept.
1 Introduction
We present a Bayesian approach for simultane-
ously extending Is-A hierarchies such as those
found in WORDNET (WN) (Fellbaum, 1998) with
additional concepts, and annotating the resulting
concept graph with attributes, i.e., generic prop-
erty fields shared by instances of that concept. Ex-
amples of attributes include ?height? and ?eye-
color? for the concept Person or ?gdp? and ?pres-
ident? for Country. Identifying and extracting
such attributes relative to a set of flat (i.e., non-
hierarchically organized) labeled classes of in-
stances has been extensively studied, using a vari-
ety of data, e.g., Web search query logs (Pas?ca and
Van Durme, 2008), Web documents (Yoshinaga
and Torisawa, 2007), and Wikipedia (Suchanek et
al., 2007; Wu and Weld, 2008).
Building on the current state of the art in at-
tribute extraction, we propose a model-based ap-
proach for mapping flat sets of attributes anno-
tated with class labels into an existing ontology.
This inference problem is divided into two main
components: (1) identifying the appropriate par-
ent concept for each labeled class and (2) learning
?Contributions made during an internship at Google.
the correct level of abstraction for each attribute in
the extended ontology. For example, consider the
task of annotating WN with the labeled class re-
naissance painters containing the class instances
Pisanello, Hieronymus Bosch, and Jan van Eyck
and associated with the attributes ?famous works?
and ?style.? Since there is no WN concept for
renaissance painters, the latter would need to be
mapped into WN under, e.g., Painter. Further-
more, since ?famous works? and ?style? are not
specific to renaissance painters (or even the WN
concept Painter), they should be placed at the
most appropriate level of abstraction, e.g., Artist.
In this paper, we show that both of these goals
can be realized jointly using a probabilistic topic
model, namely hierarchical Latent Dirichlet Allo-
cation (LDA) (Blei et al, 2003b).
There are three main advantages to using a topic
model as the annotation procedure: (1) Unlike hi-
erarchical clustering (Duda et al, 2000), the at-
tribute distribution at a concept node is not com-
posed of the distributions of its children; attributes
found specific to the concept Painter would not
need to appear in the distribution of attributes for
Person, making the internal distributions at each
concept more meaningful as attributes specific to
that concept; (2) Since LDA is fully Bayesian, its
model semantics allow additional prior informa-
tion to be included, unlike standard models such as
Latent Semantic Analysis (Hofmann, 1999), im-
proving annotation precision; (3) Attributes with
multiple related meanings (i.e., polysemous at-
tributes) are modeled implicitly: if an attribute
(e.g., ?style?) occurs in two separate input classes
(e.g., poets and car models), then that attribute
might attach at two different concepts in the ontol-
ogy, which is better than attaching it at their most
specific common ancestor (Whole) if that ancestor
is too general to be useful. However, there is also
a pressure for these two occurrences to attach to a
single concept.
We use WORDNET 3.0 as the specific test on-
tology for our annotation procedure, and evalu-
620
anticancer drugs: mechanism of action, uses, extrava-
sation, solubility, contraindications, side effects, chem-
istry, molecular weight, history, mode of action
bollywood actors: biography, filmography, age, bio-
data, height, profile, autobiography, new wallpapers, lat-
est photos, family pictures
citrus fruits: nutrition, health benefits, nutritional value,
nutritional information, calories, nutrition facts, history
european countries: population, flag, climate, presi-
dent, economy, geography, currency, population density,
topography, vegetation, religion, natural resources
london boroughs: population, taxis, local newspapers,
mp, lb, street map, renault connexions, local history
microorganisms: cell structure, taxonomy, life cycle,
reproduction, colony morphology, scientific name, vir-
ulence factors, gram stain, clipart
renaissance painters: early life, bibliography, short bi-
ography, the david, bio, painting, techniques, homosexu-
ality, birthplace, anatomical drawings, famous paintings
Figure 1: Examples of labeled attribute sets ex-
tracted using the method from (Pas?ca and Van
Durme, 2008).
ate three variants: (1) a fixed structure approach
where each flat class is attached to WN using
a simple string-matching heuristic, and concept
nodes are annotated using LDA, (2) an extension
of LDA allowing for sense selection in addition to
annotation, and (3) an approach employing a non-
parametric prior over tree structures capable of in-
ferring arbitrary ontologies.
The remainder of this paper is organized as fol-
lows: ?2 describes the full ontology annotation
framework, ?3 introduces the LDA-based topic
models, ?4 gives the experimental setup, ?5 gives
results, ?6 gives related work and ?7 concludes.
2 Ontology Annotation
Input to our ontology annotation procedure con-
sists of sets of class instances (e.g., Pisanello,
Hieronymus Bosch) associated with class labels
(e.g., renaissance painters) and attributes (e.g.,
?birthplace?, ?famous works?, ?style? and ?early
life?). Clusters of noun phrases (instances) are
constructed using distributional similarity (Lin
and Pantel, 2002; Hearst, 1992) and are labeled
by applying ?such-as? surface patterns to raw Web
text (e.g., ?renaissance painters such as Hierony-
mous Bosch?), yielding 870K instances in more
than 4500 classes (Pas?ca and Van Durme, 2008).
Attributes for each flat labeled class are ex-
tracted from anonymized Web search query
logs using the minimally supervised procedure
in (Pas?ca, 2008)1. Candidate attributes are ranked
based on their weighted Jaccard similarity to a
set of 5 manually provided seed attributes for the
1Similar query data, including query strings and fre-
quency counts, is available from, e.g., (Gao et al, 2007)
LDA
?
?
z
?
D
T
w
?
?
?
z
?
D
T
w
?
c
Fixed Structure LDA
?
?
z
?
D
?
w
?
T
c
?
nCRP
T
ww
w
Figure 2: Graphical models for the LDA variants;
shaded nodes indicate observed quantities.
class european countries. Figure 1 illustrates sev-
eral such labeled attribute sets (the underlying in-
stances are not depicted). Naturally, the attributes
extracted are not perfect, e.g., ?lb? and ?renault
connexions? as attributes for london boroughs.
We propose a set of Bayesian generative models
based on LDA that take as input labeled attribute
sets generated using an extraction procedure such
as the above and organize the attributes in WN ac-
cording to their level of generality. Annotating
WN with attributes proceeds in three steps: (1)
attaching labeled attribute sets to leaf concepts in
WN using string distance, (2) inferring an attribute
model using one of the LDA variants discussed in
? 3, and (3) generating ranked lists of attributes for
each concept using the model probabilities (? 4.3).
3 Hierarchical Topic Models
3.1 Latent Dirichlet Allocation
The underlying mechanism for our annotation
procedure is LDA (Blei et al, 2003b), a fully
Bayesian extension of probabilistic Latent Seman-
tic Analysis (Hofmann, 1999). Given D labeled
attribute sets wd, d ? D, LDA infers an unstruc-
tured set of T latent annotated concepts over
which attribute sets decompose as mixtures.2 The
latent annotated concepts represent semantically
coherent groups of attributes expressed in the data,
as shown in Example 1.
The generative model for LDA is given by
?d|? ? Dir(?), d ? 1 . . . D
?t|? ? Dir(?), t ? 1 . . . T
zi,d|?d ? Mult(?d), i ? 1 . . . |wd|
wi,d|?zi,d ? Mult(?zi,d), i ? 1 . . . |wd|
(1)
where ? and ? are hyperparameters smoothing
the per-attribute set distribution over concepts and
per-concept attribute distribution respectively (see
Figure 2 for the graphical model). We are inter-
ested in the case where w is known and we want
2In topic modeling literature, attributes are words and at-
tribute sets are documents.
621
to compute the conditional posterior of the remain-
ing random variables p(z,?,?|w). This distribu-
tion can be approximated efficiently using Gibbs
sampling. See (Blei et al, 2003b) and (Griffiths
and Steyvers, 2002) for more details.
(Example 1) Given 26 labeled attribute sets falling into
three broad semantic categories: philosophers, writers
and actors (e.g., sets for contemporary philosophers,
women writers, bollywood actors), LDA is able to infer
a meaningful set of latent annotated concepts:
quotations
teachings
virtue ethics
philosophies
biography
sayings
new movies
filmography
official website
biography
email address
autobiography
writing style
influences
achievements
bibliography
family tree
short biography
(philosopher) (writer) (actor)
(concept labels manually added for the latent annotated
concepts are shown in parentheses). Note that with a flat
concept structure, attributes can only be separated into
broad clusters, so the generality/specificity of attributes
cannot be inferred. Parameters were ?=1, ?=0.1, T=3.
3.2 Fixed-Structure LDA
In this paper, we extend LDA to model structural
dependencies between latent annotated concepts
(cf. (Li and McCallum, 2006; Sivic et al, 2008));
In particular, we fix the concept structure to cor-
respond to the WN Is-A hierarchy. Each labeled
attribute set is assigned to a leaf concept in WN
based on the edit distance between the concept la-
bel and the attribute set label. Possible latent con-
cepts for this set include the concepts along all
paths from its attachment point to the WN root,
following Is-A relation edges. Therefore, any two
labeled attribute sets share a number of latent con-
cepts based on their similarity in WN: all labeled
attribute sets share at least the root concept, and
may share more concepts depending on their most
specific, common ancestor. Under such a model,
more general attributes naturally attach to latent
concept nodes closer to the root, and more specific
attributes attach lower (Example 2).
Formally, we introduce into LDA an extra set of
random variables cd identifying the subset of con-
cepts in T available to attribute set d, as shown
in the diagram at the middle of Figure 2.3 For
example, with a tree structure, cd would be con-
strained to correspond to the concept nodes in T
on the path from the root to the leaf containing d.
Equation 1 can be adapted to this case if the in-
dex t is taken to range over concepts applicable to
attribute set d.
3Abusing notation, we use T to refer to a structured set of
concepts and to refer to the number of concepts in flat LDA
(Example 2 ) Fixing the latent concept structure to cor-
respond to WN (dark/purple nodes), and attaching each
labeled attribute set (examples depicted by light/orange
nodes) yields the annotated hierarchy:
works
picture
writings
history
biography
philosophy
natural rights
criticism
ethics
law
literary criticism
books
essays
short stories
novels 
tattoos
funeral
filmography
biographies
net worth
person
philosopher writer actor
scholar
intellectual
performer
entertainerliterate
communicator
bollywood
actors
women
writers
contemporary 
philosophers
Attribute distributions for the small nodes are not shown.
Dotted lines indicate multiple paths from the root, which
can be inferred using sense selection. Unlike with the flat
annotated concept structure, with a hierarchical concept
structure, attributes can be separated by their generality.
Parameters were set at ?=1 and ?=0.1.
3.3 Sense-Selective LDA
For each labeled attribute set, determining the ap-
propriate parent concept in WN is difficult since a
single class label may be found in many different
synsets (for example, the class bollywood actors
might attach to the ?thespian? sense of Actor or
the ?doer? sense). Fixed-hierarchy LDA can be
extended to perform automatic sense selection by
placing a distribution over the leaf concepts c, de-
scribing the prior probability of each possible path
through the concept tree. For WN, this amounts
to fixing the set of concepts to which a labeled at-
tribute set can attach (e.g., restricting it to a seman-
tically similar subset) and assigning a probability
to each concept (e.g., using the relative WN con-
cept frequencies). The probability for each sense
attachment cd becomes
p(cd|w, c?d, z) ? p(wd|c,w?d, z)p(cd|c?d),
i.e., the complete conditionals for sense selection.
p(cd|c?d) is the conditional probability for attach-
ing attribute set d at cd (e.g., simply the prior
p(cd|c?d)
def
= p(cd) in the WN case). A closed
form expression for p(wd|c,w?d, z) is derived
in (Blei et al, 2003a).
3.4 Nested Chinese Restaurant Process
In the final model, shown in the diagram on the
right side of Figure 2, LDA is extended hierarchi-
cally to infer arbitrary fixed-depth tree structures
622
from data. Unlike the fixed-structure and sense-
selective approaches which use the WN hierarchy
directly, the nCRP generates its own annotated hi-
erarchy whose concept nodes do not necessarily
correspond to WN concepts (Example 3). Each
node in the tree instead corresponds to a latent an-
notated concept with an arbitrary number of sub-
concepts, distributed according to a Dirichlet Pro-
cess (Ferguson, 1973). Due to its recursive struc-
ture, the underlying model is called the nested Chi-
nese Restaurant Process (nCRP). The model in
Equation 1 is extended with cd|? ? nCRP(?, L),
d ? D i.e., latent concepts for each attribute set are
drawn from an nCRP. The hyperparameter ? con-
trols the probability of branching via the per-node
Dirichlet Process, and L is the fixed tree depth.
An efficient Gibbs sampling procedure is given
in (Blei et al, 2003a).
(Example 3) Applying nCRP to the same three semantic
categories: philosophers, writers and actors, yields the
model:
biography
date of birth
childhood
picture
family
works
books
quotations
critics
poems
teachings
virtue ethics
structuralism
philosophies 
political theory
criticism
short stories
style
poems 
complete works
accomplishments
official website
profile
life story
achievements
filmography
pictures 
new movies
official site
works
(root)
(philosopher) (writer) (actor)
bollywood
actors
women
writers
contemporary 
philosophers
(manually added labels are shown in parentheses). Un-
like in WN, the inferred structure naturally places
philosopher and writer under the same subconcept,
which is also separate from actor. Hyperparameters were
?=0.1, ?=0.1, ?=1.0.
4 Experimental Setup
4.1 Data Analysis
We employ two data sets derived using the pro-
cedure in (Pas?ca and Van Durme, 2008): the full
set of automatic extractions generated in ? 2, and a
subset consisting of all attribute sets that fall under
the hierarchies rooted at the WN concepts living
thing#1 (i.e., the first sense of living thing), sub-
stance#7, location#1, person#1, organization#1
and food#1, manually selected to cover a high-
precision subset of labeled attribute sets. By com-
paring the results across the two datasets we can
measure each model?s robustness to noise.
In the full dataset, there are 4502 input attribute
sets with a total of 225K attributes (24K unique),
of which 8121 occur only once. The 10 attributes
occurring in the most sets (history, definition, pic-
ture(s), images, photos, clipart, timeline, clip art,
types) account for 6% of the total. For the subset,
there are 1510 attribute sets with 76K attributes
(11K unique), of which 4479 occur only once.
4.2 Model Settings
Baseline: Each labeled attribute set is mapped to
the most common WN concept with the closest la-
bel string distance (Pas?ca, 2008). Attributes are
propagated up the tree, attaching to node c if they
are contained in a majority of c?s children.
LDA: LDA is used to infer a flat set of T = 300
latent annotated concepts describing the data. The
concept selection smoothing parameter is set as
?=100. The smoother for the per-concept multi-
nomial over words is set as ?=0.1.4 The effects of
concept structure on attribute precision can be iso-
lated by comparing the structured models to LDA.
Fixed-Structure LDA (fsLDA): The latent con-
cept hierarchy is fixed based on WN (? 3.2), and
labeled attribute sets are mapped into it as in base-
line. The concept graph for each labeled attribute
set wd is decomposed into (possibly overlapping)
chains, one for each unique path from the WN root
to wd?s attachment point. Each path is assigned a
copy wd, reducing the bias in attribute sets with
many unique ancestor concepts.5 The final mod-
els contain 6566 annotated concepts on average.
Sense-Selective LDA (ssLDA): For the sense se-
lective approach (? 3.3), the set of possible sense
attachments for each attribute set is taken to be all
WN concepts with the lowest edit distance to its
label, and the conditional probability of each sense
attachment p(cd) is set proportional to its relative
frequency. This procedure results in 2 to 3 senses
per attribute set on average, yielding models with
7108 annotated concepts.
Arbitrary hierarchy (nCRP): For the arbitrary
hierarchy model (? 3.4), we set the maximum
tree depth L=5, per-concept attribute smoother
?=0.05, concept assignment smoother ?=10 and
nCRP branching proportion ?=1.0. The resulting
4(Parameter setting) Across all models, the main results
in this paper are robust to changes in ?. For nCRP, changes
in ? and ? affect the size of the learned model but have less
effect on the final precision. Larger values for L give the
model more flexibility, but take longer to train.
5Reducing the directed-acyclic graph to a tree ontology
did not significantly affect precision.
623
models span 380 annotated concepts on average.
4.3 Constructing Ranked Lists of Attributes
Given an inferred model, there are several ways to
construct ranked lists of attributes:
Per-Node Distribution: In fsLDA and ssLDA,
attribute rankings can be constructed directly for
each WN concept c, by computing the likelihood
of attribute w attaching to c, L(c|w) = p(w|c) av-
eraged over all Gibbs samples (discarding a fixed
number of samples for burn-in). Since c?s attribute
distribution is not dependent on the distributions
of its children, the resulting distribution is biased
towards more specific attributes.
Class-Entropy (CE): In all models, the inferred
latent annotated concepts can be used to smooth
the attribute rankings for each labeled attribute set.
Each sample from the posterior is composed of
two components: (1) a multinomial distribution
over a set of WN nodes, p(c|wd, ?) for each at-
tribute set wd, where the (discrete) values of c are
WN concepts, and (2) a multinomial distribution
over attributes p(w|c, ?) for each WN concept c.
To compute an attribute ranking for wd, we have
p(w|wd) =
?
c
p(w|c, ?)p(c|wd, ?).
Given this new ranking for each attribute set, we
can compute new rankings for each WN concept
c by averaging again over all the wd that appear
as (possible indirect) descendants of c. Thus, this
method uses LDA to first perform reranking on the
raw extractions before applying the baseline ontol-
ogy induction procedure (? 4.2).6
CE ranking exhibits a ?conservation of entropy?
effect, whereby the proportion of general to spe-
cific attributes in each attribute set wd remains the
same in the posterior. If set A contains 10 specific
attributes and 30 generic ones, then the latter will
be favored over the former in the resulting distri-
bution 3 to 1. Conservation of entropy is a strong
assumption, and in particular it hinders improving
the specificity of attribute rankings.
Class-Entropy+Prior: The LDA-based models
do not inherently make use of any ranking infor-
mation contained in the original extractions. How-
ever, such information can be incorporated in the
form of a prior. The final ranking method com-
bines CE with an exponential prior over the at-
tribute rank in the baseline extraction. For each
attribute set, we compute the probability of each
6One simple extension is to run LDA again on the CE
ranked output, yielding an iterative procedure; however, this
was not found to significantly affect precision.
attribute p(w|wd) = plda(w|wd)pbase(w|wd), as-
suming a parametric form for pbase(w|wd)
def
=
?r(w,wd). Here, r(w,wd) is the rank of w in at-
tribute set d. In all experiments reported, ?=0.9.
4.4 Evaluating Attribute Attachment
For the WN-based models, in addition to mea-
suring the average precision of the reranked at-
tributes, it is also useful to evaluate the assign-
ment of attributes to WN concepts. For this eval-
uation, human annotators were asked to determine
the most appropriate WN synset(s) for a set of gold
attributes, taking into account polysemous usage.
For each model, ranked lists of possible concept
assignments C(w) are generated for each attribute
w, usingL(c|w) for ranking. The accuracy of a list
C(w) for an attribute w is measured by a scoring
metric that corresponds to a modification (Pas?ca
and Alfonseca, 2009) of the mean reciprocal rank
score (Voorhees and Tice, 2000):
DRR = max
1
rank(c)? (1 + PathToGold)
where rank(c) is the rank (from 1 up to 10) of a
concept c in C(w), and PathToGold is the length
of the minimum path along Is-A edges in the con-
ceptual hierarchies between the concept c, on one
hand, and any of the gold-standard concepts man-
ually identified for the attribute w, on the other
hand. The length PathToGold is 0, if the returned
concept is the same as the gold-standard concept.
Conversely, a gold-standard attribute receives no
credit (that is, DRR is 0) if no path is found in
the hierarchies between the top 10 concepts of
C(w) and any of the gold-standard concepts, or if
C(w) is empty. The overalll precision of a given
model is the average of the DRR scores of individ-
ual attributes, computed over the gold assignment
set (Pas?ca and Alfonseca, 2009).
5 Results
5.1 Attribute Precision
Precision was manually evaluated relative to 23
concepts chosen for broad coverage.7 Table 1
shows precision at n and the Mean Average Preci-
sion (MAP); In all LDA-based models, the Bayes
average posterior is taken over all Gibbs samples
7(Precision evaluation) Attributes were hand annotated
using the procedure in (Pas?ca and Van Durme, 2008) and nu-
merical precision scores (1.0 for vital, 0.5 for okay and 0.0 for
incorrect) were assigned for the top 50 attributes per concept.
25 reference concepts were originally chosen, but 2 were not
populated with attributes in any method, and hence were ex-
cluded from the comparison.
624
Model Precision @ MAP
5 10 20 50
Base (unranked) 0.45 0.48 0.47 0.44 0.46
Base (ranked) 0.77 0.77 0.69 0.58 0.67
LDA? -24 ? 105
CE 0.64 0.53 0.52 0.56 0.55
CE+Prior 0.80 0.73 0.74 0.58 0.69
Fixed-structure (fsLDA) -22 ? 105
Per-Node 0.43 0.41 0.42 0.41 0.42
CE 0.75 0.68 0.63 0.55 0.63
CE+Prior 0.78 0.77 0.71 0.59 0.69
Sense-selective (ssLDA) -18 ? 105
Per-Node 0.37 0.44 0.42 0.41 0.42
CE 0.69 0.68 0.65 0.58 0.64
CE+Prior 0.81 0.80 0.72 0.60 0.70
nCRP? -14 ? 105
CE 0.74 0.76 0.73 0.65 0.72
CE+Prior 0.88 0.85 0.81 0.68 0.78
Subset only
Base (unranked) 0.61 0.62 0.62 0.60 0.62
Base (ranked) 0.79 0.82 0.72 0.65 0.72
?WN living thing 0.73 0.80 0.71 0.65 0.69
?WN substance 0.80 0.80 0.69 0.53 0.68
?WN location 0.95 0.93 0.84 0.75 0.84
?WN person 0.75 0.83 0.75 0.77 0.77
?WN organization 0.60 0.70 0.60 0.68 0.63
?WN food 0.90 0.85 0.58 0.45 0.64
Fixed-structure (fsLDA) -77 ? 104
Per-Node 0.64 0.58 0.52 0.56 0.55
CE 0.90 0.83 0.78 0.73 0.78
CE+Prior 0.88 0.86 0.80 0.66 0.78
?WN living thing 0.83 0.88 0.78 0.63 0.77
?WN substance 0.85 0.83 0.78 0.66 0.76
?WN location 0.95 0.95 0.88 0.75 0.85
?WN person 1.00 0.93 0.91 0.76 0.87
?WN organization 0.80 0.70 0.80 0.76 0.75
?WN food 0.80 0.70 0.63 0.40 0.59
nCRP? -45 ? 104
CE 0.88 0.88 0.78 0.71 0.79
CE+Prior 0.90 0.88 0.83 0.67 0.79
Table 1: Precision at n and mean-average preci-
sion for all models and data sets. Inset plots show
log-likelihood of each Gibbs sample, indicating
convergence except in the case of nCRP. ? indi-
cates models that do not generate annotated con-
cepts corresponding to WN nodes and hence have
no per-node scores.
after burn-in.8 The improvements in average pre-
cision are important, given the amount of noise in
the raw extracted data.
When prior attribute rank information (Per-
Node and CE scores) from the baseline extractions
is not incorporated, all LDA-based models outper-
form the unranked baseline (Table 1). In particu-
lar, LDA yields a 17% reduction in error (MAP)
8(Bayes average vs. maximum a-posteriori) The full
Bayesian average posterior consistently yielded higher preci-
sion than the maximum a-posteriori model. For the per-node
distributions, the fsLDA Bayes average model exhibits a 17%
reduction in relative error over the maximum a-posteriori es-
timate and for ssLDA there was a 26% reduction.
Model DRR Scores
all (n) found (n)
Base (unranked) 0.14 (150) 0.24 (91)
Base (ranked) 0.17 (150) 0.21 (123)
Fixed-structure
(fsLDA) 0.31 (150) 0.37 (128)
Sense-selective
(ssLDA) 0.31 (150) 0.37 (128)
Subset only
Base (unranked) 0.15 (97) 0.27 (54)
Base (ranked) 0.18 (97) 0.24 (74)
WN living thing 0.29 (27) 0.35 (22)
WN substance 0.21 (12) 0.32 (8)
WN location 0.12 (30) 0.17 (20)
WN person 0.37 (18) 0.44 (15)
WN organization 0.15 (31) 0.17 (27)
WN food 0.15 (6) 0.22 (4)
Fixed-structure
(fsLDA) 0.37 (97) 0.47 (77)
WN living thing 0.45 (27) 0.55 (22)
WN substance 0.48 (12) 0.64 (9)
WN location 0.34 (30) 0.44 (23)
WN person 0.44 (18) 0.52 (15)
WN organization 0.44 (31) 0.71 (19)
WN food 0.60 (6) 0.72 (5)
Table 2: All measures the DRR score relative to
the entire gold assignment set; found measures
DRR only for attributes with DRR(w)>0; n is the
number of scores averaged.
over the baseline, fsLDA yields a 31% reduction,
ssLDA yields a 33% reduction and nCRP yields
a 48% reduction (24% reduction over fsLDA).
Performance also improves relative to the ranked
baseline when prior ranking information is incor-
porated in the LDA-based models, as indicated
by CE+Prior scores in Table 1. LDA and fsLDA
reduce relative error by 6%, ssLDA by 9% and
nCRP by 33%. Furthermore, nCRP precision
without ranking information surpasses the base-
line with ranking information, indicating robust-
ness to extraction noise. Precision curves for indi-
vidual attribute sets are shown in Figure 3. Over-
all, learning unconstrained hierarchies (nCRP) in-
creases precision, but as the inferred node distri-
butions do not correspond to WN concepts they
cannot be used for annotation.
One benefit to using an admixture model like
LDA is that each concept node in the resulting
model contains a distribution over attributes spe-
cific only to that node (in contrast to, e.g., hierar-
chical agglomerative clustering). Although abso-
lute precision is lower as more general attributes
have higher average precision (Per-Node scores
in Table 1), these distributions are semantically
meaningful in many cases (Figure 4) and further-
more can be used to calculate concept assignment
precision for each attribute.9
9Per-node distributions (and hence DRR) were not evalu-
625
Figure 3: Precision (%) vs. rank plots (log scale) of attributes broken down across 18 labeled test attribute
sets. Ranked lists of attributes are generated using the CE+Prior method.
5.2 Concept Assignment Precision
The precision of assigning attributes to various
concepts is summarized in Table 2. Two scores are
given: all measures DRR relative to the entire gold
assignment set, and found measures DRR only for
attributes with DRR(w)>0. Comparing the scores
gives an estimate of whether coverage or precision
is responsible for differences in scores. fsLDA and
ssLDA both yield a 20% reduction in relative er-
ror (17.2% increase in absolute DRR) over the un-
ranked baseline and a 17.2% reduction (14.2% ab-
solute increase) over the ranked baseline.
5.3 Subset Precision and DRR
Precision scores for the manually selected subset
of extractions are given in the second half of Ta-
ble 1. Relative to the unranked baseline, fsLDA
and nCRP yield 42% and 44% reductions in er-
ror respectively, and relative to the ranked base-
line they both yield a 21.4% reduction. In terms of
absolute precision, there is no benefit to adding in
prior ranking knowledge to fsLDA or nCRP, in-
dicating diminishing returns as average baseline
precision increases (Baseline vs. fsLDA/nCRP CE
scores). Broken down across each of the subhier-
archies, LDA helps in all cases except food.
DRR scores for the subset are given in the lower
half of Table 2. Averaged over all gold test at-
tributes, DRR scores double when using fsLDA.
These results can be misleading, however, due
to artificially low coverage. Hence, Table 2 also
shows DRR scores broken down over each sub-
hierarchy, In this case fsLDA more than doubles
the DRR relative to the baseline for substance and
location, and triples it for organization and food.
ated for LDA or nCRP, because they are not mapped to WN.
6 Related Work
A large body of previous work exists on extend-
ing WORDNET with additional concepts and in-
stances (Snow et al, 2006; Suchanek et al, 2007);
these methods do not address attributes directly.
Previous literature in attribute extraction takes ad-
vantage of a range of data sources and extraction
procedures (Chklovski and Gil, 2005; Tokunaga
et al, 2005; Pas?ca and Van Durme, 2008; Yoshi-
naga and Torisawa, 2007; Probst et al, 2007; Van
Durme et al, 2008; Wu and Weld, 2008). How-
ever these methods do not address the task of de-
termining the level of specificity for each attribute.
The closest studies to ours are (Pas?ca, 2008), im-
plemented as the baseline method in this paper;
and (Pas?ca and Alfonseca, 2009), which relies on
heuristics rather than formal models to estimate
the specificity of each attribute.
7 Conclusion
This paper introduced a set of methods based on
Latent Dirichlet Allocation (LDA) for jointly ex-
tending the WORDNET ontology and annotating
its concepts with attributes (see Figure 4 for the
end result). LDA significantly outperformed a pre-
vious approach both in terms of the concept as-
signment precision (i.e., determining the correct
level of generality for an attribute) and the mean-
average precision of attribute lists at each concept
(i.e., filtering out noisy attributes from the base ex-
traction set). Also, relative precision of the attach-
ment models was shown to improve significantly
when the raw extraction quality increased, show-
ing the long-term viability of the approach.
626
entity
physical entity
bollywood
actors
actor
new wallpapers
upcoming movies
baby pictures
latest wallpapers
performer
filmography
new movies
schedule
new pictures
new pics
entertainer
hairstyle
hairstyles
music videos
songs
new pictures
sexy pictures
person
bio
autobiography
childhood
bibliography
accomplishments
timeline
organism
causal agent
living thing
photos
taxonomy
scientific name
reproduction
life cycle
habitat
whole
object
history
pictures
images
picture
photos
timeline
renaissance 
painters
painter
influenced
impressionist
the life
's paintings
style of
watercolor
artist
self portrait
paintings
famous works
self portraits
painting techniques
famous paintings
creator
influences
artwork
style
work
art
technique
european
countries
European
 country
recreation
national costume
prime minister
political parties
royal family
national parks
country
state codes
zipcodes
country profile
currencies
national anthem
telephone codes
administrative
 district
sights
weather forecast
culture
tourist spots
state map
district
traditional dress
per capita income
tourist spot
cuisine
folk dances
industrial policy
region
population
nightlife
street map
temperature
location
climate
tourist attractions
geography
weather
tourism
economy
drug
danger
half life
ingredients
side effects
withdrawal symptoms
sexual side effects
agent
pharmacokinetics
mechanism of action
long term effects
pharmacology
contraindications
mode of action
substance
matter
chemistry
ingredients
chemical structure
dangers
chemical formula
msds
liquors
liquor
drink mixes
apparitions
pitchers
existence
fantasy art
alcohol
carbohydrates
carbs
calories
alcohol content
pronunciation
glass
beveragedrug of abuse
sugar content
alcohol content
caffeine content
serving temperature
alcohol percentage
shelf life
liquid
food
advertisements
sugar content
adverts
brand
nutrition information
storage temperature
shelf life
nutritional facts
nutrition information
flavors
nutrition
nutritional information
fluid
recepies
gift baskets
receipes
rdi
daily allowance
fondue recipes
substance
density
uses
physical properties
melting point
chemical properties
chemical structure
abstraction
london
boroughs
borough
registry office
school term dates
local history
renault
citizens advice bureau
leisure centres
vegetables
vegetable
pests
nutritional values
music store
essential oil
nutrition value
dna extraction
produce
fiber
electricity
potassium
nutritional values
nutrition value
dna extraction
food
solid
material properties
refractive index
thermal properties
phase diagram
thermal expansion
aneurysm
parasites
parasite
pathogen
phobia
mortality rate
symptoms
treatment
orchestras
orchestra
recordings
broadcasts
recording
christmas
ticket
conductor
musical
 organization
dvorak
recordings
conductor
instrument
broadcasts
hall
organization
careers
ceo
phone number
annual report
london
company
social
group
jobs
website
logo
address
mission statement
president
group
ancient cities
city
port
cost of living
canadian embassy
city
air pollution
cheap hotels
municipality
sightseeing
weather forecast
tourist guide
american school
zoo
hospitals
?
?
?
red wines
wine
grape
vintage chart
grapes
city
food pairings
cheese
Figure 4: Example per-node attribute distribution generated by fsLDA. Light/orange nodes represent
labeled attribute sets attached to WN, and the full hypernym graph is given for each in dark/purple
nodes. White nodes depict the top attributes predicted for each WN concept. These inferred annotations
exhibit a high degree of concept specificity, naturally becoming more general at higher levels of the
ontology. Some annotations, such as for the concepts Agent, Substance, Living Thing and Person have
high precision and specificity while others, such as Liquor and Actor need improvement. Overall, the
more general concepts yield better annotations as they are averaged over many labeled attribute sets,
reducing noise. 627
References
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2003a. Hierarchical topic models and the nested
Chinese restaurant process. In Proceedings of the
17th Conference on Neural Information Process-
ing Systems (NIPS-2003), pages 17?24, Vancouver,
British Columbia.
D. Blei, A. Ng, and M. Jordan. 2003b. Latent dirich-
let alocation. Machine Learning Research, 3:993?
1022.
T. Chklovski and Y. Gil. 2005. An analysis of knowl-
edge collected from volunteer contributors. In Pro-
ceedings of the 20th National Conference on Arti-
ficial Intelligence (AAAI-05), pages 564?571, Pitts-
burgh, Pennsylvania.
R. Duda, P. Hart, and D. Stork. 2000. Pattern Classifi-
cation. John Wiley and Sons.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database and Some of its Applications. MIT
Press.
T. Ferguson. 1973. A bayesian analysis of some non-
parametric problems. Annals of Statistics, 1(2):209?
230.
W. Gao, C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and
H. Hon. 2007. Cross-lingual query suggestion using
query logs of different languages. In Proceedings of
the 30th ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR-07), pages
463?470, Amsterdam, The Netherlands.
T. Griffiths and M. Steyvers. 2002. A probabilistic ap-
proach to semantic representation. In Proceedings
of the 24th Conference of the Cognitive Science So-
ciety (CogSci02), pages 381?386, Fairfax, Virginia.
M. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING-92), pages 539?545, Nantes,
France.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of the 22nd ACM Confer-
ence on Research and Development in Information
Retrieval (SIGIR-99), pages 50?57, Berkeley, Cali-
fornia.
W. Li and A. McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic cor-
relations. In Proceedings of the 23rd International
Conference on Machine Learning (ICML-06), pages
577?584, Pittsburgh, Pennsylvania.
D. Lin and P. Pantel. 2002. Concept discovery from
text. In Proceedings of the 19th International Con-
ference on Computational linguistics (COLING-02),
pages 1?7, Taipei, Taiwan.
M. Pas?ca and E. Alfonseca. 2009. Web-derived re-
sources for Web Information Retrieval: From con-
ceptual hierarchies to attribute hierarchies. In Pro-
ceedings of the 32nd International Conference on
Research and Development in Information Retrieval
(SIGIR-09), Boston, Massachusetts.
M. Pas?ca and B. Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 19?27, Columbus, Ohio.
M. Pas?ca. 2008. Turning Web text and search
queries into factual knowledge: Hierarchical class
attribute extraction. In Proceedings of the 23rd Na-
tional Conference on Artificial Intelligence (AAAI-
08), pages 1225?1230, Chicago, Illinois.
K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu.
2007. Semi-supervised learning of attribute-value
pairs from product descriptions. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2838?2843, Hyder-
abad, India.
J. Sivic, B. Russell, A. Zisserman, W. Freeman, and
A. Efros. 2008. Unsupervised discovery of visual
object class hierarchies. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition (CVPR-08), pages 1?8, Anchorage, Alaska.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL-06), pages 801?808, Sydney, Aus-
tralia.
F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of the 16th World Wide
Web Conference (WWW-07), pages 697?706, Banff,
Canada.
K. Tokunaga, J. Kazama, and K. Torisawa. 2005. Au-
tomatic discovery of attribute words from Web doc-
uments. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 106?118, Jeju Island, Korea.
B. Van Durme, T. Qian, and L. Schubert. 2008.
Class-driven attribute extraction. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING-2008), pages 921?928,
Manchester, United Kingdom.
E.M. Voorhees and D.M. Tice. 2000. Building a
question-answering test collection. In Proceedings
of the 23rd International Conference on Research
and Development in Information Retrieval (SIGIR-
00), pages 200?207, Athens, Greece.
F. Wu and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the
17th World Wide Web Conference (WWW-08), pages
635?644, Beijing, China.
N. Yoshinaga and K. Torisawa. 2007. Open-domain
attribute-value acquisition from semi-structured
texts. In Proceedings of the 6th International Se-
mantic Web Conference (ISWC-07), Workshop on
Text to Knowledge: The Lexicon/Ontology Interface
(OntoLex-2007), pages 55?66, Busan, South Korea.
628
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173?1182,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Mixture Model with Sharing for Lexical Semantics
Joseph Reisinger
Department of Computer Science
University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
joeraii@cs.utexas.edu
Raymond Mooney
Department of Computer Science
University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
mooney@cs.utexas.edu
Abstract
We introduce tiered clustering, a mixture
model capable of accounting for varying de-
grees of shared (context-independent) fea-
ture structure, and demonstrate its applicabil-
ity to inferring distributed representations of
word meaning. Common tasks in lexical se-
mantics such as word relatedness or selec-
tional preference can benefit from modeling
such structure: Polysemous word usage is of-
ten governed by some common background
metaphoric usage (e.g. the senses of line or
run), and likewise modeling the selectional
preference of verbs relies on identifying com-
monalities shared by their typical arguments.
Tiered clustering can also be viewed as a form
of soft feature selection, where features that do
not contribute meaningfully to the clustering
can be excluded. We demonstrate the applica-
bility of tiered clustering, highlighting partic-
ular cases where modeling shared structure is
beneficial and where it can be detrimental.
1 Introduction
Word meaning can be represented as high-
dimensional vectors inhabiting a common space
whose dimensions capture semantic or syntactic
properties of interest (e.g. Erk and Pado, 2008;
Lowe, 2001). Such vector-space representations of
meaning induce measures of word similarity that can
be tuned to correlate well with judgements made
by humans. Previous work has focused on de-
signing feature representations and semantic spaces
that capture salient properties of word meaning (e.g.
Curran, 2004; Gabrilovich and Markovitch, 2007;
Landauer and Dumais, 1997), often leveraging the
distributional hypothesis, i.e. that similar words ap-
pear in similar contexts (Miller and Charles, 1991;
Pereira et al, 1993).
Since vector-space representations are con-
structed at the lexical level, they conflate multiple
word meanings into the same vector, e.g. collaps-
ing occurrences of bankinstitution and bankriver. Meth-
ods such as Clustering by Committee (Pantel, 2003)
and multi-prototype representations (Reisinger and
Mooney, 2010) address this issue by perform-
ing word-sense disambiguation across word occur-
rences, and then building meaning vectors from
the disambiguated words. Such approaches can
readily capture the structure of homonymous words
with several unrelated meanings (e.g. bat and club),
but are not suitable for representing the common
metaphor structure found in highly polysemous
words such as line or run.
In this paper, we introduce tiered clustering, a
novel probabilistic model of the shared structure
often neglected in clustering problems. Tiered
clustering performs soft feature selection, allocat-
ing features between a Dirichlet Process cluster-
ing model and a background model consisting of
a single component. The background model ac-
counts for features commonly shared by all occur-
rences (i.e. context-independent feature variation),
while the clustering model accounts for variation
in word usage (i.e. context-dependent variation, or
word senses; Table 1).
Using the tiered clustering model, we derive a
multi-prototype representation capable of capturing
varying degrees of sharing between word senses,
and demonstrate its effectiveness in lexical seman-
tic tasks where such sharing is desirable. In partic-
ular we show that tiered clustering outperforms the
multi-prototype approach for (1) selectional prefer-
ence (Resnik, 1997; Pantel et al, 2007), i.e. predict-
1173
ing the typical filler of an argument slot of a verb,
and (2) word-relatedness in the presence of highly
polysemous words. The former case exhibits a high
degree of explicit structure, especially for more se-
lectionally restrictive verbs (e.g. the set of things that
can be eaten or can shoot).
The remainder of the paper is organized as fol-
lows: Section 2 gives relevant background on the
methods compared, Section 3 outlines the multi-
prototype model based on the Dirichlet Process mix-
ture model, Section 4 derives the tiered cluster-
ing model, Section 5 discusses similarity metrics,
Section 6 details the experimental setup and in-
cludes a micro-analysis of feature selection, Section
7 presents results applying tiered clustering to word
relatedness and selectional preference, Section 8 dis-
cusses future work, and Section 9 concludes.
2 Background
Models of the attributional similarity of concepts,
i.e. the degree to which concepts overlap based on
their attributes (Turney, 2006), are commonly imple-
mented using vector-spaces derived from (1) word
collocations (Schu?tze, 1998), directly leveraging the
distributional hypothesis (Miller and Charles, 1991),
(2) syntactic relations (Pado? and Lapata, 2007), (3)
structured corpora (e.g. Gabrilovich and Markovitch
(2007)) or (4) latent semantic spaces (Finkelstein
et al, 2001; Landauer and Dumais, 1997). Such
models can be evaluated based on their correlation
with human-reported lexical similarity judgements
using e.g. the WordSim-353 collection (Finkelstein
et al, 2001). Distributional methods exhibit a high
degree of scalability (Gorman and Curran, 2006) and
have been applied broadly in information retrieval
(Manning et al, 2008), large-scale taxonomy induc-
tion (Snow et al, 2006), and knowledge acquisition
(Van Durme and Pas?ca, 2008).
Reisinger and Mooney (2010) introduced a multi-
prototype approach to vector-space lexical seman-
tics where individual words are represented as col-
lections of ?prototype? vectors. This representation
is capable of accounting for homonymy and poly-
semy, as well as other forms of variation in word
usage, like similar context-dependent methods (Erk
and Pado, 2008). The set of vectors for a word
is determined by unsupervised word sense discov-
ery (Schu?tze, 1998), which clusters the contexts in
which a word appears. Average prototype vectors
LIFE
all, about, life, would, death
my, you, real, your, about
spent, years, rest, lived, last
sentenced, imprisonment, sentence, prison
insurance, peer, Baron, member, company
Guru, Rabbi, Baba, la, teachings
RADIO
station, radio, stations, television
amateur, frequency, waves, system
show, host, personality, American
song, single, released, airplay
operator, contact, communications, message
WIZARD
evil, powerful, magic, wizard
Merlin, King, Arthur, Arthurian
fairy, wicked, scene, tale
Harry, Potter, Voldemort, Dumbledore
STOCK
stock, all, other, company, new
market, crash, markets, price, prices
housing, breeding, fish, water, horses
car, racing, cars, NASCAR, race, engine
card, cards, player, pile, game, paper
rolling, locomotives, line, new, railway
Table 1: Example tiered clustering representation of
words with varying degrees of polysemy. Each boxed
set shows the most common background (shared) fea-
tures, and each prototype captures one thematic usage
of the word. For example, wizard is broken up into a
background cluster describing features common to all us-
ages of the word (e.g., magic and evil) and several genre-
specific usages (e.g. Merlin, fairy tales and Harry Potter).
are then computed separately for each cluster, pro-
ducing a distributed representation for each word.
Distributional methods have also proven to be a
powerful approach to modeling selectional prefer-
ence (Pado? et al, 2007; Pantel et al, 2007), rivaling
methods based on existing semantic resources such
as WordNet (Clark and Weir, 2002; Resnik, 1997)
and FrameNet (Pado?, 2007) and performing nearly
as well as supervised methods (Herdag?delen and Ba-
roni, 2009). Selectional preference has been shown
to be useful for, e.g., resolving ambiguous attach-
ments (Hindle and Rooth, 1991), word sense disam-
biguation (McCarthy and Carroll, 2003) and seman-
tic role labeling (Gildea and Jurafsky, 2002).
3 Multi-Prototype Models
Representing words as mixtures over several pro-
totypes has proven to be a powerful approach to
1174
vector-space lexical semantics (Pantel, 2003; Pantel
et al, 2007; Reisinger and Mooney, 2010). In this
section we briefly introduce a version of the multi-
prototype model based on the Dirichlet Process Mix-
ture Model (DPMM), capable of inferring automat-
ically the number of prototypes necessary for each
word (Rasmussen, 2000). Similarity between two
DPMM word-representations is then computed as a
function of their cluster centroids (?5), instead of the
centroid of all the word?s occurrences.
Multiple prototypes for each word w are gener-
ated by clustering feature vectors vpcq derived from
each occurrence c P Cpwq in a large textual cor-
pus and collecting the resulting cluster centroids
pikpwq, k P r1,Kws. This approach is commonly
employed in unsupervised word sense discovery;
however, we do not assume that clusters correspond
to word senses. Rather, we only rely on clusters to
capture meaningful variation in word usage.
Instead of assuming all words can be repre-
sented by the same number of clusters, we allocate
representational flexibility dynamically using the
DPMM. The DPMM is an infinite capacity model
capable of assigning data to a variable, but finite
number of clusters Kw, with probability of assign-
ment to cluster k proportional to the number of data
points previously assigned to k. A single parameter
? controls the degree of smoothing, producing more
uniform clusterings as ? ? 8. Using this model,
the number of clusters no longer needs to be fixed
a priori, allowing the model to allocate expressivity
dynamically to concepts with richer structure. Such
a model naturally allows the word representation to
allocate additional capacity for highly polysemous
words, with the number of clusters growing loga-
rithmically with the number of occurrences. The
DPMM has been used for rational models of con-
cept organization (Sanborn et al, 2006), but to our
knowledge has not yet been applied directly to lexi-
cal semantics.
4 Tiered Clustering
Tiered clustering allocates features between two
submodels: a (context-dependent) DPMM and a sin-
gle (context-independent) background component.
This model is similar structurally to the feature se-
lective clustering model proposed by Law et al
(2002). However, instead of allocating entire feature
dimensions between model and background compo-
 
 
?
z
?
D w
 
w
?
!
?
background
!
c
 
?
 
?
 
?
clusters
d
Figure 1: Plate diagram for the tiered clustering model
with cluster indicators drawn from the Chinese Restau-
rant Process.
nents, assignment is done at the level of individual
feature occurrences, much like topic assignment in
Latent Dirichlet Allocation (LDA; Griffiths et al,
2007). At a high level, the tiered model can be
viewed as a combination of a multi-prototype model
and a single-prototype back-off model. However,
by leveraging both representations in a joint frame-
work, uninformative features can be removed from
the clustering, resulting in more semantically tight
clusters.
Concretely, each word occurrence wd first selects
a cluster ?d from the DPMM; then each feature wi,d
is generated from either the background model?back
or the selected cluster ?d, determined by the tier
indicator zi,d. The full generative model for tiered
clustering is given by
?d|?  Betap?q d P D,
?d|?, G0  DPp?, G0q d P D,
?back|?back  Dirichletp?backq
zi,d|?d  Bernoullip?dq i P |wd|,
wi,d|?d, zi,d 
$
'
'
&
'
'
%
Multp?backq
pzi,d  1q
Multp?dq
potherwiseq
i P |wd|,
where ? controls the per-data tier distribution
smoothing and ? controls the uniformity of the DP
cluster allocation. The DP is parameterized by a
base measure G0, controlling the per-cluster term
distribution smoothing; which use a Dirichlet with
hyperparameter ?, as is common (Figure 1).
Since the background topic is shared across all oc-
currences, it can account for features with context-
independent variance, such as stop words and other
high-frequency noise, as well as the central tendency
of the collection (Table 1). Furthermore, it is possi-
ble to put an asymmetric prior on ?, yielding more
fine-grained control over the assumed uniformity of
the occurrence of noisy features, unlike in the model
proposed by Law et al (2002).
1175
Although exact posterior inference is intractable
in this model, we derive an efficient collapsed Gibbs
sampler via analogy to LDA (Appendix 1).
5 Measuring Semantic Similarity
Due to its richer representational structure, comput-
ing similarity in the multi-prototype model is less
straightforward than in the single prototype case.
Reisinger and Mooney (2010) found that simply av-
eraging all similarity scores over all pairs of proto-
types (sampled from the cluster distributions) per-
forms reasonably well and is robust to noise. Given
two words w and w1, this AvgSim metric is
AvgSimpw,w1q def
1
KwKw1
Kw?
j1
Kw1
?
k1
dppikpwq, pijpw
1
qq
Kw andKw1 are the number of clusters for w and w1
respectively, and dp, q is a standard distributional
similarity measure (e.g. cosine distance). As cluster
sizes become more uniform, AvgSim tends towards
the single prototype similarity,1 hence the effective-
ness of AvgSim stems from boosting the influence
of small clusters.
Tiered clustering representations offer more pos-
sibilities for computing semantic similarity than
multi-prototype, as the background prototype can be
treated separately from the other prototypes. We
make use of a simple sum of the distance between
the two background components, and the AvgSim
of the two sets of clustering components.
6 Experimental Setup
6.1 Corpus
Word occurrence statistics are collected from a snap-
shot of English Wikipedia taken on Sept. 29th, 2009.
Wikitext markup is removed, as are articles with
fewer than 100 words, leaving 2.8M articles with a
total of 2.05B words. Wikipedia was chosen due to
its semantic breadth.
6.2 Evaluation Methodology
We evaluate the tiered clustering model on two prob-
lems from lexical semantics: word relatedness and
selectional preference. For the word relatedness
1This can be problematic for certain clustering methods
that specify uniform priors over cluster sizes; however the
DPMM naturally exhibits a linear decay in cluster sizes with
the Er# clusters of size M s  ?{M .
Rating distribution
WS-3530.0
0.5
1.0
Evocation Pado
Sense count distribution
WS-3530
3
10
80
Evocation Pado
Figure 2: (top) The distribution of ratings (scaled [0,1])
on WS-353, WN-Evocation and Pado? datasets. (bottom)
The distribution of sense counts for each data set (log-
domain), collected from WordNet 3.0.
evaluation, we compared the predicted similarity of
word pairs from each model to two collections of hu-
man similarity judgements: WordSim-353 (Finkel-
stein et al, 2001) and the Princeton Evocation rela-
tions (WN-Evocation, Ma et al, 2009).
WS-353 contains between 13 and 16 human sim-
ilarity judgements for each of 353 word pairs, rated
on a 1?10 integer scale. WN-Evocation is signif-
icantly larger than WS-353, containing over 100k
similarity comparisons collected from trained hu-
man raters. Comparisons are assigned to only 3-
5 human raters on average and contain a signifi-
cantly higher fraction of zero- and low-similarity
items than WS-353 (Figure 2), reflecting more ac-
curately real-world lexical semantics settings. In our
experiments we discard all comparisons with fewer
than 5 ratings and then sample 10% of the remain-
ing pairs uniformly at random, resulting in a test set
with 1317 comparisons.
For selectional preference, we employ the Pado?
dataset, which contains 211 verb-noun pairs with
human similarity judgements for how plausible the
noun is for each argument of the verb (2 arguments
per verb, corresponding roughly to subject and ob-
ject). Results are averaged across 20 raters; typical
inter-rater agreement is ?  0.7 (Pado? et al, 2007).
In all cases correlation with human judgements
is computed using Spearman?s nonparametric rank
correlation (?) with average human judgements
1176
(Agirre et al, 2009).
6.3 Feature Representation
In the following analyses we confine ourselves to
representing word occurrences using unordered un-
igrams collected from a window of size T10 cen-
tered around the occurrence, represented using tf-idf
weighting. Feature vectors are pruned to a fixed
length f , discarding all but the highest-weight fea-
tures (f is selected via empirical validation, as de-
scribed in the next section). Finally, semantic simi-
larity between word pairs is computed using cosine
distance (`2-normalized dot-product).2
6.4 Feature Pruning
Feature pruning is one of the most significant factors
in obtaining high correlation with human similarity
judgements using vector-space models, and has been
suggested as one way to improve sense disambigua-
tion for polysemous verbs (Xue et al, 2006). In this
section, we calibrate the single prototype and multi-
prototype methods on WS-353, reaching the limit
of human and oracle performance and demonstrat-
ing robust performance gains even with semanti-
cally impoverished features. In particular we obtain
?0.75 correlation on WS-353 using only unigram
collocations and ?0.77 using a fixed-K multi-
prototype representation (Figure 3; Reisinger and
Mooney, 2010). This result rivals average human
performance, obtaining correlation near that of the
supervised oracle approach of Agirre et al (2009).
The optimal pruning cutoff depends on the fea-
ture weighting and number of prototypes as well as
the feature representation. t-test and ?2 features are
most robust to feature noise and perform well even
with no pruning; tf-idf yields the best results but is
most sensitive to the pruning parameter (Figure 3).
As the number of features increases, more pruning
is required to combat feature noise.
Figure 4 breaks down the similarity pairs into four
quantiles for each data set and then shows corre-
lation separately for each quantile. In general the
more polarized data quantiles (1 and 4) have higher
correlation, indicating that fine-grained distinctions
2(Parameter robustness) We observe lower correlations on
average for T25 and T5 and therefore observe T10 to
be near-optimal. Substituting weighted Jaccard similarity for
cosine does not significantly affect the results in this paper.
0
0.4
0.8
0
0.4
0.8
S
p
e
a
r
m
a
n
'
s
 
?
 
0.7
0.0
-0.2
unpruned pruned (best)
Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4
human
Single-p
Multi-p
ESA
Figure 4: Correlation results on WS-353 broken down
over quantiles in the human ratings. Quantile ranges are
shown in Figure 2. In general ratings for highly sim-
ilar (dissimilar) pairs are more predictable (quantiles 1
and 4) than middle similarity pairs (quantiles 2, 3). ESA
shows results for a more semantically rich feature set de-
rived using Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007).
in semantic distance are easier for those sets.3 Fea-
ture pruning improves correlations in quantiles 2?4
while reducing correlation in quantile 1 (lowest sim-
ilarity). This result is to be expected as more fea-
tures are necessary to make fine-grained distinctions
between dissimilar pairs.
7 Results
We evaluate four models: (1) the standard single-
prototype approach, (2) the DPMM multi-prototype
approach outlined in ?3, (3) a simple combina-
tion of the multi-prototype and single-prototype ap-
proaches (MP+SP)4 and (4) the tiered clustering ap-
proach (?4). Each data set is divided into 5 quan-
tiles based on per-pair average sense counts,5 col-
lected from WordNet 3.0 (Fellbaum, 1998); ex-
amples of pairs in the high-polysemy quantile are
shown in Table 2. Unless otherwise specified,
both DPMM multi-prototype and tiered clustering
3The fact that the per-quantile correlation is significantly
lower than the full correlation e.g. in the human case indicates
that fine-grained ordering (within quantile) is more difficult than
coarse-grained (between quantile).
4(MP+SP) Tiered clustering?s ability to model both shared
and idiosyncratic structure can be easily approximated by us-
ing the single prototype model as the shared component and
multi-prototype model as the clustering. However, unlike in the
tiered model, all features are assigned to both components. We
demonstrate that this simplification actually hurts performance.
5Despite many skewed pairs (e.g. line has 36 senses while
insurance has 3), we found that arithmetic average and geomet-
ric average perform the same.
1177
00.4
0.8
0
0.4
0.8
0
0.4
0.8
0
0.4
0.8
K=1 K=10 K=50 tf-idf cosine, K=1,10,50
S
p
e
a
r
m
a
n
'
s
 
?
 
0.0
0.8
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
tf-idf
ttest
?2
tf
K=50
K=10
K=1
tf-idf
ttest
?2
tf
tf-idf
ttest
?2
tf
// //
// //
Figure 3: Effects of feature pruning and representation on WS-353 correlation broken down across multi-prototype
representation size. In general tf-idf features are the most sensitive to pruning level, yielding the highest correlation for
moderate levels of pruning and significantly lower correlation than other representations without pruning. The optimal
amount of pruning varies with the number of prototypes used, with fewer features being optimal for more clusters.
Bars show 95% confidence intervals.
WordSim-353
stock-live, start-match, line-insurance, game-
round, street-place, company-stock
Evocation
break-fire, clear-pass, take-call, break-tin,
charge-charge, run-heat, social-play
Pado?
see-drop, see-return, hit-stock, raise-bank, see-
face, raise-firm, raise-question
Table 2: Examples of highly polysemous pairs from each
data set using sense counts from WordNet.
use symmetric Dirichlet hyperparameters, ?0.1,
?0.1, and tiered clustering uses?10 for the back-
ground/clustering allocation smoother.
7.1 WordSim-353
Correlation results for WS-353 are shown in Table
3. In general the approaches incorporating multiple
prototypes outperform single prototype (?  0.768
vs. ?  0.734). The tiered clustering model does not
significantly outperform either the multi-prototype
or MP+SP models on the full set, but yields signifi-
cantly higher correlation on the high-polysemy set.
The tiered model generates more clusters than
DPMM multi-prototype (27.2 vs. 14.8), despite us-
ing the same hyperparameter settings: Since words
commonly shared across clusters have been allo-
cated to the background component, the cluster
components have less overlap and hence the model
naturally allocates more clusters.
Examples of the tiered clusterings for several
Method ?  100 ErCs background
Single prototype 73.40.5 1.0 -
high polysemy 76.00.9 1.0 -
Multi-prototype 76.80.4 14.8 -
high polysemy 79.31.3 12.5 -
MP+SP 75.40.5 14.8 -
high polysemy 80.11.0 12.5 -
Tiered 76.90.5 27.2 43.0%
high polysemy 83.11.0 24.2 43.0%
Table 3: Spearman?s correlation on the WS-353 data set.
All refers to the full set of pairs, high polysemy refers to
the top 20% of pairs, ranked by sense count. ErCs is the
average number of clusters employed by each method and
background is the average percentage of features allo-
cated by the tiered model to the background cluster. 95%
confidence intervals are computed via bootstrapping.
words from WS-353 are shown in Table 1 and corre-
sponding clusters from the multi-prototype approach
are shown in Table 4. In general the background
component does indeed capture commonalities be-
tween all the sense clusters (e.g. all wizards use
magic) and hence the tiered clusters are more se-
mantically pure. This effect is most visible in the-
matically polysemous words, e.g. radio and wizard.
7.2 Evocation
Compared to WS-353, the WN-Evocation pair set
is sampled more uniformly from English word pairs
and hence contains a significantly larger fraction of
unrelated words, reflecting the fact that word sim-
1178
LIFE
my, you, real, about, your, would
years, spent, rest, lived, last
sentenced, imprisonment, sentence, prison
years, cycle, life, all, expectancy, other
all, life, way, people, human, social, many
RADIO
station, FM, broadcasting, format, AM
radio, station, stations, amateur,
show, station, host, program, radio
stations, song, single, released, airplay
station, operator, radio, equipment, contact
WIZARD
evil, magic, powerful, named, world
Merlin, King, Arthur, powerful, court
spells, magic, cast, wizard, spell, witch
Harry, Dresden, series, Potter, character
STOCK
market, price, stock, company, value, crash
housing, breeding, all, large, stock, many
car, racing, company, cars, summer, NASCAR
stock, extended, folded, card, barrel, cards
rolling, locomotives, new, character, line
Table 4: Example DPMM multi-prototype representation
of words with varying degrees of polysemy. Compared to
the tiered clustering results in Table 1 the multi-prototype
clusters are significantly less pure for thematically poly-
semous words such as radio and wizard.
ilarity is a sparse relation (Figure 2 top). Further-
more, it contains proportionally more highly polyse-
mous words relative to WS-353 (Figure 2 bottom).
On WN-Evocation, the single prototype and
multi-prototype do not differ significantly in terms
of correlation (?0.198 and ?0.201 respectively;
Table 5), while SP+MP yields significantly lower
correlation (?0.176), and the tiered model yields
significantly higher correlation (?0.224). Restrict-
ing to the top 20% of pairs with highest human
similarity judgements yields similar outcomes, with
single prototype, multi-prototype and SP+MP sta-
tistically indistinguishable (?0.239, ?0.227 and
?0.235), and tiered clustering yielding signifi-
cantly higher correlation (?0.277). Likewise tiered
clustering achieves the most significant gains on the
high polysemy subset.
7.3 Selectional Preference
Tiered clustering is a natural model for verb selec-
tional preference, especially for more selectionally
restrictive verbs: the set of words that appear in a
particular argument slot naturally have some kind of
Method ?  100 ErCs background
Single prototype 19.80.6 1.0 -
high similarity 23.91.1 1.0 -
high polysemy 11.51.2 1.0 -
Multi-prototype 20.10.5 14.8 -
high similarity 22.71.2 14.1 -
high polysemy 13.01.3 13.2 -
MP+SP 17.60.5 14.8 -
high similarity 23.51.2 14.1 -
high polysemy 11.41.0 13.2 -
Tiered 22.40.6 29.7 46.6%
high similarity 27.71.3 29.9 47.2%
high polysemy 15.41.1 27.4 46.6%
Table 5: Spearman?s correlation on the Evocation data
set. The high similarity subset contains the top 20% of
pairs sorted by average rater score.
Method ?  100 ErCs background
Single prototype 25.80.8 1.0 -
high polysemy 17.31.7 1.0 -
Multi-prototype 20.21.0 18.5 -
high polysemy 14.12.4 17.4 -
MP+SP 19.71.0 18.5 -
high polysemy 10.52.5 17.4 -
Tiered 29.41.0 37.9 41.7%
high polysemy 28.52.4 37.4 43.2%
Table 6: Spearman?s correlation on the Pado? data set.
commonality (i.e. they can be eaten or can promise).
The background component of the tiered clustering
model can capture such general argument structure.
We model each verb argument slot in the Pado? set
with a separate tiered clustering model, separating
terms co-occurring with the target verb according to
which slot they fill.
On the Pado? set, the performance of the DPMM
multi-prototype approach breaks down and it yields
significantly lower correlation with human norms
than the single prototype (?0.202 vs. ?0.258;
Table 6), due to its inability to capture the shared
structure among verb arguments. Furthermore com-
bining with the single prototype does not signif-
icantly change its performance (?0.197). Mov-
ing to the tiered model, however, yields significant
improvements in correlation over the other models
(?0.294), primarily improving correlation in the
case of highly polysemous verbs and arguments.
1179
8 Discussion and Future Work
We have demonstrated a novel model for dis-
tributional lexical semantics capable of capturing
both shared (context-independent) and idiosyncratic
(context-dependent) structure in a set of word occur-
rences. The benefits of this tiered model were most
pronounced on a selectional preference task, where
there is significant shared structure imposed by con-
ditioning on the verb. Although our results on the
Pado? are not state of the art,6 we believe this to be
due to the impoverished vector-space design; tiered
clustering can be applied to more expressive vec-
tor spaces, such as those incorporating dependency
parse and FrameNet features.
One potential explanation for the superior perfor-
mance of the tiered model vs. the DPMM multi-
prototype model is simply that it allocates more
clusters to represent each word (Reisinger and
Mooney, 2010). However, we find that decreas-
ing the hyperparameter ? (decreasing vocabulary
smoothing and hence increasing the effective num-
ber of clusters) beyond ?  0.1 actually hurts multi-
prototype performance. The additional clusters do
not provide more semantic content due to significant
background similarity.
Finally, the DPMM multi-prototype and tiered
clustering models allocate clusters based on the vari-
ance of the underlying data set. We observe a neg-
ative correlation (?0.33) between the number of
clusters allocated by the DPMM and the number of
word senses found in WordNet. This result is most
likely due to our use of unigram context window
features, which induce clustering based on thematic
rather than syntactic differences. Investigating this
issue is future work.
(Future Work) The word similarity experiments
can be expanded by breaking pairs down further into
highly homonymous and highly polysemous pairs,
using e.g. WordNet to determine how closely related
the senses are. With this data it would be interest-
ing to validate the hypothesis that the percentage of
features allocated to the background cluster is corre-
lated with the degree of homonymy.
The basic tiered clustering can be extended with
additional background tiers, allocating more expres-
sivity to model background feature variation. This
class of models covers the spectrum between a pure
6E.g., Pado? et al (2007) report ?0.515 on the same data.
topic model (all background tiers) and a pure clus-
tering model and may be reasonable when there is
believed to be more background structure (e.g. when
jointly modeling all verb arguments). Furthermore,
it is straightforward to extend the model to a two-
tier, two-clustering structure capable of additionally
accounting for commonalities between arguments.
Applying more principled feature selection ap-
proaches to vector-space lexical semantics may
yield more significant performance gains. Towards
this end we are currently evaluating two classes of
approaches for setting pruning parameters per-word
instead of globally: (1) subspace clustering, i.e.
unsupervised feature selection (e.g., Parsons et al,
2004) and (2) multiple clustering, i.e. finding fea-
ture partitions that lead to disparate clusterings (e.g.,
Shafto et al, 2006).
9 Conclusions
This paper introduced a simple probabilistic model
of tiered clustering inspired by feature selective
clustering that leverages feature exchangeability to
allocate data features between a clustering model
and shared component. The ability to model back-
ground variation, or shared structure, is shown to be
beneficial for modeling words with high polysemy,
yielding increased correlation with human similarity
judgements modeling word relatedness and selec-
tional preference. Furthermore, the tiered clustering
model is shown to significantly outperform related
models, yielding qualitatively more precise clusters.
Acknowledgments
Thanks to Yinon Bentor and Bryan Silverthorn for
many illuminating discussions. This work was sup-
ported by an NSF Graduate Research Fellowship to
the first author, and a Google Research Award.
A Collapsed Gibbs Sampler
In order to sample efficiently from this model, we
leverage the Chinese Restaurant Process represen-
tation of the DP (cf., Aldous, 1985), introducing a
per-word-occurrence cluster indicator cd. Word oc-
currence features are then drawn from a combination
of a single cluster component indicated by cd and the
background topic.
By exploiting conjugacy, the latent variables ?, ?
and ?d can be integrated out, yielding an efficient
1180
collapsed Gibbs sampler. The likelihood of word
occurrence d is given by
P pwd|z, cd,?q 
?
i
P pwi,d|?cdq
?pzd,i0qP pwi,d|?noiseq
?pzd,i1q.
Hence, this model can be viewed as a two-topic
variant of LDA with the addition of a per-word-
occurrence (i.e. document) cluster indicator.7 The
update rule for the latent tier indicator z is similar
to the update rule for 2-topic LDA, with the back-
ground component as the first topic and the second
topic being determined by the per-word-occurrence
cluster indicator c.
We can efficiently approximate ppz|wq via Gibbs
sampling, which requires the complete conditional
posteriors for all zi,d. These are
P pzi,d  t|z
pi,dq,w, ?, ?q 
n
pwi,dq
t   ?
?
wpn
pwq
t   ?q
npdqt   ?
?
jpn
pdq
j   ?q
.
where z
pi,dq is shorthand for the set ztzi,du, n
pwq
t
is the number of occurrences of wordw in topic t not
counting wi,d and n
pdq
t is the number of features in
occurrence d assigned to topic t, not counting wi,d.
Likewise sampling the cluster indicators condi-
tioned on the data ppcd|w, cd, ?, ?q decomposes
into the DP posterior over cluster assignments
and the cluster-conditional Multinomial-Dirichlet
word-occurrence likelihood ppcd|w, cd, ?, ?q 
ppcd|cd, ?qppwd|wd, c, z, ?q given by
P pcd  kold|cd, ?, ?q9

mpdqk
mpdq

  ?

looooooomooooooon
ppcd|cd,?q
Cp? ??n pdqk  
??n pdq

qq
Cp? ??n pdqk q
loooooooooooooomoooooooooooooon
ppwd|wd,c,z,?q
P pcd  knew|cd, ?, ?q9
?
mpdq

  ?
Cp? ??n pdq

q
Cp?q
where mpdqk is the number of occurrences as-
signed to k not including d, ??n pdqk is the vector of
counts of words from occurrence wd assigned to
7Effectively, the tiered clustering model is a special case of
the nested Chinese Restaurant Process with the tree depth fixed
to two (Blei et al, 2003).
cluster k (i.e. words with zi,d  0) and Cpq is
the normalizing constant for the Dirichlet Cpaq 
?p
?m
j1 ajq
1?m
j1 ?pajq operating over vectors
of counts a.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and Wordnet-based approaches. In Proc.
of NAACL-HLT-09, pages 19?27.
David J. Aldous. 1985. Exchangeability and related
topics. In E?cole d?e?te? de probabilite?s de Saint-
Flour, XIII?1983, volume 1117, pages 1?198.
Springer, Berlin.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2003. Hierarchical topic
models and the nested Chinese restaurant process.
In Proc. NIPS-2003.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
James Richard Curran. 2004. From Distributional
to Semantic Similarity. Ph.D. thesis, University
of Edinburgh. College of Science.
Katrin Erk and Sebastian Pado. 2008. A structured
vector space model for word meaning in context.
In Proceedings of EMNLP 2008.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database and Some of its Ap-
plications. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW 2001.
Evgeniy Gabrilovich and Shaul Markovitch.
2007. Computing semantic relatedness using
Wikipedia-based explicit semantic analysis. In
Proc. of IJCAI-07, pages 1606?1611.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Lin-
guistics, 28(3):245?288.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Proc.
of ACL 2006.
1181
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:2007.
Amac? Herdag?delen and Marco Baroni. 2009. Bag-
pack: A general framework to represent semantic
relations. In Proc. of GEMS 2009.
Donald Hindle and Mats Rooth. 1991. Structural
ambiguity and lexical relations. In Proc. of ACL
1991.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction and repre-
sentation of knowledge. Psychological Review,
104(2):211?240.
Martin H. C. Law, Anil K. Jain, and Ma?rio A. T.
Figueiredo. 2002. Feature selection in mixture-
based clustering. In Proc. of NIPS 2002.
Will Lowe. 2001. Towards a theory of semantic
space. In Proceedings of the 23rd Annual Meeting
of the Cognitive Science Society, pages 576?581.
Xiaojuan Ma, Jordan Boyd-Graber, Sonya S.
Nikolova, and Perry Cook. 2009. Speaking
through pictures: Images vs. icons. In ACM Con-
ference on Computers and Accessibility.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Com-
putational Linguistics, 29(4):639?654.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1?28.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161?199.
Sebastian Pado?, Ulrike Pado?, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plau-
sibility judgements. In Proc. of EMNLP 2007.
Ulrike Pado?. 2007. The Integration of Syntax and Se-
mantic Plausibility in a Wide-Coverage Model of
Sentence Processing. Ph.D. thesis, Saarland Uni-
versity, Saarbru?cken.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski,
and Eduard Hovy. 2007. ISP: Learning inferen-
tial selectional preferences. In In Proceedings of
NAACL 2007.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, Edmonton, Alta., Canada.
Lance Parsons, Ehtesham Haque, and Huan Liu.
2004. Subspace clustering for high dimensional
data: A review. SIGKDD Explor. Newsl., 6(1).
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proc. of ACL 1993.
Carl E. Rasmussen. 2000. The infinite Gaussian
mixture model. In Advances in Neural Informa-
tion Processing Systems. MIT Press.
Joseph Reisinger and Raymond Mooney. 2010.
Multi-prototype vector-space models of word
meaning. In Proc. of NAACL 2010.
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
SIGLEX Workshop on Tagging Text with Lexical
Semantics, pages 52?57. ACL.
Adam N. Sanborn, Thomas L. Griffiths, and
Daniel J. Navarro. 2006. A more rational model
of categorization. In Proceedings of the 28th An-
nual Conference of the Cognitive Science Society.
Hinrich Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?123.
Patrick Shafto, Charles Kemp, Vikash Mansinghka,
Matthew Gordon, and Joshua B. Tenenbaum.
2006. Learning cross-cutting systems of cate-
gories. In Proc. CogSci 2006.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proc. of ACL 2006.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Benjamin Van Durme and Marius Pas?ca. 2008.
Finding cars, goddesses and enzymes:
Parametrizable acquisition of labeled instances
for open-domain information extraction. In Proc.
of AAAI 2008.
Nianwen Xue, Jinying Chen, and Martha Palmer.
2006. Aligning features with sense distinction di-
mensions. In Proc. of COLING/ACL 2006.
1182
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1405?1415,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Cross-Cutting Models of Lexical Semantics
Joseph Reisinger
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Raymond Mooney
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
mooney@cs.utexas.edu
Abstract
Context-dependent word similarity can be
measured over multiple cross-cutting dimen-
sions. For example, lung and breath are sim-
ilar thematically, while authoritative and su-
perficial occur in similar syntactic contexts,
but share little semantic similarity. Both of
these notions of similarity play a role in deter-
mining word meaning, and hence lexical se-
mantic models must take them both into ac-
count. Towards this end, we develop a novel
model, Multi-View Mixture (MVM), that rep-
resents words as multiple overlapping clus-
terings. MVM finds multiple data partitions
based on different subsets of features, sub-
ject to the marginal constraint that feature sub-
sets are distributed according to Latent Dirich-
let Allocation. Intuitively, this constraint fa-
vors feature partitions that have coherent top-
ical semantics. Furthermore, MVM uses soft
feature assignment, hence the contribution of
each data point to each clustering view is vari-
able, isolating the impact of data only to views
where they assign the most features. Through
a series of experiments, we demonstrate the
utility of MVM as an inductive bias for captur-
ing relations between words that are intuitive
to humans, outperforming related models such
as Latent Dirichlet Allocation.
1 Introduction
Humans categorize objects using multiple orthogo-
nal taxonomic systems, where category generaliza-
tion depends critically on what features are relevant
to one particular system. For example, foods can be
organized in terms of their nutritional value (high in
fiber) or situationally (commonly eaten for Thanks-
giving; Shafto et al (2006)). Human knowledge-
bases such as Wikipedia also exhibit such multiple
clustering structure (e.g. people are organized by oc-
cupation or by nationality). The effects of these
overlapping categorization systems manifest them-
selves at the lexical semantic level (Murphy, 2002),
implying that lexicographical word senses and tra-
ditional computational models of word-sense based
on clustering or exemplar activation are too impov-
erished to capture the rich dynamics of word usage.
In this work, we introduce a novel probabilis-
tic clustering method, Multi-View Mixture (MVM),
based on cross-cutting categorization (Shafto et al,
2006) that generalizes traditional vector-space or
distributional models of lexical semantics (Curran,
2004; Pado? and Lapata, 2007; Schu?tze, 1998; Tur-
ney, 2006). Cross-cutting categorization finds multi-
ple feature subsets (categorization systems) that pro-
duce high quality clusterings of the data. For exam-
ple words might be clustered based on their part of
speech, or based on their thematic usage. Context-
dependent variation in word usage can be accounted
for by leveraging multiple latent categorization sys-
tems. In particular, cross-cutting models can be used
to capture both syntagmatic and paradigmatic no-
tions of word relatedness, breaking up word features
into multiple categorization systems and then com-
puting similarity separately for each system.
MVM leverages primitives from Dirichlet-Process
Mixture Models (DPMMs) and Latent Dirichlet Al-
location (LDA). Each clustering (view) in MVM con-
sists of a distribution over features and data and
views are further subdivided into clusters based on a
DPMM. View marginal distributions are determined
by LDA, allowing data features to be distributed over
multiple views, explaining subsets of features.
1405
We evaluate MVM against several other model-
based clustering procedures in a series of human
evaluation tasks, measuring its ability to find mean-
ingful syntagmatic and paradigmatic structure. We
find that MVM finds more semantically and syntac-
tically coherent fine-grained structure, using both
common and rare n-gram contexts.
2 Mixture Modeling and Lexical
Semantics
Distributional, or vector space methods attempt to
model word meaning by embedding words in a com-
mon metric space, whose dimensions are derived
from, e.g., word collocations (Schu?tze, 1998), syn-
tactic relations (Pado? and Lapata, 2007), or latent
semantic spaces (Finkelstein et al, 2001; Landauer
and Dumais, 1997; Turian et al, 2010). The distribu-
tional hypothesis addresses the problem of modeling
word similarity (Curran, 2004; Miller and Charles,
1991; Schu?tze, 1998; Turney, 2006), and can be ex-
tended to selectional preference (Resnik, 1997) and
lexical substitution (McCarthy and Navigli, 2007) as
well. Such methods are highly scalable (Gorman
and Curran, 2006) and have been applied in infor-
mation retrieval (Manning et al, 2008), large-scale
taxonomy induction (Snow et al, 2006), and knowl-
edge acquisition (Van Durme and Pas?ca, 2008).
Vector space models fail to capture the richness
of word meaning since similarity is not a globally
consistent metric. It violates, e.g., the triangle in-
equality: the sum of distances from bat to club and
club to association is less than the distance from bat
to association (Griffiths et al, 2007; Tversky and
Gati, 1982).1 Erk (2007) circumvents this problem
by representing words as multiple exemplars derived
directly from word occurrences and embedded in a
common vector space to capture context-dependent
usage. Likewise Reisinger and Mooney (2010) take
a similar approach using mixture modeling com-
bined with a background variation model to generate
multiple prototype vectors for polysemous words.
Both of these approaches still ultimately embed
all words in a single metric space and hence argue
for globally consistent metrics that capture human
1Similarity also has been shown to violate symmetry (e.g.
people have the intuition that China is more similar to North
Korea than North Korea is to China).
intuitive notions of ?similarity.? Rather than assum-
ing a global metric embedding exists, in this work
we simply leverage the cluster assumption, e.g. that
similar words should appear in the same clusters, in
particular extending it to multiple clusterings. The
cluster assumption is a natural fit for lexical seman-
tics, as partitions can account for metric violations.
The end result is a model capable of representing
multiple, overlapping similarity metrics that result
in disparate valid clusterings leveraging the
Subspace Hypothesis: For any pair of
words, the set of ?active? features govern-
ing their apparent similarity differs. For
example wine and bottle are similar and
wine and vinegar are similar, but it would
not be reasonable to expect that the fea-
tures governing such similarity computa-
tions to overlap much, despite occurring
in similar documents.
MVM can extract multiple competing notions of sim-
ilarity, for example both paradigmatic, or thematic
similarity, and syntagmatic or syntactic similarity, in
addition to more fine grained relations.
3 Multi-View Clustering with MVM
As feature dimensionality increases, the number of
ways the data can exhibit interesting structure goes
up exponentially. Clustering is commonly used to
explain data, but often there are several equally
valid, competing clusterings, keying off of different
subsets of features, especially in high-dimensional
settings such as text mining (Niu et al, 2010). For
example, company websites can be clustered by sec-
tor or by geographic location, with one particular
clustering becoming predominant when a majority
of features correlate with it. In fact, informative fea-
tures in one clustering may be noise in another, e.g.
the occurrence of CEO is not necessarily discrimi-
native when clustering companies by industry sec-
tor, but may be useful in other clusterings. Multi-
ple clustering is one approach to inferring feature
subspaces that lead to high quality data partitions.
Multiple clustering also improves the flexibility of
generative clustering models, as a single model is
no longer required to explain all the variance in the
feature dimensions (Mansinghka et al, 2009).
1406
exceedingly
sincerely
logically
justly
appropriately
unwilling
willing
reluctant
refusing
glad
about
because
and are ___
which was ___
who are ___
and is ___
we are ___
he is ___
toyota
nissan
mercedes
volvo
audi
samsung
panasonic
toshiba
sony
epson
dunlop
yokohama
toyo
uniroyal
michelin
results for ___
the latest ___
to buy ___
brand new ___
selection of ___
___ for sale
Figure 1: Example clusterings from MVM applied to
Google n-gram data. Top contexts (features) for each
view are shown, along with examples of word clusters.
Although these particular examples are interpretable, in
general the relationship captured by the view?s context
subspace is not easily summarized.
MVM is a multinomial-Dirichlet multiple clus-
tering procedure for distributional lexical seman-
tics that fits multiple, overlapping Dirichlet Process
Mixture Models (DPMM) to a set of word data. Fea-
tures are distributed across the set of clusterings
(views) using LDA, and each DPMM is fit using a
subset of the features. This reduces clustering noise
and allows MVM to capture multiple ways in which
the data can be partitioned. Figure 1 shows a sim-
ple example, and Figure 2 shows a larger sample of
feature-view assignments from a 3-view MVM fit to
contexts drawn from the Google n-gram corpus.
We implement MVM using generative model
primitives drawn from Latent Dirichlet Allocation
(LDA) and the Dirichlet Process (DP). |M | disparate
clusterings (views) are inferred jointly from a set of
data D  twd|d P r1 . . . Dsu. Each data vector
wd is associated with a probability distribution over
views ?|M |d . Empirically, ?|M |d is represented as aset of feature-view assignments zd, sampled via the
standard LDA collapsed Gibbs sampler. Hence, each
view maintains a separate distribution over features.
The generative model for feature-view assignment is
given by
?|M |d |?  Dirichletp?q, d P D,
?m|?  Dirichletp?q, m P |M |,
zdn|?d  Discretep?dq, n P |wd|,
wdn|?zdnm  Discretep?zdnmq, n P |wd|,
where ? and ? are hyperparameters smoothing the
per-document topic distributions and per-topic word
distributions respectively.
Conditional on the feature-view assignment tzu,
a clustering is inferred for each view using the Chi-
nese Restaurant Process representation of the DP.
The clustering probability is given by
ppc|z,wq 9 pptcmu, z,wq

M
?
m1
|D|
?
d1
ppwrzmsd |cm, zqppcm|zq.
where ppcm|zq is a prior on the clustering for view
m, i.e. the DPMM, and ppwrzmsd |cm, zq is the like-lihood of the clustering cm given the data point wd
restricted to the features assigned to view m:
wrzmsd
def
 twid|zid  mu.
Thus, we treat them clusterings cm as conditionally
independent given the feature-view assignments.
The feature-view assignments tzu act as a set of
marginal constraints on the multiple clusterings, and
the impact that each data point can have on each
clustering is limited by the number of features as-
signed to it. For example, in a two-view model,
zid  1 might be set for all syntactic features (yield-
ing a syntagmatic clustering) while zid  2 is set for
document features (paradigmatic clustering).
By allowing the clustering model capacity to vary
via the DPMM, MVM can naturally account for the
semantic variance of the view. This provides a novel
mechanism for handling feature noise: noisy fea-
tures can be assigned to a separate view with poten-
tially a small number of clusters. This phenomenon
is apparent in cluster 1, view 1 in the example in
figure 2, where place names and adjectives are clus-
tered together using rare contexts
From a topic modeling perspective, MVM finds
topic refinements within each view, similar to hier-
archical methods such as the nested Chinese Restau-
rant Process (Blei et al, 2003). The main differ-
ence is that the features assigned to the second ?re-
fined topics? level are constrained by the higher
1407
word
context
___ hom
e page
___ open this result in
___ who had
a kind of ___
along the ___
and ___ their
are ___ to
be ___ to
but the ___ of
he is ___
in these ___
is an ___
m
any ___ and
m
ight be ___
of ___ have
of being ___
posts by ___
that ___ are
that was ___
the ___ fam
ily
the ___ m
ust be
the ___ of that
the am
erican ___
the very ___
were not ___
who are ___
___ som
e of
a m
ore ___
also ___ the
and ___ his
and are ___
and is ___
and was ___
as ___ as
be ___ or
been ___ and
could be ___
his ___ of
i was ___
is also ___
near the ___
of a ___ and
of the ___ were
she was ___
so m
any ___
the m
ore ___
to be ___ and
was ___ to
we are ___
were ___ in
which ___ the
which was ___
who is ___
you are ___
do not ___
___ high school
___ said that
___ was born
an ___ and
born in ___
by ___ on
by ___ to
create a ___
degree of ___
dsl ___ dsl
from
 the ___ to
going to ___
hotels in ___
in ___ the
in an ___
like ___ and
located in ___
m
essage to ___
nam
e of ___
posted by ___ at
presence of ___
private m
essage to ___
the ___ does not
the city of ___
to ___ a
town of ___
was the ___ of
welcom
e to ___
city of ___
estate in ___
hotels ___ hotels
of ___ m
ay
real estate in ___
way of ___
written by ___
and an ___
of ___ from
 the
the little ___
___ of hum
an
first ___ of
side of the ___
to an ___
0?0
arbitrary
austin
baltimore
characteristic
comparative
dallas
evolutionary
franklin
fundamental
inadequate
inferior
integral
jackson
kent
likelihood
liverpool
mystical
newcastle
pittsburgh
poetic
proportional
psychological
radical
richmond
singular
0?10
betrayed
conquered
disappointed
divorced
embarked
frustrated
guarded
hated
knocked
murdered
praised
stationed
stole
summoned
wounded
0?77secretly
1?0
arbitrary
betrayed
characteristic
conquered
disappointed
divorced
embarked
evolutionary
examine
franklin
frustrated
fundamental
guarded
hated
inadequate
inferior
integral
jackson
knocked
likelihood
murdered
mystical
poetic
praised
proportional
radical
secretly
singular
stationed
stole
summoned
systematic
wounded
1?34
kent
liverpool
manchester
newcastle
1?94
austin
baltimore
charlotte
dallas
pittsburgh
richmond
2?0
austin
betrayed
charlotte
conquered
disappointed
divorced
embarked
frustrated
guarded
hated
jackson
kent
knocked
murdered
newcastle
praised
richmond
secretly
stationed
stole
summoned
wounded
2?47
arbitrary
characteristic
comparative
evolutionary
fundamental
inadequate
inferior
integral
mystical
poetic
psychological
radical
singular
systematic
V
i
e
w
 
1
C
l
u
s
t
e
r
 
1
C
l
u
s
t
e
r
 
2
V
i
e
w
 
2
C
l
u
s
t
e
r
 
1
V
i
e
w
 
3
C
l
u
s
t
e
r
 
1
C
l
u
s
t
e
r
 
2
Figure 2: Topics with Senses: Shows top 20% of features for each view in a 3-view MVM fit to Google n-gram context
data; different views place different mass on different sets of features. Cluster groupings within each view are shown.
View 1 cluster 2 and View 3 cluster 1 both contain past-tense verbs, but only overlap on a subset of syntactic features.1408
level, similar to hierarchical clustering. Unlike hi-
erarchical clustering, however, the top level top-
ics/views form an admixture, allowing individual
features from a single data point to be assigned to
multiple views.
The most similar model to ours is Cross-cutting
categorization (CCC), which fits multiple DPMMs to
non-overlapping partitions of features (Mansinghka
et al, 2009; Shafto et al, 2006). Unlike MVM,
CCC partitions features among multiple DPMMs,
hence all occurrences of a particular feature will
end up in a single clustering, instead of assigning
them softly using LDA. Such hard feature partition-
ing does not admit an efficient sampling procedure,
and hence Shafto et al (2006) rely on Metropolis-
Hastings steps to perform feature assignment, mak-
ing the model less scalable.
3.1 Word Representation
MVM is trained as a lexical semantic model on
Web-scale n-gram and semantic context data. N-
gram contexts are drawn from a combination of the
Google n-gram and Google books n-gram corpora,
with the head word removed: e.g. for the term ar-
chitect, we collect contexts such as the of the
house, an is a, and the of the universe. Se-
mantic contexts are derived from word occurrence
in Wikipedia documents: each document a word ap-
pears in is added as a potential feature for that word.
This co-occurrence matrix is the transpose of the
standard bag-of-words document representation.
In this paper we focus on two representations:
1. Syntax-only ? Words are represented as bags
of ngram contexts derived slot-filling procedure
described above.
2. Syntax+Documents ? The syntax-only repre-
sentation is augmented with additional docu-
ment contexts drawn from Wikipedia.
Models trained on the syntax-only set are only ca-
pable of capturing syntagmatic similarity relations,
that is, words that tend to appear in similar contexts.
In contrast, the syntax+documents set broadens the
scope of modelable similarity relations, allowing for
paradigmatic similarity (e.g. words that are topically
related, but do not necessarily share common syntac-
tic contexts).
Given such word representation data, MVM gener-
ates a fixed set of M context views corresponding to
dominant eigenvectors in local syntactic or seman-
tic space. Within each view, MVM partitions words
into clusters based on each word?s local representa-
tion in that view; that is, based on the set of con-
text features it allocates to the view. Words have a
non-uniform affinity for each view, and hence may
not be present in every clustering (Figure 2). This
is important as different ways of drawing distinc-
tions between words do not necessarily apply to all
words. In contrast, LDA finds locally consistent col-
lections of contexts but does not further subdivide
words into clusters given that set of contexts. Hence,
it may miss more fine-grained structure, even with
increased model complexity.
4 Experimental Setup
4.1 Corpora
We derive word features from three corpora: (1) the
English Google Web n-gram corpus, containing n-
gram contexts up to 5-gram that occur more than 40
times in a 1T word corpus of Web text, (2) the En-
glish Google Books n-gram corpus2, consisting of
n-gram contexts up to 5-gram that occur more than
40 times in a 500B word corpus of books, and (3) a
snapshot of the English Wikipedia3 taken on Octo-
ber 11, 2010 containing over 3M articles.
MVM is trained on a sample of 20k English words
drawn uniformly at random from the top 200k En-
glish terms appearing in Wikipedia (different parts
of speech were sampled from the Google n-gram
corpus according to their observed frequency). Two
versions of the syntax-only dataset are created from
different subsets of the Google n-gram corpora: (1)
the common subset contains all syntactic contexts
appearing more than 200 times in the combined cor-
pus, and (2) the rare subset, containing only contexts
that appear 50 times or fewer.
4.2 Human Evaluation
Our main goal in this work is to find models that
capture aspects of the syntactic and semantic orga-
nization of word in text that are intuitive to humans.
2http://ngrams.googlelabs.com/datasets
3http://wikipedia.org
1409
Context Intrusion
is characterized top of the country to
symptoms of of understood or less
cases of along the a year
in cases of portion of the per day
real estate in side of the or more
Word Intrusion
metal dues humor
floral premiums ingenuity
nylon pensions advertisers
what did delight
ruby damages astonishment
Document Intrusion
Puerto Rican cuisine Adolf Hitler History of the Han Dynasty
Greek cuisine List of General Hospital characters Romance of the Three Kingdoms
ThinkPad History of France List of dog diseases
Palestinian cuisine Joachim von Ribbentrop Conquest of Wu by Jin
Field ration World War I Mongolia
Table 1: Example questions from the three intrusion tasks, in order of difficulty (left to right, easy to hard; computed
from inter-annotator agreement). Italics show intruder items.
According to the use theory of meaning, lexical se-
mantic knowledge is equivalent to knowing the con-
texts that words appear in, and hence being able to
form reasonable hypotheses about the relatedness of
syntactic contexts.
Vector space models are commonly evaluated by
comparing their similarity predictions to a nom-
inal set of human similarity judgments (Curran,
2004; Pado? and Lapata, 2007; Schu?tze, 1998; Tur-
ney, 2006). In this work, since we are evaluating
models that potentially yield many different simi-
larity scores, we take a different approach, scoring
clusters on their semantic and syntactic coherence
using a set intrusion task (Chang et al, 2009).
In set intrusion, human raters are shown a set of
options from a coherent group and asked to identify
a single intruder drawn from a different group. We
extend intrusion to three different lexical semantic
tasks: (1) context intrusion, where the top contexts
from each cluster are used, (3) document intrusion,
where the top document contexts from each clus-
ter are used, and (2) word intrusion, where the top
words from each cluster are used. For each clus-
ter, the top four contexts/words are selected and ap-
pended with another context/word from a different
cluster.4 The resulting set is then shuffled, and the
human raters are asked to identify the intruder, af-
4Choosing four elements from the cluster uniformly at ran-
dom instead of the top by probability led to lower performance
across all models.
ter being given a short introduction (with common
examples) to the task. Table 1 shows sample ques-
tions of varying degrees of difficulty. As the seman-
tic coherence and distinctness from other clusters in-
creases, this task becomes easier.
Set intrusion is a more robust way to account for
human similarity judgments than asking directly for
a numeric score (e.g., the Miller and Charles (1991)
set) as less calibration is required across raters. Fur-
thermore, the additional cluster context significantly
reduces the variability of responses.
Human raters were recruited from Amazon?s Me-
chanical Turk. A total of 1256 raters completed
30438 evaluations for 5780 unique intrusion tasks
(5 evaluations per task). 2736 potentially fraudulent
evaluations from 11 raters were rejected.5 Table 3
summarizes inter-annotator agreement. Overall we
found ?  0.4 for most tasks; a set of comments
about the task difficulty is given in Table 2, drawn
from an anonymous public message board.
5 Results
We trained DPMM, LDA and MVM models
on the syntax-only and syntax+documents
data across a wide range of settings for M P
t3, 5, 7, 10, 20, 30, 50, 100, 200, 300, 500, 1000u,6
5(Rater Quality) Fraudulent Turkers were identified using
a combination of average answer time, answer entropy, average
agreement with other raters, and adjusted answer accuracy.
6LDA is run on a different range of M settings from MVM
(50-1000 vs 3-100) in order to keep the effective number of
1410
% correct
MVM?100M?0.1?0.01
MVM?50M?0.1?0.01
MVM?30M?0.1?0.01
MVM?20M?0.1?0.01
MVM?10M?0.1?0.005
MVM?10M?0.1?0.01
MVM?5M?0.1?0.005
MVM?5M?0.1?0.01
MVM?3M?0.1?0.01
  
LDA?1000M?0.1?0.01
LDA?1000M?0.1?0.1
LDA?500M?0.1?0.01
LDA?500M?0.1?0.1
LDA?300M?0.1?0.01
LDA?300M?0.1?0.1
LDA?200M?0.1?0.01
LDA?200M?0.1?0.1
LDA?100M?0.1?0.01
LDA?100M?0.1?0.1
LDA?50M?0.1?0.01
LDA?50M?0.1?0.1
 
DPMM?0.1?0.01
DPMM?0.1?0.1
context intrusion
ll
0.0 0.2 0.4 0.6 0.8 1.0
word intrusion
ll ll
l l
ll l
l l ll
0.0 0.2 0.4 0.6 0.8 1.0
(a) Syntax-only, common n-gram contexts.
% correct
MVM?100M?0.1?0.01
MVM?50M?0.1?0.01
MVM?30M?0.1?0.01
MVM?20M?0.1?0.01
MVM?10M?0.1?0.005
MVM?10M?0.1?0.01
MVM?5M?0.1?0.005
MVM?5M?0.1?0.01
MVM?3M?0.1?0.01
  
LDA?1000M?0.1?0.01
LDA?1000M?0.1?0.1
LDA?500M?0.1?0.01
LDA?500M?0.1?0.1
LDA?300M?0.1?0.01
LDA?300M?0.1?0.1
LDA?200M?0.1?0.01
LDA?200M?0.1?0.1
LDA?100M?0.1?0.01
LDA?100M?0.1?0.1
LDA?50M?0.1?0.01
LDA?50M?0.1?0.1
 
DPMM?0.1?0.01
DPMM?0.1?0.1
context intrusion
lll
lll
0.0 0.2 0.4 0.6 0.8 1.0
word intrusion
l
ll l
ll
ll
0.0 0.2 0.4 0.6 0.8 1.0
(b) Syntax-only, rare n-gram contexts.
% correct
MVM?100M?0.1?0.01
MVM?50M?0.1?0.01
MVM?30M?0.1?0.01
MVM?20M?0.1?0.01
MVM?10M?0.1?0.005
MVM?10M?0.1?0.01
MVM?5M?0.1?0.005
MVM?5M?0.1?0.01
MVM?3M?0.1?0.01
  
LDA?1000M?0.1?0.01
LDA?1000M?0.1?0.1
LDA?500M?0.1?0.01
LDA?500M?0.1?0.1
LDA?300M?0.1?0.01
LDA?300M?0.1?0.1
LDA?200M?0.1?0.01
LDA?200M?0.1?0.1
LDA?100M?0.1?0.01
LDA?100M?0.1?0.1
LDA?50M?0.1?0.01
LDA?50M?0.1?0.1
 
DPMM?0.1?0.01
DPMM?0.1?0.1
context intrusion
l l
l
l
0.0 0.2 0.4 0.6 0.8 1.0
document intrusion
l ll
ll
0.0 0.2 0.4 0.6 0.8 1.0
word intrusion
l
l ll
l l l
l
l ll
l
lll l
0.0 0.2 0.4 0.6 0.8 1.0
(c) Syntax+Documents, common n-gram contexts.
Figure 3: Average scores for each model broken down by parameterization and data source. Error bars depict 95%
confidence intervals. X-axis labels show Model-views-?-?. Dots show average rater scores; bar-charts show standard
quantile ranges and median score. 1411
U1 I just tried 30 of the what doesn?t belong ones.
They took about 30 seconds each due to think-
ing time so not worth it for me.
U2 I don?t understand the fill in the blank ones to
be honest. I just kinda pick one,since I don?t
know what?s expected lol
U3 Your not filling in the blank just ignore the
blank and think about how the words they show
relate to each other and choose the one that
relates least. Some have just words and no
blanks.
U4 These seem very subjective to mw. i hope
there isn?t definite correct answers because
some of them make me go [emoticon of head-
scratching]
U5 I looked and have no idea. I guess I?m a word
idiot because I don?t see the relation between
the words in the preview HIT - too scared to try
any of these.
U6 I didn?t dive in but I did more than I should have
they were just too easy. Most of them I could
tell what did not belong, some were pretty iffy
though.
Table 2: Sample of comments about the task taken verba-
tim from a public Mechanical Turk user message board
(TurkerNation). Overall the raters report the task to be
difficult, but engaging.
? P t0.1, 0.01u, and ? P t0.1, 0.05, 0.01u in
order to understand how they perform relatively
on the intrusion tasks and also how sensitive they
are to various parameter settings.7 Models were
run until convergence, defined as no increase in
log-likelihood on the training set for 100 Gibbs
samples. Average runtimes varied from a few hours
to a few days, depending on the number of clusters
or topics. There is little computational overhead
for MVM compared to LDA or DPMM with a similar
number of clusters.
Overall, MVM significantly outperforms both LDA
and DPMM (measured as % of intruders correctly
identified) as the number of clusters increases.
Coarse-grained lexical semantic distinctions are
easy for humans to make, and hence models with
fewer clusters tend to outperform models with more
clusters. Since high granularity predictions are more
clusters (and hence model capacity) roughly comparable.
7We did not compare directly to Cross-cutting categoriza-
tion, as the Metropolis-Hasting steps required that model were
too prohibitively expensive to scale to the Google n-gram data.
model size (clusters)
% 
co
rre
ct 0.0
0.5
1.0
0.0
0.5
1.0
l
l
l l
ll l
ll
ll
l
l ll ll
ll
l
ll
ll ll l
ll
l
lll
ll l
102 102.5 103
context intrusion
w
ord intrusion
(a) Syntax-only, common n-gram contexts.
model size (clusters)
% 
co
rre
ct 0.0
0.5
1.0
0.0
0.5
1.0
l
lll ll
l l l l
l
l ll l
l
l
l
l
l
l l
l
l ll
lll ll l
l l
l
101.8 102 102.2 102.4 102.6 102.8 103 103.2
context intrusion
w
ord intrusion
(b) Syntax-only, rare n-gram contexts.
Figure 4: Scatterplot of model size vs. avg score for MVM
(dashed, purple) and LDA (dotted, orange).
useful for downstream tasks, we focus on the inter-
play between model complexity and performance.
5.1 Syntax-only Model
For common n-gram context features, MVM perfor-
mance is significantly less variable than LDA on both
the word intrusion and context intrusion tasks, and
furthermore significantly outperforms DPMM (Fig-
ure 3(a)). For context intrusion, DPMM, LDA, and
MVM average 57.4%, 49.5% and 64.5% accuracy
respectively; for word intrusion, DPMM, LDA, and
MVM average 66.7%, 66.1% and 73.6% accuracy
respectively (averaged over all parameter settings).
These models vary significantly in the average num-
ber of clusters used: 373.5 for DPMM, 358.3 for LDA
and 639.8 for MVM, i.e. the MVM model is signifi-
1412
Model Syntax Syntax+Documents Overall
DPMM 0.30 0.40 0.33
LDA 0.33 0.39 0.35
MVM 0.44 0.49 0.46
Overall 0.37 0.43 0.39
Table 3: Fleiss? ? scores for various model and data com-
binations. Results from MVM have higher ? scores than
LDA or DPMM; likewise Syntax+Documents data yields
higher agreement, primarily due to the relative ease of the
document intrusion task.
cantly more granular. Figure 4(a) breaks out model
performance by model complexity, demonstrating
that MVM has a significant edge over LDA as model
complexity increases.
For rare n-gram contexts, we obtain similar re-
sults, with MVM scores being less variable across
model parameterizations and complexity (Figure
3(b)). In general, LDA performance degrades faster
as model complexity increases for rare contexts, due
to the increased data sparsity (Figure 4(b)). For
context intrusion, DPMM, LDA, and MVM average
45.9%, 36.1% and 50.9% accuracy respectively;
for word intrusion, DPMM, LDA, and MVM aver-
age 67.4%, 45.6% and 67.9% accuracy; MVM per-
formance does not differ significantly from DPMM,
but both outperform LDA. Average cluster sizes are
more uniform across model types for rare contexts:
384.0 for DPMM, 358.3 for LDA and 391 for MVM.
Human performance on the context intrusion task
is significantly more variable than on the word-
intrusion task, reflecting the additional complexity.
In all models, there is a high correlation between
rater scores and per-cluster likelihood, indicating
that model confidence reflects noise in the data.
5.2 Syntax+Documents Model
With the syntax+documents training set, MVM sig-
nificantly outperforms LDA across a wide range of
model settings. MVM also outperforms DPMM for
word and document intrusion. For context intru-
sion, DPMM, LDA, and MVM average 68.0%, 51.3%
and 66.9% respectively;8 for word intrusion, DPMM,
LDA, and MVM average 56.3%, 64.0% and 74.9%
respectively; for document intrusion, DPMM, LDA,
8High DPMM accuracy is driven by the low number of clus-
ters: 46.5 for DPMM vs. 358.3 for LDA and 725.6 for MVM.
model size (clusters)
% 
co
rre
ct
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
l
ll
l
l
lllll
l
l
l
l l
lll
l l
l
l lll
lll
ll
l
l
ll ll
lll l
l
ll
l
l ll
ll ll
ll
l ll l
l ll
102 102.5 103 103.5
context intrusion
document intrusion
w
ord intrusion
Figure 5: Scatterplot of model size vs. avg score for
MVM (dashed, purple) and LDA (dotted, orange); Syn-
tax+Documents data.
and MVM average 41.5%, 49.7% and 60.6% re-
spectively. Qualitatively, models trained on syn-
tax+document yield a higher degree of paradig-
matic clusters which have intuitive thematic struc-
ture. Performance on document intrusion is sig-
nificantly lower and more variable, reflecting the
higher degree of world knowledge required. As with
the previous data set, performance of MVM mod-
els trained on syntax+documents data degrades less
slowly as the cluster granularity increases (Figure 5).
One interesting question is to what degree MVM
views partition syntax and document features versus
LDA topics. That is, to what degree do the MVM
views capture purely syntagmatic or purely paradig-
matic variation? We measured view entropy for all
three models, treating syntactic features and docu-
ment features as different class labels. MVM with
M  50 views obtained an entropy score of 0.045,
while LDA with M  50 obtained 0.073, and the
best DPMM model 0.082.9 Thus MVM views may in-
deed capture pure syntactic or thematic clusterings.
9The low entropy scores reflect the higher percentage of syn-
tactic contexts overall.
1413
5.3 Discussion
As cluster granularity increases, we find that MVM
accounts for feature noise better than either LDA
or DPMM, yielding more coherent clusters. (Chang
et al, 2009) note that LDA performance degrades
significantly on a related task as the number of top-
ics increases, reflecting the increasing difficulty for
humans in grasping the connection between terms
in the same topic. This suggests that as topics be-
come more ne-grained in models with larger num-
ber of topics, they are less useful for humans. In
this work, we find that although MVM and LDA per-
form similarity on average, MVM clusters are signif-
icantly more interpretable than LDA clusters as the
granularity increases (Figures 4 and 5). We argue
that models capable of making such fine-grained se-
mantic distinctions are more desirable.
The results presented in the previous two sections
hold both for unbiased cluster selection (e.g. where
clusters are drawn uniformly at random from the
model) and when cluster selection is biased based
on model probability (results shown). Biased selec-
tion potentially gives an advantage to MVM, which
generates many more small clusters than either LDA
or DPMM, helping it account for noise.
6 Future Work
Models based on cross-cutting categorization is
a novel approach to lexical semantics and hence
should be evaluated on standard baseline tasks, e.g.
contextual paraphrase or lexical substitution (Mc-
Carthy and Navigli, 2007). Additional areas for fu-
ture work include:
(Latent Relation Modeling) Clusterings formed
from feature partitions in MVM can be viewed as a
form of implicit relation extraction; that is, instead
of relying on explicit surface patterns in text, rela-
tions between words or concepts are identified in-
directly based on common syntactic patterns. For
example, clusterings that divide cities by geography
or clusterings partition adjectives by their polarity.
(Latent Semantic Language Modeling) Genera-
tive models such as MVM can be used to build bet-
ter priors for class-based language modeling (Brown
et al, 1992). The rare n-gram results demonstrate
that MVM is potentially useful for tail contexts; i.e.
inferring tail probabilities from low counts.
(Explicit Feature Selection) In this work we rely on
smoothing to reduce the noise of over-broad extrac-
tion rather than performing feature selection explic-
itly. All of the models in this paper can be combined
with feature selection methods to remove noisy fea-
tures, and it would be particularly interesting to draw
parallels to models of ?clutter? in vision.
(Hierarchical Cross-Categorization) Human con-
cept organization consists of multiple overlapping
local ontologies, similar to the loose ontological
structure of Wikipedia. Furthermore, each ontologi-
cal system has a different set of salient properties. It
would be interesting to extend MVM to model hier-
archy explicitly, and compare against baselines such
as Brown clustering (Brown et al, 1992), the nested
Chinese Restaurant Process (Blei et al, 2003) and
the hierarchical Pachinko Allocation Model (Mimno
et al, 2007).
7 Conclusion
This paper introduced MVM, a novel approach to
modeling lexical semantic organization using mul-
tiple cross-cutting clusterings capable of captur-
ing multiple lexical similarity relations jointly in
the same model. In addition to robustly handling
homonymy and polysemy, MVM naturally captures
both syntagmatic and paradigmatic notions of word
similarity. MVM performs favorably compared to
other generative lexical semantic models on a set of
human evaluations, over a wide range of model set-
tings and textual data sources.
Acknowledgements
We would like to thank the anonymous reviewers for
their extensive comments. This work was supported
by a Google PhD Fellowship to the first author.
References
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2003. Hierarchical topic
models and the nested Chinese restaurant process.
In Proc. NIPS-2003.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18:467?479.
1414
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models.
In NIPS.
James Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edin-
burgh.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proc. of the ACL.
Association for Computer Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW 2001.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Proc.
of ACL 2006.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:2007.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction and repre-
sentation of knowledge. Psychological Review,
104(2):211?240.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Vikash K. Mansinghka, Eric Jonas, Cap Petschu-
lat, Beau Cronin, Patrick Shafto, and Joshua B.
Tenenbaum. 2009. Cross-categorization: A
method for discovering multiple overlapping clus-
terings. In Proc. of Nonparametric Bayes Work-
shop at NIPS 2009.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical substitu-
tion task. In SemEval ?07: Proceedings of the 4th
International Workshop on Semantic Evaluations.
Association for Computational Linguistics.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1?28.
David Mimno, Wei Li, and Andrew McCallum.
2007. Mixtures of hierarchical topics with
pachinko allocation. In ICML.
Gregory L. Murphy. 2002. The Big Book of Con-
cepts. The MIT Press.
Donglin Niu, Jennifer G. Dy, and Michael I. Jor-
dan. 2010. Multiple non-redundant spectral
clustering views. In Johannes Fu?rnkranz and
Thorsten Joachims, editors, Proceedings of the
27th International Conference on Machine Learn-
ing (ICML-10), pages 831?838.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161?199.
Joseph Reisinger and Raymond J. Mooney. 2010.
A mixture model with sharing for lexical seman-
tics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2010).
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
SIGLEX Workshop on Tagging Text with Lexical
Semantics, pages 52?57. ACL.
Hinrich Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?123.
Patrick Shafto, Charles Kemp, Vikash Mansinghka,
Matthew Gordon, and Joshua B. Tenenbaum.
2006. Learning cross-cutting systems of cate-
gories. In Proc. CogSci 2006.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proc. of ACL 2006.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proc. of
the ACL.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Amos Tversky and Itamar Gati. 1982. Similarity,
separability, and the triangle inequality. Psycho-
logical Review, 89(2):123?154.
Benjamin Van Durme and Marius Pas?ca. 2008.
Finding cars, goddesses and enzymes:
Parametrizable acquisition of labeled instances
for open-domain information extraction. In Proc.
of AAAI 2008.
1415
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 109?117,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Multi-Prototype Vector-Space Models of Word Meaning
Joseph Reisinger
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233
joeraii@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233
mooney@cs.utexas.edu
Abstract
Current vector-space models of lexical seman-
tics create a single ?prototype? vector to rep-
resent the meaning of a word. However, due
to lexical ambiguity, encoding word mean-
ing with a single vector is problematic. This
paper presents a method that uses cluster-
ing to produce multiple ?sense-specific? vec-
tors for each word. This approach provides
a context-dependent vector representation of
word meaning that naturally accommodates
homonymy and polysemy. Experimental com-
parisons to human judgements of semantic
similarity for both isolated words as well as
words in sentential contexts demonstrate the
superiority of this approach over both proto-
type and exemplar based vector-space models.
1 Introduction
Automatically judging the degree of semantic sim-
ilarity between words is an important task useful
in text classification (Baker and McCallum, 1998),
information retrieval (Sanderson, 1994), textual en-
tailment, and other language processing tasks. The
standard empirical approach to this task exploits the
distributional hypothesis, i.e. that similar words ap-
pear in similar contexts (Curran and Moens, 2002;
Lin and Pantel, 2002; Pereira et al, 1993). Tra-
ditionally, word types are represented by a sin-
gle vector of contextual features derived from co-
occurrence information, and semantic similarity is
computed using some measure of vector distance
(Lee, 1999; Lowe, 2001).
However, due to homonymy and polysemy, cap-
turing the semantics of a word with a single vector is
problematic. For example, the word club is similar
to both bat and association, which are not at all simi-
lar to each other. Word meaning violates the triangle
inequality when viewed at the level of word types,
posing a problem for vector-space models (Tver-
sky and Gati, 1982). A single ?prototype? vector
is simply incapable of capturing phenomena such as
homonymy and polysemy. Also, most vector-space
models are context independent, while the meaning
of a word clearly depends on context. The word club
in ?The caveman picked up the club? is similar to bat
in ?John hit the robber with a bat,? but not in ?The
bat flew out of the cave.?
We present a new resource-lean vector-space
model that represents a word?s meaning by a set of
distinct ?sense specific? vectors. The similarity of
two isolated words A and B is defined as the mini-
mum distance between one of A?s vectors and one of
B?s vectors. In addition, a context-dependent mean-
ing for a word is determined by choosing one of the
vectors in its set based on minimizing the distance
to the vector representing the current context. Con-
sequently, the model supports judging the similarity
of both words in isolation and words in context.
The set of vectors for a word is determined by un-
supervised word sense discovery (WSD) (Schu?tze,
1998), which clusters the contexts in which a word
appears. In previous work, vector-space lexical sim-
ilarity and word sense discovery have been treated
as two separate tasks. This paper shows how they
can be combined to create an improved vector-space
model of lexical semantics. First, a word?s contexts
are clustered to produce groups of similar context
vectors. An average ?prototype? vector is then com-
puted separately for each cluster, producing a set of
vectors for each word. Finally, as described above,
these cluster vectors can be used to determine the se-
109
mantic similarity of both isolated words and words
in context. The approach is completely modular, and
can integrate any clustering method with any tradi-
tional vector-space model.
We present experimental comparisons to human
judgements of semantic similarity for both isolated
words and words in sentential context. The results
demonstrate the superiority of a clustered approach
over both traditional prototype and exemplar-based
vector-space models. For example, given the iso-
lated target word singer our method produces the
most similar word vocalist, while using a single pro-
totype gives musician. Given the word cell in the
context: ?The book was published while Piasecki
was still in prison, and a copy was delivered to his
cell.? the standard approach produces protein while
our method yields incarcerated.
The remainder of the paper is organized as fol-
lows: Section 2 gives relevant background on pro-
totype and exemplar methods for lexical semantics,
Section 3 presents our multi-prototype method, Sec-
tion 4 presents our experimental evaluations, Section
5 discusses future work, and Section 6 concludes.
2 Background
Psychological concept models can be roughly di-
vided into two classes:
1. Prototype models represented concepts by an
abstract prototypical instance, similar to a clus-
ter centroid in parametric density estimation.
2. Exemplar models represent concepts by a con-
crete set of observed instances, similar to non-
parametric approaches to density estimation in
statistics (Ashby and Alfonso-Reese, 1995).
Tversky and Gati (1982) famously showed that con-
ceptual similarity violates the triangle inequality,
lending evidence for exemplar-based models in psy-
chology. Exemplar models have been previously
used for lexical semantics problems such as selec-
tional preference (Erk, 2007) and thematic fit (Van-
dekerckhove et al, 2009). Individual exemplars can
be quite noisy and the model can incur high com-
putational overhead at prediction time since naively
computing the similarity between two words using
each occurrence in a textual corpus as an exemplar
requires O(n2) comparisons. Instead, the standard
... chose Zbigniew Brzezinski 
for the position of ...
... thus the symbol s position 
on his clothing was ...
... writes call options against 
the stock position ...
... offered a position with ...
... a position he would hold 
until his retirement in ...
... endanger their position as 
a cultural group...
... on the chart of the vessel s 
current position ...
... not in a position to help...
(cluster#2) 
post
appointme
nt, role, job
(cluster#4) 
lineman, 
tackle, role, 
scorer
(cluster#1) 
location
importance 
bombing
(collect contexts) (cluster)
(cluster#3) 
intensity, 
winds, 
hour, gust
(similarity)
single
prototype
Figure 1: Overview of the multi-prototype approach
to near-synonym discovery for a single target word
independent of context. Occurrences are clustered
and cluster centroids are used as prototype vectors.
Note the ?hurricane? sense of position (cluster 3) is
not typically considered appropriate in WSD.
approach is to compute a single prototype vector for
each word from its occurrences.
This paper presents a multi-prototype vector space
model for lexical semantics with a single parame-
ter K (the number of clusters) that generalizes both
prototype (K = 1) and exemplar (K = N , the total
number of instances) methods. Such models have
been widely studied in the Psychology literature
(Griffiths et al, 2007; Love et al, 2004; Rosseel,
2002). By employing multiple prototypes per word,
vector space models can account for homonymy,
polysemy and thematic variation in word usage.
Furthermore, such approaches require only O(K2)
comparisons for computing similarity, yielding po-
tential computational savings over the exemplar ap-
proach when K  N , while reaping many of the
same benefits.
Previous work on lexical semantic relatedness has
focused on two approaches: (1) mining monolin-
gual or bilingual dictionaries or other pre-existing
resources to construct networks of related words
(Agirre and Edmond, 2006; Ramage et al, 2009),
and (2) using the distributional hypothesis to au-
tomatically infer a vector-space prototype of word
meaning from large corpora (Agirre et al, 2009;
Curran, 2004; Harris, 1954). The former approach
tends to have greater precision, but depends on hand-
110
crafted dictionaries and cannot, in general, model
sense frequency (Budanitsky and Hirst, 2006). The
latter approach is fundamentally more scalable as it
does not rely on specific resources and can model
corpus-specific sense distributions. However, the
distributional approach can suffer from poor preci-
sion, as thematically similar words (e.g., singer and
actor) and antonyms often occur in similar contexts
(Lin et al, 2003).
Unsupervised word-sense discovery has been
studied by number of researchers (Agirre and Ed-
mond, 2006; Schu?tze, 1998). Most work has also
focused on corpus-based distributional approaches,
varying the vector-space representation, e.g. by in-
corporating syntactic and co-occurrence information
from the words surrounding the target term (Pereira
et al, 1993; Pantel and Lin, 2002).
3 Multi-Prototype Vector-Space Models
Our approach is similar to standard vector-space
models of word meaning, with the addition of a per-
word-type clustering step: Occurrences for a spe-
cific word type are collected from the corpus and
clustered using any appropriate method (?3.1). Sim-
ilarity between two word types is then computed as
a function of their cluster centroids (?3.2), instead of
the centroid of all the word?s occurrences. Figure 1
gives an overview of this process.
3.1 Clustering Occurrences
Multiple prototypes for each word w are generated
by clustering feature vectors v(c) derived from each
occurrence c ? C(w) in a large textual corpus and
collecting the resulting cluster centroids pik(w), k ?
[1,K]. This approach is commonly employed in un-
supervised word sense discovery; however, we do
not assume that clusters correspond to traditional
word senses. Rather, we only rely on clusters to cap-
ture meaningful variation in word usage.
Our experiments employ a mixture of von Mises-
Fisher distributions (movMF) clustering method
with first-order unigram contexts (Banerjee et al,
2005). Feature vectors v(c) are composed of indi-
vidual features I(c, f), taken as all unigrams occur-
ring f ? F in a 10-word window around w.
Like spherical k-means (Dhillon and Modha,
2001), movMF models semantic relatedness using
cosine similarity, a standard measure of textual sim-
ilarity. However, movMF introduces an additional
per-cluster concentration parameter controlling its
semantic breadth, allowing it to more accurately
model non-uniformities in the distribution of cluster
sizes. Based on preliminary experiments comparing
various clustering methods, we found movMF gave
the best results.
3.2 Measuring Semantic Similarity
The similarity between two words in a multi-
prototype model can be computed straightforwardly,
requiring only simple modifications to standard dis-
tributional similarity methods such as those pre-
sented by Curran (2004). Given words w and w?, we
define two noncontextual clustered similarity met-
rics to measure similarity of isolated words:
AvgSim(w,w?)
def
=
1
K2
K?
j=1
K?
k=1
d(pik(w), pij(w
?))
MaxSim(w,w?)
def
= max
1?j?K,1?k?K
d(pik(w), pij(w
?))
where d(?, ?) is a standard distributional similarity
measure. In AvgSim, word similarity is computed
as the average similarity of all pairs of prototype
vectors; In MaxSim the similarity is the maximum
over all pairwise prototype similarities. All results
reported in this paper use cosine similarity, 1
Cos(w,w?) =
?
f?F I(w, f) ? I(w
?, f)
??
f?F I(w, f)
2
??
f?F I(w
?, f)2
We compare across two different feature functions
tf-idf weighting and ?2 weighting, chosen due to
their ubiquity in the literature (Agirre et al, 2009;
Curran, 2004).
In AvgSim, all prototype pairs contribute equally
to the similarity computation, thus two words are
judged as similar if many of their senses are simi-
lar. MaxSim, on the other hand, only requires a sin-
gle pair of prototypes to be close for the words to be
judged similar. Thus, MaxSim models the similarity
of words that share only a single sense (e.g. bat and
club) at the cost of lower robustness to noisy clusters
that might be introduced when K is large.
When contextual information is available,
AvgSim and MaxSim can be modified to produce
1The main results also hold for weighted Jaccard similarity.
111
more precise similarity computations:
AvgSimC(w,w?)
def
=
1
K2
K?
j=1
K?
k=1
dc,w,kdc?,w?,jd(pik(w), pij(w
?))
MaxSimC(w,w?)
def
= d(p?i(w), p?i(w?))
where dc,w,k
def
= d(v(c), pik(w)) is the likelihood of
context c belonging to cluster pik(w), and p?i(w)
def
=
piargmax1?k?K dc,w,k(w), the maximum likelihood
cluster for w in context c. Thus, AvgSimC corre-
sponds to soft cluster assignment, weighting each
similarity term in AvgSim by the likelihood of the
word contexts appearing in their respective clus-
ters. MaxSimC corresponds to hard assignment,
using only the most probable cluster assignment.
Note that AvgSim and MaxSim can be thought of as
special cases of AvgSimC and MaxSimC with uni-
form weight to each cluster; hence AvgSimC and
MaxSimC can be used to compare words in context
to isolated words as well.
4 Experimental Evaluation
4.1 Corpora
We employed two corpora to train our models:
1. A snapshot of English Wikipedia taken on Sept.
29th, 2009. Wikitext markup is removed, as
are articles with fewer than 100 words, leaving
2.8M articles with a total of 2.05B words.
2. The third edition English Gigaword corpus,
with articles containing fewer than 100 words
removed, leaving 6.6M articles and 3.9B words
(Graff, 2003).
Wikipedia covers a wider range of sense distribu-
tions, whereas Gigaword contains only newswire
text and tends to employ fewer senses of most am-
biguous words. Our method outperforms baseline
methods even on Gigaword, indicating its advan-
tages even when the corpus covers few senses.
4.2 Judging Semantic Similarity
To evaluate the quality of various models, we first
compared their lexical similarity measurements to
human similarity judgements from the WordSim-
353 data set (Finkelstein et al, 2001). This test
corpus contains multiple human judgements on 353
word pairs, covering both monosemous and poly-
semous words, each rated on a 1?10 integer scale.
Spearman?s rank correlation (?) with average human
judgements (Agirre et al, 2009) was used to mea-
sure the quality of various models.
Figure 2 plots Spearman?s ? on WordSim-353
against the number of clusters (K) for Wikipedia
and Gigaword corpora, using pruned tf-idf and ?2
features.2 In general pruned tf-idf features yield
higher correlation than ?2 features. Using AvgSim,
the multi-prototype approach (K > 1) yields higher
correlation than the single-prototype approach (K =
1) across all corpora and feature types, achieving
state-of-the-art results with pruned tf-idf features.
This result is statistically significant in all cases for
tf-idf and for K ? [2, 10] on Wikipedia and K > 4
on Gigaword for ?2 features.3 MaxSim yields simi-
lar performance when K < 10 but performance de-
grades as K increases.
It is possible to circumvent the model-selection
problem (choosing the best value of K) by simply
combining the prototypes from clusterings of dif-
ferent sizes. This approach represents words using
both semantically broad and semantically tight pro-
totypes, similar to hierarchical clustering. Table 1
and Figure 2 (squares) show the result of such a com-
bined approach, where the prototypes for clusterings
of size 2-5, 10, 20, 50, and 100 are unioned to form a
single large prototype set. In general, this approach
works about as well as picking the optimal value of
K, even outperforming the single best cluster size
for Wikipedia.
Finally, we also compared our method to a pure
exemplar approach, averaging similarity across all
occurrence pairs.4 Table 1 summarizes the results.
The exemplar approach yields significantly higher
correlation than the single prototype approach in all
cases except Gigaword with tf-idf features (p <
0.05). Furthermore, it performs significantly worse
2(Feature pruning) We find that results using tf-idf features
are extremely sensitive to feature pruning while ?2 features are
more robust. In all experiments we prune tf-idf features by their
overall weight, taking the top 5000. This setting was found to
optimize the performance of the single-prototype approach.
3Significance is calculated using the large-sample approxi-
mation of the Spearman rank test; (p < 0.05).
4Averaging across all pairs was found to yield higher corre-
lation than averaging over the most similar pairs.
112
Spearman?s ? prototype exemplar multi-prototype (AvgSim) combined
K = 5 K = 20 K = 50
Wikipedia tf-idf 0.53?0.02 0.60?0.06 0.69?0.02 0.76?0.01 0.76?0.01 0.77?0.01
Wikipedia ?2 0.54?0.03 0.65?0.07 0.58?0.02 0.56?0.02 0.52?0.03 0.59?0.04
Gigaword tf-idf 0.49?0.02 0.48?0.10 0.64?0.02 0.61?0.02 0.61?0.02 0.62?0.02
Gigaword ?2 0.25?0.03 0.41?0.14 0.32?0.03 0.35?0.03 0.33?0.03 0.34?0.03
Table 1: Spearman correlation on the WordSim-353 dataset broken down by corpus and feature type.
Figure 2: WordSim-353 rank correlation vs. num-
ber of clusters (log scale) for both the Wikipedia
(left) and Gigaword (right) corpora. Horizontal bars
show the performance of single-prototype. Squares
indicate performance when combining across clus-
terings. Error bars depict 95% confidence intervals
using the Spearman test. Squares indicate perfor-
mance when combining across clusterings.
than combined multi-prototype for tf-idf features,
and does not differ significantly for ?2 features.
Overall this result indicates that multi-prototype per-
forms at least as well as exemplar in the worst case,
and significantly outperforms when using the best
feature representation / corpus pair.
4.3 Predicting Near-Synonyms
We next evaluated the multi-prototype approach on
its ability to determine the most closely related
words for a given target word (using the Wikipedia
corpus with tf-idf features). The top k most simi-
lar words were computed for each prototype of each
target word. Using a forced-choice setup, human
subjects were asked to evaluate the quality of these
near synonyms relative to those produced by a sin-
homonymous
carrier, crane, cell, company, issue, interest, match,
media, nature, party, practice, plant, racket, recess,
reservation, rock, space, value
polysemous
cause, chance, journal, market, network, policy,
power, production, series, trading, train
Table 2: Words used in predicting near synonyms.
gle prototype. Participants on Amazon?s Mechani-
cal Turk5 (Snow et al, 2008) were asked to choose
between two possible alternatives (one from a proto-
type model and one from a multi-prototype model)
as being most similar to a given target word. The
target words were presented either in isolation or in
a sentential context randomly selected from the cor-
pus. Table 2 lists the ambiguous words used for this
task. They are grouped into homonyms (words with
very distinct senses) and polysemes (words with re-
lated senses). All words were chosen such that their
usages occur within the same part of speech.
In the non-contextual task, 79 unique raters com-
pleted 7,620 comparisons of which 72 were dis-
carded due to poor performance on a known test set.6
In the contextual task, 127 raters completed 9,930
comparisons of which 87 were discarded.
For the non-contextual case, Figure 3 left plots
the fraction of raters preferring the multi-prototype
prediction (using AvgSim) over that of a single pro-
totype as the number of clusters is varied. When
asked to choose between the single best word for
5http://mturk.com
6(Rater reliability) The reliability of Mechanical Turk
raters is quite variable, so we computed an accuracy score for
each rater by including a control question with a known cor-
rect answer in each HIT. Control questions were generated by
selecting a random word from WordNet 3.0 and including as
possible choices a word in the same synset (correct answer) and
a word in a synset with a high path distance (incorrect answer).
Raters who got less than 50% of these control questions correct,
or spent too little time on the HIT were discarded.
113
Non-contextual Near-Synonym Prediction Contextual Near-Synonym Prediction
Figure 3: (left) Near-synonym evaluation for isolated words showing fraction of raters preferring multi-
prototype results vs. number of clusters. Colored squares indicate performance when combining across
clusterings. 95% confidence intervals computed using the Wald test. (right) Near-synonym evaluation for
words in a sentential context chosen either from the minority sense or the majority sense.
each method (top word), the multi-prototype pre-
diction is chosen significantly more frequently (i.e.
the result is above 0.5) when the number of clus-
ters is small, but the two methods perform sim-
ilarly for larger numbers of clusters (Wald test,
? = 0.05.) Clustering more accurately identi-
fies homonyms? clearly distinct senses and produces
prototypes that better capture the different uses of
these words. As a result, compared to using a sin-
gle prototype, our approach produces better near-
synonyms for homonyms compared to polysemes.
However, given the right number of clusters, it also
produces better results for polysemous words.
The near-synonym prediction task highlights one
of the weaknesses of the multi-prototype approach:
as the number of clusters increases, the number of
occurrences assigned to each cluster decreases, in-
creasing noise and resulting in some poor prototypes
that mainly cover outliers. The word similarity task
is somewhat robust to this phenomenon, but syn-
onym prediction is more affected since only the top
predicted choice is used. When raters are forced
to chose between the top three predictions for each
method (presented as top set in Figure 3 left), the ef-
fect of this noise is reduced and the multi-prototype
approach remains dominant even for a large num-
ber of clusters. This indicates that although more
clusters can capture finer-grained sense distinctions,
they also can introduce noise.
When presented with words in context (Figure
3 right),7 raters found no significant difference in
the two methods for words used in their majority
sense.8 However, when a minority sense is pre-
7Results for the multi-prototype method are generated using
AvgSimC (soft assignment) as this was found to significantly
outperform MaxSimC.
8Sense frequency determined using Google; senses labeled
manually by trained human evaluators.
114
sented (e.g. the ?prison? sense of cell), raters pre-
fer the choice predicted by the multi-prototype ap-
proach. This result is to be expected since the sin-
gle prototype mainly reflects the majority sense, pre-
venting it from predicting appropriate synonyms for
a minority sense. Also, once again, the perfor-
mance of the multi-prototype approach is better for
homonyms than polysemes.
4.4 Predicting Variation in Human Ratings
Variance in pairwise prototype distances can help
explain the variance in human similarity judgements
for a given word pair. We evaluate this hypothe-
sis empirically on WordSim-353 by computing the
Spearman correlation between the variance of the
per-cluster similarity computations, V[D], D def=
{d(pik(w), pij(w?)) : 1 ? k, j ? K}, and the vari-
ance of the human annotations for that pair. Cor-
relations for each dataset are shown in Figure 4 left.
In general, we find a statistically significant negative
correlation between these values using ?2 features,
indicating that as the entropy of the pairwise cluster
similarities increases (i.e., prototypes become more
similar, and similarities become uniform), rater dis-
agreement increases. This result is intuitive: if the
occurrences of a particular word cannot be easily
separated into coherent clusters (perhaps indicating
high polysemy instead of homonymy), then human
judgement will be naturally more difficult.
Rater variance depends more directly on the ac-
tual word similarity: word pairs at the extreme
ranges of similarity have significantly lower vari-
ance as raters are more certain. By removing word
pairs with similarity judgements in the middle two
quartile ranges (4.4 to 7.5) we find significantly
higher variance correlation (Figure 4 right). This
result indicates that multi-prototype similarity vari-
ance accounts for a secondary effect separate from
the primary effect that variance is naturally lower for
ratings in extreme ranges.
Although the entropy of the prototypes correlates
with the variance of the human ratings, we find that
the individual senses captured by each prototype do
not correspond to human intuition for a given word,
e.g. the ?hurricane? sense of position in Figure 1.
This notion is evaluated empirically by computing
the correlation between the predicted similarity us-
Figure 4: Plots of variance correlation; lower num-
bers indicate higher negative correlation, i.e. that
prototype entropy predicts rater disagreement.
ing the contextual multi-prototype method and hu-
man similarity judgements for different usages of
the same word. The Usage Similarity (USim) data
set collected in Erk et al (2009) provides such simi-
larity scores from human raters. However, we find
no evidence for correlation between USim scores
and their corresponding prototype similarity scores
(? = 0.04), indicating that prototype vectors may
not correspond well to human senses.
5 Discussion and Future Work
Table 3 compares the inferred synonyms for several
target words, generally demonstrating the ability of
the multi-prototype model to improve the precision
of inferred near-synonyms (e.g. in the case of singer
or need) as well as its ability to include synonyms
from less frequent senses (e.g., the experiment sense
of research or the verify sense of prove). However,
there are a number of ways it could be improved:
Feature representations: Multiple prototypes im-
prove Spearman correlation on WordSim-353 com-
pared to previous methods using the same under-
lying representation (Agirre et al, 2009). How-
ever we have not yet evaluated its performance
when using more powerful feature representations
such those based on Latent or Explicit Semantic
Analysis (Deerwester et al, 1990; Gabrilovich and
Markovitch, 2007). Due to its modularity, the multi-
prototype approach can easily incorporate such ad-
vances in order to further improve its effectiveness.
115
Inferred Thesaurus
bass
single guitar, drums, rhythm, piano, acoustic
multi basses, contrabass, rhythm, guitar, drums
claim
single argue, say, believe, assert, contend
multi assert, contend, allege, argue, insist
hold
single carry, take, receive, reach, maintain
multi carry, maintain, receive, accept, reach
maintain
single ensure, establish, achieve, improve, promote
multi preserve, ensure, establish, retain, restore
prove
single demonstrate, reveal, ensure, confirm, say
multi demonstrate, verify, confirm, reveal, admit
research
single studies, work, study, training, development
multi studies, experiments, study, investigations,
training
singer
single musician, actress, actor, guitarist, composer
multi vocalist, guitarist, musician, singer-
songwriter, singers
Table 3: Examples of the top 5 inferred near-
synonyms using the single- and multi-prototype ap-
proaches (with results merged). In general such
clustering improves the precision and coverage of
the inferred near-synonyms.
Nonparametric clustering: The success of the
combined approach indicates that the optimal num-
ber of clusters may vary per word. A more prin-
cipled approach to selecting the number of proto-
types per word is to employ a clustering model with
infinite capacity, e.g. the Dirichlet Process Mixture
Model (Rasmussen, 2000). Such a model would al-
low naturally more polysemous words to adopt more
flexible representations.
Cluster similarity metrics: Besides AvgSim and
MaxSim, there are many similarity metrics over
mixture models, e.g. KL-divergence, which may
correlate better with human similarity judgements.
Comparing to traditional senses: Compared to
WordNet, our best-performing clusterings are sig-
nificantly more fine-grained. Furthermore, they of-
ten do not correspond to agreed upon semantic dis-
tinctions (e.g., the ?hurricane? sense of position in
Fig. 1). We posit that the finer-grained senses actu-
ally capture useful aspects of word meaning, leading
to better correlation with WordSim-353. However, it
would be good to compare prototypes learned from
supervised sense inventories to prototypes produced
by automatic clustering.
Joint model: The current method independently
clusters the contexts of each word, so the senses dis-
covered forw cannot influence the senses discovered
for w? 6= w. Sharing statistical strength across simi-
lar words could yield better results for rarer words.
6 Conclusions
We presented a resource-light model for vector-
space word meaning that represents words as col-
lections of prototype vectors, naturally accounting
for lexical ambiguity. The multi-prototype approach
uses word sense discovery to partition a word?s con-
texts and construct ?sense specific? prototypes for
each cluster. Doing so significantly increases the ac-
curacy of lexical-similarity computation as demon-
strated by improved correlation with human similar-
ity judgements and generation of better near syn-
onyms according to human evaluators. Further-
more, we show that, although performance is sen-
sitive to the number of prototypes, combining pro-
totypes across a large range of clusterings performs
nearly as well as the ex-post best clustering. Finally,
variance in the prototype similarities is found to cor-
relate with inter-annotator disagreement, suggesting
psychological plausibility.
Acknowledgements
We would like to thank Katrin Erk for helpful dis-
cussions and making the USim data set available.
This work was supported by an NSF Graduate Re-
search Fellowship and a Google Research Award.
Experiments were run on the Mastodon Cluster, pro-
vided by NSF Grant EIA-0303609.
References
Eneko Agirre and Phillip Edmond. 2006. Word Sense
Disambiguation: Algorithms and Applications (Text,
Speech and Language Technology). Springer-Verlag
New York, Inc., Secaucus, NJ, USA.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proc. of NAACL-
HLT-09, pages 19?27.
116
F. Gregory Ashby and Leola A. Alfonso-Reese. 1995.
Categorization as probability density estimation. J.
Math. Psychol., 39(2):216?233.
L. Douglas Baker and Andrew K. McCallum. 1998. Dis-
tributional clustering of words for text classification.
In Proceedings of 21st International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 96?103.
Arindam Banerjee, Inderjit Dhillon, Joydeep Ghosh, and
Suvrit Sra. 2005. Clustering on the unit hypersphere
using von Mises-Fisher distributions. Journal of Ma-
chine Learning Research, 6:1345?1382.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
James R. Curran and Marc Moens. 2002. Improvements
in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acqui-
sition, pages 59?66.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
College of Science.
Scott C. Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41:391?407.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Machine Learning, 42:143?175.
Katrin Erk, Diana McCarthy, Nicholas Gaylord Investi-
gations on Word Senses, and Word Usages. 2009. In-
vestigations on word senses and word usages. In Proc.
of ACL-09.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics. Association for Computer Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the concept
revisited. In Proc. of WWW-01, pages 406?414, New
York, NY, USA. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proc. of IJCAI-07, pages
1606?1611.
David Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadephia.
Tom L. Griffiths, Kevin. R. Canini, Adam N. Sanborn,
and Daniel. J. Navarro. 2007. Unifying rational mod-
els of categorization via the hierarchical Dirichlet pro-
cess. In Proc. of CogSci-07.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Lillian Lee. 1999. Measures of distributional similarity.
In 37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 25?32.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of COLING-02, pages 1?7.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the Interational Joint
Conference on Artificial Intelligence, pages 1492?
1493. Morgan Kaufmann.
Bradley C. Love, Douglas L. Medin, and Todd M.
Gureckis. 2004. SUSTAIN: A network model of cat-
egory learning. Psych. Review, 111(2):309?332.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the 23rd Annual Meeting of the Cog-
nitive Science Society, pages 576?581.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proc. of SIGKDD-02, pages 613?
619, New York, NY, USA. ACM.
Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-93), pages
183?190, Columbus, Ohio.
Daniel Ramage, Anna N. Rafferty, and Christopher D.
Manning. 2009. Random walks for text seman-
tic similarity. In Proc. of the 2009 Workshop on
Graph-based Methods for Natural Language Process-
ing (TextGraphs-4), pages 23?31.
Carl E. Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Processing
Systems, pages 554?560. MIT Press.
Yves Rosseel. 2002. Mixture models of categorization.
J. Math. Psychol., 46(2):178?210.
Mark Sanderson. 1994. Word sense disambiguation and
information retrieval. In Proc. of SIGIR-94, pages
142?151.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast?but is it good? Eval-
uating non-expert annotations for natural language
tasks. In Proc. of EMNLP-08.
Amos Tversky and Itamar Gati. 1982. Similarity, sepa-
rability, and the triangle inequality. Psychological Re-
view, 89(2):123?154.
Bram Vandekerckhove, Dominiek Sandra, and Walter
Daelemans. 2009. A robust and extensible exemplar-
based model of thematic fit. In Proc. of EACL 2009,
pages 826?834. Association for Computational Lin-
guistics.
117
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1200?1209,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Fine-Grained Class Label Markup of Search Queries
Joseph Reisinger?
Department of Computer Sciences
The University of Texas at Austin
Austin, Texas 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
We develop a novel approach to the seman-
tic analysis of short text segments and demon-
strate its utility on a large corpus of Web
search queries. Extracting meaning from short
text segments is difficult as there is little
semantic redundancy between terms; hence
methods based on shallow semantic analy-
sis may fail to accurately estimate meaning.
Furthermore search queries lack explicit syn-
tax often used to determine intent in ques-
tion answering. In this paper we propose a
hybrid model of semantic analysis combin-
ing explicit class-label extraction with a la-
tent class PCFG. This class-label correlation
(CLC) model admits a robust parallel approxi-
mation, allowing it to scale to large amounts of
query data. We demonstrate its performance
in terms of (1) its predicted label accuracy on
polysemous queries and (2) its ability to accu-
rately chunk queries into base constituents.
1 Introduction
Search queries are generally short and rarely contain
much explicit syntax, making query understanding a
purely semantic endeavor. Furthermore, as in noun-
phrase understanding, shallow lexical semantics is
often irrelevant or misleading; e.g., the query [trop-
ical breeze cleaners] has little to do with island va-
cations, nor are desert birds relevant to [1970 road
runner], which refers to a car model.
This paper introduces class-label correlation
(CLC), a novel unsupervised approach to extract-
?Contributions made during an internship at Google.
ing shallow semantic content that combines class-
based semantic markup (e.g., road runner is a car
model) with a latent variable model for capturing
weakly compositional interactions between query
constituents. Constituents are tagged with IsA class
labels from a large, automatically extracted lexicon,
using a probabilistic context free grammar (PCFG).
Correlations between the resulting label?term dis-
tributions are captured using a set of latent produc-
tion rules specified by a hierarchical Dirichlet Pro-
cess (Teh et al, 2006) with latent data groupings.
Concretely, the IsA tags capture the inventory
of potential meanings (e.g., jaguar can be labeled
as european car or large cat) and relevant con-
stituent spans, while the latent variable model per-
forms sense and theme disambiguation (e.g., [jaguar
habitat] would lend evidence for the large cat la-
bel). In addition to broad sense disambiguation, CLC
can distinguish closely related usages, e.g., the use
of dell in [dell motherboard replacement] and [dell
stock price].1 Furthermore, by employing IsA class
labeling as a preliminary step, CLC can account for
common non-compositional phrases, such as big ap-
ple unlike systems relying purely on lexical seman-
tics. Additional examples can be found later, in Fig-
ure 5.
In addition to improving query understanding, po-
tential applications of CLC include: (1) relation ex-
traction (Baeza-Yates and Tiberi, 2007), (2) query
substitutions or broad matching (Jones et al, 2006),
and (3) classifying other short textual fragments
such as SMS messages or tweets.
We implement a parallel inference procedure for
1Dell the computer system vs. Dell the technology company.
1200
CLC and evaluate it on a sample of 500M search
queries along two dimensions: (1) query constituent
chunking precision (i.e., how accurate are the in-
ferred spans breaks; cf., Bergsma and Wang (2007);
Tan and Peng (2008)), and (2) class label assign-
ment precision (i.e., given the query intent, how rel-
evant are the inferred class labels), paying particu-
lar attention to cases where queries contain ambigu-
ous constituents. CLC compares favorably to sev-
eral simpler submodels, with gains in performance
stemming from coarse-graining related class labels
and increasing the number of clusters used to cap-
ture between-label correlations.
(Paper organization): Section 2 discusses relevant
background, Section 3 introduces the CLC model,
Section 4 describes the experimental setup em-
ployed, Section 5 details results, Section 6 intro-
duces areas for future work and Section 7 concludes.
2 Background
Query understanding has been studied extensively
in previous literature. Li (2010) defines the se-
mantic structure of noun-phrase queries as intent
heads (attributes) coupled with some number of in-
tent modifiers (attribute values), e.g., the query [al-
ice in wonderland 2010 cast] is comprised of an in-
tent head cast and two intent modifiers alice in won-
derland and 2010. In this work we focus on seman-
tic class markup of query constituents, but our ap-
proach could be easily extended to account for query
structure as well.
Popescu et al (2010) describe a similar class-
label-based approach for query interpretation, ex-
plicitly modeling the importance of each label for
a given entity. However, details of their implemen-
tation were not publicly available, as of publication
of this paper.
For simplicity, we extract class labels using the
seed-based approach proposed by Van Durme and
Pas?ca (2008) (in particular Pas?ca (2010)) which gen-
eralizes Hearst (1992). Talukdar and Pereira (2010)
use graph-based semi-supervised learning to acquire
class-instance labels; Wang et al (2009) introduce a
similar CRF-based approach but only apply it to a
small number of verticals (i.e., Computing and Elec-
tronics or Clothing and Shoes). Snow et al (2006)
describe a learning approach for automatically ac-
quiring patterns indicative of hypernym (IsA) rela-
tions. Semantic class label lexicons derived from
any of these approaches can be used as input to CLC.
Several authors have studied query clustering in
the context of information retrieval (e.g., Beeferman
and Berger, 2000). Our approach is novel in this
regard, as we cluster queries in order to capture cor-
relations between span labels, rather than explicitly
for query understanding.
Tratz and Hovy (2010) propose a taxonomy for
classifying and interpreting noun-compounds, fo-
cusing specifically on the relationships holding be-
tween constituents. Our approach yields similar top-
ical decompositions of noun-phrases in queries and
is completely unsupervised.
Jones et al (2006) propose an automatic method
for query substitution, i.e., replacing a given query
with another query with the similar meaning, over-
coming issues with poor paraphrase coverage in tail
queries. Correlations mined by our approach are
readily useful for downstream query substitution.
Bergsma and Wang (2007) develop a super-
vised approach to query chunking using 500 hand-
segmented queries from the AOL corpus. Tan and
Peng (2008) develop a generative model of query
segmentation that makes use of a language model
and concepts derived from Wikipedia article titles.
CLC differs fundamentally in that it learns con-
cept label markup in addition to segmentation and
uses in-domain concepts derived from queries them-
selves. This work also differs from both of these
studies significantly in scope, training on 500M
queries instead of just 500.
At the level of class-label markup, our model is
related to Bayesian PCFGs (Liang et al, 2007; John-
son et al, 2007b), and is a particular realization of an
Adaptor Grammar (Johnson et al, 2007a; Johnson,
2010).
Szpektor et al (2008) introduce a model of con-
textual preferences, generalizing the notion of selec-
tional preference (cf. Ritter et al, 2010) to arbitrary
terms, allowing for context-sensitive inference. Our
approach differs in its use of class-instance labels for
generalizing terms, a necessary step for dealing with
the lack of syntactic information in queries.
1201
 ?
C
 
?
L
 
?
L
vinyl windowsbrighton
seaside towns building materials
query clusters
label clusters
label pcfg
query constituents
Figure 1: Overview of CLC markup generation for
the query [brighton vinyl windows]. Arrows denote
multinomial distributions.
3 Latent Class-Label Correlation
Input to CLC consists of raw search queries and a
partial grammar mapping class labels to query spans
(e.g., building materials?vinyl windows). CLC in-
fers two additional latent productions types on top
of these class labels: (1) a potentially infinite set of
label clusters ?Llk coarse-graining the raw input label
productions V , and (2) a finite set of query clusters
?Cci specifying distributions over label clusters; see
Figure 1 for an overview.
Operationally, CLC is implemented as a Hierar-
chical Dirichlet Process (HDP; Teh et al, 2006) with
latent groups coupled with a Probabilistic Context
Free Grammar (PCFG) likelihood function (Figure
2). We motivate our use of an HDP latent class
model instead of a full PCFG with binary produc-
tions by the fact that the space of possible binary
rule combinations is prohibitively large (561K base
labels; 314B binary rules). The next sections discuss
the three main components of CLC: ?3.1 the raw IsA
class labels, ?3.2 the PCFG likelihood, and ?3.3 the
HDP with latent groupings.
3.1 IsA Label Extraction
IsA class labels (hypernyms) V are extracted from
a large corpus of raw Web text using the method
proposed by Van Durme and Pas?ca (2008) and ex-
tended by Pas?ca (2010). Manually specified patterns
are used to extract a seed set of class labels and the
resulting label lists are reranked using cluster purity
measures. 561K labels for base noun phrases are
collected. Table 1 shows an example set of class
labels extracted for several common noun phrases.
Similar repositories of IsA labels, extracted using
other methods, are available for experimental pur-
class label?query span
recreational facilities?jacuzzi
rural areas?wales
destinations?wales
seaside towns?brighton
building materials?vinyl windows
consumer goods?european clothing
Table 1: Example production rules collected using
the semi-supervised approach of Van Durme and
Pas?ca (2008).
poses (Talukdar and Pereira, 2010). In addition to
extracted rules, the CLC grammar is augmented with
a set of null rules, one per unigram, ensuring that
every query has a valid parse.
3.2 Class-Label PCFG
In addition to the observed class-label production
rules, CLC incorporates two sets of latent produc-
tion rules coupled via an HDP (Figure 1). Class
label?query span productions extracted from raw
text are clustered into a set of latent label produc-
tion clusters L = {l1, . . . , l?}. Each label pro-
duction cluster lk defines a multinomial distribution
over class labels V parametrized by ?Llk . Conceptu-
ally, ?Llk captures a set of class labels with similar
productions that are found in similar queries, for ex-
ample the class labels states, northeast states, u.s.
states, state areas, eastern states, and certain states
might be included in the same coarse-grained cluster
due to similarities in their productions.
Each query q ? Q is assigned to a latent query
cluster cq ? C{c1, . . . , c?}, which defines a dis-
tribution over label production clusters L, denoted
?Ccq . Query clusters capture broad correlations be-
tween label production clusters and are necessary for
performing sense disambiguation and capturing se-
lectional preference. Query clusters and label pro-
duction clusters are linked using a single HDP, al-
lowing the number of label clusters to vary over the
course of Gibbs sampling, based on the variance of
the underlying data (Section 3.3). Viewed as a gram-
mar, CLC only contains unary rules mapping labels
to query spans; production correlations are captured
directly by the query cluster, unlike in HDP-PCFG
(Liang et al, 2007), as branching parses over the en-
1202
Indices Cardinality
HDP base measure ? ? GEM(?) - |L| ? ?
Query cluster ?Ci ? DP(?
C ,?) i ? |C| |L| ? ?
Label cluster ?Lk ? Dirichlet(?
L) k ? |L| |V |
Query cluster ind
piq ? Dirichlet(?) q ? |Q| |C|
cq ? piq q ? |Q| 1
Label cluster ind zq,t ? ?
C
cq t ? q, q ? |Q| 1
Label ind lq,t ? ?
L
zq,t t ? q, q ? |Q| 1
 
c
z
?
q
t
l
!
L
?
 
?
 
?
 
?
label clusters
 
!
C
|C|
 
?0
query clusters
 
?
Figure 2: Generative process and graphical model for CLC. The top section of the model is the standard
HDP prior; the middle section is the additional machinery necessary for modeling latent groupings and the
bottom section contains the indicators for the latent class model. PCFG likelihood is not shown.
tire label sparse are intractably large.
Given a query q, a query cluster assignment cq and
a set of label production clustersL, we define a parse
of q to be a sequence of productions tq forming a
parse tree consuming all the tokens in q. As with
Bayesian PCFGs (Johnson, 2010), the probability of
a tree tq is the product of the probabilities of the
production rules used to construct it
P (tq|?L,?C , cq) =
?
r?Rq
P (r|?Llr)P (lr|?
C
cq)
where Rq is the set of production rules used to de-
rive tq, P (r|?Llr) is the probability of r given its label
cluster assignment lr, and P (lr|?Ccq) is the probabil-
ity of label cluster lr in query cluster c.
The probability of a query q is the sum of the
probabilities of the parse trees that can generate it,
P (q|?L,?C , cq) =
?
{t|y(t)=q}
P (t|?L,?C , cq)
where {t|y(t) = q} is the set of trees with q as their
yield (i.e., generate the string of tokens in q).
3.3 Hierarchical Dirichlet Process with Latent
Groups
We complete the Bayesian generative specification
of CLC with an HDP prior linking ?C and ?L. The
HDP is a Bayesian generative model of shared struc-
ture for grouped data (Teh et al, 2006). A set of
base clusters ? ? GEM(?) is drawn from a Dirich-
let Process with base measure ? using the stick-
breaking construction, and clusters for each group k,
? ? HDP-LG base-measure smoother; higher val-
ues lead to more uniform mass over label
clusters.
?C ? Query cluster smoothing; higher values lead
to more uniform mass over label clusters.
?L ? Label cluster smoothing; higher values lead
to more label diversity within clusters.
? ? Query cluster assignment smoothing; higher
values lead to more uniform assignment.
Table 2: CLC-HDP-LG hyperparameters.
?Ck ? DP(?), are drawn from a separate Dirichlet
Process with base measure ?, defined over the space
of label clusters. Data in each group k are condi-
tionally independent given ?. Intuitively, ? defines
a common ?menu? of label clusters, and each query
cluster ?Ck defines a separate distribution over the
label clusters.
In order to account for variable query-cluster as-
signment, we extend the HDP model with latent
groupings piq ? Dir(?) for each query. The re-
sulting Hierarchical Dirichlet Process with Latent
Groups (HDP-LG) can be used to define a set of
query clusters over a set of (potentially infinite) base
label clusters (Figure 2). Each query cluster ?C (la-
tent group) assigns weight to different subsets of the
available label clusters ?L, capturing correlations
between them at the query level. Each query q main-
tains a distribution over query clusters piq, capturing
its affinity for each latent group. The full generative
specification of CLC is shown in Figure 2; hyperpa-
rameters are shown in Table 2.
In addition to the full joint CLC model, we evalu-
1203
ate several simpler models:
1. CLC-BASE ? no query clusters, one label per
label cluster.
2. CLC-DPMM ? no query clusters, DPMM(?C)
distribution over labels.
3. CLC-HDP-LG ? full HDP-LG model with |C|
query clusters over a potentially infinite num-
ber of query clusters.
as well as various hyperparameter settings.
3.4 Parallel Approximate Gibbs Sampler
We perform inference in CLC via Gibbs sampling,
leveraging Multinomial-Dirichlet conjugacy to inte-
grate out pi, ?C and ?L (Teh et al, 2006; Johnson
et al, 2007b). The remaining indicator variables c, z
and l are sampled iteratively, conditional on all other
variable assignments. Although there are an expo-
nential number of parse trees for a given query, this
space can be sampled efficiently using dynamic pro-
gramming (Finkel et al, 2006; Johnson et al, 2007b)
In order to apply CLC to Web-scale data, we
implement an efficient parallel approximate Gibbs
sampler in the MapReduce framework Dean and
Ghemawat (2004). Each Gibbs iteration consists
of a single MapReduce step for sampling, followed
by an additional MapReduce step for computing
marginal counts. 2 Relevant assignments c, z and
l are stored locally with each query and are dis-
tributed across compute nodes. Each node is respon-
sible only for resampling assignments for its local
set of queries. Marginals are fetched opportunisti-
cally from a separate distributed hash server as they
are needed by the sampler. Each Map step computes
a single Gibbs step for 10% of the available data, us-
ing the marginals computed at the previous step. By
resampling only 10% of the available data each it-
eration, we minimize the potentially negative effects
of using the previous step?s marginal distribution.
4 Experimental Setup
4.1 Query Corpus
Our dataset consists of a sample of 450M En-
glish queries submitted by anonymous Web users to
2This approximation and architecture is similar to Smola
and Narayanamurthy (2010).
Query length
de
ns
ity
0.1
0.2
0.3
0.4
2 4 6 8 10 12
Figure 3: Distribution in the query corpus, bro-
ken down by query length (red/solid=all queries;
blue/dashed=queries with ambiguous spans); most
queries contain between 2-6 tokens.
Google. The queries have an average of 3.81 tokens
per query (1.7B tokens). Single token queries are re-
moved as the model is incapable of using context to
disambiguate their meaning. Figure 3 shows the dis-
tribution of remaining queries. During training, we
include 10 copies of each query (4.5B queries total),
allowing an estimate of the Bayes average posterior
from a single Gibbs sample.
4.2 Evaluations
Query markup is evaluated for phrase-chunking pre-
cision (Section 5.1) and label precision (Section 5.2)
by human raters across two different samples: (1)
an unbiased sample from the original corpus, and
(2) a biased sample of queries containing ambigu-
ous spans.
Two raters scored a total of 10K labels from 800
spans across 300 queries. Span labels were marked
as incorrect (0.0), badspan (0.0), ambiguous (0.5),
or correct (1.0), with numeric scores for label pre-
cision as indicated. Chunking precision is measured
as the percentage of labels not marked as badspan.
We report two sets of precision scores depend-
ing on how null labels are handled: Strict evaluation
treats null-labeled spans as incorrect, while Normal
evaluation removes null-labeled spans from the pre-
cision calculation. Normal evaluation was included
since the simpler models (e.g., CLC-BASE) tend to
produce a significantly higher number of null assign-
ments.
Model evaluations were broken down into max-
imum a posteriori (MAP) and Bayes average esti-
mates. MAP estimates are calculated as the single
most likely label/cluster assignment across all query
copies; all assignments in the sample are averaged
1204
% 
clu
ste
r m
ov
es
0.0
0.2
0.4
0.6
0.8
50 100 150 200 250
% 
lab
el 
mo
ve
s
0.25
0.30
0.35
0.40
0.45
0.50
50 100 150 200 250
Gibbs iterations
%
 nu
ll r
ule
s
0.040
0.045
0.050
0.055
0.060
0.065
0.070
50 100 150 200 250
Figure 4: Convergence rates of CLC-
BASE (red/solid), CLC-HDP-LG 100C,40L
(green/dashed), CLC-HDP-LG 1000C,40L
(blue/dotted) in terms of % of query cluster swaps,
label cluster swaps and null rule assignments.
to obtain the Bayes average precision estimate.3
5 Results
A total of five variants of CLC were evaluated with
different combinations of |C| and HDP prior con-
centration ?C (controlling the effective number of
label clusters). Referring to models in terms of their
parametrizations is potentially confusing. There-
fore, we will make use of the fact that models with
?C = 1 yielded roughly 40 label clusters on aver-
age, and models with ?C = 0.1 yielded roughly 200
label clusters, naming model variants simply by the
number of query and label clusters: (1) CLC-BASE,
(2) CLC-DPMM 1C-40L, (3) CLC-HDP-LG 100C-
40L, (4) CLC-HDP-LG 1000C-40L, and (5) CLC-
HDP-LG 1000C-200L. Figure 4 shows the model
convergence for CLC-BASE, CLC-HDP-LG 100C-
40L, and CLC-HDP-LG 1000C-40L.
3We calculate the Bayes average precision estimates at
the top 10 (Bayes@10) and top 20 (Bayes@20) parse trees,
weighted by probability.
5.1 Chunking Precision
Chunking precision scores for each model are
shown in Table 3 (average % of labels not marked
badspan). CLC-HDP-LG 1000C-40L has the high-
est precision across both MAP and Bayes esti-
mates (?93% accuracy), followed by CLC-HDP-LG
1000C-200L (?90% accuracy) and CLC-DPMM 1C-
40L (?85%). CLC-BASE performed the worst by
a significant margin (?78%), indicating that label
coarse-graining is more important than query clus-
tering for chunking accuracy. No significant dif-
ferences in label chunking accuracy were found be-
tween Bayes and MAP inference.
5.2 Predicting Span Labels
The full CLC-HDP-LG model variants obtain higher
label precision than the simpler models, with CLC-
HDP-LG 1000C-40L achieving the highest precision
of the three (?63% accuracy). Increasing the num-
ber of label clusters too high, however, significantly
reduces precision: CLC-HDP-LG 1000C-200L ob-
tains only ?51% accuracy. However, comparing
to CLC-DPMM 1C-40L and CLC-BASE demonstrates
that the addition of label clusters and query clusters
both lead to gains in label precision. These relative
rankings are robust across strict and normal evalua-
tion regimes.
The breakdown over MAP and Bayes posterior
estimation is less clear when considering label pre-
cision: the simpler models CLC-BASE and CLC-
DPMM 1C-40L perform significantly worse than
Bayes when using MAP estimation, while in CLC-
HDP-LG the reverse holds.
There is little evidence for correlation between
precision and query length (weak, not statistically
significant negative correlation using Spearman?s ?).
This result is interesting as the relative prevalence
of natural language queries increases with query
length, potentially degrading performance. How-
ever, we did find a strong positive correlation be-
tween precision and the number of labels produc-
tions applicable to a query, i.e., production rule fer-
tility is a potential indicator of semantic quality.
Finally, the histogram column in Table 3 shows
the distribution of rater responses for each model.
In general, the more precise models tend to have
a significantly lower proportion of missing spans
1205
Model Chunking Label Precision Ambiguous Label Precision Spearman?s ?
Precision normal strict hist normal strict q. len # labels
Class-Label Correlation Base
Bayes@10 78.7?1.1 37.7?1.2 35.8?1.2 35.4?2.0 33.2?1.9 -0.13 0.51?
Bayes@20 78.7?1.1 37.7?1.2 35.8?1.2 35.4?2.0 33.2?1.9 -0.13 0.51?
MAP 76.3?2.2 33.3?2.2 31.8?2.2 36.2?4.0 33.2?3.8 -0.13 0.52?
Class-Label Correlation DPMM 1C 40L
Bayes@10 84.9?0.4 46.6?0.6 44.3?0.5 36.0?1.1 33.7?1.0 -0.05 0.25
Bayes@20 84.8?0.4 47.4?0.5 45.2?0.5 37.8?1.0 35.5?1.0 -0.02 0.23
MAP 84.1?0.8 42.6?1.0 40.5?0.9 11.2?1.3 10.6?1.3 -0.03 0.12
Class-Label Correlation HDP-LG 100C 40L
Bayes@10 83.8?0.4 55.6?0.5 51.0?0.5 55.6?1.0 47.7?1.0 0.03 0.44?
Bayes@20 83.6?0.4 56.9?0.5 52.3?0.5 57.4?1.0 49.8?0.9 0.04 0.41?
MAP 82.7?0.5 58.5?0.5 53.6?0.5 60.4?1.1 51.5?1.0 0.02 0.41?
Class-Label Correlation HDP-LG 1000C 40L
Bayes@10 93.1?0.2 61.1?0.3 60.0?0.3 43.2?0.9 40.2?0.9 -0.06 0.26?
Bayes@20 92.8?0.2 62.6?0.3 61.7?0.3 44.9?0.8 42.2?0.8 -0.10 0.27?
MAP 92.7?0.2 63.7?0.3 62.7?0.3 44.1?0.9 41.1?0.9 -0.12 0.28?
Class-Label Correlation HDP-LG 1000C 200L
Bayes@10 90.3?0.5 50.9?0.8 48.6?0.7 45.8?1.5 42.5?1.3 -0.10 0.13
Bayes@20 89.9?0.5 50.2?0.7 48.0?0.7 44.4?1.4 41.3?1.3 -0.08 0.11
MAP 90.0?0.6 51.0?0.8 48.9?0.8 49.2?1.5 46.0?1.4 -0.07 0.04
Table 3: Chunking and label precision across five models. Confidence intervals are standard error; sparklines
show distribution of precision scores (left is zero, right is one). Hist shows the distribution of human rating
response (log y scale): green/first is correct, blue/second is ambiguous, cyan/third is missing and red/fourth
is incorrect. Spearman?s ? columns give label precision correlations with query length (weak negative corre-
lation) and the number of applicable labels (weak to strong positive correlation); dots indicate significance.
(blue/second bar; due to null rule assignment) in ad-
ditional to more correct (green/first) and fewer in-
correct (red/fourth) spans.
5.3 High Polysemy Subset
We repeat the analysis of label precision on a subset
of queries containing one of the manually-selected
polysemous spans shown in Table 4. The CLC-
HDP-LG -based models still significantly outper-
form the simpler models, but unlike in the broader
setting, CLC-HDP-LG 100C-40L significantly out-
performs CLC-HDP-LG 1000C-40L, indicating that
lower query cluster granularity helps address poly-
semy (Table 3).
5.4 Error Analysis
Figure 5 gives examples of both high-precision and
low-precision queries markups inferred by CLC-
HDP-LG. In general, CLC performs well on queries
with clear intent head / intent modifier structure (Li,
acapella, alamo, apple, atlas, bad, bank, batman,
beloved, black forest, bravo, bush, canton, casino,
champion, club, comet, concord, dallas, diamond,
driver, english, ford, gamma, ion, lemon, man-
hattan, navy, pa, palm, port, put, resident evil,
ronaldo, sacred heart, saturn, seven, solution, so-
pranos, sparta, supra, texas, village, wolf, young
Table 4: Samples from a list of 90 manually se-
lected ambiguous spans used to evaluate model per-
formance under polysemy.
2010). More complex queries, such as [never know
until you try quotes] or [how old do you have to be
a bartender in new york] do not fit this model; how-
ever, expanding the set of extracted labels to also
cover instances such as never know until you try
would mitigate this problem, motivating the use of
n-gram language models with semantic markup.
A large number of mistakes made by CLC are
1206
To
p
 
1
0
%
B
o
t
t
o
m
 
2
0
%
M
i
d
d
l
e
 
2
0
%
Figure 5: Examples of high- and low-precision query markups inferred by CLC-HDP-LG. Black text is the
original query; lines indicate potential spans; small text shows potential labels colored and numbered by
label cluster; small bar shows percentage of assignments to that label cluster.
due to named-entity categories with weak seman-
tics such as rock bands or businesses (e.g., [tropi-
cal breeze cleaners], [cosmic railroad band] or [so-
pranos cigars]). When the named entity is common
enough, it is detected by the rule set, but for the long
tail of named entities this is not the case. One poten-
tial solution is to use a stronger notion of selectional
preference and slot-filling, rather than just relying on
correlation between labels.
Other examples of common errors include inter-
preting weymouth in [weymouth train time table] as
a town in Massachusetts instead of a town in the UK
(lack of domain knowledge), and using lower qual-
ity semantic labels (e.g., neighboring countries for
france, or great retailers for target).
6 Discussion and Future Work
Adding both latent label clusters (DPMM) and la-
tent query clusters (extending to HDP-LG) improve
chunking and label precision over the baseline CLC-
BASE system. The label clusters are important be-
cause they capture intra-group correlations between
class labels, while the query clusters are important
for capturing inter-group correlations. However, the
algorithm is sensitive to the relative number of clus-
ters in each case: Too many labels/label clusters rel-
1207
ative to the number of query clusters make it difficult
to learn correlations (O(n2) query clusters are re-
quired to capture pairwise interactions). Too many
query clusters, on the other hand, make the model
intractable computationally. The HDP automates se-
lecting the number of clusters, but still requires man-
ual hyperparameter setting.
(Future Work) Many query slots have weak se-
mantics and hence are misleading for CLC. For
example [pacific breeze cleaners] or [dale hartley
subaru] should be parsed such that the type of the
leading slot is determined not by its direct content,
but by its context; seeing subaru or cleaners after
a noun-phrase slot is a strong indicator of its type
(dealership or shop name). The current CLC model
only couples these slots through their correlations in
query clusters, not directly through relative position
or context. Binary productions in the PCFG or a dis-
criminative learning model would help address this.
Finally, we did not measure label coverage with
respect to a human evaluation set; coverage is use-
ful as it indicates whether our inferred semantics are
biased with respect to human norms.
7 Conclusions
We introduced CLC, a set of latent variable PCFG
models for semantic analysis of short textual seg-
ments. CLC captures semantic information in the
form of interactions between clusters of automati-
cally extracted class-labels, e.g., finding that place-
names commonly co-occur with business-names.
We applied CLC to a corpus containing 500M search
queries, demonstrating its scalability and straight-
forward parallel implementation using frameworks
like MapReduce or Hadoop. CLC was able to chunk
queries into spans more accurately and infer more
precise labels than several sub-models even across a
highly ambiguous query subset. The key to obtain-
ing these results was coarse-graining the input class-
label set and using a latent variable model to capture
interactions between coarse-grained labels.
References
R. Baeza-Yates and A. Tiberi. 2007. Extracting semantic
relations from query logs. In Proceedings of the 13th
ACM Conference on Knowledge Discovery and Data
Mining (KDD-07), pages 76?85. San Jose, California.
D. Beeferman and A. Berger. 2000. Agglomerative clus-
tering of a search engine query log. In Proceedings of
the 6th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD-00), pages 407?416.
S. Bergsma and Q. Wang. 2007. Learning noun phrase
query segmentation. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-07), pages 819?826. Prague,
Czech Republic.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In Proceedings
of the 6th Symposium on Operating Systems Design
and Implementation (OSDI-04), pages 137?150. San
Francisco, California.
J. Finkel, C. Manning, and A. Ng. 2006. Solving the
problem of cascading errors: Approximate Bayesian
inference for linguistic annotation pipelines. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-06),
pages 618?626. Sydney, Australia.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545. Nantes, France.
M. Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-10), pages 1148?1157. Up-
psala, Sweden.
M. Johnson, T. Griffiths, and S. Goldwater. 2007a. Adap-
tor grammars: a framework for specifying composi-
tional nonparametric bayesian models. In Advances
in Neural Information Processing Systems 19, pages
641?648. Vancouver, Canada.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov Chain
Monte Carlo. In Proceedings of the 2007 Confer-
ence of the North American Association for Computa-
tional Linguistics (NAACL-HLT-07), pages 139?146.
Rochester, New York.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15h
World Wide Web Conference (WWW-06), pages 387?
396. Edinburgh, Scotland.
X. Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1337?1345. Upp-
sala, Sweden.
1208
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proceedings of the 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
07), pages 688?697. Prague, Czech Republic.
M. Pas?ca. 2010. The role of queries in ranking labeled in-
stances extracted from text. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING-10), pages 955?962. Beijing, China.
A. Popescu, P. Pantel, and G. Mishne. 2010. Seman-
tic lexicon adaptation for use in query interpretation.
In Proceedings of the 19th World Wide Web Confer-
ence (WWW-10), pages 1167?1168. Raleigh, North
Carolina.
A. Ritter, Mausam, and O. Etzioni. 2010. A latent Dirich-
let alocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
424?434. Uppsala, Sweden.
A. Smola and S. Narayanamurthy. 2010. An architec-
ture for parallel topic models. In Proceedings of the
36th Conference on Very Large Data Bases (VLDB-
10), pages 703?710. singapore.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801?808. Sydney, Australia.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-08), pages 683?691. Colum-
bus, Ohio.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1473?1481. Upp-
sala, Sweden.
B. Tan and F. Peng. 2008. Unsupervised query segmenta-
tion using generative language models and Wikipedia.
In Proceedings of the 17th World Wide Web Confer-
ence (WWW-08), pages 347?356. Beijing, China.
Y. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
S. Tratz and E. Hovy. 2010. A taxonomy, dataset, and
classifier for automatic noun compound interpretation.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-10), pages
678?687. Uppsala, Sweden.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. In Proceedings of the 23rd National Confer-
ence on Artificial Intelligence (AAAI-08), pages 1243?
1248. Chicago, Illinois.
T. Wang, R. Hoffmann, X. Li, and J. Szymanski.
2009. Semi-supervised learning of semantic classes
for query understanding: from the Web and for the
Web. In Proceedings of the 18th International Con-
ference on Information and Knowledge Management
(CIKM-09), pages 37?46. Hong Kong, China.
1209
