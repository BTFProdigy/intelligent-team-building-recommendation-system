Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 372?381,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Triplet Lexicon Models for Statistical Machine Translation
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, Jesu?s Andre?s-Ferrer??
Human Language Technology and Pattern Recognition, RWTH Aachen University, Germany
?Universidad Polite?cnica de Valencia, Dept. Sist. Informa?ticos y Computacio?n
{hasan,ganitkevitch,ney}@cs.rwth-aachen.de jandres@dsic.upv.es
Abstract
This paper describes a lexical trigger model
for statistical machine translation. We present
various methods using triplets incorporating
long-distance dependencies that can go be-
yond the local context of phrases or n-gram
based language models. We evaluate the pre-
sented methods on two translation tasks in a
reranking framework and compare it to the re-
lated IBM model 1. We show slightly im-
proved translation quality in terms of BLEU
and TER and address various constraints to
speed up the training based on Expectation-
Maximization and to lower the overall num-
ber of triplets without loss in translation per-
formance.
1 Introduction
Data-driven methods have been applied very suc-
cessfully within the machine translation domain
since the early 90s. Starting from single-word-
based translation approaches, significant improve-
ments have been made through advances in mod-
eling, availability of larger corpora and more pow-
erful computers. Thus, substantial progress made
in the past enables today?s MT systems to achieve
acceptable results in terms of translation quality for
specific language pairs such as Arabic-English. If
sufficient amounts of parallel data are available, sta-
tistical MT systems can be trained on millions of
?The work was carried out while the author was at the Hu-
man Language Technology and Pattern Recognition group at
RWTH Aachen University and partly supported by the Valen-
cian Conselleria d?Empresa, Universitat i Cie`ncia under grants
CTBPRA/2005/ and BEFPI/2007/014.
target
source
e e?
f
Figure 1: Triplet example: a source word f is triggered
by two target words e and e?, where one of the words is
within and the other outside the considered phrase pair
(indicated by the dashed line).
sentence pairs and use an extended level of context
based on bilingual groups of words which denote
the building blocks of state-of-the-art phrase-based
SMT systems.
Due to data sparseness, statistical models are of-
ten trained on local context only. Language mod-
els are derived from n-grams with n ? 5 and bilin-
gual phrase pairs are extracted with lengths up to
10 words on the target side. This captures the local
dependencies of the data in detail and is responsi-
ble for the success of data-driven phrase-based ap-
proaches.
In this work, we will introduce a new statistical
model based on lexicalized triplets (f, e, e?) which
we will also refer to as cross-lingual triggers of
the form (e, e? ? f). This can be understood
as two words in one language triggering one word
in another language. These triplets, modeled by
p(f |e, e?), are closely related to lexical translation
probabilities based on the IBM model 1, i.e. p(f |e).
Several constraints and setups will be described later
on in more detail, but as an introduction one can
372
think of the following interpretation which is de-
picted in Figure 1: Using a phrase-based MT ap-
proach, a source word f is triggered by its trans-
lation e which is part of the phrase being consid-
ered, whereas another target word e? outside this
phrase serves as an additional trigger in order to al-
low for more fine-grained distinction of a specific
word sense. Thus, this cross-lingual trigger model
can be seen as a combination of a lexicon model (i.e.
f and e) and a model similar to monolingual long-
range (i.e. distant bigram) trigger models (i.e. e and
e?, although these dependencies are reflected indi-
rectly via e? ? f ) which uses both local (in-phrase)
and global (in-sentence) information for the scoring.
The motivation behind this approach is to get non-
local information outside the current context (i.e. the
currently considered bilingual phrase pair) into the
translation process. The triplets are trained via the
EM algorithm, as will be shown later in more detail.
2 Related Work
In the past, a significant number of methods has
been presented that try to capture long-distance de-
pendencies, i.e. use dependencies in the data that
reach beyond the local context of n-grams or phrase
pairs. In language modeling, monolingual trigger
approaches have been presented (Rosenfeld, 1996;
Tillmann and Ney, 1997) as well as syntactical meth-
ods that parse the input and model long-range de-
pendencies on the syntactic level by conditioning on
the predecessing words and their corresponding par-
ent nodes (Chelba and Jelinek, 2000; Roark, 2001).
The latter approach was shown to reduce perplex-
ities and improve the WER in speech recognition
systems. One drawback is that the parsing process
might slow down the system significantly and the
approach is complicated to be integrated directly in
the search process. Thus, the effect is often shown
offline in reranking experiments using n-best lists.
One of the simplest models that can be seen in
the context of lexical triggers is the IBM model 1
(Brown et al, 1993) which captures lexical depen-
dencies between source and target words. It can be
seen as a lexicon containing correspondents of trans-
lations of source and target words in a very broad
sense since the pairs are trained on the full sentence
level. The model presented in this work is very close
to the initial IBM model 1 and can be seen as taking
another word into the conditioning part, i.e. the trig-
gering items.1 Furthermore, since the second trig-
ger can come from any part of the sentence, we also
have a link to long-range monolingual triggers as
presented above.
A long-range trigram model is presented in
(Della Pietra et al, 1994) where it is shown how to
derive a probabilistic link grammar in order to cap-
ture long-range dependencies in English using the
EM algorithm. Expectation-Maximization is used
in the presented triplet model as well which is de-
scribed in more detail in Section 3. Instead of deriv-
ing a grammar automatically (based on POS tags of
the words), we rely on a fully lexicalized approach,
i.e. the training is taking place at the word level.
Related work in the context of fine-tuning lan-
guage models by using cross-lingual lexical triggers
is presented in (Kim and Khudanpur, 2003). The
authors show how to use cross-lingual triggers on a
document level in order to extract translation lexi-
cons and domain-specific language models using a
mutual information criterion.
Recently, word-sense disambiguation (WSD)
methods have been shown to improve translation
quality (Chan et al, 2007; Carpuat and Wu, 2007).
Chan et al (2007) use an SVM based classifier for
disambiguating word senses which are directly in-
corporated in the decoder through additional fea-
tures that are part of the log-linear combination of
models. They use local collocations based on sur-
rounding words left and right of an ambiguous word
including the corresponding parts-of-speech. Al-
though no long-range dependencies are modeled, the
approach yields an improvement of +0.6% BLEU on
the NIST Chinese-English task. In Carpuat and Wu
(2007), another state-of-the-art WSD engine (a com-
bination of naive Bayes, maximum entropy, boost-
ing and Kernel PCA models) is used to dynamically
determine the score of a phrase pair under consid-
eration and, thus, let the phrase selection adapt to
the context of the sentence. Although the baseline is
significantly lower than in the work of Chan et al,
this setup reaches an improvement of 0.5% BLEU
on the NIST CE task and up to 1.1% BLEU on the
1Thus, instead of p(f |e) we model p(f |e, e?) with different
additional constraints as explained later on.
373
IWSLT?06 test sets.
The work in this paper tries to complement the
WSD approaches by using long-range dependen-
cies. If triggers from a local context determine dif-
ferent lexical choice for the word being triggered,
the setting is comparable to the mentioned WSD
approaches (although local dependencies might al-
ready be reflected sufficiently in the phrase models).
A distant second trigger, however, might have a ben-
eficial effect for specific languages, e.g. by captur-
ing word splits (as it is the case in German for verbs
with separable prefixes) or, as already mentioned, al-
lowing for a more fine-grained lexical choice of the
word being triggered, namely based on another word
which is not part of the current local, i.e. phrasal,
context.
The basic idea of triplets of the form (e, f ? ? f),
called multi-word extensions, is also mentioned in
(Tillmann, 2001) but neither evaluated nor investi-
gated in further detail.
In the following sections, we will describe the
model proposed in this work. In Section 3, a de-
tailed introduction is given, as well as the EM train-
ing and variations of the model. The different set-
tings will be evaluated in Section 4, where we show
experiments on the IWSLT Chinese-English and
TC-STAR EPPS English-Spanish/Spanish-English
tracks. A discussion of the results and further ex-
amples are given in Section 5. Final remarks and
future work are addressed in Section 6.
3 Model
As an extension to commonly used lexical word
pair probabilities p(f |e) as introduced in (Brown
et al, 1993), we define our model to operate on
word triplets. A triplet (f, e, e?) is assigned a value
?(f |e, e?) ? 0 with the constraint such that
?e, e? :
?
f
?(f |e, e?) = 1.
Throughout this paper, e and e? will be referred to as
the first and the second trigger, respectively. In view
of its triggers f will be termed the effect.
For a given bilingual sentence pair (fJ1 , e
I
1), the
probability of a source word fj given the whole tar-
get sentence eI1 for the triplet model is defined as:
pall (fj |e
I
1) =
1
Z
I?
i=1
I?
k=i+1
?(fj |ei, ek), (1)
where Z denotes a normalization factor based on the
corresponding target sentence length, i.e.
Z =
I(I ? 1)
2
. (2)
The introduction of a second trigger (i.e. ek in
Eq. 1) enables the model to combine local (i.e. word
or phrase level) and global (i.e. sentence level) infor-
mation.
In the following, we will describe the training pro-
cedure of the model via maximum likelihood esti-
mation for the unconstrained case.
3.1 Training
The goal of the training procedure is to maximize the
log-likelihood Fall of the triplet model for a given
bilingual training corpus {(fJ1 , e
I
1)}
N
1 consisting of
N sentence pairs:
Fall :=
N?
n=1
Jn?
j=1
log pall (fj |e
In
1 ),
where Jn and In are the lengths of the nth source
and target sentences, respectively. As there is no
closed form solution for the maximum likelihood es-
timate, we resort to iterative training via the EM al-
gorithm (Dempster et al, 1977). We define the aux-
iliary function Q(?; ??) based on Fall where ?? is the
new estimate within an iteration which is to be de-
rived from the current estimate ?. Here, ? stands for
the entire set of model parameters to be estimated,
i.e. the set of all {?(f |e, e?)}. Thus, we obtain
Q
(
{?(f |e, e?)}; {??(f |e, e?)}
)
=
N?
n=1
Jn?
j=1
In?
i=1
In?
k=i+1
[
Z?1n ?(fj |ei, ek)
pall (fj |e
In
1 )
? (3)
log
(
Z?1n ??(fj |ei, ek)
)
]
,
where Zn is defined as in Eq. 2. Using the
method of Lagrangian multipliers for the normaliza-
tion constraint, we take the derivative with respect to
374
??(f |e, e?) and obtain:
??(f |e, e?) =
A(f, e, e?)
?
f ? A(f
?, e, e?)
(4)
where A(f, e, e?) is a relative weight accumulator
over the parallel corpus:
A(f, e, e?) =
N?
n=1
Jn?
j=1
?(f, fj)
Z?1n ?(f |e, e
?)
pall (fj |e
In
1 )
Cn(e, e
?) (5)
and
Cn(e, e
?) =
In?
i=1
In?
k=i+1
?(e, ei)?(e
?, ek).
The function ?(?, ?) denotes the Kronecker delta.
The resulting training procedure is analogous to the
one presented in (Brown et al, 1993) and (Tillmann
and Ney, 1997).
The next section presents variants of the ba-
sic unconstrained model by putting restrictions on
the valid regions of triggers (in-phrase vs. out-of-
phrase) and using alignments obtained from either
GIZA++ training or forced alignments in order to
reduce the model size and to incorporate knowledge
already obtained in previous training steps.
3.2 Model variations
Based on the unconstrained triplet model presented
in Section 3, we introduce additional constraints,
namely the phrase-bounded and the path-aligned
triplet model in the following. The former reduces
the number of possible triplets by posing constraints
on the position of where valid triggers may originate
from. In order to obtain phrase boundaries on the
training data, we use forced alignments, i.e. translate
the whole training data by constraining the transla-
tion hypotheses to the target sentences of the training
corpus.
Path-aligned triplets use an alignment constraint
from the word alignments that are trained with
GIZA++. Here, we restrict the first trigger pair (f, e)
to the alignment path as based on the alignment ma-
trix produced by IBM model 4.
These variants require information in addition to
the bilingual sentence pair (fJ1 , e
I
1), namely a corre-
sponding phrase segmentation ? = {piij} with
piij =
{
1 ? a phrase pair that covers ei and fj
0 otherwise
for the phrase-bounded method and, similarly, a
word alignment A = {aij} where
aij =
{
1 if ei is aligned to fj
0 otherwise
.
3.2.1 Phrase-bounded triplets
The phrase-bounded triplet model (referred to as
pphr in the following), restricts the first trigger e to
the same phrase as f , whereas the second trigger e?
is set outside the phrase, resulting in
pphr (fj |e
I
1,?) =
1
Zj
I?
i=1
I?
k=1
piij(1 ? pikj)?(fj |ei, ek). (6)
3.2.2 Path-aligned triplet
The path-aligned triplet model (denoted by palign
in the following), restricts the scope of e to words
aligned to f by A, yielding:
palign(fj |e
I
1, A) =
1
Zj
I?
i=1
I?
k=1
aij?(fj |ei, ek) (7)
where the Zj are, again, the appropriate normaliza-
tion terms.
Also, to account for non-aligned words (analo-
gously to the IBM models), the empty word e0 is
considered in all three model variations. We show
the effect of the empty word in the experiments (Sec-
tion 4). Furthermore, we can train the presented
models in the inverse direction, i.e. p(e|f, f ?), and
combine the two directions in the rescoring frame-
work. The next section presents a set of experiments
that evaluate the performance of the presented triplet
model and its variations.
4 Experiments
In this section, we describe the system setup used in
this work, including the translation tasks and the cor-
responding training corpora. The experiments are
based on an n-best list reranking framework.
375
4.1 System
The experiments were carried out using a state-of-
the-art phrase-based SMT system. The dynamic
programming beam search decoder uses several
models during decoding by combining them log-
linearly. We incorporate phrase translation and word
lexicon models in both directions, a language model,
as well as phrase and word penalties including a
distortion model for the reordering. While gener-
ating the hypotheses, a word graph is created which
compactly represents the most likely translation hy-
potheses. Out of this word graph, we generate n-
best lists and use them to test the different setups as
described in Section 3.
In the experiments, we use 10,000-best lists con-
taining unique translation hypotheses, i.e. duplicates
generated due to different phrase segmentations are
reduced to one single entry. The advantage of this
reranking approach is that we can directly test the
obtained models since we already have fully gener-
ated translations. Thus, we can apply the triplet lex-
icon model based on p(f |e, e?) and its inverse coun-
terpart p(e|f, f ?) directly. During decoding, since e?
could be from anywhere outside the current phrase,
i.e. even from a part which lies beyond the current
context which has not yet been generated, we would
have to apply additional constraints during training
(i.e. make further restrictions such as i? < i for a
trigger pair (ei, ei?)).
Optimization of the model scaling factors is car-
ried out using minimum error rate training (MERT)
on the development sets. The optimization criterion
is 100-BLEU since we want to maximize the BLEU
score.
4.2 Tasks
4.2.1 IWSLT
For the first part of the experiments, we use
the corpora that were released for the IWSLT?07
evaluation campaign. The training corpus con-
sists of approximately 43K Chinese-English sen-
tence pairs, mainly coming from the BTEC cor-
pus (Basic Travel Expression Corpus). This is a
multilingual speech corpus which contains tourism-
related material, such as transcribed conversations
about making reservations, asking for directions or
conversations as taking place in restaurants. For the
experiments, we use the clean data track, i.e. tran-
scriptions of read speech. As the development set
which is used for tuning the parameters of the base-
line system and the reranking framework, we use
the IWSLT?04 evaluation set (500 sentence pairs).
The two blind test sets which are used to evaluate
the final performance of the models are the official
evaluation sets from IWSLT?05 (506 sentences) and
IWSLT?07 (489 sentences).
The average sentence length of the training cor-
pus is 10 words. Thus, the task is somewhat lim-
ited and very domain-specific. One of the advan-
tages of this setting is that preliminary experiments
can be carried out quickly in order to analyze the ef-
fects of the different models in detail. This and the
small vocabulary size (12K entries) makes the cor-
pus ideal for first ?rapid application development?-
style setups without having to care about possible
constraints due to memory requirements or CPU
time restrictions.
4.2.2 EPPS
Furthermore, additional experiments are based on
the EPPS corpus (European Parliament Plenary Ses-
sions) as used within the FTE (Final Text Edition)
track of the TC-STAR evaluations. The corpus con-
tains speeches held by politicians at plenary sessions
of the European Parliament that have been tran-
scribed, ?corrected? to make up valid written texts
and translated into several target languages. The lan-
guage pairs considered in the experiments here are
Spanish-English and English-Spanish.
The training corpus consists of roughly 1.3M sen-
tence pairs with 35.5M running words on the En-
glish side. The vocabulary sizes are considerably
larger than for the IWSLT task, namely around 170K
on the target side. As development set, we use
the development data issued for the 2006 evaluation
(1122 sentences), whereas the two blind test sets are
the official evaluation data from 2006 (TC-Star?06,
1117 sentences) and 2007 (TC-Star?07, 1130 sen-
tences).
4.3 Results
4.3.1 IWSLT experiments
One of the first questions that arises is how many
EM iterations should be carried out during training
of the triplet model. Since the IWSLT task is small,
376
 56.6
 56.8
 57
 57.2
 57.4
 57.6
 0  10  20  30  40  50 34.6
 34.8
 35
 35.2
 35.4
 35.6
BL
EU
 sc
ore
TE
R s
cor
e
EM iterations
IWSLT?04 BLEUIWSLT?04 TER
Figure 2: Effect of EM iterations on IWSLT?04, left axis
shows BLEU (higher numbers better), right axis (dashed
graph) shows TER score (lower numbers better).
IWSLT?04 IWSLT?05
BLEU TER BLEU TER
baseline 56.7 35.49 61.1 30.59
pall(e|f, f ?) 57.1 35.03 61.3 30.55
w/ singletons 57.3 35.04 61.3 30.61
w/ empties 57.3 35.00 61.2 30.65
+ pall(f |e, e?) 57.5 34.69 61.7 30.24
Table 1: Different setups showing the effect of singletons
and empty words for IWSLT CE IWSLT?04 (dev) and
IWSLT?05 (test) sets, pall triplets, 20 EM iterations.
we can quickly run the experiments on a full uncon-
strained triplet model without any cutoff or further
constraints. Figure 2 shows the rescoring perfor-
mance for different numbers of EM iterations. The
first 10 iterations significantly improve the triplet
model performance for the IWSLT task. After that,
there are no big changes. The performance even de-
grades a little bit after 30 iterations. For the IWSLT
task, we therefore set a fixed number of 20 EM iter-
ations for the following experiments since it shows a
good performance in terms of both BLEU and TER
score. The oracle TER scores of the 10k-best lists
are 14.18% for IWSLT?04, 11.36% for IWSLT?05
and 18.85% for IWSLT?07, respectively.
The next chain of experiments on the IWSLT task
investigates the impact of changes to the setup of
training an unconstrained triplet model, such as the
addition of the empty word and the inclusion of sin-
gletons (i.e. triplets that were only seen once in the
IWSLT?05 IWSLT?07
BLEU TER BLEU TER
baseline 61.1 30.59 38.9 45.60
IBM model 1 61.5 30.29 39.4 45.31
trip fe+ef pall 61.7 30.24 39.7 45.24
trip fe+ef pphr 61.5 30.32 39.1 45.36
trip fe+ef palign 61.2 30.60 39.7 45.02
Table 2: Comparison of triplet variants on IWSLT CE test
sets, 20 EM iterations, with singletons and empty words.
training data). This might show the importance of
rare events in order to derive strategies when mov-
ing to larger tasks where it is not feasible to train all
possible triplets, such as e.g. on the EPPS task (as
shown later) or the Chinese-English NIST task. The
results for the unconstrained model are shown in Ta-
ble 1, beginning with a full triplet model in reverse
direction, pall (e|f, f ?), that contains no singletons
and no empty words for the triggering side. In this
setting, singletons seem to help on dev but there is no
clear improvement on one of the test sets, whereas
empty words do not make a significant difference but
can be used since they do not harm either. The base-
line can be improved by +0.6% BLEU and around
-0.5% in TER on the IWSLT?04 set. For the vari-
ous setups, there are no big differences in the TER
score which might be an effect of optimization on
BLEU. Therefore, for further experiments using the
constraints from Section 3.2, we use both singletons
and empty words as the default.
Adding the other direction p(f |e, e?) results in an-
other increase, with a total of +0.8% BLEU and
-0.8% TER, which shows that the combination of
both directions helps overall translation quality. The
results on the two test sets are shown in Table 2.
As can be seen, we arrive at similar improvements,
namely +0.6% BLEU and -0.3% TER on IWSLT?05
and +0.8% BLEU and -0.4% TER on IWSLT?07, re-
spectively. The constrained models, i.e. the phrase-
bounded (pphr ) and path-aligned (palign ) triplets are
outperformed by the full unconstrained case, al-
though on IWSLT?07 both unconstrained and path-
aligned models are close.
For a fair comparison, we added a classical IBM
model 1 in the rescoring framework. It can be seen
that the presented triplet models slightly outperform
377
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 52.3 34.57 50.4 36.46
trip fe+ef pall 52.9 34.32 50.6 36.34
+ max dist 10 52.9 34.20 50.8 36.22
Table 3: Effect of using maximum distance constraint for
pall on EPPS Spanish-English test sets, occ3, 4 EM iter-
ations due to time constraints.
the simple IBM model 1. Note that IBM model 1
is a special case of the triplet lexicon model if the
second trigger is the empty word.
4.3.2 EPPS experiments
Since EPPS is a considerably harder task (larger
vocabulary and longer sentences), the training of a
full unconstrained triplet model cannot be done due
to memory restrictions. One possibility to reduce
the number of extracted triplets is to apply a max-
imum distance constraint in the training procedure,
i.e. only trigger pairs are considered where the dis-
tance between first and second trigger is below or
equal to the specified maximum.
Table 3 shows the effect of a maximum distance
constraint for the Spanish-English direction. Due
to the large amount of triplets (we extract roughly
two billion triplets2 for the EPPS data), we drop all
triplets that occur less than 3 times which results in
640 million triplets. Also, due to time restrictions3,
we only train 4 iterations and compare it to 4 itera-
tions of the same setting with the maximum distance
set to 10. The training with the maximum distance
constraints ends with a total of 380 million triplets.
As can be seen (Table 3), the performance is compa-
rable while cutting down the computation time from
9.2 to 3.1 hours. The experiments were carried out
on a 2.2GHz Opteron machine with 16 GB of mem-
ory. The overall gain is +0.4?0.6% BLEU and up to
-0.4% in TER. We even observe a slight increase in
BLEU for the TC-Star?07 set which might be a ran-
dom effect due to optimization on the development
set where the behavior is the same as for TC-Star?06.
2Extraction can be easily done in parallel by splitting the
corpus and merging identical triplets iteratively in a separate
step for two chunks at a time.
3One iteration needs more than 12 hours for the uncon-
strained case.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
trip fe+ef pphr 50.2 37.01 51.5 35.38
+ occ2 50.2 37.06 51.8 35.32
Table 4: Results on EPPS, English-Spanish, pphr com-
bined, occ3, 10 EM iterations.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
using FA 50.0 37.18 51.7 35.52
using IBM4 50.0 37.12 51.7 35.43
+ occ2 50.2 36.84 52.0 35.10
+ max dist 1 50.0 37.10 51.7 35.51
Table 5: Results on EPPS, English-Spanish, maximum
approximation, palign combined, occ3, 10 EM iterations.
Results on EPPS English-Spanish for the phrase-
bounded triplet model are presented in Table 4.
Since the number of triplets is less than for the un-
constrained model, we can lower the cutoff from 3
to 2 (denoted in the table by occ3 and occ2 , respec-
tively). There is a small additional gain on the TC-
Star?07 test set by this step, with a total of +0.7%
BLEU for TC-Star?06 and +0.8% BLEU for TC-
Star?07.
Table 5 shows results for a variation of the path-
aligned triplet model palign that restricts the first trig-
ger to the best aligned word as estimated in the IBM
model 1, thus using a maximum-approximation of
the given word alignment. The model was trained
on two word alignments, firstly the one contained in
the forced alignments on the training data, and sec-
ondly on an IBM-4 word alignment generated using
GIZA++. For this second model we also demon-
strate the improvement obtained when increasing the
triplet lexicon size by using less trimming.
Another experiment was carried out to investigate
the effect of immediate neighboring words used as
triggers within the palign setting. This is equivalent
to using a ?maximum distance of 1? constraint. We
obtained worse results, namely a 0.2-0.3% drop in
BLEU and a 0.3-0.4% raise in TER (cf. Table 5,
last row), although the training is significantly faster
with this setup, namely roughly 30 minutes per it-
378
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
IBM model 1 50.0 37.12 51.8 35.51
pall , occ3 50.0 37.17 51.8 35.43
pphr , occ2 50.2 37.06 51.8 35.32
palign , occ2 50.2 36.84 52.0 35.10
Table 6: Final results on EPPS English-Spanish, con-
strained triplet models, 10 EM iterations, compared to
standard IBM model 1.
eration using less than 2 GB of memory. However,
this shows that triggers outside the immediate con-
text help overall translation quality. Additionally, it
supports the claim that the presented methods are a
complementary alternative to the WSD approaches
mentioned in Section 2 which only consider the im-
mediate context of a single word.
Finally, we compare the constrained models to an
unconstrained setting and, again, to a standard IBM
model 1. Table 6 shows that the palign model con-
strained on using the IBM-4 word alignments yields
+0.7% in BLEU on TC-Star?06 which is +0.2%
more than with a standard IBM model 1. TER de-
creases by -0.3% when compared to model 1. For
the TC-Star?07 set, the observations are similar.
The oracle TER scores of the development n-best
list are 25.16% for English-Spanish and 27.0% for
Spanish-English, respectively.
5 Discussion
From the results of our reranking experiments, we
can conclude that the presented triplet lexicon model
outperforms the baseline single-best hypotheses of
the decoder. When comparing to a standard IBM
model 1, the improvements are significantly smaller
though measurable. So far, since IBM model 1
is considered one of the stronger rescoring mod-
els, these results look promising. An unconstrained
triplet model has the best performance if training is
feasible since it also needs the most memory and
time to be trained, at least for larger tasks.
In order to cut down computational requirements,
we can apply phrase-bounded and path-aligned
training constraints that restrict the possibilities of
selecting triplet candidates (in addition to simple
f e e? ?(f |e, e?)
pagar taxpayer bill 0.76
factura taxpayer bill 0.11
contribuyente taxpayer bill 0.10
f e ? pibm1 (f |e)
contribuyente taxpayer 0.40
contribuyentes taxpayer 0.18
europeo taxpayer 0.08
factura bill 0.19
ley bill 0.18
proyecto bill 0.11
Table 7: Example of triplets and related IBM model 1
lexical probabilities. The triggers ?taxpayer? and ?bill?
have a new effect (?pagar?), previously not seen in the
top ranks of the lexicon.
thresholding). Although no clear effect could be
observed for adding empty words on the trigger-
ing side, it does not harm and, thus, we get a sim-
ilar functionality to IBM model 1 being ?integrated?
in the triplet lexicon model. The phrase-bounded
training variant uses forced alignments computed
on the whole training data (i.e. search constrained
to producing the target sentences of the bilingual
corpus) but could not outperform the path-aligned
model which reuses the alignment path information
obtained in regular GIZA++ training.
Additionally, we observe a positive impact from
triggers lying outside the immediate context of one
predecessor or successor word.
5.1 Examples
Table 7 shows an excerpt of the top entries for
(e, e?) = (taxpayer , bill) and compares it to the top
entries of a lexicon based on IBM model 1. We ob-
serve a triggering effect since the Spanish word pa-
gar (to pay) is triggered at top position by the two
English words taxpayer and bill. The average dis-
tance of taxpayer and bill is 5.4 words. The models
presented in this work try to capture this property
and apply it in the scoring of hypotheses in order to
allow for better lexical choice in specific contexts.
In Table 8, we show an example translation where
rescoring with the triplet model achieves higher n-
gram coverage on the reference translation than the
variant based on IBM model 1 rescoring. The differ-
ing phrases are highlighted.
379
Source sen-
tence
. . . respecto de la Posicio?n Comu?n
del Consejo con vistas a la adopcio?n
del Reglamento del Parlamento Eu-
ropeo y del Consejo relativo al . . .
IBM-1
rescoring
. . . on the Council common position
with a view to the adoption of the
Rules of Procedure of the European
Parliament and of the Council . . .
Triplet
rescoring
. . . on the common position of the
Council with a view to the adop-
tion of the regulation of the Euro-
pean Parliament and of the Council
. . .
Reference
translation
. . . as regards the Common Position
of the Council with a view to the
adoption of a European Parliament
and Council Regulation as regards
the . . .
Table 8: A translation example on TC-Star?07 Spanish-
English comparing the effect of the triplet model to a
standard IBM-1 model.
6 Outlook
We have presented a new lexicon model based on
triplets extracted on a sentence level and trained it-
eratively using the EM algorithm. The motivation of
this approach is to add an additional second trigger
to a translation lexicon component which can come
from a more global context (on a sentence level) and
allow for a more fine-grained lexical choice given a
specific context. Thus, the method is related to word
sense disambiguation approaches.
We showed improvements by rescoring n-best
lists of the IWSLT Chinese-English and EPPS
Spanish-English/English-Spanish task. In total, we
achieve up to +1% BLEU for some of the test sets in
comparison to the decoder baseline and up to +0.3%
BLEU compared to IBM model 1.
Future work will address an integration into the
decoder since the performance of the current rescor-
ing framework is limited by the quality of the n-
best lists. For the inverse model, p(e|f, f ?), an in-
tegration into the search is directly possible. Further
experiments will be conducted, especially on large
tasks such as the NIST Chinese-English and Arabic-
English task. Training on these huge databases will
only be possible with an appropriate selection of
promising triplets.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
The authors would like to thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL 2007),
Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33?40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stephen A. Della Pietra, Vincent J. Della Pietra, John R.
Gillett, John D. Lafferty, Harry Printz, and Lubos?
Ures?. 1994. Inference and estimation of a long-range
trigram model. In J. Oncina and R. C. Carrasco, ed-
itors, Grammatical Inference and Applications, Sec-
ond International Colloquium, ICGI-94, volume 862,
pages 78?92, Alicante, Spain. Springer Verlag.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?22.
Woosung Kim and Sanjeev Khudanpur. 2003. Cross-
lingual lexical triggers in statistical language model-
ing. In Proceedings of the 2003 Conference on Empir-
ical Methods in Natural Language Processing, pages
17?24, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
380
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10(3):187?228.
Christoph Tillmann and Hermann Ney. 1997. Word trig-
gers and the EM algorithm. In Proc. Special Interest
Group Workshop on Computational Natural Language
Learning (ACL), pages 117?124, Madrid, Spain, July.
Christoph Tillmann. 2001. Word Re-Ordering and Dy-
namic Programming based Search Algorithm for Sta-
tistical Machine Translation. Ph.D. thesis, RWTH
Aachen University, Aachen, Germany, May.
381
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 152?161,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Does more data always yield better translations?
Guillem Gasco?, Martha-Alicia Rocha, Germa?n Sanchis-Trilles,
Jesu?s Andre?s-Ferrer and Francisco Casacuberta
Departament de Sistemes Informa`tics i Computacio?
Universitat Polite`cnica de Vale`ncia
Cam?? de Vera s/n, 46022 Vale`ncia, Spain
{ggasco,mrocha,gsanchis,jandres,fcn}@dsic.upv.es
Abstract
Nowadays, there are large amounts of data
available to train statistical machine trans-
lation systems. However, it is not clear
whether all the training data actually help
or not. A system trained on a subset of such
huge bilingual corpora might outperform
the use of all the bilingual data. This paper
studies such issues by analysing two train-
ing data selection techniques: one based
on approximating the probability of an in-
domain corpus; and another based on in-
frequent n-gram occurrence. Experimental
results not only report significant improve-
ments over random sentence selection but
also an improvement over a system trained
with the whole available data. Surprisingly,
the improvements are obtained with just a
small fraction of the data that accounts for
less than 0.5% of the sentences. After-
wards, we show that a much larger room for
improvement exists, although this is done
under non-realistic conditions.
1 Introduction
Globalisation and the popularisation of the Inter-
net have lead to a rapid increase in the amount of
bilingual corpora available. Entities such as the
European Union, the United Nations and other
multinational organisations need to translate all
the documentation they generate. Such transla-
tions happen every day and provide very large
multilingual corpora, which are oftentimes diffi-
cult to process and significantly increase the com-
putational requirements needed to train statistical
machine translation (SMT) systems. For instance,
the corpora made available for recent machine
translation evaluations are in the order of 1 billion
running words (Callison-Burch et al 2010).
However, two main problems arise when at-
tempting to use this huge pool of sentences for
training SMT systems: firstly, a large portion of
this data is obtained from domains that differ from
that in which the SMT system is to be used or as-
sessed; secondly, the use of all this data for train-
ing the system increases the computational train-
ing requirements. Despite the previous remarks,
the de facto standard consists in training SMT sys-
tems with all the available data. This is due to
the widespread misconception that the more data
a system is trained with, the better its performance
should be. Although the previous statement is the-
oretically true if all the data belongs to the same
domain, this is not the case in the problems tack-
led by most of the SMT systems. For instance,
enterprises often need to build on-demand sys-
tems (Yuste et al 2010). In this case, since we
are interested in translating some specific text, it
is not clear whether training a system with all data
yields better performance than training it with a
wisely selected subset of bilingual sentences.
The bilingual sentence selection (BSS) task is
stated as the problem of selecting the best sub-
set of bilingual sentences from an available pool
of sentences, with which to train a SMT system.
This paper is concerned to BSS, and mainly two
ideas are developed.
On the one hand, two BSS strategies that at-
tempt to build better translation systems are anal-
ysed. Such strategies are able to improve state-of-
the-art translation quality without the very high
computational resources that are required when
using the complete pool of sentences. Both tech-
niques span through two orthogonal criteria when
selecting bilingual sentences from the available
pool: avoiding to introduce a bias in the original
data distribution, and increasing the informative-
ness of the corpus.
On the other hand, we prove that among all pos-
sible subsets from the sentence pool, there is at
least a small one that yields large improvements
(up to 10 BLEU points) with respect to a system
trained with all the data. In order to retrieve such
subset, we had to use an oracle that employs infor-
mation extracted from the reference translations
152
only for the purpose of selecting bilingual sen-
tences. However, references are not used at any
stage within the translation system for obtaining
the hypotheses. Note that although we are not
able to achieve such an improvement without an
oracle, this result restates the BSS problem as an
interesting approach not only for reducing com-
putational effort but also for significantly boost-
ing performance. To our knowledge, no previous
work has quantified the room of improvement in
which BSS techniques could incur.
In order to assess the performance of the dif-
ferent BSS techniques, translation results are ob-
tained by using a standard state-of-the-art SMT
system (Koehn et al 2007). The most recent lit-
erature defines the SMT problem (Papineni et al
1998; Och and Ney, 2002) as follows: given an
input sentence f from a certain source language,
the purpose is to find an output sentence e? in a
certain target language such that
e? = argmax
e
K?
k=1
?khk(f , e) (1)
where hk(f , e) is a score function representing an
important feature for the translation of f into e,
as for example the language model of the target
language, a reordering model or several transla-
tion models. ?k are the log-linear combination
weights.
The main contributions of this paper are:
? A BSS technique is analysed, which im-
proves the results obtained with a random
bilingual sentence selection strategy when
the specific domain to be translated signifi-
cantly differs from that of the pool of sen-
tences.
? Another BSS technique is analysed that, us-
ing less than 0.5% of the sentences avail-
able, significantly improves over random se-
lection, beating a system trained with all the
pool of sentences.
? We prove, by means of an oracle, that a wise
BSS technique can yield large improvements
when compared with systems trained with all
data available.
The remaining of the paper is structured as fol-
lows. Section 2 summarises the related work.
Sections 3 and 4 present two BSS techniques,
namely, probabilistic sampling and recovery of
infrequent n-grams. In Section 5 experimental re-
sults are reported. Finally, the main results of the
work and several future work directions are dis-
cussed in Section 6.
2 Related Work
Training data selection has been receiving an in-
creasing amount of attention within the SMT
community. For instance, in (Li et al 2010;
Gasco? et al 2010) several BSS techniques, sim-
ilar to those analysed in this paper, have been
applied for training MT systems when there are
large training corpora available. However, nei-
ther such techniques have been formalised, nor its
performance thoroughly analysed. A similar ap-
proach that gives weights to different subcorpora
was proposed in (Matsoukas et al 2009).
In (Lu et al 2007), information retrieval meth-
ods are used in order to produce different sub-
models which are then weighted according to the
sentence to be translated. In such work, authors
define the baseline as the result obtained train-
ing only with the corpus that share the same do-
main of the test. Afterwards they claim that they
are able to improve baseline translation quality by
adding new sentences retrieved with their method.
However, they neither compare their technique
with random sentence selection, nor with a model
trained with all the corpora.
Although the techniques that are applied for
BSS are often very similar to those applied for ac-
tive learning (AL), both problems are essentially
different. Since the AL strategies assume that
the pool of sentences are not translated, they are
usually interested in finding the best monolingual
subset of sentences to be translated by a human
annotator. In contrast, in BSS, it is assumed that a
fairly large amount of bilingual corpora is readily
available, and the main goal consists in selecting
only those sentences which will maximise system
performance.
Some works have applied sentence selection in
small scale AL frameworks. These works extend
the training corpora at most with 5000 sentences.
In (Ananthakrishnan et al 2010), sentences are
selected by means of discriminative techniques.
In (Haffari et al 2009) a technique is proposed
for increasing the counts of phrases that are con-
sidered infrequent. Both works significantly dif-
fer from the current work not only on the frame-
work, but also on the scale of the experiments, the
153
proposed techniques and the obtained improve-
ments. Similar ideas applied to adaptation prob-
lems have been proposed in (Moore and Lewis,
2010; Axelrod et al 2011).
3 Probabilistic Sampling
As discussed in Section 2, BSS has inherently
attached many meaningful links with AL tech-
niques. Selecting samples for learning our mod-
els, incurs in a well-known difficulty in AL, the
so-called sample bias problem (Dasgupta, 2009).
This problem, which is spread to the BSS case,
is summarised as the distortion introduced by the
active strategy into the probability distribution un-
derlying the training corpus. This bias forces the
training algorithm to learn a distorted probability
model which can significantly differ from the ac-
tual one.
In order to further analyse the sampling bias
problem, consider the maximum likelihood esti-
mation (MLE) of a probability model, p?(e, f)
for a given corpus of N data points,{(en, fn)},
sampled from the actual probability distribution,
Pr(e, f). Recall that e denotes a target sen-
tence whereas f stands for its source counter-
part. MLE techniques aims at minimising the
Kullback-Leibler divergence between the actual
unknown probability distribution and the proba-
bility model (Bishop, 2006), defined as
KL(Pr | p?) =
?
e,f
Pr(e, f) log
(
Pr(e, f)
p?(e, f)
)
(2)
When minimising, Eq. (2) is simplified to
?? = argmax
?
?
e,f
Pr(e, f) log(p?(e, f)) (3)
which is approximated by a sufficiently large
dataset under the commonly hold assumption that
it is independently and identically distributed ac-
cording to Pr(e, f) as
?? = argmax
?
?
n
log(p?(en, fn)) (4)
Therefore, by perturbing the sample {(en, fn)}
with an active strategy, we are, in fact, modifying
the approximation to Eq.(3) and learning a differ-
ent underlying probability distribution.
In this section a statistical framework is pro-
posed to build systems with BSS while avoiding
the sample bias. The proposed approach relies in
conserving the probability distribution of the task
domain by wisely selecting the bilingual pairs to
be used from the whole pool of sentences. Hence,
it is mandatory to exclude sentences from the pool
that distort the actual probability. In order to ap-
proximate the probability distribution, we assume
that a small but representative corpus is avail-
able from the task domain. This corpus, referred
henceforth as the in-domain corpus, provides a
way to build an initial model which approximates
the actual probability of the system. The pool of
sentences will be oppositely denoted as the out-
of-domain corpus.
The actual probability of the task domain, the
so called in-domain probability, is approximated
with the following model
p(e, f , |e|, |f |) = p(e, f | |e|, |f |) ? p(|e|, |f |) (5)
where p(|e|, |f |) denotes the in-domain length
probability, and p(e, f | |e|, |f |) the in-domain
bilingual probability.
The length probability is estimated by MLE
p(|e|, |f |) =
N(|e|+ |f |)
N
(6)
where N(|e|+|f |) is the number of bilingual pairs
in the in-domain corpus such that their lengths
sum up to |e|+|f | and N denotes the total num-
ber of sentences. Note that no distinction is made
between source and target lengths since the model
is intended for sampling.
The complexity of the in-domain bilingual
probability distribution, p(e, f | |e|, |f |), requires
a more sophisticated approximation
p(e, f/|e|, |f |) =
exp(
?
k ?kfk(e, f))
Z
(7)
being Z a normalisation constant; and where
fk(. . .) and ?k are the features of the model and
their respective parametric weights. Specifically,
four logarithmic features were considered for this
sampling technique: a direct and an inverse IBM
model 4 (Brown et al 1994); and both, source
and target, 5-gram language models. All fea-
ture models are estimated in the in-domain cor-
pus with standard techniques (Brown et al 1994;
Stolcke, 2002). As a first approach, the parame-
ters of the log-linear model in Eq. (7), ?k, were
uniformly fixed to 1.
154
Once we have an appropriate model for the
in-domain probability distribution, the proposed
method randomly samples a given number of
bilingual pairs from the out-of-domain corpora
(the pool of sentences). The process of extend-
ing the in-domain corpus with additional bilin-
gual pairs from the out-of-domain corpus is sum-
marised as follows:
? Decide according to the in-domain length
probability in Eq. (6), how many samples
should be drawn for each length, i.e. divide
the number of sentences to add into length
dependent buckets.
? Randomly draw the number of samples
specified in each bucket according to the
in-domain bilingual probability in Eq. (7)
among all the bilingual sentences that share
the current bucket length.
Although the pool of sentences is typically
large, it is not large enough to gather a signifi-
cant amount of probability mass. Consequently,
a small set of sentences accumulate most of the
probability mass and tend to be selected multi-
ple times. To avoid this awkward and undesired
behaviour, the sampling is performed without re-
placement.
4 Infrequent n-gram Recovery
Another criterion when confronting the BSS task
is to increase the informativeness of the training
set. Thus, it seems important to choose sentences
that provide information not seen in the training
corpus. Note that this criterion is sometimes op-
posed to the one presented in Section 3.
The performance of phrase-based machine
translation systems strongly relies in the quality
of the phrases extracted from the training sam-
ples. In most of the cases, the inference of such
phrases or rules is based on word alignments,
which cannot be computed accurately when ap-
pearing rarely in the training corpus. The extreme
case are the out-of-vocabulary words: words that
do not appear in the training set, cannot be trans-
lated. Moreover, this problem can be extended to
sequences of words (n-grams). Consider a 2-gram
fifj appearing few or no times in the training set.
Although fi and fj may appear separately in the
training set, the system might not be able to in-
fer the translation of the 2-gram fifj , which may
be different from the concatenation of the transla-
tions of both words separately.
When selecting sentences from the pool it is
important to choose sentences that contain n-
grams that have never been seen (or have been
seen just a few times) in the training set. Such
n-grams will be henceforth referred to as infre-
quent n-grams . An n-gram is considered infre-
quent when it appears less times than an infre-
quent threshold t. If the source language sen-
tences to be translated are known beforehand, the
set of infrequent n-grams can be reduced to those
present in such sentences. Then, the technique
consists in selecting from the pool those sentences
which contain infrequent n-grams present in the
source sentences to be translated.
Sentences in the pool are sorted by their infre-
quency score in order to select first the most in-
formative. Let X the set of n-grams that appear
in the sentences to be translated and w one of
them; C(w) the counts of w in the source lan-
guage training set; and N(w) the counts of w
in the source sentence f to be scored. The infre-
quency score of f is:
i(f) =
?
w?X
min(1, N(w))max(0, t?C(w)) (8)
In order to avoid giving a high score to noisy
sentences with a lot of occurrences of the same in-
frequent n-gram, only one occurrence of each n-
gram is taken into account to compute the score.
In addition, the score gives more importance to
the n-grams with lowest counts in the training
set. Although it could be possible to select the
highest scored sentences, we updated the scores
each time a sentence is selected. This decision
was taken to avoid the selection of too many sen-
tences with the same infrequent n-gram. First,
sentences in the pool are scored using Equation
(8). Then, in each iteration, the sentence f? with
the highest score is selected, added to the training
set and removed from the pool. In addition, the
counts of the n-grams present in f? are updated
and, hence, the scores of the rest of the sentences
in the pool. Since rescoring the whole pool would
incur in a very high computational cost, a subop-
timal search strategy was followed, in which the
search was constrained to a given set of highest
scoring sentences. Here it was set to one million.
155
t = 1 t = 10 t = 25
tr all tr all tr all
1-gr 11.6 1.3 40.5 3.5 59.9 5.1
2-gr 38 9.8 73.2 21.3 84.9 27.9
3-gr 66.8 33.5 91.1 55.7 96.4 64.9
4-gr 87.1 65.8 98.2 85.5 99.4 90.7
Table 1: Percentage of infrequent n-grams in the TED
test set when considering only the TED training set
(tr), and when adding the out-of-domain pool (all),
for different infrequency thresholds t.
Table 1 shows the percentage of source lan-
guage infrequent n-grams for the test of a rela-
tively small corpus such as the TED corpus (for
details see Section 5) when considering just the
in-domain training set (? 40K sentences) and the
same percentage when adding the larger out of do-
main corpora. The percentages in the table have
been computed separately for different values of
the threshold t and for n-grams of order from 1 to
4. Note that the reduction in the number of infre-
quent n-grams is very high for the 1-grams but de-
creases progressively when considering n-grams
of higher order. This indicates that the infrequent
n-grams recovery technique should be very effec-
tive for lower order n-grams, but might have less
effect for higher order n-grams. Therefore, and
in order to lower the computational cost involved,
the experiments carried out for this paper were
performed considering only infrequent 1-grams,
2-grams and 3-grams.
5 Experiments
In the present Section, we first describe the exper-
imental framework employed to assess the perfor-
mance of the BSS techniques described. Then, re-
sults for the probabilistic sentence selection strat-
egy are shown, followed by results obtained with
the infrequent n-grams technique. Some exam-
ple translations are shown and, finally, we also
report experiments using the infrequent n-grams
technique in Oracle mode, in order to establish
the potential improvement for such technique and
for BSS in general.
5.1 Experimental Setup
All experiments were carried out using the
open-source SMT toolkit Moses (Koehn et al
2007), in its standard non-monotonic configura-
tion. The phrase tables were generated by means
of symmetrised word alignments obtained with
Subset Language |S| |W | |V |
train
English
47.5K
747K 24.6K
French 793K 31.7K
dev
English
571
9.2K 1.9K
French 10.3K 2.2K
test
English
641
12.6K 2.4K
French 12.8K 2.7K
Table 2: TED corpus main figures. K denotes thou-
sands of elements. |S| stands for number of sentences,
|W | for number of running words, and |V | for vocab-
ulary size.
Subset Language |S| |W | |V |
train
English
77.2K
1.71M 29.9K
French 1.99M 48K
dev 08
English
2.1K
49.8K 8.7K
French 55.4K 7.7K
test 09
English
2.5K
65.6K 8.9K
French 72.5K 10.6K
test 10
English
2.5K
62K 8.9K
French 70.5K 10.3K
Table 3: News Commentary corpus main figures.
GIZA++ (Och and Ney, 2003). The language
model used was a 5-gram with modified Kneser-
Ney smoothing (Kneser and Ney, 1995), built
with SRILM toolkit (Stolcke, 2002). The log-
linear combination weights in Eq. (1) were opti-
mised using Minimum Error Rate Training (Och
and Ney, 2002) on the corresponding develop-
ment sets.
Experiments were carried out on two corpora:
TED (Paul et al 2010) and News Commentary
(NC) (Callison-Burch et al 2010). TED is an
English-French corpus composed of subtitles for
a collection of public speeches on a variety of top-
ics. The same partitions as in the IWSLT2010
evaluation task (Paul et al 2010) have been used.
Subtitles have been concatenated into complete
sentences. NC is a slightly larger English-French
corpus in the news domain. Main figures of both
corpora are shown in Tables 2 and 3. As for the
pool of sentences, three large corpora have been
used: Europarl (Euro), United Nations (UN) and
Gigaword (Giga), in the partition established for
the 2010 workshop on SMT of the ACL (Callison-
Burch et al 2010). Sentences of length greater
than 50 have been pruned. Table 4 shows the main
figures of the tokenised and lowercased corpora.
When translating between some language
pairs, there are words that remain invariable, like
for example numbers or punctuation marks in the
case of European languages. In fact, an easy and
156
Corpus Language |S| |W | |V |
Euro
English
1.25M
25.6M 81K
French 28.2M 101K
UN
English
5M
94.4M 302K
French 107M 283K
Giga
English
15.5M
303M 1.6M
French 361M 1.6M
Table 4: Figures of the corpora used as sentence pool.
M stands for millions of elements.
effective technique that is commonly used is to re-
produce out-of-vocabulary words from the source
sentence in the target hypothesis. However, in-
variable n-grams are usually infrequent as well,
which implies that the infrequent n-grams tech-
nique would select sentences containing such n-
grams, even though they do not provide further
information. As a first approach, we exclude n-
grams without any letter.
Baseline experiments have been carried out for
TED and NC corpora using the corresponding
training set. For comparison purposes, we also
included results for a purely random sentence se-
lection without replacement. In the plots, each
point corresponding to random selection represent
the average of 10 repetitions. Experiments using
all data are also reported, although a 64GB ma-
chine was necessary, even with binarized phrase
and distortion tables.
Experiments were conducted by selecting a
fixed amount of sentences according to each one
of the techniques described above. Then, these
sentences were included into the training data and
subsequent SMT systems were built for translat-
ing the test set.
Results are shown in terms of BLEU (Papineni
et al 2001), which is an accuracy metric that
measures n-gram precision, with a penalty for
sentences that are too short. Although it could
be argued that improvements obtained might be
due to a side effect of the brevity penalty, this
was not found to be true: the BSS techniques (in-
cluding random) and considering all data yielded
very similar brevity penalties (?0.005), within
each corpus. In addition, TER scores (Snover et
al., 2006) were also computed, but are omitted
for clarity purposes and since they were found to
be coherent with BLEU. TER is an error metric
that computes the minimum number of edits re-
quired to modify the system hypotheses so that
they match the references translations.
 0
 0.01
 0.02
 0.03
 0  10  20  30  40  50  60  70  80  90  100
Re
lati
ve 
fre
que
ncy
Combined sentence length
EuroparlGigawordUNTEDNC
Figure 2: Combined length relative frequency.
5.2 Results for Probabilistic Sampling
In addition to the probabilistic sampling tech-
nique proposed in Section 3, we also analysed the
effect of sampling only according to the combined
source-reference length, with the purpose of es-
tablishing whether potential improvements were
only due to the length component, or rather to the
complete sampling model. Results for the 2009
test set are shown in Figure 1. Several things
should be noted:
? Performing sentence selection only according
to sentence lengths does not achieve better
performance than random selection.
? Selecting sentences according to probabilis-
tic sampling is able to improve random se-
lection in the case of the TED corpus, but
is not able to do so in the case of the NC
corpus. Significance tests for the 500K case
reported that the differences were significant
in the case of the TED corpus, but not in the
case of the NC corpus.
? In the case of the TED corpus, the perfor-
mance achieved with the system built by
sampling 500K sentences is only 0.5 BLEU
points below the performance achieved by
the system built with all the data available.
The explanation to the fact that probabilistic
sampling is able to improve over random sam-
pling only in the case of the TED corpus, but not
in the case of NC, relies in the nature of the cor-
pora. Although both of them belong to a very
generic domain, their characteristics are very dif-
ferent. In fact, the NC data is very similar to the
sentences in the pool, but, in contrast, the sen-
tences present in the TED corpus have a much
more different structure. This difference is illus-
trated in Figure 2, where the relative frequency of
157
 21
 22
 23
 24
0 100K 200K 300K 400K 500K
BL
EU
Number of sentences added
TED corpus
in domain
all
random
length
sampling
 19
 20
 21
 22
0 100K 200K 300K 400K 500K
BL
EU
Number of sentences added
NC corpus
in domain
all
random
length
sampling
Figure 1: Effect of adding sentences over the BLEU score using the probabilistic sampling, length sampling and
random selection techniques for the two corpora, TED and News Commentary. Horizontal lines represent the
scores when using just the in domain training set and all the data available.
 21
 22
 23
 24
 25
 26
0 50k 100k 200k
BL
EU
Number of sentences added
TED corpus
allt=10 in domaint=25 random
 19
 20
 21
 22
 23
0 50k 100k 200k
BL
EU
Number of sentences added
NC corpus
allin domain
randomt=10t=25
Figure 3: Effect of adding sentences over the BLEU score using the infrequent n-grams (with different thresh-
olds) and random selection techniques for the two corpora, TED and News Commentary. Horizontal lines repre-
sent the scores when using just the in domain training set and all the data available.
each combined sentence length is shown. In this
plot, it stands out clearly that the TED corpus has
a very different length distribution than the other
four corpora considered, whereas the NC corpus
presents a very similar distribution. This implies
that, when considering TED, an intelligent data
selection strategy will have better chances to im-
prove random selection than in the case of NC.
5.3 Results for Infrequent n-grams Recovery
Figure 3 shows the effect of adding sentences us-
ing the infrequent n-grams and the random se-
lection techniques on the 2009 test set. Once
all the infrequent n-grams have been covered
t times, the infrequency score for all the sen-
tences remaining in the pool is 0, and none of
them can be selected. Hence, the number of
sentences that can be selected for each t is lim-
ited. Although for clarity we only show results
for t = {10, 25}, experiments have also been car-
ried out for t = {1, 5, 10, 25}. Such results pre-
sented similar curves, although less sentences can
be selected and hence improvements obtained are
slightly lower. Several conclusions can be drawn:
? The translation quality provided by the in-
frequent n-grams technique is significantly
better than the results achieved with random
selection, comparing similar amount of sen-
tences. Specifically, the improvements ob-
tained are in the range of 3 BLEU points.
? Results for the TED corpus are more irreg-
ular. The best performance is achieved for
t = 25 and 50K sentences added. In NC, the
best result is for t = 10 and 112K.
? Selecting sentences with the infrequent n-
grams technique provides better results than
including all the available data. While using
less than 0.5% of the data, improvements be-
tween 0.5 and 1 BLEU points are achieved.
When looking at Figure 3, one might suspect
that t needs to be set specifically for a given test
158
set, and that results from one set are not to be ex-
trapolated to other test sets. For this reason, we
selected the best configuration in Figure 3 and
used it to build a new system for translating the
unseen NC 2010 test set. Such experiment, with
t = 10 and including all sentences with score
greater than 0 (? 110K), is shown in Table 5 and
evidences that improvements are actually coher-
ent among different test sets.
technique BLEU TER #phrases
in-domain 19.0 65.2 5.1M
all data 22.7 60.8 1236M
infreq. t = 10 23.6 59.2 16.5M
Table 5: Effect of the infrequent n-gram recovery tech-
nique for an unseen test set, when setting t = 10 and
number of phrases (parameters) of the models.
5.4 Oracle Results
In order to analyse the potential of BSS tech-
niques, the infrequent n-grams recovery tech-
nique in Section 4 was implemented in oracle
mode. In this way, sentences from the pool
were selected according to the infrequent n-grams
present in the reference translations of the test set.
Note that test references were not included into
the training data as such, but were rather used
to establish which bilingual sentences within the
pool were best suitable for training the SMT sys-
tem. In this way, we were able to establish the po-
tential for improvement of a BSS technique. In-
terestingly, the SMT system trained in this way
achieved 31 BLEU points on the News Commen-
tary 2009 test set, i.e. an 8 BLEU points improve-
ment over the system trained with all the data
available. This result would have beaten all the
systems that took part in the 2009 Workshop on
Machine translation (Callison-Burch et al 2009).
This result is really important: although we are
aware that the sentences were selected in a non-
realistic manner, it proves that an appropriate BSS
technique would be able to boost SMT perfor-
mance in a very significant manner. Similar re-
sults were obtained with the TED and NC 2010
test sets, with 10 and 7 points improvement, re-
spectively.
5.5 Example Translations
Example translations are shown in Figure 4. In
the first example, the baseline system is not able
Src the budget has also been criticised by klaus .
Bsl le budget a e?galement e?te? criticised par m. klaus .
Rdm le budget a e?galement e?te? critique?es par m. klaus .
PS le budget a e?galement e?te? critique?e par klaus .
All le budget a e?galement e?te? critique? par klaus .
Infr le budget a e?galement e?te? critique? par klaus .
Ref klaus critique e?galement le budget .
Src and one has come from music .
Bsl et un a de la musique .
Rdm et on vient de musique .
PS et on a viennent de musique .
All et de la musique .
Infr et un est venu de la musique .
Ref et un vient du monde de la musique .
Figure 4: Examples of two translations for each of the
SMT systems built: Src (source sentence), Bsl (base-
line), Rdm (random selection), PS (probabilistic sam-
pling), All (all the data available), Infr (Infrequent n-
grams) and Ref (reference).
to translate criticised, which is considered out-of-
vocabulary. Even though random selection is able
to solve this problem (luckily), it does not achieve
to translate it correctly, introducing a concordance
error. A similar thing happens when using prob-
abilistic sampling, where a grammatical error is
also present, and only Infr and All are able
to present a correct translation. This is not only
casual, since, by ensuring that a given n-gram ap-
pears at least a certain number of times t, the odds
of including all possible translations of criticised
are incremented significantly. Note that, even if
the Infr translation is different from the refer-
ence, it is equally correct. In the second example,
the baseline translation is pretty much correct, but
has a different meaning (something like ?and one
has music?). Similarly, when including all data
the translation obtained by the system means ?and
some music?. In this case, both random and prob-
abilistic selection present grammatically incorrect
sentences, and only Infr is able to provide a cor-
rect translation, although pretty literal and differ-
ent from the reference.
6 Discussion
Bilingual sentence selection (BSS) might be un-
derstood to be closely related to adaptation, even
though both paradigms tackle problems which
are, in essence, different. The goal of an adap-
tation technique is to adapt model parameters,
which have been estimated on a large out-of-
domain (or generic) data set, so that they are
159
best suitable for dealing with a domain-specific
test set. This adaptation process is ought to be
achieved by means of a (potentially small) adapta-
tion set, which belongs to the same domain as the
test data. In contrast, BSS tackles with the prob-
lem of how to select samples from a large pool
of training data, regardless of whether such pool
of data is in-domain or out-of-domain. Hence, in
one case we can assume to have a fairly well es-
timated translation model, which is to be adapted,
whereas in BSS we still have full control over the
estimation of such model and need not to aim at a
specific domain, although it might often be so.
BSS is related with instance weighting (Jiang
and Zhai, 2007; Foster et al 2010). Adapta-
tion and BSS can be considered to be orthogo-
nal (yet complementary) problems under the in-
stance weighting paradigm. In such case, instance
weighting can be considered to span a complete
paradigmatic space between both. At one end,
there is sample selection (BSS for SMT), while at
the other end there is adaptation. For instance, it
is quite common to confront the adaptation prob-
lem by extracting different phrase-tables from dif-
ferent corpora, and then interpolate such tables.
This technique could be also applied to promote
the performance of the system built by means of
BSS. However, this is left out as future work.
We thoroughly analysed two BSS approaches
that obtain competitive results, while using a
small fraction of the training data, although there
is still much to be gained. For instance, oracle re-
sults have also been reported in this work, yield-
ing improvements of up to 10 BLEU points. Even
though the use of an oracle typically implies that
the results obtained are not realistic, recall that
the proposed oracle is special, in the sense that it
only uses the reference sentences for the specific
purpose of selecting training samples, but the ref-
erences are not included into the training data as
such. This is useful for assessing the potential be-
hind BSS: ideally, if we were able to design a BSS
strategy that, without using the references, would
select exactly those training samples, we would be
boosting system performance by 10 BLEU points.
This re-states BSS as a compelling technique that
has not yet received the attention it deserves.
BSS is not aimed at optimising computational
requirements, but does so as a byproduct. This
may seem despicable but it would allow to run
more experiments with the same resources, use
larger corpora or even more complex techniques,
such as synchronous grammars or hierarchical
models. For instance, the infrequent n-grams
technique has beaten all the other systems using
just a small fraction of the corpus, only 0.5%, and
is yet able to outperform a system trained with all
the data by 0.9 BLEU points and the random base-
line by 3 points. This baseline has been proved to
be difficult to beat by other works.
Preliminary experiments were performed in or-
der to analyse the perplexity of the references, the
number of out of vocabulary words (OoVs) and
the ratio of target-source phrases. These exper-
iments revealed that the improvements obtained
are largely correlated with a decrease in perplex-
ity and in the number of OoVs. On the one hand,
reducing the amount of OoVs was mirrored by
an important improvement in BLEU when the
amount of additional data was small, and also
entailed a decrease in perplexity. However, a
reduction in perplexity by itself did not always
imply significant improvements. Moreover, no
real conclusion could be drawn from the analy-
sis of target-source phrase ratio. Hence, we un-
derstand that the improvements obtained are pro-
vided mainly by a more specialised estimation of
the model parameters. However, further experi-
ments should still be conducted in order to verify
this conclusion.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under
grant agreement nr. 287755. This work was
also supported by the Spanish MEC/MICINN un-
der the MIPRCV ?Consolider Ingenio 2010? pro-
gram (CSD2007-00018), and iTrans2 (TIN2009-
14511) project. Also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and Instituto Tecnolo?gico de
Leo?n, DGEST-PROMEP y CONACYT, Me?xico.
160
References
Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative sample selection for statistical machine
translation. In Proc. of the EMNLP, pages 626?635,
Cambridge, MA, October.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proc of the EMNLP, pages 355?
362.
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc of the WSMT, pages 1?28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint Workshop on Sta-
tistical Machine Translation and Metrics for Ma-
chine Translation. In Proc. of the MATR(ACL),
pages 17?53, Uppsala, Sweden, July.
Sanjoy Dasgupta. 2009. The two faces of active learn-
ing. In Proc. of The twentieth Conference on Algo-
rithmic Learning Theory, page 1, Porto (Portugal),
October.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proc. of
the EMNLP, pages 451?459, Cambridge, MA, Oc-
tober.
Guillem Gasco?, Vicent Alabau, Jesu?s Andre?s-Ferrer,
Jesu?s Gonza?lez-Rubio, Martha-Alicia Rocha,
Germa?n Sanchis-Trilles, Francisco Casacuberta,
Jorge Gonza?lez, and Joan-Andreu Sa?nchez. 2010.
ITI-UPV system description for IWSLT 2010. In
Proc. of the IWSLT 2010, Paris, France, December.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proc. of HLT/NAACL?09,
pages 415?423, Morristown, NJ, USA.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In Proc.
of ACL?07, pages 264?271.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Proc.
of ICASSP, II:181?184, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christie
Moran, Richard Zens, Chris Dyer, Ontraj Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, pages 177?180.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proc. of the MATR(ACL), pages 139?
143, Uppsala, Sweden, July.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
training data selection and optimization. In Proc. of
the EMNLP-CoNLL, pages 343?350, Prague, Czech
Republic, June.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proc. of the
EMNLP, pages 708?717, Singapore, August.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
ACL (Short Papers), pages 220?224.
Franz J. Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL, pages
295?302.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29, pages
19?51.
Kishore Papineni, Salim Roukos, and Todd Ward.
1998. Maximum likelihood and discriminative
training of direct translation models. In Proc. of
ICASSP?98, pages 189?192.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: A method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022).
Michael Paul, Marcello Federico, and Sebastian Stker.
2010. Overview of the IWSLT 2010 evaluation
campaign. In Proc. of the IWSLT 2010, Paris,
France, December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proc. of AMTA?06.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of ICSLP.
Elia Yuste, Manuel Herranz, Antonio Lagarda, Li-
onel Tarazo?n, Isa??as Sa?nchez-Cortina, and Fran-
cisco Casacuberta. 2010. Pangeamt - putting
open standards to work... well. In Proc. of the
AMTA2010. Denver, CO, USA, November.
161
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172?176,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UPV-PRHLT English?Spanish system for WMT10
Germa?n Sanchis-Trilles and Jesu?s Andre?s-Ferrer and Guillem Gasco?
Jesu?s Gonza?lez-Rubio and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{gsanchis|jandres|fcn}@dsic.upv.es
{ggasco|jegonzalez|pmartinez}@dsic.upv.es
{mrocha|jandreu}@dsic.upv.es
Abstract
In this paper, the system submitted by
the PRHLT group for the Fifth Work-
shop on Statistical Machine Translation of
ACL2010 is presented. On this evalua-
tion campaign, we have worked on the
English?Spanish language pair, putting
special emphasis on two problems derived
from the large amount of data available.
The first one, how to optimize the use of
the monolingual data within the language
model, and the second one, how to make
good use of all the bilingual data provided
without making use of unnecessary com-
putational resources.
1 Introduction
For this year?s translation shared task, the Pat-
tern Recognition and Human Language Technolo-
gies (PRHLT) research group of the Universidad
Polite?cnica de Valencia submitted runs for the
English?Spanish translation task. In this paper, we
report the configuration of such a system, together
with preliminary experiments performed to estab-
lish the final setup.
As in 2009, the central focus of the Shared Task
is on Domain Adaptation, where a system typi-
cally trained using out-of-domain data is adjusted
to translate news commentaries.
For the preliminary experiments, we used only a
small amount of the largest available bilingual cor-
pus, i.e. the United Nations corpus, by including
into our system only those sentences which were
considered similar.
Language model interpolation using a develop-
ment set was explored in this work, together with
a technique to cope with the problem of ?out of
vocabulary words?.
Finally, a reordering constraint using walls and
zones was used in order to improve the perfor-
mance of the submitted system.
In the final evaluation, our system was ranked
fifth, considering only primary runs.
2 Language Model interpolation
Nowadays, it is quite common to have very large
amounts of monolingual data available from sev-
eral different domains. Despite of this fact, in
most of the cases we are only interested in trans-
lating from one specific domain, as is the case in
this year?s shared task, where the provided mono-
lingual training data belonged to European parlia-
mentary proceedings, news related domains, and
the United Nations corpus, which consists of data
crawled from the web.
Although the most obvious thing to do is to con-
catenate all the data available and train a single
language model on the whole data, we also inves-
tigated a ?smarter? use of such data, by training
one language model for each of the available cor-
pora.
3 Similar sentences selection
Currently, it is common to of huge bilingual cor-
pora for SMT. For some common language pairs,
corpora of millions of parallel sentences are avail-
able. In some of the cases big corpora are used
as out-of-domain corpora. For example, in the
case of the shared task, we try to translate a news
text using a small in-domain bilingual news corpus
(News Commentary) and two big out-of-domain
corpora: Europarl and United Nations.
Europarl is a medium size corpus and can be
completely incorporated to the training set. How-
ever, the use of the UN corpus requires a big com-
putational effort. In order to alleviate this prob-
lem, we have chosen only those bilingual sen-
tences from the United Nations that are similar to
the in-domain corpus sentences. As a similarity
measure, we have chosen the alignment score.
Alignment scores have already been used as a
172
filter for noisy corpora (Khadivi and Ney, 2005).
We trained an IBM model 4 using GIZA++ (Och
and Ney, 2003) with the in-domain corpus and
computed the alignment scores over the United
Nations sentences. We assume that the alignment
score is a good measure of similarity.
An important factor in the alignment score is
the length of the sentences, so we clustered the
bilingual sentences in groups with the same sum of
source and target language sentence sizes. In each
of the groups, the higher the alignment score is,
the more similar the sentence is to the in-domain
corpus sentences. Hence, we computed the aver-
age alignment score for each one of the clusters
obtained for the corpus considered in-domain (i.e.
the News-Commentary corpus). This being done,
we assessed the similarity of a given sentence by
computing the probability of such sentence with
respect to the alignment model of the in-domain
corpus, and established the following similarity
levels:
? Level 1: Sentences with an alignment score
equal or higher than the in-domain average.
? Level 2: Sentences with an alignment score
equal or higher than the in-domain average,
minus one standard deviation.
? Level 3: Sentences with an alignment score
equal or higher than the in-domain average,
minus two standard deviations.
Naturally, such similarity levels establish parti-
tions of the out-of-domain corpus. Then, such par-
titions were included into the training set used for
building the SMT system, and re-built the com-
plete system from scratch.
4 Out of Vocabulary Recovery
As stated in the previous section, in order to avoid
a big computational effort, we do not use the
whole United Nations corpus to train the trans-
lation system. Out of vocabulary words are a
common problem for machine translation systems.
When translating the test set, there are test words
that are not in the reduced training set (out of vo-
cabulary words). Some of those out of vocabulary
words are present in the sentences discarded from
the United Nations Corpus. Thus, recovering the
discarded sentences with out of vocabulary words
is needed.
The out of vocabulary words recovery method
is simple: the out of vocabulary words from the
test, when taking into account the reduced training
set, are obtained and then discarded sentences that
contain at least one of them are retrieved. Then,
those sentences are added to the reduced training
set.
Finally, alignments with the resulting training
set were computed and the usual training proce-
dure for phrase-based systems was performed.
5 Walls and zones
In translation, as in other linguistics areas, punc-
tuation marks are essential as they help to un-
derstand the intention of a message and organise
the ideas to avoid ambiguity. They also indicate
pauses, hierarchies and emphasis.
In our system, punctuation marks have been
taken into account during decoding. Traditionally,
in SMT punctuation marks are treated as words
and this has undesirable effects (Koehn and Had-
dow, 2009). For example, commas have a high
probability of occurrence and many possible trans-
lations are generated. Most of them are not consis-
tent across languages. This introduces too much
noise to the phrase tables.
(Koehn and Haddow, 2009) established a
framework to specify reordering constraints with
walls and zones, where commas and end
of sentence are not mixed with various clauses.
Gains between 0.1 and 0.2 of BLEU are reported.
Specifying zones and walls with XML tags
in input sentences allows us to identify structured
fragments that the Moses decoder uses with the
following restrictions:
1. If a <zone> tag is detected, then a block
is identified and must be translated until a
</zone> tag is found. The text between tags
<zone> and </zone> is identified and trans-
lated as a block.
2. If the decoder detects a <wall/> tag, the text
is divided into a prefix and suffix and Moses
must translate all the words of the prefix be-
fore the suffix.
3. If both zones and walls are specified,
then local walls are considered where
the constraint 2 applies only to the area es-
tablished by zones.
173
corpus Language |S| |W | |V |
Europarl v5
Spanish
1272K
28M 154K
English 27M 106K
NC
Spanish
81K
1.8M 54K
English 1.6M 39K
Table 1: Main figures of the Europarl v5 and
News-Commentary (NC) corpora. K/M stands
for thousands/millions. |S| is the number of sen-
tences, |W | the number of running words, and |V |
the vocabulary size. Statistics are reported on the
tokenised and lowercased corpora.
We used quotation marks, parentheses, brackets
and dashes as zone delimiters. Quotation marks
(when appearing once in the sentence), com-
mas, colons, semicolons, exclamation and ques-
tion marks and periods are used as wall delimiters.
The use of zone delimiters do not alter the per-
formance. When using walls, a gain of 0.1
BLEU is obtained in our best model.
6 Experiments
6.1 Experimental setup
For building our SMT systems, the open-source
SMT toolkit Moses (Koehn et al, 2007) was used
in its standard setup. The decoder includes a log-
linear model comprising a phrase-based transla-
tion model, a language model, a lexicalised dis-
tortion model and word and phrase penalties. The
weights of the log-linear interpolation were opti-
mised by means of MERT (Och, 2003). In addi-
tion, a 5-gram LM with Kneser-Ney (Kneser and
Ney, 1995) smoothing and interpolation was built
by means of the SRILM (Stolcke, 2002) toolkit.
For building our baseline system, the News-
Commentary and Europarl v5 (Koehn, 2005) data
were employed, with maximum sentence length
set to 40 in the case of the data used to build the
translation models, and without restriction in the
case of the LM. Statistics of the bilingual data can
be seen in Table 1.
In all the experiments reported, MERT was run
on the 2008 test set, whereas the test set 2009 was
considered as test set as such. In addition, all the
experiments described below were performed in
lowercase and tokenised conditions. For the fi-
nal run, the detokenisation and recasing was per-
formed according to the technique described in the
Workshop baseline description.
corpus |S| |W | |V |
Europarl 1822K 51M 172K
NC 108K 3M 68K
UN 6.2M 214M 411K
News 3.9M 107M 512K
Table 2: Main figures of the Spanish resources
provided: Europarl v5, News-Commentary (NC),
United Nations (UN) and News-shuffled (News).
6.2 Language Model interpolation
The final system submitted to the shared task
included a linear interpolation of four language
models, one for each of the monolingual resources
available for Spanish (see Table 2). The results
can be seen in Table 3. As a first experiment, only
the in-domain corpus, i.e. the News-Commentary
data (NC data) was used for building the LM.
Then, all the available monolingual Spanish data
was included into a single LM, by concatenat-
ing all the data together (pooled). Next, in
interpolated, one LM for each one of the
provided monolingual resources was trained, and
then they were linearly interpolated so as to min-
imise the perplexity of the 2008 test set, and fed
such interpolation to the SMT system. We found
out that weights were distributed quite unevenly,
since the News-shuffled LM received a weight of
0.67, whereas the other three corpora received a
weight of 0.11 each. It must be noted that even
the in-domain LM received a weight of 0.11 (less
than the News-shuffled LM). The reason for this
might be that, although the in-domain LM should
be more appropriate and should receive a higher
weight, the News-shuffled corpus is also news re-
lated (hence not really out-of-domain), but much
larger. For this reason, the result of using only
such LM (News) was also analysed. As expected,
the translation quality dropped slightly. Never-
theless, since the differences are not statistically
significant, we used the News-shuffled LM for in-
ternal development purposes, and the interpolated
LM only whenever an improvement prooved to be
useful.
6.3 Including UN data
We analysed the impact of the selection technique
detailed in Section 3. In this case, the LM used
was the interpolated LM described in the previous
section. The result can be seen in Table 4. As
it can be seen, translation quality as measured by
174
Table 3: Effect of considering different LMs
LM used BLEU
NC data 21.86
pooled 23.53
interpolated 24.97
news 24.79
BLEU improves constantly as the number of sen-
tences selected increases. However, further sen-
tences were not included for computational rea-
sons.
In the same table, we also report the effect of
adding the UN sentences selected by our out-of-
vocabulary technique described in Section 4. In
this context, it should be noted that MERT was
not rerun once such sentences had been selected,
since such sentences are related with the test set,
and not with the development set on which MERT
is run.
Table 4: Effect of including selected sentences
system BLEU
baseline 24.97
+ oovs 25.08
+ Level 1 24.98
+ Level 2 25.07
+ Level 3 25.13
6.4 Final system
Since the News-shuffled, UN and Europarl cor-
pora are large corpora, a new LM interpolation
was estimated by using a 6-gram LM on each one
of these corpora, obtaining a gain of 0.17 BLEU
points by doing so. Further increments in the n-
gram order did not show further improvements.
In addition, preliminary experimentation re-
vealed that the use of walls, as described in
Section 5, also provided slight improvements, al-
though using zones or combining both did not
prove to improve further. Hence, only walls
were included into the final system.
Lastly, the final system submitted to the Work-
shop was the result of combining all the techniques
described above. Such combination yielded a fi-
nal BLEU score of 25.31 on the 2009 test set, and
28.76 BLEU score on the 2010 test set, both in
tokenised and lowercased conditions.
7 Conclusions and future work
In this paper, the SMT system presented by the
UPV-PRHLT team for WMT 2010 has been de-
scribed. Specifically, preliminary results about
how to make use of larger data collections for
translating more focused test sets have been pre-
sented.
In this context, there are still some things which
need a deeper investigation, since the results pre-
sented here give only a small insight about the po-
tential of the similar sentence selection technique
described.
However, a deeper analysis is needed in order
to assess the potential of such technique and other
strategies should be implemented to explore new
kids of reordering constraints.
Acknowledgments
This paper is based upon work supported by
the EC (FEDER/FSE) and the Spanish MICINN
under the MIPRCV ?Consolider Ingenio 2010?
program (CSD2007-00018),iTrans2 (TIN2009-
14511) project, and the FPU scholarship AP2006-
00691. This work was also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and by the Generalitat Valen-
ciana under grant Prometeo/2009/014 and schol-
arships BFPI/2007/117 and ACIF/2010/226 and
by the Mexican government under the PROMEP-
DGEST program.
References
Shahram Khadivi and Hermann Ney. 2005. Automatic
filtering of bilingual corpora for statistical machine
translation. In Natural Language Processing and In-
formation Systems, 10th Int. Conf. on Applications
of Natural Language to Information Systems, vol-
ume 3513 of Lecture Notes in Computer Science,
pages 263?274, Alicante, Spain, June. Springer.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, II:181?184, May.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In The 4th EACL Workshop on Statistical
Machine Translation, ACL, pages 160?164, Athens,
Greece, March. Springer.
P. Koehn et al 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of
175
the ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. of ICSLP?02, pages 901?
904, September.
176
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 296?300,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The UPV-PRHLT Combination System for WMT 2010
Jesu?s Gonza?lez-Rubio and Jesu?s Andre?s-Ferrer and Germa?n Sanchis-Trilles
Guillem Gasco? and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{jegonzalez|jandres|gsanchis}@dsic.upv.es
{ggasco|pmartinez|mrocha}@dsic.upv.es
{jandreu|fcn}@dsic.upv.es
Abstract
UPV-PRHLT participated in the System
Combination task of the Fifth Workshop
on Statistical Machine Translation (WMT
2010). On each translation direction, all
the submitted systems were combined into
a consensus translation. These consen-
sus translations always improve transla-
tion quality of the best individual system.
1 Introduction
The UPV-PRHLT approach to MT system combi-
nation is based on a refined version of the algo-
rithm described in (Gonza?lez-Rubio and Casacu-
berta, 2010), with additional information to cope
with hypotheses of different quality.
In contrast to most of the previous approaches
to combine the outputs of multiple MT sys-
tems (Bangalore et al, 2001; Jayaraman and
Lavie, 2005; Matusov et al, 2006; Schroeder et
al., 2009), which are variations over the ROVER
voting scheme (Fiscus, 1997), we consider the
problem of computing a consensus translation as
the problem of modelling a set of string patterns
with an adequate prototype. Under this frame-
work, the translation hypotheses of each of the
MT systems are considered as individual patterns
in a set of string patterns. The (generalised) me-
dian string, which is the optimal prototype of a set
of strings (Fu, 1982), is the chosen prototype to
model the set of strings.
2 System Combination Algorithm
The median string of a set is defined as the string
that minimises the sum of distances to the strings
in the set. Therefore, defining a distance between
strings is the primary problem to deal with.
The most common definition of distance be-
tween two strings is the Levenshtein distance,
also known as edit distance (ED). This metric
computes the optimal sequence of edit operations
(insertions, deletions and substitutions of words)
needed to transform one string into the other. The
main problem with the ED is its dependence on the
length of the compared strings. This fact led to the
definition of a new distance whose value is inde-
pendent from the length of the strings compared.
This normalised edit distance (NED) (Vidal et al,
1995) is computed by averaging the number of edit
operations by the length of the edit path. The ex-
perimentation in this work was carried out using
the NED.
2.1 Median String
Given a set E = e1, . . . , en, . . . , eN of translation
hypotheses from N MT systems, let ? be the vo-
cabulary in the target language and ?? be the free
monoid over that vocabulary (E ? ??). The me-
dian string of the set E (noted as M(E)) can be
formally defined as:
M(E) = argmin
e????
N
?
n=1
[
wn ? D(e?, en)
]
, (1)
where D is the distance used to compare two
strings and the value wn, 1 ? n ? N weights
the contribution of the hypothesis n to the sum of
distances, and therefore, it denotes the significance
of hypothesis n in the computation of the median
string. The value wn can be seen as a measure of
the ?quality? of hypothesis n.
Computing the median string is a NP-Hard
problem (de la Higuera and Casacuberta, 2000),
therefore we can only build approximations to the
median string by using several heuristics. In this
work, we follow two different approximations: the
set median string (Fu, 1982) and the approximate
median string (Mart??nez et al, 2000).
296
2.2 Set Median String
The most straightforward approximation to the
median string corresponds to the search of a set
median string. Under this approximation, the
search is constrained to the strings in the given in-
put set. The set median string can be informally
defined as the most ?centred? string in the set. The
set median string of the set E (noted as Ms(E))
is given by:
Ms(E) = argmin
e??E
N
?
n=1
[
wn ? D(e?, en)
]
. (2)
The set median string can be computed in poly-
nomial time (Fu, 1982; Juan and Vidal, 1998).
Unfortunately, in some cases, the set median may
not be a good approximation to the median string.
For example, in the extreme case of a set of two
strings, either achieves the minimum accumulated
distance to the set. However, the set median string
is a useful initialisation in the computation of the
approximate median string.
2.3 Approximate Median String
A good approximation to efficiently compute the
median string is proposed in (Mart??nez et al,
2000). To compute the approximate median string
of the set E, the algorithm starts with an initial
string e which is improved by successive refine-
ments in an iterative process. This iterative pro-
cess is based on the application of different edit
operations over each position of the string e look-
ing for a reduction of the accumulated distance to
the strings in the set. Algorithm 1 describes this
iterative process.
The initial string can be a random string or
a string computed from the set E. Martinez et
al. (2000) proposed two kinds of initial strings: the
set median string of E and a string computed by a
greedy algorithm, both of them obtained similar
results. In this work, we start with the set median
string in the initialisation of the computation of the
approximate median string of the set E. Over this
initial string we apply the iterative procedure de-
scribed in Algorithm 1 until there is no improve-
ment. The final median string may be different
from the original hypotheses.
The computational time cost of Algorithm 1 is
linear with the number of hypotheses in the com-
bination, and usually only a moderate number of
iterations is needed to converge.
For each position i in the string e:
1. Build alternatives:
Substitution: Make x = e. For each word a ? ?:
? Make x? the result string of substituting the ith
word of x by a.
? If the accumulated distance of x? to E is lower
than the accumulated distance from x to E, then
make x = x?.
Deletion: Make y the result string of deleting the ith
word of e.
Insertion: Make z = e. For each word a ? ?:
? Make z? the result of inserting a at position i of
e.
? If the accumulated distance from z? to E is lower
than the accumulated distance from z to E, then
make z = z?.
2. Choose an alternative:
? From the set {e,x,y, z} take the string e? with
less accumulated distance to E. Make e = e?.
Algorithm 1: Iterative process to refine a string
e in order to reduce its accumulated distance to a
given set E.
3 Experiments
Experiments were conducted on all the 8 transla-
tion directions cz?en, en?cz, de?en, en?de,
es?en, en?es, fr?en and en?fr. Some of the
entrants to the shared translation task submit lists
of n-best translations, but, in our experience, if a
large number of systems is available, using n-best
translations does not allow to obtain better consen-
sus translations than using single best translations,
but raises computation time significantly. Conse-
quently, we compute consensus translations only
using the single best translation of each individ-
ual MT system. Table 1 shows the number of sys-
tems submitted and gives an overview of the test
corpus on each translation direction. The number
of running words is the average number of run-
ning words in the test corpora, from where the
consensus translations were computed; the vocab-
ulary is the merged vocabulary of these test cor-
pora. All the experiments were carried out with
the true-cased, detokenised version of the tuning
and test corpora, following the WMT 2010 sub-
mission guidelines.
3.1 Evaluation Criteria
We will present translation quality results in terms
of translation edit rate (TER) (Snover et al, 2006)
and bilingual evaluation understudy (BLEU) (Pa-
297
cz?en en?cz de?en en?de es?en en?es fr?en en?fr
Submitted systems 6 11 16 12 8 10 14 13
Avg. Running words 45K 37K 47K 41K 47K 47K 47K 49K
Distinct words 24K 51K 38K 40K 23K 30K 27K 37K
Table 1: Number of systems submitted and main figures of test corpora on each translation direction. K
stands for thousands of elements.
pineni et al, 2002). TER is computed as the num-
ber of edit operations (insertions, deletions and
substitutions of single words and shifts of word se-
quences) to convert the system hypothesis into the
reference translation. BLEU computes a geomet-
ric mean of the precision of n-grams multiplied by
a factor to penalise short sentences.
3.2 Weighted Sum of Distances
In section 2, we define the median string of a set
as the string which minimises a weighted sum of
distances to the strings in the set (Eq. (1)). The
weights wn in the sum can be tuned. We compute
a weight value for each MT system as a whole, i.e.
all the hypotheses of a given MT system share the
same weight value. We study the performance of
different sets of weight looking for improvements
in the quality of the consensus translations. These
weight values are derived from different automatic
MT evaluation measures:
? BLEU score of each system.
? 1.0 minus TER score of each system.
? Number of times the hypothesis of each sys-
tem is the best TER-scoring translation.
We estimate these scores on the tuning corpora.
A normalisation is performed to transform these
scores into the range [0.0, 1.0]. After the normal-
isation, a weight value of 0.0 is assigned to the
lowest-scoring hypothesis, i.e. the lowest-scoring
hypothesis is not taking into account in the com-
putation of the median string.
3.3 System Combination Results
Our framework to compute consensus translations
allows multiple combinations varying the median
string algorithm or the set of weight values used
in the weighted sum of distances. To assure the
soundness of our submission to the WMT 2010
system combination task, the experiments on the
tuning corpora were carried out in a leaving-one-
out fashion dividing the tuning data into 5 parts
and averaging translation results over these 5 par-
titions. On each of the experiments, 4 of the par-
titions are devoted to obtain the weight values for
the weighted sum of distances while BLEU and
TER scores are calculated on the consensus trans-
lations of the remaining partition.
Table 2 shows, on each translation direction,
the performance of the consensus translations on
the tuning corpora. The consensus translations
were computed with the set median string and the
approximated median string using different sets
of weight values: Uniform, all weights are set
to 1.0, BLEU-based weights, TER-based weights
and oracle-based weights. In addition, we display
the performance of the best of the individual MT
systems for comparison purposes. The number of
MT systems combined for each translation direc-
tion is displayed between parentheses.
On all the translation directions under study, the
consensus translations improved the results of the
best individual systems. E.g. TER improved from
66.0 to 63.3 when translating from German into
English. On average, the set median strings per-
formed better than the best individual system, but
its results were always below the performance of
the approximate median string. The use of weight
values computed from MT quality measures al-
lows to improve the quality of the consensus trans-
lation computed. Specially, oracle-based weight
values that, except for the cz?en task, always per-
form equal or better than the other sets of weight
values. We have observed that no improvements
can be achieved with uniform weight values; it is
necessary to penalise low quality hypotheses.
To compute our primary submission to the
WMT 2010 system combination task we choose
the configurations that obtain consensus transla-
tions with highest BLEU score on the tuning cor-
pora. The approximate median string using oracle-
based scores is the chosen configuration for all
translation directions, except on the cz?en trans-
lation direction for which TER-based weights per-
formed better. As our secondary submission we
298
Single Set median Approximated median
best Uniform Bleu Ter Oracle Uniform Bleu Ter Oracle
cz?en (6) BLEU 17.6 16.5 17.8 18.2 17.6 17.1 18.5 18.5 18.0TER 64.5 68.7 67.6 65.2 64.5 67.0 65.9 65.4 64.4
en?cz (11) BLEU 11.4 10.1 10.9 10.7 11.0 10.1 10.7 10.7 11.0TER 75.3 75.1 74.3 74.2 74.2 73.9 73.4 73.3 73.0
de?en (16) BLEU 19.0 19.0 19.1 19.3 19.7 19.3 19.8 19.9 20.1TER 66.0 65.4 65.2 65.0 64.6 64.4 63.4 63.4 63.3
en?de (12) BLEU 11.9 11.6 11.7 11.7 12.0 11.6 11.8 11.8 12.0TER 74.3 74.1 74.1 74.0 73.7 72.7 72.9 72.7 72.6
es?en (8) BLEU 23.2 23.0 23.3 23.2 23.6 23.1 23.9 23.8 24.2TER 60.2 60.6 59.8 59.8 59.5 60.0 59.2 59.4 59.1
en?es (10) BLEU 23.3 23.0 23.3 23.4 24.0 23.6 23.8 23.8 24.2TER 60.1 60.1 59.9 59.7 59.5 59.0 59.1 58.9 58.6
fr?en (14) BLEU 23.3 22.9 23.2 23.2 23.4 23.4 23.8 23.8 23.9TER 61.1 61.2 60.9 60.9 60.7 60.6 60.0 60.1 59.9
en?fr (13) BLEU 22.7 23.4 23.5 23.6 23.8 23.3 23.6 23.7 23.8TER 62.3 61.0 61.0 60.9 60.6 60.2 60.1 60.0 60.0
Table 2: Consensus translation results (case-sensitive) on the tuning corpora with the set median string
and the approximate median string using different sets of weights: Uniform, BLEU-based, TER-based
and oracle-based. The number of systems being combined for each translation direction is in parentheses.
Best consensus translation scores are in bold.
Best Secondary Primary
BLEU TER BLEU TER BLEU TER
cz?en 18.2 63.9 18.3 66.7 19.0 65.1
en?cz 10.8 75.2 11.3 73.6 11.6 71.9
de?en 18.3 66.6 19.1 65.4 19.6 63.9
en?de 11.6 73.4 11.7 72.9 11.9 71.7
es?en 24.7 59.0 24.9 58.9 25.0 58.2
en?es 24.3 58.4 24.9 57.3 25.3 56.3
fr?en 23.7 59.7 23.6 59.8 23.9 59.4
en?fr 23.3 61.3 23.6 59.9 24.1 58.9
Table 3: Translation scores (case-sensitive) on the
test corpora of our primary and secondary submis-
sions to the WMT 2010 system combination task.
chose the set median string using the same set of
weight values chosen for the primary submission.
We compute MT quality scores on the WMT
2010 test corpora to verify the results on the tuning
data. Table 3 displays, on each translation direc-
tion, the results on the test corpora of our primary
and secondary submissions and of the best indi-
vidual system. These results confirm the results
on the tuning data. On all translation directions,
our submissions perform better than the best indi-
vidual systems as measured by BLEU and TER.
4 Summary
We have studied the performance of two consen-
sus translation algorithms that based in the compu-
tation of two different approximations to the me-
dian string. Our algorithms use a weighted sum of
distances whose weight values can be tuned. We
show that using weight values derived from auto-
matic MT quality measures computed on the tun-
ing corpora allow to improve the performance of
the best individual system on all the translation di-
rections under study.
Acknowledgements
This paper is based upon work supported
by the EC (FEDER/FSE) and the Spanish
MICINN under the MIPRCV ?Consolider In-
genio 2010? program (CSD2007-00018), the
iTransDoc (TIN2006-15694-CO2-01) and iTrans2
(TIN2009-14511) projects and the FPU scholar-
ship AP2006-00691. This work was also sup-
ported by the Spanish MITyC under the eru-
dito.com (TSI-020110-2009-439) project and by
the Generalitat Valenciana under grant Prom-
eteo/2009/014 and scholarships BFPI/2007/117
and ACIF/2010/226 and by the Mexican govern-
ment under the PROMEP-DGEST program.
299
References
S. Bangalore, G. Bodel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In IEEE Workshop on ASRU,
pages 351?354.
C. de la Higuera and F. Casacuberta. 2000. Topology
of strings: Median string is np-complete. Theoreti-
cal Computer Science, 230:39?48.
J. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recogniser output voting
error reduction (rover).
K.S. Fu. 1982. Syntactic Pattern Recognition and Ap-
plications. Prentice Hall.
J. Gonza?lez-Rubio and F. Casacuberta. 2010. On the
use of median string for multi-source translation.
In Proceedings of 20th International Conference on
Pattern Recognition, Istambul, Turkey, May 27-28.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of EAMT, pages 143?152.
A. Juan and E. Vidal. 1998. Fast Median Search in
Metric Spaces. In Proc. of SPR, volume 1451 of
Lecture Notes in Computer Science, pages 905?912.
C. D. Mart??nez, A. Juan, and F. Casacuberta. 2000.
Use of Median String for Classification. In Proc. of
ICPR, volume 2, pages 907?910.
E. Matusov, N. Ueffing, and H-Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Proc. of EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
J. Schroeder, T. Cohn, and P. Koehn. 2009. Word lat-
tices for multi-source translation. In Proc. of EACL,
pages 719?727.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of TER with targeted
human annotation. In Proc. of AMTA, pages 223?
231.
E. Vidal, A. Marzal, and P. Aibar. 1995. Fast compu-
tation of normalized edit distances. IEEE Transac-
tions on PAMI, 17(9):899?902.
300
