Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 51?56,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Reducing the Impact of Data Sparsity in Statistical Machine Translation
Karan Singla
1
, Kunal Sachdeva
1
, Diksha Yadav
1
, Srinivas Bangalore
2
, Dipti Misra Sharma
1
1
LTRC IIIT Hyderabad,
2
AT&T Labs-Research
Abstract
Morphologically rich languages generally
require large amounts of parallel data to
adequately estimate parameters in a statis-
tical Machine Translation(SMT) system.
However, it is time consuming and expen-
sive to create large collections of parallel
data. In this paper, we explore two strate-
gies for circumventing sparsity caused by
lack of large parallel corpora. First, we ex-
plore the use of distributed representations
in an Recurrent Neural Network based lan-
guage model with different morphological
features and second, we explore the use of
lexical resources such as WordNet to over-
come sparsity of content words.
1 Introduction
Statistical machine translation (SMT) models es-
timate parameters (lexical models, and distortion
model) from parallel corpora. The reliability of
these parameter estimates is dependent on the size
of the corpora. In morphologically rich languages,
this sparsity is compounded further due to lack of
large parallel corpora.
In this paper, we present two approaches that
address the issue of sparsity in SMT models for
morphologically rich languages. First, we use an
Recurrent Neural Network (RNN) based language
model (LM) to re-rank the output of a phrase-
based SMT (PB-SMT) system and second we use
lexical resources such as WordNet to minimize the
impact of Out-of-Vocabulary(OOV) words on MT
quality. We further improve the accuracy of MT
using a model combination approach.
The rest of the paper is organized as follows.
We first present our approach of training the base-
line model and source side reordering. In Section
4, we present our experiments and results on re-
ranking the MT output using RNNLM. In Section
5, we discuss our approach to increase the cover-
age of the model by using synset ID?s from the
English WordNet (EWN). Section 6 describes our
experiments on combining the model with synset
ID?s and baseline model to further improve the
translation accuracy followed by results and obser-
vations sections.We conclude the paper with future
work and conclusions.
2 Related Work
In this paper, we present our efforts of re-
ranking the n-best hypotheses produced by a PB-
MT (Phrase-Based MT) system using RNNLM
(Mikolov et al., 2010) in the context of an English-
Hindi SMT system. The re-ranking task in ma-
chine translation can be defined as re-scoring the
n-best list of translations, wherein a number of
language models are deployed along with fea-
tures of source or target language. (Dungarwal
et al., 2014) described the benefits of re-ranking
the translation hypothesis using simple n-gram
based language model. In recent years, the use
of RNNLM have shown significant improvements
over the traditional n-gram models (Sundermeyer
et al., 2013). (Mikolov et al., 2010) and (Liu et
al., 2014) have shown significant improvements in
speech recognition accuracy using RNNLM . Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. We have
also explored the use of morphological features
(Hindi being a morphologically rich language) in
RNNLM and deduced that these features further
improve the baseline RNNLM in re-ranking the n-
best hypothesis.
Words in natural languages are richly diverse
so it is not possible to cover all source language
words when training an MT system. Untranslated
out-of-vocabulary (OOV) words tend to degrade
the accuracy of the output produced by an MT
model. Huang (2010) pointed to various types
of OOV words which occur in a data set ? seg-
51
mentation error in source language, named enti-
ties, combination forms (e.g. widebody) and ab-
breviations. Apart from these issues, Hindi being
a low-resourced language in terms of parallel cor-
pora suffers from data sparsity.
In the second part of the paper, we address the
problem of data sparsity with the help of English
WordNet (EWN) for English-Hindi PB-SMT. We
increase the coverage of content words (exclud-
ing Named-Entities) by incorporating sysnset in-
formation in the source sentences.
Combining Machine Translation (MT) systems
has become an important part of statistical MT in
past few years. Works by (Razmara and Sarkar,
2013; Cohn and Lapata, 2007) have shown that
there is an increase in phrase coverage when com-
bining different systems. To get more coverage of
unigrams in phrase-table, we have explored sys-
tem combination approaches to combine models
trained with synset information and without synset
information. We have explored two methodolo-
gies for system combination based on confusion
matrix(dynamic) (Ghannay et al., 2014) and mix-
ing models (Cohn and Lapata, 2007).
3 Baseline Components
3.1 Baseline Model and Corpus Statistics
We have used the ILCI corpora (Choudhary and
Jha, 2011) for our experiments, which contains
English-Hindi parallel sentences from tourism and
health domain. We randomly divided the data into
training (48970), development (500) and testing
(500) sentences and for language modelling we
used news corpus of English which is distributed
as a part of WMT?14 translation task. The data is
about 3 million sentences which also contains MT
training data.
We trained a phrase based (Koehn et al., 2003)
MT system using the Moses toolkit with word-
alignments extracted from GIZA++ (Och and Ney,
2000). We have used the SRILM (Stolcke and
others, 2002) with Kneser-Ney smoothing (Kneser
and Ney, 1995) for training a language model for
the first stage of decoding. The result of this base-
line system is shown in Table 1.
3.2 English Transformation Module
Hindi is a relatively free-word order language and
generally tends to follow SOV (Subject-Object-
Verb) order and English tends to follow SVO
(Subject-Verb-Object) word order. Research has
Number of Number of Number of
Training Development Evaluation BLEU
Sentences Sentences Sentences
48970 500 500 20.04
Table 1: Baseline Scores for Phrase-based Moses
Model
shown that pre-ordering source language to con-
form to target language word order significantly
improves translation quality (Collins et al., 2005).
We created a re-ordering module for transform-
ing an English sentence to be in the Hindi order
based on reordering rules provided by Anusaaraka
(Chaudhury et al., 2010). The reordering rules are
based on parse output produced by the Stanford
Parser (Klein and Manning, 2003).
The transformation module requires the text to
contain only surface form of words, however, we
extended it to support surface form along with its
factors such as lemma and Part of Speech (POS).
Input : the girl in blue shirt is my sister
Output : in blue shirt the girl is my sister.
Hindi : neele shirt waali ladki meri bahen hai (
blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux)
With this transformation, the English sentence
is structurally closer to the Hindi sentence which
leads to better phrase alignments. The model
trained with the transformed corpus produces a
new baseline score of 21.84 BLEU score an
improvement over the earlier baseline of 20.04
BLEU points.
4 Re-Ranking Experiments
In this section, we describe the results of re-
ranking the output of the translation model us-
ing Recurrent Neural Networks (RNN) based lan-
guage models using the same data which is used
for language modelling in the baseline models.
Unlike traditional n-gram based discrete lan-
guage models, RNN do not make the Markov as-
sumption and potentially can take into account
long-term dependencies between words. Since the
words in RNNs are represented as continuous val-
ued vectors in low dimensions allowing for the
possibility of smoothing using syntactic and se-
mantic features. In practice, however, learning
long-term dependencies with gradient descent is
difficult as described by (Bengio et al., 1994) due
to diminishing gradients.
We have integrated the approach of re-scoring
52
100 200 300 400 500
22
24
26
28
30
Number of Hypotheses
B
L
E
U
s
c
o
r
e
s
Baseline
POS
NONE
Lemma
Oracle
All
Figure 1: BLEU Scores for Re-ranking experi-
ments with RNNLM using different feature com-
binations.
n-best output using RNNLM which has also been
shown to be helpful by (Liu et al., 2014). Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. Follow-
ing their work, we used three type of features for
building an RNNLM for Hindi : lemma (root),
POS, NC (number-case). The data used was a
Wikipedia dump, MT training data, news arti-
cles which had approximately 500,000 Hindi sen-
tences. Features were extracted using paradigm-
based Hindi Morphological Analyzer
1
Figure 1 illustrates the results of re-ranking per-
formed using RNNLM trained with various fea-
tures. The Oracle score is the highest achievable
score in a re-ranking experiment. This score is
computed based on the best translation out of n-
best translations. The best translation is found us-
ing the cosine similarity between the hypothesis
and the reference translation. It can be seen from
Figure 1, that the LM with only word and POS in-
formation is inferior to all other models. However,
morphological features like lemma, number and
case information help in re-ranking the hypothesis
significantly. The RNNLM which uses all the fea-
tures performed the best for the re-ranking exper-
iments achieving a BLEU score of 26.91, after re-
scoring 500-best obtained from the pre-order SMT
model.
1
We have used the HCU morph-analyzer.
System BLEU
Baseline 21.84
Rescoring 500-best with RNNLM
Features
NONE 25.77
POS 24.36
Lemma(root) 26.32
ALL(POS+Lemma+NC) 26.91
Table 2: Rescoring results of 500-best hypotheses
using RNNLM with different features
5 Using WordNet to Reduce Data
Sparsity
We extend the coverage of our source data by us-
ing synonyms from the English WordNet (EWN).
Our main motivation is to reduce the impact of
OOV words on output quality by replacing words
in a source sentence with their corresponding
synset IDs. However, choosing the appropriate
synset ID based upon its context and morphologi-
cal information is important. For sense selection,
we followed the approach used by (Tammewar et
al., 2013), which is also described further in this
section in the context of our task. We ignored
words that are regarded as Named-Entities as in-
dicated by Stanford NER tagger, as they should
not have synonyms in any case.
5.1 Sense Selection
Words are ambiguous, independent of their sen-
tence context. To choose an appropriate sense ac-
cording to the context for a lexical item is a chal-
lenging task typically termed as word-sense dis-
ambiguation. However, the syntactic category of
a lexical item provides an initial cue for disam-
biguating a lexical item. Among the varied senses,
we filter out the senses that are not the same POS
tag as the lexical item. But words are not just am-
biguous across different syntactic categories but
are also ambiguous within a syntactic category. In
the following, we discuss our approaches to select
the sense of a lexical item best suited in a given
context within a given category. Also categories
were filtered so that only content words get re-
placed with synset IDs.
5.1.1 Intra-Category Sense Selection
First Sense: Among the different senses,we se-
lect the first sense listed in EWN corresponding to
the POS-tag of a given lexical item. The choice is
motivated by our observation that the senses of a
53
lexical item are ordered in the descending order of
their frequencies of usage in the lexical resource.
Merged Sense: In this approach, we merge all
the senses listed in EWN corresponding to the
POS-tag of the given lexical item. The motivation
behind this strategy is that the senses in the EWN
for a particular word-POS pair are too finely clas-
sified resulting in classification of words that may
represent the same concept, are classified into dif-
ferent synsets. For example : travel and go can
mean the same concept in a similar context but the
first sense given by EWN is different for these two
words. Therefore, we merge all the senses for a
word into a super sense ( synset ID of first word
occurred in data), which is given to all its syn-
onyms even if it occurs in different synset IDs.
5.2 Factored Model
Techniques such as factored modelling (Koehn
and Hoang, 2007) are quite beneficial for Trans-
lation from English to Hindi language as shown
by (Ramanathan et al., 2008). When we replace
words in a source sentence with the synset ID?as,
we tend to lose morphological information associ-
ated with that word. We add inflections as features
in a factored SMT model to minimize the impact
of this replacement.
We show the results of the processing steps on
an example sentence below.
Original Sentence : Ram is going to market to
buy apples
New Sentence : Ram is Synset(go.v.1)
to Synset(market.n.0) to Synset(buy.v.1)
Synset(apple.n.1)
Sentence with synset ID: Ram E is E
Synset(go.v.1) ing to E Synset(market.n.0) E
to E Synset(buy.v.1) E Synset(apple.n.1) s
Then English sentences were reordered to Hindi
word-order using the module discussed in Section
3.
Reordered Sentence: Ram E Synset(apple.n.1) s
Synset(buy.v.1) E to E Synset(market.n.0) E to E
Synset(go.v.1) ing is E
In Table 3, the second row shows the BLEU
scores for the models in which there are synset IDs
for the source side. It can be seen that the factored
model also shows significant improvement in the
results.
6 Combining MT Models
Combining Machine translation (MT) systems has
become an important part of Statistical MT in
the past few years. There are two dominant ap-
proaches. (1) a system combination approach
based on confusion networks (CN) (Rosti et al.,
2007), which can work dynamically in combin-
ing the systems. (2) Combine the models by lin-
early interpolating and then using MERT to tune
the combined system.
6.1 Combination based on confusion
networks
We used the tool MANY (Barrault, 2010) for sys-
tem combination. However, since the tool is con-
figured to work with TERp evaluation metric, we
modified it to use METEOR (Gupta et al., 2010)
metric since it has been shown by (Kalyani et al.,
2014), that METEOR evaluation metric is better
correlated to human evaluation for morphologi-
cally rich Indian Languages.
6.2 Linearly Interpolated Combination
In this approach, we combined phrase-tables of
the two models (Eng (sysnset) - Hindi and Base-
line) using linear interpolation. We combined the
two models with uniform weights ? 0.5 for each
model, in our case. We again tuned this model
with the new interpolated phrase-table using stan-
dard algorithm MERT.
7 Experiments and Results
As can be seen in Table 3, the model with synset
information led to reduction in OOV words. Even
though BLEU score decreased, but METEOR
score improved for all the experiments based on
using synset IDs in the source sentence, but it has
been shown by (Gupta et al., 2010) that METEOR
is a better evaluation metrics for morphologically
rich languages. Also, when synset ID?as are used
instead of words in the source language, the sys-
tem makes incorrect morphological choices. Ex-
ample : going and goes will be replaced by same
synset ID ?aSynset(go.v.1)?a, so this has lead to loss
of information in the phrase-table but METEOR
catches these complexities as it considers features
like stems, synonyms for its evaluation metrics
and hence showed better improvements compared
to BLEU metric. Last two rows of Table 3 show
results for combination experiments and Mixture
Model (linearly interpolated model) showed best
54
System #OOV words BLEU Meteor
Baseline 253 21.8 .492
Eng(Synset ID)-Hindi
Baseline 237 19.2 .494
*factor(inflections) 225 20.3 .506
Ensembled Decoding 213 21.0 .511
Mixture Model 210 21.2 .519
Table 3: Results for the model in which there were Synset ID?s instead of word in English data
results with significant reduction in OOV words
and also some gains in METEOR score.
8 Observations
In this section, we study the coverage of different
models by categorizing the OOV words into 5 cat-
egories.
? NE(Named Entities) : As the data was
from Health & Tourism domain, these words
were mainly the names of the places and
medicines.
? VB : types of verb forms
? NN : types of nouns and pronouns
? ADJ : all adjectives
? AD : adverbs
? OTH : there were some words which did not
mean anything in English
? SM : There were some occasional spelling
mistakes seen in the test data.
Note : There were no function words seen in the
OOV(un-translated) words
Cat. Baseline Eng(synset)-Hin MixtureModel
NE 120 121 115
VB 47 37 27
NN 76 60 47
ADJ 22 15 12
AD 5 5 4
OTH 2 2 2
SM 8 8 8
Table 4: OOV words in Different Models
As this analysis was done on a small dataset and
for a fixed domain, the OOV words were few in
number as it can be seen in Table 4. But the OOV
words across the different models reduced as ex-
pected. The NE words remained almost the same
for all the three models but OOV words from cate-
gory VB,NN,ADJ decreased for Eng(synset)-Hin
model and Mixture model significantly.
9 Future Work
In the future, we will work on using the two ap-
proaches discussed: Re-Ranking & using lexical
resources to reduce sparsity together in a system.
We will work on exploring syntax based features
for RNNLM and we are planning to use a better
method for sense selection and extending this con-
cept for more language pairs. Word-sense disam-
biguation can be used for choosing more appro-
priate sense when the translation model is trained
on a bigger data data set. Also we are looking for
unsupervised techniques to learn the replacements
for words to reduce sparsity and ways to adapt our
system to different domains.
10 Conclusions
In this paper, we have discussed two approaches
to address sparsity issues encountered in training
SMT models for morphologically rich languages
with limited amounts of parallel corpora. In the
first approach we used an RNNLM enriched with
morphological features of the target words and
show the BLEU score to improve by 5 points. In
the second approach we use lexical resource such
as WordNet to alleviate sparsity.
References
Lo??c Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147?155.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Sriram Chaudhury, Ankitha Rao, and Dipti M Sharma.
2010. Anusaaraka: An expert system based machine
translation system. In Natural Language Processing
55
and Knowledge Engineering (NLP-KE), 2010 Inter-
national Conference on, pages 1?6. IEEE.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 531?540. Association for Computational
Linguistics.
Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra,
Anoop Kunchukuttan, Ritesh Shah, and Pushpak
Bhattacharyya. 2014. The iit bombay hindi-english
translation system at wmt 2014. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 90?96, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Sahar Ghannay, France Le Mans, and Lo?c Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1?5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Chung-chi Huang, Ho-ching Yen, and Jason S Chang.
2010. Using sublexical translations to handle the
oov problem in mt. In Proceedings of The Ninth
Conference of the Association for Machine Transla-
tion in the Americas (AMTA).
Aditi Kalyani, Hemant Kamud, Sashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt
systems for hindi to english translation. In In-
ternational Journal of Computer Applications, vol-
ume 89.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In EMNLP-CoNLL, pages 868?876.
Citeseer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
X Liu, Y Wang, X Chen, MJF Gales, and PC Wood-
land. 2014. Efficient lattice rescoring using recur-
rent neural network language models.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 440?447. Association for
Computational Linguistics.
Ananthakrishnan Ramanathan, Jayprasad Hegde,
Ritesh M Shah, Pushpak Bhattacharyya, and
M Sasikumar. 2008. Simple syntactic and morpho-
logical processing can help english-hindi statistical
machine translation. In IJCNLP, pages 513?520.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252?
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Yangyang Shi, Pascal Wiggers, and Catholijn M
Jonker. 2012. Towards recurrent neural networks
language models with linguistic and contextual fea-
tures. In INTERSPEECH.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Martin Sundermeyer, Ilya Oparin, J-L Gauvain, Ben
Freiberg, R Schluter, and Hermann Ney. 2013.
Comparison of feedforward and recurrent neural
network language models. In Acoustics, Speech and
Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on, pages 8430?8434. IEEE.
Aniruddha Tammewar, Karan Singla, Srinivas Banga-
lore, and Michael Carl. 2013. Enhancing asr by
mt using semantic information from hindiwordnet.
In Proceedings of ICON-2013: 10th International
Conference on Natural Language Processing.
56
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85?91,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploring System Combination approaches for Indo-Aryan MT Systems
Karan Singla
1
, Nishkarsh Shastri
2
, Megha Jhunjhunwala
2
, Anupam Singh
3
,
Srinivas Bangalore
4
, Dipti Misra Sharma
1
1
LTRC IIIT Hyderabad,
2
IIT-Kharagpur,
3
NIT-Durgapur,
4
AT&T Labs-Research
Abstract
Statistical Machine Translation (SMT)
systems are heavily dependent on the qual-
ity of parallel corpora used to train transla-
tion models. Translation quality between
certain Indian languages is often poor due
to the lack of training data of good qual-
ity. We used triangulation as a technique
to improve the quality of translations in
cases where the direct translation model
did not perform satisfactorily. Triangula-
tion uses a third language as a pivot be-
tween the source and target languages to
achieve an improved and more efficient
translation model in most cases. We also
combined multi-pivot models using linear
mixture and obtained significant improve-
ment in BLEU scores compared to the di-
rect source-target models.
1 Introduction
Current SMT systems rely heavily on large quan-
tities of training data in order to produce good
quality translations. In spite of several initiatives
taken by numerous organizations to generate par-
allel corpora for different language pairs, train-
ing data for many language pairs is either not
yet available or is insufficient for producing good
SMT systems. Indian Languages Corpora Initia-
tive (ILCI) (Choudhary and Jha, 2011) is currently
the only reliable source for multilingual parallel
corpora for Indian languages however the number
of parallel sentences is still not sufficient to create
high quality SMT systems.
This paper aims at improving SMT systems
trained on small parallel corpora using various re-
cently developed techniques in the field of SMTs.
Triangulation is a technique which has been found
to be very useful in improving the translations
when multilingual parallel corpora are present.
Triangulation is the process of using an interme-
diate language as a pivot to translate a source lan-
guage to a target language. We have used phrase
table triangulation instead of sentence based tri-
angulation as it gives better translations (Utiyama
and Isahara, 2007). As triangulation technique ex-
plores additional multi parallel data, it provides
us with separately estimated phrase-tables which
could be further smoothed using smoothing meth-
ods (Koehn et al. 2003). Our subsequent approach
will explore the various system combination tech-
niques through which these triangulated systems
can be utilized to improve the translations.
The rest of the paper is organized as follows.
We will first talk about the some of the related
works and then we will discuss the facts about the
data and also the scores obtained for the baseline
translation model. Section 3 covers the triangu-
lation approach and also discusses the possibility
of using combination approaches for combining
triangulated and direct models. Section 4 shows
results for the experiments described in previous
section and also describes some interesting obser-
vations from the results. Section 5 explains the
conclusions we reached based on our experiments.
We conclude the paper with a section about our fu-
ture work.
2 Related Works
There are various works on combining the tri-
angulated models obtained from different pivots
with the direct model resulting in increased con-
fidence score for translations and increased cov-
erage by (Razmara and Sarkar, 2013; Ghannay et
al., 2014; Cohn and Lapata, 2007). Among these
techniques we explored two of the them. The first
one is the technique based on the confusion ma-
trix (dynamic) (Ghannay et al., 2014) and the other
one is based on mixing the models as explored
by (Cohn and Lapata, 2007). The paper also dis-
cusses the better choice of combination technique
85
among these two when we have limitations on
training data which in our case was small and re-
stricted to a small domain (Health & Tourism).
As suggested in (Razmara and Sarkar, 2013),
we have shown that there is an increase in phrase
coverage when combining the different systems.
Conversely we can say that out of vocabulary
words (OOV) always decrease in the combined
systems.
3 Baseline Translation Model
In our experiment, the baseline translation model
used was the direct system between the source and
target languages which was trained on the same
amount of data as the triangulated models. The
parallel corpora for 4 Indian languages namely
Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla
(bn) was taken from Indian Languages Corpora
Initiative (ILCI) (Choudhary and Jha, 2011) . The
parallel corpus used in our experiments belonged
to two domains - health and tourism and the train-
ing set consisted of 28000 sentences. The develop-
ment and evaluation set contained 500 sentences
each. We used MOSES (Koehn et al., 2007) to
train the baseline Phrase-based SMT system for all
the language pairs on the above mentioned paral-
lel corpus as training, development and evaluation
data. Trigram language models were trained using
SRILM (Stolcke and others, 2002). Table 1 below
shows the BLEU score for all the trained pairs.
Language Pair BLEU Score
bn-mt 18.13
mt-bn 21.83
bn-gj 22.45
gj-mt 23.02
gj-bn 24.26
mt-gj 25.5
hn-mt 30.01
hn-bn 32.92
bn-hn 34.99
mt-hn 36.82
hn-gj 40.06
gj-hn 43.48
Table 1: BLEU scores of baseline models
4 Triangulation: Methodology and
Experiment
We first define the term triangulation in our con-
text. Each source phrase s is first translated to an
intermediate (pivot) language i, and then to a tar-
get language t. This two stage translation process
is termed as triangulation.
Our basic approach involved making triangu-
lated models by triangulating through different
pivots and then interpolating triangulated models
with the direct source-target model to make our
combined model.
In line with various previous works, we will
be using multiple translation models to overcome
the problems faced due to data sparseness and in-
crease translational coverage. Rather than using
sentence translation (Utiyama and Isahara, 2007)
from source to pivot and then pivot to target, a
phrase based translation model is built.
Hence the main focus of our approach is on
phrases rather than on sentences. Instead of using
combination techniques on the output of several
translation systems, we constructed a combined
phrase table to be used by the decoder thus avoid-
ing the additional inefficiencies observed while
merging the output of various translation systems.
Our method focuses on exploiting the availability
of multi-parallel data, albeit small in size, to im-
prove the phrase coverage and quality of our SMT
system.
Our approach can be divided into different steps
which are presented in the following sections.
4.1 Phrase-table triangulation
Our emphasis is on building an enhanced phrase
table that incorporates the translation phrase tables
of different models. This combined phrase table
will be used by the decoder during translation.
Phrase table triangulation depends mainly on
phrase level combination of the two different
phrase based systems mainly source (src) - pivot
(pvt) and pivot (pvt) - target (tgt) using pivot lan-
guage as a basis for combination. Before stating
the mathematical approach for triangulation, we
present an example.
4.1.1 Basic methodology
Suppose we have a Bengali-Hindi phrase-table
(T
BH
) and a Hindi-Marathi phrase-table (T
HM
).
From these tables, we have to construct a Bengali-
Marathi phrase-table (T
BM
). For that we need
86
Triangulated
System
Full-Triangulation
(phrase-table length)
Triangulation with top 40
(Length of phrase table)
Full Triangulation
(BLEU Score)
Triangulation with top 40
(BLEU SCORE)
gj - hn - mt 3,585,450 1,086,528 24.70 24.66
gj - bn - mt 7,916,661 1,968,383 20.55 20.04
Table 2: Comparison between triangulated systems in systems with full phrase table and the other having
top 40 phrase-table entries
to estimate four feature functions: phrase trans-
lation probabilities for both directions ?(
?
b|m?)
and ?(m?|
?
b), and lexical translation probabilities
for both directions lex(
?
b|m?) and lex(m?|
?
b) where
?
b and m? are Bengali and Marathi phrases that
will appear in our triangulated Bengali-Marathi
phrase-table T
BM
.
?(
?
b|m?) =
?
?
h?T
BH
?T
HM
?(
?
b|
?
h)?(
?
h|m?) (1)
?(m?|
?
b) =
?
?
h?T
BH
?T
HM
?(m?|
?
h)?(
?
h|
?
b) (2)
lex(
?
b|m?) =
?
?
h?T
BH
?T
HM
lex(
?
b|
?
h)lex(
?
h|m?) (3)
lex(m?|
?
b) =
?
?
h?T
BH
?T
HM
lex(m?|
?
h)lex(
?
h|
?
b) (4)
In these equations a conditional independence
assumption has been made that source phrase
?
b
and target phrase m? are independent given their
corresponding pivot phrase(s)
?
h. Thus, we can
derive ?(
?
b|m?), ?(m?|
?
b), lex(
?
b|m?), lex(m?|
?
b) by as-
suming that these probabilities are mutually inde-
pendent given a Hindi phrase
?
h.
The equation given requires that all phrases in
the Hindi-Marathi bitext must also be present in
the Bengali-Hindi bitext. Clearly there would be
many phrases not following the above require-
ment. For this paper we completely discarded the
missing phrases. One important point to note is
that although the problem of missing contextual
phrases is uncommon in multi-parallel corpora, as
it is in our case, it becomes more evident when the
bitexts are taken out from different sources.
In general, wider range of possible translations
are found for any source phrase through triangula-
tion. We found that in the direct model, a source
phrase is aligned to three phrases then there is
high possibility of it being aligned to three phrases
in intermediate language. The intermediate lan-
guage phrases are further aligned to three or more
phrases in target language. This results in increase
in number of translations of each source phrase.
4.1.2 Reducing the size of phrase-table
While triangulation is intuitively appealing, it suf-
fers from a few problems. First, the phrasal trans-
lation estimates are based on noisy automatic word
alignments. This leads to many errors and omis-
sions in the phrase-table. With a standard source-
target phrase-table these errors are only encoun-
tered once, however with triangulation they are en-
countered twice, and therefore the errors are com-
pounded. This leads to much noisier estimates
than in the source-target phrase-table. Secondly,
the increased exposure to noise means that trian-
gulation will omit a greater proportion of large or
rare phrases than the standard method. An align-
ment error in either of the source-intermediate bi-
text or intermediate-target bitext can prevent the
extraction of a source-target phrase pair.
As will be explained in the next section, the sec-
ond kind of problem can be ameliorated by using
the triangulated phrase-based table in conjunction
with the standard phrase based table referred to as
direct src-to-pvt phrase table in our case.
For the first kind of problem, not only the com-
pounding of errors leads to increased complex-
ity but also results in an absurdly large triangu-
lated phrase based table. To tackle the problem of
unwanted phrase-translation, we followed a novel
approach.
A general observation is that while triangulat-
ing between src-pvt and pvt-tgt systems, the re-
sultant src-tgt phrase table formed will be very
large since for a translation s? to
?
i in the src-to-
pvt table there may be many translations from
?
i to
?
t1,
?
t2...
?
tn. For example, the Bengali-Hindi
phrase-table(T
BH
) consisted of 846,106 transla-
tions and Hindi-Marathi phrase-table(T
HM
) con-
sisted of 680,415 translations and after triangu-
lating these two tables our new Bengali-Marathi
triangulated table(T
BM
) consisted of 3,585,450
translations as shown in Table 2. Tuning with
such a large phrase-table is complex and time-
consuming. To reduce the complexity of the
phrase-table, we used only the top-40 transla-
87
tions (translation with 40 maximum values of
P (
?
f |e?) for every source phrase in our triangulated
phrase-table(T
BM
) which reduced the phrase table
to 1,086,528 translations.
We relied on P (
?
f |e?)(inverse phrase translation
probability) to choose 40 phrase translations for
each phrase, since in the direct model, MERT
training assigned the most weight to this param-
eter.
It is clearly evident from Table 2 that we have
got a massive reduction in the length of the phrase-
table after taking in our phrase table and still the
results have no significant difference in our output
models.
4.2 Combining different triangulated models
and the direct model
Combining Machine translation (MT) systems has
become an important part of Statistical MT in the
past few years. There have been several works by
(Rosti et al., 2007; Karakos et al., 2008; Leusch
and Ney, 2010);
We followed two approaches
1. A system combination based on confusion
network using open-source tool kit MANY
(Barrault, 2010), which can work dynami-
cally in combining the systems
2. Combine the models by linearly interpolating
them and then using MERT to tune the com-
bined system.
4.2.1 Combination based on confusion
matrix
MANY tool was used for this and initially it was
configured to work with TERp evaluation matrix,
but we modified it to work using METEOR-Hindi
(Gupta et al., 2010), as it has been shown by
(Kalyani et al., 2014), that METEOR evaluation
metric is closer to human evaluation for morpho-
logically rich Indian Languages.
4.2.2 Linearly Interpolated Models
We used two different approaches while merging
the different triangulated models and direct src-tgt
model and we observed that both produced com-
parable results in most cases. We implemented the
linear mixture approach, since linear mixtures of-
ten outperform log-linear ones (Cohn and Lapata,
2007). Note that in our combination approaches
the reordering tables were left intact.
1. Our first approach was to use linear interpola-
tion to combine all the three models (Bangla-
Hin-Marathi, Bangla-Guj-Marathi and di-
rect Bangla-Marathi models) with uniform
weights, i.e 0.3 each in our case.
2. In the next approach, the triangulated phrase
tables are combined first into a single trian-
gulated phrase-table using uniform weights.
The combined triangulated phrase-table and
direct src-tgt phrase table is then combined
using uniform weights. In other words, we
combined all the three systems, Ban-Mar,
Ban-Hin-Mar, and Ban-Guj-Mar with 0.5,
0.25 and 0.25 weights respectively. This
weight distribution reflects the intuition that
the direct model is less noisy than the trian-
gulated models.
In the experiments below, both weight settings
produced comparable results. Since we performed
triangulation only through two languages, we
could not determine which approach would per-
form better. An ideal approach will be to train the
weights for each system for each language pair
using standard tuning algorithms such as MERT
(Zaidan, 2009).
4.2.3 Choosing Combination Approach
In order to compare the approaches on our data,
we performed experiments on Hindi-Marathi pair
following both approaches discussed in Section
4.2.1 and 4.2.2. We also generated triangulated
models through Bengali and Gujarati as pivot lan-
guages.
Also, the approach presented in section 4.2.1
depends heavily on LM (Language Model).In or-
der to study the impact of size, we worked on
training Phrase-based SMT systems with subsets
of data in sets of 5000, 10000, 150000 sentences
and LM was trained for 28000 sentences for com-
paring these approaches. The combination results
were compared following the approach mentioned
in 4.2.1 and 4.2.2.
Table 3, shows that the approach discussed in
4.2.1 works better if there is more data for LM
but we suffer from the limitation that there is no
other in-domain data available for these languages.
From the Table, it can also be seen that combin-
ing systems with the approach explained in 4.2.2
can also give similar or better results if there is
scarcity of data for LM. Therefore we followed the
88
#Training #LM Data Comb-1 Comb-2
5000 28000 21.09 20.27
10000 28000 24.02 24.27
15000 28000 27.10 27.63
Table 3: BLEU scores for Hindi-Marathi Model
comparing approaches described in 3.2.1(Comb-
1) and 3.2.2(Comb-2)
approach from Section 4.2.2 for our experiments
on other language pairs.
5 Observation and Resuslts
Table 4, shows the BLEU scores of triangulated
models when using the two languages out of the
4 Indian languages Hin, Guj, Mar, Ban as source
and target and the remaining two as the pivot lan-
guage. The first row mentions the BLEU score
of the direct src-tgt model for all the language
pairs. The second and third rows provide the tri-
angulated model scores through pivots which have
been listed. The fourth and fifth rows show the
BLEU scores for the combined models (triangu-
lated+direct) with the combination done using the
first and second approach respectively that have
been elucidated in the Section 4.2.2
As expected, both the combined models have
performed better than the direct models in all
cases.
Figure 1: Phrase-table coverage of the evaluation
set for all the language pairs
Figure 1, shows the phrase-table coverage of the
evaluation set for all the language pairs. Phrase-
table coverage is defined as the percentage of un-
igrams in the evaluation set for which translations
are present in the phrase-table. The first bar cor-
responds to the direct model for each language
pair, the second and third bars show the cover-
age for triangulated models through the 2 piv-
ots, while the fourth bar is the coverage for the
combined model (direct+triangulated). The graph
clearly shows that even though the phrase table
coverage may increase or decrease by triangula-
tion through a single pivot the combined model
(direct+triangulated) always gives a higher cover-
age than the direct model.
Moreover, there exists some triangulation mod-
els whose coverage and subsequent BLEU scores
for translation is found to be better than that of the
direct model. This is a particularly interesting ob-
servation as it increases the probability of obtain-
ing better or at least comparable translation mod-
els even when direct source-target parallel corpus
is absent.
6 Discussion
Dravidian languages are different from Indo-aryan
languages but they are closely related amongst
themselves. So we explored similar experiments
with Malayalam-Telugu pair of languages with
similar parallel data and with Hindi as pivot.
The hypothesis was that the direct model for
Malayalam-Telegu would have performed better
due to relatedness of the two languages. However
the results via Hindi were better as can be seen in
Table 5.
As Malayalam-Telegu are comparatively closer
than compared to Hindi, so the results via Hindi
should have been worse but it seems more like a
biased property of training data which considers
that all languages are closer to Hindi, as the trans-
lation data was created from Hindi.
7 Future Work
It becomes increasingly important for us to im-
prove these techniques for such languages having
rare corpora. The technique discussed in the paper
is although efficient but still have scope for im-
provements.
As we have seen from our two approaches of
combining the phrase tables and subsequent in-
terpolation with direct one, the best combination
among the two is also not fixed. If we can find the
89
BLEU scores gj-mt mt-gj gj-hn hn-gj hn-mt mt-hn
Direct model 23.02 25.50 43.48 40.06 30.01 36.82
Triangulated
through pivots
hn 24.66 hn 27.09 mt 36.76 mt 33.69 gj 29.27 gj 33.86
bn 20.04 bn 22.02 bn 35.07 bn 32.66 bn 26.72 bn 31.34
Mixture-1 26.12 27.46 43.23 39.99 33.09 38.50
Mixture-2 26.25 27.32 44.04 41.45 33.36 38.44
(a)
BLEU scores bn-gj gj-bn bn-hn hn-bn mt-bn bn-mt
Direct model 22.45 24.26 34.99 32.92 21.83 18.13
Triangulated
through pivots
hn 23.97 hn 26.26 gj 31.69 gj 29.60 hn 23.80 hn 21.04
mt 20.70 mt 22.32 mt 28.96 mt 27.95 gj 22.41 gj 18.15
Mixture-1 25.80 27.45 35.14 34.77 24.99 22.16
Mixture-2 24.66 27.39 35.02 34.85 24.86 22.75
(b)
Table 4: Table (a) & (b) show results for all language pairs after making triangulated models and then
combining them with linear interpolation with the two approaches described in 3.2.2. In Mixture-1,
uniform weights were given to all three models but in Mixture-2, direct model is given 0.5 weight relative
to the other models (.25 weight to each)
System Blue Score
Direct Model 4.63
Triangulated via Hindi 14.32
Table 5: Results for Malayalam-Telegu Pair for
same data used for other languages
best possible weights to be assigned to each table,
then we can see improvement in translation. This
can be implemented by making the machine learn
from various iterations of combining and adjusting
the scores accordingly.(Nakov and Ng, 2012) have
indeed shown that results show significant devia-
tions associated with different weights assigned to
the tables.
References
Lo??c Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147?155.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Sahar Ghannay, France Le Mans, and Lo?c Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1?5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt sys-
tems for hindi to english translation. arXiv preprint
arXiv:1404.3992.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Short Papers, pages 81?84.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
90
Machine Translation and MetricsMATR, pages 315?
320. Association for Computational Linguistics.
Preslav Nakov and Hwee Tou Ng. 2012. Improv-
ing statistical machine translation for a resource-
poor language using related resource-rich lan-
guages. Journal of Artificial Intelligence Research,
44(1):179?222.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252?
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Masao Utiyama and Hitoshi Isahara. 2007. A compari-
son of pivot methods for phrase-based statistical ma-
chine translation. In HLT-NAACL, pages 484?491.
Omar Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
91
