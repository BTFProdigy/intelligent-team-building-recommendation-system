Proceedings of ACL-08: HLT, pages 771?779,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Bilingual Lexicons from Monolingual Corpora
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,pliang,tberg,klein }@cs.berkeley.edu
Abstract
We present a method for learning bilingual
translation lexicons from monolingual cor-
pora. Word types in each language are charac-
terized by purely monolingual features, such
as context counts and orthographic substrings.
Translations are induced using a generative
model based on canonical correlation analy-
sis, which explains the monolingual lexicons
in terms of latent matchings. We show that
high-precision lexicons can be learned in a va-
riety of language pairs and from a range of
corpus types.
1 Introduction
Current statistical machine translation systems use
parallel corpora to induce translation correspon-
dences, whether those correspondences be at the
level of phrases (Koehn, 2004), treelets (Galley et
al., 2006), or simply single words (Brown et al,
1994). Although parallel text is plentiful for some
language pairs such as English-Chinese or English-
Arabic, it is scarce or even non-existent for most
others, such as English-Hindi or French-Japanese.
Moreover, parallel text could be scarce for a lan-
guage pair even if monolingual data is readily avail-
able for both languages.
In this paper, we consider the problem of learning
translations from monolingual sources alone. This
task, though clearly more difficult than the standard
parallel text approach, can operate on language pairs
and in domains where standard approaches cannot.
We take as input two monolingual corpora and per-
haps some seed translations, and we produce as out-
put a bilingual lexicon, defined as a list of word
pairs deemed to be word-level translations. Preci-
sion and recall are then measured over these bilin-
gual lexicons. This setting has been considered be-
fore, most notably in Koehn and Knight (2002) and
Fung (1995), but the current paper is the first to use
a probabilistic model and present results across a va-
riety of language pairs and data conditions.
In our method, we represent each language as a
monolingual lexicon (see figure 2): a list of word
types characterized by monolingual feature vectors,
such as context counts, orthographic substrings, and
so on (section 5). We define a generative model over
(1) a source lexicon, (2) a target lexicon, and (3) a
matching between them (section 2). Our model is
based on canonical correlation analysis (CCA)1 and
explains matched word pairs via vectors in a com-
mon latent space. Inference in the model is done
using an EM-style algorithm (section 3).
Somewhat surprisingly, we show that it is pos-
sible to learn or extend a translation lexicon us-
ing monolingual corpora alone, in a variety of lan-
guages and using a variety of corpora, even in the
absence of orthographic features. As might be ex-
pected, the task is harder when no seed lexicon is
provided, when the languages are strongly diver-
gent, or when the monolingual corpora are from dif-
ferent domains. Nonetheless, even in the more diffi-
cult cases, a sizable set of high-precision translations
can be extracted. As an example of the performance
of the system, in English-Spanish induction with our
best feature set, using corpora derived from topically
similar but non-parallel sources, the system obtains
89.0% precision at 33% recall.
1See Hardoon et al (2003) for an overview.
771
state
society
enlarge-
ment
control
import-
ance
sociedad
estado
amplifi-
caci?n
import-
ancia
control
.
.
.
.
.
.
s
t
m
Figure 1: Bilingual lexicon induction: source word types
s are listed on the left and target word types t on the
right. Dashed lines between nodes indicate translation
pairs which are in the matching m.
2 Bilingual Lexicon Induction
As input, we are given a monolingual corpus S (a
sequence of word tokens) in a source language and
a monolingual corpus T in a target language. Let
s = (s1, . . . , snS ) denote nS word types appearing
in the source language, and t = (t1, . . . , tnT ) denote
word types in the target language. Based on S and
T , our goal is to output a matching m between s
and t. We represent m as a set of integer pairs so
that (i, j) ?m if and only if si is matched with tj .
2.1 Generative Model
We propose the following generative model over
matchings m and word types (s, t), which we call
matching canonical correlation analysis (MCCA).
MCCA model
m ? MATCHING-PRIOR [matching m]
For each matched edge (i, j) ?m:
?z
i,j
? N (0, I
d
) [latent concept]
?f
S
(s
i
) ? N (W
S
z
i,j
,?
S
) [source features]
?f
T
(t
i
) ? N (W
T
z
i,j
,?
T
) [target features]
For each unmatched source word type i:
?f
S
(s
i
) ? N (0, ?
2
I
d
S
) [source features]
For each unmatched target word type j:
?f
T
(t
j
) ? N (0, ?
2
I
d
T
) [target features]
First, we generate a matching m ?M, whereM
is the set of matchings in which each word type is
matched to at most one other word type.2 We take
MATCHING-PRIOR to be uniform overM.3
Then, for each matched pair of word types (i, j) ?
m, we need to generate the observed feature vectors
of the source and target word types, fS(si) ? RdS
and fT (tj) ? RdT . The feature vector of each word
type is computed from the appropriate monolin-
gual corpus and summarizes the word?s monolingual
characteristics; see section 5 for details and figure 2
for an illustration. Since si and tj are translations of
each other, we expect fS(si) and fT (tj) to be con-
nected somehow by the generative process. In our
model, they are related through a vector zi,j ? Rd
representing the shared, language-independent con-
cept.
Specifically, to generate the feature vectors, we
first generate a random concept zi,j ? N (0, Id),
where Id is the d ? d identity matrix. The source
feature vector fS(si) is drawn from a multivari-
ate Gaussian with mean WSzi,j and covariance ?S ,
where WS is a dS ? d matrix which transforms the
language-independent concept zi,j into a language-
dependent vector in the source space. The arbitrary
covariance parameter ?S  0 explains the source-
specific variations which are not captured by WS ; it
does not play an explicit role in inference. The target
fT (tj) is generated analogously using WT and ?T ,
conditionally independent of the source given zi,j
(see figure 2). For each of the remaining unmatched
source word types si which have not yet been gen-
erated, we draw the word type features from a base-
line normal distribution with variance ?2IdS , with
hyperparameter ?2  0; unmatched target words
are similarly generated.
If two word types are truly translations, it will be
better to relate their feature vectors through the la-
tent space than to explain them independently via
the baseline distribution. However, if a source word
type is not a translation of any of the target word
types, we can just generate it independently without
requiring it to participate in the matching.
2Our choice ofM permits unmatched word types, but does
not allow words to have multiple translations. This setting facil-
itates comparison to previous work and admits simpler models.
3However, non-uniform priors could encode useful informa-
tion, such as rank similarities.
772
1.0
1.0
20.0
5.0
100.0
50.0
.
.
.
Source 
Space
Canonical 
Space
R
d
s
R
d
t
1.0
1.0
.
.
.
1.0
Target 
Space
R
d
1.0
{
{
O
r
t
h
o
g
r
a
p
h
i
c
 
F
e
a
t
u
r
e
s
C
o
n
t
e
x
t
u
a
l
 
F
e
a
t
u
r
e
s
time
tiempo
#ti
#ti
ime
mpo
me#
pe#
change
dawn
period
necessary
40.0
65.0
120.0
45.0
suficiente
per?odo
mismo
adicional
s
i
t
j
z
f
S
(s
i
)
f
T
(t
j
)
Figure 2: Illustration of our MCCA model. Each latent concept z
i,j
originates in the canonical space. The observed
word vectors in the source and target spaces are generated independently given this concept.
3 Inference
Given our probabilistic model, we would like to
maximize the log-likelihood of the observed data
(s, t):
`(?) = log p(s, t; ?) = log
?
m
p(m, s, t; ?)
with respect to the model parameters ? =
(WS ,WT ,?S ,?T ).
We use the hard (Viterbi) EM algorithm as a start-
ing point, but due to modeling and computational
considerations, we make several important modifi-
cations, which we describe later. The general form
of our algorithm is as follows:
Summary of learning algorithm
E-step: Find the maximum weighted (partial) bi-
partite matching m ?M
M-step: Find the best parameters ? by performing
canonical correlation analysis (CCA)
M-step Given a matching m, the M-step opti-
mizes log p(m, s, t; ?) with respect to ?, which can
be rewritten as
max
?
?
(i,j)?m
log p(si, tj ; ?). (1)
This objective corresponds exactly to maximizing
the likelihood of the probabilistic CCA model pre-
sented in Bach and Jordan (2006), which proved
that the maximum likelihood estimate can be com-
puted by canonical correlation analysis (CCA). In-
tuitively, CCA finds d-dimensional subspaces US ?
R
dS?d of the source and UT ? RdT?d of the tar-
get such that the components of the projections
U
>
S fS(si) and U
>
T fT (tj) are maximally correlated.
4
US and UT can be found by solving an eigenvalue
problem (see Hardoon et al (2003) for details).
Then the maximum likelihood estimates are as fol-
lows: WS = CSSUSP 1/2, WT = CTTUTP 1/2,
?S = CSS ?WSW
>
S , and ?T = CTT ?WTW
>
T ,
where P is a d? d diagonal matrix of the canonical
correlations, CSS = 1|m|
?
(i,j)?m fS(si)fS(si)
> is
the empirical covariance matrix in the source do-
main, and CTT is defined analogously.
E-step To perform a conventional E-step, we
would need to compute the posterior over all match-
ings, which is #P-complete (Valiant, 1979). On the
other hand, hard EM only requires us to compute the
best matching under the current model:5
m = argmax
m?
log p(m?, s, t; ?). (2)
We cast this optimization as a maximum weighted
bipartite matching problem as follows. Define the
edge weight between source word type i and target
word type j to be
wi,j = log p(si, tj ; ?) (3)
? log p(si; ?)? log p(tj ; ?),
4Since dS and dT can be quite large in practice and of-
ten greater than |m|, we use Cholesky decomposition to re-
represent the feature vectors as |m|-dimensional vectors with
the same dot products, which is all that CCA depends on.
5If we wanted softer estimates, we could use the agreement-
based learning framework of Liang et al (2008) to combine two
tractable models.
773
which can be loosely viewed as a pointwise mutual
information quantity. We can check that the ob-
jective log p(m, s, t; ?) is equal to the weight of a
matching plus some constant C:
log p(m, s, t; ?) =
?
(i,j)?m
wi,j + C. (4)
To find the optimal partial matching, edges with
weight wi,j < 0 are set to zero in the graph and the
optimal full matching is computed inO((nS+nT )3)
time using the Hungarian algorithm (Kuhn, 1955). If
a zero edge is present in the solution, we remove the
involved word types from the matching.6
Bootstrapping Recall that the E-step produces a
partial matching of the word types. If too few
word types are matched, learning will not progress
quickly; if too many are matched, the model will be
swamped with noise. We found that it was helpful
to explicitly control the number of edges. Thus, we
adopt a bootstrapping-style approach that only per-
mits high confidence edges at first, and then slowly
permits more over time. In particular, we compute
the optimal full matching, but only retain the high-
est weighted edges. As we run EM, we gradually
increase the number of edges to retain.
In our context, bootstrapping has a similar moti-
vation to the annealing approach of Smith and Eisner
(2006), which also tries to alter the space of hidden
outputs in the E-step over time to facilitate learn-
ing in the M-step, though of course the use of boot-
strapping in general is quite widespread (Yarowsky,
1995).
4 Experimental Setup
In section 5, we present developmental experiments
in English-Spanish lexicon induction; experiments
6Empirically, we obtained much better efficiency and even
increased accuracy by replacing these marginal likelihood
weights with a simple proxy, the distances between the words?
mean latent concepts:
wi,j = A? ||z
?
i ? z
?
j ||2, (5)
where A is a thresholding constant, z?i = E(zi,j | fS(si)) =
P 1/2U>S fS(si), and z
?
j is defined analogously. The increased
accuracy may not be an accident: whether two words are trans-
lations is perhaps better characterized directly by how close
their latent concepts are, whereas log-probability is more sensi-
tive to perturbations in the source and target spaces.
are presented for other languages in section 6. In
this section, we describe the data and experimental
methodology used throughout this work.
4.1 Data
Each experiment requires a source and target mono-
lingual corpus. We use the following corpora:
? EN-ES-W: 3,851 Wikipedia articles with both
English and Spanish bodies (generally not di-
rect translations).
? EN-ES-P: 1st 100k sentences of text from the
parallel English and Spanish Europarl corpus
(Koehn, 2005).
? EN-ES(FR)-D: English: 1st 50k sentences of
Europarl; Spanish (French): 2nd 50k sentences
of Europarl.7
? EN-CH-D: English: 1st 50k sentences of Xin-
hua parallel news corpora;8 Chinese: 2nd 50k
sentences.
? EN-AR-D: English: 1st 50k sentences of 1994
proceedings of UN parallel corpora;9 Ara-
bic: 2nd 50k sentences.
? EN-ES-G: English: 100k sentences of English
Gigaword; Spanish: 100k sentences of Spanish
Gigaword.10
Note that even when corpora are derived from par-
allel sources, no explicit use is ever made of docu-
ment or sentence-level alignments. In particular, our
method is robust to permutations of the sentences in
the corpora.
4.2 Lexicon
Each experiment requires a lexicon for evaluation.
Following Koehn and Knight (2002), we consider
lexicons over only noun word types, although this
is not a fundamental limitation of our model. We
consider a word type to be a noun if its most com-
mon tag is a noun in our monolingual corpus.11 For
7Note that the although the corpora here are derived from a
parallel corpus, there are no parallel sentences.
8LDC catalog # 2002E18.
9LDC catalog # 2004E13.
10These corpora contain no parallel sentences.
11We use the Tree Tagger (Schmid, 1994) for all POS tagging
except for Arabic, where we use the tagger described in Diab et
al. (2004).
774
 0.6 0.65 0.7 0.75 0.8 0.85
 0.9 0.95 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8Precision Recall EN-ES-PEN-ES-W
Figure 3: Example precision/recall curve of our system
on EN-ES-P and EN-ES-W settings. See section 6.1.
all languages pairs except English-Arabic, we ex-
tract evaluation lexicons from the Wiktionary on-
line dictionary. As we discuss in section 7, our ex-
tracted lexicons have low coverage, particularly for
proper nouns, and thus all performance measures are
(sometimes substantially) pessimistic. For English-
Arabic, we extract a lexicon from 100k parallel sen-
tences of UN parallel corpora by running the HMM
intersected alignment model (Liang et al, 2008),
adding (s, t) to the lexicon if s was aligned to t at
least three times and more than any other word.
Also, as in Koehn and Knight (2002), we make
use of a seed lexicon, which consists of a small, and
perhaps incorrect, set of initial translation pairs. We
used two methods to derive a seed lexicon. The
first is to use the evaluation lexicon Le and select
the hundred most common noun word types in the
source corpus which have translations in Le. The
second method is to heuristically induce, where ap-
plicable, a seed lexicon using edit distance, as is
done in Koehn and Knight (2002). Section 6.2 com-
pares the performance of these two methods.
4.3 Evaluation
We evaluate a proposed lexicon Lp against the eval-
uation lexicon Le using the F1 measure in the stan-
dard fashion; precision is given by the number of
proposed translations contained in the evaluation
lexicon, and recall is given by the fraction of pos-
sible translation pairs proposed.12 Since our model
12We should note that precision is not penalized for (s, t) if
s does not have a translation in Le, and recall is not penalized
for failing to recover multiple translations of s.
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ?- 47.4
ORTHO 76.0 81.3 80.1 52.3 55.0
CONTEXT 91.1 81.3 80.2 65.3 58.0
MCCA 87.2 89.7 89.0 89.7 72.0
Table 1: Performance of EDITDIST and our model with
various features sets on EN-ES-W. See section 5.
naturally produces lexicons in which each entry is
associated with a weight based on the model, we can
give a full precision/recall curve (see figure 3). We
summarize these curves with both the best F1 over
all possible thresholds and various precisions px at
recalls x. All reported numbers exclude evaluation
on the seed lexicon entries, regardless of how those
seeds are derived or whether they are correct.
In all experiments, unless noted otherwise, we
used a seed of size 100 obtained from Le and
considered lexicons between the top n = 2, 000
most frequent source and target noun word types
which were not in the seed lexicon; each system
proposed an already-ranked one-to-one translation
lexicon amongst these n words. Where applica-
ble, we compare against the EDITDIST baseline,
which solves a maximum bipartite matching prob-
lem where edge weights are normalized edit dis-
tances. We will use MCCA (for matching CCA) to
denote our model using the optimal feature set (see
section 5.3).
5 Features
In this section, we explore feature representations of
word types in our model. Recall that fS(?) and fT (?)
map source and target word types to vectors in RdS
and RdT , respectively (see section 2). The features
used in each representation are defined identically
and derived only from the appropriate monolingual
corpora. For a concrete example of a word type to
feature vector mapping, see figure 2.
5.1 Orthographic Features
For closely related languages, such as English and
Spanish, translation pairs often share many ortho-
graphic features. One direct way to capture ortho-
graphic similarity between word pairs is edit dis-
tance. Running EDITDIST (see section 4.3) on EN-
775
ES-W yielded 61.1 p0.33, but precision quickly de-
grades for higher recall levels (see EDITDIST in ta-
ble 1). Nevertheless, when available, orthographic
clues are strong indicators of translation pairs.
We can represent orthographic features of a word
type w by assigning a feature to each substring of
length ? 3. Note that MCCA can learn regular or-
thographic correspondences between source and tar-
get words, which is something edit distance cannot
capture (see table 5). Indeed, running our MCCA
model with only orthographic features on EN-ES-
W, labeled ORTHO in table 1, yielded 80.1 p0.33, a
31% error-reduction over EDITDIST in p0.33.
5.2 Context Features
While orthographic features are clearly effective for
historically related language pairs, they are more
limited for other language pairs, where we need to
appeal to other clues. One non-orthographic clue
that word types s and t form a translation pair is
that there is a strong correlation between the source
words used with s and the target words used with t.
To capture this information, we define context fea-
tures for each word type w, consisting of counts of
nouns which occur within a window of size 4 around
w. Consider the translation pair (time, tiempo)
illustrated in figure 2. As we become more con-
fident about other translation pairs which have ac-
tive period and periodico context features, we
learn that translation pairs tend to jointly generate
these features, which leads us to believe that time
and tiempo might be generated by a common un-
derlying concept vector (see section 2).13
Using context features alone on EN-ES-W, our
MCCA model (labeled CONTEXT in table 1) yielded
a 80.2 p0.33. It is perhaps surprising that context fea-
tures alone, without orthographic information, can
yield a best-F1comparable to EDITDIST.
5.3 Combining Features
We can of course combine context and orthographic
features. Doing so yielded 89.03 p0.33 (labeled
MCCA in table 1); this represents a 46.4% error re-
duction in p0.33 over the EDITDIST baseline. For the
remainder of this work, we will use MCCA to refer
13It is important to emphasize, however, that our current
model does not directly relate a word type?s role as a partici-
pant in the matching to that word?s role as a context feature.
(a) Corpus Variation
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES-G 75.0 71.2 68.3 ?- 49.0
EN-ES-W 87.2 89.7 89.0 89.7 72.0
EN-ES-D 91.4 94.3 92.3 89.7 63.7
EN-ES-P 97.3 94.8 93.8 92.9 77.0
(b) Seed Lexicon Variation
Corpus p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ? 47.4
MCCA 91.4 94.3 92.3 89.7 63.7
MCCA-AUTO 91.2 90.5 91.8 77.5 61.7
(c) Language Variation
Languages p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES 91.4 94.3 92.3 89.7 63.7
EN-FR 94.5 89.1 88.3 78.6 61.9
EN-CH 60.1 39.3 26.8 ?- 30.8
EN-AR 70.0 50.0 31.1 ?- 33.1
Table 2: (a) varying type of corpora used on system per-
formance (section 6.1), (b) using a heuristically chosen
seed compared to one taken from the evaluation lexicon
(section 6.2), (c) a variety of language pairs (see sec-
tion 6.3).
to our model using both orthographic and context
features.
6 Experiments
In this section we examine how system performance
varies when crucial elements are altered.
6.1 Corpus Variation
There are many sources from which we can derive
monolingual corpora, and MCCA performance de-
pends on the degree of similarity between corpora.
We explored the following levels of relationships be-
tween corpora, roughly in order of closest to most
distant:
? Same Sentences: EN-ES-P
? Non-Parallel Similar Content: EN-ES-W
? Distinct Sentences, Same Domain: EN-ES-D
? Unrelated Corpora: EN-ES-G
Our results for all conditions are presented in ta-
ble 2(a). The predominant trend is that system per-
formance degraded when the corpora diverged in
776
content, presumably due to context features becom-
ing less informative. However, it is notable that even
in the most extreme case of disjoint corpora from
different time periods and topics (e.g. EN-ES-G),
we are still able to recover lexicons of reasonable
accuracy.
6.2 Seed Lexicon Variation
All of our experiments so far have exploited a small
seed lexicon which has been derived from the eval-
uation lexicon (see section 4.3). In order to explore
system robustness to heuristically chosen seed lexi-
cons, we automatically extracted a seed lexicon sim-
ilarly to Koehn and Knight (2002): we ran EDIT-
DIST on EN-ES-D and took the top 100 most con-
fident translation pairs. Using this automatically de-
rived seed lexicon, we ran our system on EN-ES-
D as before, evaluating on the top 2,000 noun word
types not included in the automatic lexicon.14 Us-
ing the automated seed lexicon, and still evaluat-
ing against our Wiktionary lexicon, MCCA-AUTO
yielded 91.8 p0.33 (see table 2(b)), indicating that
our system can produce lexicons of comparable ac-
curacy with a heuristically chosen seed. We should
note that this performance represents no knowledge
given to the system in the form of gold seed lexicon
entries.
6.3 Language Variation
We also explored how system performance varies
for language pairs other than English-Spanish. On
English-French, for the disjoint EN-FR-D corpus
(described in section 4.1), MCCA yielded 88.3 p0.33
(see table 2(c) for more performance measures).
This verified that our model can work for another
closely related language-pair on which no model de-
velopment was performed.
One concern is how our system performs on lan-
guage pairs where orthographic features are less ap-
plicable. Results on disjoint English-Chinese and
English-Arabic are given as EN-CH-D and EN-AR
in table 2(c), both using only context features. In
these cases, MCCA yielded much lower precisions
of 26.8 and 31.0 p0.33, respectively. For both lan-
guages, performance degraded compared to EN-ES-
14Note that the 2,000 words evaluated here were not identical
to the words tested on when the seed lexicon is derived from the
evaluation lexicon.
(a) English-Spanish
Rank Source Target Correct
1. education educaci?n Y
2. pacto pact Y
3. stability estabilidad Y
6. corruption corrupci?n Y
7. tourism turismo Y
9. organisation organizaci?n Y
10. convenience conveniencia Y
11. syria siria Y
12. cooperation cooperaci?n Y
14. culture cultura Y
21. protocol protocolo Y
23. north norte Y
24. health salud Y
25. action reacci?n N
(b) English-French
Rank Source Target Correct
3. xenophobia x?nophobie Y
4. corruption corruption Y
5. subsidiarity subsidiarit? Y
6. programme programme-cadre N
8. traceability tra?abilit? Y
(c) English-Chinese
Rank Source Target Correct
1. prices ? Y
2. network ? Y
3. population ? Y
4. reporter ? N
5. oil ? Y
Table 3: Sample output from our (a) Spanish, (b) French,
and (c) Chinese systems. We present the highest con-
fidence system predictions, where the only editing done
is to ignore predictions which consist of identical source
and target words.
D and EN-FR-D, presumably due in part to the
lack of orthographic features. However, MCCA still
achieved surprising precision at lower recall levels.
For instance, at p0.1, MCCA yielded 60.1 and 70.0
on Chinese and Arabic, respectively. Figure 3 shows
the highest-confidence outputs in several languages.
6.4 Comparison To Previous Work
There has been previous work in extracting trans-
lation pairs from non-parallel corpora (Rapp, 1995;
Fung, 1995; Koehn and Knight, 2002), but gener-
ally not in as extreme a setting as the one consid-
ered here. Due to unavailability of data and speci-
ficity in experimental conditions and evaluations, it
is not possible to perform exact comparisons. How-
777
(a) Example Non-Cognate Pairs
health salud
traceability rastreabilidad
youth juventud
report informe
advantages ventajas
(b) Interesting Incorrect Pairs
liberal partido
Kirkhope Gorsel
action reaccio?n
Albanians Bosnia
a.m. horas
Netherlands Bretan?a
Table 4: System analysis on EN-ES-W: (a) non-cognate
pairs proposed by our system, (b) hand-selected represen-
tative errors.
(a) Orthographic Feature
Source Feat. Closest Target Feats. Example Translation
#st #es, est (statue, estatua)
ty# ad#, d# (felicity, felicidad)
ogy g??a, g?? (geology, geolog??a)
(b) Context Feature
Source Feat. Closest Context Features
party partido, izquierda
democrat socialistas, demo?cratas
beijing pek??n, kioto
Table 5: Hand selected examples of source and target fea-
tures which are close in canonical space: (a) orthographic
feature correspondences, (b) context features.
ever, we attempted to run an experiment as similar
as possible in setup to Koehn and Knight (2002), us-
ing English Gigaword and German Europarl. In this
setting, our MCCA system yielded 61.7% accuracy
on the 186 most confident predictions compared to
39% reported in Koehn and Knight (2002).
7 Analysis
We have presented a novel generative model for
bilingual lexicon induction and presented results un-
der a variety of data conditions (section 6.1) and lan-
guages (section 6.3) showing that our system can
produce accurate lexicons even in highly adverse
conditions. In this section, we broadly characterize
and analyze the behavior of our system.
We manually examined the top 100 errors in the
English-Spanish lexicon produced by our system
on EN-ES-W. Of the top 100 errors: 21 were cor-
rect translations not contained in the Wiktionary
lexicon (e.g. pintura to painting), 4 were
purely morphological errors (e.g. airport to
aeropuertos), 30 were semantically related (e.g.
basketball to be?isbol), 15 were words with
strong orthographic similarities (e.g. coast to
costas), and 30 were difficult to categorize and
fell into none of these categories. Since many of
our ?errors? actually represent valid translation pairs
not contained in our extracted dictionary, we sup-
plemented our evaluation lexicon with one automat-
ically derived from 100k sentences of parallel Eu-
roparl data. We ran the intersected HMM word-
alignment model (Liang et al, 2008) and added
(s, t) to the lexicon if s was aligned to t at least
three times and more than any other word. Evaluat-
ing against the union of these lexicons yielded 98.0
p0.33, a significant improvement over the 92.3 us-
ing only the Wiktionary lexicon. Of the true errors,
the most common arose from semantically related
words which had strong context feature correlations
(see table 4(b)).
We also explored the relationships our model
learns between features of different languages. We
projected each source and target feature into the
shared canonical space, and for each projected
source feature we examined the closest projected
target features. In table 5(a), we present some of
the orthographic feature relationships learned by our
system. Many of these relationships correspond to
phonological and morphological regularities such as
the English suffix ing mapping to the Spanish suf-
fix g??a. In table 5(b), we present context feature
correspondences. Here, the broad trend is for words
which are either translations or semantically related
across languages to be close in canonical space.
8 Conclusion
We have presented a generative model for bilingual
lexicon induction based on probabilistic CCA. Our
experiments show that high-precision translations
can be mined without any access to parallel corpora.
It remains to be seen how such lexicons can be best
utilized, but they invite new approaches to the statis-
tical translation of resource-poor languages.
778
References
Francis R. Bach and Michael I. Jordan. 2006. A proba-
bilistic interpretation of canonical correlation analysis.
Technical report, University of California, Berkeley.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of arabic text: From raw text to
base phrase chunks. In HLT-NAACL.
Pascale Fung. 1995. Compiling bilingual lexicon entries
from a non-parallel english-chinese corpus. In Third
Annual Workshop on Very Large Corpora.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
David R. Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2003. Canonical correlation analysis an
overview with application to learning methods. Tech-
nical Report CSD-TR-03-02, Royal Holloway Univer-
sity of London.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of ACL Workshop on Unsupervised Lexical
Acquisition.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In NIPS.
Reinhard Rapp. 1995. Identifying word translation in
non-parallel texts. In ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
N. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In ACL.
L. G. Valiant. 1979. The complexity of computing
the permanent. Theoretical Computer Science, 8:189?
201.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL.
779
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313?321,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Simple Effective Decipherment via Combinatorial Optimization
Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, klein}@cs.berkeley.edu
Abstract
We present a simple objective function that
when optimized yields accurate solutions to
both decipherment and cognate pair identifica-
tion problems. The objective simultaneously
scores a matching between two alphabets and
a matching between two lexicons, each in a
different language. We introduce a simple
coordinate descent procedure that efficiently
finds effective solutions to the resulting com-
binatorial optimization problem. Our system
requires only a list of words in both languages
as input, yet it competes with and surpasses
several state-of-the-art systems that are both
substantially more complex and make use of
more information.
1 Introduction
Decipherment induces a correspondence between
the words in an unknown language and the words
in a known language. We focus on the setting where
a close correspondence between the alphabets of the
two languages exists, but is unknown. Given only
two lists of words, the lexicons of both languages,
we attempt to induce the correspondence between
alphabets and identify the cognates pairs present in
the lexicons. The system we propose accomplishes
this by defining a simple combinatorial optimiza-
tion problem that is a function of both the alphabet
and cognate matchings, and then induces correspon-
dences by optimizing the objective using a block co-
ordinate descent procedure.
There is a range of past work that has var-
iously investigated cognate detection (Kondrak,
2001; Bouchard-Co?te? et al, 2007; Bouchard-Co?te?
et al, 2009; Hall and Klein, 2010), character-level
decipherment (Knight and Yamada, 1999; Knight
et al, 2006; Snyder et al, 2010; Ravi and Knight,
2011), and bilingual lexicon induction (Koehn and
Knight, 2002; Haghighi et al, 2008). We consider
a common element, which is a model wherein there
are character-level correspondences and word-level
correspondences, with the word matching parame-
terized by the character one. This approach sub-
sumes a range of past tasks, though of course past
work has specialized in interesting ways.
Past work has emphasized the modeling as-
pect, where here we use a parametrically simplistic
model, but instead emphasize inference.
2 Decipherment as Two-Level
Optimization
Our method represents two matchings, one at the al-
phabet level and one at the lexicon level. A vector of
variables x specifies a matching between alphabets.
For each character i in the source alphabet and each
character j in the target alhabet we define an indi-
cator variable xij that is on if and only if character i
is mapped to character j. Similarly, a vector y rep-
resents a matching between lexicons. For word u in
the source lexicon and word v in the target lexicon,
the indicator variable yuv denotes that u maps to v.
Note that the matchings need not be one-to-one.
We define an objective function on the matching
variables as follows. Let EDITDIST(u, v;x) denote
the edit distance between source word u and target
word v given alphabet matching x. Let the length
of word u be lu and the length of word w be lw.
This edit distance depends on x in the following
way. Insertions and deletions always cost a constant
.1 Substitutions also cost  unless the characters
are matched in x, in which case the substitution is
1In practice we set  = 1lu+lv . lu + lv is the maximumnumber of edit operations between words u and v. This nor-
malization insures that edit distances are between 0 and 1 for
all pairs of words.
313
free. Now, the objective that we will minimize can
be stated simply: ?u
?
v yuv ? EDITDIST(u, v;x),
the sum of the edit distances between the matched
words, where the edit distance function is parame-
terized by the alphabet matching.
Without restrictions on the matchings x and y
this objective can always be driven to zero by either
mapping all characters to all characters, or matching
none of the words. It is thus necessary to restrict
the matchings in some way. Let I be the size of
the source alphabet and J be the size of the target
alphabet. We allow the alphabet matching x to
be many-to-many but require that each character
participate in no more than two mappings and that
the total number of mappings be max(I, J), a
constraint we refer to as restricted-many-to-many.
The requirements can be encoded with the following
linear constraints on x:
?i
?
j
xij ? 2
?j
?
i
xij ? 2
?
i
?
j
xij = max(I, J)
The lexicon matching y is required to be ? -one-to-
one. By this we mean that y is an at-most-one-to-one
matching that covers proportion ? of the smaller of
the two lexicons. Let U be the size of the source
lexicon and V be this size of the target lexicon.
This requirement can be encoded with the following
linear constraints:
?u
?
v
yuv ? 1
?v
?
u
yuv ? 1
?
u
?
v
yuv = ? min(U, V )
Now we are ready to define the full optimization
problem. The first formulation is called the Implicit
Matching Objective since includes an implicit
minimization over edit alignments inside the com-
putation of EDITDIST.
(1) Implicit Matching Objective:
min
x,y
?
u
?
v
yuv ? EDITDIST(u, v;x)
s.t. x is restricted-many-to-many
y is ? -one-to-one
In order to get a better handle on the shape of the
objective and to develop an efficient optimization
procedure we decompose each edit distance compu-
tation and re-formulate the optimization problem in
Section 2.2.
2.1 Example
Figure 1 presents both an example matching prob-
lem and a diagram of the variables and objective.
Here, the source lexicon consists of the English
words (cat, bat, cart, rat, cab), and
the source alphabet consists of the characters (a,
b, c, r, t). The target alhabet is (0, 1,
2, 3). We have used digits as symbols in the target
alphabet to make it clear that we treat the alphabets
as disjoint. We have no prior knowledge about any
correspondence between alphabets, or between lexi-
cons.
The target lexicon consists of the words (23,
1233, 120, 323, 023). The bipartite graphs
show a specific setting of the matching variables.
The bold edges correspond to the xij and yuv that
are one. The matchings shown achieve an edit dis-
tance of zero between all matched word pairs ex-
cept for the pair (cat, 23). The best edit align-
ment for this pair is also diagrammed. Here, ?a?
is aligned to ?2?, ?t? is aligned to ?3?, and ?c? is
deleted and therefore aligned to the null position ?#?.
Only the initial deletion has a non-zero cost since
all other alignments correspond to substitutions be-
tween characters that are matched in x.
2.2 Explicit Objective
Computing EDITDIST(u, v;x) requires running a
dynamic program because of the unknown edit
alignments; here we define those alignments z ex-
plicitly, which makes the EDITDIST(u, v;x) easy to
write explicitly at the cost of more variables. How-
ever, by writing the objective in an explicit form that
refers to these edit variables, we are able to describe
a efficient block coordinate descent procedure that
can be used for optimization.
EDITDIST(u, v;x) is computed by minimizing
over the set of monotonic alignments between the
characters of the source word u and the characters
of the target word v. Let un be the character at the
nth position of the source word u, and similarly for
314
a
b
c
r
t
0
1
2
3
Alphabet Matching
Lexicon Matching
xij
cat
bat
rat
cart
cab
1233
120
323
023
23
yuv
Edit Distance
c
a
t
2
3
# # EditDist(u, v;x) =
min
x,y
?
u
?
v
yuv ? EditDist(u, v;x)
s.t. x is restricted-many-to-many
y is ? -one-to-one
Matching Problem
Insertion
? ?
?
Substitution
Deletion
? ?
??
n
?
m
(1? xunvm)zuv,nm
+
?
n
zuv,n# +
?
m
zuv,#m
?s.t.
minzuv
zuv is monotonic
zuv,nm
Figure 1: An example problem displaying source and target lexicons and alphabets, along with specific matchings.
The variables involved in the optimization problem are diagrammed. x are the alphabet matching indicator variables,
y are the lexicon matching indicator variables, and z are the edit alignment indicator variables. The index u refers to
a word in the source lexicon, v refers to word in the target lexicon, i refers to a character in the source alphabet, and
j refers to a character in the target alhabet. n and m refer to positions in source and target words respectively. The
matching objective function is also shown.
vm. Let zuv be the vector of alignment variables
for the edit distance computation between source
word u and target word v, where entry zuv,nm
indicates whether the character at position n of
source word u is aligned to the character at position
m of target word v. Additionally, define variables
zuv,n# and zuv,#m denoting null alignments, which
will be used to keep track of insertions and deletions.
EDITDIST(u, v;x) =
min
zuv
 ?
(
SUB(zuv, x) + DEL(zuv) + INS(zuv)
)
s.t. zuv is monotonic
We define SUB(zuv, x) to be the number of sub-
stitutions between characters that are not matched
in x, DEL(zuv) to be the number of deletions, and
INS(zuv) to be the number of insertions.
SUB(zuv, x) =
?
n,m
(1? xunvm)zuv,nm
DEL(zuv) =
?
n
zuv,n#
INS(zuv) =
?
m
zuv,#m
Notice that the variable zuv,nm being turned on in-
dicates the substitute operation, while a zuv,n# or
zuv,#m being turned on indicates an insert or delete
operation. These variables are digrammed in Fig-
ure 1. The requirement that zuv be a monotonic
alignment can be expressed using linear constraints,
but in our optimization procedure (described in Sec-
tion 3) these constraints need not be explicitly rep-
resented.
Now we can substitute the explicit edit distance
equation into the implicit matching objective (1).
315
Noticing that the mins and sums commute, we arrive
at the explicit form of the matching optimization
problem.
(2) Explicit Matching Objective:
min
x,y,z
[?
u,v
yuv ?  ?
(SUB(zuv, x) + DEL(zuv) + INS(zuv))
]
s.t. x is restricted-many-to-many
y is ? -one-to-one
?uv zuv is monotonic
The implicit and explicit optimizations are the same,
apart from the fact that the explicit optimization now
explicitly represents the edit alignment variables z.
Let the explicit matching objective (2) be denoted
as J(x, y, z). The relaxation of the explicit problem
with 0-1 constraints removed has integer solutions,2
however the objective J(x, y, z) is non-convex. We
thus turn to a block coordinate descent method in the
next section in order to find local optima.
3 Optimization Method
We now state a block coordinate descent procedure
to find local optima of J(x, y, z) under the con-
straints on x, y, and z. This procedure alternates
between updating y and z to their exact joint optima
when x is held fixed, and updating x to its exact op-
timum when y and z are held fixed.
The psuedocode for the procedure is given in Al-
gorithm 1. Note that the function EDITDIST returns
both the min edit distance euv and the argmin edit
alignments zuv. Also note that cij is as defined in
Section 3.2.
3.1 Lexicon Matching Update
Let x, the alphabet matching variable, be fixed. We
consider the problem of optimizing J(x, y, z) over
the lexicon matching variable y and and the edit
alignments z under the constraint that y is ? -one-
to-one and each zuv is monotonic.
2This can be shown by observing that optimizing x when y
and z are held fixed yields integer solutions (shown in Section
3.2), and similarly for the optimization of y and z when x is
fixed (shown in Section 3.1). Thus, every local optimum with
respect to these block coordinate updates has integer solutions.
The global optimum must be one of these local optima.
Algorithm 1 Block Coordinate Descent
Randomly initialize alphabet matching x.
repeat
for all u, v do
(euv, zuv)? EDITDIST(u, v;x)
end for
[Hungarian]
y ? argminy ? -one-to-one
[?
u,v yuveuv
]
[Solve LP]
x? argmaxx restr.-many-to-many
[?
i,j xijcij
]
until convergence
Notice that y simply picks out which edit distance
problems affect the objective. The zuv in each of
these edit distance problems can be optimized in-
dependently. zuv that do not have yuv active have
no effect on the objective, and zuv with yuv active
can be optimized using the standard edit distance dy-
namic program. Thus, in a first step we compute the
U ? V edit distances euv and best monotonic align-
ment variables zuv between all pairs of source and
target words usingU ?V calls to the standard edit dis-
tance dynamic program. Altogether, this takes time
O
(
(
?
u lu) ? (
?
v lv)
).
Now, in a second step we compute the least
weighted ? -one-to-one matching y under the
weights euv. This can be accomplished in time
O(max(U, V )3) using the Hungarian algorithm
(Kuhn, 1955). These two steps produce y and z that
exactly achieve the optimum value of J(x, y, z) for
the given value of x.
3.2 Alphabet Matching Update
Let y and z, the lexicon matching variables and the
edit alignments, be fixed. Now, we find the optimal
alphabet matching variables x subject to the con-
straint that x is restricted-many-to-many.
It makes sense that to optimize J(x, y, z) with re-
spect to x we should prioritize mappings xij that
would mitigate the largest substitution costs in the
active edit distance problems. Indeed, with a little
algebra it can be shown that solving a maximum
weighted matching problem with weights cij that
count potential substitution costs gives the correct
update for x. In particular, cij is the total cost of
substitution edits in the active edit alignment prob-
316
lems that would result if source character i were not
mapped to target character j in the alphabet match-
ing x. This can be written as:
cij =
?
u,v
?
n,m s.t. un=i,vm=j
 ? yuv ? zuv,nm
If x were constrained to be one-to-one, we
could again apply the Hungarian algorithm, this
time to find a maximum weighted matching under
the weights cij . Since we have instead allowed
restricted-many-to-many alphabet matchings we
turn to linear programming for optimizing x. We
can state the update problem as the following linear
program (LP), which is guaranteed to have integer
solutions:
min
x
?
ij
xijcij
s.t. ?i
?
j
xij ? 2, ?j
?
i
xij ? 2
?
i
?
j
xij = max(I, J)
In experiments we used the GNU Linear Program-
ming Toolkit (GLPK) to solve the LP and update
the alphabet matching x. This update yields match-
ing variables x that achieve the optimum value of
J(x, y, z) for fixed y and z.
3.3 Random Restarts
In practice we found that the block coordinate de-
scent procedure can get stuck at poor local optima.
To find better optima, we run the coordinate descent
procedure multiple times, initialized each time with
a random alphabet matching. We choose the local
optimum with the best objective value across all ini-
tializations. This approach yielded substantial im-
provements in achieved objective value.
4 Experiments
We compare our system to three different state-of-
the-art systems on three different data sets. We set
up experiments that allow for as direct a comparison
as possible. In some cases it must be pointed out
that the past system?s goals are different from our
own, and we will be comparing in a different way
than the respective work was intended. The three
systems make use of additional, or slightly different,
sources of information.
4.1 Phonetic Cognate Lexicons
The first data set we evaluate on consists of 583
triples of phonetic transcriptions of cognates in
Spanish, Portuguese, and Italian. The data set was
introduced by Bouchard-Co?te? et al (2007). For a
given pair of languages the task is to determine the
mapping between lexicons that correctly maps each
source word to its cognate in the target lexicon. We
refer to this task and data set as ROMANCE.
Hall and Klein (2010) presented a state-of-the-
art system for the task of cognate identification and
evaluated on this data set. Their model explicitly
represents parameters for phonetic change between
languages and their parents in a phylogenetic tree.
They estimate parameters and infer the pairs of cog-
nates present in all three languages jointly, while we
consider each pair of languages in turn.
Their model has similarities with our own in that
it learns correspondences between the alphabets of
pairs of languages. However, their correspondences
are probabilistic and implicit while ours are hard and
explicit. Their model also differs from our own in
a key way. Notice that the phonetic alphabets for
the three languages are actually the same. Since
phonetic change occurs gradually across languages
a helpful prior on the correspondence is to favor the
identity. Their model makes use of such a prior.
Our model, on the other hand, is unaware of any
prior correspondence between alphabets and does
not make use of this additional information about
phonetic change.
Hall and Klein (2010) also evaluate their model
on lexicons that do not have a perfect cognate map-
ping. This scenario, where not every word in one
language has a cognate in another, is more realistic.
They produced a data set with this property by prun-
ing words from the ROMANCE data set until only
about 75% of the words in each source lexicon have
cognates in each target lexicon. We refer to this task
and data set as PARTIALROMANCE.
4.2 Lexicons Extracted from Corpora
Next, we evaluate our model on a noisier data set.
Here the lexicons in source and target languages
are extracted from corpora by taking the top 2,000
words in each corpus. In particular, we used the En-
glish and Spanish sides of the Europarl parallel cor-
317
pus (Koehn, 2005). To make this set up more real-
istic (though fairly comparable), we insured that the
corpora were non-parallel by using the first 50K sen-
tences on the English side and the second 50K sen-
tences on the Spanish side. To generate a gold cog-
nate matching we used the intersected HMM align-
ment model of Liang et al (2008) to align the full
parallel corpus. From this alignment we extracted a
translation lexicon by adding an entry for each word
pair with the property that the English word was
aligned to the Spanish in over 10% of the alignments
involving the English word. To reduce this transla-
tion lexicon down to a cognate matching we went
through the translation lexicon by hand and removed
any pair of words that we judged to not be cognates.
The resulting gold matching contains cognate map-
pings in the English lexicon for 1,026 of the words
in the Spanish lexicon. This means that only about
50% of the words in English lexicon have cognates
in the Spanish lexicon. We evaluate on this data set
by computing precision and recall for the number of
English words that are mapped to a correct cognate.
We refer to this task and data set as EUROPARL.
On this data set, we compare against the state-of-
the-art orthographic system presented in Haghighi
et al (2008). Haghighi et al (2008) presents sev-
eral systems that are designed to extract transla-
tion lexicons for non-parallel corpora by learning
a correspondence between their monolingual lexi-
cons. Since our system specializes in matching cog-
nates and does not take into account additional infor-
mation from corpus statistics, we compare against
the version of their system that only takes into ac-
count orthographic features and is thus is best suited
for cognate detection. Their system requires a small
seed of correct cognate pairs. From this seed the sys-
tem learns a projection using canonical correlation
analysis (CCA) into a canonical feature space that
allows feature vectors from source words and target
words to be compared. Once in this canonical space,
similarity metrics can be computed and words can be
matched using a bipartite matching algorithm. The
process is iterative, adding cognate pairs to the seed
lexicon gradually and each time re-computing a re-
fined projection. Our system makes no use of a seed
lexicon whatsoever.
Both our system and the system of Haghighi et
al. (2008) must solve bipartite matching problems
between the two lexicons. For this data set, the lexi-
cons are large enough that finding the exact solution
can be slow. Thus, in all experiments on this data
set, we instead use a greedy competitive linking al-
gorithm that runs in time O(U2V 2log(UV )).
Again, for this dataset it is reasonable to expect
that many characters will map to themselves in the
best alphabet matching. The alphabets are not iden-
tical, but are far from disjoint. Neither our system,
nor that of Haghighi et al (2008) make use of this
expectation. As far as both systems are concerned,
the alphabets are disjoint.
4.3 Decipherment
Finally, we evaluate our model on a data set where
a main goal is to decipher an unknown correspon-
dence between alphabets. We attempt to learn a
mapping from the alphabet of the ancient Semitic
language Ugaritic to the alphabet of Hebrew, and
at the same time learn a matching between Hebrew
words in a Hebrew lexicon and their cognates in a
Ugaritic lexicon. This task is related to the task at-
tempted by Snyder et al (2010). The data set con-
sists of a Ugaritic lexicon of 2,214 words, each of
which has a Hebrew cognate, the lexicon of their
2,214 Hebrew cognates, and a gold cognate dictio-
nary for evaluation. We refer to this task and data set
as UGARITIC.
The non-parameteric Bayesian system of Snyder
et al (2010) assumes that the morphology of He-
brew is known, making use of an inventory of suf-
fixes, prefixes, and stems derived from the words
in the Hebrew bible. It attempts to learn a corre-
spondence between the morphology of Ugaritic and
that of Hebrew while reconstructing cognates for
Ugaritic words. This is a slightly different goal than
that of our system, which learns a correspondence
between lexicons. Snyder et al (2010) run their
system on a set 7,386 Ugaritic words, the same set
that we extracted our 2,214 Ugaritic words with He-
brew cognates from. We evaluate the accuracy of the
lexicon matching produced by our system on these
2,214 Ugaritic words, and so do they, measuring the
number of correctly reconstructed cognates.
By restricting the source and target lexicons to
sets of cognates we have made the task easier. This
was necessary, however, because the Ugaritic and
Hebrew corpora used by Snyder et al (2010) are not
318
Model ? Accuracy
Hall and Klein (2010) ? 90.3
MATCHER 1.0 90.1
Table 1: Results on ROMANCE data set. Our system is
labeled MATCHER. We compare against the phylogenetic
cognate detection system of Hall and Klein (2010). We
show the pairwise cognate accuracy across all pairs of
languages from the following set: Spanish, Portuguese,
and Italian.
comparable: only a small proportion of the words
in the Ugaritic lexicon have cognates in the lexicon
composed of the most frequent Hebrew words.
Here, the alphabets really are disjoint. The sym-
bols in both languages look nothing alike. There is
no obvious prior expectation about how the alpha-
bets will be matched. We evaluate against a well-
established correspondence between the alphabets
of Ugaritic and Hebrew. The Ugaritic alphabet con-
tains 30 characters, the Hebrew alphabet contains 22
characters, and the gold matching contains 33 en-
tries. We evaluate the learned alphabet matching by
counting the number of recovered entries from the
gold matching.
Due to the size of the source and target lexicons,
we again use the greedy competitive linking algo-
rithm in place of the exact Hungarian algorithm in
experiments on this data set.
5 Results
We present results on all four datasets ROMANCE,
PARTIALROMANCE, EUROPARL, and UGARITIC.
On the ROMANCE and PARTIALROMANCE data sets
we compare against the numbers published by Hall
and Klein (2010). We ran an implementation of
the orthographic system presented by Haghighi et
al. (2008) on our EUROPARL data set. We com-
pare against the numbers published by Snyder et al
(2010) on the UGARITIC data set. We refer to our
system as MATCHER in result tables and discussion.
5.1 ROMANCE
The results of running our system, MATCHER, on
the ROMANCE data set are shown in Table 1. We
recover 88.9% of the correct cognate mappings on
the pair Spanish and Italian, 85.7% on Italian and
Portuguese, and 95.6% on Spanish and Portuguese.
Model ? Precision Recall F1
Hall and Klein (2010) ? 66.9 82.0 73.6
MATCHER 0.25 99.7 34.0 50.7
0.50 93.8 60.2 73.3
0.75 81.1 78.0 79.5
Table 2: Results on PARTIALROMANCE data set. Our
system is labeled MATCHER. We compare against the
phylogenetic cognate detection system of Hall and Klein
(2010). We show the pairwise cognate precision, recall,
and F1 across all pairs of languages from the following
set: Spanish, Portuguese, and Italian. Note that approx-
imately 75% of the source words in each of the source
lexicons have cognates in each of the target lexicons.
Our average accuracy across all pairs of languages
is 90.1%. The phylogenetic system of Hall and
Klein (2010) achieves an average accuracy of 90.3%
across all pairs of languages. Our system achieves
accuracy comparable to that of the phylogenetic sys-
tem, despite the fact that the phylogenetic system is
substantially more complex and makes use of an in-
formed prior on alphabet correspondences.
The alphabet matching learned by our system is
interesting to analyze. For the pairing of Span-
ish and Portuguese it recovers phonetic correspon-
dences that are well known. Our system learns the
correct cognate pairing of Spanish /bino/ to Por-
tuguese /vinu/. This pair exemplifies two com-
mon phonetic correspondences for Spanish and Por-
tuguese: the Spanish /o/ often transforms to a /u/ in
Portuguese, and Spanish /b/ often transforms to /v/
in Portuguese. Our system, which allows many-to-
many alphabet correspondences, correctly identifies
the mappings /o/? /u/ and /b/? /v/ as well as the
identity mappings /o/? /o/ and /b/? /b/ which are
also common.
5.2 PARTIALROMANCE
In Table 2 we present the results of running our sys-
tem on the PARTIALROMANCE data set. In this data
set, only approximately 75% of the source words in
each of the source lexicons have cognates in each of
the target lexicons. The parameter ? trades off pre-
cision and recall. We show results for three different
settings of ? : 0.25, 0.5, and 0.75.
Our system achieves an average precision across
language pairs of 99.7% at an average recall of
34.0%. For the pairs Italian ? Portuguese, and Span-
319
Model Seed ? Precision Recall F1
Haghighi et al (2008) 20 0.1 72.0 14.0 23.5
20 0.25 63.6 31.0 41.7
20 0.5 44.8 43.7 44.2
50 0.1 90.5 17.6 29.5
50 0.25 75.4 36.7 49.4
50 0.5 56.4 55.0 55.7
MATCHER 0 0.1 93.5 18.2 30.5
0 0.25 83.2 40.5 54.5
0 0.5 56.5 55.1 55.8
Table 3: Results on EUROPARL data set. Our system
is labeled MATCHER. We compare against the bilingual
lexicon induction system of Haghighi et al (2008). We
show the cognate precision, recall, and F1 for the pair of
languages English and Spanish using lexicons extracted
from corpora. Note that approximately 50% of the words
in the English lexicon have cognates in the Spanish lexi-
con.
ish ? Portuguese, our system achieves prefect preci-
sion at recalls of 32.2% and 38.1% respectively. The
best average F1 achieved by our system is 79.5%,
which surpasses the average F1 of 73.6 achieved by
the phylogenetic system of Hall and Klein (2010).
The phylogenetic system observes the phyloge-
netic tree of ancestry for the three languages and
explicitly models cognate evolution and survival in
a ?survival? tree. One might expect the phyloge-
netic system to achieve better results on this data set
where part of the task is identifying which words do
not have cognates. It is surprising that our model
does so well given its simplicity.
5.3 EUROPARL
Table 3 presents results for our system on the EU-
ROPARL data set across three different settings of ? :
0.1, 0.25, and 0.5. We compare against the ortho-
graphic system presented by Haghighi et al (2008),
across the same three settings of ? , and with two dif-
ferent sizes of seed lexicon: 20 and 50. In this data
set, only approximately 50% of the source words
have cognates in the target lexicon.
Our system achieves a precision of 93.5% at a re-
call of 18.2%, and a best F1 of 55.0%. Using a seed
matching of 50 word pairs, the orthographic sys-
tem of Haghighi et al (2008) achieves a best F1 of
55.7%. Using a seed matching of 20 word pairs,
it achieves a best F1 of 44.2%. Our system out-
performs the orthographic system even though the
orthographic system makes use of important addi-
Model ? Lexicon Acc. Alphabet Acc.
Snyder et al (2010) ? 60.4* 29/33*
MATCHER 1.0 90.4 28/33
Table 4: Results on UGARITIC data set. Our system is la-
beled MATCHER. We compare against the decipherment
system of Snyder et al (2010). *Note that results for this
system are on a somewhat different task. In particular, the
MATCHER system assumes the inventories of cognates in
both Hebrew and Ugaritic are known, while the system
of Snyder et al (2010) reconstructs cognates assuming
only that the morphology of Hebrew is known, which is a
harder task. We show cognate pair identification accuracy
and alphabet matching accuracy for Ugaritic and Hebrew.
tional information: a seed matching of correct cog-
nate pairs. The results show that as the size of
this seed is decreased, the performance of the ortho-
graphic system degrades.
5.4 UGARITIC
In Table 4 we present results on the UGARITIC data
set. We evaluate both accuracy of the lexicon match-
ing learned by our system, and the accuracy of the
alphabet matching. Our system achieves a lexicon
accuracy of 90.4% while correctly identifying 28 out
the 33 gold character mappings.
We also present the results for the decipherment
model of Snyder et al (2010) in Table 4. Note that
while the evaluation data sets for our two models
are the same, the tasks are very different. In par-
ticular, our system assumes the inventories of cog-
nates in both Hebrew and Ugaritic are known, while
the system of Snyder et al (2010) reconstructs cog-
nates assuming only that the morphology of Hebrew
is known, which is a harder task. Even so, the re-
sults show that our system is effective at decipher-
ment when semantically similar lexicons are avail-
able.
6 Conclusion
We have presented a simple combinatorial model
that simultaneously incorporates both a matching
between alphabets and a matching between lexicons.
Our system is effective at both the tasks of cognate
identification and alphabet decipherment, requiring
only lists of words in both languages as input.
320
References
A. Bouchard-Co?te?, P. Liang, T.L. Griffiths, and D. Klein.
2007. A probabilistic approach to diachronic phonol-
ogy. In Proc. of EMNLP.
A. Bouchard-Co?te?, T.L. Griffiths, and D. Klein.
2009. Improved reconstruction of protolanguage word
forms. In Proc. of NAACL.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. Proceedings of ACL.
D. Hall and D. Klein. 2010. Finding cognate groups
using phylogenies. In Proc. of ACL.
K. Knight and K. Yamada. 1999. A computational ap-
proach to deciphering unknown scripts. In Proc. of
ACL Workshop on Unsupervised Learning in Natural
Language Processing.
K. Knight, A. Nair, N. Rathod, and K. Yamada. 2006.
Unsupervised analysis for decipherment problems. In
Proc. of COLING/ACL.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proc. of ACL
workshop on Unsupervised lexical acquisition.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In Proc. of Machine Trans-
lation Summit.
G. Kondrak. 2001. Identifying Cognates by Phonetic and
Semantic Similarity. In NAACL.
H.W. Kuhn. 1955. The Hungarian method for the assign-
ment problem. Naval research logistics quarterly.
P. Liang, D. Klein, and M.I. Jordan. 2008. Agreement-
based learning. Proc. of NIPS.
S. Ravi and K. Knight. 2011. Bayesian inference for Zo-
diac and other homophonic ciphers. In Proc. of ACL.
B. Snyder, R. Barzilay, and K. Knight. 2010. A statisti-
cal model for lost language decipherment. In Proc. of
ACL.
321
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 995?1005, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
An Empirical Investigation of Statistical Significance in NLP
Taylor Berg-Kirkpatrick David Burkett Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, dburkett, klein}@cs.berkeley.edu
Abstract
We investigate two aspects of the empirical
behavior of paired significance tests for NLP
systems. First, when one system appears
to outperform another, how does significance
level relate in practice to the magnitude of the
gain, to the size of the test set, to the similar-
ity of the systems, and so on? Is it true that for
each task there is a gain which roughly implies
significance? We explore these issues across
a range of NLP tasks using both large collec-
tions of past systems? outputs and variants of
single systems. Next, once significance lev-
els are computed, how well does the standard
i.i.d. notion of significance hold up in practical
settings where future distributions are neither
independent nor identically distributed, such
as across domains? We explore this question
using a range of test set variations for con-
stituency parsing.
1 Introduction
It is, or at least should be, nearly universal that NLP
evaluations include statistical significance tests to
validate metric gains. As important as significance
testing is, relatively few papers have empirically in-
vestigated its practical properties. Those that do
focus on single tasks (Koehn, 2004; Zhang et al
2004) or on the comparison of alternative hypothe-
sis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani
and Ney, 2004; Riezler and Maxwell, 2005).
In this paper, we investigate two aspects of the
empirical behavior of paired significance tests for
NLP systems. For example, all else equal, larger
metric gains will tend to be more significant. How-
ever, what does this relationship look like and how
reliable is it? What should be made of the conven-
tional wisdom that often springs up that a certain
metric gain is roughly the point of significance for
a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU
in machine translation)? We show that, with heavy
caveats, there are such thresholds, though we also
discuss the hazards in their use. In particular, many
other factors contribute to the significance level, and
we investigate several of them. For example, what
is the effect of the similarity between the two sys-
tems? Here, we show that more similar systems tend
to achieve significance with smaller metric gains, re-
flecting the fact that their outputs are more corre-
lated. What about the size of the test set? For ex-
ample, in designing a shared task it is important to
know how large the test set must be in order for sig-
nificance tests to be sensitive to small gains in the
performance metric. Here, we show that test size
plays the largest role in determining discrimination
ability, but that we get diminishing returns. For ex-
ample, doubling the test size will not obviate the
need for significance testing.
In order for our results to be meaningful, we must
have access to the outputs of many of NLP sys-
tems. Public competitions, such as the well-known
CoNLL shared tasks, provide one natural way to ob-
tain a variety of system outputs on the same test
set. However, for most NLP tasks, obtaining out-
puts from a large variety of systems is not feasible.
Thus, in the course of our investigations, we propose
a very simple method for automatically generating
arbitrary numbers of comparable system outputs and
we then validate the trends revealed by our synthetic
method against data from public competitions. This
methodology itself could be of value in, for exam-
ple, the design of new shared tasks.
Finally, we consider a related and perhaps even
more important question that can only be answered
empirically: to what extent is statistical significance
on a test corpus predictive of performance on other
test corpora, in-domain or otherwise? Focusing on
constituency parsing, we investigate the relationship
between significance levels and actual performance
995
on data from outside the test set. We show that when
the test set is (artificially) drawn i.i.d. from the same
distribution that generates new data, then signifi-
cance levels are remarkably well-calibrated. How-
ever, as the domain of the new data diverges from
that of the test set, the predictive ability of signifi-
cance level drops off dramatically.
2 Statistical Significance Testing in NLP
First, we review notation and standard practice in
significance testing to set up our empirical investi-
gation.
2.1 Hypothesis Tests
When comparing a new system A to a baseline sys-
tem B, we want to know if A is better than B on
some large population of data. Imagine that we sam-
ple a small test set x = x1, . . . , xn on which A
beats B by ?(x). Hypothesis testing guards against
the case where A?s victory over B was an unlikely
event, due merely to chance. We would therefore
like to know how likely it would be that a new, in-
dependent test set x? would show a similar victory
for A assuming that A is no better than B on the
population as a whole; this assumption is the null
hypothesis, denoted H0.
Hypothesis testing consists of attempting to esti-
mate this likelihood, written p(?(X) > ?(x)|H0),
where X is a random variable over possible test sets
of size n that we could have drawn, and ?(x) is a
constant, the metric gain we actually observed. Tra-
ditionally, if p(?(X) > ?(x)|H0) < 0.05, we say
that the observed value of ?(x) is sufficiently un-
likely that we should reject H0 (i.e. accept that A?s
victory was real and not just a random fluke). We
refer to p(?(X) > ?(x)|H0) as p-value(x).
In most cases p-value(x) is not easily computable
and must be approximated. The type of approxi-
mation depends on the particular hypothesis testing
method. Various methods have been used in the NLP
community (Gillick and Cox, 1989; Yeh, 2000; Rie-
zler and Maxwell, 2005). We use the paired boot-
strap1 (Efron and Tibshirani, 1993) because it is one
1Riezler and Maxwell (2005) argue the benefits of approx-
imate randomization testing, introduced by Noreen (1989).
However, this method is ill-suited to the type of hypothesis we
are testing. Our null hypothesis does not condition on the test
data, and therefore the bootstrap is a better choice.
1. Draw b bootstrap samples x(i) of size n by
sampling with replacement from x.
2. Initialize s = 0.
3. For each x(i) increment s if ?(x(i)) > 2?(x).
4. Estimate p-value(x) ? sb
Figure 1: The bootstrap procedure. In all of our experiments
we use b = 106, which is more than sufficient for the bootstrap
estimate of p-value(x) to stabilize.
of the most widely used (Och, 2003; Bisani and Ney,
2004; Zhang et al 2004; Koehn, 2004), and be-
cause it can be easily applied to any performance
metric, even complex metrics like F1-measure or
BLEU (Papineni et al 2002). Note that we could
perform the experiments described in this paper us-
ing another method, such as the paired Student?s t-
test. To the extent that the assumptions of the t-test
are met, it is likely that the results would be very
similar to those we present here.
2.2 The Bootstrap
The bootstrap estimates p-value(x) though a com-
bination of simulation and approximation, drawing
many simulated test sets x(i) and counting how often
A sees an accidental advantage of ?(x) or greater.
How can we get sample test sets x(i)? We lack the
ability to actually draw new test sets from the un-
derlying population because all we have is our data
x. The bootstrap therefore draws each x(i) from x
itself, sampling n items from x with replacement;
these new test sets are called bootstrap samples.
Naively, it might seem like we would then check
how often A beats B by more than ?(x) on x(i).
However, there?s something seriously wrong with
these x(i) as far as the null hypothesis is concerned:
the x(i) were sampled from x, and so their average
?(x(i)) won?t be zero like the null hypothesis de-
mands; the average will instead be around ?(x). If
we ask how many of these x(i) have A winning by
?(x), about half of them will. The solution is a re-
centering of the mean ? we want to know how often
A does more than ?(x) better than expected. We ex-
pect it to beat B by ?(x). Therefore, we count up
how many of the x(i) have A beating B by at least
2?(x).2 The pseudocode is shown in Figure 1.
2Note that many authors have used a variant where the event
tallied on the x(i) is whether ?(x(i)) < 0, rather than ?(x(i)) >
2?(x). If the mean of ?(x(i)) is ?(x), and if the distribution of
?(x(i)) is symmetric, then these two versions will be equivalent.
996
As mentioned, a major benefit of the bootstrap is
that any evaluation metric can be used to compute
?(x).3 We run the bootstrap using several metrics:
F1-measure for constituency parsing, unlabeled de-
pendency accuracy for dependency parsing, align-
ment error rate (AER) for word alignment, ROUGE
score (Lin, 2004) for summarization, and BLEU
score for machine translation.4 We report all met-
rics as percentages.
3 Experiments
Our first goal is to explore the relationship be-
tween metric gain, ?(x), and statistical significance,
p-value(x), for a range of NLP tasks. In order to say
anything meaningful, we will need to see both ?(x)
and p-value(x) for many pairs of systems.
3.1 Natural Comparisons
Ideally, for a given task and test set we could obtain
outputs from all systems that have been evaluated
in published work. For each pair of these systems
we could run a comparison and compute both ?(x)
and p-value(x). While obtaining such data is not
generally feasible, for several tasks there are pub-
lic competitions to which systems are submitted by
many researchers. Some of these competitions make
system outputs publicly available. We obtained sys-
tem outputs from the TAC 2008 workshop on auto-
matic summarization (Dang and Owczarzak, 2008),
the CoNLL 2007 shared task on dependency parsing
(Nivre et al 2007), and the WMT 2010 workshop
on machine translation (Callison-Burch et al 2010).
For cases where the metric linearly decomposes over sentences,
the mean of ?(x(i)) is ?(x). By the central limit theorem, the
distribution will be symmetric for large test sets; for small test
sets it may not.
3Note that the bootstrap procedure given only approximates
the true significance level, with multiple sources of approxima-
tion error. One is the error introduced from using a finite num-
ber of bootstrap samples. Another comes from the assumption
that the bootstrap samples reflect the underlying population dis-
tribution. A third is the assumption that the mean bootstrap gain
is the test gain (which could be further corrected for if the metric
is sufficiently ill-behaved).
4To save time, we can compute ?(x) for each bootstrap sam-
ple without having to rerun the evaluation metric. For our met-
rics, sufficient statistics can be recorded for each sentence and
then sampled along with the sentences when constructing each
x(i) (e.g. size of gold, size of guess, and number correct are suf-
ficient for F1). This makes the bootstrap very fast in practice.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
ROUGE
Different research groups
Same research group
Figure 2: TAC 2008 Summarization: Confidence vs.
ROUGE improvement on TAC 2008 test set for comparisons
between all pairs of the 58 participating systems at TAC 2008.
Comparisons between systems entered by the same research
group and comparisons between systems entered by different
research groups are shown separately.
3.1.1 TAC 2008 Summarization
In our first experiment, we use the outputs of the
58 systems that participated in the TAC 2008 work-
shop on automatic summarization. For each possi-
ble pairing, we compute ?(x) and p-value(x) on the
non-update portion of the TAC 2008 test set (we or-
der each pair so that the gain, ?(x), is always pos-
itive).5 For this task, test instances correspond to
document collections. The test set consists of 48
document collections, each with a human produced
summary. Figure 2 plots the ROUGE gain against
1 ? p-value, which we refer to as confidence. Each
point on the graph corresponds to an individual pair
of systems.
As expected, larger gains in ROUGE correspond
to higher confidences. The curved shape of the plot
is interesting. It suggests that relatively quickly we
reach ROUGE gains for which, in practice, signif-
icance tests will most likely be positive. We might
expect that systems whose outputs are highly corre-
lated will achieve higher confidence at lower met-
ric gains. To test this hypothesis, in Figure 2 we
5In order to run bootstraps between all pairs of systems
quickly, we reuse a random sample counts matrix between boot-
strap runs. As a result, we no longer need to perform quadrat-
ically many corpus resamplings. The speed-up from this ap-
proach is enormous, but one undesirable effect is that the boot-
strap estimation noise between different runs is correlated. As a
remedy, we set b so large that the correlated noise is not visible
in plots.
997
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2  2.5  3  3.5  4
1  
-  
p-
va
lue
Unlabeled Acc.
Different research groups
Same research group
Figure 3: CoNLL 2007 Dependency parsing: Confidence vs.
unlabeled dependency accuracy improvement on the Chinese
CoNLL 2007 test set for comparisons between all pairs of the
21 participating systems in CoNLL 2007 shared task. Com-
parisons between systems entered by the same research group
and comparisons between systems entered by different research
groups are shown separately.
separately show the comparisons between systems
entered by the same research group and compar-
isons between systems entered by different research
groups, with the expectation that systems entered by
the same group are likely to have more correlated
outputs. Many of the comparisons between systems
submitted by the same group are offset from the
main curve. It appears that they do achieve higher
confidences at lower metric gains.
Given the huge number of system comparisons in
Figure 2, one obvious question to ask is whether
we can take the results of all these statistical sig-
nificance tests and estimate a ROUGE improvement
threshold that predicts when future statistical sig-
nificance tests will probably be significant at the
p-value(x) < 0.05 level. For example, let?s say we
take all the comparisons with p-value between 0.04
and 0.06 (47 comparisons in all in this case). Each
of these comparisons has an associated metric gain,
and by taking, say, the 95th percentile of these met-
ric gains, we get a potentially useful threshold. In
this case, the computed threshold is 1.10 ROUGE.
What does this threshold mean? Well, based on
the way we computed it, it suggests that if somebody
reports a ROUGE increase of around 1.10 on the ex-
act same test set, there is a pretty good chance that a
statistical significance test would show significance
at the p-value(x) < 0.05 level. After all, 95% of
the borderline significant differences that we?ve al-
ready seen showed an increase of even less than 1.10
ROUGE. If we?re evaluating past work, or are in
some other setting where system outputs just aren?t
available, the threshold could guide our interpreta-
tion of reports containing only summary scores.
That being said, it is important that we don?t over-
interpret the meaning of the 1.10 ROUGE threshold.
We have already seen that pairs of systems submit-
ted by the same research group and by different re-
search groups follow different trends, and we will
soon see more evidence demonstrating the impor-
tance of system correlation in determining the rela-
tionship between metric gain and confidence. Addi-
tionally, in Section 4, we will see that properties of
the test corpus have a large effect on the trend. There
are many factors are at work, and so, of course, met-
ric gain alone will not fully determine the outcome
of a paired significance test.
3.1.2 CoNLL 2007 Dependency Parsing
Next, we run an experiment for dependency pars-
ing. We use the outputs of the 21 systems that par-
ticipated in the CoNLL 2007 shared task on depen-
dency parsing. In Figure 3, we plot, for all pairs,
the gain in unlabeled dependency accuracy against
confidence on the CoNLL 2007 Chinese test set,
which consists of 690 sentences and parses. We
again separate comparisons between systems sub-
mitted by the same research group and those submit-
ted by different groups, although for this task there
were fewer cases of multiple submission. The re-
sults resemble the plot for summarization; we again
see a curve-shaped trend, and comparisons between
systems from the same group (few that they are)
achieve higher confidences at lower metric gains.
3.1.3 WMT 2010 Machine Translation
Our final task for which system outputs are pub-
licly available is machine translation. We run an ex-
periment using the outputs of the 31 systems par-
ticipating in WMT 2010 on the system combination
portion of the German-English WMT 2010 news test
set, which consists of 2,034 German sentences and
English translations. We again run comparisons for
pairs of participating systems. We plot gain in test
BLEU score against confidence in Figure 4. In this
experiment there is an additional class of compar-
998
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
Different research groups
Same research group
System combination
Figure 4: WMT 2010 Machine translation: Confidence vs.
BLEU improvement on the system combination portion of the
German-English WMT 2010 news test set for comparisons be-
tween pairs of the 31 participating systems at WMT 2010.
Comparisons between systems entered by the same research
group, comparisons between systems entered by different re-
search groups, and comparisons between system combination
entries are shown separately.
isons that are likely to have specially correlated sys-
tems: 13 of the submitted systems are system com-
binations, and each take into account the same set
of proposed translations. We separate comparisons
into three sets: comparisons between non-combined
systems entered by different research groups, com-
parisons between non-combined systems entered by
the same research group, and comparisons between
system-combinations.
We see the same curve-shaped trend we saw for
summarization and dependency parsing. Differ-
ent group comparisons, same group comparisons,
and system combination comparisons form distinct
curves. This indicates, again, that comparisons be-
tween systems that are expected to be specially cor-
related achieve high confidence at lower metric gain
levels.
3.2 Synthetic Comparisons
So far, we have seen a clear empirical effect, but, be-
cause of the limited availability of system outputs,
we have only considered a few tasks. We now pro-
pose a simple method that captures the shape of the
effect, and use it to extend our analysis.
3.2.1 Training Set Resampling
Another way of obtaining many different sys-
tems? outputs is to obtain implementations of a
handful of systems, and then vary some aspect of
the training procedure in order to produce many dif-
ferent systems from each implementation. Koehn
(2004) uses this sort of amplification; he uses a sin-
gle machine translation implementation, and then
trains it from different source languages. We take
a slightly different approach. For each task we pick
some fixed training set. Then we generate resampled
training sets by sampling sentences with replace-
ment from the original. In this way, we can gen-
erate as many new training sets as we like, each of
which is similar to the original, but with some vari-
ation. For each base implementation, we train a new
system on each resampled training set. This results
in slightly tweaked trained systems, and is intended
to very roughly approximate the variance introduced
by incremental system changes during research. We
validate this method by comparing plots obtained by
the synthetic approach with plots obtained from nat-
ural comparisons.
We expect that each new system will be differ-
ent, but that systems originating from the same base
model will be highly correlated. This provides a use-
ful division of comparisons: those between systems
built with the same model, and those between sys-
tems built with different models. The first class can
be used to approximate comparisons of systems that
are expected to be specially correlated, and the latter
for comparisons of systems that are not.
3.2.2 Dependency Parsing
We use three base models for dependency parsing:
MST parser (McDonald et al 2005), Maltparser
(Nivre et al 2006), and the ensemble parser of Sur-
deanu and Manning (2010). We use the CoNLL
2007 Chinese training set, which consists of 57K
sentences. We resample 5 training sets of 57K sen-
tences, 10 training sets of 28K sentences, and 10
training sets of 14K sentences. Together, this yields
a total of 75 system outputs on the CoNLL 2007
Chinese test set, 25 systems for each base model
type. The score ranges of all the base models over-
lap. This ensures that for each pair of model types
we will be able to see comparisons where the metric
gains are small. The results of the pairwise compar-
isons of all 75 system outputs are shown in Figure
5, along with the results of the CoNLL 2007 shared
task system comparisons from Figure 3.
999
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2  2.5  3  3.5  4
1  
-  
p-
va
lue
Unlabeled Acc.
Different model types
Same model type
CoNLL 2007 comparisons
Figure 5: Dependency parsing: Confidence vs. unlabeled de-
pendency accuracy improvement on the Chinese CoNLL 2007
test set for comparisons between all pairs of systems gener-
ated by using resampled training sets to train either MST parser,
Maltparser, or the ensemble parser. Comparisons between sys-
tems generated using the same base model type and compar-
isons between systems generated using different base model
types are shown separately. The CoNLL 2007 shared task com-
parisons from Figure 3 are also shown.
The overlay of the natural comparisons suggests
that the synthetic approach reasonably models the
relationship between metric gain and confidence.
Additionally, the different model type and same
model type comparisons exhibit the behavior we
would expect, matching the curves corresponding to
comparisons between specially correlated systems
and standard comparisons respectively.
Since our synthetic approach yields a large num-
ber of system outputs, we can use the procedure
described in Section 3.1.1 to compute the thresh-
old above which the metric gain is probably signifi-
cant. For comparisons between systems of the same
model type, the threshold is 1.20 unlabeled depen-
dency accuracy. For comparisons between systems
of different model types, the threshold is 1.51 un-
labeled dependency accuracy. These results indi-
cate that the similarity of the systems being com-
pared is an important factor. As mentioned, rules-
of-thumb derived from such thresholds cannot be
applied blindly, but, in special cases where two sys-
tems are known to be correlated, the former thresh-
old should be preferred over the latter. For example,
during development most comparisons are made be-
tween incremental variants of the same system. If
adding a feature to a supervised parser increases un-
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
Different model types
Same model type
WMT 2010 comparisons
Figure 6: Machine translation: Confidence vs. BLEU im-
provement on the system combination portion of the German-
English WMT 2010 news test set for comparisons between all
pairs of systems generated by using resampled training sets to
train either Moses or Joshua. Comparisons between systems
generated using the same base model type and comparisons be-
tween systems generated using different base model types are
shown separately. The WMT 2010 workshop comparisons from
Figure 4 are also shown.
labeled accuracy by 1.3, it is useful to be able to
quickly estimate that the improvement is probably
significant. This still isn?t the full story; we will
soon see that properties of the test set al play a
major role. But first, we carry our analysis to sev-
eral more tasks.
3.2.3 Machine Translation
Our two base models for machine translation
are Moses (Koehn et al 2007) and Joshua (Li et
al., 2009). We use 1.4M sentence pairs from the
German-English portion of the WMT-provided Eu-
roparl (Koehn, 2005) and news commentary corpora
as the original training set. We resample 75 training
sets, 20 of 1.4M sentence pairs, 29 of 350K sentence
pairs, and 26 of 88K sentence pairs. This yields a
total of 150 system outputs on the system combi-
nation portion of the German-English WMT 2010
news test set. The results of the pairwise compar-
isons of all 150 system outputs are shown in Figure
6, along with the results of the WMT 2010 workshop
system comparisons from Figure 4.
The natural comparisons from the WMT 2010
workshop align well with the comparisons between
synthetically varied models. Again, the different
model type and same model type comparisons form
distinct curves. For comparisons between systems
1000
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
AER
Different model types
Same model type
Figure 7: Word alignment: Confidence vs. AER improve-
ment on the Hansard test set for comparisons between all pairs
of systems generated by using resampled training sets to train
either the ITG aligner, the joint HMM aligner, or GIZA++.
Comparisons between systems generated using the same base
model type and comparisons between systems generated using
different base model types are shown separately.
of the same model type the computed p-value <
0.05 threshold is 0.28 BLEU. For comparisons be-
tween systems of different model types the threshold
is 0.37 BLEU.
3.2.4 Word Alignment
Now that we have validated our simple model of
system variation on two tasks, we go on to gen-
erate plots for tasks that do not have competitions
with publicly available system outputs. The first
task is English-French word alignment, where we
use three base models: the ITG aligner of Haghighi
et al(2009), the joint HMM aligner of Liang et al
(2006), and GIZA++ (Och and Ney, 2003). The last
two aligners are unsupervised, while the first is su-
pervised. We train the unsupervised word aligners
using the 1.1M sentence pair Hansard training cor-
pus, resampling 20 training sets of the same size.6
Following Haghighi et al(2009), we train the super-
vised ITG aligner using the first 337 sentence pairs
of the hand-aligned Hansard test set; again, we re-
sample 20 training sets of the same size as the origi-
nal data. We test on the remaining 100 hand-aligned
sentence pairs from the Hansard test set.
Unlike previous plots, the points corresponding
to comparisons between systems with different base
6GIZA++ failed to produce reasonable output when trained
with some of these training sets, so there are fewer than 20
GIZA++ systems in our comparisons.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
Different model types
Same model type
Figure 8: Constituency parsing: Confidence vs. F1 improve-
ment on section 23 of the WSJ corpus for comparisons between
all pairs of systems generated by using resampled training sets
to train either the Berkeley parser, the Stanford parser, or the
Collins parser. Comparisons between systems generated us-
ing the same base model type and comparisons between sys-
tems generated using different base model types are shown sep-
arately.
model types form two distinct curves. It turns out
that the upper curve consists only of comparisons
between ITG and HMM aligners. This is likely due
to the fact that the ITG aligner uses posteriors from
the HMM aligner for some of its features, so the
two models are particularly correlated. Overall, the
spread of this plot is larger than previous ones. This
may be due to the small size of the test set, or possi-
bly some additional variance introduced by unsuper-
vised training. For comparisons between systems of
the same model type the p-value < 0.05 threshold
is 0.50 AER. For comparisons between systems of
different model types the threshold is 1.12 AER.
3.2.5 Constituency Parsing
Finally, before we move on to further types of
analysis, we run an experiment for the task of con-
stituency parsing. We use three base models: the
Berkeley parser (Petrov et al 2006), the Stanford
parser (Klein and Manning, 2003), and Dan Bikel?s
implementation (Bikel, 2004) of the Collins parser
(Collins, 1999). We use sections 2-21 of the WSJ
corpus (Marcus et al 1993), which consists of 38K
sentences and parses, as a training set. We resample
10 training sets of size 38K, 10 of size 19K, and 10
of size 9K, and use these to train systems. We test
on section 23. The results are shown in Figure 8.
For comparisons between systems of the same
1001
model type, the p-value < 0.05 threshold is 0.47
F1. For comparisons between systems of different
model types the threshold is 0.57 F1.
4 Properties of the Test Corpus
For five tasks, we have seen a trend relating met-
ric gain and confidence, and we have seen that the
level of correlation between the systems being com-
pared affects the location of the curve. Next, we
look at how the size and domain of the test set play
a role, and, finally, how significance level predicts
performance on held out data. In this section, we
carry out experiments for both machine translation
and constituency parsing, but mainly focus on the
latter because of the availability of large test corpora
that span more than one domain: the Brown corpus
and the held out portions of the WSJ corpus.
4.1 Varying the Size
Figure 9 plots comparisons for machine translation
on variously sized initial segments of the WMT
2010 news test set. Similarly, Figure 10 plots com-
parisons for constituency parsing on initial segments
of the Brown corpus. As might be expected, the
size of the test corpus has a large effect. For both
machine translation and constituency parsing, the
larger the corpus size, the lower the threshold for
p-value < 0.05 and the smaller the spread of the
plot. At one extreme, the entire Brown corpus,
which consists of approximately 24K sentences, has
a threshold of 0.22 F1, while at the other extreme,
the first 100 sentences of the Brown corpus have a
threshold of 3.00 F1. Notice that we see diminishing
returns as we increase the size of the test set. This
phenomenon follows the general shape of the cen-
tral limit theorem, which predicts that variances of
observed metric gains will shrink according to the
square root of the test size. Even using the entire
Brown corpus as a test set there is a small range
where the result of a paired significance test was not
completely determined by metric gain.
It is interesting to note that for a fixed test size,
the domain has only a small effect on the shape of
the curve. Figure 11 plots comparisons for a fixed
test size, but with various test corpora.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
100 sent.
200 sent.
400 sent.
800 sent.
1600 sent.
Figure 9: Machine translation; varying test size: Confidence
vs. BLEU improvement on portions of the German-English
WMT 2010 news test set.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
100 sent.
200 sent.
400 sent.
800 sent.
1600 sent.
3200 sent.
Entire Brown corpus
Figure 10: Constituency parsing; varying test size: Con-
fidence vs. F1 improvement on portions of the Brown corpus.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
WSJ sec 22
WSJ sec 23
WSJ sec 24
Brown corpus
Figure 11: Constituency parsing; varying domain: Confi-
dence vs. F1 improvement on the first 1,600 sentences of sec-
tions 22, 23, and 24 of the WSJ corpus, and the Brown corpus.
1002
4.2 Empirical Calibration across Domains
Now that we have a way of generating outputs for
thousands of pairs of systems, we can check empir-
ically the practical reliability of significance testing.
Recall that the bootstrap p-value(x) is an approxi-
mation to p(?(X) > ?(x)|H0). However, we often
really want to determine the probability that the new
system is better than the baseline on the underlying
test distribution or even the distribution from another
domain. There is no reason a priori to expect these
numbers to coincide.
In our next experiment, we treat the entire Brown
corpus, which consists of 24K sentences, as the true
population of English sentences. For each system
generated in the way described in Section 3.2.5 we
compute F1 on all of Brown. Since we are treat-
ing the Brown corpus as the actual population of En-
glish sentences, for each pair of parsers we can say
that the sign of the F1 difference indicates which is
the truly better system. Now, we repeatedly resam-
ple small test sets from Brown, each consisting of
1,600 sentences, drawn by sampling sentences with
replacement. For each pair of systems, and for each
resampled test set, we compute p-value(x) using the
bootstrap. Out of the 4K bootstraps computed in this
way, 942 had p-value between 0.04 and 0.06, 869
of which agreed with the sign of the F1 difference
we saw on the entire Brown corpus. Thus, 92% of
the significance tests with p-value in a tight range
around 0.05 correctly identified the better system.
This result is encouraging. It suggests that sta-
tistical significance computed using the bootstrap is
reasonably well calibrated. However, test sets are
almost never drawn i.i.d. from the distribution of in-
stances the system will encounter in practical use.
Thus, we also wish to compute how calibration de-
grades as the domain of the test set changes. In an-
other experiment, we look at how significance near
p-value = 0.05 on section 23 of the WSJ corpus
predicts performance on sections 22 and 24 and the
Brown corpus. This time, for each pair of generated
systems we run a bootstrap on section 23. Out of
all these bootstraps, 58 system pairs had p-value be-
tween 0.04 and 0.06. Of these, only 83% had the
same sign of F1 difference on section 23 as they did
on section 22, 71% the had the same sign on sec-
tion 23 as on section 24, and 48% the same sign on
Sec. 23 p-value
% Sys. A > Sys. B
Sec. 22 Sec. 24 Brown
0.00125 - 0.0025 97% 95% 73%
0.0025 - 0.005 92% 92% 60%
0.005 - 0.01 92% 85% 56%
0.01 - 0.02 88% 92% 54%
0.02 - 0.04 87% 78% 51%
0.04 - 0.08 83% 74% 48%
Table 1: Empirical calibration: p-value on section 23 of the
WSJ corpus vs. fraction of comparisons where system A beats
system B on section 22, section 24, and the Brown corpus. Note
that system pairs are ordered so that A always outperforms B on
section 23.
section 23 as on the Brown corpus. This indicates
that reliability degrades as we switch the domain. In
the extreme, achieving a p-value near 0.05 on sec-
tion 23 provides no information about performance
on the Brown corpus.
If we intend to use our system on out-of-domain
data, these results are somewhat discouraging. How
low does p-value(x) have to get before we start get-
ting good information about out-of-domain perfor-
mance? We try to answer this question for this par-
ticular parsing task by running the same domain cal-
ibration experiment for several different ranges of
p-value. The results are shown in Table 1. From
these results, it appears that for constituency pars-
ing, when testing on section 23, a p-value level be-
low 0.00125 is required to reasonably predict perfor-
mance on the Brown corpus.
It should be considered a good practice to include
statistical significance testing results with empiri-
cal evaluations. The bootstrap in particular is easy
to run and makes relatively few assumptions about
the task or evaluation metric. However, we have
demonstrated some limitations of statistical signifi-
cance testing for NLP. In particular, while statistical
significance is usually a minimum necessary condi-
tion to demonstrate that a performance difference is
real, it?s also important to consider the relationship
between test set performance and the actual goals
of the systems being tested, especially if the system
will eventually be used on data from a different do-
main than the test set used for evaluation.
5 Conclusion
We have demonstrated trends relating several im-
portant factors to significance level, which include
1003
both properties of the systems being compared and
properties of the test corpus, and have presented a
simple approach to approximating the response of
these factors for tasks where large numbers of sys-
tem outputs are not available. Our results reveal
that the relationship between metric gain and sta-
tistical significance is complex, and therefore sim-
ple thresholds are not a replacement for significance
tests. Indeed, we strongly advocate the use of statis-
tical significance testing to validate metric gains in
NLP, but also note that informal rules-of-thumb do
arise in popular discussion and that, for some set-
tings when previous systems are unavailable, these
empirical results can supplement less sensitive un-
paired tests (e.g. bar-overlaps-point test) in evalua-
tion of progress. Finally, even formal testing has its
limits. We provide cautionary evidence to this ef-
fect, showing that the information provided by a test
quickly degrades as the target corpus shifts domain.
Acknowledgements
This work was partially supported by NSF fellow-
ships to the first and second authors and by the NSF
under grant 0643742.
References
D.M. Bikel. 2004. Intricacies of collins? parsing model.
Computational Linguistics.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in asr performance evaluation. In
Proc. of ICASSP.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Proc. of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
H.T. Dang and K. Owczarzak. 2008. Overview of the
tac 2008 update summarization task. In Proc. of Text
Analysis Conference.
B. Efron and R. Tibshirani. 1993. An introduction to the
bootstrap. Chapman & Hall/CRC.
L. Gillick and S.J. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proc. of ICASSP.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch,
S. Khudanpur, L. Schwartz, W.N.G. Thornton,
J. Weese, and O.F. Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proc. of the Fourth Workshop on Statistical Machine
Translation.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Proc. of the Workshop on Text
Summarization.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
The penn treebank. Computational linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of EMNLP.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
E.W. Noreen. 1989. Computer Intensive Methods for
Hypothesis Testing: An Introduction. Wiley, New
York.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
1004
S. Riezler and J.T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing for mt.
In Proc. of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
M. Surdeanu and C.D. Manning. 2010. Ensemble mod-
els for dependency parsing: cheap and good? In Proc.
of NAACL.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In Proc. of ACL.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
bleu/nist scores: How much improvement do we need
to have a better system. In Proc. of LREC.
1005
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 874?878,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Decipherment with a Million Random Restarts
Taylor Berg-Kirkpatrick Dan Klein
Computer Science Division
University of California, Berkeley
{tberg,klein}@cs.berkeley.edu
Abstract
This paper investigates the utility and effect of
running numerous random restarts when us-
ing EM to attack decipherment problems. We
find that simple decipherment models are able
to crack homophonic substitution ciphers with
high accuracy if a large number of random
restarts are used but almost completely fail
with only a few random restarts. For partic-
ularly difficult homophonic ciphers, we find
that big gains in accuracy are to be had by run-
ning upwards of 100K random restarts, which
we accomplish efficiently using a GPU-based
parallel implementation. We run a series of
experiments using millions of random restarts
in order to investigate other empirical proper-
ties of decipherment problems, including the
famously uncracked Zodiac 340.
1 Introduction
What can a million restarts do for decipherment?
EM frequently gets stuck in local optima, so running
between ten and a hundred random restarts is com-
mon practice (Knight et al, 2006; Ravi and Knight,
2011; Berg-Kirkpatrick and Klein, 2011). But, how
important are random restarts and how many random
restarts does it take to saturate gains in accuracy?
We find that the answer depends on the cipher. We
look at both Zodiac 408, a famous homophonic sub-
stitution cipher, and a more difficult homophonic ci-
pher constructed to match properties of the famously
unsolved Zodiac 340. Gains in accuracy saturate af-
ter only a hundred random restarts for Zodiac 408,
but for the constructed cipher we see large gains
in accuracy even as we scale the number of ran-
dom restarts up into the hundred thousands. In both
cases the difference between few and many random
restarts is the difference between almost complete
failure and successful decipherment.
We also find that millions of random restarts can
be helpful for performing exploratory analysis. We
look at some empirical properties of decipherment
problems, visualizing the distribution of local op-
tima encountered by EM both in a successful deci-
pherment of a homophonic cipher and in an unsuc-
cessful attempt to decipher Zodiac 340. Finally, we
attack a series of ciphers generated to match proper-
ties of Zodiac 340 and use the results to argue that
Zodiac 340 is likely not a homophonic cipher under
the commonly assumed linearization order.
2 Decipherment Model
Various types of ciphers have been tackled by the
NLP community with great success (Knight et al,
2006; Snyder et al, 2010; Ravi and Knight, 2011).
Many of these approaches learn an encryption key
by maximizing the score of the decrypted message
under a language model. We focus on homophonic
substitution ciphers, where the encryption key is a
1-to-many mapping from a plaintext alphabet to a
cipher alphabet. We use a simple method introduced
by Knight et al (2006): the EM algorithm (Demp-
ster et al, 1977) is used to learn the emission pa-
rameters of an HMM that has a character trigram
language model as a backbone and the ciphertext
as the observed sequence of emissions. This means
that we learn a multinomial over cipher symbols for
each plaintext character, but do not learn transition
874
parameters, which are fixed by the language model.
We predict the deciphered text using posterior de-
coding in the learned HMM.
2.1 Implementation
Running multiple random restarts means running
EM to convergence multiple times, which can be
computationally intensive; luckily, restarts can be
run in parallel. This kind of parallelism is a good
fit for the Same Instruction Multiple Thread (SIMT)
hardware paradigm implemented by modern GPUs.
We implemented EM with parallel random restarts
using the CUDA API (Nickolls et al, 2008). With a
GPU workstation,1 we can complete a million ran-
dom restarts roughly a thousand times more quickly
than we can complete the same computation with a
serial implementation on a CPU.
3 Experiments
We ran experiments on several homophonic sub-
stitution ciphers: some produced by the infamous
Zodiac killer and others that were automatically
generated to be similar to the Zodiac ciphers. In
each of these experiments, we ran numerous random
restarts; and in all cases we chose the random restart
that attained the highest model score in order to pro-
duce the final decode.
3.1 Experimental Setup
The specifics of how random restarts are produced
is usually considered a detail; however, in this work
it is important to describe the process precisely. In
order to generate random restarts, we sampled emis-
sion parameters by drawing uniformly at random
from the interval [0, 1] and then normalizing. The
corresponding distribution on the multinomial emis-
sion parameters is mildly concentrated at the center
of the simplex.2
For each random restart, we ran EM for 200 itera-
1We used a single workstation with three NVIDIA GTX 580
GPUs. These are consumer graphics cards introduced in 2011.
2We also ran experiments where emission parameters were
drawn from Dirichlet distributions with various concentration
parameter settings. We noticed little effect so long as the distri-
bution did not favor the corners of the simplex. If the distribu-
tion did favor the corners of the simplex, decipherment results
deteriorated sharply.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  10  100  1000  10000  100000  1e+06
-1530
-1520
-1510
-1500
-1490
-1480
-1470
-1460
Ac
cur
acy
Log likelihood
Number of random restarts
Log likelihood
Accuracy
Figure 1: Zodiac 408 cipher. Accuracy by best model score and
best model score vs. number of random restarts. Bootstrapped
from 1M random restarts.
tions.3 We found that smoothing EM was important
for good performance. We added a smoothing con-
stant of 0.1 to the expected emission counts before
each M-step. We tuned this value on a small held
out set of automatically generated ciphers.
In all experiments we used a trigram character
language model that was linearly interpolated from
character unigram, bigram, and trigram counts ex-
tracted from both the Google N-gram dataset (Brants
and Franz, 2006) and a small corpus (about 2K
words) of plaintext messages authored by the Zodiac
killer.4
3.2 An Easy Cipher: Zodiac 408
Zodiac 408 is a homophonic cipher that is 408 char-
acters long and contains 54 different cipher sym-
bols. Produced by the Zodiac killer, this cipher was
solved, manually, by two amateur code-breakers a
week after its release to the public in 1969. Ravi and
Knight (2011) were the first to crack Zodiac 408 us-
ing completely automatic methods.
In our first experiment, we compare a decode of
Zodiac 408 using one random restart to a decode us-
ing 100 random restarts. Random restarts have high
3While this does not guarantee convergence, in practice 200
iterations seems to be sufficient for the problems we looked at.
4The interpolation between n-gram orders is uniform, and
the interpolation between corpora favors the Zodiac corpus with
weight 0.9.
875
variance, so when we present the accuracy corre-
sponding to a given number of restarts we present an
average over many bootstrap samples, drawn from
a set of one million random restarts. If we attack
Zodiac 408 with a single random restart, on aver-
age we achieve an accuracy of 18%. If we instead
use 100 random restarts we achieve a much better
average accuracy of 90%. The accuracies for vari-
ous numbers of random restarts are plotted in Fig-
ure 1. Based on these results, we expect accuracy
to increase by about 72% when using 100 random
restarts instead of a single random restart; however,
using more than 100 random restarts for this partic-
ular cipher does not appear to be useful.
Also in Figure 1, we plot a related graph, this time
showing the effect that random restarts have on the
achieved model score. By construction, the (maxi-
mum) model score must increase as we increase the
number of random restarts. We see that it quickly
saturates in the same way that accuracy did.
This raises the question: have we actually
achieved the globally optimal model score or have
we only saturated the usefulness of random restarts?
We can?t prove that we have achieved the global op-
timum,5 but we can at least check that we have sur-
passed the model score achieved by EM when it is
initialized with the gold encryption key. On Zodiac
408, if we initialize with the gold key, EM finds
a local optimum with a model score of ?1467.4.
The best model score over 1M random restarts is
?1466.5, which means we have surpassed the gold
initialization.
The accuracy after gold initialization was 92%,
while the accuracy of the best local optimum was
only 89%. This suggests that the global optimum
may not be worth finding if we haven?t already
found it. From Figure 1, it appears that large in-
creases in likelihood are correlated with increases
in accuracy, but small improvements to high like-
lihoods (e.g. the best local optimum versus the gold
initialization) may not to be.
5ILP solvers can be used to globally optimize objectives
corresponding to short 1-to-1 substitution ciphers (Ravi and
Knight, 2008) (though these objectives are slightly different
from the likelihood objectives faced by EM), but we find that
ILP encodings for even the shortest homophonic ciphers cannot
be optimized in any reasonable amount of time.
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 1  10  100  1000  10000  100000  1e+06
-1310
-1305
-1300
-1295
-1290
-1285
-1280
Ac
cur
acy
Log likelihood
Number of random restarts
Log likelihood
Accuracy
Figure 2: Synth 340 cipher. Accuracy by best model score and
best model score vs. number of random restarts. Bootstrapped
from 1M random restarts.
3.3 A Hard Cipher: Synth 340
What do these graphs look like for a harder cipher?
Zodiac 340 is the second cipher released by the Zo-
diac killer, and it remains unsolved to this day. How-
ever, it is unknown whether Zodiac 340 is actually a
homophonic cipher. If it were a homophonic cipher
we would certainly expect it to be harder than Zo-
diac 408 because Zodiac 340 is shorter (only 340
characters long) and at the same time has more ci-
pher symbols: 63. For our next experiment we gen-
erate a cipher, which we call Synth 340, to match
properties of Zodiac 340; later we will generate mul-
tiple such ciphers.
We sample a random consecutive sequence of 340
characters from our small Zodiac corpus and use
this as our message (and, of course, remove this se-
quence from our language model training data). We
then generate an encryption key by assigning each
of 63 cipher symbols to a single plain text charac-
ter so that the number of cipher symbols mapped to
each plaintext character is proportional to the fre-
quency of that character in the message (this bal-
ancing makes the cipher more difficult). Finally, we
generate the actual ciphertext by randomly sampling
a cipher token for each plain text token uniformly at
random from the cipher symbols allowed for that to-
ken under our generated key.
In Figure 2, we display the same type of plot, this
time for Synth 340. For this cipher, there is an abso-
876
 0
 10000
 20000
 30000
 40000
 50000
 60000
-1340 -1330 -1320 -1310 -1300 -1290 -1280 -1270
Fr
eq
ue
nc
y
Log likelihood
42%
ntyouldli 59%
veautital
74%
veautiful
Figure 3: Synth 340 cipher. Histogram of the likelihoods of the
local optima encountered by EM across 1M random restarts.
Several peaks are labeled with their average accuracy and a
snippet of a decode. The gold snippet is ?beautiful.?
lute gain in accuracy of about 9% between 100 ran-
dom restarts and 100K random restarts. A similarly
large gain is seen for model score as we scale up the
number of restarts. This means that, even after tens
of thousands of random restarts, EM is still finding
new local optima with better likelihoods. It also ap-
pears that, even for a short cipher like Synth 340,
likelihood and accuracy are reasonably coupled.
We can visualize the distribution of local optima
encountered by EM across 1M random restarts by
plotting a histogram. Figure 3 shows, for each range
of likelihood, the number of random restarts that
led to a local optimum with a model score in that
range. It is quickly visible that a few model scores
are substantially more likely than all the rest. This
kind of sparsity might be expected if there were
a small number of local optima that EM was ex-
tremely likely to find. We can check whether the
peaks of this histogram each correspond to a single
local optimum or whether each is composed of mul-
tiple local optima that happen to have the same like-
lihood. For the histogram bucket corresponding to a
particular peak, we compute the average relative dif-
ference between each multinomial parameter and its
mean. The average relative difference for the highest
peak in Figure 3 is 0.8%, and for the second highest
peak is 0.3%. These values are much smaller than
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
-1330 -1320 -1310 -1300 -1290 -1280
Fr
eq
ue
nc
y
Log likelihood
Figure 4: Zodiac 340 cipher. Histogram of the likelihoods of the
local optima encountered by EM across 1M random restarts.
the average relative difference between the means of
these two peaks, 40%, indicating that the peaks do
correspond to single local optima or collections of
extremely similar local optima.
There are several very small peaks that have the
highest model scores (the peak with the highest
model score has a frequency of 90 which is too
small to be visible in Figure 3). The fact that these
model scores are both high and rare is the reason we
continue to see improvements to both accuracy and
model score as we run numerous random restarts.
The two tallest peaks and the peak with highest
model score are labeled with their average accuracy
and a small snippet of a decode in Figure 3. The
gold snippet is the word ?beautiful.?
3.4 An Unsolved Cipher: Zodiac 340
In a final experiment, we look at the Zodiac 340
cipher. As mentioned, this cipher has never been
cracked and may not be a homphonic cipher or even
a valid cipher of any kind. The reading order of
the cipher, which consists of a grid of symbols, is
unknown. We make two arguments supporting the
claim that Zodiac 340 is not a homophonic cipher
with row-major reading order: the first is statistical,
based on the success rate of attempts to crack similar
synthetic ciphers; the second is qualitative, compar-
ing distributions of local optimum likelihoods.
If Zodiac 340 is a homophonic cipher should we
877
expect to crack it? In order to answer this question
we generate 100 more ciphers in the same way we
generated Synth 340. We use 10K random restarts to
attack each cipher, and compute accuracies by best
model score. The average accuracy across these 100
ciphers was 75% and the minimum accuracy was
36%. All but two of the ciphers were deciphered
with more than 51% accuracy, which is usually suf-
ficient for a human to identify a decode as partially
correct.
We attempted to crack Zodiac 340 using a row-
major reading order and 1M random restarts, but the
decode with best model score was nonsensical. This
outcome would be unlikely if Zodiac 340 were like
our synthetic ciphers, so Zodiac 340 is probably not
a homophonic cipher with a row-major order. Of
course, it could be a homophonic cipher with a dif-
ferent reading order. It could also be the case that
a large number of salt tokens were inserted, or that
some other assumption is incorrect.
In Figure 4, we show the histogram of model
scores for the attempt to crack Zodiac 340. We note
that this histogram is strikingly different from the
histogram for Synth 340. Zodiac 340?s histogram is
not as sparse, and the range of model scores is much
smaller. The sparsity of Synth 340?s histogram (but
not Zodiac 340?s histogram) is typical of histograms
corresponding to our set of 100 generated ciphers.
4 Conclusion
Random restarts, often considered a footnote of ex-
perimental design, can indeed be useful on scales
beyond that generally used in past work. In particu-
lar, we found that the initializations that lead to the
local optima with highest likelihoods are sometimes
very rare, but finding them can be worthwhile; for
the problems we looked at, local optima with high
likelihoods also achieved high accuracies. While the
present experiments are on a very specific unsuper-
vised learning problem, it is certainly reasonable to
think that large-scale random restarts have potential
more broadly.
In addition to improving search, large-scale
restarts can also provide a novel perspective when
performing exploratory analysis, here letting us ar-
gue in support for the hypothesis that Zodiac 340 is
not a row-major homophonic cipher.
References
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Sim-
ple effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, Catalog Num-
ber LDC2009T25.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the 2006 Annual Meeting
of the Association for Computational Linguistics.
John Nickolls, Ian Buck, Michael Garland, and Kevin
Skadron. 2008. Scalable parallel programming with
CUDA. Queue.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing.
Sujith Ravi and Kevin Knight. 2011. Bayesian inference
for Zodiac and other homophonic ciphers. In Proceed-
ings of the 2011 Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 2010 Annual Meeting of
the Association for Computational Linguistics.
878
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582?590,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Painless Unsupervised Learning with Features
Taylor Berg-Kirkpatrick Alexandre Bouchard-Co?te? John DeNero Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, bouchard, denero, klein}@cs.berkeley.edu
Abstract
We show how features can easily be added
to standard generative models for unsuper-
vised learning, without requiring complex
new training methods. In particular, each
component multinomial of a generative model
can be turned into a miniature logistic regres-
sion model if feature locality permits. The in-
tuitive EM algorithm still applies, but with a
gradient-based M-step familiar from discrim-
inative training of logistic regression mod-
els. We apply this technique to part-of-speech
induction, grammar induction, word align-
ment, and word segmentation, incorporating
a few linguistically-motivated features into
the standard generative model for each task.
These feature-enhanced models each outper-
form their basic counterparts by a substantial
margin, and even compete with and surpass
more complex state-of-the-art models.
1 Introduction
Unsupervised learning methods have been increas-
ingly successful in recent NLP research. The rea-
sons are varied: increased supplies of unlabeled
data, improved understanding of modeling methods,
additional choices of optimization algorithms, and,
perhaps most importantly for the present work, in-
corporation of richer domain knowledge into struc-
tured models. Unfortunately, that knowledge has
generally been encoded in the form of conditional
independence structure, which means that injecting
it is both tricky (because the connection between
independence and knowledge is subtle) and time-
consuming (because new structure often necessitates
new inference algorithms).
In this paper, we present a range of experiments
wherein we improve existing unsupervised models
by declaratively adding richer features. In particu-
lar, we parameterize the local multinomials of exist-
ing generative models using features, in a way which
does not require complex new machinery but which
still provides substantial flexibility. In the feature-
engineering paradigm, one can worry less about the
backbone structure and instead use hand-designed
features to declaratively inject domain knowledge
into a model. While feature engineering has his-
torically been associated with discriminative, super-
vised learning settings, we argue that it can and
should be applied more broadly to the unsupervised
setting.
The idea of using features in unsupervised learn-
ing is neither new nor even controversial. Many
top unsupervised results use feature-based mod-
els (Smith and Eisner, 2005; Haghighi and Klein,
2006). However, such approaches have presented
their own barriers, from challenging normalization
problems, to neighborhood design, to the need for
complex optimization procedures. As a result, most
work still focuses on the stable and intuitive ap-
proach of using the EM algorithm to optimize data
likelihood in locally normalized, generative models.
The primary contribution of this paper is to
demonstrate the clear empirical success of a sim-
ple and accessible approach to unsupervised learn-
ing with features, which can be optimized by us-
ing standard NLP building blocks. We consider
the same generative, locally-normalized models that
dominate past work on a range of tasks. However,
we follow Chen (2003), Bisani and Ney (2008), and
Bouchard-Co?te? et al (2008), and allow each com-
ponent multinomial of the model to be a miniature
multi-class logistic regression model. In this case,
the EM algorithm still applies with the E-step un-
changed. The M-step involves gradient-based train-
ing familiar from standard supervised logistic re-
gression (i.e., maximum entropy models). By inte-
grating these two familiar learning techniques, we
add features to unsupervised models without any
582
specialized learning or inference.
A second contribution of this work is to show that
further gains can be achieved by directly optimiz-
ing data likelihood with LBFGS (Liu et al, 1989).
This alternative optimization procedure requires no
additional machinery beyond what EM uses. This
approach is still very simple to implement, and we
found that it empirically outperforms EM.
This paper is largely empirical; the underlying op-
timization techniques are known, even if the overall
approach will be novel to many readers. As an em-
pirical demonstration, our results span an array of
unsupervised learning tasks: part-of-speech induc-
tion, grammar induction, word alignment, and word
segmentation. In each task, we show that declaring a
few linguistically motivated feature templates yields
state-of-the-art results.
2 Models
We start by explaining our feature-enhanced model
for part-of-speech (POS) induction. This particular
example illustrates our approach to adding features
to unsupervised models in a well-known NLP task.
We then explain how the technique applies more
generally.
2.1 Example: Part-of-Speech Induction
POS induction consists of labeling words in text
with POS tags. A hidden Markov model (HMM) is a
standard model for this task, used in both a frequen-
tist setting (Merialdo, 1994; Elworthy, 1994) and in
a Bayesian setting (Goldwater and Griffiths, 2007;
Johnson, 2007).
A POS HMM generates a sequence of words in
order. In each generation step, an observed word
emission yi and a hidden successor POS tag zi+1 are
generated independently, conditioned on the current
POS tag zi . This process continues until an absorb-
ing stop state is generated by the transition model.
There are two types of conditional distributions in
the model?emission and transition probabilities?
that are both multinomial probability distributions.
The joint likelihood factors into these distributions:
P?(Y = y,Z = z) = P?(Z1 = z1) ?
|z|
?
i=1
P?(Yi = yi|Zi = zi) ? P?(Zi+1 = zi+1|Zi = zi)
The emission distribution P?(Yi = yi|Zi = zi) is
parameterized by conditional probabilities ?y,z,EMIT
for each word y given tag z. Alternatively, we can
express this emission distribution as the output of a
logistic regression model, replacing the explicit con-
ditional probability table by a logistic function pa-
rameterized by weights and features:
?y,z,EMIT(w) =
exp ?w, f(y, z, EMIT)?
?
y? exp ?w, f(y?, z, EMIT)?
This feature-based logistic expression is equivalent
to the flat multinomial in the case that the feature
function f(y, z, EMIT) consists of all indicator fea-
tures on tuples (y, z, EMIT), which we call BASIC
features. The equivalence follows by setting weight
wy,z,EMIT = log(?y,z,EMIT).1 This formulation is
known as the natural parameterization of the multi-
nomial distribution.
In order to enhance this emission distribution, we
include coarse features in f(y, z, EMIT), in addi-
tion to the BASIC features. Crucially, these features
can be active across multiple (y, z) values. In this
way, the model can abstract general patterns, such
as a POS tag co-occurring with an inflectional mor-
pheme. We discuss specific POS features in Sec-
tion 4.
2.2 General Directed Models
Like the HMM, all of the models we propose are
based on locally normalized generative decisions
that condition on some context. In general, let X =
(Z,Y) denote the sequence of generation steps (ran-
dom variables) where Z contains all hidden random
variables and Y contains all observed random vari-
ables. The joint probability of this directed model
factors as:
Pw(X = x) =
?
i?I
Pw
(
Xi = xi
?
?Xpi(i) = xpi(i)
)
,
where Xpi(i) denotes the parents of Xi and I is the
index set of the variables in X.
In the models that we use, each factor in the above
expression is the output of a local logistic regression
1As long as no transition or emission probabilities are equal
to zero. When zeros are present, for instance to model that an
absorbing stop state can only transition to itself, it is often possi-
ble to absorb these zeros into a base measure. All the arguments
in this paper carry with a structured base measure; we drop it for
simplicity.
583
model parameterized by w:
Pw
`
Xi = d
?
?Xpi(i) = c
?
=
exp?w, f(d, c, t)?
P
d? exp?w, f(d?, c, t)?
Above, d is the generative decision value for Xi
picked by the model, c is the conditioning context
tuple of values for the parents of Xi, and t is the
type of decision being made. For instance, the POS
HMM has two types of decisions: transitions and
emissions. In the emission model, the type t is EMIT,
the decision d is a word and the context c is a tag.
The denominator normalizes the factor to be a prob-
ability distribution over decisions.
The objective function we derive from this model
is the marginal likelihood of the observations y,
along with a regularization term:
L(w) = log Pw(Y = y)? ?||w||22 (1)
This model has two advantages over the more preva-
lent form of a feature-rich unsupervised model, the
globally normalized Markov random field.2 First,
as we explain in Section 3, optimizing our objec-
tive does not require computing expectations over
the joint distribution. In the case of the POS HMM,
for example, we do not need to enumerate an in-
finite sum of products of potentials when optimiz-
ing, in contrast to Haghighi and Klein (2006). Sec-
ond, we found that locally normalized models em-
pirically outperform their globally normalized coun-
terparts, despite their efficiency and simplicity.
3 Optimization
3.1 Optimizing with Expectation Maximization
In this section, we describe the EM algorithm ap-
plied to our feature-rich, locally normalized models.
For models parameterized by standard multinomi-
als, EM optimizes L(?) = log P?(Y = y) (Demp-
ster et al, 1977). The E-step computes expected
counts for each tuple of decision d, context c, and
multinomial type t:
ed,c,t?E?
[
?
i?I
 (Xi =d, Xpi(i) =c, t)
?
?
?
?
Y = y
]
(2)
2The locally normalized model class is actually equivalent
to its globally normalized counterpart when the former meets
the following three conditions: (1) The graphical model is a
directed tree. (2) The BASIC features are included in f . (3) We
do not include regularization in the model (? = 0). This follows
from Smith and Johnson (2007).
These expected counts are then normalized in the
M-step to re-estimate ?:
?d,c,t ?
ed,c,t
?
d? ed?,c,t
Normalizing expected counts in this way maximizes
the expected complete log likelihood with respect to
the current model parameters.
EM can likewise optimize L(w) for our locally
normalized models with logistic parameterizations.
The E-step first precomputes multinomial parame-
ters from w for each decision, context, and type:
?d,c,t(w)?
exp?w, f(d, c, t)?
?
d? exp?w, f(d?, c, t)?
Then, expected counts e are computed accord-
ing to Equation 2. In the case of POS induction,
expected counts are computed with the forward-
backward algorithm in both the standard and logistic
parameterizations. The only change is that the con-
ditional probabilities ? are now functions of w.
The M-step changes more substantially, but still
relies on canonical NLP learning methods. We wish
to choose w to optimize the regularized expected
complete log likelihood:
?(w, e) =
?
d,c,t
ed,c,t log ?d,c,t(w)? ?||w||22 (3)
We optimize this objective via a gradient-based
search algorithm like LBFGS. The gradient with re-
spect to w takes the form
??(w, e) =
?
d,c,t
ed,c,t ??d,c,t(w)? 2? ?w (4)
?d,c,t(w) = f(d, c, t)?
?
d?
?d?,c,t(w)f(d?, c, t)
This gradient matches that of regularized logis-
tic regression in a supervised model: the differ-
ence ? between the observed and expected features,
summed over every decision and context. In the su-
pervised case, we would observe the count of occur-
rences of (d, c, t), but in the unsupervised M-step,
we instead substitute expected counts ed,c,t.
This gradient-based M-step is an iterative proce-
dure. For each different value of w considered dur-
ing the search, we must recompute ?(w), which re-
quires computation in proportion to the size of the
584
parameter space. However, e stays fixed throughout
the M-step. Algorithm 1 outlines EM in its entirety.
The subroutine climb(?, ?, ?) represents a generic op-
timization step such as an LBFGS iteration.
Algorithm 1 Feature-enhanced EM
repeat
Compute expected counts e  Eq. 2
repeat
Compute ?(w, e)  Eq. 3
Compute??(w, e)  Eq. 4
w ? climb(w, ?(w, e),??(w, e))
until convergence
until convergence
3.2 Direct Marginal Likelihood Optimization
Another approach to optimizing Equation 1 is to
compute the gradient of the log marginal likelihood
directly (Salakhutdinov et al, 2003). The gradient
turns out to have the same form as Equation 4, with
the key difference that ed,c,t is recomputed for every
different value of w. Algorithm 2 outlines the proce-
dure. Justification for this algorithm appears in the
Appendix.
Algorithm 2 Feature-enhanced direct gradient
repeat
Compute expected counts e  Eq. 2
Compute L(w)  Eq. 1
Compute ??(w, e)  Eq. 4
w ? climb(w, L(w),??(w, e))
until convergence
In practice, we find that this optimization ap-
proach leads to higher task accuracy for several
models. However, in cases where computing ed,c,t
is expensive, EM can be a more efficient alternative.
4 Part-of-Speech Induction
We now describe experiments that demonstrate the
effectiveness of locally normalized logistic models.
We first use the bigram HMM described in Sec-
tion 2.1 for POS induction, which has two types of
multinomials. For type EMIT, the decisions d are
words and contexts c are tags. For type TRANS, the
decisions and contexts are both tags.
4.1 POS Induction Features
We use the same set of features used by Haghighi
and Klein (2006) in their baseline globally normal-
ized Markov random field (MRF) model. These are
all coarse features on emission contexts that activate
for words with certain orthographic properties. We
use only the BASIC features for transitions. For
an emission with word y and tag z, we use the
following feature templates:
BASIC:  (y = ?, z = ?)
CONTAINS-DIGIT: Check if y contains digit and conjoin
with z:
 (containsDigit(y) = ?, z = ?)
CONTAINS-HYPHEN:  (containsHyphen(x) = ?, z = ?)
INITIAL-CAP: Check if the first letter of y is
capitalized:  (isCap(y) = ?, z = ?)
N-GRAM: Indicator functions for character n-
grams of up to length 3 present in y.
4.2 POS Induction Data and Evaluation
We train and test on the entire WSJ tag corpus (Mar-
cus et al, 1993). We attempt the most difficult ver-
sion of this task where the only information our sys-
tem can make use of is the unlabeled text itself. In
particular, we do not make use of a tagging dictio-
nary. We use 45 tag clusters, the number of POS tags
that appear in the WSJ corpus. There is an identifi-
ability issue when evaluating inferred tags. In or-
der to measure accuracy on the hand-labeled corpus,
we map each cluster to the tag that gives the highest
accuracy, the many-1 evaluation approach (Johnson,
2007). We run all POS induction models for 1000
iterations, with 10 random initializations. The mean
and standard deviation of many-1 accuracy appears
in Table 1.
4.3 POS Induction Results
We compare our model to the basic HMM and a bi-
gram version of the feature-enhanced MRF model of
Haghighi and Klein (2006). Using EM, we achieve
a many-1 accuracy of 68.1. This outperforms the
basic HMM baseline by a 5.0 margin. The same
model, trained using the direct gradient approach,
achieves a many-1 accuracy of 75.5, outperforming
the basic HMM baseline by a margin of 12.4. These
results show that the direct gradient approach can of-
fer additional boosts in performance when used with
a feature-enhanced model. We also outperform the
585
globally normalized MRF, which uses the same set
of features and which we train using a direct gradi-
ent approach.
To the best of our knowledge, our system achieves
the best performance to date on the WSJ corpus for
totally unsupervised POS tagging.3
5 Grammar Induction
We next apply our technique to a grammar induction
task: the unsupervised learning of dependency parse
trees via the dependency model with valence (DMV)
(Klein and Manning, 2004). A dependency parse is
a directed tree over tokens in a sentence. Each edge
of the tree specifies a directed dependency from a
head token to a dependent, or argument token. Thus,
the number of dependencies in a parse is exactly the
number of tokens in the sentence, not counting the
artificial root token.
5.1 Dependency Model with Valence
The DMV defines a probability distribution over de-
pendency parse trees. In this head-outward attach-
ment model, a parse and the word tokens are derived
together through a recursive generative process. For
each token generated so far, starting with the root, a
set of left dependents is generated, followed by a set
of right dependents.
There are two types of multinomial distributions
in this model. The Bernoulli STOP probabilities
?d,c,STOP capture the valence of a particular head. For
this type, the decision d is whether or not to stop
generating arguments, and the context c contains the
current head h, direction ? and adjacency adj. If
a head?s stop probability is high, it will be encour-
aged to accept few arguments. The ATTACH multi-
nomial probability distributions ?d,c,ATTACH capture
attachment preferences of heads. For this type, a de-
cision d is an argument token a, and the context c
consists of a head h and a direction ?.
We take the same approach as previous work
(Klein and Manning, 2004; Cohen and Smith, 2009)
and use gold POS tags in place of words.
3Haghighi and Klein (2006) achieve higher accuracies by
making use of labeled prototypes. We do not use any external
information.
5.2 Grammar Induction Features
One way to inject knowledge into a dependency
model is to encode the similarity between the vari-
ous morphological variants of nouns and verbs. We
encode this similarity by incorporating features into
both the STOP and the ATTACH probabilities. The
attachment features appear below; the stop feature
templates are similar and are therefore omitted.
BASIC:  (a = ?, h = ?, ? = ?)
NOUN: Generalize the morphological variants of
nouns by using isNoun(?):
 (a = ?, isNoun(h) = ?, ? = ?)
 (isNoun(a) = ?, h = ?, ? = ?)
 (isNoun(a) = ?, isNoun(h) = ?, ? = ?)
VERB: Same as above, generalizing verbs instead
of nouns by using isVerb(?)
NOUN-VERB: Same as above, generalizing with
isVerbOrNoun(?) = isVerb(?)? isNoun(?)
BACK-OFF: We add versions of all other features that
ignore direction or adjacency.
While the model has the expressive power to al-
low specific morphological variants to have their
own behaviors, the existence of coarse features en-
courages uniform analyses, which in turn gives bet-
ter accuracies.
Cohen and Smith?s (2009) method has similar
characteristics. They add a shared logistic-normal
prior (SLN) to the DMV in order to tie multinomial
parameters across related derivation events. They
achieve their best results by only tying parame-
ters between different multinomials when the cor-
responding contexts are headed by nouns and verbs.
This observation motivates the features we choose to
incorporate into the DMV.
5.3 Grammar Induction Data and Evaluation
For our English experiments we train and report di-
rected attachment accuracy on portions of the WSJ
corpus. We work with a standard, reduced version of
WSJ, WSJ10, that contains only sentences of length
10 or less after punctuation has been removed. We
train on sections 2-21, and use section 22 as a de-
velopment set. We report accuracy on section 23.
These are the same training, development, and test
sets used by Cohen and Smith (2009). The regular-
ization parameter (?) is tuned on the development
set to maximize accuracy.
For our Chinese experiments, we use the same
corpus and training/test split as Cohen and Smith
586
(2009). We train on sections 1-270 of the Penn Chi-
nese Treebank (Xue et al, 2002), similarly reduced
(CTB10). We test on sections 271-300 of CTB10,
and use sections 400-454 as a development set.
The DMV is known to be sensitive to initializa-
tion. We use the deterministic harmonic initializer
from Klein and Manning (2004). We ran each op-
timization procedure for 100 iterations. The results
are reported in Table 1.
5.4 Grammar Induction Results
We are able to outperform Cohen and Smith?s (2009)
best system, which requires a more complicated
variational inference method, on both English and
Chinese data sets. Their system achieves an accu-
racy of 61.3 for English and an accuracy of 51.9 for
Chinese.4 Our feature-enhanced model, trained us-
ing the direct gradient approach, achieves an accu-
racy of 63.0 for English, and an accuracy of 53.6 for
Chinese. To our knowledge, our method for feature-
based dependency parse induction outperforms all
existing methods that make the same set of condi-
tional independence assumptions as the DMV.
6 Word Alignment
Word alignment is a core machine learning com-
ponent of statistical machine translation systems,
and one of the few NLP tasks that is dominantly
solved using unsupervised techniques. The pur-
pose of word alignment models is to induce a cor-
respondence between the words of a sentence and
the words of its translation.
6.1 Word Alignment Models
We consider two classic generative alignment mod-
els that are both used heavily today, IBM Model 1
(Brown et al, 1994) and the HMM alignment model
(Ney and Vogel, 1996). These models generate a
hidden alignment vector z and an observed foreign
sentence y, all conditioned on an observed English
sentence e. The likelihood of both models takes the
form:
P (y, z|e) =
?
j
p(zj = i|zj?1) ? ?yj ,ei,ALIGN
4Using additional bilingual data, Cohen and Smith (2009)
achieve an accuracy of 62.0 for English, and an accuracy of
52.0 for Chinese, still below our results.
Model Inference Reg Eval
POS Induction ? Many-1
W
SJ
Basic-HMM EM ? 63.1 (1.3)
Feature-MRF LBFGS 0.1 59.6 (6.9)
Feature-HMM EM 1.0 68.1 (1.7)
LBFGS 1.0 75.5 (1.1)
Grammar Induction ? Dir
W
SJ
10
Basic-DMV EM ? 47.8
Feature-DMV EM 0.05 48.3
LBFGS 10.0 63.0
(Cohen and Smith, 2009) 61.3
C
T
B
10
Basic-DMV EM ? 42.5
Feature-DMV EM 1.0 49.9
LBFGS 5.0 53.6
(Cohen and Smith, 2009) 51.9
Word Alignment ? AER
N
IS
T
C
hE
n Basic-Model 1 EM ? 38.0
Feature-Model 1 EM ? 35.6
Basic-HMM EM ? 33.8
Feature-HMM EM ? 30.0
Word Segmentation ? F1
B
R
Basic-Unigram EM ? 76.9 (0.1)
Feature-Unigram EM 0.2 84.5 (0.5)
LBFGS 0.2 88.0 (0.1)
(Johnson and Goldwater, 2009) 87
Table 1: Locally normalized feature-based models outperform
all proposed baselines for all four tasks. LBFGS outperformed
EM in all cases where the algorithm was sufficiently fast to run.
Details of each experiment appear in the main text.
The distortion term p(zj = i|zj?1) is uniform in
Model 1, and Markovian in the HMM. See Liang et
al. (2006) for details on the specific variant of the
distortion model of the HMM that we used. We use
these standard distortion models in both the baseline
and feature-enhanced word alignment systems.
The bilexical emission model ?y,e,ALIGN differen-
tiates our feature-enhanced system from the base-
line system. In the former, the emission model is a
standard conditional multinomial that represents the
probability that decision word y is generated from
context word e, while in our system, the emission
model is re-parameterized as a logistic regression
model and feature-enhanced.
Many supervised feature-based alignment models
have been developed. In fact, this logistic parame-
terization of the HMM has been proposed before and
yielded alignment improvements, but was trained
using supervised estimation techniques (Varea et al,
2002).5 However, most full translation systems to-
5Varea et al (2002) describes unsupervised EM optimiza-
tion with logistic regression models at a high level?their dy-
namic training approach?but provides no experiments.
587
day rely on unsupervised learning so that the models
may be applied easily to many language pairs. Our
approach provides efficient and consistent unsuper-
vised estimation for feature-rich alignment models.
6.2 Word Alignment Features
The BASIC features on pairs of lexical items
provide strong baseline performance. We add
coarse features to the model in order to inject
prior knowledge and tie together lexical items with
similar characteristics.
BASIC:  (e = ?, y = ?)
EDIT-DISTANCE:  (dist(y, e) = ?)
DICTIONARY:  ((y, e) ? D) for dictionary D.
STEM:  (stem(e) = ?, y = ?) for Porter stemmer.
PREFIX:  (prefix(e) = ?, y = ?) for prefixes of
length 4.
CHARACTER:  (e = ?, charAt(y, i) = ?) for index i in
the Chinese word.
These features correspond to several common
augmentations of word alignment models, such as
adding dictionary priors and truncating long words,
but here we integrate them all coherently into a sin-
gle model.
6.3 Word Alignment Data and Evaluation
We evaluate on the standard hand-aligned portion
of the NIST 2002 Chinese-English development set
(Ayan et al, 2005). The set is annotated with sure S
and possible P alignments. We measure alignment
quality using alignment error rate (AER) (Och and
Ney, 2000).
We train the models on 10,000 sentences of FBIS
Chinese-English newswire. This is not a large-scale
experiment, but large enough to be relevant for low-
resource languages. LBFGS experiments are not
provided because computing expectations in these
models is too computationally intensive to run for
many iterations. Hence, EM training is a more ap-
propriate optimization approach: computing the M-
step gradient requires only summing over word type
pairs, while the marginal likelihood gradient needed
for LBFGS requires summing over training sentence
alignments. The final alignments, in both the base-
line and the feature-enhanced models, are computed
by training the generative models in both directions,
combining the result with hard union competitive
thresholding (DeNero and Klein, 2007), and us-
ing agreement training for the HMM (Liang et al,
2006). The combination of these techniques yields
a state-of-the-art unsupervised baseline for Chinese-
English.
6.4 Word Alignment Results
For both IBM Model 1 and the HMM alignment
model, EM training with feature-enhanced models
outperforms the standard multinomial models, by
2.4 and 3.8 AER respectively.6 As expected, large
positive weights are assigned to both the dictionary
and edit distance features. Stem and character fea-
tures also contribute to the performance gain.
7 Word Segmentation
Finally, we show that it is possible to improve upon
the simple and effective word segmentation model
presented in Liang and Klein (2009) by adding
phonological features. Unsupervised word segmen-
tation is the task of identifying word boundaries in
sentences where spaces have been removed. For a
sequence of characters y = (y1, ..., yn), a segmen-
tation is a sequence of segments z = (z1, ..., z|z|)
such that z is a partition of y and each zi is a con-
tiguous subsequence of y. Unsupervised models for
this task infer word boundaries from corpora of sen-
tences of characters without ever seeing examples of
well-formed words.
7.1 Unigram Double-Exponential Model
Liang and Klein?s (2009) unigram double-
exponential model corresponds to a simple
derivational process where sentences of characters
x are generated a word at a time, drawn from a
multinomial over all possible strings ?z,SEGMENT.
For this type, there is no context and the decision is
the particular string generated. In order to avoid the
degenerate MLE that assigns mass only to single
segment sentences it is helpful to independently
generate a length for each segment from a fixed
distribution. Liang and Klein (2009) constrain in-
dividual segments to have maximum length 10 and
generate lengths from the following distribution:
?l,LENGTH = exp(?l1.6) when 1 ? l ? 10. Their
model is deficient since it is possible to generate
6The best published results for this dataset are supervised,
and trained on 17 times more data (Haghighi et al, 2009).
588
lengths that are inconsistent with the actual lengths
of the generated segments. The likelihood equation
is given by:
P (Y = y,Z = z) =
?STOP
|z|
?
i=1
[
(1? ?STOP) ?zi,SEGMENT exp(?|zi|1.6)
]
7.2 Segmentation Data and Evaluation
We train and test on the phonetic version of the
Bernstein-Ratner corpus (1987). This is the same
set-up used by Liang and Klein (2009), Goldwater
et al (2006), and Johnson and Goldwater (2009).
This corpus consists of 9790 child-directed utter-
ances transcribed using a phonetic representation.
We measure segment F1 score on the entire corpus.
We run all word segmentation models for 300 iter-
ations with 10 random initializations and report the
mean and standard deviation of F1 in Table 1.
7.3 Segmentation Features
The SEGMENT multinomial is the important distri-
bution in this model. We use the following features:
BASIC:  (z = ?)
LENGTH:  (length(z) = ?)
NUMBER-VOWELS:  (numVowels(z) = ?)
PHONO-CLASS-PREF:  (prefix(coarsePhonemes(z)) = ?)
PHONO-CLASS-PREF:  (suffix(coarsePhonemes(z)) = ?)
The phonological class prefix and suffix features
project each phoneme of a string to a coarser class
and then take prefix and suffix indicators on the
string of projected characters. We include two ver-
sions of these features that use projections with dif-
ferent levels of coarseness. The goal of these fea-
tures is to help the model learn general phonetic
shapes that correspond to well-formed word bound-
aries.
As is the case in general for our method, the
feature-enhanced unigram model still respects the
conditional independence assumptions that the stan-
dard unigram model makes, and inference is still
performed using a simple dynamic program to com-
pute expected sufficient statistics, which are just seg-
ment counts.
7.4 Segmentation Results
To our knowledge our system achieves the best per-
formance to date on the Bernstein-Ratner corpus,
with an F1 of 88.0. It is substantially simpler than
the non-parametric Bayesian models proposed by
Johnson et al (2007), which require sampling pro-
cedures to perform inference and achieve an F1 of
87 (Johnson and Goldwater, 2009). Similar to our
other results, the direct gradient approach outper-
forms EM for feature-enhanced models, and both
approaches outperform the baseline, which achieves
an F1 of 76.9.
8 Conclusion
We have shown that simple, locally normalized
models can effectively incorporate features into un-
supervised models. These enriched models can
be easily optimized using standard NLP build-
ing blocks. Beyond the four tasks explored in
this paper?POS tagging, DMV grammar induc-
tion, word alignment, and word segmentation?the
method can be applied to many other tasks, for ex-
ample grounded semantics, unsupervised PCFG in-
duction, document clustering, and anaphora resolu-
tion.
Acknowledgements
We thank Percy Liang for making his word segmen-
tation code available to us, and the anonymous re-
viewers for their comments.
Appendix: Optimization
In this section, we derive the gradient of the log marginal likeli-
hood needed for the direct gradient approach. Let w0 be the cur-
rent weights in Algorithm 2 and e = e(w0) be the expectations
under these weights as computed in Equation 2. In order to jus-
tify Algorithm 2, we need to prove that ?L(w0) = ??(w0, e).
We use the following simple lemma: if ?, ? are real-valued
functions such that: (1) ?(w0) = ?(w0) for some w0; (2)
?(w) ? ?(w) on an open set containing w0; and (3), ? and ?
are differentiable at w0; then ??(w0) = ??(w0).
We set ?(w) = L(w) and ?(w) = ?(w, e)?P
z
Pw0(Z =
z|Y = y) log Pw0(Z = z|Y = y). If we can show that ?, ?
satisfy the conditions of the lemma we are done since the second
term of ? depends on w0, but not on w.
Property (3) can be easily checked, and property (2) follows
from Jensen?s inequality. Finally, property (1) follows from
Lemma 2 of Neal and Hinton (1998).
589
References
N. F. Ayan, B. Dorr, and C. Monz. 2005. Combining
word alignments using neural networks. In Empirical
Methods in Natural Language Processing.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. K. Nelson and A. van Kleeck.
M. Bisani and H. Ney. 2008. Joint-sequence models for
grapheme-to-phoneme conversion.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change.
In Neural Information Processing Systems.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In North American Chapter
of the Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological).
J. DeNero and D. Klein. 2007. Tailoring word align-
ments to syntactic machine translation. In Association
for Computational Linguistics.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Association for Computational Lin-
guistics.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Association for Computa-
tional Linguistics.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Association for Computational Linguistics.
M. Johnson and S. Goldwater. 2009. Improving non-
parametric Bayesian inference: Experiments on unsu-
pervised word segmentation with adaptor grammars.
In North American Chapter of the Association for
Computational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: a framework for specifying com-
positional nonparametric Bayesian models. In Neural
Information Processing Systems.
M. Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing/Computational Natural Language
Learning.
D. Klein and C. D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Association for Computational
Linguistics.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In North American Chapter of the As-
sociation for Computational Linguistics.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Chapter of the Associ-
ation for Computational Linguistics.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics.
R. Neal and G. E. Hinton. 1998. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models. Kluwer
Academic Publishers.
H. Ney and S. Vogel. 1996. HMM-based word alignment
in statistical translation. In International Conference
on Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Association for Computational Lin-
guistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003.
Optimization with EM and expectation-conjugate-
gradient. In International Conference on Machine
Learning.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In As-
sociation for Computational Linguistics.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Linguistics.
I. G. Varea, F. J. Och, H. Ney, and F. Casacuberta. 2002.
Refined lexicon models for statistical machine transla-
tion using a maximum entropy approach. In Associa-
tion for Computational Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building a
large-scale annotated Chinese corpus. In International
Conference on Computational Linguistics.
590
Proceedings of NAACL-HLT 2013, pages 1120?1130,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning Whom to Trust with MACE
Dirk Hovy1 Taylor Berg-Kirkpatrick2 Ashish Vaswani1 Eduard Hovy3
(1) Information Sciences Institute, University of Southern California, Marina del Rey
(2) Computer Science Division, University of California at Berkeley
(3) Language Technology Institute, Carnegie Mellon University, Pittsburgh
{dirkh,avaswani}@isi.edu, tberg@cs.berkeley.edu, hovy@cmu.edu
Abstract
Non-expert annotation services like Amazon?s
Mechanical Turk (AMT) are cheap and fast
ways to evaluate systems and provide categor-
ical annotations for training data. Unfortu-
nately, some annotators choose bad labels in
order to maximize their pay. Manual iden-
tification is tedious, so we experiment with
an item-response model. It learns in an un-
supervised fashion to a) identify which an-
notators are trustworthy and b) predict the
correct underlying labels. We match perfor-
mance of more complex state-of-the-art sys-
tems and perform well even under adversarial
conditions. We show considerable improve-
ments over standard baselines, both for pre-
dicted label accuracy and trustworthiness es-
timates. The latter can be further improved
by introducing a prior on model parameters
and using Variational Bayes inference. Ad-
ditionally, we can achieve even higher accu-
racy by focusing on the instances our model is
most confident in (trading in some recall), and
by incorporating annotated control instances.
Our system, MACE (Multi-Annotator Compe-
tence Estimation), is available for download1.
1 Introduction
Amazon?s MechanicalTurk (AMT) is frequently
used to evaluate experiments and annotate data in
NLP (Callison-Burch et al, 2010; Callison-Burch
and Dredze, 2010; Jha et al, 2010; Zaidan and
Callison-Burch, 2011). However, some turkers try to
maximize their pay by supplying quick answers that
have nothing to do with the correct label. We refer to
1Available under http://www.isi.edu/
publications/licensed-sw/mace/index.html
this type of annotator as a spammer. In order to mit-
igate the effect of spammers, researchers typically
collect multiple annotations of the same instance so
that they can, later, use de-noising methods to infer
the best label. The simplest approach is majority
voting, which weights all answers equally. Unfor-
tunately, it is easy for majority voting to go wrong.
A common and simple spammer strategy for cate-
gorical labeling tasks is to always choose the same
(often the first) label. When multiple spammers
follow this strategy, the majority can be incorrect.
While this specific scenario might seem simple to
correct for (remove annotators that always produce
the same label), the situation grows more tricky
when spammers do not annotate consistently, but in-
stead choose labels at random. A more sophisticated
approach than simple majority voting is required.
If we knew whom to trust, and when, we could
reconstruct the correct labels. Yet, the only way
to be sure we know whom to trust is if we knew
the correct labels ahead of time. To address this
circular problem, we build a generative model of the
annotation process that treats the correct labels as
latent variables. We then use unsupervised learning
to estimate parameters directly from redundant
annotations. This is a common approach in the
class of unsupervised models called item-response
models (Dawid and Skene, 1979; Whitehill et al,
2009; Carpenter, 2008; Raykar and Yu, 2012).
While such models have been implemented in
other fields (e.g., vision), we are not aware of their
availability for NLP tasks (see also Section 6).
Our model includes a binary latent variable that
explicitly encodes if and when each annotator is
spamming, as well as parameters that model the
annotator?s specific spamming ?strategy?. Impor-
1120
tantly, the model assumes that labels produced by
an annotator when spamming are independent of
the true label (though, a spammer can still produce
the correct label by chance).
In experiments, our model effectively differenti-
ates dutiful annotators from spammers (Section 4),
and is able to reconstruct the correct label with high
accuracy (Section 5), even under extremely adver-
sarial conditions (Section 5.2). It does not require
any annotated instances, but is capable of including
varying levels of supervision via token constraints
(Section 5.2). We consistently outperform major-
ity voting, and achieve performance equal to that of
more complex state-of-the-art models. Additionally,
we find that thresholding based on the posterior la-
bel entropy can be used to trade off coverage for ac-
curacy in label reconstruction, giving considerable
gains (Section 5.1). In tasks where correct answers
are more important than answering every instance,
e.g., when constructing a new annotated corpus, this
feature is extremely valuable. Our contributions are:
? We demonstrate the effectiveness of our model
on real world AMT datasets, matching the ac-
curacy of more complex state-of-the-art sys-
tems
? We show how posterior entropy can be used to
trade some coverage for considerable gains in
accuracy
? We study how various factors affect perfor-
mance, including number of annotators, anno-
tator strategy, and available supervision
? We provide MACE (Multi-Annotator Compe-
tence Estimation), a Java-based implementa-
tion of a simple and scalable unsupervised
model that identifies malicious annotators and
predicts labels with high accuracy
2 Model
We keep our model as simple as possible so that it
can be effectively trained from data where annotator
quality is unknown. If the model has too many
parameters, unsupervised learning can easily pick
up on and exploit coincidental correlations in the
data. Thus, we make a modeling assumption that
keeps our parameterization simple. We assume that
an annotator always produces the correct label when
N
T
i
M
A
ij
S
ij
T
A
2
C
2
A
3
C
3
A
1
C
1
Figure 1: Graphical model: Annotator j produces
label Aij on instance i. Label choice depends on
instance?s true label Ti, and whether j is spam-
ming on i, modeled by binary variable Sij . N =
|instances|, M = |annotators|.
for i = 1 . . . N :
Ti ? Uniform
for j = 1 . . .M :
Sij ? Bernoulli(1? ?j)
if Sij = 0 :
Aij = Ti
else :
Aij ? Multinomial(?j)
Figure 2: Generative process: see text for descrip-
tion.
he tries to. While this assumption does not reflect
the reality of AMT, it allows us to focus the model?s
power where it?s important: explaining away labels
that are not correlated with the correct label.
Our model generates the observed annotations as
follows: First, for each instance i, we sample the
true label Ti from a uniform prior. Then, for each
annotator j we draw a binary variable Sij from a
Bernoulli distribution with parameter 1 ? ?j . Sij
represents whether or not annotator j is spamming
on instance i. We assume that when an annotator
is not spamming on an instance, i.e. Sij = 0, he
just copies the true label to produce annotation Aij .
If Sij = 1, we say that the annotator is spamming
on the current instance, and Aij is sampled from
a multinomial with parameter vector ?j . Note that
in this case the annotation Aij does not depend on
the true label Ti. The annotations Aij are observed,
1121
while the true labels Ti and the spamming indicators
Sij are unobserved. The graphical model is shown
in Figure 1 and the generative process is described
in Figure 2.
The model parameters are ?j and ?j . ?j specifies
the probability of trustworthiness for annotator j
(i.e. the probability that he is not spamming on
any given instance). The learned value of ?j will
prove useful later when we try to identify reliable
annotators (see Section 4). The vector ?j determines
how annotator j behaves when he is spamming. An
annotator can produce the correct answer even while
spamming, but this can happen only by chance since
the annotator must use the same multinomial param-
eters ?j across all instances. This means that we only
learn annotator biases that are not correlated with
the correct label, e.g., the strategy of the spammer
who always chooses a certain label. This contrasts
with previous work where additional parameters are
used to model the biases that even dutiful annotators
exhibit. Note that an annotator can also choose not
to answer, which we can naturally accommodate be-
cause the model is generative. We enhance our gen-
erative model by adding Beta and Dirichlet priors on
?j and ?j respectively which allows us to incorporate
prior beliefs about our annotators (section 2.1).
2.1 Learning
We would like to set our model parameters to
maximize the probability of the observed data, i.e.,
the marginal data likelihood:
P (A; ?, ?) =
X
T,S
h NY
i=1
P (Ti) ?
MY
j=1
P (Sij ; ?j) ? P (Aij |Sij , Ti; ?j)
i
where A is the matrix of annotations, S is the
matrix of competence indicators, and T is the vector
of true labels.
We maximize the marginal data likelihood using
Expectation Maximization (EM) (Dempster et al,
1977), which has successfully been applied to
similar problems (Dawid and Skene, 1979). We ini-
tialize EM randomly and run for 50 iterations. We
perform 100 random restarts, and keep the model
with the best marginal data likelihood. We smooth
the M-step by adding a fixed value ? to the fractional
counts before normalizing (Eisner, 2002). We find
that smoothing improves accuracy, but, overall,
learning is robust to varying ?, and set ? = 0.1num labels .
We observe, however, that the average annota-
tor proficiency is usually high, i.e., most annota-
tors answer correctly. The distribution learned by
EM, however, is fairly linear. To improve the cor-
relation between model estimates and true annotator
proficiency, we would like to add priors about the
annotator behavior into the model. A straightfor-
ward approach is to employ Bayesian inference with
Beta priors on the proficiency parameters, ?j . We
thus also implement Variational-Bayes (VB) train-
ing with symmetric Beta priors on ?j and symmet-
ric Dirichlet priors on the strategy parameters, ?j .
Setting the shape parameters of the Beta distribution
to 0.5 favors the extremes of the distribution, i.e.,
either an annotator tried to get the right answer, or
simply did not care, but (almost) nobody tried ?a lit-
tle?. With VB training, we observe improved corre-
lations over all test sets with no loss in accuracy. The
hyper-parameters of the Dirichlet distribution on ?j
were clamped to 10.0 for all our experiments with
VB training. Our implementation is similar to John-
son (2007), which the reader can refer to for details.
3 Experiments
We evaluate our method on existing annotated
datasets from various AMT tasks. However, we
also want to ensure that our model can handle
adversarial conditions. Since we have no control
over the factors in existing datasets, we create
synthetic data for this purpose.
3.1 Natural Data
In order to evaluate our model, we use the
datasets from (Snow et al, 2008) that use discrete
label values (some tasks used continuous values,
which we currently do not model). Since they
compared AMT annotations to experts, gold anno-
tations exist for these sets. We can thus evaluate
the accuracy of the model as well as the proficiency
of each annotator. We show results for word sense
disambiguation (WSD: 177 items, 34 annotators),
recognizing textual entailment (RTE: 800 items,
164 annotators), and recognizing temporal relation
(Temporal: 462 items, 76 annotators).
3.2 Synthetic Data
In addition to the datasets above, we generate
synthetic data in order to control for different
1122
factors. This also allows us to create a gold standard
to which we can compare. We generate data sets
with 100 items, using two or four possible labels.
For each item, we generate answers from 20
different annotators. The ?annotators? are functions
that return one of the available labels according
to some strategy. Better annotators have a smaller
chance of guessing at random.
For various reasons, usually not all annotators see
or answer all items. We thus remove a randomly
selected subset of answers such that each item is
only answered by 10 of the annotators. See Figure
3 for an example annotation of three items.
annotators
ite
m
s ? 0 0 1 ? 0 ? ? 0 ?
1 ? ? 0 ? 1 0 ? ? 0
? ? 0 ? 0 1 ? 0 ? 0
Figure 3: Annotations: 10 annotators on three items,
labels {1, 0}, 5 annotations/item. Missing annota-
tions marked ???
3.3 Evaluations
First, we want to know which annotators to trust.
We evaluate whether our model?s learned trustwor-
thiness parameters ?j can be used to identify these
individuals (Section 4).
We then compare the label predicted by our model
and by majority voting to the correct label. The
results are reported as accuracy (Section 5). Since
our model computes posterior entropies for each
instance, we can use this as an approximation for the
model?s confidence in the prediction. If we focus on
predictions with high confidence (i.e., low entropy),
we hope to see better accuracy, even at the price of
leaving some items unanswered. We evaluate this
trade-off in Section 5.1. In addition, we investigate
the influence of the number of spammers and their
strategy on the accuracy of our model (Section 5.2).
4 Identifying Reliable Annotators
One of the distinguishing features of the model
is that it uses a parameter for each annotator to
estimate whether or not they are spamming. Can
we use this parameter to identify trustworthy indi-
viduals, to invite them for future tasks, and block
untrustworthy ones?
RTE Temporal WSD
raw agreement 0.78 0.73 0.81
Cohen?s ? 0.70 0.80 0.13
G-index 0.76 0.73 0.81
MACE-EM 0.87 0.88 0.44
MACE-VB (0.5,0.5) 0.91 0.90 0.90
Table 1: Correlation with annotator proficiency:
Pearson ? of different methods for various data sets.
MACE-VB?s trustworthiness parameter (trained
with Variational Bayes with ? = ? = 0.5) corre-
lates best with true annotator proficiency.
It is natural to apply some form of weighting.
One approach is to assume that reliable annotators
agree more with others than random annotators.
Inter-annotator agreement is thus a good candidate
to weigh the answers. There are various measures
for inter-annotator agreement.
Tratz and Hovy (2010) compute the average
agreement of each annotator and use it as a weight
to identify reliable ones. Raw agreement can be
directly computed from the data. It is related to
majority voting, since it will produce high scores for
all members of the majority class. Raw agreement
is thus a very simple measure.
In contrast, Cohen?s ? corrects the agreement
between two annotators for chance agreement. It
is widely used for inter-annotator agreement in
annotation tasks. We also compute the ? values
for each pair of annotators, and average them for
each annotator (similar to the approach in Tratz and
Hovy (2010)). However, whenever one label is more
prevalent (a common case in NLP tasks), ? overesti-
mates the effect of chance agreement (Feinstein and
Cicchetti, 1990) and penalizes disproportionately.
The G-index (Gwet, 2008) corrects for the number
of labels rather than chance agreement.
We compare these measures to our learned trust-
worthiness parameters ?j in terms of their ability to
select reliable annotators. A better measure should
lend higher score to annotators who answer correctly
more often than others. We thus compare the ratings
of each measure to the true proficiency of each
annotator. This is the percentage of annotated items
the annotator answered correctly. Methods that can
identify reliable annotators should highly correlate
1123
to the annotator?s proficiency. Since the methods
use different scales, we compute Pearson?s ? for the
correlation coefficient, which is scale-invariant. The
correlation results are shown in Table 1.
The model?s ?j correlates much more strongly
with annotator proficiency than either ? or raw
agreement. The variant trained with VB performs
consistently better than standard EM training, and
yields the best results. This show that our model
detects reliable annotators much better than any
of the other measures, which are only loosely
correlated to annotator proficiency.
The numbers for WSD also illustrate the low ?
score resulting when all annotators (correctly) agree
on a small number of labels. However, all inter-
annotator agreement measures suffer from an even
more fundamental problem: removing/ignoring
annotators with low agreement will always improve
the overall score, irrespective of the quality of their
annotations. Worse, there is no natural stopping
point: deleting the most egregious outlier always
improves agreement, until we have only one anno-
tator with perfect agreement left (Hovy, 2010). In
contrast, MACE does not discard any annotators,
but weighs their contributions differently. We are
thus not losing information. This works well even
under adversarial conditions (see Section 5.2).
5 Recovering the Correct Answer
RTE Temporal WSD
majority 0.90 0.93 0.99
Raykar/Yu 2012 0.93 0.94 ?
Carpenter 2008 0.93 ? ?
MACE-EM/VB 0.93 0.94 0.99
MACE-EM@90 0.95 0.97 0.99
MACE-EM@75 0.95 0.97 1.0
MACE-VB@90 0.96 0.97 1.0
MACE-VB@75 0.98 0.98 1.0
Table 2: Accuracy of different methods on data sets
from (Snow et al, 2008). MACE-VB uses Varia-
tional Bayes training. Results @n use the n% items
the model is most confident in (Section 5.1). Results
below double line trade coverage for accuracy and
are thus not comparable to upper half.
The previous sections showed that our model reli-
ably identifies trustworthy annotators. However, we
also want to find the most likely correct answer. Us-
ing majority voting often fails to find the correct la-
bel. This problem worsens when there are more than
two labels. We need to take relative majorities into
account or break ties when two or more labels re-
ceive the same number of votes. This is deeply un-
satisfying.
Figure 2 shows the accuracy of our model on
various data sets from Snow et al (2008). The
model outperforms majority voting on both RTE
and Temporal recognition sets. It performs as well
as majority voting for the WSD task. This last set
is somewhat of an exception, though, since almost
all annotators are correct all the time, so majority
voting is trivially correct. Still, we need to ensure
that the model does not perform worse under such
conditions. The results for RTE and Temporal data
also rival those reported in Raykar and Yu (2012)
and Carpenter (2008), yet were achieved with a
much simpler model.
Carpenter (2008) models instance difficulty as
a parameter. While it seems intuitively useful to
model which items are harder than other, it increases
the parameter space more than our trustworthiness
variable. We achieve comparable performance with-
out modeling difficulty, which greatly simplifies
inference. The model of Raykar and Yu (2012) is
more similar to our approach, in that it does not
model item difficulty. However, it adds an extra step
that learns priors from the estimated parameters. In
our model, this is part of the training process. For
more details on both models, see Section 6.
5.1 Trading Coverage for Accuracy
Sometimes, we want to produce an answer for ev-
ery item (e.g., when evaluating a data set), and some-
times, we value good answers more than answering
all items (e.g., when developing an annotated
corpus). Jha et al (2010) have demonstrated how to
achieve better coverage (i.e., answer more items) by
relaxing the majority voting constraints. Similarly,
we can improve accuracy if we only select high qual-
ity annotations, even if this incurs lower coverage.
We provide a parameter in MACE that allows
users to set a threshold for this trade-off: the
model only returns a label for an instance if it is
sufficiently confident in its answer. We approximate
the model?s confidence by the posterior entropy of
1124
0$&((0
0$&(9%
PDMRULW\
Figure 4: Tradeoff between coverage and accuracy for RTE (left) and temporal (right). Lower thresholds
lead to less coverage, but result in higher accuracy.
each instance. However, entropy depends strongly
on the specific makeup of the dataset (number of
annotators and labels, etc.), so it is hard for the user
to set a specific threshold.
Instead of requiring an exact entropy value, we
provide a simple thresholding between 0.0 and 1.0
(setting the threshold to 1.0 will include all items).
After training, MACE orders the posterior entropies
for all instances and selects the value that covers
the selected fraction of the instances. The threshold
thus roughly corresponds to coverage. It then only
returns answers for instances whose entropy is
below the threshold. This procedure is similar to
precision/recall curves.
Jha et al (2010) showed the effect of varying the
relative majority required, i.e., requiring that at least
n out of 10 annotators have to agree to count an
item. We use that method as baseline comparison,
evaluating the effect on coverage and accuracy
when we vary n from 5 to 10.
Figure 4 shows the tradeoff between coverage
and accuracy for two data sets. Lower thresholds
produce more accurate answers, but result in lower
coverage, as some items are left blank. If we pro-
duce answers for all items, we achieve accuracies
of 0.93 for RTE and 0.94 for Temporal, but by
excluding just the 10% of items in which the model
is least confident, we achieve accuracies as high as
0.95 for RTE and 0.97 for Temporal. We omit the
results for WSD here, since there is little headroom
and they are thus not very informative. Using Varia-
tional Bayes inference consistently achieves higher
results for the same coverage than the standard im-
plementation. Increasing the required majority also
improves accuracy, although not as much, and the
loss in coverage is larger and cannot be controlled.
In contrast, our method allows us to achieve better
accuracy at a smaller, controlled loss in coverage.
5.2 Influence of Strategy, Number of
Annotators, and Supervision
Adverse Strategy We showed that our model
recovers the correct answer with high accuracy.
However, to test whether this is just a function of
the annotator pool, we experiment with varying
the trustworthiness of the pool. If most annotators
answer correctly, majority voting is trivially correct,
as is our model. What happens, however, if more
and more annotators are unreliable? While some
agreement can arise from randomness, majority
voting is bound to become worse?can our model
overcome this problem? We set up a second set of
experiments to test this, using synthetic data. We
choose 20 annotators and vary the amount of good
annotators among them from 0 to 10 (after which
the trivial case sets in). We define a good annotator
as one who answers correctly 95% of the time.2
Adverse annotators select their answers randomly or
always choose a certain value (minimal annotators).
These are two frequent strategies of spammers.
For different numbers of labels and varying
percentage of spammers, we measure the accuracy
2The best annotators on the Snow data sets actually found
the correct answer 100% of the time.
1125
a) random annotators b) minimal annotators
Figure 5: Influence of adverse annotator strategy on label accuracy (y-axis). Number of possible labels
varied between 2 (top row) and 4 (bottom row). Adverse annotators either choose at random (a) or always
select the first label (b). MACE needs fewer good annotators to recover the correct answer.
0$&((0
0$&(9%
PDMRULW\
Figure 6: Varying number of annotators: effect on prediction accuracy. Each point averaged over 10 runs.
Note different scale for WSD.
of our model and majority voting on 100 items,
averaged over 10 runs for each condition. Figure
5 shows the effect of annotator proficiency on both
majority voting and our method for both kinds of
spammers. Annotator pool strategy affects majority
voting more than our model. Even with few good
annotators, our model learns to dismiss the spam-
mers as noise. There is a noticeable point on each
graph where MACE diverges from the majority
voting line. It thus reaches good accuracy much
1126
faster than majority voting, i.e., with fewer good an-
notators. This divergence point happens earlier with
more label values when adverse annotators label
randomly. In general, random annotators are easier
to deal with than the ones always choosing the first
label. Note that in cases where we have a majority
of adversarial annotators, VB performs worse than
EM, since this condition violates the implicit as-
sumptions we encoded with the priors in VB. Under
these conditions, setting different priors to reflect
the annotator pool should improve performance.
Obviously, both of these pools are extremes: it is
unlikely to have so few good or so many malicious
annotators. Most pools will be somewhere in
between. It does show, however, that our model
can pick up on reliable annotators even under very
unfavorable conditions. The result has a practical
upshot: AMT allows us to require a minimum rating
for annotators to work on a task. Higher ratings
improve annotation quality, but delay completion,
since there are fewer annotators with high ratings.
The results in this section suggest that we can find
the correct answer even in annotator pools with low
overall proficiency. We can thus waive the rating
requirement and allow more annotators to work on
the task. This considerably speeds up completion.
Number of Annotators Figure 6 shows the effect
different numbers of annotators have on accuracy.
As we increase the number of annotators, MACE
and majority voting achieve better accuracy results.
We note that majority voting results level or even
drop when going from an odd to an even number.
In these cases, the new annotator does not improve
accuracy if it goes with the previous majority (i.e.,
going from 3:2 to 4:2), but can force an error when
going against the previous majority (i.e., from 3:2 to
3:3), by creating a tie. MACE-EM and MACE-VB
dominate majority voting for RTE and Temporal.
For WSD, the picture is less clear, where majority
voting dominates when there are fewer annotators.
Note that the differences are minute, though (within
1 percentage point). For very small pool sizes (< 3),
MACE-VB outperforms both other methods.
Amount of Supervision So far, we have treated
the task as completely unsupervised. MACE does
not require any expert annotations in order to
achieve high accuracy. However, we often have
annotations for some of the items. These annotated
data points are usually used as control items (by
removing annotators that answer them incorrectly).
If such annotated data is available, we would like
to make use of it. We include an option that lets
users supply annotations for some of the items,
and use this information as token constraints in the
E-step of training. In those cases, the model does
not need to estimate the correct value, but only has
to adjust the trust parameter. This leads to improved
performance.3
We explore for RTE and Temporal how per-
formance changes when we vary the amount of
supervision in increments of 5%.4 We average over
10 runs for each value of n, each time supplying an-
notations for a random set of n items. The baseline
uses the annotated label whenever supplied, other-
wise the majority vote, with ties split at random.
Figure 7 shows that, unsurprisingly, all methods
improve with additional supervision, ultimately
reaching perfect accuracy. However, MACE uses
the information more effectively, resulting in
higher accuracy for a given amount of supervision.
This gain is more pronounced when only little
supervision is available.
6 Related Research
Snow et al (2008) and Sorokin and Forsyth
(2008) showed that Amazon?s MechanicalTurk use
in providing non-expert annotations for NLP tasks.
Various models have been proposed for predicting
correct annotations from noisy non-expert annota-
tions and for estimating annotator trustworthiness.
These models divide naturally into two categories:
those that use expert annotations for supervised
learning (Snow et al, 2008; Bian et al, 2009), and
completely unsupervised ones. Our method falls
into the latter category because it learns from the
redundant non-expert annotations themselves, and
makes no use of expertly annotated data.
Most previous work on unsupervised models
belongs to a class called ?Item-response models?,
used in psychometrics. The approaches differ with
respect to which aspect of the annotation process
3If we had annotations for all items, accuracy would be per-
fect and require no training.
4Given the high accuracy for the WSD data set even in the
fully unsupervised case, we omit the results here.
1127
Figure 7: Varying the amount of supervision: effect on prediction accuracy. Each point averaged over 10
runs. MACE uses supervision more efficiently.
they choose to focus on, and the type of annotation
task they model. For example, many methods ex-
plicitly model annotator bias in addition to annotator
competence (Dawid and Skene, 1979; Smyth et al,
1995). Our work models annotator bias, but only
when the annotator is suspected to be spamming.
Other methods focus modeling power on instance
difficulty to learn not only which annotators are
good, but which instances are hard (Carpenter,
2008; Whitehill et al, 2009). In machine vision,
several models have taken this further by parameter-
izing difficulty in terms of complex features defined
on each pairing of annotator and annotation instance
(Welinder et al, 2010; Yan et al, 2010). While
such features prove very useful in vision, they are
more difficult to define for the categorical problems
common to NLP. In addition, several methods are
specifically tailored to annotation tasks that involve
ranking (Steyvers et al, 2009; Lee et al, 2011),
which limits their applicability in NLP.
The method of Raykar and Yu (2012) is most
similar to ours. Their goal is to identify and filter
out annotators whose annotations are not correlated
with the gold label. They define a function of the
learned parameters that is useful for identifying
these spammers, and then use this function to build
a prior. In contrast, we use simple priors, but incor-
porate a model parameter that explicitly represents
the probability that an annotator is spamming. Our
simple model achieves the same accuracy on gold
label predictions as theirs.
7 Conclusion
We provide a Java-based implementation, MACE,
that recovers correct labels with high accuracy, and
reliably identifies trustworthy annotators. In
addition, it provides a threshold to control the
accuracy/coverage trade-off and can be trained with
standard EM or Variational Bayes EM. MACE
works fully unsupervised, but can incorporate token
constraints via annotated control items. We show
that even small amounts help improve accuracy.
Our model focuses most of its modeling power
on learning trustworthiness parameters, which
are highly correlated with true annotator relia-
bility (Pearson ? 0.9). We show on real-world
and synthetic data sets that our method is more
accurate than majority voting, even under ad-
versarial conditions, and as accurate as more
complex state-of-the-art systems. Focusing on high-
confidence instances improves accuracy consider-
ably. MACE is freely available for download under
http://www.isi.edu/publications/
licensed-sw/mace/index.html.
Acknowledgements
The authors would like to thank Chris Callison-
Burch, Victoria Fossum, Stephan Gouws, Marc
Schulder, Nathan Schneider, and Noah Smith for
invaluable discussions, as well as the reviewers for
their constructive feedback.
1128
References
Jiang Bian, Yandong Liu, Ding Zhou, Eugene Agichtein,
and Hongyuan Zha. 2009. Learning to recognize re-
liable users and content in social media with coupled
mutual reinforcement. In Proceedings of the 18th in-
ternational conference on World wide web, pages 51?
60. ACM.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Unpublished manuscript.
A. Philip Dawid and Allan M. Skene. 1979. Maximum
likelihood estimation of observer error-rates using the
EM algorithm. Applied Statistics, pages 20?28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
Eduard Hovy. 2010. Annotation. A Tutorial. In 48th
Annual Meeting of the Association for Computational
Linguistics.
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-
thal, and Kathleen McKeown. 2010. Corpus creation
for new genres: A crowdsourced approach to pp at-
tachment. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 13?20. Asso-
ciation for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Michael D. Lee, Mark Steyvers, Mindy de Young, and
Brent J. Miller. 2011. A model-based approach to
measuring expertise in ranking tasks. In L. Carlson,
C. Ho?lscher, and T.F. Shipley, editors, Proceedings of
the 33rd Annual Conference of the Cognitive Science
Society, Austin, TX. Cognitive Science Society.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating
Spammers and Ranking Annotators for Crowdsourced
Labeling Tasks. Journal of Machine Learning Re-
search, 13:491?518.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Advances
in neural information processing systems, pages 1085?
1092.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with Amazon Mechanical Turk. In
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition Workshops, CVPRW ?08,
pages 1?8. IEEE.
Mark Steyvers, Michael D. Lee, Brent Miller, and
Pernille Hemmer. 2009. The wisdom of crowds in the
recollection of order information. Advances in neural
information processing systems, 23.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 678?687. Association for Computational
Linguistics.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In Neural Information Processing Systems
Conference (NIPS), volume 6.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. Advances in Neural In-
formation Processing Systems, 22:2035?2043.
Yan Yan, Ro?mer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
1129
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In International Conference on Artificial Intelligence
and Statistics.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, USA, June. Association for
Computational Linguistics.
1130
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288?1297,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Phylogenetic Grammar Induction
Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division
University of California, Berkeley
{tberg, klein}@cs.berkeley.edu
Abstract
We present an approach to multilin-
gual grammar induction that exploits a
phylogeny-structured model of parameter
drift. Our method does not require any
translated texts or token-level alignments.
Instead, the phylogenetic prior couples
languages at a parameter level. Joint in-
duction in the multilingual model substan-
tially outperforms independent learning,
with larger gains both from more articu-
lated phylogenies and as well as from in-
creasing numbers of languages. Across
eight languages, the multilingual approach
gives error reductions over the standard
monolingual DMV averaging 21.1% and
reaching as high as 39%.
1 Introduction
Learning multiple languages together should be
easier than learning them separately. For exam-
ple, in the domain of syntactic parsing, a range
of recent work has exploited the mutual constraint
between two languages? parses of the same bi-
text (Kuhn, 2004; Burkett and Klein, 2008; Kuz-
man et al, 2009; Smith and Eisner, 2009; Sny-
der et al, 2009a). Moreover, Snyder et al (2009b)
in the context of unsupervised part-of-speech in-
duction (and Bouchard-Co?te? et al (2007) in the
context of phonology) show that extending be-
yond two languages can provide increasing ben-
efit. However, multitexts are only available for
limited languages and domains. In this work, we
consider unsupervised grammar induction without
bitexts or multitexts. Without translation exam-
ples, multilingual constraints cannot be exploited
at the sentence token level. Rather, we capture
multilingual constraints at a parameter level, us-
ing a phylogeny-structured prior to tie together the
various individual languages? learning problems.
Our joint, hierarchical prior couples model param-
eters for different languages in a way that respects
knowledge about how the languages evolved.
Aspects of this work are closely related to Co-
hen and Smith (2009) and Bouchard-Co?te? et al
(2007). Cohen and Smith (2009) present a model
for jointly learning English and Chinese depen-
dency grammars without bitexts. In their work,
structurally constrained covariance in a logistic
normal prior is used to couple parameters between
the two languages. Our work, though also differ-
ent in technical approach, differs most centrally in
the extension to multiple languages and the use of
a phylogeny. Bouchard-Co?te? et al (2007) consid-
ers an entirely different problem, phonological re-
construction, but shares with this work both the
use of a phylogenetic structure as well as the use
of log-linear parameterization of local model com-
ponents. Our work differs from theirs primarily
in the task (syntax vs. phonology) and the vari-
ables governed by the phylogeny: in our model it
is the grammar parameters that drift (in the prior)
rather than individual word forms (in the likeli-
hood model).
Specifically, we consider dependency induction
in the DMV model of Klein and Manning (2004).
Our data is a collection of standard dependency
data sets in eight languages: English, Dutch, Dan-
ish, Swedish, Spanish, Portuguese, Slovene, and
Chinese. Our focus is not the DMV model itself,
which is well-studied, but rather the prior which
couples the various languages? parameters. While
some choices of prior structure can greatly com-
plicate inference (Cohen and Smith, 2009), we
choose a hierarchical Gaussian form for the drift
term, which allows the gradient of the observed
data likelihood to be easily computed using stan-
dard dynamic programming methods.
In our experiments, joint multilingual learning
substantially outperforms independent monolin-
gual learning. Using a limited phylogeny that
1288
only couples languages within linguistic families
reduces error by 5.6% over the monolingual base-
line. Using a flat, global phylogeny gives a greater
reduction, almost 10%. Finally, a more articu-
lated phylogeny that captures both inter- and intra-
family effects gives an even larger average relative
error reduction of 21.1%.
2 Model
We define our model over two kinds of random
variables: dependency trees and parameters. For
each language ? in a set L, our model will generate
a collection t? of dependency trees ti?. We assume
that these dependency trees are generated by the
DMV model of Klein and Manning (2004), which
we write as ti? ? DMV(??). Here, ?? is a vector
of the various model parameters for language ?.
The prior is what couples the ?? parameter vectors
across languages; it is the focus of this work. We
first consider the likelihood model before moving
on to the prior.
2.1 Dependency Model with Valence
A dependency parse is a directed tree t over tokens
in a sentence s. Each edge of the tree specifies a
directed dependency from a head token to a de-
pendent, or argument token. The DMV is a gen-
erative model for trees t, which has been widely
used for dependency parse induction. The ob-
served data likelihood, used for parameter estima-
tion, is the marginal probability of generating the
observed sentences s, which are simply the leaves
of the trees t. Generation in the DMV model in-
volves two types of local conditional probabilities:
CONTINUE distributions that capture valence and
ATTACH distributions that capture argument selec-
tion.
First, the Bernoulli CONTINUE probability dis-
tributions P CONTINUE(c|h, dir, adj; ??) model the
fertility of a particular head type h. The outcome
c ? {stop, continue} is conditioned on the head
type h, direction dir, and adjacency adj. If a head
type?s continue probability is low, tokens of this
type will tend to generate few arguments.
Second, the ATTACH multinomial probability
distributions P ATTACH(a|h, dir; ??) capture attach-
ment preferences of heads, where a and h are both
token types. We take the same approach as pre-
vious work (Klein and Manning, 2004; Cohen and
Smith, 2009) and use gold part-of-speech labels as
tokens. Thus, the basic observed ?word? types are
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
Global
Indo-
European
Germanic
West
Germanic
North
Germanic
Ibero-
Romance
Italic Balto-Slavic
Slavic
Sino-
Tibetan
Sinitic
Figure 1: An example of a linguistically-plausible phylo-
genetic tree over the languages in our training data. Leaves
correspond to (observed) modern languages, while internal
nodes represent (unobserved) ancestral languages.
actually word classes.
2.1.1 Log-Linear Parameterization
The DMV?s local conditional distributions were
originally given as simple multinomial distribu-
tions with one parameter per outcome. However,
they can be re-parameterized to give the following
log-linear form (Eisner, 2002; Bouchard-Co?te? et
al., 2007; Berg-Kirkpatrick et al, 2010):
P CONTINUE(c|h, dir, adj; ??) =
exp
?
??T f CONTINUE(c, h, dir, adj)
?
P
c? exp
?
??T f CONTINUE(c?, h, dir, adj)
?
P ATTACH(a|h, dir; ??) =
exp
?
??T f ATTACH(a, h, dir)
?
P
a? exp
?
??T f ATTACH(a?, h, dir)
?
The parameters are weights ?? with one weight
vector per language. In the case where the vec-
tor of feature functions f has an indicator for each
possible conjunction of outcome and conditions,
the original multinomial distributions are recov-
ered. We refer to these full indicator features as
the set of SPECIFIC features.
2.2 Phylogenetic Prior
The focus of this work is coupling each of the pa-
rameters ?? in a phylogeny-structured prior. Con-
sider a phylogeny like the one shown in Fig-
ure 1, where each modern language ? in L is a
leaf. We would like to say that the leaves? pa-
rameter vectors arise from a process which slowly
1289
drifts along each branch. A convenient choice is
to posit additional parameter variables ??+ at in-
ternal nodes ?+ ? L+, a set of ancestral lan-
guages, and to assume that the conditional dis-
tribution P (??|?par(?)) at each branch in the phy-
logeny is a Gaussian centered on ?par(?), where
par(?) is the parent of ? in the phylogeny and
? ranges over L ? L+. The variance structure
of the Gaussian would then determine how much
drift (and in what directions) is expected. Con-
cretely, we assume that each drift distribution is
an isotropic Gaussian with mean ?par(?) and scalar
variance ?2. The root is centered at zero. We have
thus defined a joint distribution P (?|?2) where
? = (?? : ? ? L?L+). ?2 is a hyperparameter for
this prior which could itself be re-parameterized to
depend on branch length or be learned; we simply
set it to a plausible constant value.
Two primary challenges remain. First, infer-
ence under arbitrary priors can become complex.
However, in the simple case of our diagonal co-
variance Gaussians, the gradient of the observed
data likelihood can be computed directly using the
DMV?s expected counts and maximum-likelihood
estimation can be accomplished by applying stan-
dard gradient optimization methods. Second,
while the choice of diagonal covariance is effi-
cient, it causes components of ? that correspond
to features occurring in only one language to be
marginally independent of the parameters of all
other languages. In other words, only features
which fire in more than one language are coupled
by the prior. In the next section, we therefore in-
crease the overlap between languages? features by
using coarse projections of parts-of-speech.
2.3 Projected Features
With diagonal covariance in the Gaussian drift
terms, each parameter evolves independently of
the others. Therefore, our prior will be most
informative when features activate in multiple
languages. In phonology, it is useful to map
phonemes to the International Phonetic Alphabet
(IPA) in order to have a language-independent
parameterization. We introduce a similarly neu-
tral representation here by projecting language-
specific parts-of-speech to a coarse, shared inven-
tory.
Indeed, we assume that each language has a dis-
tinct tagset, and so the basic configurational fea-
tures will be language specific. For example, when
SPECIFIC: Activate for only one conjunction of out-
come and conditions:
1(c = ?, h = ?, dir = ?, adj = ?)
SHARED: Activate for heads from multiple languages
using cross-lingual POS projection pi(?):
1(c = ?, pi(h) = ?, dir = ?, adj = ?)
CONTINUE distribution feature templates.
SPECIFIC: Activate for only one conjunction of out-
come and conditions:
1(a = ?, h = ?, dir = ?)
SHARED: Activate for heads and arguments from
multiple languages using cross-lingual
POS projection pi(?):
1(pi(a) = ?, pi(h) = ?, dir = ?)
1(pi(a) = ?, h = ?, dir = ?)
1(a = ?, pi(h) = ?, dir = ?)
ATTACH distribution feature templates.
Table 1: Feature templates for CONTINUE and ATTACH con-
ditional distributions.
an English VBZ takes a left argument headed by a
NNS, a feature will activate specific to VBZ-NNS-
LEFT. That feature will be used in the log-linear
attachment probability for English. However, be-
cause that feature does not show up in any other
language, it is not usefully controlled by the prior.
Therefore, we also include coarser features which
activate on more abstract, cross-linguistic config-
urations. In the same example, a feature will fire
indicating a coarse, direction-free NOUN-VERB at-
tachment. This feature will now occur in multiple
languages and will contribute to each of those lan-
guages? attachment models. Although such cross-
lingual features will have different weight param-
eters in each language, those weights will covary,
being correlated by the prior.
The coarse features are defined via a projec-
tion ? from language-specific part-of-speech la-
bels to coarser, cross-lingual word classes, and
hence we refer to them as SHARED features. For
each corpus used in this paper, we use the tagging
annotation guidelines to manually define a fixed
mapping from the corpus tagset to the following
coarse tagset: noun, verb, adjective, adverb, con-
junction, preposition, determiner, interjection, nu-
meral, and pronoun. Parts-of-speech for which
this coarse mapping is ambiguous or impossible
are not mapped, and do not have corresponding
SHARED features.
We summarize the feature templates for the
CONTINUE and ATTACH conditional distributions
in Table 1. Variants of all feature templates that
ignore direction and/or adjacency are included. In
practice, we found it beneficial for all language-
1290
independent features to ignore direction.
Again, only the coarse features occur in mul-
tiple languages, so all phylogenetic influence is
through those. Nonetheless, the effect of the phy-
logeny turns out to be quite strong.
2.4 Learning
We now turn to learning with the phylogenetic
prior. Since the prior couples parameters across
languages, this learning problem requires param-
eters for all languages be estimated jointly. We
seek to find ? = (?? : ? ? L ? L+) which
optimizes log P (?|s), where s aggregates the ob-
served leaves of all the dependency trees in all the
languages. This can be written as
log P (?) + logP (s|?) ? log P (s)
The third term is a constant and can be ignored.
The first term can be written as
logP (?) =
?
??L?L+
1
2?2 ??? ? ?par(?)?
2
2 + C
where C is a constant. The form of logP (?) im-
mediately shows how parameters are penalized for
being different across languages, more so for lan-
guages that are near each other in the phylogeny.
The second term
log P (s|?) =
?
??L
log P (s?|??)
is a sum of observed data likelihoods under
the standard DMV models for each language,
computable by dynamic programming (Klein
and Manning, 2004). Together, this yields the
following objective function:
l(?) =
?
??L?L+
1
2?2 ??? ? ?par(?)?22 +
?
??L logP (s?|??)
which can be optimized using gradient methods
or (MAP) EM. Here we used L-BFGS (Liu et al,
1989). This requires computation of the gradient
of the observed data likelihood log P (s?|??)
which is given by:
? logP (s?|??) = Et?|s?
[
? log P (s?, t?|??)
]
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
c,h,dir,adj ec,h,dir,adj(s?; ??) ?
[
f CONTINUE(c, h, dir, adj) ?
?
c? P CONTINUE(c?|h, dir, adj; ??)f CONTINUE(c?, h, dir, adj)
]
?
a,h,dir ea,h,dir(s?; ??) ?
[
f ATTACH(a, h, dir) ?
?
a? P ATTACH(a?|h, dir; ??)f ATTACH(a?, h, dir)
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The expected gradient of the log joint likelihood
of sentences and parses is equal to the gradient of
the log marginal likelihood of just sentences, or
the observed data likelihood (Salakhutdinov et al,
2003). ea,h,dir(s?; ??) is the expected count of the
number of times head h is attached to a in direc-
tion dir given the observed sentences s? and DMV
parameters ??. ec,h,dir,adj(s?; ??) is defined simi-
larly. Note that these are the same expected counts
required to perform EM on the DMV, and are com-
putable by dynamic programming.
The computation time is dominated by the com-
putation of each sentence?s posterior expected
counts, which are independent given the parame-
ters, so the time required per iteration is essentially
the same whether training all languages jointly or
independently. In practice, the total number of it-
erations was also similar.
3 Experimental Setup
3.1 Data
We ran experiments with the following languages:
English, Dutch, Danish, Swedish, Spanish, Por-
tuguese, Slovene, and Chinese. For all languages
but English and Chinese, we used corpora from the
2006 CoNLL-X Shared Task dependency parsing
data set (Buchholz and Marsi, 2006). We used the
shared task training set to both train and test our
models. These corpora provide hand-labeled part-
of-speech tags (except for Dutch, which is auto-
matically tagged) and provide dependency parses,
which are either themselves hand-labeled or have
been converted from hand-labeled parses of other
kinds. For English and Chinese we use sections
2-21 of the Penn Treebank (PTB) (Marcus et al,
1993) and sections 1-270 of the Chinese Tree-
bank (CTB) (Xue et al, 2002) respectively. Sim-
ilarly, these sections were used for both training
and testing. The English and Chinese data sets
have hand-labeled constituency parses and part-of-
speech tags, but no dependency parses. We used
the Bikel Chinese head finder (Bikel and Chiang,
2000) and the Collins English head finder (Collins,
1999) to transform the gold constituency parses
into gold dependency parses. None of the corpora
are bitexts. For all languages, we ran experiments
on all sentences of length 10 or less after punctua-
tion has been removed.
When constructing phylogenies over the lan-
guages we made use of their linguistic classifica-
tions. English and Dutch are part of the West Ger-
1291
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
West
Germanic
North
Germanic
Ibero-
Romance Slavic Sinitic
Global
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
Global
(a)
(b)
(c)
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
West
Germanic
North
Germanic
Ibero-
Romance Slavic Sinitic
Figure 2: (a) Phylogeny for FAMILIES model. (b) Phylogeny
for GLOBAL model. (c) Phylogeny for LINGUISTIC model.
manic family of languages, whereas Danish and
Swedish are part of the North Germanic family.
Spanish and Portuguese are both part of the Ibero-
Romance family. Slovene is part of the Slavic
family. Finally, Chinese is in the Sinitic family,
and is not an Indo-European language like the oth-
ers. We interchangeably speak of a language fam-
ily and the ancestral node corresponding to that
family?s root language in a phylogeny.
3.2 Models Compared
We evaluated three phylogenetic priors, each with
a different phylogenetic structure. We compare
with two monolingual baselines, as well as an all-
pairs multilingual model that does not have a phy-
logenetic interpretation, but which provides very
similar capacity for parameter coupling.
3.2.1 Phylogenetic Models
The first phylogenetic model uses the shallow phy-
logeny shown in Figure 2(a), in which only lan-
guages within the same family have a shared par-
ent node. We refer to this structure as FAMILIES.
Under this prior, the learning task decouples into
independent subtasks for each family, but no reg-
ularities across families can be captured.
The family-level model misses the constraints
between distant languages. Figure 2(b) shows an-
other simple configuration, wherein all languages
share a common parent node in the prior, meaning
that global regularities that are consistent across
all languages can be captured. We refer to this
structure as GLOBAL.
While the global model couples the parameters
for all eight languages, it does so without sensi-
tivity to the articulated structure of their descent.
Figure 2(c) shows a more nuanced prior struc-
ture, LINGUISTIC, which groups languages first
by family and then under a global node. This
structure allows global regularities as well as reg-
ularities within families to be learned.
3.2.2 Parameterization and ALLPAIRS Model
Daume? III (2007) and Finkel and Manning (2009)
consider a formally similar Gaussian hierarchy for
domain adaptation. As pointed out in Finkel and
Manning (2009), there is a simple equivalence be-
tween hierarchical regularization as described here
and the addition of new tied features in a ?flat?
model with zero-meaned Gaussian regularization
on all parameters. In particular, instead of param-
eterizing the objective in Section 2.4 in terms of
multiple sets of weights, one at each node in the
phylogeny (the hierarchical parameterization, de-
scribed in Section 2.4), it is equivalent to param-
eterize this same objective in terms of a single set
of weights on a larger of group features (the flat
parameterization). This larger group of features
contains a duplicate set of the features discussed in
Section 2.3 for each node in the phylogeny, each
of which is active only on the languages that are its
descendants. A linear transformation between pa-
rameterizations gives equivalence. See Finkel and
Manning (2009) for details.
In the flat parameterization, it seems equally
reasonable to simply tie all pairs of languages by
adding duplicate sets of features for each pair.
This gives the ALLPAIRS setting, which we also
compare to the tree-structured phylogenetic mod-
els above.
3.3 Baselines
To evaluate the impact of multilingual constraint,
we compared against two monolingual baselines.
The first baseline is the standard DMV with
only SPECIFIC features, which yields the standard
multinomial DMV (weak baseline). To facilitate
comparison to past work, we used no prior for this
monolingual model. The second baseline is the
DMV with added SHARED features. This model
includes a simple isotropic Gaussian prior on pa-
1292
Monolingual Multilingual
Phylogenetic
Corpus Size Ba
se
lin
e
B
as
el
in
e
w
/S
H
A
R
ED
A
LL
PA
IR
S
FA
M
IL
IE
S
B
ES
TP
A
IR
G
LO
B
A
L
LI
N
G
U
IS
TI
C
West Germanic English 6008 47.1 51.3 48.5 51.3 51.3 (Ch) 51.2 62.3Dutch 6678 36.3 36.0 44.0 36.1 36.2 (Sw) 44.0 45.1
North Germanic Danish 1870 33.5 33.6 40.5 31.4 34.2 (Du) 39.6 41.6Swedish 3571 45.3 44.8 56.3 44.8 44.8 (Ch) 44.5 58.3
Ibero-Romance Spanish 712 28.0 40.5 58.7 63.4 63.8 (Da) 59.4 58.4Portuguese 2515 38.5 38.5 63.1 37.4 38.4 (Sw) 37.4 63.0
Slavic Slovene 627 38.5 39.7 49.0 ? 49.6 (En) 49.4 48.4
Sinitic Chinese 959 36.3 43.3 50.7 ? 49.7 (Sw) 50.1 49.6
Macro-Avg. Relative Error Reduction 17.1 5.6 8.5 9.9 21.1
Table 2: Directed dependency accuracy of monolingual and multilingual models, and relative error reduction over the monolin-
gual baseline with SHARED features macro-averaged over languages. Multilingual models outperformed monolingual models
in general, with larger gains from increasing numbers of languages. Additionally, more nuanced phylogenetic structures out-
performed cruder ones.
rameters. This second baseline is the more direct
comparison to the multilingual experiments here
(strong baseline).
3.4 Evaluation
For each setting, we evaluated the directed de-
pendency accuracy of the minimum Bayes risk
(MBR) dependency parses produced by our mod-
els under maximum (posterior) likelihood parame-
ter estimates. We computed accuracies separately
for each language in each condition. In addition,
for multilingual models, we computed the relative
error reduction over the strong monolingual base-
line, macro-averaged over languages.
3.5 Training
Our implementation used the flat parameteriza-
tion described in Section 3.2.2 for both the phy-
logenetic and ALLPAIRS models. We originally
did this in order to facilitate comparison with the
non-phylogenetic ALLPAIRS model, which has no
equivalent hierarchical parameterization. In prac-
tice, optimizing with the hierarchical parameteri-
zation also seemed to underperform.1
1We noticed that the weights of features shared across lan-
guages had larger magnitude early in the optimization proce-
dure when using the flat parameterization compared to us-
ing the hierarchical parameterization, perhaps indicating that
cross-lingual influences had a larger effect on learning in its
initial stages.
All models were trained by directly optimizing
the observed data likelihood using L-BFGS (Liu et
al., 1989). Berg-Kirkpatrick et al (2010) suggest
that directly optimizing the observed data likeli-
hood may offer improvements over the more stan-
dard expectation-maximization (EM) optimization
procedure for models such as the DMV, espe-
cially when the model is parameterized using fea-
tures. We stopped training after 200 iterations in
all cases. This fixed stopping criterion seemed to
be adequate in all experiments, but presumably
there is a potential gain to be had in fine tuning.
To initialize, we used the harmonic initializer pre-
sented in Klein and Manning (2004). This type of
initialization is deterministic, and thus we did not
perform random restarts.
We found that for all models ?2 = 0.2 gave rea-
sonable results, and we used this setting in all ex-
periments. For most models, we found that vary-
ing ?2 in a reasonable range did not substantially
affect accuracy. For some models, the directed ac-
curacy was less flat with respect to ?2. In these
less-stable cases, there seemed to be an interac-
tion between the variance and the choice between
head conventions. For example, for some settings
of ?2, but not others, the model would learn that
determiners head noun phrases. In particular, we
observed that even when direct accuracy did fluc-
tuate, undirected accuracy remained more stable.
1293
4 Results
Table 2 shows the overall results. In all cases,
methods which coupled the languages in some
way outperformed the independent baselines that
considered each language independently.
4.1 Bilingual Models
The weakest of the coupled models was FAMI-
LIES, which had an average relative error reduc-
tion of 5.6% over the strong baseline. In this case,
most of the average improvement came from a sin-
gle family: Spanish and Portuguese. The limited
improvement of the family-level prior compared
to other phylogenies suggests that there are impor-
tant multilingual interactions that do not happen
within families. Table 2 also reports the maximum
accuracy achieved for each language when it was
paired with another language (same family or oth-
erwise) and trained together with a single common
parent. These results appear in the column headed
by BESTPAIR, and show the best accuracy for the
language on that row over all possible pairings
with other languages. When pairs of languages
were trained together in isolation, the largest bene-
fit was seen for languages with small training cor-
pora, not necessarily languages with common an-
cestry. In our setup, Spanish, Slovene, and Chi-
nese have substantially smaller training corpora
than the rest of the languages considered. Other-
wise, the patterns are not particularly clear; com-
bined with subsequent results, it seems that pair-
wise constraint is fairly limited.
4.2 Multilingual Models
Models that coupled multiple languages per-
formed better in general than models that only
considered pairs of languages. The GLOBAL
model, which couples all languages, if crudely,
yielded an average relative error reduction of
9.9%. This improvement comes as the number
of languages able to exert mutual constraint in-
creases. For example, Dutch and Danish had large
improvements, over and above any improvements
these two languages gained when trained with a
single additional language. Beyond the simplistic
GLOBAL phylogeny, the more nuanced LINGUIS-
TIC model gave large improvements for English,
Swedish, and Portuguese. Indeed, the LINGUIS-
TIC model is the only model we evaluated that
gave improvements for all the languages we con-
sidered.
It is reasonable to worry that the improvements
from these multilingual models might be partially
due to having more total training data in the mul-
tilingual setting. However, we found that halv-
ing the amount of data used to train the English,
Dutch, and Swedish (the languages with the most
training data) monolingual models did not sub-
stantially affect their performance, suggesting that
for languages with several thousand sentences or
more, the increase in statistical support due to ad-
ditional monolingual data was not an important ef-
fect (the DMV is a relatively low-capacity model
in any case).
4.3 Comparison of Phylogenies
Recall the structures of the three phylogenies
presented in Figure 2. These phylogenies dif-
fer in the correlations they can represent. The
GLOBAL phylogeny captures only ?universals,?
while FAMILIES captures only correlations be-
tween languages that are known to be similar. The
LINGUISTIC model captures both of these effects
simultaneously by using a two layer hierarchy.
Notably, the improvement due to the LINGUISTIC
model is more than the sum of the improvements
due to the GLOBAL and FAMILIES models.
4.4 Phylogenetic vs. ALLPAIRS
The phylogeny is capable of allowing appropri-
ate influence to pass between languages at mul-
tiple levels. We compare these results to the
ALLPAIRS model in order to see whether limi-
tation to a tree structure is helpful. The ALL-
PAIRS model achieved an average relative error
reduction of 17.1%, certainly outperforming both
the simple phylogenetic models. However, the
rich phylogeny of the LINGUISTIC model, which
incorporates linguistic constraints, outperformed
the freer ALLPAIRS model. A large portion of
this improvement came from English, a language
for which the LINGUISTIC model greatly outper-
formed all other models evaluated. We found that
the improved English analyses produced by the
LINGUISTIC model were more consistent with this
model?s analyses of other languages. This consis-
tency was not present for the English analyses pro-
duced by other models. We explore consistency in
more detail in Section 5.
4.5 Comparison to Related Work
The likelihood models for both the strong mono-
lingual baseline and the various multilingual mod-
1294
els are the same, both expanding upon the standard
DMV by adding coarse SHARED features. These
coarse features, even in a monolingual setting, im-
proved performance slightly over the weak base-
line, perhaps by encouraging consistent treatment
of the different finer-grained variants of parts-
of-speech (Berg-Kirkpatrick et al, 2010).2 The
only difference between the multilingual systems
and the strong baseline is whether or not cross-
language influence is allowed through the prior.
While this progression of model structure is
similar to that explored in Cohen and Smith
(2009), Cohen and Smith saw their largest im-
provements from tying together parameters for the
varieties of coarse parts-of-speech monolinugally,
and then only moderate improvements from allow-
ing cross-linguistic influence on top of monolin-
gual sharing. When Cohen and Smith compared
their best shared logistic-normal bilingual mod-
els to monolingual counter-parts for the languages
they investigate (Chinese and English), they re-
ported a relative error reduction of 5.3%. In com-
parison, with the LINGUISTIC model, we saw a
much larger 16.9% relative error reduction over
our strong baseline for these languages. Evaluat-
ing our LINGUISTIC model on the same test sets
as (Cohen and Smith, 2009), sentences of length
10 or less in section 23 of PTB and sections 271-
300 of CTB, we achieved an accuracy of 56.6 for
Chinese and 60.3 for English. The best models
of Cohen and Smith (2009) achieved accuracies of
52.0 and 62.0 respectively on these same test sets.
Our results indicate that the majority of our
model?s power beyond that of the standard DMV
is derived from multilingual, and in particular,
more-than-bilingual, interaction. These are, to the
best of our knowledge, the first results of this kind
for grammar induction without bitext.
5 Analysis
By examining the proposed parses we found that
the LINGUISTIC and ALLPAIRS models produced
analyses that were more consistent across lan-
guages than those of the other models. We
also observed that the most common errors can
be summarized succinctly by looking at attach-
ment counts between coarse parts-of-speech. Fig-
ure 3 shows matrix representations of dependency
2Coarse features that only tie nouns and verbs are ex-
plored in Berg-Kirkpatrick et al (2010). We found that these
were very effective for English and Chinese, but gave worse
performance for other languages.
counts. The area of a square is proportional to the
number of order-collapsed dependencies where
the column label is the head and the row label is
the argument in the parses from each system. For
ease of comprehension, we use the cross-lingual
projections and only show counts for selected in-
teresting classes.
Comparing Figure 3(c), which shows depen-
dency counts proposed by the LINGUISTIC model,
to Figure 3(a), which shows the same for the
strong monolingual baseline, suggests that the
analyses proposed by the LINGUISTIC model are
more consistent across languages than are the
analyses proposed by the monolingual model. For
example, the monolingual learners are divided
as to whether determiners or nouns head noun
phrases. There is also confusion about which la-
bels head whole sentences. Dutch has the problem
that verbs modify pronouns more often than pro-
nouns modify verbs, and pronouns are predicted
to head sentences as often as verbs are. Span-
ish has some confusion about conjunctions, hy-
pothesizing that verbs often attach to conjunctions,
and conjunctions frequently head sentences. More
subtly, the monolingual analyses are inconsistent
in the way they head prepositional phrases. In
the monolingual Portuguese hypotheses, preposi-
tions modify nouns more often than nouns mod-
ify prepositions. In English, nouns modify prepo-
sitions, and prepositions modify verbs. Both the
Dutch and Spanish models are ambivalent about
the attachment of prepositions.
As has often been observed in other contexts
(Liang et al, 2008), promoting agreement can
improve accuracy in unsupervised learning. Not
only are the analyses proposed by the LINGUISTIC
model more consistent, they are also more in ac-
cordance with the gold analyses. Under the LIN-
GUISTIC model, Dutch now attaches pronouns to
verbs, and thus looks more like English, its sister
in the phylogenetic tree. The LINGUISTIC model
has also chosen consistent analyses for preposi-
tional phrases and noun phrases, calling preposi-
tions and nouns the heads of each, respectively.
The problem of conjunctions heading Spanish sen-
tences has also been corrected.
Figure 3(b) shows dependency counts for the
GLOBAL multilingual model. Unsurprisingly, the
analyses proposed under global constraint appear
somewhat more consistent than those proposed
under no multi-lingual constraint (now three lan-
1295
Figure 3: Dependency counts in proposed parses. Row label modifies column label. (a) Monolingual baseline with SHARED
features. (b) GLOBAL model. (c) LINGUISTIC model. (d) Dependency counts in hand-labeled parses. Analyses proposed by
monolingual baseline show significant inconsistencies across languages. Analyses proposed by LINGUISTIC model are more
consistent across languages than those proposed by either the monolingual baseline or the GLOBAL model.
guages agree that prepositional phrases are headed
by prepositions), but not as consistent as those pro-
posed by the LINGUISTIC model.
Finally, Figure 3(d) shows dependency counts
in the hand-labeled dependency parses. It appears
that even the very consistent LINGUISTIC parses
do not capture the non-determinism of preposi-
tional phrase attachment to both nouns and verbs.
6 Conclusion
Even without translated texts, multilingual con-
straints expressed in the form of a phylogenetic
prior on parameters can give substantial gains
in grammar induction accuracy over treating lan-
guages in isolation. Additionally, articulated phy-
logenies that are sensitive to evolutionary structure
can outperform not only limited flatter priors but
also unconstrained all-pairs interactions.
7 Acknowledgements
This project is funded in part by the NSF un-
der grant 0915265 and DARPA under grant
N10AP20007.
1296
References
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In North American Chapter of the
Association for Computational Linguistics.
D. M. Bikel and D. Chiang. 2000. Two statistical pars-
ing models applied to the Chinese treebank. In Sec-
ond Chinese Language Processing Workshop.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Grif-
fiths. 2007. A probabilistic approach to diachronic
phonology. In Empirical Methods in Natural Lan-
guage Processing.
S. Buchholz and E. Marsi. 2006. Computational Nat-
ural Language Learning-X shared task on multilin-
gual dependency parsing. In Conference on Compu-
tational Natural Language Learning.
D. Burkett and D. Klein. 2008. Two languages are
better than one (for syntactic parsing). In Empirical
Methods in Natural Language Processing.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In North American
Chapter of the Association for Computational Lin-
guistics.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. In Ph.D. thesis, University
of Pennsylvania, Philadelphia.
H. Daume? III. 2007. Frustratingly easy domain adap-
tation. In Association for Computational Linguis-
tics.
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Association for Compu-
tational Linguistics.
J. R. Finkel and C. D. Manning. 2009. Hierarchi-
cal bayesian domain adaptation. In North American
Chapter of the Association for Computational Lin-
guistics.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Association for Compu-
tational Linguistics.
J. Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Association for Computa-
tional Linguistics.
G. Kuzman, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Association for Computational Lin-
guistics/International Joint Conference on Natural
Language Processing.
P. Liang, D. Klein, and M. I. Jordan. 2008.
Agreement-based learning. In Advances in Neural
Information Processing Systems.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the
limited memory BFGS method for large scale opti-
mization. Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani.
2003. Optimization with EM and expectation-
conjugate-gradient. In International Conference on
Machine Learning.
D. A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Empirical Methods in Natural Lan-
guage Processing.
B. Snyder, T. Naseem, and R. Barzilay. 2009a. Unsu-
pervised multilingual grammar induction. In Asso-
ciation for Computational Linguistics/International
Joint Conference on Natural Language Processing.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzi-
lay. 2009b. Adding more languages improves un-
supervised multilingual part-of-speech tagging: A
Bayesian non-parametric approach. In North Amer-
ican Chapter of the Association for Computational
Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building
a large-scale annotated Chinese corpus. In Interna-
tional Conference on Computational Linguistics.
1297
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481?490,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Jointly Learning to Extract and Compress
Taylor Berg-Kirkpatrick Dan Gillick Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, dgillick, klein}@cs.berkeley.edu
Abstract
We learn a joint model of sentence extraction
and compression for multi-document summa-
rization. Our model scores candidate sum-
maries according to a combined linear model
whose features factor over (1) the n-gram
types in the summary and (2) the compres-
sions used. We train the model using a margin-
based objective whose loss captures end sum-
mary quality. Because of the exponentially
large set of candidate summaries, we use a
cutting-plane algorithm to incrementally de-
tect and add active constraints efficiently. In-
ference in our model can be cast as an ILP
and thereby solved in reasonable time; we also
present a fast approximation scheme which
achieves similar performance. Our jointly
extracted and compressed summaries outper-
form both unlearned baselines and our learned
extraction-only system on both ROUGE and
Pyramid, without a drop in judged linguis-
tic quality. We achieve the highest published
ROUGE results to date on the TAC 2008 data
set.
1 Introduction
Applications of machine learning to automatic sum-
marization have met with limited success, and, as a
result, many top-performing systems remain largely
ad-hoc. One reason learning may have provided lim-
ited gains is that typical models do not learn to opti-
mize end summary quality directly, but rather learn
intermediate quantities in isolation. For example,
many models learn to score each input sentence in-
dependently (Teufel and Moens, 1997; Shen et al,
2007; Schilder and Kondadadi, 2008), and then as-
semble extractive summaries from the top-ranked
sentences in a way not incorporated into the learn-
ing process. This extraction is often done in the
presence of a heuristic that limits redundancy. As
another example, Yih et al (2007) learn predictors
of individual words? appearance in the references,
but in isolation from the sentence selection proce-
dure. Exceptions are Li et al (2009) who take a
max-margin approach to learning sentence values
jointly, but still have ad hoc constraints to handle
redundancy. One main contribution of the current
paper is the direct optimization of summary quality
in a single model; we find that our learned systems
substantially outperform unlearned counterparts on
both automatic and manual metrics.
While pure extraction is certainly simple and does
guarantee some minimal readability, Lin (2003)
showed that sentence compression (Knight and
Marcu, 2001; McDonald, 2006; Clarke and Lap-
ata, 2008) has the potential to improve the resulting
summaries. However, attempts to incorporate com-
pression into a summarization system have largely
failed to realize large gains. For example, Zajic et
al (2006) use a pipeline approach, pre-processing
to yield additional candidates for extraction by ap-
plying heuristic sentence compressions, but their
system does not outperform state-of-the-art purely
extractive systems. Similarly, Gillick and Favre
(2009), though not learning weights, do a limited
form of compression jointly with extraction. They
report a marginal increase in the automatic word-
overlap metric ROUGE (Lin, 2004), but a decline in
manual Pyramid (Nenkova and Passonneau, 2004).
A second contribution of the current work is to
show a system for jointly learning to jointly com-
press and extract that exhibits gains in both ROUGE
and content metrics over purely extractive systems.
Both Martins and Smith (2009) and Woodsend and
Lapata (2010) build models that jointly extract and
compress, but learn scores for sentences (or phrases)
using independent classifiers. Daume? III (2006)
481
learns parameters for compression and extraction
jointly using an approximate training procedure, but
his results are not competitive with state-of-the-art
extractive systems, and he does not report improve-
ments on manual content or quality metrics.
In our approach, we define a linear model that
scores candidate summaries according to features
that factor over the n-gram types that appear in the
summary and the structural compressions used to
create the sentences in the summary. We train these
parameters jointly using a margin-based objective
whose loss captures end summary quality through
the ROUGE metric. Because of the exponentially
large set of candidate summaries, we use a cutting
plane algorithm to incrementally detect and add ac-
tive constraints efficiently. To make joint learning
possible we introduce a new, manually-annotated
data set of extracted, compressed sentences. Infer-
ence in our model can be cast as an integer linear
program (ILP) and solved in reasonable time using
a generic ILP solver; we also introduce a fast ap-
proximation scheme which achieves similar perfor-
mance. Our jointly extracted and compressed sum-
maries outperform both unlearned baselines and our
learned extraction-only system on both ROUGE and
Pyramid, without a drop in judged linguistic quality.
We achieve the highest published comparable results
(ROUGE) to date on our test set.
2 Joint Model
We focus on the task of multi-document summariza-
tion. The input is a collection of documents, each
consisting of multiple sentences. The output is a
summary of length no greater than Lmax. Let x be
the input document set, and let y be a representation
of a summary as a vector. For an extractive sum-
mary, y is as a vector of indicators y = (ys : s ? x),
one indicator ys for each sentence s in x. A sentence
s is present in the summary if and only if its indica-
tor ys = 1 (see Figure 1a). Let Y (x) be the set of
valid summaries of document set x with length no
greater than Lmax.
While past extractive methods have assigned
value to individual sentences and then explicitly rep-
resented the notion of redundancy (Carbonell and
Goldstein, 1998), recent methods show greater suc-
cess by using a simpler notion of coverage: bigrams
Figure 1: Diagram of (a) extractive and (b) joint extrac-
tive and compressive summarization models. Variables
ys indicate the presence of sentences in the summary.
Variables yn indicate the presence of parse tree nodes.
Note that there is intentionally a bigram missing from (a).
contribute content, and redundancy is implicitly en-
coded in the fact that redundant sentences cover
fewer bigrams (Nenkova and Vanderwende, 2005;
Gillick and Favre, 2009). This later approach is as-
sociated with the following objective function:
max
y?Y (x)
?
b?B(y)
vb (1)
Here, vb is the value of bigram b, andB(y) is the set
of bigrams present in the summary encoded by y.
Gillick and Favre (2009) produced a state-of-the-art
system1 by directly optimizing this objective. They
let the value vb of each bigram be given by the num-
ber of input documents the bigram appears in. Our
implementation of their system will serve as a base-
line, referred to as EXTRACTIVE BASELINE.
We extend objective 1 so that it assigns value not
just to the bigrams that appear in the summary, but
also to the choices made in the creation of the sum-
mary. In our complete model, which jointly extracts
and compresses sentences, we choose whether or not
to cut individual subtrees in the constituency parses
1See Text Analysis Conference results in 2008 and 2009.
482
of each sentence. This is in contrast to the extractive
case where choices are made on full sentences.
max
y?Y (x)
?
b?B(y)
vb +
?
c?C(y)
vc (2)
C(y) is the set of cut choices made in y, and vc
assigns value to each.
Next, we present details of our representation of
compressive summaries. Assume a constituency
parse ts for every sentence s. We represent a com-
pressive summary as a vector y = (yn : n ? ts, s ?
x) of indicators, one for each non-terminal node in
each parse tree of the sentences in the document set
x. A word is present in the output summary if and
only if its parent parse tree node n has yn = 1 (see
Figure 1b). In addition to the length constraint on
the members of Y (x), we require that each node
n may have yn = 1 only if its parent pi(n) has
ypi(n) = 1. This ensures that only subtrees may
be deleted. While we use constituency parses rather
than dependency parses, this model has similarities
with the vine-growth model of Daume? III (2006).
For the compressive model we define the set of
cut choices C(y) for a summary y to be the set of
edges in each parse that are broken in order to delete
a subtree (see Figure 1b). We require that each sub-
tree has a non-terminal node for a root, and say that
an edge (n, pi(n)) between a node and its parent is
broken if the parent has ypi(n) = 1 but the child has
yn = 0. Notice that breaking a single edge deletes
an entire subtree.
2.1 Parameterization
Before learning weights in Section 3, we parameter-
ize objectives 1 and 2 using features. This entails to
parameterizing each bigram score vb and each sub-
tree deletion score vc. For weights w ? Rd and
feature functions g(b, x) ? Rd and h(c, x) ? Rd we
let:
vb = w
Tg(b, x)
vc = wTh(c, x)
For example, g(b, x) might include a feature the
counts the number of documents in x that b appears
in, and h(c, x) might include a feature that indicates
whether the deleted subtree is an SBAR modifying
a noun.
This parameterization allows us to cast summa-
rization as structured prediction. We can define a
feature function f(y, x) ? Rd which factors over
summaries y through B(y) and C(y):
f(y, x) =
?
b?B(y)
g(b, x) +
?
c?C(y)
h(c, x)
Using this characterization of summaries as feature
vectors we can define a linear predictor for summa-
rization:
d(x;w) = argmax
y?Y (x)
wTf(y, x) (3)
= argmax
y?Y (x)
?
b?B(y)
vb +
?
c?C(y)
vc
The arg max in Equation 3 optimizes Objective 2.
Learning weights for Objective 1 where Y (x) is
the set of extractive summaries gives our LEARNED
EXTRACTIVE system. Learning weights for Objec-
tive 2 where Y (x) is the set of compressive sum-
maries, and C(y) the set of broken edges that pro-
duce subtree deletions, gives our LEARNED COM-
PRESSIVE system, which is our joint model of ex-
traction and compression.
3 Structured Learning
Discriminative training attempts to minimize the
loss incurred during prediction by optimizing an ob-
jective on the training set. We will perform discrim-
inative training using a loss function that directly
measures end-to-end summarization quality.
In Section 4 we show that finding summaries that
optimize Objective 2, Viterbi prediction, is efficient.
Online learning algorithms like perceptron or the
margin-infused relaxed algorithm (MIRA) (Cram-
mer and Singer, 2003) are frequently used for struc-
tured problems where Viterbi inference is available.
However, we find that such methods are unstable on
our problem. We instead turn to an approach that
optimizes a batch objective which is sensitive to all
constraints on all instances, but is efficient by adding
these constraints incrementally.
3.1 Max-margin objective
For our problem the data set consists of pairs of doc-
ument sets and label summaries, D = {(xi,y?i ) :
i ? 1, . . . , N}. Note that the label summaries
483
can be expressed as vectors y? because our training
summaries are variously extractive or extractive and
compressive (see Section 5). We use a soft-margin
support vector machine (SVM) (Vapnik, 1998) ob-
jective over the full structured output space (Taskar
et al, 2003; Tsochantaridis et al, 2004) of extractive
and compressive summaries:
min
w
1
2
?w?2 +
C
N
N?
i=1
?i (4)
s.t. ?i,?y ? Y (xi) (5)
wT
(
f(y?i , xi)? f(y, xi)
)
? `(y,y?i )? ?i
The constraints in Equation 5 require that the differ-
ence in model score between each possible summary
y and the gold summary y?i be no smaller than the
loss `(y,y?i ), padded by a per-instance slack of ?i.
We use bigram recall as our loss function (see Sec-
tion 3.3). C is the regularization constant. When the
output space Y (xi) is small these constraints can be
explicitly enumerated. In this case it is standard to
solve the dual, which is a quadratic program. Un-
fortunately, the size of the output space of extractive
summaries is exponential in the number of sentences
in the input document set.
3.2 Cutting-plane algorithm
The cutting-plane algorithm deals with the expo-
nential number of constraints in Equation 5 by per-
forming constraint induction (Tsochantaridis et al,
2004). It alternates between solving Objective 4
with a reduced set of currently active constraints,
and adding newly active constraints to the set. In
our application, this approach efficiently solves the
structured SVM training problem up to some speci-
fied tolerance .
Suppose w? and ?? optimize Objective 4 under the
currently active constraints on a given iteration. No-
tice that the y?i satisfying
y?i = argmax
y?Y (xi)
[
w?Tf(y, xi) + `(y,y?i )
]
(6)
corresponds to the constraint in the fully constrained
problem, for training instance (xi,y?i ), most vio-
lated by w? and ??. On each round of constraint induc-
tion the cutting-plane algorithm computes the arg
max in Equation 6 for a training instance, which is
referred to as loss-augmented prediction, and adds
the corresponding constraint to the active set.
The constraints from Equation (5) are equivalent
to: ?i wTf(y?i , xi) ? maxy?Y (xi)
[
wTf(y, xi) +
`(y,y?i )
]
? ?i. Thus, if loss-augmented prediction
turns up no new constraints on a given iteration, the
current solution to the reduced problem, w? and ??,
is the solution to the full SVM training problem. In
practice, constraints are only added if the right hand
side of Equation (5) exceeds the left hand side by at
least . Tsochantaridis et al (2004) prove that only
O(N ) constraints are added before constraint induc-
tion finds a C-optimal solution.
Loss-augmented prediction is not always
tractable. Luckily, our choice of loss function,
bigram recall, factors over bigrams. Thus, we can
easily perform loss-augmented prediction using
the same procedure we use to perform Viterbi
prediction (described in Section 4). We simply
modify each bigram value vb to include bigram
b?s contribution to the total loss. We solve the
intermediate partially-constrained max-margin
problems using the factored sequential minimal
optimization (SMO) algorithm (Platt, 1999; Taskar
et al, 2004). In practice, for  = 10?4, the
cutting-plane algorithm converges after only three
passes through the training set when applied to our
summarization task.
3.3 Loss function
In the simplest case, 0-1 loss, the system only re-
ceives credit for exactly identifying the label sum-
mary. Since there are many reasonable summaries
we are less interested in exactly matching any spe-
cific training instance, and more interested in the de-
gree to which a predicted summary deviates from a
label.
The standard method for automatically evaluating
a summary against a reference is ROUGE, which we
simplify slightly to bigram recall. With an extractive
reference denoted by y?, our loss function is:
`(y,y?) =
|B(y)
?
B(y?)|
|B(y?)|
We verified that bigram recall correlates well with
ROUGE and with manual metrics.
484
4 Efficient Prediction
We show how to perform prediction with the extrac-
tive and compressive models by solving ILPs. For
many instances, a generic ILP solver can find exact
solutions to the prediction problems in a matter of
seconds. For difficult instances, we present a fast
approximate algorithm.
4.1 ILP for extraction
Gillick and Favre (2009) express the optimization of
Objective 1 for extractive summarization as an ILP.
We begin here with their algorithm. Let each input
sentence s have length ls. Let the presence of each
bigram b inB(y) be indicated by the binary variable
zb. LetQsb be an indicator of the presence of bigram
b in sentence s. They specify the following ILP over
binary variables y and z:
max
y,z
?
b
vbzb
s.t.
?
s
lsys ? Lmax
?b
?
s
Qsb ? zb (7)
?s, b ysQsb ? zb (8)
Constraints 7 and 8 ensure consistency between sen-
tences and bigrams. Notice that the Constraint 7 re-
quires that selecting a sentence entails selecting all
its bigrams, and Constraint 8 requires that selecting
a bigram entails selecting at least one sentence that
contains it. Solving the ILP is fast in practice. Us-
ing the GNU Linear Programming Kit (GLPK) on
a 3.2GHz Intel machine, decoding took less than a
second on most instances.
4.2 ILP for joint compression and extraction
We can extend the ILP formulation of extraction
to solve the compressive problem. Let ln be the
number of words node n has as children. With
this notation we can write the length restriction as
?
n lnyn ? Lmax. Let the presence of each cut c in
C(y) be indicated by the binary variable zc, which
is active if and only if yn = 0 but ypi(n) = 1, where
node pi(n) is the parent of node n. The constraints
on zc are diagrammed in Figure 2.
While it is possible to let B(y) contain all bi-
grams present in the compressive summary, the re-
Figure 2: Diagram of ILP for joint extraction and com-
pression. Variables zb indicate the presence of bigrams
in the summary. Variables zc indicate edges in the parse
tree that have been cut in order to remove subtrees. The
figure suppresses bigram variables zstopped,in and zfrance,he
to reduce clutter. Note that the edit shown is intentionally
bad. It demonstrates a loss of bigram coverage.
duction of B(y) makes the ILP formulation effi-
cient. We omit fromB(y) bigrams that are the result
of deleted intermediate words. As a result the re-
quired number of variables zb is linear in the length
of a sentence. The constraints on zb are given in
Figure 2. They can be expressed in terms of the vari-
ables yn.
By solving the following ILP we can compute the
arg max required for prediction in the joint model:
max
y,z
?
b
vbzb +
?
c
vczc
s.t.
?
n
lnyn ? Lmax
?n yn ? ypi(n) (9)
?b zb = 1
[
b ? B(y)
]
(10)
?c zc = 1
[
c ? C(y)
]
(11)
485
Constraint 9 encodes the requirement that only full
subtrees may be deleted. For simplicity, we have
written Constraints 10 and 11 in implicit form.
These constraints can be encoded explicitly using
O(N) linear constraints, where N is the number
of words in the document set x. The reduction of
B(y) to include only bigrams not resulting from
deleted intermediate words avoids O(N2) required
constraints.
In practice, solving this ILP for joint extraction
and compression is, on average, an order of magni-
tude slower than solving the ILP for pure extraction,
and for certain instances finding the exact solution is
prohibitively slow.
4.3 Fast approximate prediction
One common way to quickly approximate an ILP
is to solve its LP relaxation (and round the results).
We found that, while very fast, the LP relaxation of
the joint ILP gave poor results, finding unacceptably
suboptimal solutions. This appears possibly to have
been problematic for Martins and Smith (2009) as
well. We developed an alternative fast approximate
joint extractive and compressive solver that gives
better results in terms of both objective value and
bigram recall of resulting solutions.
The approximate joint solver first extracts a sub-
set of the sentences in the document set that total no
more than M words. In a second step, we apply the
exact joint extractive and compressive summarizer
(see Section 4.2) to the resulting extraction. The ob-
jective we maximize in performing the initial extrac-
tion is different from the one used in extractive sum-
marization. Specifically, we pick an extraction that
maximizes
?
s?y
?
b?s vb. This objective rewards
redundant bigrams, and thus is likely to give the joint
solver multiple options for including the same piece
of relevant content.
M is a parameter that trades-off between approx-
imation quality and problem difficulty. When M
is the size of the document set x, the approximate
solver solves the exact joint problem. In Figure 3
we plot the trade-off between approximation quality
and computation time, comparing to the exact joint
solver, an exact solver that is limited to extractive
solutions, and the LP relaxation solver. The results
show that the approximate joint solver yields sub-
stantial improvements over the LP relaxation, and
Figure 3: Plot of objective value, bigram recall, and
elapsed time for the approximate joint extractive and
compressive solver against size of intermediate extraction
set. Also shown are values for an LP relaxation approx-
imate solver, a solver that is restricted to extractive so-
lutions, and finally the exact compressive solver. These
solvers do not use an intermediate extraction. Results are
for 44 document sets, averaging about 5000 words per
document set.
can achieve results comparable to those produced by
the exact solver with a 5-fold reduction in compu-
tation time. On particularly difficult instances the
parameter M can be decreased, ensuring that all in-
stances are solved in a reasonable time period.
5 Data
We use the data from the Text Analysis Conference
(TAC) evaluations from 2008 and 2009, a total of
92 multi-document summarization problems. Each
problem asks for a 100-word-limited summary of
10 related input documents and provides a set of
four abstracts written by experts. These are the non-
update portions of the TAC 2008 and 2009 tasks.
To train the extractive system described in Sec-
tion 2, we use as our labels y? the extractions with
the largest bigram recall values relative to the sets
of references. While these extractions are inferior
to the abstracts, they are attainable by our model, a
quality found to be advantageous in discriminative
training for machine translation (Liang et al, 2006;
486
COUNT: 1(docCount(b) = ?) where docCount(b) is the
number of documents containing b.
STOP: 1(isStop(b1) = ?, isStop(b2) = ?) where
isStop(w) indicates a stop word.
POSITION: 1(docPosition(b) = ?) where docPosition(b) is
the earliest position in a document of any sen-
tence containing b, buckets earliest positions? 4.
CONJ: All two- and three-way conjunctions of COUNT,
STOP, and POSITION features.
BIAS: Bias feature, active on all bigrams.
Table 1: Bigram features: component feature functions
in g(b, x) that we use to characterize the bigram b in both
the extractive and compressive models.
Chiang et al, 2008).
Previous work has referred to the lack of ex-
tracted, compressed data sets as an obstacle to joint
learning for summarizaiton (Daume? III, 2006; Mar-
tins and Smith, 2009). We collected joint data via
a Mechanical Turk task. To make the joint anno-
tation task more feasible, we adopted an approx-
imate approach that closely matches our fast ap-
proximate prediction procedure. Annotators were
shown a 150-word maximum bigram recall extrac-
tions from the full document set and instructed to
form a compressed summary by deleting words un-
til 100 or fewer words remained. Each task was per-
formed by two annotators. We chose the summary
we judged to be of highest quality from each pair
to add to our corpus. This gave one gold compres-
sive summary y? for each of the 44 problems in the
TAC 2009 set. We used these labels to train our joint
extractive and compressive system described in Sec-
tion 2. Of the 288 total sentences presented to anno-
tators, 38 were unedited, 45 were deleted, and 205
were compressed by an average of 7.5 words.
6 Features
Here we describe the features used to parameterize
our model. Relative to some NLP tasks, our fea-
ture sets are small: roughly two hundred features
on bigrams and thirteen features on subtree dele-
tions. This is because our data set is small; with
only 48 training documents we do not have the sta-
tistical support to learn weights for more features.
For larger training sets one could imagine lexical-
ized versions of the features we describe.
COORD: Indicates phrase involved in coordination. Four
versions of this feature: NP, VP, S, SBAR.
S-ADJUNCT: Indicates a child of an S, adjunct to and left of
the matrix verb. Four version of this feature:
CC, PP, ADVP, SBAR.
REL-C: Indicates a relative clause, SBAR modifying a
noun.
ATTR-C: Indicates a sentence-final attribution clause,
e.g. ?the senator announced Friday.?
ATTR-PP: Indicates a PP attribution, e.g. ?according to the
senator.?
TEMP-PP: Indicates a temporal PP, e.g. ?on Friday.?
TEMP-NP: Indicates a temporal NP, e.g. ?Friday.?
BIAS: Bias feature, active on all subtree deletions.
Table 2: Subtree deletion features: component feature
functions in h(c, x) that we use to characterize the sub-
tree deleted by cutting edge c = (n, pi(n)) in the joint
extractive and compressive model.
6.1 Bigram features
Our bigram features include document counts, the
earliest position in a document of a sentence that
contains the bigram, and membership of each word
in a standard set of stopwords. We also include all
possible two- and three-way conjuctions of these
features. Table 1 describes the features in detail.
We use stemmed bigrams and prune bigrams that
appear in fewer than three input documents.
6.2 Subtree deletion features
Table 2 gives a description of our subtree tree dele-
tion features. Of course, by training to optimize a
metric like ROUGE, the system benefits from re-
strictions on the syntactic variety of edits; the learn-
ing is therefore more about deciding when an edit
is worth the coverage trade-offs rather than fine-
grained decisions about grammaticality.
We constrain the model to only allow subtree
deletions where one of the features in Table 2 (aside
from BIAS) is active. The root, and thus the entire
sentence, may always be cut. We choose this par-
ticular set of allowed deletions by looking at human
annotated data and taking note of the most common
types of edits. Edits which are made rarely by hu-
mans should be avoided in most scenarios, and we
simply don?t have enough data to learn when to do
them safely.
487
System BR R-2 R-SU4 Pyr LQ
LAST DOCUMENT 4.00 5.85 9.39 23.5 7.2
EXT. BASELINE 6.85 10.05 13.00 35.0 6.2
LEARNED EXT. 7.43 11.05 13.86 38.4 6.6
LEARNED COMP. 7.75 11.70 14.38 41.3 6.5
Table 3: Bigram Recall (BR), ROUGE (R-2 and R-SU4)
and Pyramid (Pyr) scores are multiplied by 100; Linguis-
tic Quality (LQ) is scored on a 1 (very poor) to 10 (very
good) scale.
7 Experiments
7.1 Experimental setup
We set aside the TAC 2008 data set (48 problems)
for testing and use the TAC 2009 data set (44 prob-
lems) for training, with hyper-parameters set to max-
imize six-fold cross-validation bigram recall on the
training set. We run the factored SMO algorithm
until convergence, and run the cutting-plane algo-
rithm until convergence for  = 10?4. We used
GLPK to solve all ILPs. We solved extractive ILPs
exactly, and joint extractive and compressive ILPs
approximately using an intermediate extraction size
of 1000. Constituency parses were produced using
the Berkeley parser (Petrov and Klein, 2007). We
show results for three systems, EXTRACTIVE BASE-
LINE, LEARNED EXTRACTIVE, LEARNED COM-
PRESSIVE, and the standard baseline that extracts
the first 100 words in the the most recent document,
LAST DOCUMENT.
7.2 Results
Our evaluation results are shown in Table 3.
ROUGE-2 (based on bigrams) and ROUGE-SU4
(based on both unigrams and skip-bigrams, sepa-
rated by up to four words) are given by the offi-
cial ROUGE toolkit with the standard options (Lin,
2004).
Pyramid (Nenkova and Passonneau, 2004) is a
manually evaluated measure of recall on facts or
Semantic Content Units appearing in the reference
summaries. It is designed to help annotators dis-
tinguish information content from linguistic qual-
ity. Two annotators performed the entire evaluation
without overlap by splitting the set of problems in
half.
To evaluate linguistic quality, we sent all the sum-
maries to Mechanical Turk (with two times redun-
System Sents Words/Sent Word Types
LAST DOCUMENT 4.0 25.0 36.5
EXT. BASELINE 5.0 20.8 36.3
LEARNED EXT. 4.8 21.8 37.1
LEARNED COMP. 4.5 22.9 38.8
Table 4: Summary statistics for the summaries gener-
ated by each system: Average number of sentences per
summary, average number of words per summary sen-
tence, and average number of non-stopword word types
per summary.
dancy), using the template and instructions designed
by Gillick and Liu (2010). They report that Turk-
ers can faithfully reproduce experts? rankings of av-
erage system linguistic quality (though their judge-
ments of content are poorer). The table shows aver-
age linguistic quality.
All the content-based metrics show substantial
improvement for learned systems over unlearned
ones, and we see an extremely large improvement
for the learned joint extractive and compressive sys-
tem over the previous state-of-the-art EXTRACTIVE
BASELINE. The ROUGE scores for the learned
joint system, LEARNED COMPRESSIVE, are, to our
knowledge, the highest reported on this task. We
cannot compare Pyramid scores to other reported
scores because of annotator difference. As expected,
the LAST DOCUMENT baseline outperforms other
systems in terms of linguistic quality. But, impor-
tantly, the gains achieved by the joint extractive and
compressive system in content-based metrics do not
come at the cost of linguistic quality when compared
to purely extractive systems.
Table 4 shows statistics on the outputs of the sys-
tems we evaluated. The joint extractive and com-
pressive system fits more word types into a sum-
mary than the extractive systems, but also produces
longer sentences on average. Reading the output
summaries more carefully suggests that by learning
to extract and compress jointly, our joint system has
the flexibility to use or create reasonable, medium-
length sentences, whereas the extractive systems are
stuck with a few valuable long sentences, but several
less productive shorter sentences. Example sum-
maries produced by the joint system are given in Fig-
ure 4 along with reference summaries produced by
humans.
488
LEARNED COMPRESSIVE: The country?s work safety authority will
release the list of the first batch of coal mines to be closed down said
Wang Xianzheng, deputy director of the National Bureau of Produc-
tion Safety Supervision and Administration. With its coal mining
safety a hot issue, attracting wide attention from both home and over-
seas, China is seeking solutions from the world to improve its coal
mining safety system. Despite government promises to stem the car-
nage the death toll in China?s disaster-plagued coal mine industry is
rising according to the latest statistics released by the government Fri-
day. Fatal coal mine accidents in China rose 8.5 percent in the first
eight months of this year with thousands dying despite stepped-up ef-
forts to make the industry safer state media said Wednesday.
REFERENCE: China?s accident-plagued coal mines cause thousands
of deaths and injuries annually. 2004 saw over 6,000 mine deaths.
January through August 2005, deaths rose 8.5% over the same period
in 2004. Most accidents are gas explosions, but fires, floods, and cave-
ins also occur. Ignored safety procedures, outdated equipment, and
corrupted officials exacerbate the problem. Official responses include
shutting down thousands of ill-managed and illegally-run mines, pun-
ishing errant owners, issuing new safety regulations and measures,
and outlawing local officials from investing in mines. China also
sought solutions at the Conference on South African Coal Mining
Safety Technology and Equipment held in Beijing.
LEARNED COMPRESSIVE: Karl Rove the White House deputy chief
of staff told President George W. Bush and others that he never en-
gaged in an effort to disclose a CIA operative?s identity to discredit
her husband?s criticism of the administration?s Iraq policy according
to people with knowledge of Rove?s account in the investigation. In a
potentially damaging sign for the Bush administration special counsel
Patrick Fitzgerald said that although his investigation is nearly com-
plete it?s not over. Lewis Scooter Libby Vice President Dick Cheney?s
chief of staff and a key architect of the Iraq war was indicted Friday on
felony charges of perjury making false statements to FBI agents and
obstruction of justice for impeding the federal grand jury investigating
the CIA leak case.
REFERENCE: Special Prosecutor Patrick Fitzgerald is investigating
who leaked to the press that Valerie Plame, wife of former Ambas-
sador Joseph Wilson, was an undercover CIA agent. Wilson was a
critic of the Bush administration. Administration staffers Karl Rove
and I. Lewis Libby are the focus of the investigation. NY Times cor-
respondent Judith Miller was jailed for 85 days for refusing to testify
about Libby. Libby was eventually indicted on five counts: 2 false
statements, 1 obstruction of justice, 2 perjury. Libby resigned imme-
diately. He faces 30 years in prison and a fine of $1.25 million if
convicted. Libby pleaded not guilty.
Figure 4: Example summaries produced by our learned
joint model of extraction and compression. These are
each 100-word-limited summaries of a collection of ten
documents from the TAC 2008 data set. Constituents that
have been removed via subtree deletion are grayed out.
References summaries produced by humans are provided
for comparison.
8 Conclusion
Jointly learning to extract and compress within a
unified model outperforms learning pure extraction,
which in turn outperforms a state-of-the-art extrac-
tive baseline. Our system gives substantial increases
in both automatic and manual content metrics, while
maintaining high linguistic quality scores.
Acknowledgements
We thank the anonymous reviewers for their com-
ments. This project is supported by DARPA under
grant N10AP20007.
References
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of SIGIR.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
J. Clarke and M. Lapata. 2008. Global Inference for Sen-
tence Compression: An Integer Linear Programming
Approach. Journal of Artificial Intelligence Research,
31:399?429.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991.
H.C. Daume? III. 2006. Practical structured learning
techniques for natural language processing. Ph.D.
thesis, University of Southern California.
D. Gillick and B. Favre. 2009. A scalable global model
for summarization. In Proc. of ACL Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing.
D. Gillick and Y. Liu. 2010. Non-Expert Evaluation of
Summarization Systems is Risky. In Proc. of NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
K. Knight and D. Marcu. 2001. Statistics-based
summarization-step one: Sentence compression. In
Proc. of AAAI.
L. Li, K. Zhou, G.R. Xue, H. Zha, and Y. Yu. 2009.
Enhancing diversity, coverage and balance for summa-
rization through structure learning. In Proc. of the 18th
International Conference on World Wide Web.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of the ACL.
489
C.Y. Lin. 2003. Improving summarization performance
by sentence compression: a pilot study. In Proc. of
ACL Workshop on Information Retrieval with Asian
Languages.
C.Y. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. In Proc. of ACL Workshop on Text
Summarization Branches Out.
A.F.T. Martins and N.A. Smith. 2009. Summarization
with a joint model for sentence extraction and com-
pression. In Proc. of NAACL Workshop on Integer Lin-
ear Programming for Natural Language Processing.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. of EACL.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: The pyramid method.
In Proc. of NAACL.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical report, MSR-
TR-2005-101. Redmond, Washington: Microsoft Re-
search.
S. Petrov and D. Klein. 2007. Learning and inference for
hierarchically split PCFGs. In AAAI.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
Kernel Methods. MIT press.
F. Schilder and R. Kondadadi. 2008. Fastsum: Fast and
accurate query-based multi-document summarization.
In Proc. of ACL.
D. Shen, J.T. Sun, H. Li, Q. Yang, and Z. Chen. 2007.
Document summarization using conditional random
fields. In Proc. of IJCAI.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. of NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
S. Teufel and M. Moens. 1997. Sentence extraction as
a classification task. In Proc. of ACL Workshop on
Intelligent and Scalable Text Summarization.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proc. of ICML.
V.N. Vapnik. 1998. Statistical learning theory. John
Wiley and Sons, New York.
K. Woodsend and M. Lapata. 2010. Automatic genera-
tion of story highlights. In Proc. of ACL.
W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximizing
informative content-words. In Proc. of IJCAI.
D.M. Zajic, B.J. Dorr, R. Schwartz, and J. Lin. 2006.
Sentence compression as a component of a multi-
document summarization system. In Proc. of the 2006
Document Understanding Workshop.
490
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207?217,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised Transcription of Historical Documents
Taylor Berg-Kirkpatrick Greg Durrett Dan Klein
Computer Science Division
University of California at Berkeley
{tberg,gdurrett,klein}@cs.berkeley.edu
Abstract
We present a generative probabilistic
model, inspired by historical printing pro-
cesses, for transcribing images of docu-
ments from the printing press era. By
jointly modeling the text of the docu-
ment and the noisy (but regular) process
of rendering glyphs, our unsupervised sys-
tem is able to decipher font structure and
more accurately transcribe images into
text. Overall, our system substantially out-
performs state-of-the-art solutions for this
task, achieving a 31% relative reduction
in word error rate over the leading com-
mercial system for historical transcription,
and a 47% relative reduction over Tesser-
act, Google?s open source OCR system.
1 Introduction
Standard techniques for transcribing modern doc-
uments do not work well on historical ones. For
example, even state-of-the-art OCR systems pro-
duce word error rates of over 50% on the docu-
ments shown in Figure 1. Unsurprisingly, such er-
ror rates are too high for many research projects
(Arlitsch and Herbert, 2004; Shoemaker, 2005;
Holley, 2010). We present a new, generative
model specialized to transcribing printing-press
era documents. Our model is inspired by the un-
derlying printing processes and is designed to cap-
ture the primary sources of variation and noise.
One key challenge is that the fonts used in his-
torical documents are not standard (Shoemaker,
2005). For example, consider Figure 1a. The fonts
are not irregular like handwriting ? each occur-
rence of a given character type, e.g. a, will use the
same underlying glyph. However, the exact glyphs
are unknown. Some differences between fonts are
minor, reflecting small variations in font design.
Others are more severe, like the presence of the
archaic long s character before 1804. To address
the general problem of unknown fonts, our model
(a)
(b)
(c)
Figure 1: Portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.
learns the font in an unsupervised fashion. Font
shape and character segmentation are tightly cou-
pled, and so they are modeled jointly.
A second challenge with historical data is that
the early typesetting process was noisy. Hand-
carved blocks were somewhat uneven and often
failed to sit evenly on the mechanical baseline.
Figure 1b shows an example of the text?s baseline
moving up and down, with varying gaps between
characters. To deal with these phenomena, our
model incorporates random variables that specifi-
cally describe variations in vertical offset and hor-
izontal spacing.
A third challenge is that the actual inking was
also noisy. For example, in Figure 1c some charac-
ters are thick from over-inking while others are ob-
scured by ink bleeds. To be robust to such render-
ing irregularities, our model captures both inking
levels and pixel-level noise. Because the model
is generative, we can also treat areas that are ob-
scured by larger ink blotches as unobserved, and
let the model predict the obscured text based on
visual and linguistic context.
Our system, which we call Ocular, operates by
fitting the model to each document in an unsuper-
vised fashion. The system outperforms state-of-
the-art baselines, giving a 47% relative error re-
duction over Google?s open source Tesseract sys-
tem, and giving a 31% relative error reduction over
ABBYY?s commercial FineReader system, which
has been used in large-scale historical transcrip-
tion projects (Holley, 2010).
207
Over-inked
It appeared that the Prisoner was veryE :
X :
Wandering baseline Historical font
Figure 2: An example image from a historical document (X)
and its transcription (E).
2 Related Work
Relatively little prior work has built models specif-
ically for transcribing historical documents. Some
of the challenges involved have been addressed
(Ho and Nagy, 2000; Huang et al, 2006; Kae and
Learned-Miller, 2009), but not in a way targeted
to documents from the printing press era. For ex-
ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (Ho
and Nagy, 2000; Huang et al, 2006), which is not
feasible for noisy historical documents. Kae and
Learned-Miller (2009) jointly learn the font and
image segmentation but do not outperform mod-
ern baselines.
Work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (Vamvakas et al, 2008; Kluzner et al,
2009; Kae et al, 2010; Kluzner et al, 2011).
The most comparable work is that of Kopec and
Lomelin (1996) and Kopec et al (2001). They
integrated typesetting models with language mod-
els, but did not model noise. In the NLP com-
munity, generative models have been developed
specifically for correcting outputs of OCR systems
(Kolak et al, 2003), but these do not deal directly
with images.
A closely related area of work is automatic de-
cipherment (Ravi and Knight, 2008; Snyder et al,
2010; Ravi and Knight, 2011; Berg-Kirkpatrick
and Klein, 2011). The fundamental problem is
similar to our own: we are presented with a se-
quence of symbols, and we need to learn a corre-
spondence between symbols and letters. Our ap-
proach is also similar in that we use a strong lan-
guage model (in conjunction with the constraint
that the correspondence be regular) to learn the
correct mapping. However, the symbols are not
noisy in decipherment problems and in our prob-
lem we face a grid of pixels for which the segmen-
tation into symbols is unknown. In contrast, deci-
pherment typically deals only with discrete sym-
bols.
3 Model
Most historical documents have unknown fonts,
noisy typesetting layouts, and inconsistent ink lev-
els, usually simultaneously. For example, the por-
tion of the document shown in Figure 2 has all
three of these problems. Our model must handle
them jointly.
We take a generative modeling approach in-
spired by the overall structure of the historical
printing process. Our model generates images of
documents line by line; we present the generative
process for the image of a single line. Our pri-
mary random variables are E (the text) andX (the
pixels in an image of the line). Additionally, we
have a random variable T that specifies the layout
of the bounding boxes of the glyphs in the image,
and a random variable R that specifies aspects of
the inking and rendering process. The joint distri-
bution is:
P (E, T,R,X) =
P (E) [Language model]
? P (T |E) [Typesetting model]
? P (R) [Inking model]
? P (X|E, T,R) [Noise model]
We let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
For example, E represents the entire sequence of
text, while ei represents ith character in the se-
quence.
3.1 Language Model P (E)
Our language model, P (E), is a Kneser-Ney
smoothed character n-gram model (Kneser and
Ney, 1995). We generate printed lines of text
(rather than sentences) independently, without
generating an explicit stop character. This means
that, formally, the model must separately generate
the character length of each line. We choose not to
bias the model towards longer or shorter character
sequences and let the line length m be drawn uni-
formly at random from the positive integers less
than some large constant M.1 When i < 1, let ei
denote a line-initial null character. We can now
write:
P (E) = P (m) ?
m?
i=1
P (ei|ei?1, . . . , ei?n)
1In particular, we do not use the kind of ?word bonus?
common to statistical machine translation models.
208
ei 1 ei+1ei
li gi ri
XRPADiX
LPAD
i X
GLYPH
i
P ( ? | th)P ( ? | th)
a b c . . . z
Offset: ?VERT
LM params
cb
b
1 30
15 1 5
a
Glyph weights:  c
Bounding box probs:
Left padwidth: ?LPADc
Right padwidth: ?RPADc
Glyph width: ?GLYPHc
Font params
a a a
a a a
P ( ? | pe)
Inking: ?INK
Inking params
Figure 3: Character tokens ei are generated by the language model. For each token index i, a glyph bounding box width gi,
left padding width li, and a right padding width ri, are generated. Finally, the pixels in each glyph bounding box XGLYPHi aregenerated conditioned on the corresponding character, while the pixels in left and right padding bounding boxes, XLPADi and
XRPADi , are generated from a background distribution.
3.2 Typesetting Model P (T |E)
Generally speaking, the process of typesetting
produces a line of text by first tiling bounding
boxes of various widths and then filling in the
boxes with glyphs. Our generative model, which
is depicted in Figure 3, reflects this process. As
a first step, our model generates the dimensions
of character bounding boxes; for each character
token index i we generate three bounding box
widths: a glyph box width gi, a left padding box
width li, and a right padding box width ri, as
shown in Figure 3. We let the pixel height of all
lines be fixed to h. Let Ti = (li, gi, ri) so that Ti
specifies the dimensions of the character box for
token index i; T is then the concatenation of all
Ti, denoting the full layout.
Because the width of a glyph depends on its
shape, and because of effects resulting from kern-
ing and the use of ligatures, the components of
each Ti are drawn conditioned on the character
token ei. This means that, as part of our param-
eterization of the font, for each character type c
we have vectors of multinomial parameters ?LPADc ,
?GLYPHc , and ?RPADc governing the distribution of the
dimensions of character boxes of type c. These
parameters are depicted on the right-hand side of
Figure 3. We can now express the typesetting lay-
out portion of the model as:
P (T |E) =
m?
i=1
P (Ti|ei)
=
m?
i=1
[
P (li; ?LPADei ) ? P (gi; ?
GLYPH
ei ) ? P (ri; ?
RPAD
ei )
]
Each character type c in our font has another set
of parameters, a matrix ?c. These are weights that
specify the shape of the character type?s glyph,
and are depicted in Figure 3 as part of the font pa-
rameters. ?c will come into play when we begin
generating pixels in Section 3.3.
3.2.1 Inking Model P (R)
Before we start filling the character boxes with
pixels, we need to specify some properties of
the inking and rendering process, including the
amount of ink used and vertical variation along
the text baseline. Our model does this by gener-
ating, for each character token index i, a discrete
value di that specifies the overall inking level in
the character?s bounding box, and a discrete value
vi that specifies the glyph?s vertical offset. These
variations in the inking and typesetting process are
mostly independent of character type. Thus, in
209
our model, their distributions are not character-
specific. There is one global set of multinomial
parameters governing inking level (?INK), and an-
other governing offset (?VERT); both are depicted
on the left-hand side of Figure 3. LetRi = (di, vi)
and let R be the concatenation of all Ri so that we
can express the inking model as:
P (R) =
m?
i=1
P (Ri)
=
m?
i=1
[
P (di; ?INK) ? P (vi; ?VERT)
]
The di and vi variables are suppressed in Figure 3
to reduce clutter but are expressed in Figure 4,
which depicts the process of rendering a glyph
box.
3.3 Noise Model P (X|E, T,R)
Now that we have generated a typesetting layout
T and an inking context R, we have to actually
generate each of the pixels in each of the charac-
ter boxes, left padding boxes, and right padding
boxes; the matrices that these groups of pixels
comprise are denoted XGLYPHi , XLPADi , and XRPADi ,
respectively, and are depicted at the bottom of Fig-
ure 3.
We assume that pixels are binary valued and
sample their values independently from Bernoulli
distributions.2 The probability of black (the
Bernoulli parameter) depends on the type of pixel
generated. All the pixels in a padding box have
the same probability of black that depends only on
the inking level of the box, di. Since we have al-
ready generated this value and the widths li and ri
of each padding box, we have enough information
to generate left and right padding pixel matrices
XLPADi and XRPADi .
The Bernoulli parameter of a pixel inside a
glyph bounding box depends on the pixel?s loca-
tion inside the box (as well as on di and vi, but
for simplicity of exposition, we temporarily sup-
press this dependence) and on the model param-
eters governing glyph shape (for each character
type c, the parameter matrix ?c specifies the shape
of the character?s glyph.) The process by which
glyph pixels are generated is depicted in Figure 4.
The dependence of glyph pixels on location
complicates generation of the glyph pixel matrix
XGLYPHi since the corresponding parameter matrix
2We could generate real-valued pixels with a different
choice of noise distribution.
}
}
}
}
}
aa a
a a a
a a
a
}
Interpolate, apply logistic
Sample pixels
Choosewidth
Chooseoffset
Glyph weights
gi
di
vi
 ei
?PIXEL(j, k, gi, di, vi; ei)
?XGLYPHi
?
jk ? Bernoulli
Bernoulli parameters
Pixel values
Chooseinking
Figure 4: We generate the pixels for the character token ei
by first sampling a glyph width gi, an inking level di, and
a vertical offset vi. Then we interpolate the glyph weights
?ei and apply the logistic function to produce a matrix ofBernoulli parameters of width gi, inking di, and offset vi.
?PIXEL(j, k, gi, di, vi;?ei) is the Bernoulli parameter at row jand column k. Finally, we sample from each Bernoulli distri-
bution to generate a matrix of pixel values, XGLYPHi .
?ei has some type-level width w which may dif-
fer from the current token-level width gi. Intro-
ducing distinct parameters for each possible width
would yield a model that can learn completely dif-
ferent glyph shapes for slightly different widths of
the same character. We, instead, need a parame-
terization that ties the shapes for different widths
together, and at the same time allows mobility in
the parameter space during learning.
Our solution is to horizontally interpolate the
weights of the shape parameter matrix ?ei down
to a smaller set of columns matching the token-
level choice of glyph width gi. Thus, the type-
level matrix ?ei specifies the canonical shape of
the glyph for character ei when it takes its max-
imum width w. After interpolating, we apply
the logistic function to produce the individual
Bernoulli parameters. If we let [XGLYPHi ]jk denote
the value of the pixel at the jth row and kth col-
umn of the glyph pixel matrix XGLYPHi for token i,
and let ?PIXEL(j, k, gi;?ei) denote the token-level
210
?PIXEL :
Interpolate, apply logistic
 c :
Glyph weights
Bernoulli params
?
Figure 5: In order to produce Bernoulli parameter matrices
?PIXEL of variable width, we interpolate over columns of ?c
with vectors ?, and apply the logistic function to each result.
Bernoulli parameter for this pixel, we can write:
[XGLYPHi ]jk ? Bernoulli
(
?PIXEL(j, k, gi;?ei)
)
The interpolation process for a single row is de-
picted in Figure 5. We define a constant interpola-
tion vector ?(gi, k) that is specific to the glyph box
width gi and glyph box column k. Each ?(gi, k)
is shaped according to a Gaussian centered at the
relative column position in ?ei . The glyph pixel
Bernoulli parameters are defined as follows:
?PIXEL(j, k,gi;?ei) =
logistic
( w?
k?=1
[
?(gi, k)k? ? [?ei ]jk?
])
The fact that the parameterization is log-linear will
ensure that, during the unsupervised learning pro-
cess, updating the shape parameters ?c is simple
and feasible.
By varying the magnitude of ? we can change
the level of smoothing in the logistic model and
cause it to permit areas that are over-inked. This is
the effect that di controls. By offsetting the rows
of ?c that we interpolate weights from, we change
the vertical offset of the glyph, which is controlled
by vi. The full pixel generation process is dia-
grammed in Figure 4, where the dependence of
?PIXEL on di and vi is also represented.
4 Learning
We use the EM algorithm (Dempster et al, 1977)
to find the maximum-likelihood font parameters:
?c, ?LPADc , ?GLYPHc , and ?RPADc . The image X is the
only observed random variable in our model. The
identities of the characters E the typesetting lay-
out T and the inking R will all be unobserved. We
do not learn ?INK and ?VERT, which are set to the
uniform distribution.
4.1 Expectation Maximization
During the E-step we compute expected counts
for E and T , but maximize over R, for which
we compute hard counts. Our model is an in-
stance of a hidden semi-Markov model (HSMM),
and therefore the computation of marginals is
tractable with the semi-Markov forward-backward
algorithm (Levinson, 1986).
During the M-step, we update the parame-
ters ?LPADc , ?RPADc using the standard closed-form
multinomial updates and use a specialized closed-
form update for ?GLYPHc that enforces unimodal-
ity of the glyph width distribution.3 The glyph
weights, ?c, do not have a closed-form update.
The noise model that ?c parameterizes is a lo-
cal log-linear model, so we follow the approach
of Berg-Kirkpatrick et al (2010) and use L-BFGS
(Liu and Nocedal, 1989) to optimize the expected
likelihood with respect to ?c.
4.2 Coarse-to-Fine Learning and Inference
The number of states in the dynamic programming
lattice grows exponentially with the order of the
language model (Jelinek, 1998; Koehn, 2004). As
a result, inference can become slow when the lan-
guage model order n is large. To remedy this, we
take a coarse-to-fine approach to both learning and
inference. On each iteration of EM, we perform
two passes: a coarse pass using a low-order lan-
guage model, and a fine pass using a high-order
language model (Petrov et al, 2008; Zhang and
Gildea, 2008). We use the marginals4 from the
coarse pass to prune states from the dynamic pro-
gram of the fine pass.
In the early iterations of EM, our font parame-
ters are still inaccurate, and to prune heavily based
on such parameters would rule out correct anal-
yses. Therefore, we gradually increase the ag-
gressiveness of pruning over the course of EM. To
ensure that each iteration takes approximately the
same amount of computation, we also gradually
increase the order of the fine pass, only reaching
the full order n on the last iteration. To produce a
decoding of the image into text, on the final iter-
ation we run a Viterbi pass using the pruned fine
model.
3We compute the weighted mean and weighted variance
of the glyph width expected counts. We set ?GLYPHc to be pro-
portional to a discretized Gaussian with the computed mean
and variance. This update is approximate in the sense that it
does not necessarily find the unimodal multinomial that max-
imizes expected log-likelihood, but it works well in practice.
4In practice, we use max-marginals for pruning to ensure
that there is still a valid path in the pruned lattice.
211
Old Bailey, 1725:
Old Bailey, 1875:
Trove, 1883:
Trove, 1823:
(a)
(b)
(c)
(d)
Figure 6: Portions of several documents from our test set rep-
resenting a range of difficulties are displayed. On document
(a), which exhibits noisy typesetting, our system achieves a
word error rate (WER) of 25.2. Document (b) is cleaner in
comparison, and on it we achieve a WER of 15.4. On doc-
ument (c), which is also relatively clean, we achieve a WER
of 12.5. On document (d), which is severely degraded, we
achieve a WER of 70.0.
5 Data
We perform experiments on two historical datasets
consisting of images of documents printed be-
tween 1700 and 1900 in England and Australia.
Examples from both datasets are displayed in Fig-
ure 6.
5.1 Old Bailey
The first dataset comes from a large set of im-
ages of the proceedings of the Old Bailey, a crimi-
nal court in London, England (Shoemaker, 2005).
The Old Bailey curatorial effort, after deciding
that current OCR systems do not adequately han-
dle 18th century fonts, manually transcribed the
documents into text. We will use these manual
transcriptions to evaluate the output of our system.
From the Old Bailey proceedings, we extracted a
set of 20 images, each consisting of 30 lines of
text to use as our first test set. We picked 20 doc-
uments, printed in consecutive decades. The first
document is from 1715 and the last is from 1905.
We choose the first document in each of the corre-
sponding years, choose a random page in the doc-
ument, and extracted an image of the first 30 con-
secutive lines of text consisting of full sentences.5
The ten documents in the Old Bailey dataset that
were printed before 1810 use the long s glyph,
while the remaining ten do not.
5.2 Trove
Our second dataset is taken from a collection of
digitized Australian newspapers that were printed
between the years of 1803 and 1954. This col-
lection is called Trove, and is maintained by the
the National Library of Australia (Holley, 2010).
We extracted ten images from this collection in the
same way that we extracted images from Old Bai-
ley, but starting from the year 1803. We manually
produced our own gold annotations for these ten
images. Only the first document of Trove uses the
long s glyph.
5.3 Pre-processing
Many of the images in historical collections are
bitonal (binary) as a result of how they were cap-
tured on microfilm for storage in the 1980s (Arl-
itsch and Herbert, 2004). This is part of the reason
our model is designed to work directly with bi-
narized images. For consistency, we binarized the
images in our test sets that were not already binary
by thresholding pixel values.
Our model requires that the image be pre-
segmented into lines of text. We automatically
segment lines by training an HSMM over rows of
pixels. After the lines are segmented, each line
is resampled so that its vertical resolution is 30
pixels. The line extraction process also identifies
pixels that are not located in central text regions,
and are part of large connected components of ink,
spanning multiple lines. The values of such pixels
are treated as unobserved in the model since, more
often than not, they are part of ink blotches.
5This ruled out portions of the document with extreme
structural abnormalities, like title pages and lists. These
might be interesting to model, but are not within the scope
of this paper.
212
6 Experiments
We evaluate our system by comparing our text
recognition accuracy to that of two state-of-the-art
systems.
6.1 Baselines
Our first baseline is Google?s open source OCR
system, Tesseract (Smith, 2007). Tesseract takes
a pipelined approach to recognition. Before rec-
ognizing the text, the document is broken into
lines, and each line is segmented into words.
Then, Tesseract uses a classifier, aided by a word-
unigram language model, to recognize whole
words.
Our second baseline, ABBYY FineReader 11
Professional Edition,6 is a state-of-the-art com-
mercial OCR system. It is the OCR system that
the National Library of Australia used to recognize
the historical documents in Trove (Holley, 2010).
6.2 Evaluation
We evaluate the output of our system and the base-
line systems using two metrics: character error
rate (CER) and word error rate (WER). Both these
metrics are based on edit distance. CER is the edit
distance between the predicted and gold transcrip-
tions of the document, divided by the number of
characters in the gold transcription. WER is the
word-level edit distance (words, instead of char-
acters, are treated as tokens) between predicted
and gold transcriptions, divided by the number of
words in the gold transcription. When computing
WER, text is tokenized into words by splitting on
whitespace.
6.3 Language Model
We ran experiments using two different language
models. The first language model was trained
on the initial one million sentences of the New
York Times (NYT) portion of the Gigaword cor-
pus (Graff et al, 2007), which contains about 36
million words. This language model is out of do-
main for our experimental documents. To inves-
tigate the effects of using an in domain language
model, we created a corpus composed of the man-
ual annotations of all the documents in the Old
Bailey proceedings, excluding those used in our
test set. This corpus consists of approximately 32
million words. In all experiments we used a char-
acter n-gram order of six for the final Viterbi de-
6http://www.abbyy.com
System CER WER
Old Bailey
Google Tesseract 29.6 54.8
ABBYY FineReader 15.1 40.0
Ocular w/ NYT (this work) 12.6 28.1
Ocular w/ OB (this work) 9.7 24.1
Trove
Google Tesseract 37.5 59.3
ABBYY FineReader 22.9 49.2
Ocular w/ NYT (this work) 14.9 33.0
Table 1: We evaluate the predicted transcriptions in terms of
both character error rate (CER) and word error rate (WER),
and report macro-averages across documents. We compare
with two baseline systems: Google?s open source OCR sys-
tem, Tessearact, and a state-of-the-art commercial system,
ABBYY FineReader. We refer to our system as Ocular w/
NYT and Ocular w/ OB, depending on whether NYT or Old
Bailey is used to train the language model.
coding pass and an order of three for all coarse
passes.
6.4 Initialization and Tuning
We used as a development set ten additional docu-
ments from the Old Bailey proceedings and five
additional documents from Trove that were not
part of our test set. On this data, we tuned the
model?s hyperparameters7 and the parameters of
the pruning schedule for our coarse-to-fine ap-
proach.
In experiments we initialized ?RPADc and ?LPADc to
be uniform, and initialized ?GLYPHc and ?c based
on the standard modern fonts included with the
Ubuntu Linux 12.04 distribution.8 For documents
that use the long s glyph, we introduce a special
character type for the non-word-final s, and ini-
tialize its parameters from a mixture of the modern
f and | glyphs.9
7 Results and Analysis
The results of our experiments are summarized in
Table 1. We refer to our system as Ocular w/
NYT or Ocular w/ OB, depending on whether the
language model was trained using NYT or Old
Bailey, respectively. We compute macro-averages
7One of the hyperparameters we tune is the exponent of
the language model. This balances the contributions of the
language model and the typesetting model to the posterior
(Och and Ney, 2004).
8http://www.ubuntu.com/
9Following Berg-Kirkpatrick et al (2010), we use a reg-
ularization term in the optimization of the log-linear model
parameters ?c during the M-step. Instead of regularizing to-
wards zero, we regularize towards the initializer. This slightly
improves performance on our development set and can be
thought of as placing a prior on the glyph shape parameters.
213
(c) Trove, 1883:
(b) Old Bailey, 1885:
(a) Old Bailey, 1775: the prisoner at the bar. Jacob Lazarus and his
taken ill and taken away ? I remember
how the murderers came to learn the nation in
Predicted text:
Predicted typesetting:
Image:
Predicted text:
Predicted typesetting:
Image:
Predicted text:
Predicted typesetting:
Image:
Figure 7: For each of these portions of test documents, the first line shows the transcription predicted by our model and the
second line shows a representation of the learned typesetting layout. The grayscale glyphs show the Bernoulli pixel distributions
learned by our model, while the padding regions are depicted in blue. The third line shows the input image.
across documents from all years. Our system, us-
ing the NYT language model, achieves an average
WER of 28.1 on Old Bailey and an average WER
of 33.0 on Trove. This represents a substantial er-
ror reduction compared to both baseline systems.
If we average over the documents in both Old
Bailey and Trove, we find that Tesseract achieved
an average WER of 56.3, ABBYY FineReader
achieved an average WER of 43.1, and our system,
using the NYT language model, achieved an aver-
age WER of 29.7. This means that while Tesseract
incorrectly predicts more than half of the words in
these documents, our system gets more than three-
quarters of them right. Overall, we achieve a rela-
tive reduction in WER of 47% compared to Tesser-
act and 31% compared to ABBYY FineReader.
The baseline systems do not have special pro-
visions for the long s glyph. In order to make
sure the comparison is fair, we separately com-
puted average WER on only the documents from
after 1810 (which do no use the long s glyph). We
found that using this evaluation our system actu-
ally acheives a larger relative reduction in WER:
50% compared to Tesseract and 35% compared to
ABBYY FineReader.
Finally, if we train the language model using
the Old Bailey corpus instead of the NYT corpus,
we see an average improvement of 4 WER on the
Old Bailey test set. This means that the domain of
the language model is important, but, the results
are not affected drastically even when using a lan-
guage model based on modern corpora (NYT).
7.1 Learned Typesetting Layout
Figure 7 shows a representation of the typesetting
layout learned by our model for portions of several
Initializer
1700
1740
1780 1820
1860
1900
Figure 8: The central glyph is a representation of the initial
model parameters for the glyph shape for g, and surrounding
this are the learned parameters for documents from various
years.
test documents. For each portion of a test doc-
ument, the first line shows the transcription pre-
dicted by our model, and the second line shows
padding and glyph regions predicted by the model,
where the grayscale glyphs represent the learned
Bernoulli parameters for each pixel. The third line
shows the input image.
Figure 7a demonstrates a case where our model
has effectively explained both the uneven baseline
and over-inked glyphs by using the vertical offsets
vi and inking variables di. In Figure 7b the model
has used glyph widths gi and vertical offsets to ex-
plain the thinning of glyphs and falling baseline
that occurred near the binding of the book. In sep-
arate experiments on the Old Bailey test set, using
the NYT language model, we found that remov-
ing the vertical offset variables from the model in-
creased WER by 22, and removing the inking vari-
ables increased WER by 16. This indicates that it
is very important to model both these aspects of
printing press rendering.
214
Figure 9: This Old Bailey document from 1719 has severe ink bleeding from the facing page. We annotated these blotches (in
red) and treated the corresponding pixels as unobserved in the model. The layout shown is predicted by the model.
Figure 7c shows the output of our system on
a difficult document. Here, missing characters
and ink blotches confuse the model, which picks
something that is reasonable according to the lan-
guage model, but incorrect.
7.2 Learned Fonts
It is interesting to look at the fonts learned by our
system, and track how historical fonts changed
over time. Figure 8 shows several grayscale im-
ages representing the Bernoulli pixel probabilities
for the most likely width of the glyph for g under
various conditions. At the center is the representa-
tion of the initial parameter values, and surround-
ing this are the learned parameters for documents
from various years. The learned shapes are visibly
different from the initializer, which is essentially
an average of modern fonts, and also vary across
decades.
We can ask to what extent learning the font
structure actually improved our performance. If
we turn off learning and just use the initial pa-
rameters to decode, WER increases by 8 on the
Old Bailey test set when using the NYT language
model.
7.3 Unobserved Ink Blotches
As noted earlier, one strength of our generative
model is that we can make the values of certain
pixels unobserved in the model, and let inference
fill them in. We conducted an additional experi-
ment on a document from the Old Bailey proceed-
ings that was printed in 1719. This document, a
fragment of which is shown in Figure 9, has se-
vere ink bleeding from the facing page. We manu-
ally annotated the ink blotches (shown in red), and
made them unobserved in the model. The result-
ing typesetting layout learned by the model is also
shown in Figure 9. The model correctly predicted
most of the obscured words. Running the model
with the manually specified unobserved pixels re-
duced the WER on this document from 58 to 19
when using the NYT language model.
7.4 Remaining Errors
We performed error analysis on our development
set by randomly choosing 100 word errors from
the WER alignment and manually annotating them
with relevant features. Specifically, for each word
error we recorded whether or not the error con-
tained punctuation (either in the predicted word or
the gold word), whether the text in the correspond-
ing portion of the original image was italicized,
and whether the corresponding portion of the im-
age exhibited over-inking, missing ink, or signif-
icant ink blotches. These last three feature types
are subjective in nature but may still be informa-
tive. We found that 56% of errors were accompa-
nied by over-inking, 50% of errors were accom-
panied by ink blotches, 42% of errors contained
punctuation, 21% of errors showed missing ink,
and 12% of errors contained text that was itali-
cized in the original image.
Our own subjective assessment indicates that
many of these error features are in fact causal.
More often than not, italicized text is incorrectly
transcribed. In cases of extreme ink blotching,
or large areas of missing ink, the system usually
makes an error.
8 Conclusion
We have demonstrated a model, based on the his-
torical typesetting process, that effectively learns
font structure in an unsupervised fashion to im-
prove transcription of historical documents into
text. The parameters of the learned fonts are inter-
pretable, as are the predicted typesetting layouts.
Our system achieves state-of-the-art results, sig-
nificantly outperforming two state-of-the-art base-
line systems.
215
References
Kenning Arlitsch and John Herbert. 2004. Microfilm,
paper, and OCR: Issues in newspaper digitization.
the Utah digital newspapers program. Microform &
Imaging Review.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings
of the 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies:.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword third edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Tin Kam Ho and George Nagy. 2000. OCR with no
shape training. In Proceedings of the 15th Interna-
tional Conference on Pattern Recognition.
Rose Holley. 2010. Trove: Innovation in access to
information in Australia. Ariadne.
Gary Huang, Erik G Learned-Miller, and Andrew Mc-
Callum. 2006. Cryptogram decoding for optical
character recognition. University of Massachusetts-
Amherst Technical Report.
Fred Jelinek. 1998. Statistical methods for speech
recognition. MIT press.
Andrew Kae and Erik Learned-Miller. 2009. Learn-
ing on the fly: font-free approaches to difficult OCR
problems. In Proceedings of the 2009 International
Conference on Document Analysis and Recognition.
Andrew Kae, Gary Huang, Carl Doersch, and Erik
Learned-Miller. 2010. Improving state-of-the-
art OCR through high-precision document-specific
modeling. In Proceedings of the 2010 IEEE Confer-
ence on Computer Vision and Pattern Recognition.
Vladimir Kluzner, Asaf Tzadok, Yuval Shimony, Eu-
gene Walach, and Apostolos Antonacopoulos. 2009.
Word-based adaptive OCR for historical books. In
Proceedings of the 2009 International Conference
on on Document Analysis and Recognition.
Vladimir Kluzner, Asaf Tzadok, Dan Chevion, and Eu-
gene Walach. 2011. Hybrid approach to adaptive
OCR for historical books. In Proceedings of the
2011 International Conference on Document Anal-
ysis and Recognition.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. Machine translation: From real users
to research.
Okan Kolak, William Byrne, and Philip Resnik. 2003.
A generative probabilistic OCR model for NLP ap-
plications. In Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Gary Kopec and Mauricio Lomelin. 1996. Document-
specific character template estimation. In Proceed-
ings of the International Society for Optics and Pho-
tonics.
Gary Kopec, Maya Said, and Kris Popat. 2001. N-
gram language models for document image decod-
ing. In Proceedings of Society of Photographic In-
strumentation Engineers.
Stephen Levinson. 1986. Continuously variable du-
ration hidden Markov models for automatic speech
recognition. Computer Speech & Language.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming.
Franz Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing.
Sujith Ravi and Kevin Knight. 2011. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Robert Shoemaker. 2005. Digital London: Creating a
searchable web of interlinked sources on eighteenth
century London. Electronic Library and Informa-
tion Systems.
Ray Smith. 2007. An overview of the tesseract ocr
engine. In Proceedings of the Ninth International
Conference on Document Analysis and Recognition.
216
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics.
Georgios Vamvakas, Basilios Gatos, Nikolaos Stam-
atopoulos, and Stavros Perantonis. 2008. A com-
plete optical character recognition methodology for
historical documents. In The Eighth IAPR Interna-
tional Workshop on Document Analysis Systems.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
217
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 208?217,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Sparser, Better, Faster GPU Parsing
David Hall Taylor Berg-Kirkpatrick John Canny Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,tberg,jfc,klein}@cs.berkeley.edu
Abstract
Due to their origin in computer graph-
ics, graphics processing units (GPUs)
are highly optimized for dense problems,
where the exact same operation is applied
repeatedly to all data points. Natural lan-
guage processing algorithms, on the other
hand, are traditionally constructed in ways
that exploit structural sparsity. Recently,
Canny et al (2013) presented an approach
to GPU parsing that sacrifices traditional
sparsity in exchange for raw computa-
tional power, obtaining a system that can
compute Viterbi parses for a high-quality
grammar at about 164 sentences per sec-
ond on a mid-range GPU. In this work,
we reintroduce sparsity to GPU parsing
by adapting a coarse-to-fine pruning ap-
proach to the constraints of a GPU. The
resulting system is capable of computing
over 404 Viterbi parses per second?more
than a 2x speedup?on the same hard-
ware. Moreover, our approach allows us
to efficiently implement less GPU-friendly
minimum Bayes risk inference, improv-
ing throughput for this more accurate algo-
rithm from only 32 sentences per second
unpruned to over 190 sentences per second
using pruning?nearly a 6x speedup.
1 Introduction
Because NLP models typically treat sentences in-
dependently, NLP problems have long been seen
as ?embarrassingly parallel? ? large corpora can
be processed arbitrarily fast by simply sending dif-
ferent sentences to different machines. However,
recent trends in computer architecture, particularly
the development of powerful ?general purpose?
GPUs, have changed the landscape even for prob-
lems that parallelize at the sentence level. First,
classic single-core processors and main memory
architectures are no longer getting substantially
faster over time, so speed gains must now come
from parallelism within a single machine. Second,
compared to CPUs, GPUs devote a much larger
fraction of their computational power to actual
arithmetic. Since tasks like parsing boil down to
repeated read-multiply-write loops, GPUs should
be many times more efficient in time, power, or
cost. The challenge is that GPUs are not a good
fit for the kinds of sparse computations that most
current CPU-based NLP algorithms rely on.
Recently, Canny et al (2013) proposed a GPU
implementation of a constituency parser that sac-
rifices all sparsity in exchange for the sheer horse-
power that GPUs can provide. Their system uses a
grammar based on the Berkeley parser (Petrov and
Klein, 2007) (which is particularly amenable to
GPU processing), ?compiling? the grammar into a
sequence of GPU kernels that are applied densely
to every item in the parse chart. Together these
kernels implement the Viterbi inside algorithm.
On a mid-range GPU, their system can compute
Viterbi derivations at 164 sentences per second on
sentences of length 40 or less (see timing details
below).
In this paper, we develop algorithms that can
exploit sparsity on a GPU by adapting coarse-to-
fine pruning to a GPU setting. On a CPU, pruning
methods can give speedups of up to 100x. Such
extreme speedups over a dense GPU baseline cur-
rently seem unlikely because fine-grained sparsity
appears to be directly at odds with dense paral-
lelism. However, in this paper, we present a sys-
tem that finds a middle ground, where some level
of sparsity can be maintained without losing the
parallelism of the GPU. We use a coarse-to-fine
approach as in Petrov and Klein (2007), but with
only one coarse pass. Figure 1 shows an overview
of the approach: we first parse densely with a
coarse grammar and then parse sparsely with the
208
fine grammar, skipping symbols that the coarse
pass deemed sufficiently unlikely. Using this ap-
proach, we see a gain of more than 2x over the
dense GPU implementation, resulting in overall
speeds of up to 404 sentences per second. For
comparison, the publicly available CPU imple-
mentation of Petrov and Klein (2007) parses ap-
proximately 7 sentences per second per core on a
modern CPU.
A further drawback of the dense approach in
Canny et al (2013) is that it only computes
Viterbi parses. As with other grammars with
a parse/derivation distinction, the grammars of
Petrov and Klein (2007) only achieve their full
accuracy using minimum-Bayes-risk parsing, with
improvements of over 1.5 F1 over best-derivation
Viterbi parsing on the Penn Treebank (Marcus et
al., 1993). To that end, we extend our coarse-to-
fine GPU approach to computing marginals, along
the way proposing a new way to exploit the coarse
pass to avoid expensive log-domain computations
in the fine pass. We then implement minimum-
Bayes-risk parsing via the max recall algorithm of
Goodman (1996). Without the coarse pass, the
dense marginal computation is not efficient on a
GPU, processing only 32 sentences per second.
However, our approach allows us to process over
190 sentences per second, almost a 6x speedup.
2 A Note on Experiments
We build up our approach incrementally, with ex-
periments interspersed throughout the paper, and
summarized in Tables 1 and 2. In this paper, we
focus our attention on current-generation NVIDIA
GPUs. Many of the ideas described here apply to
other GPUs (such as those from AMD), but some
specifics will differ. All experiments are run with
an NVIDIA GeForce GTX 680, a mid-range GPU
that costs around $500 at time of writing. Unless
otherwise noted, all experiments are conducted on
sentences of length ? 40 words, and we estimate
times based on batches of 20K sentences.
1
We
should note that our experimental condition dif-
fers from that of Canny et al (2013): they evaluate
on sentences of length ? 30. Furthermore, they
1
The implementation of Canny et al (2013) cannot han-
dle batches so large, and so we tested it on batches of 1200
sentences. Our reimplementation is approximately the same
speed for the same batch sizes. For batches of 20K sentences,
we used sentences from the training set. We verified that there
was no significant difference in speed for sentences from the
training set and from the test set.
use two NVIDIA GeForce GTX 690s?each of
which is essentially a repackaging of two 680s?
meaning that our system and experiments would
run approximately four times faster on their hard-
ware. (This expected 4x factor is empirically con-
sistent with the result of running their system on
our hardware.)
3 Sparsity and CPUs
One successful approach for speeding up con-
stituency parsers has been to use coarse-to-fine
inference (Charniak et al, 2006). In coarse-to-
fine inference, we have a sequence of increasingly
complex grammars G
`
. Typically, each succes-
sive grammar G
`
is a refinement of the preceding
grammar G
`?1
. That is, for each symbol A
x
in
the fine grammar, there is some symbol A in the
coarse grammar. For instance, in a latent variable
parser, the coarse grammar would have symbols
like NP , V P , etc., and the fine pass would have
refined symbols NP
0
, NP
1
, V P
4
, and so on.
In coarse-to-fine inference, one applies the
grammars in sequence, computing inside and out-
side scores. Next, one computes (max) marginals
for every labeled span (A, i, j) in a sentence.
These max marginals are used to compute a prun-
ing mask for every span (i, j). This mask is the set
of symbols allowed for that span. Then, in the next
pass, one only processes rules that are licensed by
the pruning mask computed at the previous level.
This approach works because a low quality
coarse grammar can still reliably be used to prune
many symbols from the fine chart without loss of
accuracy. Petrov and Klein (2007) found that over
98% of symbols can be pruned from typical charts
using a simple X-bar grammar without any loss
of accuracy. Thus, the vast majority of rules can
be skipped, and therefore most computation can
be avoided. It is worth pointing out that although
98% of labeled spans can be skipped due to X-bar
pruning, we found that only about 79% of binary
rule applications can be skipped, because the un-
pruned symbols tend to be the ones with a larger
grammar footprint.
4 GPU Architectures
Unfortunately, the standard coarse-to-fine ap-
proach does not na??vely translate to GPU archi-
tectures. GPUs work by executing thousands of
threads at once, but impose the constraint that
large blocks of threads must be executing the same
209
RAM
CPU
GPU
RAM
Instruction Cache
Parse Charts
Work Array
Grammar
Queue
Sentences
Queue
Masks
Masks
Queue
Trees
Figure 1: Overview of the architecture of our system, which is an extension of Canny et al (2013)?s
system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to
the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask
that is used by the CPU when deciding which items to queue during the fine pass. The original system
of Canny et al (2013) only used the fine pass, with no pruning.
instructions in lockstep, differing only in their in-
put data. Thus sparsely skipping rules and sym-
bols will not save any work. Indeed, it may ac-
tually slow the system down. In this section, we
provide an overview of GPU architectures, focus-
ing on the details that are relevant to building an
efficient parser.
The large number of threads that a GPU exe-
cutes are packaged into blocks of 32 threads called
warps. All threads in a warp must execute the
same instruction at every clock cycle: if one thread
takes a branch the others do not, then all threads in
the warp must follow both code paths. This situa-
tion is called warp divergence. Because all threads
execute all code paths that any thread takes, time
can only be saved if an entire warp agrees to skip
any particular branch.
NVIDIA GPUs have 8-15 processors called
streaming multi-processors or SMs.
2
Each SM
can process up to 48 different warps at a time:
it interleaves the execution of each warp, so that
when one warp is stalled another warp can exe-
cute. Unlike threads within a single warp, the 48
warps do not have to execute the same instruc-
tions. However, the memory architecture is such
that they will be faster if they access related mem-
ory locations.
2
Older hardware (600 series or older) has 8 SMs. Newer
hardware has more.
A further consideration is that the number of
registers available to a thread in a warp is rather
limited compared to a CPU. On the 600 series,
maximum occupancy can only be achieved if each
thread uses at most 63 registers (Nvidia, 2008).
3
Registers are many times faster than variables lo-
cated in thread-local memory, which is actually
the same speed as global memory.
5 Anatomy of a Dense GPU Parser
This architecture environment puts very different
constraints on parsing algorithms from a CPU en-
vironment. Canny et al (2013) proposed an imple-
mentation of a PCFG parser that sacrifices stan-
dard sparse methods like coarse-to-fine pruning,
focusing instead on maximizing the instruction
and memory throughput of the parser. They as-
sume that they are parsing many sentences at once,
with throughput being more important than la-
tency. In this section, we describe their dense algo-
rithm, which we take as the baseline for our work;
we present it in a way that sets up the changes to
follow.
At the top level, the CPU and GPU communi-
cate via a work queue of parse items of the form
(s, i, k, j), where s is an identifier of a sentence,
i is the start of a span, k is the split point, and j
3
A thread can use more registers than this, but the full
complement of 48 warps cannot execute if too many are used.
210
Clustering Pruning Sent/Sec Speedup
Canny et al ? 164.0 ?
Reimpl ? 192.9 1.0x
Reimpl Empty, Coarse 185.5 0.96x
Reimpl Labeled, Coarse 187.5 0.97x
Parent ? 158.6 0.82x
Parent Labeled, Coarse 278.9 1.4x
Parent Labeled, 1-split 404.7 2.1x
Parent Labeled, 2-split 343.6 1.8x
Table 1: Performance numbers for computing
Viterbi inside charts on 20,000 sentences of length
?40 from the Penn Treebank. All times are
measured on an NVIDIA GeForce GTX 680.
?Reimpl? is our reimplementation of their ap-
proach. Speedups are measured in reference to this
reimplementation. See Section 7 for discussion of
the clustering algorithms and Section 6 for a de-
scription of the pruning methods. The Canny et al
(2013) system is benchmarked on a batch size of
1200 sentences, the others on 20,000.
is the end point. The GPU takes large numbers of
parse items and applies the entire grammar to them
in parallel. These parse items are enqueued in or-
der of increasing span size, blocking until all items
of a given length are complete. This approach is
diagrammed in Figure 2.
Because all rules are applied to all parse items,
all threads are executing the same sequence of in-
structions. Thus, there is no concern of warp di-
vergence.
5.1 Grammar Compilation
One important feature of Canny et al (2013)?s sys-
tem is grammar compilation. Because registers
are so much faster than thread-local memory, it
is critical to keep as many variables in registers
as possible. One way to accomplish this is to un-
roll loops at compilation time. Therefore, they in-
lined the iteration over the grammar directly into
the GPU kernels (i.e. the code itself), which al-
lows the compiler to more effectively use all of its
registers.
However, register space is limited on GPUs.
Because the Berkeley grammar is so large, the
compiler is not able to efficiently schedule all of
the operations in the grammar, resulting in regis-
ter spills. Canny et al (2013) found they had to
partition the grammar into multiple different ker-
nels. We discuss this partitioning in more detail in
Section 7. However, in short, the entire grammar
G is broken into multiple clusters G
i
where each
rule belongs to exactly one cluster.
NP
DT NN
VB
VP
NP
NP
PP
IN
NP
S
VP
(0, 1, 3)
(0, 2, 3)
(1, 2, 4)
(1, 3, 4)
(2, 3, 5)
(2, 4, 5)
Grammar
Queue
(i, k, j)
Figure 2: Schematic representation of the work
queue used in Canny et al (2013). The Viterbi
inside loop for the grammar is inlined into a ker-
nel. The kernel is applied to all items in the queue
in a blockwise manner.
NP
DT NN
NP
DT NN
NP
DT NN
NP
NP
PP
IN
NP
PP
IN
NP
PP
IN
PP
VB
VP
NP
VB
VP
NP
VB
VP
NP
VP
(0, 1, 3)
(1, 2, 4)
(3, 5, 6)
(1, 3, 4)
(1, 2, 4)
(0, 2, 3)
(2, 4, 5)
(3, 4, 6)
Queues
(i, k, j)
Grammar Clusters
Figure 3: Schematic representation of the work
queue and grammar clusters used in the fine pass
of our work. Here, the rules of the grammar are
clustered by their coarse parent symbol. We then
have multiple work queues, with parse items only
being enqueued if the span (i, j) allows that sym-
bol in its pruning mask.
All in all, Canny et al (2013)?s system is able
to compute Viterbi charts at 164 sentences per sec-
ond, for sentences up to length 40. On larger batch
sizes, our reimplementation of their approach is
able to achieve 193 sentences per second on the
same hardware. (See Table 1.)
6 Pruning on a GPU
Now we turn to the algorithmic and architectural
changes in our approach. First, consider trying to
211
directly apply the coarse-to-fine method sketched
in Section 3 to the dense baseline described above.
The natural implementation would be for each
thread to check if each rule is licensed before
applying it. However, we would only avoid the
work of applying the rule if all threads in the warp
agreed to skip it. Since each thread in the warp is
processing a different span (perhaps even from a
different sentence), consensus from all 32 threads
on any skip would be unlikely.
Another approach would be to skip enqueu-
ing any parse item (s, i, k, j) where the pruning
mask for any of (i, j), (i, k), or (k, j) is entirely
empty (i.e. all symbols are pruned in this cell by
the coarse grammar). However, our experiments
showed that only 40% of parse items are pruned in
this manner. Because of the overhead associated
with creating pruning masks and the further over-
head of GPU communication, we found that this
method did not actually produce any time savings
at all. The result is a parsing speed of 185.5 sen-
tences per second, as shown in Table 1 on the row
labeled ?Reimpl? with ?Empty, Coarse? pruning.
Instead, we take advantage of the partitioned
structure of the grammar and organize our com-
putation around the coarse symbol set. Recall that
the baseline already partitions the grammar G into
rule clusters G
i
to improve register sharing. (See
Section 7 for more on the baseline clustering.) We
create a separate work queue for each partition.
We call each such queue a labeled work queue, and
each one only queues items to which some rule in
the corresponding partition applies. We call the set
of coarse symbols for a partition (and therefore the
corresponding labeled work queue) a signature.
During parsing, we only enqueue items
(s, i, k, j) to a labeled queue if two conditions are
met. First, the span (i, j)?s pruning mask must
have a non-empty intersection with the signature
of the queue. Second, the pruning mask for the
children (i, k) and (k, j) must be non-empty.
Once on the GPU, parse items are processed us-
ing the same style of compiled kernel as in Canny
et al (2013). Because the entire partition (though
not necessarily the entire grammar) is applied to
each item in the queue, we still do not need to
worry about warp divergence.
At the top level, our system first computes prun-
ing masks with a coarse grammar. Then it pro-
cesses the same sentences with the fine gram-
mar. However, to the extent that the signatures
are small, items can be selectively queued only to
certain queues. This approach is diagrammed in
Figure 3.
We tested our new pruning approach using an
X-bar grammar as the coarse pass. The result-
ing speed is 187.5 sentences per second, labeled
in Table 1 as row labeled ?Reimpl? with ?Labeled,
Coarse? pruning. Unfortunately, this approach
again does not produce a speedup relative to our
reimplemented baseline. To improve upon this re-
sult, we need to consider how the grammar clus-
tering interacts with the coarse pruning phase.
7 Grammar Clustering
Recall that the rules in the grammar are partitioned
into a set of clusters, and that these clusters are
further divided into subclusters. How can we best
cluster and subcluster the grammar so as to maxi-
mize performance? A good clustering will group
rules together that use the same symbols, since
this means fewer memory accesses to read and
write scores for symbols. Moreover, we would
like the time spent processing each of the subclus-
ters within a cluster to be about the same. We can-
not move on to the next cluster until all threads
from a cluster are finished, which means that the
time a cluster takes is the amount of time taken
by the longest-running subcluster. Finally, when
pruning, it is best if symbols that have the same
coarse projection are clustered together. That way,
we are more likely to be able to skip a subcluster,
since fewer distinct symbols need to be ?off? for a
parse item to be skipped in a given subcluster.
Canny et al (2013) clustered symbols of the
grammar using a sophisticated spectral clustering
algorithm to obtain a permutation of the symbols.
Then the rules of the grammar were laid out in
a (sparse) three-dimensional tensor, with one di-
mension representing the parent of the rule, one
representing the left child, and one representing
the right child. They then split the cube into 6x2x2
contiguous ?major cubes,? giving a partition of the
rules into 24 clusters. They then further subdi-
vided these cubes into 2x2x2 minor cubes, giv-
ing 8 subclusters that executed in parallel. Note
that the clusters induced by these major and minor
cubes need not be of similar sizes; indeed, they of-
ten are not. Clustering using this method is labeled
?Reimplementation? in Table 1.
The addition of pruning introduces further con-
siderations. First, we have a coarse grammar, with
212
many fewer rules and symbols. Second, we are
able to skip a parse item for an entire cluster if that
item?s pruning mask does not intersect the clus-
ter?s signature. Spreading symbols across clusters
may be inefficient: if a parse item licenses a given
symbol, we will have to enqueue that item to any
queue that has the symbol in its signature, no mat-
ter how many other symbols are in that cluster.
Thus, it makes sense to choose a clustering al-
gorithm that exploits the structure introduced by
the pruning masks. We use a very simple method:
we cluster the rules in the grammar by coarse par-
ent symbol. When coarse symbols are extremely
unlikely (and therefore have few corresponding
rules), we merge their clusters to avoid the over-
head of beginning work on clusters where little
work has to be done.
4
In order to subcluster, we
divide up rules among subclusters so that each
subcluster has the same number of active parent
symbols. We found this approach to subclustering
worked well in practice.
Clustering using this method is labeled ?Parent?
in Table 1. Now, when we use a coarse pruning
pass, we are able to parse nearly 280 sentences
per second, a 70% increase in parsing performance
relative to Canny et al (2013)?s system, and nearly
50% over our reimplemented baseline.
It turns out that this simple clustering algorithm
produces relatively efficient kernels even in the un-
pruned case. The unpruned Viterbi computations
in a fine grammar using the clustering method of
Canny et al (2013) yields a speed of 193 sen-
tences per second, whereas the same computation
using coarse parent clustering has a speed of 159
sentences per second. (See Table 1.) This is not
as efficient as Canny et al (2013)?s highly tuned
method, but it is still fairly fast, and much simpler
to implement.
8 Pruning with Finer Grammars
The coarse to fine pruning approach of Petrov and
Klein (2007) employs an X-bar grammar as its
first pruning phase, but there is no reason why
we cannot begin with a more complex grammar
for our initial pass. As Petrov and Klein (2007)
have shown, intermediate-sized Berkeley gram-
mars prune many more symbols than the X-bar
system. However, they are slower to parse with
4
Specifically, after clustering based on the coarse parent
symbol, we merge all clusters with less than 300 rules in them
into one large cluster.
in a CPU context, and so they begin with an X-bar
grammar.
Because of the overhead associated with trans-
ferring work items to GPU, using a very small
grammar may not be an efficient use of the GPU?s
computational resources. To that end, we tried
computing pruning masks with one-split and two-
split Berkeley grammars. The X-bar grammar can
compute pruning masks at just over 1000 sen-
tences per second, the 1-split grammar parses 858
sentences per second, and the 2-split grammar
parses 526 sentences per second.
Because parsing with these grammars is still
quite fast, we tried using them as the coarse pass
instead. As shown in Table 1, using a 1-split gram-
mar as a coarse pass allows us to produce over 400
sentences per second, a full 2x improvement over
our original system. Conducting a coarse pass
with a 2-split grammar is somewhat slower, at a
?mere? 343 sentences per second.
9 Minimum Bayes risk parsing
The Viterbi algorithm is a reasonably effective
method for parsing. However, many authors
have noted that parsers benefit substantially from
minimum Bayes risk decoding (Goodman, 1996;
Simaan, 2003; Matsuzaki et al, 2005; Titov and
Henderson, 2006; Petrov and Klein, 2007). MBR
algorithms for parsing do not compute the best
derivation, as in Viterbi parsing, but instead the
parse tree that maximizes the expected count of
some figure of merit. For instance, one might want
to maximize the expected number of correct con-
stituents (Goodman, 1996), or the expected rule
counts (Simaan, 2003; Petrov and Klein, 2007).
MBR parsing has proven especially useful in la-
tent variable grammars. Petrov and Klein (2007)
showed that MBR trees substantially improved
performance over Viterbi parses for latent variable
grammars, earning up to 1.5F1.
Here, we implement the Max Recall algorithm
of Goodman (1996). This algorithm maximizes
the expected number of correct coarse symbols
(A, i, j) with respect to the posterior distribution
over parses for a sentence.
This particular MBR algorithm has the advan-
tage that it is relatively straightforward to imple-
ment. In essence, we must compute the marginal
probability of each fine-labeled span ?(A
x
, i, j),
and then marginalize to obtain ?(A, i, j). Then,
for each span (i, j), we find the best possible split
213
point k that maximizes C(i, j) = ?(A, i, j) +
max
k
(C(i, k) + C(k, j)). Parse extraction is
then just a matter of following back pointers from
the root, as in the Viterbi algorithm.
9.1 Computing marginal probabilities
The easiest way to compute marginal probabilities
is to use the log space semiring rather than the
Viterbi semiring, and then to run the inside and
outside algorithms as before. We should expect
this algorithm to be at least a factor of two slower:
the outside pass performs at least as much work as
the inside pass. Moreover, it typically has worse
memory access patterns, leading to slower perfor-
mance.
Without pruning, our approach does not han-
dle these log domain computations well at all:
we are only able to compute marginals for 32.1
sentences/second, more than a factor of 5 slower
than our coarse pass. To begin, log space addition
requires significantly more operations than max,
which is a primitive operation on GPUs. Beyond
the obvious consequence that executing more op-
erations means more time taken, the sheer number
of operations becomes too much for the compiler
to handle. Because the grammars are compiled
into code, the additional operations are all inlined
into the kernels, producing much larger kernels.
Indeed, in practice the compiler will often hang if
we use the same size grammar clusters as we did
for Viterbi. In practice, we found there is an effec-
tive maximum of 2000 rules per kernel using log
sums, while we can use more than 10,000 rules
rules in a single kernel with Viterbi.
With coarse pruning, however, we can avoid
much of the increased cost associated with log
domain computations. Because so many labeled
spans are pruned, we are able to skip many of the
grammar clusters and thus avoid many of the ex-
pensive operations. Using coarse pruning and log
domain calculations, our system produces MBR
trees at a rate of 130.4 sentences per second, a
four-fold increase.
9.2 Scaling with the Coarse Pass
One way to avoid the expense of log domain com-
putations is to use scaled probabilities rather than
log probabilities. Scaling is one of the folk tech-
niques that are commonly used in the NLP com-
munity, but not generally written about. Recall
that floating point numbers are composed of a
mantissa m and an exponent e, giving a number
System Sent/Sec Speedup
Unpruned Log Sum MBR 32.1 ?
Pruned Log Sum MBR 130.4 4.1x
Pruned Scaling MBR 190.6 5.9x
Pruned Viterbi 404.7 12.6x
Table 2: Performance numbers for computing max
constituent (Goodman, 1996) trees on 20,000 sen-
tences of length 40 or less from the Penn Tree-
bank. For convenience, we have copied our pruned
Viterbi system?s result.
f = m ? 2
e
. When a float underflows, the ex-
ponent becomes too low to represent the available
number of bits. In scaling, floating point numbers
are paired with an additional number that extends
the exponent. That is, the number is represented
as f
?
= f ? exp(s). Whenever f becomes either
too big or too small, the number is rescaled back
to a less ?dangerous? range by shifting mass from
the exponent e to the scaling factor s.
In practice, one scale s is used for an entire span
(i, j), and all scores for that span are rescaled in
concert. In our GPU system, multiple scores in
any given span are being updated at the same time,
which makes this dynamic rescaling tricky and ex-
pensive, especially since inter-warp communica-
tion is fairly limited.
We propose a much simpler static solution that
exploits the coarse pass. In the coarse pass, we
compute Viterbi inside and outside scores for ev-
ery span. Because the grammar used in the coarse
pass is a projection of the grammar used in the
fine pass, these coarse scores correlate reasonably
closely with the probabilities computed in the fine
pass: If a span has a very high or very low score
in the coarse pass, it typically has a similar score
in the fine pass. Thus, we can use the coarse
pass?s inside and outside scores as the scaling val-
ues for the fine pass?s scores. That is, in addition
to computing a pruning mask, in the coarse pass
we store the maximum inside and outside score in
each span, giving two arrays of scores s
I
i,j
and s
O
i,j
.
Then, when applying rules in the fine pass, each
fine inside score over a split span (i, k, j) is scaled
to the appropriate s
I
i,j
by multiplying the score by
exp
(
s
I
i,k
+ s
I
k,j
? s
I
i,j
)
, where s
I
i,k
, s
I
k,j
, s
I
i,j
are
the scaling factors for the left child, right child,
and parent, respectively. The outside scores are
scaled analogously.
By itself, this approach works on nearly ev-
ery sentence. However, scores for approximately
214
0.5% of sentences overflow (sic). Because we are
summing instead of maxing scores in the fine pass,
the scaling factors computed using max scores are
not quite large enough, and so the rescaled inside
probabilities grow too large when multiplied to-
gether. Most of this difference arises at the leaves,
where the lexicon typically has more uncertainty
than higher up in the tree. Therefore, in the fine
pass, we normalize the inside scores at the leaves
to sum to 1.0.
5
Using this slight modification, no
sentences from the Treebank under- or overflow.
We know of no reason why this same trick can-
not be employed in more traditional parsers, but
it is especially useful here: with this static scal-
ing, we can avoid the costly log sums without in-
troducing any additional inter-thread communica-
tion, making the kernels much smaller and much
faster. Using scaling, we are able to push our
parser to 190.6 sentences/second for MBR extrac-
tion, just under half the speed of the Viterbi sys-
tem.
9.3 Parsing Accuracies
It is of course important verify the correctness of
our system; one easy way to do so is to exam-
ine parsing accuracy, as compared to the original
Berkeley parser. We measured parsing accuracy
on sentences of length? 40 from section 22 of the
Penn Treebank. Our Viterbi parser achieves 89.7
F1, while our MBR parser scores 91.0. These re-
sults are nearly identical to the Berkeley parsers
most comparable numbers: 89.8 for Viterbi, and
90.9 for their ?Max-Rule-Sum? MBR algorithm.
These slight differences arise from the usual mi-
nor variation in implementation details. In partic-
ular, we use one coarse pass instead of several, and
a different MBR algorithm. In addition, there are
some differences in unary processing.
10 Analyzing System Performance
In this section we attempt to break down how ex-
actly our system is spending its time. We do this in
an effort to give a sense of how time is spent dur-
ing computation on GPUs. These timing numbers
are computed using the built-in profiling capabil-
ities of the programming environment. As usual,
profiles exhibit an observer effect, where the act of
measuring the system changes the execution. Nev-
5
One can instead interpret this approach as changing the
scaling factors to s
I
?
i,j
= s
I
i,j
?
?
i?k<j
?
A
inside(A, k, k +
1), where inside is the array of scores for the fine pass.
System Coarse Pass Fine Pass
Unpruned Viterbi ? 6.4
Pruned Viterbi 1.2 1.5
Unpruned Logsum MBR ? 28.6
Pruned Scaling MBR 1.2 4.3
Table 3: Time spent in the passes of our differ-
ent systems, in seconds per 1000 sentences. Prun-
ing refers to using a 1-split grammar for the coarse
pass.
ertheless, the general trends should more or less be
preserved as compared to the unprofiled code.
To begin, we can compute the number of sec-
onds needed to parse 1000 sentences. (We use sec-
onds per sentence rather than sentences per second
because the former measure is additive.) The re-
sults are in Table 3. In the case of pruned Viterbi,
pruning reduces the amount of time spent in the
fine pass by more than 4x, though half of those
gains are lost to computing the pruning masks.
In Table 4, we break down the time taken by
our system into individual components. As ex-
pected, binary rules account for the vast majority
of the time in the unpruned Viterbi case, but much
less time in the pruned case, with the total time
taken for binary rules in the coarse and fine passes
taking about 1/5 of the time taken by binaries in
the unpruned version. Queueing, which involves
copying memory around within the GPU to pro-
cess the individual parse items, takes a fairly con-
sistent amount of time in all systems. Overhead,
which includes transport time between the CPU
and GPU and other processing on the CPU, is rela-
tively small for most system configurations. There
is greater overhead in the scaling system, because
scaling factors are copied to the CPU between the
coarse and fine passes.
A final question is: how many sentences per
second do we need to process to saturate the
GPU?s processing power? We computed Viterbi
parses of successive powers of 10, from 1 to
100,000 sentences.
6
In Figure 4, we then plotted
the throughput, in terms of number of sentences
per second. Throughput increases through parsing
10,000 sentences, and then levels off by the time it
reaches 100,000 sentences.
6
We replicated the Treebank for the 100,000 sentences
pass.
215
System Coarse Pass Fine Pass
Binary Unary Queueing Masks Overhead Binary Unary Queueing Overhead
Unpruned Viterbi ? ? ? ? ? 5.42 0.14 0.33 0.40
Pruned Viterbi 0.59 0.02 0.19 0.04 0.22 0.56 0.10 0.34 0.22
Pruned Scaling 0.59 0.02 0.19 0.04 0.20 1.74 0.24 0.46 0.84
Table 4: Breakdown of time spent in our different systems, in seconds per 1000 sentences. Binary and
Unary refer to spent processing binary rules. Queueing refers to the amount of time used to move memory
around within the GPU for processing. Overhead includes all other time, which includes communication
between the GPU and the CPU.
Sentenc
es/Seco
nd
0
100
200
300
400
Number of Sentences1 10 100 1K 10K 100K
Figure 4: Plot of speeds (sentences / second) for
various sizes of input corpora. The full power of
the GPU parser is only reached when run on large
numbers of sentences.
11 Related Work
Apart from the model of Canny et al (2013), there
have been a few attempts at using GPUs in NLP
contexts before. Johnson (2011) and Yi et al
(2011) both had early attempts at porting pars-
ing algorithms to the GPU. However, they did
not demonstrate significantly increased speed over
a CPU implementation. In machine translation,
He et al (2013) adapted algorithms designed for
GPUs in the computational biology literature to
speed up on-demand phrase table extraction.
12 Conclusion
GPUs represent a challenging opportunity for nat-
ural language processing. By carefully design-
ing within the constraints imposed by the architec-
ture, we have created a parser that can exploit the
same kinds of sparsity that have been developed
for more traditional architectures.
One of the key remaining challenges going
forward is confronting the kind of lexicalized
sparsity common in other NLP models. The
Berkeley parser?s grammars?by virtue of being
unlexicalized?can be applied uniformly to all
parse items. The bilexical features needed by
dependency models and lexicalized constituency
models are not directly amenable to acceleration
using the techniques we described here. Deter-
mining how to efficiently implement these kinds
of models is a promising area for new research.
Our system is available as open-source at
https://www.github.com/dlwh/puck.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Google PhD fellowship to the first author, and
an NSF fellowship to the second. We further
gratefully acknowledge a hardware donation by
NVIDIA Corporation.
References
John Canny, David Hall, and Dan Klein. 2013. A
multi-teraflop constituency parser using GPUs. In
Proceedings of EMNLP, pages 1898?1907, October.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R Shrivaths, Jeremy Moore, Michael Pozar,
et al 2006. Multilevel coarse-to-fine pcfg pars-
ing. In Proceedings of the main conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 168?175. Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In ACL, pages 177?183.
Hua He, Jimmy Lin, and Adam Lopez. 2013. Mas-
sively parallel suffix array queries and on-demand
phrase extraction for statistical machine translation
using gpus. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 325?334, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.
Mark Johnson. 2011. Parsing in parallel on multiple
cores and gpus. In Proceedings of the Australasian
Language Technology Association Workshop.
216
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
CUDA Nvidia. 2008. Programming guide.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT.
Khalil Simaan. 2003. On maximizing metrics for syn-
tactic disambiguation. In Proceedings of IWPT.
Ivan Titov and James Henderson. 2006. Loss min-
imization in parse reranking. In Proceedings of
EMNLP, pages 560?567. Association for Computa-
tional Linguistics.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel cky parsing on
gpus. In Proceedings of the 2011 Conference on
Parsing Technologies, Dublin, Ireland, October.
217
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118?123,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improved Typesetting Models for Historical OCR
Taylor Berg-Kirkpatrick Dan Klein
Computer Science Division
University of California, Berkeley
{tberg,klein}@cs.berkeley.edu
Abstract
We present richer typesetting models
that extend the unsupervised historical
document recognition system of Berg-
Kirkpatrick et al (2013). The first
model breaks the independence assump-
tion between vertical offsets of neighbor-
ing glyphs and, in experiments, substan-
tially decreases transcription error rates.
The second model simultaneously learns
multiple font styles and, as a result, is
able to accurately track italic and non-
italic portions of documents. Richer mod-
els complicate inference so we present a
new, streamlined procedure that is over
25x faster than the method used by Berg-
Kirkpatrick et al (2013). Our final sys-
tem achieves a relative word error reduc-
tion of 22% compared to state-of-the-art
results on a dataset of historical newspa-
pers.
1 Introduction
Modern OCR systems perform poorly on histor-
ical documents from the printing-press era, often
yielding error rates that are too high for down-
stream research projects (Arlitsch and Herbert,
2004; Shoemaker, 2005; Holley, 2010). The two
primary reasons that historical documents present
difficultly for automatic systems are (1) the type-
setting process used to produce such documents
was extremely noisy and (2) the fonts used in the
documents are unknown. Berg-Kirkpatrick et al
(2013) proposed a system for historical OCR that
generatively models the noisy typesetting process
of printing-press era documents and learns the font
for each input document in an unsupervised fash-
ion. Their system achieves state-of-the-art results
on the task of historical document recognition.
We take the system of Berg-Kirkpatrick et al
(2013) as a starting point and consider extensions
of the typesetting model that address two short-
comings of their model: (1) their layout model as-
sumes that baseline offset noise is independent for
each glyph and (2) their font model assumes a sin-
gle font is used in every document. Both of these
assumptions are untrue in many historical datasets.
The baseline of the text in printing-press era
documents is not rigid as in modern documents but
rather drifts up and down noisily (see Figure 2).
In practice, the vertical offsets of character glyphs
change gradually along a line. This means the ver-
tical offsets of neighboring glyphs are correlated,
a relationship that is not captured by the original
model. In our first extension, we let the vertical
offsets of character glyphs be generated from a
Markov chain, penalizing large changes in offset.
We find that this extension decreases transcription
error rates. Our system achieves a relative word
error reduction of 22% compared to the state-of-
the-art original model on a test set of historical
newspapers (see Section 4.1), and a 11% relative
reduction on a test set of historical court proceed-
ings.
Multiple font styles are also frequently used in
printing-press era documents; the most common
scenario is for a basic font style to co-occur with
an italic variant. For example, it is common for
proper nouns and quotations to be italicized in
the Old Bailey corpus (Shoemaker, 2005). In our
second extension, we incorporate a Markov chain
over font styles, extending the original model so
that it is capable of simultaneously learning italic
and non-italic fonts within a single document. In
experiments, this model is able to detect which
words are italicized with 93% precision at 74%
recall in a test set of historical court proceedings
(see Section 4.2).
These richer models that we propose do in-
crease the state space and therefore make infer-
ence more costly. To remedy this, we stream-
line inference by replacing the coarse-to-fine in-
ference scheme of Berg-Kirkpatrick et al (2013)
118
ei 1 ei
gi pi
vi
gi 1
vi 1
pi 1
X
PAD
iX
GLYPH
iX
PAD
i 1X
GLYPH
i 1
fi 1 fi
O
r
i
g
i
n
a
l
m
o
d
e
l
S
l
o
w
-
v
a
r
y
I
t
a
l
i
c
{
{
{
Vertical offset
Glyph width
Pad width
Character
Font
Pixels
Figure 1: See Section 2 for a description of the generative process. We consider an extension of Berg-Kirkpatrick et al (2013)
that generates v
i
conditioned on the previous vertical offset v
i?1
(labeled Slow-vary) and an extension that generates a sequence
of font styles f
i
(labeled Italic).
with a forward-cost-augmented beaming scheme.
Our method is over 25x faster on a typical docu-
ment, yet actually yields improved transcriptions.
2 Model
We first describe the generative model used by
the ?Ocular? historical OCR system of Berg-
Kirkpatrick et al (2013)
1
and then describe our
extensions. The graphical model corresponding
to their basic generative process for a single line
of text is diagrammed in Figure 1. A Kneser-
Ney (Kneser and Ney, 1995) character 6-gram lan-
guage model generates a sequence of characters
E = (e
1
, e
2
, . . . , e
n
). For each character index i, a
glyph box width g
i
and a pad box width p
i
are gen-
erated, conditioned on the character e
i
. g
i
specifies
the width of the bounding box that will eventually
house the pixels of the glyph for character e
i
. p
i
specifies the width of a padding box which con-
tains the horizontal space before the next character
begins. Next, a vertical offset v
i
is generated for
the glyph corresponding to character e
i
. v
i
allows
the model to capture variance in the baseline of the
text in the document. We will later let v
i
depend
on v
i?1
, as depicted in Figure 1, but in the baseline
1
The model we describe and extend has two minor dif-
ferences from the one described by Berg-Kirkpatrick et al
(2013). While Berg-Kirkpatrick et al (2013) generate two
pad boxes for each character token, one to the left and one to
the right, we only generate one pad box, always to the right.
Additionally, Berg-Kirkpatrick et al (2013) do not carry over
the language model context between lines, while we do.
system they are independent. Finally, the pixels in
the ith glyph bounding box X
GLYPH
i
are generated
conditioned on the character e
i
, width g
i
, and ver-
tical offset v
i
, and the pixels in the ith pad bound-
ing box X
PAD
i
are generated conditioned on the
width p
i
. We refer the reader to Berg-Kirkpatrick
et al (2013) for the details of the pixel generation
process. We have omitted the token-level inking
random variables for the purpose of brevity. These
can be treated as part of the pixel generation pro-
cess.
Let X denote the matrix of pixels for the entire
line, V = (v
1
, . . . , v
n
), P = (p
1
, . . . , p
n
), and
G = (g
1
, . . . , g
n
). The joint distribution is writ-
ten:
P (X,V, P,G,E) =
P (E) [Language model]
?
n
?
i=1
P (g
i
|e
i
; ?) [Glyph widths]
?
n
?
i=1
P (p
i
|e
i
; ?) [Pad widths]
?
n
?
i=1
P (v
i
) [Vertical offsets]
?
n
?
i=1
P (X
PAD
i
|p
i
) [Pad pixels]
?
n
?
i=1
P (X
GLYPH
i
|v
i
, g
i
, e
i
; ?) [Glyph pixels]
119
Document image:
Learned typsetting
independent offsets:
slow-varying offsets:
Learned typsetting
Figure 2: The first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-SV model. The second line
depicts the same, but for the OCULAR-BEAM model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display
the Bernoulli template probabilities used to generate the observed pixels. The third line shows the corresponding portion of the
input image.
The font is parameterized by the vector ? which
governs the shapes of glyphs and the distributions
over box widths. ? is learned in an unsupervised
fashion. Document recognition is accomplished
via Viterbi decoding over the character random
variables e
i
.
2.1 Slow-varying Offsets
The original model generates the vertical offsets
v
i
independently, and therefore cannot model how
neighboring offsets are correlated. This correla-
tion is actually strong in printing-press era docu-
ments. The baseline of the text wanders in the in-
put image for two reasons: (1) the physical groove
along which character templates were set was un-
even and (2) the original document was imaged in
a way that produced distortion. Both these under-
lying causes are likely to yield baselines that wan-
der slowly up and down across a document. We
refer to this behavior of vertical offsets as slow-
varying, and extend the model to capture it.
In our first extension, we augment the model
by incorporating a Markov chain over the verti-
cal offset random variables v
i
, as depicted in Fig-
ure 1. Specifically, v
i
is generated from a dis-
cretized Gaussian centered at v
i?1
:
P (v
i
|v
i?1
) ? exp
(
(v
i
? v
i?1
)
2
2?
2
)
This means that the if v
i
differs substantially from
v
i?1
, a large penalty is incurred. As a result,
the model should prefer sequences of v
i
that vary
slowly. In experiments, we set ?
2
= 0.05.
2.2 Italic Font Styles
Many of the documents in the Old Bailey corpus
contain both italic and non-italic font styles (Shoe-
maker, 2005). The way that italic fonts are used
depends on the year the document was printed,
but generally italics are reserved for proper nouns,
quotations, and sentences that have a special role
(e.g. the final judgment made in a court case). The
switch between font styles almost always occurs
at space characters.
Our second extension of the typesetting model
deals with both italic and non-italic font styles.
We augment the model with a Markov chain
over font styles f
i
, as depicted in Figure 1.
Each font style token f
i
takes on a value in
{ITALIC, NON-ITALIC} and is generated condi-
tioned on the previous font style f
i?1
and the cur-
rent character token e
i
. Specifically, after generat-
ing a character token that is not a space, the lan-
guage model deterministically generates the last
font used. If the language model generates a space
character token, the decision of whether to switch
font styles is drawn from a Bernoulli distribution.
This ensures that the font style only changes at
space characters.
The font parameters ? are extended to contain
entries for the italic versions of all characters. This
means the shapes and widths of italic glyphs can
be learned separately from non-italic ones. Like
Berg-Kirkpatrick et al (2013), we initialize the
font parameters from mixtures of modern fonts,
using mixtures of modern italic font styles for
italic characters.
3 Streamlined Inference
Inference in our extended typesetting models is
costly because the state space is large; we propose
an new inference procedure that is fast and simple.
Berg-Kirkpatrick et al (2013) used EM to learn
the font parameters ?, and therefore required ex-
pected sufficient statistics (indicators on (e
i
, g
i
, v
i
)
tuples), which they computed using coarse-to-
fine inference (Petrov et al, 2008; Zhang and
Gildea, 2008) with a semi-Markov dynamic pro-
gram (Levinson, 1986). This approach is effec-
120
Document image:
Learned typesetting:
Figure 3: This first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-IT model. Pad boxes are shown
in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities used to generate the observed pixels.
The second line shows the corresponding portion of the input image.
tive, but slow. For example, while transcribing a
typical document consisting of 30 lines of text,
their system spends 63 minutes computing ex-
pected sufficient statistics and decoding when run
on a 4.5GHz 4-core CPU.
We instead use hard counts of the sufficient
statistics for learning (i.e. perform hard-EM). As a
result, we are free to use inference procedures that
are specialized for Viterbi computation. Specif-
ically, we use beam-search with estimated for-
ward costs. Because the model is semi-Markov,
our beam-search procedure is very similar the
one used by Pharaoh (Koehn, 2004) for phrase-
based machine translation, only without a distor-
tion model. We use a beam of size 20, and estimate
forward costs using a character bigram language
model. On the machine mentioned above, tran-
scribing the same document, our simplified system
that uses hard-EM and beam-search spends only
2.4 minutes computing sufficient statistics and de-
coding. This represents a 26x speedup.
4 Results
We ran experiments with four different systems.
The first is our baseline, the system presented
by Berg-Kirkpatrick et al (2013), which we re-
fer to as OCULAR. The second system uses the
original model, but uses beam-search for infer-
ence. We refer to this system as OCULAR-BEAM.
The final two systems use beam-search for infer-
ence, but use extended models: OCULAR-BEAM-
SV uses the slow-varying vertical offset extension
described in Section 2.1 and OCULAR-BEAM-
IT uses the italic font extension described in Sec-
tion 2.2.
We evaluate on two different test sets of histor-
ical documents. The first test set is called Trove,
and is used by Berg-Kirkpatrick et al (2013) for
evaluation. Trove consists of 10 documents that
were printed between 1803 and 1954, each con-
sisting of 30 lines, all taken from a collection of
historical Australian newspapers hosted by the Na-
tional Library of Australia (Holley, 2010). The
second test set, called Old Bailey, consists of 20
documents that were printed between 1716 and
1906, each consisting of 30 lines, all taken from
a the proceedings of the Old Bailey Courthouse
in London (Shoemaker, 2005).
2
Following Berg-
Kirkpatrick et al (2013), we train the language
model using 36 millions words from the New York
Times portion of the Gigaword corpus (Graff et al,
2007).
3
4.1 Document Recognition Performance
We evaluate predicted transcriptions using both
character error rate (CER) and word error rate
(WER). CER is the edit distance between the
guessed transcription and the gold transcription,
divided by the number of characters in the gold
transcription. WER is computed in the same way,
but words are treated as tokens instead of charac-
ters.
First we compare the baseline, OCULAR, to
our system with simplified inference, OCULAR-
BEAM. To our surprise, we found that OCULAR-
BEAM produced better transcriptions than OCU-
LAR. On Trove, OCULAR achieved a WER of
33.0 while OCULAR-BEAM achieved a WER of
30.7. On Old Bailey, OCULAR achieved a WER
of 30.8 while OCULAR-BEAM achieved a WER of
28.8. These results are shown in Table 1, where we
also report the performance of Google Tesseract
(Smith, 2007) and ABBYY FineReader, a state-
of-the-art commercial system, on the Trove test set
(taken from Berg-Kirkpatrick et al (2013)).
Next, we evaluate our slow-varying vertical off-
set model. OCULAR-BEAM-SV out-performs
OCULAR-BEAM on both test sets. On Trove,
OCULAR-BEAM-SV achieved a WER of 25.6,
and on Old Bailey, OCULAR-BEAM-SV achieved
a WER of 27.5. Overall, compared to our baseline
2
Old Bailey is comparable to the the second test set used
by Berg-Kirkpatrick et al (2013) since it is derived from the
same collection and covers a similar time span, but it consists
of different documents.
3
This means the language model is out-of-domain on both
test sets. Berg-Kirkpatrick et al (2013) also consider a per-
fectly in-domain language model, though this setting is some-
what unrealistic.
121
system, OCULAR-BEAM-SV achieved a relative
reduction in WER of 22% on Trove and 11% on
Old Bailey.
By looking at the predicted typesetting layouts
we can make a qualitative comparison between the
vertical offsets predicted by OCULAR-BEAM and
OCULAR-BEAM-SV. Figure 2 shows representa-
tions of the Viterbi estimates of the typesetting
random variables predicted by the models on a
portion of an example document. The first line
is the typesetting layout predicted by OCULAR-
BEAM-SV and the second line is same, but for
OCULAR-BEAM. The locations of padding boxes
are depicted in blue. The white glyph bounding
boxes reveal the values of the Bernoulli template
probabilities used to generate the observed pixels.
The Bernoulli templates are produced from type-
level font parameters, but are modulated by token-
level widths g
i
and vertical offsets v
i
(and ink-
ing random variables, whose description we have
omitted for brevity). The predicted vertical off-
sets are visible in the shifted baselines of the tem-
plate probabilities. The third line shows the corre-
sponding portion of the input image. In this ex-
ample, the text baseline predicted by OCULAR-
BEAM-SV is contiguous, while the one predicted
by OCULAR-BEAM is not. Given how OCULAR-
BEAM-SV was designed, this meets our expecta-
tions. The text baseline predicted by OCULAR-
BEAM has a discontinuity in the middle of its pre-
diction for the gold word Surplus. In contrast,
the vertical offsets predicted by OCULAR-BEAM-
SV at this location vary smoothly and more ac-
curately match the true text baseline in the input
image.
4.2 Font Detection Performance
We ran experiments with the italic font style
model, OCULAR-BEAM-IT, on the Old Bai-
ley test set (italics are infrequent in Trove). We
evaluated the learned styles by measuring how ac-
curately OCULAR-BEAM-IT was able to distin-
guish between italic and non-italic styles. Specifi-
cally, we computed the precision and recall for the
system?s predictions about which words were ital-
icized. We found that, across the entire Old Bai-
ley test set, OCULAR-BEAM-IT was able to detect
which words were italicized with 93% precision
at 74% recall, suggesting that the system did suc-
cessfully learn both italic and non-italic styles.
4
4
While it seems plausible that learning italics could also
improve transcription accuracy, we found that OCULAR-
System CER WER
Trove
Google Tesseract 37.5 59.3
ABBYY FineReader 22.9 49.2
OCULAR (baseline) 14.9 33.0
OCULAR-BEAM 12.9 30.7
OCULAR-BEAM-SV 11.2 25.6
Old Bailey
OCULAR (baseline) 14.9 30.8
OCULAR-BEAM 10.9 28.8
OCULAR-BEAM-SV 10.3 27.5
Table 1: We evaluate the output of each system on two test
sets: Trove, a collection of historical newspapers, and Old
Bailey, a collection of historical court proceedings. We report
character error rate (CER) and word error rate (WER), macro-
averaged across documents.
We can look at the typesetting layout predicted
by OCULAR-BEAM-IT to gain insight into what
has been learned by the model. The first line of
Figure 3 shows the typesetting layout predicted by
the OCULAR-BEAM-IT model for a line of a doc-
ument image that contains italics. The second line
of Figure 3 displays the corresponding portion of
the input document image. From this example,
it appears that the model has effectively learned
separate glyph shapes for italic and non-italic ver-
sions of certain characters. For example, compare
the template probabilities used to generate the d?s
in defraud to the template probabilities used to
generate the d in hard.
5 Conclusion
We began with an efficient simplification of the
state-of-the-art historical OCR system of Berg-
Kirkpatrick et al (2013) and demonstrated two ex-
tensions to its underlying model. We saw an im-
provement in transcription quality as a result of re-
moving a harmful independence assumption. This
suggests that it may be worthwhile to consider still
further extensions of the model, designed to more
faithfully reflect the generative process that pro-
duced the input documents.
Acknowledgments
This work was supported by Grant IIS-1018733
from the National Science Foundation and also a
National Science Foundation fellowship to the first
author.
BEAM-IT actually performed slightly worse than OCULAR-
BEAM. This negative result is possibly due to the extra diffi-
culty of learning a larger number of font parameters.
122
References
Kenning Arlitsch and John Herbert. 2004. Microfilm,
paper, and OCR: Issues in newspaper digitization.
the Utah digital newspapers program. Microform &
Imaging Review.
Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.
2013. Unsupervised transcription of historical doc-
uments. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword third edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Rose Holley. 2010. Trove: Innovation in access to
information in Australia. Ariadne.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Machine translation: From real
users to research, pages 115?124. Springer.
Stephen Levinson. 1986. Continuously variable du-
ration hidden Markov models for automatic speech
recognition. Computer Speech & Language.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing.
Robert Shoemaker. 2005. Digital London: Creating a
searchable web of interlinked sources on eighteenth
century London. Electronic Library and Informa-
tion Systems.
Ray Smith. 2007. An overview of the Tesseract OCR
engine. In Proceedings of the Ninth International
Conference on Document Analysis and Recognition.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
123
