Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 57?64,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Conversational QA: Automatic Identification of Problematic
Situations and User Intent ?
Joyce Y. Chai Chen Zhang Tyler Baldwin
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{jchai, zhangch6, baldwi96}@cse.msu.edu
Abstract
To enable conversational QA, it is impor-
tant to examine key issues addressed in
conversational systems in the context of
question answering. In conversational sys-
tems, understanding user intent is criti-
cal to the success of interaction. Recent
studies have also shown that the capabil-
ity to automatically identify problematic
situations during interaction can signifi-
cantly improve the system performance.
Therefore, this paper investigates the new
implications of user intent and problem-
atic situations in the context of question
answering. Our studies indicate that, in
basic interactive QA, there are different
types of user intent that are tied to dif-
ferent kinds of system performance (e.g.,
problematic/error free situations). Once
users are motivated to find specific infor-
mation related to their information goals,
the interaction context can provide useful
cues for the system to automatically iden-
tify problematic situations and user intent.
1 Introduction
Interactive question answering (QA) has been
identified as one of the important directions in QA
research (Burger et al, 2001). One ultimate goal is
to support intelligent conversation between a user
and a QA system to better facilitate user informa-
tion needs. However, except for a few systems that
use dialog to address complex questions (Small et
al., 2003; Harabagiu et al, 2005), the general di-
alog capabilities have been lacking in most ques-
?This work was partially supported by IIS-0347548 from
the National Science Foundation.
tion answering systems. To move towards conver-
sational QA, it is important to examine key issues
relevant to conversational systems in the context
of interactive question answering.
This paper focuses on two issues related to con-
versational QA. The first issue is concerned with
user intent. In conversational systems, understand-
ing user intent is the key to the success of the inter-
action. In the context of interactive QA, one ques-
tion is what type of user intent should be captured.
Unlike most dialog systems where user intent can
be characterized by dialog acts such as question,
reply, and statement, in interactive QA, user in-
puts are already in the form of question. Then
the problems become whether there are different
types of intent behind these questions that should
be handled differently by a QA system and how to
automatically identify them.
The second issue is concerned with problem-
atic situations during interaction. In spoken di-
alog systems, many problematic situations could
arise from insufficient speech recognition and lan-
guage understanding performance. Recent work
has shown that the capability to automatically
identify problematic situations (e.g., speech recog-
nition errors) can help control and adapt dialog
strategies to improve performance (Litman and
Pan, 2000). Similarly, QA systems also face chal-
lenges of technology limitation from language un-
derstanding and information retrieval. Thus one
question is, in the context of interactive QA, how
to characterize problematic situations and auto-
matically identify them when they occur.
In interactive QA, these two issues are inter-
twined. Questions formed by a user not only de-
pend on his/her information goals, but are also in-
fluenced by the answers from the system. Prob-
lematic situations will impact user intent in the
57
follow-up questions, which will further influence
system performance. Both the awareness of prob-
lematic situations and understanding of user in-
tent will allow QA systems to adapt better strate-
gies during interaction and move towards intelli-
gent conversational QA.
To address these two questions, we conducted
a user study where users interacted with a con-
trolled QA system to find information of inter-
est. These controlled studies allowed us to fo-
cus on the interaction aspect rather than informa-
tion retrieval or answer extraction aspects. Our
studies indicate that in basic interactive QA where
users always ask questions and the system always
provides some kind of answers, there are differ-
ent types of user intent that are tied to differ-
ent kinds of system performance (e.g., problem-
atic/error free situations). Once users are moti-
vated to find specific information related to their
information goals, the interaction context can pro-
vide useful cues for the system to automatically
identify problematic situations and user intent.
2 Related Work
Open domain question answering (QA) systems
are designed to automatically locate answers from
large collections of documents to users? natural
language questions. In the past few years, au-
tomated question answering techniques have ad-
vanced tremendously, partly motivated by a se-
ries of evaluations conducted at the Text Retrieval
Conference (TREC) (Voorhees, 2001; Voorhees,
2004). To better facilitate user information needs,
recent trends in QA research have shifted towards
complex, context-based, and interactive question
answering (Voorhees, 2001; Small et al, 2003;
Harabagiu et al, 2005). For example, NIST initi-
ated a special task on context question answering
in TREC 10 (Voorhees, 2001), which later became
a regular task in TREC 2004 (Voorhees, 2004) and
2005. The motivation is that users tend to ask a
sequence of related questions rather than isolated
single questions to satisfy their information needs.
Therefore, the context QA task was designed to
investigate the system capability to track context
through a series of questions. Based on context
QA, some work has been done to identify clarifica-
tion relations between questions (Boni and Man-
andhar, 2003). However context QA is different
from interactive QA in that context questions are
specified ahead of time rather than incrementally
as in an interactive setting.
Interactive QA has been applied to process com-
plex questions. For analytical and non-factual
questions, it is hard to anticipate answers. Clari-
fication dialogues can be applied to negotiate with
users about the intent of their questions (Small et
al., 2003). Recently, an architecture for interactive
question answering has been proposed based on a
notion of predictive questioning (Harabagiu et al,
2005). The idea is that, given a complex ques-
tion, the system can automatically identify a set of
potential follow-up questions from a large collec-
tion of question-answer pairs. The empirical re-
sults have shown the system with predictive ques-
tioning is more efficient and effective for users to
accomplish information seeking tasks in a partic-
ular domain (Harabagiu et al, 2005).
The work reported in this paper addresses a
different aspect of interactive question answering.
Both issues raised earlier (Section 1) are inspired
by earlier work on intelligent conversational sys-
tems. Automated identification of user intent has
played an important role in conversational sys-
tems. Tremendous amounts of work has focused
on this aspect (Stolcke et al, 2000). To improve
dialog performance, much effort has also been put
on techniques to automatically detect errors during
interaction. It has shown that during human ma-
chine dialog, there are sufficient cues for machines
to automatically identify error conditions (Levow,
1998; Litman et al, 1999; Hirschberg et al, 2001;
Walker et al, 2002). The awareness of erroneous
situations can help systems make intelligent de-
cisions about how to best guide human partners
through the conversation and accomplish the tasks.
Motivated by these earlier studies, the goal of this
paper is to investigate whether these two issues can
be applied in question answering to facilitate intel-
ligent conversational QA.
3 User Studies
We conducted a user study to collect data concern-
ing user behavior in a basic interactive QA set-
ting. We are particularly interested in how users
respond to different system performance and its
implication in identifying problematic situations
and user intent. As a starting point, we charac-
terize system performance as either problematic,
which indicates the answer has some problem, or
error-free, which indicates the answer is correct.
In this section, we first describe the methodology
58
and the system used in this effort and then discuss
the observed user behavior and its relation to prob-
lematic situations and user intent.
3.1 Methodology and System
The system used in our experiments has a user in-
terface that takes a natural language question and
presents an answer passage. Currently, our inter-
face only presents to the user the top one retrieved
result. This simplification on one hand helps us
focus on the investigation of user responses to dif-
ferent system performances and on the other hand
represents a possible situation where a list of po-
tential answers may not be practical (e.g., through
PDA or telephone line).
We implemented a Wizard-of-Oz (WOZ) mech-
anism in the interaction loop to control and simu-
late problematic situations. Users were not aware
of the existence of this human wizard and were
led to believe they were interacting with a real
QA system. This controlled setting allowed us
to focus on the interaction aspect rather than in-
formation retrieval or answer extraction aspect of
question answering. More specifically, during in-
teraction after each question was issued, a ran-
dom number generator was used to decide if a
problematic situation should be introduced. If
the number indicated no, the wizard would re-
trieve a passage from a database with correct ques-
tion/answer pairs. Note that in our experiments
we used specific task scenarios (described later),
so it was possible to anticipate user information
needs and create this database. If the number in-
dicated that a problematic situation should be in-
troduced, then the Lemur retrieval engine 1 was
used on the AQUAINT collection to retrieve the
answer. Our assumption is that AQUAINT data
are not likely to provide an exact answer given our
specific scenarios, but they can provide a passage
that is most related to the question. The use of the
random number generator was to control the ratio
between the occurrence of problematic situations
and error-free situations. In our initial investiga-
tion, since we are interested in observing user be-
havior in problematic situations, we set the ratio as
50/50. In our future work, we will vary this ratio
(e.g., 70/30) to reflect the performance of state-of-
the-art factoid QA and investigate the implication
of this ratio in automated performance assessment.
1http://www-2.cs.cmu.edu/ lemur/
3.2 Experiments
Eleven users participated in our study. Each user
was asked to interact with our system to com-
plete information seeking tasks related to four
specific scenarios: the 2004 presidential debates,
Tom Cruise, Hawaii, and Pompeii. The exper-
imental scenarios were further divided into two
types: structured and unstructured. In the struc-
tured task scenarios (for topics Tom Cruise and
Pompeii), users had to fill in blanks on a dia-
gram pertaining to the given topic. Using the dia-
gram was to avoid the influence of these scenarios
on the language formation of the relevant ques-
tions. Because users must find certain informa-
tion, they were constrained in the range of ques-
tions in which they could ask, but not the way they
ask those questions. The task was completed when
all of the blanks on the diagram were filled. The
structured scenarios were designed to mimic the
real information seeking practice in which users
have real motivation to find specific information
related to their information goals. In the unstruc-
tured scenarios (for topics the 2004 presidential
debates and Hawaii), users were given a general
topic to investigate, but were not required to find
specific information. This gave the user the abil-
ity to ask a much wider range of questions than
the structured scenarios. Users were generally in
an exploration mode when performing these un-
structured tasks. They were not motivated to find
specific information and were content with any in-
formation provided by the system. In our view,
the unstructured scenarios are less representative
of the true information seeking situations.
3.3 Observations and Analysis
From our studies, a total of 44 interaction sessions
with 456 questions were collected. Figure 1 shows
an example of a fragment of interaction related to
Tom Cruise. In this example, both problematic sit-
uations applied to answers (e.g., Problematic and
Error-Free) and user intent (described later) ap-
plied to questions are annotated.
There are several observations from this data.
First, questions formed during interactive QA tend
to be self-contained and free of definite noun
phrases, pronouns, or ellipsis. Only one question
in the entire data set has a pronoun (i.e., What
are the best movies with Tom Cruise in them?).
Even in this case, the pronoun them did not re-
fer to any entities that occurred previously in the
59
# Question/Answer Annotation
Q1 When was Tom Cruise born?
Q2 What year was Tom Cruise born? Re-try
Q3 What does Tom Cruise do for a living? Continue
Q4 What is Tom Cruise's profession? Re-try
Q5 What are the best movies with Tom Cruise 
in them?
Switch
A1 ? Based on the memoir of combat veteran 
Ron Kovic, the film stars Tom Cruise as 
Kovic, whose gunshot wound in Vietnam 
left him paralyzed from the chest down.
?.a powerfully intimate portrait that 
unfolds on an epic scale, Born on the 
Fourth of July is arguably Stone's best film 
(if you can forgive its often strident tone), 
?..
Proble-
matic
A2 ?Thomas Cruise Mapother IV was born
on the 3rd of July, 1962 (eerily similar to 
his film Born on the 4th of July), in 
Syracuse, New York. He was the only boy
of four children?.
Error-Free
A3 ?you get a very nice role that shows you
differently, not the heavy, and you're
working with a George Clooney or a Tom
Cruise or a Nicolas Cage or a Martin
Scorsese. I can live with that ?
Proble-
matic
A4 ?So we may agree that Cruise?s
professional standing accounts for some 
measure of his fame?.
Proble-
matic
A5 ?Cruise proved his dramatic talents in the 
1988 drama Rain Man, where he co-
starred with Oscar-winner Dustin Hoffman. 
Oliver Stone's Born on the Fourth of July
(1989) earned him a Best Actor Oscar 
nomination for his hard-hitting portrayal of 
anti-war activist Ron Kovic?.
Error-Free
Figure 1: An example fragment of interaction
QA process. This phenomenon could be caused by
how the answers are presented. Unlike specific an-
swer entities, the answer passages provided by our
system do not support the natural use of referring
expressions in the follow-up questions. Another
possible explanation could be that in an interac-
tive environment, users seem to be more aware of
the potential limitation of a computer system and
thus tend to specify self-contained questions in a
hope to reduce the system?s inference load.
The second observation is about user behavior
in response to different system performances (i.e.,
problematic or error-free situations). We were
hoping to see different strategies users might ap-
ply to deal with the problematic situations. How-
ever, based on the data, we found that when a prob-
lem occurred, users either rephrased their ques-
tions (i.e., the same question expressed in a dif-
ferent way) or gave up the question and went on
specifying a new question. (Here we use Rephrase
and New to denote these two kinds of behaviors.)
We have not observed any sub-dialogs initiated by
Problematic Error-free Total
New Switch Continue
unstruct. 29 90 119
struct. 29 133 162
entire 58 223 281
Rephrase Re-try Negotiate
unstruct. 19 4 23
struct. 102 6 108
entire 121 10 131
Total-unst 48 94 142
Total-st 131 139 270
Total-ent 179 233 412
Table 1: Categorization of user intent with the cor-
responding number of occurrences from the un-
structured scenarios, the structured scenarios, and
the entire dataset.
the user to clarify a previous question or answer.
One possible explanation is that the current inves-
tigation was conducted in a basic interactive mode
where the system was only capable of providing
some sort of answers. This may limit users? expec-
tation in the kind of questions that can be handled
by the system. Our assumption is that, once the
QA system becomes more intelligent and able to
carry on conversation, different types of questions
(i.e., other than rephrase or new) will be observed.
This hypothesis certainly needs to be validated in
a conversational setting.
The third observation is that the rephrased ques-
tions seem to strongly correlate with problematic
situations, although not always. New questions
cannot distinguish a problematic situation from
an error-free situation. Table 1 shows the statis-
tics from our data about different combinations
of new/rephrase questions and performance situ-
ations2. What is interesting is that these different
combinations can reflect different types of user in-
tent behind the questions. More specifically, given
a question, four types of user intent can be cap-
tured with respect to the context (e.g., the previous
question and answer)
Continue indicates that the user is satisfied with
the previous answer and now moves on to this
new question.
Switch indicates that the user has given up on the
previous question and now moves on to this
2The last question from each interaction session is not in-
cluded in these statistics because there is no follow-up ques-
tion after that.
60
new question.
Re-try indicates that the user is not satisfied with
the previous answer and now tries to get a
better answer.
Negotiate indicates that the user is not satisfied
with the previous answer (although it ap-
pears to be correct from the system?s point
of view) and now tries to get a better answer
for his/her own needs.
Table 1 summarizes these different types of
intent together with the number of correspond-
ing occurrences from both structured and unstruc-
tured scenarios. Since in the unstructured sce-
narios it was hard to anticipate user?s questions
and therefore take a correct action to respond to a
problematic/error-free situation, the distribution of
these two situations is much more skewed than the
distribution for the structured scenarios. Also as
mentioned earlier, in unstructured scenarios, users
lacked the motivation to pursue specific informa-
tion, so the ratio between switch and re-try is much
larger than that observed in the structured scenar-
ios. Nevertheless, we did observe different user
behavior in response to different situations. As
discussed later in Section 5, identifying these fine-
grained intents will allow QA systems to be more
proactive in helping users find satisfying answers.
4 Automatic Identification of
Problematic Situations and User Intent
Given the discussion above, the next question is
how to automatically identify problematic situa-
tions and user intent. We formulate this as a classi-
fication problem. Given a question Qi, its answer
Ai, and the follow-up question Qi+1:
(1) Automatic identification of problematic situa-
tions is to decide whether Ai is problematic (i.e.,
correct or incorrect) based on the follow-up ques-
tion Qi+1 and the interaction context. This is a
binary classification problem.
(2) Automatic identification of user intent is to
identify the intent of Qi+1 given the interaction
context. Because we only have very limited in-
stances of Negotiate (see Table 1), we currently
merge Negotiate with Re-try since both of them
represent a situation where a better answer is re-
quested. Thus, this problem becomes a trinary
classification problem.
To build these classifiers, we identified a set of
features, which are illustrated next.
4.1 Features
Given a question Qi, its answer Ai, and the follow-
up question Qi+1, the following set of features are
used:
Target matching(TM): a binary feature indicat-
ing whether the target type of Qi+1 is the same as
the target type of Qi. Our data shows that the rep-
etition of the target type may indicate a rephrase,
which could signal a problematic situation has just
happened.
Named entity matching (NEM): a binary feature
indicating whether all the named entities in Qi+1
also appear in Qi. If no new named entity is in-
troduced in Qi+1, it is likely Qi+1 is a rephrase of
Qi.
Similarity between questions (SQ): a numeric
feature measuring the similarity between Qi+1 and
Qi. Our assumption is that the higher the simi-
larity is, the more likely the current question is a
rephrase to the previous one.
Similarity between content words of questions
(SQC): this feature is similar to the previous fea-
ture (i.e., SQ) except that the similarity measure-
ment is based on the content words excluding
named entities. This is to prevent the similarity
measurement from being dominated by the named
entities.
Similarity between Qi and Ai (SA): this feature
measures how close the retrieved passage matches
the question. Our assumption is that although a re-
trieved passage is the most relevant passage com-
pared to others, it still may not contain the answer
(e.g., when an answer does not even exist in the
data collection).
Similarity between Qi and Ai based on the con-
tent words (SAC): this feature is essentially the
same as the previous feature (SA) except that the
similarity is calculated after named entities are re-
moved from the questions and answers.
Note that since our data is currently collected
from simulation studies, we do not have the confi-
dence score from the retrieval engine associated
with every answer. In practice, the confidence
score can be used as an additional feature.
Since our focus is not on the similarity measure-
ment but rather the use of the measurement in the
classification models, our current similarity mea-
surement is based on a simple approach that mea-
sures commonality and difference between two
objects as proposed by Lin (1998). More specifi-
cally, the following equation is applied to measure
61
the similarity between two chunks of text T
1
and
T
2
:
sim
1
(T
1
, T
2
) =
? logP (T
1
? T
2
)
? logP (T
1
? T
2
)
Assume the occurrence of each word is indepen-
dent, then:
sim
1
(T
1
, T
2
) =
?
?
w?T
1
?T
2
log P (w)
?
?
w?T
1
?T
2
log P (w)
where P (w) was calculated based on the data used
in the previous TREC evaluations.
4.2 Identification of Problematic Situations
To identify problematic situations, we experi-
mented with three different classifiers: Maxi-
mum Entropy Model (MEM) from MALLET3,
SVM from SVM-Light4, and Decision Trees from
WEKA5. A leave-one-out validation was applied
where one interaction session was used for testing
and the remaining interaction sessions were used
for training.
Table 2 shows the performance of the three
models based on different combinations of fea-
tures in terms of classification accuracy. The base-
line result is the performance achieved by sim-
ply assigning the most frequently occurred class.
For the unstructured scenarios, the performance
of the classifiers is rather poor, which indicates
that it is quite difficult to make any generaliza-
tion based on the current feature sets when users
are less motivated in finding specific information.
For the structured scenarios, the best performance
for each model is highlighted in bold in Table 2.
The Decision Tree model achieves the best per-
formance of 77.8% in identifying problematic sit-
uations, which is more than 25% better than the
baseline performance.
4.3 Identification of User Intent
To identify user intent, we formulate the problem
as follows: given an observation feature vector f
where each element of the vector corresponds to
a feature described earlier, the goal is to identify
an intent c? from a set of intents I ={Continue,
Switch, Re-try/Negotiate} that satisfies the follow-
ing equation:
c? = argmaxc?IP (c|f)
3http://mallet.cs.umass.edu/index.php/
4http://svmlight.joachims.org/
5http://www.cs.waikato.ac.nz/ml/weka/
Our assumption is that user intent for a ques-
tion can be potentially influenced by the intent
from a preceding question. For example, Switch
is likely to follow Re-try. Therefore, we have im-
plemented a Maximum Entropy Markov Model
(MEMM) (McCallum et al, 2000) to take the se-
quence of interactions into account.
Given a sequence of questions Q
1
, Q
2
, up to Qt,
there is an observation feature vector f
i
associated
with each Qi. In MEMM, the prediction of user
intent ct for Qt not only depends on the observa-
tion f
t
, but also the intent ct?1 from the preceding
question Qt?1. In fact, this approach finds the best
sequence of user intent C? for Q
1
up to Qt based
on a sequence of observations f
1
, f
2
, ..., ft as fol-
lows:
C? = argmaxC?ItP (C|f1, f2, ..., ft)
where C is a sequence of intent and It is the set of
all possible sequences of intent with length t.
To find this sequence of intent C?, MEMM
keeps a variable ?t(i) which is defined to be the
maximum probability of seeing a particular se-
quence of intent ending at intent i (i ? I) for
question Qt, given the observation sequence for
questions Q
1
up to Qt:
?t(i) = maxc
1
,...,c
t?1
P (c
1
, . . . , ct?1, ct = i|f1, . . . , ft)
This variable can be calculated by a dynamic
optimization procedure similar to the Viterbi algo-
rithm in the Hidden Markov Model:
?t(i) = maxj ?t?1(j) ? P (ct = i|ct?1 = j, ft)
where P (ct = i|ct?1 = j, ft) is estimated by the
Maximum Entropy Model.
Table 3 shows the best results of identifying
user intent based on the Maximum Entropy Model
and MEMM using the leave-one-out approach.
The results have shown that both models did not
work for the data collected from unstructured sce-
narios (i.e., the baseline accuracy for intent iden-
tification is 63.4%). For structured scenarios, in
terms of the overall accuracy, both models per-
formed significantly better than the baseline (i.e.,
49.3%). The MEMM worked only slightly better
than the MEM. Given our limited data, it is not
conclusive whether the transitions between ques-
tions will help identify user intent in a basic inter-
active mode. However, we expect to see more in-
fluence from the transitions in fully conversational
QA.
62
MEM SVM DTree
Features un s ent un s ent un s ent
Baseline 66.2 51.5 56.3 66.2 51.5 56.3 66.2 51.5 56.3
TM, SQC 50.0 57.4 54.9 53.5 60.0 57.8 53.5 55.9 55.1
NEM, SQC 37.3 74.4 61.7 37.3 74.4 61.7 37.3 74.4 61.7
TM, SQ 61.3 64.8 63.6 57.0 64.1 61.7 59.9 64.4 62.9
NEM, SQC, SAC 40.8 76.7 64.3 38.0 74.4 61.9 49.3 77.8 68.0
TM, SQ, SAC 59.2 67.4 64.6 61.3 66.3 64.6 62.7 65.6 64.6
TM, NEM, SQC 54.2 75.2 68.0 54.2 75.2 68.0 53.5 74.4 67.2
TM, SQ, SA 63.4 71.9 68.9 58.5 71.5 67.0 67.6 75.6 72.8
TM, NEM, SQC, SAC 54.9 75.6 68.4 54.2 75.2 68.0 55.6 74.4 68.0
* un - unstructured, s - structured, ent - entire
Table 2: Performance of automatic identification of problematic situations
MEM MEMM
un s un s
CONTINUE P 64.4 69.7 67.3 70.8
R 96.7 85.8 80.0 88.8
F 77.3 76.8 73.1 78.7
RE-TRY P 28.6 76.2 37.1 79.0
/NEGOTIATE R 8.7 74.1 56.5 73.1
F 13.3 75.1 44.8 75.9
SWITCH P - - - 50.0
R 0 0 0 3.6
F - - - 6.7
Overall accuracy 62.7 72.2 59.9 73.7
* un - unstructured, s - structured
Table 3: Performance of automatic identification
of user intent
5 Implications of Problematic Situations
and User Intent
Automated identification of problematic situations
and user intent have potential implications in the
design of conversational QA systems. Identifica-
tion of problematic situations can be considered as
implicit feedback. The system can use this feed-
back to improve its answer retrieval performance
and proactively adapt its strategy to cope with
problematic situations. One might think that an
alternative way is to explicitly ask users for feed-
back. However, this explicit approach will defeat
the purpose of intelligent conversational systems.
Soliciting feedback after each question not only
will frustrate users and lengthen the interaction,
but also will interrupt the flow of user thoughts and
conversation. Therefore, our focus here is to inves-
tigate the more challenging end of implicit feed-
back. In practice, the explicit feedback and im-
plicit feedback should be intelligently combined.
For example, if the confidence for automatically
identifying a problematic situation or an error-free
situation is low, then perhaps explicit feedback can
be solicited.
Automatic identification of user intent also has
important implications in building intelligent con-
versational QA systems. For example, if Con-
tinue is identified during interaction, then the sys-
tem can automatically collect the question answer
pairs for potential future use. If Switch is identi-
fied, the system may put aside the question that has
not been correctly answered and proactively come
back to that question later after more information
is gathered. If Re-try is identified, the system may
avoid repeating the same answer and at the same
time may take the initiative to guide users on how
to rephrase a question. If Negotiate is identified,
the system may want to investigate the user?s par-
ticular needs that may be different from the gen-
eral needs. Overall, different strategies can be de-
veloped to address problematic situations and dif-
ferent intents. We will investigate these strategies
in our future work.
This paper reports our initial effort in investi-
gating interactive QA from a conversational point
of view. The current investigation has several
simplifications. First, our current work has fo-
cused on factoid questions where it is relatively
easy to judge a problematic or error-free situation.
However, as discussed in earlier work (Small et
al., 2003), sometimes it is very hard to judge the
truthfulness of an answer, especially for analyti-
cal questions. Therefore, our future work will ex-
amine the new implications of problematic situa-
tions and user intent for analytical questions. Sec-
63
ond, our current investigation is based on a ba-
sic interactive mode. As mentioned earlier, once
the QA systems become more intelligent and con-
versational, more varieties of user intent are an-
ticipated. How to characterize and automatically
identify more complex user intent under these dif-
ferent situations is another direction of our future
work.
6 Conclusion
This paper presents our initial investigation on
automatic identification of problematic situations
and user intent in interactive QA. Our results have
shown that, once users are motivated in finding
specific information related to their information
goals, user behavior and interaction context can
help automatically identify problematic situations
and user intent. Although our current investigation
is based on the data collected from a controlled
study, the same approaches can be applied dur-
ing online processing as the question answering
proceeds. The identified problematic situations
and/or user intent will provide immediate feed-
back for a QA system to adjust its behavior and
adapt better strategies to cope with different situa-
tions. This is an important step toward intelligent
conversational question answering.
References
Marco De Boni and Suresh Manandhar. 2003. An
analysis of clarification dialogues for question an-
swering. In Proceedings of HLT-NAACL 2003,
pages 48?55.
John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
George Miller, Dan Moldovan, Bill Ogden, John
Prager, Ellen Riloff, Amit Singhal, Rohini Shrihari,
Tomek Strzalkowski, Ellen Voorhees, and Ralph
Weishedel. 2001. Issues, tasks and program struc-
tures to roadmap research in question & answering.
In NIST Roadmap Document.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005. Experiments with interactive
question-answering. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 205?214, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Julia Hirschberg, Diane J. Litman, and Marc Swerts.
2001. Identifying user corrections automatically
in spoken dialogue systems. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?01).
Gina-Anne Levow. 1998. Characterizeing and recog-
nizing spoken corrections in human-computer dia-
logue. In Proceedings of the 36th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL-98), pages 736?742.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning, Madison, Wis-
consin, July.
Diane J. Litman and Shimei Pan. 2000. Predicting
and adapting to poor speech recognition in a spo-
ken dialogue system. In Proceedings of the Seven-
teenth National Conference on Artificial Intelligence
(AAAI-2000), pages 722?728.
Diane J. Litman, Marilyn A. Walker, and Michael S.
Kearns. 1999. Automatic detection of poor speech
recognition at the dialogue level. In Proceedings of
the 37th Annual meeting of the Association of Com-
putational Linguistics (ACL-99), pages 309?316.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov mod-
els for information extraction and segmentation. In
Proceedings of Internatioanl Conference on Ma-
chine Learning (ICML 2000), pages 591?598.
Sharon Small, Ting Liu, Nobuyuki Shimizu, and
Tomek Strzalkowski. 2003. HITIQA: An interac-
tive question answering system: A preliminary re-
port. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answer-
ing.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Marie Meteer, and Carol Van
Ess-Dykema. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. In Computational Linguistics, volume 26.
Ellen Voorhees. 2001. Overview of TREC 2001 ques-
tion answering track. In Proceedings of TREC.
Ellen Voorhees. 2004. Overview of TREC 2004. In
Proceedings of TREC.
Marilyn Walker, Irene Langkilde-Geary, Helen Wright
Hastie, Jerry Wright, and Allen Gorin. 2002. Auto-
matically training a problematic dialogue predictor
for the HMIHY spoken dialog system. In Journal of
Artificial Intelligence Research.
64
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 206?215,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
What do We Know about Conversation Participants: Experiments on
Conversation Entailment
Chen Zhang Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{zhangch6, jchai}@cse.msu.edu
Abstract
Given the increasing amount of conversa-
tion data, techniques to automatically ac-
quire information about conversation par-
ticipants have become more important.
Towards this goal, we investigate the prob-
lem of conversation entailment, a task
that determines whether a given conversa-
tion discourse entails a hypothesis about
the participants. This paper describes
the challenges related to conversation en-
tailment based on our collected data and
presents a probabilistic framework that in-
corporates conversation context in entail-
ment prediction. Our preliminary exper-
imental results have shown that conver-
sation context, in particular dialogue act,
plays an important role in conversation en-
tailment.
1 Introduction
Conversation is a joint activity between its partic-
ipants (Clark, 1996). Their goals and their under-
standing of mutual beliefs of each other shape the
linguistic discourse of conversation. In turn, this
linguistic discourse provides tremendous informa-
tion about conversation participants. Given the
increasing amount of available conversation data
(e.g., conversation scripts such as meeting scripts,
court records, and online chatting), an important
question is what do we know about conversation
participants? The capability to automatically ac-
quire such information can benefit many appli-
cations, for example, development of social net-
works and discovery of social dynamics.
Related to this question, previous work has de-
veloped techniques to extract profiling informa-
tion about participants from conversation inter-
views (Jing et al, 2007) and to automatically iden-
tify dynamics between conversation participants
such as agreement/disagreement from multiparty
meeting scripts (Galley et al, 2004). We approach
this question from a different angle as a conversa-
tion entailment problem: given a conversation dis-
course D and a hypothesis H concerning its par-
ticipant, the goal is to identify whether D entails
H. For instance, in the following example, the first
hypothesis can be entailed from the dialogue seg-
ment while the second hypothesis cannot.
Example 1:
Dialogue Segment:
A: And where about were you born?
B: Up in Person Country.
Hypothesis:
(1) B was born in Person Country.
(2) B lives in Person Country.
Inspired by textual entailment (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007), conversation entailment provides an inter-
mediate step towards acquiring information about
conversation participants. What we should know
or would like to know about a participant can be
rather open. The type of information needed about
participants is also application-dependent and dif-
ficult to generalize. In conversation entailment, we
will not face this problem since hypotheses can be
used to express any type of information about a
participant one might be interested in. Although
hypotheses are currently given in our investiga-
tion, they can potentially be automatically gener-
ated based on information needs and/or theories
on cognitive status/mental models of conversation
participants. The capability to make correct entail-
ment judgements based on these hypotheses will
benefit many applications such as information ex-
traction, question answering, and summarization.
As a first step in our investigation, we collected
a corpus of conversation entailment data from
nineteen human annotators. Our data showed that
conversation entailment is more challenging than
206
the textual entailment task due to unique charac-
teristics about conversation and conversational im-
plicature. To predict entailment, we developed a
probabilisitic framework that incorporates seman-
tic representation of conversation context. Our
preliminary experimental results have shown that
conversation context, in particular dialogue acts,
play an important role in conversation entailment.
2 Related Work
Recent work has applied different approaches
to acquire information about conversation par-
ticipants based on human-human conversation
scripts, for example, to extract profiling infor-
mation from conversation interviews (Jing et al,
2007) and to identify agreement/disagreement
between participants from multiparty meeting
scripts (Galley et al, 2004). In human-machine
conversation, inference about conversation partic-
ipants has been studied as a part of user modeling.
For example, earlier work has investigated infer-
ence of user intention from utterances to control
clarification dialogue (Horvitz and Paek, 2001)
and recognition of user emotion and attitude from
utterances for intelligent tutoring systems (Litman
and Forbes-Riley, 2006). In contrast to previous
work, we propose a new angle to address informa-
tion acquisition about conversation participants,
namely, through conversation entailment.
This work is inspired by a large body of recent
work on textual entailment initiated by the PAS-
CAL RTE Challenge (Dagan et al, 2005; Bar-
Haim et al, 2006; Giampiccolo et al, 2007). Nev-
ertheless, conversation discourse is very different
from written monologue discourse. The conversa-
tion discourse is shaped by the goals of its partici-
pants and their mutual beliefs. The key distinctive
features include turn-taking between participants,
grounding between participants, and different lin-
guistic phenomena of utterances (e.g., utterances
in conversation tend to be shorter, with disfluency,
and sometimes incomplete or ungrammatical). It
is the goal of this paper to explore how techniques
developed for textual entailment can be extended
to address these unique behaviors in conversation
entailment.
3 Experimental Data
The first step in our investigation is to collect en-
tailment data to help us better understand the prob-
lem and facilitate algorithm development and eval-
uation.
3.1 Data Collection Procedure
We selected 50 dialogues from the Switchboard
corpus (Godfrey and Holliman, 1997). In each of
these dialogues, two participants discuss a topic
of interest (e.g., sports activities, corporate cul-
ture, etc.). To focus our work on the entailment
problem, we use the transcribed scripts of the di-
alogues in our experiments. We also make use of
available annotations such as syntactic structures,
disfluency markers, and dialogue acts.
We had 15 volunteer annotators read the se-
lected dialogues and create hypotheses about par-
ticipants. As a result, a total of 1096 entailment
examples were created. Each example consists of
a snippet from the dialogue (referred to as dia-
logue segment in the rest of this paper), a hypothe-
sis statement, and a truth value indicating whether
the hypothesis can be inferred from the snippet
given the whole history of that dialogue session.
During annotation, we asked the annotators to pro-
vide balanced examples for each dialogue. That is,
roughly half of the hypotheses are truly entailed
and half are not. Special attention was given to
negative entailment examples. Since any arbitrary
hypotheses that are completely irrelevant can be
negative examples, a special criteria is enforced
that any negative examples should have a major-
ity word overlap with the snippet. In addition, in-
spired by previous work (Jing et al, 2007; Galley
et al, 2004), we particularly asked annotators to
provide hypotheses that address the profiling in-
formation of the participants, their opinions and
desires, as well as the dynamic communicative re-
lations between participants.
A recent study shows that for many NLP an-
notation tasks, the reliability of a small number
of non-expert annotations is on par with that of
an expert annotator (Snow et al, 2008). It also
found that for tasks such as affection recogni-
tion, an average of four non-expert labels per item
are capable of emulating expert-level label qual-
ity. Based on this finding, in our study the en-
tailment judgement for each example was further
independently annotated by four annotators (who
were not the original contributors of the hypothe-
ses). As a result, on average each entailment ex-
ample (i.e., a pair of snippet and hypothesis) re-
ceived five judgements.
207
Figure 1: Agreement histogram of entailment
judgements
3.2 Data and Examples
Figure 1 shows a histogram of the agreements of
collected judgements. It indicates that conversa-
tion entailment is in fact a quite difficult task even
for humans. Only 53% of all the examples (586
out of 1096) are agreed upon by all human annota-
tors. The disagreement between users sometimes
is caused by language ambiguity since conversa-
tion scripts are often short and without clear sen-
tence boundaries. For example,
Example 2:
Dialogue Segment:
A: Margaret Thatcher was prime minister, uh,
uh, in India, so many, uh, women are heads
of state.
Hypothesis:
A believes that Margaret Thatcher was prime
minister of India.
In the utterance of speaker A, the prepositional
phrase in India is ambiguous because it can either
be attached to the preceding sentence, which suffi-
ciently entails the hypothesis; or it can be attached
to the succeeding sentence, which leaves it unclear
which country A believes Margaret Thatcher was
prime minister of.
Difference in recognition and handling of con-
versational implicature is another issue that led to
disagreement among annotators. For example:
Example 3:
Dialogue Segment:
A: Um, I had a friend who had fixed some, uh,
chili, buffalo chili and, about a week before
we went to see the movie.
Hypothesis:
A ate some buffalo chili.
Example 4:
Dialogue Segment:
B: Um, I?ve visited the Wyoming area. I?m
not sure exactly where Dances with Wolves
was filmed.
Hypothesis:
B thinks Dances with Wolves was filmed in
Wyoming.
In the first example, a listener could assume
that A follows the maxim of relevance. Therefore,
a natural inference that makes ?fixing of buffalo
chili? relevant is that A ate the buffalo chili. Sim-
ilarly, in the second example, the speaker A men-
tions a visit to Wyoming, which can be considered
relevant to the filming place of DANCES WITH
WOLVES. Some annotators recognized such rele-
vance and some did not.
Given the discrepencies between annotators, we
selected 875 examples which have at least 75%
agreement among the judgements in our current
investigation. We further selected one-third of this
data (291 examples) as our development data. The
experiments reported in Section 5 are based on this
development set.
3.3 Types of Hypotheses
The hypotheses collected from our study can be
categorzied into the following four types:
Fact. Facts about the participants. This includes:
(1) profiling information about individual partici-
pants (e.g., occupation, birth place, etc.); (2) activ-
ities associated with individual participants (e.g.,
A bikes to work everyday); and (3) social rela-
tions between participants (e.g., A and B are co-
workers, A and B went to college together).
Belief. Participants? beliefs and opinions about the
physical world. Any statement about the physical
world in fact is a belief of the speaker. Technically,
the state of the physical world that involves the
speaker him/herself is also a type of belief. How-
ever, here we assume a statement about oneself is
true and is considered as a fact.
Desire. Participants? desire of certain actions or
outcomes (e.g., A wants to find a university job).
These desires represent the states of the world the
participant finds pleasant (although they could be
conflicting to each other).
Intent. Participants? deliberated intent, in partic-
ular communicative intention which captures the
intent from one participant on the other partici-
pant such as whether A agrees/disagrees with B
208
on some issue, whether A intends to convince B
on something, etc.
Most of these types are motivated by the Belief-
Desire-Intention (BDI) model, which represents
key mental states and reflects the thoughts of
a conversation participant. Desire is different
from intention. The former arises subconsciously
and the latter arise from rational deliberation that
takes into consideration desires and beliefs (Allen,
1995). The fact type represents the facts about
a participant. Both thoughts and facts are criti-
cal to characterize a participant and thus impor-
tant to serve many other downstream applications.
The above four types account for 47.1%, 34.0%,
10.7%, and 8.2% of our development set respec-
tively.
4 A Probabilistic Framework
Following previous work (Haghighi et al, 2005;
de Salvo Braz et al, 2005; MacCartney et al,
2006), we approach conversation entailment using
a probabilistic framework. To predict whether a
hypothesis statement H can be inferred from a di-
alogue segment D, we estimate the probability
P (D  H|D,H)
Suppose we have a representation of a dia-
logue segment D in m clauses d1, . . . , dm and a
representation of the hypothesis H in n clauses
h1, . . . , hn. Since a hypothesis is the conjunc-
tion of the decomposed clauses, whether it can be
inferred from a segment is equivalent to whether
all of its clauses can be inferred from the seg-
ment. We further simplify the problem by assum-
ing that whether a clause is entailed from a dia-
logue segment is conditionally independent from
other clauses. Note that this conditional indepen-
dence assumption is an over-simplification, but it
gets things started. Therefore:
P (D  H|D,H)
= P (d1 . . . dm  h1 . . . hn|d1, . . . , dm, h1, . . . , hn)
= P (D  h1, . . . , D  hn|D,h1, . . . , hn)
=
n?
j=1
P (D  hj |D = d1 . . . dm, hj)
=
n?
j=1
P (d1 . . . dm  hj |d1, . . . , dm, hj) (1)
If this likelihood is above a certain threshold
(e.g., 0.5 in our experiments), then H is consid-
ered as a true entailment from D.
Given this framework, two important questions
are: (1) how to represent and automatically create
the clauses from each pair of dialogue segment and
hypothesis; and (2) how to estimate probabilities
as shown in Equation 1?
4.1 Clause Representation
Our clause representation is inspired by previ-
ous work on textual entailment (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007). Clause representation has several advan-
tages. First, it can be acquired automatically from
a parse tree (e.g., dependency parser). Second,
it can be used to facilitate both logic-based rea-
soning as in (Tatu and Moldovan, 2005; Bos and
Markert, 2005; Raina et al, 2005) or probabilis-
tic reasoning as in (Haghighi et al, 2005; de
Salvo Braz et al, 2005; MacCartney et al, 2006).
The key difference between our work and previ-
ous work on textual entailment is the representa-
tion of conversation discourse, which has not been
considered in previous work but is important for
conversation entailment, as we will see later.
More specifically, a clause is made up by two
components: Term and Predicate.
Term: A term can be an entity or an event. An
entity refers to a person, a place, an organization,
or other real world entities. This follows the con-
cept of mention in the Automatic Content Extrac-
tion (ACE) evaluation (Doddington et al, 2004).
An event refers to an action or an activity. For
example, from the sentence ?John married Eva in
1940? we can identify an event of marriage. Fol-
lowing the neo-Davidsonian representation (Par-
sons, 1990), all the events are reified as terms in
our representation.
Predicate: A predicate represents either a prop-
erty (i.e., unary) for a term or a relation (i.e., bi-
nary) between two terms. For example, an entity
company has a property of Russian as in the phrase
?a Russian company? (i.e., Russian(company)).
An event visit has a property of recently (i.e.,
recently(visit)) as in the phrase ?visit Brazil re-
cently?. From the phrase ?Prime Minister re-
cently visited Brazil?, there are binary relations:
PrimeMinister is the subject of the event visit (i.e.,
subj(visit, Prime Minister)) and Brazil is the
object of the visit (i.e., obj(visit, Brazil)).
This representation is a direct conversion from
the dependency structure and can be used to rep-
resent the semantics of utterances in the dialogue
209
segments and the semantics of hypotheses. For ex-
ample,
Example 5:
Dialogue Segment:
B: Have you seen Sleeping with the Enemy?
A: No. I?ve heard that?s really great, though.
B: You have to go see that one.
Hypothesis:
B suggests A to watch Sleeping with the Enemy.
Appendix A shows the dependency structure of
the dialogue utterances and the hypothesis from
Example 5. Appendix B shows the correspond-
ing clause representation of the dialogue segment
and the hypothesis. Note that in this represen-
tation, you and I are replaced with the respec-
tive participants. Since the clauses are generated
based on parse trees, most relational predicates are
syntactic-driven.
To facilitate conversation entailment, we fur-
ther augment the representation of a dialogue seg-
ment by incorporating conversation context. Ap-
pendix C shows the augmented representation for
Example 5. It represents the following additional
information:
? Utterance: A group of pseudo terms u1,
u2, . . . are used to represent individual utter-
ances.
? Participant: A relational clause
speaker(?, ?) is used to represent the speaker
of this utterance, e.g., speaker(u1, B).
? Content: A relational clause content(?, ?) is
used to represent the content of an utterance
where the second term is the head of the ut-
terance as identified in the parsing structure.
e.g., content(u3, heard)
? Dialogue act: A relational clause act(?, ?)
is used to represent the dialogue act of the
speaker for a particular utterance. e.g.,
act(u2, no answer). A set of 42 dialogue
acts from the Switchboard annotation are
used here (Godfrey and Holliman, 1997).
? Utterance flow: A relational clause
follow(?, ?) is used to connect each pair of
adjacent utterances. e.g., follow(u2, u1).
We currently do not consider overlap in utter-
ances, but our representation can be modified
to handle this situation by introducing
additional predicates.
4.2 Entailment Prediction
Given the clause representation for a conversation
segment and a hypothesis, the next step is to make
an entailment prediction (as in Equation 1) based
on two models: an Alignment Model and an Infer-
ence Model.
4.2.1 Alignment Model
The alignment model is to find alignments (or
matches) between terms in the clause representa-
tion for a hypothesis and those in the clause rep-
resentation for a conversation segment. We define
an alignment as a mapping function g between a
term x in the dialogue segment and a term y in the
hypothesis. g(x, y) = 1 if x and y are aligned;
otherwise g(x, y) = 0. Note that a verb can be
aligned to a noun as in g(sell, sale) = 1. It is also
possible that there are multiple terms from the seg-
ment mapped to one term in the hypothesis, or vice
versa.
For any two terms x and y, the problem of pre-
dicting the alignment function g(x, y) can be for-
mulated as a binary classification problem. We
used several features to train the classifier, which
include whether x and y are the same (or have the
same stem), whether one term is an acronym of the
other, and their WordNet and distributional simi-
larities (Lin, 1998).
Given an augmented representation with con-
versation context (as in Appendix C), we also
align event terms in the hypothesis (e.g., suggest
in Example 5) to (pseudo) utterance terms in the
dialogue segment. We call it a pseudo alignment.
This is currently done by a set of rules which asso-
ciate event terms in the hypotheses with dialogue
acts. For example, the event term suggest may be
aligned to an utterance with dialogue act of opin-
ion. Appendix D gives a correct alignment for Ex-
ample 5, in which g(u4, x1) = 1 is a pseudo align-
ment.
4.2.2 Inference Model
As shown in Equation 1, to predict the infer-
ence of the entire hypothesis, we need to calculate
the probability that the dialogue segment entails
each clause from the hypothesis. More specifi-
cally, given a clause from the hypothesis hj , a set
of clauses from the dialogue segment d1, . . . , dm,
and an alignment function g between them derived
by the method described in Section 4.2.1, we pre-
dict whether d1, . . . , dm entails hj under the align-
ment g using two different classification models,
210
depending on whether hj is a property or a rela-
tion (i.e. whether it takes one argument (hj(?)) or
two arguments (hj(?, ?))):
Given a property clause from the hypothe-
sis, hj(x), we look for all the property clauses
in the dialogue segment that describes the
same term as x, i.e. a clause set D? =
{di(x?)|di(x?) ? D, g(x?, x) = 1}. Then we pre-
dict whether hj(x) can be inferred from the
clauses in D? by binary classification, using a set
of features similar to those used in the alignment
model.
Given a relational clause from the hypothe-
sis, hj(x, y), we look for the relation between
the counterparts of x and y in the dialogue seg-
ment. That is, we find the set of terms X ? =
{x?|x? ? D, g(x?, x) = 1} and the set of terms
Y ? = {y?|y? ? D, g(y?, y) = 1} and look for the
closest relation between these two sets of terms in
the dependency structure. If there is a path be-
tween any x? ? X ? and any y? ? Y ? in the de-
pendency structure with a length smaller than a
threshold ?L, we predict that hj(x, y) can be in-
ferred. Note that our current handling of the re-
lational clauses is rather simplified. It only cap-
tures whether two terms from an hypothesis are
connected by any relation in the dialogue segment.
Appendix E shows the inference procedure of
the four hypothesis clauses in Example 5. For
each relational clause hj(x, y), the shortest path
between the correspondingX ? and Y ? has a length
of 3 or less, so each of these four clauses is en-
tailed from the dialogue segment. Based on Equa-
tion 1 we can conclude that the overall hypothesis
is entailed.
We trained the alignment model and the in-
ference model (e.g.,the threshold ?L) based on
the development data provided by the PASCAL 3
challenges on textual entailment.
5 Experimental Results
To understand unique behaviors of conversation
entailment, we focused our current experiments
on the development dataset (see Section 3.2).
We are particularly interested in how the tech-
niques for textual entailment can be improved for
conversation entailment. To do so, we applied
our entailment framework on the test data of the
PASCAL-3 RTE Challenge (Giampiccolo et al,
2007). Among 800 testing examples, our ap-
proach achieved an accuracy of 60.6%. This re-
sult is on par with the performance of the me-
dian system of accuracy 61.8% (z-test, p=0.63) in
the PASCAL-3 RTE Challenge. Our current ap-
proach is very lean on the use of external knowl-
edge. Its competitive performance sets up a rea-
sonable baseline for our investigation on conversa-
tion entailment. This same system, modified to tai-
lor linguistic characteristics of conversation (e.g.,
removal of disfluency), was used as the baseline in
our experiments.
5.1 Event Alignment
To understand the effect of conversation context
in the event alignment, we compared two configu-
rations of alignment model for events. The first
configuration is based on the clause representa-
tion of semantics of utterances (as shown in Ap-
pendix B). This is the same configuration as used
in textual entailment. The second configuration
is based on representation of both semantics from
utterances and conversation context (as shown in
Appendix C). We evaluate how well each config-
uration aligns the event terms based on the pair-
wise alignment decision: for any event term tH in
the hypothesis and any term tD in the dialogue,
whether the model can correctly predict that the
two terms should be aligned.
Figure 2(a) shows the comparison of F-measure
between the two models. Depending on the thresh-
old of alignment prediction, the precision and re-
call of the prediction vary. When the thresh-
old is lower, the models tend to give more align-
ments, resulting in lower precision and higher re-
call. When the threshold is higher, the models tend
to give fewer alignments, thus resulting in higher
precision but lower recall. When the threshold
is around 0.5, the alignment reaches its best F-
measure. Regardless of what threshold is cho-
sen, the model based on both utterance and con-
text consistently works better. Figure 2(b) shows
the breakdown based on the types of hypothesis (at
threshold 0.5). The model that incorporates con-
versation context consistently performs better for
all types. Its improvement is particularly signifi-
cant for the intent type of hypothesis.
These results are not surprising. Many event
terms in hypotheses (e.g., suggest, think, etc.) do
not have their counterparts directly expressed in
utterances in the dialogue discourse. Only through
the modeling of dialog acts, these terms can be
aligned to potential pseudo terms in the dialogue
211
segment. For the fact type hypotheses, the event
terms in the hypotheses generally have their coun-
terparts in the dialogue discourse. That explains
why the improvement for the fact type using con-
versation context is minimal.
(a) Overall comparison on F-measure
(b) Comparison for different types of hypothesis
Figure 2: Experimental results on event alignment
5.2 Entailment Prediction
Given correct alignments, we further evaluated
entailment prediction based on three configura-
tions of the inference model: (1) the same infer-
ence model learned from the textual entailment
data and tested on the PASCAL-3 RTE Challenge
(Text); (2) an improved model incorporating a
number of features relevant to dialogues (espe-
cially syntactic structure of utterances) based on
representations without conversation context as in
Appendix B (+Dialogue); (3) a further improved
model based on augmented representations of con-
versation context and using dialogue acts during
the prediction of entailment as in Appendix C
(+Context).
System Acc Prec Recall F
Text 53.6% 71.6% 29.3% 41.6%
+Dialogue 58.4% 84.1% 32.3% 46.7%
+Context 67.7% 91.7% 47.0% 62.1%
Table 1: Experimental results on entailment pre-
diction
For each configuration we present two evalua-
tion metrics: an accuracy of the overall prediction
and a precision-recall measurement for the posi-
tive entailment examples. All the evaluations are
performed on our development data, which has
56.4% of positive examples and 43.6% of negative
examples.
The evaluations results are shown in Table 1.
The system learned from textual entailment per-
forms lower than the prediction based on the
majority class (56.4%). Incorporating syntactic
features of dialogues did better but the differ-
ence is not statistically significant. Incorporat-
ing conversation context, especially dialogue acts,
achieves significantly better performance (z-test,
p < 0.005).
Table 2 shows the comparison of the three con-
figurations based on different types of hypothesis.
As expected, the basic system trained on textual
entailment is not capable for any intent type of
hypotheses. Modeling conversation context with
dialogue acts improves inference for all types of
hypothesis, with most significant improvement for
the belief, desire, and intent types of hypothesis.
6 Conclusion
This paper describes our initial investigation on
conversation entailment to address information ac-
quisition about conversation participants. Since
there are so many variables involved in the pre-
diction, our experiments have been focused on a
set of development data where most of the features
are annotated. This allowed us to study the effect
of conversation context in both alignment and en-
tailment. Our future work will enhance the cur-
rent approach by training the models based on our
development data and evaluate them on the test-
ing data. Conversation entailment is an important
task. Although the current exercise is targeted to
process conversation scripts from human-human
conversation, it can potentially benefit human ma-
chine conversation by enabling automated agents
to gain better understanding of their conversation
212
Fact Belief Desire Intent
System Acc F Acc F Acc F Acc F
Text 58.4% 51.3% 52.5% 37.3% 51.6% 34.8% 33.3% 0
+Dialogue 68.6% 62.6% 53.5% 36.1% 48.4% 33.3% 33.3% 0
+Context 70.8% 64.9% 67.7% 62.8% 58.1% 47.8% 62.5% 60.9%
Table 2: Experimental results on entailment prediction for different types of hypotheses
partners.
Acknowledgments
This work was partially supported by IIS-0347548
and IIS-0840538 from the National Science Foun-
dation. We thank the anonymous reviewers for
their valuable comments and suggestions.
References
James Allen. 1995. Natural language understanding.
The Benjamin/Cummings Publishing Company, Inc.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of HLT-EMNLP, pages 628?635.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In PASCAL Challenges Workshop on
Recognising Textual Entailment.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural
language. In Proceedings of AAAI.
G. Doddington, A. Mitchell, M. Przybocki, and
L. Ramshaw. 2004. The automatic content extrac-
tion (ace) programctasks, data, and evaluation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC).
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of bayesian networks to model pragmatic de-
pendencies. In Proceedings of ACL, pages 669?676.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 1?9.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Linguistic Data Consor-
tium, Philadelphia.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching.
In Proceedings of HLT-EMNLP, pages 387?394.
Eric Horvitz and Tim Paek. 2001. Harnessing mod-
els of users? goals to mediate clarification dialog in
spoken language systems. In Proceedings of the 8th
International Conference on User Modeling, pages
3?13.
Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2007. Extracting social networks and biographical
facts from conversational speech transcripts. In Pro-
ceedings of ACL, pages 1040?1047.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning, pages 296?304.
Diane Litman and Katherine Forbes-Riley. 2006. Rec-
ognizing student emotions and attitudes on the basis
of utterances in spoken tutoring dialogues with both
human and computer tutors. Speech Communica-
tion, 48(5):559?590.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of HLT-NAACL,
pages 41?48.
Terence Parsons. 1990. Events in the Semantics of En-
glish. A Study in Subatomic Semantics. MIT Press.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning
and abductive reasoning. In Proceedings of AAAI,
pages 1099?1105.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP, pages 254?
263.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of HLT-EMNLP, pages 371?378.
213
APPENDIX
A Dependency Structure of Dialogue Utterances and Hypothesis in Example 5
Di
alo
gu
e S
eg
m
en
t:
B:
 H
av
e y
ou
se
en
Sl
ee
pin
g w
ith
 th
e E
ne
my
?
A:
 N
o. 
I 'v
e h
ea
rd
tha
t's
 re
all
y g
rea
t, t
ho
ug
h.
B:
 Y
ou
ha
ve
to 
go
se
et
ha
t o
ne
.
x 1
A
x 2
x 3
A
x 4
x 5
x 6
tho
ug
h(
?)
A
x 7
x 8
x 9
x 10
B
A
x 1
x 2
x 3
Hy
po
th
es
is:
B
su
gg
es
ts
A
to 
wa
tch
Sl
ee
pin
g w
ith
 th
e E
ne
my
.
ter
ms
pr
ed
ica
tes
B Clause Representation of Dialogue Segment and Hypothesis for Example 5s
ub
j(x
1,B
), 
ob
j(x
1,A
), 
ob
j(x
1,x
2),
 ob
j(x
2,x
3)
x 1=
su
gg
es
ts,
 x 2
=w
atc
h, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
, B
Hy
po
th
es
is:
su
bj(
x 7,
A)
, o
bj(
x 7,
x 8)
, o
bj(
x 8,
x 9)
, o
bj(
x 9,
x 10
)
x 7=
ha
ve
, x
8=
go
, x
9=
se
e, 
x 10
=o
ne
, A
B:
su
bj(
x 4,
A)
, o
bj(
x 4,
x 6)
, s
ub
j(x
6,x
5),
 th
ou
gh
(x 4
)
x 4=
ha
ve
 he
ar
d, 
x 5=
tha
t, 
x 6=
is 
re
all
yg
re
at,
 A
A:
su
bj(
x 2,
A)
, o
bj(
x 2,
x 3)
, a
ux
(x 2
,x 1
)
x 1=
ha
ve
, x
2=
se
en
, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
B:
Cl
au
se
s
Te
rm
s
Di
alo
gu
e S
eg
m
en
t:
C Augmented Clause Representation of Dialogue Segment in Example 5s
pe
ak
er
(u
4,B
), 
co
nte
nt(
u 4
,x 7
), 
ac
t(u
4,o
pin
ion
), 
su
bj(
x 7,
A)
, o
bj(
x 7,
x 8)
, o
bj(
x 8,
x 9)
, o
bj(
x 9,
x 10
)
u 4
,x
7=
ha
ve
, x
8=
go
, x
9=
se
e, 
x 10
=o
ne
, A
, B
B:
fol
low
(u
2,u
1),
 fo
llo
w(
u 3
,u 2
), 
fol
low
(u
4,u
3)
sp
ea
ke
r(u
2,A
), 
co
nte
nt(
u 2
,-)
, a
ct(
u 2
,no
_a
ns
we
r),
 
sp
ea
ke
r(u
3,A
), 
co
nte
nt(
u 3
,x 4
), 
ac
t(u
3,s
tat
em
en
t),
su
bj(
x 4,
A)
, o
bj(
x 4,
x 6)
, s
ub
j(x
6,x
5),
 th
ou
gh
(x 4
)
u 2
,u
3,x
4=
ha
ve
 he
ar
d, 
x 5=
tha
t, 
x 6=
is 
re
all
yg
re
at,
 A
A:
sp
ea
ke
r(u
1,B
), 
co
nte
nt(
u 1
,x 2
), 
ac
t(u
1,w
h_
qu
es
tio
n)
, 
su
bj(
x 2,
A)
, o
bj(
x 2,
x 3)
, a
ux
(x 2
,x 1
)
u 1
,x
1=
ha
ve
, x
2=
se
en
, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
, B
B:
Cl
au
se
s
Te
rm
s
Di
alo
gu
e S
eg
m
en
t (
wi
th
 co
nt
ex
t r
ep
re
se
nt
at
ion
):
214
D The Alignment for Example 5
x 2=
se
en
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
 
BA x 5=
tha
t
x 7=
ha
ve
u 4
: a
ct(
u 4
,op
ini
on
)
Di
alo
gu
e S
eg
m
en
t
x 1=
su
gg
es
ts
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
 
BA x 2=
wa
tch
Hy
po
th
es
is
x 4=
ha
ve
 he
ar
d
x 8=
go
x 9=
se
e
x 10
=o
ne
x 6=
is 
re
all
yg
re
at
u 3
: a
ct(
u 3
,st
ate
me
nt)
u 2
: a
ct(
u 2
,no
_a
ns
we
r)
u 1
: a
ct(
u 1
,w
h_
qu
es
tio
n)
x 1=
ha
ve
E The Prediction of Inference for the Hypothesis Clauses in Example 5
x 3,
 x 5
, 
x 10x 3
x 2,
 x 9x 2
AA
BB
ye
s
ye
s
ye
s
ye
s
Hy
po
the
sis
 C
lau
se
 
En
tai
led
?
1
3
2
1
Pa
th 
Le
ng
th
ob
j(x
9,x
10
)
co
nte
nt(
u 4
,x 7
), 
ob
j(x
7,x
8),
 
ob
j(x
8,x
9)
co
nte
nt(
u 4
,x 7
), 
su
bj(
x 7,
A)
sp
ea
ke
r(u
4,B
)
Sh
or
tes
t P
ath
 be
tw
ee
n t
he
 
Al
ign
ed
 T
erm
s i
n t
he
 
De
pe
nd
en
cy
 St
ru
ctu
re 
of
 
Di
alo
gu
e S
eg
me
nt
x 2,
 x 9
u 4
u 4
u 4
Al
ign
ed
 T
erm
s i
n t
he
 
Di
alo
gu
e S
eg
me
nt
x 2
x 1
x 1
x 1
Te
rm
s i
n t
his
 C
lau
se
rel
ati
on
rel
ati
on
rel
ati
on
rel
ati
on
Cl
au
se
 T
yp
e
ob
j(x
2,x
3)
ob
j(x
1,x
2)
ob
j(x
1,A
)
su
bj(
x 1,
B)
 
Hy
po
the
sis
 C
lau
se
215
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756?766,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Towards Conversation Entailment: An Empirical Investigation
Chen Zhang Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{zhangch6, jchai}@cse.msu.edu
Abstract
While a significant amount of research has
been devoted to textual entailment, automated
entailment from conversational scripts has re-
ceived less attention. To address this limi-
tation, this paper investigates the problem of
conversation entailment: automated inference
of hypotheses from conversation scripts. We
examine two levels of semantic representa-
tions: a basic representation based on syntac-
tic parsing from conversation utterances and
an augmented representation taking into con-
sideration of conversation structures. For each
of these levels, we further explore two ways of
capturing long distance relations between lan-
guage constituents: implicit modeling based
on the length of distance and explicit mod-
eling based on actual patterns of relations.
Our empirical findings have shown that the
augmented representation with conversation
structures is important, which achieves the
best performance when combined with ex-
plicit modeling of long distance relations.
1 Introduction
Textual entailment has received increasing attention
in recent years (Dagan et al, 2005; Bar-Haim et al,
2006; Giampiccolo et al, 2007; Giampiccolo et al,
2008; Bentivogli et al, 2009). Given a segment from
a textual document, the task of textual entailment is
to automatically determine whether a given hypoth-
esis can be entailed from the segment. The capa-
bility of such kind of inference can benefit many
text-based applications such as information extrac-
tion and question answering.
Textual entailment has mainly focused on infer-
ence from written text in monologue. Recent years
also observed an increasing amount of conversa-
tional data such as conversation scripts of meetings,
call center records, court proceedings, as well as on-
line chatting. Although conversation is a form of
language, it is different from monologue text with
several unique characteristics. The key distinctive
features include turn-taking between participants,
grounding between participants, different linguistic
phenomena of utterances, and conversation impli-
catures. Traditional approaches dealing with tex-
tual entailment were not designed to handle these
unique conversation behaviors and thus to support
automated entailment from conversation scripts.
Example 1:
Conversation Segment:
B: My mother also was very very independent.
She had her own, still had her own little house
and still driving her own car,
A: Yeah.
B: at age eighty-three.
Hypothesis:
(1) B?s mother is eighty-three.
(2) B is eighty-three.
To address this limitation, our previous
work (Zhang and Chai, 2009) has initiated an
investigation on the problem of conversation en-
tailment. The problem was formulated as follows:
given a conversation discourse D and a hypothesis
H concerning its participant, the goal was to identify
whether D entails H. For instance, as in Example
1, the first hypothesis can be entailed from the
756
conversation segment while the second hypothesis
cannot. While our previous work has provided
some interesting preliminary observations, it mostly
focused on data collection and initial experiments
and analysis using a small set of development data.
It is not clear whether the previous results are
generally applicable, how different components in
the entailment framework interact with each other,
and how different representations may influence the
entailment outcome.
To reach a better understanding of conversation
entailment, we conducted a further investigation
based on the larger set of test data collected in our
previous work (Zhang and Chai, 2009). We specifi-
cally examined two levels of representations: a basic
representation based on syntactic parsing from con-
versation utterances and an augmented representa-
tion taking into consideration of conversation struc-
tures. For each of these levels, we further explored
two ways of capturing long distance relations: (1)
implicit modeling based on the length of distance
and (2) explicit modeling based on actual patterns
of relations. Our empirical findings have shown that
augmented representation with conversation struc-
tures is important in conversation entailment. Com-
bining conversation structures with explicit model-
ing of long distance relations results in the best per-
formance.
2 Related Work
Our work here is related to recent advances in tex-
tual entailment, automated processing of conversa-
tion scripts, and our initial investigation on conver-
sation entailment.
There is a large body of work on textual en-
tailment initiated by the Pascal Recognizing Tex-
tual Entailment (RTE) Challenges (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007; Giampiccolo et al, 2008; Bentivogli et al,
2009). Different approaches have been developed,
for example, based on logic proving (Tatu and
Moldovan, 2005; Bos and Markert, 2005; Raina et
al., 2005) and graph match (Haghighi et al, 2005;
de Salvo Braz et al, 2005; MacCartney et al, 2006).
Supervised learning approaches have also been ap-
plied to measure the similarities between training
and testing pairs (Zanzotto and Moschitti, 2006). In
the most recent RTE Challenge (Bentivogli et al,
2009), the best system achieves 73.5% of accuracy,
while the median performance among all partici-
pants is 60.4%. These results indicate that, while
progress has been made, textual entailment remains
a challenging problem.
As more and more conversation data becomes
available, researchers have investigated automated
processing of conversation data to acquire useful
information, for example, related to opinions (So-
masundaran et al, 2007; Somasundaran et al,
2008; Somasundaran et al, 2009), biographic at-
tributes (Garera and Yarowsky, 2009), social net-
works (Jing et al, 2007), and agreements and
disagreements between participants (Galley et al,
2004). Recent studies have also developed ap-
proaches to summarize conversations (Murray and
Carenini, 2008) and to model conversation struc-
tures (dialogue acts) from online Twitter conversa-
tions (Ritter et al, 2010). Here we address a dif-
ferent angle regarding conversation scripts, namely
conversation entailment.
In our previous work (Zhang and Chai, 2009),
we started an initial investigation on conversation
entailment. We have collected a dataset of 875
instances. Each instance consists of a conversa-
tion segment and a hypothesis (as described in Sec-
tion 1). The hypotheses are statements about conver-
sation participants and are further categorized into
four types: about their profile information, their be-
liefs and opinions, their desires, and their commu-
nicative intentions. We developed an approach that
is motivated by previous work on textual entailment.
We use clauses in the logic-based approaches as the
underlying representation of our system. Based on
this representation, we apply a two stage entailment
process similar to MacCartney et al (2006) devel-
oped for textual entailment: an alignment stage fol-
lowed by an entailment stage.
Building upon our previous work, in this paper,
we systematically examine different representations
of the conversation segment and different modeling
of long distance relations between language con-
stituents. We compare the roles of these different
representations on the performance of entailment
prediction using a larger testing dataset that was not
previously evaluated. This analysis allows better un-
derstanding of the problem and provides insight on
757
potential solutions.
3 Overall Framework
In our previous work (Zhang and Chai, 2009), con-
versation entailment is formulated as the follow-
ing: given a conversation segment D which is rep-
resented by a set of clauses D = d1 ? . . . ? dm,
and a hypothesis H represented by another set of
clauses H = h1 ? . . . ? hn, the prediction on
whether D entails H is determined by the product
of probabilities that each hypothesis clause hj is
entailed from all the conversation segment clauses
d1 . . . dm as follows. This is based on a simple as-
sumption that whether a clause is entailed from a
conversation segment is conditionally independent
from other clauses.
P (D  H|D,H)
= P (D  h1, . . . , D  hn|D,h1, . . . , hn)
=
n?
j=1
P (D  hj |D = d1 . . . dm, hj)
=
n?
j=1
P (d1 . . . dm  hj |d1, . . . , dm, hj) (1)
A clause here is similar to a sentence in first-
order predicate calculus. It is made up by terms
and predicates. A term is either: 1) an entity
described by a noun phrase, e.g., John Lennon,
mother, or she; or 2) an action or event de-
scribed by a verb phrase, e.g., marry in ?John
married Eva in 1940?. A predicate represents
either: 1) a property (i.e., unary) for a term,
e.g., Russian(company), or recently(visit);
or 2) a relation (i.e., binary) between two
terms, e.g., subj(visit, Prime Minister) and
obj(visit, Brazil) in ?Prime Minister recently vis-
ited Brazil?.
Given the clause representation, we follow the
idea similar to MacCartney et al (2006), and predict
the entailment decision in two stages of processing:
(1) an alignment model aligns terms in the hypothe-
sis to terms in the conversation segment; and (2) an
inference model predicts the entailment based on the
alignment between the hypothesis and the conversa-
tion segment.
3.1 Alignment Model
An alignment is defined as a mapping function g
between a term x in the conversation segment and a
term y in the hypothesis. g(x, y) = 1 if x and y are
aligned; otherwise g(x, y) = 0. It is possible that
multiple terms from the segment are mapped to one
term in the hypothesis (g(x1, y) = g(x2, y) = 1),
or vice versa (g(x, y1) = g(x, y2) = 1). To predict
these alignments, the problem is formulated as bi-
nary classification: given any two terms x from the
conversation and y from the hypothesis, decide the
value of their alignment function g(x, y).
3.2 Inference Model
Once an alignment between a hypothesis and a con-
versation segment is established, an inference model
is applied to predict whether the conversation seg-
ment entails the hypothesis given such alignment.
More specifically, as shown in Equation 1, given a
clause from the hypothesis hj , a set of clauses from
the conversation segment d1, . . . , dm, and an align-
ment g between them, the goal is to predict whether
d1, . . . , dm entails hj under the alignment g.
The prediction is treated differently according to
different types of clauses. If hj is a property clause
(i.e., takes one argument hj(?)), a property inference
model is applied; otherwise (i.e., relational clauses
with two arguments hj(?, ?)), a relational inference
model is applied.
In this paper we follow the same framework.
However our focus here is on the new question that
how different levels of semantic representation and
different approaches of modeling long distance rela-
tionship affect the alignment and inference models
as well as the overall entailment performance.
4 Semantic Representation
Given the clause representation described earlier,
an important question is what information from the
conversation segment should be captured and repre-
sented. To address this question, we examined two
levels of shallow semantic representation. The first
level is basic representation which only captures the
information from all the utterances in the conversa-
tion segment. The second representation includes
conversation structures (e.g., speakers and dialogue
758
acts). Next we use Example 2 to illustrate these rep-
resentations.
Example 2:
Conversation Segment:
B: Have you seen Sleeping with the Enemy?
A: No. I?ve heard that?s really great, though.
B: You have to go see that one.
Hypothesis:
B suggests A to watch Sleeping with the Enemy.
4.1 Basic Representation
The first representation is based on the syntactic
parsing from conversation utterances and we call it
a basic representation. Figure 1(a) shows an exam-
ple of dependency structures for several utterances
that are derived from the Stanford parser (Klein and
Manning, 2003), and Figure 1(b) shows the corre-
sponding clause representation. In the dependency
structure, the vertices represent entities (e.g., x1) and
actions (e.g., x3) within an utterance. They corre-
spond to terms in the clause representation. An edge
between vertices captures a dependency relation and
is represented as predicates in the clause representa-
tion. For example, the edge between x1 and x3 indi-
cates x1 is the subject of x3, which is represented by
the clause representation subj(x3, x1). Similar rep-
resentation also applies to the hypothesis as shown
in Figure 1(c), 1(d).
4.2 Augmented Representation
The second representation is built upon the basic
representation and incorporates conversation struc-
ture across turns and utterances. We call it an aug-
mented representation. Figure 2(a) shows the aug-
mented structures of the conversation segment and
Figure 2(b) shows the corresponding clause repre-
sentation. Compared to the basic representation,
there are two additional types of vertices (i.e., terms)
highlighted in the figures:
? Vertices representing utterances (e.g.,
u1 . . . u4). Their corresponding terms capture
the dialogue acts for the utterances (e.g.
u1 = yes no question). To focus our effort,
currently we only apply annotated dialogue
acts provided in the Switchboard corpus (God-
frey and Holliman, 1997). Two edges are
added to connect different utterances. The
first edge connects each utterance vertex to
the head of the corresponding utterance to
indicate the specific content of the utterance
(e.g., content(u1, x3)). The second edge con-
nects an utterance to its succeeding utterance
to indicate the temporal progression of the
conversation (e.g., follow(u2, u1)).
? Vertices representing speakers or participants
(e.g., sA, sB). One edge is added to
connect each utterance to its speaker (e.g.,
speaker(u1, sB)).
Note that since our clause representations are
mainly based on the dependency relations, they are
mostly syntactic-driven. However, it does capture
some shallow semantics such as who is the agent
(i.e., subject) or the patient (i.e., object) of an event.
The incorporation of speakers and dialogue acts in
our augmented representations provides additional
semantics of conversation discourse.
5 Modeling LDR
A critical part in predicting entailment is to recog-
nize the semantic relationship between two language
constituents, especially when these two constituents
are not directly related. In Figure 2(a), for exam-
ple, we want to recognize that x9 (You) is the (log-
ical) subject of x11 (see). Here we experimented
two ways of modeling such long distance relations
(LDR).
5.1 Implicit Modeling of LDR
The first method characterizes the relationship sim-
ply by the distance between two constituents in the
dependency structure (or augmented structure). For
example, in Figure 2(a) the distance between x11
and x9 is 3. We call this method an implicit mod-
eling of long distance relationship.
The advantage of implicit modeling is that it is
easy to implement based on the dependency struc-
ture. However, its limitation is that the distance mea-
sure does not capture sufficient information of se-
mantic relations between language constituents.
5.2 Explicit Modeling of LDR
The second way of modeling long distance relation-
ship is called explicit modeling. It uses a string to
759
B: 
Ha
ve
you
see
nS
lee
pin
g w
ith
 th
e E
nem
y?
A: 
No
. I'
ve 
hea
rd
tha
t's 
rea
lly
 gr
eat
, th
oug
h.
B: 
Yo
u h
ave
to 
go
see
tha
t o
ne.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
obj
(x 3
,x 2
)
sub
j(x
3,x
1)
aux
(x 3
,x 4
)
x 1=
A
x 2=
Sle
epi
ng
wit
h t
he 
En
em
y
x 3=
see
n, x
4=h
ave
obj
(x 1
1,x
10)
obj
(x 1
2,x
11)
obj
(x 1
3,x
12)
sub
j(x
13,x
9)
x 9=
A, 
x 10
=o
ne,
x 11
=se
e, x
12=
go,
 
x 13
=h
ave
sub
j(x
7,x
6)
obj
(x 8
,x 7
)
sub
j(x
8,x
5)
x 5=
A, 
x 6=
tha
t
x 7=
is r
eal
ly g
rea
t
x 8=
hav
e h
ear
d
Cla
us
es
Te
rm
s
(a) dependency structure of the conversation
utterances
B:
 H
av
ey
ou
see
nS
lee
pin
g w
ith
 th
e E
ne
my
?
A:
 N
o. 
I'v
e h
ear
dt
ha
t's 
rea
lly
 gr
eat
, th
ou
gh
.
B:
 Y
ou
ha
ve
to 
go
see
tha
t o
ne
.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
ob
j(x
3,x
2)
sub
j(x
3,x
1)
au
x(x
3,x
4)
x 1=
A
x 2=
Sle
ep
ing
wi
th 
the
 En
em
y
x 3=
see
n, 
x 4=
ha
ve
ob
j(x
11
,x 1
0)
ob
j(x
12
,x 1
1)
ob
j(x
13
,x 1
2)
sub
j(x
13
,x 9
)
x 9=
A, 
x 10
=o
ne
,
x 11
=s
ee,
 x 1
2=g
o, 
x 13
=h
av
e
sub
j(x
7,x
6)
ob
j(x
8,x
7)
sub
j(x
8,x
5)
x 5=
A, 
x 6=
tha
t
x 7=
is r
ea
lly
 gr
ea
t
x 8=
ha
ve 
he
ard
Cla
us
es
Te
rm
s
(b) basic representation of the conver-
sation segment
x 1
x 2
x 5
x 4
x 3
Bs
ugg
est
sA
to 
wa
tch
Sle
epi
ng 
wit
h t
he 
En
em
y.
sub
j
sub
j
obj
obj
B: 
Ha
ve
you
see
nS
lee
pin
g w
ith
 th
e E
nem
y?
A: 
No
. I'v
e h
ear
dt
hat
's r
eal
ly 
gre
at, 
tho
ugh
.
B: 
Yo
u h
ave
to 
go
see
tha
t o
ne.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
u 1 u 2 u 3 u 4
s B s A
(c) dependency structure of the hypothesis
ob
j(x
3,x
2),
 su
bj(
x 3,
x 1)
au
x(x
3,x
4)
x 1=
A, 
x 3=
see
n, 
x 4=
ha
ve
x 2=
Sle
ep
ing
 wi
th 
the
 En
em
y
ob
j(x
11
,x 1
0),
 ob
j(x
12
,x 1
1)
ob
j(x
13
,x 1
2),
 su
bj(
x 13
,x 9
)
x 9=
A, 
x 10
=o
ne
, x
11
=s
ee
x 12
=g
o, 
x 13
=h
av
e
speaker
(u 4
,s B
)
content
(u 4
,x 1
3)
follow
(u 4
,u 3
)
u 4=
viewpoint
sub
j(x
7,x
6),
 ob
j(x
8,x
7)
sub
j(x
8,x
5)
x 5=
A, 
x 7=
is r
ea
lly
 gr
ea
t
x 6=
tha
t, x
8=h
av
e h
ea
rd
speaker
(u 3
,s A
)
content
(u 3
.x 8
)
follow
(u 3
,u 2
)
u 3=
statement
speaker
(u 2
,s A
)
follow
(u 2
,u 1
)
u 2=
no_answer
speaker
(u 1
,s B
)
content
(u 1
, x
3)
s A,
s B
u 1=
yes_no_qu
estion
Cla
us
es
Te
rm
s
sub
j(x
4,x
2)
ob
j(x
4,x
3)
sub
j(x
5,x
1)
ob
j(x
5,x
4)
x 1=
B, 
x 2=
A
x 3=
Sle
ep
ing
wi
th 
the
 En
em
y
x 4=
wa
tch
x 5=
sug
ge
sts
Cla
us
es
Te
rm
s
(d) representation of the hy-
pothesis
Figure 1: The dependency structures and corresponding basic representation of Example 2x 1
x 2
x 5
x 4
x 3
Bs
ug
ge
sts
A
to 
wa
tch
Sle
ep
ing
 w
ith
 th
e E
ne
my
.
sub
j
sub
j
ob
j
ob
j
B:
 H
av
ey
ou
see
nS
lee
pin
g w
ith
 th
e E
ne
my
?
A:
 N
o. 
I'v
e h
ear
dt
ha
t's 
rea
lly
 gr
eat
, th
ou
gh
.
B:
 Y
ou
ha
ve
to 
go
see
tha
t o
ne
.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
u 1 u 2 u 3 u 4
s B s A
(a) dependency and conversation structures of the conversation
segment
obj
(x 3
,x 2
), s
ubj
(x 3
,x 1
)
aux
(x 3
,x 4
)
x 1=
A, 
x 3=
see
n, x
4=h
ave
x 2=
Sle
epi
ng 
wit
h t
he 
En
em
y
obj
(x 1
1,x
10)
, ob
j(x
12,x
11)
obj
(x 1
3,x
12)
, su
bj(
x 13
,x 9
)
x 9=
A, 
x 10
=o
ne,
 x 1
1=s
ee
x 12
=g
o, x
13=
hav
e
speaker
(u 4
,s B
)
content
(u 4
,x 1
3)
follow
(u 4
,u 3
)
u 4=
viewpoint
sub
j(x
7,x
6), 
obj
(x 8
,x 7
)
sub
j(x
8,x
5)
x 5=
A, 
x 7=
is r
eal
ly g
rea
t
x 6=
tha
t, x
8=h
ave
 he
ard
speaker
(u 3
,s A
)
content
(u 3
.x 8
)
follow
(u 3
,u 2
)
u 3=
statement
speaker
(u 2
,s A
)
follow
(u 2
,u 1
)
u 2=
no_answer
speaker
(u 1
,s B
)
content
(u 1
, x 3
)
s A,
s B
u 1=
yes_no_ques
tion
Cla
us
es
Te
rm
s
sub
j(x
4,x
2)
obj
(x 4
,x 3
)
sub
j(x
5,x
1)
obj
(x 5
,x 4
)
x 1=
B, 
x 2=
A
x 3=
Sle
epi
ng
wit
h t
he 
En
em
y
x 4=
wa
tch
x 5=
sug
ges
ts
Cla
us
es
Te
rm
s
(b) augmented representation of the conversation seg-
ment
Figure 2: The dependency and conversation structures and corresponding augmented representation of Example 2
760
describe the path from one constituent to the other:
v1e1 . . . vl?1el?1vl, where v1, . . . , vl are the vertices
on the path and e1, . . . , el?1 are the edges. Each vi
describes the type of the vertex in the dependency
structure, which is either a noun (N ), a verb (V ),
or an utterance (U ). Each ei describes whether the
edge is forward (?) or backward (?). For ex-
ample, in Figure 2(a), the path from x11 to x9 is
V ? V ? V ? N .
This kind of string representation of paths in syn-
tactic parse is known as a way of modeling ?shal-
low semantics? between any two constituents in a
language structure. It is largely used in other NLP
tasks such as semantic role labeling (Pradhan et al,
2008). The difference here is our paths are extracted
from dependency parses as opposed to traditional
constituent parses, and our paths also incorporate the
representation of conversation structures (e.g., utter-
ances and speakers).
6 Applications in Entailment Models
In this section we describe how different representa-
tions and modeling of LDR are used in the alignment
and inference models.
6.1 Applications in Alignment Model
Although a noun and a verb can potentially be
aligned, to simplify the problem, we restrict the
problem to the alignment between two nouns or two
verbs. We trained an alignment model for nouns and
one for verbs separately.
Table 1 summarizes a set of features used in the
alignment models. Most of these features are shared
by the model for noun alignment and the model for
verb alignment. These features include whether the
two strings are the same, two terms have the same
stem, the similarity between the two terms either
based on WordNet or distributional statistics (Lin,
1998).
To learn the alignment model for nouns, we anno-
tated the noun alignments for the development data
used in PASCAL RTE-3 Challenge (Giampiccolo et
al., 2007) and trained a logistic regression model
based on the features in Table 1. Cross-validation
on the same dataset shows relatively satisfying per-
formance (96.4% precision and 94.9% recall). In
this paper, we focus on the alignment between verbs
Noun Verb
Align. Align.
Verb be identification X
String equality X X
Stemmed equality X X
Acronym equality X
Named entity equality X
WordNet similarity X X
Distributional similarity X X
Subject consistency X
Object consistency X
Table 1: Features for alignment models
since it appears more difficult.
A major difference between noun alignment and
verb alignment is that, for verb alignment the con-
sistency of their arguments is also important. For
two events (described by two verbs) to be aligned, at
least their subjects (usually denoting the executers of
actions) and objects (usually denoting the receivers
of actions) should match to each other respectively.
Note that, although actions/events also depend on
other arguments or adjuncts, here we only consider
the subjects and objects and leave the consistency
check of other arguments/adjuncts to downstream
processes. Based on two different ways of model-
ing long distance relationship (as described in Sec-
tion 5), we explored two methods for modeling ar-
gument consistency (AC) in verb alignment models.
6.1.1 Implicit Modeling of AC
The first approach models argument consistency
based on implicit modeling of the relationship be-
tween a verb and its aligned subject/object. Specif-
ically, given a pair of verb terms (x, y) where x is
from the conversation segment and y is from the hy-
pothesis, let sy be the subject of y and sx be the
aligned entity of sy in the conversation (in case of
multiple alignments, sx is the one closest to x). The
subject consistency of the verbs (x, y) is then mea-
sured by the distance between sx and x in the de-
pendency structure. Similarly, the distance between
a verb and its aligned object is used as a measure of
the object consistency.
In Example 2, to decide whether the conversa-
tion term see (x11 in Figure 1(a), 1(b), and 2) and
the hypothesis term watch (x4 in Figure 1(c), 1(d))
should be aligned, we first identify the subject of x4
in the hypothesis, which is x2 (A). We then look for
761
x2?s alignments in the conversation segment, among
which x9 (You) is the closest to x11 (see). In Fig-
ure 2(a), we find the distance between x11 and x9 is
3.
Using the implicit modeling of argument consis-
tency, we follow the same approach as in our pre-
vious work (Zhang and Chai, 2009) and trained a
logistic regression model to predict verb alignment
based on the features in Table 1.
6.1.2 Explicit Modeling of AC
The second approach captures argument consis-
tency based on explicit modeling of the relationship
between a verb and its aligned subject (or object).
Given a pair of verb terms (x, y), let sy be the sub-
ject of y and sx be the aligned entity of sy in the
conversation closest to x, we use the string describ-
ing the path from x to sx as the feature to capture
subject consistency. For example, in Figure 2(a), the
path from x11 to x9 is V ? V ? V ? N .
This string representation of paths is used to cap-
ture both the subject consistency and the object con-
sistency. Since they are non-numerical features, and
the variability of their values can be extremely large,
so we applied an instance-based classification model
(e.g., k-nearest neighbor) to determine alignments
between verb terms. We measure the distance be-
tween two path features by their minimal string edit
distance, and then simply use the Euclidean distance
to measure the closeness between any two verbs.
Again this model is trained from our development
data described in Zhang and Chai (2009).
Figure 3 shows an example of alignment between
the conversation terms and hypothesis terms in Ex-
ample 2. Note that in this figure the alignment
between x5 = suggests from the hypothesis and
u4 = opinion from the conversation segment is a
pseudo alignment, which directly maps a verb term
in the hypothesis to an utterance term represented
by its dialogue act. This alignment is obtained by
following the same set of rules learned from the de-
velopment dataset as in (Zhang and Chai, 2009).
6.2 Applications in Inference Model
As mentioned earlier, once an alignment is estab-
lished, the inference model is to predict whether
each clause in the hypothesis is entailed from the
conversation segment. Two separate models were
x 4=
ha
ve
x 5=Ax 2=
Sle
ep
ing
 
wi
th 
the
 En
em
y
x 1=A x 7=
is r
ea
lly
gre
at
x 10
=o
ne
u 4=
op
ini
on
Conversation Segment
x 3=
Sle
ep
ing
 
wi
th 
the
 En
em
y
x 5=
sug
ge
sts
x 2=
A
x 1=
B
x 4=
wa
tch
Hypothesis
x 6=
tha
t
x 11
=s
ee
x 12
=g
o
x 13
=h
av
e
x 8=
ha
ve 
he
ard
u 3=
sta
tem
en
t
u 2=
no
_an
sw
er
u 1=
yes
_no
_que
sti
on
x 3=
see
n
x 9=As B s A
Figure 3: The alignment result for Example 2
used to handle the inference of property clauses
(hj(x)) and and the inference of relational clauses
(hj(x, y)). Property clauses involve less variables
and are relatively simple, so we used the same prop-
erty inference model as in (Zhang and Chai, 2009).
Here we focus on relational inference model and ex-
amine how different modeling of long distance rela-
tionship may affect relation inference.
For a relation h between x and y to be entailed
from a conversation segment, we need to find a same
or similar relation in the conversation segment be-
tween x?s and y?s counterparts (i.e., aligned entities
of x and y in the conversation segment).
More specifically, given a relational clause from
the hypothesis, hj(x, y), we find the sets of
terms X ? = {x?|x? ? D, g(x?, x) = 1} and Y ? =
{y?|y? ? D, g(y?, y) = 1}, which are aligned with x
and y, respectively. We then find the closest re-
lation between these two sets of terms, (x?, y?),
such that the distance between x? and y? is the
smallest for any x? ? X ? and y? ? Y ?. For in-
stance, in the hypothesis of Example 2 there are
terms x5=suggests and x4=watch, and a relational
clause obj(x5, x4) describing an action-object rela-
tion between them. Their counterparts in the con-
762
versation segment are X ? = {u4=viewpoint} and
Y ? = {x3=seen, x11=see}. So the closest pair of
terms between these two sets is u4 and x11. Conse-
quently, whether the target relational clause hj(x, y)
is entailed is determined by the relationship between
x? and y?. Such relationship can be modeled either
implicitly or explicitly.
6.3 Implicit modeling of relation inference
In this model we follow the simple idea that the
shorter a path is between two terms, the more likely
these two terms have a direct relationship. So we
predefine a threshold, ?L. We predict that hj(x, y) is
entailed if the distance between x? and y? is smaller
than ?L. However, as can be seen, this distance does
not reflect whether the type of relationship between
x? and y? is similar to the relationship that holds be-
tween x and y.
6.4 Explicit modeling of relation inference
In order to capture more semantics from the rela-
tion between two terms, we use explicit modeling
of the relationship between terms x? and y?. In
the previous example, the relationship between u4
and x11 is modeled by the path from u4 to x11,
U ? V ? V ? V .
Given this characterization, the prediction of
whether hj(x, y) is entailed from the conversation
segment is formulated as a binary classification
problem, using a k-nearest neighbor classification
model with following features:
1. Explicit modeling of long distance relationship,
i.e., the path from x? to y? in the dependency
structure of the conversation segment;
2. The types (N, V, or U) of x, y, x?, and y?;
3. The type of relation between x and y, for ex-
ample, obj in obj(x, y);
4. The order (i.e., before or after) between x and
y, and between x? and y?;
5. The specific type of the hypothesis.
7 Evaluation and Analysis
We evaluated different model configurations using
our data1. This dataset consists of 291 development
instances and 584 testing instances. The hypotheses
1The data is available for download at http:
//links.cse.msu.edu:8000/lair/projects/
conversationentailment_data.html.
(a) Based on basic representation
(b) Based on augmented representation
Figure 4: Evaluation of verb alignment
were categorized into four types: (1) fact: profile
and social relations of conversation participants (ac-
counted for 47% of the development data and 49%
of the testing data); (2) belief: participants? beliefs
and opinions (34% and 35%); (3) desire: partici-
pants? desire of certain actions or outcomes (11%
and 4%); (4) intent: communicative intent that cap-
tures some perlocutionary force from one participant
to the other (e.g. A stops B from doing something;
A disagreees with B on something, 8% and 12%)
Note that in our original work (Zhang and Chai,
2009), only development data were used to show
some initial observations. Here we trained our mod-
els on the development data and results shown are
from the testing data.
7.1 Evaluation of Alignment Models
The evaluation of alignment models is based on pair-
wise decision. For each pair of terms (x, y), where
x is from a conversation segment and y is from
a hypothesis, we measure whether the model cor-
rectly predicts that the two terms should or should
not be aligned. Because the alignment classification
has extremely unbalanced classes, we use precision-
recall of true alignments as evaluation metrics.
Figure 4(a) and 4(b) shows the comparison (F-
measure) of two alignment models for verb align-
763
Figure 5: Evaluation of inference models based on different representations
ment, based on the basic representation and the aug-
mented representation, respectively. Note that we
cannot directly compare the results between these
two figures since they involve different number of
alignment instances2. Nevertheless, we can see the
overall trend within each figure: the explicit model
outperforms the implicit model. This suggests that
the explicit modeling of semantic relationship be-
tween verbs and arguments works better than the im-
plicit modeling used in previous work. Furthermore,
the improvement is most noticeable when hypothe-
ses are facts (24.8% with the basic representation
and 24.1% with the augmented representation), and
least when hypotheses are intents (12.2% with the
basic representation and 6.2% with the augmented
representation).
7.2 Evaluation of Inference Models
In order to compare different inference models, in
this section (and this section only) we use gold-
standard alignment results. They are obtained from
manual annotation in our evaluation. We evaluated
two inference models, one with implicit modeling
of long distance relationship and one with explicit
modeling. Evaluations were conducted based on
both the basic representation and the augmented rep-
resentation. Figure 5 shows the four groups of eval-
uation results.
Overall speaking, the augmented representation
outperforms the basic representation for both im-
plicit modeling and explicit modeling of long dis-
tance relationship (McNemar?s tests, p < 0.05). The
explicit model performs better than implicit model
only based on augmented representation (McNe-
mar?s test, p < 0.05).
2The alignment based on the augmented representation in
Figure 4(b) also includes pseudo alignments.
Clause Rep- Relation modeling Improve-
resentation Implicit Explicit ment
Basic 53.9% 53.9% 0
Augmented 54.8% 58.7% 3.9%
Table 2: Entailment performance with different represen-
tations and LDR modeling
The results were further broken down by different
hypothesis types. For the fact type of hypotheses,
there is no difference between different represen-
tations and modeling of long distance relationship.
This is not surprising since most hypotheses about
partipants? profiling information can be inferred di-
rectly from the utterances. The augmented repre-
sentation affects the intent type of hypothesis most
significantly, so does the explicit modeling of long
distance relationship.
7.3 Interaction between Clause
Representations and LDR Modeling
It was shown in previous sections that the aug-
mented representation helps entailment prediction
compared to the basic representation. Here we want
to study how they interact with other entailment
components and what is their effect in the enhanced
modeling of long distance relations. Specifically, we
test the performance of implicit and explicit mod-
eling of long distance relations under two different
representation settings: the basic representation and
the augmented representation.
Table 2 compares the performance (accuracy) of
entailment models with different relationship mod-
eling. We can see that the explicit model makes im-
provement over the implicit model for augmented
representation (McNemar?s test, p < 0.05), while
no improvement is made for basic representation.
These evaluation results appear to suggest that there
764
is an interaction between clause representations and
semantic modeling of long distance relations: the
modeling of long distance relations between lan-
guage constituents appears only effective when con-
versation structure is incorporated in the representa-
tion.
It is interesting to see the difference in the predic-
tion performances on fact hypotheses and intent hy-
potheses. For fact, the most benefit of incorporating
explicit modeling of long distance relationship ap-
pears at the alignment stage, but not much at the in-
ference stage. However, this situation is different for
intent, where the benefit of explicitly modeling long
distance relationship mostly happened at the infer-
ence stage. This observation suggests that the effects
of different types of modeling may vary for different
types of hypotheses, which indicates that hypothesis
type dependent models may be beneficial.
8 Discussion and Conclusion
This paper presents an empirical investigation on
conversation entailment. We specifically examine
two levels of representation of conversation seg-
ments and two different ways of modeling long dis-
tance relations between language constituents. Our
findings indicate that, although traditional architec-
ture and approaches for textual entailment remain
important, additional representation and processing
that address conversation structures is critical. The
augmented representation with conversation struc-
tures, together with explicit modeling of semantic
relations between language constituents, results in
the best performance (58.7% accuracy).
The work here only represents an initial step to-
wards conversation entailment. Conversation phe-
nomena are rich and complex. Conversation entail-
ment is extremely difficult. Besides the same chal-
lenges faced by textual entailment, it is further com-
plicated by conversation implicature. Although our
current data enables us to start an initial investiga-
tion, its small size poses significant limitations on
technology development and evaluation. For ex-
ample, our studies have indicated hypothesis type-
dependent approaches may be beneficial, however
we do not have sufficient data to yield reasonable
models. A more systematical approach to collect
and create a larger set of data is crucial. Inno-
vative community-based approaches (e.g., through
web) for data collection and annotation can be pur-
sued in the future. As more techniques in semantic
processing (e.g., semantic role) become available,
future work should also capture deeper semantics,
address pragmatics, and incorporate richer world
knowledge.
Finally, as the technology in conversation entail-
ment is developed, its applications in NLP problems
should be explored. Example applications include
information extraction, question answering, summa-
rization from conversation scripts, and modeling of
conversation participants. These applications may
provide new insights on the nature of the conversa-
tion entailment problem and its potential solutions.
Acknowledgments
This work was supported by grant IIS-0347548 from
the National Science Foundation. We thank the
anonymous reviewers for their valuable comments
and suggestions.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, Venice, Italy.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of the Second Text Analysis Conference
(TAC 2009).
Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of
HLT-EMNLP, pages 628?635.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In PASCAL Challenges Workshop on Recognis-
ing Textual Entailment.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL, pages 669?676.
765
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710?718, Suntec, Singapore, Au-
gust.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9.
Danilo Giampiccolo, Hoa Trang Dang, Bernardog
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth pascal recognizing textual entail-
ment challenge. In Proceedings of the First Text Anal-
ysis Conference (TAC 2008).
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Linguistic Data Consor-
tium, Philadelphia.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of HLT-EMNLP, pages 387?394.
Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2007. Extracting social networks and biographical
facts from conversational speech transcripts. In Pro-
ceedings of ACL, pages 1040?1047.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423?430, Morristown, NJ,
USA.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296?304.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of HLT-NAACL,
pages 41?48.
Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 773?782, Hon-
olulu, Hawaii, October.
Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Compu-
tational Linguistics, 34(2):289?310.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI, pages
1099?1105.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 172?180, Los An-
geles, California, June.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the 8th SIGdial Workshop
on Discourse and Dialogue, Antwerp, September.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 801?808, Manchester, UK, August.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 170?179, Singapore,
August.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of HLT-EMNLP, pages 371?378.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In ACL-44: Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 401?408,
Morristown, NJ, USA.
Chen Zhang and Joyce Chai. 2009. What do we know
about conversation participants: Experiments on con-
versation entailment. In Proceedings of the SIGDIAL
2009 Conference, pages 206?215.
766
