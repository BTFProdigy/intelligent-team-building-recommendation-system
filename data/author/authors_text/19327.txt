Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 89?92,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
RelationFactory: A Fast, Modular and Effective System for Knowledge
Base Population
Benjamin Roth
?
Tassilo Barth
?
Grzegorz Chrupa?a
*
Martin Gropp
?
Dietrich Klakow
?
?
Spoken Language Systems, Saarland University, 66123 Saarbr?ucken, Germany
*
Tilburg University, PO Box 90153, 5000 LE Tilburg, The Netherlands
?
{beroth|tbarth|mgropp|dietrich.klakow}@lsv.uni-saarland.de
*
g.chrupala@uvt.nl
Abstract
We present RelationFactory, a highly ef-
fective open source relation extraction sys-
tem based on shallow modeling tech-
niques. RelationFactory emphasizes mod-
ularity, is easily configurable and uses a
transparent pipelined approach.
The interactive demo allows the user to
pose queries for which RelationFactory re-
trieves and analyses contexts that contain
relational information about the query en-
tity. Additionally, a recall error analy-
sis component categorizes and illustrates
cases in which the system missed a correct
answer.
1 Introduction and Overview
Knowledge base population (KBP) is the
task of finding relational information in large
text corpora, and structuring and tabulariz-
ing that information in a knowledge base.
Given an entity (e.g. of type PERSON) with
an associated relational schema (a set of re-
lations, e.g. city of birth(PERSON,
CITY), schools attended(PERSON,
ORGANIZATION), spouse(PERSON,
PERSON)), all relations about the entity that
are expressed in a text corpus would be rele-
vant, and the correct answers would have to be
extracted.
The TAC KBP benchmarks
1
are an effort to for-
malize this task and give researchers in the field
the opportunity to evaluate their algorithms on a
set of currently 41 relations. In TAC KBP, the
task and evaluation setup is established by well-
defined information needs about query entities of
types PERSON and ORGANIZATION (e.g. who is
the spouse of a person, how many employees
1
http://www.nist.gov/tac/about/
does an organization have). A perfect system
would have to return all relevant information (and
only this) contained in the text corpus. TAC KBP
aims at giving a realistic picture of not only pre-
cision but also recall of relation extraction sys-
tems on big corpora, and is therefore an advance-
ment over many other evaluations done for rela-
tion extraction that are often precision oriented
(Suchanek et al., 2007) or restrict the gold key to
answers from a fixed candidate set (Surdeanu et
al., 2012) or to answers contained in a data base
(Riedel et al., 2010). Similar to the classical TREC
evaluation campaigns in document retrieval, TAC
KBP aims at approaching a true recall estimate by
pooling, i.e. merging the answers of a timed-out
manual search with the answers of all participat-
ing systems. The pooled answers are then evalu-
ated by human judges.
It is a big advantage of TAC KBP that the end-
to-end setup (from the query, through retrieval of
candidate contexts and judging whether a relation
is expressed, to normalizing answers and putting
them into a knowledge base) is realistic. At the
same time, the task is very complex and may in-
volve too much work overhead for researchers
only interested in a particular step in relation ex-
traction such as matching and disambiguation of
entities, or judging relational contexts. We there-
fore introduce RelationFactory, a fast, modular
and effective relation extraction system, to the re-
search community as open source software.
2
Rela-
tionFactory is based on distantly supervised classi-
fiers and patterns (Roth et al., 2013), and was top-
ranked (out of 18 systems) in the TAC KBP 2013
English Slot-filling benchmark (Surdeanu, 2013).
In this demo, we give potential users the possi-
bility to interact with the system and to get a feel
for use cases, strengths and limitations of the cur-
rent state of the art in knowledge base population.
2
https://github.com/beroth/
relationfactory
89
The demo illustrates how RelationFactory arrives
at its conclusions and where future potentials in
relation extraction lie. We believe that Relation-
Factory provides an easy start for researchers in-
terested in relation extraction, and we hope that
it may serve as a baseline for new advances in
knowledge base population.
2 System Philosophy and Design
Principles
The design principles of RelationFactory conform
to what is known as the Unix philosophy.
3
For Re-
lationFactory this philosophy amounts to a set of
modules that solve a certain step in the pipeline
and can be run (and tested) independently of the
other modules. For most modules, input and out-
put formats are column-based text representations
that can be conveniently processed with standard
Linux tools for easy diagnostics or prototyping.
Data representation is compact: the system is de-
signed in a way that each module ideally outputs
one new file. Because of modularization and sim-
ple input and output formats, RelationFactory al-
lows for easy extensibility, e.g. for research that
focuses solely on novel algorithms at the predic-
tion stage.
The single modules are connected by a make-
file that controls the data flow and allows for easy
parallelization. RelationFactory is highly config-
urable: new relations can be added without chang-
ing any of the source code, only by changing con-
figuration files and adding or training respective
relational models.
Furthermore, RelationFactory is designed to be
highly scalable: Thanks to feature hashing, large
amounts of training data can be used in a memory-
friendly way. Predicting relations in real-time is
possible using shallow representations. Surface
patterns, ngrams and skip-ngrams allow for highly
accurate relational modeling (Roth et al., 2013),
without incurring the cost of resource-intensive
processing, such as parsing.
3
One popular set of tenets (Gancarz, 2003) summarizes
the Unix philosophy as:
1. Small is beautiful.
2. Make each program do one thing well.
3. Build a prototype as soon as possible.
4. Choose portability over efficiency.
5. Store data in flat text files.
6. Use software leverage to your advantage.
7. Use shell scripts to increase leverage and portability.
8. Avoid captive user interfaces.
9. Make every program a filter.
Figure 1: TAC KBP: Given a set of queries, return
a correct, complete and non-redundant response
with relevant information extracted from the text
corpus.
Figure 2: Data flow of the relation extraction sys-
tem: The candidate generation stage retrieves pos-
sible relational contexts. The candidate validation
stage predicts whether relations actually hold and
produces a valid response.
3 Component Overview
A simplified input and output to RelationFactory
is shown in Figure 1. In general, the pipeline
is divided in a candidate generation stage, where
documents are retrieved and candidate sentences
are identified, and the candidate validation stage,
which predicts and generates a response from the
retrieved candidates (see Figure 2).
In a first step, the system generates aliases for
the query using statistical and rule-based expan-
sion methods, for example:
Query Expansion
Adam Gadahn Azzam the American, Adam Yahiye Gadahn, Gadahn
STX Finland Kvaerner Masa Yards, Aker Finnyards, STX Finland Ltd
The expansions are used for retrieving docu-
ments from a Lucene index. All those sen-
90
tences are retained where the query (or one of
the query aliases) is contained and the named-
entity tagger has identified another entity with
the type of a potential answer for one of the
sought relations. The system is easily con-
figurable to include matching of non-standard
named-entity types from lists. RelationFac-
tory uses lists obtained from Freebase (www.
freebase.com) to match answer candidates
for the types CAUSE-OF-DEATH, JOB-TITLE,
CRIMINAL-CHARGES and RELIGION.
The candidate sentences are output line-by-line
and processed by one of the validation modules,
which determine whether actually one of the rela-
tions is expressed. RelationFactory currently uses
three standard validation modules: One based on
SVM classifiers, one based on automatically in-
duced and scored patterns, and one based on man-
ually crafted patterns. The validation modules
function as a filter to the candidates file. They
do not have to add a particular formatting or con-
form to other requirements of the KBP task such
as establishing non-redundancy or finding the cor-
rect offsets in the text corpus. This is done by
other modules in the pipeline, most notably in
the post-processing step, where statistical meth-
ods and heuristics are applied to produce a well-
formed TAC KBP response.
4 User Perspective
From a user perspective, running the system is as
easy as calling:
./run.sh system.config
The configuration file contains all information
about the general run configuration of the system,
such as the query file to use, the format of the re-
sponse file (e.g. TAC 2012 or TAC 2013 format),
the run directory that will contain the response,
and the Lucene index with the corpus. Optional
configuration can control non-standard validation
modules, and special low or high-recall query ex-
pansion schemes.
The relevant parts of the configuration file for a
standard 2013 TAC KBP run would look like the
following:
query /TAC_EVAL/2013/query.xml
goal response2013
rundir /TAC_RUNS/run2013/
index /TAC_CORPORA/2013/index
rellist /CFG/rellist2013
relations.config /CFG/relations2013.config
The last two lines refer to relation-specific con-
figuration files: The list of relations to use and in-
formation about them. Changing these files (and
adding respective models) allows for inclusion of
further relations. The relation-specific configura-
tion file contains information about the query en-
tity type, the expected answer named-entity tag
and whether a list of answers is expected (com-
pared to relations with just one correct answer):
per:religion enttype PER
per:religion argtag RELIGION
per:religion listtype false
org:top_members_employees enttype ORG
org:top_members_employees argtag PERSON
org:top_members_employees listtype true
RelationFactory comes with batteries included:
The models and configurations for TAC KBP 2013
work out-of-the-box and can easily be used as a
relation extraction module in a bigger setting or as
a baseline for new experiments.
4
5 Illustrating RelationFactory
In TAC KBP 2013, 6 out of 18 systems achieved
an F1 score of over 30%. RelationFactory as
the top-performing system achieved 37.28% com-
pared to 68.49% achieved by human control an-
notators (Surdeanu, 2013). These numbers clearly
show that current systems have just gone halfway
toward achieving human-like performance on an
end-to-end relation extraction task.
The aim of the RelationFactory demo is to il-
lustrate what the current challenges in TAC KBP
are. The demonstration interface therefore not
only shows the answers generated for populating
a potential knowledge base, but also what text was
used to justify the extraction.
The real-time performance of RelationFactory
allows for trying arbitrary queries and changing
the configuration files and immediately seeing the
effects. Different expansion schemes, validation
modules and patterns can be turned on and off, and
intuitions can be obtained about the bottlenecks
and error sources of relation extraction. The demo
also allows for seeing the effect of extracting infor-
mation from different corpora: a Wikipedia corpus
and different TAC KBP corpora, such as newswire
and web text.
4
Training models for new relations requires is a bigger
effort and includes generation of distant supervision train-
ing data by getting argument pairs from relational patterns
or a knowledge base like Freebase. RelationFactory includes
some training scripts but since they are typically run once
only, they are significantly less documented.
91
Figure 3: Screenshot of the RelationFactory demo user interface.
RelationFactory contains a number of diagnos-
tic tools: With a gold key for a set of queries, error
classes can be broken down and examples for cer-
tain error classes can be shown. For example, the
diagnostic tool for missed recall performs the fol-
lowing checks:
1. Is document retrieved?
2. Is query matched? This determines whether a sen-
tence is considered for further processing.
3. Is answer in query sentence? Whether the answer is
in one of the sentences with the query. Our system only
can find answers when this is the case, as there is no co-
reference module included.
4. Do answer tags overlap with gold answer?
5. Do they overlap exactly?
6. Other (validation). If all previous checks are passed,
the candidate has correctly been generated by the can-
didate generation stage, but the validation modules
have failed to predict the relation.
On the TAC KBP 2013 queries, the resulting re-
call error analysis is:
error class missing recall
Doc not retrieved 5.59%
Query not matched 10.37%
Answer not in query sentence 16.63%
Answer tag inexact 5.36%
Answer not tagged 24.85%
Other (validation) 37.17%
The demonstration tool allows for inspection of
instances of each of the error classes.
6 Conclusion
This paper illustrates RelationFactory, a modular
open source knowledge-base population system.
We believe that RelationFactory will become es-
pecially valuable for researchers in the field of re-
lation extraction that focus on one particular prob-
lem of knowledge-base-population (such as entity
expansion or relation prediction) and want to inte-
grate their algorithms in an end-to-end setting.
Acknowledgments
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by this Google
Fellowship. Tassilo Barth was supported in part
by IARPA contract number W911NF-12-C-0015.
References
Mike Gancarz. 2003. Linux and the Unix philosophy.
Digital Press.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148?163.
Springer.
Benjamin Roth, Tassilo Barth, Michael Wiegand, Mit-
tul Singh, and Dietrich Klakow. 2013. Effective slot
filling based on shallow distant supervision methods.
In Proceedings of the Sixth Text Analysis Conference
(TAC 2013).
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697?706. ACM.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing and Natural
Language Learning (EMNLP-CoNLL), pages 455?
465. ACL.
Mihai Surdeanu. 2013. Overview of the tac2013
knowledge base population evaluation: English slot
filling and temporal slot filling. In Proceedings of
the Sixth Text Analysis Conference (TAC 2013).
92
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 680?686,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Normalizing tweets with edit scripts and recurrent neural embeddings
Grzegorz Chrupa?a
Tilburg Center for Cognition and Communication
Tilburg University
g.chrupala@uvt.nl
Abstract
Tweets often contain a large proportion of
abbreviations, alternative spellings, novel
words and other non-canonical language.
These features are problematic for stan-
dard language analysis tools and it can
be desirable to convert them to canoni-
cal form. We propose a novel text nor-
malization model based on learning edit
operations from labeled data while incor-
porating features induced from unlabeled
data via character-level neural text embed-
dings. The text embeddings are generated
using an Simple Recurrent Network. We
find that enriching the feature set with text
embeddings substantially lowers word er-
ror rates on an English tweet normaliza-
tion dataset. Our model improves on state-
of-the-art with little training data and with-
out any lexical resources.
1 Introduction
A stream of posts from Twitter contains text writ-
ten in a large variety of languages and writing sys-
tems, in registers ranging from formal to inter-
net slang. Substantial effort has been expended
in recent years to adapt standard NLP process-
ing pipelines to be able to deal with such con-
tent. One approach has been text normaliza-
tion, i.e. transforming tweet text into a more
canonical form which standard NLP tools ex-
pect. A multitude of resources and approaches
have been used to deal with normalization: hand-
crafted and (semi-)automatically induced dictio-
naries, language models, finite state transduc-
ers, machine translation models and combinations
thereof. Methods such as those of Han and Bald-
win (2011), Liu et al (2011), Gouws et al (2011)
or Han et al (2012) are unsupervised but they
typically use many adjustable parameters which
need to be tuned on some annotated data. In this
work we suggest a simple, supervised character-
level string transduction model which easily incor-
porates features automatically learned from large
amounts of unlabeled data and needs only a lim-
ited amount of labeled training data and no lexical
resources.
Our model learns sequences of edit operations
from labeled data using a Conditional Random
Field (Lafferty et al, 2001). Unlabeled data
is incorporated following recent work on using
character-level text embeddings for text segmen-
tation (Chrupa?a, 2013), and word and sentence
boundary detection (Evang et al, 2013). We
train a recurrent neural network language model
(Mikolov et al, 2010; Mikolov, 2012b) on a large
collection of tweets. When run on new strings, the
activations of the units in the hidden layer at each
position in the string are recorded and used as fea-
tures for training the string transduction model.
The principal contributions of our work are: (i)
we show that a discriminative sequence labeling
model is apt for text normalization and performs
at state-of-the-art levels with small amounts of la-
beled training data; (ii) we show that character-
level neural text embeddings can be used to effec-
tively incorporate information from unlabeled data
into the model and can substantially boost text nor-
malization performance.
2 Methods
Many approaches to text normalization adopt the
noisy channel setting, where the model normaliz-
ing source string s into target canonical form t is
factored into two parts:
?
t = argmax
t
P (t)P (s|t).
The error term P (s|t) models how canonical
strings are transformed into variants such as e.g.
misspellings, emphatic lengthenings or abbrevia-
tions. The language model P (t) encodes which
target strings are probable.
We think this decomposition is less appropriate
680
Input c w a t
Edit DEL INS(see) NIL INS(h) NIL
Output see w ha t
Table 1: Example edit script.
in the context of text normalization than in appli-
cations from which it was borrowed such as Ma-
chine Translations. This is because it is not obvi-
ous what kind of data can be used to estimate the
language model: there is plentiful text from the
source domain, but little of it is in normalized tar-
get form. There is also much edited text such as
news text, but it comes from a very different do-
main. One of the main advantages of the noisy
channel decomposition is that is makes it easy to
exploit large amounts of unlabeled data in the form
of a language model. This advantage does not hold
for text normalization.
We thus propose an alternative approach where
normalization is modeled directly, and which en-
ables easy incorporation of unlabeled data from
the source domain.
2.1 Learning to transduce strings
Our string transduction model works by learning
the sequence of edits which transform the input
string into the output string. Given a pair of strings
such a sequence of edits (known as the shortest
edit script) can be found using the DIFF algorithm
(Miller and Myers, 1985; Myers, 1986). Our ver-
sion of DIFF uses the following types of edits:
? NIL ? no edits,
? DEL ? delete character at this position,
? INS(?) ? insert specified string before charac-
ter at this position.
1
Table 1 shows a shortest edit script for the pair
of strings (c wat, see what).
We use a sequence labeling model to learn to
label input strings with edit scripts. The train-
ing data for the model is generated by comput-
ing shortest edit scripts for pairs of original and
normalized strings. As a sequence labeler we use
Conditional Random Fields (Lafferty et al, 2001).
Once trained the model is used to label new strings
and the predicted edit script is applied to the in-
put string producing the normalized output string.
Given source string s the predicted target string
?
t
1
The input string is extended with an empty symbol to
account for the cases where an insertion is needed at the end
of the string.
is:
?
t = argmax
t
P (ses(s, t)|s)
where e = ses(s, t) is the shortest edit script map-
ping s to t. P (e|s) is modeled with a linear-chain
Conditional Random Field.
2.2 Character-level text embeddings
Simple Recurrent Networks (SRNs) were intro-
duced by Elman (1990) as models of temporal, or
sequential, structure in data, including linguistic
data (Elman, 1991). More recently SRNs were
used as language models for speech recognition
and shown to outperform classical n-gram lan-
guage models (Mikolov et al, 2010; Mikolov,
2012b). Another version of recurrent neural nets
has been used to generate plausible text with a
character-level language model (Sutskever et al,
2011). We use SRNs to induce character-level text
representations from unlabeled Twitter data to use
as features in the string transduction model.
The units in the hidden layer at time t receive
connections from input units at time t and also
from the hidden units at the previous time step
t ? 1. The hidden layer predicts the state of the
output units at the next time step t + 1. The input
vector w(t) represents the input element at current
time step, here the current character. The output
vector y(t) represents the predicted probabilities
for the next character. The activation s
j
of a hid-
den unit j is a function of the current input and the
state of the hidden layer at the previous time step:
t? 1:
s
j
(t) = ?
(
I
?
i=1
w
i
(t)U
ji
+
L
?
l=1
s
j
(t? 1)W
jl
)
where ? is the sigmoid function and U
ji
is the
weight between input component i and hidden unit
j, while W
jl
is the weight between hidden unit l
at time t ? 1 and hidden unit j at time t. The
representation of recent history is stored in a lim-
ited number of recurrently connected hidden units.
This forces the network to make the representation
compressed and abstract rather than just memo-
rize literal history. Chrupa?a (2013) and Evang
et al (2013) show that these text embeddings can
be useful as features in textual segmentation tasks.
We use them to bring in information from unla-
beled data into our string transduction model and
then train a character-level SRN language model
on unlabeled tweets. We run the trained model on
681
Figure 1: Tweets randomly generated with an SRN
new tweets and record the activation of the hid-
den layer at each position as the model predicts the
next character. These activation vectors form our
text embeddings: they are discretized and used as
input features to the supervised sequence labeler
as described in Section 3.4.
3 Experimental Setup
We limit the size of the string alphabet by always
working with UTF-8 encoded strings, and using
bytes rather than characters as basic units.
3.1 Unlabeled tweets
In order to train our SRN language model we col-
lected a set of tweets using the Twitter sampling
API. We use the raw sample directly without fil-
tering it in any way, relying on the SRN to learn
the structure of the data. The sample consists of
414 million bytes of UTF-8 encoded in a variety
of languages and scripts text. We trained a 400-
hidden-unit SRN, to predict the next byte in the
sequence using backpropagation through time. In-
put bytes were encoded using one-hot representa-
tion. We modified the RNNLM toolkit (Mikolov,
2012a) to record the activations of the hidden layer
and ran it with the default learning rate schedule.
Given that training SRNs on large amounts of text
takes a considerable amount of time we did not
vary the size of the hidden layer. We did try to
filter tweets by language and create specific em-
beddings for English but this had negligible effect
on tweet normalization performance.
The trained SRN language model can be used
to generate random text by sampling the next byte
from its predictive distribution and extending the
string with the result. Figure 1 shows example
strings generated in this way: the network seems
to prefer to output pseudo-tweets written consis-
tently in a single script with words and pseudo-
words mostly from a single language. The gener-
ated byte sequences are valid UTF-8 strings.
In Table 2 in the first column we show the suf-
fix of a string for which the SRN is predicting the
last byte. The rest of each row shows the nearest
neighbors of this string in embedding space, i.e.
should h should d will s will m should a
@justth @neenu @raven @lanae @despic
maybe u maybe y cause i wen i when i
Table 2: Nearest neighbors in embedding space.
strings for which the SRN is activated in a similar
way when predicting its last byte as measured by
cosine similarity.
3.2 Normalization datasets
A difficulty in comparing approaches to tweet nor-
malization is the sparsity of publicly available
datasets. Many authors evaluate on private tweet
collections and/or on the text message corpus of
Choudhury et al (2007).
For English, Han and Baldwin (2011) created
a small tweet dataset annotated with normalized
variants at the word level. It is hard to inter-
pret the results from Han and Baldwin (2011),
as the evaluation is carried out by assuming that
the words to be normalized are known in ad-
vance: Han et al (2012) remedy this shortcoming
by evaluating a number of systems without pre-
specifying ill-formed tokens. Another limitation
is that only word-level normalization is covered in
the annotation; e.g. splitting or merging of words
is not allowed. The dataset is also rather small:
549 tweets, which contain 2139 annotated out-
of-vocabulary (OOV) words. Nevertheless, we
use it here for training and evaluating our model.
This dataset does not specify a development/test
split. In order to maximize the size of the training
data while avoiding tuning on test data we use a
split cross-validation setup: we generate 10 cross-
validation folds, and use 5 of them during devel-
opment to evaluate variants of our model. The best
performing configuration is then evaluated on the
remaining 5 cross-validation folds.
3.3 Model versions
The simplest way to normalize tweets with a string
transduction model is to treat whole tweets as in-
put sequences. Many other tweet normalization
methods work in a word-wise fashion: they first
identify OOV words and then replace them with
normalized forms. Consequently, publicly avail-
able normalization datasets are annotated at word
level. We can emulate this setup by training the se-
quence labeler on words, instead of whole tweets.
This approach sacrifices some generality, since
transformations involving multiple words cannot
682
be learned. However, word-wise models are more
comparable with previous work. We investigated
the following models:
? OOV-ONLY is trained on individual words and
in-vocabulary (IV) words are discarded for
training, and left unchanged for prediction.
2
? ALL-WORDS is trained on all words and al-
lowed to change IV words.
? DOCUMENT is trained on whole tweets.
Model OOV-ONLY exploits the setting when the
task is constrained to only normalize words absent
from a reference dictionary, while DOCUMENT is
the one most generally applicable but does not
benefit from any constraints. To keep model size
within manageable limits we reduced the label set
for models ALL-WORDS and DOCUMENT by re-
placing labels which occur less than twice in the
training data with NIL. For OOV-ONLY we were
able to use the full label set. As our sequence la-
beling model we use the Wapiti implementation
of Conditional Random Fields (Lavergne et al,
2010) with the L-BFGS optimizer and elastic net
regularization with default settings.
3.4 Features
We run experiments with two feature sets: N-
GRAM and N-GRAM+SRN. N-GRAM are char-
acter n-grams of size 1?3 in a window of
(?2,+2) around the current position. For the N-
GRAM+SRN feature set we augment N-GRAM with
features derived from the activations of the hidden
units as the SRN is trying to predict the current
character. In order to use the activations in the
CRF model we discretize them as follows. For
each of the K = 10 most active units out of
total J = 400 hidden units, we create features
(f(1) . . . f(K)) defined as f(k) = 1 if s
j(k)
>
0.5 and f(k) = 0 otherwise, where s
j(k)
returns
the activation of the k
th
most active unit.
3.5 Evaluation metrics
As our evaluation metric we use word error rate
(WER) which is defined as the Levenshtein edit
distance between the predicted word sequence
?
t
and the target word sequence t, normalized by the
total number of words in the target string. A more
generally applicable metric would be character er-
ror rate, but we report WERs to make our results
easily comparable with previous work. Since the
2
We used the IV/OOV annotations in the Han et al (2012)
dataset, which are automatically derived from the aspell dic-
tionary.
Model Features WER (%)
NO-OP 11.7
DOCUMENT NGRAM 6.8
DOCUMENT NGRAM+SRN 5.7
ALL WORDS NGRAM 7.2
ALL WORDS NGRAM+SRN 5.0
OOV-ONLY NGRAM 5.1
OOV-ONLY NGRAM+SRN 4.5
Table 3: WERs on development data.
9 cont continued 5 gon gonna
4 bro brother 4 congrats congratulations
3 yall you 3 pic picture
2 wuz what?s 2 mins minutes
2 juss just 2 fb facebook
Table 4: Improvements from SRN features.
English dataset is pre-tokenized and only covers
word-to-word transformations, this choice has lit-
tle importance here and character error rates show
a similar pattern to word error rates.
4 Results
Table 3 shows the results of our development ex-
periments. NO-OP is a baseline which leaves text
unchanged. As expected the most constrained
model OOV-ONLY outperforms the more generic
models on this dataset. For all model variations,
adding SRN features substantially improves per-
formance: the relative error reductions range from
12% for OOV-ONLY to 30% for ALL-WORDS. Ta-
ble 4 shows the non-unique normalizations made
by the OOV-ONLY model with SRN features which
were missed without them. SRN features seem
to be especially useful for learning long-range,
multi-character edits, e.g. fb for facebook.
Table 5 shows the non-unique normalizations
which were missed by the best model: they are
a mixture of relatively standard variations which
happen to be infrequent in our data, like tonite or
gf, and a few idiosyncratic respellings like uu or
bhee. Our supervised approach makes it easy to
address the first type of failure by simply annotat-
ing additional training examples.
Table 6 presents evaluation results of several ap-
proaches reported in Han et al (2012) as well as
the model which did best in our development ex-
periments. HB-dict is the Internet slang dictio-
nary from Han and Baldwin (2011). GHM-dict
is the automatically constructed dictionary from
683
4 1 one 2 withh with
2 uu you 2 tonite tonight
2 thx thanks 2 thiis this
2 smh somehow 2 outta out
2 n in 2 m am
2 hmwrk homework 2 gf girlfriend
2 fxckin fucking 2 dha the
2 de the 2 d the
2 bhee be 2 bb baby
Table 5: Missed transformations.
Method WER (%)
NO-OP 11.2
HB-dict 6.6
GHM-dict 7.6
S-dict 9.7
Dict-combo 4.9
Dict-combo+HB-norm 7.9
OOV-ONLY NGRAM+SRN (test) 4.8
Table 6: WERs compared to previous work.
Gouws et al (2011); S-dict is the automatically
constructed dictionary from (Han et al, 2012);
Dict-combo are all the dictionaries combined and
Dict-combo+HB-norm are all dictionaries com-
bined with approach of Han and Baldwin (2011).
The WER reported for OOV-ONLY NGRAM+SRN
is on the test folds only. The score on the full
dataset is a bit better: 4.66%. As can be seen our
approach it the best performing approach overall
and in particular it does much better than all of the
single dictionary-based methods. Only the combi-
nation of all the dictionaries comes close in per-
formance.
5 Related work
In the field of tweet normalization the approach
of Liu et al (2011, 2012) shows some similarities
to ours: they gather a collection of OOV words
together with their canonical forms from the web
and train a character-level CRF sequence labeler
on the edit sequences computed from these pairs.
They use this as the error model in a noisy-channel
setup combined with a unigram language model.
In addition to character n-gram features they use
phoneme and syllable features, while we rely on
the SRN embeddings to provide generalized rep-
resentations of input strings.
Kaufmann and Kalita (2010) trained a phrase-
based statistical translation model on a parallel
text message corpus and applied it to tweet nor-
malization. In comparison to our first-order linear-
chain CRF, an MT model with reordering is more
flexible but for this reason needs more training
data. It also suffers from language model mis-
match mentioned in Section 2: optimal results
were obtained by using a low weight for the lan-
guage model trained on a balanced text corpus.
Many other approaches to tweet normalization
are more unsupervised in nature (e.g. Han and
Baldwin, 2011; Gouws et al, 2011; Xue et al,
2011; Han et al, 2012). They still require an-
notated development data for tuning parameters
and a variety of heuristics. Our approach works
well with similar-sized training data, and unlike
unsupervised approaches can easily benefit from
more if it becomes available. Further afield,
our work has connections to research on mor-
phological analysis: for example Chrupa?a et al
(2008) use edit scripts to learn lemmatization rules
while Dreyer et al (2008) propose a discrimina-
tive model for string transductions and apply it
to morphological tasks. While Chrupa?a (2013)
and Evang et al (2013) use character-level SRN
text embeddings for learning segmentation, and
recurrent nets themselves have been used for se-
quence transduction (Graves, 2012), to our knowl-
edge neural text embeddings have not been previ-
ously applied to string transduction.
6 Conclusion
Learning sequences of edit operations from exam-
ples while incorporating unlabeled data via neu-
ral text embeddings constitutes a compelling ap-
proach to tweet normalization. Our results are es-
pecially interesting considering that we trained on
only a small annotated data set and did not use
any other manually created resources such as dic-
tionaries. We want to push performance further
by expanding the training data and incorporating
existing lexical resources. It will also be impor-
tant to check how our method generalizes to other
language and datasets (e.g. de Clercq et al, 2013;
Alegria et al, 2013).
The general form of our model can be used
in settings where normalization is not limited to
word-to-word transformations. We are planning
to find or create data with such characteristics and
evaluate our approach under these conditions.
684
References
I?naki Alegria, Nora Aranberri, V??ctor Fresno, Pablo
Gamallo, Lluis Padr?o, I?naki San Vicente, Jordi
Turmo, and Arkaitz Zubiaga. 2013. Introducci?on a la
tarea compartida Tweet-Norm 2013: Normalizaci?on
l?exica de tuits en espa?nol. In Workshop on Tweet
Normalization at SEPLN (Tweet-Norm), pages 36?
45.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the struc-
ture of texting language. International Journal of
Document Analysis and Recognition (IJDAR), 10(3-
4):157?174.
Grzegorz Chrupa?a. 2013. Text segmentation with
character-level text embeddings. In ICML Workshop
on Deep Learning for Audio, Speech and Language
Processing.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef
Van Genabith. 2008. Learning morphology with
Morfette. In Proceedings of the 6th edition of the
Language Resources and Evaluation Conference.
Orph?ee de Clercq, Bart Desmet, Sarah Schulz, Els
Lefever, and V?eronique Hoste. 2013. Normaliza-
tion of Dutch user-generated content. In 9th Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP-2013), pages 179?
188. INCOMA Ltd.
Markus Dreyer, Jason R Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with finite-state methods. In Proceedings of
the conference on empirical methods in natural lan-
guage processing, pages 1080?1089. Association
for Computational Linguistics.
Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179?211.
Jeffrey L Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical struc-
ture. Machine learning, 7(2-3):195?225.
Kilian Evang, Valerio Basile, Grzegorz Chrupa?a, and
Johan Bos. 2013. Elephant: Sequence labeling
for word and sentence segmentation. In Empirical
Methods in Natural Language Processing.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82?90. Association
for Computational Linguistics.
Alex Graves. 2012. Sequence transduction with recur-
rent neural networks. arXiv:1211.3711.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 368?378.
Association for Computational Linguistics.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421?432. Association for
Computational Linguistics.
Max Kaufmann and Jugal Kalita. 2010. Syntactic
normalization of Twitter messages. In Interna-
tional conference on natural language processing,
Kharagpur, India.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 504?513. As-
sociation for Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 1035?1044. As-
sociation for Computational Linguistics.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normaliz-
ing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 71?76. Association for Computa-
tional Linguistics.
Tom?a?s Mikolov. 2012a. Recurrent neural network lan-
guage models. http://rnnlm.org.
Tom?a?s Mikolov. 2012b. Statistical language models
based on neural networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Tom?a?s Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan
?
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech, pages 1045?1048.
Webb Miller and Eugene W Myers. 1985. A file com-
parison program. Software: Practice and Experi-
ence, 15(11):1025?1040.
Eugene W Myers. 1986. An O(ND) difference algo-
rithm and its variations. Algorithmica, 1(1-4):251?
266.
685
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017?1024.
Zhenzhen Xue, Dawei Yin, and Brian D Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI-
11 Workshop on Analyzing Microtext, pages 74?79.
686
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127?132,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
DCU-UVT: Word-Level Language Classification with Code-Mixed Data
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a
?
and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Tilburg School of Humanities, Department of Communication and Information Sciences
Tilburg University, Tilburg, The Netherlands
{ubarman,jwagner,jfoster}@computing.dcu.ie
G.A.Chrupala@uvt.nl
Abstract
This paper describes the DCU-UVT
team?s participation in the Language Iden-
tification in Code-Switched Data shared
task in the Workshop on Computational
Approaches to Code Switching. Word-
level classification experiments were car-
ried out using a simple dictionary-based
method, linear kernel support vector ma-
chines (SVMs) with and without con-
textual clues, and a k-nearest neighbour
approach. Based on these experiments,
we select our SVM-based system with
contextual clues as our final system and
present results for the Nepali-English and
Spanish-English datasets.
1 Introduction
This paper describes DCU-UVT?s participation
in the shared task Language Identification in
Code-Switched Data (Solorio et al., 2014) at
the Workshop on Computational Approaches to
Code Switching, EMNLP, 2014. The task is to
make word-level predictions (six labels: lang1,
lang2, ne, mixed, ambiguous and other) for mixed-
language user generated content. We submit pre-
dictions for Nepali-English and Spanish-English
data and perform experiments using dictionaries, a
k-nearest neighbour (k-NN) classifier and a linear-
kernel SVM classifier.
In our dictionary-based approach, we investi-
gate the use of different English dictionaries as
well as the training data. In the k-NN based
approach, we use string edit distance, character-
n-gram overlap and context similarity to make
predictions. For the SVM approach, we experi-
ment with context-independent (word, character-
n-grams, length of a word and capitalisation in-
formation) and context-sensitive (adding the pre-
vious and next word as bigrams) features in differ-
ent combinations. We also experiment with adding
features from the k-NN approach and another set
of features from a neural network. Based on per-
formance in cross-validation, we select the SVM
classifier with basic features (word, character-n-
grams, length of a word, capitalisation information
and context) as our final system.
2 Background
While the problem of automatically identify-
ing and analysing code-mixing has been iden-
tified over 30 years ago (Joshi, 1982), it has
only recently drawn wider attention. Specific
problems addressed include language identifica-
tion in multilingual documents, identification of
code-switching points and POS tagging (Solorio
and Liu, 2008b) of code-mixing data. Ap-
proaches taken to the problem of identifying code-
mixing include the use of dictionaries (Nguyen
and Do?gru?oz, 2013; Barman et al., 2014; El-
fardy et al., 2013; Solorio and Liu, 2008b), lan-
guage models (Alex, 2008; Nguyen and Do?gru?oz,
2013; Elfardy et al., 2013), morphological and
phonological analysis (Elfardy et al., 2013; El-
fardy and Diab, 2012) and various machine learn-
ing algorithms such as sequence labelling with
Hidden Markov Models (Farrugia, 2004; Ros-
ner and Farrugia, 2007) and Conditional Random
Fields (Nguyen and Do?gru?oz, 2013; King and
Abney, 2013), as well as word-level classifica-
tion using Naive Bayes (Solorio and Liu, 2008a),
logistic regression (Nguyen and Do?gru?oz, 2013)
and SVMs (Barman et al., 2014), using features
such as word, POS, lemma and character-n-grams.
Language pairs that have been explored include
English-Maltese (Farrugia, 2004; Rosner and Far-
rugia, 2007), English-Spanish (Solorio and Liu,
2008b), Turkish-Dutch (Nguyen and Do?gru?oz,
127
2013), modern standard Arabic-Egyptian di-
alect (Elfardy et al., 2013), Mandarin-English (Li
et al., 2012; Lyu et al., 2010), and English-Hindi-
Bengali (Barman et al., 2014).
3 Data Statistics
The training data provided for this task consists of
tweets. Unfortunately, because of deleted tweets,
the full training set could not be downloaded. Out
of 9,993 Nepali-English training tweets, we were
able to download 9,668 and out of 11,400 Spanish-
English training tweets, we were able to download
11,353. Table 1 shows the token-level statistics of
the two datasets.
Label Nepali-English Spanish-English
lang1 (en) 43,185 76,204
lang2 (ne/es) 59,579 32,477
ne 3,821 2,814
ambiguous 125 341
mixed 112 51
other 34,566 21,813
Table 1: Number of tokens in the Nepali-English
and Spanish-English training data for each label
Nepali (lang2) is the dominant language in
the Nepali-English training data but for Spanish-
English, English (lang1) is dominant. The third
largest group contains tokens with the label other.
These are mentions (@username), punctuation
symbols, emoticons, numbers (except numbers
that represent words such as 2 for to), words in a
language other than lang1 and lang2 and unintel-
ligible words. Named entities (ne) are much less
frequent and mixed language words (e.g. ramri-
ness) and words for which there is not enough con-
text to disambiguate them are rare. Hash tags are
annotated as if the hash symbol was not there, e.g.
#truestory is labelled lang1.
4 Experiments
All experiments are carried out for Nepali-English
data. Later we apply the best approach to Spanish-
English. We train our systems in a five-fold cross-
validation and obtain best parameters based on
average cross-validation results. Cross-validation
splits are made based on users, i.e. we avoid the
occurrence of a user?s tweets both in training and
test splits for each cross-validation run. We ad-
dress the task with the following approaches:
1. a simple dictionary-based classifier,
Resource Accuracy
BNC 43.61
LexNorm 54.60
TrainingData 89.53
TrainingData+BNC+LexNorm 90.71
Table 2: Average cross-validation accuracy of
dictionary-based prediction for Nepali-English
2. classification using supervised machine
learning with k-nearest neighbour, and
3. classification using supervised machine
learning with SVMs.
4.1 Dictionary-Based Detection
We start with a simple dictionary-based approach
using as dictionaries (a) the British National Cor-
pus (BNC) (Aston and Burnard, 1998), (b) Han
et al.?s lexical normalisation dictionary (LexNorm)
(Han et al., 2012) and (c) the training data.
The BNC and LexNorm dictionaries are built by
recording all words occurring in the respective
corpus or word list as English. For the BNC, we
also collect word frequency information. For the
training data, we obtain dictionaries for each of the
six labels and each of the five cross-validation runs
(using the relevant 4/5 of training data).
To make a prediction, we consult all dictionar-
ies. If there are more than one candidate label,
we choose the label for which the frequency for
the query token is highest. To account for the fact
that the BNC is much larger than the training data,
we normalise all frequencies before comparison.
LexNorm has no frequency information, hence it
is added to our system as a simple word list (we
consider the language of a word to be English if it
appears in LexNorm). If a word appears in multi-
ple dictionaries with the same frequency or if the
word does not appear in any dictionary or list, the
predicted language is chosen based on the domi-
nant language(s)/label(s) of the corpus.
We experiment with the individual dictionar-
ies and the combination of all three dictionaries,
among which the combination achieves the high-
est cross-validation accuracy (90.71%). Table 2
shows the results of dictionary-based detection ob-
tained in five-fold cross-validation.
4.2 Classification with k-NN
For Nepali-English, we also experiment with a
simple k-nearest neighbour (k-NN) approach. For
each test item, we select a subset of the training
data using string edit distance and n-gram overlap
128
and choose the majority label of the subset as our
prediction. For efficiency, we first select k
1
items
that share an n-gram with the token to be classi-
fied.
1
The set of k
1
items is then re-ranked ac-
cording to string edit distance to the test item and
the best k
2
matches are used to make a prediction.
Apart from varying k
1
and k
2
, we experiment
with (a) lowercasing strings, (b) including context
by concatenating the previous, current and next
token, and (c) weighting context by first calcu-
lating edit distances for the previous, current and
next token separately and using a weighted aver-
age. The best configuration we found in cross-
validation uses lowercasing with k
1
= 800 and
k
2
= 16 but no context information. It achieves
an accuracy of 94.97%.
4.3 SVM Classification
We experiment with linear kernel SVM classifiers
using Liblinear (Fan et al., 2008). Parameter opti-
misation
2
is performed for each feature set combi-
nation to obtain best cross-validation accuracy.
4.3.1 Basic Features
Following Barman et al. (2014), our basic features
are:
Char-N-Grams (G): We start with a charac-
ter n-gram-based approach (Cavnar and Trenkle,
1994). Following King and Abney (2013), we se-
lect lowercased character n-grams (n=1 to 5) and
the word as the features in our experiments.
Dictionary-Based Labels (D): We use presence
in the dictionary of the 5,000 most frequent words
in the BNC and presence in the LexNorm dictio-
nary as binary features.
3
Length of words (L): We create multiple fea-
tures for token length using a decision tree (J48).
We use length as the only feature to train a deci-
sion tree for each fold and use the nodes obtained
from the tree to create boolean features (Rubino et
al., 2013; Wagner et al., 2014).
1
Starting with n = 5, we decrease n until there are at
least k
1
items and then we randomly remove items added in
the last augmentation step to arrive at exactly k
1
items. (For
n = 0, we randomly sample from the full training data.)
2
C = 2
i
with i = ?15,?14, ..., 10
3
We chose these parameters based on experiments with
each dictionary, combinations of dictionaries and various fre-
quency thresholds. We apply a frequency threshold to the
BNC to increase precision. We rank the words according to
frequency and used the rank as a threshold (e.g. top-5K, top-
10K etc.). With the top 5,000 ranked words and C = 0.25,
we obtained best accuracy (96.40%).
Features Accuracy Features Accuracy
G 96.02 GD 96.27
GL 96.11 GDL 96.32
GC 96.15 GDC 96.20
GLC 96.21 GDLC 96.40
Table 3: Average cross-validation accuracy of 6-
way SVMs on the Nepali-English data set; G =
char-n-gram, L = binary length features, D = dict.-
based labels and C = capitalisation features
Context Accuracy(%)
GDLC + P
1
96.41
GDLC + P
2
96.38
GDLC + N
1
96.41
GDLC + N
2
96.41
GDLC + P
1
+ N
1
96.42
GDLC + P
2
+ N
2
96.41
Table 4: Average cross-validation accuracy of 6-
way SVMs using contextual features for Nepali-
English
Capitalisation (C): We choose 3 boolean
features to encode capitalisation information:
whether any letter in the word is capitalised,
whether all letters in the word are capitalised and
whether the first letter is capitalised.
Context (P
i
and N
j
): We consider the previous
i and next j token to be combined with the current
token, forming an (i+1)-gram and a (j+1)-gram,
which we add as features. Six settings are tested.
Table 4 shows that using the bigrams formed with
the previous and next word are the best combina-
tion for the task (among those tested).
Among the eight combinations of the first four
feature sets that contain the first set (G), Table 3
shows that the 6-way SVM classifier
4
performs
best with all features sets (GDLC), achieving
96.40% accuracy. Adding contextual information
P
i
N
j
to GDLC, Table 4 shows best results for
i=j=1, achieving 96.42% accuracy, only slightly
ahead of the context-independent system.
4.3.2 Neural Network (Elman) and k-NN
Features
We experiment with two additional features sets
not covered by Barman et al. (2014):
Neural Network (Elman): We extract features
from the hidden layer of a recurrent neural net-
4
We also test 3-way SVM classification (lang1, lang2 and
other) and heuristic post-processing, but it does not outper-
form our 6-way classification runs.
129
Systems Accuracy
GDLC 96.40
k-NN 95.10
Elman 89.96
GDLC+k-NN 96.31
GDLC+Elman 96.46
GDLC+k-NN+Elman 96.40
GDLC+P
1
N
1
96.42
k-NN+P
1
N
1
95.11
Elman+P
1
N
1
91.53
GDLC+P
1
N
1
+k-NN 96.33
GDLC+P
1
N
1
+Elman 96.45
GDLC+P
1
N
1
+k-NN+Elman 96.40
Table 5: Average cross-validation accuracy of 6-
way SVMs of combinations of GDLC, k-NN, El-
man and P
1
N
1
features for Nepali-English
work that has been trained to predict the next char-
acter in a string (Chrupa?a, 2014). The 10 most ac-
tive units of the hidden layer for each of the initial
4 bytes and final 4 bytes of each token are bina-
rised by using a threshold of 0.5.
k-Nearest Neighbour (kNN): We obtain fea-
tures from our basic k-NN approach (Section 4.2),
encoding the prediction of the k-NN model with
six binary features (one for each label) and a nu-
meric feature for each label stating the relative
number of votes for the label, e.g. if k
2
= 16
and 12 votes are for lang1 the value of the fea-
ture votes4lang1 will be 0.75. Furthermore, we
add two features stating the minimum and maxi-
mum edit distance between the test token and the
k
2
selected training tokens.
Table 5 shows cross-validation results for these
new feature sets with and without the P
1
N
1
con-
text features. Excluding the GDLC features, we
can see that best accuracy is with k-NN and P
1
N
1
features (95.11%). For Elman features, the accu-
racy is lower (91.53% with context). In combina-
tion with the GDLC features, however, the Elman
features can achieve a small improvement over
the GDLC+P
1
N
1
combination (+0.04 percentage
points): 96.46% accuracy for the GDLC+Elman
setting (without P
1
N
1
features). Furthermore, the
k-NN features do not combine well.
5
4.3.3 Final System and Test Results
At the time of submission of predictions, we had
an error in our GDLC+Elman feature combiner re-
5
A possible explanation may be that the k-NN features
are based on only 3 of 5 folds for the training data (3 folds
are used to make predictions for the 4th set) but 4 of 5 folds
are used for test data predictions in each cross-validation run.
Tweets
Token-Level Tweet-Level
Nepali-English 96.3 95.8
Spanish-English 84.4 80.4
Surprise Genre
Token-Level Post-Level
Nepali-English 85.6 77.5
Spanish-English 94.4 80.0
Table 6: Test set results (overall accuracy) for
Nepali-English and Spanish-English tweet data
and surprise genre
sulting in slightly lower performance. Therefore,
we selected SVM-GDLC-P
1
N
1
as our final ap-
proach and trained the final two systems using the
full training data for Nepali-English and Spanish-
English respectively. While we knew that C =
0.125 is best for Nepali-English from our experi-
ments, we had to re-tune parameter C for Spanish-
English using cross-validation on the training data.
We found best accuracy of 94.16% for Spanish-
English with C = 128. Final predictions for the
test sets are made using these systems.
Table 6 shows the test set results. The test
set for this task is divided into tweets and a sur-
prise genre. For the tweets, we achieve 96.3%
and 84.4% accuracy (overall token-level accuracy)
in Nepali-English and in Spanish-English respec-
tively. For this surprise genre (a collection of posts
from Facebook and blogs), we achieve 85.6% for
Nepali-English and 94.4% for Spanish-English.
5 Conclusion
To summarise, we achieved reasonable accuracy
with a 6-way SVM classifier by employing basic
features only. We found that using dictionaries
is helpful, as are contextual features. The perfor-
mance of the k-NN classifier is also notable: it is
only 1.45 percentage points behind the final SVM-
based system (in terms of cross-validation accu-
racy). Adding neural network features can further
increase the accuracy of systems.
Briefly opening the test files to check for for-
matting issues, we notice that the surprise genre
data contains language-specific scripts that could
easily be addressed in an English vs. non-English
scenario.
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University.
130
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code-mixing: A challenge
for language identification in the language of so-
cial media. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Grzegorz Chrupa?a. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 680?686, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of Proceedings of COLING 2012: Posters
(the 24th International Conference on Computa-
tional Linguistics), pages 287?296, Mumbai, India.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in Ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Uur Doan, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, and
Haizhou Li. 2010. SEAME: A Mandarin-English
code-switching speech corpus in South-East Asia.
In INTERSPEECH 2010, 11th Annual Conference
of the International Speech Communication Asso-
ciation, volume 10, pages 1986?1989, Makuhari,
Chiba, Japan. ISCA Archive.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
131
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), pages 392?397, Dublin, Ireland,
August. Association for Computational Linguistics.
132
