Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 69?72,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Directional Distributional Similarity for Lexical Expansion
Lili Kotlerman, Ido Dagan, Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
lili.dav@gmail.com
{dagan,szpekti}@cs.biu.ac.il
Maayan Zhitomirsky-Geffet
Department of Information Science
Bar-Ilan University
Ramat Gan, Israel
maayan.geffet@gmail.com
Abstract
Distributional word similarity is most
commonly perceived as a symmetric re-
lation. Yet, one of its major applications
is lexical expansion, which is generally
asymmetric. This paper investigates the
nature of directional (asymmetric) similar-
ity measures, which aim to quantify distri-
butional feature inclusion. We identify de-
sired properties of such measures, specify
a particular one based on averaged preci-
sion, and demonstrate the empirical bene-
fit of directional measures for expansion.
1 Introduction
Much work on automatic identification of seman-
tically similar terms exploits Distributional Simi-
larity, assuming that such terms appear in similar
contexts. This has been now an active research
area for a couple of decades (Hindle, 1990; Lin,
1998; Weeds and Weir, 2003).
This paper is motivated by one of the prominent
applications of distributional similarity, namely
identifying lexical expansions. Lexical expansion
looks for terms whose meaning implies that of a
given target term, such as a query. It is widely
employed to overcome lexical variability in ap-
plications like Information Retrieval (IR), Infor-
mation Extraction (IE) and Question Answering
(QA). Often, distributional similarity measures are
used to identify expanding terms (e.g. (Xu and
Croft, 1996; Mandala et al, 1999)). Here we de-
note the relation between an expanding term u and
an expanded term v as ?u ? v?.
While distributional similarity is most promi-
nently modeled by symmetric measures, lexical
expansion is in general a directional relation. In
IR, for instance, a user looking for ?baby food?
will be satisfied with documents about ?baby pap?
or ?baby juice? (?pap ? food?, ?juice ? food?);
but when looking for ?frozen juice? she will not
be satisfied by ?frozen food?. More generally, di-
rectional relations are abundant in NLP settings,
making symmetric similarity measures less suit-
able for their identification.
Despite the need for directional similarity mea-
sures, their investigation counts, to the best of
our knowledge, only few works (Weeds and Weir,
2003; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor and Dagan, 2008; Michelbacher et
al., 2007) and is utterly lacking. From an expan-
sion perspective, the common expectation is that
the context features characterizing an expanding
word should be largely included in those of the ex-
panded word.
This paper investigates the nature of directional
similarity measures. We identify their desired
properties, design a novel measure based on these
properties, and demonstrate its empirical advan-
tage in expansion settings over state-of-the-art
measures
1
. In broader prospect, we suggest that
asymmetric measures might be more suitable than
symmetric ones for many other settings as well.
2 Background
The distributional word similarity scheme follows
two steps. First, a feature vector is constructed
for each word by collecting context words as fea-
tures. Each feature is assigned a weight indicating
its ?relevance? (or association) to the given word.
Then, word vectors are compared by some vector
similarity measure.
1
Our directional term-similarity resource will be available
at http://aclweb.org/aclwiki/index.php?
title=Textual_Entailment_Resource_Pool
69
To date, most distributional similarity research
concentrated on symmetric measures, such as the
widely cited and competitive (as shown in (Weeds
and Weir, 2003)) LIN measure (Lin, 1998):
LIN(u, v) =
?
f?FV
u
?FV
v
[w
u
(f) + w
v
(f)]
?
f?FV
u
w
u
(f) +
?
f?FV
v
w
v
(f)
where FV
x
is the feature vector of a word x and
w
x
(f) is the weight of the feature f in that word?s
vector, set to their pointwise mutual information.
Few works investigated a directional similarity
approach. Weeds and Weir (2003) and Weeds et
al. (2004) proposed a precision measure, denoted
here WeedsPrec, for identifying the hyponymy re-
lation and other generalization/specification cases.
It quantifies the weighted coverage (or inclusion)
of the candidate hyponym?s features (u) by the hy-
pernym?s (v) features:
WeedsPrec(u ? v) =
?
f?FV
u
?FV
v
w
u
(f)
?
f?FV
u
w
u
(f)
The assumption behind WeedsPrec is that if one
word is indeed a generalization of the other then
the features of the more specific word are likely to
be included in those of the more general one (but
not necessarily vice versa).
Extending this rationale to the textual entail-
ment setting, Geffet and Dagan (2005) expected
that if the meaning of a word u entails that of
v then all its prominent context features (under
a certain notion of ?prominence?) would be in-
cluded in the feature vector of v as well. Their
experiments indeed revealed a strong empirical
correlation between such complete inclusion of
prominent features and lexical entailment, based
on web data. Yet, such complete inclusion cannot
be feasibly assessed using an off-line corpus, due
to the huge amount of required data.
Recently, (Szpektor and Dagan, 2008) tried
identifying the entailment relation between
lexical-syntactic templates using WeedsPrec, but
observed that it tends to promote unreliable rela-
tions involving infrequent templates. To remedy
this, they proposed to balance the directional
WeedsPrec measure by multiplying it with the
symmetric LIN measure, denoted here balPrec:
balPrec(u?v)=
?
LIN(u, v)?WeedsPrec(u?v)
Effectively, this measure penalizes infrequent tem-
plates having short feature vectors, as those usu-
ally yield low symmetric similarity with the longer
vectors of more common templates.
3 A Statistical Inclusion Measure
Our research goal was to develop a directional
similarity measure suitable for learning asymmet-
ric relations, focusing empirically on lexical ex-
pansion. Thus, we aimed to quantify most effec-
tively the above notion of feature inclusion.
For a candidate pair ?u ? v?, we will refer to
the set of u?s features, which are those tested for
inclusion, as tested features. Amongst these fea-
tures, those found in v?s feature vector are termed
included features.
In preliminary data analysis of pairs of feature
vectors, which correspond to a known set of valid
and invalid expansions, we identified the follow-
ing desired properties for a distributional inclusion
measure. Such measure should reflect:
1. the proportion of included features amongst
the tested ones (the core inclusion idea).
2. the relevance of included features to the ex-
panding word.
3. the relevance of included features to the ex-
panded word.
4. that inclusion detection is less reliable if the
number of features of either expanding or ex-
panded word is small.
3.1 Average Precision as the Basis for an
Inclusion Measure
As our starting point we adapted the Average
Precision (AP) metric, commonly used to score
ranked lists such as query search results. This
measure combines precision, relevance ranking
and overall recall (Voorhees and Harman, 1999):
AP =
?
N
r=1
[P (r) ? rel(r)]
total number of relevant documents
where r is the rank of a retrieved document
amongst the N retrieved, rel(r) is an indicator
function for the relevance of that document, and
P (r) is precision at the given cut-off rank r.
In our case the feature vector of the expanded
word is analogous to the set of all relevant docu-
ments while tested features correspond to retrieved
documents. Included features thus correspond to
relevant retrieved documents, yielding the follow-
70
ing analogous measure in our terminology:
AP (u ? v) =
?
|FV
u
|
r=1
[P (r) ? rel(f
r
)]
|FV
v
|
rel(f) =
{
1, if f ? FV
v
0, if f /? FV
v
P (r) =
|included features in ranks 1 to r|
r
where f
r
is the feature at rank r in FV
u
.
This analogy yields a feature inclusion measure
that partly addresses the above desired properties.
Its score increases with a larger number of in-
cluded features (correlating with the 1
st
property),
while giving higher weight to highly ranked fea-
tures of the expanding word (2
nd
property).
To better meet the desired properties we in-
troduce two modifications to the above measure.
First, we use the number of tested features |FV
u
|
for normalization instead of |FV
v
|. This captures
better the notion of feature inclusion (1
st
property),
which targets the proportion of included features
relative to the tested ones.
Second, in the classical AP formula all relevant
documents are considered relevant to the same ex-
tent. However, features of the expanded word dif-
fer in their relevance within its vector (3
rd
prop-
erty). We thus reformulate rel(f) to give higher
relevance to highly ranked features in |FV
v
|:
rel
?
(f) =
{
1 ?
rank(f,FV
v
)
|FV
v
|+1
, if f ? FV
v
0 , if f /? FV
v
where rank(f, FV
v
) is the rank of f in FV
v
.
Incorporating these twomodifications yields the
APinc measure:
APinc(u?v)=
?
|FV
u
|
r=1
[P (r) ? rel
?
(f
r
)]
|FV
u
|
Finally, we adopt the balancing approach in
(Szpektor and Dagan, 2008), which, as explained
in Section 2, penalizes similarity for infrequent
words having fewer features (4
th
property) (in our
version, we truncated LIN similarity lists after top
1000 words). This yields our proposed directional
measure balAPinc:
balAPinc(u?v) =
?
LIN(u, v) ? APinc(u?v)
4 Evaluation and Results
4.1 Evaluation Setting
We tested our similarity measure by evaluating its
utility for lexical expansion, compared with base-
lines of the LIN, WeedsPrec and balPrec measures
(Section 2) and a balanced version of AP (Sec-
tion 3), denoted balAP. Feature vectors were cre-
ated by parsing the Reuters RCV1 corpus and tak-
ing the words related to each term through a de-
pendency relation as its features (coupled with the
relation name and direction, as in (Lin, 1998)). We
considered for expansion only terms that occur at
least 10 times in the corpus, and as features only
terms that occur at least twice.
As a typical lexical expansion task we used
the ACE 2005 events dataset
2
. This standard IE
dataset specifies 33 event types, such as Attack,
Divorce, and Law Suit, with all event mentions
annotated in the corpus. For our lexical expan-
sion evaluation we considered the first IE subtask
of finding sentences that mention the event.
For each event we specified a set of representa-
tive words (seeds), by selecting typical terms for
the event (4 on average) from its ACE definition.
Next, for each similarity measure, the terms found
similar to any of the event?s seeds (?u ? seed?)
were taken as expansion terms. Finally, to mea-
sure the sole contribution of expansion, we re-
moved from the corpus all sentences that contain
a seed word and then extracted all sentences that
contain expansion terms as mentioning the event.
Each of these sentences was scored by the sum of
similarity scores of its expansion terms.
To evaluate expansion quality we compared the
ranked list of sentences for each event to the gold-
standard annotation of event mentions, using the
standard Average Precision (AP) evaluation mea-
sure. We report Mean Average Precision (MAP)
for all events whose AP value is at least 0.1 for at
least one of the tested measures
3
.
4.1.1 Results
Table 1 presents the results for the different tested
measures over the ACE experiment. It shows that
the symmetric LIN measure performs significantly
worse than the directional measures, assessing that
a directional approach is more suitable for the ex-
pansion task. In addition, balanced measures con-
sistently perform better than unbalanced ones.
According to the results, balAPinc is the best-
performing measure. Its improvement over all
other measures is statistically significant accord-
ing to the two-sided Wilcoxon signed-rank test
2
http://projects.ldc.upenn.edu/ace/, training part.
3
The remaining events seemed useless for our compar-
ative evaluation, since suitable expansion lists could not be
found for them by any of the distributional methods.
71
LIN WeedsPrec balPrec AP balAP balAPinc
0.068 0.044 0.237 0.089 0.202 0.312
Table 1: MAP scores of the tested measures on the
ACE experiment.
seed LIN balAPinc
death murder, killing, inci-
dent, arrest, violence
suicide, killing, fatal-
ity, murder, mortality
marry divorce, murder, love, divorce, remarry,
dress, abduct father, kiss, care for
arrest detain, sentence,
charge, jail, convict
detain, extradite,
round up, apprehend,
imprison
birth abortion, pregnancy, wedding day,
resumption, seizure, dilation, birthdate,
passage circumcision, triplet
injure wound, kill, shoot, wound, maim, beat
detain, burn up, stab, gun down
Table 2: Top 5 expansion terms learned by LIN
and balAPinc for a sample of ACE seed words.
(Wilcoxon, 1945) at the 0.01 level. Table 2
presents a sample of the top expansion terms
learned for some ACE seeds with either LIN or
balAPinc, demonstrating the more accurate ex-
pansions generated by balAPinc. These results
support the design of our measure, based on the
desired properties that emerged from preliminary
data analysis for lexical expansion.
Finally, we note that in related experiments we
observed statistically significant advantages of the
balAPincmeasure for an unsupervised text catego-
rization task (on the 10 most frequent categories in
the Reuters-21578 collection). In this setting, cat-
egory names were taken as seeds and expanded by
distributional similarity, further measuring cosine
similarity with categorized documents similarly to
IR query expansion. These experiments fall be-
yond the scope of this paper and will be included
in a later and broader description of our work.
5 Conclusions and Future work
This paper advocates the use of directional similar-
ity measures for lexical expansion, and potentially
for other tasks, based on distributional inclusion of
feature vectors. We first identified desired proper-
ties for an inclusion measure and accordingly de-
signed a novel directional measure based on av-
eraged precision. This measure yielded the best
performance in our evaluations. More generally,
the evaluations supported the advantage of multi-
ple directional measures over the typical symmet-
ric LIN measure.
Error analysis showed that many false sentence
extractions were caused by ambiguous expanding
and expanded words. In future work we plan to
apply disambiguation techniques to address this
problem. We also plan to evaluate the performance
of directional measures in additional tasks, and
compare it with additional symmetric measures.
Acknowledgements
This work was partially supported by the NEGEV
project (www.negev-initiative.org), the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886 and by the Israel
Science Foundation grant 1112/08.
References
R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR: An
unsupervised algorithm for learning directionality of
inference rules. In Proceedings of EMNLP-CoNLL.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL.
R. Mandala, T. Tokunaga, and H. Tanaka. 1999. Com-
bining multiple evidence from different types of the-
saurus for query expansion. In Proceedings of SI-
GIR.
L. Michelbacher, S. Evert, and H. Schutze. 2007.
Asymmetric association measures. In Proceedings
of RANLP.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COL-
ING.
E. M. Voorhees and D. K. Harman, editors. 1999. The
Seventh Text REtrieval Conference (TREC-7), vol-
ume 7. NIST.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP.
J. Weeds, D. Weir, and D. McCarthy. 2004. Character-
ising measures of lexical distributional similarity. In
Proceedings of COLING.
F. Wilcoxon. 1945. Individual comparisons by ranking
methods. Biometrics Bulletin, 1:80?83.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings
of SIGIR.
72
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145?150,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ParaQuery: Making Sense of Paraphrase Collections
Lili Kotlerman
Bar-Ilan University
Israel
lili.dav@gmail.com
Nitin Madnani and Aoife Cahill
Educational Testing Service
Princeton, NJ, USA
{nmadnani,acahill}@ets.org
Abstract
Pivoting on bilingual parallel corpora is a
popular approach for paraphrase acquisi-
tion. Although such pivoted paraphrase
collections have been successfully used to
improve the performance of several dif-
ferent NLP applications, it is still difficult
to get an intrinsic estimate of the qual-
ity and coverage of the paraphrases con-
tained in these collections. We present
ParaQuery, a tool that helps a user inter-
actively explore and characterize a given
pivoted paraphrase collection, analyze its
utility for a particular domain, and com-
pare it to other popular lexical similarity
resources ? all within a single interface.
1 Introduction
Paraphrases are widely used in many Natural Lan-
guage Processing (NLP) tasks, such as informa-
tion retrieval, question answering, recognizing
textual entailment, text simplification etc. For ex-
ample, a question answering system facing a ques-
tion ?Who invented bifocals and lightning rods??
could retrieve the correct answer from the text
?Benjamin Franklin invented strike termination
devices and bifocal reading glasses? given the in-
formation that ?bifocal reading glasses? is a para-
phrase of ?bifocals? and ?strike termination de-
vices? is a paraphrase of ?lightning rods?.
There are numerous approaches for automati-
cally extracting paraphrases from text (Madnani
and Dorr, 2010). We focus on generating para-
phrases by pivoting on bilingual parallel corpora
as originally suggested by Bannard and Callison-
Burch (2005). This technique operates by attempt-
ing to infer semantic equivalence between phrases
in the same language by using a second language
as a bridge. It builds on one of the initial steps used
to train a phrase-based statistical machine transla-
tion system. Such systems rely on phrase tables ?
a tabulation of correspondences between phrases
in the source language and phrases in the target
language. These tables are usually extracted by in-
ducing word alignments between sentence pairs in
a parallel training corpus and then incrementally
building longer phrasal correspondences from in-
dividual words and shorter phrases. Once such a
tabulation of bilingual correspondences is avail-
able, correspondences between phrases in one lan-
guage may be inferred simply by using the phrases
in the other language as pivots, e.g., if both ?man?
and ?person? correspond to ?personne? in French,
then they can be considered paraphrases. Each
paraphrase pair (rule) in a pivoted paraphrase col-
lection is defined by a source phrase e1, the target
phrase e2 that has been inferred as its paraphrase,
and a probability score p(e2|e1) obtained from the
probability values in the bilingual phrase table.1
Pivoted paraphrase collections have been suc-
cessfully used in different NLP tasks including
automated document summarization (Zhou et al,
2006), question answering (Riezler et al, 2007),
and machine translation (Madnani, 2010). Yet, it
is still difficult to get an estimate of the intrinsic
quality and coverage of the paraphrases contained
in these collections. To remedy this, we propose
ParaQuery ? a tool that can help explore and ana-
lyze pivoted paraphrase collections.
2 ParaQuery
In this section we first briefly describe how to set
up ParaQuery (?2.1) and then demonstrate its use
in detail for interactively exploring and character-
izing a paraphrase collection, analyzing its util-
ity for a particular domain, and comparing it with
other word-similarity resources (?2.2). Detailed
documentation will be included in the tool.
1There may be other values associated with each pair, but
we ignore them for the purposes of this paper.
145
2.1 Setting up
ParaQuery operates on pivoted paraphrase collec-
tions and can accept collections generated using
any set of tools that are preferred by the user, as
long as the collection is stored in a pre-defined
plain-text format containing the source and target
phrases, the probability values, as well as informa-
tion on pivots (optional but useful for pivot-driven
analysis, as shown later). This format is com-
monly used in the machine translation and para-
phrase generation community. In this paper, we
adapt the Thrax and Joshua (Ganitkevitch et al,
2012) toolkits to generate a pivoted paraphrase
collection using the English-French EuroParl par-
allel corpus, which we use as our example col-
lection for demonstrating ParaQuery. Once a piv-
oted collection is generated, ParaQuery needs to
convert it into an SQLite database against which
queries can be run. This is done by issuing the
index command at the ParaQuery command-line
interface (described in ?2.2.1).
2.2 Exploration and Analysis
In order to provide meaningful exploration and
analysis, we studied various scenarios in which
paraphrase collections are used, and found that the
following issues typically interest the developers
and users of such collections:
1. Semantic relations between the paraphrases
in the collection (e.g. synonymy, hyponymy)
and their frequency.
2. The frequency of inaccurate paraphrases,
possible ways of de-noising the collection,
and the meaningfulness of scores (better
paraphrases should be scored higher).
3. The utility of the collection for a specific do-
main, i.e. whether domain terms of interest
are present in the collection.
4. Comparison of different collections based on
the above dimensions.
We note that paraphrase collections are used in
many tasks with different acceptability thresholds
for semantic relations, noisy paraphrases etc. We
do not intend to provide an exhaustive judgment
of paraphrase quality, but instead allow users to
characterize a collection, enabling an analysis of
the aforesaid issues and providing information for
them to decide whether a given collection is suit-
able for their specific task and/or domain.
2.2.1 Command line interface
ParaQuery allows interactive exploration and
analysis via a simple command line interface, by
processing user issued queries such as:
show <query>: display the rules which satisfy
the conditions of the given query.
show count <query>: display the number of
such rules.
explain <query>: display information about the
pivots which yielded each of these rules.
analyze <query>: display statistics about these
rules and save a report to an output file.
The following information is stored in the
SQLite database for each paraphrase rule:2
? The source and the target phrases, and the
probability score of the rule.
? Are the source and the target identical?
? Do the source and the target have the same
part of speech?3
? Length of the source and the target, and the
difference in their lengths.
? Number of pivots and the list of pivots.
? Are both the source and the target found in
WordNet (WN)? If yes, the WN relation be-
tween them (synonym, derivation, hypernym,
hyponym, co-hyponym, antonym, meronym,
holonym, pertainym) or the minimal dis-
tance, if they are not connected directly.
Therefore, all of the above can be used, alone or
in combination, to constrain the queries and de-
fine the rule(s) of interest. Figure 1 presents sim-
ple queries processed by the show command: the
first query displays top-scoring rules with ?man?
as their source phrase, while the second adds re-
striction on the rules? score. By default, the tool
displays the 10 best-scoring rules per query, but
this limit can be changed as shown. For each
rule, the corresponding score and semantic rela-
tion/distance is displayed.
2Although some of this information is available in the
paraphrase collection that was indexed, the remaining is auto-
matically computed and injected into the database during the
indexing process. Indexing the French-pivoted paraphrase
collection (containing 3,633,015 paraphrase rules) used in
this paper took about 6 hours.
3We use the simple parts of speech provided by WordNet
(nouns, verbs, adjectives and adverbs).
146
The queries provide a flexible way to define and
work with the rule set of interest, starting from fil-
tering low-scoring rules till extracting specific se-
mantic relations or constraining on the number of
pivots. Figure 2 presents additional examples of
queries. The tool also enables filtering out target
terms with a recurrent lemma, as illustrated in the
same figure. Note that ParaQuery also contains a
batch mode (in addition to the interactive mode il-
lustrated so far) to automatically extract the output
for a set of queries contained in a batch script.
Figure 1: Examples of the show command and the
probability constraint.
2.2.2 Analyzing pivot information
It is well known that pivoted paraphrase collec-
tions contain a lot of noisy rules. To understand
the origins of such rules, an explain query can be
used, which displays the pivots that yielded each
paraphrase rule, and the probability share of each
pivot in the final probability score. Figure 3 shows
an example of this command.
We see that noisy rules can originate from stop-
word pivots, e.g. ?l?. It is common to filter rules
containing stop-words, yet perhaps it is also im-
portant to exclude stop-word pivots, which was
never considered in the past. We can use Para-
Query to further explore whether discarding stop-
word pivots is a good idea. Figure 4 presents
a more complex query showing paraphrase rules
that were extracted via a single pivot ?l?. We see
that the top 5 such rules are indeed noisy, indicat-
ing that perhaps all of the 5,360 rules satisfying
the query can be filtered out.
2.2.3 Analysis of rule sets
In order to provide an overall analysis of a rule set
or a complete collection, ParaQuery includes the
Figure 2: Restricting the output of the show com-
mand using WordNet relations and distance, and
the unique lemma constraint.
Figure 3: An example of the explain command.
analyze command. Figure 5 shows the typical in-
formation provided by this command. In addition,
a report is generated to a file, including the anal-
ysis information for the whole rule set and for its
three parts: top, middle and bottom, as defined by
the scores of the rules in the set. The output to the
file is more detailed and expands on the informa-
tion presented in Figure 5. For example, it also
includes, for each part, rule samples and score dis-
tributions for each semantic relation and different
WordNet distances.
The information contained in the report can be
147
Figure 4: Exploring French stop-word pivots using the pivots condition of the show command.
Figure 5: An example of the analyze command (full output not shown for space reasons).
148
TOP BOTTOM
finest? better approach? el
outdoors? external effect? parliament
unsettled? unstable comment? speak up
intelligentsia? intelligence propose? allotted
caretaker? provisional prevent? aimed
luckily? happily energy? subject matter
Table 1: A random sample of undefined relation
rules from our collection?s top and bottom parts.
easily used for generating graphs and tables. For
example, Figure 6 shows the distribution of se-
mantic relations in the three parts of our exam-
ple paraphrase collection. The figure character-
izes the collection in terms of semantic relations
it contains and illustrates the fact that the scores
agree with their desired behavior: (1) the collec-
tion?s top-scoring part contains significantly more
synonyms than its middle and bottom parts, (2)
similar trends hold for derivations and hypernyms,
which are more suitable for paraphrasing than co-
hyponyms and other relations not defined in Word-
Net (we refer to these relations as undefined rela-
tions), (3) such undefined relations have the high-
est frequency in the collection?s bottom part, and
are least frequent in its top part. Among other
conclusions, the figure shows, that discarding the
lower-scoring middle and bottom parts of the col-
lection would allow retaining almost all the syn-
onyms and derivations, while filtering out most of
the co-hyponyms and a considerable number of
undefined relations.
Yet from Figure 6 we see that undefined rela-
tions constitute the majority of the rules in the col-
lection. To better understand this, random rule
samples provided in the analysis output can be
used, as shown in Table 1. From this table, we see
that the top-part rules are indeed mostly valid for
paraphrasing, unlike the noisy bottom-part rules.
The score distributions reported as part of the anal-
ysis can be used to further explore the collec-
tion and set sound thresholds suitable for different
tasks and needs.
2.2.4 Analysis of domain utility
One of the frequent questions of interest is
whether a given collection is suitable for a specific
domain. To answer this question, ParaQuery al-
lows the user to run the analysis from ?2.2.3 over
rules whose source phrases belong to a specific
domain, by means of the analyze <query> us-
ing <file> command. The file can hold either a
list of domain terms or a representative domain
text, from which frequent terms and term collo-
cations will be automatically extracted, presented
to the user, and utilized for analysis. The analysis
includes the coverage of the domain terms in the
paraphrase collection, and can also be restricted to
top-K rules per source term, a common practice in
many NLP applications. We do not show an exam-
ple of this command due to space considerations.
2.2.5 Comparison with other collections
The output of the analyze command can also be
used to compare different collections, either in
general or for a given domain. Although Para-
Query is designed for pivoted paraphrase collec-
tions, it allows comparing them to non-pivoted
paraphrase collections as well. Next we present an
example of such a comparative study, performed
using ParaQuery via several analyze commands.
Table 2 compares three different collections:
the French pivoted paraphrase collection, a dis-
tributional similarity resource (Kotlerman et al,
2010) and a Wikipedia-based resource (Shnarch et
al., 2009). The table shows the collection sizes,
as well as the number of different (unique) source
phrases in them and, correspondingly, the average
number of target phrases per source. From the
table we can see that the distributional similarity
resource contains a lot of general language terms
found in WordNet, while the Wikipedia resource
includes only a small amount of such terms. A
sample of rules from the Wikipedia collection ex-
plains this behavior, e.g. ?Yamaha SR500 ? mo-
torcycle?. The table provides helpful information
to decide which collection is (more) suitable for
specific tasks, such as paraphrase recognition and
generation, query expansion, automatic generation
of training data for different supervised tasks, etc.
3 Conclusions and Future Work
We presented ParaQuery?a tool for interactive
exploration and analysis of pivoted paraphrase
collections?and showed that it can be used to
estimate the intrinsic quality and coverage of the
paraphrases contained in these collections, a task
that is still somewhat difficult. ParaQuery can also
be used to answer the questions that users of such
collections are most interested in. We plan to re-
lease ParaQuery under an open-source license, in-
cluding our code for generating paraphrase col-
lections that can then be indexed and analyzed by
149
0%
20%
40%
60%
80% Top Middle Bottom
22% 11% 8% 4% 8% 0%
45%
6% 1% 4% 4%
17%
0%
68%
1% 0% 1% 1% 11% 0%
86%
Synonym Derivation Hypernym Hyponym Co-hyponym Antonym Undefined
Figure 6: Distribution of semantic relations in the top, middle and bottom parts of the example collection.
The parts are defined by binning the scores of the rules in the collection.
Collection Size (rules) In WordNet Unique Src Avg. Tgts per Src davg for UR
Pivoted (FR) 3,633,015 757,994 (21%) 188,898 16.064 2.567
Dist.Sim. 7,298,321 3,252,967 (45%) 113,444 64.334 6.043
Wikipedia 7,880,962 295,161 (4%) 2,727,362 2.890 8.556
Table 2: Comparing the French-pivoted paraphrase collection to distributional-similarity based and
Wikipedia-based similarity collections, in terms of total size, percentage of rules in WordNet, number
of unique source phrases, average number of target phrases per source phrase, and the average WordNet
distance between the two sides of the undefined relation (UR) rules.
ParaQuery. We also plan to include pre-generated
paraphrase collections in the release so that users
of ParaQuery can use it immediately.
In the future, we plan to use this tool for analyz-
ing the nature of pivoted paraphrases. The quality
and coverage of these paraphrases is known to de-
pend on several factors, including (a) the genre of
the bilingual corpus, (b) the word-alignment algo-
rithm used during bilingual training, and (c) the
pivot language itself. However, there have been
no explicit studies designed to measure such vari-
ations. We believe that ParaQuery is perfectly
suited to conducting such studies and moving the
field of automated paraphrase generation forward.
Acknowledgments
This work was partially supported by the European Commu-
nity?s Seventh Framework Programme (FP7/2007-2013) un-
der grant agreement no. 287923 (EXCITEMENT).
References
Colin Bannard and Chris Callison-Burch. 2005. Paraphras-
ing with Bilingual Parallel Corpora. In Proceedings of
ACL, pages 597?604.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and
Chris Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and Paraphrases. In Proceedings of WMT, pages 283?291.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distributional
Similarity for Lexical Inference. Natural Language En-
gineering, 16(4):359?389.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
driven Methods. Computational Linguistics, 36(3):341?
387.
Nitin Madnani. 2010. The Circle of Meaning: From Trans-
lation to Paraphrasing and Back. Ph.D. thesis, Depart-
ment of Computer Science, University of Maryland Col-
lege Park.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu O. Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer Re-
trieval. In Proceedings of ACL, pages 464?471.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting
lexical reference rules from Wikipedia. In Proceedings of
ACL-IJCNLP, pages 450?458.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Muntenau, and
Eduard Hovy. 2006. ParaEval: Using Paraphrases to
Evaluate Summaries Automatically. In Proceedings of
HLT-NAACL, pages 447?454.
150
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38?43,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Sentence Clustering via Projection over Term Clusters
Lili Kotlerman, Ido Dagan
Bar-Ilan University
Israel
Lili.Kotlerman@biu.ac.il
dagan@cs.biu.ac.il
Maya Gorodetsky, Ezra Daya
NICE Systems Ltd.
Israel
Maya.Gorodetsky@nice.com
Ezra.Daya@nice.com
Abstract
This paper presents a novel sentence cluster-
ing scheme based on projecting sentences over
term clusters. The scheme incorporates exter-
nal knowledge to overcome lexical variability
and small corpus size, and outperforms com-
mon sentence clustering methods on two real-
life industrial datasets.
1 Introduction
Clustering is a popular technique for unsupervised
text analysis, often used in industrial settings to ex-
plore the content of large amounts of sentences. Yet,
as may be seen from the results of our research,
widespread clustering techniques, which cluster sen-
tences directly, result in rather moderate perfor-
mance when applied to short sentences, which are
common in informal media.
In this paper we present and evaluate a novel
sentence clustering scheme based on projecting
sentences over term clusters. Section 2 briefly
overviews common sentence clustering approaches.
Our suggested clustering scheme is presented in
Section 3. Section 4 describes an implementation of
the scheme for a particular industrial task, followed
by evaluation results in Section 5. Section 6 lists
directions for future research.
2 Background
Sentence clustering aims at grouping sentences with
similar meanings into clusters. Commonly, vector
similarity measures, such as cosine, are used to de-
fine the level of similarity over bag-of-words encod-
ing of the sentences. Then, standard clustering algo-
rithms can be applied to group sentences into clus-
ters (see Steinbach et al (2000) for an overview).
The most common practice is representing the
sentences as vectors in term space and applying the
K-means clustering algorithm (Shen et al (2011);
Pasquier (2010); Wang et al (2009); Nomoto and
Matsumoto (2001); Boros et al (2001)). An alterna-
tive approach involves partitioning a sentence con-
nectivity graph by means of a graph clustering algo-
rithm (Erkan and Radev (2004); Zha (2002)).
The main challenge for any sentence clustering
approach is language variability, where the same
meaning can be phrased in various ways. The
shorter the sentences are, the less effective becomes
exact matching of their terms. Compare the fol-
lowing newspaper sentence ?The bank is phasing out
the EZ Checking package, with no monthly fee charged
for balances over $1,500, and is instead offering cus-
tomers its Basic Banking account, which carries a fee?
with two tweets regarding the same event: ?Whats
wrong.. charging $$ for checking a/c? and ?Now they
want a monthly fee!?. Though each of the tweets can
be found similar to the long sentence by exact term
matching, they do not share any single term. Yet,
knowing that the words fee and charge are semanti-
cally related would allow discovering the similarity
between the two tweets.
External resources can be utilized to provide such
kind of knowledge, by which sentence representa-
tion can be enriched. Traditionally, WordNet (Fell-
baum, 1998) has been used for this purpose (She-
hata (2009); Chen et al (2003); Hotho et al (2003);
Hatzivassiloglou et al (2001)). Yet, other resources
38
of semantically-related terms can be beneficial, such
as WordNet::Similarity (Pedersen et al, 2004), sta-
tistical resources like that of Lin (1998) or DIRECT
(Kotlerman et al, 2010), thesauri, Wikipedia (Hu et
al., 2009), ontologies (Suchanek et al, 2007) etc.
3 Sentence Clustering via Term Clusters
This section presents a generic sentence clustering
scheme, which involves two consecutive steps: (1)
generating relevant term clusters based on lexical se-
mantic relatedness and (2) projecting the sentence
set over these term clusters. Below we describe each
of the two steps.
3.1 Step 1: Obtaining Term Clusters
In order to obtain term clusters, a term connectivity
graph is constructed for the given sentence set and is
clustered as follows:
1. Create initially an undirected graph with
sentence-set terms as nodes and use lexical re-
sources to extract semantically-related terms
for each node.
2. Augment the graph nodes with the extracted
terms and connect semantically-related nodes
with edges. Then, partition the graph into term
clusters through a graph clustering algorithm.
Extracting and filtering related terms. In Sec-
tion 2 we listed a number of lexical resources pro-
viding pairs of semantically-related terms. Within
the suggested scheme, any combination of resources
may be utilized.
Often resources contain terms, which are
semantically-related only in certain contexts. E.g.,
the words visa and passport are semantically-related
when talking about tourism, but cannot be consid-
ered related in the banking domain, where visa usu-
ally occurs in its credit card sense. In order to dis-
card irrelevant terms, filtering procedures can be em-
ployed. E.g., a simple filtering applicable in most
cases of sentence clustering in a specific domain
would discard candidate related terms, which do not
occur sufficiently frequently in a target-domain cor-
pus. In the example above, this procedure would
allow avoiding the insertion of passport as related to
visa, when considering the banking domain.
Clustering the graph nodes. Once the term
graph is constructed, a graph clustering algorithm
is applied resulting in a partition of the graph nodes
(terms) into clusters. The choice of a particular al-
gorithm is a parameter of the scheme. Many clus-
tering algorithms consider the graph?s edge weights.
To address this trait, different edge weights can be
assigned, reflecting the level of confidence that the
two terms are indeed validly related and the reliabil-
ity of the resource, which suggested the correspond-
ing edge (e.g. WordNet synonyms are commonly
considered more reliable than statistical thesauri).
3.2 Step 2: Projecting Sentences to Term
Clusters
To obtain sentence clusters, the given sentence set
has to be projected in some manner over the term
clusters obtained in Step 1. Our projection pro-
cedure resembles unsupervised text categorization
(Gliozzo et al, 2005), with categories represented
by term clusters that are not predefined but rather
emerge from the analyzed data:
1. Represent term clusters and sentences as vec-
tors in term space and calculate the similarity
of each sentence with each of the term clusters.
2. Assign each sentence to the best-scoring term
cluster. (We focus on hard clustering, but the
procedure can be adapted for soft clustering).
Various metrics for feature weighting and vector
comparison may be chosen. The top terms of term-
cluster vectors can be regarded as labels for the cor-
responding sentence clusters.
Thus each sentence cluster corresponds to a sin-
gle coherent cluster of related terms. This is con-
trasted with common clustering methods, where if
sentence A shares a term with B, and B shares an-
other term with C, then A and C might appear in the
same cluster even if they have no related terms in
common. This behavior turns out harmful for short
sentences, where each incidental term is influential.
Our scheme ensures that each cluster contains only
sentences related to the underlying term cluster, re-
sulting in more coherent clusters.
4 Application: Clustering Customer
Interactions
In industry there?s a prominent need to obtain busi-
ness insights from customer interactions in a contact
center or social media. Though the number of key
39
sentences to analyze is often relatively small, such
as a couple hundred, manually analyzing just a hand-
ful of clusters is much preferable. This section de-
scribes our implementation of the scheme described
in Section 3 for the task of clustering customer in-
teractions, as well as the data used for evaluation.
Results and analysis are presented in Section 5.
4.1 Data
We apply our clustering approach over two real-life
datasets. The first one consists of 155 sentences
containing reasons of account cancelation, retrieved
from automatic transcripts of contact center interac-
tions of an Internet Service Provider (ISP). The sec-
ond one contains 194 sentences crawled from Twit-
ter, expressing reasons for customer dissatisfaction
with a certain banking company. The sentences in
both datasets were gathered automatically by a rule-
based extraction algorithm. Each dataset is accom-
panied by a small corpus of call transcripts or tweets
from the corresponding domain.1
The goal of clustering these sentences is to iden-
tify the prominent reasons of cancelation and dissat-
isfaction. To obtain the gold-standard (GS) anno-
tation, sentences were manually grouped to clusters
according to the reasons stated in them.
Table 1 presents examples of sentences from the
ISP dataset. The sentences are short, with only one
or two words expressing the actual reason stated in
them. We see that exact term matching is not suffi-
cient to group the related sentences. Moreover, tra-
ditional clustering algorithms are likely to mix re-
lated and unrelated sentences, due to matching non-
essential terms (e.g. husband or summer). We note
that such short and noisy sentences are common
in informal media, which became a most important
channel of information in industry.
4.2 Implementation of the Clustering Scheme
Our proposed sentence clustering scheme presented
in Section 3 includes a number of choices. Below
we describe the choices we made in our current im-
plementation.
Input sentences were tokenized, lemmatized and
cleaned from stopwords in order to extract content-
word terms. Candidate semantically-related terms
1The bank dataset with the output of the tested methods will
be made publicly available.
he hasn?t been using it all summer long
it?s been sitting idle for about it almost a year
I?m getting married my husband has a computer
yeah I bought a new laptop this summer so
when I said faces my husband got laid off from work
well I?m them going through financial difficulties
Table 1: Example sentences expressing 3 reasons for can-
celation: the customer (1) does not use the service, (2)
acquired a computer, (3) cannot afford the service.
were extracted for each of the terms, using Word-
Net synonyms and derivations, as well as DIRECT2,
a directional statistical resource learnt from a news
corpus. Candidate terms that did not appear in the
accompanying domain corpus were filtered out as
described in Section 3.1.
Edges in the term graph were weighted with the
number of resources supporting the corresponding
edge. To cluster the graph we used the Chinese
Whispers clustering tool3 (Biemann, 2006), whose
algorithm does not require to pre-set the desired
number of clusters and is reported to outperform
other algorithms for several NLP tasks.
To generate the projection, sentences were rep-
resented as vectors of terms weighted by their fre-
quency in each sentence. Terms of the term-cluster
vectors were weighted by the number of sentences
in which they occur. Similarity scores were calcu-
lated using the cosine measure. Clusters were la-
beled with the top terms appearing both in the un-
derlying term cluster and in the cluster?s sentences.
5 Results and Analysis
In this section we present the results of evaluating
our projection approach, compared to the common
K-means clustering method4 applied to:
(A) Standard bag-of-words representation of sen-
tences;
2Available for download at www.cs.biu.ac.il/
?nlp/downloads/DIRECT.html. For each term we
extract from the resource the top-5 related terms.
3Available at http://wortschatz.informatik.
uni-leipzig.de/?cbiemann/software/CW.html
4We use the Weka (Hall et al, 2009) implementation. Due
to space limitations and for more meaningful comparison we re-
port here one value of K, which is equal to the number of clus-
ters returned by projection (60 for the ISP and 65 for the bank
dataset). For K = 20, 40 and 70 the performance was similar.
40
(B) Bag-of-words representation, where sentence?s
words are augmented with semantically-related
terms (following the common scheme of prior
work, see Section 2). We use the same set of
related terms as is used by our method.
(C) Representation of sentences in term-cluster
space, using the term clusters generated by our
method as vector features. A feature is acti-
vated in a sentence vector if it contains a term
from the corresponding term cluster.
Table 2 shows the results in terms of Purity, Recall
(R), Precision (P) and F1 (see ?Evaluation of clus-
tering?, Manning et al (2008)). Projection signifi-
cantly5 outperforms all baselines for both datasets.
Dataset Algorithm Purity R P F1
ISP
Projection .74 .40 .68 .50
K-means A .65 .18 .22 .20
K-means B .65 .13 .24 .17
K-means C .65 .18 .26 .22
Bank
Projection .79 .26 .53 .35
K-means A .61 .14 .14 .14
K-means B .64 .13 .19 .16
K-means C .67 .17 .21 .19
Table 2: Evaluation results.
For completeness we experimented with applying
Chinese Whispers clustering to sentence connectiv-
ity graphs, but the results were inferior to K-means.
Table 3 presents sample sentences from clusters
produced by projection and K-means for illustration.
Our initial analysis showed that our approach indeed
produces more homogenous clusters than the base-
line methods, as conjectured in Section 3.2. We con-
sider it advantageous, since it?s easier for a human to
merge clusters than to reveal sub-clusters. E.g., a GS
cluster of 20 sentences referring to fees and charges
is covered by three projection clusters labeled fee,
charge and interest rate, with 9, 8 and 2 sentences
correspondingly. On the other hand, K-means C
method places 11 out of the 20 sentences in a messy
cluster of 57 sentences (see Table 3), scattering the
remaining 9 sentences over 7 other clusters.
In our current implementation fee, charge and in-
terest rate were not detected by the lexical resources
we used as semantically similar and thus were not
5p=0.001 according to McNemar test (Dietterich, 1998).
grouped in one term cluster. However, adding more
resources may introduce additional noise. Such de-
pendency on coverage and accuracy of resources is
apparently a limitation of our approach. Yet, as
our experiments indicate, using only two generic re-
sources already yielded valuable results.
a. Projection
credit card, card, mastercard, visa (38 sentences)
XXX has the worst credit cards ever
XXX MasterCard is the worst credit card I?ve ever had
ntuc do not accept XXX visa now I have to redraw $150...
XXX card declined again , $40 dinner in SF...
b. K-means C
fee, charge (57 sentences)
XXX playing games wit my interest
arguing w incompetent pol at XXX damansara perdana
XXX?s upper management are a bunch of rude pricks
XXX are ninjas at catching fraudulent charges.
Table 3: Excerpt from resulting clusterings for the bank
dataset. Bank name is substituted with XXX. Cluster la-
bels are given in italics. Two most frequent terms are
assigned as cluster labels for K-means C.
6 Conclusions and Future Work
We presented a novel sentence clustering scheme
and evaluated its implementation, showing signifi-
cantly superior performance over common sentence
clustering techniques. We plan to further explore
the suggested scheme by utilizing additional lexical
resources and clustering algorithms. We also plan
to compare our approach with co-clustering meth-
ods used in document clustering (Xu et al (2003),
Dhillon (2001), Slonim and Tishby (2000)).
Acknowledgments
This work was partially supported by the MAGNE-
TON grant no. 43834 of the Israel Ministry of Indus-
try, Trade and Labor, the Israel Ministry of Science
and Technology, the Israel Science Foundation grant
1112/08, the PASCAL-2 Network of Excellence of
the European Community FP7-ICT-2007-1-216886
and the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
41
References
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City, USA.
Endre Boros, Paul B. Kantor, and David J. Neu. 2001. A
clustering based approach to creating multi-document
summaries.
Hsin-Hsi Chen, June-Jei Kuo, and Tsei-Chun Su.
2003. Clustering and visualization in a multi-lingual
multi-document summarization system. In Proceed-
ings of the 25th European conference on IR re-
search, ECIR?03, pages 266?280, Berlin, Heidelberg.
Springer-Verlag.
Inderjit S. Dhillon. 2001. Co-clustering documents and
words using bipartite spectral graph partitioning. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, KDD ?01, pages 269?274, New York, NY,
USA. ACM.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457?479, Decem-
ber.
C. Fellbaum. 1998. WordNet ? An Electronic Lexical
Database. MIT Press.
Alfio Massimiliano Gliozzo, Carlo Strapparava, and Ido
Dagan. 2005. Investigating unsupervised learning for
text categorization bootstrapping. In HLT/EMNLP.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.
Holcombe, Regina Barzilay, Min yen Kan, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible clus-
tering tool for summarization. In In Proceedings of the
NAACL Workshop on Automatic Summarization, pages
41?49.
A. Hotho, S. Staab, and G. Stumme. 2003. Word-
net improves text document clustering. In Ying
Ding, Keith van Rijsbergen, Iadh Ounis, and Joe-
mon Jose, editors, Proceedings of the Semantic Web
Workshop of the 26th Annual International ACM SI-
GIR Conference on Research and Development in In-
formaion Retrieval (SIGIR 2003), August 1, 2003,
Toronto Canada. Published Online at http://de.
scientificcommons.org/608322.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, E. K. Park, and
Xiaohua Zhou. 2009. Exploiting wikipedia as exter-
nal knowledge for document clustering. In Proceed-
ings of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?09, pages 389?396, New York, NY, USA. ACM.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. JNLE, 16:359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics - Vol-
ume 2, COLING ?98, pages 768?774, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, Juli.
Tadashi Nomoto and Yuji Matsumoto. 2001. A new ap-
proach to unsupervised text summarization. In Pro-
ceedings of the 24th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ?01, pages 26?34, New York, NY,
USA. ACM.
Claude Pasquier. 2010. Task 5: Single document
keyphrase extraction using sentence clustering and la-
tent dirichlet alocation. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 154?157, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Shady Shehata. 2009. A wordnet-based semantic model
for enhancing text clustering. Data Mining Work-
shops, International Conference on, 0:477?482.
Chao Shen, Tao Li, and Chris H. Q. Ding. 2011. Integrat-
ing clustering and multi-document summarization by
bi-mixture probabilistic latent semantic analysis (plsa)
with sentence bases. In AAAI.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?00, pages
208?215, New York, NY, USA. ACM.
M. Steinbach, G. Karypis, and V. Kumar. 2000. A
comparison of document clustering techniques. KDD
Workshop on Text Mining.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A large ontology from
wikipedia and wordnet.
42
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, SIGIR ?03, pages 267?273, New
York, NY, USA. ACM.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement prin-
ciple and sentence clustering. In SIGIR, pages 113?
120.
43
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 20?29,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Classification-based Contextual Preferences
Shachar Mirkin, Ido Dagan, Lili Kotlerman
Bar-Ilan University
Ramat Gan, Israel
{mirkins,dagan,davidol}@cs.biu.ac.il
Idan Szpektor
Yahoo! Research
Haifa, Israel
idan@yahoo-inc.com
Abstract
This paper addresses context matching in tex-
tual inference. We formulate the task under
the Contextual Preferences framework which
broadly captures contextual aspects of infer-
ence. We propose a generic classification-
based scheme under this framework which co-
herently attends to context matching in infer-
ence and may be employed in any inference-
based task. As a test bed for our scheme we use
the Name-based Text Categorization (TC) task.
We define an integration of Contextual Prefer-
ences into the TC setting and present a concrete
self-supervised model which instantiates the
generic scheme and is applied to address con-
text matching in the TC task. Experiments on
standard TC datasets show that our approach
outperforms the state of the art in context mod-
eling for Name-based TC.
1 Introduction
Textual inference is prevalent in text understanding
applications. For example, in Question Answering
(QA) the expected answer should be inferred from
retrieved passages, and in Information Extraction (IE)
the meaning of the target event is inferred from its
mention in the text.
Lexical inferences make a substantial part of the
inference process. In such cases, a target term is
inferred from text expressions based on either one of
two types of lexical matches: (i) a direct match of
the target term in the text. For instance, the IE event
injure may be detected by finding the word injure in
the text; (ii) an indirect match, through a term that
implies the meaning of the target term, e.g. inferring
injure from hurt.
In either case, due to word ambiguity, it is nec-
essary to validate that the context of the match con-
forms with the intended meaning of the target term
before carrying out an inference operation based on
this match. For example, ?You hurt my feelings? con-
stitutes an invalid context for the injure event as hurt
in this text does not refer to a physical injury. Simi-
larly, inferring the protest-related event demonstrate
based on demo is deemed invalid although demo im-
plies the meaning of the word demonstrate in other
contexts, e.g., concerning software demonstration.
Although seemingly equivalent, a closer look re-
veals that the above two examples correspond to two
distinct contextual mismatch situations. While the
match of hurt is invalid for injure in the particular
given context, an inference based on demo is invalid
for the protest demonstrate event in any context.
Thus, several types of context matching are in-
volved in textual inference. While most prior work
addressed only specific context matching scenarios,
Szpektor et al (2008) presented a broader view,
proposing a generic framework for context match-
ing in inference, termed Contextual Preferences (CP).
CP specifies the types of context matching that need
to be considered in inference, allowing a model of
choice to be applied for validating each type of match.
Szpektor et al applied CP to an IE task using differ-
ent models to validate each type of context match.
In this work we adopt CP as our context matching
framework and propose a novel classification-based
scheme which provides unified modeling for CP. We
represent typical contexts of the textual objects that
participate in inference using classifiers; at inference
time, each match is assessed by the respective classi-
fiers which determine its contextual validity.
As a test bed we applied our scheme to the task
20
of Name-based Text Categorization. This is an unsu-
pervised setting of TC where the only input given is
the category name, and in which context validation
is of high importance. We instantiate the scheme
with a novel self-supervised model and apply it to
the TC task. We suggest a method for integrating any
CP-based context matching model into TC and use it
to combine the context matching scores generated by
our model. Results on two standard TC datasets show
that our approach outperforms the state of the art con-
text model for this task and suggest applying this
scheme to additional inference-based applications.
2 Background
2.1 Context matching in inference
Word ambiguity has been traditionally addressed
through Word Sense Disambiguation (WSD) (Nav-
igli, 2009). The WSD task requires selecting the
meaning of a target term from amongst a predefined
set of senses, based on sense-inventories such as
WordNet (Fellbaum, 1998).
An alternative approach eliminates the reliance on
such inventories. Instead of explicit sense identifi-
cation, a direct sense-match between terms is pur-
sued (Dagan et al, 2006). Lexical substitution (Mc-
Carthy and Navigli, 2009) is probably the most com-
monly known task that follows this approach. Con-
text matching is a generalization of lexical substitu-
tion, which seeks a match between terms in context,
not necessarily for the purpose of substitution. For in-
stance, the word played in ?U2 played their first-ever
concert in Russia? contextually matches music, al-
though music cannot substitute played in this context.
The context matching task, therefore, is to determine
(by quantifying or giving a binary decision) the va-
lidity of a match between two terms in context.
In Section 1 we informally presented two cases of
contextual mismatches. A comprehensive view of
context matching types is provided by the Contextual
Preferences framework (Szpektor et al, 2008). CP
is phrased in terms of the Textual Entailment (TE)
paradigm (Dagan et al, 2009). In TE, a text t entails
a textual hypothesis h if the meaning of h can be
inferred from t. Formulating the IE example from
Section 1 within TE, h may be the name of the target
event, injure, and t is a text segment from which h
can be inferred. A direct match occurs when a term
in h is identical to a term in t. An inference based
on an indirect match is viewed as the application of
a lexical entailment rule, r, such as ?hurt? injure?,
where the entailing left-hand side (LHS) of the rule
(hurt) is matched in the text, while the entailed right-
hand side (RHS), injure, is matched in the hypothesis.
Hence, three inference objects take part in infer-
ence operations: t, h and r. Most prior work ad-
dressed only specific contextual matches between
these objects. For example, Harabagiu et al (2003)
matched the contexts of t and h for QA (answer and
question, respectively); Barak et al (2009) matched
t and h (document and category) in TC, while other
works, including those applying lexical substitution,
typically validated the context match between t and r
(Kauchak and Barzilay, 2006; Dagan et al, 2006;
Pantel et al, 2007; Connor and Roth, 2007).
In comparison, in the CP framework, all possible
contextual matches among t, h and r are considered:
t?h, t? r and r?h. The three context matches are
depicted in Figure 1 (left). In CP, the representation
of each inference object is enriched with contextual
information which is used to characterize its valid
contexts. Such information may be the words of the
event description in IE, corpus instances based on
which a rule was learned, or an annotation of relevant
WordNet senses in Name-based TC. For example,
a category name hockey may be assigned with the
sense number corresponding to ice hockey, but not
to field hockey, in order to designate information that
limits the valid contexts of the category to the former
among the two meanings of the name.
Before an inference operation is performed, the
context representations of each pair among the partic-
ipating objects should be matched by a context model
in order to assess the contextual validity of the opera-
tion. Along with the context representation and the
specific context matching models, the way context
model decisions are combined needs to be specified
in a concrete implementation of the CP framework.
2.2 Context matching models
Several approaches were taken in prior work to
model context matching, mostly within the scope
of learning selectional preferences of templatized
lexical-syntactic rules (e.g. ?X
subj
???? hit
obj
??? Y ??
?X
subj
???? attack
obj
??? Y ?).
21
Pantel et al (2007) and Szpektor et al (2008) rep-
resented the context of such rules as the intersection
of preferences of the rule?s LHS and RHS, namely the
observed argument instantiations or their semantic
classes. A rule is deemed applicable to a given text if
the argument instantiations in the text are similar to
the selectional preferences of the rule. To overcome
sparseness, other works represented context in latent
space. Pennacchiotti et al (2007) and Szpektor et al
(2008) measured the similarity between the Latent
Semantic Analysis (LSA) (Deerwester et al, 1990)
representations of matched contexts. Dinu and La-
pata (2010) used Latent Dirichlet Allocation (LDA)
(Blei et al, 2003) to model templates? latent senses,
determining rule applicability based on the similarity
between the two sides of the rule when instantiated
by the context, while Ritter et al (2010) used LDA
to model argument classes, considering a rule valid
for a given argument instantiation if its instantiated
templates are drawn from the same hidden topic.
A different approach is provided by classification-
based models which learn classifiers for inference
objects. A classifier is trained based on positive and
negative examples which represent valid or invalid
contexts of the object; from those, features charac-
terizing the context are extracted, e.g. words in a
window around the target term or syntactic links with
it. Given a new context, the classifier assesses its va-
lidity with respect to the learned classification model.
Classifiers in prior work were applied to determine
rule applicability in a given context (t ? r). Train-
ing a classifier for word paraphrasing, Kauchak and
Barzilay (2006) used occurrences of the rule?s RHS as
positive context examples, and randomly picked neg-
ative examples. A similar approach was applied by
Dagan et al (2006), which used a single-class SVM
to avoid selecting negative examples. In both works,
a resulting classifier represents a word with all its
senses intermixed. Clearly, this poses no problem for
monosemous words, but is biased towards the more
common senses of polysemous words. Indeed, Dagan
et al (2006) report a negative correlation between the
degree of polysemy of a word and the performance of
its classifier. Connor and Roth (2007) used per-rule
classifiers to produce a noisy training set for learning
a global classifier for verb substitution.
In this work we follow the classification-based ap-
proach which seems appealing for several reasons.
t
r
t
C h(
r)
h
C r(
t)
C h(
t)r
h
Figure 1: Left: An illustration of the CP relationships as in
(Szpektor et al, 2008), with arrows indicting the context
matching direction; Right: The application of classifiers
to the tested contexts under our scheme.
First, it allows seamlessly integrating various types
of information via classifiers? features; unlike some
of the above models, it is not inherently dependent on
the type of rules that are utilized and easily accom-
modates to both lexical and lexical-syntactic rules
through the choice of features. In addition, it does
not rely on a predefined similarity measure and pro-
vides flexibility in terms of model?s parameters. Fi-
nally, this approach captures the notion of direction-
ality which is fundamental in textual inference, and
is therefore better suited to applied inference than
previously proposed symmetric context models.
In comparison to prior classification-based models,
our approach addresses all three context matches
specified by CP, rather than only the rule-text match.
It is not limited to substitutable terms or even to
terms with the same part of speech. In addition, we
avoid learning a classifier for all senses combined,
but rather learn it for the specific intended meaning.
2.3 Name-based Text Categorization
Name-based TC (Gliozzo et al, 2009) is an unsu-
pervised setting of Text Categorization in which the
only input provided is the category name, e.g. trade,
?mergers and acquisitions? or guns. When category
names are ambiguous, e.g. space, categories are not
well defined; thus, auxiliary information is expected
to accompany the name for disambiguation, such as
a list of relevant senses or a category description.
Typically, unsupervised TC consists of two steps.
First, an unsupervised method is applied to an unla-
beled corpus, automatically labeling some of the doc-
uments to categories. Then, the labeled documents
from the first step are used to train a supervised TC
classifier which is used to label any document in the
test set (Gliozzo et al, 2009; Downey and Etzioni,
2009; Barak et al, 2009).
22
In this work we focus on the above unsupervised
step. Gliozzo et al (2009) addressed this task by rep-
resenting both documents and categories by LSA vec-
tors which implicitly capture contextual similarities
between terms. Each document was then assigned
to the most similar category based on cosine simi-
larity between the LSA vectors. Barak et al (2009)
required an occurrence of a term entailing the cat-
egory name (or the category name itself) in order
to regard the category as a candidate for the docu-
ment. To assess the contextual validity of the match,
they used LSA document-category similarity as in
(Gliozzo et al, 2009). For example, to classify a doc-
ument into the category medicine, at least one lexical
entailment rule, e.g. ?drug? medicine?, should be
matched in the document. Then, the validity of drug
for medicine in the matched document is assessed by
the LSA context model. In this work we adopt Barak
et al?s requirement for a match for the category in
the document, but address context matching in an
entirely different way.
Name-based TC provides a convenient setting for
evaluating context matching approaches for two main
reasons. First, all types of context matchings are real-
ized in this application (see Section 3); second, as the
hypothesis consists of a single term or a few terms,
the TC gold standard annotation corresponds quite
directly to the context matching task for lexical infer-
ences; in other applications where longer hypotheses
are involved, context matching performance may be
masked by other factors.
3 Contextual Matches in TC
Within Name-based TC, the Textual Entailment ter-
minology is mapped as follows: h is a term denoting
the category name (e.g. merger or acquisition); t is
a matched term in the document to be categorized
from which h may be inferred; and a match refers
to an occurrence in the document of either h (direct
match) or the LHS of an entailment rule r whose RHS
is a category name (indirect match).1
Under the CP view, a context model needs to ad-
dress the following three context matching cases
within a TC setting.
t?h: Assessing the validity of a match in the docu-
ment with respect to the category?s intended meaning.
1Note that t and h both refer here to individual terms.
For example, the occurrence of the category name
space (in the sense of outer space) in ?the server ran
out of disk space? does not indicate a space-related
text, and should be dismissed by the context model.
t? r: This case refers to a rule match in the docu-
ment. A context model should ensure that the mean-
ing of a match is compatible with that of the rule.
For example, ?alien? space? is a valid rule for the
space category. Yet, it should not be applied to ?The
US welcomes a large number of aliens every year?,
since alien in this sentence has a different meaning
than the intended meaning of the rule.
r ? h: The match between the intended meanings
of the category name and the RHS of the rule. For
instance, the rule ?room? space? is not suitable at
all for the (outer) space category.
4 A Classification-based Scheme for CP
Szpektor et al (2008) introduced a vector-space
model to implement CP, in which the text t, the rule
r and the hypothesis h share the same contextual
representation. However, in CP, r, h and t have non-
symmetric roles: the context of t should be tested as
valid for r and h and not vice versa, and the context
of r should be validated for h and not the other way
around. This stems from the need to consider direc-
tionality in context matching. For instance, a text
about football typically constitutes a valid context
for the more general sports context, but not vice
versa. Indeed, directionality may be captured in
vector-space models by using a directional similarity
measure (Kotlerman et al, 2010), but only symmetric
measures were used in context matching work so far.
Based on this distinction between the inference
objects? roles, we present a novel scheme that uses
two types of classifiers to represent context:
Ch: A classifier that identifies valid contexts for h. It
tests contexts of t (for t? h matching) or r (for
r ? h matching), assigning them scores Ch(t)
and Ch(r), respectively.
Cr: A classifier that identifies valid contexts for ap-
plying the rule r. It tests the context of t, assign-
ing it a score Cr(t).
Figure 1 (right) shows the classifiers scores which
are assigned to each of the matching types.
23
Hence, h always acts as the classifying object, t is
always the classified object, while r acts as both. Con-
text matching is quantified by the degree by which
the classified object represents a valid context for the
classifying object in a given inference scenario.
In comparison to the CP implementation in (Szpek-
tor et al, 2008), our approach uses a unified model
which captures directionality in context matching.
To instantiate the scheme, one needs to define the
way training examples are obtained and processed.
This may be done within supervised classification,
where labeled examples are provided, or ? as we do
in this work ? using self-supervised classifiers which
obtain training examples automatically. We present
such an instantiation in Section 5, where a classifier is
trained for each category and each rule. When more
complex hypotheses are involved, Ch classifiers can
be trained separately for each relevant part of the
hypothesis, using the rest for disambiguation.
A combination of the three model scores provides
a final context matching score. In Section 6 we sug-
gest a way to combine the actual classification scores
as part of the integration of CP into TC, but other
combinations are plausible. In particular, binary clas-
sifications (valid vs. invalid) may be used as filters.
That is, the context is classified as valid only if all
relevant models classify it as such.
5 A Self-supervised Context Model
We now turn to demonstrate how our classification-
based scheme may be implemented. The model be-
low is exemplified on Name-based TC, but may be
applicable to other tasks, with few changes.
5.1 Training-set generation
Our implementation is self-supervised as we want
to integrate it within the unsupervised TC setting.
That is, the classifiers automatically obtain training
examples for the classifying object (a category or a
rule) without relying on labeled documents.
We obtain examples by querying the TC training
corpus with automatically-generated search queries.
The difficulty lies in correctly constructing queries
that will retrieve documents representing either valid
or invalid contexts for the classifying object. To this
end, we retrieve examples through a gradual process
in which the most accurate (least ambiguous) query
is used first and less accurate queries follow, until the
designated number of examples is acquired.
5.1.1 Obtaining positive examples
To acquire positive training examples, we con-
struct queries which are comprised of two main
clauses. The first contains the seeds, terms which
characterize the classifying object. Primarily, these
are the category name or the LHS of the rule. The sec-
ond consists of context words which are used when
the seeds are polysemous, and are intended to assist
disambiguation. When context words are used, at
least one seed and at least one context word must
be matched to retrieve a document. For example,
given the highly ambiguous category name space,
we first construct the query using only the monose-
mous term outer space; if the number of retrieved
documents does not meet the requirement, a second
query may be constructed: (?outer space? OR space)
AND (infinite OR science OR . . . ).
To generate a rule classifier Cr, we retrieve posi-
tive examples as follows. If the LHS term is monose-
mous according to WordNet2, we first query using
this term alone (e.g. decrypt), and add its monose-
mous synonyms and hyponyms if more examples are
required (e.g. decrypt OR decode). If the LHS is
polysemous, we carry out Procedure 1. Intuitively,
this procedure tries to minimize ambiguity by using
monosemous terms as much as possible; when poly-
semous terms must be used, it tries to ensure there are
monosemous terms to disambiguate them. Note that
entailment directionality is maintained throughout
the process, as seeds are only expanded with more
specific (entailing) terms, while context words are
only expanded with more general (entailed) terms.
Procedure 1 : Retrieval of Cr positive examples
Apply sequentially until sufficient examples are obtained:
1: Set the LHS as seed and the RHS?s monosemous syn-
onyms, hypernyms and derivations as context words.
2: Add monosemous synonyms and hyponyms of the
LHS to the seeds.
3: As in 2, but use polysemous terms as well.
4: Add polysemous context words.
Positive examples for category classifiers (Ch) are
obtained through a similar procedure as for rule clas-
2Terms not in WordNet are assumed monosemous.
24
sifiers. If the category is part of a hierarchy, we also
use the name of the parent category (e.g. sport for
rec.sport.hockey) as a context word.
5.1.2 Obtaining negative examples
Negative examples are even more challenging to
acquire. In prior work negative examples were se-
lected randomly (Kauchak and Barzilay, 2006; Con-
nor and Roth, 2007). We follow this method, but
also attempt to identify negative examples that are
semantically similar to the positive ones in order to
improve the discriminative power of the classifier
(Smith and Eisner, 2005). We do that by applying
a similar procedure which uses cohyponyms of the
seeds, e.g. baseball for hockey or islam for christian-
ity. Cohyponymy is a non-entailing relation; hence,
by using it we expect to obtain semantically-related,
yet invalid contexts. If not enough negative exam-
ples are retrieved using cohyponyms, we select the
remaining required examples randomly.
As the distribution of positive and negative ex-
amples in the data is unknown, we set the ratio of
negative to positive examples as a parameter of the
model, as in (Bergsma et al, 2008).
5.1.3 Insufficient examples
When the number of training examples for a rule
or a category is below a certain minimum, the re-
sulting classifier is expected to be of poor quality.
This usually happens for positive examples in any of
the following two cases: (i) the seed is rare in the
training set; (ii) the desired sense of the seed is rarely
found in the training set, and unwanted senses were
filtered by our retrieval query. For instance, nazarene
does not occur at all in the training set, and the classi-
fier corresponding to the rule ?nazarene? christian?
cannot be generated. On the other hand, cone does
appear in the corpus but not in the astrophysical sense
the rule ?cone? space? refers to. In such cases we
refrain from generating the classifier and use instead
a default score of 0 for each classified object. The
idea is that rare terms will also occur infrequently in
the test set, while cases where the term is found in
the corpus, but in a different sense than the desired
one, will be blocked.
5.1.4 Feature extraction
We extract global and local lexical features that are
standard in WSD work. Global features include all
the terms in the document or in the sentence in which
a match was found. Local features are extracted
around matches of seeds which comprised the query
that retrieved the document. These features include
the terms in a window around the match, and the
noun, verb, adjective and adverb nearest to the match
in either direction. For randomly sampled negative
examples, where no matched query terms exist, we
randomly select terms in the document as ?matches?
for local feature extraction. If more than one match of
the same term is found in a document, we assume one-
sense-per-discourse (Gale et al, 1992) and jointly
extract features for all matches of the term.
5.2 Applying the classifiers
During inference, for each direct match in a docu-
ment, the corresponding Ch is applied. For an indi-
rect match, the respective Cr is also applied.
In addition, Ch is applied to the matched rules.
Unlike t, a rule is not represented by a single text.
Therefore, to test a rule?s match with the category,
we randomly sample from the training set documents
containing the rule?s LHS. We apply Ch to each sam-
pled example and compute the ratio of positive classi-
fications. The result is a score indicating the domain-
specific probability of the rule to be applicable to
the category, and may be interpreted as an in-domain
prior. For instance, the rule ?check ? hockey? is
assigned a score of 0.05, since the sense of check
as a hockey defense technique is rare in the corpus.
On the other hand, non ambiguous rules, e.g. ?war-
ship ? ship? are assigned a high probability (1.0),
and so are rules whose LHS is ambiguous but its dom-
inant sense in the training corpus is the same one the
rule refers to, e.g. ?margin? earnings?(0.85).
We do not assign negative classifier scores to in-
valid matches but rather set them to zero instead. The
reason is that an invalid context only indicates that
the term cannot be used for entailing the category
name, but not that the document itself is irrelevant.
6 CP for Text Categorization
CP may be employed in any inference-based task,
but the integration with each task is somewhat dif-
ferent and needs to be specified. Below we present
a methodology for integrating CP into Name-based
Text Categorization.
25
As in (Barak et al, 2009) (Barak09 below), we
represent documents and categories by term-vectors
in the following way: a document vector contains
the document terms; a category vector contains two
sets of terms: C, the terms denoting the category
name, and E , their entailing terms. For example, oil
is added to the vector of the category crude by the
rule ?oil? crude? (i.e. crude ? C and oil ? E).
Barak09 assigned equal values of 1 to all vector
entries. We suggest integrating a CP-based context
model into TC by re-weighting the terms in the vec-
tors, prior to determining the final document-category
categorization score through vector similarity. Given
a category c, with term vector C, and a document d
with term vector D, the model re-weights vector en-
tries of matching terms (i.e., terms in C ?D), based
on the validity of the context match. Valid matches
should be assigned with higher scores than invalid
ones, leading to higher overall vector similarity for
documents with valid matches for the given category.
Non-matching terms are ignored as their weights are
canceled out in the subsequent vector product.
Specifically, the model assigns a new weight
wD(u) to a matching term u in the document vec-
tor D based on the model?s assessment of: (a) t? h,
the context match between the (match in the) doc-
ument and the category; and (if an indirect match)
(b) t ? r, the context match between the document
and the rule ?u? ci?, where ci ? C. The model also
sets a new weight wC(v) to a term v in the category
vector C based on the context match for r ? h, be-
tween the rule ?v ? cj? (cj ? C) and the category.
For instance, using our context matching scheme in
TC, wD(u) is set to Ch(u) or
Ch(u)+Cr(u)
2 for direct
and indirect matches, respectively; wC(v) is left as 1
if v ? C and set to Ch(v) when v ? E .
Barak09 assigned a single global context score to
a document-category pair using the LSA representa-
tions of their vectors. In our approach, however, we
consider the actual matches from the three different
views, hence the re-weighting of the vector entries
using three model scores.
7 Experimental Setting
7.1 Datasets and knowledge resources
Following (Gliozzo et al, 2009) and (Barak et al,
2009), we evaluated our method on two standard TC
datasets: Reuters-10 and 20-Newsgroups.
The Reuters-10 (R10, for short) is a sub-corpus
of the Reuters-21578 collection3, constructed from
the ten most frequent categories in the Reuters tax-
onomy. We used the Apte split of the Reuters-21578
collection, often used in TC tasks. The top 10 cate-
gories include about 9,000 documents, split into train-
ing (70%) and test (30%) sets. The 20-Newsgroups
(20NG) corpus is a collection of newsgroup postings
gathered from twenty different categories from the
Usenet Newsgroups hierarchy4. We used the ?by-
date? version of the corpus, which contains approxi-
mately 20,000 documents partitioned (nearly) evenly
across the categories and divided in advance to train-
ing (60%) and test (40%) sets.
As in (Gliozzo et al, 2009; Barak et al, 2009), we
adjusted non-standard category names (e.g. forsale
was renamed to sale) and manually specified for each
category its relevant WordNet senses. The sense tag-
ging properly defines the categories, and is expected
to accompany such hypotheses. Other types of in-
formation may be used for this purpose, e.g. words
from category descriptions, if such exist.
We applied standard preprocessing (sentence split-
ting, tokenization, lemmatization and part of speech
tagging) to all documents in the datasets. All terms,
including those denoting category names and rules,
are represented by their lemma and part of speech.
As sources for lexical entailment rules we used
WordNet 3.0 (synonyms, hyponyms, derivations
and meronyms) and a Wikipedia-derived rule-base
(Shnarch et al, 2009). Unlike Barak09 we did not
limit the rules extracted from WordNet to the most
frequent senses and used all rule types from the
Wikipedia-based resource.
7.2 Self-supervised model tuning
Tuning of the self-supervised context model?s pa-
rameters (number of training examples, negative to
positive ratio, feature set and the way negative exam-
ples are obtained) was performed over development
sets sampled from the training sets. Based on this tun-
ing, some parameters varied between the datasets and
between classifier types (Ch vs. Cr). For example,
3http://kdd.ics.uci.edu/databases/
reuters21578/reuters21578.html
4http://people.csail.mit.edu/jrennie/
20Newsgroups/
26
selection of negative examples based on cohyponyms
was found useful for Cr classifiers in R10, while ran-
dom examples were used in the rest of the cases.
We used SVMperf (Joachims, 2006) with a linear
kernel and binary feature weighting.
For querying the corpus we used the Lucene search
engine5 in its default setting. Up to 150 positive
examples were retrieved for each classifier, with 5
examples set as the required minimum. This resulted
in generating 100% of the hypothesis classifiers for
both datasets and 95% and 70% of the rule classifiers
for R10 and 20NG, respectively.
We computed Ch(r) scores based on up to 20 sam-
pled instances. If less than 2 examples were found in
the training set, we assigned an ?unknown? context
match probability of 0.5, since a rare LHS occurrence
does not indicate anything about its meaning in the
corpus. Such cases constituted 2% (R10) and 11%
(20NG) of the utilized rules.
7.3 Baseline models
To provide a more meaningful comparison with prior
work, we focus on the first unsupervised step in the
typical Name-based TC flow, without the subsequent
supervised training. Our goal is to improve the accu-
racy of this first step, and we therefore compare our
context model?s performance to two unsupervised
methods used by Barak09.
The first baseline, denoted Barakno-cxt, is the co-
sine similarity score between the document and cate-
gory vectors where all terms are equally weighted to
a score of 1.6 This baseline shows the performance
when no context model is employed.
The second baseline, denoted Barakfull, is a repli-
cation of the state of the art context model for Name-
based TC. In this method, LSA vectors are con-
structed for a document by averaging the LSA vectors
of its individual terms, and for a category by averag-
ing the LSA vectors of the terms denoting its name.
The categorization score of a document-category pair
is set to be the product between the cosine similarity
score of the LSA vectors and the score given by the
above Barakno-cxt method. We note that LSA-based
context models performed best also in (Gliozzo et al,
2009) and (Szpektor et al, 2008).
5http://lucene.apache.org
6Other attempted weighting schemes, such as tf-idf, did not
yield better performance.
Model
Reuters-10
Accuracy P R F1
Barakno-cxt 73.2 63.6 77.0 69.7
Barakfull 76.3 68.0 79.2 73.2
Class.-based 79.3 71.8 83.6 77.2
Model
20-Newsgroups
Accuracy P R F1
Barakno-cxt 63.7 44.5 74.6 55.8
Barakfull 69.4 50.1 82.8 62.4
Class.-based 73.4 54.7 76.4 63.7
Table 1: Evaluation results.
All models were constructed based on the TC train-
ing sets, using no external corpora. The vocabulary
consists of terms that appear more than once in the
training set. The terms we consider include nouns,
verbs, adjectives and adverbs, as well as nominal
multi-word expressions.
8 Results and Analysis
Given a document, all categories for which a lexical
match was found in the document are considered,
and the document is classified to the highest scoring
category. If all categories are assigned non-positive
scores, the document is not assigned to any of them.
Based on this requirement that a document con-
tains at least one match for the category, 4862
document-category pairs were considered for clas-
sification in R10 and 9955 pairs in 20NG. We eval-
uated our context model, as well as the baselines,
based on the accuracy of these classifications, i.e.
the percentage of correct decisions among the candi-
date document-category pairs. We also measured the
models? performance in terms of micro-averaged pre-
cision (P ), relative recall (R) and F1. Like Barak09,
recall is computed relative to the potential recall of
the rule-set which provides the entailing terms.
Table 1 presents the evaluation results. As in
Barak09, the LSA-based model outperforms the first
baseline, supporting its usefulness as a context model.
In both datasets our model outperformed the base-
lines in terms of accuracy. This result is statistically
significant with p < 0.01 according to McNemar?s
test (McNemar, 1947). Recall is lower for our model
in 20NG but F1 scores are higher for both datasets.
These results indicate that the classification-based
context model provides a favorable alternative to the
27
Removed
Reuters-10 20-Newsgroups
Accuracy F1 Accuracy F1
- 79.3 77.2 73.4 63.7
Ch(t) 76.2 72.3 71.9 61.0
Cr(t) 80.5 77.6 74.3 64.5
Ch(r) 78.4 75.7 73.1 63.4
Table 2: Ablation tests results.
state of the art LSA-based method.
Table 2 presents ablation tests of our model. In
each test we measured the classification performance
when one of the three classification scores is ignored.
Clearly, Ch(t) is the most beneficial component, and
in general the category classifiers help improving
overall performance. The limited performance of Cr
may be related to higher ambiguity in rules relative to
category names, resulting in noisier training data. In
addition, the small size of the training set limits the
number of training examples for rule classifiers. This
problem affects Cr more than Ch since, by nature,
the corpus includes more occurrences of category
names. Still, Cr contributes to improved recall (this
fact is not visible in Table 2).
The coverage of the utilized rule-set determines
the maximal (absolute) recall that can be achieved
by any model. With the rule-set we used in this ex-
periment, the recall upper bound was 59.1% for R10
and 40.6% for 20NG. However, rule coverage af-
fects precision as well: In many cases documents are
assigned to incorrect categories because the correct
category is not even a candidate as no entailing term
was matched for it in the document. For instance,
a document with the sentence ?For sale or trade!!!
BMW R60US. . . ? was classified by our method to
the category forsale, while its gold-standard category
is motorcycles. Yet, none of the rules in our rule-set
triggered motorcycles as a candidate category for this
document. Ideally, a context model would rule out
all incorrect candidate categories; in practice even a
single low score for one of the competing categories
results in a false positive error in such cases (in addi-
tion to the recall loss). To reduce these problems we
intend to employ additional knowledge resources in
future work.
Our algorithm for retrieving training examples
turned out to be not sufficiently accurate, particularly
for negative examples. This is a challenging task that
requires further research. Although useful for some
classifier types, the use of cohyponyms may retrieve
potentially positive examples as negative ones, since
terms that are considered cohyponyms in WordNet
are often perceived as near synonyms in common
usage, e.g. buyout and purchase in the context of
acquisitions. Likewise, using WordNet senses to de-
termine ambiguity is also inaccurate. Rare or too
fine-grained senses, common in WordNet, cause a
term to be considered ambiguous, which in turn trig-
gers the use of less accurate retrieval methods. For
example, auction has a bridge-related WordNet sense
which is irrelevant for our dataset, but made the term
be considered ambiguous. This calls for develop-
ment of other methods for determining word ambigu-
ity, which consider the actual usage of terms in the
domain rather than relying solely on WordNet.
9 Conclusions
In this paper we presented a generic classification-
based scheme for comprehensively addressing con-
text matching in textual inference scenarios. We
presented a concrete implementation of the proposed
scheme for Name-based TC, and showed how CP
decisions can be integrated within the TC setting.
Utilizing classifiers for context matching offers
several advantages. They naturally incorporate di-
rectionality and allow integrating various types of
information, including ones not used in this work
such as syntactic features. Our results indeed support
this approach. Still, further research is required re-
garding issues raised by the use of multiple classifiers,
scalability in particular.
Hypotheses in TC are available in advance. While
also the case in other applications, it constitutes a
practical challenge when hypotheses are given ?on-
line?, like Information Retrieval queries, since classi-
fiers will have to be generated on the fly. We intend
to address this issue in future work.
Lastly, we plan to apply the generic classification-
based approach to address context matching in other
inference-based applications.
Acknowledgments
This work was partially supported by the Israel Sci-
ence Foundation grant 1112/08 and the NEGEV
project (www.negev-initiative.org).
28
References
Libby Barak, Ido Dagan, and Eyal Shnarch. 2009. Text
Categorization from Category Name via Lexical Refer-
ence. In HLT-NAACL (Short Papers).
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative Learning of Selectional Preference from
Unlabeled Text. In In Proceedings of EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022, March.
Michael Connor and Dan Roth. 2007. Context Sensitive
Paraphrasing with a Global Unsupervised Classifier. In
Proceedings of ECML.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct Word
Sense Matching for Lexical Substitution. In Proceed-
ings of COLING-ACL.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing Textual Entailment: Rational, Eval-
uation and Approaches. Natural Language Engineer-
ing, pages 15(4):1?17.
Scott Deerwester, Scott Deerwester, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by Latent Semantic Anal-
ysis. Journal of the American Society for Information
Science, 41:391?407.
Georgiana Dinu and Mirella Lapata. 2010. Topic Models
for Meaning Similarity in Context. In Proceedings of
Coling 2010: Posters.
Doug Downey and Oren Etzioni. 2009. Look Ma, No
Hands: Analyzing the Monotonic Feature Abstraction
for Text Classification. In Proceedings of NIPS.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense per Discourse. In Proceed-
ings of the workshop on Speech and Natural Language.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2009.
Improving Text Categorization Bootstrapping via Unsu-
pervised Learning. ACM Trans. Speech Lang. Process.,
6:1:1?1:24, October.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain Textual Question Answer-
ing Techniques. Natural Language Engineering, 9:231?
267, September.
Thorsten Joachims. 2006. Training Linear SVMs in
Linear Time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD).
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distributional
Similarity for Lexical Inference. Natural Language
Engineering, 16(4):359?389.
Diana McCarthy and Roberto Navigli. 2009. The English
Lexical Substitution Task. Language Resources and
Evaluation, 43(2):139?159.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157, June.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys, 41(2):1?69.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timo-
thy Chklovski, and Eduard Hovy. 2007. ISP: Learning
Inferential Selectional Preferences. In Proceedings of
NAACL-HLT.
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning Selectional Prefer-
ences for Entailment or Paraphrasing Rules. In Pro-
ceedings of RANLP.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Prefer-
ences. In Proceedings of ACL.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting Lexical Reference Rules from Wikipedia. In
Proceedings of IJCNLP-ACL.
Noah A. Smith and Jason Eisner. 2005. Contrastive
Estimation: Training Log-linear Models on Unlabeled
Data. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Gold-
berger. 2008. Contextual Preferences. In Proceedings
of ACL-08: HLT.
29
