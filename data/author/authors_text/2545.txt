Natural Language and Inference in a Computer Game
Malte Gabsdil and Alexander Koller and Kristina Striegnitz
Dept. of Computational Linguistics
Saarland University, Saarbru?cken, Germany
{gabsdil|koller|kris}@coli.uni-sb.de
Abstract
We present an engine for text adventures ? computer
games with which the player interacts using natu-
ral language. The system employs current meth-
ods from computational linguistics and an efficient
inference system for description logic to make the
interaction more natural. The inference system is
especially useful in the linguistic modules dealing
with reference resolution and generation and we
show how we use it to rank different readings in
the case of referential and syntactic ambiguities. It
turns out that the player?s utterances are naturally
restricted in the game scenario, which simplifies the
language processing task.
1 Introduction
Text adventures are computer games with which
the player interacts via a natural language dialogue.
Texts describe the game world and how it evolves,
and the player can manipulate objects in this game
world by typing in commands; Fig. 1 shows a sam-
ple interaction. Text adventures were very popu-
lar and commercially successful in the eighties, but
have gone out of fashion since then ? mostly be-
cause the parsers were rather limited and forced the
user into very restricted forms of interaction.
We describe an engine for text adventures that
attempts to overcome these limitations by using
current methods from computational linguistics for
processing the natural language input and output,
and a state-of-the-art inference system based on de-
scription logic (DL) to represent the dynamic state
of the game world and what the player knows about
it. The DL prover is used in all language-processing
modules except for parsing and surface realization,
and supports the inferences we need very well.
This shows in particular in the modules for the
resolution and generation of referring expressions.
By keeping track of the true state of the world
and the player?s knowledge in separate knowledge
bases, we can evaluate definite descriptions with re-
spect to what the player knows. In generation, such
inferences allow us to produce smaller while still
sufficiently informative references.
Another interesting aspect which we discuss in
this paper is the treatment of syntactic and referen-
tial ambiguities that come up in understanding input
sentences. Here, too, the player knowledge restricts
the way in which the input should be interpreted and
guides the resolution process. We use inferences
about the player knowledge to rule out inconsistent
analyses and pragmatic heuristics to possibly select
the preferred one.
Players of a text adventure are effectively situ-
ated in a game world and have to accomplish a
specific task, which severely restricts the utterances
they will naturally produce. For example, they will
typically only refer to objects they could ?see? in
the simulated world. This simplifies the language
processing tasks tremendously. The scenario of the
game can be freely specified and tailored to different
applications and levels of complexity. Along with
the modularity of our implementation, this makes
our system an interesting testbed for evaluating
more sophisticated NLP modules, which also have
access to the inference infrastructure. The same rea-
son makes it useful as a teaching tool; in fact, our
implementation was created as an advanced project
for students.
Plan of the paper. We will start with an overview
over the architecture of our system in Section 2.
Section 3 will give a brief introduction to descrip-
tion logics, and then describes the knowledge bases
we use. We will discuss how they interact with the
linguistic modules in Section 4, which focuses on
the resolution and generation of referring expres-
sions. Section 5 deals with the resolution of am-
biguities in the player input. Finally, Section 6 con-
cludes and points to further research.
2 Architecture
The general architecture of the game engine, shown
in Fig. 2, consists of a number of language-
processing modules (drawn as ellipses), which in-
Observation Lounge
This is where the station staff and visitors come to relax. There are a lot of tables
and chairs here, a large observation window, and a plush carpet. In the corner you can
see an AstroCola dispenser. A tube leads up to the station?s main corridor.
> put my galakmid coin into the dispenser
Click.
The dispenser display now reads "Credit = 1.00".
> push diet astrocola button
You hear a rumbling noise in the dispenser, but nothing appears in the tray.
> kick dispenser
A can drops into the tray. Amazing! The oldest trick in the book, and it
actually worked.
Figure 1: An example interaction with a text adventure, taken from (Ledgard, 1999).
Content
Determination
Model
Discourse
Reference
Resolution
Reference
Generation
RealizationParsing
Actions
A-Box: User Knowledge
T-Box
A-Box: World Model
Figure 2: The architecture.
terface with knowledge bases and a discourse model
(drawn as rectangles). There are two separate
knowledge bases, which share a set of common def-
initions: One represents the true state of the world
in a world model, the other keeps track of what the
player knows about the world. Solid arrows indi-
cate the general flow of information, dashed arrows
indicate access to the knowledge bases.
The user?s input is first parsed using an efficient
parser for dependency grammar (Duchier and De-
busmann, 2001). Next, referring expressions are re-
solved to individuals in the game world. The result
is a ground term or a sequence of ground terms that
indicates the action(s) the user wants to take. The
Actions module looks up these actions in a database
(where they are specified in a STRIPS-like format),
checks whether the action?s preconditions are met in
the world, and, if yes, updates the world state with
the effects of the action.
The action can also specify effects on the user?s
knowledge. This information is further enriched
by the Content Determination module; for example,
this module computes detailed descriptions of ob-
jects the player wants to look at. The Reference
Generation module translates the internal names
of individuals into descriptions that can be verbal-
ized. In the last step, an efficient realization mod-
ule (Koller and Striegnitz, 2002) builds the output
sentences according to a TAG grammar. The player
knowledge is updated after Reference Generation
when the content of the game?s response, including
the new information carried e.g. by indefinite NPs,
is fully established.
If an error occurs at any stage, e.g. because a pre-
condition of the action fails, an error message spec-
ifying the reasons for the failure is generated by
using the normal generation track (Content Deter-
mination, Reference Generation, Realization) of the
game.
The system is implemented in the programming
language Mozart (Mozart Consortium, 1999) and
provides an interface to the DL reasoning system
RACER (Haarslev and Mo?ller, 2001), which is used
for mainting and accessing the knowledge bases.
3 The World Model
Now we will look at the way that the state of the
world is represented in the game, which will be
important in the language processing modules de-
scribed in Sections 4 and 5. We will first give a short
overview of description logic (DL) and the theorem
prover we use and then discuss some aspects of the
world model in more detail.
3.1 Description Logic
Description logic (DL) is a family of logics in the
tradition of knowledge representation formalisms
such as KL-ONE (Woods and Schmolze, 1992). DL
is a fragment of first-order logic which only allows
unary and binary predicates (concepts and roles)
and only very restricted quantification. A knowl-
edge base consists of a T-Box, which contains ax-
ioms relating the concepts and roles, and one or
more A-Boxes, which state that individuals belong
to certain concepts, or are related by certain roles.
Theorem provers for description logics support
a range of different reasoning tasks. Among the
most common are consistency checking, subsump-
tion checking, and instance and relation check-
ing. Consistency checks decide whether a combina-
tion of T-Box and A-Box can be satisfied by some
model, subsumption is to decide of two concepts
whether all individuals that belong to one concept
must necessarily belong to another, and instance and
relation checking test whether an individual belongs
to a certain concept and whether a certain relation
holds between a pair of individuals, respectively. In
addition to these basic reasoning tasks, description
logic systems usually also provide some retrieval
functionality which e.g. allows to compute all con-
cepts that a given individual belongs to or all indi-
viduals that belong to a given concept.
There is a wide range of different description log-
ics today which add different extensions to a com-
mon core. Of course, the more expressive these ex-
tensions become, the more complex the reasoning
problems are. ?Traditional? DL systems have con-
centrated on very weak logics with simple reasoning
tasks. In the last few years, however, new systems
such as FaCT (Horrocks et al, 1999) and RACER
(Haarslev and Mo?ller, 2001) have shown that it is
possible to achieve surprisingly good average-case
performance for very expressive (but still decidable)
logics. In this paper, we employ the RACER sys-
tem, mainly because it allows for A-Box inferences.
3.2 The World Model
The T-Box we use in the game specifies the con-
cepts and roles in the world and defines some useful
complex concepts, e.g. the concept of all objects the
player can see. This T-Box is shared by two differ-
ent A-Boxes representing the state of the world and
what the player knows about it respectively.
The player A-Box will typically be a sub-part of
the game A-Box because the player will not have
explored the world completely and will therefore
not have encountered all individuals or know about
all of their properties. Sometimes, however, it may
also be useful to deliberately hide effects of an ac-
tion from the user, e.g. if pushing a button has an
effect in a room that the player cannot see. In this
case, the player A-Box can contain information that
is inconsistent with the world A-Box.
A fragment of the A-Box describing the state of
the world is shown in Fig. 3; Fig. 4 gives a graphical
representation. The T-Box specifies that the world
is partitioned into three parts: rooms, objects, and
players. The individual ?myself? is the only instance
that we ever define of the concept ?player?. Indi-
viduals are connected to their locations (i.e. rooms,
container objects, or players) via the ?has-location?
role; the A-Box also specifies what kind of object
an individual is (e.g. ?apple?) and what properties it
has (?red?). The T-Box then contains axioms such
as ?apple  object?, ?red  colour?, etc., which es-
tablish a taxonomy among concepts.
These definitions allow us to add axioms to the
T-Box which define more complex concepts. One
is the concept ?here?, which contains the room in
which the player currently is ? that is, every indi-
vidual which can be reached over a ?has-location?
role from a player object.
here .= ?has-location?1.player
In this definition, ?has-location?1? is the inverse role
of the role ?has-location?, i.e. it links a and b iff
?has-location? links b and a. Inverse roles are one of
the constructions available in more expressive de-
scription logics. The quantification builds a more
complex concept from a concept and a role: ?R.C
is the concept containing all individuals which are
linked via an R role to some individual in C . In the
example in Fig. 3, ?here? denotes the singleton set
{kitchen}.
Another useful concept is ?accessible?, which
contains all individuals which the player can ma-
nipulate.
accessible .= ?has-location.here unionsq
?has-location.(accessible  open)
All objects in the same room as the player are
accessible; if such an object is an open container,
its contents are also accessible. The T-Box con-
tains axioms that express that some concepts (e.g.
?table?, ?bowl?, and ?player?) contain only ?open?
room(kitchen) player(myself)
table(t1) apple(a1)
apple(a2) worm(w1)
red(a1) green(a2)
bowl(b1) bowl(b2)
has-location(t1, kitchen) has-location(b1, t1)
has-location(b2, kitchen) has-location(a1, b2)
has-location(a2, kitchen) has-detail(a2,w1)
has-location(myself, kitchen) . . .
Figure 3: A fragment of a world A-Box.
objects. This permits access to the player?s inven-
tory. In the simple scenario above, ?accessible? de-
notes the set {myself, t1, a1, a2, b1, b2}. Finally,
we can define the concept ?visible? in a similar way
as ?accessible?. The definition is a bit more com-
plex, including more individuals, and is intended to
denote all individuals that the player can ?see? from
his position in the game world.1
4 Referring Expressions
The interaction between the game and the player re-
volves around performing actions on objects in the
game world and the effects that these actions have
on the objects. This means that the resolution and
generation of referring expressions, which identify
those objects to the user, are central tasks in our ap-
plication.
Our implementation illustrates how useful the
availability of an inference system as provided by
RACER to access the world model is, once such an
infrastructure is available. The inference engine is
complemented by a simple discourse model, which
keeps track of available referents.
4.1 The Discourse Model
Our discourse model (DM) is based on Strube?s
(1998) salience list approach, due to its simplic-
ity. The DM is a data structure that stores an or-
dered list of the most salient discourse entities ac-
cording to their ?information status? and text po-
sition and provides methods for retrieving and in-
serting elements. Following Strube, hearer-old dis-
course entities (which include definites) are ranked
1Remember that ?seeing? in our application does not in-
volve any graphical representations. The player acquires
knowledges about the world only through the textual output
generated by the game engine. This allows us to simplify the
DL modeling of the world because we don?t have to specify
all (e.g. spatial) relations that would implicitly be present in a
picture.
Figure 4: Example Scenario
higher in the DM (i.e. are more available for refer-
ence) than hearer-new discourse entities (including
indefinites). Within these categories, elements are
sorted according to their position in the currently
processed sentence. For example, the ranking of
discourse entities for the sentence take a banana,
the red apple, and the green apple would look as
follows:
[red apple ? green apple]old ? [banana]new
The DM is built incrementally and updated af-
ter each input sentence. Updating removes all dis-
course entities from the DM which are not realized
in the current utterance. That is, there is an assump-
tion that referents mentioned in the previous utter-
ance are much more salient than older ones.
4.2 Resolving Referring Expressions
The task of the resolution module is to map def-
inite and indefinite noun phrases and pronouns to
individuals in the world. This task is simplified in
the adventure setting by the fact that the commu-
nication is situated in a sense: Players will typi-
cally only refer to objects which they can ?see? in
the virtual environment, as modeled by the concept
?visible? above. Furthermore, they should not re-
fer to objects they haven?t seen yet. Hence, we
perform all RACER queries in this section on the
player knowledge A-Box, avoiding unintended am-
biguities when the player?s expression would e.g.
not refer uniquely with respect to the true state of
the world.
The resolution of a definite description means to
find a unique entity which, according to the player?s
knowledge, is visible and matches the description.
To compute such an entity, we construct a DL con-
cept expression corresponding to the description
and then send a query to RACER asking for all in-
stances of this concept. In the case of the apple,
for instance, we would retrieve all instances of the
concept
apple  visible
from the player A-Box. The query concept for the
apple with the worm would be
apple  (?has-detail.worm)  visible.
If this yields only one entity ({a2} for the apple with
the worm for the A-Box in Fig. 3), the reference
has been unambiguous and we are done. It may,
however, also be the case that more than one entity
is returned; e.g. the query for the apple would return
the set {a1,a2}. We will show in the next section
how we deal with this kind of ambiguity. We reject
input sentences with an error message indicating a
failed reference if we cannot resolve an expression
at all, i.e. when no object in the player knowledge
matches the description.
We resolve indefinite NPs, such as an apple, by
querying the player knowledge in the same way as
described above for definites. Unlike in the definite
case, however, we do not require unique reference.
Instead, we assume that the player did not have a
particular object in mind and arbitrarily choose one
of the possible referents. The reply of the game will
automatically inform the player which one was cho-
sen, as a unique definite reference will be generated
(see below).
Pronouns are simply resolved to the most salient
entity in the DM that matches their agreement con-
straints. The restrictions our grammar imposes
on the player input (no embeddings, no reflexive
pronouns) allow us to analyze sentences including
intra-sentential anaphora like take the apple and eat
it. The incremental construction of the DM ensures
that by the time we encounter the pronoun it, the
apple has already been processed and can serve as a
possible antecedent.
4.3 Generating Referring Expressions
The converse task occurs when we generate the
feedback to show to the player: It is necessary to
construct descriptions of individuals in the game
world that enable the player to identify these.
This task is quite simple for objects which are
new to the player. In this case, we generate an indef-
inite NP containing the type and (if it has one) color
of the object, as in the bowl contains a red apple.
We use RACER?s retrieval functionality to extract
this information from the knowledge base.
To refer to an object that the player already has
encountered, we try to construct a definite descrip-
tion that, given the player knowledge, uniquely
identifies this object. For this purpose we use a vari-
ant of Dale and Reiter?s (1995) incremental algo-
rithm, extended to deal with relations between ob-
jects (Dale and Haddock, 1991). The properties of
the target referent are looked at in some predefined
order (e.g. first its type, then its color, its location,
parts it may have, . . .). A property is added to the
description if at least one other object (a distrac-
tor) is excluded from it because it doesn?t share this
property. This is done until the description uniquely
identifies the target referent.
The algorithm uses RACER?s reasoning and re-
trieval functionality to access the relevant informa-
tion about the context, which included e.g. comput-
ing the properties of the target referent and find-
ing the distracting instances. Assuming we want to
refer to entity a1 in the A-Box in Fig. 3 e.g., we
first have to retrieve all concepts and roles of a1
from the player A-Box. This gives us {apple(a1),
red(a1), has-location(a1,b1)}. As we have to have at
least one property specifying the type of a1, we use
RACER?s subsumption checks to extract all those
properties that match this requirement; in this case,
?apple?. Then we retrieve all instances of the con-
cept ?apple? to determine the set of distractors which
is {a1, a2}. Hence, ?apple? alone is not enough to
uniquely identify a1. So, we consider the apple?s
color. Again using subsumption checks, we filter
the colors from the properties of a1 (i.e. ?red?) and
then retrieve all instances belonging to the concept
apple red to check whether and how the set of dis-
tractors gets reduced by adding this property. This
concept has only one member in the example, so we
generate the expression the red apple.
5 Ambiguity Resolution
The other aspect of the game engine which we want
to highlight here is how we deal with referential
and syntactic ambiguity. We handle the former by
a combination of inference and discourse informa-
tion, and the latter by taking psycholinguistically
motivated preferences into account.
5.1 Resolving Referential Ambiguities
When the techniques for reference resolution de-
scribed in the previous section are not able to map
a definite description to a single entity in the player
knowledge, the resolution module returns a set of
possible referents. We then try to narrow this set
down in two steps.
First, we filter out individuals which are com-
pletely unsalient according to the discourse model.
In our (simplified) model, these are all individuals
that haven?t been mentioned in the previous sen-
tence. This heuristic permits the game to deal with
the following dialogue, as the red but not the green
apple is still accessible in the final turn, and is there-
fore chosen as the patient of the ?eat? action.
Game: . . . red apple . . . green apple.
Player: Take the red apple.
Game: You have the red apple.
Player: Eat the apple.
Game: You eat the red apple.
If this narrows down the possible referents to just
one, we are done. Otherwise ? i.e. if several or none
of the referents were mentioned in the previous sen-
tence ?, we check whether the player?s knowledge
rules out some of them. The rationale is that an in-
telligent player would not try to perform an action
on an object on which she knows it cannot be per-
formed.
Assume, by way of example, that the player
knows about the worm in the green apple. This
violates a precondition of the ?eat? action for ap-
ples. Thus if both apples were equally salient, we
would read eat the apple as eat the red apple. We
can test if a combination of referents for the various
referring expressions of a sentence violates precon-
ditions by first instantiating the appropriate action
with these referents. Then we independently add
each instantiated precondition to fresh copies of the
player knowledge A-Box and test them for consis-
tency. If one of the A-Boxes becomes inconsistent,
we conclude that the player knows this precondition
would fail, and conclude that this is not the intended
combination of referents.
If neither of these heuristics manages to pick out
a unique entity, we consider the definite description
to be truly ambiguous and return an error message
to the user, indicating the ambiguity.
5.2 Resolving Syntactic Ambiguities
Another class of ambiguities which we consider are
syntactic ambiguities, especially of PP attachment.
We try to resolve them, too, by taking referential
information into account.
In the simplest case, the referring expressions in
some of the syntactic readings have no possible ref-
erent in the player A-Box at all. If this happens, we
filter these readings out and only continue with the
others (Schuler, 2001). For example, the sentence
unlock the toolbox with the key is ambiguous. In a
scenario where there is a toolbox and a key, but the
key is not attached to the toolbox, resolution fails for
one of the analyses and thereby resolves the syntac-
tic ambiguity.
If more than one syntactic reading survives this
first test, we perform the same computations as
above to filter out possible referents which are either
unsalient or violate the player?s knowledge. Some-
times, only one syntactic reading will have a refer-
ent in this narrower sense; in this case, we are done.
Otherwise, i.e. if more than one syntactic reading
has referents, we remove those readings which are
referentially ambiguous. Consider once more the
example scenario depicted in Fig. 4. The sentence
put the apple in the bowl on the table has two differ-
ent syntactic analyses: In the first, the bowl on the
table is the target of the put action whereas in the
second, in the bowl modifies the apple. Now, note
that in the first reading, we will get two possible ref-
erents for the apple, whereas in the second reading
the apple in the bowl is unique. In cases like this we
pick out the reading which only includes unique ref-
erences (reading 2 in the present example). This ap-
proach assumes that the players are cooperative and
try to refer unambiguously. It is furthermore similar
to what people seem to do. Psycholinguistic eye-
tracking studies (Chambers et al, 2000) indicate
that people prefer interpretations with unambiguous
references: subjects who are faced with scenarios
similar to Fig. 4 and hear the sentence put the ap-
ple in the bowl on the table do not look at the bowl
on the table at all but only at the apple in the bowl
(which is unique) and the table.
At this point, there can still be more than one syn-
tactic reading left; if so, all of these will have unam-
biguous, unique referents. In such a case we cannot
decide which syntactic reading the player meant,
and ask the player to give the game a less ambiguous
command.
6 Conclusion and Outlook
We have described an engine for text adventures
which uses techniques from computational linguis-
tics to make the interaction with the game more nat-
ural. The input is analyzed using a dependency
parser and a simple reference resolution module,
and the output is produced by a small generation
system. Information about the world and about
the player?s knowledge is represented in descrip-
tion logic knowledge bases, and accessed through
a state-of-the-art inference system. Most modules
use the inference component; to illustrate its useful-
ness, we have looked more closely at the resolution
and generation of referring expressions, and at the
resolution of referential and syntactic ambiguities.
Preliminary experiments indicate that the perfor-
mance of our game engine is good enough for flu-
ent gameplay. The constraint based dependency
parser we use for parsing and generation achieves
very good average case runtimes on the grammars
and inputs we use. More interestingly, the infer-
ence system also performs very well. With the cur-
rent knowledge bases, reasoning on the world model
and user knowledge takes 546ms per turn on aver-
age (with a mean of 39 queries per turn). How well
this performance scales to bigger game worlds re-
mains to be seen. One lesson we take from this is
that the recent progress in optimizing inference en-
gines for expressive description logics is beginning
to make them useful for applications.
All the language-processing modules in our sys-
tem are rather simplistic. We can get away with this
because the utterances that players seem to want to
produce in this setting are restricted, e.g. to objects
in the same simulated ?location? as the player. (The
precise extent of this, of course, remains to be eval-
uated.) The result is a system which exceeds tradi-
tional text adventures by far in the flexibility offered
to the user.
Unlike the input, the output that our game gen-
erates is far away from the quality of the com-
mercial text adventures of the eighties, which pro-
duced canned texts, sometimes written by profes-
sional book authors. A possible solution could be to
combine the full generation with a template based
approach, to which the TAG-based generation ap-
proach we take lends itself well. Another problem is
the generation of error messages asking the user to
resolve an ambiguous input. The game should ide-
ally generate and present the player with a choice
of possible (unambiguous) readings. So, the gen-
eration strategy would have to be augmented with
some kind of monitoring, such as the one proposed
by Neumann and van Noord (1994). Finally, we
want to come up with a way of synchronizing the
grammars for parsing and generation, in order to en-
sure that expressions used by the game can always
be used by the player as well.
The system is designed in a way that should make
it reasonably easy to replace our simple modules
by more sophisticated ones. We will shortly make
our adventure engine available over the web, and
want to invite colleagues and students to test their
own language processing modules within our sys-
tem. Generally, we believe that the prototype can
serve as a starting point for an almost unlimited
range of extensions.
References
C.G. Chambers, M.K. Tanenhaus, and J.S. Magnu-
son. 2000. Does real-world knowledge modulate
referential effects on PP-attachment? Evidence
from eye movements in spoken language compre-
hension. In 14th CUNY Conference on Human
Sentence Processing.
R. Dale and N. Haddock. 1991. Generating re-
ferring expressions involving relations. In EACL
?91.
R. Dale and E. Reiter. 1995. Computational inter-
pretations of the gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
18.
D. Duchier and R. Debusmann. 2001. Topological
dependency trees: A constraint-based account of
linear precedence. In ACL ?01.
V. Haarslev and R. Mo?ller. 2001. RACER System
Description. In IJCAR ?01.
I. Horrocks, U. Sattler, and S. Tobies. 1999. Practi-
cal reasoning for expressive description logics. In
H. Ganzinger, D. McAllester, and A. Voronkov,
editors, LPAR?99.
A. Koller and K. Striegnitz. 2002. Generation as
dependency parsing. In ACL ?02.
D. Ledgard. 1999. Space Station. Text adventure,
modelled after a sample transcript of Infocom?s
Planetfall game. http://members.tripod.
com/?infoscripts/planetfa.htm.
Mozart Consortium. 1999. The Mozart Pro-
gramming System web pages. http://www.
mozart-oz.org/.
G. Neumann and G.-J. van Noord. 1994.
Self-monitoring with reversible grammars. In
T. Strzalkowski, editor, Reversible Grammar in
Natural Language Processing.
W. Schuler. 2001. Computational properties of
environment-based disambiguation. In ACL ?01.
M. Strube. 1998. Never Look Back: An Alternative
to Centering. In COLING-ACL ?98.
W. Woods and J. Schmolze. 1992. The KL-ONE
Family. Computer and Mathematics with Appli-
cations, 23(2?5).
Combining Acoustic and Pragmatic Features to Predict Recognition
Performance in Spoken Dialogue Systems
Malte Gabsdil
Department of Computational Linguistics
Saarland University
Germany
gabsdil@coli.uni-sb.de
Oliver Lemon
School of Informatics
Edinburgh University
Scotland
olemon@inf.ed.ac.uk
Abstract
We use machine learners trained on a combina-
tion of acoustic confidence and pragmatic plausi-
bility features computed from dialogue context to
predict the accuracy of incoming n-best recogni-
tion hypotheses to a spoken dialogue system. Our
best results show a 25% weighted f-score improve-
ment over a baseline system that implements a
?grammar-switching? approach to context-sensitive
speech recognition.
1 Introduction
A crucial problem in the design of spoken dialogue
systems is to decide for incoming recognition hy-
potheses whether a system should accept (consider
correctly recognized), reject (assume misrecogni-
tion), or ignore (classify as noise or speech not di-
rected to the system) them. In addition, a more so-
phisticated dialogue system might decide whether
to clarify or confirm certain hypotheses.
Obviously, incorrect decisions at this point can
have serious negative effects on system usability
and user satisfaction. On the one hand, accepting
misrecognized hypotheses leads to misunderstand-
ings and unintended system behaviors which are
usually difficult to recover from. On the other hand,
users might get frustrated with a system that be-
haves too cautiously and rejects or ignores too many
utterances. Thus an important feature in dialogue
system engineering is the tradeoff between avoiding
task failure (due to misrecognitions) and promoting
overall dialogue efficiency, flow, and naturalness.
In this paper, we investigate the use of machine
learners trained on a combination of acoustic confi-
dence and pragmatic plausibility features (i.e. com-
puted from dialogue context) to predict the qual-
ity of incoming n-best recognition hypotheses to
a spoken dialogue system. These predictions are
then used to select a ?best? hypothesis and to de-
cide on appropriate system reactions. We evalu-
ate this approach in comparison with a baseline
system that combines fixed recognition confidence
rejection thresholds with dialogue-state dependent
recognition grammars (Lemon, 2004).
The paper is organized as follows. After a short
relation to previous work, Section 3 introduces the
WITAS multimodal dialogue system, which we use
to collect data (Section 4) and to derive baseline re-
sults (Section 5). Section 6 describes our learning
experiments for classifying and selecting from n-
best recognition hypotheses and Section 7 reports
our results.
2 Relation to Previous Work
(Litman et al, 2000) use acoustic-prosodic infor-
mation extracted from speech waveforms, together
with information derived from their speech recog-
nizer, to automatically predict misrecognized turns
in a corpus of train-timetable information dialogues.
In our experiments, we also use recognizer con-
fidence scores and a limited number of acoustic-
prosodic features (e.g. amplitude in the speech sig-
nal) for hypothesis classification. (Walker et al,
2000) use a combination of features from the speech
recognizer, natural language understanding, and di-
alogue manager/discourse history to classify hy-
potheses as correct, partially correct, or misrecog-
nized. Our work is related to these experiments in
that we also combine confidence scores and higher-
level features for classification. However, both (Lit-
man et al, 2000) and (Walker et al, 2000) con-
sider only single-best recognition results and thus
use their classifiers as ?filters? to decide whether the
best recognition hypothesis for a user utterance is
correct or not. We go a step further in that we clas-
sify n-best hypotheses and then select among the al-
ternatives. We also explore the use of more dialogue
and task-oriented features (e.g. the dialogue move
type of a recognition hypothesis) for classification.
The main difference between our approach and
work on hypothesis reordering (e.g. (Chotimongkol
and Rudnicky, 2001)) is that we make a decision re-
garding whether a dialogue system should accept,
clarify, reject, or ignore a user utterance. Fur-
thermore, our approach is more generally applica-
ble than preceding research, since we frame our
methodology in the Information State Update (ISU)
approach to dialogue management (Traum et al,
1999) and therefore expect it to be applicable to a
range of related multimodal dialogue systems.
3 The WITAS Dialogue System
The WITAS dialogue system (Lemon et al, 2002)
is a multimodal command and control dialogue sys-
tem that allows a human operator to interact with
a simulated ?unmanned aerial vehicle? (UAV): a
small robotic helicopter. The human operator is pro-
vided with a GUI ? an interactive (i.e. mouse click-
able) map ? and specifies mission goals using nat-
ural language commands spoken into a headset, or
by using combinations of GUI actions and spoken
commands. The simulated UAV can carry out dif-
ferent activities such as flying to locations, follow-
ing vehicles, and delivering objects. The dialogue
system uses the Nuance 8.0 speech recognizer with
language models compiled from a grammar (written
using the Gemini system (Dowding et al, 1993)),
which is also used for parsing and generation.
3.1 WITAS Information States
The WITAS dialogue system is part of a larger
family of systems that implement the Information
State Update (ISU) approach to dialogue manage-
ment (Traum et al, 1999). The ISU approach has
been used to formalize different theories of dia-
logue and forms the basis of several dialogue sys-
tem implementations in domains such as route plan-
ning, home automation, and tutorial dialogue. The
ISU approach is a particularly useful testbed for
our technique because it collects information rele-
vant to dialogue context in a central data structure
from which it can be easily extracted. (Lemon et al,
2002) describe in detail the components of Informa-
tion States (IS) and the update procedures for pro-
cessing user input and generating system responses.
Here, we briefly introduce parts of the IS which are
needed to understand the system?s basic workings,
and from which we will extract dialogue-level and
task-level information for our learning experiments:
? Dialogue Move Tree (DMT): a tree-structure,
in which each subtree of the root node repre-
sents a ?thread? in the conversation, and where
each node in a subtree represents an utterance
made either by the system or the user. 1
? Active Node List (ANL): a list that records all
?active? nodes in the DMT; active nodes indi-
1A tree is used in order to overcome the limitations of stack-
based processing, see (Lemon and Gruenstein, 2004).
cate conversational contributions that are still
in some sense open, and to which new utter-
ances can attach.
? Activity Tree (AT): a tree-structure represent-
ing the current, past, and planned activities that
the back-end system (in this case a UAV) per-
forms.
? Salience List (SL): a list of NPs introduced in
the current dialogue ordered by recency.
? Modality Buffer (MB): a temporary store that
registers click events on the GUI.
The DMT and AT are the core components of In-
formation States. The SL and MB are subsidiary
data-structures needed for interpreting and generat-
ing anaphoric expressions and definite NPs. Finally,
the ANL plays a crucial role in integrating new user
utterances into the DMT.
4 Data Collection
For our experiments, we use data collected in a
small user study with the grammar-switching ver-
sion of the WITAS dialogue system (Lemon, 2004).
In this study, six subjects from Edinburgh Univer-
sity (4 male, 2 female) had to solve five simple tasks
with the system, resulting in 30 complete dialogues.
The subjects? utterances were recorded as 8kHz
16bit waveform files and all aspects of the Informa-
tion State transitions during the interactions were
logged as html files. Altogether, 303 utterances
were recorded in the user study (? 10 user utter-
ances/dialogue).
4.1 Labeling
We transcribed all user utterances and parsed the
transcriptions offline using WITAS? natural lan-
guage understanding component in order to get a
gold-standard labeling of the data. Each utter-
ance was labeled as either in-grammar or out-of-
grammar (oog), depending on whether its transcrip-
tion could be parsed or not, or as crosstalk: a spe-
cial marker that indicated that the input was not di-
rected to the system (e.g. noise, laughter, self-talk,
the system accidentally recording itself). For all
in-grammar utterances we stored their interpreta-
tions (quasi-logical forms) as computed by WITAS?
parser. Since the parser uses a domain-specific se-
mantic grammar designed for this particular appli-
cation, each in-grammar utterance had an interpre-
tation that is ?correct? with respect to the WITAS
application.
4.2 Simplifying Assumptions
The evaluations in the following sections make two
simplifying assumptions. First, we consider a user
utterance correctly recognized only if the logical
form of the transcription is the same as the logical
form of the recognition hypothesis. This assump-
tion can be too strong because the system might re-
act appropriately even if the logical forms are not
literally the same. Second, if a transcribed utter-
ance is out-of-grammar, we assume that the system
cannot react appropriately. Again, this assumption
might be too strong because the recognizer can ac-
cidentally map an utterance to a logical form that is
equivalent to the one intended by the user.
5 The Baseline System
The baseline for our experiments is the behavior of
the WITAS dialogue system that was used to col-
lect the experimental data (using dialogue context
as a predictor of language models for speech recog-
nition, see below). We chose this baseline because it
has been shown to perform significantly better than
an earlier version of the system that always used the
same (i.e. full) grammar for recognition (Lemon,
2004).
We evaluate the performance of the baseline by
analyzing the dialogue logs from the user study.
With this information, it is possible to decide how
the system reacted to each user utterance. We dis-
tinguish between the following three cases:
1. accept: the system accepted the recognition
hypothesis of a user utterance as correct.
2. reject: the system rejected the recognition hy-
pothesis of a user utterance given a fixed con-
fidence rejection threshold.
3. ignore: the system did not react to a user utter-
ance at all.
These three classes map naturally to the gold-
standard labels of the transcribed user utterances:
the system should accept in-grammar utterances, re-
ject out-of-grammar input, and ignore crosstalk.
5.1 Context-sensitive Speech Recognition
In the the WITAS dialogue system, the ?grammar-
switching? approach to context-sensitive speech
recognition (Lemon, 2004) is implemented using
the ANL. At any point in the dialogue, there is a
?most active node? at the top of the ANL. The dia-
logue move type of this node defines the name of a
language model that is used for recognizing the next
user utterance. For instance, if the most active node
is a system yes-no-question then the appropriate
language model is defined by a small context-free
grammar covering phrases such as ?yes?, ?that?s
right?, ?okay?, ?negative?, ?maybe?, and so on.
The WITAS dialogue system with context-
sensitive speech recognition showed significantly
better recognition rates than a previous version of
the system that used the full grammar for recogni-
tion at all times ((Lemon, 2004) reports a 11.5%
reduction in overall utterance recognition error
rate). Note however that an inherent danger with
grammar-switching is that the system may have
wrong expectations and thus might activate a lan-
guage model which is not appropriate for the user?s
next utterance, leading to misrecognitions or incor-
rect rejections.
5.2 Results
Table 1 summarizes the evaluation of the baseline
system.
System behavior
User utterance accept reject ignore
in-grammar 154/22 8 4
out-of-grammar 45 43 4
crosstalk 12 9 2
Accuracy: 65.68%
Weighted f-score: 61.81%
Table 1: WITAS dialogue system baseline results
Table 1 should be read as follows: looking at the
first row, in 154 cases the system understood and
accepted the correct logical form of an in-grammar
utterance by the user. In 22 cases, the system ac-
cepted a logical form that differed from the one for
the transcribed utterance.2 In 8 cases, the system re-
jected an in-grammar utterance and in 4 cases it did
not react to an in-grammar utterance at all. The sec-
ond row of Table 1 shows that the system accepted
45, rejected 43, and ignored 4 user utterances whose
transcriptions were out-of-grammar and could not
be parsed. Finally, the third row of the table shows
that the baseline system accepted 12 utterances that
were not addressed to it, rejected 9, and ignored 2.
Table 1 shows that a major problem with the base-
line system is that it accepts too many user utter-
ances. In particular, the baseline system accepts the
wrong interpretation for 22 in-grammar utterances,
45 utterances which it should have rejected as out-
of-grammar, and 12 utterances which it should have
2For the computation of accuracy and weighted f-scores,
these were counted as wrongly accepted out-of-grammar ut-
terances.
ignored. All of these cases will generally lead to
unintended actions by the system.
6 Classifying and Selecting N-best
Recognition Hypotheses
We aim at improving over the baseline results by
considering the n-best recognition hypotheses for
each user utterance. Our methodology consists of
two steps: i) we automatically classify the n-best
recognition hypotheses for an utterance as either
correctly or incorrectly recognized and ii) we use a
simple selection procedure to choose the ?best? hy-
pothesis based on this classification. In order to get
multiple recognition hypotheses for all utterances
in the experimental data, we re-ran the speech rec-
ognizer with the full recognition grammar and 10-
best output and processed the results offline with
WITAS? parser, obtaining a logical form for each
recognition hypothesis (every hypothesis has a log-
ical form since language models are compiled from
the parsing grammar).
6.1 Hypothesis Labeling
We labeled all hypotheses with one of the follow-
ing four classes, based on the manual transcriptions
of the experimental data: in-grammar, oog (WER ?
50), oog (WER > 50), or crosstalk. The in-grammar
and crosstalk classes correspond to those described
for the baseline. However, we decided to divide up
the out-of-grammar class into the two classes oog
(WER? 50) and oog (WER > 50) to get a more fine-
grained classification. In order to assign hypotheses
to the two oog classes, we compute the word er-
ror rate (WER) between recognition hypotheses and
the transcription of corresponding user utterances.
If the WER is ? 50%, we label the hypothesis as
oog (WER ? 50), otherwise as oog (WER > 50).
We also annotate all misrecognized hypotheses of
in-grammar utterances with their respective WER
scores.
The motivation behind splitting the out-of-
grammar class into two subclasses and for anno-
tating misrecognized in-grammar hypotheses with
their WER scores is that we want to distinguish be-
tween different ?degrees? of misrecognition that can
be used by the dialogue system to decide whether
it should initiate clarification instead of rejection.3
We use a threshold (50%) on a hypothesis? WER
as an indicator for whether hypotheses should be
3The WITAS dialogue system currently does not support
this type of clarification dialogue; the WER annotations are
therefore only of theoretical interest. However, an extended
system could easily use this information to decide when clari-
fication should be initiated.
clarified or rejected. This is adopted from (Gabs-
dil, 2003), based on the fact that WER correlates
with concept accuracy (CA, (Boros et al, 1996)).
The WER threshold can be set differently according
to the needs of an application. However, one would
ideally set a threshold directly on CA scores for this
labeling, but these are currently not available for our
data.
We also introduce the distinction between out-of-
grammar (WER ? 50) and out-of-grammar (WER
> 50) in the gold standard for the classification
of (whole) user utterances. We split the out-of-
grammar class into two sub-classes depending on
whether the 10-best recognition results include at
least one hypothesis with a WER ? 50 compared
to the corresponding transcription. Thus, if there is
a recognition hypothesis which is close to the tran-
scription, an utterance is labeled as oog (WER ?
50). In order to relate these classes to different sys-
tem behaviors, we define that utterances labeled as
oog (WER ? 50) should be clarified and utterances
labeled as oog (WER > 50) should be rejected by
the system. The same is done for all in-grammar
utterances for which only misrecognized hypothe-
ses are available.
6.2 Classification: Feature Groups
We represent recognition hypotheses as 20-
dimensional feature vectors for automatic classifica-
tion. The feature vectors combine recognizer con-
fidence scores, low-level acoustic information, in-
formation from WITAS system Information States,
and domain knowledge about the different tasks in
the scenario. The following list gives an overview
of all features (described in more detail below).
1. Recognition (6): nbestRank, hypothe-
sisLength, confidence, confidenceZScore,
confidence-StandardDeviation, minWordCon-
fidence
2. Utterance (3): minAmp, meanAmp, RMS-amp
3. Dialogue (9): currentDM, currentCommand,
mostActiveNode, DMBigramFrequency, qa-
Match, aqMatch, #unresolvedNPs, #unre-
solvedPronouns, #uniqueIndefinites
4. Task (2): taskConflict, #taskConstraintCon-
flict
All features are extracted automatically from the
output of the speech recognizer, utterance wave-
forms, IS logs, and a small library of plan operators
describing the actions the UAV can perform. The
recognition (REC) feature group includes the posi-
tion of a hypothesis in the n-best list (nbestRank),
its length in words (hypothesisLength), and five fea-
tures representing the recognizer?s confidence as-
sessment. Similar features have been used in the
literature (e.g. (Litman et al, 2000)). The minWord-
Confidence and standard deviation/zScore features
are computed from individual word confidences in
the recognition output. We expect them to help the
machine learners decide between the different WER
classes (e.g. a high overall confidence score can
sometimes be misleading). The utterance (UTT)
feature group reflects information about the ampli-
tude in the speech signal (all features are extracted
with the UNIX sox utility). The motivation for
including the amplitude features is that they might
be useful for detecting crosstalk utterances which
are not directly spoken into the headset microphone
(e.g. the system accidentally recognizing itself).
The dialogue features (DIAL) represent informa-
tion derived from Information States and can be
coarsely divided into two sub-groups. The first
group includes features representing general co-
herence constraints on the dialogue: the dialogue
move types of the current utterance (currentDM)
and of the most active node in the ANL (mostAc-
tiveNode), the command type of the current utter-
ance (currentCommand, if it is a command, null
otherwise), statistics on which move types typi-
cally follow each other (DMBigramFrequency), and
two features (qaMatch and aqMatch) that explic-
itly encode whether the current and the previous
utterance form a valid question answer pair (e.g.
yn-question followed by yn-answer). The second
group includes features that indicate how many def-
inite NPs and pronouns cannot be resolved in the
current Information State (#unresolvedNP, #unre-
solvedPronouns, e.g. ?the car? if no car was men-
tioned before) and a feature indicating the number
of indefinite NPs that can be uniquely resolved in
the Information State (#uniqueIndefinites, e.g. ?a
tower? where there is only one tower in the do-
main). We include these features because (short)
determiners are often confused by speech recogniz-
ers. In the WITAS scenario, a misrecognized deter-
miner/demonstrative pronoun can lead to confusing
system behavior (e.g. a wrongly recognized ?there?
will cause the system to ask ?Where is that??).
Finally, the task features (TASK) reflect conflict-
ing instructions in the domain. The feature taskCon-
flict indicates a conflict if the current dialogue move
type is a command and that command already ap-
pears as an active task in the AT. #taskConstraint-
Conflict counts the number of conflicts that arise
between the currently active tasks in the AT and the
hypothesis. For example, if the UAV is already fly-
ing somewhere the preconditions of the action op-
erator for take off (altitude = 0) conflict with
those for fly (altitude 6= 0), so that ?take off?
would be an unlikely command in this context.
6.3 Learners and Selection Procedure
We use the memory based learner TiMBL (Daele-
mans et al, 2002) and the rule induction learner
RIPPER (Cohen, 1995) to predict the class of each
of the 10-best recognition hypotheses for a given ut-
terance. We chose these two learners because they
implement different learning strategies, are well es-
tablished, fast, freely available, and easy to use. In a
second step, we decide which (if any) of the classi-
fied hypotheses we actually want to pick as the best
result and how the user utterance should be classi-
fied as a whole. This task is decided by the follow-
ing selection procedure (see Figure 1) which imple-
ments a preference ordering accept > clarify > re-
ject > ignore.4
1. Scan the list of classified n-best recognition
hypotheses top-down. Return the first result
that is classified as accept and classify the
utterance as accept.
2. If 1. fails, scan the list of classified n-best
recognition hypotheses top-down. Return
the first result that is classified as clarify and
classify the utterance as clarify.
3. If 2. fails, count the number of rejects and
ignores in the classified recognition hypothe-
ses. If the number of rejects is larger or equal
than the number of ignores classify the utter-
ance as reject.
4. Else classify the utterance as ignore.
Figure 1: Selection procedure
This procedure is applied to choose from the clas-
sified n-best hypotheses for an utterance, indepen-
dent of the particular machine learner, in all of the
following experiments.
Since we have a limited amount experimental
data in this study (10 hypotheses for each of the 303
user utterances), we use a ?leave-one-out? crossval-
idation setup for classification. This means that we
classify the 10-best hypotheses for a particular ut-
terance based on the 10-best hypotheses of all 302
other utterances and repeat this 303 times.
4Note that in a dialogue application one would not always
need to classify all n-best hypotheses in order to select a result
but could stop as soon as a hypothesis is classified as correct,
which can save processing time.
7 Results and Evaluation
The middle part of Table 2 shows the classifica-
tion results for TiMBL and RIPPER when run with
default parameter settings (the other results are in-
cluded for comparison). The individual rows show
the performance when different combinations of
feature groups are used for training. The results for
the three-way classification are included for com-
parison with the baseline system and are obtained
by combining the two classes clarify and reject.
Note that we do not evaluate the performance of the
learners for classifying the individual recognition
hypotheses but the classification of (whole) user ut-
terances (i.e. including the selection procedure to
choose from the classified hypotheses).
The results show that both learners profit from
the addition of more features concerning dialogue
context and task context for classifying user speech
input appropriately. The only exception from this
trend is a slight performance decrease when task
features are added in the four-way classification for
RIPPER. Note that both learners already outperform
the baseline results even when only recognition fea-
tures are considered. The most striking result is the
performance gain for TiMBL (almost 10%) when
we include the dialogue features. As soon as dia-
logue features are included, TiMBL also performs
slightly better than RIPPER.
Note that the introduction of (limited) task fea-
tures, in addition to the DIAL and UTT features, did
not have dramatic impact in this study. One aim for
future work is to define and analyze the influence of
further task related features for classification.
7.1 Optimizing TiMBL Parameters
In all of the above experiments we ran the machine
learners with their default parameter settings.
However, recent research (Daelemans and Hoste,
2002; Marsi et al, 2003) has shown that machine
learners often profit from parameter optimization
(i.e. finding the best performing parameters on
some development data). We therefore selected
40 possible parameter combinations for TiMBL
(varying the number of nearest neighbors, feature
weighting, and class voting weights) and nested a
parameter optimization step into the ?leave-one-
out? evaluation paradigm (cf. Figure 2).5
Note that our optimization method is not as so-
phisticated as the ?Iterative Deepening? approach
5We only optimized parameters for TiMBL because it per-
formed better with default settings than RIPPER and because
the findings in (Daelemans and Hoste, 2002) indicate that
TiMBL profits more from parameter optimization.
1. Set aside the recognition hypotheses for one
of the user utterances.
2. Randomly split the remaining data into an
80% training and 20% test set.
3. Run TiMBL with all possible parameter set-
tings on the generated training and test sets
and store the best performing settings.
4. Classify the left-out hypotheses with the
recorded parameter settings.
5. Iterate.
Figure 2: Parameter optimization
described by (Marsi et al, 2003) but is similar in the
sense that it computes a best-performing parameter
setting for each data fold.
Table 3 shows the classification results when we
run TiMBL with optimized parameter settings and
using all feature groups for training.
System Behavior
User Utterance accept clarify reject ignore
in-grammar 159/2 11 16 0
out-of-grammar 0 25 5 0(WER ? 50)
out-of-grammar 6 6 50 0(WER > 50)
crosstalk 2 5 0 16
Acc/wf-score (3 classes): 86.14/86.39%
Acc/wf-score (4 classes): 82.51/83.29%
Table 3: TiMBL classification results with opti-
mized parameters
Table 3 shows a remarkable 9% improvement for
the 3-way and 4-way classification in both accuracy
and weighted f-score, compared to using TiMBL
with default parameter settings. In terms of WER,
the baseline system (cf. Table 1) accepted 233 user
utterances with a WER of 21.51%, and in contrast,
TiMBL with optimized parameters (Ti OP) only ac-
cepted 169 user utterances with a WER of 4.05%.
This low WER reflects the fact that if the machine
learning system accepts an user utterance, it is al-
most certainly the correct one. Note that although
the machine learning system in total accepted far
fewer utterances (169 vs. 233) it accepted more cor-
rect utterances than the baseline (159 vs. 154).
7.2 Evaluation
The baseline accuracy for the 3-class problem is
65.68% (61.81% weighted f-score). Our best re-
sults, obtained by using TiMBL with parameter op-
System or features used Acc/wf-score Acc/wf-score Acc/wf-score Acc/wf-score
for classification (3 classes) (4 classes) (3 classes) (4 classes)
Baseline 65.68/61.81%
TiMBL RIPPER
REC 67.66/67.51% 63.04/63.03% 69.31/69.03% 66.67/65.14%
REC+UTT 68.98/68.32% 64.03/63.08% 72.61/72.33% 70.30/68.61%
REC+UTT+DIAL 77.56/77.59% 72.94/73.70% 74.92/75.34% 71.29/71.62%
REC+UTT+DIAL+TASK 77.89/77.91% 73.27/74.12% 75.25/75.61% 70.63/71.54%
TiMBL (optimized params.) 86.14/86.39% 82.51/83.29%
Oracle 94.06/94.17% 94.06/94.18%
Table 2: Classification Results
timization, show a 25% weighted f-score improve-
ment over the baseline system.
We can compare these results to a hypothetical
?oracle? system in order to obtain an upper bound
on classification performance. This is an imagi-
nary system which performs perfectly on the ex-
perimental data given the 10-best recognition out-
put. The oracle results reveal that for 18 of the
in-grammar utterances the 10-best recognition hy-
potheses do not include the correct logical form at
all and therefore have to be classified as clarify or
reject (i.e. it is not possible to achieve 100% accu-
racy on the experimental data). Table 2 shows that
our best results are only 8%/12% (absolute) away
from the optimal performance.
7.2.1 Costs and ?2 Levels of Significance
We use the ?2 test of independence to statistically
compare the different classification results. How-
ever, since ?2 only tells us whether two classifica-
tions are different from each other, we introduce a
simple cost measure (Table 4) for the 3-way classi-
fication problem to complement the ?2 results.6
System behavior
User utterance accept reject ignore
in-grammar 0 2 2
out-of-grammar 4 2 2
crosstalk 4 2 0
Table 4: Cost measure
Table 4 captures the intuition that the correct be-
havior of a dialogue system is to accept correctly
recognized utterances and ignore crosstalk (cost 0).
The worst a system can do is to accept misrec-
ognized utterances or utterances that were not ad-
dressed to the system. The remaining classes are as-
6We only evaluate the 3-way classification problem because
there are no baseline results for the 4-way classification avail-
able.
signed a value in-between these two extremes. Note
that the cost assignment is not validated against user
judgments. We only use the costs to interpret the ?2
levels of significance (i.e. as an indicator to compare
the relative quality of different systems).
Table 5 shows the differences in cost and ?2 lev-
els of significance when we compare the classifica-
tion results. Here, Ti OP stands for TiMBL with op-
timized parameters and the stars indicate the level of
statistical significance as computed by the ?2 statis-
tics (??? indicates significance at p = .001, ?? at
p = .01, and ? at p = .05).7
Baseline RIPPER TiMBL Ti OP
Oracle ?232??? ?116??? ?100??? ?56
Ti OP ?176??? ?60? ?44
TiMBL ?132??? ?16
RIPPER ?116???
Table 5: Cost comparisons and ?2 levels of signifi-
cance for 3-way classification
The cost measure shows the strict ordering: Or-
acle < Ti OP < TiMBL < RIPPER < Baseline.
Note however that according to the ?2 test there is
no significant difference between the oracle system
and TiMBL with optimized parameters. Table 5 also
shows that all of our experiments significantly out-
perform the baseline system.
8 Conclusion
We used a combination of acoustic confidence and
pragmatic plausibility features (i.e. computed from
dialogue context) to predict the quality of incom-
ing recognition hypotheses to a multi-modal dia-
logue system. We classified hypotheses as accept,
(clarify), reject, or ignore: functional categories that
7Following (Hinton, 1995), we leave out categories with ex-
pected frequencies < 5 in the ?2 computation and reduce the
degrees of freedom accordingly.
can be used by a dialogue manager to decide appro-
priate system reactions. The approach is novel in
combining machine learning with n-best processing
for spoken dialogue systems using the Information
State Update approach.
Our best results, obtained using TiMBL with op-
timized parameters, show a 25% weighted f-score
improvement over a baseline system that uses a
?grammar-switching? approach to context-sensitive
speech recognition, and are only 8% away from the
optimal performance that can be achieved on the
data. Clearly, this improvement would result in bet-
ter dialogue system performance overall. Parameter
optimization improved the classification results by
9% compared to using the learner with default set-
tings, which shows the importance of such tuning.
Future work points in two directions: first, inte-
grating our methodology into working ISU-based
dialogue systems and determining whether or not
they improve in terms of standard dialogue eval-
uation metrics (e.g. task completion). The ISU
approach is a particularly useful testbed for our
methodology because it collects information per-
taining to dialogue context in a central data struc-
ture from which it can be easily extracted. This av-
enue will be further explored in the TALK project8.
Second, it will be interesting to investigate the im-
pact of different dialogue and task features for clas-
sification and to introduce a distinction between
?generic? features that are domain independent and
?application-specific? features which reflect proper-
ties of individual systems and application scenarios.
Acknowledgments
We thank Nuance Communications Inc. for the use
of their speech recognition and synthesis software
and Alexander Koller and Dan Shapiro for read-
ing draft versions of this paper. Oliver Lemon was
partially supported by Scottish Enterprise under the
Edinburgh-Stanford Link programme.
References
M. Boros, W. Eckert, F. Gallwitz, G. Go?rz, G. Han-
rieder, and H. Niemann. 1996. Towards Under-
standing Spontaneous Speech: Word Accuracy
vs. Concept Accuracy. In Proc. ICSLP-96.
Ananlada Chotimongkol and Alexander I. Rud-
nicky. 2001. N-best Speech Hypotheses Re-
ordering Using Linear Regression. In Proceed-
ings of EuroSpeech 2001, pages 1829?1832.
William W. Cohen. 1995. Fast Effective Rule In-
duction. In Proceedings of the 12th International
Conference on Machine Learning.
8EC FP6 IST-507802, http://www.talk-project.org
Walter Daelemans and Ve?ronique Hoste. 2002.
Evaluation of Machine Learning Methods for
Natural Language Processing Tasks. In Proceed-
ings of LREC-02.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2002. TIMBL: Tilburg
Memory Based Learner, version 4.2, Reference
Guide. In ILK Technical Report 02-01.
John Dowding, Jean Mark Gawron, Doug Appelt,
John Bear, Lynn Cherny, Robert Moore, and
Douglas Moran. 1993. GEMINI: a natural lan-
guage system for spoken-language understand-
ing. In Proceedings of ACL-93.
Malte Gabsdil. 2003. Classifying Recognition Re-
sults for Spoken Dialogue Systems. In Proceed-
ings of the Student Research Workshop at ACL-
03.
Perry R. Hinton. 1995. Statistics Explained ? A
Guide For Social Science Students. Routledge.
Oliver Lemon and Alexander Gruenstein. 2004.
Multithreaded context for robust conversational
interfaces: context-sensitive speech recognition
and interpretation of corrective fragments. ACM
Transactions on Computer-Human Interaction.
(to appear).
Oliver Lemon, Alexander Gruenstein, and Stanley
Peters. 2002. Collaborative activities and multi-
tasking in dialogue systems. Traitement Automa-
tique des Langues, 43(2):131?154.
Oliver Lemon. 2004. Context-sensitive speech
recognition in ISU dialogue systems: results for
the grammar switching approach. In Proceedings
of the 8th Workshop on the Semantics and Prag-
matics of Dialogue, CATALOG?04.
Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000. Predicting Automatic Speech Recognition
Performance Using Prosodic Cues. In Proceed-
ings of NAACL-00.
Erwin Marsi, Martin Reynaert, Antal van den
Bosch, Walter Daelemans, and Ve?ronique Hoste.
2003. Learning to predict pitch accents and
prosodic boundaries in Dutch. In Proceedings of
ACL-03.
David Traum, Johan Bos, Robin Cooper, Staffan
Larsson, Ian Lewin, Colin Matheson, and Mas-
simo Poesio. 1999. A Model of Dialogue Moves
and Information State Revision. Technical Re-
port D2.1, Trindi Project.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using Natural Language Processing and
Discourse Features to Identify Understanding Er-
rors in a Spoken Dialogue System. In Proceed-
ings of ICML-2000.
Combining Acoustic Confidences and Pragmatic Plausibility for Classifying
Spoken Chess Move Instructions
Malte Gabsdil
Department of Computational Linguistics
Saarland University
Germany
gabsdil@coli.uni-sb.de
Abstract
This paper describes a machine learning ap-
proach to classifying n-best speech recogni-
tion hypotheses as either correctly or incor-
rectly recognised. The learners are trained on
a combination of acoustic confidence features
and move evaluation scores in a chess-playing
scenario. The results show significant improve-
ments over sharp baselines that use confidence
rejection thresholds for classification.
1 Introduction
An important task in designing spoken dialogue systems
is to decide whether a system should accept (consider
correctly recognised) or reject (assume misrecognition)
a user utterance. This decision is often based on acoustic
confidence scores computed by the speech recogniser and
a fixed confidence rejection threshold. However, a draw-
back of this approach is that it does not take into account
that in particular dialogue situations some utterances are
pragmatically more plausible than others.1
This paper describes machine learning experiments
that combine acoustic confidences with move evaluation
scores to classify n-best recognition hypothesis of spoken
chess move instructions (e.g. ?pawn e2 e4?) as correctly
or incorrectly recognised. Classifying the n-best recog-
nition hypotheses instead of the single-best (e.g. (Walker
et al, 2000)) has the advantage that a correct hypothe-
sis can be accepted even when it is not the highest scor-
ing recognition result. Previous work on n-best hypothe-
sis reordering (e.g. (Chotimongkol and Rudnicky, 2001))
has focused on selecting hypotheses with the lowest rel-
ative word error rate. In contrast, our approach makes
1Although it is possible to use dialogue-state dependent
recognition grammars that reflect expectations of what the user
is likely to say next, these expectations do not say anything
about the plausibility of hypotheses.
predictions about whether hypotheses should be accepted
or rejected. The learning experiments show significantly
improved classification results over competitive baselines
and underline the usefulness of incorporating higher-level
information for utterances classification.
2 Domain and Data Collection
The domain of our research are spoken chess move in-
structions. We chose this scenario as a testbed for our
approach for three main reasons. First, we can use move
evaluation scores computed by a computer chess pro-
gram as a measure of the pragmatic plausibility of hy-
potheses. Second, the domain is simple and allows us
to collect data in a controlled way (e.g. we can control
for player strength), and third, the domain is restricted in
the sense that there is only a finite set of possible legal
moves in every situation. Similar considerations already
let researchers in the 1970s choose chess-playing as an
example scenario for the HEARSAY integrated speech
understanding system (Reddy and Newell, 1974).
We collected spoken chess move instructions in a small
experiment from six pairs of chess players. All sub-
jects were German native speakers and familiar with
the rules of chess. The subject?s task was to re-play
chess games (given to them as graphical representations)
by instructing each other to move pieces on the board.
Altogether, we collected 1978 move instructions under
different experimental conditions (e.g. strong games vs.
weak games) in the following four data sets: 1) language
model, 2) training, 3) development, and 4) test.
The recordings of the language model games were
transcribed and served to construct a context free recog-
nition grammar for the Nuance 8.02 speech recogniser
which was then used to process all other move instruc-
tions with 10-best output.
2http://www.nuance.com . We thank Nuance Inc. for
making their speech recognition software available to us.
3 Baseline Systems
The general aim of our experiments is to decide whether
a recognised move instruction is the one intended by the
speaker. A system should accept correct recognition hy-
potheses and reject incorrect ones. We define the follow-
ing two baseline systems for this binary decision task.
3.1 First Hypothesis Baseline
The first hypothesis baseline uses a confidence rejection
threshold to decide whether the best recognition hypoth-
esis should be accepted or rejected. To find an optimal
value, we linearly vary the confidence threshold returned
by the Nuance 8.0 recogniser (integral values in the range
  	 ) and use it to classify the training and develop-
ment data.
The best performing confidence threshold on the com-
bined training and development data was 17 with an accu-
racy of 63.8%. This low confidence threshold turned out
to be equal to the majority class baseline which is to clas-
sify all hypotheses as correctly recognised. In order to
get a more balanced distribution of classification errors,
we also optimised the confidence threshold according to
the cost measure defined in Section 5. According to this
measure, the optimal confidence rejection threshold is 45
with a classification accuracy of 60.5%.3
3.2 First Legal Move Baseline
The first legal move baseline makes use of the constraint
that user utterances only contain moves that are legal in
the current board configuration. We thus first eliminate
all hypotheses that denote illegal moves from the 10-best
output and then apply a confidence rejection threshold to
decide whether the best legal hypothesis should be ac-
cepted or rejected.
The best performing confidence threshold on the com-
bined training and test data for the first legal move base-
line was 23 with an accuracy of 92.4%. This threshold
also optimised the cost measure defined in Section 5. The
performance of both baseline systems on the test data is
reported below in Table 2 together with the results for the
machine learning experiments.
4 ML Experiments
We devise two different machine learning experiments
for selecting hypotheses from the recogniser?s n-best out-
put and from a list of all legal moves given a certain board
configuration.
In Experiment 1, we first filter out all illegal moves
from the n-best recognition results and represent the re-
maining legal moves in terms of 32 dimensional fea-
ture vectors including acoustic confidence scores from
345 is also the default confidence rejection threshold of the
Nuance 8.0 speech recogniser.
the recogniser as well as move evaluation scores from a
computer chess program. We then use machine learners
to decide for each move hypothesis whether it was the
one intended by the speaker. If more than one hypothesis
is classified as correct, we pick the one with the highest
acoustic confidence. If there is no legal move among the
recognition hypotheses or all hypotheses are classified as
incorrect, the input is rejected.
Experiment 2 adds a second classification step to Ex-
periment 1. In case an utterance is rejected in Experiment
1, we try to find the intended move among all legal moves
in the current situation. This is again defined in terms of
a classification problem. All legal moves are represented
by 31 dimensional feature vectors that include ?similar-
ity features? with respect to the interpretation of the best
recognition hypothesis and move evaluation scores. Each
move is then classified as either correct or incorrect. We
pick a move if it is the only one that is classified as correct
and all others as incorrect; otherwise the input is rejected.
The average number of legal moves in the development
and training games was 35.3 with a maximum of 61.
4.1 Feature Sets
The feature set for the classification of legal move hy-
potheses in the recogniser?s n-best list (Experiment 1)
consists of 32 features that can be coarsely grouped into
six categories (see below). All features were automati-
cally extracted or computed from the output of the speech
recogniser, move evaluation scores, and game logs.
1. Recognition statistics (3): position in n-best list;
relative position among and total number of legal
moves in n-best list
2. Acoustic confidences (6): overall acoustic confi-
dence; min, max, mean, variance, standard deviation
of individual word confidences
3. Text (1): hypothesis length (in words)
4. Depth1 plausibility (10): raw & normalised move
evaluation score wrt. scores for all legal moves;
score rank; raw score difference to max score;
min, max, mean of raw scores; raw z-score; move
evaluation rank & z-score among n-best legal moves
5. Depth10 plausibility (10): same features as for
depth1 plausibility (at search depth 10)
6. Game (2): ELO (strength) of player; ply number
The feature set for the classification of all legal moves
in Experiment 2 is summarised below. Each move is rep-
resented in terms of 31 (automatically derived) features
which can again be grouped into 6 different categories.
1. Similarity (5): difference size; difference bags;
overlap size; overlap bag
2. Acoustic confidences (6): same as in Experiment 1
for best recognition hypothesis
3. Text (2): length of best recognition hypothesis (in
words) and recognised string (bag of words)
4. Depth1 plausibility (8): same as in Experiment 1
(w/o features relating to n-best legal moves)
5. Depth10 plausibility (8): same as in Experiment 1
(w/o features relating to n-best legal moves)
6. Game (2): same as in Experiment 1
The similarity features are meant to represent how
close a move is to the interpretation of the best recogni-
tion result. The motivation for these features is that the
machine learner might find regularities about what likely
confusions arise in the data. For example, the letters ?b?,
?c?, ?d?, ?e? and ?g? are phonemically similar in Ger-
man (as are the letters ?a? and ?h? and the two digits
?zwei? and ?drei?). Although the move representations
are abstractions from the actual verbalisations, the lan-
guage model data showed that most of the subjects re-
ferred to coordinates with single letters and digits and
therefore there is some correspondence between the ab-
stract representations and what was actually said.
4.2 Learners
We considered three different machine learners for
the two classification tasks: the memory-based learner
TiMBL (Daelemans et al, 2002), the rule induction
learner RIPPER (Cohen, 1995), and an implementation
of Support Vector Machines, SVM  
 (Joachims, 1999).
We trained all learners with various parameter settings
on the training data and tested them on the development
data. The best results for the first task (selecting legal
moves from n-best lists) were achieved with SVM   
whereas RIPPER outperformed the other two learners on
the second task (selecting from all possible legal moves).
SVM  
	
and RIPPER where therefore chosen to clas-
sify the test data in the actual experiments.
5 Results and Evaluation
5.1 Cost Measure
We evaluate the task of selecting correct hypotheses with
two different metrics: i) classification accuracy and ii) a
simple cost measure that computes a score for different
classifications on the basis of their confusion matrices.
Table 1 shows how we derived costs from the additional
number of steps (verbal and non-verbal) that have to be
taken in order to carry out a user move instruction. Note
that the cost measure is not validated against user judge-
ments and should therefore only be considered an indica-
tor for the (relative) quality of a classification.
5.2 Results
Table 2 reports the raw classification results for the differ-
ent baselines and machine learning experiments together
Class Cost Sequence
accept correct 0 instruct ? move
reject correct/ 2 instruct ? reject ? instruct ?
reject incorrect move
accept incorrect 4 instruct ? move ? object ?
move ? instruct ? move
Table 1: Cost measure
with their accuracy and associated cost. Here and in sub-
sequent tables, FH 
 and FH  refer to the first hypoth-
esis baselines with confidence thresholds 17 and 45 re-
spectively, FLM to the first legal move baseline, and Exp1
and Exp2 to Experiments 1 and 2 respectively.
accept reject
FH 
 (Acc: 61.7% Cost: 1230)
correct 489 0
incorrect 306 3
FH  (Acc: 64.3% Cost: 1188)
correct 441 48
incorrect 237 72
FLM (Acc: 93.5% Cost: 358)
correct 671 0
incorrect 52 75
Exp1 (Acc: 97.2% Cost: 246)
correct 695 2
incorrect 20 81
Exp2 (Acc: 97.2% Cost: 176)
correct 731 1
incorrect 21 45
Table 2: Raw classification results
The most striking result in Table 2 is the huge classifi-
cation improvement between the first hypothesis and the
first legal move baselines. For our domain, this shows a
clear advantage of n-best recognition processing filtered
with ?hard? domain constraints (i.e. legal moves) over
single-best processing.
Note that the results for Exp1 and Exp2 in Table 2 are
given ?by utterance? (i.e. they do not reflect the classi-
fication performance for individual hypotheses from the
n-best lists and the lists of all legal moves). Note also
that both the different baselines and the machine learning
systems have access to different information sources and
therefore what counts as correctly or incorrectly classi-
fied varies. For example, the gold standard for the first
hypothesis baseline only considers the best recognition
result for each move instruction. If this is not the one in-
tended by the speaker, it counts as incorrect in the gold
standard. On the other hand, the first legal move among
the 10-best recognition hypotheses for the same utterance
might well be the correct one and would therefore count
as correct in the gold standard for the FLM baseline.
5.3 Comparing Classification Systems
We use the  

test of independence to compute whether
the classification results are significantly different from
each other. Table 3 reports significance results for com-
paring the different classifications of the test data. The
table entries include the differences in cost and the level
of statistical difference between the confusion matrices
as computed by the  

statistics (  denotes significance
at 
 
,  at 	

 
, and  at 	
 ). The table
should be read row by row. For example, the top row
in Table 3 compares the classification from Exp2 to all
other classifications. The value 

 means that the
cost compared to FH 
 is reduced by 1054 and that the
confusion matrices are significantly different at 
 
.
FH  FH  FLM Exp1
Exp2 

 
  
