Applying System Combinat ion to Base Noun Phrase Identif ication 
Er ik  F. T jong  K im Sang",  Wal ter  Dae lemans  '~, Herv6  D6 jean  ~, 
Rob  Koel ingT,  Yuva l  Krymolowski /~,  Vas in  Punyakanok  '~, Dan Roth" 
~University of Antwert) 
Uifiversiteitsplohl 1 
13.-261.0 Wilri jk, Belgium 
{erikt,daelem}@uia.ua.ac.be 
r Unive.rsitiil; Tii l) ingen 
Kleine Wilhehnstrat./e 113 
I)-72074 T/il)ingen, Germany 
(lejean((~sl:q, ni)hil.ulfi-l;uebingen.de, 
7S1{,I Cambridge 
23 Millers Yard,Mil l  Lane 
Cambridge, CB2 ll{Q, UK 
koeling@caln.sri.coIn 
;~Bal'-Ilan University 
lbunat  Gan, 52900, Israel 
yuwdk(c~)macs. 1)iu.ac.il 
"University of Illinois 
1304: W. Sl)ringfield Ave. 
Url)ana, IL 61801, USA 
{lmnyakan,(lanr} ((~cs.uiuc.edu 
A1)s t rac t  
We us('. seven machine h;arning algorithms tbr 
one task: idenl;it~ying l)ase holm phrases. The 
results have 1)een t)rocessed by ditt'erent system 
combination methods and all of these (mtt)er- 
formed the t)est individual result. We have ap- 
t)lied the seven learners with the best (:omt)ina- 
tot, a majority vote of the top tive systenls, to a 
standard (lata set and lllallage(1 I;O ilnl)rov(', 1;11(' 
t)est pul)lished result %r this (lata set. 
1 In t roduct ion  
Van Haltor(m eta\ ] .  (1998) and Brill and Wu 
(1998) show that part-ofst)ee(:h tagger l)erfor- 
mance can 1)e iml)roved 1)y (:oml)ining ditl'erent 
tatters. By using te(:hni(tues su(:h as majority 
voting, errors made l)y 1;11(; minority of the tag- 
gers can 1)e r(;moved. Van Ilaltere, n et al (1998) 
rel)ort that the results of such a ('oml)ined al)- 
proach can improve ll\])Oll the aCcllracy error of 
the best individual system with as much as 19%. 
Tim positive (;tl'e(:t of system combination tbr 
non-language t)ro(:essing tasks has t)een shown 
in a large l)o(ly of mac\]fine l arning work. 
In this 1)aper we will use system (:omt)ination 
for identifying base noun 1)hrases (1)aseNt)s). 
W(; will at)l)ly seven machine learning algo- 
rithms to the same 1)aseNP task. At two l)oints 
we will al)ply confl)ination methods. We will 
start with making the systems process five out- 
trot representations and combine the l'esults t)y 
(:hoosing the majority of the outtmt tL'atures. 
Three of the seven systems use this al)l)roaeh. 
Afl, er this w(; will make an overall eoml)ination 
of the results of the seven systems. There we 
will evaluate several system combination meth- 
()(Is. The 1)est l)erforming method will 1)e at)- 
t)lied to a standard ata set tbr baseNP identi- 
tication. 
2 Methods  and exper iments  
in this se(:tion we will describe our lem:ning task: 
recognizing 1)ase noun phrases. After this we 
will (tes(:ril)e the data representations we used 
and the ma('hine learning algorithms that we 
will at)l)ly to the task. We will con(:ludc with 
an overview of the (:ombination metllo(ls that 
we will test. 
2.1 Task descript ion 
Base noun \])hrases (1)aseNPs) are n(mn phrases 
whi(:h do not (:ontain another noun l)hrase. \]?or 
cxamt)le , the sentence 
In \[early trading\] in \[ IIong Kong\] 
\[ Mo,l,tay \], \[ g,,la \] was q, loted at 
\[ $ 366. 0 \] \ [a .  o1,,,.(; \] .  
contains six baseN1)s (marked as phrases be- 
tween square 1)rackets). The phrase $ 266.50  
an  ounce  ix a holm phrase as well. However, it 
is not a baseNP since it contains two other noun 
phrases. Two baseNP data sets haw.' been put 
forward by Ramshaw and Marcus (1995). The 
main data set consist of tbur sections of the Wall 
Street Journal (WSJ) part of the Penn Tree- 
bank (Marcus et al, 1.993) as training mate- 
rial (sections 15-18, 211727 tokens) and one sec- 
tion aS test material (section 20, 47377 tokens)5. 
The data contains words, their part-of-speech 
1This Ramshaw and Marcus (1995) bascNP data set 
is availal)le via ffp://fti).cis.upe,m.edu/pub/chunker/ 
857 
(POS) tags as computed by the Brill tagger and 
their baseNP segmentation asderived from the 
%'eebank (with some modifications). 
In the baseNP identitication task, perfor- 
mance is measured with three rates. First, 
with the percentage of detected noun phrases 
that are correct (precision). Second, with the 
1)ercentage of noun phrases in the data that 
were found by the classifier (recall). And third, 
with the F#=~ rate which is equal to (2*preci- 
sion*recall)/(precision+recall). The latter rate 
has been used as the target for optimization. 
2.2 Data representat ion 
In our example sentence in section 2.1, noun 
phrases are represented by bracket structures. 
It has been shown by Mufioz et al (1999) 
that for baseNP recognition, the representa- 
tion with brackets outperforms other data rep- 
resentations. One classifier can be trained to 
recognize open brackets (O) and another can 
handle close brackets (C). Their results can be 
combined by making pairs of open and close 
brackets with large probability scores. We have 
used this bracket representation (O+C) as well. 
However, we have not used the combination 
strategy from Mufioz et al (1999) trot in- 
stead used the strategy outlined in Tjong Kim 
Sang (2000): regard only the shortest possi- 
ble phrases between candidate open and close 
brackets as base noun phrases. 
An alternative representation for baseNPs 
has been put tbrward by Ramshaw and Mar- 
cus (1995). They have defined baseNP recog- 
nition as a tagging task: words can be inside a 
baseNP (I) or outside a baseNP (O). In the case 
that one baseNP immediately follows another 
baseNP, the first word in the second baseNP 
receives tag B. Example: 
Ino early1 trading1 ino Hongi Kongi 
MondayB ,o gold1 waso quotedo ato 
$I 366.501 anu ounce1 .o 
This set of three tags is sufficient for encod- 
ing baseNP structures since these structures are 
nonrecursive and nonoverlapping. 
Tjong Kiln Sang (2000) outlines alternative 
versions of this tagging representation. First, 
the B tag can be used for tile first word of ev- 
ery baseNP (IOB2 representation). Second, in- 
stead of the B tag an E tag can be used to 
nlark the last word of a baseNP immediately 
before another baseNP (IOE1). And third, the 
E tag call be used for every noun phrase final 
word (IOE2). He used the Ramshaw and Mar- 
cus (1995) representation as well (IOB1). We 
will use these tbur tagging representations and 
the O+C representation for the system-internal 
combination experiments. 
2.a Machine learning algorithms 
This section contains a brief description of tile 
seven machine learning algorithms that we will 
apply to the baseNP identification task: AL- 
LiS, c5.0, IO~?ee, MaxEnt, MBL, MBSL and 
SNOW. 
ALLiS 2 (Architecture for Learning Linguistic 
Structures) is a learning system which uses the- 
ory refinement in order to learn non-recursive 
NP and VP structures (Ddjean, 2000). ALLiS 
generates a regular expression grammar which 
describes the phrase structure (NP or VP). This 
grammar is then used by the CASS parser (Ab- 
hey, 1996). Following the principle of theory re- 
finement, tile learning task is composed of two 
steps. The first step is the generation of an 
initial wa, mmar. The generation of this grmn- 
mar uses the notion of default values and some 
background knowledge which provides general 
expectations concerning the immr structure of 
NPs and VPs. This initial grammar provides 
an incomplete and/or incorrect analysis of tile 
data. The second step is the refinement of this 
grammar. During this step, the validity of the 
rules of the initial grammar is checked and the 
rules are improved (refined) if necessary. This 
refinement relies on the use of two operations: 
the contextualization (i which contexts uch a 
tag always belongs to the phrase) and lexical- 
ization (use of information about the words and 
not only about POS). 
05.0 a, a commercial version of 04.5 (Quin- 
lan, 1993), performs top-do,vn induction of de- 
cision trees (TDIDT). O,1 the basis of an in- 
stance base of examples, 05.0 constructs a deci- 
sion tree which compresses the classification i - 
formation in the instance base by exploiting dif- 
tbrences in relative importance of different fea- 
tures. Instances are stored in the tree as paths 
2A demo f the NP and VP ctmnker is available at 
ht;t:p: / /www.sfb441.unituebingen.de/~ dej an/chunker.h 
tml 
aAvailable fl'om http://www.rulequest.com 
858 
of commcted nodes ending in leaves which con- 
tain classification information. Nodes are con- 
nected via arcs denoting feature wflues. Feature 
inff)rmation gain (nmt;ual inforniation 1)etween 
features and class) is used to determine the or- 
der in which features are mnt)loyed as tests at all 
levels of the tree (Quinlan, 1993), With the full 
inlmt representation (words and POS tags)~ we 
were not able to run comt)lete xperiments. We 
therefore xperimented only with the POS tags 
(with a context of two left; and right). We have 
used the default parameter setting with decision 
trees coml)ined with wflue groul)ing. 
We have used a nearest neighbor algoritlml 
(IBI.-1G, here listed as MBL) and a decision tree 
algoritlmi (llG\[lh:ee) from the TiMBL learning 
package (Da(flmnans et al, 19991)). Both algo- 
rithms store the training data and ('lassi(y new 
it;eros by choosing the most frequent (:lassiti(:a- 
lion among training items which are closest to 
this new item. l)ata it(uns rare rel)resented as 
sets of thature-vahu; 1)airs. Each ti;ature recc'ives 
a weight which is t)ased on the amount of in- 
formation whi(:h it t/rovides fi)r comtmting the 
classification of t;t1(; items in the training data. 
IBI-IG uses these weights tbr comt)uting the dis- 
lance l)etween a t)air of data items and IGTree 
uses them fi)r deciding which feature-value de- 
cisions shouM t)e made in the top nod(;s of the 
decision tree (l)a(;lenJans et al, 19991)). We 
will use their det, mlt pm:amet('a:s excel)t for the 
IBI-IG t)arameter for the numl)er of exmnine(t 
m',arest n(,ighl)ors (k) whi('h we h~ve s(,t to 3 
(Daelemans et al, 1999a). The classifiers use a 
left and right context of four words and part- 
ofsl)eech tags. t~i)r |;lie four IO representations 
we have used a second i)rocessing stage which 
used a smaller context lint which included in- 
formation at)out the IO tags 1)redicted by the 
first processing phase (Tjong Kim Sang, 2000). 
When /)uilding a classifier, one must gather 
evidence ti)r predicting the correct class of an 
item from its context. The Maxinmm Entropy 
(MaxEnt) fl:mnework is especially suited tbr 
integrating evidence tiom various inti)rmal;ion 
sources. Frequencies of evidence/class combi~ 
nations (called features) are extracted fl'om a 
sample corlms and considere(t to be t)roperties 
of the classification process. Attention is con- 
strained to models with these l)roperties. The 
MaxEnt t)rinciph; now demands that among all 
1;11(; 1)robability distributions that obey these 
constraints, the most mfiform is chosen, l)ur- 
ing training, features are assigned weights in 
such a way that, given the MaxEnt principle, 
the training data is matched as well as possible. 
During evaluation it is tested which features are 
active (i.e. a feature is active when the context 
meets the requirements given by t;11(', feature). 
For every class the weights of the active fea- 
tures are combined and the best scoring class 
is chosen (Berger et al, 1996). D)r the classi- 
tier built here the surromlding words, their POS 
tags and lmseNP tags predicted for the previous 
words are used its evidence. A mixture of simple 
features (consisting of one of the mentioned in- 
formation sources) and complex features (com- 
binations thereof) were used. The left context 
never exceeded 3 words, the right context was 
maximally 2 words. The model wits (:ah:ulated 
using existing software (l)ehaspe, 1997). 
MBSL (Argalnon et al, 1999) uses POS data 
in order to identit~y t/aseNPs, hfferenee re- 
lies on a memory which contains all the o(:- 
cm:rences of P()S sequences which apt)ear in 
the t)egimfing, or the end, of a 1)aseNl? (in- 
(:hiding complete t)hrases). These sequences 
may include a thw context tags, up to a 1)re- 
st)ecifi('d max_(:ont<~:t. \])uring inti',rence, MBSL 
tries to 'tile' each POS string with parts of 
noun-l)hrases from l;he memory. If the string 
coul(1 l)e fully covered t)y the tiles, il; becomes 
l)art of a (:andidate list, anfl)iguities 1)etween 
candidates are resolved by a constraint )ropa- 
gation algorithm. Adding a (:ontext extends the 
possil)ilities for tiling, thereby giving more op- 
portunities to 1)etter candidates. The at)t)roaeh 
of MBSL to the i)rot)lem of identifying 1)aseNPs 
is sequence-1)ased rather than word-based, that 
is, decisions are taken per POS sequence, or per 
candidate, trot not for a single word. In addi- 
tion, the tiling l)rocess gives no preference to 
any (tirection in the sentence. The tiles may 1)e 
of any length, up to the maximal ength of a 
1)hrase in the training (ILl;L, which gives MBSL 
a generalization power that compensates for the 
setup of using only POS tags. The results t)re- 
seated here were obtained by optimizing MBSL 
parameters based on 5-fold CV on the training 
data. 
SNoW uses the Open/Close model, described 
in Mufioz et al (1999). As is shown there, this 
859 
section 21 
IOB1 
IOB2 
IOE1 
IOE2 
O+C 
0 
97.81% 
97.63% 
97.80% 
97.72% 
97.72% 
MBL 
Majority 98.04% 98.20% 
C Ffl=l 
97.97% 91.68 
97.96% 91.79 
97.92% 91.54 
97.94% 92.06 
98.04% 92.03 
92.82 
MaxEnt 
O C 
97.90% 98.11% 
97.81% 98.14% 
97.88% 98.12% 
97.84% 98.12% 
97.82% 98.15% 
97.94% 98.24% 
Ffl=l 
92.43 
92.14 
92.37 
92.13 
92.26 
92.60 
IGTree 
O C 
96.62% 96.89% 
97.27% 97.30% 
95.88% 96.01% 
97.19% 97.62% 
96.89% 97.49% 
97.70% 97.99% 
F\[~=1 
87.88 
90.03 
82.80 
89.98 
89.37 
91.92 
Table 1: The effects of system-internal combination by using different output representations. A 
straight-forward majority vote of the output yields better bracket accuracies and Ffl=l rates than 
any included individual classifier. The bracket accuracies in the cohmms O and C show what 
percentage of words was correctly classified as baseNP start, baseNP end or neither. 
model produced better results than the other 
paradigm evaluated there, the Inside/Outside 
paradigm. The Open/Close model consists of 
two SNoW predictors, one of which predicts the 
beginning of baseNPs (Open predictor), and the 
other predicts the end of the ptlrase (Close pre- 
dictor). The Open predictor is learned using 
SNoW (Carlson el; al., 1999; Roth, 1998) as a 
flmction of features that utilize words and POS 
tags in the sentence and, given a new sentence, 
will predict for each word whether it is the first 
word in the phrase or not. For each Open, the 
Close predictor is learned using SNoW as a func- 
tion of features that utilize the words ill the sen- 
tence, the POS tags and the open prediction. It 
will predict, tbr each word, whether it Call be 
the end of" the I)hrase, given the previously pre- 
dicted Open. Each pair of predicted Open mid 
Close forms a candidate of a baseNP. These can- 
didates may conflict due to overlapping; at this 
stage, a graph-based constraint satisfaction al- 
gorithm that uses the confidence values SNoW 
associates with its predictions i elnployed. This 
algorithln ("the combinator') produces tile list 
of" the final baseNPs fbr each sentence. Details 
of SNOW, its application in shallow parsing and 
the combinator% Mgorithm are in Mufioz et al 
(1999). 
2.4 Combinat ion techniques 
At two points in our noun phrase recognition 
process we will use system combination. We will 
start with system-internal combination: apply 
the same learning algorithm to variants of the 
task and combine the results. The approach 
we have chosen here is the same as in Tjong 
Kim Sang (2000): generate different variants 
of the task by using different representations 
of the output (IOB1, IOB2, IOE1, IOE2 and 
O+C). The five outputs will converted to the 
open bracket representation (O) and the close 
bracket; representation (C) and M'ter this, tile 
most frequent of the five analyses of each word 
will chosen (inajority voting, see below). We 
expect the systems which use this combination 
phase to perform better than their individuM 
members (Tjong Kim Sang, 2000). 
Our seven learners will generate different clas- 
sifications of tile training data and we need to 
find out which combination techniques are most 
appropriate. For the system-external combi- 
nation experiment, we have evaluated itfi;rent 
voting lllechanisms~ effectively the voting meth- 
ods as described in Van Halteren et al (1998). 
In the first method each classification receives 
the same weight and the most frequent classifi- 
cation is chosen (Majority). The second nmthod 
regards as tile weight of each individual clas- 
sification algorithm its accuracy on solne part 
of the data, tile tuning data (TotPrecision). 
The third voting method computes the preci- 
sion of each assigned tag per classifer and uses 
this value as a weight for tile classifier in those 
cases that it chooses the tag (TagPrecision). 
The fourth method uses both the precision of 
each assigned tag and tile recall of the com- 
peting tags (Precision-Recall). Finally, tile fifth 
lnethod uses not only a weight for tile current 
classification but it also computes weights tbr 
other possible classifications. The other classi- 
fications are deternfined by exalnining the tun- 
860 
ing data and registering the correct wflues for 
(;very pair of classitier esults (pair-wise voting, 
see Van Halteren et al (1998) tbr an elaborate 
explanation). 
Apart from these five voting methods we have 
also processed the output streams with two clas- 
sifters: MBL and IG%'ee. This approach is 
called classifier stacking. Like Van Halteren et 
al. (1998), we have used diff'erent intmt ver- 
sions: olle containing only the classitier Otltl)ut 
and another containing both classifier outlmt 
and a compressed representation of the data 
item tamer consideration. \]?or the latter lmr- 
pose we have used the part-of-speech tag of the 
carrent word. 
3 Resul ts  4 
We want to find out whether system combi- 
nation could improve performmlce of baseNP 
recognition and, if this is the fact, we want to 
seJect the best confl)ination technique. For this 
lmrpose we have pertbrmed an experiment with 
sections 15-18 of the WSJ part of the Prom %'ee- 
bank as training data (211727 tokens) and sec- 
tion 21 as test data (40039 tokens). Like the 
data used by Ramshaw and Marcus (1995), this 
data was retagged by the Brill tagger in order 
to obtain realistic part-of  speech (POS) tags 5. 
The data was seglnente.d into baseNP parts and 
non-lmseNP t)arts ill a similar fitshion as the 
data used 1)y Ramshaw and Marcus (1995). Of 
the training data, only 90% was used for train- 
ing. The remaining 10% was used as laming 
data for determining the weights of the combi- 
nation techniques. 
D)r three classifiers (MBL, MaxEnt and 
IGTree) we haw; used system-internal coral)i- 
nation. These learning algorithms have pro- 
cessed five dittbrent representations of the out- 
put (IOB1, IOB2, IOE1, IOE2 and O-t-C) and 
the results have been combined with majority 
voting. The test data results can 1)e fimnd in 
Table 1. In all cases, the combined results were 
better than that of the best included system. 
Tile results of ALLiS, 05.0, MB SL and SNoW 
have tmen converted to the O and the C repre- 
4Detailed results of our experiments me available on 
http: / /lcg-www.uia.ae.be/-erikt /np('oml,i / 
SThe retagging was necessary to assure that the per- 
formance rates obtained here would be similar to rates 
obtained for texts for which no Treebank POS tags are 
available. 
section 21 
Classifier 
ALLiS 
05.0 
IGTree 
MaxEnt 
MBL 
MBSL 
SNoW 
Simple Voting 
Majority 
TotPrecision 
TagPrecision 
Precision-Recall 
0 
97.87% 
97.05% 
97.70% 
97.94% 
98.04% 
97.27% 
97.78% 
98.08% 
98.08% 
98.08% 
98.08% 
C FS=j 
98.08% 92.15 
97.76% 89.97 
97.99% 91.92 
98.24% 92.60 
98.20% 92.82 
97.66% 90.71 
97.68% 91.87 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
Pairwise Voting 
TagPair 98.13% 98.23% 
Memory-Based 
Tags 98.24% 98.35% 
Tags 4- P()S 98.14% 98.33% 
Deeision Trees 
Tags 98.24% 98.35% 
Tags + POS 98.13% 98.32% 
93.07 
93.39 
93.24 
93.39 
93.21 
Table 2: Bracket accuracies and Ff~=l scores 
for section WSJ 21 of the Penn ~15'eebank with 
seve, n individual classifiers and combinations of 
them. Each combination t)erforms t)etter than 
its best individual me, tuber. The stacked classi- 
tiers without COllte, xt intbrmation perform best. 
sentation. Together with the bracket; ret)resen- 
tations of the other three techniques, this gave 
us a total of seven O results and seven C results. 
These two data streams have been combined 
with the combination techniques described in 
section 2.4. After this, we built baseNPs from 
the, O and C results of each combinatkm tech- 
nique, like, described in section 2.2. The bracket 
accuracies and tile F~=I scores tbr test data can 
be found in Table 2. 
All combinations iml)rove the results of the 
best individual classifier. The best results were 
obtained with a memory-based stacked classi- 
ter. This is different from the combination re- 
sults presented in Van Ilalteren et al (1998), 
in which pairwise voting pertbrmed best. How- 
eves, in their later work stacked classifiers out- 
perIbrm voting methods as well (Van Halteren 
et al, to appear). 
861 
section 20 accuracy precision recall 
Best-five combination 0:98.32% C:98.41% 94.18% 93.55% 
Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 
Mufioz et al (1999) O:98.1% C:98.2% 92.4% 93.1% 
Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 
Argamon et al (1999) - 91.6% 91.6% 
F/3=1 
93.86 
93.26 
92.8 
92.03 
91.6 
Table 3: The overall pertbrmance of the majority voting combination of our best five systems 
(selected on tinting data perfbrnmnce) applied to the standard data set pnt tbrward by Ramshaw 
and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how 
often a word was classified correctly with the representation used (O, C or IOB1). The combined 
system outperforms all earlier reported results tbr this data set. 
Based on an earlier combination study 
(Tjong Kim Sang, 2000) we had expected the 
voting methods to do better. We suspect hat 
their pertbrmance is below that of the stacked 
classifiers because the diflhrence between tile 
best and the worst individual system is larger 
than in our earlier study. We assume that the 
voting methods might perform better if they 
were only applied to the classifiers that per- 
form well on this task. In order to test this 
hypothesis, we have repeated the combination 
experiments with the best n classitiers, where 
n took vahms from 3 to 6 and the classifiers 
were ranked based on their performance on the 
tnning data. The t)est pertbrmances were ob- 
tained with five classifiers: F/~=1=93.44 for all 
five voting methods with tile best stacked classi- 
tier reaching 93.24. With the top five classifiers, 
tile voting methods outpertbrm the best; combi- 
nation with seven systems G. Adding extra clas- 
sification results to a good combination system 
should not make overall performance worse so 
it is clear that there is some room left for im- 
provement of our combination algorithms. 
We conclude that the best results ill this 
task can be obtained with tile simplest voting 
method, majority voting, applied to the best 
five of our classifiers. Our next task was to 
apply the combination apt)roach to a standard 
data set so that we could compare our results 
with other work. For this purpose we have used 
6V~re are unaware of a good method for determining 
the significance of F~=I differences but we assume that 
this F~=I difference is not significant. However, we be- 
lieve that the fact that more colnbination methods per- 
tbrm well, shows that it easier to get a good pertbrmmlce 
out of the best; five systems than with all seven. 
tile data put tbrward by ll,amshaw and Marcus 
(1995). Again, only 90% of the training data 
was used tbr training while the remaining 11)% 
was reserved tbr ranking the classifiers. The 
seven learners were trained with the same pa- 
rameters as in the previous experiment. Three 
of the classifiers (MBL, MaxEnt and iG%'ee) 
used system-internal combination by processing 
different output representations. 
The classifier output was converted to the 
O and the C representation. Based on the 
tuning data performance, the classifiers ALLiS, 
IGTREE, MaxEnt, MBL and SNoW were se- 
lected for being combined with majority vot- 
ing. After this, the resulting O and C repre- 
sentations were combined to baseNPs by using 
the method described in section 2.2. The re- 
sults can be found in Table 3. Our combined 
system obtains an F/~=I score of 93.86 which 
corresponds to an 8% error reduction compared 
with tile best published result tbr this data set 
(93.26). 
4 Conc lud ing  remarks  
In this paper we have examined two methods for 
combining the results of machine learuing algo- 
rithms tbr identii}cing base noun phrases. Ill the 
first Inethod, the learner processed ifferent out- 
put data representations and tile results were 
combined by majority voting. This approach 
yielded better results than the best included 
classifier. Ill the second combination approach 
we have combined the results of seven learning 
systems (ALLiS, c5.0, IGTree, MaxEnt, MBL, 
MBSL and SNOW). Here we have tested d i f  
ferent confl)ination methods. Each coilfl)ination 
862 
nmthod outt)erformed the best individual learn- 
ing algorithm and a majority vote of the tol) 
five systems peribrmed best. We, have at}i}lie, d 
this approach of system-internal nd system- 
external coral}|nation to a standard ata set for 
base noun phrase identification and the 1}ertbr- 
mance of our system was 1)etter than any other 
tmblished result tbr this data set. 
Our study shows that the c, omt)ination meth- 
(}{Is that we have tested are sensitive for the in- 
clusion of classifier esults of poor quality. This 
leaves room for imt)rovement of our results t}y 
evaluating other coml}inators. Another interest- 
ing apl)roach which might lead to a l}etter t)er- 
f{}rmance is taking into a{-com~t more context 
inibrmation, for example by coral)in|rig com- 
plete 1}hrases instead of indet}endent t}ra{:kets. 
It would also be worthwhile to evaluate using 
more elaborate me, thods lbr building baseNPs 
out of ot}en and close t}ra{:ket (:an{ti{tates. 
Acknowledgements  
l)djean, Koeling and 'l?jong Kim Sang are 
funded by the TMII. 1\]etwork Learning (Jompu- 
tational Grammars r. 1}unyakanok and Roth are 
SUl)t}orted by NFS grants IIS-98{}1638 an{t SBR- 
9873450. 
Re ferences  
Steven Alm{',y. 1996. Partial t)a\]'sing via finite- 
state cascades. In l'n, l}~wce, di'ngs of the /~,gS- 
LLI '95 l?,obust 1)arsi'n9 Worlcsh, op. 
SMomo Argam(m, Ido l)agan, an(l YllV~t\] Kry- 
molowsld. 1999. A memory-1}ased at}proach 
to learning shalh}w natural anguage patterns. 
Journal of E:rperimental and Th, eovetical AL 
11(3). 
Adam L. Berge, r, SteI}hen A. l)ellaPietra, and 
Vincent J. DellaPietra. 1996. A inaximum 
entrol)y apI)roach to natural language pro- 
cessing. Computational Linguistics, 22(1). 
Eric Bri\]l and ,lun Wu. 1998. Classifier com- 
bination tbr improved lexical disaml)iguation. 
In P~vccedings o.f COLING-A 6'15 '98. Associ- 
ation for Computational Linguistics. 
A. Carlson, C. Cunfl)y, J. Rosen, and 
D. l/,oth. 1.999. The SNoW learning archi- 
tecture. Technical Report UIUCDCS-11,-99- 
2101, UIUC Computer Science Department, 
May. 
r httl): / /lcg-www.ui',,.ac.be~/ 
Walter Daelemans, A.ntal van den Bosch, and 
Jakub Zavrel. 1999a. \])brgetting exceptions 
is harmflll in language learning. Machine 
Learning, 34(1). 
Walter Daelemans, Jakub Zavrel, Ko wmder  
Sloot, and Antal van den Bosch. 1999b. 
TiMBL: Tilb'arg Memory Bused Learner, ver- 
sion 2.G Rqfi;rence Guide. ILK Te(:hnical 
th',port 99-01. http:// i lk.kub.nl/.  
Luc Dehaspe. 1997. Maximum entropy model- 
ing with clausal constraints, in PTvcecdings oJ' 
th, c 7th, 1}l, ternational Workshop on ind'uctivc 
Logic Programming. 
Hervd Ddjean. 200(I. Theory refinement and 
natural language processing. In Proceedings 
of the ColingEO00. Association for Computa- 
tional Linguistics. 
Mitchell 17 }. Marcus, Beatrice Santorini, and 
Mary Aim Marcinkiewicz. 1993. Building a 
large mmotated corpus of english: the penn 
treebank. Computational Linguistics, 19(2). 
Marcia Munoz,  Vasin Punyakanok, l)an l l,oth, 
and Day Zimak. 1999. A learning ap- 
t}roa(:h to shallow t)arsing. In P~vceedings of 
EMNLP-WVLC'99.  Asso('iation for Coml)u- 
tational Linguisti(:s. 
J. Ross Quinlan. 1993. c/t.5: Programs for Ma- 
th,|he Learning. Morgan Kauflnann. 
Lance A. Ramshaw and Mitchell P. Marcus. 
1995. Text chunking using transformation- 
l)ase{t learn|Jig. In 1}roceeding s o\[ the Th, i'rd 
A CL Worksh, op on Ve, r~.l LacTic Corpora. As- 
sociation for Comlmtational Linguistics. 
D. Roth. 1.9!t8. Learning to resolve natural an- 
guage aml}iguities: A unified approach. In 
AAAL98.  
Erik F. Tjong Kim Sang. 2000. N{mn phrase 
recognition by system {:ombination. In Pro- 
ceedings of th, e ANLP-NAA CL-2000. Seattle, 
Washington, USA. Morgan Kauflnan Pub- 
lishers. 
Hans van Halteren, Jakub Zavrel, and Wal- 
ter Daelemans. 1998. Iml)roving data driven 
wordclass tagging by system corot}|nation. In
P~veeedings of COLING-ACL '98. Associa- 
tion tbr Computational Linguistics. 
Hans van Halteren, Jakub Zavrel, and Walter 
Daelemans. to appear, hnproving accuracy 
ill nlp through coati)|nation ofmachine learn- 
ing systems. 
863 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 139-141, Lisbon, Portugal, 2000. 
Chunking with Maximum Entropy Models 
Rob Koe l ing  
SRI Cambridge 
koel?ng0cam, srJ.. com 
1 In t roduct ion  
In this paper I discuss a first attempt to create a
text chunker using a Maximum Entropy model. 
The first experiments, implementing classifiers 
that tag every word in a sentence with a phrase- 
tag using very local lexical information, part- 
of-speech tags and phrase tags of surrounding 
words, give encouraging results. 
2 Max imum Ent ropy  mode ls  
Maximum Entropy (MaxEnt) models (Jaynes, 
1957) are exponential models that implement 
the intuition that if there is no evidence to 
favour one alternative solution above another, 
both alternatives should be equally likely. In 
order to accomplish this, as much information 
as possible about the process you want to model 
must be collected. This information consists 
of frequencies of events relevant o the process. 
The frequencies of relevant events are consid- 
ered to be properties of the process. When 
building a model we have to constrain our at- 
tention to models with these properties. In 
most cases the process is only partially de- 
scribed. The MaxEnt framework now demands 
that from all the models that satisfy these con- 
straints, we choose the model with the flattest 
probability distribution. This is the model with 
the highest entropy (given the fact that the con- 
straints are met). When we are looking for a 
conditional model P(w\]h), the MaxEnt solution 
has the form: 
1 . e~ i Aifi(h,w) P(wlh) = Z(h) 
where fi(h,w) refers to a (binary valued) fea- 
ture function that describes a certain event; Ai 
is a parameter that indicates how important fea- 
ture fi is for the model and Z(h) is a normali- 
sation factor. 
In the last few years there has been an in- 
creasing interest in applying MaxEnt models for 
NLP applications (Ratnaparkhi, 1998; Berger et 
al., 1996; Rosenfeld, 1994; Ristad, 1998). The 
attraction of the framework lies in the ease with 
which different information sources used in the 
modelling process are combined and the good 
results that are reported with the use of these 
models. Another strong point of this framework 
is the fact that general software can easily be 
applied to a wide range of problems. For these 
experiments we have used off-the-shelf software 
(Maccent) (Dehaspe, 1997). 
3 An  MaxEnt  chunker  
3.1 At t r ibutes  used 
First need to be decided which information 
sources might help to predict he chunk tag. We 
need to work with the information that is in- 
cluded in the WSJ corpus, so the choice is first 
limited to: 
? Current word 
? POS tag of current word 
? Surrounding words 
? POS tags of surrounding words 
All these sources will be used, but in case of the 
information sources using surrounding words we 
will have to decide how much context is taken 
into account. I did not perform exhaustive t sts 
on finding the best configuration, but following 
(Tjong Kim Sang and Veenstra, 1999; Ratna- 
parkhi, 1997) I only used very local context. In 
these experiments I used a left context of three 
words and a right context of two words. Exper- 
iments described in (Mufioz et al, 1999) suc- 
cessfully used larger contexts, but the few tests 
that I performed to confirm this did not give 
evidence that we could benefit significantly by 
extending the context. Apart from information 
139 
given by the WSJ corpus, information generated 
by the model itself will also be used: 
? Chunk tags of previous words 
It would of course sometimes be desirable to use 
the chunk tags of the following words also, but 
these are not instantly available and therefore 
we will need a cascaded approach. I have exper- 
imented with a cascaded chunker, but I did not 
improve the results significantly. 
In order to use previously predicted chunk 
tags, the evaluation part of the Maccent soft- 
ware had to be modified. The evaluation pro- 
gram needs a previously created file with all 
the attributes and the actual class, but the 
chunk tag of the previous two words cannot 
be provided beforehand as they are produced 
in the process of evaluation. A ca~scaded ap- 
proach where after the first run the predicted 
tags are added to the file with test data is also 
not completely satisfactory as the provided tags 
are then predicted on basis of all the other at- 
tributes, but not the previous chunk tags. Ide- 
ally the information about the tags of the pre- 
vious words would be added during evaluation. 
This required some modification of the evalua- 
tion script. 
3.2 Results  
The experiments are evaluated using the follow- 
ing standard evaluation measures: 
? Tagging accuracy = 
Number of correct ta~6ed words 
Total number of words 
? Recall = 
Number of correct proposed baseNWs 
Number of correct baseNWs 
? Precision = 
Number of correct proposed baseNWs 
Number of proposed baseNWs 
? F~-score (f12 + 1)'Recall'Precisi?n 
~ fl2.Recall+Precision 
(fl=l in all experiments.) 
In all the experiments a left context of 3 words 
and a right context of 2 words was used. The 
part of speech tags of the surrounding words and 
the word itself were all used as atomic features. 
The lexical information used consisted of the 
previous word, the current word and the next 
word. The word W-2 was omitted because it 
did not seem to improve the model. Using only 
these atomic features, the model scored an tag- 
ging accuracy of about 95.5% and a F-score of 
about 90.5 %. Well below the reported results in 
the literature. Adding i~atures combining POS 
tags improved the results significantly to just 
below state of the art scores. Finally 2 complex 
features involving NP chunk tags predicted for 
previous words were added. The most successful 
set of features used in our experiments is given 
in figure 1. It is not claimed that this is the best 
Template 
TAG_3 
POS-3 
POS-3/POSo 
POS-3/POS_2 
TAG_3/TAG_2/TAG_i/POSo 
TAG-2 
POS-2 
W-1 Previous word 
TAG_I 
POS-1 
Wo Current word 
POSo 
W+i 
POS+i 
POS_2/POSo 
POS-2/POS_I 
TAG_2/TAG_i/POSo 
POS_~/TAG_~/POSo 
POS_i/POSo/POS+i 
POS-2/POS-1/POSo/POS+i 
POSo/POS+i 
POS+2 
POSo/POS+2 
POSo/POS+i/POS+2 
Meaning 
Base-NP tag of W-3 
Part of Speech tag of W-3 
Figure 1: Feature set-up for best scoring exper- 
iment 
set of features possible for this task. Trying new 
feature combinations, by adding them manually 
and testing the new configuration is a time con- 
suming and not very interesting activity. Es- 
pecially when the scores are close to the best 
published scores, adding new features have little 
impact on the behaviour of the model. An al- 
gorithm that discovers the interaction between 
features and suggests which features could be 
combined to improve the model would be very 
helpful here. I did not include any complex fea- 
tures involving lexical information. It might be 
140 
useful to include more features with lexical in- 
formation if more training data is available (for 
example the full R&M data set consisting of sec- 
tion 2-21 of WSJ). 
For feature selection a simple count cut-off 
was used. I experimented with several combi- 
nations of thresholds and the number of itera- 
tions used to train the model. When the thresh- 
old was set to 2, unique contexts (can be prob- 
lematic during training of the model; see (Rat- 
naparkhi, 1998)) did not occur very frequently 
anymore and an upper bound on the number of 
iterations did not seem to be necessary. It was 
found that (using a threshold of 2 for every sin- 
gle feature) after about 100 iterations the model 
did not improve very much anymore. Using the 
feature setup given in figure 1 a threshold of 2 
for all the features and allowing the model to 
train over 100 iterations, the scores given in ta- 
ble 1 were obtained. 
4 Conc lud ing  remarks  
The first observation that I would like to make 
here, is the fact that it was relatively easy to 
get results that are comparable with previously 
published results. Even though some improve- 
ment is to be expected when more detailed fea- 
tures, more context and/or more training data 
is used, it seems to be necessary to incorporate 
other sources of information to improve signifi- 
cantly on these results. 
Further, it is not satisfactory to find out what 
attribute combinations to use by trying new 
combinations and testing them. It might be 
worth to examine ways to automatically de- 
tect which feature combinations are promis- 
ing (Mikheev, forthcoming; Della Pietra et al, 
1997). 
Re ferences  
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A maximum entropy 
approach to natural anguage processing. Com- 
putational Linguistics, 22(1). 
Luc Dehaspe. 1997. Maximum entropy modeling 
with clausal constraints. In Proceedings of the 7th 
International Workshop on Inductive Logic Pro- 
gramming. 
S. Della Pietra, V. Della Pietra, and J. Laf- 
ferty. 1997. Inducing features from random fields. 
IEEE Transactions on Patterns Analysis and Ma- 
chine Intelligence, 19(4). 
test data 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
precision 
65.53 % 
78.98 % 
55.56 % 
50.00 % 
0.00 % 
93.18 % 
97.05 % 
58.49 % 
63.36 % 
93.22 % 
all 92.08 % 91.86 % 
recall Ff~=l 
75.33 % 70.09 
78.08 % 78.53 
45.45 % 50.00 
100.00 % 66.67 
0.00 % 0.00 
92.84 % 93.01 
93.60 % 95.30 
73.81% 65.26 
82.68 % 71.75 
92.54 % 92.88 
i 91.97 
Table 1: Results 
E.T. Jaynes. 1957. Information theory and statisti- 
cal mechanics. Physical Review, 108:171-190. 
Andrei Mikheev. forthcoming. Feature lattices and 
maximum entropy models. Journal of Machine 
Learning. 
Marcia Mufioz, Vasin Punyakanok, Dan Roth, and 
Dav Zimak. 1999. A learning approach to shallow 
parsing. In Proceedings of/EMNLP- WVLC'99. 
Adwait Ratnaparkhi. 1997. A linear observed time 
statistical parser based on maximum entropy 
models. In Proceedings of the Second Conference 
on Empirical Methods in Natural Language Pro- 
cessing, Brown University, Providence, Rhode Is- 
land. 
Adwait Ratnaparkhi. 1998. Maximum Entropy 
Models for Natural Language Ambiguity Resolu- 
tion. Ph.D. thesis, UPenn. 
Sven Eric Ristad. 1998. Maximum entropy mod- 
elling toolkit. Technical report. 
Ronald Rosenfeld. 1994. Adaptive Statistical Lan- 
guage Modelling: A Maximum Entropy Approach. 
Ph.D. thesis, Carnegy Mellon University. 
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. 
Strategy and tactics: a model for language pro- 
duction. In Ninth Conference of the European 
Chapter of the Association for Computational 
Linguistics, University of Bergen, Bergen, Nor- 
way. 
141 
Learning Computational Grammars
John Nerbonne   , Anja Belz  , Nicola Cancedda  , Herve? De?jean  ,
James Hammerton

, Rob Koeling

, Stasinos Konstantopoulos   ,
Miles Osborne   , Franck Thollard

and Erik Tjong Kim Sang 
Abstract
This paper reports on the LEARNING
COMPUTATIONAL GRAMMARS (LCG)
project, a postdoc network devoted to
studying the application of machine
learning techniques to grammars suit-
able for computational use. We were in-
terested in a more systematic survey to
understand the relevance of many fac-
tors to the success of learning, esp. the
availability of annotated data, the kind
of dependencies in the data, and the
availability of knowledge bases (gram-
mars). We focused on syntax, esp. noun
phrase (NP) syntax.
1 Introduction
This paper reports on the still preliminary, but al-
ready satisfying results of the LEARNING COM-
PUTATIONAL GRAMMARS (LCG) project, a post-
doc network devoted to studying the application
of machine learning techniques to grammars suit-
able for computational use. The member insti-
tutes are listed with the authors and also included
ISSCO at the University of Geneva. We were im-
pressed by early experiments applying learning
to natural language, but dissatisfied with the con-
centration on a few techniques from the very rich
area of machine learning. We were interested in

University of Groningen,  nerbonne,konstant  @let.
rug.nl, osborne@cogsci.ed.ac.uk
	
SRI Cambridge, anja.belz@cam.sri.com, Rob.Koe-
ling@netdecisions.co.uk

XRCE Grenoble, nicola.cancedda@xrce.xerox.com

University of Tu?bingen, Herve.Dejean@xrce.xerox.
com, thollard@sfs.nphil.uni-tuebingen.de

University College Dublin, james.hammerton@ucd.ie

University of Antwerp, erikt@uia.ua.ac.be
a more systematic survey to understand the rele-
vance of many factors to the success of learning,
esp. the availability of annotated data, the kind
of dependencies in the data, and the availability
of knowledge bases (grammars). We focused on
syntax, esp. noun phrase (NP) syntax from the
beginning. The industrial partner, Xerox, focused
on more immediate applications (Cancedda and
Samuelsson, 2000).
The network was focused not only by its sci-
entific goal, the application and evaluation of
machine-learning techniques as used to learn nat-
ural language syntax, and by the subarea of syn-
tax chosen, NP syntax, but also by the use of
shared training and test material, in this case ma-
terial drawn from the Penn Treebank. Finally, we
were curious about the possibility of combining
different techniques, including those from statisti-
cal and symbolic machine learning. The network
members played an important role in the organi-
sation of three open workshops in which several
external groups participated, sharing data and test
materials.
2 Method
This section starts with a description of the three
tasks that we have worked on in the framework of
this project. After this we will describe the ma-
chine learning algorithms applied to this data and
conclude with some notes about combining dif-
ferent system results.
2.1 Task descriptions
In the framework of this project, we have worked
on the following three tasks:
1. base phrase (chunk) identification
2. base noun phrase recognition
3. finding arbitrary noun phrases
Text chunks are non-overlapping phrases which
contain syntactically related words. For example,
the sentence:
 
He 
 
reckons 
 
the current
account deficit 
 
will narrow 
 
to 
 
only  1.8 billion 
 
in 
 
September  .
contains eight chunks, four NP chunks, two VP
chunks and two PP chunks. The latter only con-
tain prepositions rather than prepositions plus the
noun phrase material because that has already
been included in NP chunks. The process of
finding these phrases is called CHUNKING. The
project provided a data set for this task at the
CoNLL-2000 workshop (Tjong Kim Sang and
Buchholz, 2000)1. It consists of sections 15-18 of
the Wall Street Journal part of the Penn Treebank
II (Marcus et al, 1993) as training data (211727
tokens) and section 20 as test data (47377 tokens).
A specialised version of the chunking task is
NP CHUNKING or baseNP identification in which
the goal is to identify the base noun phrases. The
first work on this topic was done back in the
eighties (Church, 1988). The data set that has
become standard for evaluation machine learn-
ing approaches is the one first used by Ramshaw
and Marcus (1995). It consists of the same train-
ing and test data segments of the Penn Treebank
as the chunking task (respectively sections 15-18
and section 20). However, since the data sets
have been generated with different software, the
NP boundaries in the NP chunking data sets are
slightly different from the NP boundaries in the
general chunking data.
Noun phrases are not restricted to the base lev-
els of parse trees. For example, in the sentence In
early trading in Hong Kong Monday , gold was
quoted at $ 366.50 an ounce ., the noun phrase
  $ 366.50 an ounce  contains two embedded
noun phrases
  $ 366.50  and   an ounce  .
In the NP BRACKETING task, the goal is to find
all noun phrases in a sentence. Data sets for this
task were defined for CoNLL-992. The data con-
sist of the same segments of the Penn Treebank as
1Detailed information about chunking, the CoNLL-
2000 shared task, is also available at http://lcg-
www.uia.ac.be/conll2000/chunking/
2Information about NP bracketing can be found at
http://lcg-www.uia.ac.be/conll99/npb/
the previous two tasks (sections 15-18) as train-
ing material and section 20 as test material. This
material was extracted directly from the Treebank
and therefore the NP boundaries at base levels are
different from those in the previous two tasks.
In the evaluation of all three tasks, the accu-
racy of the learners is measured with three rates.
We compare the constituents postulated by the
learners with those marked as correct by experts
(gold standard). First, the percentage of detected
constituents that are correct (precision). Second,
the percentage of correct constituents that are de-
tected (recall). And third, a combination of pre-
cision and recall, the F ffAutomatic Identification of Infrequent Word Senses
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In this paper we show that an unsupervised method for
ranking word senses automatically can be used to iden-
tify infrequently occurring senses. We demonstrate this
using a ranking of noun senses derived from the BNC
and evaluating on the sense-tagged text available in both
SemCor and the SENSEVAL-2 English all-words task.
We show that the method does well at identifying senses
that do not occur in a corpus, and that those that are erro-
neously filtered but do occur typically have a lower fre-
quency than the other senses. This method should be
useful for word sense disambiguation systems, allowing
effort to be concentrated on more frequent senses; it may
also be useful for other tasks such as lexical acquisition.
Whilst the results on balanced corpora are promising, our
chief motivation for the method is for application to do-
main specific text. For text within a particular domain
many senses from a generic inventory will be rare, and
possibly redundant. Since a large domain specific cor-
pus of sense annotated data is not available, we evaluate
our method on domain-specific corpora and demonstrate
that sense types identified for removal are predominantly
senses from outside the domain.
1 Introduction
Much about the behaviour of words is most appro-
priately expressed in terms of word senses rather
than word forms. However, an NLP application
computing over word senses is faced with consid-
erable extra ambiguity. There are systems which
can perform word sense disambiguation (WSD) on
the words in input text, however there is room for
improvement since the best systems on the English
SENSEVAL-2 all-words task obtained at most 69%
for precision and recall. Whilst there are systems
that obtain higher precision (Magnini et al, 2001),
these typically suffer from a low recall. WSD per-
formance is affected by the degree of polysemy, but
even more so by the entropy of the frequency distri-
butions of the words? senses (Kilgarriff and Rosen-
zweig, 2000) since the distribution for many words
is highly skewed. Many of the senses in such an
inventory are rare and WSD and lexical acquisition
systems do best when they take this into account.
There are many ways that the skewed distribution
can be taken into account. One successful approach
is to back-off to the first (predominant) sense (Wilks
and Stevenson, 1998; Hoste et al, 2001). Another
possibility would be concentrate the selection pro-
cess to senses with higher frequency, and filter out
rare senses. This is implicitly done by systems
which rely on hand-tagged training corpora, since
rare senses often do not occur in the available data.
In this paper we use an unsupervised method to rank
word senses from an inventory according to preva-
lence (McCarthy et al, 2004a), and utilise the rank-
ing scores to identify senses which are rare. We use
WordNet for our inventory, since it is widely used
and freely available, but our method could in prin-
ciple be used with another MRD (we comment on
this in the conclusions). We report work with nouns
here, and leave evaluation on other PoS for the fu-
ture.
Our approach exploits automatically acquired
thesauruses which provide ?nearest neighbours? for
a given word entry. The neighbours are ordered
in terms of the distributional similarity that they
share with the target word. The neighbours relate
to different senses of the target word, so for exam-
ple the word competition in such a thesaurus pro-
vided by Lin 1 has neighbours tournament, event,
championship and then further down the ordered list
we see neighbours pertaining to a different sense
competitor,...market...price war. Pantel and Lin
(2002) demonstrate that it is possible to cluster the
neighbours into senses and relate these to WordNet
senses. In contrast, we use the distributional sim-
ilarity scores of the neighbours to rank the various
senses of the target word since we expect that the
quantity and similarity of the neighbours pertain-
ing to different senses will reflect the relative dom-
inance of the senses. This is because there will
1Available from
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
be more data for the more prevalent senses com-
pared to the less frequent senses. We use a measure
of semantic similarity from the WordNet Similarity
package to relate the senses of the target word to the
neighbours in the thesaurus.
The paper is structured as follows. The ranking
method is described elsewhere (McCarthy et al,
2004a), but we summarise in the following section
and describe how ranking scores can be used for fil-
tering word senses. Section 3 describes two exper-
iments using the BNC for acquisition of the sense
rankings with evaluation using the hand-tagged data
in i) SemCor and ii) the English SENSEVAL-2 all-
words task. We demonstrate that the majority of
senses identified by the method do not occur in these
gold-standards, and that for those that do, only a
small percentage of the sense tokens would be re-
moved in error by filtering these senses. In section 4
we use domain labels produced by (Magnini and
Cavaglia`, 2000) to demonstrate differences in the
senses filtered for a sample of words in two domain
specific corpora. We describe some related work in
section 5 and conclude in section 6.
2 Method
McCarthy et al (2004a) describe a method to pro-
duce a ranking over senses and find the predominant
sense of a word just using raw text. We summarise
the method below, and describe how we use it for
identifying candidate senses for filtering.
2.1 Ranking the Senses
In order to rank the senses of a target word (e.g.
plant) we use a thesaurus acquired from automati-
cally parsed text (section 2.2 below). This provides
the  nearest neighbours to each target word (e.g.
factory, refinery, tree etc...) along with the distribu-
tional similarity score between the target word and
its neighbour. We then use the WordNet similar-
ity package (Patwardhan and Pedersen, 2003) (see
section 2.3) to give us a semantic similarity mea-
sure (hereafter referred to as the WordNet similarity
measure) to weight the contribution that each neigh-
bour (e.g. factory) makes to the various senses of
the target word (e.g. flora, industrial, actor etc...).
We take each sense of the target word (  ) in turn
and obtain a score reflecting the prevalence which is
used for ranking. Let 
	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 419?426, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Domain-Specific Sense Distributions and Predominant Sense Acquisition
Rob Koeling & Diana McCarthy & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
robk,dianam,johnca  @sussex.ac.uk
Abstract
Distributions of the senses of words are
often highly skewed. This fact is exploited
by word sense disambiguation (WSD) sys-
tems which back off to the predominant
sense of a word when contextual clues are
not strong enough. The domain of a doc-
ument has a strong influence on the sense
distribution of words, but it is not feasi-
ble to produce large manually annotated
corpora for every domain of interest. In
this paper we describe the construction of
three sense annotated corpora in different
domains for a sample of English words.
We apply an existing method for acquir-
ing predominant sense information auto-
matically from raw text, and for our sam-
ple demonstrate that (1) acquiring such
information automatically from a mixed-
domain corpus is more accurate than de-
riving it from SemCor, and (2) acquiring
it automatically from text in the same do-
main as the target domain performs best
by a large margin. We also show that
for an all words WSD task this automatic
method is best focussed on words that are
salient to the domain, and on words with
a different acquired predominant sense in
that domain compared to that acquired
from a balanced corpus.
1 Introduction
From analysis of manually sense tagged corpora,
Kilgarriff (2004) has demonstrated that distributions
of the senses of words are often highly skewed. Most
researchers working on word sense disambiguation
(WSD) use manually sense tagged data such as Sem-
Cor (Miller et al, 1993) to train statistical classi-
fiers, but also use the information in SemCor on the
overall sense distribution for each word as a back-
off model. In WSD, the heuristic of just choosing the
most frequent sense of a word is very powerful, es-
pecially for words with highly skewed sense distri-
butions (Yarowsky and Florian, 2002). Indeed, only
5 out of the 26 systems in the recent SENSEVAL-3
English all words task (Snyder and Palmer, 2004)
outperformed the heuristic of choosing the most fre-
quent sense as derived from SemCor (which would
give 61.5% precision and recall1). Furthermore, sys-
tems that did outperform the first sense heuristic did
so only by a small margin (the top score being 65%
precision and recall).
Over a decade ago, Gale et al (1992) observed
the tendency for one sense of a word to prevail in a
given discourse. To take advantage of this, a method
for automatically determining the ?one sense? given
a discourse or document is required. Magnini et al
(2002) have shown that information about the do-
main of a document is very useful for WSD. This is
because many concepts are specific to particular do-
mains, and for many words their most likely mean-
ing in context is strongly correlated to the domain of
the document they appear in. Thus, since word sense
distributions are skewed and depend on the domain
at hand we would like to know for each domain of
application the most likely sense of a word.
However, there are no extant domain-specific
sense tagged corpora to derive such sense distribu-
tion information from. Producing them would be ex-
tremely costly, since a substantial corpus would have
to be annotated by hand for every domain of interest.
In response to this problem, McCarthy et al (2004)
proposed a method for automatically inducing the
1This figure is the mean of two different estimates (Sny-
der and Palmer, 2004), the difference being due to multiword
handling.
419
predominant sense of a word from raw text. They
carried out a limited test of their method on text in
two domains using subject field codes (Magnini and
Cavaglia`, 2000) to assess whether the acquired pre-
dominant sense information was broadly consistent
with the domain of the text it was acquired from.
But they did not evaluate their method on hand-
tagged domain-specific corpora since there was no
such data publicly available.
In this paper, we evaluate the method on domain
specific text by creating a sense-annotated gold stan-
dard2 for a sample of words. We used a lexical sam-
ple because the cost of hand tagging several corpora
for an all-words task would be prohibitive. We show
that the sense distributions of words in this lexical
sample differ depending on domain. We also show
that sense distributions are more skewed in domain-
specific text. Using McCarthy et al?s method, we
automatically acquire predominant sense informa-
tion for the lexical sample from the (raw) corpora,
and evaluate the accuracy of this and predominant
sense information derived from SemCor. We show
that in our domains and for these words, first sense
information automatically acquired from a general
corpus is more accurate than first senses derived
from SemCor. We also show that deriving first sense
information from text in the same domain as the tar-
get data performs best, particularly when focusing
on words which are salient to that domain.
The paper is structured as follows. In section 2
we summarise McCarthy et al?s predominant sense
method. We then (section 3) describe the new gold
standard corpora, and evaluate predominant sense
accuracy (section 4). We discuss the results with
a proposal for applying the method to an all-words
task, and an analysis of our results in terms of this
proposal before concluding with future directions.
2 Finding Predominant Senses
We use the method described in McCarthy et al
(2004) for finding predominant senses from raw
text. The method uses a thesaurus obtained from
the text by parsing, extracting grammatical relations
and then listing each word (  ) with its top  nearest
neighbours, where  is a constant. Like McCarthy
2This resource will be made publicly available for research
purposes in the near future.
et al (2004) we use 	 and obtain our thesaurus
using the distributional similarity metric described
by Lin (1998). We use WordNet (WN) as our sense
inventory. The senses of a word  are each assigned
a ranking score which sums over the distributional
similarity scores of the neighbours and weights each
neighbour?s score by a WN Similarity score (Pat-
wardhan and Pedersen, 2003) between the sense of
 and the sense of the neighbour that maximises the
WN Similarity score. This weight is normalised by
the sum of such WN similarity scores between all
senses of  and and the senses of the neighbour that
maximises this score. We use the WN Similarity jcn
score (Jiang and Conrath, 1997) since this gave rea-
sonable results for McCarthy et al and it is efficient
at run time given precompilation of frequency infor-
mation. The jcn measure needs word frequency in-
formation, which we obtained from the British Na-
tional Corpus (BNC) (Leech, 1992). The distribu-
tional thesaurus was constructed using subject, di-
rect object adjective modifier and noun modifier re-
lations.
3 Creating the Three Gold Standards
In our experiments, we compare for a sample
of nouns the sense rankings created from a bal-
anced corpus (the BNC) with rankings created from
domain-specific corpora (FINANCE and SPORTS)
extracted from the Reuters corpus (Rose et al,
2002). In more detail, the three corpora are:
BNC: The ?written? documents, amounting to 3209
documents (around 89.7M words), and covering a
wide range of topic domains.
FINANCE: 117734 FINANCE documents (around
32.5M words) topic codes: ECAT and MCAT
SPORTS: 35317 SPORTS documents (around 9.1M
words) topic code: GSPO
We computed thesauruses for each of these corpora
using the procedure outlined in section 2.
3.1 Word Selection
In our experiments we used FINANCE and SPORTS
domains. To ensure that a significant number of
the chosen words are relevant for these domains,
we did not choose the words for our experiments
completely randomly. The first selection criterion
we applied used the Subject Field Code (SFC) re-
420
source (Magnini and Cavaglia`, 2000), which assigns
domain labels to synsets in WN version 1.6. We se-
lected all the polysemous nouns in WN 1.6 that have
at least one synset labelled SPORT and one synset
labelled FINANCE. This reduced the set of words
to 38. However, some of these words were fairly
obscure, did not occur frequently enough in one of
the domain corpora or were simply too polysemous.
We narrowed down the set of words using the crite-
ria: (1) frequency in the BNC 
 1000, (2) at most
12 senses, and (3) at least 75 examples in each cor-
pus. Finally a couple of words were removed be-
cause the domain-specific sense was particularly ob-
scure3. The resulting set consists of 17 words4: club,
manager, record, right, bill, check, competition, con-
version, crew, delivery, division, fishing, reserve, re-
turn, score, receiver, running
We refer to this set of words as F&S cds. The first
four words occur in the BNC with high frequency ( 

10000 occurrences), the last two with low frequency
(  2000) and the rest are mid-frequency.
Three further sets of words were selected on the
basis of domain salience. We chose eight words that
are particularly salient in the Sport corpus (referred
to as S sal), eight in the Finance corpus (F sal), and
seven that had equal (not necessarily high) salience
in both, (eq sal). We computed salience as a ratio of
normalised document frequencies, using the formula

	Gloss-Based Semantic Similarity Metrics for Predominant Sense Acquisition
Ryu Iida
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
ryu-i@is.naist.jp
Diana McCarthy and Rob Koeling
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
{dianam,robk}@sussex.ac.uk
Abstract
In recent years there have been various ap-
proaches aimed at automatic acquisition of
predominant senses of words. This infor-
mation can be exploited as a powerful back-
off strategy for word sense disambiguation
given the zipfian distribution of word senses.
Approaches which do not require manually
sense-tagged data have been proposed for
English exploiting lexical resources avail-
able, notably WordNet. In these approaches
distributional similarity is coupled with a se-
mantic similarity measure which ties the dis-
tributionally related words to the sense in-
ventory. The semantic similarity measures
that have been used have all taken advantage
of the hierarchical information in WordNet.
We investigate the applicability to Japanese
and demonstrate the feasibility of a mea-
sure which uses only information in the dic-
tionary definitions, in contrast with previ-
ous work on English which uses hierarchi-
cal information in addition to dictionary def-
initions. We extend the definition based
semantic similarity measure with distribu-
tional similarity applied to the words in dif-
ferent definitions. This increases the recall
of our method and in some cases, precision
as well.
1 Introduction
Word sense disambiguation (WSD) has been an ac-
tive area of research over the last decade because
many researches believe it will be important for
applications which require, or would benefit from,
some degree of semantic interpretation. There has
been considerable skepticism over whether WSD
will actually improve performance of applications,
but we are now starting to see improvement in per-
formance due to WSD in cross-lingual information
retrieval (Clough and Stevenson, 2004; Vossen et
al., 2006) and machine translation (Carpuat and Wu,
2007; Chan et al, 2007) and we hope that other ap-
plications such as question-answering, text simplifi-
cation and summarisation might also benefit as WSD
methods improve.
In addition to contextual evidence, most WSD sys-
tems exploit information on the most likely mean-
ing of a word regardless of context. This is a pow-
erful back-off strategy given the skewed nature of
word sense distributions. For example, in the En-
glish coarse grained all words task (Navigli et al,
2007) at the recent SemEval Workshop the base-
line of choosing the most frequent sense using the
first WordNet sense attained precision and recall of
78.9%which is only a few percent lower than the top
scoring system which obtained 82.5%. This finding
is in line with previous results (Snyder and Palmer,
2004). Systems using a first sense heuristic have
relied on sense-tagged data or lexicographer judg-
ment as to which is the predominant sense of a word.
However sense-tagged data is expensive and further-
more the predominant sense of a word will vary de-
pending on the domain (Koeling et al, 2005; Chan
and Ng, 2007).
One direction of research following McCarthy et
al. (2004) has been to learn the most predominant
561
sense of a word automatically. McCarthy et als
method relies on two methods of similarity. Firstly,
distributional similarity is used to estimate the pre-
dominance of a sense from the number of distribu-
tionally similar words and the strength of their dis-
tributional similarity to the target word. This is done
on the premise that more prevalent meanings have
more evidence in the corpus data used for the distri-
butional similarity calculations and the distribution-
ally similar words (nearest neighbours) to a target
reflect the more predominant meanings as a conse-
quence. Secondly, the senses in the sense inventory
are linked to the nearest neighbours using semantic
similarity which incorporates information from the
sense inventory. It is this semantic similarity mea-
sure which is the focus of our paper in the context of
the method for acquiring predominant senses.
Whilst the McCarthy et al?s method works well
for English, other inventories do not always have
WordNet style resources to tie the nearest neigh-
bours to the sense inventory. WordNet has many se-
mantic relations as well as glosses associated with
its synsets (near synonym sets). While traditional
dictionaries do not organise senses into synsets, they
do typically have sense definitions associated with
the senses. McCarthy et al (2004) suggest that dic-
tionary definitions can be used with their method,
however in the implementation of the measure based
on dictionary definitions that they use, the dictionary
definitions are extended to those of related words us-
ing the hierarchical structure of WordNet (Banerjee
and Pedersen, 2002). This extension to the original
method (Lesk, 1986) was proposed because there is
not always sufficient overlap of the individual words
for which semantic similarity is being computed. In
this paper we refer to the original method (Lesk,
1986) as lesk and the extended measure proposed
by Banerjee and Pedersen as Elesk.
This paper investigates the potential of using
the overlap of dictionary definitions with the Mc-
Carthy et al?s method. We test the method for
obtaining a first sense heuristic using two publicly
available datasets of sense-tagged data in Japanese,
EDR (NICT, 2002) and the SENSEVAL-2 Japanese
dictionary task (Shirai, 2001). We contrast an imple-
mentation of lesk (Lesk, 1986) which uses only dic-
tionary definitions with the Jiang-Conrath measure
(jcn) (Jiang and Conrath, 1997) which uses man-
ually produced hyponym links and was used pre-
viously for this purpose on English datasets (Mc-
Carthy et al, 2004). The jcn measure is only ap-
plicable to the EDR dataset because the dictionary
has hyponymy links which are not available in the
SENSEVAL-2 Japanese dictionary task. We also pro-
pose a new extension to lesk which does not require
hand-crafted hyponym links but instead uses distri-
butional similarity to increase the possibilities for
overlap of the word definitions. We refer to this new
measure as DSlesk. We compare this to the original
lesk on both datasets and show that it increases re-
call, and sometimes precision too whilst not requir-
ing hyponym links.
In the next section we place our contribution in re-
lation to previous work. In section 3 we summarise
the methods we adopt from previous work, and de-
scribe our proposal for a semantic similarity method
that can supplement the information from dictionary
definitions with information from raw text. In sec-
tion 4 we describe the experiments on EDR and the
SENSEVAL-2 Japanese dictionary task and we con-
clude in section 5.
2 Related Work
This work builds upon that of McCarthy et al (2004)
which acquires predominant senses for target words
from a large sample of text using distributional sim-
ilarity (Lin, 1998) to provide evidence for predomi-
nance. The evidence from the distributional similar-
ity is allocated to the senses using semantic similar-
ity fromWordNet (Patwardhan and Pedersen, 2003).
We will describe the method more fully below in
section 3. McCarthy et al (2004) reported results
for English using their automatically acquired first
sense heuristic on SemCor (Miller et al, 1993) and
the SENSEVAL-2 English all words dataset (Sny-
der and Palmer, 2004). The results from this are
promising, given that hand-labelled data is not re-
quired. On polysemous nouns from SemCor they
obtained 48% WSD using their method with Elesk
and 46% with jcn where the random baseline was
24% and the upper-bound was 67% (derived from
the SemCor test data itself). On SENSEVAL-2 all
words dataset using the jcn measure 1 they obtained
63% recall which is encouraging compared to the
1They did not apply lesk to this dataset.
562
SemCor heuristic which obtained 68% but requires
hand-labelled data. The upper-bound on the dataset
was 72% from the test data itself. These results cru-
cially depend on the information in the sense inven-
tory WordNet. WordNet contains hierarchical rela-
tions between word senses which are used in both
jcn and Elesk. There is an issue that such infor-
mation may not be available in other sense invento-
ries, and other inventories will be needed for other
languages. In this paper, we implement the lesk se-
mantic similarity (Lesk, 1986) for the two Japanese
lexicons used in our test datasets, i) the EDR dic-
tionary (NICT, 2002) ii) the Iwanami Kokugo Jiten
Dictionary (Nishio et al, 1994). We investigate the
potential of lesk and jcn, where the latter is applica-
ble. In addition to implementing the original lesk
measure, we propose an extension to the method
inspired by Mihalcea et al (2006). Mihalcea et
al. (2006) used various text based similarity mea-
sures, including WordNet and corpus based similar-
ity methods, to determine if two phrases are para-
phrases. They contrasted this approach with previ-
ous methods which used overlap of the words be-
tween the candidate paraphrases. For each word in
each of the two texts they obtain the maximum sim-
ilarity between the word and any of the words from
the putative paraphrase. The similarity scores for
each word of both phrases contribute to an overall
semantic similarity between 0 and 1 and a threshold
of 0.5 is used to decide if the candidate phrases are
paraphrases. In our work, we compare glosses of
words senses (senses of the target word and senses
of the nearest neighbour) rather than paraphrases. In
this approach we extend the definition overlap by
considering the distributional similarity (Lin, 1998)
rather than identify of the words in the two defini-
tions.
In addition to McCarthy et al (2004) there are
other approaches to finding predominant senses.
Chan and Ng (2005) use parallel data to provide
estimates for sense frequency distributions to feed
into a supervised WSD system. Mohammad and
Hirst (2006) propose an approach to acquiring pre-
dominant senses from corpora which makes use
of the category information in the Macquarie The-
saurus (Barnard, 1986). Lexical chains (Galley and
McKeown, 2003) may also provide a useful first
sense heuristic (Brody et al, 2006) but are produced
usingWordNet relations. We use theMcCarthy et al
approach because this is applicable without aligned
corpus data, semantic category and relation informa-
tion and is applicable to any language assuming the
minimum requirements of i) dictionary definitions
associated with the sense inventory and ii) raw cor-
pus data. We adapt their technique to remove the
reliance on hyponym links.
3 Gloss-based semantic similarity
We first summarise the McCarthy et al method
and the WordNet based semantic similarity func-
tions (jcn and Elesk) that they use for automatic
acquisition of a first sense heuristic applied to dis-
ambiguation of English WordNet datasets. We then
describe the additional semantic similarity method
that we propose for comparison with lesk and jcn.
McCarthy et al use a distributional similarity the-
saurus acquired from corpus data using the method
of Lin (1998) for finding the predominant sense of
a word where the senses are defined by WordNet.
The thesaurus provides the k nearest neighbours to
each target word, along with the distributional sim-
ilarity score between the target word and its neigh-
bour. The WordNet similarity package (Patwardhan
and Pedersen, 2003) is used to weight the contribu-
tion that each neighbour makes to the various senses
of the target word.
Let w be a target word and Nw = {n1,n2...nk}
be the ordered set of the top scoring k
neighbours of w from the thesaurus with
associated distributional similarity scores
{dss(w,n1),dss(w,n2), ...dss(w,nk)} using (Lin,
1998). Let senses(w) be the set of senses of w
for each sense of w (wsi ? senses(w)) a ranking is
obtained using:
Prevalence Score(wsi) =
?
n j?Nw
dss(w,n j)?
wnss(wsi,n j)
?wsi??senses(w)wnss(wsi? ,n j)
(1)
where wnss is the maximum WordNet similarity
score between wsi and the WordNet sense of the
neighbour (n j) that maximises this score. McCarthy
et al compare two different WordNet similarity
scores, jcn and Elesk.
jcn (Jiang and Conrath, 1997) uses corpus data
to estimate a frequency distribution over the classes
563
(synsets) in the WordNet hierarchy. Each synset, is
incremented with the frequency counts from the cor-
pus of all words belonging to that synset, directly or
via the hyponymy relation. The frequency data is
used to calculate the ?information content? (IC) of a
class or sense (s):
IC(s) =?log(p(s))
Jiang and Conrath specify a distance measure be-
tween two senses (s1,s2):
D jcn(s1,s2) = IC(s1)+ IC(s2)?2? IC(s3)
where the third class (s3) is the most informative, or
most specific, superordinate synset of the two senses
s1 and s2. This is transformed from a distance mea-
sure in the WordNet Similarity package by taking
the reciprocal:
jcn(s1,s2) = 1/D jcn(s1,s2)
McCarthy et al use the above measure with wsi
as s1 and whichever sense of the neigbour (n j) that
maximises this WordNet similarity score.
Elesk (Banerjee and Pedersen, 2002) extends the
original lesk algorithm (Lesk, 1986) so we describe
that original algorithm lesk first. This simply cal-
culates the overlap of the content words in the defi-
nitions, frequently referred to as glosses, of the two
word senses.
lesk(s1,s2) =
?
a?g1
member(a,g2)
member(a,g2) =
{
1 if a appears in g2
0 otherwise
where g1 is the gloss of word sense s1, g2 is the gloss
of s2 and a is one of words appearing in g1. In Elesk
which McCarthy et al use the measure is extended
by considering related synsets to s1 and s2, again
where s1 is wsi and s2 is the sense from all senses
of n j that maximises the Elesk WordNet similar-
ity score. Elesk relies heavily on the relationships
that are encoded in WordNet such as hyponymy and
meronymy. Not all languages have resources sup-
plied with these relations, and where they are sup-
plied there may not be as much detail as there is in
WordNet.
In this paper we will examine the use of jcn and
the original lesk in Japanese on the EDR dataset
to see how well the pure definition based measure
fares compared to one using hyponym links. EDR
has hyponym links so we can make this comparison.
The performance of jcn will depend on the coverage
of the hyponym links. For lesk meanwhile there is
an issue that using only overlap of sense definitions
may give poor results because the sense definitions
are usually succinct and the overlap of words may
be low. For example, given the glosses for the words
pigeon and bird:2
pigeon: a fat grey and white bird with
short legs.
bird: a creature that is covered with feath-
ers and has wings and two legs.
If only content words are considered then there
is only one word (leg) which overlaps in the two
glosses, so the resultant lesk score is low (1) even
though the word pigeon is intuitively similar to bird.
The Elesk extension addressed this issue using
WordNet relations to extend the definitions over
which the overlap is calculated for a given pair of
senses. We propose addressing the same issue us-
ing corpus data to supplement the lesk overlap mea-
sure. We propose using distributional similarity (us-
ing (Lin, 1998)) as an approximation of semantic
distance between the words in the two glosses, rather
than requiring an exact match. We refer to this mea-
sure as DSlesk as defined:
DSlesk(s1,s2) = 1
|a ? g1| ?a?g1
max
b?g2
dss(a,b) (2)
where g1 is the gloss of word sense s1, g2 is the gloss
of s2, again s1 is the target word sense wsi in equa-
tion 1 for which we are obtaining the predominance
ranking score and s2 is whichever sense of the neigh-
bour (n j) in equation 1 which maximises this seman-
tic similarity score, as McCarthy et al did with the
wnss in equation 1. a (b) is a word appearing in g1
(g2).
In the calculation of equation (2), we first extract
the most similar word b from g2 to each word (a) in
2These two glosses are defined in OXFORD Advanced
Learner?s Dictionary.
564
dss(bird,creature) = 0.84, dss(bird, f eather) = 0.77,
dss(bird,wing) = 0.55, dss(bird, leg) = 0.43,
dss(leg,creature) = 0.56, dss(leg, f eather) = 0.66,
dss(leg,wing) = 0.74, dss(leg, leg) = 1.00
Figure 1: Examples of distributional similarity
the gloss of s1. We then output the average of the
maximum distributional similarity of all the words
in g1 to any of the words in g2 as the similarity score
between s1 and s2. We acknowledge that DSlesk is
not symmetrical since it depends on the number of
words in the gloss of s1, but not s2. Also our sum-
mation is over these words in s1 and we are not look-
ing for identity but maximum distributional similar-
ity with any of the words in g2 so the summation
will not give the same result as if we did the sum-
mation over the words in g2. It is perfectly reason-
able to have a semantic similarity measure which is
not symmetrical. One may want a measure where
a more specific sense, such as the meat sense of
chicken is closer to the ?animal flesh used as food?
sense of meat than vice versa. We do not believe
that this asymmetry is problematic for our applica-
tion as all the senses of w which we are ranking are
all treated equally with respect to the neighbour n,
and the ranking measure is concerned with finding
evidence for the meaning of w, which we do by fo-
cusing on its definitions, and not the meaning of n.
It would however be worthwhile investigating sym-
metrical versions of the score in the future.
Here is an example given the definitions of bird
and pigeon above and the distributional similarity
scores of all combinations of the two nouns as shown
in Figure 1. In this case, the similarity is estimated
as 1/2(0.84+1.00) = 0.92.
4 Experiments
To investigate how well the McCarthy et al method
ports to other language, we conduct empirical eval-
uation of word sense disambiguation by using the
two available sense-tagged datasets, EDR and the
SENSEVAL-2 Japanese dictionary task. In the ex-
periments, we compare the three semantic similari-
ties, jcn, lesk and DSlesk3, for use in the method to
3Elesk can be used when several semantic relations such as
hypnoymy and meronomy are available. However, we cannot
directly apply Elesk as it was used in (McCarthy et al, 2004) to
find the most likely sense in the set of word senses
defined in each inventory following the approach
of McCarthy et al (2004). For the thesaurus con-
struction we used <verb, case, noun> triplets ex-
tracted from Japanese newspaper articles (9 years of
the Mainichi Shinbun (1991-1999) and 10 years of
the Nihon Keizai Shinbun (1991-2000)) and parsed
by CaboCha (Kudo and Matsumoto, 2002). This re-
sulted in 53 million triplet instances for acquiring
the distributional thesaurus. We adopt the similarity
score proposed by Lin (1998) as the distributional
similarity score and use 50 nearest neighbours in
line with McCarthy et al
For the random baseline we select one word sense
at random for each word token and average the pre-
cision over 100 trials. For contrast with a supervised
approach we show the performance if we use hand-
labelled training data for obtaining the predominant
sense of the test words. This method usually outper-
forms an automatic approach, but crucially relies on
there being hand-labelled data which is expensive to
produce. The method cannot be applied where there
is no hand-labelled training data, it will be unreli-
able for low frequency data and a general dataset
may not be applicable when one moves to domain
specific text (Koeling et al, 2005). Since we are
not using context for disambiguation, but just a first
sense heuristic, we also give the upper-bound which
is the first sense heuristic calculated from the test
data itself.
4.1 EDR
We conduct empirical evaluation using 3,836 poly-
semous nouns in the sense-tagged corpus provided
with EDR (183,502 instances) where the glosses are
defined in the EDR dictionary. We evaluated on this
dataset using WSD precision and recall of this corpus
using only our first-sense heuristic (no context). The
results are shown in Table 1. The WSD performance
of all the automatic methods is much lower than the
supervised method, however, the main point of this
paper is to compare the McCarthy et al method for
finding a first sense in Japanese using jcn, lesk and
our experiments because the meronomy relation is not defined
in the EDR dictionary. In the experiments reported here we fo-
cus on the comparison of the three similarity measures jcn, lesk
and DSlesk for use in the method to determine the predomi-
nant sense of each word. We leave further exploration of other
adaptations of semantic similarity scores for future work.
565
Table 1: Results of EDR
recall precision
baseline 0.402 0.402
jcn 0.495 0.495
lesk 0.474 0.488
DSlesk 0.495 0.495
upper-bound 0.745 0.745
supervised 0.731 0.731
Table 2: Precision on EDR at low frequencies
all freq ? 10 freq ? 5
baseline 0.402 0.405 0.402
jcn 0.495 0.445 0.431
lesk 0.474 0.448 0.426
DSlesk 0.495 0.453 0.433
upper-bound 0.745 0.674 0.639
supervised 0.731 0.519 0.367
DSlesk. Table 1 shows that DSlesk is comparable to
jcn without the requirement for semantic relations
such as hyponymy.
Furthermore, we evaluate precision of each
method at low frequencies of words (? 10, ? 5),
shown in Table 2. Table 2 shows that all methods for
finding a predominant sense outperform the super-
vised one for items with little data (? 5), indicating
that these methods robustly work even for low fre-
quency data where hand-tagged data is unreliable.
Whilst the results are significantly different to the
baseline 4 we note that the difference to the random
baseline is less than for McCarthy et al who ob-
tained 48% for Elesk on polysemous nouns in Sem-
Cor and 46% for jcn against a random baseline of
24%. These differences are probably explained by
differences in the lexical resources. Both Elesk and
jcn rely on semantic relations including hyponymy
with Elesk also using the glosses. jcn in both ap-
proaches use the hyponym links. WordNet 1.6 (used
by McCarthy et al) has 66025 synsets with 66910
hyponym links between these 5. For EDR there are
166868 nodes (word sense groupings) and 53747
4For significance testing we used McNemar?s test ? = 0.05.
5These figures are taken from
http://www.lsi.upc.es/?batalla/wnstats.html#wn16
Table 3: Results of SENSEVAL-2
precision = recall
fine coarse
baseline 0.282 0.399
lesk 0.344 0.501
DSlesk 0.386 0.593
upper-bound 0.747 0.834
supervised 0.742 0.842
hyponym links. So in EDR the ratio of these links
to the nodes is much lower. This and other differ-
ences between EDR and WordNet are likely to be
the reason for the difference in results.
4.2 SENSEVAL-2
We also evaluate the performance using the Japanese
dictionary task in SENSEVAL-2 (Shirai, 2001). In
this experiment, we use 50 nouns (5,000 instances).
For this task, since semantic relations such as hy-
ponym links are not defined, use of jcn is not pos-
sible. Therefore, we just compare lesk and DSlesk
along with our random baseline, the supervised ap-
proach and the upper-bound as before.
The results are evaluated in two ways; one is for
fine-grained senses in the original task definition and
the other is coarse-grained version which is evalu-
ated discarding the finer categorical information of
each definition. The results are shown in Table 3. As
with the EDR results, all unsupervised methods sig-
nificantly outperform the baseline method, though
the supervised methods still outperform the unsu-
pervised ones. In this experiment, DSlesk is also
significantly better than lesk in both fine and coarse-
grained evaluations. It indicates that applying dis-
tributional similarity score to calculating inter-gloss
similarities improves performance.
5 Conclusion
In this paper, we examined different measures of se-
mantic similarity for finding a first sense heuristic
for WSD automatically in Japanese. We defined a
new gloss-based similarity (DSlesk) and evaluated
the performance on two Japanese WSD datasets, out-
performing lesk and achieving a performance com-
parable to the jcn method which relies on hyponym
links which are not always available.
566
There are several issues for future directions of
automatic detection of a first sense heuristic. In this
paper, we proposed an adaptation of the lesk mea-
sure of gloss-based similarity, by using the aver-
age similarity between nouns in the two glosses un-
der comparison in a bag-of-words approach without
recourse to other information. However, it would
be worthwhile exploring other information in the
glosses, such as words of other PoS and predicate
argument relations. We also hope to investigate ap-
plying alignment techniques introduced for entail-
ment recognition (Hickl and Bensley, 2007).
Another important issue in WSD is to group fine-
grained word senses into clusters, making the task
suitable for NLP applications (Ide and Wilks, 2006).
We believe that our gloss-based similarity DSlesk
might be very suitable for this task and we plan to
investigate the possibility.
There are other approaches we would like to ex-
plore in future. Mihalcea (2005) uses dictionary def-
initions alongside graphical algorithms for unsuper-
vised WSD. Whilst the results are not directly com-
parable to ours because we have not included con-
textual evidence in our models, it would be worth-
while exploring if unsupervised graphical models
using only the definitions we have in our lexical re-
sources can perform WSD on a document and give
more reliable first sense heuristics.
Acknowledgements
This work was supported by the UK EPSRC project
EP/C537262 ?Ranking Word Senses for Disam-
biguation: Models and Applications?, and a UK
Royal Society Dorothy Hodgkin Fellowship to the
second author. We would like to thank John Carroll
for several useful discussions on this work.
References
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
Lesk algorithm for word sense disambiguation using
WordNet. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-02), Mexico City.
J.R.L. Barnard, editor. 1986. Macquaire Thesaurus.
Macquaire Library, Sydney.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 61?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2005. Word sense
disambiguation with distribution estimation. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005), Edinburgh, Scot-
land.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense disam-
biguation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, Prague, Czech Republic, June. Association for
Computational Linguistics.
Paul Clough and Mark Stevenson. 2004. Evaluating the
contribution of EuroWordNet and word sense disam-
biguation to cross-language retrieval. In Second In-
ternational Global WordNet Conference (GWC-2004),
pages 97?105.
Michel Galley and Kathleen McKeown. 2003. Improv-
ing word sense disambiguation in lexical chaining. In
IJCAI-03, Proceedings of the Eighteenth International
Joint Conference on Artificial Intelligence, Acapulco,
Mexico, August 9-15, 2003, pages 1486?1488. Morgan
Kaufmann.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual
entailment. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, pages
171?176.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Jay Jiang and David Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
567
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the joint confer-
ence on Human Language Technology and Empirical
methods in Natural Language Processing, pages 419?
426, Vancouver, B.C., Canada.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning 2002 (CoNLL), pages 63?69.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from and ice cream cone. In Proceedings of the ACM
SIGDOC Conference, pages 24?26, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL 98,
Montreal, Canada.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence (AAAI
2006), Boston, MA, July.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the joint conference on Human Language Technology
and Empirical methods in Natural Language Process-
ing, Vancouver, B.C., Canada.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?.308. Morgan Kaufman.
Saif Mohammad and Graeme Hirst. 2006. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), pages 121?128, Trento, Italy, April.
Roberto Navigli, C. Litkowski, Kenneth, and Orin Har-
graves. 2007. SemEval-2007 task 7: Coarse-
grained English all-words task. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 30?35, Prague,
Czech Republic.
NICT. 2002. EDR electronic dic-
tionary version 2.0, technical guide.
http://www2.nict.go.jp/kk/e416/EDR/.
Minoru Nishio, Etsutaro Iwabuchi, and ShizuoMitzutani.
1994. Iwanami kokugo jiten dai go han.
Siddharth Patwardhan and Ted Pedersen. 2003.
The CPAN WordNet::Similarity Package.
http://search.cpan.org/author/SID/WordNet-
Similarity-0.03/.
Kiyoaki Shirai. 2001. SENSEVAL-2 Japanese Dictio-
nary Task. In Proceedings of the SENSEVAL-2 work-
shop, pages 33?36.
Benjamin Snyder and Martha Palmer. 2004. The English
all-words task. In Proceedings of the ACL SENSEVAL-
3 workshop, pages 41?43, Barcelona, Spain.
Piek Vossen, German Rigau, Inaki Alegria, Eneko Agirre,
David Farwell, and Manuel Fuentes. 2006. Mean-
ingful results for information retrieval in the meaning
project. In Proceedings of the 3rd Global WordNet
Conference. http://nlpweb.kaist.ac.kr/gwc/.
568
Proceedings of NAACL HLT 2009: Short Papers, pages 233?236,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Estimating and Exploiting the Entropy of Sense Distributions
Peng Jin
Institute of Computational Linguistics
Peking University
Beijing China
jandp@pku.edu.cn
Diana McCarthy, Rob Koeling and John Carroll
University of Sussex
Falmer, East Sussex
BN1 9QJ, UK
{dianam,robk,johnca}@sussex.ac.uk
Abstract
Word sense distributions are usually skewed.
Predicting the extent of the skew can help a
word sense disambiguation (WSD) system de-
termine whether to consider evidence from the
local context or apply the simple yet effec-
tive heuristic of using the first (most frequent)
sense. In this paper, we propose a method to
estimate the entropy of a sense distribution to
boost the precision of a first sense heuristic by
restricting its application to words with lower
entropy. We show on two standard datasets
that automatic prediction of entropy can in-
crease the performance of an automatic first
sense heuristic.
1 Introduction
Word sense distributions are typically skewed and
WSD systems do best when they exploit this ten-
dency. This is usually done by estimating the most
frequent sense (MFS) for each word from a training
corpus and using that sense as a back-off strategy for
a word when there is no convincing evidence from
the context. This is known as the MFS heuristic 1
and is very powerful since sense distributions are
usually skewed. The heuristic becomes particularly
hard to beat for words with highly skewed sense dis-
tributions (Yarowsky and Florian, 2002). Although
the MFS can be estimated from tagged corpora, there
are always cases where there is insufficient data, or
where the data is inappropriate, for example because
1It is also referred to as the first sense heuristic in the WSD
literature and in this paper.
it comes from a very different domain. This has mo-
tivated some recent work attempting to estimate the
distributions automatically (McCarthy et al, 2004;
Lapata and Keller, 2007). This paper examines the
case for determining the skew of a word sense distri-
bution by estimating entropy and then using this to
increase the precision of an unsupervised first sense
heuristic by restricting application to those words
where the system can automatically detect that it has
the most chance. We use a method based on that
proposed by McCarthy et al (2004) as this approach
does not require hand-labelled corpora. The method
could easily be adapted to other methods for predic-
ing predominant sense.
2 Method
Given a listing of senses from an inventory, the
method proposed by McCarthy et al (2004) pro-
vides a prevalence ranking score to produce a MFS
heuristic. We make a slight modification to Mc-
Carthy et al?s prevalence score and use it to es-
timate the probability distribution over the senses
of a word. We use the same resources as Mc-
Carthy et al (2004): a distributional similarity the-
saurus and a WordNet semantic similarity measure.
The thesaurus was produced using the metric de-
scribed by Lin (1998) with input from the gram-
matical relation data extracted using the 90 mil-
lion words of written English from the British Na-
tional Corpus (BNC) (Leech, 1992) using the RASP
parser (Briscoe and Carroll, 2002). The thesaurus
consists of entries for each word (w) with the top
50 ?nearest neighbours? to w, where the neighbours
are words ranked by the distributional similarity that
233
they share with w. The WordNet similarity score
is obtained with the jcn measure (Jiang and Con-
rath, 1997) using the WordNet Similarity Package
0.05 (Patwardhan and Pedersen, 2003) and WordNet
version 1.6. The jcn measure needs word frequency
information, which we obtained from the BNC.
2.1 Estimates of Predominance, Probability
and Entropy
Following McCarthy et al (2004), we calculate
prevalence of each sense of the word (w) using a
weighted sum of the distributional similarity scores
of the top 50 neighbours of w. The sense of w that
has the highest value is the automatically detected
MFS (predominant sense). The weights are deter-
mined by the WordNet similarity between the sense
in question and the neighbour. We make a modi-
fication to the original method by multiplying the
weight by the inverse rank of the neighbour from
the list of 50 neighbours. This modification magni-
fies the contribution to each sense depending on the
rank of the neighbour while still allowing a neigh-
bour to contribute to all senses that it relates too.
We verified the effect of this change compared to the
original ranking score by measuring cross-entropy. 2
Let Nw = n1,n2 . . .nk denote the ordered set of the
top k = 50 neighbours of w according to the distri-
butional similarity thesaurus, senses(w) is the set of
senses of w and dss(w,n j) is the distributional sim-
ilarity score of a word w and its jth neighbour. Let
wsi be a sense of w then wnss(wsi,n j) is the maxi-
mum WordNet similarity score between wsi and the
WordNet sense of the neighbour (n j) that maximises
this score. The prevalence score is calculated as fol-
lows with 1rankn j being our modification to McCarthyet al
Prevalence Score(wsi) = ?n j?Nw dss(w,n j)?
wnss(wsi,n j)
?wsi??senses(w)wnss(wsi? ,n j)
? 1rankn j
(1)
To turn this score into a probability estimate we sum
the scores over all senses of a word and the proba-
bility for a sense is the original score divided by this
sum:
2Our modified version of the score gave a lower cross-
entropy with SemCor compared to that in McCarthy et al The
result was highly significant with p < 0.01 on the t-test.
p?(wsi) = prevalence score(wsi)?ws j?w prevalence score(ws j)
(2)
To smooth the data, we evenly distribute 1/10 of the
smallest prevalence score to all senses with a unde-
fined prevalence score values. Entropy is measured
as:
H(senses(w)) =? ?
wsi?senses(w)
p(wsi)log(p(wsi))
using our estimate (p?) for the probability distribu-
tion p over the senses of w.
3 Experiments
We conducted two experiments to evaluate the ben-
efit of using our estimate of entropy to restrict appli-
cation of the MFS heuristic. The two experiments
are conducted on the polysemous nouns in SemCor
and the nouns in the SENSEVAL-2 English all words
task (we will refer to this as SE2-EAW).
3.1 SemCor
For this experiment we used all the polysemous
nouns in Semcor 1.6 (excluding multiwords and
proper nouns). We depart slightly from (McCarthy
et al, 2004) in including all polysemous nouns
whereas they limited the experiment to those with
a frequency in SemCor of 3 or more and where there
is one sense with a higher frequency than the others.
Table 1 shows the precision of finding the predomi-
nant sense using equation 1 with respect to different
entropy thresholds. At each threshold, the MFS in
Semcor provides the upper-bound (UB). The random
baseline (RBL) is computed by selecting one of the
senses of the target word randomly as the predomi-
nant sense. As we hypothesized, precision is higher
when the entropy of the sense distribution is lower,
which is an encouraging result given that the entropy
is automatically estimated. The performance of the
random baseline is higher at lower entropy which
shows that the task is easier and involves a lower de-
gree of polysemy of the target words. However, the
gains over the random baseline are greater at lower
entropy levels indicating that the merits of detect-
ing the skew of the distribution cannot all be due to
lower polysemy levels.
234
H precision #
(?) eq 1 RBL UB tokens
0.5 - - - 0
0.9 80.3 50.0 84.8 466
0.95 85.1 50.0 90.9 1360
1 68.5 50.0 87.4 9874
1.5 67.6 42.6 86.9 11287
2 58.0 36.7 79.5 25997
2.5 55.7 34.4 77.6 31599
3.0 50.2 30.6 73.4 41401
4.0 47.6 28.5 70.8 46987
5.0 (all) 47.3 27.3 70.5 47539
Table 1: First sense heuristic on SemCor
Freq ? P #tokens
1 45.9 1132
5 50.1 5765
10 50.7 10736
100 49.4 39543
1000(all) 47.3 47539
#senses ? P #tokens
2 67.2 10736
5 55.4 31181
8 50.1 41393
12 47.8 46041
30(all) 47.3 47539
Table 2: Precision (P) of equation 1 on SemCor with re-
spect to frequency and polysemy
We also conducted a frequency and polysemy
analysis shown in Table 2 to demonstrate that the
increase in precision is not all due to frequency or
polysemy. This is important, since both frequency
and polysemy level (assuming a predefined sense in-
ventory) could be obtained without the need for au-
tomatic estimation. As we can see, while precision
is higher for lower polysemy, the automatic estimate
of entropy can provide a greater increase in preci-
sion than polysemy, and frequency does not seem to
be strongly correlated with precision.
3.2 SENSEVAL-2 English All Words Dataset
The SE2-EAW task provides a hand-tagged test suite
of 5,000 words of running text from three articles
from the Penn Treebank II (Palmer et al, 2001).
Again, we examine whether precision of the MFS
H precision #
(?) eq 1 RBL SC UB tokens
0.5 - - - - 0
0.9 1 50.0 1 1 7
0.95 94.7 50.0 94.7 1 19
1 69.6 50.0 81.3 94.6 112
1.5 68.0 49.0 81.3 93.8 128
2 69.6 34.7 68.2 87.7 421
2.5 65.0 33.0 65.0 86.5 488
3.0 56.6 27.5 60.8 80.1 687
4.0 52.6 25.6 58.8 79.2 766
5.0 (all) 51.5 25.6 58.5 79.3 769
Table 3: First sense heuristic on SE2-EAW
heuristic can be increased by restricting application
depending on entropy. We use the same resources as
for the SemCor experiment. 3 Table 3 gives the re-
sults. The most frequent sense (MFS) from SE2-EAW
itself provides the upper-bound (UB). We also com-
pare performance with the Semcor MFS (SC). Per-
formance is close to the Semcor MFS while not re-
lying on any manual tagging. As before, precision
increases significantly for words with low estimated
entropy, and the gains over the random baseline are
higher compared to the gains including all words.
4 Related Work
There is promising related work on determining the
predominant sense for a MFS heuristic (Lapata and
Keller, 2007; Mohammad and Hirst, 2006) but our
work is the first to use the ranking score to estimate
entropy and apply it to determine the confidence in
the MFS heuristic. It is likely that these methods
would also have increased precision if the ranking
scores were used to estimate entropy. We leave such
investigations for further work.
Chan and Ng (2005) estimate word sense distri-
butions and demonstrate that sense distribution esti-
mation improves a supervised WSD classifier. They
use three sense distribution methods, including that
of McCarthy et al (2004). While the other two
methods outperform the McCarthy et al method,
3We also used a tool for mapping from WordNet 1.7 to
WordNet 1.6 (Daude? et al, 2000) to map the SE2-EAW noun
data (originally distributed with 1.7 sense numbers) to 1.6 sense
numbers.
235
they rely on parallel training data and are not appli-
cable on 9.6% of the test data for which there are
no training examples. Our method does not require
parallel training data.
Agirre and Mart??nez (2004) show that sense dis-
tribution estimation is very important for both super-
vised and unsupervised WSD. They acquire tagged
examples on a large scale by querying Google with
monosemous synonyms of the word senses in ques-
tion. They show that the method of McCarthy et
al. (2004) can be used to produce a better sampling
technique than relying on the bias from web data
or randomly selecting the same number of exam-
ples for each sense. Our work similarly shows that
the automatic MFS is an unsupervised alternative to
SemCor but our work does not focus on sampling
but on an estimation of confidence in an automatic
MFS heuristic.
5 Conclusions
We demonstrate that our variation of the McCarthy
et al (2004) method for finding a MFS heuristic can
be used for estimating the entropy of a sense dis-
tribution which can be exploited to boost precision.
Words which are estimated as having lower entropy
in general get higher precision. This suggests that
automatic estimation of entropy is a good criterion
for getting higher precision. This is in agreement
with Kilgarriff and Rosenzweig (2000) who demon-
strate that entropy is a good measure of the difficulty
of WSD tasks, though their measure of entropy was
taken from the gold-standard distribution itself.
As future work, we want to compare this approach
of estimating entropy with other methods for es-
timating sense distributions which do not require
hand-labelled data or parallel texts. Currently, we
disregard local context. We wish to couple the con-
fidence in the MFS with contextual evidence and in-
vestigate application on coarse-grained datasets.
Acknowledgements
This work was funded by the China Scholarship Council,
the National Grant Fundamental Research 973 Program
of China: Grant No. 2004CB318102, the UK EPSRC
project EP/C537262 ?Ranking Word Senses for Disam-
biguation?, and a UK Royal Society Dorothy Hodgkin
Fellowship to the second author.
References
E. Agirre and D. Mart??nez. 2004. Unsupervised wsd
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP-2004,
pages 25?32, Barcelona, Spain.
E. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings of
LREC-2002, pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Y.S. Chan and H.T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proceedings of
IJCAI 2005, pages 1010?1015, Edinburgh, Scotland.
J. Daude?, L. Padro?, and G. Rigau. 2000. Mapping word-
nets using structural information. In Proceedings of
the 38th Annual Meeting of the Association for Com-
putational Linguistics, Hong Kong.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Interna-
tional Conference on Research in Computational Lin-
guistics, Taiwan.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for english SENSEVAL. Computers and the
Humanities. Senseval Special Issue, 34(1?2):15?48.
M. Lapata and F. Keller. 2007. An information retrieval
approach to sense ranking. In Proceedings of NAACL-
2007, pages 348?355, Rochester.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING-ACL 98, Mon-
treal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of ACL-2004, pages 280?287, Barcelona,
Spain.
S. Mohammad and G. Hirst. 2006. Determining word
sense dominance using a thesauru s. In Proceedings of
EACL-2006, pages 121?128, Trento, Italy.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H. Trang Dang. 2001. English tasks: All-words and
verb lexical sample. In Proceedings of the SENSEVAL-
2 workshop, pages 21?24.
S. Patwardhan and T. Pedersen. 2003. The
wordnet::similarity package. http://wn-
similarity.sourceforge.net/.
D. Yarowsky and R. Florian. 2002. Evaluating sense
disambiguation performance across diverse parame-
ter spaces. Natural Language Engineering, 8(4):293?
310.
236
Finding Predominant Word Senses in Untagged Text
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The problem with using the
predominant, or first sense heuristic, aside from the
fact that it does not take surrounding context into
account, is that it assumes some quantity of hand-
tagged data. Whilst there are a few hand-tagged
corpora available for some languages, one would
expect the frequency distribution of the senses of
words, particularly topical words, to depend on the
genre and domain of the text under consideration.
We present work on the use of a thesaurus acquired
from raw textual corpora and the WordNet similar-
ity package to find predominant noun senses auto-
matically. The acquired predominant senses give a
precision of 64% on the nouns of the SENSEVAL-
2 English all-words task. This is a very promising
result given that our method does not require any
hand-tagged text, such as SemCor. Furthermore,
we demonstrate that our method discovers appropri-
ate predominant senses for words from two domain-
specific corpora.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account. This is shown by the results of
the English all-words task in SENSEVAL-2 (Cot-
ton et al, 1998) in figure 1 below, where the first
sense is that listed in WordNet for the PoS given
by the Penn TreeBank (Palmer et al, 2001). The
senses in WordNet are ordered according to the fre-
quency data in the manually tagged resource Sem-
Cor (Miller et al, 1993). Senses that have not oc-
curred in SemCor are ordered arbitrarily and af-
ter those senses of the word that have occurred.
The figure distinguishes systems which make use
of hand-tagged data (using HTD) such as SemCor,
from those that do not (without HTD). The high per-
formance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where ev-
idence from the context is not sufficient (Hoste et
al., 2001). Whilst a first sense heuristic based on a
sense-tagged corpus such as SemCor is clearly use-
ful, there is a strong case for obtaining a first, or pre-
dominant, sense from untagged corpus data so that
a WSD system can be tuned to the genre or domain
at hand.
SemCor comprises a relatively small sample of
250,000 words. There are words where the first
sense in WordNet is counter-intuitive, because of
the size of the corpus, and because where the fre-
quency data does not indicate a first sense, the or-
dering is arbitrary. For example the first sense of
tiger in WordNet is audacious person whereas one
might expect that carnivorous animal is a more
common usage. There are only a couple of instances
of tiger within SemCor. Another example is em-
bryo, which does not occur at all in SemCor and
the first sense is listed as rudimentary plant rather
than the anticipated fertilised egg meaning. We be-
lieve that an automatic means of finding a predomi-
nant sense would be useful for systems that use it as
a means of backing-off (Wilks and Stevenson, 1998;
Hoste et al, 2001) and for systems that use it in lex-
ical acquisition (McCarthy, 1997; Merlo and Ley-
bold, 2001; Korhonen, 2002) because of the limited
size of hand-tagged resources. More importantly,
when working within a specific domain one would
wish to tune the first sense heuristic to the domain at
hand. The first sense of star in SemCor is celestial
body, however, if one were disambiguating popular
news celebrity would be preferred.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
020
40
60
80
100
0 20 40 60 80 100
re
ca
ll

precision
First Sense
"using HTD" "without HTD" "First Sense"
Figure 1: The first sense heuristic compared with
the SENSEVAL-2 English all-words task results
are therefore investigating a method of automati-
cally ranking WordNet senses from raw text.
Many researchers are developing thesauruses
from automatically parsed data. In these each tar-
get word is entered with an ordered list of ?near-
est neighbours?. The neighbours are words ordered
in terms of the ?distributional similarity? that they
have with the target. Distributional similarity is
a measure indicating the degree that two words, a
word and its neighbour, occur in similar contexts.
From inspection, one can see that the ordered neigh-
bours of such a thesaurus relate to the different
senses of the target word. For example, the neigh-
bours of star in a dependency-based thesaurus pro-
vided by Lin 1 has the ordered list of neighbours:
superstar, player, teammate, actor early in the list,
but one can also see words that are related to another
sense of star e.g. galaxy, sun, world and planet fur-
ther down the list. We expect that the quantity and
similarity of the neighbours pertaining to different
senses will reflect the dominance of the sense to
which they pertain. This is because there will be
more relational data for the more prevalent senses
compared to the less frequent senses. In this pa-
per we describe and evaluate a method for ranking
senses of nouns to obtain the predominant sense of
a word using the neighbours from automatically ac-
quired thesauruses. The neighbours for a word in a
thesaurus are words themselves, rather than senses.
In order to associate the neighbours with senses we
make use of another notion of similarity, ?semantic
similarity?, which exists between senses, rather than
words. We experiment with several WordNet Sim-
ilarity measures (Patwardhan and Pedersen, 2003)
which aim to capture semantic relatedness within
1Available at
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
the WordNet hierarchy. We use WordNet as our
sense inventory for this work.
The paper is structured as follows. We discuss
our method in the following section. Sections 3 and
4 concern experiments using predominant senses
from the BNC evaluated against the data in SemCor
and the SENSEVAL-2 English all-words task respec-
tively. In section 5 we present results of the method
on two domain specific sections of the Reuters cor-
pus for a sample of words. We describe some re-
lated work in section 6 and conclude in section 7.
2 Method
In order to find the predominant sense of a target
word we use a thesaurus acquired from automati-
cally parsed text based on the method of Lin (1998).
This provides the  nearest neighbours to each tar-
get word, along with the distributional similarity
score between the target word and its neighbour. We
then use the WordNet similarity package (Patward-
han and Pedersen, 2003) to give us a semantic simi-
larity measure (hereafter referred to as the WordNet
similarity measure) to weight the contribution that
each neighbour makes to the various senses of the
target word.
To find the first sense of a word (  ) we
take each sense in turn and obtain a score re-
flecting the prevalence which is used for rank-
ing. Let   	


 be the ordered
set of the top scoring  neighbours of  from
the thesaurus with associated distributional similar-
ity scores 	ffUsing Automatically Acquired Predominant Senses for Word Sense
Disambiguation
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The first (or predominant)
sense heuristic assumes the availability of hand-
tagged data. Whilst there are hand-tagged corpora
available for some languages, these are relatively
small in size and many word forms either do not
occur, or occur infrequently. In this paper we in-
vestigate the performance of an unsupervised first
sense heuristic where predominant senses are ac-
quired automatically from raw text. We evaluate on
both the SENSEVAL-2 and SENSEVAL-3 English all-
words data. For accurate WSD the first sense heuris-
tic should be used only as a back-off, where the evi-
dence from the context is not strong enough. In this
paper however, we examine the performance of the
automatically acquired first sense in isolation since
it turned out that the first sense taken from SemCor
outperformed many systems in SENSEVAL-2.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account (McCarthy et al, 2004). The high
performance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where evi-
dence from the context is not sufficient (Hoste et al,
2001).
The first sense heuristic is a powerful one. Us-
ing the first sense listed in SemCor on the SENSE-
VAL-2 English all-words data we obtained the re-
sults given in table 1, (where the PoS was given by
the gold-standard data in the SENSEVAL-2 data it-
self). 1 Recall is lower than precision because there
are many words which do not occur in SemCor. Use
1We did not include items which were tagged ?U?
(unassignable) by the human annotators.
PoS precision recall baseline
Noun 70 60 45
Verb 48 44 22
Adjective 71 59 44
Adverb 83 79 59
All PoS 67 59 41
Table 1: The SemCor first sense on the SENSEVAL-
2 English all-words data
of the first sense listed in WordNet gives 65% pre-
cision and recall for all PoS on these items. The
fourth column on table 1 gives the random base-
line which reflects the polysemy of the data. Ta-
ble 2 shows results obtained when we use the most
common sense for an item and PoS using the fre-
quency in the SENSEVAL-2 English all-words data
itself. Recall is lower than precision since we only
use the heuristic on lemmas which have occurred
more than once and where there is one sense which
has a greater frequency than the others, apart from
trivial monosemous cases. 2 Precision is higher in
table 2 than in table 1 reflecting the difference be-
tween an a priori first sense determined by Sem-
Cor, and an upper bound on the performance of this
heuristic for this data. This upper bound is quite
high because of the very skewed sense distributions
in the test data itself. The upper bound for a docu-
ment, or document collection, will depend on how
homogenous the content of that document collec-
tion is, and the skew of the word sense distributions
therein. Indeed, the bias towards one sense for a
given word in a given document or discourse was
observed by Gale et al (1992).
Whilst a first sense heuristic based on a sense-
tagged corpus such as SemCor is clearly useful,
there is a case for obtaining a first, or predomi-
nant, sense from untagged corpus data so that a WSD
2If we include polysemous items that have only occurred
once in the data we obtain a precision of 92% and a recall of
85% over all PoS.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
PoS precision recall baseline
Noun 95 73 45
Verb 79 43 22
Adjective 88 59 44
Adverb 91 72 59
All PoS 90 63 41
Table 2: The SENSEVAL-2 first sense on the SEN-
SEVAL-2 English all-words data
system can be tuned to a given genre or domain
(McCarthy et al, 2004) and also because there will
be words that occur with insufficient frequency in
the hand-tagged resources available. SemCor com-
prises a relatively small sample of 250,000 words.
There are words where the first sense in WordNet is
counter-intuitive, because this is a small sample, and
because where the frequency data does not indicate
a first sense, the ordering is arbitrary. For exam-
ple the first sense of tiger in WordNet is audacious
person whereas one might expect that carnivorous
animal is a more common usage.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
are investigating a method of automatically ranking
WordNet senses from raw text, with no reliance on
manually sense-tagged data such as that in SemCor.
The paper is structured as follows. We discuss
our method in the following section. Section 3 de-
scribes an experiment using predominant senses ac-
quired from the BNC evaluated on the SENSEVAL-2
English all-words task. In section 4 we present our
results on the SENSEVAL-3 English all-words task.
We discuss related work in section 5 and conclude
in section 6.
2 Method
The method is described in (McCarthy et al, 2004),
which we summarise here. We acquire thesauruses
for nouns, verbs, adjectives and adverbs based on
the method proposed by Lin (1998) using grammat-
ical relations output from the RASP parser (Briscoe
and Carroll, 2002). The grammatical contexts used
are listed in table 3, but there is scope for extending
or restricting the contexts for a given PoS.
We use the thesauruses for ranking the senses of
the target words. Each target word (  ) e.g. plant
in the thesaurus is associated with a list of nearest
PoS grammatical contexts
Noun verb in direct object or subject relation
adjective or noun modifier
Verb noun as direct object or subject
Adjective modified noun, modifing adverb
Adverb modified adjective or verb
Table 3: Grammatical contexts used for acquiring
the thesauruses
neighbours ( 	
 ) with distributional similarity
scores (  ) e.g. factory 0.28, refinery 0.17,
tree 0.14 etc... 3 Distributional similarity is a mea-
sure indicating the degree that two words, a word
and its neighbour, occur in similar contexts. The
neighbours reflect the various senses of the word
( Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 314?317,
Prague, June 2007. c?2007 Association for Computational Linguistics
Sussx: WSD using Automatically Acquired Predominant Senses
Rob Koeling and Diana McCarthy
Department of Informatics
University of Sussex
Brighton BN1 9QJ, UK
robk,dianam@sussex.ac.uk
1 Introduction
We introduced a method for discovering the predom-
inant sense of words automatically using raw (unla-
belled) text in (McCarthy et al, 2004) and partici-
pated with this system in SENSEVAL3. Since then,
we worked on further developing ideas to improve
upon the base method. In the current paper we tar-
get two areas where we believe there is potential for
improvement. In the first one we address the fine-
grained structure of WordNet?s (WN) sense inven-
tory (i.e. the topic of the task in this particular track).
The second issue we address here, deals with topic
domain specilisation of the base method.
Error analysis tought us that the method is sensi-
tive to the fine-grained nature of WN. When two dis-
tinct senses in the WN sense inventory are closely re-
lated, the method often has difficulties discriminat-
ing between the two senses. If, for example, sense 1
and sense 7 for a word are closely related, choosing
sense 7 in stead of sense 1 has serious consequences
if you are using a first-sense heuristic (considering
the highly skewed distribution of word senses). We
expect that applying our method on a coarser grained
sense inventory might help us resolve some of the
more unfortunate errors.
(Magnini et al, 2002) have shown that informa-
tion about the domain of a document is very useful
for WSD. This is because many concepts are spe-
cific to particular domains, and for many words their
most likely meaning in context is strongly correlated
to the domain of the document they appear in. Thus,
since word sense distributions are skewed and de-
pend on the domain at hand we would like to explore
if we can estimate the most likely sense of a word
for each domain of application and exploit this in
a WSD system.
2 Predominant Sense Acquisition
We use the method described in (McCarthy et al,
2004) for finding predominant senses from raw text.
The method uses a thesaurus obtained from the
text by parsing, extracting grammatical relations and
then listing each word (w) with its top k nearest
neighbours, where k is a constant. Like (McCarthy
et al, 2004) we use k = 50 and obtain our thesaurus
using the distributional similarity metric described
by (Lin, 1998) and we use WordNet (WN) as our
sense inventory. The senses of a word w are each
assigned a ranking score which sums over the dis-
tributional similarity scores of the neighbours and
weights each neighbour?s score by a WN Similarity
score (Patwardhan and Pedersen, 2003) between the
sense of w and the sense of the neighbour that max-
imises the WN Similarity score. This weight is nor-
malised by the sum of such WN similarity scores be-
tween all senses of w and the senses of the neighbour
that maximises this score. We use the WN Similarity
jcn score (Jiang and Conrath, 1997) since this gave
reasonable results for (McCarthy et al, 2004) and it
is efficient at run time given precompilation of fre-
quency information. The jcn measure needs word
frequency information, which we obtained from the
British National Corpus (BNC) (Leech, 1992). The
distributional thesaurus was constructed using sub-
ject, direct object adjective modifier and noun mod-
ifier relations.
314
3 Coarse Sense Inventory Adaptation
We contrasted ranking of the original WordNet
senses with ranking produced using the coarse
grained mapping between WordNet senses and the
clusters provided for this task. In the first, which we
refer to as fine-grained training (SUSSX-FR), we use
the original method as described in section 2 using
WordNet 2.1 as our sense inventory. For the second
method which we refer to as coarse-grained train-
ing (SUSSX-CR), we use the clusters of the target
word as our senses. The distributional similarity of
each neighbour is apportioned to these clusters us-
ing the maximum WordNet similarity between any
of the WordNet senses in the cluster and any of the
senses of the neighbour. This WordNet similarity is
normalised as in the original method, but for the de-
nominator we use the sum of the WordNet similarity
scores between this neighbour and all clusters of the
target word.
4 Domain Adaptation
The topic domain of a document has a strong influ-
ence on the sense distribution of words. Unfortu-
nately, it is not feasible to produce large manually
sense-annotated corpora for every domain of inter-
est. Since the method described in section 2 works
with raw text, we can specialize our sense rank-
ings for a particular topic domain, simply by feed-
ing a domain specific corpus to the algorithm. Pre-
vious experiments have shown that unsupervised es-
timation of the predominant sense of certain words
using corpora whose domain has been determined
by hand outperforms estimates based on domain-
independent text for a subset of words and even
outperforms the estimates based on counting oc-
currences in an annotated corpus (Koeling et al,
2005). A later experiment (using SENSEVAL2 and
3 data) showed that using domain specific predomi-
nant senses can slightly improve the results for some
domains (Koeling et al, 2007). However, a firm idea
of when domain specilisation should be considered
could not (yet) be given.
4.1 Creating the Domain Corpora
In order to estimate topic domain specific sense
rankings, we need to specify what we consider ?do-
mains? and we need to collect corpora of texts for
these domains. We decided to use text classifica-
tion for determining the topic domain and adopted
the domain hierarchy as defined for the topic domain
extension for WN (Subject Field Codes or WordNet
Domains (WN-DOMAINS) (Magnini et al, 2002)).
Domains In WN-DOMAINS the Princeton English
WordNet is augmented with domain labels. Ev-
ery synset in WN?s sense inventory is annotated
with at least one domain label, selected from a set
of about 200 labels hierarchically organized (based
on the Dewey Decimal Classification (Diekema, )).
Each synset of Wordnet 1.6 was labeled with one
or more labels. The label ?factotum? was assigned
if any other was inadequate. The first level con-
sists of 5 main categories (e.g. ?doctrines? and ?so-
cial science?) and ?factotum?. ?doctrines?, for exam-
ple, has subcategories such as ?art?, ?religion? and
?psychology?. Some subcategories are divided in
sub-subcategories, e.g. ?dance?, ?music? or ?theatre?
are subcategories of ?art?.
Classifier We extracted bags of domain-specific
words from WordNet for all the defined domains by
collecting all the word senses (synsets) and corre-
sponding glosses associated with a certain domain
label. These bags of words define the domains and
we used them to train a Support Vector Machine
(SVM) text classifier using ?TwentyOne?1.
The classifier distinguishes between 48 classes
(first and second level of the WN-DOMAINS hierar-
chy). When a document is evaluated by the classi-
fier, it returns a list of all the classes (domains) it
recognizes and an associated confidence score re-
flecting the certainty that the document belongs to
that particular domain.
Corpora We used the Gigaword English Corpus
as our data source. This corpus is a comprehen-
sive archive of newswire text data that has been
acquired over several years by the Linguistic Data
Consortium, at the University of Pennsylvania. For
the experiments described in this paper, we use the
first 20 months worth of data of all four sources
(Agence France Press English Service, Associated
Press Worldstream English Service, The New York
Times Newswire Service and The Xinhua News
Agency English Service). There are 4 different types
1TwentyOne Classifier is an Irion Technologies product:
www.irion.ml/products/english/products classify.html
315
Doc.Id. Class Conf. Score
d001 Medicine (Economy) 0.75 (0.75)
d002 Economy (Politics) 0.76 (0.74)
d003 Transport (Biology) 0.75 (0.68)
d004 Comp-Sci (Architecture) 0.81 (0.68)
d005 Psychology (Art) 0.78 (0.74)
Table 1: Output of the classifier for the 5 docu-
ments. The classifiers second choice is given be-
tween brackets.
of documents identified in the corpus. The vast ma-
jority of the documents are of type ?story?. We are
using all the data.
The five documents were fed to the classifier. The
results are given in table 1. Unfortunately, only one
document (d004) was considered to be a clear-cut
example of a particular domain by the classifier (i.e.
a high score is given to the first class and a much
lower score to the following classes).
4.2 Domain rankings
We created domain corpora by feeding the Giga-
Word documents to the classifier and adding each
document to the domain corpus corresponding to
the classifier?s first choice. The five corpora we
needed for these documents were parsed using
RASP (Briscoe and Carroll, 2002) and the result-
ing grammatical relations were used to create a dis-
tributional similarity thesaurus, which in turn was
used for computing the predominant senses (see sec-
tion 2). The only pre-processing we performed
was stripping the XML codes from the documents.
No other filtering was undertaken. This resulted in
five sets of sense inventories with domain-dependent
sense rankings. Each of them has a slightly different
set of words. The words they have in common do
have the same senses, but not necessarily the same
estimated most frequently used sense.
5 Results from Semeval
Coarse Disambiguation of coarse-grained senses is
obviously an easier task than fine grained training.
We had hoped that the coarse-grained training might
show superior performance by removing the noise
created by related but less frequent senses. Since
the mapping between fine-grained senses and clus-
ters is used anyway in the scorer the noise from
related senses does not seem to be an issue. Re-
lated senses are scored correctly. Indeed the per-
formance of the fine-grained training is superior to
that of the coarse-grained training. We believe this
is because predominant meanings have more related
senses. There are therefore more chances that the
distributional similarity of the neighbours will get
apportioned to one of the related senses when there
are more related senses. The coarse grained rank-
ing would have an advantage on occasions when in
the fine-grained ranking the credit between related
senses is split and an unrelated sense ends up with
a higher ranking score. Since the coarse-grained
ranker lumps the credit for related sense together it
would be at an advantage. Clearly this doesn?t hap-
pen enough in the data to outweigh the beneficial
effect of the number of related senses compensating
for other noise in the data.
Doc.Id. Class SUSSX-FR SUSSX-C-WD
d001 Medicine 0.556 0.560
d002 Economy 0.508 0.515
d003 Transport 0.487 0.454
d004 Comp-Sci 0.407 0.424
d005 Psychology 0.356 0.372
Table 2: Impact of domain specialisation for each of
the five documents (F1 scores).
Domain Unfortuately, the system specialised for
domain (SUSSX-C-WD) did not improve the results
over the 5 documents significantly. However, if we
look at the contributions made by each document,
we might learn something about the relation beteen
the output of the classifier and the impact on the
WSDresults. Table 2 shows the per-document results
for the systems SUSSX-FR and SUSSX-C-WD. The
first two documents show very little difference with
the domain independent results. The documents
?d004? and ?d005? show a small but clear improved
performance for the domain results. Unfortunately,
document ?d003? displays a very disappointing drop
of more than 3% in performance, and cancels out all
the gains made by the last two documents.
The output of the classifier seems to be indica-
tive of the results for all documents except ?d003?.
The classifier doesn?t seem to find enough evidence
for a marked preference for a particular domain
316
for documents ?d001? and ?d002?. This could be
an indication that there is no strong domain ef-
fect to be expected. The strong preference for the
?computer science? domain for ?d004? is reflected in
good performance of SUSSX-C-WD and even though
the confidence scores for the first 2 alternatives of
?d005? are fairly close, there is a clear drop in con-
fidence score for the third alternative, which might
indicate that the topic of this document is related to
both first choices of the classifier. It will be interest-
ing to evaluate the results for ?d005? using the ?Art?
sense rankings. One would expect those results to be
similar to the results found here. Finally, the results
for ?d003? are hard to explain. We will need to do an
extensive error analysis as soon as the gold-standard
is available.
6 Conclusions
In this paper we investigated two directions where
we expect potential for improving the performance
of our method for acquiring predominant senses.
In order to fully appreciate what the effects of the
coarse grained sense inventory are (i.e. whether
some of the more unfortunate errors are resolved),
we will have to do an extensive error analysis as
soon as the gold standard becomes available. Con-
sidering the fairly low number of attempted tokens
(only 72.8% of the tokens are attempted), we are at
a disadvantage compared to systems that back-off to
(for example) the first sense in WN. However, we are
well pleased with the high precision (71.7%) of the
method SUSSX-FR, considering this is a completely
unsupervised method. There seems to be potential
gains for domain adaptation, but applying it to each
document does not seem to be advisable. More re-
search needs to be done to identify in which cases a
performance boost can be expected. Five documents
is not enough to fully investigate the matter. At the
moment we are performing a larger scale experiment
with the documents in SemCor. These documents
seem to cover a fairly wide range of domains (ac-
cording to our text classifier) and many domains are
represented by several documents.
Acknowledgements
This work was funded by UK EPSRC project
EP/C537262 ?Ranking Word Senses for Disam-
biguation: Models and Applications?, and by a UK
Royal Society Dorothy Hodgkin Fellowship to the
second author. We would also like to thank Piek
Vossen for giving us access to the Irion Technolo-
gies text categoriser.
References
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC-2002, pages 1499?1504, Las Palmas de GranCanaria.
Anne Diekema. http://www.oclc.org/dewey/.
Jay Jiang and David Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominantsense acquisition. In Proceedings of the Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing.,pages 419?426, Vancouver, Canada.
Rob Koeling, Diana McCarthy, and John Carroll. 2007.Text categorization for improved priors of word mean-
ing. In Proceedings of the Eighth International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics (Cicling 2007), pages 241?252,
Mexico City, Mexico.
Geoffrey Leech. 1992. 100 million words of English:the British National Corpus. Language Research,28(1):1?13.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL 98,Montreal, Canada.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,and Alfio Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,pages 280?287, Barcelona, Spain.
Siddharth Patwardhan and Ted Pedersen.
2003. The cpan wordnet::similarity package.http://search.cpan.org/s?id/WordNet-Similarity/.
317
211
212
213
214
Unsupervised Acquisition of Predominant
Word Senses
Diana McCarthy
University of Sussex
Rob Koeling
University of Sussex
Julie Weeds
University of Sussex
John Carroll?
University of Sussex
There has been a great deal of recent research into word sense disambiguation, particularly
since the inception of the Senseval evaluation exercises. Because a word often has more than
one meaning, resolving word sense ambiguity could benefit applications that need some level
of semantic interpretation of language input. A major problem is that the accuracy of word
sense disambiguation systems is strongly dependent on the quantity of manually sense-tagged
data available, and even the best systems, when tagging every word token in a document,
perform little better than a simple heuristic that guesses the first, or predominant, sense of a
word in all contexts. The success of this heuristic is due to the skewed nature of word sense
distributions. Data for the heuristic can come from either dictionaries or a sample of sense-
tagged data. However, there is a limited supply of the latter, and the sense distributions and
predominant sense of a word can depend on the domain or source of a document. (The first
sense of ?star? for example would be different in the popular press and scientific journals).
In this article, we expand on a previously proposed method for determining the predominant
sense of a word automatically from raw text. We look at a number of different data sources and
parameterizations of the method, using evaluation results and error analyses to identify where
the method performs well and also where it does not. In particular, we find that the method
does not work as well for verbs and adverbs as nouns and adjectives, but produces more accurate
predominant sense information than the widely used SemCor corpus for nouns with low coverage
in that corpus. We further show that the method is able to adapt successfully to domains when
using domain specific corpora as input and where the input can either be hand-labeled for domain
or automatically classified.
? Department of Informatics, Brighton BN1 9QH, UK. E-mail: {dianam,robk,juliewe,johnca}@sussex.ac.uk.
Submission received: 16 November 2005; revised submission received: 12 July 2006; accepted for publication
16 February 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
1. Introduction
In word sense disambiguation, the ?first sense? heuristic (choosing the first, or predom-
inant sense of a word) is used by most state-of-the-art systems as a back-off method
when information from the context is not sufficient to make a more informed choice.
In this article, we present an in-depth study of a method for automatically acquiring
predominant senses for words from raw text (McCarthy et al 2004a).
The method uses distributionally similar words listed as ?nearest neighbors?
in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the
observation that the more prevalent a sense of a word, the more neighbors will relate
to that sense, and the higher their distributional similarity scores will be. The senses
of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because
this is widely used, is publicly available, and has plenty of gold-standard evaluation
data available (Miller et al 1993; Cotton et al 2001; Preiss and Yarowsky 2001; Mihalcea
and Edmonds 2004). The distributional strength of the neighbors is associated with the
senses of a word using a measure of semantic similarity which relies on the relationships
between word senses, such as hyponyms (available in an inventory such as WordNet)
or overlap in the definitions of word senses (available in most dictionaries), or both.
In this article we provide a detailed discussion and quantitative analysis of the
motivation behind the first sense heuristic, and a full description of our method. We
extend previously reported work in a number of different directions:
 We evaluate the method on all parts of speech (PoS) on SemCor (Miller
et al 1993). Previous experiments (McCarthy et al 2004c) evaluated only
nouns on SemCor, or all PoS but only on the Senseval-2 (Cotton et al 2001)
and Senseval-3 (Mihalcea and Edmonds 2004) data. The evaluation on all
PoS is much more extensive because the SemCor corpus is composed of
220,000 words in contrast to the 6 documents in the Senseval-2 and -3
English all words data (10,000 words).
 We compare two WordNet similarity measures in our evaluation on
nouns, and also contrast performance using two publicly available
thesauruses, both produced from the same NEWSWIRE corpus, but one
derived using a proximity-based approach and the other using
dependency relations from a parser. It turns out that the results from the
proximity-based thesaurus are comparable to those from the dependency-
based thesaurus; this is encouraging for applying the method to languages
without sophisticated analysis tools.
 We manually analyze a sample of errors from the SemCor evaluation. A
small number of errors can be traced back to inherent shortcomings of our
method, but the main source of error is due to noise from related senses.
This is a common problem for all WSD systems (Ide and Wilks 2006) but
one which is only recently starting to be addressed by the WSD
community (Navigli, Litkowski, and Hargraves 2007).
 One motivation for an automatic method for acquiring predominant
senses is that there will always be words for which there are insufficient
data available in manually sense-tagged resources. We compare the
performance of our automatic method with the first sense heuristic
derived from SemCor on nouns in the Senseval-2 data. We find that the
554
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
automatic method outperforms the one obtained from manual annotations
in SemCor for nouns with fewer than five occurrences in SemCor.
 Aside from the lack of coverage of manually annotated data, there is a
need for first sense heuristics to be specific to domain. We explore the
potential for applying the method with domain-specific text for all PoS in
an experiment using a gold-standard domain-specific resource (Magnini
and Cavaglia` 2000) which we have used previously only with nouns. We
show that although there is a little mileage to be had from domain-specific
first sense heuristics for verbs, nouns benefit greatly from domain-specific
training.
 In previous work (Koeling, McCarthy, and Carroll 2005) we produced
manually sense-annotated domain-specific test corpora for a lexical
sample, and demonstrated that predominant senses acquired (from
hand-classified corpora) in the same domain as the test data outperformed
the SemCor first sense. We further this exploration by contrasting with
results from training on automatically categorized text from the English
Gigaword Corpus and show that the results are comparable to those using
hand-classified domain data.
The article is organized as follows. In the next section we motivate the use of pre-
dominant sense information in WSD systems and the need for acquiring this information
automatically. In Section 3 we give an overview of related work in WSD, focusing on the
acquisition of prior sense distributions and domain-specific sense information. Section 4
describes our acquisition method. Section 5 describes the experimental setup for the
work reported in this article. Section 6 describes four experiments. The first evaluates
the first sense heuristic using predominant sense information acquired for all PoS on
SemCor; for nouns we compare two semantic similarity methods and three different
types of distributional thesaurus. We also report an error analysis for all PoS of our
method. The second experiment compares the performance of the automatic method
to the manually produced data in SemCor, on nouns in the Senseval-2 data, looking
particularly at nouns which have a low frequency in SemCor. The third uses corpora in
restricted domains and the subject field code gold standard of Magnini and Cavaglia`
(2000) to investigate the potential for domain-specific rankings for different PoS. The
fourth compares results when we train and test on domain-specific corpora, where
the training data is (1) manually categorized for domain and from the same corpus
as the test data, and (2) where the training data is harvested automatically from another
corpus which is categorized automatically. Finally, we conclude (Section 7) and discuss
directions for future work (Section 8).
2. Motivation
The problem of disambiguating the meanings of words in text has received much
attention recently, particularly since the inception of the Senseval evaluation exercises
(Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004).
One of the standard Senseval tasks (the ?all words? task) is to tag each open class word
with one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum
1998). The most accurate word sense disambiguation (WSD) systems use supervised
machine learning approaches (Stevenson and Wilks 2001), trained on text which has
been sense tagged by hand. However, the performance of these systems is strongly
555
Computational Linguistics Volume 33, Number 4
dependent on the quantity of training data available (Yarowsky and Florian 2002),
and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998). The
largest all words sense tagged corpus is SemCor, which is 220,000 words taken from
103 passages, each of about 2,000 words, from the Brown corpus (Francis and Kuc?era
1979) and the complete text of a 19th-century American novel, The Red Badge of Courage,
which totals 45,600 words (Landes, Leacock, and Tengi 1998). Approximately half of the
words in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) and
these have been linked to WordNet senses by human taggers using a software interface.
The shortage of training data due to the high costs of tagging texts has motivated
research into unsupervised methods for WSD. But in the English all-words tasks in
Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make
use of hand-tagged data (in some form or other) performed substantially worse than
those that did. Table 1 summarizes the situation. It gives the precision and recall of
the best1 two supervised (S) and unsupervised (U)2 systems for the English all words
and English lexical sample for Senseval-23 and -3, along with the first sense baseline
(FS) reported by the task organizers.4 This is a simple application of the ?first sense?
heuristic?that is, using the most common sense of a word for every instance of it in the
test corpus, regardless of context. Although contextual WSD is of course preferable, the
baseline is a very powerful one and unsupervised systems find it surprisingly hard to
beat (indeed, some of the systems that report themselves as unsupervised actually make
some use of a manually obtained first-sense heuristic). Considering both precision and
recall, only 5 of 26 systems in the Senseval-3 English all-words task beat the first sense
heuristic as derived from SemCor (61.5%5), and then by only a few percentage points
(the top system scoring 65% precision and recall) despite using hand-tagged training
data available from SemCor and previous Senseval data sets, large sets of contextual
features, and sophisticated machine learning algorithms.
The performance of WSD systems, at least for all-words tasks, seems to have
plateaued at a level just above the first sense heuristic (Snyder and Palmer 2004). This is
due to the shortage of training data and the often fine granularity of sense distinctions.
Ide and Wilks (2006) argue that it is best to concentrate effort on distinctions which
are useful for applications and where systems can be confident of high precision. In
cases where systems are less confident, but word senses, rather than words, are needed,
the first sense heuristic is a powerful back-off strategy. This strategy is dependent on
information provided in dictionaries. Two dictionaries that have been used by English
WSD systems are the Longman Dictionary of Contemporary English (LDOCE) (Procter
1 We rank the systems by the recall scores, because this is the accuracy over the entire test set regardless of
how many items were attempted.
2 Note that the classification of systems as unsupervised is not straightforward. Systems reported as
unsupervised in the Senseval proceedings sometimes make use of some manual annotations. For
example, the top scoring system that reported itself unsupervised in the Senseval-3 lexical sample task
used manually sense-tagged training data for constructing glosses.
3 The verb lexical sample was done as a separate exercise for Senseval-2, and for brevity we have not
included the results from this task.
4 The all-words task organizers used the first sense as listed in WordNet. This is based on the SemCor first
sense because WordNet senses are ordered according to the frequency data in SemCor. However, where
senses are not found in WordNet, the ordering is arbitrarily determined as a function of the ?grind?
program (see http://wordnet.princeton.edu/man/grind.1WN.htm). The lexical sample task organizers
state that they use the ?most frequent sense? but do not stipulate if this is taken from WordNet, or
directly from SemCor.
5 This figure is the arithmetic mean of two published estimates (Snyder and Palmer 2004), the difference
being due to the treatment of multiwords.
556
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 1
The best two performing systems of each type (according to fine-grained recall) in Senseval-2
and -3.
All words Lexical sample
Precision (%) Recall (%) Precision (%) Recall (%)
Senseval-2 S 69.0 69.0 64.2 64.2
Senseval-2 S 63.6 63.6 63.8 63.8
Senseval-2 U 45.1 45.1 40.2 40.1
Senseval-2 U 36.0 36.0 58.1 31.9
FS baseline 57.0 57.0 47.6 47.6
Senseval-3 S 65.1 65.1 72.9 72.9
Senseval-3 S 65.1 64.2 72.6 72.6
Senseval-3 U 58.3 58.2 66.1 65.7
Senseval-3 U 55.7 54.6 56.3 56.3
FS baseline 61.5 61.5 55.2 55.2
1978) and WordNet (Fellbaum 1998). These both provide a ranking of senses accord-
ing to their predominance. The sense ordering in LDOCE is based on lexicographer
intuition, whereas in WordNet the senses are ordered according to their frequency in
SemCor (Miller et al 1993).
There are two major problems with deriving a first sense heuristic from these types
of resources. The first is that the predominant sense of a word varies according to
the source of the document (McCarthy and Carroll 2003) and with the domain. For
example, the first sense of star as derived from SemCor is celestial body, but if one were
disambiguating popular news stories then celebrity would be more likely. Domain,
topic, and genre are important in WSD (Martinez and Agirre 2000; Magnini et al 2002)
and the sense-frequency distributions of words depend on all of these factors. Any
dictionary will provide only a single sense ranking, whether this is derived from sense-
tagged data as in WordNet, lexicographer intuition as in LDOCE, or inspection of corpus
data as in the Oxford Advanced Learner?s Dictionary (Hornby 1989). A fixed order of
senses may not reflect the data that an NLP system is dealing with.
The second problem with obtaining predominant sense information applies to the
use of hand-tagged resources, such as SemCor. Such resources are relatively small due
to the cost of manual tagging (Kilgarriff 1998). Many words will simply not be covered,
or occur only a few times. For many words in WordNet the ordering of word senses is
based on a very small number of occurrences in SemCor. For example, the first sense
of tiger is an audacious person whereas most people would assume the carnivorous
animal sense is more prevalent. This is because the two senses each occur exactly once
in SemCor, and when there is no frequency information to break the tie the WordNet
sense ordering is assigned arbitrarily. There are many fairly common words (such as
the noun crane) which do not occur at all in SemCor. Table 2 gives the number and
percentage of words6 in WordNet and the BNC which do not occur in SemCor. As one
would expect from Zipf?s law, a substantial number of words do not occur in SemCor,
even when we do not consider multiwords. Many of these words are extremely rare, but
6 Here and elsewhere in this article we give figures only for words without embedded spaces, that is, not
multiwords.
557
Computational Linguistics Volume 33, Number 4
Table 2
Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor.
WordNet types BNC types
PoS No. % No. %
noun 43,781 81.9 360,535 97.5
verb 4,741 56.4 25,292 87.6
adjective 14,991 72.3 95,908 95.4
adverb 2,405 64.4 10,223 89.2
Table 3
Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no
data in SemCor (0 columns), or with very little data (? 1 and ? 5 occurrences). Note that there
are no annotations for adverbs in the Senseval-3 documents.
Senseval-2 Senseval-3
0 ? 1 ? 5 0 ? 1 ? 5
PoS No. % No. % No. % No. % No. % No. %
noun 12 3.2 28 7.4 49 12.9 13 3.1 26 6.3 69 16.7
verb 7 2.1 11 3.4 28 8.6 3 0.9 10 2.9 36 10.4
adjective 9 4.2 16 7.4 50 23.1 8 4.7 15 8.9 33 19.5
adverb 1 0.9 1 0.9 2 1.8 ? ? ? ? ? ?
in any given document it is likely that there will be at least some words without SemCor
data. Table 3 quantifies this, for the Senseval-2 and -3 all-words tasks test data, showing
the percentage of polysemous word types with no frequency information in SemCor, the
percentage with zero or one occurrences, and the percentage with up to five occurrences.
(For example, the table indicates that 12.9% of nouns in the Senseval-2 data, and 16.7%
in Senseval-3, have five or fewer occurrences in SemCor.) Thus, although SemCor may
cover many frequently occurring word types in a given document, there are likely to be
a substantial proportion for which there is very little or no information available.
Tables 4 and 5 present an analysis of the actual ambiguity of polysemous words
within the six documents making up the Senseval-2 and -3 all-words test data. They
show the extent to which these words are used in a predominant sense, within a
document, and the extent to which this is the same as that given by SemCor. The two
tables share a common format: columns 2?5 give percentages over all ?document/word
type? combinations. The second column shows the percentage of the ?document/word
type? combinations where the word is used in the document in only one of its senses.
The fourth column shows the same percentage but for ?document/word type? combi-
nations where the word is used in more than one sense in the document. The third and
fifth columns give the percentage of the words in the preceding columns (second and
fourth, respectively) where the first sense for the word in the document is the same as in
SemCor (FS = SC FS). For the third column, this is the only sense that this word appears
in within the document. (Note that for any row, columns 2 and 4 account for all possibil-
ities so will always add up to 100.) The sixth column gives the mean degree of polysemy,
according to WordNet, for the set of words that these figures are calculated for.
558
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 4
Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than
once in a document (adverb data is only from Senseval-2).
1 sense > 1 sense
PoS % FS = SC FS % % FS = SC FS % Mean polysemy
noun 72.2 52.2 27.8 7.3 5.9
verb 45.6 25.1 54.4 16.9 12.7
adjective 62.9 40.5 37.1 10.3 4.8
adverb 64.7 50.0 35.3 17.6 4.7
The figures in Table 4 are for words occurring more than once in a given Senseval
test document. The tendency for words to be used in only one sense in any given
document7 is strongest for nouns, although adverbs and adjectives also tend towards
one sense. Verbs are on average much more polysemous than the other parts of speech
yet still 45.6% of polysemous verbs which occur more than once are used in only a single
sense. However, because verbs are in general more polysemous, it makes it less likely
that if a verb occurs in only one sense in a document then it will be the one indicated by
SemCor.
The figures in Table 5 are for all words in the Senseval documents (not just those oc-
curring more than once), showing the accuracy of a SemCor-derived first-sense heuristic
for words with a frequency below a specified threshold (column 1) in SemCor. The table
shows that although having a first sense from SemCor is certainly useful, when looking
at figures for all the words in the Senseval documents a good proportion have first
senses other than the one indicated by SemCor. Furthermore, the lower the frequency
in SemCor the more likely that the first sense indicated by SemCor is wrong. (However,
the situation is slightly different for adverbs because there are not many with low
frequency in SemCor and they are on average not very polysemous, so for them a first
sense derived from a resource like SemCor?where one exists?is possibly sufficient.)
These results show that although SemCor is a useful resource, there will always be
words for which its coverage is inadequate. In addition, few languages have extensive
hand-tagged resources or sense orderings produced by lexicographers. Moreover, gen-
eral resources containing word sense information are not likely to be appropriate when
processing language for a wider variety of domains, topics, and genres. What is needed
is a means to find predominant senses automatically.
3. Related Work
Most research in WSD to date has concentrated on using contextual features, typically
neighboring words, to help infer the correct sense of a target word. In contrast, our
work is aimed at discovering the predominant sense of a word from raw text because
7 The tendency for words to be used in only one sense in a given discourse is weaker for fine-grained
distinctions (Krovetz 1998) compared to coarse-grained distinctions (Gale, Church, and Yarowsky 1992).
Nevertheless, even with a fine-grained inventory the first sense heuristic is certainly powerful, as shown
in Table 1.
559
Computational Linguistics Volume 33, Number 4
Table 5
Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data,
broken down by their frequencies of occurrence in SemCor (adverb data is only from
Senseval-2).
1 sense > 1 sense
Frequency % FS = SC FS % % FS = SC FS % Mean polysemy
noun
? 1 (54) 96.3 24.1 3.7 0.0 2.8
? 5 (118) 96.6 43.2 3.4 0.0 3.2
? 10 (191) 96.9 48.7 3.1 0.0 3.3
all (792) 88.8 51.6 11.2 2.5 5.5
verb
? 1 (21) 100.0 33.3 0.0 0.0 2.4
? 5 (64) 98.4 35.9 1.6 1.6 3.2
? 10 (110) 98.2 38.2 1.8 1.8 3.5
all (671) 82.6 39.3 17.4 5.1 9.0
adjective
? 1 (31) 93.5 19.4 6.5 0.0 2.5
? 5 (83) 95.2 34.9 4.8 1.2 2.7
? 10 (120) 90.8 40.8 9.2 1.7 2.8
all (385) 82.6 46.2 17.4 3.6 5.1
adverb
? 1 (1) 0.0 0.0 100.0 0.0 2.0
? 5 (2) 50.0 50.0 50.0 0.0 2.0
? 10 (8) 87.5 62.5 12.5 0.0 2.3
all (111) 82.9 62.2 17.1 5.4 4.0
the first sense heuristic is so powerful, and because manually sense-tagged data is not
always available.
Lapata and Brew (2004) highlighted the importance of a good prior in WSD. They
used syntactic evidence to find a prior distribution for Levin (1993) verb classes, and
incorporated this in a WSD system. Lapata and Brew obtained their priors for verb
classes directly from subcategorization evidence in a parsed corpus, whereas we use
parsed data to find distributionally similar words (nearest neighbors) to the target
word which reflect the different senses of the word and have associated distributional
similarity scores which can be used for ranking the senses according to prevalence.
We would, however, agree that subcategorization evidence should be very useful for
disambiguating verbs, and would hope to combine such evidence with our ranking
models for context-based WSD.
A major benefit of our work is that this method permits us to produce predominant
senses for any desired domain and text type. Buitelaar and Sacaleanu (2001) explored
ranking and selection of synsets in GermaNet for specific domains using the words
in a given synset, and those related by hyponymy, and a term relevance measure
taken from information retrieval. Buitelaar and Sacaleanu evaluated their method on
identifying domain-specific concepts using human judgments on 100 items. We evaluate
560
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
our method using publicly available resources for balanced text, and, for domain-
specific investigations, resources we have developed ourselves (Koeling, McCarthy,
and Carroll 2005). Magnini and Cavaglia` (2000) associated WordNet word senses with
particular domains, and this has proved useful for high precision WSD (Magnini et
al. 2001); indeed, we have used their domain labels (or subject field codes, SFCs) for
evaluation (Section 6.3). Identification of these SFCs for word senses was semi-automatic
and required a considerable amount of hand-labeling. Our approach requires only raw
text from the given domain and because of this it can easily be applied to a new domain
or sense inventory, as long as there is enough appropriate text.
There are other approaches aimed at gleaning domain-specific information from
raw data. Gliozzo, Giuliano, and Strapparava (2005) induced domain models from raw
data using unsupervised latent semantic models and then fed this into a supervised
WSD model and evaluated on Senseval-3 lexical sample data in four languages. Chan
and Ng (2005) obtained probability distributions to feed into their supervised WSD mod-
els. They used multilingual parallel corpus data to provide probability estimates for a
subset of 22 nouns from the lexical sample task. They then fed this into a supervised WSD
model and verified that the estimates for prior distributions improved performance for
supervised WSD. We intend eventually to use our prevalence scores to feed into un-
supervised WSD models. Although unsupervised models seem to be beaten whenever
there is training data to be had, we anticipate that unsupervised models with improved
priors from the ranking might outperform supervised systems in situations where there
is little training data available. Whereas this article is about finding predominant senses
for back-off in a WSD system, the method could be applied to finding a prior distribution
over all word senses of each target word. It is our intention that the back-off models pro-
duced by our prevalence ranking, either as predominant senses or prior distributions
over word senses, could be combined with contextual information for WSD.
Mohammad and Hirst (2006) describe an approach to acquiring predominant senses
from corpora which makes use of the category information in the Macquarie Thesaurus.
Evaluation is performed on an artificially constructed test set from unambiguous words
in the same category as the 27 test words (nouns, verbs, and adjectives). The senses of
the words are the categories of the thesaurus and the experiment uses only two senses
of each word, the two most predominant ones. The predominance of the two senses is
altered systematically. The results are encouraging because a much smaller amount of
corpus data is needed compared to our approach. However, their method has only been
applied to an artificially constructed test set, rather than a publicly available corpus, and
has yet to be applied in a domain-specific setting, which is the chief motivation of our
work.
The work of Pantel and Lin (2002) is probably the most closely related study
that predates ours, although their ultimate goal is different. Pantel and Lin devised
a method called CBC (clustering by committee) where the 10 nearest neighbors of
a word in a distributional thesaurus are clustered to identify the various senses of
the word. Pantel and Lin use a measure of semantic similarity (Lin 1997) to evaluate
the discovered classes with respect to WordNet as a gold standard. The CBC method
obtained a precision of 61% (the percentage of senses discovered that did exist in
WordNet) and a recall of 51% (the percentage of senses discovered from the union of
those discovered with different clustering algorithms that they tried).8
8 The calculation of recall was over the union of senses discovered automatically, rather than over the
senses in WordNet, because senses in WordNet may be unattested in the data.
561
Computational Linguistics Volume 33, Number 4
Pantel and Lin?s approach is related to ours in that, in their sense discovery pro-
cedure, predominant senses have more of a chance of being found than other senses,
although their algorithm is specifically tailored to look for senses regardless of fre-
quency. To do this the algorithm removes neighbors of the target word once they
are assigned to a cluster so that less frequent senses can be discovered. Our method,
described in detail in Section 4, associates the nearest neighbors to the senses of the
target in a predefined inventory (we use WordNet). We rank the senses using a measure
which sums over the distributional similarity of neighbors weighted by the strength of
the association between the neighbors and the sense. This is done on the assumption
that more prevalent senses will have strong associations with more nearest neighbors
because they have occurred in more contexts in the corpus used for producing the
thesaurus. Both the number and the distributional similarity of the neighbors are used
in our prevalence ranking measure. Pantel and Lin process the possible clusters in order
of their average distributional similarity and number of neighbors but do not take the
number of neighbors into account in the scores given for the clusters. The measures
that Pantel and Lin associate with their clusters are determined by the cohesiveness
of the cluster with the target word because their aim is one of sense discovery. Their
measure is the similarity between the cluster and the target word and does not retain
the distributional similarity of the neighbors within the cluster. It is quite possible that
there is a low frequency sense of a target word with synonyms that form a nice cohesive
group.
Although the number of neighbors assigned to a cluster may correlate with our
ranking score, intuition suggests that a combination of the quantity and distributional
similarity of neighbors to the target word sense is best for determining the relative
predominance of senses. In Section 6 we test this hypothesis using a simplified version
of our method which only uses the number of neighbors, and assigns each to one
sense. Comparisons with the CBC algorithm as it stands would be difficult because
in order to evaluate acquisition of predominance information we have used publicly
available gold-standard sense-tagged corpora, and these have WordNet senses. CBC
will not always find WordNet senses. For example, using the on-line demonstration of
CBC,9 several common senses from nouns from the Senseval-2 lexical sample are not
discovered, including the upright object sense of post, the block of something sense
of bar, the daytime sense of day and the meaning of the word sense of the word sense.
Automatic acquisition of sense inventories is an important endeavor, and we hope to
look at ways of combining our method for detecting predominance with automatically
induced inventories such as those produced by CBC. Evaluation of induced inventories
should be done in the context of an application, because the senses will be keyed to the
acquisition corpus and not to WordNet.
Induction of senses allows coverage of senses appearing in the data that are not
present in a predefined inventory. Although we could adapt our method for use with
an automatically induced inventory, our method which uses WordNet might also be
combined with one that can automatically find new senses from text and then relate
these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with
unknown nouns.
9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with the
option to include all corpora (TREC-2002, TREC-9, and COSMOS).
562
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
4. Method
In our method, the predominant sense for a target word is determined from a preva-
lence ranking of the possible senses for that word. The senses come from a predefined
inventory (which might be a dictionary or WordNet-like resource). The ranking is
derived using a distributional thesaurus automatically produced from a large corpus,
and a semantic similarity measure defined over the sense inventory. The distributional
thesaurus contains a set of words that are ?nearest neighbors? to the target word with
respect to similarity of the way in which they are distributed. (Distributional similarity
is based on the hypothesis of Harris, 1968, that words which occur in similar contexts
have related meanings.) The thesaurus assigns a distributional similarity score to each
neighbor word, indicating its closeness to the target word. For example, the nearest10
neighbors of sandwich might be:
salad, pizza, bread, soup...
and the nearest neighbors of the polysemous noun star11 might be:
actor, footballer, planet, circle...
These neighbors reflect the various senses of the word, which for star might be:
 a celebrity
 a celestial body
 a shape
 a sign of the zodiac12
We assume that the number and distributional similarity scores of neighbors pertaining
to a given sense of a target word will reflect the prevalence of that sense in the corpus
from which the thesaurus was derived. This is because the more prevalent senses of the
word will appear more frequently and in more contexts than other, less prevalent senses.
The neighbors of the target word relate to its senses, but are themselves word forms
rather than senses. The senses of the target word are predefined in a sense inventory
and we use a semantic similarity score defined over the sense inventory to relate the
neighbors to the various senses of the target word. The two semantic similarity scores
that we use in this article are implemented in the WordNet similarity package. One uses
the overlap in definitions of word senses, based on Lesk (1986), and the other uses a
combination of corpus statistics and the WordNet hyponym hierarchy, based on Jiang
and Conrath (1997). We describe these fully in Section 4.2. We now describe intuitively
10 In this and other examples we restrict ourselves to four neighbors for brevity.
11 In this example we assume that the sense inventory assigns four senses to star, but the inventory could
assign fewer or more depending on its level of granularity and level of detail.
12 Note that this zodiac or horoscope sense of star usually occurs as part of the multiword star sign (e.g.,
your star sign secrets revealed) or in plural form (your stars today?free online).
563
Computational Linguistics Volume 33, Number 4
the measure for ranking the senses according to predominance, and then give a more
formal definition.
The measure uses the sum total of the distributional similarity scores of the k nearest
neighbors. This total is divided between the senses of the target word by apportioning
the distributional similarity of each neighbor to the senses. The contribution of each
neighbor is measured in terms of its distributional similarity score so that ?nearer?
neighbors count for more. The distributional similarity score of each neighbor is divided
between the various senses rather than attributing the neighbor to only one sense. This
is done because neighbors can relate to more than one sense due to relationships such
as systematic polysemy. For example, in the thesaurus we describe subsequently in
Section 4.1 acquired from the BNC, chicken has neighbors duck and goose which relate to
both the meat and animal senses. We apportion the contribution of a neighbor to each
of the word senses according to a weight which is the normalized semantic similarity
score between the sense and the neighbor. We normalize the semantic similarity scores
because some of the semantic similarity scores that we use, described in Section 4.2,
can get disproportionately large. Because we normalize the semantic similarity scores,
the sum of the ranking scores for a word equals the sum of the distributional similarity
scores. To summarize, we rank the senses of the target word, such as star, by apportion-
ing the distributional similarity scores of the top k neighbors between the senses. Each
distributional similarity score (dss) is weighted by a normalized semantic similarity
score (sss) between the sense and the neighbor. This process is illustrated in Figure 1.
More formally, to find the predominant sense of a word (w) we take each sense
in turn and obtain a prevalence score. Let Nw = {n1, n2...nk} be the ordered set of the
top scoring k neighbors of w from the distributional thesaurus with associated scores
{dss(w, n1), dss(w, n2), ...dss(w, nk)}. Let senses(w) be the set of senses of w in the sense
inventory. For each sense of w (si ? senses(w)) we obtain a prevalence score by summing
over the dss(w, nj) of each neighbor (nj ? Nw) multiplied by a weight. This weight is the
sss between the target sense (si) and nj divided by the sum of all sss scores for senses(w)
Figure 1
The prevalence ranking process for the noun star.
564
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
and nj. sss is the maximum WordNet similarity score (sss?) between si and the senses of
nj (sx ? senses(nj)).13 Each sense si ? senses(w) is therefore assigned a score as follows:
Prevalence Score(w, si) =
?
nj?Nw
dss(w, nj) ?
sss(si, nj)
?
si??senses(w) sss(si? , nj)
(1)
where
sss(si, nj) = max
sx?senses(nj )
sss?(si, sx) (2)
We describe dss and sss? in Sections 4.1 and 4.2. Note that the dss for a given neighbor
is shared between the different senses of w depending on the weight given by the
normalized sss.
4.1 The Distributional Similarity Score
Measures of distributional similarity take into account the shared contexts of the two
words. Several measures of distributional similarity have been described in the litera-
ture. In our experiments, dss is computed using Lin?s similarity measure (Lin 1998a).
We set the number of nearest neighbors to equal 50.14 We use three different sources of
data for our first two experiments, resulting in three distributional thesauruses. These
are described in the next section. We use domain-specific data for our third and fourth
experiments. The data sources for these are described in Sections 6.3 and 6.4.
A word, w, is described by a set of features, f , each with an associated frequency,
where each feature is a pair ?r, x? consisting of a grammatical relation name and the
other word in the relation. We computed distributional similarity scores for every pair of
words of the same PoS where each word?s total feature frequency was at least 10. A the-
saurus entry of size k for a target word w is then defined as the k most similar words to w.
A large number of distributional similarity measures have been proposed in the
literature (see Weeds 2003 for a review) and comparing them is outside the scope of this
work. However, the study of Weeds and Weir (2005) provides interesting insights into
what makes a ?good? distributional similarity measure in the contexts of semantic simi-
larity prediction and language modeling. In particular, weighting features by pointwise
mutual information (Church and Hanks 1989) appears to be beneficial. The pointwise
mutual information (I(w, f )) between a word and a feature is calculated as
I(w, f ) = log
P( f |w)
P( f )
(3)
Intuitively, this means that the occurrence of a less-common feature is more important
in describing a word than a more-common feature. For example, the verb eat is more
selective and tells us more about the meaning of its arguments than the verb be.
13 We use sss for the semantic similarity between a WordNet sense and another word, the neighbor. We use
sss? for the semantic similarity between two WordNet senses, si and a sense of the neighbor (sx).
14 From previous work (McCarthy et al 2004b), the value of k has a minimal effect on finding the
predominant sense; however, we will continue experimentation with this in the future for using our
ranking score for estimating probability distributions of senses, because a sufficiently large value of k will
be needed to include neighbors for rarer senses.
565
Computational Linguistics Volume 33, Number 4
We chose to use the distributional similarity score described by Lin (1998a) because
it is an unparameterized measure which uses pointwise mutual information to weight
features and which has been shown (Weeds 2003) to be highly competitive in making
predictions of semantic similarity. This measure is based on Lin?s information-theoretic
similarity theorem (Lin 1997):
The similarity between A and B is measured by the ratio between the amount of
information needed to state the commonality of A and B and the information needed to
fully describe what A and B are.
In our application, if T(w) is the set of features f such that I(w, f ) is positive, then the
similarity between two words, w and n, is
dss(w, n) =
?
f?T(w)?T(n)
(
I(w, f ) + I(n, f )
)
?
f?T(w) I(w, f ) +
?
f?T(n) I(n, f )
(4)
However, due to this choice of dss and the openness of the domain, we restrict ourselves
to only considering words with a total feature frequency of at least 10. Weeds et al (2005)
do show that distributional similarity can be computed for lower frequency words but
this is using a highly specialized corpus of 400,000 words from the biomedical domain.
Further, it has been shown (Weeds et al 2005; Weeds and Weir 2005) that performance
of Lin?s distributional similarity score decreases more significantly than other measures
for low frequency nouns. We leave the investigation of other distributional similarity
scores and the application to smaller corpora as areas for further study.
4.2 The Semantic Similarity Scores
WordNet is widely used for research in WSD because it is publicly available and there
are a number of associated sense-tagged corpora (Miller et al 1993; Cotton et al 2001;
Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) available for testing purposes.
Several semantic similarity scores have been proposed that leverage the structure of
WordNet; for sss? we experiment with two of these, as implemented in the WordNet
Similarity Package (Patwardhan and Pedersen 2003).
The WordNet Similarity Package implements a range of similarity scores. McCarthy
et al (2004b) experimented with six of these for the sss? used in the prevalence score,
Equation (2). In the experiments reported here we use the two scores that performed
best in that previous work. We briefly summarize them here; Patwardhan, Banerjee,
and Pedersen (2003) give a more detailed discussion. The scores measure the similarity
between two WordNet senses (s1 and s2).
lesk This measure (Banerjee and Pedersen 2002) maximizes the number of overlap-
ping words in the gloss, or definition, of the senses. It uses the glosses of semanti-
cally related (according to WordNet) senses too. We use the default version of the
measure in the package with no normalizing for gloss length, and the default set
of relations:
lesk(s1, s2) = |{W1 ? definition(s1)}| ? |{W2 ? definition(s2)}| (5)
where definitions(s) is the gloss definition of sense s concatenated with the gloss
definitions of the senses related to s where the relationships are defined by the de-
566
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
fault set of relations in the relations.dat file supplied with the WordNet Similarity
package. W ? definition(s) is the set of words from the concatenated definitions.
jcn This measure (Jiang and Conrath 1997) uses corpus data to populate classes
(synsets) in the WordNet hierarchy with frequency counts. Each synset is incre-
mented with the frequency counts (from the corpus) of all words belonging to
that synset, directly or via the hyponymy relation. The frequency data is used to
calculate the ?information content? (IC; Resnik 1995) of a class as follows:
IC(s) = ?log(p(s)) (6)
Jiang and Conrath specify a distance measure:
Djcn(s1, s2) = IC(s1) + IC(s2) ? 2 ? IC(s3) (7)
where the third class (s3) is the most informative, or most specific, superordinate
synset of the two senses s1 and s2. This is converted to a similarity measure
in the WordNet Similarity package by taking the reciprocal as in Equation (8)
(which follows). For this reason, the jcn values can get very large indeed when
the distances are negligible, for example where the neighbor has a sense which is
a synonym. This is a motivation for our normalizing the sss in Equation (1).
jcn(s1, s2) = 1/Djcn(s1, s2) (8)
The IC data required for the jcn measure can be acquired automatically from raw
text. We used raw data from the BNC to create the IC files. There are various parameters
that can be set in the WordNet Similarity Package when creating these files; we used
the RESNIK method of counting frequencies in WordNet (Resnik 1995), the stop words
provided with the package, and no smoothing.
The lesk score is applicable to all parts of speech, whereas the jcn is applicable
only to nouns and verbs because it relies on IC counts which are obtained using the
hyponym links and these only exist for nouns and verbs.15 However, we did not use
jcn for verbs because in previous experiments (McCarthy et al 2004c) the lesk measure
outperformed jcn because the structure of the hyponym hierarchy is very shallow for
verbs and the measure is therefore considerably less informative for verbs than it is for
nouns.
4.3 An Example
We illustrate the application of our measure with an example. For star, if we set16 k = 4
and have the dss for the previously given neighbors as in the first row of Table 6, and
15 For verbs these pointers actually encode troponymy, which is a particular kind of entailment relation,
rather than hyponymy.
16 In this example, as before, we set k to 4 for the sake of brevity.
567
Computational Linguistics Volume 33, Number 4
Table 6
Example dss and sss scores for star and its neighbors.
Neighbors of star (dss)
Senses actor (0.22) footballer (0.12) planet (0.08) circle (0.03)
celebrity 0.42 0.53 0.02 0.01
celestial body 0.01 0.01 0.68 0.10
shape 0.0 0.0 0.02 0.78
zodiac 0.03 0.03 0.21 0.01
Total 0.46 0.57 0.93 0.90
the sss between the senses and the neighbors as in the remaining rows, the prevalence
score for celebrity would be:
= 0.22 ? 0.420.46 + 0.12 ? 0.530.57 + 0.08 ? 0.020.93 + 0.03 ? 0.010.90
= 0.2009 + 0.1116 + 0.0017 + 0.0003
= 0.3145
The prevalence score for each of the senses would be:
prevalence score(celebrity) = 0.3145
prevalence score(celestial body) = 0.0687
prevalence score(shape) = 0.0277
prevalence score(zodiac) = 0.0390
so the method would select celebrity as the predominant sense.
5. Experimental Setup
5.1 The Distributional Thesauruses
The three thesauruses used in our first two experiments were all created automatically
from raw corpus data, based either on grammatical relations between words computed
by syntactic parsers or alternatively on word proximity relations.
We created the first thesaurus, which we call BNC, from grammatical relation output
produced by the RASP system (Briscoe and Carroll 2002) applied to the 90M words of
the ?written? portion of the British National Corpus (Leech 1992), for all polysemous
nouns, verbs, adjectives, and adverbs in WordNet. For each word we considered co-
occurring words in the grammatical contexts listed in Table 7.
In the first two experiments, we also use two further automatically computed
distributional thesauruses, produced by Dekang Lin from 125M words of text from the
Wall Street Journal, San Jose Mercury News, and AP Newswire, using the same similarity
measure. The thesauruses are publicly available.17 One was constructed based on word
17 The thesauruses are available for download from http://www.cs.ualberta.ca/~lindek/
downloads.htm.
568
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 7
Grammatical contexts used for acquiring the BNC thesaurus.
PoS Grammatical contexts
noun verb in direct object or subject relation, adjective or noun modifier
verb noun as direct object or subject
adjective modified noun, modifying adverb
adverb modified adjective or verb
Table 8
Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6.
PoS Thesaurus types NISC NITH
noun BNC 7,090 2,436 115
noun DEP 6,583 2,176 217
noun PROX 6,582 2,176 217
verb BNC 2,958 553 45
adjective BNC 3,659 1,208 123
adverb BNC 505 132 38
similarities computed from syntactic dependencies produced by MINIPAR (Lin 1998b),
and the other was constructed based on textual proximity relationships between words.
We refer below to the original corpus as NEWSWIRE, and these two thesauruses as DEP
and PROX, respectively. We restricted our experiments to the nouns in these thesauruses.
Table 8 contains details of the numbers of polysemous (according to WordNet 1.6)
words contained in these thesauruses, the number of words in SemCor that were not
found in these thesauruses (NITH) and the number of words in the thesauruses that
were not in SemCor (NISC).
For the experiments described in Sections 6.3 and 6.4 we use exactly the same
method as that proposed for the BNC thesaurus, however the data source is different
and is described in those sections.
5.2 The Sense Inventory
We use WordNet version 1.6 as the sense inventory for our first three experiments, and
1.7.1 for our last experiment.18
For sss? we use the WordNet Similarity Package version 0.05 (Patwardhan and
Pedersen 2003).
18 We use 1.6 which is a rather old version of WordNet so that we can directly evaluate on the SemCor data
released with this version; we also use it to enable comparison with the results of McCarthy et al (2004a).
We use WordNet 1.7.1 for the fourth experiment, because this is the version that was used for annotating
the test data in that experiment. We plan to move to more recent versions of WordNet and experiment
with other sense inventories in the future.
569
Computational Linguistics Volume 33, Number 4
6. Experiments
In this section we describe four experiments using our method for acquiring predomi-
nant sense information.
The first experiment evaluates automatically acquired predominant senses for all
parts of speech, using SemCor as the test corpus. This extends previous work which
had only evaluated all PoS on Senseval-2 (Cotton et al 2001) and Senseval-3 (Mihalcea
and Edmonds 2004) data. The SemCor corpus is composed of 220,000 words, in contrast
to the 6 documents in the Senseval-2 and -3 English all-words data (10,000 words). We
examine the effects of using the two different semantic similarity scores that performed
well in previous work: jcn is quick to compute but lesk has the advantage that it is
applicable to all PoS and can be implemented for any dictionary with sense defini-
tions. We compare three thesauruses: one is derived from the BNC and two from the
NEWSWIRE corpus. The two from the NEWSWIRE corpus examine the requirement for
a parser by contrasting results obtained when the thesaurus is built using parsed data
compared to a proximity approach. We contrast the results of the BNC thesaurus with
a simplified version of the prevalence score which uses the number of the k neighbors
closest to a sense for ranking without using the dss and without sharing the credit for
a neighbor between senses. We also perform an error analysis on a random sample
of words for which a predominant sense was found that differed from that given by
SemCor, identifying and giving an indication of the frequencies of the main sources of
error.
The second experiment is on nouns in the Senseval-2 all-words data, again using
predominant senses acquired using each of the three distributional thesauruses, but
in this experiment we explore the benefits of an automatic first sense heuristic when
there is inadequate data in available resources. Although McCarthy et al (2004c) show
that on Senseval-2 and Senseval-3 test data a first sense heuristic derived from SemCor
outperforms the automatic method, we look at whether the method?s performance is
relatively stronger on words for which there is little data in SemCor. This is important
because, as we have shown in Table 5, low frequency words are used often in senses
other than the sense that is ranked first according to SemCor.
In addition to the issue of lack of coverage of manually annotated resources, sense
frequency will depend on the domain of the data. In the third experiment, we revisit
some previous work on noun senses and domain (McCarthy et al 2004a) using corpora
of news text about sports and finance. Using distributional thesauruses computed from
these corpora and a gold standard domain labeling of word senses we look at the
potential for computing domain-specific predominant senses for parts of speech other
than nouns.
Continuing the line of research on automatic acquisition of domain-specific pre-
dominant senses, the fourth experiment compares results when we train and test on
domain-specific corpora, where the training data is (1) manually categorized for domain
and from the same corpus as the gold-standard test data, and (2) where the training data
is harvested automatically from another corpus which is categorized automatically.
6.1 Experiment 1: All Parts of Speech
In this experiment, we evaluate the accuracy of automatically acquired predominant
senses for all open class parts of speech, taking SemCor as the gold standard. For nouns
we use the semantic similarity measures lesk and jcn, and for other parts of speech, lesk.
We use the three distributional thesauruses BNC, DEP, and PROX.
570
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The gold standard is derived from the Brown Corpus files publicly released as part
of SemCor, rather than the processed data provided in the cntlist file in the WordNet
distribution. The released SemCor files contain only the tagged data from the Brown
Corpus and do not include data from The Red Badge of Courage. We use the released data
rather than that in cntlist because this includes the actual tagged examples which are
marked for genre by the Brown files. We envisage the possibility of further experiments
with these genre markers. We only evaluate on instances where a single, unique sense
is supplied by the annotators. So, for example, we ignore instances like the following
with multiple wnsn values:
<wf cmd=done pos=NN lemma=tooth wnsn=3;1 lexsn=1:05:02::;1:08:00::>tooth</wf>
We also only evaluate on polysemous words (according to WordNet) having one sense
in SemCor which is more frequent than any other, and for which both SemCor and our
thesauruses have at least a minimal amount of data. Specifically, a word must occur
three or more times in SemCor; it must also occur in ten or more grammatical relations
in the parsed version of the BNC and have neighbors in the distributional thesaurus, or
be present in Dekang Lin?s thesaurus.19
We evaluate on nouns, verbs, adjectives, and adverbs separately, computing a num-
ber of accuracy measures, both type-based and token-based. PSacc is calculated over
word types in SemCor which have one sense which occurs more than any other. It is the
accuracy of identifying the predominant sense in SemCor. If the automatic ranking has
a tie for the top ranked sense then we score that word as incorrect.20 So we have
PSacc =
|correcttyp|
|typesmf |
? 100 (9)
where typesmf are the types in SemCor such that one sense is more frequent than
any other, the word has occurred at least three times in SemCor and has an entry
in the thesaurus. |correcttyp| is the number of these where the automatically acquired
predominant sense matches the first sense in SemCor.
PSaccBL is the predominant sense random baseline, obtained as follows:
PSaccBL =
?
w?typesmf
1
|senses(w)|
|typesmf |
? 100 (10)
WSDsc is a token-based measure. It is the WSD accuracy that would be obtained
by using the first sense heuristic with the automatically acquired predominant sense
information, in cases where there was a unique automatic top ranked sense:
WSDsc =
|correcttok|
|SCtokensafs|
? 100 (11)
19 Although we do not evaluate words for which there were no neighbors in the thesaurus, we could extend
the thesaurus to include some of these by widening the range of grammatical relations covered and
compensating for some systematic PoS tagging errors.
20 If we exclude these words with joint top ranking from the automatic method (precision rather than recall)
then we obtain marginally higher accuracy for the jcn measure but no difference for lesk.
571
Computational Linguistics Volume 33, Number 4
where |correcttok| is the number of tokens disambiguated correctly out of the tokens in
SemCor having an automatically acquired first sense (SCtokensafs).
SC FS is the WSD accuracy of the SemCor first sense heuristic on the same set of
tokens (SCtokensafs), which is the upper bound because the information it uses is derived
from the test data itself. RBL is the random baseline for the WSD task, calculated by
splitting the credit for each token to be tagged in the test data evenly between all of the
word?s senses.
RBL =
?
w?SCtokensafs
1
|senses(w)|
|SCtokensafs|
? 100 (12)
The results are shown in Table 9. We examined differences between the semantic
similarity measures (lesk and jcn), the BNC and DEP thesauruses, and the DEP and
PROX thesauruses using the ?2 test of significance with one degree of freedom (Siegel
and Castellan 1988). None of the differences between the different combinations of
similarity measures and thesauruses for the type-based measure PSacc are significant.
The differences between lesk and jcn are significant for the token-based measure WSDsc
for both the BNC and PROX thesauruses (both p < .001), however not when comparing
lesk and jcn for the DEP thesaurus. Although lesk is more accurate than jcn, at least on
the WSD task, jcn is much faster because of the precompilation of IC in the WordNet
similarity package; however, lesk has the additional benefit of being applicable to other
parts of speech. The method gives particularly good results for adjectives, given that
they have a similar random baseline to nouns. It does not do so well for adverbs and
verbs, but still performs well above the random baseline which is low for verbs due
to their high degree of polysemy. Given that the first sense heuristic from SemCor is
particularly strong for adverbs, it is disappointing that the automatic method does not
perform as well as it does on adjectives. One possible reason for this might be that
adverbs are often less strongly associated to the verbs that they modify than adjectives
are to the nouns that they modify, so the distributional thesaurus information is less
reliable. Another reason may be that less data are available for adverbs, both in the
thesaurus and also in WordNet.
Table 9
Evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 54.5 32.3 53,468 48.7 68.6 24.7
noun lesk DEP 2,437 56.3 32.1 52,158 49.2 68.4 24.6
noun lesk PROX 2,437 55.9 32.1 52,158 49.0 68.4 24.6
noun jcn BNC 2,555 54.0 32.3 53,429 46.1 68.6 24.7
noun jcn DEP 2,436 56.4 32.1 52,122 48.8 68.4 24.6
noun jcn PROX 2,436 55.9 32.1 52,117 47.7 68.4 24.6
verb lesk BNC 1,149 45.6 27.1 31,182 36.1 57.1 17.1
adjective lesk BNC 1,154 60.4 32.8 18,216 56.8 73.8 24.9
adverb lesk BNC 230 52.2 39.9 8,810 43.2 76.1 33.0
572
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Comparing the results for the DEP and the PROX thesauruses, we see that although
there is no significant difference in PSacc (with either lesk or jcn), there is for WSDsc
when using jcn (p < .001), but not when comparing the lesk values for these thesauruses.
Even though the differences between jcn DEP and jcn PROX are significant, the absolute
differences are nevertheless relatively small; this bodes well for applying the automatic
predominant sense method to languages less well resourced than English, because
the PROX thesaurus was produced without using a parser. The differences in results
between jcn BNC and jcn DEP for nouns are statistically significant (p < .001).21 The
better accuracy with DEP may be because the NEWSWIRE corpus is larger than the
BNC. We intend to investigate the effects of corpus size in the future. The differences in
results between lesk BNC and lesk DEP for nouns are not significant.
6.1.1 Results Using Simplified Prevalence Score. A simple variation of our method is just to
associate each neighbor with just one sense and use the number of neighbors associated
with a sense for the prevalence score. This gives a modified version of Equation (1)
where each sense si ? senses(w) is assigned a score as follows:
Simplified Prevalence Score(w, si) = |{nj ? Nw} : arg max
sk?senses(w)
(sss(sk, nj)) = si| (13)
where
sss(sk, nj) = max
sx?senses(nj )
sss?(sk, sx) (14)
For the example in Table 6, celebrity would get the top score of 2 (due to it having
the highest sss for actor and footballer), celestial body would get a score of 1 (due to its
sss with planet), shape would get 1 (due to circle), and zodiac would obtain a Simplified
Prevalence Score of 0 because it does not have the highest sss for any of the neighbors.
As the results from Table 10 show, we do not get such good results with this score.
This supports our intuition that a combination of both the number of neighbors and
their distributional similarity scores is important for determining predominance. The
rest of the article gives results and analysis for our original prevalence score as given in
Equation (1).
6.1.2 Error Analysis. We took a random sample of 80 words that occurred more than five
times in SemCor, 20 words for each PoS, from those where the automatically identified
predominant sense was different from the SemCor first sense when using the lesk sss
and BNC thesaurus and our ranking score as defined in Equation (1) (i.e., the data
represented by the first result line and the last three result lines of Table 9). Herein, we
call the automatically identified sense AUTO FS, and the SemCor sense SemCor FS. We
21 The coverage of the SemCor data by the DEP and PROX thesauruses is slightly lower than that of the
BNC-derived thesaurus due to mismatches in spelling and capitalization and also probably because the
NEWSWIRE corpus is narrower in genre and domain than the BNC.
573
Computational Linguistics Volume 33, Number 4
Table 10
Simplified prevalence score, evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 52.9 32.3 53,175 47.2 68.6 24.7
noun jcn BNC 2,555 50.1 32.3 52,033 46.7 69.2 24.8
verb lesk BNC 1,149 45.1 27.1 30,364 36.7 58.0 17.4
adjective lesk BNC 1,154 58.3 32.8 18,136 56.0 73.7 24.8
adverb lesk BNC 230 50.0 39.9 8,802 42.2 76.1 33.0
manually inspected the data for each of the words to find the source of the problem.
We did not have the (substantial) resources that would be required to sense tag all
occurrences of these words in the BNC to see what their actual first senses were. Instead,
we examined the parses, grammatical relations, and sense definitions for the words to
see why the AUTO FS was ranked above the SemCor FS. We found the following main
types of error:22
corpora The difference appears to be due to genuine divergence between the BNC
and SemCor. For this error type we looked at the BNC parses to see if the acquired
predominant sense was clearly due to differences in the corpus data. There may
be other errors that should have been assigned this category, but without access
to sense tagged BNC data we could not be sure of this, so we used this category
conservatively. An example of this error is the adjective solid which has the good
quality first sense in the Brown files in SemCor, but the firm sense according to
our BNC automatic ranking.
related The automatic predominant sense is closely related to the SemCor first sense.
Although many word senses are related to some extent, the category was picked
where a close relationship seemed to be the main cause of the error. An example
is the noun straw which has two senses in WordNet 1.6, fibre used for hats and
fodder and plant material. The SemCor FS was the former whereas our AUTO FS
was the latter.
competing Two or more related senses are ranked highly but they are overtaken by
an unrelated sense. For example, the ranking and scores for the noun transmission
are:
WordNet sense Description Prevalence score
5 gears 1.79
2 communication 1.20
1 act of sending a message 1.19
3 fraction of radiant energy 0.48
4 infection 0.15
22 There were a few other, less numerous types of error, for example systematic PoS mis-tagging of particles
(such as down) as adverbs.
574
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The act of sending a message sense is overtaken by the gears sense because the
credit from shared distributional neighbors is split between it and the communi-
cation sense.
neighbors There are not many neighbors related to the sense. There can be various
reasons for this, such as the sense having restricted contexts of occurrence or only
a small number of near synonyms existing for the sense. An example of this is
the adjective live where the SemCor FS unrecorded sense seems to occur in the
BNC corpus more than the alive sense; there are plenty of grammatical relations
pertaining to this sense, but there are few distributional neighbors near in meaning
to unrecorded.23
spurious similarity The WordNet similarity scores were misled by spurious relation-
ships to neighbors; this can occur in dense areas such as the ?physical object?
region of the noun hyponym hierarchy. An example of this is the verb tap which
has neighbors push and press which are related to the AUTO FS (solicit) as well as
the SemCor FS (strike lightly).
The results of the error analysis are shown in Table 11. The analysis shows that
differences between the training (BNC) and testing (SemCor) corpora are not a major
source of error. Although SemCor itself (the released files from the Brown corpus
comprising only 200,000 words) is not large enough to build a thesaurus with entries
for a reasonable portion of the words, we did build a thesaurus from the entire Brown
corpus (1 million words) to see the effect of corpus data. The results are compared
to those from the BNC in Table 12 on the set of words which had thesaurus entries
in the Brown data (to make the results more comparable, because the corpora are of
such different sizes). We also show the average results for 10 random selections of a
1 million word random sample of the BNC. To do this we randomly selected 190 th of the
tuples.24 The differences in the WSDsc for the BNC 190 sample and the Brown corpus are
significant (p < .01 on the ?2), but the differences in PSacc are not significant. Although
the entire BNC produced better results than the Brown data, this is undoubtedly due to
the difference in size of the corpus. Taking a comparably sized sample, the results are
slightly better from Brown which is the corpus from which SemCor is taken.
For nouns, it was apparent that in two cases less-prevalent senses were receiving a
higher ranking simply because the credit for some neighbors associated with another
meaning was split between related senses (error type competing). This was not ob-
served for other parts of speech, possibly because the AUTO FS was rarely unrelated to
the SemCor FS.
There were some problems arising from spurious similarity. One possible source
of such problems is due to the ambiguity of the neighbor; in the future we will look
at reducing this source of error by removing neighbors which have a value for sx in
Equation (2) which is not the same as that preferred by the other senses of the target
word (w). For adverbs, all the cases that were categorized as spurious similarity were
also noted to be related to the SemCor FS, though they were not categorized as related
as this was not considered the primary cause of the error.
The analysis was hardest for verbs. Verbs are on average highly polysemous, and
often have senses that are related. Furthermore, the structure of the WordNet verb tro-
ponym hierarchy is very shallow compared to the noun hyponymy hierarchy, so there
23 The closest neighbors to the adjective live are adult, forthcoming, lively, solo, excellent, stuffed, living, dead,
and australian weekly.
24 The variance for the 190 sample for PSacc was 0.46 and for WSDsc it was 0.49.
575
Computational Linguistics Volume 33, Number 4
Table 11
Results of the error analysis for the sample of 80 words.
PoS
noun verb adjective adverb All PoS
corpora 1 2 1 1 5
related 8 12 13 8 41
competing 2 0 0 0 2
neighbors 4 3 2 2 11
spurious similarity 5 3 4 9 21
Table 12
SemCor results for Nouns using jcn.
Thesaurus PSacc% WSDsc %
full BNC 53.8 44.9
1
90 BNC 46.6 40.8
Brown 47.2 41.7
are more possibilities for spurious similarities from overlap of glosses. So, although we
tried to identify the main problem source, for verbs the problems usually arose from a
combination of factors and the relatedness of the senses was usually one of these.
Relatedness of senses and fine-grained distinctions are major sources of error. There
have been various attempts to group WordNet senses both manually and automati-
cally (Agirre and Lopez de Lacalle 2003; McCarthy 2006; Palmer, Dang, and Fellbaum
2007). Indeed, McCarthy demonstrated that distributional and semantic similarity can
be used for relating word senses and that such methods increase accuracy of first
sense heuristics, including the automatic method proposed here. WSD is improved with
coarser-grained inventories but ultimately, performance depends on the application.
For example, the noun bar has 11 senses in WordNet 1.6. These include the pub sense
as well as the counter sense and these are related to a certain extent. One might want
to group them when acquiring predominant senses, but there may be situations where
they should be distinguished. For example, if one were to ask a robot to ?go to the bar?
one would hope it could use the context to go get the drinks rather than replying that it
is already there! Even in cases where fine-grained distinctions are ultimately required, it
may be helpful to have a coarse-grained prior and then use contextual features to tease
apart subtle sense distinctions.
From our error analysis, the problem of semantically isolated senses (identified as
neighbors) was not a major source of error, but still causes some problems. One possible
remedy might be to identify these cases by looking for neighbors which relate strongly
to a sense which none of the other neighbors relate to and weighting the contribution
from these neighbors more. This may however give rise to further errors because of the
noise introduced by focusing on individual neighbors. We will explore such directions
in future work.
576
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
In this experiment we did not assign any credit for near misses. In many cases
of error the SemCor FS nonetheless received a high prevalence score. In the future
we hope to use the score for probability estimation, and combine this with contextual
information for WSD as in related work by Lapata and Brew (2004) and Chan and Ng
(2005).
6.2 Experiment 2: Frequency and the SemCor First Sense Heuristic
In the previous section we described an evaluation of the accuracy of automatically
acquired predominant sense information. We carried out the evaluation with respect to
SemCor in order to have as much test data as possible. To obtain reasonably reliable
gold-standard first-sense data and first-sense heuristic upper bounds, we limited the
evaluation to words occurring at least three times in SemCor. Clearly this scenario is
unrealistic. For many words, and particularly for nouns, there is very little or no data in
SemCor; Table 2 shows that 81.9% of nouns (excluding multiwords) listed in WordNet
do not occur at all in SemCor. Thus, even for English, which has substantial manually
sense-tagged resources, coverage is severely limited for many words.
For a more realistic comparison of automatic and manual heuristics, we therefore
now change to a different test corpus, the Senseval-2 English all-words task data set. We
focus on nouns and evaluate using all words regardless of their frequencies in SemCor.
We examine the effect of frequency in SemCor on performance of a SemCor-derived
heuristic in comparison to results from our automatic method on the same words. Our
hypothesis is that although automatically acquired predominant sense information may
not outperform first-sense data obtained from a hand-tagged resource over all words in
a text, the information may well be more accurate for low frequency items.
We use a mapping between different WordNet versions25 (Daude?, Padro?, and Rigau
2000) to obtain the Senseval-2 all words noun data (originally distributed with 1.7 sense
numbers) with 1.6 sense numbers. As well as examining the performance of our method
in contrast to the SemCor heuristic, we calculate an upper bound for this using the
first sense heuristic from the Senseval-2 all-words data itself. This is obtained for nouns
with two or more occurrences in the Senseval-2 data and where one sense occurs more
than any of the others. We calculate type, precision, and recall, using this Senseval-2
first-sense as the gold standard. The recall measure is the same as PSacc described
previously, except that we include items which do not have entries in the thesaurus,
scoring them incorrect. Precision only includes items where there is a sense ranked
higher than any other for that word with the prevalence score, that is, it does not include
items with a joint automatic ranking. We also calculate token precision and recall (WSD).
These measures relate to WSDsc, but again, recall includes words not in the thesaurus
which are scored incorrect, and precision does not include items with a joint automatic
ranking. We also separately compute WSD precision for words not in SemCor (NISC).
The results are shown in Table 13.26
The automatically acquired predominant sense results (the first six lines of results
in the table) are approaching the SemCor-derived results (third line from the bottom of
the table). The NISC results are particularly encouraging, but with the caveat that there
are only 17 such words in the data. The precision for these items is higher than the
25 This mapping is available at http://www.lsi.upc.es/~nlp/tools/mapping.html.
26 Note that these figures are lower than those of McCarthy et al (2004a) in a similar experiment because
the evaluation here is only on polysemous words.
577
Computational Linguistics Volume 33, Number 4
Table 13
Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words
task data.
Type WSD/token
Settings Precision (%) Recall (%) Precision (%) Recall (%) Precision NISC (%)
lesk BNC 56.3 53.7 54.6 53.4 58.3
lesk DEP 52.0 47.2 52.6 48.7 58.3
lesk PROX 52.0 47.2 52.3 48.5 58.3
jcn BNC 52.4 50.0 51.8 50.6 66.7
jcn DEP 52.0 47.2 58.0 53.7 83.3
jcn PROX 53.1 48.1 57.3 53.1 83.3
SemCor 64.8 63.0 58.5 57.3 0.0
Senseval-2 ? ? 90.8 60.1 100.0
RBL 26.5 26.5 26.0 26.0 50.0
overall figure. This is because the nouns involved are less frequent so tend to be less
polysemous and consequently have a higher random baseline. There are a few nouns
that are not in the automatic ranking, but this is due to the fact that neighbors were
not collected for these nouns in the thesaurus because of tagging or parser errors or
the particular set of grammatical relations used. It should be possible to extend the
range of grammatical relations, or use proximity-based relations, so that neighbors can
be obtained in these cases. It would also be possible to assign some credit in the case of
joint top ranked senses to increase coverage.
Looking at Table 13 in more detail, it seems to be the case that although the BNC
thesaurus does well in identifying the first sense of a word (the type results), the PROX
and DEP thesauruses from the NEWSWIRE corpus return better WSD results when
used with the jcn measure. This is possibly because jcn works well for more frequent
items due to its incorporation of frequency information, and the NEWSWIRE corpus
has more data for frequent words, although coverage is not as good as the BNC as
seen by the bigger differences in precision and recall and the figures in Table 8. The
lower coverage may be due to the narrower domain and genre of the NEWSWIRE
corpus, though spelling and capitalization differences probably also account for some
differences.
Table 14 shows results on the Senseval-2 nouns for the best similarity measure
and thesaurus combinations in Table 13 for nouns at or below various frequencies
in SemCor. (The differences between the DEP and PROX thesauruses are negligible at
frequencies of 10 or below, so for those we report only the results for DEP.) As we
anticipated, for low frequency words the automatic methods do give more accurate
predominant sense information than SemCor. The low number of test items at frequency
five or less means that results for jcn with the BNC thesaurus are not significantly better
when compared with SemCor (p = .05); however the lesk WSD results are significantly
better (p < .01 for the ? 1 threshold and p < .05 for the ? 5 threshold). On the whole, we
see that the automatic method, using either jcn or lesk and any of the three thesauruses,
tend to give better results than SemCor on nouns which have low coverage in SemCor.
Figures 2 and 3 show the precision for type and token (WSD) evaluation where the
items have a frequency at or below given thresholds in SemCor. Although the manually
578
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 14
Senseval-2 results, polysemous nouns only, broken down by their frequencies of occurrence in
SemCor.
Type WSD/token
No. of occurrences in Precision Recall Precision Recall
SemCor (no. of words) Settings (%) (%) (%) (%)
jcn BNC 100.0 33.3 66.7 47.1
lesk BNC 100.0 33.3 58.3 41.2
0 (17) jcn DEP 100.0 33.3 83.3 58.8
lesk DEP 100.0 33.3 58.3 41.2
SemCor 0.0 0.0 0.0 0.0
Senseval-2 ? ? 100.0 52.9
RBL 38.9 38.9 46.1 46.1
jcn BNC 66.7 44.4 54.1 45.5
lesk BNC 83.3 55.6 67.6 56.8
jcn DEP 50.0 22.2 51.7 34.1
? 1 (44) lesk DEP 75.0 33.3 69.0 45.5
SemCor 50.0 33.3 33.3 20.5
Senseval-2 ? ? 93.3 63.6
RBL 40.7 40.7 42.8 42.8
jcn BNC 80.0 61.5 63.0 57.5
lesk BNC 90.0 69.2 71.2 65.0
? 5 (80) jcn DEP 71.4 38.5 56.7 42.5
lesk DEP 85.7 46.2 70.0 52.5
SemCor 60.0 46.2 54.0 42.5
Senseval-2 ? ? 95.9 58.8
RBL 38.1 38.1 39.1 39.1
jcn BNC 75.0 63.2 59.3 55.8
lesk BNC 68.8 57.9 62.8 59.2
? 10 (120) jcn DEP 66.7 42.1 56.8 45.0
lesk DEP 58.3 36.8 58.9 46.7
SemCor 68.8 57.9 57.3 49.2
Senseval-2 ? ? 96.8 50.8
RBL 37.5 37.5 38.0 38.0
jcn BNC 76.0 67.9 66.7 64.8
lesk BNC 64.0 57.1 71.6 69.6
? 15 (250) jcn DEP 60.0 42.9 68.8 55.6
lesk DEP 55.0 39.3 67.3 54.4
jcn PROX 70.0 50.0 72.3 58.4
lesk PROX 55.0 39.3 66.8 54.0
SemCor 64.0 57.1 66.5 62.0
Senseval-2 ? ? 98.8 68.4
RBL 32.9 32.9 30.4 30.4
jcn BNC 52.4 50.0 51.8 50.6
lesk BNC 56.3 53.7 54.6 53.4
all (786) jcn DEP 52.0 47.2 58.0 53.7
lesk DEP 52.0 47.2 52.6 48.7
jcn PROX 53.1 48.1 57.3 53.1
lesk PROX 52.0 47.2 52.3 48.5
SemCor 64.8 63.0 58.5 57.3
Senseval-2 ? ? 90.8 60.1
RBL 26.5 26.5 26.0 26.0
579
Computational Linguistics Volume 33, Number 4
Figure 2
?TYPE? precision on finding the predominant sense for the Senseval-2 English all-words test
data for nouns having a frequency less than or equal to various thresholds.
produced SemCor first-sense heuristic outperforms the automatic methods over all the
test items (see the ?all? results in Table 14), when items are below a frequency threshold
of five the automatic methods give better results. Indeed, as the threshold is moved
up to 20 and even 30, more nouns are covered, and the automatic methods are still
comparable and in some cases competitive with the SemCor heuristic.
6.3 Experiment 3: The Influence of Domain
In this experiment, we investigate the potential of the automatic ranking method for
computing predominant senses with respect to particular domains. We have previ-
ously demonstrated that the method produces intuitive domain-specific models for
nouns (McCarthy et al 2004a), and that these can be more accurate than first senses de-
rived from SemCor for words salient to a domain (Koeling, McCarthy, and Carroll 2005).
Here we investigate the behavior for other parts of speech, using a similar experimental
setup to that of McCarthy et al That work used the subject field codes (SFC) (Magnini
and Cavaglia` 2000)27 as a gold standard. In SFC the Princeton English WordNet is
augmented with some domain labels. Every synset in WordNet?s sense inventory is
annotated with at least one domain label, selected from a set of about 200 labels. These
labels are organized in a tree structure. Each synset of WordNet 1.6 is labeled with one
or more labels. The label factotum is assigned if any other is inadequate. The first level
consists of five main categories (e.g., doctrines and social science) and factotum.doctrines
27 More recently referred to as WordNet Domains (WN-DOMAINS).
580
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Figure 3
WSD precision on the Senseval-2 English all-words test data for nouns having a frequency less
than or equal to various thresholds.
has subcategories such as art, religion, and psychology. Some subcategories are further
divided in subcategories (e.g., dance, music, and theatre are subcategories of art).
McCarthy et al (2004a) used two domain-specific corpora for input to the method
for finding predominant senses. The corpora were obtained from the Reuters Corpus,
Volume 1 (RCV1; Rose, Stevenson, and Whitehead 2002) using the Reuters topic codes.
The two domain-specific corpora were:
SPORTS (Reuters topic code GSPO), 9.1 million words
FINANCE (Reuters topic codes ECAT and MCAT), 32.5 million words
In that work we produced sense rankings for a set of 38 nouns which have at
least one synset with an economy SFC label and one with a sport SFC label. We then
demonstrated that there were more sport labels assigned to the predominant senses
acquired from the SPORTS corpus and more economy labels assigned to those from the
FINANCE corpus. The predominant senses from both domains had a similarly high
percentage of factotum (domain-independent) labels. We reproduce the results here (in
Figure 4) for ease of reference, and for comparison with other results presented in this
section. The y-axis in this figure shows the percentage of the predominant sense labels
for these 38 nouns that have the SFC label indicated by the x-axis.
We envisaged running the same experiment with verbs, adjectives, and adverbs,
although we suspected that these would show less domain-specific tendencies and
there would be fewer candidate words to work with. The SFC labels for all senses of
polysemous words (excluding multiwords) in the various parts of speech are shown in
Table 15. We see from the distribution of factotum labels across the parts of speech that
nouns are certainly the PoS most likely to be influenced by domain.
To produce results like Figure 4 for each PoS, we needed words having at least one
synset with a sport label and one with an economy label. There were 20 such verbs but
581
Computational Linguistics Volume 33, Number 4
Figure 4
Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the
SPORTS and FINANCE corpora.
only two adjectives and no adverbs meeting this condition. We therefore performed
the experiment only with verbs. To do this we used the SPORTS and FINANCE corpora
as before, computing thesauruses for verbs using the grammatical relations specified
in Table 7. The results for the distribution of domain labels of the predominant senses
Table 15
Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech.
Domain % Domain %
noun biology 29.3 verb factotum 67.0
factotum 20.7 psychology 3.5
art 6.2 sport 2.9
sport 3.1 art 2.5
medicine 3.1 biology 2.5
other 37.6 other 21.6
adjective factotum 67.8 adverb factotum 81.4
biology 6.5 psychology 7.5
art 3.2 art 1.8
psychology 2.7 physics 1.1
physics 1.9 economy 1.1
other 17.9 other 7.1
582
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
acquired from the SPORTS and FINANCE corpora are shown in Figure 5. We see the same
tendency for sport labels for predominant senses from the SPORTS corpus and economy
labels for the predominant senses from the FINANCE corpus, but the relationship is
less marked compared with nouns because of the high proportions of factotum senses
in both corpora for verbs. We believe that acquisition of domain-specific predominant
senses should be focused on those words which show domain-specific tendencies. We
hope to put more work into automatic detection of these tendencies using indicators
such as domain salience and words that have different sense rankings in a given domain
compared to the BNC (as discussed by Koeling, McCarthy, and Carroll 2005).
6.4 Experiment 4: Domain-Specific Predominant Sense Acquisition
In the final set of experiments we evaluate the acquired predominant senses for domain-
specific corpora. The first of the two experiments was reported by Koeling, McCarthy,
and Carroll (2005), but we extend it by the second experiment reported subsequently.
Because there are no publicly available domain-specific manually sense-tagged corpora,
we created our own gold standard. The two chosen domains (SPORTS and FINANCE) and
the domain-neutral corpus (BNC) are the same as we used in the previous experiment.
We selected 40 words and we sampled (randomly) sentences containing these words
from the three corpora and asked annotators to choose the correct sense for the target
words. The set consists of 17 words which have at least one sense assigned an economy
domain label and at least one sense assigned a sports label: club, manager, record, right,
bill, check, competition, conversion, crew, delivery, division, fishing, reserve, return, score,
receiver, running; eight words that are particular salient in the SPORTS domain: fan,
star, transfer, striker, goal, title, tie, coach; eight words that are particular salient in the
Figure 5
Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the
SPORTS and FINANCE corpora.
583
Computational Linguistics Volume 33, Number 4
Table 16
WSD using predominant senses, training, and testing on all domain combinations
(hand-classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 39.1 49.9 24.0
SPORTS 25.7 19.7 43.7
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
FINANCE domain: package, chip, bond, market, strike, bank, share, target; and seven words
that are equally salient in both domains: will, phase, half, top, performance, level, country.
Koeling, McCarthy, and Carroll (2005) give further details of the construction of the gold
standard.
In the first experiment, we train on a corpus of documents with manually assigned
domain labels (i.e., sub-corpora of the Reuters corpus, see Section 6.3), and we test on
data from the same source. In a second experiment we build a text classifier, use the
text classifier to obtain SPORTS and FINANCE corpora (using general newswire text from
the English Gigaword Corpus; Graff 2003) and test on the gold-standard data from the
Reuters corpus. The second experiment eliminates issues about dependencies between
training and test data and will shed light on the question of how robust the acquired
predominant sense method is with respect to noise in the input data. At the same time,
the second experiment paves the way towards creating predominant sense inventories
for any conceivable domain.
6.4.1 Experiment Using Hand-Labeled Data. In this section we focus on the predominant
sense evaluation of the experiments described by Koeling, McCarthy, and Carroll (2005).
After running the predominant sense finding algorithms on the raw text of the two do-
main corpora (SPORTS and FINANCE) and the domain-neutral corpus (BNC), we evaluate
the accuracy of performing WSD on the sample of 40 words purely with the first sense
heuristic using all nine combinations of training and test corpora. The results (as given
in Table 16) are compared with a random baseline (?Random BL?)28 and the accuracy
using the first sense heuristic from SemCor (?SemCor FS?).29
The results in Table 16 show that the best results are obtained when the predominant
senses are acquired using the appropriate domain (i.e., test and training data from the
same domain). Moreover, when trained on the domain-relevant corpora, the random
baseline as well as the baseline provided by SemCor are comfortably beaten. It can be
observed from these results that apparently the BNC is more similar to the FINANCE
corpus than it is to the SPORTS corpus. The results for the SPORTS domain lag behind the
results for the FINANCE domain by almost 6 percentage points. This could be because
28 The random baseline is
?
i?tokens
1
#senses(i) .
29 The precision is given alongside in brackets because a predominant sense for the word striker is not
supplied by SemCor. The automatic method proposes a predominant sense in every case.
584
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 17
WSD using predominant senses, training, and testing on all domain combinations (automatically
classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 38.2 44.0 29.0
SPORTS 27.0 23.4 45.0
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
of the smaller amount of training data available (32M words versus 9M words), but it
could also be an artifact of this particular selection of words.
6.4.2 Experiment Using Automatically Classified Data. Although the previous experiment
shows that it is possible to acquire domain-specific predominant senses successfully, the
usefulness of doing this will be far greater if there is no need to classify corpora with
respect to domain by hand. There is no such thing as a standard domain specification
because the definition of a domain depends on user and application. It would be
advantageous if we could automatically obtain a user-/application-specific corpus from
which to acquire predominant senses.
In this section we describe an experiment where we build a text classifier using
WordNet as a sense inventory and the SFC domain extension (see Section 6.3). We
extracted bags of domain-specific words from WordNet for all the defined domains by
collecting all the word senses (synsets) and corresponding glosses associated with each
domain label. These bags of words are the fingerprints for the domains and we used
them to train a Support Vector Machine (SVM) text classifier using TwentyOne.30
The classifier distinguishes between 48 classes (the first and second levels of the
SFC hierarchy). When a document is evaluated by the classifier, it returns a list of
all the classes (domains) it recognizes and an associated confidence score reflecting the
certainty that the document belongs to that particular domain. We classified 10 months?
worth of data from the English Gigaword Corpus using this classifier and assigned each
document to the corpus belonging to the highest scoring class of the classifier?s output.
The level of confidence was ignored at this stage.
This resulted in a SPORTS corpus comprising about 11M words and a FINANCE
corpus of about 27M words. The predominant sense finding algorithm was run on the
raw text of these two corpora and we followed exactly the same evaluation strategy as
in the previous section. The results are summarized in Table 17 and are very similar
to those based on hand-labeled corpora. Again, the best results are obtained when test
and training data are derived from the same domain. The FINANCE?FINANCE result
is slightly worse, but is still well above both Random and the SemCor baseline. The
SPORTS?SPORTS result has slightly improved over the result reported in the previous
30 TwentyOne Classifier is an Irion Technologies product: www.irion.ml/products/english/
products classify.html.
585
Computational Linguistics Volume 33, Number 4
section. The reason for these differences may well be because the FINANCE corpus used
for this experiment is smaller and the SPORTS corpus is slightly larger than those used in
the hand-labeled experiment.
Automatically classifying documents inherently introduces noise in the training
corpora. This experiment to test the robustness of our method for finding predominant
senses suggests that it deals well with the noise. Further experiments that take the
confidence levels of the classifier into account will allow us to create corpora with less
noise and will allow us to find the right balance between corpus size and corpus quality.
7. Conclusions
In this article we have argued that information on the predominant sense of words is
important, and that it is desirable to be able to infer this automatically from unlabeled
text. We presented a number of evaluations investigating various facets of a previously
proposed method for automatically acquiring this information (McCarthy et al 2004a).
The evaluations extend ones in previous publications in a number of ways: they use
larger, balanced test data sets, and they compare alternative semantic similarity scores
and distributional thesauruses derived from different corpora and based on different
kinds of relations. We also looked in detail at areas where the method performs well
and also where it does not, and carried out a manual error analysis to identify the types
of mistakes it makes.
Our main results are:
 The predominant sense acquisition method produces promising results
overall for all open class parts of speech, when evaluated on SemCor, a
large balanced corpus.
 The highest accuracies are for nouns and adjectives; overall accuracy for
verbs is lower, but they have the lowest random baseline; adverbs have the
lowest average polysemy but gains over the random baseline are lower
than for other PoS.
 Using a thesaurus computed from proximity-based relations produces
almost as good results as using an otherwise identical one computed from
syntactic dependency-based relations.
 Lesk?s semantic similarity score (Banerjee and Pedersen 2002, lesk)
produces particularly good results for nouns which have low corpus
frequencies; Jiang and Conrath?s (1997, jcn) score does well on higher
frequency words.31
 For low frequency nouns in SemCor, the method, using any combination
of automatically acquired thesaurus and semantic similarity score that we
tried, produces more accurate predominant sense information than
SemCor. In particular, for nouns with a frequency of five or less (12.9% of
the polysemous nouns in the Senseval-2 data) it outperforms the SemCor
first sense heuristic. As the threshold is increased, the SemCor first sense
31 The lesk score has wider applicability than jcn since it can be applied to all parts of speech. It can also be
used with any sense inventory which has textual definitions for its senses even if the inventory does not
contain WordNet-like semantic relations.
586
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
heuristic becomes more competitive, but some of the automatic methods
are still outperforming it for nouns occurring 20 or fewer times in SemCor.
 Nouns show a stronger tendency for domain-specific meanings than other
parts of speech, but predominant senses for verbs acquired automatically
with respect to domain-specific corpora also correlate with the appropriate
domain labeling for those senses.
 Predominant senses acquired using domain-specific corpora outperform
those from SemCor in a WSD task, for a selection of nouns, using corpora
consisting of either hand-classified or automatically-classified documents.
8. Further Work
We are continuing to work on automatic ranking of word senses for WSD. Our next step
will be to use the numeric values of sense prevalence scores to compare the skews in
the distributions of word senses across different corpora and see if this enables us to
detect automatically words for which a domain- or genre-specific ranking is warranted.
Looking at skews should also help in predicting words for which contextual WSD is
likely to be particularly powerful, for example when more than one sense is scored
as being highly prevalent. In such situations we will combine our method with an
approach to unsupervised context-based WSD which uses the collocates of the distri-
butional neighbors associated with each of the senses as contextual features.
Our error analysis shows that many errors in identifying predominant senses are
caused by the sense distinctions in WordNet being particularly fine-grained. We have
recently (Koeling and McCarthy 2007) evaluated our method on the coarse-grained
English all words task at SemEval (Navigli, Litkowski, and Hargraves 2007). We will fol-
low work on finding relationships between WordNet senses to induce coarser-grained
classes (McCarthy 2006), and on automatic induction of senses (Pantel and Lin 2002)
and adapt our method to acquire prevalence rankings for these. The granularity of the
inventory will depend on the application and we plan to apply rankings over such
inventories for WSD within the context of a task, such as lexical substitution (McCarthy
and Navigli 2007).
To date we have only applied our methods to English. We plan to apply our
approach to other languages for which sense tagged resources of the size of SemCor are
not available. Given the good results with Lin?s proximity based thesaurus we believe
our method should work even for languages which do not have high quality parsers
available.
Acknowledgments
This work was supported by the UK EPSRC
project EP/C537262 ?Ranking Word Senses
for Disambiguation: Models and
Applications,? and a UK Royal Society
Dorothy Hodgkin Fellowship to the first
author. We are grateful to Dekang Lin for
making his thesaurus data publicly available
and to Siddharth Patwardhan and Ted
Pedersen for the WordNet Similarity
Package. We thank the anonymous reviewers
for the many helpful comments and
suggestions they made.
References
Agirre, Eneko and Oier Lopez de Lacalle.
2003. Clustering WordNet word senses. In
Recent Advances in Natural Language
Processing, pages 121?130, Borovets,
Bulgaria.
Banerjee, Satanjeev and Ted Pedersen. 2002.
An adapted Lesk algorithm for word sense
disambiguation using WordNet. In
Proceedings of the Third International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-02),
pages 136?145, Mexico City.
587
Computational Linguistics Volume 33, Number 4
Briscoe, Edward and John Carroll. 2002.
Robust accurate statistical annotation of
general text. In Proceedings of the Third
International Conference on Language
Resources and Evaluation (LREC),
pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Buitelaar, Paul and Bogdan Sacaleanu.
2001. Ranking and selecting synsets by
domain relevance. In Proceedings of
WordNet and Other Lexical Resources:
Applications, Extensions and Customizations,
NAACL 2001 Workshop, pages 119?124,
Pittsburgh, PA.
Chan, Yee Seng and Hwee Tou Ng. 2005.
Word sense disambiguation with
distribution estimation. In Proceedings of
the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005),
pages 1010?1015, Edinburgh, UK.
Church, Kenneth W. and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In
Proceedings of the 27th Annual Conference of
the Association for Computational Linguistics
(ACL-89), pages 76?82, Vancouver, British
Columbia, Canada.
Ciaramita, Massimiliano and Mark Johnson.
2003. Supersense tagging of unknown
nouns in WordNet. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
pages 168?175, Sapporo, Japan.
Cotton, Scott, Phil Edmonds, Adam
Kilgarriff, and Martha Palmer. 2001.
Senseval-2. http://www.sle.sharp.
co.uk/senseval2.
Curran, James. 2005. Supersense tagging
of unknown nouns using semantic
similarity. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages
26?33, Ann Arbor, MI.
Daude?, Jordi, Lluis Padro?, and German
Rigau. 2000. Mapping WordNets using
structural information. In Proceedings of the
38th Annual Meeting of the Association for
Computational Linguistics, pages 504?511,
Hong Kong.
Fellbaum, Christiane, editor. 1998. WordNet,
An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
Francis, W. Nelson and Henry Kuc?era, 1979.
Manual of Information to Accompany a
Standard Corpus of Present-Day Edited
American English, for Use with Digital
Computers. Department of Linguistics,
Brown University, Providence, RI. Revised
and amplified ed.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. One sense per discourse.
In Proceedings of the 4th DARPA Speech and
Natural Language Workshop, pages 233?237,
Harriman, NY.
Gliozzo, Alfio, Claudio Giuliano, and
Carlo Strapparava. 2005. Domain kernels
for word sense disambiguation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 403?410, Ann Arbor, MI.
Graff, David. 2003. English Gigaword.
Linguistic Data Consortium, Philadelphia,
PA.
Harris, Zellig S. 1968. Mathematical Structures
of Languages. Wiley, New York, NY.
Hornby, A. S. 1989. Oxford Advanced Learner?s
Dictionary of Current English. Oxford
University Press, Oxford, UK.
Ide, Nancy and Yorick Wilks. 2006. Making
sense about sense. In Eneko Agirre and
Phil Edmonds, editors, Word Sense
Disambiguation, Algorithms and Applications.
Springer, Dordrecht, The Netherlands,
pages 47?73.
Jiang, Jay and David Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In 10th
International Conference on Research in
Computational Linguistics, pages 19?33,
Taiwan.
Kilgarriff, Adam. 1998. Gold standard
datasets for evaluating word sense
disambiguation programs. Computer
Speech and Language, 12(3):453?472.
Kilgarriff, Adam and Martha Palmer, editors.
2000. Senseval: Special Issue of the Journal
Computers and the Humanities, volume
34(1?2). Kluwer, Dordrecht, The
Netherlands.
Koeling, Rob and Diana McCarthy. 2007.
Sussx: WSD using automatically acquired
predominant senses. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 314?317,
Prague, Czech Republic.
Koeling, Rob, Diana McCarthy, and John
Carroll. 2005. Domain-specific sense
distributions and predominant sense
acquisition. In Proceedings of the Human
Language Technology Conference and
EMNLP, pages 419?426, Vancouver, British
Columbia, Canada.
Krovetz, Robert. 1998. More than one sense
per discourse. In Proceedings of the
ACL-SIGLEX Senseval Workshop.
http://www.itri.bton.ac.uk/events/
senseval/ARCHIVE/PROCEEDINGS/.
Landes, Shari, Claudia Leacock, and
Randee I. Tengi, editors. 1998. Building
588
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Semantic Concordances. The MIT Press,
Cambridge, MA.
Lapata, Mirella and Chris Brew. 2004. Verb
class disambiguation using informative
priors. Computational Linguistics,
30(1):45?75.
Leech, Geoffrey. 1992. 100 million words of
English: The British National Corpus.
Language Research, 28(1):1?13.
Lesk, Michael. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of the
ACM SIGDOC Conference, pages 24?26,
Toronto, Canada.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago and
London.
Lin, Dekang. 1997. Using syntactic
dependency as local context to resolve
word sense ambiguity. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association
for Computational Linguistics (ACL-97),
pages 64?71, Madrid, Spain.
Lin, Dekang. 1998a. Automatic retrieval and
clustering of similar words. In Proceedings
of COLING-ACL?98, pages 768?774,
Montreal, Canada.
Lin, Dekang. 1998b. Dependency-based
evaluation of MINIPAR. In Proceedings of
the Workshop on the Evaluation of Parsing
Systems, pages 48?56, Granada, Spain.
http://www.cs.ualberta.ca/~lindek/
minipar.htm.
Magnini, Bernardo and Gabriela Cavaglia`.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413?1418, Athens, Greece.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzuli, and Alfio Gliozzo. 2001.
Using domain information for word sense
disambiguation. In Proceedings of the
Senseval-2 Workshop, pages 111?114,
Toulouse, France.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Martinez, David and Eneko Agirre. 2000.
One sense per collocation and genre/topic
variations. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora, pages 207?215, Hong Kong.
McCarthy, Diana. 2006. Relating WordNet
senses for word sense disambiguation. In
Proceedings of the EACL 06 Workshop:
Making Sense of Sense: Bringing
Psycholinguistics and Computational
Linguistics Together, pages 17?24, Trento,
Italy.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004a. Finding
predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004b. Ranking WordNet
senses automatically. CSRP 569,
Department of Informatics, University of
Sussex, January.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004c. Using
automatically acquired predominant
senses for word sense disambiguation. In
Proceedings of the ACL Senseval-3 Workshop,
pages 151?154, Barcelona, Spain.
McCarthy, Diana and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical
substitution task. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 48?53,
Prague, Czech Republic.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings Senseval-3 3rd
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Barcelona, Spain.
Miller, George A., Claudia Leacock, Randee
Tengi, and Ross T. Bunker. 1993. A
semantic concordance. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 303?308, San Francisco,
CA.
Mohammad, Saif and Graeme Hirst.
2006. Determining word sense dominance
using a thesaurus. In Proceedings of
the 11th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL-2006), pages 121?128,
Trento, Italy.
Navigli, Roberto, Ken Litkowski, and
Orin Hargraves. 2007. SemEval-2007
task 7: Coarse-grained English all-words
task. In Proceedings of ACL/SIGLEX
SemEval-2007, pages 30?35, Prague,
Czech Republic.
589
Computational Linguistics Volume 33, Number 4
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13(02):137?163.
Pantel, Patrick and Dekang Lin.
2002. Discovering word senses from
text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and
Data Mining, pages 613?619, Edmonton,
Alberta, Canada.
Patwardhan, Siddharth, Satanjeev Banerjee,
and Ted Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing 2003), pages 241?257, Mexico
City, Mexico.
Patwardhan, Siddharth and Ted Pedersen.
2003. The CPAN WordNet::Similarity
Package. http://search.cpan.org/~sid/
WordNet-Similarity-0.05/.
Preiss, Judita and David Yarowsky, editors.
2001. Proceedings of Senseval-2 Second
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Toulouse, France.
Procter, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman Group Ltd., Harlow, UK.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity
in a taxonomy. In 14th International
Joint Conference on Artificial Intelligence,
pages 448?453, Montreal, Canada.
Rose, Tony G., Mary Stevenson, and Miles
Whitehead. 2002. The Reuters Corpus
volume 1?From yesterday?s news to
tomorrow?s language resources. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 827?833, Las Palmas,
Canary Islands, Spain.
Siegel, Sidney and N. John Castellan.
1988. Non-Parametric Statistics for the
Behavioral Sciences. McGraw-Hill,
New York, NY.
Snyder, Benjamin and Martha Palmer.
2004. The English all-words task.
In Proceedings of the ACL Senseval-3
Workshop, pages 41?43, Barcelona,
Spain.
Stevenson, Mark and Yorick Wilks. 2001.
The interaction of knowledge sources for
word sense disambiguation. Computational
Linguistics, 27(3):321?350.
Weeds, Julie. 2003. Measures and
Applications of Lexical Distributional
Similarity. Ph.D. thesis, Department of
Informatics, University of Sussex,
Brighton, UK.
Weeds, Julie, James Dowdall, Gerold
Schneider, Bill Keller, and David Weir.
2005. Using distributional similarity to
organise biomedical terminology.
Terminology, 11(1):107?141.
Weeds, Julie and David Weir. 2005.
Co-occurrence Retrieval: A flexible
framework for lexical distributional
similarity. Computational Linguistics,
31(4):439?476.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation
performance across diverse parameter
spaces. Natural Language Engineering,
8(4):293?310.
590
From Predicting Predominant
Senses to Local Context for
Word Sense Disambiguation
Rob Koeling
Diana McCarthy
University of Sussex (UK)
email: robk@sussex.ac.uk
Abstract
Recent work on automatically predicting the predominant sense of a word
has proven to be promising (McCarthy et al, 2004). It can be applied (as a
first sense heuristic) to Word Sense Disambiguation (WSD) tasks, without
needing expensive hand-annotated data sets. Due to the big skew in the
sense distribution of many words (Yarowsky and Florian, 2002), the First
Sense heuristic for WSD is often hard to beat. However, the local context
of an ambiguous word can give important clues to which of its senses was
intended. The sense ranking method proposed by McCarthy et al (2004)
uses a distributional similarity thesaurus. The k nearest neighbours in the
thesaurus are used to establish the predominant sense of a word. In this
paper we report on a first investigation on how to use the grammatical
relations the target word is involved with, in order to select a subset of
the neighbours from the automatically created thesaurus, to take the local
context into account. This unsupervised method is quantitatively evalu-
ated on SemCor. We found a slight improvement in precision over using
the predicted first sense. Finally, we discuss strengths and weaknesses of
the method and suggest ways to improve the results in the future.
129
130 Koeling and McCarthy
1 Introduction
In recent years, a lot of research was done on establishing the predominant sense of
ambiguous words automatically using untagged texts (McCarthy et al, 2004, 2007).
The motivation for that work is twofold: on the one hand it builds on the strength
of the first sense heuristic in Word Sense Disambiguation (WSD) (i.e. the heuristic of
choosing themost commonly used sense of a word, irrespective of the context in which
the word occurs) and on the other hand it recognizes that manually created resources
for establishing word sense distributions are expensive to create and therefore hard
to find. The one resource that is used most widely, SemCor (Miller et al, 1993), is
only available for English and only representative for ?general? (non domain specific)
text. McCarthy et als method was successfully applied to a corpus of modern English
text (the BNC (Leech, 1992)) and the predicted predominant senses compared well
with the gold standard given by SemCor. Other experiments showed that the method
can successfully be adapted to domain specific text (Koeling et al, 2005) and other
languages (for example, Japanese (Iida et al, 2008)).
Even though the first sense heuristic is powerful, it would be preferable to only
use it for WSD, when either the sense distribution is so skewed that the most com-
monly used sense is by far the most dominant, or as a back-off when few other clues
are available to decide otherwise. The use of local context is ultimately necessary to
find evidence for the intended sense of an ambiguous word. In this paper we inves-
tigate how we can exploit results from intermediate steps taken when calculating the
predominant senses to this end.
The work on automatically finding predominant senses1 was partly inspired by the
observation that you can identify word senses by looking at the nearest neighbours of a
target word in a distributional thesaurus. For example, consider the following (simpli-
fied) entry for the word plant in such a thesaurus (omitting the scores for distributional
similarity):
(9) plant : factory, industry, facility, business, company, species, tree, crop, en-
gine, flower, farm, leaf, market, garden, field, seed, shrub...
Just by looking at the neighbours you can identify two main groups of neighbours,
each pointing at separate senses of the word. First there is the set of words consist-
ing of factory, industry, facility, business, company, engine that hint at the ?industrial
plant? sense of the word and then there is the set consisting of tree, crop, flower, leaf,
species, garden, field, seed, shrub that are more closely related to the ?flora? sense of
the word. A few words, like farm and possibly market could be associated equally
strongly with either sense. The idea behind ?sense ranking? is, that the right mix of
1. number of neighbours with a strong associations with one or more of the senses,
2. the strength of the association (semantic similarity) between neighbour and
sense and
1McCarthy et al (2004) concentrates on evaluating the predominant sense, but the method does in fact
rank all the senses in order of frequency of use.
From Predicting Predominant Senses to Local Context for WSD 131
3. the strength of the distributional similarity of the contributing neighbour and the
target word, will allow us to estimate the relative importance (i.e. frequency of
use) of each sense.
What we want to explore here, is how we can use the local context of an occurrence
of the target word, to select a subset of these neighbours. This subset should consist
of words that are related more strongly to the sense of the word in the target sentence.
For example, consider the word plant in a sentence like:
(10) The gardener grows plants from vegetable seeds.
Plant is used in this sentence as the ?subject of grow?. A simple way of zooming
in on potentially relevant neighbours is by using the most informative contexts shared
between neighbours and the word in the target sentence. This is implemented by
selecting just those words that occur in the same grammatical context (i.e. as subject
of the verb ?grow?) in a reference corpus2. If we apply that to the example in 9, we
end up with the following subset: business, industry, species, tree, crop, flower, seed,
shrub. Even though the first two words are still associated with the ?industrial plant?
sense, we can see that the majority of the words in this subset is strongly associated
with the intended sense.
In the next section we first give a quick introduction to the sense ranking algorithm
introduced in McCarthy et al (2004). Then we explain howwe can use the database of
grammatical relations that we used for creating the thesaurus, for selecting a subset of
neighbours in the thesaurus. The following section describes an evaluation performed
on the SemCor data. In the last two sections we discuss the results and especially
why both recall and precision are lower than we had hoped and what can be done to
improve the results.
2 Predominant Senses and Local Context
For a full review of McCarthy et als ranking method, we refer to McCarthy et al
(2004) or McCarthy et al (2007). Here we give a short description of the method.
Since we need the grammatical relations used for building the thesaurus, for selecting
a subset of the neighbours, we explain the procedure for building the thesaurus in 2.2.
In the last part of this section we explain how we exploit local context for SD.
2.1 Finding Predominant Senses
We use the method described inMcCarthy et al (2004) for finding predominant senses
from raw text. It can be applied to all parts of speech, but the experiments in this pa-
per all focus on nouns only. The method uses a thesaurus obtained from the text by
parsing, extracting grammatical relations and then listing each word (w) with its top k
nearest neighbours, where k is a constant. Like McCarthy et al (2004) we use k = 50
and obtain our thesaurus using the distributional similarity metric described by Lin
(1998). We use WordNet (WN) as our sense inventory. The senses of a word w are
each assigned a ranking score which sums over the distributional similarity scores of
2We use the same corpus used for generating the thesaurus as for the reference corpus (in all our
experiments).
132 Koeling and McCarthy
the neighbours and weights each neighbour?s score by a WN Similarity score (Pat-
wardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour
that maximises the WN Similarity score. This weight is normalised by the sum of
such WN similarity scores between all senses of w and and the senses of the neigh-
bour that maximises this score. We use the WN Similarity jcn score on nouns (Jiang
and Conrath, 1997) since this gave reasonable results for McCarthy et al and it is
efficient at run time given precompilation of frequency information. The jcn measure
needs word frequency information, which we obtained from the British National Cor-
pus (BNC) (Leech, 1992). The distributional thesaurus was constructed using subject,
direct object adjective modifier and noun modifier relations.
Thus we rank each sense wsi ?WSw using Prevalence Score wsi =
(11) ?
n j?Nw
dssn j ?
wnss(wsi,n j)
?wsi??WSw wnss(wsi? ,n j)
where the WordNet similarity score (wnss) is defined as:
wnss(wsi,n j) = max
nsx?NSn j
(wnss(wsi,nsx))
2.2 Building the Thesaurus
The thesaurus was acquired using the method described by Lin (1998). For input we
used grammatical relation data extracted using an automatic parser (Briscoe and Car-
roll, 2002). For the experiments in this paper we used the 90 million words of written
English from the BNC. For each noun we considered the co-occurring verbs in the
direct object and subject relation, the modifying nouns in noun-noun relations and the
modifying adjectives in adjective-noun relations. This limited set of grammatical rela-
tions was chosen since accuracy of the parser is particularly high for these 4 relations.
We could easily extend the set of relations to more in the future. A noun,w, is thus de-
scribed by a set of co-occurrence triples < w,r,x > and associated frequencies, where
r is a grammatical relation and x is a possible co-occurrence with w in that relation.
For every pair of nouns, where each noun had a total frequency in the triple data of 10
or more, we computed their distributional similarity using the measure given by Lin
(1998). If T (w) is the set of co-occurrence types (r,x) such that I(w,r,x) is positive
then the similarity between two nouns, w and n, can be computed as:
(12)
?(r,x)?T(w)?T (n) (I(w,r,x)+ I(n,r,x))
?(r,x)?T (w) I(w,r,x)+ ?(r,x)?T(n) I(n,r,x)
where I(w,r,x) = log P(x|w?r)P(x|r)
A thesaurus entry of size k for a target nounw can then be defined as the k most similar
nouns to noun w.
2.3 Local Context
The basis for building the distributional similarity thesaurus, is the set of grammatical
relations that the target word shares with other words. For example, if we look at
the thesaurus entry for the noun bike, then we see that the closest neighbours are (the
synonym) bicycle and the closely related motorbike (and motorcycle). The next 10
closest neighbours are all other vehicles (car, van, boat, bus, etc.). This is something
From Predicting Predominant Senses to Local Context for WSD 133
we would expect to see, since all these words do occur in similar grammatical contexts.
We travel by bike, as well as by motorcycle, car and bus. We park them, drive_off
with them, hire them, abandon them and repair them. Many of these relations can be
applied to a wide range of vehicles (or even a wider range of objects). However, some
relations are more specific to two-wheeled vehicles. For example, it is quite common
to mount a bike or a motorbike, whereas it is less common to mount a car or a van.
(Motor)bikes are chained to stop people from stealing them and it is probably more
common to ride a (motor)bike as opposed to driving a car or truck. Of course there
are many other more general things you can do with these vehicles: buy, sell, steal
them; there are yellow bikes, cars and boats, just like other objects. Therefore, we can
see many other types of objects lower in the list of neighbours that share these more
general grammatical relations, but not those that are specific to, say, vehicles or even
the sub-category of two-wheeled vehicles.
Consider the following sentence containing the ambiguous noun body:
(13) ?Regular exercise keeps the body healthy.?
(14) ?The funding body approved the final report.?
We would like our algorithm to be able to recognize that Wordnet?s first sense of
the word body (the entire physical structure of an organism (especially an animal or
human being)) is the most appropriate for sentence 13 and the third sense (a group
of persons associated by some common tie or occupation and regarded as an entity)
for sentence 14. If we calculate the most likely sense using all of the first 50 nearest
neighbours in the thesaurus, we predict that sense 4 is the most frequently used sense
(the body excluding the head and neck and limbs).
However, the two uses of the target word in 13 and 14 appear each in a very spe-
cific grammatical context. How can we exploit this local context to single out a certain
subset of the 50 nearest neighbours, containing those words that are particularly rel-
evant for (or more closely related to) the grammatical relation that the target word is
involved in this particular sentence. The idea we pursue here is to look at those neigh-
bours in the thesaurus that occur in the same grammatical relation as our target word
and share a high mutual information (i.e. word and grammatical relation do not only
occur frequently together, but also when you see one, there is a high probability that
you see the other).
While creating the thesaurus we consider all the words that co-occur with a certain
target word (where co-occur means that it appears in the same grammatical relation).
We also calculate the mutual information of both the target word and the co-occurring
word and the grammatical relation. Instead of throwing this information away after
finishing an entry in the thesaurus, we now store this information in the grammatical
relation database.
Since this database grows to enormous proportions (in the order of 200GB for the
one built up while processing the BNC), we need to reduce its size to be able to
work with it. If we only keep those entries in the database that involve the words
in the thesaurus and their 50 neighbours, we can reduce the database to manageable
proportions. We experimented with reducing the number of entries in the database
even further by limiting the number of entries per grammatical relations to the ones
134 Koeling and McCarthy
with the highest mutual information scores, but this only had a negative effect on the
recall, without improving the precision. As we will see later, data sparseness is a
serious issue and it is therefore not advisable to cut-out any usable information that
we have at our disposal. The word sense disambiguation procedure that uses the local
context is then straightforward:
1. Parse the sentence with the target word (the word to be disambiguated).
2. If the target word is not involved with any of the 4 grammatical relations we
considered for building up the thesaurus, local context can not be used.
3. Otherwise, consult the database to retrieve the co-occurring words:
? Let GR be the set of triples < w,r,x > from equation 12 in Section 2.2 for
target word w.
? Let NGR be the set of triples < n j,r,x > from equation 12 for any neigh-
bour n j ? Nw
? For all w ? T and all top 50 n ? Nw, keep entries with < ?,r,x > in
database.
? Let SGR be the set of relations < r,x > in the target sentence, where I <
w,r,x > and I < n,r,x > are both positive (i.e. r,x are both in the target
sentence and have high MI in BNC for both w and n.)
4. Compute the ranking score for each sense by applying to a modified version of
the ranking equation (15) (compared to the original given in (11)), where the k
nearest neighbours are replaced by the subset found in the step 3.
(15) Prevalence Score ws_lci = ?n j?Nw MI?dssn j ?
wnss(wsi,n j)
?wsi? ?WSw wnss(wsi? ,n j)
where the WordNet similarity score (wnss) is defined as before and let MI be I <
n,r,x >, i.e. the Mutual Information given by the events of seeing the grammatical
relation in question and seeing the neighbour.
2.4 An example
The fact that a subset of the neighbours in the thesaurus share some specific relations
with the target word in a particular sentence is something that we wish to exploit for
Word Sense Disambiguation. Let us have a closer look at the two example sentences
13 and 14 that we introduced in the previous section.
The grammatical relations that our target word body is involved with are (from
sentences 13 and 14 respectively):3
(16) ?body? object of ?keep? for sentence 13 and
(17) ?body? subject of ?approved? and ?body? modified by the noun ?funding? for
sentence 14
3At the moment we only take 4 grammatical relations into account: Verb-Subject, Verb-Object, Adj-
Noun and Noun-Noun modifier.
From Predicting Predominant Senses to Local Context for WSD 135
Table 1: Results of evaluation on the nouns in SemCor
Method Attempted Correct Wrong Precision Recall
Local Context 23,235 11,904 11,331 0.512 0.161
First sense 23,235 11,795 11,440 0.508 ?
Since keep is a fairly general verb, it is not surprising that quite a few of the neighbours
occur as object of keep. As a matter of fact, 28 of the first 50 neighbours share this
relation. However, the good news is, that pretty much all the words associated with
body-parts (such as arm, hand, leg, face and head) are among them.
The two grammatical relations that body is involved with in sentence 14, are more
specific. There are just 6 neighbours that share the ?subject of approve? relation with
body and another 5 that are used to modify the noun body. Among these words are the
highly relevant words organisation, institution and board.
3 Evaluation on SemCor
The example in the last section shows that in certain cases the method performs the
way we envisaged. However, we need a quantitative evaluation to get a proper picture
of the method?s usefulness. We performed a full evaluation on SemCor. In this experi-
ment we limited our attention to nouns only. We further eliminated Proper Names and
multi-word units from the test set. Since the nouns in both these categories are mostly
monosemous, they are less interesting as test material and apart from that, they intro-
duce problems (mostly parser related) that have little to do with the proposed method.
A total of 73,918 words were left to evaluate. Table 1 summarizes the results. The fig-
ure for recall for the ?First Sense? method is not given, because we want to contrast the
local context method with the first sense method. Whilst the first sense method will
return an answer in most cases, the local context method proposed in this paper will
not. Here we want to focus on how we can improve on using the first sense heuristic
by taking local context into account, rather than give complete results for a WSD task.
There are several things to say about these results. First of all, even though the
results for ?local context? are slightly better than for ?first sense?, we expected more
from it. We had identified quite a few cases like 13 and 14 above, where the local
context seemed to be able to help to identify the right neigbours in order to make
the difference. Below, we will discuss a few cases where the grammatical relations
involved are so general, that the subset of neighbours is large and most importantly,
not discriminative enough. It seems to be reasonable to expect that the latter cases will
not influence the precision too much (i.e. a smaller group of neighbourswill often give
a different result, but some better, some worse).
The recall is also lower than expected. The first thought was that data sparseness
was the main problem here, but additional experiments showed us that that is unlike to
be the case. In one experiment we took a part of the GigaWord corpus (Graff, 2003),
similar in size to the written part of the BNC (used in our original experiment) and
built our grammatical relation database using the combined corpus. The recall went
up a little, but at the price of a slightly lower precision.
136 Koeling and McCarthy
4 Discussion
The main problem causing the low recall seems to be the small number of grammatical
relations that we use for building the thesaurus. The four relations used (verb-subject,
verb-object, noun-noun-modifier and adjective-noun-modifier) were chosen because
of the parsers? high accuracy for these. For building the thesaurus, these grammatical
relations suffice, since every word will occur in one of these relations sooner or later.
However, whenever in a sentence the target word occurs outside these four relations,
we are not able to look it up in our database. Nouns within prepositional phrases seem
to be a major victim here. It should be straightforward to experiment with including
prepositional phrase related grammatical relations. We will have to evaluate the influ-
ence of the introduced noise on creating the thesaurus. Alternatively, it is possible to
use the four relations as before for creating the thesaurus and store the extra relations
in our database just for look-up.
A second cause for missing target words is parser errors. Even though RASP will
produce partial parses whenever a full parse of a sentence is not available, some loss
is inevitable. This is a harder problem to solve. One way of solving this problem
might be by using a proximity thesaurus instead of a thesaurus build using grammat-
ical relatons. McCarthy et al (2007) reported promising results for using proximity
based thesaurus for predicting predominant senses, with accuracy figures closely be-
hind those achieved with a dependency based thesaurus.
One plausible reason why the method is not working in many cases, is the fact that
the word to be disambiguated in the target sentence often occurs in a very general
grammatical relation. For example, ?subject of? or ?direct object of? a verb like have.
In these cases, most of the neighbors in the thesaurus will be selected. Even though it
is clear that that would minimize the positive effect, it is not immediately obvious that
this would have a negative effect. It might therefore be the case that the number of
cases where the grammatical relation is a good selection criterion, is just lower than
we thought (although this is not the impression that you get when you look at the
data). We will need to establish a way of quantitatively evaluating this.
The Mutual Information score gives us a measure of the dependence between the
grammatical relation and the word (neighbour of the target word) we are interested
it. It gives us a handle on ?generality? of the combination of seeing both events. This
means that for a very common grammatical relation, many words will be expected to
co-occur with a frequency comparable to their general frequency in texts. The contrast
with relation/word combinations for which this is not the case might be usable for
identifying the cases that we want to exclude here.
5 Conclusions
In this paper we propose a completely unsupervised method for Word Sense Disam-
biguation that takes the local context of the target word into account. The starting
point for this method is a method for automatically predicting the predominant senses
of words. The grammatical relations that were used to create the distributional simi-
larity thesaurus is exploited to select a subset of the k neighbours in the thesaurus, to
focus on those neighbours that are used in the same grammatical context as the word
we want to disambiguate in the target sentence.
From Predicting Predominant Senses to Local Context for WSD 137
Even though the precision of our proposed method is slightly higher than for the
predominant sense method, we are disappointed by the current results. We do believe
that there is moremileage to be had from the methodwe suggest. Improvement of both
recall and precision is on the agenda for future research. As we stated in the previous
section, we believe that the lower than expected recall can be addressed fairly easily,
by considering more grammatical relations. This is straightforward to implement and
results can be expected in the near future.
A second approach, involving a thesaurus built on proximity, rather than grammat-
ical relations will also be investigated. Considering the expected lower precision for
this approach, we plan to use the proximity-based thesaurus as a ?back off? solution
in case we fail to produce an answer with the dependency-based thesaurus. When
the proximity-based thesaurus is in place, we plan to perform a full evaluation of the
dependency versus the proximity approach.
Before we can deal with improving the local context method?s precision, we need
to have a better idea of the circumstances in which the method gets it wrong. We have
identified a large group of examples, where it is unlikely that the method will be suc-
cessful. A first step will be to develop a method to identify these cases automatically
and eliminate those from the targets that we are attempting to try. In the previous
section, we sketched how we think that we can achieve this by applying a Pointwise
Mutual Information threshold. If we are successful, this will at least give us the op-
portunity to focus on the strengths and weaknesses of the method. At the moment, the
virtues of the method seem to be obscured too much by dealing with cases that should
not be considered.
More insight in the method can also be gained from trying to identify in which
situations the method is more likely to get it right. At the moment we haven?t broken
down the results yet in terms of the target word?s polysemy and/or frequency of use.
Some grammatical relations might be more useful for identifying the intended sense
than other. A detailed analysis could give us these insights.
We do believe there is a strong case to be made for using unsupervised methods
for Word Sense Disambiguation (apart from McCarthy et al (2004)?s predominant
sense method, other approaches include e.g. Basili et al (2006)). The predominant
sense method has proven to be successful. However, applying the first sense heuristic
should be limited to certain cases. We can think of the cases where the dominance
of the predominant sense is so strong, that there is little to gain from doing a proper
attempt to disambiguation or to the cases where ?everything else fails?. Ultimately,
our goal is to find a balance between the dominance of the predominant sense and the
strength of the evidence from the supporting context. If we are able to recognize the
correct clues from the local context and use these clues to focus on those words with
a high distributional similarity to the target word in the context in which the word is
actually used, we can build on work on predicting predominant senses, to rely less on
the first sense heuristic. This would be a good step forward for unsupervised WSD.
Acknowledgments This workwas funded byUK EPSRC project EP/C537262 ?Rank-
ing Word Senses for Disambiguation: Models and Applications?, and by a UK Royal
Society Dorothy Hodgkin Fellowship to the second author. We would like to thank
Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package avail-
able and Julie Weeds for the thesaurus software.
138 Koeling and McCarthy
References
Basili, R., M. Cammisa, and A. Gliozzo (2006). Integrating domain and paradig-
matic similarity for unsupervised sense tagging. In Proceedings of 7th European
Conference on Artificial Intelligence (ECAI06).
Briscoe, E. and J. Carroll (2002). Robust accurate statistical annotation of general
text. In Proceedings of the Third International Conference on Language Resources
and Evaluation (LREC), Las Palmas, Canary Islands, Spain, pp. 1499?1504.
Graff, D. (2003). English gigaword. Linguistic Data Consortium, Philadelphia.
Iida, R., D. McCarthy, and R. Koeling (2008). Gloss-based semantic similarity met-
rics for predominant sense acquisition. In Proceedings of the Third International
Joint Conference on Natural Language Processing, Hyderabad, India, pp. 561?568.
Jiang, J. and D. Conrath (1997). Semantic similarity based on corpus statistics and
lexical taxonomy. In 10th International Conference on Research in Computational
Linguistics, Taiwan, pp. 19?33.
Koeling, R., D. McCarthy, , and J. Carroll (2005). Domain-specific sense distribu-
tions and predominant sense acquisition. In Proceedings of the Human Language
Technology Conference and EMNLP, Vancouver, Canada, pp. 419?426.
Leech, G. (1992). 100 million words of English: the British National Corpus. Lan-
guage Research 28(1), 1?13.
Lin, D. (1998). Automatic retrieval and clustering of similar words. In Proceedings
of COLING-ACL?98, Montreal, Canada, pp. 768?774.
McCarthy, D., R. Koeling, J. Weeds, and J. Carroll (2004). Finding predominant
senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, Barcelona, Spain, pp. 280?287.
McCarthy, D., R. Koeling, J. Weeds, and J. Carroll (2007). Unsupervised acquisition
of predominant word senses. Computational Linguistics 33(4), 553?590.
Miller, G. A., C. Leacock, R. Tengi, and R. T. Bunker (1993). A semantic concor-
dance. In Proceedings of the ARPA Workshop on Human Language Technology,
pp. 303?308. Morgan Kaufman.
Patwardhan, S. and T. Pedersen (2003). The CPAN WordNet::Similarity Package.
http://search.cpan.org/?sid/WordNet-Similarity-0.05/.
Yarowsky, D. and R. Florian (2002). Evaluating sense disambiguation performance
across diverse parameter spaces. Natural Language Engineering 8(4), 293?310.
