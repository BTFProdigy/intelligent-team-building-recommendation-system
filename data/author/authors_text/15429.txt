Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 421?432,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Bootstrapping Semantic Parsers from Conversations
Yoav Artzi and Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{yoav,lsz}@cs.washington.edu
Abstract
Conversations provide rich opportunities for
interactive, continuous learning. When some-
thing goes wrong, a system can ask for clari-
fication, rewording, or otherwise redirect the
interaction to achieve its goals. In this pa-
per, we present an approach for using con-
versational interactions of this type to induce
semantic parsers. We demonstrate learning
without any explicit annotation of the mean-
ings of user utterances. Instead, we model
meaning with latent variables, and introduce
a loss function to measure how well potential
meanings match the conversation. This loss
drives the overall learning approach, which in-
duces a weighted CCG grammar that could be
used to automatically bootstrap the semantic
analysis component in a complete dialog sys-
tem. Experiments on DARPA Communica-
tor conversational logs demonstrate effective
learning, despite requiring no explicit mean-
ing annotations.
1 Introduction
Conversational interactions provide significant op-
portunities for autonomous learning. A well-defined
goal allows a system to engage in remediations when
confused, such as asking for clarification, reword-
ing, or additional explanation. The user?s response
to such requests provides a strong, if often indirect,
signal that can be used to learn to avoid the orig-
inal confusion in future conversations. In this pa-
per, we show how to use this type of conversational
feedback to learn to better recover the meaning of
user utterances, by inducing semantic parsers from
unannotated conversational logs. We believe that
this style of learning will contribute to the long term
goal of building self-improving dialog systems that
continually learn from their mistakes, with little or
no human intervention.
Many dialog systems use a semantic parsing com-
ponent to analyze user utterances (e.g., Allen et al,
2007; Litman et al, 2009; Young et al, 2010). For
example, in a flight booking system, the sentence
Sent: I want to go to Seattle on Friday
LF: ?x.to(x, SEA) ? date(x, FRI)
might be mapped to the logical form (LF) meaning
representation above, a lambda-calculus expression
defining the set of flights that match the user?s de-
sired constraints. This LF is a representation of the
semantic content that comes from the sentence, and
would be input to a context-dependent understand-
ing component in a full dialog system, for example
to find the date that the symbol FRI refers to.
To induce semantic parsers from interactions, we
consider user statements in conversational logs and
model their meaning with latent variables. We
demonstrate that it is often possible to use the dia-
log that follows a statement (including remediations
such as clarifications, simplifications, etc.) to learn
the meaning of the original sentence. For example,
consider the first user utterance in Figure 1, where
the system failed to understand the user?s request.
To complete the task, the system must use a reme-
diation strategy. Here, it takes the initiative by ask-
ing for and confirming each flight constraint in turn.
This strategy produces an unnatural conversation but
provides supervision for learning the meaning of the
421
original utterance. We can easily record representa-
tions of the meanings the system intended to convey
at each step, as seen in Figure 1, and use this indirect
supervision for learning.
Learning from this weak signal is challenging. In
any specific conversation, the system?s remediations
can fail to recover aspects of the original user mean-
ing and can introduce spurious constraints, for ex-
ample when users change their goals mid conversa-
tion. To learn effectively, the model must accumu-
late evidence from many interactions to best recover
the meaning of each specific sentence.
We will learn semantic parsers defined by prob-
abilistic Combinatory Categorial Grammars (PC-
CGs), which include both a lexicon and a weighted
linear model for parse selection. The lexicon speci-
fies the meanings of individual words and phrases,
while the parameters of a parsing model define
how to best combine word- and phrase-level mean-
ings to analyze complete sentences. To learn with-
out labeled meaning representations, we make use
of a variant of the loss-sensitive Perceptron algo-
rithm (Singh-Miller and Collins, 2007). We define
loss functions to provide a rough measure of (1) how
well a candidate meaning for a utterance matches
the conversation that follows it and (2) how well the
candidate matches our expectations about the types
of things that are often said in the dialog?s domain.
These notions of loss drive not only the parameter
estimation but also the grammar induction process
that constructs the CCG lexicon. Experiments on
conversation logs from the DARPA Communicator
corpus (Walker et al, 2002) demonstrate the feasi-
bility of our approach.
This paper makes the following contributions:
? A formalization of the problem of learning
the meaning of user statements from conversa-
tional feedback, without requiring annotation.
? A new loss-sensitive learning algorithm for this
problem that induces semantic parsers from
conversation logs.
? Loss functions to measure the quality of hypo-
thetical utterance meanings within the conver-
sation in which they appear.
? An evaluation on logs from two dialog systems
SYSTEM: how can i help you? (OPEN TASK)
USER: i would like to fly from atlanta georgia to lon-
don england on september twenty fourth in the early
evening i would like to return on october first departing
from london in the late morning
SYSTEM: leaving what city? (ASK:?x.from(fl, x))
USER: atlanta georgia
SYSTEM: leaving atlanta. (CONFIRM:from(fl, ATL))
going to which city? (ASK:?x.to(fl, x))
USER: london
SYSTEM: arriving in london england.
(CONFIRM:to(fl, LON)) what date would you like to
depart atlanta? (ASK:?x.from(fl, ATL) ? depart-
date(fl, x))
USER: september twenty fourth in the early evening
[conversation continues]
Figure 1: Conversational excerpt from a DARPA Com-
municator travel-planning dialog. Each system statement
is labeled with representations of its speech act and log-
ical meaning, in parentheses. The user utterances have
no labels. Conversations of this type provide the training
data to learn semantic parsers for user utterances.
that demonstrate effective learning from con-
versations alone.
2 Problem
Our goal is to learn a function that maps a sentence
x to a lambda-calculus expression z. We assume ac-
cess to logs of conversations with automatically gen-
erated annotation of system utterance meanings, but
no explicit labeling of each user utterance meaning.
We define a conversation C = (~U,O) to be a se-
quence of utterances ~U = [u0, . . . , um] and a set
of conversational objects O. An object o ? O
is an entity that is being discussed, for example
there would be a unique object for each flight leg
discussed in a travel planning conversation. Each
utterance ui = (s, x, a, z) represents the speaker
s ? {User, System} producing the natural lan-
guage statement x which asserts a speech act a ?
{ASK,CONFIRM, . . .} with meaning represen-
tation z. For example, from the second system ut-
terance in Figure 1 the question x =?Leaving what
city?? is an a=ASK speech act with lambda-calculus
meaning z = ?x.from(fl, x). This meaning repre-
sents the fact that the system asked for the departure
city for the conversational object o = fl represent-
ing the flight leg that is currently being discussed.
We will learn from conversations where the speech
422
acts a and logical forms z for user utterances are un-
labeled. Such data can be generated by recording
interactions, along with each system?s internal rep-
resentation of its own utterances.
Finally, since we will be analyzing sentences at a
specific point in a complete conversation, we define
our training data as a set {(ji, Ci)|i = 1 . . . n}. Each
pair is a conversation Ci and the index ji of the user
utterance x in Ci whose meaning we will attempt to
learn to recover. In general, the same conversation
C can be used in multiple examples, each with a dif-
ferent sentence index. Section 8 provides the details
of how the data was gathered for our experiments.
3 Overview of Approach
We will present an algorithm for learning a weighted
CCG parser, as defined in Section 5, that can be used
to map sentences to logical forms. The approach
induces a lexicon to represent the meanings of words
and phrases while also estimating the parameters of
a weighted linear model for selecting the best parse
given the lexicon.
Learning As defined in Section 2, the algorithm
takes a set of n training examples, {(ji, Ci) : i =
1, . . . , n}. For each example, our goal is to learn to
parse the user utterance x at position ji in Ci. The
training data contains no direct evidence about the
logical form z that should be paired with x, or the
CCG analysis that would be used to construct z. We
model all of these choices as latent variables.
To learn effectively in this complex, latent space,
we introduce a loss function L(z, j, C) ? R that
measures how well a logical form z models the
meaning for the user utterance at position j in C. In
Section 6, we will present the details of the loss we
use, which is designed to be sensitive to remedia-
tions in C (system requests for clarification, etc.) but
also be robust to the fact that conversations often do
not uniquely determine which z should be selected,
for example when the user prematurely ends the dis-
cussion. Then, in Section 7, we present an approach
for incorporating this loss function into a complete
algorithm that induces a CCG lexicon and estimates
the parameters of the parsing model.
This learning setup focuses on a subproblem in
dialog; semantic interpretation. We do not yet learn
to recover user speech acts or integrate the logical
form into the context of the conversation. These are
important areas for future work.
Evaluation We will evaluate performance on a
test set {(xi, zi)|i = 1, . . . ,m} of m sentences xi
that have been explicitly labeled with logical forms
zi. This data will allow us to directly evaluate the
quality of the learned model. Each sentence is an-
alyzed with the learned model alone; the loss func-
tion and any conversational context are not used dur-
ing evaluation. Parsers that perform well in this set-
ting will be strong candidates for inclusion in a more
complete dialog system, as motivated in Section 1.
4 Related Work
Most previous work on learning from conversational
interactions has focused on the dialog sub-problems
of response planning (e.g., Levin et al, 2000; Singh
et al, 2002) and natural language generation (e.g.,
Lemon, 2011). We are not aware of previous work
on inducing semantic parsers from conversations.
There has been significant work on supervised
learning for inducing semantic parsers. Various
techniques were applied to the problem includ-
ing machine translation (Papineni et al, 1997;
Ramaswamy and Kleindienst, 2000; Wong and
Mooney, 2006; 2007; Matuszek et al, 2010), higher-
order unification (Kwiatkowski et al, 2010), parsing
(Ruifang and Mooney, 2006; Lu et al, 2008), induc-
tive logic programming (Zelle and Mooney, 1996;
Thompson and Mooney, 2003; Tang and Mooney,
2000), probabilistic push-down automata (He and
Young, 2005; 2006) and ideas from support vec-
tor machines and string kernels (Kate and Mooney,
2006; Nguyen et al, 2006). The algorithms we de-
velop in this paper build on previous work on su-
pervised learning of CCG parsers (Zettlemoyer and
Collins, 2005; 2007), as we describe in Section 5.3.
There is also work on learning to do semantic
analysis with alternate forms of supervision. Clarke
et al (2010) and Liang et al (2011) describe ap-
proaches for learning semantic parsers from ques-
tions paired with database answers, while Gold-
wasser et al (2011) presents work on unsuper-
vised learning. Our approach provides an alterna-
tive method of supervision that could complement
these approaches. Additionally, there has been sig-
nificant recent work on learning to do other, re-
423
I want to go from Boston to New York and then to Chicago
S/N (N\N)/NP NP (N\N)/NP NP CONJ[] (N\N)/NP NP
?f.f ?y.?f.?x.f(x) ? from(x, y) BOS ?y.?f.?x.f(x) ? to(x, y) NYC ?y.?f.?x.f(x) ? to(x, y) CHI
> > >
(N\N) (N\N) (N\N)
?f.?x.f(x) ? from(x,BOS) ?f.?x.f(x) ? to(x,NY C) ?f.?x.f(x) ? to(x,CHI)
<B
(N\N)
?f.?x.f(x) ? from(x,BOS) ? to(x,NY C)
<?>
(N\N)
?f.?x[].f(x) ? from(x[1], BOS) ? to(x[1], NY C) ? before(x[1], x[2]) ? to(x[2], CHI)
N
?x[].from(x[1], BOS) ? to(x[1], NY C) ? before(x[1], x[2]) ? to(x[2], CHI)
>
S
?x[].from(x[1], BOS) ? to(x[1], NY C) ? before(x[1], x[2]) ? to(x[2], CHI)
Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[]
that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the
lower ones create new nonterminals according the CCG combinators (>, <, etc.), see Steedman (2000) for details.
lated, natural language semantic analysis tasks from
context-dependent database queries (Miller et al,
1996; Zettlemoyer and Collins, 2009), grounded
event streams (Chen et al, 2010; Liang et al, 2009),
environment interactions (Branavan et al, 2009;
2010; Vogel and Jurafsky, 2010), and even unanno-
tated text (Poon and Domingos, 2009; 2010).
Finally, the DARPA Communicator data (Walker
et al, 2002) has been previously studied. Walker and
Passonneau (2001) introduced a schema of speech
acts for evaluation of the DARPA Communicator
system performance. Georgila et al (2009) extended
this annotation schema to user utterances using an
automatic process. Our speech acts extend this work
to additionally include full meaning representations.
5 Mapping Sentences to Logical Form
We will use a weighted linear CCG grammar for se-
mantic parsing, as briefly reviewed in this section.
5.1 Combinatory Categorial Grammars
Combinatory categorial grammars (CCGs) are a
linguistically-motivated model for a wide range of
language phenomena (Steedman, 1996; 2000). A
CCG is defined by a lexicon and a set of combina-
tors. The grammar defines a set of possible parse
trees, where each tree includes syntactic and seman-
tic information that can be used to construct logical
forms for sentences.
The lexicon contains entries that define categories
for words or phrases. For example, the second
lexical entry in the parse in Figure 2 is:
from := (N\N)/NP : ?y.?f.?x.f(x) ? from(x, y)
Each category includes both syntactic and seman-
tic information. For example, the phrase ?from?
is assigned the category with syntax (N\N)/NP
and semantics ?y.?f.?x.f(x) ? from(x, y). The
outermost syntactic forward slash specifies that the
entry must first be combined with an NP to the
right (the departure city), while the inner back slash
specifies that it will later modify a noun N to the
left (to add a constraint to a set of flights). The
lambda-calculus semantic expression is designed
to build the appropriate meaning representation at
each of these steps, as seen in the parse in Figure 2.
In general, we make use of typed lambda cal-
culus to represent meaning (Carpenter, 1997), both
in the lexicon and in intermediate parse tree nodes.
We also introduce an extension for modeling array-
typed variables to represent lists of individual en-
tries. These constructions are used, for example, to
model sentences describing a sequence of segments
while specifying flight preferences.
Figure 2 shows how a CCG parse builds a logical
form for a complete sentence with an array-typed
variable. Each intermediate node in the tree is con-
structed with one of a small set of CCG combina-
tor rules, see the explanation from Steedman (1996;
2000). We make use of the standard application,
composition and coordination combinators, as well
as type-shifting rules introduced by Zettlemoyer and
Collins (2007) to model spontaneous, unedited text.
5.2 Weighted Linear CCGs
A weighted linear CCG (Clark and Curran, 2007)
provides a ranking on the space of possible parses
under the grammar, which can be used to select
the best logical form for a sentence. This type of
model is closely related to several other approaches
(Ratnaparkhi et al, 1994; Johnson et al, 1999;
424
Lafferty et al, 2001; Collins, 2004; Taskar et al,
2004). Let x be a sentence, y be a CCG parse, and
GEN(x; ?) be the set of all possible CCG parses for
x given the lexicon ?. Define ?(x, y) ? Rd to be
a d-dimensional feature?vector representation and
? ? Rd to be a parameter vector. The optimal parse
for sentence x is
y?(x) = arg max
y?GEN(x;?) ? ? ?(x, y)
and the final output logical form z is the lambda-
calculus expression at the root of y?(x).
We compute y?(x) with a CKY-style chart pars-
ing algorithm. Since each chart entry contains a
full lambda-calculus meaning expression, we use
N -best pruning to control the number of options we
consider at each span. Learning a model of this form
involves learning the parameters ? and the lexicon
?. We will show that this is possible from conversa-
tional logs that do not contain any explicit labeling
of the logical forms for user utterances.
5.3 Supervised learning with GENLEX
Previous work on lexical induction, including the
GENLEX approach which we briefly review here,
has required labeled logical meaning representa-
tions. In Section 7, we will introduce a new way
of using GENLEX to learn from unannotated con-
versation logs.
The supervised CCG learning algorithms of
Zettlemoyer and Collins (2005; 2007) induce a
weighted CCG from training examples (xi, zi) for
i = 1 . . . n, where xi is a sentence and zi is the
corresponding lambda-calculus meaning representa-
tion. The output from the algorithm is a pair (?,?)
containing the learned parameters and CCG lexicon.
They defined the function GENLEX(x, z) to map
a sentence x and its meaning z to a large set of poten-
tial lexical entries. These entries are generated with
rules that consider the logical form z and guess po-
tential CCG categories. For example, the rule p ?
(N\N)/NP : ?y.?f.?x.f(x) ? p(x, y) introduces
categories commonly used to model certain types of
prepositions. This rule would, for example, intro-
duce the category (N\N)/NP : ?y.?f.?x.f(x) ?
to(x, y) for any logical form z that contains the con-
stant to. GENLEX uses a small set of rules of this
type to generate categories that are paired with all
Example Conversation:
(1) SYSTEM: how can i help you? (OPEN TASK)
(2) USER: i would like to fly from boston arriving to new york
and back from new york to boston
(3) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))
on what date? (ASK:?x.departdate(fl1, x))
(4) USER: september twenty four
(5) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))
on september twenty four (CONFIRM:depart-
date(fl1, sept 24)) to new york
(CONFIRM:to(fl1, NY C)) is new york your final
destination? (ASK:??x.fr(x,NY C))
(6) USER: yes i want to return to boston
(7) SYSTEM: sorry, i didn?t understand. (APOL-
OGY) where are you flying to from new york?
(ASK:?x.fr(fl2, NY C) ? to(fl2, x))
[conversation ends]
Candidate Logical Expressions for Utterance #2:
(a) ?x.to(x,BOS) ? from(x,NY C)
(b) ?x.from(x,BOS) ? to(x,NY C)
(c) ?x.to(x,BOS) ? to(x,NY C)
(d) ?x[].from(x[1], BOS) ? to(x[1], NY C)
? before(x[1], x[2]) ? return(x[2])
? from(x[2], NY C) ? to(x[2], BOS))
(e) ?x[].from(x[1], BOS) ? to(x[1], NY C)
? before(x[1], x[2]) ? return(x[2])
? from(x[2], BOS) ? to(x[2], NY C)
Figure 3: Conversation reflecting an interaction as seen
in the DARPA Communicator travel-planning dialogs.
possible substrings in x to form an overly general
lexicon. The complete learning algorithm then si-
multaneously selects a small subset of all entries
generated by GENLEX and estimates parameter val-
ues ?. Zettlemoyer and Collins (2005) present a
more detailed explanation.
6 Measuring Loss
In Section 7, we will present a loss-sensitive learn-
ing algorithm that models the meaning of user utter-
ances as latent variables to be estimated from con-
versational interactions.
We first introduce a loss function to measure the
quality of potential meaning representations. This
loss function L(z, j, C) ? R indicates how well a
logical expression z represents the meaning of the
j-th user utterance in conversation C. For example,
425
consider the first user utterance (j = 2) in Figure 3,
which is a request for a return trip from Boston to
New York. We would like to assign the lowest loss
to the meaning representation (d) in Figure 3 that
correctly encodes all of the stated constraints.
We make use of a loss function with two parts:
L(z, j, C) = Lc(z, j, C) + Ld(z). The conversa-
tion loss Lc (defined in Section 6.1) measures how
well the candidate meaning representation fits the
conversation, for example incorporating informa-
tion recovered through conversational remediations
as motivated in Section 1. The domain loss Ld (de-
scribed in Section 6.2) measures how well a logi-
cal form z matches domain expectations, such as
the fact that flights can only have a single origin.
These functions guide the types of meaning repre-
sentations we expect to see, but in many cases will
fail to specify a unique best option, for example
in conversations where the user prematurely termi-
nates the interaction. In Section 7, we will present a
complete, loss-driven learning algorithm that is ro-
bust to these types of ambiguities while inducing a
weighted CCG parser from conversations.
6.1 Conversation Loss
We will use a conversation loss function Lc(z, j, C)
that provides a rough indication of how well the log-
ical expression z represents a potential meaning for
the user utterance at position j in C. For example,
the first user utterance (j = 2) in Figure 3 is a re-
quest for a return trip from Boston to New York
where the user has explicitly mentioned both legs.
The figure also shows five options (a-e) for the logi-
cal form z. We want to assign the lowest loss to op-
tion (d), which includes all of the stated constraints.
The loss is computed in four steps for a user ut-
terance x at position j by (1) selecting a subset of
system utterances in the conversation C, (2) extract-
ing and computing loss for semantic content from
selected system utterances, (3) aligning the subex-
pressions in z to the extracted semantic content, and
(4) computing the minimal loss value from the best
alignment. In Figure 3, the loss for the candidate
logical forms is computed by considering the seg-
ment of system utterances up until the conversation
end. Within this segment, the matching for expres-
sion (d) involves mapping the origin and departure
constraints for the first leg (Boston - New York) onto
the earlier system confirmations while also align-
ing the ones for the second leg to system utterances
later in the selected portion of the conversation. Fi-
nally, the overall score depends on the quality of the
alignment, for example how many of the constraints
match to confirmations. This section presents the
full approach.
Segmentation For a user utterance at position j,
we select all system utterances from j ? 1 until the
system believes it has completed the current subtask,
as indicated by a reset action or final offer. We call
this selected segment C?. In Figure 3, C? ends with a
reset, but in a successful interaction it would have
ended with the offer of a specific flight.
Extracting Properties A property is a predicate-
entity-value triplet, where the entity can be a vari-
able from z or a conversational object. For example,
?from, fl, BOS? is a property where fl is a ob-
ject from C? and ?from, x,BOS? is a property from
z = ?x.from(x,BOS). We define PC? to be the
set of properties from logical forms for system ut-
terances in C?. Similarly, we define Pz to be the set
of properties in z.
Scoring System Properties For each system
property p ? PC? we compute its position value
pos(p), which is a normalized weighted average
over all the positions where it appears in a logi-
cal form. For each mention the weight is obtained
from its speech act. For example, properties that are
explicitly confirmed contribute more to the average
than those that were merely offered to the user in a
select statement.
We use pos(p) to compute a loss loss(p) for
each property p ? PC? . We first define P eC? to be allproperties in PC? with entity e. For entity e and po-
sition d, we define the entity-normalization function:
ne(d) =
d?minp?P eC? pos(p)
maxp?P eC? pos(p)?minp?P eC? pos(p)
.
For a given property p ? PC? with an entity e we
compute the loss value:
loss(p) = n?1e (1? ne(pos(p)))? 1 .
Where n?1e is the inverse of ne. This loss value is de-
signed to, first, provide less loss for later properties
so that it, for example, favors the last property in a
series of statements that finally resolves a confusion
426
in the conversation. Second, the loss value is lower
for objects mentioned closer to the user utterance x,
thereby preferring objects discussed sooner.
Matching Properties An alignment A maps vari-
ables in z to conversational objects in C?, for exam-
ple the flight legs fl1 and fl2 being discussed in
Figure 3. We will use alignments to match prop-
erties of z and C?. To do this we extend the align-
ment function A to apply to properties, for example
A(?from, x,BOS?) = ?from,A(x), BOS?.
Scoring Alignments Finally, we compute the
conversation loss Lc(z, j, C) as follows:
Lc(z, j, C) = minA
?
pu?Pz
?
ps?PC?
s(A(pu), ps) .
The function s(A(pu), ps) ? R computes the com-
patibility of the two input properties. It is zero if
A(pu) 6= ps. Otherwise, it returns loss(ps).
We approximate the min computation in Lc over
alignments A as follows. For a logical form z at
position j, we align the outer-most variable to the
conversational object in C? that is being discussed at
j. The remaining variables are aligned greedily to
minimize the loss, by selecting a single conversa-
tional object for each in turn.
Finally, for each aligned variable, we increase the
loss by one for each unmatched property from Pz .
This increases the loss of logical forms that include
spurious information. However, since a conversation
might stop prematurely and therefore won?t discuss
the entire user request, we only increase the loss for
variables that are already aligned. For this purpose,
we define an aligned variable to be one that has at
least one property matched successfully.
6.2 Domain Loss
We also make use of a domain loss functionLd(z) ?
R. The function takes a logical form z and returns
the number of violations there are in z to a set of
constraints on logical forms that occur commonly in
the dialog domain. For example, in a travel domain,
a violation might occur if a flight leg has two differ-
ent destination cities. The set of possible violations
must be specified for each dialog system, but can of-
ten be compiled from existing resources, such as a
database of valid flight ticketing options.
In our experiments, we will use a set of eight
simple constraints to check for violations in flight
Inputs: Training set {(ji, Ci) : i = 1 . . . n} where each exam-
ple includes the index ji of a sentence xi in the conversation
Ci. Initial lexicon ?0. Number of iterations T . Margin ?.
Beam size k for lexicon generation. Loss function L(x, j, C),
as described in Section 6.
Definitions: GENLEX(x, C) takes as input a sentence and a
conversation and returns a set of lexical items as described in
Section 7. GEN(x; ?) is the set of all possible CCG parses
for x given the lexicon ?. LF (y) returns the logical form
z at the root of the parse tree y. Let ?i(y) be shorthand for
the feature function ?(xi, y) defined in Section 5. Define
LEX(y) to be the set of lexical entries used in parse y. Fi-
nally, let MINLi(Y ) be {y|?y? ? Y,L(LF (y), ji, Ci) ?
L(LF (y?), ji, Ci)}, the set of minimal loss parses in Y .
Algorithm:
? = 0? , ? = ?0
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Lexical generation)
a. Set ? = ? ?GENLEX(xi, Ci)
b. Let Y be the k highest scoring parses of xi using ?
c. Select new lexical entries from the lowest loss parses
?i =
?
y?MINLi(Y ){l|l ? LEX(y)}d. Set lexicon to ? = ? ? ?i
Step 2: (Update parameters)
a. Define Gi = MINLi(GEN(xi,?, ?)) and
Lmin to be the minimal loss
b. Set Bi = GEN(xi,?, ?)?Gi
c. Set the relative loss function: ?i(y) = L(y, Ci)?Lmin
d. Construct sets of margin violating good and bad parses:
Ri = {r|r ? Gi ?
?y? ? Bi s.t. ??,?i(r)? ?i(y?)? < ??i(r)}
Ei = {e|e ? Bi ?
?y? ? Gi s.t. ??,?i(y?)? ?i(e)? < ??i(e)}
e. Apply the additive update:
? = ? +
?
r?Ri
1
|Ri|?i(r)?
?
e?Ei
1
|Ei|?i(e)
Output: Parameters ? and lexicon ?
Figure 4: The learning algorithm.
itineraries, which can have multiple legs. These
include, for example, checking that the legs have
unique origins and destinations that match across the
entire itinerary. For example, in Figure 3 the logical
forms (a), (b) and (d) will have no violations; they
describe valid flights. Example (c) has a single vio-
lation: a flight has two origins. Example (e) violates
a more complex constraint: the second flight?s origin
is different from the first flight?s destination.
7 Learning
Figure 4 presents the complete learning algorithm.
We assume access to training examples, {(ji, Ci) :
i = 1, . . . , n}, where each example includes the in-
427
dex ji of a sentence xi in the conversation Ci. The al-
gorithm learns a weighted CCG parser, described in
Section 5, including both a lexicon ? and parameters
?. The approach is online, considering each example
in turn and performing two steps: (1) expanding the
lexicon and (2) updating the parameters.
Step 1: Lexical Induction We introduce new lex-
ical items by selecting candidates from the function
GENLEX , following previous work (Zettlemoyer
and Collins, 2005; 2007) as reviewed in Section 5.3.
However, we face the new challenge that there is
no labeled logical-form meaning z. Instead, let ZC?
be set of all logical forms that appear in system
utterances in the relevant conversation segment C?.
We will now define the conversational lexicon set:
GENLEX(x, C?) =
?
z?ZC?
GENLEX(x, z)
where we use logical forms from system utterances
to guess possible CCG categories for analyzing the
user utterance. This approach will overgeneralize,
when the system talks about things that are unrelated
to what the user said, and will also often be incom-
plete, for example when the system does not repeat
parts of the original content. However, it provides a
way of guessing lexical items that can be combined
with previously learned ones, which can fill in any
gaps and help select the best analysis.
Step 1(a) in Figure 4 uses GENLEX to tem-
porarily create a large set of potential categories
based on the conversation. Steps (b-d) select a small
subset of these entries to add to the current lexicon
?: we find the k-best parses under the model, re-
rank them according to loss, find the lexical items
used in the best trees, and add them to ?. This
approach favors lexical items that are used in high-
scoring but low-loss analyses, as computed given the
current model.
Step 2: Parameter Updates Given the loss func-
tion L(x, i, C), we use a variant of a loss-sensitive
perceptron to update the parameters (Singh-Miller
and Collins, 2007). In Steps (a-c), for the current
example i, we compute the relative loss function ?i
that scales with the loss achieved by the best and
worst possible parses under the model. In contrast
to previous work, we do not only compute the loss
over a fixed n-best list of possible outputs, but in-
stead use the current model score to recompute the
options at each update. Then, Steps (d-e) find the set
Ri of least loss analyses and Ei of higher-loss can-
didates whose models scores are not separated by at
least ??i, where ? is a margin scale constant. The
final update (Step f) is additive and increases the pa-
rameters for features indicative of the analyses with
less loss while down weighting those for parses that
were not sufficiently separated.
Discussion This algorithm uses the conversation
to drive learning in two ways: it guides the lexi-
cal items that are proposed while also providing the
conversational feedback that defines the loss used to
update the parameters. The resulting approach is,
at every step, using information about how the con-
versation progressed after a user utterance to recon-
struct the meaning of the original statement.
8 Data Sets
For evaluation, we used conversation logs from the
Lucent and BBN dialog systems in the DARPA
Communicator corpus (Walker et al, 2002). We se-
lected these systems since they provide significant
opportunities for learning. They asked relatively
open ended questions, allowing for more complex
user responses, while also using a number of sim-
ple remediating strategies to recover from misun-
derstandings. The original conversational logs in-
cluded unannotated transcripts of system and user
utterances. Inspired by the speech act labeling ap-
proach of Walker and Passonneau (2001), we wrote
a set of scripts to label the speech acts and logical
forms for system statements. This could be done
with high accuracy since the original text was gener-
ated with templates. These labels represent what the
system explicitly said and do not require complex,
potentially error-prone annotation of the full state of
the original dialog system. The set of speech acts in-
cludes confirmations, information requests, selects,
offers, instructions, and a miscellaneous category.
The data sets include a total of 376 conversations,
divided into training and testing sets. Table 1 pro-
vides details about the training and testing sets, as
well as general data set statistics. We developed our
system using 4-fold cross validation on the training
sets. Although there are approximately 12,000 user
428
Lucent BBN
# Conversations 214 162
Total # of utterances 11,974 12,579
Avg. utterances per conversation 55.95 77.65
Avg. tokens per user utterance 3.24 2.39
Total # of training utterances 208 67
Total # of testing utterances 96 67
Avg. tokens per selected utterance 11.72 9.53
Table 1: Data set statistics for Lucent and BBN systems.
utterances in the data sets, the vast majority are sim-
ple, short phrases (such as ?yes? or ?no?) which are
not useful for learning a semantic parser. We se-
lect user utterances with a small set of heuristics, in-
cluding a threshold (6 for Lucent, 4 for BBN) on the
number of words and requiring that at least one noun
phrase is present from our initial lexicon. This ap-
proach was manually developed to perform well on
the training sets, but is not perfect and does intro-
duce a small amount of noise into the data.
9 Experimental Setup
This section describes our experimental setup and
comparisons. We follow the setup of Zettlemoyer
and Collins (2007) where possible, including fea-
ture design, initialization of the semantic parser, and
evaluation metrics, as reviewed below.
Features and Parser The features include indica-
tors for lexical item use, properties of the logical
form that is being constructed, and indicators for
parsing operators used to build the tree. The parser
attempts to boost recall with a two-pass strategy that
allows for word skipping if the initial parse fails.
Initialization and Parameters We use an initial
lexicon that includes a list of domain-specific noun
phrases, such as city and airport names, and a list
of domain-independent categories for closed-class
words such as ?the? and ?and?. We also used a time
and number parser to expand this lexicon for each
input sentence with the BIU Number Normalizer.1
The learning parameters were tuned using the devel-
opment sets: the margin constant ? is set to 0.5, we
use 6 iterations and take the top 30 parses for lexical
generation (step 1, figure 4). The parser used for pa-
rameter update (step 2, figure 4) has a beam of 250.
The parameter vector is initialized to 0?.
1http://www.cs.biu.ac.il/?nlp/downloads/
Evaluation Metrics For evaluation, we measure
performance against gold standard labels. We report
both the number of exact matches, fully correct log-
ical forms, and a partial-credit number. We measure
partial-credit accuracy by mapping logical forms to
attribute-value pairs (for example, the expression
from(x, LA) will be mapped to from = LA) and
report precision and recall on attribute sets. This
more lenient measure does not test the overall struc-
ture of the logical expression, only its components.
Systems We compare performance with the fol-
lowing systems:
Full Supervision: We measured how a fully super-
vised approach would perform on our data by hand-
labeling the training data and using a 0-1 loss func-
tion that tests if the output logical form matches the
labeled one. For lexicon generation, the labels were
used instead of the conversation.
No Conversation Baseline: We also report results
for a no conversation baseline. This baseline sys-
tem is constructed by making two modifications to
the full approach. We remove the conversation loss
function and apply the GENLEX templates to every
possible logical constant, instead of only those in the
conversation. This baseline allows us to measure the
importance of having access to the conversations by
completely ignoring the context for each sentence.
Ablations: In addition to the baseline above, we
also do ablation tests by turning off various individ-
ual components of the complete algorithm.
10 Results
Table 2 shows exact match results for the develop-
ment sets, including different system configurations.
We report mean results across four folds. To ver-
ify their contributions, we include results where we
ablate the conversational loss and domain loss func-
tions. Both are essential.
The test results are listed in Table 3. The full
method significantly outperforms the baseline, indi-
cating that we are making effective use of the con-
versational feedback, although we do not yet match
the fully supervised result. The poor baseline per-
formance is not surprising, given the difficulty of the
task and lack of guidance when the conversations are
removed. The partial-credit numbers also demon-
strate an empirical trend that we observed; in many
429
Exact Match Metric Lucent BBNPrec. Rec. F1 Prec. Rec. F1
Without conversational loss 0.35 0.34 0.35 0.66 0.54 0.59
Without domain loss 0.42 0.42 0.42 0.69 0.56 0.61
Our Approach 0.63 0.61 0.62 0.77 0.64 0.69
Supervised method 0.76 0.75 0.75 0.81 0.67 0.73
Table 2: Mean exact-match results for cross fold evaluation on the development sets.
Exact Match Metric Lucent BBNPrec. Rec. F1 Prec. Rec. F1
No Conversations Baseline 0 0 0 0.16 0.15 0.15
Our Approach 0.58 0.55 0.56 0.85 0.75 0.79
Supervised method 0.7 0.68 0.69 0.87 0.78 0.82
Partial Credit Metric Lucent BBNPrec. Rec. F1 Prec. Rec. F1
No Conversations Baseline 0.26 0.35 0.29 0.26 0.33 0.29
Our Approach 0.68 0.63 0.65 0.97 0.57 0.72
Supervised method 0.75 0.68 0.72 0.96 0.68 0.79
Table 3: Exact- and partial-match results on the test sets.
cases where we do not produce the correct logical
form, the output is often close to correct, with only
one or two missed flight constraints.
The difference between the two systems is evi-
dent. The BBN system presents a simpler approach
to the dialog problem by creating a more constrained
conversation. This is done by handling one flight
at a time, in the case of flight planing, and pos-
ing simple and close ended questions to the user.
Such an approach encourages the user to make sim-
pler requests, with relatively few constraints in each
request. In contrast, the Lucent system presents a
less-constrained approach: interactions start with an
open ended prompt and the conversations flow in a
more natural, less constrained fashion. BBN?s sim-
plified approach makes it easier for learning, giving
us superior performance when compared to the Lu-
cent system, despite the smaller training set. This is
true for both our approach and supervised learning.
We compared the logical forms recovered by the
best conversational model to the labeled ones in the
training set. Many of the errors came from cases
where the dialog system never fully recovered from
confusions in the conversation. For example, the Lu-
cent system almost never understood user utterances
that specified flight arrival times. Since it was unable
to consistently recover and introduce this constraint,
the user would often just recalculate and specify a
departure time that would achieve the original goal.
This type of failure provides no signal for our learn-
ing algorithm, whereas the fully supervised algo-
rithm would use labeled logical forms to resolve the
confusion. Interestingly, the test set had more sen-
tences that suffered such failures than the develop-
ment set, which contributed to the performance gap.
11 Discussion
We presented a loss-driven learning approach that
induces the lexicon and parameters of a CCG parser
for mapping sentences to logical forms. The loss
was defined over the conversational context, without
requiring annotation of user utterances meaning.
The overall approach assumes that, in aggregate,
the conversations contain sufficient signal (remedia-
tions such as clarification, etc.) to learn effectively.
In this paper, we satisfied this requirement by us-
ing logs from automated systems that deployed rea-
sonably effective recovery strategies. An important
area for future work is to consider how this learning
can be best integrated into a complete dialog system.
This would include designing remediation strategies
that allow for the most effective learning and consid-
ering how similar techniques could be used simulta-
neously for other dialog subproblems.
Acknowledgments
The research was supported by funding from the
DARPA Computer Science Study Group. Thanks
to Dan Weld, Raphael Hoffmann, Jonathan Berant,
Hoifung Poon and Mark Yatskar for their sugges-
tions and comments. We also thank Shachar Mirkin
for providing access to the BIU Normalizer.
430
References
Allen, J., M. Manshadi, M. Dzikovska, and M. Swift.
2007. Deep linguistic processing for spoken dialogue
systems. In Proceedings of the Workshop on Deep Lin-
guistic Processing.
Branavan, SRK, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the Association for Computational Linguis-
tics and the International Joint Conference on Natural
Language Processing.
Branavan, SRK, L.S. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: learning to map high-level
instructions to commands. In Proceedings of the Asso-
ciation for Computational Linguistics.
Carpenter, B. 1997. Type-Logical Semantics. The MIT
Press.
Chen, D.L., J. Kim, and R.J. Mooney. 2010. Training a
multilingual sportscaster: using perceptual context to
learn language. Journal of Artificial Intelligence Re-
search 37(1):397?436.
Clark, S. and J.R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics 33(4):493?552.
Clarke, J., D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In Proceedings of the Conference on Computational
Natural Language Learning.
Collins, M. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-
free methods. In New Developments in Parsing Tech-
nology.
Georgila, K., O. Lemon, J. Henderson, and J.D. Moore.
2009. Automatic annotation of context and speech acts
for dialogue corpora. Natural Language Engineering
15(03):315?353.
Goldwasser, D., R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Proceedings. of the Association of Computa-
tional Linguistics.
He, Y. and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and
Language 19:85?106.
He, Y. and S. Young. 2006. Spoken language understand-
ing using the hidden vector state model. Speech Com-
munication 48(3-4).
Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic ?unification-based?
grammars. In Proceedings of the Association for Com-
putational Linguistics.
Kate, R.J. and R.J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proceedings of the
Association for Computational Linguistics.
Kwiatkowski, T., L.S. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic ccg gram-
mars from logical form with higher-order unification.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning.
Lemon, O. 2011. Learning what to say and how to say
it: Joint optimisation of spoken dialogue management
and natural language generation. Computer Speech &
Language 25(2):210?221.
Levin, E., R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing 8(1):11?23.
Liang, P., M.I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In Pro-
ceedings of the Joint Conference of the Association
for Computational Linguistics the International Joint
Conference on Natural Language Processing.
Liang, P., M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Litman, D., J. Moore, M.O. Dzikovska, and E. Farrow.
2009. Using Natural Language Processing to Analyze
Tutorial Dialogue Corpora Across Domains Modali-
ties. In Proceeding of the Conference on Artificial In-
telligence in Education.
Lu, W., H.T. Ng, W.S. Lee, and L.S. Zettlemoyer. 2008.
A generative model for parsing natural language to
meaning representations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Matuszek, C., D. Fox, and K. Koscher. 2010. Follow-
ing directions using statistical machine translation. In
Proceeding of the international conference on Human-
robot interaction.
Miller, S., D. Stallard, R.J. Bobrow, and R.L. Schwartz.
1996. A fully statistical approach to natural language
interfaces. In Proceedings of the Association for Com-
putational Linguistics.
Nguyen, L., A. Shimazu, and X. Phan. 2006. Seman-
tic parsing with structured SVM ensemble classifica-
tion models. In Proceedings of the joint conference
431
of the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics.
Papineni, K.A., S. Roukos, and T.R. Ward. 1997. Feature-
based language understanding. In Proceedings of the
European Conference on Speech Communication and
Technology.
Poon, H. and P. Domingos. 2009. Unsupervised semantic
parsing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Poon, H. and P. Domingos. 2010. Unsupervised ontology
induction from text. In Proceedings of the Association
for Computational Linguistics.
Ramaswamy, G.N. and J. Kleindienst. 2000. Hierarchi-
cal feature-based translation for scalable natural lan-
guage understanding. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994. A
maximum entropy model for parsing. In Proceedings
of the International Conference on Spoken Language
Processing.
Ruifang, G. and R.J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Porceedings of the
Association for Computational Linguistics.
Singh, S.P., D.J. Litman, M.J. Kearns, and M.A. Walker.
2002. Optimizing dialogue management with re-
inforcement learning: Experiments with the NJFun
system. Journal of Artificial Intelligence Research
16(1):105?133.
Singh-Miller, N. and M. Collins. 2007. Trigger-based
language modeling using a loss-sensitive perceptron
algorithm. In IEEE International Conference on
Acoustics, Speech and Signal Processing.
Steedman, M. 1996. Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. 2000. The Syntactic Process. The MIT
Press.
Tang, L.R. and R.J. Mooney. 2000. Automated construc-
tion of database interfaces: Integrating statistical and
relational learning for semantic parsing. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora.
Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Thompson, C.A. and R.J. Mooney. 2003. Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research 18:1?
44.
Vogel, A. and D. Jurafsky. 2010. Learning to follow nav-
igational directions. In Proceedings of the Association
for Computational Linguistics.
Walker, M. and R. Passonneau. 2001. DATE: a dia-
logue act tagging scheme for evaluation of spoken di-
alogue systems. In Proceedings of the First Inter-
national Conference on Human Language Technology
Research.
Walker, M., A. Rudnicky, R. Prasad, J. Aberdeen,
E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom,
A. Potamianos, et al 2002. DARPA Communicator:
Cross-system results for the 2001 evaluation. In Pro-
ceedings of the International Conference on Spoken
Language Processing.
Wong, Y.W. and R.J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the North American Association for Compu-
tational Linguistics.
Wong, Y.W. and R.J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Young, S., M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
POMDP-based spoken dialogue management. Com-
puter Speech & Language 24(2):150?174.
Zelle, J.M. and R.J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L.S. and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L.S. and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Zettlemoyer, L.S. and Michael Collins. 2009. Learning
context-dependent mappings from sentences to logical
form. In Proceedings of the Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
432
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545?1556,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Scaling Semantic Parsers with On-the-fly Ontology Matching
Tom Kwiatkowski Eunsol Choi Yoav Artzi Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{tomk,eunsol,yoav,lsz}@cs.washington.edu
Abstract
We consider the challenge of learning seman-
tic parsers that scale to large, open-domain
problems, such as question answering with
Freebase. In such settings, the sentences cover
a wide variety of topics and include many
phrases whose meaning is difficult to rep-
resent in a fixed target ontology. For ex-
ample, even simple phrases such as ?daugh-
ter? and ?number of people living in? can-
not be directly represented in Freebase, whose
ontology instead encodes facts about gen-
der, parenthood, and population. In this pa-
per, we introduce a new semantic parsing ap-
proach that learns to resolve such ontologi-
cal mismatches. The parser is learned from
question-answer pairs, uses a probabilistic
CCG to build linguistically motivated logical-
form meaning representations, and includes
an ontology matching model that adapts the
output logical forms for each target ontology.
Experiments demonstrate state-of-the-art per-
formance on two benchmark semantic parsing
datasets, including a nine point accuracy im-
provement on a recent Freebase QA corpus.
1 Introduction
Semantic parsers map sentences to formal represen-
tations of their underlying meaning. Recently, al-
gorithms have been developed to learn such parsers
for many applications, including question answering
(QA) (Kwiatkowski et al, 2011; Liang et al, 2011),
relation extraction (Krishnamurthy and Mitchell,
2012), robot control (Matuszek et al, 2012; Kr-
ishnamurthy and Kollar, 2013), interpreting instruc-
tions (Chen and Mooney, 2011; Artzi and Zettle-
moyer, 2013), and generating programs (Kushman
and Barzilay, 2013).
In each case, the parser uses a predefined set
of logical constants, or an ontology, to construct
meaning representations. In practice, the choice
of ontology significantly impacts learning. For
example, consider the following questions (Q) and
candidate meaning representations (MR):
Q1: What is the population of Seattle?
Q2: How many people live in Seattle?
MR1: ?x.population(Seattle, x)
MR2: count(?x.person(x) ? live(x, Seattle))
A semantic parser might aim to construct MR1 for
Q1 and MR2 for Q2; these pairings align constants
(count, person, etc.) directly to phrases (?How
many,? ?people,? etc.). Unfortunately, few ontologies
have sufficient coverage to support both meaning
representations, for example many QA databases
would only include the population relation required
for MR1. Most existing approaches would, given
this deficiency, simply aim to produce MR1 for Q2,
thereby introducing significant lexical ambiguity
that complicates learning. Such ontological mis-
matches become increasingly common as domain
and language complexity increases.
In this paper, we introduce a semantic parsing ap-
proach that supports scalable, open-domain ontolog-
ical reasoning. The parser first constructs a linguis-
tically motivated domain-independent meaning rep-
resentation. For example, possibly producing MR1
for Q1 and MR2 for Q2 above. It then uses a learned
ontology matching model to transform this represen-
1545
x : How many people visit the public library of New York annually
l0 : ?x.eq(x, count(?y.people(y) ? ?e.visit(y, ?z.public(z) ? library(z) ? of(z, new york), e) ? annually(e)))
y : ?x.library.public library system.annual visits(x, new york public library)
a : 13,554,002
x : What works did Mozart dedicate to Joseph Haydn
l0 : ?x.works(x) ? ?e.dedicate(mozart, x, e) ? to(haydn, e)))
y : ?x.dedicated work(x) ? ?e.dedicated by(mozart, e) ? dedication(x, e) ? dedicated to(haydn, e)))
a : { String Quartet No. 19, Haydn Quartets, String Quartet No. 16, String Quartet No. 18, String Quartet No. 17 }
Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0, fully specified
logical forms y, and answers a drawn from the Freebase domain.
tation for the target domain. In our example, pro-
ducing either MR1, MR2 or another more appropri-
ate option, depending on the QA database schema.
This two stage approach enables parsing without
any domain-dependent lexicon that pairs words with
logical constants. Instead, word meaning is filled
in on-the-fly through ontology matching, enabling
the parser to infer the meaning of previously un-
seen words and more easily transfer across domains.
Figure 1 shows the desired outputs for two example
Freebase sentences.
The first parsing stage uses a probabilistic combi-
natory categorial grammar (CCG) (Steedman, 2000;
Clark and Curran, 2007) to map sentences to
new, underspecified logical-form meaning represen-
tations containing generic logical constants that are
not tied to any specific ontology. This approach en-
ables us to share grammar structure across domains,
instead of repeatedly re-learning different grammars
for each target ontology. The ontology-matching
step considers a large number of type-equivalent
domain-specific meanings. It enables us to incorpo-
rate a number of cues, including the target ontology
structure and lexical similarity between the names of
the domain-independent and dependent constants, to
construct the final logical forms.
During learning, we estimate a linear model over
derivations that include all of the CCG parsing de-
cisions and the choices for ontology matching. Fol-
lowing a number of recent approaches (Clarke et al,
2010; Liang et al, 2011), we treat all intermediate
decisions as latent and learn from data containing
only easily gathered question answer pairs. This ap-
proach aligns naturally with our two-stage parsing
setup, where the final logical expression can be di-
rectly used to provide answers.
We report performance on two benchmark
datasets: GeoQuery (Zelle and Mooney, 1996) and
Freebase QA (FQ) (Cai and Yates, 2013a). Geo-
Query includes a geography database with a small
ontology and questions with relatively complex,
compositional structure. FQ includes questions to
Freebase, a large community-authored database that
spans many sub-domains. Experiments demonstrate
state-of-the-art performance in both cases, including
a nine point improvement in recall for the FQ test.
2 Formal Overview
Task Let an ontology O be a set of logical con-
stants and a knowledge base K be a collection of
logical statements constructed with constants from
O. For example, K could be facts in Freebase (Bol-
lacker et al, 2008) and O would define the set
of entities and relation types used to encode those
facts. Also, let y be a logical expression that can
be executed against K to return an answer a =
EXEC(y,K). Figure 1 shows example queries and
answers for Freebase. Our goal is to build a function
y = PARSE(x,O) for mapping a natural language
sentence x to a domain-dependent logical form y.
Parsing We use a two-stage approach to define
the space of possible parses GEN(x,O) (Section 5).
First, we use a CCG and word-class information
from Wiktionary1 to build domain-independent un-
derspecified logical forms, which closely mirror the
linguistic structure of the sentence but do not use
constants from O. For example, in Figure 1, l0 de-
notes the underspecified logical forms paired with
each sentence x. The parser then maps this interme-
diate representation to a logical form that uses con-
stants from O, such as the y seen in Figure 1.
1www.wiktionary.com
1546
Learning We assume access to data containing
question-answer pairs {(xi, ai) : i = 1 . . . n} and
a corresponding knowledge base K. The learn-
ing algorithm (Section 7.1) estimates the parame-
ters of a linear model for ranking the possible en-
tires in GEN(x,O). Unlike much previous work
(e.g., Zettlemoyer and Collins (2005)), we do not
induce a CCG lexicon. The lexicon is open domain,
using no symbols from the ontology O for K. This
allows us to write a single set of lexical templates
that are reused in every domain (Section 5.1). The
burden of learning word meaning is shifted to the
second, ontology matching, stage of parsing (Sec-
tion 5.2), and modeled with a number of new fea-
tures (Section 7.2) as part of the joint model.
Evaluation We evaluate on held out question-
answer pairs in two benchmark domains, Freebase
and GeoQuery. Following Cai and Yates (2013a),
we also report a cross-domain evaluation where the
Freebase data is divided by topics such as sports,
film, and business. This condition ensures that the
test data has a large percentage of previously unseen
words, allowing us to measure the effectiveness of
the real time ontology matching.
3 Related Work
Supervised approaches for learning semantic parsers
have received significant attention, e.g. (Kate and
Mooney, 2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al, 2010, 2011, 2012; Jones
et al, 2012). However, these techniques require
training data with hand-labeled domain-specific log-
ical expressions. Recently, alternative forms of su-
pervision were introduced, including learning from
question-answer pairs (Clarke et al, 2010; Liang
et al, 2011), from conversational logs (Artzi and
Zettlemoyer, 2011), with distant supervision (Kr-
ishnamurthy and Mitchell, 2012; Cai and Yates,
2013b), and from sentences paired with system
behavior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013). Our
work adds to these efforts by demonstrating a new
approach for learning with latent meaning represen-
tations that scales to large databases like Freebase.
Cai and Yates (2013a) present the most closely
related work. They applied schema matching tech-
niques to expand a CCG lexicon learned with the
UBL algorithm (Kwiatkowski et al, 2010). This ap-
proach was one of the first to scale to Freebase, but
required labeled logical forms and did not jointly
model semantic parsing and ontological reasoning.
This method serves as the state of the art for our
comparison in Section 9.
We build on a number of existing algorithmic
ideas, including using CCGs to build meaning rep-
resentations (Zettlemoyer and Collins, 2005, 2007;
Kwiatkowski et al, 2010, 2011), building deriva-
tions to transform the output of the CCG parser
based on context (Zettlemoyer and Collins, 2009),
and using weakly supervised margin-sensitive pa-
rameter updates (Artzi and Zettlemoyer, 2011,
2013). However, we introduce the idea of learning
an open-domain CCG semantic parser; all previous
methods suffered, to various degrees, from the onto-
logical mismatch problem that motivates our work.
The challenge of ontological mismatch has been
previously recognized in many settings. Hobbs
(1985) describes the need for ontological promiscu-
ity in general language understanding. Many pre-
vious hand-engineered natural language understand-
ing systems (Grosz et al, 1987; Alshawi, 1992; Bos,
2008) are designed to build general meaning rep-
resentations that are adapted for different domains.
Recent efforts to build natural language interfaces to
large databases, for example DBpedia (Yahya et al,
2012; Unger et al, 2012), have also used hand-
engineered ontology matching techniques. Fader
et al (2013) recently presented a scalable approach
to learning an open domain QA system, where onto-
logical mismatches are resolved with learned para-
phrases. Finally, the databases research commu-
nity has a long history of developing schema match-
ing techniques (Doan et al, 2004; Euzenat et al,
2007), which has inspired more recent work on dis-
tant supervision for relation extraction with Free-
base (Zhang et al, 2012).
4 Background
Semantic Modeling We use the typed lambda cal-
culus to build logical forms that represent the mean-
ings of words, phrases and sentences. Logical forms
contain constants, variables, lambda abstractions,
and literals. In this paper, we use the term literal to
refer to the application of a constant to a sequence of
1547
library of new york
N N\N/NP NP
?x.library(x) ?y?f?x.f(x) ? loc(x, y) NY C
>
N\N
?f.?x.f(x) ? loc(x,NY C)
<
N
?x.library(x) ? loc(x,NY C)
Figure 2: A sample CCG parse.
arguments. We include types for entities e, truth val-
ues t, numbers i, events ev, and higher-order func-
tions, such as ?e, t? and ??e, t?, e?. We use David-
sonian event semantics (Davidson, 1967) to explic-
itly represent events using event-typed variables and
conjunctive modifiers to capture thematic roles.
Combinatory Categorial Grammars (CCG)
CCGs are a linguistically-motivated formalism
for modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by
a lexicon and a set of combinators. The lexicon
contains entries that pair words or phrases with
CCG categories. For example, the lexical entry
library ` N : ?x.library(x) in Figure 2 pairs
the word ?library? with the CCG category that has
syntactic category N and meaning ?x.library(x).
A CCG parse starts from assigning lexical entries to
words and phrases. These are then combined using
the set of CCG combinators to build a logical form
that captures the meaning of the entire sentence. We
use the application, composition, and coordination
combinators. Figure 2 shows an example parse.
5 Parsing Sentences to Meanings
The function GEN(x,O) defines the set of possible
derivations for an input sentence x. Each derivation
d = ??,M? builds a logical form y using constants
from the ontology O. ? is a CCG parse tree that
maps x to an underspecified logical form l0. M is an
ontological match that maps l0 onto the fully spec-
ified logical form y. This section describes, with
reference to the example in Figure 3, the operations
used by ? and M .
5.1 Domain Independent Parsing
Domain-independent CCG parse trees ? are built
using a predefined set of 56 underspecified lexi-
cal categories, 49 domain-independent lexical items,
and the combinatory rules introduced in Section 4.
An underspecified CCG lexical category has a
syntactic category and a logical form containing no
constants from the domain ontology O. Instead, the
logical form includes underspecified constants that
are typed placeholders which will later be replaced
during ontology matching. For example, a noun
might be assigned the lexical category N : ?x.p(x),
where p is an underspecified ?e, t?-type constant.
During parsing, lexical categories are created dy-
namically. We manually define a set of POS tags for
each underspecified lexical category, and use Wik-
tionary as a tag dictionary to define the possible POS
tags for words and phrases. Each phrase is assigned
every matching lexical category. For example, the
word ?visit? can be either a verb or a noun in Wik-
tionary. We accordingly assign it all underspecified
categories for the classes, including:
N :?x.p(x) , S\NP/NP :?x?y?ev.p(y, x, ev)
for nouns and transitive verbs respectively.
We also define domain-independent lexical items
for function words such as ?what,? ?when,? and
?how many,? ?and,? and ?is.? These lexi-
cal items pair a word with a lexical cate-
gory containing only domain-independent con-
stants. For example, how many ` S/(S\NP)/N :
?f.?g.?x.eq(x, count(?y.f(y) ? g(y))) contains
the function count and the predicate eq.
Figure 3a shows the lexical categories and combi-
nator applications used to construct the underspeci-
fied logical form l0. Underspecified constants in this
figure have been labeled with the words that they are
associated with for readability.
5.2 Ontological Matching
The second, domain specific, step M maps the un-
derspecified logical form l0 onto the fully specified
logical form y. The mapping from constants in l0
to constants in y is not one-to-one. For example, in
Figure 3, l0 contains 11 constants but y contains only
2. The ontological match is a sequence of matching
operations M = ?o1 . . . , on? that can transform the
structure of the logical form or replace underspeci-
fied constants with constants from O.
1548
(a) Underspecified CCG parse ?: Map words onto underspecified lexical categories as described in Section 5.1. Use
the CCG combinators to combine lexical categories to give the full underpecified logical form l0.
how many people visit the public library of new york annually
S/(S\NP )/N N S\NP/NP NP/N N/N N N\N/NP NP AP
?f.?g.?x.eq(x, count( ?x.People(x) ?x.?y.?ev. ?f.?x.f(x) ?f.?x.f(x)? ?x.Library(x) ?y.?f.?x.Of NewY ork ?ev.Annually(ev)
?y.f(y) ? g(y))) V isit(y, x, ev) Public(x) (x, y) ? f(x)
> >
<
>
>
> <
>
S
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ? Of(z,NewY ork)) ? Annually(e)))
(b) Structure Matching Steps in M : Use the operators described in Section 5.2.1 and Figure 4 to transform l0. In
each step one of the operators is applied to a subexpression of the existing logical form to generate a modified logical
form with a new underspecified constant marked in bold.
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ?Of(z,NewY ork), e) ?Annually(e)))
l1 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y,PublicLibraryOfNewYork, e) ?Annually(e)))
l2 : ?x.HowManyPeopleVisitAnnually(x, PublicLibraryOfNewY ork)))
(c) Constant Matching Steps in M : Replace all underspecified constants in the transformed logical form with a
similarly typed constant from O, as described in Section 5.2.2. The underspecified constant to be replaced is marked
in bold and constants from O are written in typeset.
?x.HowManyPeopleV isitAnnually(x,PublicLibraryOfNewYork)
l3 : 7? ?x.HowManyPeopleV isitAnnually(x, new york public library)
?x.HowManyPeopleVisitAnnually(x, new york public library)
y : 7? ?x.public library system.annual visits(x, new york public library)
Figure 3: Example derivation for the query ?how many people visit the public library of new york annu-
ally.? Underspecified constants are labelled with the words from the query that they are associated with for
readability. Constants from O, written in typeset, are introduced in step (c).
Operator Definition and Conditions Example
a.
Collapse
Literal
to
Constant
P (a1, . . . , an) 7? c
?z.Public(z) ? Library(z) ?Of(z,NewY ork))
7? PublicLibraryOfNewY ork
s.t. type(P (a1, . . . , an)) = type(c) Input and output have type e.
type(c) ? {e, i} e is allowed in O.
freev(P (a1, . . . , an)) = ? Input contains no free variables.
b.
Collapse
Literal
to
Literal
P (a1, . . . , an) 7? Q(b1, . . . , bm)
eq(x, count(?y.People(y) ? ?e.V isit(y,
PublicLibraryOfNewY ork) ?Annually(e)))
7? CountPeopleV isitAnnually(x,
PublicLibraryOfNewY ork)
s.t. type(P (a1, . . . , an)) = type(Q(b1, . . . , bm)) Input and output have type t.
type(Q) ? {type(c) : c ? O} New constant has type ?i, ?e, t??, allowed in O.
freev(P (a1, . . . , an)) = freev(Q(b1, . . . , bm)) Input and output contain single free variable x.
{b1, . . . , bm} ? subexps(P (a1, . . . , an)) Arguments of output literal are subexpressions of input.
c. Split
Literal
P (a1, . . . , ak, x, ak+1, . . . , an)
7? Q(b1, . . . , x, . . . bn) ?Q??(c1, . . . , x, . . . cm)
Dedicate(Mozart,Haydn, ev)
7? Dedicate(Mozart, ev) ?Dedicate??(Haydn, ev)
s.t. type(P (. . . )) = t Input has type t. This matches output type by definition.
{type(Q), type(Q??)} ? {type(c) : c ? O} New constants have allowed type ?e, ?ev, t??.
{b1, . . . , bn, c1, . . . , cm} = {a1, . . . , an} All arguments of input literal are preserved in output.
Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to
match the ontology O. The function type(c) calculates a constant c?s type. The function freev(lf) returns
the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf)
generates the set of all subexpressions of the lambda calculus expression lf .
1549
5.2.1 Structure Matching
Three structure matching operators, illustrated in
Figure 4, are used to collapse or expand literals in
l0. Collapses merge a subexpression from l0 to cre-
ate a new underspecified constant, generating a log-
ical form with fewer constants. Expansions split a
subexpression from l0 to generate a new logical form
containing one extra constant.
Collapsing Operators The collapsing operator
defined in Figure 4a merges all constants in a
literal to generate a single constant of the same
type. This operator is used to map ?z.Public(z)?
Library(z)?Of(z,NewY ork) to PublicLibraryOfNewY ork
in Figure 3b. Its operation is limited to entity typed
expressions that do not contain free variables.
The operator in Figure 4b, in contrast, can be used
to collapse the expression eq(x,count(?y.People(y)?
?e.V isit(y,PublicLibraryOfNewY ork,e))?Annually(e))),
which contains free variable x onto a new expression
CountPeopleV isitAnnually(x,PublicLibraryOfNewY ork).
This is only possible when the type of the newly
created constant is allowed in O and the variable x
is free in the output expression. Subsets of conjuncts
can be collapsed using the operator in Figure 4b by
creating ad-hoc conjunctions that encapsulate them.
Disjunctions are treated similarly.
Performing collapses on the underspecified logi-
cal form allows non-contiguous phrases to be rep-
resented in the collapsed form. In this exam-
ple, the logical form representing the phrase ?how
many people visit? has been merged with the logi-
cal form representing the non-adjacent adverb ?an-
nually.? This generates a new underspecified con-
stant that can be mapped onto the Freebase relation
public library system annual visits that re-
lates to both phrases.
The collapsing operations preserve semantic type,
ensuring that all logical forms generated by the
derivation sequence are well typed. The full set of
allowed collapses of l0 is given by the transitive clo-
sure of the collapsing operations. The size of this
set is limited by the number of constants in l0, since
each collapse removes at least one constant. At each
step, the number of possible collapses is polynomial
in the number of constants in l0 and exponential in
the arity of the most complex type in O. For do-
mains of interest this arity is unlikely to be high and
for triple stores such as Freebase it is 2.
Expansion Operators The fully specified logical
form y can contain constants relating to multiple
words in x. It can also use multiple constants to rep-
resent the meaning of a single word. For example,
Freebase does not contain a relation representing the
concept ?daughter?, instead using two relations rep-
resenting ?female? and ?child?. The expansion oper-
ator in Figure 4c allows a single predicate to be split
into a pair of conjoined predicates sharing an argu-
ment variable. For example, in Figure 1, the constant
for ?dedicate? is split in two to match its represen-
tation in Freebase. Underspecified constants from
l0 can be split once. For the experiments in Sec-
tion 8, we constrain the expansion operator to work
on event modifiers but the procedure generalizes to
all predicates.
5.2.2 Constant Matching
To build an executable logical form y, all under-
specified constants must be replaced with constants
from O. This is done through a sequence of con-
stant replacement operations, each of which replaces
a single underspecified constant with a constant of
the same type from O. Two example replacements
are shown in Figure 3c. The output from the last re-
placement operation is a fully specified logical form.
6 Building and Scoring Derivations
This section introduces a dynamic program used to
construct derivations and a linear scoring model.
6.1 Building Derivations
The space of derivations is too large to explicitly
enumerate. However, each logical form (both final
and interim) can be constructed with many differ-
ent derivations, and we only need to find the highest
scoring one. This allows us to develop a simple dy-
namic program for our two-stage semantic parser.
We use a CKY style chart parser to calculate the
k-best logical forms output by parses of x. We then
store each interim logical form generated by an op-
erator in M once in a hyper-graph chart structure.
The branching factor of this hypergraph is polyno-
mial in the number of constants in l0 and linear in
the size of O. Subsequently, there are too many
possible logical forms to enumerate explicitly; we
1550
prune as follows. We allow the top N scoring on-
tological matches for each original subexpression in
l0 and remove matches that differ from score from
the maximum scoring match by more than a thresh-
old ? . When building derivations, we apply constant
matching operators as soon as they are applicable to
new underspecified constants created by collapses
and expansions. This allows the scoring function
used by the pruning strategy to take advantage of all
features defined in Section 7.2.
6.2 Ranking Derivations
Given feature vector ? and weight vector ?, the score
of a derivation d = ??,M? is a linear function that
decomposes over the parse tree ? and the individual
ontology-matching steps o.
SCORE(d) = ?(d)? (1)
= ?(?)? +
?
o?M
?(o)?
The function PARSE(x,O) introduced as our goal in
Section 2 returns the logical form associated with
the highest scoring derivation of x:
PARSE(x,O) = arg max
d?GEN(x,O)
(SCORE(d))
The features and learning algorithm used to estimate
? are defined in the next section.
7 Learning
This section describes an online learning algorithm
for question-answering data, along with the domain-
independent feature set.
7.1 Learning Model Parameters
Our learning algorithm estimates the parameters ?
from a set {(xi, ai) : i = 1 . . . n} of questions xi
paired with answers ai from the knowledge base
K. Each derivation d generated by the parser is
associated with a fully specified logical form y =
YIELD(d) that can be executed in K. A derivation d
of xi is correct if EXEC(YIELD(d),K) = ai. We use
a perceptron to estimate a weight vector ? that sup-
port a separation of ? between correct and incorrect
answers. Figure 5 presents the learning algorithm.
Input: Q/A pairs {(xi, ai) : i = 1 . . . n}; Knowledge base
K; Ontology O; Function GEN(x,O) that computes deriva-
tions of x; Function YIELD(d)that returns logical form yield
of derivation d; Function EXEC(y,K) that calculates execu-
tion of y in K; Margin ?; Number of iterations T .
Output: Linear model parameters ?.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
C = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) = ai}
W = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) 6= ai}
C? = argmaxd?C(?(d)?)
W ? = {d : d ?W ; ?c ? C? s.t. ?(c)? ? ?(d)? < ?)}
If |C?| > 0 ? |W ?| > 0 :
? = ? + 1|C?|
?
c?C? ?(c)?
1
|W?|
?
e?W? ?(e)
Figure 5: Parameter estimation from Q/A pairs.
7.2 Features
The feature vector ?(d) introduced in Section 6.2
decomposes over each of the derivation steps in d.
CCG Parse Features Each lexical item in ? has
three indicator features. The first indicates the num-
ber of times each underspecified category is used.
For example, the parse in Figure 3a uses the under-
specified category N : ?x.p(x) twice. The second
feature indicates (word, category) pairings ? e.g.
that N : ?x.p(x) is paired with ?library? and ?pub-
lic? once each in Figure 3a. The final lexical feature
indicates (part-of-speech, category) pairings for all
parts of speech associated with the word.
Structural Features The structure matching op-
erators (Section 5.2.1) in M generate new under-
specified constants that define the types of constants
in the output logical form y. These operators are
scored using features that indicate the type of each
complex-typed constant present in y and the iden-
tity of domain-independent functional constants in
y. The logical form y generated in Figure 3 contains
one complex typed constant with type ?i, ?e, t?? and
no domain-independent functional constants. Struc-
tural features allow the model to adapt to different
knowledge bases K. They allow it to determine, for
example, whether a numeric quantity such as ?pop-
ulation? is likely to be explicitly listed in K or if it
should be computed with the count function.
Lexical Features Each constant replacement op-
erator (Section 5.2.2) in M replaces an underspec-
1551
ified constant cu with a constant cO from O. The
underspecified constant cu is associated with the se-
quence of words ~wu used in the CCG lexical en-
tries that introduced it in ?. We assume that each
of the constants cO in O is associated with a string
label ~wO. This allows us to introduce five domain-
independent features that measure the similarity of
~wu and ~wO.
The feature ?np(cu, cO) signals the replacement
of an entity-typed constant cu with entity cO that has
label ~wu. For the second example in Figure 1 this
feature indicates the replacement of the underspeci-
fied constant associated with the word ?mozart? with
the Freebase entity mozart. Stem and synonymy
features ?stem(cu, cO) and ?syn(cu, cO) signal the
existence of words wu ? ~wu and wu ? ~wO that
share a stem or synonym respectively. Stems are
computed with the Porter stemmer and synonyms
are extracted from Wiktionary. A single Freebase
specific feature ?fp:stem(cu, cO) indicates a word
stem match between wu ? ~wu and the word filling
the most specific position in ~wu under Freebase?s hi-
erarchical naming schema.
A final feature ?gl(cu, cO) calculates the overlap
between Wiktionary definitions for ~wu and ~wO. Let
gl(w) be the Wiktionary definition for w. Then:
?gl(cu, cO) =
?
wu? ~wu;wO? ~wO
2?|gl(wO)?gl(wc)|
| ~wO |?| ~wu|?|gl(wO)|+|gl(wc)|
Domain-indepedent lexical features allow the
model to reason about the meaning of unseen words.
In small domains, however, the majority of word us-
ages may be covered by training data. We make use
of this fact in the GeoQuery domain with features
?m(cu, cO) that indicate the pairing of ~wu with cO.
Knowledge Base Features Guided by the obser-
vation that we generally want to create queries y
which have answers in knowledge base K, we de-
fine features to signal whether each operation could
build a logical form y with an answer in K.
If a predicate-argument relation in y does not
exist in K, then the execution of y against K
will not return an answer. Two features indicate
whether predicate-argument relations in y exist inK.
?direct(y,K) indicates predicate-argument applica-
tions in y that exists in K. For example, if the appli-
cation of dedicated by to mozart in Figure 1 ex-
ists in Freebase, ?direct(y,K) will fire. ?join(y,K)
indicates entities separated from a predicate by one
join in y, such as mozart and dedicated to in Fig-
ure 1, that exist in the same relationship in K.
If two predicates that share a variable in y
do not share an argument in that position in K
then the execution of y against K will fail. The
predicate-predicate ?pp(y,K) feature indicates pairs
of predicates that share a variable in y but can-
not occur in this relationship in K. For ex-
ample, since the subject of the Freebase prop-
erty date of birth does not take arguments of
type location, ?pp(y,K) will fire if y con-
tains the logical form ?x?y.date of birth(x, y)?
location(x).
Both the predicate-argument and predicate-
predicate features operate on subexpressions of y.
We also define the execution features: ?emp(y,K) to
signal an empty answer for y in K; ?0(y,K) to sig-
nal a zero-valued answer created by counting over
an empty set; and ?1(y,K) to signal a one-valued
answer created by counting over a singleton set.
As with the lexical cues, we use knowledge base
features as soft constraints since it is possible for
natural language queries to refer to concepts that do
not exist in K.
8 Experimental Setup
Data We evaluate performance on the benchmark
GeoQuery dataset (Zelle and Mooney, 1996), and a
newly introduced Freebase Query (FQ) dataset (Cai
and Yates, 2013a). FQ contains 917 questions la-
beled with logical form meaning representations for
querying Freebase. We gathered question answer la-
bels by executing the logical forms against Freebase,
and manually correcting any inconsistencies.
Freebase (Bollacker et al, 2008) is a large, col-
laboratively authored database containing almost 40
million entities and two billion facts, covering more
than 100 domains. We filter Freebase to cover the
domains contained in the FQ dataset resulting in a
database containing 18 million entities, 2072 rela-
tions, 635 types, 135 million facts and 81 domains,
including for example film, sports, and business. We
use this schema to define our target domain, allow-
ing for a wider variety of queries than could be en-
coded with the 635 collapsed relations previously
used to label the FQ data.
1552
We report two different experiments on the FQ
data: test results on the existing 642/275 train/test
split and domain adaptation results where the data is
split three ways, partitioning the topics so that the
logical meaning expressions do not share any sym-
bols across folds. We report on the standard 600/280
training/test split for GeoQuery.
Parameter Initialization and Training We ini-
tialize weights for ?np and ?direct to 10, and weights
for ?stem and ?join to 5. This promotes the use of
entities and relations named in sentences. We ini-
tialize weights for ?pp and ?emp to -1 to favour log-
ical forms that have an interpretation in the knowl-
edge base K. All other feature weights are initial-
ized to 0. We run the training algorithm for one it-
eration on the Freebase data, at which point perfor-
mance on the development set had converged. This
fast convergence is due to the very small number of
matching parameters used (5 lexical features and 8
K features). For GeoQuery, we include the larger
domain specific feature set introduced in Section 7.2
and train for 10 iterations. We set the pruning pa-
rameters from Section 6.1 as follows: k = 5 for
Freebase, k = 30 for GeoQuery, N = 50, ? = 10.
Comparison Systems We compare performance
to state-of-the-art systems in both domains. On
GeoQuery, we report results from DCS (Liang
et al, 2011) without special initialization (DCS) and
with an small hand-engineered lexicon (DCS with
L+). We also include results for the FUBL algo-
rithm (Kwiatkowski et al, 2011), the CCG learning
approach that is most closely related to our work. On
FQ, we compare to Cai and Yates (2013a) (CY13).
Evaluation We evaluate by comparing the pro-
duced question answers to the labeled ones, with no
partial credit. Because the parser can fail to pro-
duce a complete query, we report recall, the percent
of total questions answered correctly, and precision,
the percentage of produced queries with correct an-
swers. CY13 and FUBL report fully correct logical
forms, which is a close proxy to our numbers.
9 Results
Quantitative Analysis For FQ, we report results
on the test set and in the cross-domain setting, as de-
fined in Section 8. Figure 6 shows both results. Our
Setting System R P F1
Test Our Approach 68.0 76.7 72.1
CY13 59 67 63
Cross Our Approach 67.9 73.5 71.5
Domain CY13 60 69 65
Figure 6: Results on the FQ dataset.
R P F1
All Features 68.6 72.0 70.3
Without Wiktionary 67.2 70.7 68.9
Without K Features 61.8 62.5 62.1
Figure 7: Ablation Results
approach outperforms the previous state of the art,
achieving a nine point improvement in test recall,
while not requiring labeled logical forms in train-
ing. We also see consistent improvements on both
scenarios, indicating that our approach is generaliz-
ing well across topic domains. The learned ontology
matching model is able to reason about previously
unseen ontological subdomains as well as if it was
provided explicit, in-domain training data.
We also performed feature ablations with 5-fold
cross validation on the training set, as seen in Fig-
ure 7. Both the Wiktionary features and knowledge
base features were helpful. Without the Wiktionary
features, the model must rely on word stem matches
which, in combination with graph constraints, can
still recover many of the correct queries. However,
without the knowledge base constraints, the model
produces many queries that return empty answers,
and significantly impacts overall performance.
For GeoQuery, we report test results in Figure 8.
Our approach outperforms the most closely related
CCG model (FUBL) and DCS without initialization,
but falls short of DCS with a small hand-built initial
lexicon. Given the small size of the test set, it is fair
to say that all algorithms are performing at state-of-
the-art levels. This result demonstrates that our ap-
Recall
FUBL 88.6
DCS 87.9
DCS with L+ 91.1
Our Approach 89.0
Figure 8: GeoQuery Results
1553
Parse Failures (20%)
1. Query in what year did motorola have the most revenue
2 Query on how many projects was james walker a design engineer
Structural Matching Failure (30%)
Query how many children does jerry seinfeld have
3. Labeled ?x.eq(x, count(?y.people.person.children(jerry seinfeld, y)))
Predicted ?x.eq(x, count(?y.people.person.children(y, jerry seinfeld)))
Incomplete Database (10%)
Query how many countries participated in the 2006 winter olympics
4. Labeled ?y.olympics.olympic games.number of countries(2006 winter olympics, y)
Predicted ?y.eq(y, count(?y.olympic participation country.olympics participated in(x, 2006 winter olympics)))
Query what programming languages were used for aol instant messenger
5. Labeled ?y.computer.software.languages used(aol instant messenger, y)
Predicted ?y.computer.software.languages used(aol instant messenger, y) ? computer.programming language(y)
Lexical Ambiguity (35%)
Query when was the frida kahlo exhibit at the philadelphia art museum
Labeled ?y.?x.exhibition run.exhibition(x, frida kahlo)?
6. exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.opened on(x, y)
Predicted ?y.?x.exhibition run.exhibition(x, frida kahlo)?
exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.closed on(x, y)
Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard
references. 5% of the cases were miscellaneous or otherwise difficult to categorize.
proach can handle the high degree of lexical ambi-
guity in the FQ data, without sacrificing the ability
to understanding the rich, compositional phenomena
that are common in the GeoQuery data.
Qualitative Analysis We also did a qualitative
analysis of errors in the FQ domain. The model
learns to correctly produce complex forms that join
multiple relations. However, there are a number of
systematic error cases, grouped into four categories
as seen in Figure 9.
The first and second examples show parse fail-
ures, where the underspecified CCG grammar did
not have sufficient coverage. The third shows a
failed structural match, where all of the correct logi-
cal constants are selected, but the argument order is
reversed for one of the literals. The fourth and fifth
examples demonstrate a failures due to database in-
completeness. In both cases, the predicted queries
would have returned the same answers as the gold-
truth ones if Freebase contained all of the required
facts. Developing models that are robust to database
incompleteness is a challenging problem for future
work. Finally, the last example demonstrates a lex-
ical ambiguity, where the system was unable to de-
termine if the query should include the opening date
or the closing date for the exhibit.
10 Conclusion
We considered the problem of learning domain-
independent semantic parsers, with application to
QA against large knowledge bases. We introduced
a new approach for learning a two-stage semantic
parser that enables scalable, on-the-fly ontological
matching. Experiments demonstrated state-of-the-
art performance on benchmark datasets, including
effective generalization to previously unseen words.
We would like to investigate more nuanced no-
tions of semantic correctness, for example to support
many of the essentially equivalent meaning repre-
sentations we found in the error analysis. Although
we focused exclusively on QA applications, the gen-
eral two-stage analysis approach should allow for
the reuse of learned grammars across a number of
different domains, including robotics or dialog ap-
plications, where data is more challenging to gather.
11 Acknowledgements
This research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), the NSF (IIS-1115966), and
by a gift from Google. The authors thank Anthony
Fader, Nicholas FitzGerald, Adrienne Wang, Daniel
Weld, and the anonymous reviewers for their helpful
comments and feedback.
1554
References
Alshawi, H. (1992). The core language engine. The
MIT Press.
Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping
semantic parsers from conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Artzi, Y. and Zettlemoyer, L. (2013). Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Associ-
ation for Computational Linguistics, 1(1):49?62.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and
Taylor, J. (2008). Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In Proceedings of the ACM SIGMOD Inter-
national Conference on Management of Data.
Bos, J. (2008). Wide-coverage semantic analysis
with boxer. In Proceedings of the Conference on
Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale semantic
parsing via schema matching and lexicon exten-
sion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic parsing
freebase: Towards open-domain semantic pars-
ing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Chen, D. and Mooney, R. (2011). Learning to inter-
pret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S. and Curran, J. (2007). Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?
552.
Clarke, J., Goldwasser, D., Chang, M., and Roth,
D. (2010). Driving semantic parsing from the
world?s response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, pages 105?
148.
Doan, A., Madhavan, J., Domingos, P., and Halevy,
A. (2004). Ontology matching: A machine
learning approach. In Handbook on ontologies.
Springer.
Euzenat, J., Euzenat, J., Shvaiko, P., et al (2007).
Ontology matching. Springer.
Fader, A., Zettlemoyer, L., and Etzioni, O. (2013).
Paraphrase-driven learning for open question an-
swering. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Goldwasser, D. and Roth, D. (2011). Learning from
natural instructions. In Proceedings of the In-
ternational Joint Conference on Artificial Intelli-
gence.
Grosz, B. J., Appelt, D. E., Martin, P. A., and
Pereira, F. (1987). TEAM: An experiment in
the design of transportable natural language inter-
faces. Artificial Intelligence, 32(2):173?243.
Hobbs, J. R. (1985). Ontological promiscuity. In
Proceedings of the Annual Meeting on Associa-
tion for Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S. (2012).
Semantic parsing with bayesian tree transducers.
In Proceedings of the 50th Annual Meeting of the
Association of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013). Jointly
learning to parse and perceive: Connecting nat-
ural language to the physical world. Transactions
of the Association for Computational Linguistics,
1(2).
Krishnamurthy, J. and Mitchell, T. (2012). Weakly
supervised training of semantic parsers. In Pro-
ceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Kushman, N. and Barzilay, R. (2013). Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics.
1555
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L.,
and Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter
of the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical generalization
in CCG grammar induction for semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Liang, P., Jordan, M., and Klein, D. (2011). Learn-
ing dependency-based compositional semantics.
In Proceedings of the Conference of the Associ-
ation for Computational Linguistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,
L., and Fox, D. (2012). A joint model of language
and perception for grounded attribute learning. In
Proceedings of the International Conference on
Machine Learning.
Muresan, S. (2011). Learning for deep language un-
derstanding. In Proceedings of the International
Joint Conference on Artificial Intelligence.
Steedman, M. (1996). Surface Structure and Inter-
pretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The
MIT Press.
Unger, C., Bu?hmann, L., Lehmann, J.,
Ngonga Ngomo, A., Gerber, D., and Cimiano, P.
(2012). Template-based question answering over
RDF data. In Proceedings of the International
Conference on World Wide Web.
Wong, Y. and Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Confer-
ence of the Association for Computational Lin-
guistics.
Yahya, M., Berberich, K., Elbassuoni, S., Ramanath,
M., Tresp, V., and Weikum, G. (2012). Natural
language questions for the web of data. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Zelle, J. and Mooney, R. (1996). Learning to parse
database queries using inductive logic program-
ming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sentences
to logical form. In Proceedings of the Joint Con-
ference of the Association for Computational Lin-
guistics and International Joint Conference on
Natural Language Processing.
Zhang, C., Hoffmann, R., and Weld, D. S. (2012).
Ontological smoothing for relation extraction
with minimal supervision. In Proceeds of the
Conference on Artificial Intelligence.
1556
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1914?1925,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Distributions over Logical Forms
for Referring Expression Generation
Nicholas FitzGerald Yoav Artzi Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{nfitz,yoav,lsz}@cs.washington.edu
Abstract
We present a new approach to referring ex-
pression generation, casting it as a density es-
timation problem where the goal is to learn
distributions over logical expressions identi-
fying sets of objects in the world. Despite
an extremely large space of possible expres-
sions, we demonstrate effective learning of
a globally normalized log-linear distribution.
This learning is enabled by a new, multi-stage
approximate inference technique that uses a
pruning model to construct only the most
likely logical forms. We train and evaluate
the approach on a new corpus of references
to sets of visual objects. Experiments show
the approach is able to learn accurate models,
which generate over 87% of the expressions
people used. Additionally, on the previously
studied special case of single object reference,
we show a 35% relative error reduction over
previous state of the art.
1 Introduction
Understanding and generating natural language re-
quires reasoning over a large space of possible
meanings; while many statements might achieve the
same goal in a certain situation, some are more
likely to be used than others. In this paper, we model
these preferences by learning distributions over sit-
uated meaning use.
We focus on the task of referring expression gen-
eration (REG), where the goal is to produce an ex-
pression which uniquely identifies a pre-defined ob-
ject or set of objects in an environment. In prac-
tice, many such expressions can be produced. Fig-
ure 1 shows referring expressions provided by hu-
man subjects for a set of objects (Figure 1a), demon-
strating variation in utterances (Figure 1b) and their
corresponding meaning representations (Figure 1c).
Although nearly a third of the people simply listed
the colors of the desired objects, many other strate-
gies were also used and no single option dominated.
Learning to model such variation would enable sys-
tems to better anticipate what people are likely to
say and avoid repetition during generation, by pro-
ducing appropriately varied utterances themselves.
With these goals in mind, we cast REG as a den-
sity estimation problem, where the goal is to learn a
distribution over logical forms.
Learning such distributions is challenging. For a
target set of objects, the number of logical forms
that can be used to describe it grows combinatori-
ally with the number of observable properties, such
as color and shape. However, only a tiny fraction
of these possibilities are ever actually used by peo-
ple. We must learn to efficiently find these few, and
accurately estimate their associated likelihoods.
We demonstrate effective learning of a globally
normalized log-linear distribution with features to
account for context dependence and communicative
goals. We use a stochastic gradient descent algo-
rithm, where the key challenge is the need to com-
pute feature expectations over all possible logical
forms. For that purpose, we present a multi-stage
inference algorithm, which progressively constructs
meaning representations with increasing complex-
ity, and learns a pruning model to retain only those
that are likely to lead to high probability expres-
sions. This approach allows us to consider a large
1914
(a)
The green, red, orange and yellow toys. (1)
The green, red, yellow, and orange objects. (1)
The red, green, yellow and orange toys. (1)
The red, yellow, orange and green objects. (1)
All the green, red, yellow and orange toys. (1)
All the yellow, orange, red and green objects. (1)
All the pieces that are not blue or brown. (2)
All items that are not brown or blue. (2)
All items that are not brown or blue. (2)
Everything that is not brown or blue. (3)
Everything that is not purple or blue. (3)
All but the black and blue ones. (4)
Any toy but the blue and brown toys. (4)
Everything that is green, red, orange or yellow. (5)
All objects that are not triangular or blue. (6)
Everything that is not blue or a wedge. (7)
Everything that is not a brown or blue toy. (8)
All but the blue piece and brown wedge. (9)
Everything except the brown wedge and the blue object. (10)
All pieces but the blue piece and brown triangle shape. (11)
(b)
P? (z|S,G) z
0.30 ?(?x.(yellow(x) ? orange(x) ? red(x) ? green(x)) ? object(x) ? plu(x)) (1)
0.15 ?(?x.?(brown(x) ? blue(x)) ? object(x) ? plu(x)) (2)
0.10 Every(?x.?(brown(x) ? blue(x)) ? object(x) ? sg(x)) (3)
0.10 Every(?x.object(x) ? sg(x)) \ [?(?x.(blue(x) ? brown(x)) ? object(x) ? plu(x)] (4)
0.05 Every(?x.(yellow(x) ? orange(x) ? red(x) ? green(x)) ? object(x) ? sg(x)) (5)
0.05 ?(?x.(triangle(x) ? blue(x)) ? object(x) ? plu(x)) (6)
0.05 Every(?x.object(x) ? sg(x)?(blue(x) ? equal(x,A(?y.triangle(y) ? sg(y))))) (7)
0.05 Every(?x.object(x) ? sg(x) ? ?equal(x,A(?y.(brown(y) ? blue(y)) ? object(y) ? sg(y)))) (8)
0.05 Every(?x.object(x) ? sg(x)) \ [?(?x.(blue(x) ? object(x) ? sg(x)) ? (brown(x) ? triangle(x) ? sg(x))] (9)
0.05 Every(?x.object(x) ? sg(x)) \ [?(?x.brown(x) ? triangle(x) ? sg(x)) ? ?(?y.blue(y) ? object(y) ? sg(x))] (10)
0.05 ?(?x.object(x) ? plu(x)) \ [?(?x.(blue(x) ? object(x) ? sg(x)) ? (brown(x) ? triangle(x) ? object(x) ? sg(x))] (11)
(c)
Figure 1: An example scene from our object selection dataset. Figure 1a shows the image shown to subjects
on Amazon Mechanical Turk. The target set G is the circled objects. Figure 1b shows the 20 sentences
provided as responses. Figure 1c shows the empirical distribution P? (z|G,S) for this scene, estimated by
labeling the sentences in Figure 1b. The correspondence between a sentence in 1b and its labeled logical
expression in 1c is indicated by the number in parentheses. Section 5.1 presents a discussion of the space of
possible logical forms.
set of possible meanings, while maintaining compu-
tational tractability.
To represent meaning we build on previous ap-
proaches that use lambda calculus (Carpenter, 1997;
Zettlemoyer and Collins, 2005; Artzi and Zettle-
moyer, 2013b). We extend these techniques by mod-
eling the types of plurality and coordination that are
prominent in expressions which refer to sets.
We also present a new corpus for the task of re-
ferring expression generation.1 While most previ-
ous REG data focused on naming single objects,
1The corpus was collected using Amazon Mechanical Turk
and is available on the authors? websites.
to the best of our knowledge, this is the first cor-
pus with sufficient coverage for learning to name
sets of objects. Experiments demonstrate highly ac-
curate learned models, able to generate over 87%
of the expressions people used. On the previously
studied special case of single object reference, we
achieve state-of-the-art performance, with over 35%
relative error reduction over previous state of the
art (Mitchell et al, 2013).
2 Related Work
Referring expression generation has been exten-
sively studied in the natural language generation
1915
community, dating as far back as SHRDLU (Wino-
grad, 1972). Most work has built on variations of
the Incremental Algorithm (Dale and Reiter, 1995),
a deterministic algorithm for naming single ob-
jects that constructs conjunctive logical expressions.
REG systems are used in generation pipelines (Dale
and Reiter, 2000) and are also commonly designed
to be cognitively plausible, for example by following
Gricean maxims (Grice, 1975). Krahmer and van
Deemter (2012) and van Deemter et al (2012a) sur-
vey recent literature on REG.
Different approaches have been proposed for gen-
erating referring expressions for sets of objects.
Van Deemter (2002) extended the Incremental Al-
gorithm to allow disjunction and negation, enabling
reference to sets. Further work attempted to re-
solve the unnaturally long expressions which could
be generated by this approach (Gardent, 2002; Ho-
racek, 2004; Gatt and van Deemter, 2007). Later, de-
scription logic was used to name sets (Areces et al,
2008; Ren et al, 2010). All of these algorithms are
manually engineered and deterministic.
In practice, human utterances are surprisingly
varied, loosely following the Gricean ideals (van
Deemter et al, 2012b). Much recent work in REG
has identified the importance of modeling the vari-
ation observed in human-generated referring ex-
pressions (Viethen and Dale, 2010; Viethen et al,
2013; van Deemter et al, 2012b; Mitchell et al,
2013), and some approaches have applied machine-
learning techniques to single-object references (Vi-
ethen and Dale, 2010; Mitchell et al, 2011a,b). Re-
cently, Mitchell et al (2013) introduced a proba-
bilistic approach for conjunctive descriptions of sin-
gle objects, which will provide a comparison base-
line for experiments in Section 8. To the best of
our knowledge, this paper presents the first learned
probabilistic model for referring expressions defin-
ing sets, and is the first effort to treat REG as a den-
sity estimation problem.
REG is related to content selection, which
has been studied for generating text from
databases (Konstas and Lapata, 2012), event
streams (Chen et al, 2010), images (Berg et al,
2012; Zitnick and Parikh, 2013), and text (Barzilay
and Lapata, 2005; Carenini et al, 2006). However,
most approaches to this problem output bags of con-
cepts, while we construct full logical expressions,
allowing our approach to capture complex relations
between attributes.
Finally, our approach to modeling meaning us-
ing lambda calculus is related to a number of ap-
proaches that used similar logical representation
in various domains, including database query in-
terfaces (Zelle and Mooney, 1996; Zettlemoyer
and Collins, 2005, 2007), natural language instruc-
tions (Chen and Mooney, 2011; Matuszek et al,
2012b; Kim and Mooney, 2012; Artzi and Zettle-
moyer, 2013b), event streams (Liang et al, 2009;
Chen et al, 2010), and visual descriptions (Ma-
tuszek et al, 2012a; Krishnamurthy and Kollar,
2013). Our use of logical forms follows this line of
work, while extending it to handle plurality and co-
ordination, as described in Section 4.1. In addition,
lambda calculus was shown to enable effective nat-
ural language generation from logical forms (White
and Rajkumar, 2009; Lu and Ng, 2011). If com-
bined with these approaches, our approach would
allow the creation of a complete REG pipeline.
3 Technical Overview
Task Let Z be a set of logical expressions that se-
lect a target set of objects G in a world state S, as
formally defined in Section 5.1. We aim to learn a
probability distribution P (z | S,G), with z ? Z .
For example, in the referring expressions domain
we work with, the state S = {o1, . . . , on} is a set
of n objects oi. Each oi has three properties: color,
shape and type. The target setG ? S is the subset of
objects to be described. Figure 1a shows an example
scene. The world state S includes the 11 objects in
the image, where each object is assigned color (yel-
low, green . . . ), shape (cube, cylinder . . . ) and type
(broccoli, apple . . . ). The target set G contains the
circled objects. Our task is to predict a distribution
which closely matches the empirical distribution in
Figure 1c.
Model and Inference We model P (z|S,G) as a
globally normalized log-linear model, using features
of the logical form z, and its execution with respect
to S and G. Since enumerating all z ? Z is in-
tractable, we develop an approximate inference al-
gorithm which constructs a high quality candidate
set, using a learned pruning model. Section 5.2 de-
scribes the globally scored log-linear model. Sec-
1916
tion 5.3 presents a detailed description of the infer-
ence procedure.
Learning We use stochastic gradient descent to
learn both the global scoring model and the explicit
pruning model, as described in section 6. Our data
consists of human-generated referring expressions,
gathered from Amazon Mechanical Turk. These
sentences are automatically labelled with logical
forms with a learned semantic parser, providing a
stand-in for manually labeled data (see Section 7).
Evaluation Our goal is to output a distribution
that closely matches the distribution that would be
produced by humans. We therefore evaluate our
model with gold standard labeling of crowd-sourced
referring expressions, which are treated as samples
from the implicit distribution we are trying to model.
The data and evaluation procedure are described in
Section 7. The results are presented in Section 8.
4 Modeling Referring Expressions
4.1 Semantic Modeling
Our semantic modeling approach uses simply-typed
lambda-calculus following previous work (Carpen-
ter, 1997; Zettlemoyer and Collins, 2005; Artzi and
Zettlemoyer, 2013b), extending it in one important
way: we treat sets of objects as a primitive type,
rather than individuals. This allows us to model plu-
rality, cardinality, and coordination for the language
observed in our data, and is further motivated by re-
cent cognitive science evidence that sets and their
properties are represented as single units in human
cognition (Scontras et al, 2012).
Plurals Traditionally, noun phrases are identified
with the entity-type e and pick out individual ob-
jects (Carpenter, 1997). This makes it difficult to
interpret plural noun-phrases which pick out a set of
objects, like ?The red cubes?. Previous approaches
would map this sentence to the same logical expres-
sion as the singular ?The red cube?, ignoring the se-
mantic distinction encoded by the plural.
Instead, we define the primitive entity e to range
over sets of objects. ?e, t?-type expressions are
therefore functions from sets to a truth-value. These
are used in two ways, modeling both distributive and
collective predicates (cf. Stone, 2000):
1. Distributive predicates are ?e, t?-type expres-
sions which will return true if every individual
in the set has a given property. For example, the
expression ?x.red(x) will be true for all sets
which contain only objects for which the value
red is true.
2. Collective predicates are ?e, t?-type expres-
sions which indicate a property of the set it-
self. For example, in the phrase ?the two
cubes?, ?two? corresponds to the expression
?x.cardinality2(x) which will return true
only for sets which have exactly two members.
We define semantic plurality in terms of two spe-
cial collective predicates: sg for singular and plu
for plural. For examples, ?cube? is interpreted as
?x.cube(x) ? sg(x), whereas ?cubes? is interpreted
as ?x.cube(x) ? plu(x). The sg predicate returns
true only for singleton sets. The plu predicate re-
turns true for sets that contain two or more objects.
We also model three kinds of determiners,
functional-type ??e, t?, e?-type expressions which
select a single set from the power-set represented
by their ?e, t?-type argument. The definite deter-
miner ?the? is modeled with the predicate ?, which
resolves to the maximal set amongst those licensed
by its argument. The determinerEvery only accepts
?e, t?-type arguments that define singleton sets (i.e.
the argument includes the sg predicate) and returns
a set containing the union of these singletons. For
example, although ?red cube? is a singular expres-
sion, ?Every red cube? refers to a set. Finally, the
indefinite determiner ?a? is modeled with the logical
constant A, which picks a singleton set by implic-
itly introducing an existential quantifier (Artzi and
Zettlemoyer, 2013b).2
Coordination Two types of coordination are
prominent in set descriptions. The first is attribute
coordination, which is typically modeled with the
boolean operators: ? for conjunction and ? for dis-
junction. For example, the phrase ?the red cubes
and green rectangle? involves a disjunction that joins
two conjunctive expressions, both within the scope
of the definite determiner: ?(?x.(red(x)?cube(x)?
plu(x)) ? (green(x) ? rectangle(x) ? sg(x))).
2This treatment of the indefinite determiner is related to gen-
eralized skolem terms as described by Steedman (2011).
1917
The second kind of coordination, a new addition
of this work, occurs when two sets are coordinated.
This can either be set union (?) as in the phrase ?The
cubes and the rectangle? (?(?x.cube(x)? plu(x))?
?(?x.rectangle(x) ? sg(x)))), or set difference
(\) as in the phrase ?All blocks except the green
cube?: (?(?x.object(x)?plu(x))\?(?x.green(x)?
cube(x) ? sg(x))).
4.2 Visual Domain
Objects in our scenes are labeled with attribute val-
ues for four attribute types: color (7 values, such
as red, green), shape (9 values, such as cube,
sphere), type (16 values, such as broccoli, apple)
and a special object property, which is true for
all objects. The special object property captures
the role of descriptions that are true for all objects,
such as ?toy? or ?item?. Each of these 33 attribute
values corresponds to an ?e, t?-type predicate.
5 Model and Inference
In this section, we describe our approach to mod-
eling the probability P (z | S,G) of a logical form
z ? Z that names a set of objects G in a world S, as
defined in Section 3. We first define Z (Section 5.1),
and then present the distribution (Section 5.2) and an
approximate inference approach that makes use of a
learned pruning model (Section 5.3).
5.1 Space of Possible Meanings
The set Z defines logical expressions that we will
consider for picking the target set G in state S. In
general, we can construct infinitely many such ex-
pressions. For example, every z ? Z can be triv-
ially extended to form a new candidate z? for Z
by adding a true clause to any conjunct it contains.
However, the vast majority of such expressions are
overly complex and redundant, and would never be
used in practice as a referring expression.
To avoid this explosion, we limit the type and
complexity of the logical expressions that are in-
cluded in Z . We consider only e-type expressions,
since they name sets, and furthermore only include
expressions that name the desired target set G.3 We
3We do not attempt to model underspecified or otherwise
incorrect expressions, although our model could handle this by
considering all e-type expressions.
? p : ??e, t?, e?, e1 : ?e, t? ? p(e1) : e
e.g.
p = ? : ??e, t?, e?
e1 = ?x.cube(x) ? sg(x) : ?e, t?
?(?x.cube(x) ? sg(x)) : e
? p : ?t, t?, e1 : ?e, t? ? ?x.p(e1(x)) : ?e, t?
e.g.
p = ? : ?t, t?
e1 = ?x.red(x) : ?e, t?
?x.?(red(x)) : ?e, t?
? p : ?e, ?e, t??, e1 : e? ?x.(p(x))(e1)
e.g.
p = equal : ?e, ?e, t??
e1 = A(?y.cube(y) ? sg(y)) : e
?x.equal(x,A(?y.cube(y) ? sg(y)))
? p : ?e, ?e, e??, e1 : e, e2 : e? (p(e1))(e2) : e
e.g.
p = \ : ?e, ?e, e??
e1 = ?(?x.cube(x) ? plu(x)) : e
e2 = Every(?x.object(x) ? sg(x)) : e
Every(?x.object(x) ? sg(x)) \
?(?x.cube(x) ? plu(x)) : e
? p : ?t, ?t, t??, e1 : ?e, t?, e2?e, t? ?
?x.(p(e1(x)))(e2(x)) : ?e, t?
e.g.
p = ? : ?t, ?t, t??
e1 = ?x.red(x) : ?e, t?
e2 = ?x.cube(x) : ?e, t?
?x.red(x) ? cube(x) : e
Figure 2: The five rules used during generation.
Each rule is a template which takes a predicate p : t
of type t and one or two arguments ei : ti, with type
ti. The output is the logical expression after the ar-
row?, constructed using the inputs as shown.
also limit the overall complexity of each z ? Z , to
contain not more than M logical constants.
To achieve these constraints, we define an induc-
tive procedure for enumerating Z , in order of com-
plexity. We first define Aj to be the set of all e- and
?e, t?-type expressions that contain exactly j logi-
cal constants. Figure 2 presents five rules that can be
used to constructAj by induction, for j = 1, . . . ,?,
by repeatedly adding new constants to expressions
in Aj? for j? < j. Intuitively, Aj is the set of all
complexity j expressions that can be used as sub-
expressions for higher complexity entires in our final
set Z . Next, we define Zj to be the e-type expres-
sions inAj that name the correct setG. And, finally,
Z = ?j=1...MZj of all correct expressions up to a
maximum complexity of M .
This construction allows for a finite Z with good
1918
empirical coverage, as we will see in the experi-
ments in Section 8. However, Z is still prohibitively
large for the maximum complexities used in practise
(for example M = 20). Section 5.3 presents an ap-
proach for learning models to prune Z , while still
achieving good empirical coverage.
5.2 Global Model
Given a finite Z , we can now define our desired
globally normalized log-linear model, conditioned
on the state S and set of target objects G:
PG(z | S,G; ?) =
1
C
e???(z,S,G) (1)
where ? ? Rn is a parameter vector, ?(z, S,G) ?
Rn is a feature function and C is the normalization
constant. Section 5.4 defines the features we use.
5.3 Pruning Z
As motivated in Section 5.1, the key challenge for
our global model in Equation 1 is that the set Z is
too large to be explicitly enumerated. Instead, we
designed an approach for learning to approximate Z
with a subset of the highly likely entries, and use this
subset as a proxy for Z during inference.
More specifically, we define a binary distribution
that is used to classify whether each a ? Aj is likely
to be used as a sub-expression in Z , and prune each
Aj to keep only the top k most likely entries. This
distribution is a logistic regression model:
Pj(a | S,G;pij) =
epij ??(a,S,G)
1 + epij ??(a,S,G)
(2)
with features ?(a, S,G) ? Rn and parameters pij ?
Rn. This distribution uses the same features as the
global model presented in Equation 1, which we de-
scribe in Section 5.4.
Together, the pruning model and global model de-
fine the distribution P? (z | G,S; ?,?) over z ? Z ,
conditioned on the world state S and target set G,
and parameterized by both the parameters ? of the
global model and the parameters ? = {pi1, . . . , piM}
of the pruning models.
5.4 Features
We use three kinds of features: logical expression
structure features, situated features and a complexity
feature. All features but the complexity feature are
shared between the global model in Equation 1 and
the pruning model in Equation 2. In order to avoid
overly specific features, the attribute value predi-
cates in the logical expressions are replaced with
their attribute type (ie. red ? color). In addition,
the special constants sg and plu are ignored when
computing features.
In the following description of our features, all
examples are computed for the logical expression
?(?x.red(x) ? object(x) ? plu(x)), with respect to
the scene and target set in Figure 1a.
Structure Features We use binary features that
account for the presence of certain structures in the
logical form, allowing the model to learn common
usage patterns.
? Head Predicate - indicator for use of a logi-
cal constant as a head predicate in every sub-
expression of the expression. A head predicate
is the top-level operator of an expression. For
example, the head predicate of the expression
??x.red(x) ? object(x)? is ??? and the head
of ?x.red(x) is red. For our running example,
the head features are ?, ?, color, object.
? Head-Predicate Bigrams and Trigrams -
head-predicate bigrams are defined to be the
head predicate of a logical form, and the head
predicate of one of its children. Trigrams
are similarly defined. E.g. bigrams: [?,?],
[?, color], [?, object], and trigrams: [?,?, red],
[?,?, object].
? Conjunction Duplicate - this feature fires if a
conjunctive expression contains duplicate sub-
expressions amongst its children.
? Coordination Children - this feature set indi-
cates the presence of a coordination subexpres-
sion (?, ?, ? or \) and the head expressions
of all pairs and triples of its child expressions.
E.g. [?; red, object].
Situated Features These features take into ac-
count the evaluation of the logical form z with re-
spect to the state S and target set G. They capture
common patterns between the target set G and the
object groups named by subexpressions of z.
1919
? Head Predicate and Coverage - this fea-
ture set indicates the head predicate of ev-
ery sub-expression of the logical form, com-
bined with a comparison between the execu-
tion of the sub-expression and the target set
G. The possible values for this comparison
(which we call the ?coverage? of the expres-
sion with respect to G) are: EQUAL, SUBSET
(SUB), SUPERSET (SPR), DISJOINT, ALL,
EMPTY and OTHER. E.g. [?,SUB], [?,SUB],
[color,SUB], [object,ALL]
? Coordination Child Coverage - this feature
set indicates the head-predicate of a coordina-
tion subexpression, combined with the cover-
age of all pairs and triples of its child expres-
sions. E.g. [?;SUB,ALL].
? Coordination Child Relative Coverage - this
feature set indicates, for every pair of child sub-
expressions of coordination expressions in the
logical form, the coverage of the child sub-
expressions relative to each other. The pos-
sible relative coverage values are: SUB-SPR,
DISJOINT, OTHER. E.g. [?;SUB-SPR].
Complexity Features We use a single real-
numbered feature to account for the complexity of
the logical form. We define the complexity of a log-
ical form to be the number of logical constants used.
Our running example has a complexity of 4. This
feature is only used in the global model, since the
pruning model always considers logical expressions
of fixed complexity.
6 Learning
Figure 3 presents the complete learning algorithm.
The algorithm is online, using stochastic gradi-
ent descent updates for both the globally scored
density estimation model and the learned pruning
model. The algorithm assumes a dataset of the form
{(Zi, Si, Gi) : i = 1 . . . n} where each example
scene includes a list of logical expressions Zi, a
world state Si, and a target set of objects, Gi, which
will be identified by the resulting logical expres-
sions. The output is learned parameters for both the
globally scored density estimation model ?, and for
the learned pruning models ?.
Inputs: Training set {(Zi, Si, Gi) : i = 1 . . . n}, where Zi is
a list of logical forms, Si is a world state, and Gi is a target
set of objects. Number of iterations T . Learning rate ?0.
Decay parameter c. Complexity threshold M , as described
in Section 5.3.
Definitions: Let P? (z | Gi, Si; ?,?) be the predicted global
probability from Equation 1. Let P?j(z | Gi, Si;pij) be the
predicted pruning probability from Equation 2. Let A?j be
the set of all complexity-M logical expressions, after prun-
ing (see Section 5.1). Let SUB(j, z) be all complexity-j
sub-expressions of logical expression z. Let Qi(z | Si, Gi)
be the empirical probability over z ? Z , estimated from
Zi. Finally, let ?i(z) be a shorthand for the feature function
?(z, Si, Gi) as defined in Section 5.4.
Algorithm:
Initialize ? ? ~0, pij ? ~0 for j = 1 . . .M
For t = 1 . . . T, i = 1 . . . n:
Step 1: (Update Global Model)
a. Compute the stochastic gradient:
?? ? EQi(z|Si,Gi)[?i(z)]? EP? (z|Gi,Si;?,?)[?i(z)]
b. Update the parameters:
? ? ?01+c?? where ? = i+ t? n
? ? ? + ???
Step 2: (Update Pruning Model)
For j = 1 . . .M
a. Construct a set of positive and negative examples:
D+ ?
?
z?Zi
SUB(j, z).
D? ? A?j \ D+
b. Compute mini-batch stochastic gradient, normalizing
for data skew:
?pij ?
1
|D+|
?
z?D+(1? Pj(z | Si, Gi;pij))?i(z)
? 1
|D?|
?
z?D? Pj(z | Si, G;pij)?i(z)
c. Update complexity-j pruning parameters:
pij ? pij + ??pij
Output: ? and ? = [pi1, . . . , piM ]
Figure 3: The learning algorithm.
6.1 Global Model Updates
The parameters ? of the globally scored density es-
timation model are trained to maximize the log-
likelihood of the data:
Oi = log
?
z?Zi
PG(z | Si, Gi) (3)
Taking the derivative of this objective with re-
spect to ? yields the gradient in Step 1a of Fig-
ure 3. The marginals, EP? (z|Gi,Si;?,?)(?i(z)), are
computed over the approximate finite subset of Z
constructed with the inference procedure described
in Section 5.3.
1920
6.2 Pruning Model Updates
To update each of the M pruning models, we first
construct a set of positive and negative examples
(Step 2a). The positive examples, D+, include those
sub-expressions which should be in the beam - these
are all complexity j sub-expressions of logical ex-
pressions in Zi. The negative examples, D?, in-
clude all complexity-j expressions constructed dur-
ing beam search, minus those which are in D+. The
gradient (Set 2b) is a binary mini-batch gradient,
normalized to correct for data skew.
7 Experimental Setup
Data Collection Our dataset consists of 118 im-
ages, taken with a Microsoft Kinect camera. These
are the same images used by Matuszek et al
(2012a), but we create multiple prompts for each im-
age by circling different objects, giving 269 scenes
in total. These scenes were shown to workers on
Amazon Mechanical Turk4 who were asked to imag-
ine giving instructions to a robot and complete the
sentence ?Please pick up ? in reference to the
circled objects. Twenty referring expressions were
collected for each scene, a total of 5380 expressions.
From this data, 43 scenes (860 expressions) were
held-out for use in a test set. Of the remaining
scenes, the sentences of 30 were labeled with log-
ical forms. 10 of these scenes (200 expressions) are
used as a labeled initialization set, and 20 are used
as a development test set (400 expressions). A small
number of expressions (?5%) from the labeled ini-
tial set were discarded, either because they did not
correctly name the target set, or because they used
very rare attributes (such as texture, or location) to
name the target objects.
Surrogate Labeling To avoid hand labeling the
large majority of the scenes, we label the data
with a learned semantic parser (Zettlemoyer and
Collins, 2005). We created a hand-made lexicon
for the entire training set, which greatly simplifies
the learning problem, and learned the parameters
of the parsing moder on the 10-scene initialization
set. The weights were then further tuned using
semi-supervised techniques (Artzi and Zettlemoyer,
2011, 2013b) on the data to be labeled. Testing on
4http://www.mturk.com
the development set shows that this parser achieves
roughly 95% precision and 70% recall.
Using this parser, we label the sentences in our
training set. We only use scenes where at least 15
sentences were successfully parsed. This gives a
training set of 141 scenes (2587 expressions). Com-
bining the automatically labeled training set with the
hand-labelled initialization, development and held-
out data, our labelled corpus totals 3938 labeled ex-
pressions. By contrast, the popular TUNA furniture
sub corpus (Gatt et al, 2007) contains 856 descrip-
tions of 20 scenes, and although some of these refer
to sets, these sets contain two objects at most.
Framework Our experiments were implemented
using the University of Washington Semantic Pars-
ing Framework (Artzi and Zettlemoyer, 2013a).
Hyperparameters Our inference procedure re-
quires two hyperparameters: M , the maximum com-
plexity threshold, and k, the beam size. In practice,
we set these to the highest possible values which
still allow for training to complete in a reasonable
amount of time (under 12 hours). M is set to 20,
which is sufficient to cover 99.5% of the observed
expressions. The beam-size k is 100 for the first
three complexity levels, and 50 thereafter.
For learning, we use the following hyperparam-
eters, which were tuned on the development set:
learning rate ?0 = .25, decay rate c = .02, num-
ber of epochs T = 10.
Evaluation Metrics Evaluation metrics used in
REG research have assumed a system that produces
a single output. Our goal is to achieve a distribution
over logical forms that closely matches the distribu-
tion observed from human subjects. Therefore, we
compare our learned model to the labeled test data
with mean absolute error:
MAE =
1
2n
n?
i=1
?
z?Z
|P (z | Si, Gi)?Q(z | Si, Gi)|
where Q is the empirical distribution observed in the
training data. MAE measures the total probability
mass which is assigned differently in the predicted
distribution than in the empirical distribution. We
use MAE as opposed to KL divergence or data like-
lihood as both of these measures are uninformative
when the support of the two distributions differ.
1921
? MAE %dup %uniq Top1
VOA 39.7 98.2 92.5 72.7
GenX
25.8 100 100 72.7
(5.0) (0) (0) (0)
Table 1: Single object referring expression gener-
ation results. Our approach (GenX) is compared
to the approach from Mitchell et al (2013) (VOA).
Standard deviation over five shuffles of training set
is reported in parentheses.
This metric is quite strict; small differences in the
estimated probabilities over a large number of logi-
cal expressions can result in a large error, even if the
relative ordering is quite similar. Therefore, we re-
port the percentage of observed logical expressions
which the model produces, either giving credit mul-
tiple times for duplicates (%dup) or counting each
unique logical expression in a scene once (%uniq).
Put another way, %dup counts logical expression to-
kens, whereas %uniq counts types. We also report
the proportion of scenes where the most likely log-
ical expression according to the model matched the
most common one in the data (Top1).
Single Object Baseline In order to compare our
method against the state of the art for generating
referring expressions for single objects, we use the
subset of our corpus where the target set is a sin-
gle object. This sub-corpus consists of 44 scenes for
training and 11 held out for testing.
For comparison we re-implemented the proba-
bilistic Visual Objects Algorithm (VOA) of Mitchell
et al (2013). We refer the readers to the original
paper for details of the approach. The parameters
of the model were tuned on the training data: the
prior likelihood estimates for each of the four at-
tribute types (?att) were estimated as the relative
frequency of each attribute in the data. We pick the
ordering of attributes and the length penalty, ?, from
the cross-product of all possible 4! orderings and all
integers on the range of [1,10], choosing the setting
which results in the lowest average absolute error
(AAE) on the training set. This process resulted
in the following parameter settings: ?color = .916,
?shape = .586, ?type = .094, ?object = .506, AP
ordering = [type, shape, object, color], ? = 4. In-
ference was done using 10,000 samples per scene.
? MAE %dup %uniq Top1
Full GenX
54.3 87.4 72.9 52.6
(4.5) (0.6) (1.1) (8.3)
NoPrune
71.8 42.2 16.1 40.0
(2.5) (2.7) (1.7) (5.0)
NoCOV
87.0 26.0 11.2 14.9
(6.7) (3.7) (2.1) (9.7)
NoSTRUC
60.2 79.6 61.9 44.6
(1.7) (0.3) (0.5) (4.5)
HeadExpOnly
88.8 21.9 9.3 14.0
(6.4) (8.6) (3.5) (7.9)
Table 2: Results on the complete corpus for the
complete system (Full GenX), ablating the pruning
model (NoPrune) and the different features: without
coverage features (NoCOV), without structure fea-
tures (NoSTRUC) and using only the logical expres-
sion HeadExp features (HeadExpOnly). Standard
deviation over five runs is shown in parentheses.
8 Results
We report results on both the single-object subset of
our data and the full dataset. Since our approach is
online, and therefore sensitive to data ordering, we
average results over 5 different runs with randomly
ordered data, and report variance.
Single Objects Table 1 shows the different metrics
for generating referring expression for single objects
only. Our approach outperforms VOA (Mitchell
et al, 2013) on all metrics, including an average of
approximately 35% relative reduction in MAE. In
addition, unlike VOA, our system (GenX) produces
every logical expression used to refer to single ob-
jects in our dataset, including a small number which
use negation and equality.
Object Sets Table 2 lists results on the full dataset.
Our learned pruning approach produces an average
72.9% of the unique logical expressions used present
in our dataset ? over 87% when these counts are
weighted by their frequency. The globally scored
model achieves a mean absolute error of 54.3, and
correctly assigns the highest probability to the most
likely expression over 52% of the time.
Also shown in Table 2 are results obtained when
elements of our approach are ablated. Using the
global model for pruning instead of an explicitly
trained model causes a large drop in performance,
demonstrating that our global model is inappropri-
1922
Q P? z
.750 .320 ?(?x.object(x) ? (yellow(x) ? red(x)))
.114 ?(?x.lego(x)) ? ?(?x.red(x) ? apple(x))
.114 ?(?x.yellow(x) ? lego(x))) ? ?(?x.apple(x))
.044 ?(?x.lego(x) ? (red(x) ? apple(x)))
.044 ?(?x.(yellow(x) ? lego(x)) ? apple(x))
.036 ?(?x.lego(x)) ? ?(?x.red(x) ? sphere(x))
.026 ?(?x.red(x) ? lego(x)) ? ?(?x.red(x) ? sphere(x))
.050 .021 ?(?x.(lego(x) ? yellow(x)) ? (red(x) ? apple(x)))
.017 ?(?x.(lego(x) ? yellow(x)) ? (red(x) ? sphere(x)))
.014 ?(?x.yellow(x) ? lego(x)) ? ?(?x.red(x) ? sphere(x))
.100 .010 ?(?x.yellow(x) ? object(x)) ? ?(?x.apple(x))
.050 .007 ?(?x.yellow(x) ? object(x)) ? ?(?x.red(x) ? sphere(x))
.050 .005 ?(?x.yellow(x) ? object(x)) ? ?(?x.red(x) ? object(x))
(a) (b)
Figure 4: Example output of our system for the scene on the right. We show the top 10 expressions (z) from
the predicted distribution (P? ) compared to the empirical distribution estimated from our labeled data (Q).
The bottom section shows the predicted probability of the three expressions which were not in the top 10 of
the predicted distribution. Although the mean absolute error (MAE) of P? and Q is 63.8, P? covers all of the
entries in Q in the correct relative order and also fills in many other plausible candidates.
ate for pruning. We also ablate subsets of our fea-
tures, demonstrating that the coverage and structural
features are both crucial for performance.
Qualitatively, we found the learned distributions
were often higher quality than the seemingly high
mean absolute error would imply. Figure 4 shows
an example output where the absolute error of the
predicted distribution was 63.8. Much of the error
can be attributed to probability mass assigned to log-
ical expressions which, although not observed in our
test data, are reasonable referring expressions. This
might be due to the fact that our estimate of the em-
pirical distribution comes from a fairly small sample
(20), or other factors which we do not model that
make these expressions less likely.
9 Conclusion
In this paper, we modeled REG as a density-
estimation problem. We demonstrated that we can
learn to produce distributions over logical referring
expressions using a globally normalized model. Key
to the approach was the use of a learned pruning
model to define the space of logical expression that
are explicitly enumerated during inference. Exper-
iments demonstrate state-of-the-art performance on
single object reference and the first results for learn-
ing to name sets of objects, correctly recovering over
87% of the observed logical forms.
This approach suggests several directions for fu-
ture work. Lambda-calculus meaning represen-
tations can be designed for many semantic phe-
nomena, such as spatial relations, superlatives, and
graded properties, that are not common in our data.
Collecting new datasets would allow us to study the
extent to which the approach would scale to domains
with such phenomena.
Although the focus of this paper is on REG, the
approach is also applicable to learning distributions
over logical meaning representations for many other
tasks. Such learned models could provide a range
of possible inputs for systems that map logical ex-
pressions to sentences (White and Rajkumar, 2009;
Lu and Ng, 2011), and could also provide a valuable
prior on the logical forms constructed by semantic
parsers in grounded settings (Artzi and Zettlemoyer,
2013b; Matuszek et al, 2012a).
Acknowledgements
This research was supported in part by the In-
tel Science and Technology Center for Pervasive
Computing, by DARPA under the DEFT program
through the AFRL (FA8750-13-2-0019) and the
CSSG (N11AP20020), the ARO (W911NF-12-1-
0197), and the NSF (IIS-1115966). The authors
wish to thank Margaret Mitchell, Mark Yatskar,
Anthony Fader, Kenton Lee, Eunsol Choi, Gabriel
Schubiner, Leila Zilles, Adrienne Wang, and the
anonymous reviewers for their helpful comments.
1923
References
Areces, C., Koller, A., and Striegnitz, K. (2008).
Referring expressions as formulas of description
logic. In Proceedings of the International Natu-
ral Language Generation Conference.
Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping
semantic parsers from conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Artzi, Y. and Zettlemoyer, L. (2013a). UW SPF:
The University of Washington Semantic Parsing
Framework.
Artzi, Y. and Zettlemoyer, L. (2013b). Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the As-
sociation for Computational Linguistics, 1(1):49?
62.
Barzilay, R. and Lapata, M. (2005). Collective
content selection for concept-to-text generation.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Berg, A. C., Berg, T. L., Daume, H., Dodge, J.,
Goyal, A., Han, X., Mensch, A., Mitchell, M.,
Sood, A., Stratos, K., et al (2012). Under-
standing and predicting importance in images. In
IEEE Conference on Computer Vision and Pattern
Recognition.
Carenini, G., Ng, R. T., and Pauls, A. (2006). Multi-
document summarization of evaluative text. In
Proceedings of the Conference of the European
Chapter of the Association for Computational
Linguistics.
Carpenter, B. (1997). Type-Logical Semantics. The
MIT Press.
Chen, D., Kim, J., and Mooney, R. (2010). Train-
ing a multilingual sportscaster: using perceptual
context to learn language. Journal of Artificial In-
telligence Research, 37(1):397?436.
Chen, D. and Mooney, R. (2011). Learning to inter-
pret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Dale, R. and Reiter, E. (1995). Computational in-
terpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19:233?264.
Dale, R. and Reiter, E. (2000). Building natural lan-
guage generation systems. Cambridge University
Press.
Gardent, C. (2002). Generating minimal definite de-
scriptions. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.
Gatt, A. and van Deemter, K. (2007). Incremental
generation of plural descriptions: Similarity and
partitioning. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Gatt, A., Van Der Sluis, I., and Van Deemter, K.
(2007). Evaluating algorithms for the generation
of referring expressions using a balanced corpus.
In Proceedings of the European Workshop on Nat-
ural Language Generation.
Grice, H. P. (1975). Logic and conversation. 1975,
pages 41?58.
Horacek, H. (2004). On referring to sets of objects
naturally. In Natural Language Generation, pages
70?79. Springer.
Kim, J. and Mooney, R. J. (2012). Unsupervised
PCFG induction for grounded language learning
with highly ambiguous supervision. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Konstas, I. and Lapata, M. (2012). Unsupervised
concept-to-text generation with hypergraphs. In
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics.
Krahmer, E. and van Deemter, K. (2012). Computa-
tional generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Krishnamurthy, J. and Kollar, T. (2013). Jointly
learning to parse and perceive: Connecting nat-
ural language to the physical world. Transactions
of the Association for Computational Linguistics,
1(2):193?206.
Liang, P., Jordan, M., and Klein, D. (2009). Learn-
ing semantic correspondences with less supervi-
sion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
1924
Lu, W. and Ng, H. T. (2011). A probabilis-
tic forest-to-string model for language generation
from typed lambda calculus expressions. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,
L., and Fox, D. (2012a). A joint model of lan-
guage and perception for grounded attribute learn-
ing. Proceedings of the International Conference
on Machine Learning.
Matuszek, C., Herbst, E., Zettlemoyer, L. S., and
Fox, D. (2012b). Learning to parse natural lan-
guage commands to a robot control system. In
Proceedings of the International Symposium on
Experimental Robotics.
Mitchell, M., van Deemter, K., and Reiter, E.
(2011a). Applying machine learning to the choice
of size modifiers. In Proceedings of the PRE-
CogSci Workshop.
Mitchell, M., Van Deemter, K., and Reiter, E.
(2011b). Two approaches for generating size
modifiers. In Proceedings of the European Work-
shop on Natural Language Generation.
Mitchell, M., van Deemter, K., and Reiter, E. (2013).
Generating expressions that refer to visible ob-
jects. In Proceedings of Conference of the North
American Chapter of the Association for Compu-
tational Linguistics.
Ren, Y., Van Deemter, K., and Pan, J. Z. (2010).
Charting the potential of description logic for the
generation of referring expressions. In Proceed-
ings of the International Natural Language Gen-
eration Conference.
Scontras, G., Graff, P., and Goodman, N. D. (2012).
Comparing pluralities. Cognition, 123(1):190?
197.
Steedman, M. (2011). Taking Scope. The MIT Press.
Stone, M. (2000). On identifying sets. In Proceed-
ings of the International Conference on Natural
Language Generation.
van Deemter, K. (2002). Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28:37?52.
van Deemter, K., Gatt, A., Sluis, I. v. d., and Power,
R. (2012a). Generation of referring expressions:
Assessing the incremental algorithm. Cognitive
Science, 36(5):799?836.
van Deemter, K., Gatt, A., van Gompel, R. P., and
Krahmer, E. (2012b). Toward a computational
psycholinguistics of reference production. Topics
in Cognitive Science, 4(2):166?183.
Viethen, J. and Dale, R. (2010). Speaker-dependent
variation in content selection for referring ex-
pression generation. In Proceedings of the Aus-
tralasian Language Technology Workshop.
Viethen, J., Mitchell, M., and Krahmer, E. (2013).
Graphs and spatial relations in the generation of
referring expressions. In Proceedings of the Eu-
ropean Workshop on Natural Language Genera-
tion.
White, M. and Rajkumar, R. (2009). Perceptron
reranking for ccg realization. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Winograd, T. (1972). Understanding natural lan-
guage. Cognitive Psychology, 3(1):1?191.
Zelle, J. and Mooney, R. (1996). Learning to parse
database queries using inductive logic program-
ming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Zitnick, C. L. and Parikh, D. (2013). Bringing se-
mantics into focus using visual abstraction. In
IEEE Conference on Computer Vision and Pattern
Recognition.
1925
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1273?1283,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Compact Lexicons for CCG Semantic Parsing
Yoav Artzi
?
Computer Science & Engineering
University of Washington
Seattle, WA 98195
yoav@cs.washington.edu
Dipanjan Das Slav Petrov
Google Inc.
76 9th Avenue
New York, NY 10011
{dipanjand,slav}@google.com
Abstract
We present methods to control the lexicon
size when learning a Combinatory Cate-
gorial Grammar semantic parser. Existing
methods incrementally expand the lexicon
by greedily adding entries, considering a
single training datapoint at a time. We pro-
pose using corpus-level statistics for lexi-
con learning decisions. We introduce vot-
ing to globally consider adding entries to
the lexicon, and pruning to remove entries
no longer required to explain the training
data. Our methods result in state-of-the-art
performance on the task of executing se-
quences of natural language instructions,
achieving up to 25% error reduction, with
lexicons that are up to 70% smaller and are
qualitatively less noisy.
1 Introduction
Combinatory Categorial Grammar (Steedman,
1996, 2000, CCG, henceforth) is a commonly
used formalism for semantic parsing ? the task
of mapping natural language sentences to for-
mal meaning representations (Zelle and Mooney,
1996). Recently, CCG semantic parsers have been
used for numerous language understanding tasks,
including querying databases (Zettlemoyer and
Collins, 2005), referring to physical objects (Ma-
tuszek et al., 2012), information extraction (Kr-
ishnamurthy and Mitchell, 2012), executing in-
structions (Artzi and Zettlemoyer, 2013b), gen-
erating regular expressions (Kushman and Barzi-
lay, 2013), question-answering (Cai and Yates,
2013) and textual entailment (Lewis and Steed-
man, 2013). In CCG, a lexicon is used to map
words to formal representations of their meaning,
which are then combined using bottom-up opera-
tions. In this paper we present learning techniques
?
This research was carried out at Google.
chair ` N : ?x.chair(x)
chair ` N : ?x.sofa(x)
chair ` AP : ?a.len(a, 3)
chair ` NP : A(?x.corner(x))
chair ` ADJ : ?x.hall(x)
Figure 1: Lexical entries for the word chair as learned
with no corpus-level statistics. Our approach is able to
correctly learn only the top two bolded entries.
to explicitly control the size of the CCG lexicon,
and show that this results in improved task perfor-
mance and more compact models.
In most approaches for inducing CCGs for se-
mantic parsing, lexicon learning and parameter es-
timation are performed jointly in an online algo-
rithm, as introduced by Zettlemoyer and Collins
(2007). To induce the lexicon, words extracted
from the training data are paired with CCG cat-
egories one sample at a time (for an overview of
CCG, see ?2). Joint approaches have the potential
advantage that only entries participating in suc-
cessful parses are added to the lexicon. However,
new entries are added greedily and these decisions
are never revisited at later stages. In practice, this
often results in a large and noisy lexicon.
Figure 1 lists a sample of CCG lexical entries
learned for the word chair with a greedy joint al-
gorithm (Artzi and Zettlemoyer, 2013b). In the
studied navigation domain, the word chair is often
used to refer to chairs and sofas, as captured by the
first two entries. However, the system also learns
several spurious meanings: the third shows an er-
roneous usage of chair as an adverbial phrase de-
scribing action length, while the fourth treats it as
a noun phrase and the fifth as an adjective. In con-
trast, our approach is able to correctly learn only
the top two lexical entries.
We present a batch algorithm focused on con-
trolling the size of the lexicon when learning CCG
semantic parsers (?3). Because we make updates
only after processing the entire training set, we
1273
can take corpus-wide statistics into account be-
fore each lexicon update. To explicitly control
the size of the lexicon, we adopt two complemen-
tary strategies: voting and pruning. First, we con-
sider the lexical evidence each sample provides as
a vote towards potential entries. We describe two
voting strategies for deciding which entries to add
to the model lexicon (?4). Second, even though
we use voting to only conservatively add new lex-
icon entries, we also prune existing entries if they
are no longer necessary for parsing the training
data. These steps are incorporated into the learn-
ing framework, allowing us to apply stricter crite-
ria for lexicon expansion while maintaining a sin-
gle learning algorithm.
We evaluate our approach on the robot navi-
gation semantic parsing task (Chen and Mooney,
2011; Artzi and Zettlemoyer, 2013b). Our exper-
imental results show that we outperform previous
state of the art on executing sequences of instruc-
tions, while learning significantly more compact
lexicons (?6 and Table 3).
2 Task and Inference
To present our lexicon learning techniques, we
focus on the task of executing natural language
navigation instructions (Chen and Mooney, 2011).
This domain captures some of the fundamental
difficulties in recent semantic parsing problems.
In particular, it requires learning from weakly-
supervised data, rather than data annotated with
full logical forms, and parsing sentences in a
situated environment. Additionally, successful
task completion requires interpreting and execut-
ing multiple instructions in sequence, requiring
accurate models to avoid cascading errors. Al-
though this overview centers around the aforemen-
tioned task, our methods are generalizable to any
semantic parsing approach that relies on CCG.
We approach the navigation task as a situated
semantic parsing problem, where the meaning of
instructions is represented with lambda calculus
expressions, which are then deterministically ex-
ecuted. Both the mapping of instructions to logi-
cal forms and their execution consider the current
state of the world. This problem was recently ad-
dressed by Artzi and Zettlemoyer (2013b) and our
experimental setup mirrors theirs. In this section,
we provide a brief background on CCG and de-
scribe the task and our inference method.
walk forward twice
S/NP NP AP
?x.?a.move(a) ? direction(a, x) forward ?a.len(a, 2)
>
S S\S
?a.move(a) ? direction(a, forward) ?f.?a.f(a) ? len(a, 2)
<
S
?a.move(a) ? direction(a, forward) ? len(a, 2)
in the red hallway
PP/NP NP/N ADJ N
?x.?y.intersect(y, x) ?f.?(f) ?x.brick(x) ?x.hall(x)
N/N
?f.?x.f(x)?
brick(x)
<
N
?x.hall(x) ? brick(x)
>
NP
?(?x.hall(x) ? brick(x)
>
PP
?y.intersect(y, ?(?x.hall(x) ? brick(x)))
Figure 2: Two CCG parses. The top shows a complete
parse with an adverbial phrase (AP ), including unary
type shifting and forward (>) and backward (<) ap-
plication. The bottom fragment shows a prepositional
phrase (PP ) with an adjective (ADJ).
2.1 Combinatory Categorial Grammar
CCG is a linguistically-motivated categorial for-
malism for modeling a wide range of language
phenomena (Steedman, 1996; Steedman, 2000).
In CCG, parse tree nodes are categories, which are
assigned to strings (single words or n-grams) and
combined to create a complete derivation. For ex-
ample, S/NP : ?x.?a.move(a)? direction(a, x)
is a CCG category describing an imperative verb
phrase. The syntactic type S/NP indicates the
category is expecting an argument of type NP
on its right, and the returned category will have
the syntax S. The directionality is indicated by
the forward slash /, where a backward slash \
would specify the argument is expected on the left.
The logical form in the category represents its se-
mantic meaning. For example, ?x.?a.move(a) ?
direction(a, x) in the category above is a function
expecting an argument, the variable x, and return-
ing a function from events to truth-values, the se-
mantic representation of imperatives. In this do-
main, the conjunction in the logical form specifies
conditions on events. Specifically, the event must
be a move event and have a specified direction.
A CCG is defined by a lexicon and a set of com-
binators. The lexicon provides a mapping from
strings to categories. Figure 2 shows two CCG
parses in the navigation domain. Parse trees are
read top to bottom. Parsing starts by matching cat-
egories to strings in the sentence using the lexicon.
For example, the lexical entry walk ` S/NP :
?x.?a.move(a) ? direction(a, x) pairs the string
walk with the example category above. Each in-
termediate parse node is constructed by applying
1274
one of a small set of binary CCG combinators or
unary operators. For example, in Figure 2 the cat-
egory of the span walk forward is combined with
the category of twice using backward application
(<). Parsing concludes with a logical form that
captures the meaning of the complete sentence.
We adopt a factored representation for CCG
lexicons (Kwiatkowski et al., 2011), where
entries are dynamically generated by combining
lexemes and templates. A lexeme is a pair
that consists of a natural language string and
a set of logical constants, while the template
contains the syntactic and semantic components
of a CCG category, abstracting over logical
constants. For example, consider the lexical entry
walk ` S/NP : ?x.?a.move(a) ? direction(a, x).
Under the factored representation, this entry
can be constructed by combining the lexeme
?walk, {move,direction}? and the template
?v
1
.?v
2
.[S/NP : ?x.?a.v
1
(a) ? v
2
(a, x)]. This
representation allows for better generalization
over unseen lexical entries at inference time,
allowing for pairings of templates and lexemes
not seen during training.
2.2 Situated Log-Linear CCGs
We use a CCG to parse sentences to logical forms,
which are then executed. Let S be a set of states,
X be the set of all possible sentences, and E be
the space of executions, which are S ? S func-
tions. For example, in the navigation task from
Artzi and Zettlemoyer (2013b), S is a set of po-
sitions on a map, as illustrated in Figure 3. The
map includes an agent that can perform four ac-
tions: LEFT, RIGHT, MOVE, and NULL. An execu-
tion e is a sequence of actions taken consecutively.
Given a state s ? S and a sentence x ? X , we aim
to find the execution e ? E described in x. Let Y
be the space of CCG parse trees and Z the space
of all possible logical forms. Given a sentence x
we generate a CCG parse y ? Y , which includes a
logical form z ? Z . An execution e is then gener-
ated from z using a deterministic process.
Parsing with a CCG requires choosing appro-
priate lexical entries from an often ambiguous lex-
icon and the order in which operations are ap-
plied. In a situated scenario such choices must
account for the current state of the world. In gen-
eral, given a CCG, there are many parses for each
sentence-state pair. To discriminate between com-
peting parses, we use a situated log-linear CCG,
facing the chair in the intersection move forward twice
?a.pre(a, front(you, ?(?x.chair(x)?
intersect(x, ?(?y.intersection(y))))))?
move(a) ? len(a, 2)
?FORWARD, FORWARD?
turn left
?a.turn(a) ? direction(a, left)
?LEFT?
go to the end of the hall
?x.move(a) ? to(a, ?(?x.end(x, ?(?y.hall(y)))))
?FORWARD, FORWARD?
Figure 3: Fragment of a map and instructions for the
navigation domain. The fragment includes two inter-
secting hallways (red and blue), two chairs and an agent
facing left (green pentagon), which follows instructions
such as these listed below. Each instruction is paired
with a logical form representing its meaning and its ex-
ecution in the map.
inspired by Clark and Curran (2007).
Let GEN(x, s; ?) ? Y be the set of all possi-
ble CCG parses given the sentence x, the current
state s and the lexicon ?. In GEN(x, s; ?), multi-
ple parse trees may have the same logical form;
let Y(z) ? GEN(x, s; ?) be the subset of such
parses with the logical form z at the root. Also,
let ? ? R
d
be a d-dimensional parameter vector.
We define the probability of the logical form z as:
p(z|x, s; ?,?) =
?
y?Y(z)
p(y|x, s; ?,?) (1)
Above, we marginalize out the probabilities of all
parse trees with the same logical form z at the root.
The probability of a parse tree y is defined as:
p(y|x, s; ?,?) =
e
???(x,s,y)
?
y
?
?GEN(x,s;?)
e
???(x,s,y
?
)
(2)
Where ?(x, s, y) ? R
d
is a feature vector. Given
a logical form z, we deterministically map it to an
execution e ? E . At inference time, given a sen-
tence x and state s, we find the best logical form
z
?
(and its corresponding execution) by solving:
z
?
= arg max
z
p(z|x, s; ?,?) (3)
1275
The above arg max operation sums over all trees
y ? Y(z), as described in Equation 1. We use a
CKY chart for this computation. The chart signa-
ture in each span is a CCG category. Since ex-
act inference is prohibitively expensive, we fol-
low previous work and perform bottom-up beam
search, maintaining only the k-best categories for
each span in the chart. The logical form z
?
is taken
from the k-best categories at the root of the chart.
The partition function in Equation 2 is approxi-
mated by summing the inside scores of all cate-
gories at the root. We describe the choices of hy-
perparameters and details of our feature set in ?5.
3 Learning
Learning a CCG semantic parser requires inducing
the entries of the lexicon ? and estimating pars-
ing parameters ?. We describe a batch learning
algorithm (Figure 4), which explicitly attempts to
induce a compact lexicon, while fully explaining
the training data. At training time, we assume ac-
cess to a set of N examples D =
{
d
(i)
}
N
1
, where
each datapoint d
(i)
= ?x
(i)
, s
(i)
, e
(i)
?, consists of
an instruction x
(i)
, the state s
(i)
where the instruc-
tion is issued and its execution demonstration e
(i)
.
In particular, we know the correct execution for
each state and instruction, but we do not know the
correct CCG parse and logical form. We treat the
choices that determine them, including selection
of lexical entries and parsing operators, as latent.
Since there can be many logical forms z ? Z that
yield the same execution e
(i)
, we marginalize over
the logical forms (using Equation 1) when maxi-
mizing the following regularized log-likelihood:
L (?,?,D) = (4)
?
d
(i)
?D
?
z?Z(e
(i)
)
p(z|x
(i)
, s
(i)
; ?,?)?
?
2
???
2
2
WhereZ(e
(i)
) is the set of logical forms that result
in the execution e
(i)
and the hyperparameter ? is
a regularization constant. Due to the large number
of potential combinations,
1
it is impractical to con-
sider the complete set of lexical entries, where all
strings (single words and n-grams) are associated
with all possible CCG categories. Therefore, simi-
lar to prior work, we gradually expand the lexicon
during learning. As a result, the parameter space
1
For the navigation task, given the set of CCG category
templates (see ?2.1) and parameters used there would be be-
tween 7.5-10.2M lexical entries to consider, depending on the
corpus used (?5).
Algorithm 1 Batch algorithm for maximizing L (?,?,D).
See ?3.1 for details.
Input: Training dataset D =
{
d
(i)
}
N
1
, number of learning
iterations T , seed lexicon ?
0
, a regularization constant
?, and a learning rate ?. VOTE is defined in ?4.
Output: Lexicon ? and model parameters ?
1: ?? ?
0
2: for t = 1 to T do
?
Generate lexical entries for all datapoints.
3: for i = 1 to N do
4: ?
(i)
? GENENTRIES(d
(i)
, ?,?)
?
Add corpus-wide voted entries to model lexicon.
5: ?? ? ? VOTE(?, {?
(1)
, . . . , ?
(N)
})
?
Compute gradient and entries to prune.
6: for i = 1 to N do
7: ??
(i)
?
,?
(i)
? ? COMPUTEUPDATE(d
(i)
, ?,?)
?
Prune lexicon.
8: ?? ? \
N?
i=1
?
(i)
?
?
Update model parameters.
9: ? ? ? + ?
N?
i=1
?
(i)
? ??
10: return ? and ?
Algorithm 2 GENENTRIES: Algorithm to generate lexical
entries from one training datapoint. See ?3.2 for details.
Input: Single datapoint d = ?x, s, e?, current model param-
eters ? and lexicon ?.
Output: Datapoint-specific lexicon entries ?.
?
Augment lexicon with sentence-specific entries.
1: ?
+
? ? ? GENLEX(d,?, ?)
?
Get max-scoring parses producing correct execution.
2: y
+
? GENMAX(x, s, e; ?
+
, ?)
?
Extract lexicon entries from max-scoring parses.
3: ??
?
y?y
+
LEX(y)
4: return ?
Algorithm 3 COMPUTEUPDATE: Algorithm to compute the
gradient and the set of lexical entries to prune for one data-
point. See ?3.3 for details.
Input: Single datapoint d = ?x, s, e?, current model param-
eters ? and lexicon ?.
Output: ??
?
,??, lexical entries to prune for d and gradient.
?
Get max-scoring correct parses given ? and ?.
1: y
+
? GENMAX(x, s, e; ?, ?)
?
Create the set of entries to prune.
2: ?
?
? ? \
?
y?y
+
LEX(y)
?
Compute gradient.
3: ?? E(y | x, s, e; ?,?)? E(y | x, s; ?,?)
4: return ??
?
,??
Figure 4: Our learning algorithm and its subroutines.
changes throughout training whenever the lexicon
is modified. The learning problem involves jointly
finding the best set of parameters and lexicon en-
tries. In the remainder of this section, we describe
how we optimize Equation 4, while explicitly con-
trolling the lexicon size.
1276
3.1 Optimization Algorithm
We present a learning algorithm to optimize the
data log-likelihood, where both lexicon learning
and parameter updates are performed in batch, i.e.,
after observing all the training corpus. The batch
formulation enables us to use information from the
entire training set when updating the model lexi-
con. Algorithm 1 presents the outline of our op-
timization procedure. It takes as input a training
dataset D, number of iterations T , seed lexicon
?
0
, learning rate ? and regularization constant ?.
Learning starts with initializing the model lex-
icon ? using ?
0
(line 1). In lines 2-9, we run T
iterations; in each, we make two passes over the
corpus, first to generate lexical entries, and second
to compute gradient updates and lexical entries to
prune. To generate lexical entries (lines 3-4) we
use the subroutine GENENTRIES to independently
generate entries for each datapoint, as described
in ?3.2. Given the entries for each datapoint, we
vote on which to add to the model lexicon. The
subroutine VOTE (line 5) chooses a subset of the
proposed entries using a particular voting strategy
(see ?4). Given the updated lexicon, we process
the corpus a second time (lines 6-7). The sub-
routine COMPUTEUPDATE, as described in ?3.3,
computes the gradient update for each datapoint
d
(i)
, and also generates the set of lexical entries not
included in the max-scoring parses of d
(i)
, which
are candidates for pruning. We prune from the
model lexicon all lexical entries not used in any
correct parse (line 8). During this pruning step, we
ensure that no entries from ?
0
are removed from
?. Finally, the gradient updates are accumulated
to update the model parameters (line 9).
3.2 Lexical Entries Generation
For each datapoint d = ?x, s, e?, the subroutine
GENENTRIES, as described in Algorithm 2, gen-
erates a set of potential entries. The subroutine
uses the function GENLEX, originally proposed
by Zettlemoyer and Collins (2005), to generate
lexical entries from sentences paired with logical
forms. We use the weakly-supervised variant of
Artzi and Zettlemoyer (2013b). Briefly, GENLEX
uses the sentence and expected execution to gen-
erate new lexemes, which are then paired with a
set of templates factored from ?
0
to generate new
lexical entries. For more details, see ?8 of Artzi
and Zettlemoyer (2013b).
Since GENLEX over-generates entries, we need
to determine the set of entries that participate
in max-scoring parses that lead to the correct
execution e. We therefore create a sentence-
specific lexicon ?
+
by taking the union of the
GENLEX-generated entries for the current sen-
tence and the model lexicon (line 1). We define
GENMAX(x, s, e; ?
+
, ?) to be the set of all max-
scoring parses according to the parameters ? that
are in GEN(x, s; ?
+
) and result in the correct ex-
ecution e (line 2). In line 3 we use the function
LEX(y), which returns the lexical entries used in
the parse y, to compute the set of all lexical en-
tries used in these parses. This final set contains
all newly generated entries for this datapoint and
is returned to the optimization algorithm.
3.3 Pruning and Gradient Computation
Algorithm 3 describes the subroutine COMPUTE-
UPDATE that, given a datapoint d, the current
model lexicon ? and model parameters ?, returns
the gradient update and the set of lexical entries
to prune for d. First, similar to GENENTRIES we
compute the set of correct max-scoring parses us-
ing GENMAX (line 1). This time, however, we do
not use a sentence-specific lexicon, but instead use
the model lexicon that has been expanded with all
voted entries. As a result, the set of max-scoring
parses producing the correct execution may be
different compared to GENENTRIES. LEX(y) is
then used to extract the lexical entries from these
parses, and the set difference (?
?
) between the
model lexicon and these entries is set to be pruned
(line 2). Finally, the partial derivative for the data-
point is computed using the difference of two ex-
pected feature vectors, according to two distribu-
tions (line 3): (a) parses conditioned on the correct
execution e, the sentence x, state s and the model,
and (b) all parses not conditioned on the execution
e. The derivatives are approximate due to the use
of beam search, as described in ?2.2.
4 Global Voting for Lexicon Learning
Our goal is to learn compact and accurate CCG
lexicons. To this end, we globally reason about
adding new entries to the lexicon by voting (VOTE,
Algorithm 1, line 5), and remove entries by prun-
ing the ones no longer required for explaining the
training data (Algorithm 1, line 8). In voting, each
datapoint can be considered as attempting to in-
fluence the learning algorithm to update the model
lexicon with the entries required to parse it. In this
1277
Round 1 Round 2 Round 3 Round 4
d
(1)
?chair, {chair}?
?chair, {hatrack}?
?chair, {turn,direction}?
1
/3
1
/3
1
/3
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}? 1 ?chair, {chair}? 1
d
(2)
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}? 1 ?chair, {chair}? 1
d
(3)
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}? 1
d
(4)
?chair, {easel}? 1 ?chair, {easel}? 1 ?chair, {easel}? 1 ?chair, {easel}? 1
Votes
?chair, {chair}?
?chair, {easel}?
?chair, {hatrack}?
?chair, {turn,direction}?
1
1
/3
1
1
/2
5
/6
1
/3
?chair, {chair}?
?chair, {easel}?
?chair, {hatrack}?
1
1
/2
1
1
/2
1
?chair, {chair}?
?chair, {easel}?
2
1
/2
1
1
/2
?chair, {chair}?
?chair, {easel}?
3
1
Discard ?chair, {turn, direction}? ?chair, {hatrack}? ?chair, {easel}?
Figure 5: Four rounds of CONSENSUSVOTE for the string chair for four training datapoints. For each datapoint,
we specify the set of lexemes generated in the Round 1 column, and update this set after each round. At the end,
the highest voted new lexeme according to the final votes is returned. In this example, MAXVOTE and CONSEN-
SUSVOTE lead to different outcomes. MAXVOTE, based on the initial sets only, will select ?chair, {easel}?.
section we describe two alternative voting strate-
gies. Both strategies ensure that new entries are
only added when they have wide support in the
training data, but count this support in different
ways. For reproducibility, we also provide step-
by-step pseudocode for both methods in the sup-
plementary material.
Since we only have access to executions and
treat parse trees as latent, we consider as correct
all parses that produce correct executions. Fre-
quently, however, incorrect parses spuriously lead
to correct executions. Lexical entries extracted
from such spurious parses generalize poorly. The
goal of voting is to eliminate such entries.
Voting is formulated on the factored lexicon
representation, where each lexical entry is factored
into a lexeme and a template, as described in ?2.1.
Each lexeme is a pair containing a natural lan-
guage string and a set of logical constants.
2
A lex-
eme is combined with a template to create a lexical
entry. In our lexicon learning approach only new
lexemes are generated, while the set of templates
is fixed; hence, our voting strategies reason over
lexemes and only create complete lexicon entries
at the end. Decisions are made for each string in-
dependently of all other strings, but considering all
occurrences of that string in the training data.
In lines 3-4 of Algorithm 1 GENENTRIES is
used to propose new lexical entries for each train-
ing datapoint d
(i)
. For each d
(i)
a set ?
(i)
, that
includes all lexical entries participating in parses
that lead to the correct execution, is generated. In
these sets, the same string can appear in multiple
2
Recall, for example, that in one lexeme the string walk
may be paired with the set of constants {move, direction}.
lexemes. To normalize its influence, each data-
point is given a vote of 1.0 for each string, which
is distributed uniformly among all lexemes con-
taining the same string.
For example, a specific ?
(i)
may consist of
the following three lexemes: ?chair, {chair}?,
?chair, {hatrack}?, ?face, {post, front, you}?. In
this set, the phrase chair has two possible mean-
ings, which will therefore each receive a vote of
0.5, while the third lexeme will be given a vote of
1.0. Such ambiguity is common and occurs when
the available supervision is insufficient to discrim-
inate between different parses, for example, if they
lead to identical executions.
Each of the two following strategies reasons
over these votes to globally select the best lex-
emes. To avoid polluting the model lexicon, both
strategies adopt a conservative approach and only
select at most one lexeme for each string in each
training iteration.
4.1 Strategy 1: MAXVOTE
The first strategy for selecting voted lexical entries
is straightforward. For each string it simply aggre-
gates all votes and selects the new lexeme with the
most votes. A lexeme is considered new if it is
not already in the model lexicon. If no such sin-
gle lexeme exists (e.g., no new entries were used
in correctly executing parses or in the case of a tie)
no lexeme is selected in this iteration.
A potential limitation of MAXVOTE is that the
votes for all rejected lexemes are lost. However,
it is often reasonable to re-allocate these votes to
other lexemes. For example, consider the sets of
lexemes for the word chair in the Round 1 col-
1278
umn of Figure 5. Using MAXVOTE on these sets
will select the lexeme ?chair, {easel}?, rather than
the correct lexeme ?chair, {chair}?. This occurs
when the datapoints supporting the correct lexeme
distribute their votes over many spurious lexemes.
4.2 Strategy 2: CONSENSUSVOTE
Our second strategy CONSENSUSVOTE aims to
capture the votes that are lost in MAXVOTE. In-
stead of discarding votes that do not go to the max-
imum scoring lexeme, voting is done in several
rounds. In each round the lowest scoring lexeme
is discarded and votes are re-assigned uniformly
to the remaining lexemes. This procedure is con-
tinued until convergence. Finally, given the sets of
lexemes in the last round, the votes are computed
and the new lexeme with most votes is selected.
Figure 5 shows a complete voting process for
four training datapoints. In each round, votes
are aggregated over the four sets of lexemes, and
the lexeme with the fewest votes is discarded.
For each set of lexemes, the discarded lexeme
is removed, unless it will lead to an empty set.
3
In the example, while ?chair, {easel}? is dis-
carded in Round 3, it remains in the set of d
(4)
.
The process converges in the fourth round, when
there are no more lexemes to discard. The fi-
nal sets include two entries: ?chair, {chair}? and
?chair, {easel}?. By avoiding wasting votes on
lexemes that have no chance of being selected, the
more widely supported lexeme ?chair, {chair}?
receives the most votes, in contrast to Round 1,
where ?chair, {easel}? was the highest voted one.
5 Experimental Setup
To isolate the effect of our lexicon learning tech-
niques we closely follow the experimental setup of
previous work (Artzi and Zettlemoyer, 2013b, ?9)
and use its publicly available code.
4
This includes
the provided beam-search CKY parser, two-pass
parsing for testing, beam search for executing se-
quences of instructions and the same seed lexicon,
weight initialization and features. Finally, except
3
This restriction is meant to ensure that discarding lex-
emes will not change the set of sentences that can be parsed.
In addition, it means that the total amount of votes given to a
string is invariant between rounds. Allowing for empty sets
will change the sum of votes, and therefore decrease the num-
ber of datapoints contributing to the decision.
4
Their implementation, based on the University of Wash-
ington Semantic Parsing Framework (Artzi and Zettlemoyer,
2013a), is available at http://yoavartzi.com/navi.
the optimization parameters specified below, we
use the same parameter settings.
Data For evaluation we use two related cor-
pora: SAIL (Chen and Mooney, 2011) and ORA-
CLE (Artzi and Zettlemoyer, 2013b). Due to how
the original data was collected (MacMahon et al.,
2006), SAIL includes many wrong executions and
about 30% of all instruction sequences are infeasi-
ble (e.g., instructing the agent to walk into a wall).
To better understand system performance and the
effect of noise, ORACLE was created with the
subset of valid instructions from SAIL paired with
their gold executions. Following previous work,
we use a held-out set for the ORACLE corpus and
cross-validation for the SAIL corpus.
Systems We report two baselines. Our batch
baseline uses the same regularized algorithm, but
updates the lexicon by adding all entries without
voting and skips pruning. Additionally, we added
post-hoc pruning to the algorithm of Artzi and
Zettlemoyer (2013b) by discarding all learned en-
tries that are not participating in max-scoring cor-
rect parses at the end of training. For ablation,
we study the influence of the two voting strategies
and pruning, while keeping the same regulariza-
tion setting. Finally, we compare our approach to
previous published results on both corpora.
Optimization Parameters We optimized the
learning parameters using cross validation on the
training data to maximize recall of complete se-
quence execution and minimize lexicon size. We
use 10 training iterations and the learning rate
? = 0.1. For SAIL we set the regularization pa-
rameter ? = 1.0 and for ORACLE ? = 0.5.
Full Sequence Inference To execute sequences
of instructions we use the beam search procedure
of Artzi and Zettlemoyer (2013b) with an identical
beam size of 10. The beam stores states, and is
initialized with the starting state. Instructions are
executed in order, each is attempted from all states
currently in the beam, the beam is then updated
and pruned to keep the 10-best states. At the end,
the best scoring state in the beam is returned.
Evaluation Metrics We evaluate the end-to-end
task of executing complete sequences of instruc-
tions against an oracle final state. In addition, to
better understand the results, we also measure task
completion for single instructions. We repeated
1279
ORACLE corpus cross-validation
Single sentence Sequence Lexicon
P R F1 P R F1 size
Artzi and Zettlemoyer (2013b) 84.59 82.74 83.65 68.35 58.95 63.26 5383
w/ post-hoc pruning 84.32 82.89 83.60 66.83 61.23 63.88 3104
Batch baseline 85.14 81.91 83.52 72.64 60.13 65.76 6323
w/ MAXVOTE 84.04 82.25 83.14 72.79 64.86 68.55 2588
w/ CONSENSUSVOTE 84.51 82.23 83.36 72.99 63.45 67.84 2446
w/ pruning 85.58 83.51 84.53 75.15 65.97 70.19 2791
w/ MAXVOTE + pruning 84.50 82.89 83.69 72.91 66.40 69.47 2186
w/ CONSENSUSVOTE + pruning 85.22 83.00 84.10 75.65 66.15 70.55 2101
Table 1: Ablation study using cross-validation on the ORACLE corpus training data. We report mean precision
(P), recall (R) and harmonic mean (F1) of execution accuracy on single sentences and sequences of instructions
and mean lexicon sizes. Bold numbers represent the best performing method on a given metric.
Final results
Single sentence Sequence Lexicon
P R F1 P R F1 size
SAIL
Chen and Mooney (2011) 54.40 16.18
Chen (2012) 57.28 19.18
+ additional data 57.62 20.64
Kim and Mooney (2012) 57.22 20.17
Kim and Mooney (2013) 62.81 26.57
Artzi and Zettlemoyer (2013b) 67.60 65.28 66.42 38.06 31.93 34.72 10051
Our Approach 66.67 64.36 65.49 41.30 35.44 38.14 2873
ORACLE
Artzi and Zettlemoyer (2013b) 81.17 (0.68) 78.63 (0.84) 79.88 (0.76) 68.07 (2.72) 58.05 (3.12) 62.65 (2.91) 6213 (217)
Our Approach 79.86 (0.50) 77.87 (0.41) 78.85 (0.45) 76.05 (1.79) 68.53 (1.76) 72.10 (1.77) 2365 (57)
Table 2: Our final results compared to previous work on the SAIL and ORACLE corpora. We report mean precision
(P), recall (R), harmonic mean (F1) and lexicon size results and standard deviation between runs (in parenthesis)
when appropriate. Our Approach stands for batch learning with a consensus voting and pruning. Bold numbers
represent the best performing method on a given metric.
each experiment five times and report mean preci-
sion, recall,
5
harmonic mean (F1) and lexicon size.
For held-out test results we also report standard
deviation. For the baseline online experiments we
shuffled the training data between runs.
6 Results
Table 1 shows ablation results for 5-fold cross-
validation on the ORACLE training data. We
evaluate against the online learning algorithm of
Artzi and Zettlemoyer (2013b), an extension of it
to include post-hoc pruning and a batch baseline.
Our best sequence execution development result
is obtained with CONSENSUSVOTE and pruning.
The results provide a few insights. First, sim-
ply switching to batch learning provides mixed re-
sults: precision increases, but recall drops and the
learned lexicon is larger. Second, adding pruning
results in a much smaller lexicon, and, especially
in batch learning, boosts performance. Adding
voting further reduces the lexicon size and pro-
vides additional gains for sequence execution. Fi-
nally, while MAXVOTE and CONSENSUSVOTE
give comparable performance on their own, CON-
SENSUSVOTE results in more precise and compact
5
Recall is identical to accuracy as reported in prior work.
models when combined with pruning.
Table 2 lists our test results. We significantly
outperform previous state of the art on both cor-
pora when evaluating sequence accuracy. In both
scenarios our lexicon is 60-70% smaller. In con-
trast to the development results, single sentence
performance decreases slightly compared to Artzi
and Zettlemoyer (2013b). The discrepancy be-
tween single sentence and sequence results might
be due to the beam search performed when execut-
ing sequences of instructions. Models with more
compact lexicons generate fewer logical forms for
each sentence: we see a decrease of roughly 40%
in our models compared to Artzi and Zettlemoyer
(2013b). This is especially helpful during se-
quence execution, where we use a beam size of
10, resulting in better sequences of executions. In
general, this shows the potential benefit of using
more compact models in scenarios that incorpo-
rate reasoning about parsing uncertainty.
To illustrate the types of errors avoided with
voting and pruning, Table 3 describes common
error classes and shows example lexical entries
for batch trained models with CONSENSUSVOTE
and pruning and without. Quantitatively, the mean
number of entries per string on development folds
1280
String
# lexical entries
Example categoriesBatch With voting
baseline and pruning
The algorithm often treats common bigrams as multiword phrases, and later learns the more general separate entries.
Without pruning the initial entries remain in the lexicon and compete with the correct ones during inference.
octagon carpet 45 0 N : ?x.wall(x) N : ?x.hall(x)
N : ?x.honeycomb(x)
carpet 51 5 N : ?x.hall(x)
N/N : ?f.?x.x == argmin(f, ?y.dist(y))
octagon 21 5 N : ?x.honeycomb(x) N : ?x.cement(x)
ADJ : ?x.honeycomb(x)
We commonly see in the lexicon a long tail of erroneous entries, which compete with correctly learned ones. With voting
and pruning we are often able to avoid such noisy entries. However, some noise still exists, e.g., the entry for ?intersection?.
intersection 45 7 N : ?x.intersection(x) S\N : ?f.intersect(you, (f))
AP : ?a.len(a, 1) N/NP : ?x.?y.intersect(y, x)
twice 46 2 AP : ?a.len(a, 2) AP : ?a.pass(a,A(?x.empty(x)))
AP : ?a.pass(a,A(?x.hall(x)))
stone 31 5 ADJ : ?x.stone(x) ADJ : ?x.brick(x)
ADJ : ?x.honeycomb(x) NP/N : ?f.A(f)
Not all concepts mentioned in the corpus are relevant to the task and some of these are not semantically modeled. However,
the baseline learner doesn?t make this distinction and induces many erroneous entries. With voting the model better handles
such cases, either by pairing such words with semantically empty entries or learning no entries for them. During inference
the system can then easily skip such words.
now 28 0 AP : ?a.len(a, 3) AP : ?a.direction(a, forward)
only 38 0 N/NP : ?x.?y.intersect(y, x)
N/NP : ?x.?y.front(y, x)
here 31 8 NP : you S/S : ?x.x
S\N : ?f.intersect(you,A(f))
Without pruning the learner often over-splits multiword phrases and has no way to reverse such decisions.
coat 25 0 N : ?x.intersection(x) ADJ : ?x.hatrack(x)
rack 45 0 N : ?x.hatrack(x) N : ?x.furniture(x)
coat rack 55 5 N : ?x.hatrack(x) N : ?x.wall(x)
N : ?x.furniture(x)
Voting helps to avoid learning entries for rare words when the learning signal is highly ambiguous.
orange 20 0 N : ?x.cement(x) N : ?x.grass(x)
pics of towers 26 0 N?x.intersection(x) N : ?x.hall(x)
Table 3: Example entries from a learned ORACLE corpus lexicon using batch learning. For each string we
report the number of lexical entries without voting (CONSENSUSVOTE) and pruning and with, and provide a few
examples. Struck entries were successfully avoided when using voting and pruning.
decreases from 16.77 for online training to 8.11.
Finally, the total computational cost of our ap-
proach is roughly equivalent to online approaches.
In both approaches, each pass over the data makes
the same number of inference calls, and in prac-
tice, Artzi and Zettlemoyer (2013b) used 6-8 it-
erations for online learning while we used 10. A
benefit of the batch method is its insensitivity to
data ordering, as expressed by the lower standard
deviation between randomized runs in Table 2.
6
7 Related Work
There has been significant work on learning for se-
mantic parsing. The majority of approaches treat
grammar induction and parameter estimation sep-
arately, e.g. Wong and Mooney (2006), Kate and
Mooney (2006), Clarke et al. (2010), Goldwasser
et al. (2011), Goldwasser and Roth (2011), Liang
6
Results still vary slightly due to multi-threading.
et al. (2011), Chen and Mooney (2011), and Chen
(2012). In all these approaches the grammar struc-
ture is fixed prior to parameter estimation.
Zettlemoyer and Collins (2005) proposed the
learning regime most related to ours. Their learner
alternates between batch lexical induction and on-
line parameter estimation. Our learning algo-
rithm design combines aspects of previously stud-
ied approaches into a batch method, including
gradient updates (Kwiatkowski et al., 2010) and
using weak supervision (Artzi and Zettlemoyer,
2011). In contrast, Artzi and Zettlemoyer (2013b)
use online perceptron-style updates to optimize a
margin-based loss. Our work also focuses on CCG
lexicon induction but differs in the use of corpus-
level statistics through voting and pruning for ex-
plicitly controlling the size of the lexicon.
Our approach is also related to the grammar in-
duction algorithm introduced by Carroll and Char-
1281
niak (1992). Similar to our method, they process
the data using two batch steps: the first proposes
grammar rules, analogous to our step that gener-
ates lexical entries, and the second estimates pars-
ing parameters. Both methods use pruning after
each iteration, to remove unused entries in our ap-
proach, and low probability rules in theirs. How-
ever, while we use global voting to add entries
to the lexicon, they simply introduce all the rules
generated by the first step. Their approach also
relies on using disjoint subsets of the data for the
two steps, while we use the entire corpus.
Using voting to aggregate evidence has been
studied for combining decisions from an ensem-
ble of classifiers (Ho et al., 1994; Van Erp and
Schomaker, 2000). MAXVOTE is related to ap-
proval voting (Brams and Fishburn, 1978), where
voters are required to mark if they approve each
candidate or not. CONSENSUSVOTE combines
ideas from approval voting, Borda counting, and
instant-runoff voting. Van Hasselt (2011) de-
scribed all three systems and applied them to pol-
icy summation in reinforcement learning.
8 Conclusion
We considered the problem of learning for se-
mantic parsing, and presented voting and pruning
methods based on corpus-level statistics for induc-
ing compact CCG lexicons. We incorporated these
techniques into a batch modification of an exist-
ing learning approach for joint lexicon induction
and parameter estimation. Our evaluation demon-
strates that both voting and pruning contribute to-
wards learning a compact lexicon and illustrates
the effect of lexicon quality on task performance.
In the future, we wish to study various aspects
of learning more robust lexicons. For example, in
our current approach, words not appearing in the
training set are treated as unknown and ignored at
inference time. We would like to study the bene-
fit of using large amounts of unlabeled text to al-
low the model to better hypothesize the meaning
of such previously unseen words. Moreover, our
model?s performance is currently sensitive to the
set of seed lexical templates provided. While we
are able to learn the meaning of new words, the
model is unable to correctly handle syntactic and
semantic structures not covered by the seed tem-
plates. To alleviate this problem, we intend to fur-
ther explore learning novel lexical templates.
Acknowledgements
We thank Kuzman Ganchev, Emily Pitler, Luke
Zettlemoyer, Tom Kwiatkowski and Nicholas
FitzGerald for their comments on earlier drafts,
and the anonymous reviewers for their valuable
feedback. We also wish to thank Ryan McDon-
ald and Arturas Rozenas for their valuable input
about voting procedures.
References
Yoav Artzi and Luke S. Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Yoav Artzi and Luke S. Zettlemoyer. 2013a. UW
SPF: The University of Washington Semantic Pars-
ing Framework.
Yoav Artzi and Luke S. Zettlemoyer. 2013b. Weakly
supervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Steven J. Brams and Peter C. Fishburn. 1978. Ap-
proval voting. The American Political Science Re-
view, pages 831?847.
Qingqing Cai and Alexander Yates. 2013. Seman-
tic parsing freebase: Towards open-domain semantic
parsing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Gelnn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Working Notes of the Workshop
Statistically-Based NLP Techniques.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
David L. Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the Conference
on Computational Natural Language Learning.
Dan Goldwasser and Dan Roth. 2011. Learning from
natural instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
1282
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the Association
of Computational Linguistics.
Tin K. Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier
systems. IEEE Transactions on Pattern Analysis
and Machine Intelligence, pages 66?75.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the Conference of the Association for
Computational Linguistics.
Joohyun Kim and Raymond J. Mooney. 2012. Un-
supervised pcfg induction for grounded language
learning with highly ambiguous supervision. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Joohyun Kim and Raymond J. Mooney. 2013. Adapt-
ing discriminative reranking to grounded language
learning. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Hu-
man Language Technology Conference of the North
American Association for Computational Linguis-
tics.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing prob-
abilistic CCG grammars from logical form with
higher-order unification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical Gener-
alization in CCG Grammar Induction for Semantic
Parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1(1):179?192.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Conference of the As-
sociation for Computational Linguistics.
Matt MacMahon, Brian Stankiewics, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, action in route instructions. In Proceed-
ings of the National Conference on Artificial Intelli-
gence.
Cynthia Matuszek, Nicholas FitzGerald, Luke S.
Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In Proceedings of the Interna-
tional Conference on Machine Learning.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Merijn Van Erp and Lambert Schomaker. 2000.
Variants of the borda count method for combining
ranked classifier hypotheses. In In the International
Workshop on Frontiers in Handwriting Recognition.
Hado Van Hasselt. 2011. Insights in Reinforcement
Learning: formal analysis and empirical evaluation
of temporal-difference learning algorithms. Ph.D.
thesis, University of Utrecht.
Yuk W. Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the North American Asso-
ciation for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
1283
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 602?606,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Predicting Responses to Microblog Posts
Yoav Artzi ?
Computer Science & Engineering
University of Washington
Seattle, WA, USA
yoav@cs.washington.edu
Patrick Pantel, Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA, USA
{ppantel,mgamon}@microsoft.com
Abstract
Microblogging networks serve as vehicles for
reaching and influencing users. Predicting
whether a message will elicit a user response
opens the possibility of maximizing the viral-
ity, reach and effectiveness of messages and
ad campaigns on these networks. We propose
a discriminative model for predicting the like-
lihood of a response or a retweet on the Twit-
ter network. The approach uses features de-
rived from various sources, such as the lan-
guage used in the tweet, the user?s social net-
work and history. The feature design process
leverages aggregate statistics over the entire
social network to balance sparsity and infor-
mativeness. We use real-world tweets to train
models and empirically show that they are ca-
pable of generating accurate predictions for a
large number of tweets.
1 Introduction
Microblogging networks are increasingly evolving
into broadcasting networks with strong social as-
pects. The most popular network today, Twitter, re-
ported routing 200 million tweets (status posts) per
day in mid-2011. As the network is increasingly
used as a channel for reaching out and marketing
to its users, content generators aim to maximize the
impact of their messages, an inherently challeng-
ing task. However, unlike for conventionally pro-
duced news, Twitter?s public network allows one to
observe how messages are reaching and influencing
users. One such direct measure of impact are mes-
sage responses.
? This work was conducted at Microsoft Research.
In this work, we describe methods to predict if a
given tweet will elicit a response. Twitter provides
two methods to respond to messages: replies and
retweets (re-posting of a message to one?s follow-
ers). Responses thus serve both as a measure of dis-
tribution and as a way to increase it. Being able to
predict responses is valuable for any content gener-
ator, including advertisers and celebrities, who use
Twitter to increase their exposure and maintain their
brand. Furthermore, this prediction ability can be
used for ranking, allowing the creation of better op-
timized news feeds.
To predict if a tweet will receive a response prior
to its posting we use features of the individual tweet
together with features aggregated over the entire so-
cial network. These features, in combination with
historical activity, are used to train a prediction
model.
2 Related Work
The public nature of Twitter and the unique char-
acteristics of its content have made it an attractive
research topic over recent years. Related work can
be divided into several types:
Twitter Demographics One of the most fertile av-
enues of research is modeling users and their inter-
actions on Twitter. An extensive line of work char-
acterizes users (Pear Analytics, 2009) and quantifies
user influence (Cha et al, 2010; Romero et al, 2011;
Wu et al, 2011; Bakshy et al, 2011). Popescu and
Jain (2011) explored how businesses use Twitter to
connect with their customer base. Popescu and Pen-
nacchiotti (2011) and Qu et al (2011) investigated
602
how users react to events on social media. There
also has been extensive work on modeling conver-
sational interactions on Twitter (Honeycutt and Her-
ring, 2009; Boyd et al, 2010; Ritter et al, 2010;
Danescu-Niculescu-Mizil et al, 2011). Our work
builds on these findings to predict response behavior
on a large scale.
Mining Twitter Social media has been used to de-
tect events (Sakaki et al, 2010; Popescu and Pennac-
chiotti, 2010; Popescu et al, 2011), and even predict
their outcomes (Asur and Huberman, 2010; Culotta,
2010). Similarly to this line of work, we mine the
social network for event prediction. In contrast, our
focus is on predicting events within the network.
Response Prediction There has been significant
work addressing the task of response prediction in
news articles (Tsagkias et al, 2009; Tsagkias et al,
2010) and blogs (Yano et al, 2009; Yano and Smith,
2010; Balasubramanyan et al, 2011). The task of
predicting responses in social networks has been in-
vestigated previously: Hong et al (2011) focused
on predicting responses for highly popular items,
Rowe et al (2011) targeted the prediction of con-
versations and their length and Suh et al (2010) pre-
dicted retweets. In contrast, our work targets tweets
regardless of their popularity and attempts to predict
both replies and retweets. Furthermore, we present
a scalable method to use linguistic lexical features in
discriminative models by leveraging global network
statistics. A related task to ours is that of response
generation, as explored by Ritter et al (2011). Our
work complements their approach by allowing to
detect when the generation of a response is appro-
priate. Lastly, the task of predicting the spread of
hashtags in microblogging networks (Tsur and Rap-
poport, 2012) is also closely related to our work and
both approaches supplement each other as measures
of impact.
Ranking in News Feeds Different approaches
were suggested for ranking items in social media
(Das Sarma et al, 2010; Lakkaraju et al, 2011). Our
work provides an important signal, which can be in-
corporated into any ranking approach.
3 Response Prediction on Twitter
Our goal is to learn a function f that maps a tweet
x to a binary value y ? {0, 1}, where y indicates if
x will receive a response. In this work we make no
distinction between different kinds of responses.
In addition to x, we assume access to a social net-
work S, which we view as a directed graph ?U,E?.
The set of vertices U represents the set of users. For
each u?, u?? ? U , ?u?, u??? ? E if and only if there
exists a following relationship from u? to u??.
For the purpose of defining features we denote xt
as the text of the tweet x and xu ? U the user who
posted x. For training we assume access to a set of
n labeled examples {?xi, yi? : i = 1 . . . n}, where
the label indicates whether the tweet has received a
response or not.
3.1 Features
For prediction we represent a given tweet x using six
feature families:
Historical Features Historical behavior is often
strong evidence of future trends. To account for this
information, we compute the following features: ra-
tio of tweets by xu that received a reply, ratio of
tweets by xu that were retweeted and ratio of tweets
by xu that received both a reply and retweet.
Social Features The immediate audience of a user
xu is his followers. Therefore, incorporating social
features into our model is likely to contribute to its
prediction ability. For a user xu ? U we include
features for the number of followers (indegree in S),
the number of users xu follows (outdegree in S) and
the ratio between the two.
Aggregate Lexical Features To detect lexical
items that trigger certain response behavior we de-
fine features for all bigrams and hashtags in our set
of tweets. To avoid sparsity and maintain a manage-
able feature space we compress the features using
the labels: for each lexical item l we define Rl to
be the set of tweets that include l and received a re-
sponse, and Nl to be the set of tweets that contain l
and received no response. We then define the inte-
ger n to be the rounding of |Rl||Nl| to the nearest integer.
For each such integer we define a feature, which we
increase by 1 when the lexical item l is present in xt.
603
We use this process separately for bigrams and hash-
tags, creating separate sets of aggregate features.
Local Content Features We introduce 45 features
to capture how the content of xt influences response
behavior, including features such as the number of
stop words and the percentage of English words. In
addition we include features specific to Twitter, such
as the number of hash tags and user references.
Posting Features Past analysis of Twitter showed
that posting time influences response potential (Pear
Analytics, 2009). To examine temporal influences,
we include features to account for the user?s local
time and day of the week when x was created.
Sentiment Features To measure how sentiment
influences response behavior we define features that
count the number of positive and negative sentiment
words in xt. To detect sentiment words we use a pro-
prietary Microsoft lexicon of 7K positive and nega-
tive terms.
4 Evaluation
4.1 Learning Algorithm
We experimented with two different learning al-
gorithms: Multiple Additive Regression-Trees
(MART) (Wu et al, 2008) and a maximum entropy
classifier (Berger et al, 1996). Both provide fast
classification, a natural requirement for large-scale
real-time tasks.
4.2 Dataset
In our evaluation we focus on English tweets only.
Since we use local posting time in our features, we
filtered users whose profile did not contain location
information. To collect Tweeter messages we used
the entire public feed of Twitter (often referred to as
the Twitter Firehose). We randomly sampled 943K
tweets from one week of data. We allowed an ex-
tra week for responses, giving a response window
of two weeks. The majority of tweets in our set
(90%) received no response. We used 750K tweets
for training and 188K for evaluation. A separate data
set served as a development set. For the computation
of aggregate lexical features we used 186M tweets
from the same week, resulting in 14M bigrams and
400K hash tags. To compute historical features, we
sampled 2B tweets from the previous three months.
Figure 1: Precision-recall curves for predicting that a
tweet will get a response. The marked area highlights
the area of the curve we focus on in our evaluation.
Figure 2: Precision-recall curves with increasing number
of features removed for the marked area in Figure 1. For
each curve we removed one additional feature set from
the one above it.
4.3 Results
Our evaluation focuses on precision-recall curves
for predicting that a given tweet will get a response.
The curves were generated by varying the confi-
dence measure threshold, which both classifiers pro-
vided. As can be seen in Figure 1, MART outper-
forms the maximum entropy model. We can also see
that it is hard to predict response behavior for most
tweets, but for a large subset we can provide a rela-
tively accurate prediction (highlighted in Figure 1).
The rest of our analysis focuses on this subset and
on results based on MART.
To better understand the contribution of each fea-
ture set, we removed features in a greedy manner.
After learning a model and testing it, we removed
the feature family that was overall most highly
ranked by MART (i.e., was used in high-level splits
in the decision trees) and learned a new model. Fig-
ure 2 shows how removing feature sets degrades pre-
diction performance. Removing historical features
lowers the model?s prediction abilities, although pre-
diction quality remains relatively high. Removing
social features creates a bigger drop in performance.
Lastly, removing aggregate lexical features and lo-
604
cal content features further decreases performance.
At this point, removing posting time features is not
influential. Following the removal of posting time
features, the model includes only sentiment features.
5 Discussion and Conclusion
The first trend seen by removing features is that local
content matters less, or at least is more complex to
capture and use for response prediction. Despite the
influence of chronological trends on posting behav-
ior on Twitter (Pear Analytics, 2009), we were un-
able to show influence of posting time on response
prediction. Historical features were the most promi-
nent in our experiments. Second were social fea-
tures, showing that developing one?s network is crit-
ical for impact. The third most prominent set of fea-
tures, aggregate lexical features, shows that users are
sensitive to certain expressions and terms that tend
to trigger responses.
The natural path for future work is to improve per-
formance using new features. These may include
clique-specific language features, more properties of
the user?s social network, mentions of named enti-
ties and topics of tweets. Another direction is to dis-
tinguish between replies and retweets and to predict
the number of responses and the length of conversa-
tions that a tweet may generate. There is also po-
tential in learning models for the prediction of other
measures of impact, such as hashtag adoption and
inclusion in ?favorites? lists.
Acknowledgments
We would like to thank Alan Ritter, Bill Dolan,
Chris Brocket and Luke Zettlemoyer for their sug-
gestions and comments. We wish to thank Chris
Quirk and Qiang Wu for providing us with access
to their learning software. Thanks to the reviewers
for the helpful comments.
References
S. Asur and B.A. Huberman. 2010. Predicting the future
with social media. In Proceedings of the International
Conference on Web Intelligence and Intelligent Agent
Technology.
E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts.
2011. Everyone?s an influencer: quantifying influence
on twitter. In Peoceedings of the ACM International
Conference on Web Search and Data Mining.
R. Balasubramanyan, W.W. Cohen, D. Pierce, and D.P.
Redlawsk. 2011. What pushes their buttons? predict-
ing comment polarity from the content of political blog
posts. In Proceedings of the Workshop on Language in
Social Media.
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics.
D. Boyd, S. Golder, and G. Lotan. 2010. Tweet, tweet,
retweet: Conversational aspects of retweeting on twit-
ter. In Proceedings of the International Conference on
System Sciences.
M. Cha, H. Haddadi, F. Benevenuto, and K.P. Gummadi.
2010. Measuring user influence in twitter: The million
follower fallacy. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
A. Culotta. 2010. Towards detecting influenza epidemics
by analyzing twitter messages. In Proceedings of the
Workshop on Social Media Analytics.
C. Danescu-Niculescu-Mizil, M. Gamon, and S. Dumais.
2011. Mark my words!: linguistic style accommoda-
tion in social media. In Proceedings of the Interna-
tional Conference on World Wide Web.
A. Das Sarma, A. Das Sarma, S. Gollapudi, and R. Pan-
igrahy. 2010. Ranking mechanisms in twitter-like fo-
rums. In Proceedings of the ACM International Con-
ference on Web Search and Data Mining.
C. Honeycutt and S.C. Herring. 2009. Beyond mi-
croblogging: Conversation and collaboration via twit-
ter. In Proceedings of the International Conference on
System Sciences.
L. Hong, O. Dan, and B. D. Davison. 2011. Predict-
ing popular messages in twitter. In Proceedings of the
International Conference on World Wide Web.
H. Lakkaraju, A. Rai, and S. Merugu. 2011. Smart
news feeds for social networks using scalable joint la-
tent factor models. In Proceedings of the International
Conference on World Wide Web.
Pear Analytics. 2009. Twitter study.
A.M. Popescu and A. Jain. 2011. Understanding the
functions of business accounts on twitter. In Proceed-
ings of the International Conference on World Wide
Web.
A.M. Popescu and M. Pennacchiotti. 2010. Detect-
ing controversial events from twitter. In Proceedings
of the International Conference on Information and
Knowledge Management.
A.M. Popescu and M. Pennacchiotti. 2011. Dancing
with the stars, nba games, politics: An exploration of
twitter users response to events. In Proceedings of the
605
International AAAI Conference on Weblogs and Social
Media.
A.M. Popescu, M. Pennacchiotti, and D. Paranjpe. 2011.
Extracting events and event descriptions from twit-
ter. In Proceedings of the International Conference
on World Wide Web.
Y. Qu, C. Huang, P. Zhang, and J. Zhang. 2011. Mi-
croblogging after a major disaster in china: a case
study of the 2010 yushu earthquake. In Proceedings
of the ACM Conference on Computer Supported Co-
operative Work.
A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised
modeling of twitter conversations. In Proceedings of
the Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
A. Ritter, C. Cherry, and B. Dolan. 2011. Data-driven
response generation in social media. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
D. Romero, W. Galuba, S. Asur, and B. Huberman. 2011.
Influence and passivity in social media. Machine
Learning and Knowledge Discovery in Databases,
pages 18?33.
M. Rowe, S. Angeletou, and H. Alani. 2011. Predicting
discussions on the social semantic web. In Proceed-
ings of the Extended Semantic Web Conference.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors. In Proceedings of the International
Conference on World Wide Web.
B. Suh, L. Hong, P. Pirolli, and E. H. Chi. 2010. Want to
be retweeted? large scale analytics on factors impact-
ing retweet in twitter network. In Proceedings of the
IEEE International Conference on Social Computing.
M. Tsagkias, W. Weerkamp, and M. De Rijke. 2009.
Predicting the volume of comments on online news
stories. In Proceedings of the ACM Conference on In-
formation and Knowledge Management.
M. Tsagkias, W. Weerkamp, and M. De Rijke. 2010.
News comments: Exploring, modeling, and online
prediction. Advances in Information Retrieval, pages
191?203.
O. Tsur and A. Rappoport. 2012. What?s in a hash-
tag?: content based prediction of the spread of ideas
in microblogging communities. In Proceedings of the
ACM International Conference on Web Search and
Data Mining.
Q. Wu, C.J.C. Burges, K.M. Svore, and J. Gao. 2008.
Ranking, boosting, and model adaptation. Tecnical
Report, MSR-TR-2008-109.
S. Wu, J.M. Hofman, W.A. Mason, and D.J. Watts. 2011.
Who says what to whom on twitter. In Proceedings of
the International Conference on World Wide Web.
T. Yano and N.A. Smith. 2010. Whats worthy of com-
ment? content and comment volume in political blogs.
Proceedings of the International AAAI Conference on
Weblogs and Social Media.
T. Yano, W.W. Cohen, and N.A. Smith. 2009. Predict-
ing response to political blog posts with topic mod-
els. In Proceedings of the Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
606
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, page 2,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Parsing with Combinatory Categorial Grammars
Yoav Artzi, Nicholas FitzGerald and Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{yoav,nfitz,lsz}@cs.washington.edu
1 Abstract
Semantic parsers map natural language sentences
to formal representations of their underlying
meaning. Building accurate semantic parsers
without prohibitive engineering costs is a long-
standing, open research problem.
The tutorial will describe general principles for
building semantic parsers. The presentation will
be divided into two main parts: modeling and
learning. The modeling section will include best
practices for grammar design and choice of se-
mantic representation. The discussion will be
guided by examples from several domains. To il-
lustrate the choices to be made and show how they
can be approached within a real-life representation
language, we will use ?-calculus meaning repre-
sentations. In the learning part, we will describe
a unified approach for learning Combinatory Cat-
egorial Grammar (CCG) semantic parsers, that in-
duces both a CCG lexicon and the parameters of
a parsing model. The approach learns from data
with labeled meaning representations, as well as
from more easily gathered weak supervision. It
also enables grounded learning where the seman-
tic parser is used in an interactive environment, for
example to read and execute instructions.
The ideas we will discuss are widely appli-
cable. The semantic modeling approach, while
implemented in ?-calculus, could be applied to
many other formal languages. Similarly, the al-
gorithms for inducing CCGs focus on tasks that
are formalism independent, learning the meaning
of words and estimating parsing parameters. No
prior knowledge of CCGs is required. The tuto-
rial will be backed by implementation and exper-
iments in the University of Washington Semantic
Parsing Framework (UW SPF).1
1http://yoavartzi.com/spf
2 Outline
1. Introduction to CCGs
2. Modeling
(a) Questions for database queries
(b) Plurality and determiner resolution in
grounded applications
(c) Event semantics and imperatives in in-
structional language
3. Learning
(a) A unified learning algorithm
(b) Learning with supervised data
i. Lexical induction with templates
ii. Unification-based learning
(c) Weakly supervised learning without la-
beled meaning representations
3 Instructors
Yoav Artzi is a Ph.D. candidate in the Computer
Science & Engineering department at the Univer-
sity of Washington. His research studies the acqui-
sition of grounded natural language understanding
within interactive systems. His work focuses on
modeling semantic representations and designing
weakly supervised learning algorithms. He is a re-
cipient of the 2012 Yahoo KSC award.
Nicholas FitzGerald is a Ph.D. student at the
University of Washington. His research interests
are grounded natural language understanding and
generation. He is a recipient of an Intel Science
and Technology Center Fellowship and an NSERC
Postgraduate Scholarship.
Luke Zettlemoyer is an Assistant Professor in
the Computer Science & Engineering department
at the University of Washington. His research in-
terests are in the intersections of natural language
processing, machine learning and decision mak-
ing under uncertainty. Honors include best paper
awards at UAI 2005 and ACL 2009, selection to
the DARPA CSSG, and an NSF CAREER Award.
2
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 271?281,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning to Automatically Solve Algebra Word Problems
Nate Kushman
?
, Yoav Artzi
?
, Luke Zettlemoyer
?
, and Regina Barzilay
?
?
Computer Science and Articial Intelligence Laboratory, Massachusetts Institute of Technology
{nkushman, regina}@csail.mit.edu
?
Computer Science & Engineering, University of Washington
{yoav, lsz}@cs.washington.edu
Abstract
We present an approach for automatically
learning to solve algebra word problems.
Our algorithm reasons across sentence
boundaries to construct and solve a sys-
tem of linear equations, while simultane-
ously recovering an alignment of the vari-
ables and numbers in these equations to
the problem text. The learning algorithm
uses varied supervision, including either
full equations or just the final answers. We
evaluate performance on a newly gathered
corpus of algebra word problems, demon-
strating that the system can correctly an-
swer almost 70% of the questions in the
dataset. This is, to our knowledge, the first
learning result for this task.
1 Introduction
Algebra word problems concisely describe a world
state and pose questions about it. The described
state can be modeled with a system of equations
whose solution specifies the questions? answers.
For example, Figure 1 shows one such problem.
The reader is asked to infer how many children and
adults were admitted to an amusement park, based
on constraints provided by ticket prices and overall
sales. This paper studies the task of learning to
automatically solve such problems given only the
natural language.
1
Solving these problems requires reasoning
across sentence boundaries to find a system of
equations that concisely models the described se-
mantic relationships. For example, in Figure 1,
the total ticket revenue computation in the second
equation summarizes facts about ticket prices and
total sales described in the second, third, and fifth
1
The code and data for this work are available
at http://groups.csail.mit.edu/rbg/code/
wordprobs/.
Word problem
An amusement park sells 2 kinds of tickets.
Tickets for children cost $1.50. Adult tickets
cost $4. On a certain day, 278 people entered
the park. On that same day the admission fees
collected totaled $792. How many children
were admitted on that day? How many adults
were admitted?
Equations
x+ y = 278
1.5x+ 4y = 792
Solution
x = 128 y = 150
Figure 1: An example algebra word problem. Our
goal is to map a given problem to a set of equations
representing its algebraic meaning, which are then
solved to get the problem?s answer.
sentences. Furthermore, the first equation models
an implicit semantic relationship, namely that the
children and adults admitted are non-intersecting
subsets of the set of people who entered the park.
Our model defines a joint log-linear distribu-
tion over full systems of equations and alignments
between these equations and the text. The space
of possible equations is defined by a set of equa-
tion templates, which we induce from the train-
ing examples, where each template has a set of
slots. Number slots are filled by numbers from
the text, and unknown slots are aligned to nouns.
For example, the system in Figure 1 is gener-
ated by filling one such template with four spe-
cific numbers (1.5, 4, 278, and 792) and align-
ing two nouns (?Tickets? in ?Tickets for children?,
and ?tickets? in ?Adult tickets?). These inferred
correspondences are used to define cross-sentence
features that provide global cues to the model.
For instance, in our running example, the string
271
pairs (?$1.50?, ?children?) and (?$4?,?adults?)
both surround the word ?cost,? suggesting an out-
put equation with a sum of two constant-variable
products.
We consider learning with two different levels
of supervision. In the first scenario, we assume ac-
cess to each problem?s numeric solution (see Fig-
ure 1) for most of the data, along with a small
set of seed examples labeled with full equations.
During learning, a solver evaluates competing hy-
potheses to drive the learning process. In the sec-
ond scenario, we are provided with a full system
of equations for each problem. In both cases, the
available labeled equations (either the seed set, or
the full set) are abstracted to provide the model?s
equation templates, while the slot filling and align-
ment decisions are latent variables whose settings
are estimated by directly optimizing the marginal
data log-likelihood.
The approach is evaluated on a new corpus of
514 algebra word problems and associated equa-
tion systems gathered from Algebra.com. Pro-
vided with full equations during training, our al-
gorithm successfully solves over 69% of the word
problems from our test set. Furthermore, we find
the algorithm can robustly handle weak supervi-
sion, achieving more than 70% of the above per-
formance when trained exclusively on answers.
2 Related Work
Our work is related to three main areas of research:
situated semantic interpretation, information ex-
traction, and automatic word problem solvers.
Situated Semantic Interpretation There is a
large body of research on learning to map nat-
ural language to formal meaning representations,
given varied forms of supervision. Reinforcement
learning can be used to learn to read instructions
and perform actions in an external world (Brana-
van et al, 2009; Branavan et al, 2010; Vogel
and Jurafsky, 2010). Other approaches have re-
lied on access to more costly annotated logical
forms (Zelle and Mooney, 1996; Thompson and
Mooney, 2003; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2005; Kwiatkowski et al,
2010). These techniques have been generalized
more recently to learn from sentences paired with
indirect feedback from a controlled application.
Examples include question answering (Clarke et
al., 2010; Cai and Yates, 2013a; Cai and Yates,
2013b; Berant et al, 2013; Kwiatkowski et al,
2013), dialog systems (Artzi and Zettlemoyer,
2011), robot instruction (Chen and Mooney, 2011;
Chen, 2012; Kim and Mooney, 2012; Matuszek et
al., 2012; Artzi and Zettlemoyer, 2013), and pro-
gram executions (Kushman and Barzilay, 2013;
Lei et al, 2013). We focus on learning from varied
supervision, including question answers and equa-
tion systems, both can be obtained reliably from
annotators with no linguistic training and only ba-
sic math knowledge.
Nearly all of the above work processed sin-
gle sentences in isolation. Techniques that con-
sider multiple sentences typically do so in a se-
rial fashion, processing each in turn with limited
cross-sentence reasoning (Branavan et al, 2009;
Zettlemoyer and Collins, 2009; Chen and Mooney,
2011; Artzi and Zettlemoyer, 2013). We focus on
analyzing multiple sentences simultaneously, as
is necessary to generate the global semantic rep-
resentations common in domains such as algebra
word problems.
Information Extraction Our approach is related
to work on template-based information extraction,
where the goal is to identify instances of event
templates in text and extract their slot fillers. Most
work has focused on the supervised case, where
the templates are manually defined and data is la-
beled with alignment information, e.g. (Grishman
et al, 2005; Maslennikov and Chua, 2007; Ji and
Grishman, 2008; Reichart and Barzilay, 2012).
However, some recent work has studied the au-
tomatic induction of the set of possible templates
from data (Chambers and Jurafsky, 2011; Ritter et
al., 2012). In our approach, systems of equations
are relatively easy to specify, providing a type of
template structure, and the alignment of the slots
in these templates to the text is modeled primar-
ily with latent variables during learning. Addition-
ally, mapping to a semantic representation that can
be executed allows us to leverage weaker supervi-
sion during learning.
Automatic Word Problem Solvers Finally, there
has been research on automatically solving vari-
ous types of mathematical word problems. The
dominant existing approach is to hand engineer
rule-based systems to solve math problem in spe-
cific domains (Mukherjee and Garain, 2008; Lev
et al, 2004). Our focus is on learning a model
for the end-to-end task of solving word problems
given only a training corpus of questions paired
with equations or answers.
272
Derivation 1
Word
problem
An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult
tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the
admission fees collected totaled $ 792 . How many children were admitted on that
day? How many adults were admitted?
Aligned
template
u
1
1
+ u
1
2
? n
1
= 0 n
2
? u
2
1
+ n
3
? u
2
2
? n
4
= 0
Instantiated
equations
x+ y ? 278 = 0 1.5x+ 4y ? 792 = 0
Answer
x = 128
y = 150
Derivation 2
Word
problem
A motorist drove 2 hours at one speed and then for 3 hours at another speed. He
covered a distance of 252 kilometers. If he had traveled 4 hours at the first speed and
1 hour at the second speed , he would have covered 244 kilometers. Find two speeds?
Aligned
template
n
1
? u
1
1
+ n
2
? u
1
2
? n
3
= 0 n
4
? u
2
1
+ n
5
? u
2
2
? n
6
= 0
Instantiated
equations
2x+ 3y ? 252 = 0 4x+ 1y ? 244 = 0
Answer
x = 48
y = 52
Figure 2: Two complete derivations for two different word problems. Derivation 1 shows an alignment
where two instances of the same slot are aligned to the same word (e.g., u
1
1
and u
2
1
both are aligned to
?Tickets?). Derivation 2 includes an alignment where four identical nouns are each aligned to different
slot instances in the template (e.g., the first ?speed? in the problem is aligned to u
1
1
).
3 Mapping Word Problems to Equations
We define a two step process to map word prob-
lems to equations. First, a template is selected
to define the overall structure of the equation sys-
tem. Next, the template is instantiated with num-
bers and nouns from the text. During inference we
consider these two steps jointly.
Figure 2 shows both steps for two derivations.
The template dictates the form of the equations in
the system and the type of slots in each: u slots
represent unknowns and n slots are for numbers
that must be filled from the text. In Derivation 1,
the selected template has two unknown slots, u
1
and u
2
, and four number slots, n
1
to n
4
. Slots
can be shared between equations, for example, the
unknown slots u
1
and u
2
in the example appear
in both equations. A slot may have different in-
stances, for example u
1
1
and u
2
1
are the two in-
stances of u
1
in the example.
We align each slot instance to a word in the
problem. Each number slot n is aligned to a num-
ber, and each unknown slot u is aligned to a noun.
For example, Derivation 1 aligns the number 278
to n
1
, 1.50 to n
2
, 4 to n
3
, and 792 to n
4
. It also
aligns both instances of u
1
(e.g., u
1
1
and u
2
1
) to
?Tickets?, and both instances of u
2
to ?tickets?.
In contrast, in Derivation 2, instances of the same
unknown slot (e.g. u
1
1
and u
2
1
) are aligned to two
different words in the problem (different occur-
rences of the word ?speed?). This allows for a
tighter mapping between the natural language and
the system template, where the words aligned to
the first equation in the template come from the
first two sentences, and the words aligned to the
second equation come from the third.
Given an alignment, the template can then be
instantiated: each number slot n is replaced with
the aligned number, and each unknown slot u with
a variable. This output system of equations is then
automatically solved to generate the final answer.
273
3.1 Derivations
Definitions Let X be the set of all word problems.
A word problem x ? X is a sequence of k words
?w
1
, . . . w
k
?. Also, define an equation template t
to be a formulaA = B, whereA andB are expres-
sions. An expression A is one of the following:
? A number constant f .
? A number slot n.
? An unknown slot u.
? An application of a mathematical relation R
to two expressions (e.g., n
1
? u
1
).
We define a system template T to be a set of l
equation templates {t
0
, . . . , t
l
}. T is the set of
all system templates. A slot may occur more than
once in a system template, to allow variables to
be reused in different equations. We denote a spe-
cific instance i of a slot, u for example, as u
i
. For
brevity, we omit the instance index when a slot ap-
pears only once. To capture a correspondence be-
tween the text of x and a template T , we define an
alignment p to be a set of pairs (w, s), where w is
a token in x and s is a slot instance in T .
Given the above definitions, an equation e can
be constructed from a template t where each num-
ber slot n is replaced with a real number, each un-
known slot u is replaced with a variable, and each
number constant f is kept as is. We call the pro-
cess of turning a template into an equation tem-
plate instantiation. Similarly, an equation system
E is a set of l equations {e
0
, . . . , e
l
}, which can
be constructed by instantiating each of the equa-
tion templates in a system template T . Finally, an
answer a is a tuple of real numbers.
We define a derivation y from a word problem
to an answer as a tuple (T, p, a), where T is the se-
lected system template, p is an alignment between
T and x, and a is the answer generated by instan-
tiating T using x through p and solving the gener-
ated equations. Let Y be the set of all derivations.
The Space of Possible Derivations We aim to
map each word problem x to an equation system
E. The space of equation systems considered is
defined by the set of possible system templates T
and the words in the original problem x, that are
available for filling slots. In practice, we gener-
ate T from the training data, as described in Sec-
tion 4.1. Given a system template T ? T , we
create an alignment p between T and x. The set
of possible alignment pairs is constrained as fol-
An amusement park sells 2 kinds of tickets.
Tickets for children cost $ 1.50 . Adult tick-
ets cost $ 4 . On a certain day, 278 people
entered the park. On that same day the ad-
mission fees collected totaled $ 792 . How
many children were admitted on that day?
How many adults were admitted?
u
1
1
+ u
1
2
? n
1
= 0
n
2
? u
2
1
+ n
3
? u
2
2
? n
4
= 0
Figure 3: The first example problem and selected
system template from Figure 2 with all potential
aligned words marked. Nouns (boldfaced) may be
aligned to unknown slot instances u
j
i
, and num-
ber words (highlighted) may be aligned to number
slots n
i
.
lows: each number slot n ? T can be aligned to
any number in the text, a number word can only
be aligned to a single slot n, and must be aligned
to all instances of that slot. Additionally, an un-
known slot instance u ? T can only be aligned to
a noun word. A complete derivation?s alignment
pairs all slots in T with words in x.
Figure 3 illustrates the space of possible align-
ments for the first problem and system template
from Figure 2. Nouns (shown in boldface) can
be aligned to any of the unknown slot instances
in the selected template (u
1
1
, u
2
1
, u
1
2
, and u
2
2
for the
template selected). Numbers (highlighted) can be
aligned to any of the number slots (n
1
, n
2
, n
3
, and
n
4
in the template).
3.2 Probabilistic Model
Due to the ambiguity in selecting the system tem-
plate and alignment, there will be many possible
derivations y ? Y for each word problem x ? X .
We discriminate between competing analyses us-
ing a log-linear model, which has a feature func-
tion ? : X ? Y ? R
d
and a parameter vector
? ? R
d
. The probability of a derivation y given a
problem x is defined as:
p(y|x; ?) =
e
???(x,y)
?
y
?
?Y
e
???(x,y
?
)
Section 6 defines the full set of features used.
The inference problem at test time requires us
to find the most likely answer a given a problem
274
x, assuming the parameters ? are known:
f(x) = argmax
a
p(a|x; ?)
Here, the probability of the answer is marginalized
over template selection and alignment:
p(a|x; ?) =
?
y?Y
s.t. AN(y)=a
p(y|x; ?) (1)
where AN(y) extracts the answer a out of deriva-
tion y. In this way, the distribution over deriva-
tions y is modeled as a latent variable. We use a
beam search inference procedure to approximately
compute Equation 1, as described in Section 5.
4 Learning
To learn our model, we need to induce the struc-
ture of system templates in T and estimate the
model parameters ?.
4.1 Template Induction
It is possible to generate system templates T when
provided access to a set of n training examples
{(x
i
, E
i
) : i = 1, . . . , n}, where x
i
is a word
problem and E
i
is a set of equations. We general-
ize eachE to a system template T by (a) replacing
each variable with an unknown slot, and (b) re-
placing each number mentioned in the text with a
number slot. Numbers not mentioned in the prob-
lem text remain in the template as constants. This
allows us to solve problems that require numbers
that are implied by the problem semantics rather
than appearing directly in the text, such as the per-
cent problem in Figure 4.
4.2 Parameter Estimation
For parameter estimation, we assume access to
n training examples {(x
i
,V
i
) : i = 1, . . . , n},
each containing a word problem x
i
and a val-
idation function V
i
. The validation function
V : Y ? {0, 1} maps a derivation y ? Y to 1 if
it is correct, or 0 otherwise.
We can vary the validation function to learn
from different types of supervision. In Sec-
tion 8, we will use validation functions that check
whether the derivation y has either (1) the cor-
rect system of equations E, or (2) the correct an-
swer a. Also, using different types of validation
functions on different subsets of the data enables
semi-supervised learning. This approach is related
to Artzi and Zettlemoyer (2013).
Word problem
A chemist has a solution that is 18 % alco-
hol and one that is 50 % alcohol. He wants
to make 80 liters of a 30 % solution. How
many liters of the 18 % solution should he
add? How many liters of the 30 % solution
should he add?
Labeled equations
18? 0.01? x + 50? 0.01? y = 30? 0.01? 80
x + y = 80
Induced template system
n
1
? 0.01? u
1
1
+ n
2
? 0.01? u
1
2
= n
3
? 0.01? n
4
u
2
1
+ u
2
2
= n
5
Figure 4: During template induction, we automat-
ically detect the numbers in the problem (high-
lighted above) to generalize the labeled equations
to templates. Numbers not present in the text are
considered part of the induced template.
We estimate ? by maximizing the conditional
log-likelihood of the data, marginalizing over all
valid derivations:
O =
?
i
?
y?Y
s.t. V
i
(y)=1
log p(y|x
i
; ?)
We use L-BFGS (Nocedal and Wright, 2006) to
optimize the parameters. The gradient of the indi-
vidual parameter ?
j
is given by:
?O
??
j
=
?
i
E
p(y|x
i
,V
i
(y)=1;?)
[?
j
(x
i
, y)]?
E
p(y|x
i
;?)
[?
j
(x
i
, y)]
(2)
Section 5 describes how we approximate the
two terms of the gradient using beam search.
5 Inference
Computing the normalization constant for Equa-
tion 1 requires summing over all templates and all
possible ways to instantiate them. This results in
a search space exponential in the number of slots
in the largest template in T , the set of available
system templates. Therefore, we approximate this
computation using beam search. We initialize the
beam with all templates in T and iteratively align
slots from the templates in the beam to words in
the problem text. For each template, the next slot
275
to be considered is selected according to a pre-
defined canonicalized ordering for that template.
After each iteration we prune the beam to keep the
top-k partial derivations according to the model
score. When pruning the beam, we allow at most l
partial derivations for each template, to ensure that
a small number of templates don?t monopolize the
beam. We continue this process until all templates
in the beam are fully instantiated.
During learning we compute the second term in
the gradient (Equation 2) using our beam search
approximation. Depending on the available vali-
dation function V (as defined in Section 4.2), we
can also accurately prune the beam for the com-
putation of the first half of the gradient. Specifi-
cally, when assuming access to labeled equations,
we can constrain the search to consider only par-
tial hypotheses that could possibly be completed
to produce the labeled equations.
6 Model Details
Template Canonicalization There are many syn-
tactically different but semantically equivalent
ways to express a given system of equations. For
example, the phrase ?John is 3 years older than
Bill? can be written as j = b+ 3 or j ? 3 = b.
To avoid such ambiguity, we canonicalize tem-
plates into a normal form representation. We per-
form this canonicalization by obtaining the sym-
bolic solution for the unknown slots in terms of
the number slots and constants using the mathe-
matical solver Maxima (Maxima, 2014).
Slot Signature In a template like s
1
+s
2
= s
3
, the
slot s
1
is distinct from the slot s
2
, but we would
like them to share many of the features used in de-
ciding their alignment. To facilitate this, we gener-
ate signatures for each slot and slot pair. The sig-
nature for a slot indicates the system of equations
it appears in, the specific equation it is in, and the
terms of the equation it is a part of. Pairwise slot
signatures concatenate the signatures for the two
slots as well as indicating which terms are shared.
This allows, for example, n
2
and n
3
in Derivation
1 in Figure 2 to have the same signature, while the
pairs ?n
2
, u
1
? and ?n
3
, u
1
? have different ones. To
share features across templates, slot and slot-pair
signatures are generated for both the full template,
as well as for each of the constituent equations.
Features The features ?(x, y) are computed for a
derivation y and problem x and cover all deriva-
Document level
Unigrams
Bigrams
Single slot
Has the same lemma as a question object
Is a question object
Is in a question sentence
Is equal to one or two (for numbers)
Word lemma X nearby constant
Slot pair
Dep. path contains: Word
Dep. path contains: Dep. Type
Dep. path contains: Word X Dep. Type
Are the same word instance
Have the same lemma
In the same sentence
In the same phrase
Connected by a preposition
Numbers are equal
One number is larger than the other
Equivalent relationship
Solution Features
Is solution all positive
Is solution all integer
Table 1: The features divided into categories.
tion decisions, including template and alignment
selection. When required, we use standard tools
to generate part-of-speech tags, lematizations, and
dependency parses to compute features.
2
For each
number word in y we also identify the closest noun
in the dependency parse. For example, the noun
for 278 in Derivation 1, Figure 2 would be ?peo-
ple.? The features are calculated based on these
nouns, rather than the number words.
We use four types of features: document level
features, features that look at a single slot entry,
features that look at pairs of slot entries, and fea-
tures that look at the numeric solutions. Table 1
lists all the features used. Unless otherwise noted,
when computing slot and slot pair features, a sep-
arate feature is generated for each of the signature
types discussed earlier.
Document level features Oftentimes the natural
language in x will contain words or phrases which
are indicative of a certain template, but are not as-
sociated with any of the words aligned to slots in
the template. For example, the word ?chemist?
2
In our experiments these are generated using the Stan-
ford parser (de Marneffe et al, 2006)
276
might indicate a template like the one seen in Fig-
ure 4. We include features that connect each tem-
plate with the unigrams and bigrams in the word
problem. We also include an indicator feature for
each system template, providing a bias for its use.
Single Slot Features The natural language x al-
ways contains one or more questions or commands
indicating the queried quantities. For example, the
first problem in Figure 2 asks ?How many children
were admitted on that day?? The queried quanti-
ties, the number of children in this case, must be
represented by an unknown in the system of equa-
tions. We generate a set of features which look at
both the word overlap and the noun phrase overlap
between slot words and the objects of a question or
command sentence. We also compute a feature in-
dicating whether a slot is filled from a word in a
question sentence. Additionally, algebra problems
frequently use phrases such as ?2 kinds of tickets?
(e.g., Figure 2). These numbers do not typically
appear in the equations. To account for this, we
add a single feature indicating whether a number
is one or two. Lastly, many templates contain con-
stants which are identifiable from words used in
nearby slots. For example, in Figure 4 the con-
stant 0.01 is related to the use of ?%? in the text.
To capture such usage, we include a set of lexical-
ized features which concatenate the word lemma
with nearby constants in the equation. These fea-
tures do not include the slot signature.
Slot Pair Features The majority of features we
compute account for relationships between slot
words. This includes features that trigger for
various equivalence relations between the words
themselves, as well as features of the dependency
path between them. We also include features that
look at the numerical relationship of two num-
bers, where the numeric values of the unknowns
are generated by solving the system of equations.
This helps recognize that, for example, the total of
a sum is typically larger than each of the (typically
positive) summands.
Additionally, we also have a single feature look-
ing at shared relationships between pairs of slots.
For example, in Figure 2 the relationship between
?tickets for children? and ?$1.50? is ?cost?. Sim-
ilarly the relationship between ?Adult tickets? and
?$4? is also ?cost?. Since the actual nature of this
relationship is not important, this feature is not
lexicalized, instead it is only triggered for the pres-
ence of equality. We consider two cases: subject-
# problems 514
# sentences 1616
# words 19357
Vocabulary size 2352
Mean words per problem 37
Mean sentences per problem 3.1
Mean nouns per problem 13.4
# unique equation systems 28
Mean slots per system 7
Mean derivations per problem 4M
Table 2: Dataset statistics.
object relationships where the intervening verb
is equal, and noun-to-preposition object relation-
ships where the intervening preposition is equal.
Solution Features By grounding our semantics in
math, we are able to include features which look
at the final answer, a, to learn which answers are
reasonable for the algebra problems we typically
see. For example, the solution to many, but not all,
of the problems involves the size of some set of
objects which must be both positive and integer.
7 Experimental Setup
Dataset We collected a new dataset of alge-
bra word problems from Algebra.com, a crowd-
sourced tutoring website. The questions were
posted by students for members of the community
to respond with solutions. Therefore, the problems
are highly varied, and are taken from real prob-
lems given to students. We heuristically filtered
the data to get only linear algebra questions which
did not require any explicit background knowl-
edge. From these we randomly chose a set of
1024 questions. As the questions are posted to a
web forum, the posts often contained additional
comments which were not part of the word prob-
lems and the solutions are embedded in long free-
form natural language descriptions. To clean the
data we asked Amazon Mechanical Turk workers
to extract from the text: the algebra word prob-
lem itself, the solution equations, and the numeric
answer. We manually verified both the equations
and the numbers to ensure they were correct. To
ensure each problem type is seen at least a few
times in the training data, we removed the infre-
quent problem types. Specifically, we induced the
system template from each equation system, as de-
scribed in Section 4.1, and removed all problems
for which the associated system template appeared
277
less than 6 times in the dataset. This left us with
514 problems. Table 2 provides the data statistics.
Forms of Supervision We consider both semi-
supervised and supervised learning. In the semi-
supervised scenario, we assume access to the nu-
merical answers of all problems in the training cor-
pus and to a small number of problems paired with
full equation systems. To select which problems
to annotate with equations, we identified the five
most common types of questions in the data and
annotated a randomly sampled question of each
type. 5EQ+ANS uses this form of weak supervi-
sion. To show the benefit of using the weakly su-
pervised data, we also provide results for a base-
line scenario 5EQ, where the training data includes
only the five seed questions annotated with equa-
tion systems. In the fully supervised scenario
ALLEQ, we assume access to full equation sys-
tems for the entire training set.
Evaluation Protocol We run all our experiments
using 5-fold cross-validation. Since our model
generates a solution for every problem, we report
only accuracy. We report two metrics: equation
accuracy to measure how often the system gener-
ates the correct equation system, and answer accu-
racy to evaluate how often the generated numerical
answer is correct. When comparing equations, we
avoid spurious differences by canonicalizing the
equation system, as described in Section 6. To
compare answer tuples we disregard the ordering
and require each number appearing in the refer-
ence answer to appear in the generated answer.
Parameters and Solver In our experiments we set
k in our beam search algorithm (Section 5) to 200,
and l to 20. We run the L-BFGS computation for
50 iterations. We regularize our learning objec-
tive using the L
2
-norm and a ? value of 0.1. The
set of mathematical relations supported by our im-
plementation is {+,?,?, /}.Our implementation
uses the Gaussian Elimination function in the Effi-
cient Java Matrix Library (EJML) (Abeles, 2014)
to generate answers given a set of equations.
8 Results
8.1 Impact of Supervision
Table 3 summarizes the results. As expected, hav-
ing access to the full system of equations (ALLEQ)
at training time results in the best learned model,
with nearly 69% accuracy. However, training
from primarily answer annotations (5EQ+ANS)
Equation Answer
accuracy accuracy
5EQ 20.4 20.8
5EQ+ANS 45.7 46.1
ALLEQ 66.1 68.7
Table 3: Cross-validation accuracy results for var-
ious forms of supervision.
Equation Answer % of
accuracy accuracy data
? 10 43.6 50.8 25.5
11? 15 46.6 45.1 10.5
16? 20 44.2 52.0 11.3
> 20 85.7 86.1 52.7
Table 4: Performance on different template fre-
quencies for ALLEQ.
results in performance which is almost 70% of
ALLEQ, demonstrating the value of weakly super-
vised data. In contrast, 5EQ, which cannot use this
weak supervision, performs much worse.
8.2 Performance and Template Frequency
To better understand the results, we also measured
equation accuracy as a function of the frequency
of each equation template in the data set. Table 4
reports results for ALLEQ after grouping the prob-
lems into four different frequency bins. We can
see that the system correctly answers more than
85% of the question types which occur frequently
while still achieving more than 50% accuracy on
those that occur relatively infrequently. We do not
include template frequency results for 5EQ+ANS
since in this setup our system is given only the top
five most common templates. This limited set of
templates covers only those questions in the > 20
bin, or about 52% of the data. However, on this
subset 5EQ+ANS performs very well, answering
88% of them correctly, which is approximately the
same as the 86% achieved by ALLEQ. Thus while
the weak supervision is not helpful in generating
the space of possible equations, it is very helpful
in learning to generate the correct answer when
given an appropriate space of equations.
8.3 Ablation Analysis
Table 5 shows ablation results for each group of
features. The results along the diagonal show the
performance when a single group of features is
ablated, while the off-diagonal numbers show the
278
w/o w/o w/o w/o
pair document solution single
w/o pair 42.8 25.7 19.0 39.6
w/o document ? 63.8 50.4 57.6
w/o solution ? ? 63.6 62.0
w/o single ? ? ? 65.9
Table 5: Cross-validation accuracy results with
different feature groups ablated for ALLEQ. Re-
sults are for answer accuracy which is 68.7% with-
out any features ablated.
performance when two groups of features are ab-
lated together. We can see that all of the features
contribute to the overall performance, and that the
pair features are the most important followed by
the document and solution features. We also see
that the pair features can compensate for the ab-
sence of other features. For example, the perfor-
mance drops only slightly when either the docu-
ment or solution features are removed in isolation.
However, the drop is much more dramatic when
they are removed along with the pair features.
8.4 Qualitative Error Analysis
We examined our system output on one fold of
ALLEQ and identified two main classes of errors.
The first, accounting for approximately one-
quarter of the cases, includes mistakes where
more background or world knowledge might have
helped. For example, Problem 1 in Figure 5 re-
quires understanding the relation between the di-
mensions of a painting, and how this relation is
maintained when the painting is printed, and Prob-
lem 2 relies on understanding concepts of com-
merce, including cost, sale price, and profit. While
these relationships could be learned in our model
with enough data, as it does for percentage prob-
lems (e.g., Figure 4), various outside resources,
such as knowledge bases (e.g. Freebase) or distri-
butional statistics from a large text corpus, might
help us learn them with less training data.
The second category, which accounts for about
half of the errors, includes mistakes that stem from
compositional language. For example, the second
sentence in Problem 3 in Figure 5 could generate
the equation 2x?y = 5, with the phrase ?twice of
one of them? generating the expression 2x. Given
the typical shallow nesting, it?s possible to learn
templates for these cases given enough data, and in
the future it might also be possible to develop new,
cross-sentence semantic parsers to enable better
generalization from smaller datasets.
(1)
A painting is 10 inches tall and 15 inches
wide. A print of the painting is 25 inches
tall, how wide is the print in inches?
(2)
A textbook costs a bookstore 44 dollars,
and the store sells it for 55 dollars. Find
the amount of profit based on the selling
price.
(3)
The sum of two numbers is 85. The dif-
ference of twice of one of them and the
other one is 5. Find both numbers.
(4)
The difference between two numbers is
6. If you double both numbers, the sum
is 36. Find the two numbers.
Figure 5: Examples of problems our system does
not solve correctly.
9 Conclusion
We presented an approach for automatically learn-
ing to solve algebra word problems. Our algorithm
constructs systems of equations, while aligning
their variables and numbers to the problem text.
Using a newly gathered corpus we measured the
effects of various forms of weak supervision on
performance. To the best of our knowledge, we
present the first learning result for this task.
There are still many opportunities to improve
the reported results, and extend the approach to
related domains. We would like to develop tech-
niques to learn compositional models of mean-
ing for generating new equations. Furthermore,
the general representation of mathematics lends it-
self to many different domains including geome-
try, physics, and chemistry. Eventually, we hope
to extend the techniques to synthesize even more
complex structures, such as computer programs,
from natural language.
Acknowledgments
The authors acknowledge the support of Battelle
Memorial Institute (PO#300662) and NSF (grant
IIS-0835652). We thank Nicholas FitzGerald, the
MIT NLP group, the UW NLP group and the
ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
279
References
Peter Abeles. 2014. Efficient java matrix library.
https://code.google.com/p/efficient
-java-matrix-library/.
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to
map high-level instructions to commands. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics.
Qingqing Cai and Alexander Yates. 2013b. Seman-
tic parsing freebase: Towards open-domain seman-
tic parsing. In Proceedings of the Joint Conference
on Lexical and Computational Semantics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of the Confer-
ence on Artificial Intelligence.
David Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of the Conference on Language Re-
sources and Evaluation.
Ralph Grishman, David Westbrook, and Adam Mey-
ers. 2005. NYUs English ACE 2005 System De-
scription. In Proceedings of the Automatic Content
Extraction Evaluation Workshop.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised pcfg induction for grounded language learning
with highly ambiguous supervision. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceeding of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the Conference
on Empirical Methods on Natural Language Pro-
cessing.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Tao Lei, Fan Long, Regina Barzilay, and Martin Ri-
nard. 2013. From natural language specifications to
program input parsers. In Proceeding of the Associ-
ation for Computational Linguistics.
Iddo Lev, Bill MacCartney, Christopher Manning, and
Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proceed-
ings of the Workshop on Text Meaning and Interpre-
tation. Association for Computational Linguistics.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extrac-
tion from free text. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
Maxima. 2014. Maxima, a computer algebra system.
version 5.32.1.
280
Anirban Mukherjee and Utpal Garain. 2008. A review
of methods for automatic understanding of natural
language mathematical problems. Artificial Intelli-
gence Review, 29(2).
Jorge Nocedal and Stephen Wright. 2006. Numeri-
cal optimization, series in operations research and
financial engineering. Springer, New York.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the Conference on Knowledge Dis-
covery and Data Mining.
Cynthia Thompson and Raymond Mooney. 2003.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence
Research, 18(1).
Adam Vogel and Dan Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Annual Meeting
of the North American Chapter of the Association of
Computational Linguistics. Association for Compu-
tational Linguistics.
John Zelle and Raymond Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the Conference on Ar-
tificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial gram-
mars. In Proceedings of the Conference on Uncer-
tainty in Artificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Confer-
ence of the Association for Computational Linguis-
tics and International Joint Conference on Natural
Language Processing.
281
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1437?1447,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Context-dependent Semantic Parsing for Time Expressions
Kenton Lee
?
, Yoav Artzi
?
, Jesse Dodge
??
, and Luke Zettlemoyer
?
?
Computer Science & Engineering, University of Washington, Seattle, WA
{kentonl, yoav, lsz}@cs.washington.edu
?
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA
jessed@cs.cmu.edu
Abstract
We present an approach for learning
context-dependent semantic parsers to
identify and interpret time expressions.
We use a Combinatory Categorial Gram-
mar to construct compositional meaning
representations, while considering contex-
tual cues, such as the document creation
time and the tense of the governing verb,
to compute the final time values. Exper-
iments on benchmark datasets show that
our approach outperforms previous state-
of-the-art systems, with error reductions of
13% to 21% in end-to-end performance.
1 Introduction
Time expressions present a number of challenges
for language understanding systems. They have
rich, compositional structure (e.g., ?2nd Friday of
July?), can be easily confused with non-temporal
phrases (e.g., the word ?May? can be a month
name or a verb), and can vary in meaning in dif-
ferent linguistic contexts (e.g., the word ?Friday?
refers to different dates in the sentences ?We met
on Friday? and ?We will meet on Friday?). Recov-
ering the meaning of time expressions is therefore
challenging, but provides opportunities to study
context-dependent language use. In this paper, we
present the first context-dependent semantic pars-
ing approach for learning to identify and interpret
time expressions, addressing all three challenges.
Existing state-of-the-art methods use hand-
engineered rules for reasoning about time expres-
sions (Str?otgen and Gertz, 2013). This includes
both detection, identifying a phrase as a time ex-
pression, and resolution, mapping such a phrase
into a standardized time value. While rule-based
approaches provide a natural way to express ex-
pert knowledge, it is relatively difficult to en-
?
Work conducted at the University of Washington.
code preferences between similar competing hy-
potheses and provide prediction confidence. Re-
cently, methods for learning probabilistic seman-
tic parsers have been shown to address such limi-
tations (Angeli et al, 2012; Angeli and Uszkoreit,
2013). However, these approaches do not account
for any surrounding linguistic context and were
mainly evaluated with gold standard mentions.
We propose to use a context-dependent se-
mantic parser for both detection and resolution
of time expressions. For both tasks, we make
use of a hand-engineered Combinatory Catego-
rial Grammar (CCG) to construct a set of mean-
ing representations that identify the time being
described. For example, this grammar maps the
phrase ?2nd Friday of July? to the meaning repre-
sentation intersect(nth(2 , friday), july), which
encodes the set of all such days. Detection is then
performed with a binary classifier to prune the set
of text spans that can be parsed with the gram-
mar (e.g., to tell that ?born in 2000? has a time
expression but ?a 2000 piece puzzle? does not).
For resolution, we consider mentions sequentially
and use a log-linear model to select the most likely
meaning for each. This choice depends on contex-
tual cues such as previous time expressions and
the tense of the governing verb (e.g., as required
to correctly resolve cases like ?We should meet on
the 2nd Friday of July?).
Such an approach provides a good balance be-
tween hand engineering and learning. For the rel-
atively closed-class time expressions, we demon-
strate that it is possible to engineer a high quality
CCG lexicon. We take a data-driven approach for
grammar design, preferring a grammar with high
coverage even if it results in parsing ambiguities.
We then learn a model to accurately select between
competing parses and incorporate signals from the
surrounding context, both more difficult to model
with deterministic rules.
For both problems, we learn from TimeML an-
1437
notations (Pustejovsky et al, 2005), which mark
mentions and the specific times they reference.
Training the detector is a supervised learning
problem, but resolution is more challenging, re-
quiring us to reason about latent parsing and
context-dependent decisions.
We evaluate performance in two domains: the
TempEval-3 corpus of newswire text (Uzzaman et
al., 2013) and the WikiWars corpus of Wikipedia
history articles (Mazur and Dale, 2010). On these
benchmark datasets, we present new state-of-the-
art results, with error reductions of up to 28% for
the detection task and 21% for the end-to-end task.
2 Formal Overview
Time Expressions We follow the TIMEX3 stan-
dard (Pustejovsky et al, 2005) for defining time
expressions within documents. Let a document
D = ?w
1
, . . . , w
n
? be a sequence of n words w
i
and a mention m = (i, j) indicate start and end
indices for a phrase ?w
i
, . . . , w
j
? in D. Define
a time expression e = (t, v) to include both a
temporal type t and value v.
1
The temporal type
t ? {Date, Time, Duration, Set} can take one of
four possible values, indicating if the expression
e is a date (e.g., ?January 10, 2014?), time (e.g.,
?11:59 pm?), duration (e.g., ?6 months?), or set
(e.g., ?every year?). The value v is an extension
of the ISO 8601 standard, which encodes the time
that mentionm refers to in the context provided by
document D. For example, in a document written
on Tuesday, January 7, 2014, ?Friday,? ?three days
later,? and ?January 10th? would all resolve to the
value 2014-01-10. The time values are similarly
defined for a wide range of expressions, such as
underspecified dates (e.g., XXXX-01-10 for ?Ja-
nunary 10th? when the year is not inferable from
context) and durations (P2D for ?two days?).
Tasks Our goal is to find all time expressions in
an input document. We divide the problem into
two parts: detection and resolution. The detection
problem is to take an input documentD and output
a mention set M = {m
i
| i = 1 . . . n} of phrases
in D that describe time expressions. The resolu-
tion problem (often also called normalization) is,
given a document D and a set of mentions M , to
1
Time expressions also have optional modifier values
for non-TIMEX properties (e.g., the modifier would contain
EARLY for the phrase ?early march?). We do recover these
modifiers but omit them from the discussion since they are
not part of the official evaluation metrics.
map each m ? M to the referred time expression
e. This paper addresses both of these tasks.
Approach We learn separate, but related, mod-
els for detection and resolution. For both tasks, we
define the space of possible compositional mean-
ing representations Z , where each z ? Z defines
a unique time expression e. We use a log-linear
CCG (Steedman, 1996; Clark and Curran, 2007)
to rank possible meanings z ? Z for each men-
tion m in a document D, as described in Sec-
tion 4. Both detection (Section 5) and resolution
(Section 6) rely on the semantic parser to identify
likely mentions and resolve them within context.
For learning we assume access to TimeML data
containing documents labeled with time expres-
sions. Each document D has a set {(m
i
, e
i
)|i =
1 . . . n}, where each mention m
i
marks a phrase
that resolves to the time expression e
i
.
Evaluation We evaluate performance (Sec-
tion 8) for both newswire text and Wikipedia
articles. We compare to the state-of-the-art
systems for end-to-end resolution (Str?otgen and
Gertz, 2013) and resolution given gold men-
tions (Bethard, 2013b), both of which do not use
any machine learning techniques.
3 Representing Time
We use simply typed lambda calculus to represent
time expressions. Our representation draws heav-
ily from the representation proposed by Angeli et
al. (2012), who introduced semantic parsing for
this task. There are five primitive types: duration
d, sequence s, range r, approximate reference a,
and numeral n, as described below. Table 1 lists
the available constants for each type.
Duration A period of time. Each duration is a
multiple of one of a closed set of possible base
durations (e.g., hour, day, and quarter), which
we refer to as its granularity. Table 1 includes the
complete set of base durations used.
Range A specific interval of time, following an
interval-based theory of time (Allen, 1981). The
interval length is one of the base durations, which
is the granularity of the range. Given two ranges
R and R
?
, we say that R ? R
?
if the endpoints of
R lie on or within R
?
.
Sequence A set of ranges with identical granu-
larity. The granularity of the sequence is that of
its members. For example, thursday , which has a
1438
Type Primitive Constants
Duration second , minute , hour , timeofday , day ,
month , season , quarter , weekend ,
week , year , decade , century , temp d
Sequence monday , tuesday , wednesday ,
thursday , friday , saturday , sunday ,
january , february , march , april ,
may , june , july , august , september ,
october , november , december , winter ,
spring , summer , fall , night , morning ,
afternoon , evening
Range ref time
Approximate
reference
present , future , past , unknown
Numeral 1 , 2 , 3 , 1999 , 2000 , 2001 , . . .
Table 1: The types and primitive logical constants sup-
ported by the logical language for time.
day granularity, denotes the set of all day-granular
ranges enclosing specific Thursdays. Given a
range R and sequence S, we say that R ? S if
R is a member of S. Given two sequences S and
S
?
we say that S ? S
?
if R ? S implies R ? S
?
.
Approximate Reference An approximate time
relative to the reference time. For example, past
and future. To handle mentions such as ?a while,?
we add the constant unknown .
Numeral An integer, for example, 5 or 1990 .
Numerals are used to denote specific ranges, such
as the year 2001, or to modify a duration?s length.
Functions We also allow for functional types,
for example ?s, r? is assigned to a function that
maps from sequences to ranges. Table 2 lists all
supported functions with example mentions.
Context Dependent Constants To mark places
where context-dependent choices will need to be
made during resolution, we use two placeholder
constants. First, ref time denotes the mention ref-
erence time, which is later set to either the docu-
ment time or a previously resolved mention. Sec-
ond, temp d is used in the shift function to deter-
mine its return granularity, as described in Table 2,
and is later replaced with the granularity of either
the first or second argument of the enclosing shift
function. Section 4.3 describes how these deci-
sions are made.
4 Parsing Time Expressions
We define a three-step derivation to resolve men-
tions to their TIMEX3 value. First, we use a CCG
to generate an initial logical form for the mention.
Next, we apply a set of operations that modify the
one week ago
C N NP\NP
1 week ?x.shift(ref time,?1 ? x, temp d)
N/N
?x.1 ? x
>
N
1 ? week
NP
1 ? week
<
NP
shift(ref time,?1 ? 1 ? week , temp d)
Figure 1: A CCG parse tree for the mention ?one week
ago.? The tree includes forward (>) and backward (<)
application, as well as two type-shifting operations
initial logical form, as appropriate for its context.
Finally, the logical form is resolved to a TIMEX3
value using a deterministic process.
4.1 Combinatory Categorial Grammars
CCG is a linguistically motivated categorial for-
malism for modeling a wide range of language
phenomena (Steedman, 1996; Steedman, 2000). A
CCG is defined by a lexicon and a set of combina-
tors. The lexicon pairs words with categories and
the combinators define how to combine categories
to create complete parse trees.
For example, Figure 1 shows a CCG parse tree
for the phrase ?one week ago.? The parse tree is
read top to bottom, starting from assigning cate-
gories to words using the lexicon. The lexical en-
try ago ` NP\NP : ?x.shift(ref time,?1 ?
x, temp d) for the word ?ago? pairs it with a cate-
gory that has syntactic type NP\NP and seman-
tics ?x.shift(ref time,?1 ? x, temp d). Each
intermediate parse node is then constructed by ap-
plying one of a small set of binary or unary opera-
tions (Steedman, 1996; Steedman, 2000), which
modify both the syntax and semantics. We use
backward (<) and forward (>) application and
several unary type-shifting rules to handle number
combinations. For example, in Figure 1 the cate-
gory of the span ?one week? is combined with the
category of ?ago? using backward application (<).
Parsing concludes with a logical form representing
the meaning of the complete mention.
Hand Engineered Lexicon To parse time ex-
pressions, we use a CCG lexicon that includes 287
manually designed entries, along with automati-
cally generated entries such as numbers and com-
mon formats of dates and times. Figure 2 shows
example entries from our lexicon.
1439
Function Description Example
Operations on durations.
?
?n,?d,d??
Given a duration D and a numeral N , return a duration D
?
that is N times longer than D.
?after three days of questioning?
3? day
some
?d,d?
Given a durationD, returnsD
?
, s.t. D
?
is the result ofD?n
for some n > 1.
?he left for a few days?
some(day)
seq
?d,s?
Given a duration D, generate a sequence S, s.t. S includes
all ranges of type D.
?went to last year?s event?
previous(seq(year), ref time)
Operations for extracting a specific range from a sequence.
this
?s,?r,r??
Given a sequence S and a range R, returns the range R
?
?
S, s.t. there exists a range R
??
where R ? R
??
and R
?
?
R
??
, and the length of R
??
is minimal.
?a meeting this friday?
this(friday, ref time)
next
?s,?r,r??
previous
?s,?r,r??
Given a sequenceS and a rangeR, returns the range R
?
? S
that is the one after/before this(S,R).
?arriving next month?
next(seq(month), ref time)
nearest forward
?s,?r,r??
nearest backward
?s,?r,r??
Given a sequenceS and a rangeR, returns the range R
?
? S
that is closest to R in the forward/backward direction.
?during the coming weekend?
nearest forward(seq(weekend), ref time)
Operations for sequences.
nth
?n,?s,?s,s???
nth
?n,?s,s??
Given a number N , a sequence S and a sequence S
?
, returns
a sequence S
??
? S s.t. for each Q ? S
??
there exists
P ? S
?
and Q is the N -th entry in S that is a sub-interval
of P . For the two-argument version, we use heuristics to
infer the third argument by determining a sequence of higher
granularity that is likely to contain the second argument.
?until the second quarter of the year?
nth(2 , seq(quarter), seq(year))
intersect
?s,?s,s??
Given sequences S, S
?
, where the duration of entries in S is
shorter than these in S
?
, return a sequence S
??
? S, where
for each R ? S
??
there exists R
?
? S
?
s.t. R ? R
?
.
?starts on June 28?
intersect(june,nth(28 , seq(day),
seq(month)))
shift
?r,?d,?d,r???
Given a range R, a duration D, and a duration G, return the
range R
?
, s.t. the starting point of R
?
is moved by the length
of D. R
?
is converted to represent a range of granularity G
by expanding if G has larger granularity, and is undefined if
G has smaller granularity.
?a week ago, we went home?
shift(ref time,?1 ? 1 ? week , temp d)
Operations on numbers.
?
?n,?n,n??
Given two numerals, N
?
and N
??
, returns a numeral N
???
representing their product N
?
?N
??
.
?the battle lasted for one hundred days?
1 ? 100 ? day
+
?n,?n,n??
Given two numerals, N
?
and N
??
, returns a numeral N
???
representing their sum N
?
+ N
??
.
?open twenty four hours?
(20 + 4)? hour
Operations to mark sequences for specific TIMEX3 type annotations.
every
?s,s?
Given a sequence S, returns a sequence with SET temporal
type.
?one dose each day?
every(seq(day))
bc
?s,s?
Convert a year to BC.
?during five hundred BC?
bc(nth(500 , seq(year)))
Table 2: Functional constants used to build logical expressions for representing time.
Manually Designed Entries:
several ` NP/N : ?x.some(x)
this ` NP/N : ?x.this(x, ref time)
each ` NP/N : ?x.every(x)
before ` N\NP/NP :
?x.?y.shift(x,?1 ? y, temp d)
year ` N : year
wednesday ` N : wednesday
?20s ` N : nth(192 , seq(decade))
yesterday ` N : shift(ref time,?1 ? day , temp d)
Automatically Generated Entries:
1992 ` N : nth(1992 , seq(year))
nineteen ninety two ` N : nth(1992 , seq(year))
09:30 ` N : intersect(nth(10 , seq(hour), seq(day)),
nth(31 , seq(minute), seq(hour)))
3rd ` N\N :
?x.intersect(x,nth(3 , seq(day), seq(month)))
Figure 2: Example lexical entries.
4.2 Context-dependent Operations
To correctly resolve mentions to TIMEX3 val-
ues, the system must account for contextual in-
formation from various sources, including previ-
ous mentions in the document, the document cre-
ation time, and the sentence containing the men-
tion. We consider three types of context opera-
tions, each takes as input a logical form z
?
, mod-
ifies it and returns a new logical form z. Each
context-dependent parse y specifies one operator
of each type, which are applied to the logical form
constructed by the CCG grammar, to produce the
final, context-dependent logical form LF(y).
Reference Time Resolution The logical con-
stant ref time is replaced by either dct , repre-
senting the document creation time, or last range,
the last r-typed mention resolved in the document.
For example, consider the mention ?the follow-
ing year?, which is represented using the logical
form next(seq(year), ref time). Within the sen-
tence ?1998 was colder than the following year?,
the resolution of ?the following year? depends on
the previous mention ?1998?. In contrast, in ?The
following year will be warmer?, its resolution de-
pends on the document creation time.
1440
Directionality Resolution If z
?
is s-typed
we modify it to nearest forward(z
?
, ref time),
nearest backward(z
?
, ref time), or z
?
. For ex-
ample, given the sentence ?. . . will be launched
in april?, the mention ?april?, and its logi-
cal form april , we would like to resolve it to
the coming April, and therefore modify it to
nearest forward(april , ref time).
Shifting Granularity Every occurrence of the
logical constant temp d , which is used as an ar-
gument to the function shift (see Table 2), is re-
placed with the granularity of either the first argu-
ment, the origin of the shift, or the second argu-
ment, the delta of the shift. This determines the
final granularity of the output. For example, if the
reference time is 2002-01, the mention ?two years
earlier? would resolve to either a month (since the
reference time is of month granularity) or a year
(since the delta is of year granularity).
4.3 Resolving Logical Forms
For a context-dependent parse y, we compute the
TIMEX3 value TM(y) from the logical form z =
LF(y) with a deterministic step that performs a
single traversal of z. Each primitive logical con-
stant from Table 1 contributes to setting part of the
TIMEX3 value (for example, specifying the day of
the week) and the functional constants in Table 2
dictate transformations on the TIMEX3 values (for
example, shifting forward or backward in time).
2
5 Detection
The detection problem is to take an input docu-
ment D and output a mention set M = {m
i
| i =
1, . . . , n}, where each mention m
i
indexes a spe-
cific phrase in D that delimits a time expression.
Algorithm The detection algorithm considers
all phrases that our CCG grammar ? (Section 4)
can parse, uses a learned classifier to further filter
this set, and finally resolves conflicts between any
overlapping predictions. We use a CKY algorithm
to efficiently determine which phrases the CCG
grammar can parse and only allow logical forms
for which there exists some context in which they
would produce a valid time expression, e.g. rul-
ing out intersect(monday , tuesday). Finally, we
build the set M of non-overlapping mentions us-
ing a step similar to non-maximum suppression:
2
The full details are beyond the scope of this paper, but an
implementation is available on the author?s website.
the mentions are sorted by length (longest first)
and iteratively added to M , as long as they do not
overlap with any mention already in M .
Filtering Model Given a mention m, its docu-
ment D, a feature function ?, the CCG lexicon ?,
and feature weights ?, we use a logistic regression
model to define the probability distribution:
P (t|m,D; ?, ?) =
e
???(m,D,?)
1 + e
???(m,D,?)
where t indicates whether m is a time expression.
Features We use three types of indicator fea-
tures that test properties of the words in and
around the potential mention m.
Context tokens Indicate the presence of a set of
manually specified tokens near the mention. These
include quotations around the mention, the word
?old? after the mention, and prepositions of time
(such as ?in?, ?until?, and ?during?) before.
Part of speech Indicators that pair each word
with its part of speech, as assigned by the Stanford
tagger (Toutanova et al, 2003).
Lexical group Each lexical entry belongs to one
of thirteen manually defined lexical groups which
cluster entries that contribute to the final time ex-
pression similarly. These groups include numbers,
days of the week, months, seasons, etc. For each
group, we include a feature indicating whether the
parse includes a lexical entry from that group.
Determiner dependency Indicates the presence
of a determiner in the mention and whether its par-
ent in the dependency tree (generated by the Stan-
ford parser (de Marneffe et al, 2006)) also resides
within the mention.
Learning Finally, we construct the training data
by considering all spans that (1) the CCG tempo-
ral grammar can parse and (2) are not strict sub-
spans of an annotated mention. All spans that ex-
actly matched the gold labels are used as positive
examples and all others are negatives. Given this
relaxed data, we learn the feature weights ? with
L1-regularization. We set the probability thresh-
old for detecting a time expression by optimizing
the F1 score over the training data.
6 Resolution
The resolution problem is to, given a document D
and a set of mentions M , map each m ? M to
the correct time expression e. Section 4 defined
1441
the space of possible time expression that can be
constructed for an input mention m in the context
of a document D. In general, there will be many
different possible derivations, and we will learn a
model for selecting the best one.
Model Let y be a context-dependent CCG parse,
which includes a parse tree TR(y), a set of context
operations CNTX(y) applied to the logical form
at the root of the tree, a final context-dependent
logical form LF(y) and a TIMEX3 value TM(y).
Define ?(m,D, y) ? R
d
to be a d-dimensional
feature?vector representation and ? ? R
d
to be a
parameter vector. The probability of a parse y for
mention m and document D is:
P (y|m,D; ?,?) =
e
???(m,D,y)
?
y
?
e
???(m,D,y
?
)
The inference problem at test time requires find-
ing the best resolution by solving y
?
(m,D) =
arg max
y
P (y|m,D; ?,?), where the final output
TIMEX3 value is TM(y
?
(m,D)).
Inference We find the best context-dependent
parse y by enumeration, as follows. We first
parse the input mention m with a CKY-style algo-
rithm, following previous work (Zettlemoyer and
Collins, 2005). Due to the short length of time
expressions and the manually constructed lexicon,
we can perform exact inference. Given a parse,
we then enumerate all possible outcomes for the
context resolution operators. In practice, there are
never more than one hundred possibilities.
Features The resolution features test properties
of the linguistic context surrounding the mention
m, relative to the context-dependent CCG parse y.
Governor verb We define the governor verb to be
the nearest ancestor verb in the dependency parse
of any token in m. We include features indicat-
ing the concatenation of the part-of-speech of the
governor verb, its auxiliary verb if present, and
the selected direction resolution operator (see Sec-
tion 4.2). This feature helps to distinguish ?They
met on Friday? from ?They will meet on Friday.?
Temporal offset If the final logical form LF (y)
is a range, we define t to be the time difference
between TM(y) and the reference time. For ex-
ample, if the reference time is 2000-01-10 and the
mention resolves to 2000-01-01, then t is -9 days.
This feature indicates one of eleven bucketed val-
ues for t, including same day, less than a week,
less than a month, etc. It allows the model to en-
code the likely temporal progression of a narrative.
This feature is ignored if the granularity of TM(y)
or the reference time is greater than a year.
Shift granularity The logical constant shift (Ta-
ble 2) takes three arguments: the origin (range),
the delta (duration), and the output granularity
(duration). This indicator feature is the concate-
nation of each argument?s granularity for every
shift in LF (y). It allows the model to determine
whether ?a year ago? refers to a year or a day.
Reference type Let r denote whether the refer-
ence time is the document creation time dct or the
last range last range. Let g
l
and g
r
denote the
granularities of LF (y) and the reference time, re-
spectively. We include features indicating the con-
catenations: r+g
l
, r+g
r
, and r+g
l
+g
r
. Addition-
ally, we include features indicating the concatena-
tion of r with each lexical entry used in the parse
TR(y). These features allow the model to encode
preferences in selecting the correct reference time.
Fine-grained type These features indicate the
fine-grained type of TM(y), such as day of the
month or week of the year. We also include a fea-
ture indicating the concatenation of each of these
features with the direction resolution operator that
was used. These features allow the model to repre-
sent, for example, that minutes of the year are less
likely than days of the month.
Intersections These features indicate the concate-
nation of the granularities of any two sequences
that appear as arguments to an intersect constant.
Learning To estimate the model parameters ?
we assume access to a set of training examples
{(m
i
, d
i
, e
i
) : i = 1, . . . , n}, where each mention
m
i
is paired with a document d
i
and a TIMEX3
value e
i
. We use the AdaGrad algorithm (Duchi
et al, 2011) to optimize the conditional, marginal
log-likelihood of the data. For each mention, we
marginalize over all possible context-dependent
parses, using the predictions from the model on the
previous gold mentions to fill in missing context,
where necessary. After parameter estimation, we
set a probability threshold for retaining a resolved
time expression by optimizing value F1 (see Sec-
tion 8) over the training data.
7 Related Work
Semantic parsers map sentences to logical repre-
sentations of their underlying meaning, e.g., Zelle
1442
and Mooney (1996), Zettlemoyer and Collins
(2005), and Wong and Mooney (2007). Re-
cently, research in this area has focused on learn-
ing for various forms of relatively weak but eas-
ily gathered supervision. This includes learn-
ing from question-answer pairs (Clarke et al,
2010; Liang et al, 2011; Kwiatkowski et al,
2013), from conversational logs (Artzi and Zettle-
moyer, 2011), with distant supervision (Krish-
namurthy and Mitchell, 2012; Cai and Yates,
2013), and from sentences paired with system be-
havior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013b).
Recently, Angeli et al introduced the idea of
learning semantic parsers to resolve time expres-
sions (Angeli et al, 2012) and showed that the ap-
proach can generalize to multiple languages (An-
geli and Uszkoreit, 2013). Similarly, Bethard
demonstrated that a hand-engineered semantic
parser is also effective (Bethard, 2013b). How-
ever, these approaches did not use the semantic
parser for detection and did not model linguistic
context during resolution.
We build on a number of existing algorithmic
ideas, including using CCGs to build meaning
representations (Zettlemoyer and Collins, 2005;
Zettlemoyer and Collins, 2007; Kwiatkowski et
al., 2010; Kwiatkowski et al, 2011), building
derivations to transform the output of the CCG
parser based on context (Zettlemoyer and Collins,
2009), and using weakly supervised parameter up-
dates (Artzi and Zettlemoyer, 2011; Artzi and
Zettlemoyer, 2013b). However, we are the first to
use a semantic parsing grammar within a mention
detection algorithm, thereby avoiding the need to
represent the meaning of complete sentences, and
the first to develop a context-dependent model for
semantic parsing of time expressions.
Time expressions have been extensively stud-
ied as part of the TimeEx task, including 9 teams
who competed in the 2013 TempEval-3 com-
petition (Uzzaman et al, 2013). This line of
work builds on ideas from TimeBank (Puste-
jovsky et al, 2003) and a number of different
formal models for temporal reasoning, e.g. Allen
(1983), Moens and Steedman (1988). In 2013,
HeidelTime (Str?otgen and Gertz, 2013) was the
top performing system. It used deterministic rules
defined over regular expressions to perform both
detection and resolution, and will provide a com-
parison system for our evaluation in Section 9. In
Corpus Doc. Token TimeEx
TempEval-3 (Dev) 256 95,391 1,822
TempEval-3 (Test) 20 6,375 138
WikiWars (Dev) 17 98,746 2,228
WikiWars (Test) 5 19,052 363
Figure 3: Corpus statistics.
general, many different rule-based systems, e.g.
NavyTime (Chambers, 2013) and SUTime (Chang
and Manning, 2012), and learning systems, e.g.
ClearTK (Bethard, 2013a) and MANTime (Filan-
nino et al, 2013), did well for detection. How-
ever, rule-based approaches dominated in resolu-
tion; none of the top performers attempted to learn
to do resolution. Our approach is a hybrid of rule
based and learning, by using latent-variable learn-
ing techniques to estimate CCG parsing and con-
text resolution models from the provided data.
8 Experimental Setup
Data We evaluate performance on the
TempEval-3 (Uzzaman et al, 2013) and Wiki-
Wars (Mazur and Dale, 2010) datasets. Figure 3
shows summary statistics for both datasets. For
the TempEval-3 corpus, we use the given training
and testing set splits. Since the training set
has lower inter-annotator agreement than the
testing set (Uzzaman et al, 2013), we manually
corrected all of the mistakes we found in the
training data.
3
The original training set is denoted
Dev* and the corrected Dev. We report (1)
cross-validation development results on Dev*, (2)
cross-validation development and ablation results
for Dev, and (3) held-out test results after training
with Dev. For WikiWars, we randomly assigned
the data to include 17 training documents (2,228
time expressions) and 5 test documents (363 time
expressions). We use cross-validation on the train-
ing data for development. All cross-validation
experiments used 10 folds.
Implementation Our system was implemented
using the open source University of Washington
Semantic Parsing Framework (Artzi and Zettle-
moyer, 2013a). We used LIBLINEAR (Fan et al,
2008) to learn the detection model.
Parameter Settings We use the same set of pa-
rameters for both datasets, chosen based on devel-
opment experiments. For detection, we set the reg-
ularization parameter to 10 with a stopping crite-
3
We modified the annotations for 18% of the mentions.
This relabeled corpus is available on the author?s website.
1443
System
Strict Detection Relaxed Detection Type Res. Value Resolution
Pre. Rec. F1 Pre. Rec. F1 Acc. F1 Acc. Pre. Rec. F1
D
e
v
*
This work 84.6 83.4 84.0 92.8 91.5 92.1 94.6 87.1 84.0 77.9 76.8 77.4
HeidelTime 83.7 83.4 83.5 91.7 91.4 91.6 95.0 87.0 84.1 77.1 76.8 77.0
D
e
v
This work 92.7 89.6 91.1 97.4 94.1 95.7 97.1 92.9 91.5 89.1 86.1 87.6
Context ablation 92.7 89.3 91.0 97.5 93.9 95.7 97.1 92.9 89.8 87.6 84.3 85.9
HeidelTime 90.2 84.8 87.4 96.5 90.7 93.5 96.1 89.9 88.4 85.3 80.2 82.7
T
e
s
t
This work 86.1 80.4 83.1 94.6 88.4 91.4 93.4 85.4 90.2 85.3 79.7 82.4
HeidelTime 83.9 79.0 81.3 93.1 87.7 90.3 90.9 82.1 86.0 80.1 75.4 77.7
NavyTime 78.7 80.4 79.6 89.4 91.3 90.3 88.9 80.3 78.6 70.3 71.8 71.0
ClearTK 85.9 79.7 82.7 93.8 87.0 90.2 93.3 84.2 71.7 67.3 62.4 64.7
Figure 4: TempEval-3 development and test results, compared to the top systems in the shared task.
System
Strict Detection Relaxed Detection Value Resolution
Pre. Rec. F1 Pre. Rec. F1 Acc. Pre. Rec. F1
D
e
v
This work 90.3 83.0 86.5 98.1 90.1 93.9 87.6 85.9 78.9 82.3
Context ablation 90.9 80.1 85.2 98.2 86.5 92.0 68.5 67.3 59.3 63.0
HeidelTime 86.0 75.3 80.3 95.4 83.5 89.0 90.5 86.3 75.6 80.6
T
e
s
t
This work 87.7 78.8 83.0 97.6 87.6 92.3 84.6 82.5 74.1 78.1
HeidelTime 85.2 79.3 82.1 92.6 86.2 89.3 83.7 77.5 72.1 74.7
Figure 5: WikiWars development and test results.
rion of 0.01. For resolution, we set the learning
rate to 0.25 and ran AdaGrad for 5 iterations. All
features are initialized to have zero weights.
Evaluation Metrics We use the official
TempEval-3 scoring script and report the standard
metrics. We report detection precision, recall and
F1 with relaxed and strict metrics; a gold mention
is considered detected for the relaxed metric if
any of the output candidates overlap with it and is
detected for the strict metric if the extent of any
output candidates matches exactly. For resolution,
we report value accuracy, measuring correctness
of time expressions detected according to the
relaxed metric. We also report value precision,
recall, and F1, which score an expression as
correct if it is both correctly detected (relaxed)
and resolved. For end-to-end performance, value
F1 is the primary metric. Finally, we report
accuracy and F1 for temporal types, as defined in
Section 2, for the TempEval dataset (WikiWars
does not include type labels).
Comparison Systems We compare our system
primarily to HeidelTime (Str?otgen and Gertz,
2013), which is state of the art in the end-to-
end task. For the TempEval-3 dataset, we also
compare to two other strong participants of the
shared task. These include NavyTime (Chambers,
2013), which had the top relaxed detection score,
and ClearTK (Bethard, 2013a), which had the top
strict detection score and type F1 score. We also
include a comparison with Bethard?s synchronous
System Dev* Dev Test
This work 81.8 90.1 82.6
SCFG 77.0 81.6 78.9
Figure 6: TempEval-3 gold mention value accuracy.
context free grammar (SCFG) (Bethard, 2013b),
which is state-of-the-art in the task of resolution
with gold mention boundaries.
9 Results
End-to-end results Figure 4 shows develop-
ment and test results for TempEval-3. Figure 5
shows these numbers for WikiWars. In both
datasets, we achieve state-of-the-art test scores.
For detection, we show up to 3-point improve-
ments in strict and relaxed F1 scores. These num-
bers outperform all systems participating in the
shared task, which used a variety of techniques in-
cluding hand-engineered rules, CRF tagging mod-
els, and SVMs. For resolution, we show up to
4-point improvements in the value F1 score, also
outperforming participating systems, all of which
used hand-engineered rules for resolution.
Gold Mentions Figure 6 reports development
and test results with gold mentions.
4
Our approach
outperforms the state of the art, SCFG (Bethard,
2013b), which also used a hand engineered gram-
mar, but did not use machine learning techniques.
4
These numbers vary slightly from those reported; we did
not count the document creation times as mentions.
1444
65 70 75 80 85 90
84
86
88
90
92
94
V
a
l
u
e
P
r
e
c
i
s
i
o
n
(
%
)
65 70 75 80 85 90
84
86
88
90
92
94
This work
HeidelTime
TempEval-3 Dev
WikiWars Dev
Value Recall (%)
Figure 7: Value precision vs. recall for 10-fold cross
validation on TempEval-3 Dev and WikiWars Dev.
Precision vs. Recall Our probabilistic model
of time expression resolution allows us to eas-
ily tradeoff precision and recall for end-to-end
performance by varying the resolution probability
threshold. Figure 7 shows the precision vs. recall
of the resolved values from 10-fold cross valida-
tion of TempEval-3 Dev and WikiWars Dev. We
are able to achieve precision at or above 90% with
reasonable recall, nearly 70% for WikiWars and
over 85% for TempEval-3.
Ablation Study Figures 4-5 also show compar-
isons for our system with no context. We ablate
the ability to refer to the context during resolution
by removing contextual information from the res-
olution features and only allowing the document
creation time to be the reference time.
We see an interesting asymmetry in the effect of
modeling context across the two domains. We find
that context is much more important in WikiWars
(19 point difference) than in TempEval (2 point
difference). This result reaffirms the difference in
domains that Str?otgen and Gertz (2012) noted dur-
ing the development of HeidelTime: history arti-
cles have narrative structure that moves back and
forth through time while newspaper text typically
describes events happening near the document cre-
ation time. This difference helps us to understand
why previous learning systems have been able to
ignore context and perform well on newswire text.
Error Analysis To investigate the source of er-
ror, we compute oracle results for resolving gold
mentions over the TempEval-3 Dev dataset. We
found that our system produces a correct candidate
derivation for 96% of the mentions.
We also manually categorized all resolution
errors for end-to-end performance with 10-fold
cross validation of the TempEval-3 Dev dataset,
Error description %
Wrong directionality context operator 34.6
Wrong reference time context operator 15.7
Wrong shifting granularity context operator 14.4
Requires joint reasoning with events 9.2
Cascading error due to wrong detection 7.8
CCG parse error 2.0
Other error 16.3
Figure 8: Resolution errors from 10-fold cross valida-
tion of the TempEval-3 Dev dataset.
shown in Figure 8. The lexicon allows for effec-
tive parsing, contributing to only 2% of the overall
errors. However, context is more challenging. The
three largest categories, responsible for 64.7% of
the errors, were incorrect use of the context oper-
ators. More expressive modeling will be required
to fully capture the complex pragmatics involved
in understanding time expressions.
10 Conclusion
We presented the first context-dependent semantic
parsing system to detect and resolve time expres-
sions. Both models used a Combinatory Catego-
rial Grammar (CCG) to construct a set of possible
temporal meaning representations. This grammar
defined the possible phrases for detection and the
inputs to a context-dependent reasoning step that
was used to construct the output time expression
during resolution. Experiments demonstrated that
our approach outperforms state-of-the-art systems.
In the future, we aim to develop joint models
for reasoning about events and time expressions,
including detection and resolution of temporal re-
lations. We are also interested in testing coverage
in new domains and investigating techniques for
semi-supervised learning and learning with noisy
data. We hypothesize that semantic parsing tech-
niques could help in all of these settings, provid-
ing a unified mechanism for compositional analy-
sis within temporal understanding problems.
Acknowledgments
The research was supported in part by
DARPA under the DEFT program through
the AFRL (FA8750-13-2-0019) and the CSSG
(N11AP20020), and the NSF (IIS-1115966, IIS-
1252835). The authors thank Nicholas FitzGerald,
Tom Kwiatkowski, and Mark Yatskar for helpful
discussions, and the anonymous reviewers for
helpful comments.
1445
References
James F. Allen. 1981. An interval-based representa-
tion of temporal knowledge. In Proceedings of the
7th International Joint Conference on Artificial In-
telligence.
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Gabor Angeli and Jakob Uszkoreit. 2013. Language-
independent discriminative parsing of temporal ex-
pressions. In Proceedings of the Conference of the
Association of Computational Linguistics.
Gabor Angeli, Christopher D Manning, and Daniel Ju-
rafsky. 2012. Parsing time: Learning to interpret
time expressions. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics.
Y. Artzi and L.S. Zettlemoyer. 2011. Bootstrapping se-
mantic parsers from conversations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Y. Artzi and L.S. Zettlemoyer. 2013a. UW SPF: The
University of Washington Semantic Parsing Frame-
work.
Y. Artzi and L.S. Zettlemoyer. 2013b. Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Steven Bethard. 2013a. Cleartk-timeml: A minimalist
approach to tempeval 2013. In Second Joint Confer-
ence on Lexical and Computational Semantics.
Steven Bethard. 2013b. A synchronous context free
grammar for time normalization. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Q. Cai and A. Yates. 2013. Semantic parsing free-
base: Towards open-domain semantic parsing. In
Joint Conference on Lexical and Computational Se-
mantics: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity.
Nathanael Chambers. 2013. Navytime: Event and
time ordering from raw text. In Second Joint Con-
ference on Lexical and Computational Semantics.
Angel X Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proceedings of the 8th International
Conference on Language Resources and Evaluation.
D.L. Chen and R.J. Mooney. 2011. Learning to in-
terpret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
S. Clark and J. R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?552.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Proceedings of the Conference on Com-
putational Natural Language Learning.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Michele Filannino, Gavin Brown, and Goran Nenadic.
2013. Mantime: Temporal expression identification
and normalization in the tempeval-3 challenge. In
Second Joint Conference on Lexical and Computa-
tional Semantics.
D. Goldwasser and D. Roth. 2011. Learning from
natural instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
J. Krishnamurthy and T. Mitchell. 2012. Weakly su-
pervised training of semantic parsers. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
T. Kwiatkowski, L.S. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uni-
fication. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
T. Kwiatkowski, L.S. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical Generalization in CCG
Grammar Induction for Semantic Parsing. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
P. Liang, M.I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Proceedings of the Conference of the Association for
Computational Linguistics.
1446
Pawet Mazur and Robert Dale. 2010. Wikiwars: a new
corpus for research on temporal expressions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15?28.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The timebank corpus. In Corpus
linguistics.
James Pustejovsky, Bob Ingria, Roser Sauri, Jose Cas-
tano, Jessica Littman, Rob Gaizauskas, Andrea Set-
zer, Graham Katz, and Inderjeet Mani. 2005. The
specification language timeml. The language of
time: A reader, pages 545?557.
M. Steedman. 1996. Surface Structure and Interpreta-
tion. The MIT Press.
M. Steedman. 2000. The Syntactic Process. The MIT
Press.
Jannik Str?otgen and Michael Gertz. 2012. Tempo-
ral tagging on different domains: Challenges, strate-
gies, and gold standards. In Proceedings of the Eigth
International Conference on Language Resources
and Evaluation.
Jannik Str?otgen and Michael Gertz. 2013. Multilin-
gual and cross-domain temporal tagging. Language
Resources and Evaluation, 47(2):269?298.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1.
N. Uzzaman, H. Llorens, L. Derczynski, M. Verhagen,
J. Allen, and J. Pustejovsky. 2013. Semeval-2013
task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Proceedings of the
International Workshop on Semantic Evaluation.
Y.W. Wong and R.J. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Conference
of the Association for Computational Linguistics.
J.M. Zelle and R.J. Mooney. 1996. Learning to
parse database queries using inductive logic pro-
gramming. In Proceedings of the National Confer-
ence on Artificial Intelligence.
L.S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artifi-
cial Intelligence.
L.S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
L.S. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proceedings of the Joint Conference
of the Association for Computational Linguistics
and International Joint Conference on Natural Lan-
guage Processing.
1447
Transactions of the Association for Computational Linguistics, 1 (2013) 49?62. Action Editor: Jason Eisner.
Submitted 11/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Weakly Supervised Learning of Semantic Parsers
for Mapping Instructions to Actions
Yoav Artzi and Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{yoav,lsz}@cs.washington.edu
Abstract
The context in which language is used pro-
vides a strong signal for learning to recover
its meaning. In this paper, we show it can be
used within a grounded CCG semantic parsing
approach that learns a joint model of mean-
ing and context for interpreting and executing
natural language instructions, using various
types of weak supervision. The joint nature
provides crucial benefits by allowing situated
cues, such as the set of visible objects, to di-
rectly influence learning. It also enables algo-
rithms that learn while executing instructions,
for example by trying to replicate human ac-
tions. Experiments on a benchmark naviga-
tional dataset demonstrate strong performance
under differing forms of supervision, includ-
ing correctly executing 60% more instruction
sets relative to the previous state of the art.
1 Introduction
The context in which natural language is used pro-
vides a strong signal to reason about its meaning.
However, using such a signal to automatically learn
to understand unrestricted natural language remains
a challenging, unsolved problem.
For example, consider the instructions in Figure 1.
Correct interpretation requires us to solve many sub-
problems, such as resolving all referring expres-
sions to specific objects in the environment (includ-
ing, ?the corner? or ?the third intersection?), disam-
biguating word sense based on context (e.g., ?the
chair? could refer to a chair or sofa), and finding
executable action sequences that satisfy stated con-
straints (such as ?twice? or ?to face the blue hall?).
move forward twice to the chair
?a.move(a) ? dir(a, forward) ? len(a, 2) ?
to(a, ?x.chair(x))
at the corner turn left to face the blue hall
?a.pre(a, ?x.corner(x)) ? turn(a) ? dir(a, left) ?
post(a, front(you, ?x.blue(x) ? hall(x)))
move to the chair in the third intersection
?a.move(a) ? to(a, ?x.sofa(x)) ?
intersect(order(?y.junction(y), frontdist, 3), x)
Figure 1: A sample navigation instruction set, paired
with lambda-calculus meaning representations.
We must also understand implicit requests, for ex-
ample from the phrase ?at the corner,? that describe
goals to be achieved without specifying the specific
steps. Finally, to do all of this robustly without pro-
hibitive engineering effort, we need grounded learn-
ing approaches that jointly reason about meaning
and context to learn directly from their interplay,
with as little human intervention as possible.
Although many of these challenges have been
studied separately, as we will review in Section 3,
this paper represents, to the best of our knowledge,
the first attempt at a comprehensive model that ad-
dresses them all. Our approach induces a weighted
Combinatory Categorial Grammar (CCG), includ-
ing both the parameters of the linear model and a
CCG lexicon. To model complex instructional lan-
guage, we introduce a new semantic modeling ap-
proach that can represent a number of key linguistic
constructs that are common in spatial and instruc-
tional language. To learn from indirect supervision,
we define the notion of a validation function, for
example that tests the state of the agent after in-
terpreting an instruction. We then show how this
function can be used to drive online learning. For
49
that purpose, we adapt the loss-sensitive Perceptron
algorithm (Singh-Miller & Collins, 2007; Artzi &
Zettlemoyer, 2011) to use a validation function and
coarse-to-fine inference for lexical induction.
The joint nature of this approach provides crucial
benefits in that it allows situated cues, such as the
set of visible objects, to directly influence parsing
and learning. It also enables the model to be learned
while executing instructions, for example by trying
to replicate actions taken by humans. In particular,
we show that, given only a small seed lexicon and
a task-specific executor, we can induce high quality
models for interpreting complex instructions.
We evaluate the method on a benchmark naviga-
tional instructions dataset (MacMahon et al, 2006;
Chen & Mooney, 2011). Our joint approach suc-
cessfully completes 60% more instruction sets rel-
ative to the previous state of the art. We also re-
port experiments that vary supervision type, finding
that observing the final position of an instruction ex-
ecution is nearly as informative as observing the en-
tire path. Finally, we present improved results on a
new version of the MacMahon et al (2006) corpus,
which we filtered to include only executable instruc-
tions paired with correct traces.
2 Technical Overview
Task Let S be the set of possible environment
states and A be the set of possible actions. Given
a start state s ? S and a natural language instruc-
tion x, we aim to generate a sequence of actions
~a = ?a1, . . . , an?, with each ai ? A, that performs
the steps described in x.
For example, in the navigation domain (MacMa-
hon et al, 2006), S is a set of positions on a map.
Each state s = (x, y, o) is a triple, where x and y are
integer grid coordinates and o ? {0, 90, 180, 270} is
an orientation. Figure 2 shows an example map with
36 states; the ones we use in our experiments con-
tain an average of 141. The space of possible actions
A is {LEFT, RIGHT,MOVE, NULL}. Actions change
the state of the world according to a transition func-
tion T : A ? S ? S. In our navigation example,
moving forward can change the x or y coordinates
while turning changes the orientation o.
Model To map instructions to actions, we jointly
reason about linguistic meaning and action execu-
tion. We use a weighted CCG grammar to rank pos-
sible meanings z for each instruction x. Section 6
defines how to design such grammars for instruc-
tional language. Each logical form z is mapped to a
sequence of actions ~a with a deterministic executor,
as described in Section 7. The final grounded CCG
model, detailed in Section 6.3, jointly constructs and
scores z and ~a, allowing for robust situated reason-
ing during semantic interpretation.
Learning We assume access to a training set con-
taining n examples {(xi, si,Vi) : i = 1 . . . n}, each
containing a natural language sentence xi, a start
state si, and a validation function Vi. The validation
function Vi : A ? {0, 1} maps an action sequence
~a ? A to 1 if it?s correct according to available su-
pervision, or 0 otherwise. This training data contains
no direct evidence about the logical form zi for each
xi, or the grounded CCG analysis used to construct
zi. We model all these choices as latent variables.
We experiment with two validation functions. The
first, VD(~a), has access to an observable demonstra-
tion of the execution ~ai, a given ~a is valid iff ~a = ~ai.
The second, VSi (~a), only encodes the final state s?i
of the execution of x, therefore ~a is valid iff its final
state is s?i. Since numerous logical forms often ex-
ecute identically, both functions provide highly am-
biguous supervision.
Evaluation We evaluate task completion for sin-
gle instructions on a test set {(xi, si, s?i) : i =
1 . . . n}, where s?i is the final state of an oracle agent
following the execution of xi starting at state si. We
will also report accuracies for correctly interpreting
instruction sequences ~x, where a single error can
cause the entire sequence to fail. Finally, we report
accuracy on recovering correct logical forms zi on a
manually annotated subset of the test set.
3 Related Work
Our learning is inspired by the reinforcement learn-
ing (RL) approach of Branavan et al (2009), and
related methods (Vogel & Jurafsky, 2010), but uses
latent variable model updates within a semantic
parser. Branavan et al (2010) extended their RL ap-
proach to model high-level instructions, which cor-
respond to implicit actions in our domain. Wei et al
(2009) and Kollar et al (2010) used shallow linguis-
tic representations for instructions. Recently, Tellex
50
et al (2011) used a graphical model semantics rep-
resentation to learn from instructions paired with
demonstrations. In contrast, we model significantly
more complex linguistic phenomena than these ap-
proaches, as required for the navigation domain.
Other research has adopted expressive meaning
representations, with differing learning approaches.
Matuszek et al (2010, 2012) describe supervised al-
gorithms that learn semantic parsers for navigation
instructions. Chen and Mooney (2011), Chen (2012)
and Kim and Mooney (2012) present state-of-the-
art algorithms for the navigation task, by training a
supervised semantic parser from automatically in-
duced labels. Our work differs in the use of joint
learning and inference approaches.
Supervised approaches for learning semantic
parsers have received significant attention, e.g. Kate
and Mooney (2006), Wong and Mooney (2007),
Muresan (2011) and Kwiatkowski et al (2010,
2012). The algorithms we develop in this pa-
per combine ideas from previous supervised CCG
learning work (Zettlemoyer & Collins, 2005, 2007;
Kwiatkowski et al, 2011), as we describe in Sec-
tion 4. Recently, various alternative forms of su-
pervision were introduced. Clarke et al (2010),
Goldwasser and Roth (2011) and Liang et al (2011)
describe approaches for learning semantic parsers
from sentences paired with responses, Krishna-
murthy and Mitchell (2012) describe using distant
supervision, Artzi and Zettlemoyer (2011) use weak
supervision from conversational logs and Gold-
wasser et al (2011) present work on unsupervised
learning. We discuss various forms of supervision
that complement these approaches. There has also
been work on learning for semantic analysis tasks
from grounded data, including event streams (Liang
et al, 2009; Chen et al, 2010) and language paired
with visual perception (Matuszek et al, 2012).
Finally, the topic of executing instructions in
non-learning settings has received significant atten-
tion (e.g., Winograd (1972), Di Eugenio and White
(1992), Webber et al (1995), Bugmann et al (2004),
MacMahon et al (2006) and Dzifcak et al (2009)).
4 Background
We use a weighted linear CCG grammar for seman-
tic parsing, as briefly reviewed in this section.
Combinatory Categorial Grammars (CCGs)
CCGs are a linguistically-motivated formalism for
modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by a
lexicon and a set of combinators. The lexicon con-
tains entries that pair words or phrases with cate-
gories. For example, the lexical entry chair ` N :
?x.chair(x) for the word ?chair? in the parse in Fig-
ure 4 pairs it with a category that has syntactic type
N and meaning ?x.chair(x). Figure 4 shows how a
CCG parse builds a logical form for a complete sen-
tence in our example navigation domain. Starting
from lexical entries, each intermediate parse node,
including syntax and semantics, is constructed with
one of a small set of CCG combinators (Steedman,
1996, 2000). We use the application, composition
and coordination combinators, and three others de-
scribed in Section 6.3.
Factored CCG Lexicons Recently, Kwiatkowski
et al (2011) introduced a factored CCG lexicon
representation. Each lexical item is composed of
a lexeme and a template. For example, the entry
chair ` N : ?x.chair(x) would be constructed by
combining the lexeme chair ` [chair], which con-
tains a word paired with logical constants, with the
template ?v.[N : ?x.v(x)], that defines the rest of
the category by abstracting over logical constants.
This approach allows the reuse of common syntactic
structures through a small set of templates. Section 8
describes how we learn such lexical entries.
Weighted Linear CCGs A weighted linear
CCG (Clark & Curran, 2007) ranks the space of
possible parses under the grammar, and is closely
related to several other approaches (Lafferty et al,
2001; Collins, 2004; Taskar et al, 2004). Let x be a
sentence, y be a CCG parse, and GEN(x; ?) be the
set of all possible CCG parses for x given the lexi-
con ?. Define ?(x, y) ? Rd to be a d-dimensional
feature?vector representation and ? ? Rd to be a pa-
rameter vector. The optimal parse for sentence x is
y?(x) = arg max
y?GEN(x;?) ? ? ?(x, y)
and the final output logical form z is the ?-calculus
expression at the root of y?(x). Section 7.2 de-
scribes how we efficiently compute an approxima-
tion to y?(x) within the joint interpretation and exe-
cution model.
51
Supervised learning with GENLEX Previous
work (Zettlemoyer & Collins, 2005) introduced a
function GENLEX(x, z) to map a sentence x and its
meaning z to a large set of potential lexical entries.
These entries are generated by rules that consider the
logical form z and guess potential CCG categories.
For example, the rule p ? N : ?x.p(x) introduces
categories commonly used to model certain types of
nouns. This rule would, for example, introduce the
category N : ?x.chair(x) for any logical form z
that contains the constant chair. GENLEX uses a
small set of such rules to generate categories that
are paired with all possible substrings in x, to create
a large set of lexical entries. The complete learning
algorithm then simultaneously selects a small sub-
set of these entries and estimates parameter values
?. In Section 8, we will introduce a new way of
using GENLEX to learn from different signals that,
crucially, do not require a labeled logical form z.
5 Spatial Environment Modeling
We will execute instructions in an environment, see
Section 2, which has a set of positions. A position
is a triple (x, y, o), where x and y are horizontal and
vertical coordinates, and o ? O = {0, 90, 180, 270}
is an orientation. A position also includes properties
indicating the object it contains, its floor pattern and
its wallpaper. For example, the square at (4, 3) in
Figure 2 has four positions, one per orientation.
Because instructional language refers to objects
and other structures in an environment, we introduce
the notion of a position set. For example, in Figure 2,
the position set D = {(5, 3, o) : o ? O} represents
a chair, while B = {(x, 3, o) : o ? O, x ? [0 . . . 5]}
represents the blue floor. Both sets contain all ori-
entations for each (x, y) pair, thereby representing
properties of regions. Position sets can have many
properties. For example, E, in addition to being a
chair, is also an intersection because it overlaps with
the neighboring halls A and B. The set of possi-
ble entities includes all position sets and a few addi-
tional entries. For example, set C = {(4, 3, 90)} in
Figure 2 represents the agent?s position.
6 Modeling Instructional Language
We aim to design a semantic representation that is
learnable, models grounded phenomena such as spa-
X	 ?y	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
1	 ?
2	 ?
3	 ?
4	 ?
5	 ?
270	 ?90	 ?
0	 ?
180	 ?
C	 ?
D	 ?E	 ?
A	 ?
B	 ?
{ D	 ? E	 ? } (a) chair
?x.chair(x){ A	 ? B	 ?} (b) hall
?x.hall(x)E	 ? (c) the chair
?x.chair(x)C	 ? (d) you
you{ B	 ?} (e) blue hall
?x.hall(x) ? blue(x)
{ E	 ? } (f) chair in the intersection?x.chair(x) ?
intersect(?y.junction(y), x){ A	 ? B	 ? E	 ? } (g) in front of you
?x.in front of(you, x)
Figure 2: Schematic diagram of a map environment
and example of semantics of spatial phrases.
tial relations and object reference, and is executable.
Our semantic representation combines ideas from
Carpenter (1997) and Neo-Davidsonian event se-
mantics (Parsons, 1990) in a simply typed ?-
calculus. There are four basic types: (1) entities e
that are objects in the world, (2) events ev that spec-
ify actions in the world, (3) truth values t, and (4)
meta-entities m, such as numbers or directions. We
also allow functional types, which are defined by in-
put and output types. For example, ?e, t? is the type
of function from entities to truth values.
6.1 Spatial Language Modeling
Nouns and Noun Phrases Noun phrases are
paired with e-type constants that name specific en-
tities and nouns are mapped to ?e, t?-type expres-
sions that define a property. For example, the noun
?chair? (Figure 2a) is paired with the expression
?x.chair(x), which defines the set of objects for
52
which the constant chair returns true. The deno-
tation of this expression is the set {D,E} in Fig-
ure 2 and the denotation of ?x.hall(x) (Figure 2b)
is {A,B}. Also, the noun phrase ?you? (Figure 2d),
which names the agent, is represented by the con-
stant you with denotation C, the agent?s position.
Determiners Noun phrases can also be formed by
combining nouns with determiners that pick out spe-
cific objects in the world. We consider both definite
reference, which names contextually unique objects,
and indefinites, which are less constrained.
The definite article is paired with a logical expres-
sion ? of type ??e, t?, e?,1 which will name a sin-
gle object in the world. For example, the phrase
?the chair? in Figure 2c will be represented by
?x.chair(x) which will denote the appropriate chair.
However, computing this denotation is challenging
when there is perceptual ambiguity, for positions
where multiple chairs are visible. We adopt a sim-
ple heuristic approach that ranks referents based on
a combination of their distance from the agent and
whether they are in front of it. For our example,
from position C our agent would pick the chair E
in front of it as the denotation. The approach dif-
fers from previous, non-grounded models that fail to
name objects when faced with such ambiguity (e.g.,
Carpenter (1997), Heim and Kratzer (1998)).
To model the meaning of indefinite articles, we
depart from the Frege-Montague tradition of us-
ing existential quantifiers (Lewis, 1970; Montague,
1973; Barwise & Cooper, 1981), and instead in-
troduce a new quantifier A that, like ?, has type
??e, t?, e?. For example, the phrase ?a chair? would
be paired with Ax.chair(x) which denotes an arbi-
trary entry from the set of chairs in the world. Com-
puting the denotation for such expressions in a world
will require picking a specific object, without fur-
ther restrictions. This approach is closely related to
Steedman?s generalized Skolem terms (2011).2
Meta Entities We use m-typed terms to represent
non-physical entities, such as numbers (1, 2, etc.)
and directions (left, right, etc.) whose denotations
1Although quantifiers are logical constants with type
??e, t?, e? or ??e, t?, t?, we use a notation similar to that used
for first-order logic. For example, the notation ?x.f(x) repre-
sents the logical expression ?(?x.f(x))
2Steedman (2011) uses generalized Skolem terms as a tool
for resolving anaphoric pronouns, which we do not model.
are fixed. The ability to refer to directions allows
us to manipulate position sets. For example, the
phrase ?your left? is mapped to the logical expres-
sion orient(you, left), which denotes the position
set containing the position to the left of the agent.
Prepositions and Adjectives Noun phrases with
modifiers, such as adjectives and prepositional
phrases are ?e, t?-type expressions that implement
set intersection with logical conjunctions. For ex-
ample in Figure 2, the phrase ?blue hall? is paired
with ?x.hall(x)? blue(x) with denotation {B} and
the phrase ?chair in the intersection? is paired with
?x.chair(x) ? intersect(?y.junction(y), x) with
denotation {E}. Intuitively, the adjective ?blue?
introduces the constant blue and ?in the? adds a
intersect. We will describe the full details of how
these expressions are constructed in Section 6.3.
Spatial Relations The semantic representation al-
lows more complex reasoning over position sets and
the relations between them. For example, the bi-
nary relation in front of (Figure 2g) tests if the
first argument is in front of the second from the point
of view of the agent. Additional relations are used
to model set intersection, relative direction, relative
distance, and relative position by distance.
6.2 Modeling Instructions
To model actions in the world, we adopt Neo-
Davidsonian event semantics (Davidson, 1967; Par-
sons, 1990), which treats events as ev-type primitive
objects. Such an approach allows for a compact lex-
icon where adverbial modifiers introduce predicates,
which are linked by a shared event argument.
Instructional language is characterized by heavy
usage of imperatives, which we model as func-
tions from events to truth values.3 For example, an
imperative such as ?move? would have the mean-
ing ?a.move(a), which defines a set of events that
match the specified constraints. Here, this set would
include all events that involve moving actions.
The denotation of ev-type terms is a sequence
of n instances of the same action. In this way, an
event defines a function ev : s ? s?, where s is
the start state and s? the end state. For example, the
3Imperatives are ?ev, t?-type, much like ?e, t?-type wh-
interrogatives. Both define sets, the former includes actions to
execute, the later defines answers to a question.
53
denotation of ?a.move(a) is the set of move action
sequences {?MOVE1, . . . ,MOVEn? : n ? 1}. Al-
though performing actions often require performing
additional ones (e.g., the agent might have to turn
before being able to move), we treat such actions as
implicit (Section 7.1), and don?t model them explic-
itly within the logical form.
Predicates such as move (seen above) and
turn are introduced by verbs. Events can also
be modified by adverbials, which are intersective,
much like prepositional phrases. For example in the
imperative, logical form (LF) pair:
Imp.: move from the sofa to the chair
LF: ?a.move(a) ? to(a, ?x.chair(x)) ?
from(a, ?y.sofa(y))
Each adverbial phrase provides a constraint, and
changing their order will not change the LF.
6.3 Parsing Instructional Language with CCG
To compose logical expressions from sentences we
use CCG, as described in Section 4. Figures 3 and 4
present a sample of lexical entries and how they are
combined, as we will describe in this section. The
basic syntactic categories are N (noun), NP (noun
phrase), S (sentence), PP (prepositional phrase),
AP (adverbial phrase), ADJ (adjective) and C (a
special category for coordinators).
Type Raising To compactly model syntactic vari-
ations, we follow Carpenter (1997), who argues for
polymorphic typing. We include the more simple, or
lower type, entry in the lexicon and introduce type-
raising rules to reconstruct the other when necessary
at parse time. We use four rules:
PP : g ? N\N : ?f.?x.f(x) ? g(x)
ADJ : g ? N/N : ?f.?x.f(x) ? g(x)
AP : g ? S\S : ?f.?a.f(a) ? g(a)
AP : g ? S/S : ?f.?a.f(a) ? g(a)
where the first three are for prepositional, adjectival
and adverbial modifications, and the fourth models
the fact that adverbials are often topicalized.4 Fig-
ures 3 and 4 show parses that use type-raising rules.
Indefinites As discussed in Section 6.1, we use
a new syntactic analysis for indefinites, follow-
4Using type-raising rules can be particularly useful when
learning from sparse data. For example, it will no longer be
necessary to learn three lexical entries for each adverbial phrase
(with syntax AP , S\S, and S/S).
chair in the corner
N PP/NP NP/N N
?x.chair(x) ?x.?y.intersect(x, y) ?f.Ax.f(x) ?x.corner(x)
>
NP
?x.corner(x)
>
PP
?y.intersect(?x.corner(x), y)
N\N
?f.?y.f(y) ? intersect(?x.chair(x), y)
<
N
?y.chair(y) ? intersect(?x.chair(x), y)
Figure 3: A CCG parse with a prepositional phrase.
ing Steedman (2011). Previous approaches would
build parses such as
with a lamp
PP/NP PP\(PP/NP )/N N
?x.?y.intersect(x, y) ?f.?g.?y.?x.g(x, y) ? f(x) ?x.lamp(x)
>
PP\(PP/NP )
?g.?y.?x.g(x, y) ? lamp(x)
<
PP
?y.?x.intersect(x, y) ? lamp(x)
where ?a? has the relatively complex syntactic cate-
gory PP\(PP/NP )/N and where similar entries
would be needed to quantify over different types
of verbs (e.g., S\(S/NP )/N ) and adverbials (e.g.,
AP\(AP/NP )/N ). Instead, we include a single
lexical entry a ` NP/N : ?f.Ax.f(x) which can
be used to construct the correct meaning in all cases.
7 Joint Parsing and Execution
Our inference includes an execution component and
a parser. The parser maps sentences to logical forms,
and incorporates the grounded execution model. We
first discuss how to execute logical forms, and then
describe the joint model for execution and parsing.
7.1 Executing Logical Expressions
Dynamic Models In spatial environments, such as
the ones in our task, the agent?s ability to observe the
world depends on its current state. Taking this aspect
of spatial environments into account is challenging,
but crucial for correct evaluation.
To represent the agent?s point of view, for each
state s ? S, as defined in Section 2, let Ms be the
state-dependent logical model. A model M consists
of a domain DM,T of objects for each type T and
an interpretation function IM,T : OT ? DM,T ,
where OT is the set of T -type constants. IM,T
maps logical symbols to T -type objects, for exam-
ple, it will map you to the agent?s position. We have
domains for position sets, actions and so on. Fi-
nally, let VT be the set of variables of type T , and
54
facing the lamp go until you reach a chair
AP/NP NP/N N S AP/S NP S\NP/NP NP/N N
?x.?a.pre(a, ?f.?x.f(x) ?x.lamp(x) ?a.move(a) ?s.?a.post(a, s) you ?x.?y.intersect(x, y) ?f.Ax.f(x) ?x.chair(x)
front(you, x))
> >
NP NP
?x.lamp(x) Ax.chair(x)
> >
AP S\NP
?a.pre(a, front(you, ?x.lamp(x))) ?y.intersect(Ax.chair(x), y)
<
S/S S
?f.?a.f(a) ? pre(a, front(you, ?x.lamp(x))) intersect(Ax.chair(x), you)
>
AP
?a.post(a, intersect(Ax.chair(x), you))
S\S
?f.?a.f(a) ? post(a, intersect(Ax.chair(x), you))
<
S
?a.move(a) ? post(a, intersect(Ax.chair(x), you))
>
S
?a.move(a) ? post(a, intersect(Ax.chair(x), you)) ? pre(a, front(you, ?x.lamp(x)))
Figure 4: A CCG parse showing adverbial phrases and topicalization.
AT : VT ?
?
s?S DMs,T be the assignment func-
tion, which maps variables to domain objects.
For each model Ms the domain DMs,ev is a set
of action sequences {?a1, ..., an? : n ? 1}. Each ~a
defines a sequences of states si, as defined in Sec-
tion 6.2, and associated models Msi . The key chal-
lenge for execution is that modifiers of the event will
need to be evaluated under different models from
this sequence. For example, consider the sentence
in Figure 4. To correctly execute, the pre literal, in-
troduced by the ?facing? phrase, it must be evaluated
in the model Ms0 for the initial state s0. Similarly,
the literal including post requires the final model
Msn+1 . Such state dependent predicates, including
pre and post, are called stateful. The list of stateful
predicates is pre-defined and includes event modi-
fiers, as well the ? quantifier, which is evaluated un-
der Ms0 , since definite determiners are assumed to
name objects visible from the start position. In gen-
eral, a logical expression is traversed depth first and
the model is updated every time a stateful predicate
is reached. For example, the two e-type you con-
stants in Figure 4 will be evaluated under different
models: the one within the pre literal under Ms0 ,
and the one inside the post literal under Msn+1 .
Evaluation Given a logical expression l, we can
compute the interpretation IMs0 ,T (l) by recursivelymapping each subexpression to an entry on the ap-
propriate model M .
To reflect the changing state of the agent during
evaluation, we define the function update(~a, pred).
Given an action sequence ~a and a stateful predi-
cate pred, update returns a model Ms, where s
is the state under which the literal containing pred
should be interpreted, either the initial state or one
visited while executing ~a. For example, given the
predicate post and the action sequence ?a1, . . . , an?,
update(?a1, . . . , an?, post) = Msn+1 , where sn+1
the state of the agent following action an. By con-
vention, we place the event variable as the first argu-
ment in literals that include one.
Given a T -type logical expression l and a start-
ing state s0, we compute its interpretation IMs0 ,T (l)recursively, following these three base cases:
? If l is a ? operator of type ?T1, T2? binding vari-
able v and body b, IMs,T (l) is a set of pairs
from DT1 ?DT2 , where DT1 , DT2 ? Ms. For
each object o ? DT1 , we create a pair (o, i)
where i is the interpretation IMs,T2(b) com-
puted under a variable assignment function ex-
tended to map AT2(v) = o.
? If l is a literal c(c1, . . . , cn) with n argu-
ments where c has type P and each ci has
type Pi, IMs,T (l) is computed by first in-
terpreting the predicate c to the function
f = IMs,T (c). In most cases, IMs,T (l) =
f(IMs,P1(c1), . . . , IMs,Pn(cn)). However, if c
is a stateful predicate, such as pre or post, we
instead first retrieve the appropriate new model
Ms? = update(IMs,P1(c1), c), where c1 is the
event argument and IMs,P1(c1) is its interpre-
tation. Then, the final results is IMs,T (l) =
f(IMs? ,P1(c1), . . . , IMs? ,Pn(cn)).
? If l is a T -type constant or variable, IMs,T (l).
The worst case complexity of the process is ex-
ponential in the number of bound variables. Al-
though in practice we observed tractable evaluation
in the majority of development cases we considered,
a more comprehensive and tractable evaluation pro-
cedure is an issue that we leave for future work.
55
Implicit Actions Instructional language rarely
specifies every action required for execution, see
MacMahon (2007) for a detailed discussion in the
maps domain. For example, the sentence in Fig-
ure 4 can be said even if the agent is not facing a
blue hallway, with the clear implicit request that it
should turn to face such a hallway before moving.
To allow our agent to perform implicit actions, we
extend the domain of ev-type variables by allowing
the agent to prefix up to kI action sequences before
each explicit event. For example, in the agent?s po-
sition in Figure 2 (set C), the set of possible events
includes ?MOVEI ,MOVEI , RIGHTI ,MOVE?, which
contains two implicit sequences (marked by I).
Resolving Action Ambiguity Logical forms of-
ten fail to determine a unique action sequences,
due to instruction ambiguity. For example, con-
sider the instruction ?go forward? and the agent state
as specified in Figure 2 (set C). The instruction,
which maps to ?a.move(a) ? forward(a), evalu-
ates to the set containing ?MOVE?, ?MOVE,MOVE?
and ?MOVE,MOVE,MOVE?, as well as five other se-
quences that have implicit prefixes followed by ex-
plicit MOVE actions. To resolve such ambiguity, we
prefer shorter actions without implicit actions. In
the example above, we will select ?MOVE?, which
includes a single action and no implicit actions.
7.2 Joint Inference
We incorporate the execution procedure described
above with a linear weighted CCG parser, as de-
scribed in Section 4, to create a joint model of pars-
ing and execution. Specifically, we execute logi-
cal forms in the current state and observe the result
of their execution. For example, the word ?chair?
can be used to refer to different types of objects, in-
cluding chairs, sofas, and barstools, in the maps do-
mains. Our CCG grammar would include a lexical
item for each meaning, but execution might fail de-
pending on the presence of objects in the world, in-
fluencing the final parse output. Similarly, allowing
implicit actions provides robustness when resolv-
ing these and other ambiguities. For example, an
instruction with the precondition phrase ?from the
chair? might require additional actions to reach the
position with the named object.
To allow such joint reasoning we define an ex-
ecution e to include a parse tree ey and trace e~a,
and define our feature function to be ?(xi, si, e),
where xi is an instruction and si is the start state.
This approach allows joint dependencies: the state
of the world influences how the agent interprets
words, phrases and even complete sentences, while
language understanding determines actions.
Finally, to execute sequences of instructions, we
execute each starting from the end state of the previ-
ous one, using a beam of size ks.
8 Learning
Figure 5 presents the complete learning algorithm.
Our approach is online, considering each example in
turn and performing two steps: expanding the lex-
icon and updating parameters. The algorithm as-
sumes access to a training set {(xi, si,Vi) : i =
1 . . . n}, where each example includes an instruction
xi, starting state si and a validation function Vi, as
defined in Section 2. In addition the algorithm takes
a seed lexicon ?0. The output is a joint model, that
includes a lexicon ? and parameters ?.
Coarse Lexical Generation To generate po-
tential lexical entries we use the function
GENLEX(x, s,V; ?, ?), where x is an in-
struction, s is a state and V is a validation function.
? is the current lexicon and ? is a parameter vector.
In GENLEX we use coarse logical constants,
as described below, to efficiently prune the set of
potential lexical entries. This set is then pruned
further using more precise inference in Step 1.
To compute GENLEX , we initially generate a
large set of lexical entries and then prune most of
them. The full set is generated by taking the cross
product of a set of templates, computed by factor-
ing out all templates in the seed lexicon ?0, and all
logical constants. For example, if ?0 has a lexical
item with the categoryAP/NP : ?x.?a.to(a, x) we
would create entries w ` AP/NP : ?x.?a.p(a, x)
for every phrase w in x and all constants p with the
same type as to.5
In our development work, this approach often
generated nearly 100k entries per sentence. To ease
5Generalizing previous work (Kwiatkowski et al, 2011), we
allow templates that abstract subsets of the constants in a lex-
ical item. For example, the seed entry facing ` AP/NP :
?x.?a.pre(a, front(you, x)) would create 7 templates.
56
Inputs: Training set {(xi, si,Vi) : i = 1 . . . n} where xi is a
sentence, si is a state and Vi is a validation function, as de-
scribed in Section 2. Initial lexicon ?0. Number of iterations
T . Margin ?. Beam size k for lexicon generation.
Definitions: Let an execution e include a parse tree ey and
a trace e~a. GEN(x, s; ?) is the set of all possible execu-
tions for the instruction x and state s, given the lexicon ?.
LEX(y) is the set of lexical entries used in the parse tree y.
Let ?i(e) be shorthand for the feature function ?(xi, si, e)
defined in Section 7.2. Define ?i(e, e?) = |?i(e)??i(e?)|1.
GENLEX(x, s,V;?, ?) takes as input an instruction x,
state s, validation function V , lexicon ? and model param-
eters ?, and returns a set of lexical entries, as defined in Sec-
tion 8. Finally, for a set of executions E let MAXVi(E; ?)
be {e|?e? ? E, ??,?i(e?)? ? ??,?i(e)? ? Vi(e~a) = 1}, the
set of highest scoring valid executions.
Algorithm:
Initialize ? using ?0 , ?? ?0
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Lexical generation)
a. Set ?G ? GENLEX(xi, si,Vi; ?, ?), ?? ? ? ?G
b. Let E be the k highest scoring executions from
GEN(xi, si;?) which use at most one entry from ?G
c. Select lexical entries from the highest scoring valid
parses: ?i ? ?e?MAXVi(E;?) LEX(ey)d. Update lexicon: ?? ? ? ?i
Step 2: (Update parameters)
a. Set Gi ?MAXVi(GEN(xi, si; ?); ?)
and Bi ? {e|e ? GEN(xi, si; ?) ? Vi(e~a) 6= 1}
b. Construct sets of margin violating good and bad parses:
Ri ? {g|g ? Gi ?
?b ? Bi s.t. ??,?i(g)? ?i(b)? < ??i(g, b)}
Ei ? {b|b ? Bi ?
?g ? Gi s.t. ??,?i(g)? ?i(b)? < ??i(g, b)}
c. Apply the additive update:
? ? ? + 1|Ri|
?
r?Ri ?i(r)?
1
|Ei|
?
e?Ei ?i(e)
Output: Parameters ? and lexicon ?
Figure 5: The learning algorithm.
the cost of parsing at this scale, we developed a
coarse-to-fine two-pass parsing approach that lim-
its the number of new entries considered. The algo-
rithm first parses with coarse lexical entries that ab-
stract the identities of the logical constants in their
logical forms, thereby greatly reducing the search
space. It then uses the highest scoring coarse parses
to constrain the lexical entries for a final, fine parse.
Formally, we construct the coarse lexicon ?a by
replacing all constants of the same type with a single
newly created, temporary constant. We then parse to
create a set of trees A, such that each y ? A
1. is a parse for sentence x, given the world state
s with the combined lexicon ? ? ?a,
2. scored higher than ey by at least a margin of
?L, where ey is the tree of e, the highest scoring
execution of x, at position s under the current
model, s.t. V(e~a) = 1,
3. contains at most one entry from ?a.
Finally, from each entry l ? {l|l ? ?a ? l ?
y ? y ? A}, we create multiple lexical entries by
replacing all temporary constants with all possible
appropriately typed constants from the original set.
GENLEX returns all these lexical entries, which
will be used to form our final fine-level analysis.
Step 1: Lexical Induction To expand our model?s
lexicon, we use GENLEX to generate candidate
lexical entries and then further refine this set by pars-
ing with the current model. Step 1(a) in Figure 5
uses GENLEX to create a temporary set of po-
tential lexical entries ?G. Steps (b-d) select a small
subset of these lexical entries to add to the current
lexicon ?: we find the k-best executions under the
model, which use at most one entry from ?G, find
the entries used in the best valid executions and add
them to the current lexicon.
Step 2: Parameter Update We use a variant of
a loss-driven perceptron (Singh-Miller & Collins,
2007; Artzi & Zettlemoyer, 2011) for parameter up-
dates. However, instead of taking advantage of a loss
function we use a validation signal. In step (a) we
collect the highest scoring valid parses and all in-
valid parses. Then, in step (b) we construct the set
Ri of valid analyses and Ei of invalid ones, such
that their model scores are not separated by a mar-
gin ? scaled by the number of wrong features (Taskar
et al, 2003). Finally, step (f) applies the update.
Discussion The algorithm uses the validation sig-
nal to drive both lexical induction and parameter
updates. Unlike previous work (Zettlemoyer &
Collins, 2005, 2007; Artzi & Zettlemoyer, 2011),
we have no access to a set of logical constants,
either through the the labeled logical form or the
weak supervision signal, to guide the GENLEX
procedure. Therefore, to avoid over-generating lex-
ical entries, thereby making parsing and learning
intractable, we leverage typing for coarse parsing
to prune the generated set. By allowing a single
57
Oracle SAIL
# of instruction sequences 501 706
# of instruction sequences
with implicit actions
431
Total # of sentences 2679 3233
Avg. sentences per sequence 5.35 4.61
Avg. tokens per sentence 7.5 7.94
Vocabulary size 373 522
Table 1: Corpora statistics (lower-cased data).
new entry per parse, we create a conservative, cas-
cading effect, whereas a lexical entry that is intro-
duced opens the way for many other sentence to be
parsed and introduce new lexical entries. Further-
more, grounded features improve parse selection,
thereby generating higher quality lexical entries.
9 Experimental Setup
Data For evaluation, we use the navigation task
from MacMahon et al (2006), which includes three
environments and the SAIL corpus of instructions
and follower traces. Chen and Mooney (2011) seg-
mented the data, aligned traces to instructions, and
merged traces created by different subjects. The
corpus includes raw sentences, without any form of
linguistic annotation. The original collection pro-
cess (MacMahon et al, 2006) created many unin-
terpretable instructions and incorrect traces. To fo-
cus on the learning and interpretation tasks, we also
created a new dataset that includes only accurate in-
structions labeled with a single, correct execution
trace. From this oracle corpus, we randomly sam-
pled 164 instruction sequences (816 sentences) for
evaluation, leaving 337 (1863 sentences) for train-
ing. This simple effort will allow us to measure the
effects of noise on the learning approach and pro-
vides a resource for building more accurate algo-
rithms. Table 1 compares the two sets.
Features and Parser Following Zettlemoyer and
Collins (2005), we use a CKY parser with a beam
of k. To boost recall, we adopt a two-pass strategy,
which allows for word skipping if the initial parse
fails. We use features that indicate usage of lexical
entries, templates, lexemes and type-raising rules, as
described in Section 6.3, and repetitions in logical
coordinations. Finally, during joint parsing, we con-
sider only parses executable at si as complete.
Seed Lexicon To construct our seed lexicon we la-
beled 12 instruction sequences with 141 lexical en-
Single Sentence Sequence
Final state validation
Complete system 81.98 (2.33) 59.32 (6.66)
No implicit actions 77.7 (3.7) 38.46 (1.12)
No joint execution 73.27 (3.98) 31.51 (6.66)
Trace validation
Complete system 82.74 (2.53) 58.95 (6.88)
No implicit actions 77.64 (3.46) 38.34 (6.23)
No joint execution 72.85 (4.73) 30.89 (6.08)
Table 2: Cross-validation development accuracy and
standard deviation on the oracle corpus.
tries. The sequences were randomly selected from
the training set, so as to include two sequences for
each participant in the original experiment. Fig-
ures 3 and 4 include a sample of our seed lexicon.
Initialization and Parameters We set the weight
of each template indicator feature to the number of
times it is used in the seed lexicon and each repeti-
tion feature to -10. Learning parameters were tuned
using cross-validation on the training set: the mar-
gin ? is set to 1, the GENLEX margin ?L is set to
2, we use 6 iterations (8 for experiments on SAIL)
and take the 250 top parses during lexical genera-
tion (step 1, Figure 5). For parameter update (step
2, Figure 5) we use a parser with a beam of 100.
GENLEX generates lexical entries for token se-
quences up to length 4. ks, the instruction sequence
execution beam, is set to 10. Finally, kI is set to
2, allowing up to two implicit action sequences per
explicit one.
Evaluation Metrics To evaluate single instruc-
tions x, we compare the agent?s end state to a labeled
state s?, as described in Section 2. We use a similar
method to evaluate the execution of instruction se-
quences ~x, but disregard the orientation, since end
goals in MacMahon et al (2006) are defined with-
out orientation. When evaluating logical forms we
measure exact match accuracy.
10 Results
We repeated each experiment five times, shuffling
the training set between runs. For the development
cross-validation runs, we also shuffled the folds. As
our learning approach is online, this allows us to ac-
count for performance variations arising from train-
ing set ordering. We report mean accuracy and stan-
dard deviation across all runs (and all folds).
58
Single Sentence Sequence
Chen and Mooney (2011) 54.4 16.18
Chen (2012) 57.28 19.18
+ additional data 57.62 20.64
Kim and Mooney (2012) 57.22 20.17
Trace validation 65.28 (5.09) 31.93 (3.26)
Final state validation 64.25 (5.12) 30.9 (2.16)
Table 3: Cross-validation accuracy and standard de-
viation for the SAIL corpus.
Table 2 shows accuracy for 5-fold cross-
validation on the oracle training data. We first varied
the validation signal by providing the complete ac-
tion sequence or the final state only, as described in
Section 2. Although the final state signal is weaker,
the results are similar. The relatively large difference
between single sentence and sequence performance
is due to (1) cascading errors in the more difficult
task of sequential execution, and (2) corpus repe-
titions, where simple sentences are common (e.g.,
?turn left?). Next, we disabled the system?s ability
to introduce implicit actions, which was especially
harmful to the full sequence performance. Finally,
ablating the joint execution decreases performance,
showing the benefit of the joint model.
Table 3 lists cross validation results on the SAIL
corpus. To compare to previous work (Chen &
Mooney, 2011), we report cross-validation results
over the three maps. The approach was able to cor-
rectly execute 60% more sequences then the previ-
ous state of the art (Kim & Mooney, 2012). We
also outperform the results of Chen (2012), which
used 30% more training data.6 Using the weaker
validation signal creates a marginal decrease in per-
formance. However, we still outperform all previ-
ous work, despite using weaker supervision. Inter-
estingly, these increases were achieved with a rel-
atively simple executor, while previous work used
MARCO (MacMahon et al, 2006), which supports
sophisticated recovery strategies.
Finally, we evaluate our approach on the held out
test set for the oracle corpus (Table 4). In contrast
to experiments on the Chen and Mooney (2011) cor-
pus, we use a held out set for evaluation. Due to this
discrepancy, all development was done on the train-
ing set only. The increase in accuracy over learning
with the original corpus demonstrates the significant
impact of noise on our performance. In addition to
6This additional training data isn?t publicly available.
Validation Single Sentence Sequence LF
Final state 77.6 (1.14) 54.63 (3.5) 44 (6.12)
Trace 78.63 (0.84) 58.05 (3.12) 51.05 (1.14)
Table 4: Oracle corpus test accuracy and standard
deviation results.
execution results, we also report exact match logi-
cal form (LF) accuracy results. For this purpose, we
annotated 18 instruction sequences (105 sentences)
with logical forms. The gap between execution and
LF accuracy can be attributed to the complexity of
the linguistic representation and redundancy in in-
structions. These results provide a new baseline for
studying learning from cleaner supervision.
11 Discussion
We showed how to do grounded learning of a CCG
semantic parser that includes a joint model of mean-
ing and context for executing natural language in-
structions. The joint nature allows situated cues
to directly influence parsing and also enables algo-
rithms that learn while executing instructions.
This style of algorithm, especially when using the
weaker end state validation, is closely related to re-
inforcement learning approaches (Branavan et al,
2009, 2010). However, we differ on optimization
and objective function, where we aim for minimal
loss. We expect many RL techniques to be useful
to scale to more complex environments, including
sampling actions and using an exploration strategy.
We also designed a semantic representation to
closely match the linguistic structure of instructional
language, combining ideas from many semantic
theories, including, for example, Neo-Davidsonian
events (Parsons, 1990). This approach allowed us to
learn a compact and executable grammar that gen-
eralized well. We expect, in future work, that such
modeling can be reused for more general language.
Acknowledgments
The research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), and the NSF (IIS-1115966).
The authors thank Tom Kwiatkowski, Nicholas
FitzGerald and Alan Ritter for helpful discussions,
David Chen for providing the evaluation corpus, and
the anonymous reviewers for helpful comments.
59
References
Artzi, Y., & Zettlemoyer, L. (2011). Bootstrapping Se-
mantic Parsers from Conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Barwise, J., & Cooper, R. (1981). Generalized Quanti-
fiers and Natural Language. Linguistics and Phi-
losophy, 4(2), 159?219.
Branavan, S., Chen, H., Zettlemoyer, L., & Barzilay, R.
(2009). Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint
Conference of the Association for Computational
Linguistics and the International Joint Conference
on Natural Language Processing.
Branavan, S., Zettlemoyer, L., & Barzilay, R. (2010).
Reading between the lines: learning to map high-
level instructions to commands. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics.
Bugmann, G., Klein, E., Lauria, S., & Kyriacou, T.
(2004). Corpus-based robotics: A route instruc-
tion example. In Proceedings of Intelligent Au-
tonomous Systems.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Chen, D. L. (2012). Fast Online Lexicon Learning for
Grounded Language Acquisition. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics.
Chen, D., Kim, J., & Mooney, R. (2010). Training a mul-
tilingual sportscaster: using perceptual context to
learn language. Journal of Artificial Intelligence
Research, 37(1), 397?436.
Chen, D., & Mooney, R. (2011). Learning to Interpret
Natural Language Navigation Instructions from
Observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S., & Curran, J. (2007). Wide-coverage efficient
statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493?552.
Clarke, J., Goldwasser, D., Chang, M., & Roth, D.
(2010). Driving Semantic Parsing from the
World?s Response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Collins, M. (2004). Parameter estimation for statis-
tical parsing models: Theory and practice of
distribution-free methods. In New Developments
in Parsing Technology.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, 105?148.
Di Eugenio, B., & White, M. (1992). On the Interpre-
tation of Natural Language Instructions. In Pro-
ceedings of the Conference of the Association of
Computational Linguistics.
Dzifcak, J., Scheutz, M., Baral, C., & Schermerhorn, P.
(2009). What to Do and How to Do It: Trans-
lating Natural Language Directives Into Temporal
and Dynamic Logic Representation for Goal Man-
agement and Action Execution. In Proceedings
of the IEEE International Conference on Robotics
and Automation.
Goldwasser, D., Reichart, R., Clarke, J., & Roth, D.
(2011). Confidence Driven Unsupervised Seman-
tic Parsing. In Proceedings of the Association of
Computational Linguistics.
Goldwasser, D., & Roth, D. (2011). Learning from Nat-
ural Instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
Heim, I., & Kratzer, A. (1998). Semantics in Generative
Grammar. Blackwell Oxford.
Kate, R., & Mooney, R. (2006). Using String-Kernels for
Learning Semantic Parsers. In Proceedings of the
Conference of the Association for Computational
Linguistics.
Kim, J., & Mooney, R. J. (2012). Unsupervised PCFG
Induction for Grounded Language Learning with
Highly Ambiguous Supervision. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing.
Kollar, T., Tellex, S., Roy, D., & Roy, N. (2010). Toward
Understanding Natural Language Directions. In
Proceedings of the ACM/IEEE International Con-
ference on Human-Robot Interaction.
Krishnamurthy, J., & Mitchell, T. (2012). Weakly Super-
vised Training of Semantic Parsers. In Proceed-
ings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning.
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., &
Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter of
the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &
Steedman, M. (2010). Inducing probabilistic CCG
grammars from logical form with higher-order
60
unification. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &
Steedman, M. (2011). Lexical Generalization in
CCG Grammar Induction for Semantic Parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Con-
ditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Pro-
ceedings of the International Conference on Ma-
chine Learning.
Lewis, D. (1970). General Semantics. Synthese, 22(1),
18?67.
Liang, P., Jordan, M., & Klein, D. (2009). Learning se-
mantic correspondences with less supervision. In
Proceedings of the Joint Conference of the Asso-
ciation for Computational Linguistics the Interna-
tional Joint Conference on Natural Language Pro-
cessing.
Liang, P., Jordan, M., & Klein, D. (2011). Learning
Dependency-Based Compositional Semantics. In
Proceedings of the Conference of the Association
for Computational Linguistics.
MacMahon, M. (2007). Following Natural Language
Route Instructions. Ph.D. thesis, University of
Texas at Austin.
MacMahon, M., Stankiewics, B., & Kuipers, B. (2006).
Walk the Talk: Connecting Language, Knowl-
edge, Action in Route Instructions. In Proceed-
ings of the National Conference on Artificial Intel-
ligence.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo, L., &
Fox, D. (2012). A Joint Model of Language and
Perception for Grounded Attribute Learning. Pro-
ceedings of the International Conference on Ma-
chine Learning.
Matuszek, C., Fox, D., & Koscher, K. (2010). Follow-
ing directions using statistical machine translation.
In Proceedings of the international conference on
Human-robot interaction.
Matuszek, C., Herbst, E., Zettlemoyer, L. S., & Fox, D.
(2012). Learning to Parse Natural Language Com-
mands to a Robot Control System. In Proceedings
of the International Symposium on Experimental
Robotics.
Montague, R. (1973). The Proper Treatment of Quantifi-
cation in Ordinary English. Approaches to natural
language, 49, 221?242.
Muresan, S. (2011). Learning for Deep Language Under-
standing. In Proceedings of the International Joint
Conference on Artificial Intelligence.
Parsons, T. (1990). Events in the Semantics of English.
The MIT Press.
Singh-Miller, N., & Collins, M. (2007). Trigger-based
language modeling using a loss-sensitive percep-
tron algorithm. In IEEE International Conference
on Acoustics, Speech and Signal Processing.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Steedman, M. (2011). Taking Scope. The MIT Press.
Taskar, B., Guestrin, C., & Koller, D. (2003). Max-
Margin Markov Networks. In Proceedings of
the Conference on Neural Information Processing
Systems.
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning,
C. (2004). Max-Margin Parsing. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee,
A., Teller, S., & Roy, N. (2011). Understanding
Natural Language Commands for Robotic Naviga-
tion and Mobile Manipulation. In Proceedings of
the National Conference on Artificial Intelligence.
Vogel, A., & Jurafsky, D. (2010). Learning to follow nav-
igational directions. In Proceedings of the Con-
ference of the Association for Computational Lin-
guistics.
Webber, B., Badler, N., Di Eugenio, B., Geib, C., Lev-
ison, L., & Moore, M. (1995). Instructions, In-
tentions and Expectations. Artificial Intelligence,
73(1), 253?269.
Wei, Y., Brunskill, E., Kollar, T., & Roy, N. (2009).
Where To Go: Interpreting Natural Directions
Using Global Inference. In Proceedings of the
IEEE International Conference on Robotics and
Automation.
Winograd, T. (1972). Understanding Natural Language.
Cognitive Psychology, 3(1), 1?191.
Wong, Y., & Mooney, R. (2007). Learning Synchronous
Grammars for Semantic Parsing with Lambda Cal-
culus. In Proceedings of the Conference of the As-
sociation for Computational Linguistics.
61
Zettlemoyer, L., & Collins, M. (2005). Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Ar-
tificial Intelligence.
Zettlemoyer, L., & Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical
form. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing.
62
