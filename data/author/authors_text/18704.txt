Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 460?470,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
NLP on Spoken Documents without ASR
Mark Dredze, Aren Jansen, Glen Coppersmith, Ken Church
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
mdredze,aren,coppersmith,Kenneth.Church@jhu.edu
Abstract
There is considerable interest in interdis-
ciplinary combinations of automatic speech
recognition (ASR), machine learning, natu-
ral language processing, text classification and
information retrieval. Many of these boxes,
especially ASR, are often based on consid-
erable linguistic resources. We would like
to be able to process spoken documents with
few (if any) resources. Moreover, connect-
ing black boxes in series tends to multiply er-
rors, especially when the key terms are out-of-
vocabulary (OOV). The proposed alternative
applies text processing directly to the speech
without a dependency on ASR. The method
finds long (? 1 sec) repetitions in speech,
and clusters them into pseudo-terms (roughly
phrases). Document clustering and classi-
fication work surprisingly well on pseudo-
terms; performance on a Switchboard task ap-
proaches a baseline using gold standard man-
ual transcriptions.
1 Introduction
Can we do IR-like tasks without ASR? Information
retrieval (IR) typically makes use of simple features
that count terms within/across documents such as
term frequency (tf) and inverse document frequency
(IDF). Crucially, to compute these features, it is suf-
ficient to count repetitions of a term. In particular,
for many IR-like tasks, there is no need for an au-
tomatic speech recognition (ASR) system to label
terms with phonemes and/or words.
This paper builds on Jansen et al (2010), a
method for discovering terms with zero resources.
This approach identifies long, faithfully repeated
patterns in the acoustic signal. These acoustic repe-
titions often correspond to terms useful for informa-
tion retrieval tasks. Critically, this method does not
require a phonetically interpretable acoustic model
or knowledge of the target language.
By analyzing a large untranscribed corpus of
speech, this discovery procedure identifies a vast
number of repeated regions that are subsequently
grouped using a simple graph-based clustering
method. We call the resulting groups pseudo-terms
since they typically represent a single word or phrase
spoken at multiple points throughout the corpus.
Each pseudo-term takes the place of a word or
phrase in bag of terms vector space model of a text
document, allowing us to apply standard NLP algo-
rithms. We show that despite the fully automated
and noisy method by which the pseudo-terms are
created, we can still successfully apply NLP algo-
rithms with performance approaching that achieved
with the gold standard manual transcription.
Natural language processing tools can play a key
role in understanding text document collections.
Given a large collection of text, NLP tools can clas-
sify documents by category (classification) and or-
ganize documents into similar groups for a high
level view of the collection (clustering). For exam-
ple, given a collection of news articles, these tools
can be applied so that the user can quickly see the
topics covered in the news articles, and organize the
collection to find all articles on a given topic. These
tools require little or no human input (annotation)
and work across languages.
Given a large collection of speech, we would like
460
tools that perform many of the same tasks, allow-
ing the user to understand the contents of the col-
lection while listening to only small portions of the
audio. Previous work has applied these NLP tools
to speech corpora with similar results (see Hazen
et al (2007) and the references therein.) However,
unlike text, which requires little or no preprocess-
ing, audio files are typically first transcribed into
text before applying standard NLP tools. Automatic
speech recognition (ASR) solutions, such as large
vocabulary continuous speech recognition (LVCSR)
systems, can produce an automatic transcript from
speech, but they require significant development ef-
forts and training resources, typically hundreds of
hours of manually transcribed speech. Moreover,
the terms that may be most distinctive in particular
spoken documents often lie outside the predefined
vocabulary of an off-the-shelf LVCSR system. This
means that unlike with text, where many tools can be
applied to new languages and domains with minimal
effort, the equivalent tools for speech corpora often
require a significant investment. This greatly raises
the entry threshold for constructing even a minimal
tool set for speech corpora analysis.
The paper proceeds as follows. After a review
of related work, we describe Jansen et al (2010),
a method for finding repetitions in speech. We
then explain how these repetitions are grouped into
pseudo-terms. Document clustering and classifica-
tion work surprisingly well on pseudo-terms; perfor-
mance on a Switchboard task approaches a baseline
based on gold standard manual transcriptions.
2 Related Work
In the low resource speech recognition regime,
most approaches have focused on coupling small
amounts of orthographically transcribed speech (10s
of hours) with much larger collections of untran-
scribed speech (100s or 1000s of hours) to train ac-
curate acoustic models with semi-supervised meth-
ods (Novotney and Schwartz, 2009). In these ef-
forts, the goal is to reduce the annotation require-
ments for the construction of competent LVCSR sys-
tems. This semi-supervised paradigm was relaxed
even further with the pursuit of self organizing units
(SOUs), phone-like units for which acoustic mod-
els are trained with completely unsupervised meth-
ods (Garcia and Gish, 2006). Even though the move
away from phonetic acoustic models improves the
universality of the architecture, small amounts of or-
thographic transcription are still required to connect
the SOUs with the lexicon.
The segmental dynamic time warping (S-DTW)
algorithm (Park and Glass, 2008) was the first truly
zero resource effort, designed to discover portions of
the lexicon directly by searching for repeated acous-
tic patterns in the speech signal. This work im-
plicitly defined a new direction for speech process-
ing research: unsupervised spoken term discovery,
the entry point of our speech corpora analysis sys-
tem. Subsequent extensions of S-DTW (Jansen et
al., 2010) permit applications to much larger speech
collections, a flexibility that is vital to our efforts.
As mentioned above, the application of NLP
methods to speech corpora have traditionally relied
on high resource ASR systems to provide automatic
word or phonetic transcripts. Spoken document
topic classification has been an application of partic-
ular interest (Hazen et al, 2007), for which the rec-
ognized words or phone n-grams are used to charac-
terize the documents. These efforts have produced
admirable results, with ASR transcript-based per-
formance approached that obtained using the gold
standard manual transcripts. Early efforts to per-
form automatic topic segmentation of speech input
without the aid of ASR systems have been promis-
ing (Malioutov et al, 2007), but have yet to exploit
the full the range of NLP tools.
3 Identifying Matched Regions
Our goal is to identify pairs of intervals within and
across utterances of several speakers that contain
the same linguistic content, preferably meaningful
words or terms.
The spoken term discovery algorithm of Jansen et
al. (2010) efficiently searches the space of
(n
2
)
in-
tervals, where n is the number of speech frames.1
Jansen et al (2010) is based on dotplots (Church and
Helfman, 1993), a method borrowed from bioinfor-
matics for finding repetitions in DNA sequences.
1Typically, each frame represents a 25 or 30 ms window of
speech sampled every 10 ms
461
  
t e x t  p r o c e s s i n g  v s .  s p e e c h  p r o c e s s i n gt
ex
t 
pr
oc
es
si
ng
 v
s.
 s
pe
ec
h 
pr
oc
es
si
ng
Figure 1: An example of a dotplot for the string ?text
processing vs. speech processing? plotted against itself.
The box calls out the repeated substring: ?processing.?
3.1 Acoustic Dotplots
When applied to text, the dotplot construct is re-
markably simple: given character strings s1 and s2,
the dotplot is a Boolean similarity matrix K(s1, s2)
defined as
Kij(s1, s2) = ?(s1[i], s2[j]).
Substrings common to s1 and s2 manifest them-
selves as diagonal line segments in the visualization
of K. Figure 1 shows an example text dotplot where
both s1 and s2 are taken to be the string ?text pro-
cessing vs. speech processing.? The boxed diago-
nal line segment arises from the repeat of the word
?processing,? while the main diagonal line trivially
arises from self-similarity. Thus, the search for line
segments inK off the main diagonal provides a sim-
ple algorithmic means to identify repeated terms of
possible interest, albeit sometimes partial, in a col-
lection of text documents. The challenge is to gen-
eralize these dotplot techniques for application to
speech, an inherently noisy, real-valued data stream.
The strategy is to replace character strings with
frame-based speech representations of the form
x = x1, x2, . . . xN , where each xi ? Rd is a d-
dimensional vector space representation of the ith
overlapping window of the signal. Given vector time
series x = x1, x2, . . . xN and y = y1, y2, . . . yM for
two spoken documents, the acoustic dotplot is the
real-valuedN?M cosine similarity matrixK(x,y)
defined as
Time (s)
Tim
e (s
)
K(qi,qj)
 
 
1 2 3 4 5 6 7 8
1
2
3
4
5
6
7
8
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Figure 2: An example of an acoustic dotplot for 8 seconds
of speech (posteriorgrams) plotted against itself. The box
calls out a repetition of interest.
Kij(x,y) =
1
2
[
1 +
?xi, yj?
?xi??yj?
]
. (1)
Even though the application to speech is a distinctly
noisier endeavor, sequences of frames repeated be-
tween the two audio clips will still produce approx-
imate diagonal lines in the visualization of the ma-
trix. The search for matched regions thus reduces
to the robust search for diagonal line segments in
K, which can be efficiently performed with standard
image processing techniques.
Included in this procedure is the application of a
diagonal median filter of duration ? seconds. The
choice of ? determines an approximate threshold
on the duration of the matched regions discovered.
Large ? values (? 1 sec) will produce a relatively
sparse list of matches corresponding to long words
or short phrases; smaller ? values (< 0.5 sec)
will admit shorter words and syllables that may be
less informative from a document analysis perspec-
tive. Given the approximate nature of the procedure,
shorter ? values also admit less reliable matches.
3.2 Posteriorgram Representation
The acoustic dotplot technique can operate on any
vector time series representation of the speech sig-
nal, including a standard spectrogram. However, at
the individual frame level, the cosine similarities be-
tween frequency spectra of distinct speakers produc-
ing the same phoneme are not guaranteed to be high.
462
Time (s)
Pho
ne
 
 
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
silaa
aeah
aoaw
axaxr
ayb
chd
dheh
elem
ener
eyf
ghh
ihiy
jhk
lm
nng
owoy
pr
ssh
tth
uhuw
vw
yz
zh
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 3: An example of a posteriorgram.
Thus, to perform term discovery across a multi-
speaker corpus, we require a speaker-independent
representation. Phonetic posteriorgrams are a suit-
able choice, as each frame is represented as the pos-
terior probability distribution over a set of speech
sounds given the speech observed at the particular
point in time, which is largely speaker-independent
by construction. Figure 3 shows an example poste-
riorgram for the utterance ?I had to do that,? com-
puted with a multi-layer perceptron (MLP)-based
English phonetic acoustic model (see Section 5 for
details). Each row of the figure represents the pos-
terior probability of the given phone as a function of
time through the utterance and each column repre-
sents the posterior distribution over the phone set at
that particular point in time.
The construction of speaker independent acous-
tic models typically requires a significant amount of
transcribed speech. Our proposed strategy is to em-
ploy a speaker independent acoustic model trained
in a high resource language or domain to interpret
multi-speaker data in the zero resource target set-
ting.2 Indeed, we do not need to know a language
to detect when a word of sufficient length has been
repeated in it.3 By computing cosine similarities
2A similarly-minded approach was taken in Hazen et al
(2007) and extended in Hazen and Margolis (2008), where the
authors use Hungarian phonetic trigrams features to character-
ize English spoken documents for a topic classification task.
3While in this paper our acoustic model is based on our eval-
uation corpus, this is not a requirement of our approach. Future
work will investigate performance of other acoustic models.
of phonetic posterior distribution vectors (as op-
posed to reducing the speech to a one-best phonetic
token sequence), the phone set used need not be
matched to the target language. With this approach,
a speaker-independent model trained on the phone
set of a reference language may be used to perform
speaker independent term discovery in any other.
In addition to speaker independence, the use of
phonetic posteriorgrams introduces representational
sparsity that permits efficient dotplot computation
and storage. Notice that the posteriorgram dis-
played in Figure 3 consists of mostly near-zero val-
ues. Since cosine similarity (Equation 1) between
two frames can only be high if they have significant
mass on the same phone, most comparisons need not
be made. Instead, we can apply a threshold and store
each posteriorgram as an inverted file, performing
inner product multiplies and adds only when they
contribute. Using a grid of approximately 100 cores,
we were able to perform theO(n2) dotplot computa-
tion and line segment search for 60+ hours of speech
(corresponding to a 500 terapixel dotplot) in approx-
imately 5 hours.
Figure 2 displays the posteriorgram dotplot for
8 seconds of speech against itself (i.e., x = y).
The prominent main diagonal line results from self-
similarity, and thus is ignored in the search. The
boxed diagonal line segment results from two dis-
tinct occurrences of the term one million dollars.
The large black boxes in the image result from
long stretches of silence of filled pauses; fortunately,
these are easily filtered with speech activity detec-
tion or simple measures of posteriorgram stability.
4 Creating Pseudo-Terms
Spoken documents will be represented as bags of
pseudo-terms, where pseudo-terms are computed
from acoustic repetitions described in the previous
section. Let M be a set of matched regions (m),
each consisting of a pair of speech intervals con-
tained in the corpus (m = [t(i)1 , t
(i)
2 ], [t
(j)
1 , t
(j)
2 ] indi-
cates the speech from t(i)1 to t
(i)
2 is an acoustic match
to the speech from t(j)1 to t
(j)
2 ). If a particular term
occurs k times, the setM can include as many as
(k
2
)
distinct elements corresponding to that term, so we
require a procedure to group them into clusters. We
call the resulting clusters pseudo-terms since each
463
cluster is a placeholder for a term (word or phrase)
spoken in the collection. Given the match list M
and the pseudo-term clusters, it is relatively straight-
forward to represent spoken documents as bags of
pseudo-terms.
To perform this pseudo-term clustering we repre-
sented matched regions as vertices in a graph with
edges representing similarities between these re-
gions. We employ a graph-clustering algorithm that
extracts connected components. Let G = (V,E) be
an unweighted, undirected graph with vertex set V
and edge set E. Each vi ? V corresponds to a sin-
gle speech interval ([t(i)1 , t
(i)
2 ]) present in M (each
m ?M has a pair of such intervals, so |V | = 2|M|)
and each eij ? E is an edge between vertex vi and
vj .
The set E consists of two types of edges. The
first represents repeated speech at distinct points
in the corpus as determined by the match list M.
The second represents near-identical intervals in the
same utterance (i.e. the same speech) since a sin-
gle interval can show up in several matches in M
and the algorithm in Section 3 explicitly ignores
self-similarity. Given the intervals [t(i)1 , t
(i)
2 ] and
[t(j)1 , t
(j)
2 ] contained in the same utterance and with
corresponding vertices vi, vj ? V , we introduce
an edge eij if fractional overlap fij exceeds some
threshold ? , where fij = max(0, rij) and
rij =
(t(i)2 ? t
(i)
1 ) + (t
(j)
2 ? t
(j)
1 )
max(t(i)2 , t
(j)
2 )?min(t
(i)
1 , t
(j)
1 )
? 1. (2)
From the graph G, we produce one pseudo-term
for each connected component. More sophisticated
edge weighting schemes would likely provide ben-
efit. In particular, we expect improved clustering
by introducing weights that reflect acoustic sim-
ilarity between match intervals, rather than rely-
ing solely upon the term discovery algorithm to
make a hard decision. Such confidence weights
would allow even shorter pseudo-terms to be con-
sidered (by reducing ?) without greatly increasing
false alarms. With such a shift, more sophisticated
graph-clustering mechanisms would be warranted
(e.g. Clauset et al (2004)). We plan to pursue this
in future work.
Counts Terms
5 keep track of
5 once a month
2 life insurance
2 capital punishment
9 paper; newspaper
3 talking to you
Table 1: Pseudo-terms resulting from a graph clustering
of matched regions (? = 0.75, ? = 0.95). Counts indi-
cate the number of times the times the pseudo-terms ap-
pear across 360 conversation sides in development data.
Table 1 contains several examples of pseudo-
terms and the matched regions included in each
group. The orthographic forms are taken from the
transcripts in the data (see Section 5). Note that for
some pseudo-terms, the words match exactly, while
for others, the phrases are distinct but phonetically
similar. However, even in this case, there is often
substantial overlap in the spoken terms.
5 Data
For our experiments we used the Switchboard
Telephone Speech Corpus (Godfrey et al, 1992).
Switchboard is a collection of roughly 2,400 two-
sided telephone conversations with a single partici-
pant per side. Over 500 participants were randomly
paired and prompted with a topic for discussion.
Each conversation belongs to one of 70 pre-selected
topics with the two sides restricted to separate chan-
nels of the audio.
To develop and evaluate our methods, we cre-
ated three data sets from the Switchboard corpus: a
development data set, a held out tuning data
set and an evaluation data set. The develop-
ment data set was created by selecting the six most
commonly prompted topics (recycling, capital pun-
ishment, drug testing, family finance, job benefits,
car buying) and randomly selecting 60 sides of con-
versations evenly across the topics (total 360 con-
versation sides.) This corresponds to 35.7 hours of
audio. Note that each participant contributed at most
one conversation side per topic, so these 360 conver-
sation sides represent 360 distinct speakers. All al-
gorithm development and experimentation was con-
ducted exclusively on the development data.
For the tuning data set, we selected an additional
60 sides of conversations evenly across the same six
topics used for development, for a total of 360 con-
464
versations and 37.5 hours of audio. This data was
used to validate our experiments on the develop-
ment data by confirming the heuristic used to select
algorithmic parameters, as described below. This
data was not used for algorithm development. The
evaluation data set was created once parameters had
been selected for a final evaluation of our methods.
We selected this data by sampling 100 conversation
sides from the next six most popular conversation
topics (family life, news media, public education,
exercise/fitness, pets, taxes), yielding 600 conversa-
tion sides containing 61.6 hours of audio.
In our experiments below, we varied the match
duration ? between 0.6 s and 1.0 s and the overlap
threshold ? between 0.75 and 1.0. We measured the
resulting effects on the number of unique pseudo-
terms generated by the process. In general, de-
creasing ? results in more matched regions increas-
ing the number of pseudo-terms. Similarly, increas-
ing ? forces fewer regions to be merged, increasing
the total number of pseudo-terms. Table 2 shows
how these parameters change the number of pseudo-
terms (features) per document and the average num-
ber of occurrences of each pseudo-term. The user
could tune these parameters to select pseudo-terms
that were long and occurred in many documents. In
the next sections, we consider how these parameters
effect performance of various learning settings.
To provide the requisite speaker independent
acoustic model, we compute English phone pos-
teriorgrams using the multi-stream multi-layer
perceptron-based architecture of Thomas et al
(2009), trained on 300 hours of conversational tele-
phone speech. While this is admittedly a large
amount of supervision, it is important to emphasize
our zero resource term discovery algorithm does not
rely on the phonetic interpretability of this refer-
ence acoustic model. The only requirement is that
the same target language phoneme spoken by dis-
tinct speakers map to similar posterior distributions
over the reference language phoneme set. Thus,
even though we evaluate the system on matched-
language Switchboard data, it can be just as easily
applied to any target language with no language-
specific knowledge or training resources required.4
4The generalization of the speaker independence of acous-
tic models across languages is not well understood. Indeed, the
performance of our proposed system would depend to some ex-
? ? Features Feat. Frequency Feat./Doc.
0.6 0.75 5,809 2.15 34.7
0.6 0.85 23,267 2.22 143.4
0.6 0.95 117,788 2.38 779.8
0.6 1.0 333,816 2.32 2153.4
0.75 0.75 8,236 2.31 52.8
0.75 0.85 18,593 2.36 121.7
0.75 0.95 48,547 2.36 318.2
0.75 1.0 90,224 2.18 546.9
0.85 0.75 5,645 2.52 39.5
0.85 0.85 8,832 2.44 59.8
0.85 0.95 15,805 2.24 98.3
0.85 1.0 24,480 2.10 142.4
1.0 0.75 1,844 2.39 12.3
1.0 0.85 2,303 2.24 14.4
1.0 0.95 3,239 2.06 18.6
1.0 1.0 4,205 1.93 22.7
Table 2: Statistics on the number of features (pseudo-
terms) generated for different settings of the match dura-
tion ? and the overlap threshold ? .
6 Document Clustering
We begin by considering document clustering, a
popular approach to discovering latent structure in
document collections. Unsupervised clustering al-
gorithms sort examples into groups, where each
group contains documents that are similar. A user
exploring a corpus can look at a few documents in
each cluster to gain an overview of the content dis-
cussed in the corpus. For example, clustering meth-
ods can be used on search results to provide quick
insight into the coverage of the returned documents
(Zeng et al, 2004).
Typically, documents are clustered based on a bag
of words representation. In the case of clustering
conversations in our collection, we would normally
obtain a transcript of the conversation and then ex-
tract a bag of words representation for clustering.
The resulting clusters may represent topics, such as
the six topics used in our switchboard data. Such
groupings, available with no topic labeled training
data, can be a valuable tool for understanding the
contents of a speech data collection. We would like
to know if similar clustering results can be obtained
without the use of a manual or automatic transcript.
In our case, we substitute the pseudo-terms discov-
ered in a conversation for the transcript, representing
tent on the phonetic similarity of the target and reference lan-
guage. Unsupervised learning of speaker independent acoustic
models remains an important area of future research.
465
the document as a bag of pseudo-terms instead of ac-
tual words. Can a clustering algorithm achieve sim-
ilar results along topical groups with our transcript-
free representation as it can with a full transcript?
In our experiments, we use the six topic labels
provided by Switchboard as the clustering labels.
The goal is to cluster the data into six balanced
groups according to these topics. While Switch-
board topics are relatively straightforward to iden-
tify since the conversations were prompted with spe-
cific topics, we believe this task can still demon-
strate the effectiveness of our representation relative
to the baseline methods. After all, topic classifica-
tion without ASR is still a difficult task.
6.1 Evaluation Metrics
There are numerous approaches to evaluating clus-
tering algorithms. We consider several methods: Pu-
rity, Entropy and B-Cubed. For a full treatment of
these metrics, see Amigo? et al (2009).
Purity measures the precision of each cluster, i.e.,
how many examples in each cluster belong to the
same true topic. Purity ranges between zero and one,
with one being optimal. While optimal purity can be
obtained by putting each document in its own clus-
ter, we fix the number of clusters in all experiments
so purity numbers are comparable. The purity of a
cluster is defined as the largest percentage of exam-
ples in a cluster that have the same topic label. Purity
of the entire clustering is the average purity of each
cluster:
purity(C,L) =
1
N
?
ci?C
max
lj?L
|ci ? lj | (3)
where C is the clustering, L is the reference label-
ing, and N are the number of examples. Following
this notation, ci is a specific cluster and lj is a spe-
cific true label.
Entropy measures how the members of a cluster
are distributed amongst the true labels. The global
metric is computed by taking the weighted aver-
age of the entropy of the members of each cluster.
Specifically, entropy(C,L) is given by:
?
?
ci?C
Ni
N
?
lj?L
P (ci, lj) log2 P (ci, lj) (4)
where Ni is the number of instances in cluster i,
P (ci, lj) is the probability of seeing label lj in clus-
ter ci and the other variables are defined as above.
B-Cubed measures clustering effectiveness from
the perspective of a user?s inspecting the clustering
results (Bagga and Baldwin, 1998). B-Cubed preci-
sion can be defined as an algorithm as follows: sup-
pose a user randomly selects a single example. She
then proceeds to inspect every other example that
occurs in the same cluster. How many of these items
will have the same true label as the selected exam-
ple (precision)? B-Cubed recall operates in a sim-
ilar fashion, but it measures what percentage of all
examples that share the same label as the selected
example will appear in the selected cluster. Since B-
Cubed averages its evaluation over each document
and not each cluster, it is less sensitive to small er-
rors in large clusters as opposed to many small errors
in small clusters. We include results for B-Cubed
F1, the harmonic mean of precision and recall.
6.2 Clustering Algorithms
We considered several clustering algorithms: re-
peated bisection, globally optimal repeated bisec-
tion, and agglomerative clustering (see Karypis
(2003) for implementation details). Each bisection
algorithm is run 10 times and the optimal clustering
is selected according to a provided criteria function
(no true labels needed). For each clustering method,
we evaluated several criteria functions. Addition-
ally, we considered different scalings of the feature
values (the number of times the pseudo-terms ap-
pear in each document). We found that scaling each
feature by the inverse document frequency, effec-
tively TFIDF, produced the best results, so we use
that scaling in all of our experiments. We also ex-
plored various similarity metrics and found cosine
similarity to be the most effective.
We used the Cluto clustering library for all clus-
tering experiments (Karypis, 2003). In the following
section, we report results for the optimal clustering
configuration based on experiments on the develop-
ment data.
6.3 Baselines
We compared our pseudo-term feature set perfor-
mance to two baselines: (1) Phone Trigrams and
466
(2) Word Transcripts. The Phone Trigram base-
line is derived automatically using an approach sim-
ilar to Hazen et al (2007). This baseline is based
on a vanilla phone recognizer on top of the same
MLP-based acoustic model (see Section 5 and the
references therein for details) used to discover the
pseudo-terms. In particular, the phone posterior-
grams were transformed to frame-level monophone
state likelihoods (through division by the frame-
level priors). These state likelihoods were then used
along with frame-level phone transition probabilities
to Viterbi decode each conversation side. It is impor-
tant to emphasize that the reliability of phone recog-
nizers depends on the phone set matching the appli-
cation language. Using the English acoustic model
in this manner on another language will significantly
degrade the performance numbers reported below.
The Word Transcript baseline starts with Switch-
board transcripts. This baseline serves as an upper
bound of what large vocabulary recognition can pro-
vide for this task. n-gram features are computed
from the transcript. Performance is reported sepa-
rately for unigrams, bigrams and trigrams.
6.4 Results
To optimize parameter settings, match duration (?)
and overlap threshold (? ) were swept over a wide
range (0.6 < ? < 1.0 and 0.75 < ? < 1.0) using a
variety of clustering algorithms and training criteria.
Initial results on development data showed promis-
ing performance for the default I2 criteria in Cluto
(repeated bisection set to maximize the square root
of within cluster similarity). Representative results
on development data with various parameter settings
for this clustering configuration appear in Table 3.
A few observations about results on development
data. First, the three evaluation metrics are strongly
correlated. Second, for each ? the same narrow
range of ? values achieve good results. In general,
settings of ? > 0.9 were all comparable. Essen-
tially, setting a high threshold for merging matched
regions was sufficient without further tuning. Third,
we observed that decreasing ? meant more features,
but that these additional features did not necessarily
lead to more useful features for clustering. For ex-
ample, ? = 0.70 gave a small number of reasonably
good features, while ? = 0.60 can give an order of
magnitude more features without much of a change
Pseudo-term Results
? ? Features Purity Entropy B3 F1
0.60 0.95 117,788 0.9639 0.2348 0.9306
0.60 0.96 143,299 0.9750 0.1664 0.9518
0.60 0.97 178,559 0.9667 0.2116 0.9366
0.60 0.98 223,511 0.9528 0.2717 0.9133
0.60 0.99 333,630 0.9583 0.2641 0.9210
0.60 1.0 333,816 0.9583 0.2641 0.9210
0.70 0.93 58,303 0.9528 0.3114 0.9105
0.70 0.94 66,054 0.9667 0.2255 0.9358
0.70 0.95 74,863 0.9583 0.2669 0.9210
0.70 0.96 86,070 0.9611 0.2529 0.9260
0.70 0.97 100,623 0.9639 0.2326 0.9312
0.70 0.98 117,535 0.9556 0.2821 0.9158
0.70 0.99 161,219 0.9056 0.4628 0.8372
0.70 1.0 161,412 0.9333 0.4011 0.8760
Phone Recognizer Baseline
Type Features Purity Entropy B3 F1
Phone Trigram 28,110 0.6194 1.3657 0.5256
Manual Word Transcript Baselines
Type Features Purity Entropy B3 F1
Word Unigram 7,330 0.9917 0.0559 0.9839
Word Bigram 74,216 0.9833 0.1111 0.9678
Word Trigram 224,934 0.9889 0.0708 0.9787
Table 3: Clustering results on development data using
globally optimal repeated bisection and I2 criteria. The
best results over the manual word transcript baselines
and for each match duration (?) are highlighted in bold.
Pseudo-term results are better than the phonetic baseline
and almost as good as the transcript baseline.
in clustering performance. Finally, while pseudo-
term results are not as good as with the manual
transcripts (unigrams), they achieve similar results.
Compared with the phone trigram features deter-
mined by the phone recognizer output, the pseudo-
terms perform significantly better. Note that these
two automatic approaches were built using the iden-
tical MLP-based phonetic acoustic model.
We sought to select the optimal parameter settings
for running on the evaluation data using the devel-
opment data and the held out tuning data. We de-
fined the following heuristic to select the optimal pa-
rameters. We choose settings for ?, ? and the clus-
tering parameters that independently maximize the
performance averaged over all runs on development
data. We then selected the single run correspond-
ing to these parameter settings and checked the re-
sult on the held out tuning data. This setting was
also the best performer on the held out set, so we
used these parameters for evaluation. The best per-
forming parameters were globally optimal repeated
467
? ? Features Purity Entropy B3 F1
0.70 0.98 123,901 0.9778 0.1574 0.9568
Phone Trigram 28,374 0.6389 1.2345 0.5513
Word Unigram 7,640 0.9972 0.0204 0.9945
Word Bigram 77,201 0.9972 0.0204 0.9945
Word Trigram 233,744 0.9972 0.0204 0.9945
Table 4: Results on held out tuning data. The parameters
(globally optimal repeated bisection clustering with I2
criteria, ? = 0.70 seconds and ? = 0.98) were selected
using the development data and validated on tuning data.
Note that the clusters produced by each manual transcript
test were identical in this case.
? ? Features Purity Entropy B3 F1
0.70 0.98 279,239 0.9517 0.3366 0.9073
Phone Trigram 31,502 0.7000 1.0496 0.6355
Word Unigram 9939 0.9883 0.0831 0.9772
Word Bigram 110,859 0.9883 0.0910 0.9771
Word Trigram 357,440 0.9900 0.0775 0.9803
Table 5: Results on evaluation data. The parameters
(globally optimal repeated bisection clustering with I2
criteria, ? = 0.7 seconds and ? = 0.98) were selected
using the development data and validated on tuning data.
bisection clustering with I2 criteria, ? = 0.7 s and
? = 0.98. Note that examining Table 3 alone may
suggest other parameters, but we found our selection
method to yield optimal results on the tuning data.
Results on held out tuning and evaluation data for
this setting compared to the manual word transcripts
and phone recognizer output are shown in Tables
4 and 5. On both the tuning data and evaluation
data, we obtain similar results as on the development
data. While the manual transcript baseline is bet-
ter than our pseudo-term representations, the results
are quite competitive. This demonstrates that use-
ful clustering results can be obtained without a full-
blown word recognizer. Notice also that the pseudo-
term performance remains significantly higher than
the phone recognizer baseline on both sets.
7 Supervised Document Classification
Unsupervised clustering methods are attractive since
they require no human annotations. However, ob-
taining a few labeled examples for a simple label-
ing task can be done quickly, especially with crowd
sourcing systems such as CrowdFlower and Ama-
zon?s Mechanical Turk (Snow et al, 2008; Callison-
Burch and Dredze, 2010). In this setting, a user
may listen to a few conversations and label them by
topic. A supervised classification algorithm can then
be trained on these labeled examples and used to au-
tomatically categorize the rest of the data. In this
section, we evaluate if supervised algorithms can be
trained using the pseudo-term representation of the
speech.
We set up a multi-class supervised classification
task, where each document is labeled using one of
the six Switchboard topics. A supervised learning
algorithm is trained on a sample of labeled docu-
ments and is then asked to label some test data. Re-
sults are measured in terms of accuracy. Since the
documents are a balanced sample of the six topics,
random guessing would yield an accuracy of 0.1667.
We proceed as with the clustering experiments.
We evaluate different representations for various set-
tings of ? and ? and different classifier parameters
on the development data. We then select the opti-
mal parameter settings and validate this selection on
the held out tuning data, before generating the final
representations for the evaluation once the optimal
parameters have been selected.
For learning we require a multi-class classifier
training algorithm. We evaluated four popular
learning algorithms: a) MIRA?a large margin on-
line learning algorithm (Crammer et al, 2006); b)
Confidence Weighted (CW) learning?a probabilis-
tic large margin online learning algorithm (Dredze
et al, 2008; Crammer et al, 2009); c) Maxi-
mum Entropy?a log-linear discriminative classi-
fier (Berger et al, 1996); and d) Support Vec-
tor Machines (SVM)?a large margin discriminator
(Joachims, 1998).5 For each experiment, we used
default settings of the parameters (tuning did not sig-
nificantly change the results) and 10 online iterations
for the online methods (MIRA, CW). Each reported
result is based on 10-fold cross validation.
Table 6 shows results for various parameter set-
tings and the four learning algorithms on develop-
ment data. As before, we observe that values for
? > 0.9 tend to do well. The CW learning algo-
rithm performs the best on this data, followed by
Maximum Entropy, MIRA and SVM. The optimal
? for classification is 0.75, close to the 0.7 value
selected in clustering. As before, pseudo-terms do
5We used the ?variance? formulation with k = 1 for CW
learning, Gaussian regularization for the Maximum Entropy
classifier, and a linear kernel for the SVM.
468
? ? MaxEnt SVM CW MIRA
0.60 0.99 0.8972 0.6944 0.8667 0.8972
0.60 1.0 0.8972 0.6944 0.8639 0.8944
0.70 0.97 0.9000 0.7722 0.8500 0.8056
0.70 0.98 0.8806 0.7417 0.8917 0.8167
0.70 0.99 0.9000 0.6556 0.9194 0.9056
0.70 1.0 0.8917 0.6556 0.9194 0.9083
0.75 0.94 0.8778 0.7806 0.8639 0.8056
0.75 0.95 0.8778 0.7694 0.8889 0.8111
0.75 0.96 0.9028 0.7778 0.9000 0.8778
0.75 0.97 0.9111 0.7722 0.9250 0.9278
0.75 0.98 0.9056 0.7417 0.9194 0.9167
0.85 0.85 0.8639 0.7833 0.8500 0.8167
0.85 0.90 0.8611 0.7528 0.8611 0.8583
0.85 0.91 0.8389 0.7500 0.8722 0.8556
0.85 0.92 0.8528 0.7222 0.8944 0.8556
Phone Trigram 0.6111 0.7139 0.9138 0.5000
Word Unigram 0.9472 0.8861 0.9861 0.9306
Word Bigram 0.9250 0.8833 0.9917 0.9278
Word Trigram 0.9278 0.8611 0.9889 0.9222
Table 6: The top 15 results (measured as average accu-
racy across the 4 algorithms) for pseudo-terms on de-
velopment data. The best pseudo-term and manual tran-
script results for each algorithm are bolded. All results
are based on 10-fold cross validation. Pseudo-term re-
sults are better than the phonetic baseline and almost as
good as the transcript baseline.
well, though not as well as the upper bound based
on manual transcripts. The performance for pseudo-
terms and phone trigrams are roughly comparable,
though we expect pseudo-terms to be more robust
across languages.
Using the same selection heuristic as in cluster-
ing, we select the optimal parameter settings, vali-
date them on the held out tuning data, and compute
results on evaluation data. The best performing con-
figuration was for ? = 0.75 seconds and ? = 0.97.
Notice these parameters are very similar to the best
parameters selected for clustering. Results on held
out tuning and evaluation data for this setting com-
pared to the manual transcripts are shown in Tables
7 and 8. As with clustering, we see good overall
performance as compared with manual transcripts.
While the performance drops, results suggest that
useful output can be obtained without a transcript.
8 Conclusions
We have presented a new strategy for applying stan-
dard NLP tools to speech corpora without the aid
of a large vocabulary word recognizer. Built in-
stead on top of the unsupervised discovery of term-
? ? MaxEnt SVM CW MIRA
0.75 0.97 0.8722 0.7389 0.8972 0.8750
Phone Trigram 0.7167 0.6972 0.9056 0.5083
Word Unigram 0.9500 0.9056 0.9806 0.9250
Word Bigram 0.9444 0.9111 0.9833 0.9250
Word Trigram 0.9417 0.8972 0.9778 0.9250
Table 7: Results on held out tuning data. The parameters
(? = 0.75 seconds and ? = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
? ? MaxEnt SVM CW MIRA
0.75 0.97 0.8683 0.7167 0.7850 0.7150
Phone Trigram 0.8600 0.7750 0.9183 0.6233
Word Unigram 0.9533 0.9317 0.9850 0.9267
Word Bigram 0.9467 0.9200 0.9900 0.9367
Word Trigram 0.9383 0.9233 0.9817 0.9367
Table 8: Results on evaluation data. The parameters
(? = 0.75 seconds and ? = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
like units in the speech, we perform unsupervised
topic clustering as well as supervised classification
of spoken documents with performance approaching
that achieved with the manual word transcripts, and
generally matching or exceeding that achieved with
a phonetic recognizer. Our study identified several
opportunities and challenges in the development of
NLP tools for spoken documents that rely on little
or no linguistic resources such as dictionaries and
training corpora.
References
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4).
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the 17th international conference on
Computational linguistics-Volume 1, pages 79?85. As-
sociation for Computational Linguistics.
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
469
Turk. In Workshop on Creating Speech and Language
Data With Mechanical Turk at NAACL-HLT.
K. W. Church and J. I. Helfman. 1993. Dotplot: A
program for exploring self-similarity in millions of
lines of text and code. Journal of Computational and
Graphical Statistics.
Aaron Clauset, Mark E J Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Physical Review E, 70.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
Koby Crammer, Mark Dredze, and Alex Kulesza. 2009.
Multi-class confidence weighted algorithms. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
ICASSP.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In ICASSP.
Timothy J. Hazen and Anna Margolis. 2008. Discrimi-
native feature weighting using MCE training for topic
identification of spoken audio recordings. In ICASSP.
Timothy J. Hazen, Fred Richardson, and Anna Margo-
lis. 2007. Topic identification from audio recordings
using word and phone recognition lattices. In IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding.
Aren Jansen, Kenneth Church, and Hynek Hermansky.
2010. Towards spoken term discovery at scale with
zero resources. In Interspeech.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learning
(ECML).
George Karypis. 2003. CLUTO: A software package for
clustering high-dimensional data sets. Technical Re-
port 02-017, University of Minnesota, Dept. of Com-
puter Science.
Igor Malioutov, Alex Park, Regina Barzilay, and James
Glass. 2007. Making Sense of Sound: Unsupervised
Topic Segmentation Over Acoustic Input. In ACL.
Scott Novotney and Richard Schwartz. 2009. Analysis
of low-resource acoustic model self-training. In Inter-
speech.
Alex Park and James R. Glass. 2008. Unsupervised pat-
tern discovery in speech. IEEE Transactions of Audio,
Speech, and Language Processing.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: Evaluating non-
expert annotations for natural language tasks. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 254?263. Asso-
ciation for Computational Linguistics.
S. Thomas, S. Ganapathy, and H. Hermansky. 2009.
Phoneme recognition using spectral envelope and
modulation frequency features. In Proc. of ICASSP.
H.J. Zeng, Q.C. He, Z. Chen, W.Y. Ma, and J. Ma. 2004.
Learning to cluster web search results. In Conference
on Research and development in information retrieval
(SIGIR).
470
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 186?196,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Inferring User Political Preferences from Streaming Communications
Svitlana Volkova,
1
Glen Coppersmith
2
and Benjamin Van Durme
1,2
1
Center for Language and Speech Processing,
2
Human Language Technology Center of Excellence,
Johns Hopkins University, Baltimore, MD 21218
svitlana@jhu.edu, coppersmith@jhu.edu, vandurme@cs.jhu.edu
Abstract
Existing models for social media per-
sonal analytics assume access to thou-
sands of messages per user, even though
most users author content only sporadi-
cally over time. Given this sparsity, we:
(i) leverage content from the local neigh-
borhood of a user; (ii) evaluate batch mod-
els as a function of size and the amount
of messages in various types of neighbor-
hoods; and (iii) estimate the amount of
time and tweets required for a dynamic
model to predict user preferences. We
show that even when limited or no self-
authored data is available, language from
friend, retweet and user mention commu-
nications provide sufficient evidence for
prediction. When updating models over
time based on Twitter, we find that polit-
ical preference can be often be predicted
using roughly 100 tweets, depending on
the context of user selection, where this
could mean hours, or weeks, based on the
author?s tweeting frequency.
1 Introduction
Inferring latent user attributes such as gender, age,
and political preferences (Rao et al, 2011; Za-
mal et al, 2012; Cohen and Ruths, 2013) auto-
matically from personal communications and so-
cial media including emails, blog posts or public
discussions has become increasingly popular with
the web getting more social and volume of data
available. Resources like Twitter
1
or Facebook
2
become extremely valuable for studying the un-
derlying properties of such informal communica-
tions because of its volume, dynamic nature, and
diverse population (Lunden, 2012; Smith, 2013).
1
http://www.demographicspro.com/
2
http://www.wolframalpha.com/facebook/
The existing batch models for predicting latent
user attributes rely on thousands of tweets per
author (Rao et al, 2010; Conover et al, 2011;
Pennacchiotti and Popescu, 2011a; Burger et al,
2011; Zamal et al, 2012; Nguyen et al, 2013).
However, most Twitter users are less prolific than
those examined in these works, and thus do not
produce the thousands of tweets required to obtain
their levels of accuracy e.g., the median number of
tweets produced by a random Twitter user per day
is 10. Moreover, recent changes to Twitter API
querying rates further restrict the speed of access
to this resource, effectively reducing the amount of
data that can be collected in a given time period.
In this paper we analyze and go beyond static
models formulating personal analytics in social
media as a streaming task. We first evaluate batch
models that are cognizant of low-resource predic-
tion setting described above, maximizing the effi-
ciency of content in calculating personal analytics.
To the best of our knowledge, this is the first work
that makes explicit the tradeoff between accuracy
and cost (manifest as calls to the Twitter API),
and optimizes to a different tradeoff than state-of-
the-art approaches, seeking maximal performance
when limited data is available. In addition, we
propose streaming models for personal analytics
that dynamically update user labels based on their
stream of communications which has been ad-
dressed previously by Van Durme (2012b). Such
models better capture the real-time nature of evi-
dence being used in latent author attribute predic-
tions tasks. Our main contributions include:
- develop low-resource and real-time dynamic
approaches for personal analytics using as an
example the prediction of political preference
of Twitter users;
- examine the relative utility of six different
notions of ?similarity? between users in an
implicit Twitter social network for personal
analytics;
186
- experiments are performed across multiple
datasets supporting the prediction of politi-
cal preference in Twitter, to highlight the sig-
nificant differences in performance that arise
from the underlying collection and annota-
tion strategies.
2 Identifying Twitter Social Graph
Twitter users interact with one another and en-
gage in direct communication in different ways
e.g., using retweets, user mentions e.g., @youtube
or hashtags e.g., #tcot, in addition to having ex-
plicit connections among themselves such as fol-
lowing, friending. To investigate all types of social
relationships between Twitter users and construct
Twitter social graphs we collect lists of followers
and friends, and extract user mentions, hashtags,
replies and retweets from communications.
3
2.1 Social Graph Definition
Lets define an attributed, undirected graph G =
(V,E), where V is a set of vertices and E is a set
of edges. Each vertex v
i
represents someone in
a communication graph i.e., communicant: here
a Twitter user. Each vertex is attributed with a
feature vector
~
f(v
i
) which encodes communica-
tions e.g., tweets available for a given user. Each
vertex is associated with a latent attribute a(v
i
),
in our case it is binary a(v
i
) ? {D,R}, where
D stands for Democratic and R for Republican
users. Each edge e
ij
? E represents a connec-
tion between v
i
and v
j
, e
ij
= (v
i
, v
j
) and defines
different social circles between Twitter users e.g.,
follower (f), friend (b), user mention (m), hash-
tag (h), reply (y) and retweet (w). Thus, E ?
V
(2)
?{f, b, h,m,w, y}. We denote a set of edges
of a given type as ?
r
(E) for r ? {f, b, h,m,w, y}.
We denote a set of vertices adjacent to v
i
by so-
cial circle type r as N
r
(v
i
) which is equivalent to
{v
j
| e
ij
? ?
r
(E)}. Following Filippova (2012)
we refer to N
r
(v
i
) as v
i
?s social circle, otherwise
known as a neighborhood. In most cases, we only
work with a sample of a social circle, denoted by
N
?
r
(v
i
) where |N
?
r
(v
i
)| = k is its size for v
i
.
Figure 1 presents an example of a social graph
derived from Twitter. Notably, users from differ-
ent social circles can be shared across the users of
the same or different classes e.g., a user v
j
can be
3
The code and detailed explanation on how we col-
lected all six types of user neighbors and their com-
munications using Twitter API can be found here:
http://www.cs.jhu.edu/ svitlana/
Figure 1: An example of a social graph with follower, friend,
@mention, reply, retweet and hashtag social circles for each
user of interest e.g., blue: Democratic, red: Republican.
in both follower circle v
j
? N
f
(v
i
), v
i
? D and
retweet circle v
j
? N
w
(v
k
), v
k
? R.
2.2 Candidate-Centric Graph
We construct candidate-centric graph G
cand
by
looking into following relationships between the
users and Democratic or Republican candidates
during the 2012 US Presidential election. In the
Fall of 2012, leading up to the elections, we ran-
domly sampled n = 516 Democratic and m =
515 Republican users. We labeled users as Demo-
cratic if they exclusively follow both Democratic
candidates
4
? BarackObama and JoeBiden but
do not follow both Republican candidates ? Mit-
tRomney and RepPaulRyan and vice versa. We
collectively refer to D and R as our ?users of in-
terest? for which we aim to predict political prefer-
ence. For each such user we collect recent tweets
and randomly sample their immediate k = 10
neighbors from follower, friend, user mention, re-
ply, retweet and hashtag social circles.
2.3 Geo-Centric Graph
We construct a geo-centric graph G
geo
by col-
lecting n = 135 Democratic and m = 135 Re-
publican users from the Maryland, Virginia and
Delaware region of the US with self-reported po-
litical preference in their biographies. Similar to
the candidate-centric graph, for each user we col-
lect recent tweets and randomly sample user social
circles in the Fall of 2012. We collect this data to
get a sample of politically less active users com-
pared to the users from candidate-centric graph.
2.4 ZLR Graph
We also consider a G
ZLR
graph constructed from
a dataset previously used for political affiliation
4
As of Oct 12, 2012, the number of followers for Obama,
Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k.
187
classification (Zamal et al, 2012). This dataset
consists of 200 Republican and 200 Democratic
users associated with 925 tweets on average per
user.
5
Each user has on average 6155 friends with
642 tweets per friend. Sharing restrictions and rate
limits on Twitter data collection only allowed us to
recreate a semblance of ZLR data
6
? 193 Demo-
cratic and 178 Republican users with 1K tweets
per user, and 20 neighbors of four types including
follower, friends, user mention and retweet with
200 tweets per neighbor for each user of interest.
3 Batch Models
Baseline User Model As input we are given a set
of vertices representing users of interest v
i
? V
along with feature vectors
~
f(v
i
) derived from con-
tent authored by the user of interest. Each user
is associated with a non-zero number of publicly
posted tweets. Our goal is assign to a category
each user of interest v
i
based on
~
f(v
i
). Here we
focus on a binary assignment into the categories
Democratic D or Republican R. The log-linear
model
7
for such binary classification is:
?
v
i
=
{
D (1 + exp[?
~
? ?
~
f(v
i
)])
?1
? 0.5,
R otherwise.
(1)
where features are normalized word ngram counts
extracted from v
i
?s tweets
~
f
t
(v
i
) : D?t(v
i
)? R.
The proposed baseline model follows the same
trends as the existing state-of-the-art approaches
for user attribute classification in social media as
described in Section 8. Next we propose to ex-
tend the baseline model by taking advantage of
language in user social circles as describe below.
Neighbor Model As input we are given user-local
neighborhood N
r
(v
i
), where r is a neighborhood
type. Besides the neighborhood?s type r, each is
characterized by:
? the number of communications per neighbor
~
f
t
(N
r
), t = {5, 10, 15, 25, 50, 100, 200};
5
The original dataset was collected in 2012 and has
been recently released at http://icwsm.cs.mcgill.ca/. Politi-
cal labels are extracted from http://www.wefollow.com as de-
scribed by Pennacchiotti and Popescu (2011b).
6
This inability to perfectly replicate prior work based on
Twitter is a recognized problem throughout the community of
computational social science, arising from the data policies of
Twitter itself, it is not specific to this work.
7
We use log-linear models over reasonable alternatives
such as perceptron or SVM, following the practice of a wide
range of previous work in related areas (Smith, 2004; Liu et
al., 2005; Poon et al, 2009) including text classification in so-
cial media (Van Durme, 2012b; Yang and Eisenstein, 2013).
? the order of the social circle ? the num-
ber of neighbors per user of interest |N
r
| =
deg(v
i
), n = {1, 2, 5, 10}.
Our goal is to classify users of interest using
evidence (e.g., communications) from their local
neighborhood
?
n
~
f
t
[N
r
(v
i
)] ?
~
f(N
r
) as Demo-
cratic or Republican. The corresponding log-
linear model is defined as:
?
N
r
=
{
D (1 + exp[?
~
? ?
~
f(N
r
)])
?1
? 0.5,
R otherwise.
(2)
To check whether our static models are cog-
nizant of low-resource prediction settings we com-
pare the performance of the user model from Eq.1
and the neighborhood model from Eq.2. Follow-
ing the streaming nature of social media, we see
the scarce available resource as the number of re-
quests allowed per day to the Twitter API. Here
we abstract this to a model assumption where we
receive one tweet t
k
at a time and aim to maximize
classification performance with as few tweets per
user as possible:
8
? for the baseline user model:
minimize
k
?
k
t
k
(v
i
),
(3)
? for the neighborhood model:
minimize
k
?
n
?
k
t
k
[N
r
(v
i
)].
(4)
4 Streaming Models
We rely on straightforward Bayesian rule update
to our batch models in order to simulate a real-
time streaming prediction scenario as a first step
beyond the existing models as shown in Figure 2.
The model makes predictions of a latent user at-
tribute e.g., Republican under a model assumption
of sequentially arriving, independent and identi-
cally distributed observations T = (t
1
, . . . , t
k
)
9
.
The model dynamically updates posterior proba-
bility estimates p(a(v
i
) = R|t
k
) for a given user
8
The separate issue is that many authors simply don?t
tweet very often. For instance, 85.3% of all Twitter
users post less than one update per day as reported at
http://www.sysomos.com/insidetwitter/. Thus, their commu-
nications are scare even if we could get al of them without
rate limiting from Twitter API.
9
Given the dynamic character of online discourse it will
clearly be of interest in the future to consider models that go
beyond the iid assumption.
188
p(R|t
1
)
0.6
v
i
v
i
v
i
p(R|t
1
, t
2
)
0.7
p(R|t
1
, . . . t
k
)
0.9
N
r
N
r
N
r
Time, ??2?1 ?k
Figure 2: Stream-based classification of an attribute a(v
i
) ?
{R,D} given a stream of communications t
1
, t
2
, . . . , t
k
au-
thored by a user v
i
or user immediate neighbors from N
r
social circles at time ?
1
, ?
2
, . . . , ?
k
.
v
i
as an additional evidence t
k
is acquired, as de-
fined in a general form below for any latent at-
tribute a(v
i
) ? A given the tweets T of user v
i
:
p(a(v
i
) = x ? A | T ) =
p(T | a(v
i
) = x) ? p(a(v
i
) = x)
?
y?A
p(T | a(v
i
) = y) ? p(a(v
i
) = y)
=
?
k
p(t
k
| a(v
i
) = x) ? p(a(v
i
) = x)
?
y?A
?
k
p(t
k
| a(v
i
) = y) ? p(a(v
i
) = y),
(5)
where y is the number of all possible attribute val-
ues, and k is the number of tweets per user.
For example, to predict user political prefer-
ence, we start with a prior P (R) = 0.5, and se-
quentially update the posterior p(R | T ) by accu-
mulating evidence from the likelihood p(t
k
|R):
p(R | T ) =
?
k
p(t
k
|R) ? p(R)
?
k
P (t
k
|R) ? p(R) +
?
k
P (t
k
|D) ? p(D).
(6)
Our goal is to maximize posterior probability
estimates given a stream of communications for
each user in the data over (a) time ? and (b) the
number of tweets T . For that, for each user we
take tweets that arrive continuously over time and
apply two different streaming models:
? User Model with Dynamic Updates: re-
lies exclusively on user tweets t
(v
i
)
1
, . . . , t
(v
i
)
k
following the order they arrive over time ? ,
where for each user v
i
we dynamically up-
date the posterior p(R | t
(v
i
)
1
, . . . , t
(v
i
)
k
).
? User-Neighbor Model with Dynamic Up-
dates: relies on both neighbor N
r
commu-
nications including friend, follower, retweet,
user mention and user tweets t
(v
i
)
1
, . . . , t
(N
r
)
k
following the order they arrive over time ? ;
here we dynamically update the posterior
probability p(R | t
(v
i
)
1
, . . . , t
(N
r
)
k
).
5 Experimental Setup
We design a set of experiments to analyze static
and dynamic models for political affiliation classi-
fication defined in Sections 3 and 4.
5.1 Batch Classification Experiments
We first answer whether communications from
user-local neighborhoods can help predict politi-
cal preference for the user. To explore the con-
tribution of different neighborhood types we learn
static user and neighbor models on G
cand
, G
geo
and G
ZLR
graphs. We also examine the ability of
our static models to predict user political prefer-
ences in low-resource setting e.g., 5 tweets.
The existing models follow a standard setup
when either user or neighbor tweets are available
during train and test. For a static neighbor model
we go beyond that, and train our the model on all
data available per user, but only apply part of the
data at the test time, pushing the boundaries of
how little is truly required for classification. For
example, we only use follower tweets for G
test
,
but we use tweets from all types of neighbors for
G
train
. Such setup will simulate different real-
world prediction scenarios which have not been
previously explored, to our knowledge e.g., when
a user has a private profile or has not tweeted yet,
and only user neighbor tweets are available.
We experiment with our static neighbor model
defined in Eq.2 with the aim to:
1. evaluate neighborhood size influence, we
change the number of neighbors and try n =
[1, 2, 5, 10] neighbor(s) per user;
2. estimate neighbor content influence, we alter-
nate the amount of content per neighbor and
try t = [5, 10, 15, 25, 50, 100, 200] tweets.
We perform 10-fold cross validation
10
and run
100 random restarts for every n and t parame-
ter combination. We compare our static neigh-
bor and user models using the cost functions
from Eq.3 and Eq.4. For all experiments we use
LibLinear (Fan et al, 2008), integrated in the
Jerboa toolkit (Van Durme, 2012a). Both mod-
els defined in Eq.1 and Eq.2 are learned using
normalized count-based word ngram features ex-
tracted from either user or neighbor tweets.
11
10
For each fold we split the data into 3 parts: 70% train,
10% development and 20% test.
11
For brevity we omit reporting results for bigram and tri-
gram features, since unigrams showed superior performance.
189
5.2 Streaming Classification Experiments
We evaluate our models with dynamic Bayesian
updates on a continuous stream of communica-
tions over time as shown in Figure 2. Unlike static
model experiments, we are not modeling the in-
fluence of the number of neighbors or the amount
of content per neighbor. Here, we order user and
neighbor communication streams by real world
time of posting and measure changes in posterior
probabilities over time. The main purpose of these
experiments is to quantitatively evaluate (1) the
number of tweets and (2) the amount of real world
time it takes to observe enough evidence on Twit-
ter to make reliable predictions.
We experiment with log-linear models defined
in Eq. 1 and 2 and continuously estimate the poste-
rior probabilities P (R | T ) as defined in Eq.6. We
average the posterior probability results over the
users in G
cand
, G
geo
and G
ZLR
graphs. We train
streaming models on an attribute balanced subset
of tweets for each user v
i
excluding v
i
?s tweets (or
v
i
?s neighbor tweets for a joint model). This setup
is similar to leave-one-out classification. The clas-
sifier is learned using binary word ngram features
extracted from user or user-neighbor communi-
cations. We prefer binary to normalized count-
based features to overcome sparsity issues caused
by making predictions on each tweet individually.
6 Static Classification Results
6.1 Modeling User Content Influence
We investigate classification decision probabilities
for our static user model ?
v
i
by making predic-
tions on a random set of 5 vs. 100 tweets per user.
To our knowledge only limited work on personal
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
1.0
User
Clas
sific
ation
 dec
ision
 (pro
babi
lity)
misclassified
misclassifiedcorrect
correct
Figure 3: Classification probabilities for ?
v
i
estimated over
100 users in G
cand
tested on 5 (blue) vs. 100 (green) tweets
per user where Republican = 1, Democratic = 0, filled mark-
ers = correctly classified, not filled = misclassified users.
l l l l
l l l
5 10 20 50 100 2000
.500
.550
.600
.650
.70
log(Tweets Per Neighbor)
Accurac
y
10 50 100 200 400
FriendFollowerHashtagUsermention
RetweetReplyUser
l l l l l
l l
l l l l
l l l
l l
l l l l
l
l l l l l
l l
l l l
l l
(a) G
geo
: 2 neighbors
l l l l
l l l
5 10 20 50 100 2000
.500
.550
.600
.650
.70
log(Tweets Per Neighbor)
Accurac
y
50 100 250 500 1000 2000
FriendFollowerHashtagUsermention
RetweetReplyUser
l l l l l l ll l l l
l l
l l
l l l l
l
l l l
l l l l
l l l l
l l l
(b) G
geo
: 10 neighbors
l
l l
l l
l l
5 10 20 50 100 2000.
500.5
50.60
0.650
.700.7
5
log(Tweets Per Neighbor)
Accurac
y
10 50 100 200 400
FriendFollowerHashtagUsermention
RetweetReplyUser
l
l
l l
l l l l ll
l l
l l
l l
l l
l l
l l
l l
l l
l l l
(c) G
cand
: 2 neighbors
l
l l
l l l
l
5 10 20 50 100 2000.
500.5
50.60
0.650
.700.7
5
log(Tweets Per Neighbor)
Accurac
y
50 100 250 500 1000 2000
FriendFollowerHashtagUsermention
RetweetReplyUser
l l
l l l
l l l
l l l l
l
l l l
l
l l
l l l
l l
l l
l l l
(d) G
cand
: 10 neighbors
Figure 4: Modeling the influence of the number of tweets per
neighbor t=[5, .., 200] for G
cand
and G
geo
graphs.
analytics (Burger et al, 2011; Van Durme, 2012b)
have performed this straight-forward comparison.
For that purpose, we take a random partition con-
taining 100 users ofG
cand
graph and perform four
independent classification experiments ? two runs
using 5 and two runs using 100 tweets per user.
Figure 3 demonstrates that more tweets during
prediction time lead to higher accuracy by show-
ing that more users with 100 tweets are correctly
classified e.g., filled green markers in the right up-
per quadrant are true Republicans and in the left
lower quadrant are true Democrats. Moreover, a
lot of users with 100 tweets are close to 0.5 deci-
sion probability which suggests that the classifier
is just uncertain rather then being completely off,
e.g., misclassified Republican users with 5 tweets
(not filled blue markers in the right lower quad-
rant) are close to 0. These results follow natu-
rally from the underlying feature representation:
having more tweets per user leads to a lower vari-
ance estimate of a target multinomial distribution.
The more robustly this distribution is estimated
(based on having more tweets) the more confident
we should be in the classifier output.
6.2 Modeling Neighbor Content Influence
Here we discuss the results for our static neighbor-
hood model. We study the influence of the neigh-
borhood type r and size in terms of the number of
neighbors n and tweets t per neighbor.
190
l
l
l
l
1 2 5 100.50
0.550
.600.
650.7
00.75
log(Number of Neighbors)
Accurac
y
5 10 25 50
FriendFollowerHashtag UsermentionRetweetReply
l
l
l
l l
l
l
l
l
l
l
l
l
l l
l
l
(a) G
cand
: 5 tweets
l
l
l
l
1 2 5 100.50
0.550
.600.
650.7
00.75
log(Number of Neighbors)
Accurac
y
200 400 1000 2000
FriendFollowerHashtag UsermentionRetweetReply
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
(b) G
cand
: 200 tweets
l
l
l l
1 2 5 100.50
0.55
0.60
0.65
0.70
log(Number of Neighbors)
Accurac
y
5 10 25 50
FriendFollowerHashtag UsermentionRetweetReply
l l
l l
l l
l l
l
l
l
l
l
l l
l
l
l
(c) G
geo
: 5 tweets
l
l
l l
1 2 5 100.50
0.55
0.60
0.65
0.70
log(Number of Neighbors)
Accurac
y
200 400 1000 2000
FriendFollowerHashtag UsermentionRetweetReply
l l
l l
l l
l
l
l
l
l
l
l
l
l
l
l l
l l
(d) G
geo
: 200 tweets
Figure 5: Modeling the influence of the number of neighbors
per user n=[1, .., 10] for G
cand
and G
geo
graphs.
In Figure 4 we present accuracy results for
G
cand
and G
geo
graphs. Following Eq.3 and 4, we
spent an equal amount of resources to obtain 100
user tweets and 10 tweets from 10 neighbors. We
annotate these ?points of equal number of commu-
nications? with a line on top marked with a corre-
sponding number of user tweets.
We show that three of six social circles ? friend,
retweet and user-mention yield better accuracy
compared to the user model for all graphs when
t ? 250. Thus, for effectively classifying a given
user v
i
it is better to take 200 tweets each from 10
neighbors rather than 2,000 tweets from the user.
The best accuracy for G
cand
is 0.75 for friend,
follower, retweet and user-mention neighborhoods
which is 0.03 higher than the user baseline; for
G
geo
is 0.67 for user-mention and 0.64 for retweet
circles compared to 0.57 for the user model; for
G
ZLR
is 0.863 for retweet and 0.849 for friend
circles which is 0.11 higher that the user baseline.
Finally, similarly to the results for the user model
given in Figure 3, increasing the number of tweets
per neighbor from 5 to 200 leads to a significant
gain in performance for all neighborhood types.
6.3 Modeling Neighborhood Size
In Figure 5 we present accuracy results to show
neighborhood size influence on classification per-
formance for G
geo
and G
cand
graphs. Our re-
sults demonstrate that even small changes to the
neighborhood size n lead to better performance
which does not support the claims by Zamal et al
(2012). We demonstrate that increasing the size
of the neighborhood leads to better performance
across six neighborhood types. Friend, user men-
tion and retweet neighborhoods yield the highest
accuracy for all graphs. We observe that when the
number of neighbors is n = 1, the difference in
accuracy across all neighborhood types is less sig-
nificant but for n ? 2 it becomes more significant.
7 Streaming Classification Results
7.1 Modeling Dynamic Posterior Updates
from a User Stream
Figures 6a and 6b demonstrate dynamic user
model prediction results averaged over users from
G
cand
and G
ZLR
graphs. Each figure outlines
changes in sequential average probability esti-
mates p
?
(R | T ) for each individual self-authored
tweet t
k
as defined in Eq. 6. The average proba-
bility estimates p
?
(R | T ) are reported for every 5
tweets in a stream T = (t
1
, . . . t
k
) as
?
n
P (R|t
k
)
n
,
where n is the total number of users with the same
attribute R or D. We represent p
?
(R | T ) as a
box and whisker plot with the median, lower and
upper quantiles to show the variance; the length of
whiskers indicate lower and upper extreme values.
We find similar behavior across all three graphs.
In particular, the posterior estimates converge
faster when predicting Democratic than Republi-
can users but it has been trained on an equal num-
ber of tweets per class. We observe that average
posterior estimates P
?
(R | T ) converge faster to 0
0.5
0.6
0.7
0.8
0.9
1.0
0 20 40 60
p(R
epu
blic
an|T
)
0.0
0.1
0.2
0.3
0.4
0.5
0 20 40 60
Tweet Stream (T)
p(R
epu
blic
an|T
)
(a) User G
cand
0.5
0.6
0.7
0.8
0.9
1.0
0 50 100 150
p(R
epu
blic
an|T
)
0.0
0.1
0.2
0.3
0.4
0.5
0 50 100 150
Tweet Stream (T)
p(R
epu
blic
an|T
)
(b) User G
ZLR
Figure 6: Streaming classification results from user commu-
nications for G
cand
and G
ZLR
graphs averaged over every 5
tweets (red - Republican, blue - Democratic).
191
300
400
500
0 5 10 15
Time in Weeks
Us
ers
(a) User G
cand
0
100
200
0 20 40 60 80
Time in Weeks
Us
ers
(b) User G
ZLR
300
400
500
0 1 2 3 4 5
Time in Weeks
Us
ers
(c) User-Neigh G
cand
0
100
200
0 10 20 30 40
Time in Weeks
Us
ers
(d) User-Neigh G
ZLR
Figure 7: Time needed for (a) - (b) dynamic user model and
(c) - (d) joint user-neighbor model to infer political prefer-
ences of Democratic (blue) and Republican (red) users at
75% (dotted line) and 95% (solid line) accuracy levels.
(Democratic) than to 1 (Republican) in Figures 6a
and 6b. It suggests that language of Democrats is
more expressive of their political preference than
language of Republicans. For example, frequent
politically influenced terms used widely by Demo-
cratic users include faith4liberty, constitutionally,
pass, vote2012, terroristic.
The variance for average posterior estimates
decreases when the number of tweets increases
for all three datasets. Moreover, we detect that
P
?
(R|T ) estimates for users in G
cand
converge 2-
3 times faster in terms of number of tweets than
for users in G
ZLR
. The lowest convergence is de-
tected for G
geo
where after t
k
= 250 tweets the
average posterior estimate P
?
(R | t
k
) = 0.904 ?
0.044 and P
?
(D | t
k
) = 0.861 ? 0.008. It means
that users inG
cand
are more politically vocal com-
pared to users in G
ZLR
and G
geo
. As a result,
less active users in G
geo
just need more than 250
tweets to converge to a true 0 or 1 class. These re-
sults are coherent with the outcomes for our static
models shown in Figures 4 and 5. These findings
further confirm that differences in performance are
caused by various biases present in the data due to
distinct sampling and annotation approaches.
Figure 7a and 7b illustrate the amount of time
required for the user model to infer political pref-
erences estimated for 1,031 users inG
cand
and 371
users inG
ZLR
. The amount of time needed can be
evaluated for different accuracy levels e.g., 0.75
and 0.95. Thus, with 75% accuracy we classify:
? 100 (?20%) Republican users in 3.6 hours
and Democratic users in 2.2 hours for G
cand
;
? 100 (?56%) R users in 20 weeks and 100
(?52%) D users in 8.9 weeks for G
ZLR
which is 800 times longer that for G
cand
;
? 100 (?75%) R users in 12 weeks and 80
(?60%) D users in 19 weeks for G
geo
.
Such extreme divergences in the amount of time
required for classification across all graphs should
be of strong interest to researchers concerned with
latent attribute prediction tasks because Twitter
users produce messages with extremely different
frequencies. In our case, users in G
ZLR
tweet ap-
proximately 800 times less frequently than users
in G
cand
.
7.2 Modeling Dynamic Posterior Updates
from a Joint User-Neighbor Stream
We estimate dynamic posterior updates from a
joint stream of user and neighbor communications
in G
geo
, G
cand
and G
ZLR
graphs. To make a fair
comparison with a streaming user model, we start
with the same user tweet t
0
(v
i
). Then instead of
waiting for the next user tweet we rely on any
neighbor tweets that appear until the user produces
the next tweet t
1
(v
i
). We rely on communications
from four types of neighbors such as friends, fol-
lowers, retweets and user mentions.
The convergence rate for the average posterior
probability estimates P
?
(R|T ) depending on the
number of tweets is similar to the user model re-
sults presented in Figure 6. However, for G
geo
the variance for P
?
(R|T ) is higher for Democratic
users; for G
ZLR
P
?
(R|T ) ? 1 for Republicans
in less than 110 tweets which is ?t = 40 tweets
faster than the user model; for G
cand
the conver-
gence for both P
?
(R|T ) ? 1 and P
?
(D|T ) ? 0
is not significantly different than the user model.
Figures 7c and 7d show the amount of time re-
quired for a joint user-neighbor model to infer po-
litical preferences estimated for users inG
cand
and
G
ZLR
. We find that with 75% accuracy we can
classify 100 users for:
? G
cand
: Republican users in 23 minutes and
Democratic users in 10 minutes;
? G
ZLR
: R users in 3.2 weeks and D users in
1.1 weeks which is 7 times faster on average
across attributes than for the user model;
? G
geo
: R users in 1.2 weeks and D users in
3.5 weeks which is on average 6 times faster
across attributes than for the user model.
Similar or better P
?
(R|T ) convergence in terms
of the number of tweets and, especially, in the
amount of time needed for user and user-neighbor
192
models further confirms that neighborhood con-
tent is useful for political preference prediction.
Moreover, communications from a joint stream al-
low to make an inference up to 7 times faster.
8 Related Work
Supervised Batch Approaches The vast major-
ity of work on predicting latent user attributes in
social media apply supervised static SVM mod-
els for discrete categorical e.g., gender and re-
gression models for continuous attributes e.g., age
with lexical bag-of-word features for classifying
user gender (Garera and Yarowsky, 2009; Rao et
al., 2010; Burger et al, 2011; Van Durme, 2012b),
age (Rao et al, 2010; Nguyen et al, 2011; Nguyen
et al, 2013) or political orientation. We present an
overview of the existing models for political pref-
erence prediction in Table 1.
Bergsma et al (2012) following up on Rao?s
work (2010) on adding socio-linguistic features
to improve gender, ethnicity and political prefer-
ence prediction show that incorporating stylistic
and syntactic information to the bag-of-word fea-
tures improves gender classification.
Other methods characterize Twitter users by ap-
plying limited amounts of network structure in-
formation in addition to lexical features. Con-
Approach Users Tweets Features Accur.
Rao et al
(2010)
1K 2M
ngrams
socio-ling
stacked
0.824
0.634
0.809
Pennacchiotti
and Popescu
(2011a)
10.3K ?
ling-all
soc-all
full
0.770
0.863
0.889
Conover et
al. (2011)
1,000 1M
full-text
hashtags
clusters
0.792
0.908
0.949
Zamal et al
(2012)
400
400K
3.85M
4.25M
UserOnly
Nbr
User-Nbr
11
0.890
0.920
0.932
Cohen and
Ruths
(2013)
397
1.8K
262
196
397K
1.8M
262K
196K
features
from (Za-
mal et al,
2012)
0.910
0.840
0.680
0.870
This paper
(batch clas-
sification)
G
cand
1,031
G
geo
270
G
ZLR
371
206K
2M
54K
540K
371K
1.5M
user ngrams
neighbor
user ngrams
neighbor
user ngrams
neighbor
0.720
0.750
0.570
0.670
0.886
0.920
This paper
(dynamic
Bayesian
update clas-
sification)
G
cand
1,031
G
geo
270
G
ZLR
371
103K
130K
54K
67K
74K
185K
user stream
user-neigh.
user stream
user-neigh.
user stream
user-neigh.
0.995
0.999
0.843
0.882
0.892
0.999
Table 1: Overview of the existing approaches for political
preference classification in Twitter.
nover et al (2011) rely on identifying strong parti-
san clusters of Democratic and Republican users
in a Twitter network based on retweet and user
mention degree of connectivity, and then combine
this clustering information with the follower and
friend neighborhood size features. Pennacchiotti
et al (2011a; 2011b) focus on user behavior, net-
work structure and linguistic features. Similar to
our work, they assume that users from a partic-
ular class tend to reply and retweet messages of
the users from the same class. We extend this as-
sumption and study other relationship types e.g.,
friends, user mentions etc. Recent work by Wong
et al (2013) investigates tweeting and retweet-
ing behavior for political learning during 2012 US
Presidential election. The most similar work to
ours is by Zamal et al (2012), where the authors
apply features from the tweets authored by a user?s
friend to infer attributes of that user. In this paper,
we study different types of user social circles in
addition to a friend network.
Additionally, using social media for mining po-
litical opinions (O?Connor et al, 2010a; May-
nard and Funk, 2012) or understanding socio-
political trends and voting outcomes (Tumasjan
et al, 2010; Gayo-Avello, 2012; Lampos et al,
2013) is becoming a common practice. For in-
stance, Lampos et al (2013) propose a bilinear
user-centric model for predicting voting intentions
in the UK and Australia from social media data.
Other works explore political blogs to predict what
content will get the most comments (Yano et al,
2013) or analyze communications from Capitol
Hill
12
to predict campaign contributors based on
this content (Yano and Smith, 2013).
Unsupervised Batch Approaches Bergsma et
al. (2013) show that large-scale clustering of user
names improves gender, ethnicity and location
classification on Twitter. O?Connor et al (2010b)
following the work by Eisenstein (2010) propose
a Bayesian generative model to discover demo-
graphic language variations in Twitter. Rao et
al. (2011) suggest a hierarchical Bayesian model
which takes advantage of user name morphology
for predicting user gender and ethnicity. Golbeck
et al (2010) incorporate Twitter data in a spatial
model of political ideology.
Streaming Approaches Van Durme (2012b)
proposed streaming models to predict user gen-
der in Twitter. Other works suggested to process
12
http://www.tweetcongress.org
193
text streams for a variety of NLP tasks e.g., real-
time opinion mining and sentiment analysis in so-
cial media (Pang and Lee, 2008), named entity
disambiguation (Sarmento et al, 2009), statistical
machine translation (Levenberg et al, 2011), first
story detection (Petrovi?c et al, 2010), and unsu-
pervised dependency parsing (Goyal and Daum?e,
2011). Massive Online Analysis (MOA) toolkit
developed by Bifet et al (2010) is an alternative to
the Jerboa package used in this work developed
by Van Durme (2012a). MOA has been effec-
tively used to detect sentiment changes in Twitter
streams (Bifet et al, 2011).
9 Conclusions and Future Work
In this paper, we extensively examined state-of-
the-art static approaches and proposed novel mod-
els with dynamic Bayesian updates for streaming
personal analytics on Twitter. Because our stream-
ing models rely on communications from Twitter
users and content from various notions of user-
local neighborhood they can be effectively applied
to real-time dynamic data streams. Our results
support several key findings listed below.
Neighborhood content is useful for personal
analytics. Content extracted from various notions
of a user-local neighborhood can be as effective
or more effective for political preference classifi-
cation than user self-authored content. This may
be an effect of ?sparseness? of relevant user data,
in that users talk about politics very sporadically
compared to a random sample of their neighbors.
Substantial signal for political preference
prediction is distributed in the neighborhood.
Querying for more neighbors per user is more ben-
eficial than querying for extra content from the
existing neighbors e.g., 5 tweets from 10 neigh-
bors leads to higher accuracy than 25 tweets from
2 neighbors or 50 tweets from 1 neighbor. This
may be also the effect of data heterogeneity in
social media compared to e.g., political debate
text (Thomas et al, 2006). These findings demon-
strate that a substantial signal is distributed over
the neighborhood content.
Neighborhoods constructed from friend,
user mention and retweet relationships are
most effective. Friend, user mention and retweet
neighborhoods show the best accuracy for predict-
ing political preferences of Twitter users. We think
that friend relationships are more effective than
e.g., follower relationships because it is very likely
that users share common interests and preferences
with their friends, e.g. Facebook friends can even
be used to predict a user?s credit score.
13
User
mentions and retweets are two primary ways of in-
teraction on Twitter. They both allow to share in-
formation e.g., political news, events with others
and to be involved in direct communication e.g.,
live political discussions, political groups.
Streaming models are more effective than
batch models for personal analytics. The predic-
tions made using dynamic models with Bayesian
updates over user and joint user-neighbor commu-
nication streams demonstrate higher performance
with lower resources spent compared to the batch
models. Depending on user political involvement,
expressiveness and activeness, the perfect predic-
tion (approaching 100% accuracy) can be made
using only 100 - 500 tweets per user.
Generalization of the classifiers for political
preference prediction. This work raises a very
important but under-explored problem of the gen-
eralization of classifiers for personal analytics in
social media, also recently discussed by Cohen
and Ruth (2013). For instance, the existing models
developed for political preference prediction are
all trained on Twitter data but report significantly
different results even for the same baseline mod-
els trained using bag-of-word lexical features as
shown in Table 1. In this work we experiment with
three different datasets. Our results for both static
and dynamic models show that the accuracy in-
deed depends on the way the data was constructed.
Therefore, publicly available datasets need to be
released for a meaningful comparison of the ap-
proaches for personal analytics in social media.
In future work, we plan to incorporate itera-
tive model updates from newly classified com-
munications similar to online perceptron-style up-
dates. In addition, we aim to experiment with
neighborhood-specific classifiers applied towards
the tweets from neighborhood-specific streams
e.g., friend classifier used for friend tweets,
retweet classifier applied to retweet tweets etc.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their helpful comments.
13
http://money.cnn.com/2013/08/26/technology/social/
facebook-credit-score/
194
References
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 327?337.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 1010?1019.
Albert Bifet, Geoff Holmes, Bernhard Pfahringer,
Philipp Kranen, Hardy Kremer, Timm Jansen, and
Thomas Seidl. 2010. MOA: Massive online analy-
sis, a framework for stream classification and clus-
tering. Journal of Machine Learning Research,
11:44?50.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, 17:5?11.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1301?1309.
Raviv Cohen and Derek Ruths. 2013. Classifying Po-
litical Orientation on Twitter: It?s Not Easy! In Pro-
ceedings of the International AAAI Conference on
Weblogs and Social Media (ICWSM), pages 91?99.
Michael D. Conover, Bruno Gonc?alves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011. Predicting the political alignment
of Twitter users. In Proceedings of Social Comput-
ing, pages 192?199.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1277?1287.
Rong En Fan, Kai Wei Chang, Cho Jui Hsieh, Xi-
ang Rui Wang, and Chih Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. Jour-
nal of Machine Learning Research, 9:1871?1874.
Katja Filippova. 2012. User demographics and lan-
guage in an implicit social network. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1478?1488.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 710?718.
Daniel Gayo-Avello. 2012. No, you cannot predict
elections with Twitter. Internet Computing, IEEE,
16(6):91?94.
Jennifer Golbeck, Justin M. Grimes, and Anthony
Rogers. 2010. Twitter use by the u.s. congress.
Journal of the American Society for Information Sci-
ence and Technology, 61(8):1612?1621.
Amit Goyal and Hal Daum?e, III. 2011. Approxi-
mate scalable bounded space sketch for large data
NLP. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 250?261.
Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from social media. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), pages
993?1003.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statis-
tical machine translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT), pages 177?186.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 459?466.
Ingrid Lunden. 2012. Analyst: Twitter
passed 500M users in june 2012, 140m of
them in US; Jakarta ?biggest tweeting? city.
http://techcrunch.com/2012/07/30/analyst-twitter-
passed-500m-users-in-june-2012-140m-of-them-in-
us-jakarta-biggest-tweeting-city/.
Diana Maynard and Adam Funk. 2012. Automatic de-
tection of political opinions in tweets. In Proceed-
ings of the 8th International Conference on The Se-
mantic Web (ESWC), pages 88?99.
Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and
Mung Chiang. 2013. Quantifying political leaning
from tweets and retweets. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).
Dong Nguyen, Noah A. Smith, and Carolyn P. Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities
(LaTeCH), pages 115?123.
195
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. ?How old do you think I am??
A study of language and age in Twitter. In Proceed-
ings of the AAAI Conference on Weblogs and Social
Media (ICWSM), pages 439?448.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 122?129.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010b. A mixture model of de-
mographic lexical variation. In Proceedings of the
NIPS Workshop on Machine Learning and Social
Computing.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations of Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Marco Pennacchiotti and Ana-Maria Popescu. 2011a.
Democrats, republicans and starbucks afficionados:
user classification in twitter. In Proceedings of the
17th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 430?438.
Marco Pennacchiotti and Ana Maria Popescu. 2011b.
A machine learning approach to Twitter user clas-
sification. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM),
pages 281?288.
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to Twitter. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL).
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 209?217.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents (SMUC), pages 37?44.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hier-
archical Bayesian models for latent attribute detec-
tion in social media. In Proceedings of the Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM).
Lu??s Sarmento, Alexander Kehlenbeck, Eug?enio
Oliveira, and Lyle Ungar. 2009. An approach to
web-scale named-entity disambiguation. In Pro-
ceedings of the 6th International Conference on Ma-
chine Learning and Data Mining in Pattern Recog-
nition (MLDM), pages 689?703.
Noah A. Smith. 2004. Log-linear models.
Craig Smith. 2013. May 2013 by the
numbers: 16 amazing Twitter stats.
http://expandedramblings.com/index.php/march-
2013-by-the-numbers-a-few-amazing-twitter-stats/.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 327?335.
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting elections with Twitter:
What 140 characters reveal about political senti-
ment. In Proceedings of the International AAAI
Conference on Weblogs and Social Media, pages
178?185.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical re-
port, Human Language Technology Center of Excel-
lence.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 48?58.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing, pages 61?72.
Tao Yano and Noah A. Smith. 2013. What?s worthy
of comment? content and comment volume in po-
litical blogs. In International AAAI Conference on
Weblogs and Social Media (ICWSM).
Tao Yano, Dani Yogatama, and Noah A. Smith. 2013.
A penny for your tweets: Campaign contributions
and capitol hill microblogs. In Proceedings of the
International AAAI Conference on Weblogs and So-
cial Media (ICWSM).
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media, pages 387?390.
196
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 22?29,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Dynamic Wordclouds and Vennclouds for Exploratory Data Analysis
Glen Coppersmith
Human Language Technology Center of Excellence
Johns Hopkins University
coppersmith@jhu.edu
Erin Kelly
Department of Defense
elkelly8@gmail.com
Abstract
The wordcloud is a ubiquitous visualiza-
tion of human language, though it falls
short when used for exploratory data anal-
ysis. To address some of these shortcom-
ings, we give the viewer explicit control
over the creation of the wordcloud, allow-
ing them to interact with it in real time?
a dynamic wordcloud. This allows itera-
tive adaptation of the visualization to the
data and inference task at hand. We next
present a principled approach to visualiza-
tion which highlights the similarities and
differences between two sets of documents
? a Venncloud. We make all the visual-
ization code (primarily JavaScript) freely
available.
1 Introduction
A cornerstone of exploratory data analysis is visu-
alization. Tremendous academic effort and engi-
neering expertise has created and refined a myriad
of visualizations available to the data explorer, yet
there still exists a paucity of options for visualizing
language data. While visualizing human language
is a broad subject, we apply Polya?s dictum, and
examine a pair of simpler questions for which we
still lack an answer:
? (1) what is in this corpus of documents?
? (2) what is the relationship between these
two corpora of documents?
We assert that addressing these two questions is
a step towards creating visualizations of human
language more suitable for exploratory data anal-
ysis. In order to create a meaningful visualiza-
tion, one must understand the inference question
the visualization is meant to inform (i.e., the rea-
son for which (1) is being asked), so the appro-
priate aspects of the data can be highlighted with
the aesthetics of the visualization. Different infer-
ence questions require different aspects to be high-
lighted, so we aim to create a maximally-flexible,
yet simple and intuitive method to enable a user
to explore the relevant aspects of their data, and
adapt the visualization to their task at hand.
The primary contributions of this paper are:
? A visualization of language data tailored for
exploratory data analysis, designed to exam-
ine a single corpus (the dynamic wordcloud)
and to compare two corpora (the Venncloud);
? The framing and analysis of the problem in
terms of the existing psychophysical litera-
ture;
? Distributable JavaScript code, designed to be
simple to use, adapt, and extend.
We base our visualizations on the wordcloud,
which we deconstruct and analyze in ?3 and ?4.
We then discuss the literature on wordclouds and
relevant psychophysical findings in ?5, taking
guidance from the practical and theoretical foun-
dations explored there. We then draw heavily on
similarities to more common and well understood
visualizations to create a more useful version of
the wordcloud. Question (1) is addressed in ?7,
and with only a small further expansion described
in ?8, an approach to (2) becomes evident.
2 Motivating Inference Tasks
Exploratory data analysis on human language en-
compasses a diverse set of language and infer-
ence tasks, so we select the following subset for
their variety. One task in line with question (1)
is getting the general subject of a corpus, high-
lighting content-bearing words. One might want
to examine a collection of social media missives,
too numerous to read individually, perhaps to de-
tect emerging news (Petrovic et al., 2013). Sepa-
rately, author identification (or idiolect analysis)
22
attempts attribution of documents (e.g., Shake-
speare?s plays or the Federalist papers) by com-
paring the author?s writing style, focusing on
stylistic and contentless words ? for a review see
(Juola, 2006). Further, some linguistic psycho-
metric analysis depends on the relative distribu-
tion of pronouns and other seemingly contentless
words (Coppersmith et al., 2014a; Chung and Pen-
nebaker, 2007).
Each of these questions involves some analy-
sis of unigram statistics, but exactly what analy-
sis differs significantly, thus no single wordcloud
can display all of them. Any static wordcloud is
a single point in a distribution of possible word-
clouds ? one way of calculating statistics from the
underlying language and mapping those calcula-
tions to the visual representation. Many such com-
binations and mappings are available, and the opti-
mal wordcloud, like the optimal plot, is a function
of the data and the inference task at hand. Thus,
we enable the wordcloud viewer to adjust the rela-
tionship between the aspects of the data and the
aesthetics of the display, which allows them to
view different points in the distribution of possi-
ble wordclouds. The dynamic wordcloud was im-
plicitly called for in (Rayson and Garside, 2000)
since human expertise (specifically knowledge of
broader contexts and common sense) is needed
to separate meaningful and non-meaningful differ-
ences in wordclouds. We enable this dynamic in-
teraction between human and visualization in real-
time with a simple user interface, requiring only a
modicum more engineering than the creation of a
static wordcloud, though the depth of extra infor-
mation conveyed is significant.
3 Wordcloud Aesthetics
We refer to each visual component of the visual-
ization as an aesthetic (ala (Wickham, 2009)) ?
each aesthetic can convey some information to the
viewer. For context, the aesthetics of a scatterplot
include the x and y position, color, and size of
each point. Some are best suited for ordinal data
(e.g., font size), while others for categorical data
(e.g., font color).
Ordinal data can be encoded by font size,
the most prominent and noticeable to the viewer
(Bateman et al., 2008). Likewise, the opacity
(transparency) of the word is a prominent and or-
dinal aesthetic. The order in which words are dis-
played can convey a significant amount of infor-
mation as well, but using order in this fashion gen-
erally constrains the use of x and y position.
Categorical data can be encoded by the color
of each word ? both the foreground of the word
itself and the background space that surrounds it
(though that bandwidth is severely limited by hu-
man perception). Likewise for font weight (bold-
ness) and font decoration (italics and underlines).
While font face itself could encode a categorical
variable, making comparisons of all the other as-
pects across font faces is likely to be at best unin-
formative and at worst misleading.
4 Data Aspects
As the wordcloud has visual aesthetics that we can
control (?3), the data we need to model has aspects
that we want to represent with those aesthetics.
This aspect-to-aesthetic mapping is what makes a
useful and informative visualization, and needs to
be flexible enough to allow it be used for a range
of inference tasks.
For clarity, we define a word (w) as a unique
set of characters and a word token (w) as a sin-
gle usage of a word in a document. We can ob-
serve multiple word tokens (w) of the same word
(w) in a single document (d). For any document d
we represent the term frequency of w as tf
d
(w).
Similarly, the inverse document frequency of w
as idf(w). A combination of tf and idf is often
used to determine important words in a document
or corpus. We focus on tf and idf here, but this is
just an example of an ordinal value associated with
a word, there are many other such word-ordinal
pairings that are worth exploring (e.g., weights in
a classifier).
The dynamic range (?scaling? in (Wickham,
2009)) also needs to be considered, since the data
has a natural dynamic range ? where meaningful
differences can be observed (unsurprisingly, the
definition of meaningful depends on the inference
task). Likewise, each aesthetic has a range of val-
ues for which the users can perceive and differen-
tiate (e.g., words in a font size too small are illeg-
ible, those too large prevent other words from be-
ing displayed; not all differences are perceptible).
Mapping the relevant dynamic range of the data
to the dynamic range of the visualization is at the
heart of a good visualization, but to do this algo-
rithmically for all possible inference tasks remains
a challenge. We, instead, enable the user to adjust
the dynamic range of the visualization explicitly.
23
5 Prior Art
Wordclouds have a mixed history, stemming from
Jim Flanagan?s ?Search Referral Zeitgeist?, used
to display aggregate information about websites
linking to his, to its adoption as a visual gim-
mick, to the paradoxical claim that ?wordclouds
work in practice, but not in theory? (see (Vi?egas
and Wattenberg, 2008) for more). A number
of wordcloud-generators exist on the web (e.g.,
(Feinberg, 2013; Davies, 2013)), though these
tend towards creating art rather than informative
visualizations. The two cited do allow the user
limited interaction with some of the visual aesthet-
ics, though not of sufficient scope or response time
for general exploratory data analysis.
Enumerating all possible inference tasks involv-
ing the visualization of natural language is impos-
sible, but the prior art does provide empirical data
for some relevant tasks. This further stresses the
importance of allowing the user to interact with
the visualization, since optimizing the visualiza-
tion a priori for all inference tasks simultaneously
is not possible, much like creating a single plot for
all numerical inference tasks is not possible.
5.1 Psychophysical Analyses
The quintessential studies on how a wordcloud
is interpreted by humans can be found in (Ri-
vadeneira et al., 2007) and (Bateman et al.,
2008). They both investigated various measures of
impression-forming and recall to determine which
aesthetics conveyed information most effectively
? font size chief among them.
Rivandeneira et al. (Rivadeneira et al., 2007)
also found that word-order was important for im-
pression forming (displaying words from most fre-
quent to least frequent was most effective here),
while displaying words alphabetically was best
when searching for a known word. They also
found that users prefer a search box when search-
ing for something specific and known, and a word-
cloud for exploratory tasks and things unknown.
Bateman et al. (Bateman et al., 2008) examined
the relative utility of other aesthetics to convey in-
formation, finding that font-weight (boldness) and
intensity (opacity) are effective, but not as good
as font-size. Aesthetics such as color, number of
characters or the area covered by the word were
less effective.
Significant research has gone in to the place-
ment of words in the wordcloud (e.g., (Seifert et
al., 2008)), though seemingly little information
can be conveyed by these layouts (Schrammel et
al., 2009). Indeed, (Rivadeneira et al., 2007) in-
dicates that words directly adjacent to the largest
word in the wordcloud had slightly worse recall
than those not-directly-adjacent ? in essence, get-
ting the most important words in the center may
be counterproductive. Thus we eschew these algo-
rithms in favor of more interpretable (but perhaps
less aesthetically pleasing) linear ordered layouts.
5.2 Wordclouds as a tool
Illustrative investigations of the wordcloud as a
tool for exploratory data analysis are few, but en-
couraging.
In relation to question (1), even static word-
clouds can be useful for this task. Users per-
forming an open-ended search task preferred us-
ing a wordcloud to a search box (Sinclair and
Cardew-Hall, 2008), possibly because the word-
cloud prevented them from having to hypothesize
what might be in the collection before searching
for it. Similarly, wordclouds can be used as a
follow-up display of search results from a query
performed via a standard text search box (Knautz
et al., 2010), providing the user a crude summary
of the results. In both of these cases, a simple
static wordcloud is able to provide some useful
information to the user, though less research has
been done to determine the optimal composition
of the wordcloud. What?s more, the need for a
dynamic interactive wordcloud was made explicit
(Knautz et al., 2010), given the way the users iter-
atively refined their queries and wordclouds.
Question (2) has also been examined. One ap-
proach is to make a set of wordclouds with soft
constraints that the same word appears in roughly
the same position across multiple clouds to fa-
cilitate comparisons (Castella and Sutton, 2013).
Each of these clouds in a wordstorm visualizes a
different collection of documents (e.g., subdivi-
sions via metadata of a larger corpus).
Similarly addressing our second question, Par-
allel Tag Clouds (Collins et al., 2009) allow the
comparison of multiple sets of documents (or dif-
ferent partitions of a corpus). This investigation
provides a theoretically-justified approach to find-
ing ?the right? static wordcloud (for a single in-
ference task), though this does depend on some
language-specific resources (e.g., stopword lists
and stemming). Interestingly, they opt for ex-
24
plicit removal of words and outliers that the user
does not wish to have displayed (an exclusion
list), rather than adjusting calculations of the en-
tire cloud to remove them in a principled and fair
manner.
5.3 Wordclouds and Metadata
Wordclouds have previously been extended to
convey additional information, though these adap-
tations have been optimized generally for artistic
purposes rather than exploratory data analysis.
Wordclouds can been used to display how lan-
guage interacts with a temporal dimension in (Du-
binko et al., 2007; Cui et al., 2010; Lee et al.,
2010). Dubinko and colleagues created a tag cloud
variant that displays trends in tag usage over time,
coupled with images that have that tag (Dubinko et
al., 2007). An information-theoretic approach to
displaying information changing in time gives rise
to a theoretically grounded approach for display-
ing pointwise tag clouds, and highlighting those
pieces that have changed significantly as com-
pared to a previous time period (Cui et al., 2010).
This can be viewed as measuring the change in
overall language usage over time. In contrast, us-
ing spark lines on each individual word or tag can
convey temporal trends for individual words (Lee
et al., 2010).
Meanwhile, combining tag clouds with geospa-
tial data yields a visualization where words can be
displayed on a map of the world in locations they
are frequently tagged in, labeling famous land-
marks, for example (Slingsby et al., 2007).
6 Desiderata
In light of the diverse inference tasks (?2) and
prior art (?5), the following desiderata emerge for
the visualization. These desiderata are explicit
choices, not all of which are ideal for all infer-
ence tasks. Thus, chief among them is the first:
flexibility to allow maximum extensions and mod-
ifications as needed.
Flexible and adjustable in real time: Any sin-
gle static wordcloud is guaranteed to be subopti-
mal for at least some inference tasks, so allowing
the user to adjust the aspect-to-aesthetic mapping
of the wordcloud in real time enables adaptation
of the visualization to the data and inference task
at hand. The statistics described in ?4 are relevant
to every language collection (and most inference
tasks), yet there are a number of other ordinal val-
ues to associate a word (e.g., the weight assigned
to it by a classifier). Thus, tf and idf are meant
to be illustrative examples though the visualization
code should generalize well to others.
Though removal of the most frequent words
(stopwords) is useful in many natural language
processing tasks, there are many ways to define
which words fall under this category. Unsurpris-
ingly, the optimal selection of these words can also
depend upon the task at hand (e.g., psychiatric v.
thematic analysis as in ?2), so maximum flexibility
and minimum latency are desirable.
Interpretable: An explicit legend is needed to
interpret the differences in visual aesthetics and
what these differences mean with respect to the
underlying data aspects.
Language-Agnostic: We need methods for ex-
ploratory data analysis that work well regard-
less of the language(s) being investigated. This
is crucial for multilingual corpora, yet decidedly
nontrivial. These techniques must be maximally
language-agnostic, relying on only the most rudi-
mentary understanding of the linguistic structure
of the data (e.g., spaces separate words in English,
but not in Chinese), so they can be extended to
many languages easily.
This precludes the use of a fixed set of stop
words for each language examined, since a new set
of stopwords would be required for each language
explored. Alternatively, the set of stopwords can
be dealt with automatically, either by granting the
user the ability to filter out words in the extremes
of the distributions (tf and df alike) through the
use of a weight which penalizes these ubiquitous
or too-rare words. Similarly precluded is the use
of stemming to deal with the many surface forms
of a given root word (e.g., type, typing, typed).
7 Dynamic Wordclouds
We address Question (1) and a number of our
desiderata with the addition of explicitly labeled
controls to the static wordcloud display, which al-
lows the user to control the mapping from data
aspects to the visualization aesthetics. We sup-
plement these controls with an explicit explana-
tion of how each aesthetic is affected by each
aspect, so the user can easily read the relevant
mappings, rather than trying to interpret the loca-
tion of the sliders. An example of which is that
?Larger words are those that frequently occur in
the query?, when the aspect tf is mapped to the
25
[X] entities
common
cloud
tf
filter
idf
filter
size
controls
opacity
controls
sort
by
wordcloud
description
do not
redraw
highlight
keywords
orioles at to for in
baltimore a game i
and yankees of go
on is this vs w/ o's
are it win with be i'm
my you that others new
have just 2 3 up 1 let's first so all
tonight fans but out season we now get sox
an time md series al baseball will about me not
like as 5 playoff lets from over was they if what 4 good no
great year day one love back amp fan magic can east playoffs beat gt
red sports rt today do going how last more place team games night blue
file:///Users/gcoppersmith/Desktop/dwc_dev/orioles.html
1 of 2 2/24/14, 4:36 PM
[X] entities
common
cloud
tf
filter
idf
filter
size
controls
opacity
controls
sort
by
wordcloud
description
do not
redraw
highlight
keywords
13 espn fox leads md ny report star
usa buck chris glove mark move press
14 gold hamilton jim third birds gonzalez j
o september white homers rangers tigers 15 fall
yankee sox yard al happen pick blue cc extra m old
card closer hits tie tied face omg ravens says yahoo city
saying second starting stay ass manager players real lt three
bout football left sunday sweep goes hey gets wild yanks adam job
news times won't magic place innings 2013 base gt hr pitching does
fucking os rain 10 friday outs suck coming makes also check wearing
haha show through god los losing may always being keep 9 hate gotta looks
making posted away free im life we're weekend call give little bar bring didn't
doing least look stop thing top where giants long photo record johnson league
most thank tv ? please proud walk wish 2012 any end every hell post same
because many pitch said its new runs everyone excited field finally bullpen let red
yes believe fun made these boys hit into those against mlb race i'll teams amazing say
w years another people 1st both postseason wait before 8 looking loss him lead man shit
feel inning damn oh 7 baby i've again should bad then thanks two wins nice much that's
winning yankees than gonna playing tickets yeah awesome beat why division lol wow
congrats lost ready lose us big could never even fuck our want since ball ever happy start am
better make run hope think only over he were getting has sports really his u work 6 next won
checked chicago did them some after well best tomorrow watch take their when had too orioles there an
games world need who 0 stadium clinch your been down love don't way by east or know off play would 4
more o's it's no can't rt playoffs right as still how can back here playoff 1 5 year 3 night watching team do last
one amp got day series home see come great today from like 2 will fan going what pic they about time good lets first
season if get was tonight we not fans let's all me baseball out now up just so but have be win you that with i'm it are my
others this is washington on vs of w/ go i and
file:///Users/gcoppersmith/Desktop/dwc_dev/orioles.html
1 of 1 2/24/14, 4:35 PM
[X] entities
common
cloud
tf
filter
idf
filter
size
controls
opacity
controls
sort
by
wordcloud
description
do not
redraw
highlight
keywords
0 1 1st 2 2012 3 4 5 6 7 ? a about after again against al all amp an and are as
at back baltimore baseball be beat been best better
big birds blue buck but by can can't card checked chicago come could
day did do don't down east espn even ever fan fans first for from fuck game
games get go going gonna good got great gt had has have he here his hit home how i i'm
if in inning is it it's johnson just keep know last lead let's lets like lol lose love
magic make mark md me mlb more my need new news next
night no not now o's of off on one only or orioles others our
out over photo pic place play playoff playoffs post postseason rangers
ravens really red report right rt run season see series should since so some
sox sports start still take team than that their them there they think this tied time
times to today tomorrow tonight two up vs w/ was watch watching way we well were
what when white who why wild will win with won world would yahoo
yankees yanks year years you your
file:///Users/gcoppersmith/Desktop/dwc_dev/orioles.html
1 of 1 2/24/14, 4:37 PM
Figure 1: Three example settings of the dynamic wordcloud for the same set of tweets containing ?Orioles?. Left: size
reflects tf , sorted by tf ; Center: size reflects idf , sorted by idf ; Right: size reflects tf*idf , sorted alphabetically.
aesthetic font-size (and this description is tied to
the appropriate sliders so it updates as the slid-
ers are changed). The manipulation of the visu-
alization in real time allows us to take advantage
of the human?s adept visual change-detection to
highlight and convey the differences between set-
tings (or a range of settings), even subtle ones.
The data aspects from ?4 are precomputed and
mapped to the aesthetics from ?3 in a JavaScript
visualization displayed in a standard web browser.
This visualization enables the user to manipulate
the aspect-to-aesthetic mapping via an intuitive
set of sliders and buttons, responsive in real time.
The sliders are roughly segmented into three cat-
egories: those that control which words are dis-
played, those that control how size is calculated,
and those that control how opacity is calculated.
The buttons control the order in which words ap-
pear.
One set of sliders controls which words are
displayed by examining the frequency and rar-
ity of the words. We define the range ?
Freq
=
[t
min
Freq
, t
max
Freq
] as the range of tf values for words to
be displayed (i.e., tf(w) ? ?
Freq
). The viewer is
granted a range slider to manipulate both t
min
Freq
and
t
max
Freq
to eliminate words from the extremes of the
distribution. Similarly for df and ?
Rarity
. Those
words that fall outside ?
Freq
or ?
Rarity
are not dis-
played. Importantly, tf is computed from the cur-
rent corpus displayed while df is computed over a
much larger collection (in our running examples,
all the works of Shakespeare or all the tweets for
the last 6 months). Those with high df or high
tf are often stopwords, those with low tf and low
df are often rare, sometimes too rare to get good
estimates of tf or idf (e.g., names).
A second set of sliders controls the mapping be-
tween aspects and aesthetics for each individual
word. Each aesthetic has a weight for the impor-
tance of rarity (?
Rarity
) and the importance of fre-
quency (?
Freq
), corresponding to the current val-
ues of their respective slider (each in the range
[0, 1]). For size, we compute a weight attributed
to each data aspect:
?
Freq
(w) = (1? ?
Freq
) + ?
Freq
tf(w)
and similarly for Rarity.
In both cases, the aesthetic?s value is computed
via an equation similar to the following:
a(w) = ?
Freq
(w)?
Rarity
(w)?
Range
b
where a(w) is either font size or opacity, and b
is some base value of the aesthetic (scaled by a
dynamic range slider, ?
Range
) and the weights for
frequency and rarity of the word. In this manner,
the weights are multiplicative, so interactions be-
tween the variables (e.g., tf*idf ) are apparent.
Though unigram statistics are informative, see-
ing the unigrams in context is also important for
many inference tasks. To enable this, we use reser-
voir sampling (Vitter, 1985) to maintain a repre-
sentative sample of the observed occurrences of
each word in context, which the user can view by
clicking on the word in the wordcloud display.
Examples of the dynamic wordcloud in various
settings can be found in Figure 1, using a set of
tweets containing ?Orioles?. The left wordcloud
has tf mapped to size, the center with idf mapped
to size, and the right with both high tf and high
idf mapped to size. We only manipulate the size
aesthetic, since the opacity aesthetic is sometimes
hard to interpret in print. To fit the wordclouds
26
to the small format, various values for ?
Freq
and
?
Rarity
are employed, and order is varied ? the
left is ordered in descending order in terms of fre-
quency, the center is ordered in descending order
in terms of rarity, and the right is in alphabetical
order.
8 Vennclouds
Question (2) ? ?how are these corpora related? re-
quires only a single change to the dynamic sin-
gle wordcloud described in ?7. We refer to two
corpora, left and right, which we abbreviate L
and R (perhaps a set of tweets containing ?Ori-
oles? for left and those containing ?Nationals? for
right as in Figure 2). For the right documents, let
R = {d
1
, ..., d
n
R
} so |R| = n
R
and let T
R
be the
total number of tokens in all the documents in R
T
R
=
?
d?R
|T
d
|
We separate the wordcloud display into three re-
gions, one devoted to words most closely associ-
ated with R, one devoted to words most closely
associated with L, and one for words that should
be associated with both. ?Association? here can be
defined in a number of ways, but for the nonce we
define it as the probability of occurrence in that
corpus ? essentially term frequency, normalized
by corpus length. Normalizing by length is re-
quired to prevent bias incurred when the corpora
are different sizes (T
L
6= T
R
). Specifically, we
define the number of times w occurs in left (tf ) as
tf
L
(w) =
?
d
i
?L
T (w, d
i
)
and this quantity normalized by the number of to-
kens in L,
tf
L
(w) = tf
L
(w)/T
L
and this quantity as it relates to the term frequency
of this w in both corpora
tf
L|R
(w) =
tf
L
(w)
tf
L
(w) + tf
R
(w)
Each word is only displayed once in the Ven-
ncloud (see Figure 2, so if a word (w) only occurs
in R, it is always present in the right region, and
likewise for L and left. If w is in both L and R,
we examine the proportion of documents in each
that w is in and use this to determine in which re-
gion it should be displayed. In order to deal with
Figure 2: Three example Vennclouds, with tweets contain-
ing ?Orioles? on the left, ?Nationals? on the right, and com-
mon words in the middle. From top to bottom we allow pro-
gressively larger common clouds. The large common words
make sense ? both teams played a Chicago team and made
the playoffs in the time covered by these corpora.
the cases where w occurs in approximately similar
proportions of left and right documents, we have
a center region (in the center in Figure 2). We
define a threshold (?
Common
) to concretely define
?approximately similar?. Specifically,
? if tf
R
(w) = 0, w is displayed in left.
? if tf
L
(w) = 0, w is displayed in right.
? if tf
R
(w) > 0 and tf
L
(w) > 0,
? if tf
R|L
(w) > tf
L|R
(w) + ?
Common
, w
is displayed in right.
? if tf
L|R
(w) > tf
R|L
(w) + ?
Common
, w
is displayed in left.
? Otherwise, w is displayed in center.
The user is given a slider to control ?
Common
, al-
lowing them to determine what value of ?approx-
imately similar? best fits the data and their task at
hand.
9 Anecdotal Evaluation
We have not yet done a proper psychophysical
evaluation of the utility of dynamic wordclouds
27
[X] entitiesi i common cloudl tf filterl idf filteri l size controlsi l opacity controlsi l sort by legendl do not redraw highlight keywords i li
@ 
@nationals am 
baseball best bryce but 
cardinals cards clinch
come cubs dc did
getting go going got 
here home if
know 
louis lt marlins me 
nationals 
nats nl not or others
park philadelphia phillies pic 
see st stadium 
strasburg teddy 
today too u w/ 
was washington watching well werth 
work 
0 2 4 5 6 7 8 9 ? a about 
after again all 
amp and another any are as awesome baby back
bar be because been
before being believe better big boys
bring bullpen by call can can't 
checked chicago 
could damn day 
division do don't down east end
even ever every everyone excited fan fans feel 
field finally first for free
from fuck game games get 
give gonna good great had 
has have he hell him his
hope how i i'll i'm im
in inning into is it it's its just 
last league let let's lets life like little
lol long look looking lose losing loss love made
make man many 
more most much my need
next night no now of off oh on one only 
our out pitch play
playing playoff playoffs please postseason proud 
ready really right rt run runs said
say season series 
should since so some 
start still 
take team than
thank thanks that that's their them then there these they thing 
think this those time to 
tomorrow tonight two 
up vs wait want watch way we 
were what when where
who why will win winning wish with won world
would yeah year yes you your
1 1st 2012 3 @orioles against al an at
baltimore beat birds 
blue buck card 
espn 
gt hit 
johnson keep lead magic mark md
mlb new news o's 
orioles over photo place
post rangers ravens
red report 
sox sports 
tied 
white wild yahoo yankees yanks years
Words displayed occur at least 22 and at most 6590 times in the query.
Words displayed occur in fewer than 5 and more than 10000 documents in the whole corpus.
Larger words frequently occur in the query and rarely occur in the corpus (TF*IDF). [TF:0.92,IDF:1]
Darker words are more frequent in the query (TF). [TF:1]
Words are sorted alphabetically.
[X]
go o's @ oriole park at camden yards for boston red
sox vs baltimore orioles w/ 163 others
i'm at oriole park at camden yards for chicago white
sox vs baltimore orioles baltimore md
#orioles 5 things the boston red sox can learn from the
baltimore orioles oakland a's bleacher
#sportsroadhouse
[X]
nats white sox south carolina 100 to win 450
#orioles welcome to red sox nation orioles' and
nationals' fans #sportsroadhouse
god damn nats just tried to boo in a dc bar almost got
chased out since my sox are out i just want the cards
to do well #problems #mlb
file:///Users/gcoppersmith/src/dwc_dev/nats_v_orioles.html
1 of 1 4/24/14, 9:53 AM
Figure 3: Screenshot of a Venncloud, with controls. The sliders are accessible from the buttons across the top, displaying as
a floating window above the wordcloud itself (replacing the current display of the legend). Also note the examples in the lower
left and right corners, accessed by clicking on a word of interest (in this case ?Sox?).
and Vennclouds for various tasks as compared to
their static counterparts (and other visualizations).
In part, this is because such an evaluation requires
selection of inference tasks to be examined, pre-
cisely what we do not claim to be able to do. We
leave for future work the creation and evaluation
of a representative sample of such inference tasks.
We strongly believe that the plural of anecdote
is not data ? so these anecdotes are intended as
illustrations of use, rather than some data regard-
ing utility. The dynamic wordclouds and Ven-
nclouds were used on data from across the spec-
trum, from tweets to Shakespeare and political
speeches to health-related conversations in devel-
oping nations. In Shakespeare, character and place
names can easily be highlighted with one set of
slider settings (high tf*idf ), while comparisons
of stopwords are made apparent with another (high
tf , no idf ). Emerging from the debates between
Mitt Romney and Barack Obama are the common
themes that they discuss using similar (economics)
and dissimilar language (Obama talks about the
?affordable care act? and Romney calls it ?Oba-
macare?). These wordclouds were also used to do
some introspection on the output of classifiers in
sentiment analysis (Mitchell et al., 2013) and men-
tal health research (Coppersmith et al., 2014b) to
expose the linguistic signals that give rise to suc-
cessful (and unsuccessful) classification.
10 Conclusions and Future Directions
Exploratory data analysis tools for human lan-
guage data and inference tasks have long lagged
behind their numerical counterparts, and here we
investigate another step towards filling that need.
Rather than determining the optimal wordcloud,
we enable the wordcloud viewer to adapt the visu-
alization to the data and inference task at hand. We
suspect that the pendulum of control has swung
too far, and that there is a subset of the possi-
ble control configurations that produce useful and
informative wordclouds. Work is underway to
collect feedback via instrumented dynamic word-
clouds and Vennclouds as they are used for various
inference tasks to address this.
Previous research, logic, and intuition were
used to create this step, though it requires fur-
ther improvement and validation. We provide
anecdotes about the usefulness of these dynamic
wordclouds, but those anecdotes do not provide
sufficient evidence that this method is somehow
more efficient (in terms of human time) than ex-
isting methods. To make such claims, a controlled
human-factors study is required, investigating (for
a particular inference task) how this method af-
fects the job of an exploratory data analyst. In
the meantime, we hope making the code freely
available
1
will better enable our fellow researchers
to perform principled exploratory data analysis of
human language content quickly and encourage a
deeper understanding of data, within and across
disciplines.
Acknowledgments
We would like to thank Carey Priebe for in-
sightful discussions on exploratory data analysis,
1
from https://github.com/Coppersmith/vennclouds
28
Aleksander Yelskiy, Jacqueline Aguilar, Kristy
Hollingshead for their analysis, comments, and
improvements on early versions, and Ainsley
R. Coppersmith for permitting this research to
progress in her early months.
References
Scott Bateman, Carl Gutwin, and Miguel Nacenta.
2008. Seeing things in the clouds: the effect of vi-
sual features on tag cloud selections. In Proceedings
of the nineteenth ACM conference on Hypertext and
hypermedia, pages 193?202. ACM.
Quim Castella and Charles A. Sutton. 2013. Word
storms: Multiples of word clouds for visual compar-
ison of documents. CoRR, abs/1301.0503.
Cindy Chung and James W Pennebaker. 2007. The
psychological functions of function words. Social
communication, pages 343?359.
Christopher Collins, Fernanda B Viegas, and Martin
Wattenberg. 2009. Parallel tag clouds to explore
and analyze faceted text corpora. In Visual Analyt-
ics Science and Technology, 2009. VAST 2009. IEEE
Symposium on, pages 91?98. IEEE.
Glen Coppersmith, Mark Dredze, and Craig Harman.
2014a. Quantifying mental health signals in twitter.
In Proceedings of ACL Workshop on Computational
Linguistics and Clinical Psychology. Association for
Computational Linguistics.
Glen Coppersmith, Craig Harman, and Mark Dredze.
2014b. Measuring post traumatic stress disorder in
Twitter. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM).
Weiwei Cui, Yingcai Wu, Shixia Liu, Furu Wei,
Michelle X Zhou, and Huamin Qu. 2010. Context
preserving dynamic word cloud visualization. In
Pacific Visualization Symposium (PacificVis), 2010
IEEE, pages 121?128. IEEE.
Jason Davies. 2013. Wordcloud generator using d3,
April.
Micah Dubinko, Ravi Kumar, Joseph Magnani, Jas-
mine Novak, Prabhakar Raghavan, and Andrew
Tomkins. 2007. Visualizing tags over time. ACM
Transactions on the Web (TWEB), 1(2):7.
Jason Feinberg. 2013. Wordle, April.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in information Retrieval, 1(3):233?
334.
Kathrin Knautz, Simone Soubusta, and Wolfgang G
Stock. 2010. Tag clusters as information retrieval
interfaces. In System Sciences (HICSS), 2010 43rd
Hawaii International Conference on, pages 1?10.
IEEE.
Bongshin Lee, Nathalie Henry Riche, Amy K Karl-
son, and Sheelagh Carpendale. 2010. Spark-
clouds: Visualizing trends in tag clouds. Visualiza-
tion and Computer Graphics, IEEE Transactions on,
16(6):1182?1189.
Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open domain
targeted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654. Association for Com-
putational Linguistics.
Sa?sa Petrovic, Miles Osborne, Richard McCreadie,
Craig Macdonald, Iadh Ounis, and Luke Shrimpton.
2013. Can twitter replace newswire for breaking
news. In Seventh International AAAI Conference on
Weblogs and Social Media.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In Proceedings
of the workshop on Comparing Corpora, pages 1?6.
Association for Computational Linguistics.
AW Rivadeneira, Daniel M Gruen, Michael J Muller,
and David R Millen. 2007. Getting our head in the
clouds: toward evaluation studies of tagclouds. In
Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 995?998. ACM.
Johann Schrammel, Michael Leitner, and Manfred
Tscheligi. 2009. Semantically structured tag
clouds: an empirical evaluation of clustered presen-
tation approaches. In Proceedings of the 27th inter-
national conference on Human factors in computing
systems, pages 2037?2040. ACM.
Christin Seifert, Barbara Kump, Wolfgang Kienreich,
Gisela Granitzer, and Michael Granitzer. 2008. On
the beauty and usability of tag clouds. In Informa-
tion Visualisation, 2008. IV?08. 12th International
Conference, pages 17?25. IEEE.
James Sinclair and Michael Cardew-Hall. 2008. The
folksonomy tag cloud: when is it useful? Journal of
Information Science, 34(1):15?29.
Aidan Slingsby, Jason Dykes, Jo Wood, and Keith
Clarke. 2007. Interactive tag maps and tag
clouds for the multiscale exploration of large spatio-
temporal datasets. In Information Visualization,
2007. IV?07. 11th International Conference, pages
497?504. IEEE.
Fernanda B Vi?egas and Martin Wattenberg. 2008.
Timelines tag clouds and the case for vernacular vi-
sualization. interactions, 15(4):49?52.
Jeffrey S Vitter. 1985. Random sampling with a reser-
voir. ACM Transactions on Mathematical Software
(TOMS), 11(1):37?57.
Hadley Wickham. 2009. ggplot2: elegant graphics for
data analysis. Springer Publishing Company, Incor-
porated.
29
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 51?60,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Quantifying Mental Health Signals in Twitter
Glen Coppersmith Mark Dredze Craig Harman
Human Language Technology Center of Excellence
Johns Hopkins University
Balitmore, MD, USA
Abstract
The ubiquity of social media provides a
rich opportunity to enhance the data avail-
able to mental health clinicians and re-
searchers, enabling a better-informed and
better-equipped mental health field. We
present analysis of mental health phe-
nomena in publicly available Twitter data,
demonstrating how rigorous application of
simple natural language processing meth-
ods can yield insight into specific disor-
ders as well as mental health writ large,
along with evidence that as-of-yet undis-
covered linguistic signals relevant to men-
tal health exist in social media. We present
a novel method for gathering data for
a range of mental illnesses quickly and
cheaply, then focus on analysis of four in
particular: post-traumatic stress disorder
(PTSD), depression, bipolar disorder, and
seasonal affective disorder (SAD). We in-
tend for these proof-of-concept results to
inform the necessary ethical discussion re-
garding the balance between the utility of
such data and the privacy of mental health
related information.
1 Introduction
While mental health issues pose a significant
health burden on the general public, mental health
research lacks the quantifiable data available to
many physical health disciplines. This is partly
due to the complexity of the underlying causes
of mental illness and partly due to longstanding
societal stigma making the subject all but taboo.
Lack of data has hampered mental health research
in terms of developing reliable diagnoses and ef-
fective treatment for many disorders. Moreover,
population-level analysis via traditional methods
is time consuming, expensive, and often comes
with a significant delay.
In contrast, social media is plentiful and has
enabled diverse research on a wide range of top-
ics, including political science (Boydstun et al.,
2013), social science (Al Zamal et al., 2012), and
health at an individual and population level (Paul
and Dredze, 2011; Dredze, 2012; Aramaki et al.,
2011; Hawn, 2009). Of the numerous health top-
ics for which social media has been considered,
mental health may actually be the most appropri-
ate. A major component of mental health research
requires the study of behavior, which may be man-
ifest in how an individual acts, how they com-
municate, what activities they engage in and how
they interact with the world around them includ-
ing friends and family. Additionally, capturing
population level behavioral trends from Web data
has previously provided revolutionary capabilities
to health researchers (Ayers et al., 2014). Thus,
social media seems like a perfect fit for study-
ing mental health in both individual and overall
trends in the population. Such topics have already
been the focus of several studies (Coppersmith et
al., 2014; De Choudhury et al., 2014; De Choud-
hury et al., 2013d; De Choudhury et al., 2013b;
De Choudhury et al., 2013c; Ayers et al., 2013).
What can we expect to learn about mental health
by studying social media? How does a service like
Twitter inform our knowledge in this area? Nu-
merous studies indicate that language use, social
expression and interaction are telling indicators of
mental health. The well-known Linguistic Inquiry
Word Count (LIWC), a validated tool for the psy-
chometric analysis of language data (Pennebaker
et al., 2007), has been repeatedly used to study
language associated with all types of disorders
(Resnik et al., 2013; Alvarez-Conrad et al., 2001;
Tausczik and Pennebaker, 2010). Furthermore, so-
cial media is by nature social, which means that
social patterns, a critical part of mental health and
illness, may be readily observable in raw Twitter
data. Thus, Twitter and other social media provide
51
a unique quantifiable perspective on human behav-
ior that may otherwise go unobserved, suggesting
it as a powerful tool for mental health researchers.
The main vehicle for studying mental health in
social media has been the use of surveys, e.g.,
depression battery (De Choudhury, 2013) or per-
sonality test (Schwartz et al., 2013), to deter-
mine characteristics of a user coupled with analyz-
ing their corresponding social media data. Work
in this area has mostly focused on depression
(De Choudhury et al., 2013d; De Choudhury et al.,
2013b; De Choudhury et al., 2013c), and the num-
ber of users is limited by those that can complete
the appropriate survey. For example, De Choud-
hury et al. (2013d) solicited Twitter users to take
the CES-D and to share their public Twitter pro-
file, analyzing linguistic and behavioral patterns.
While this type of study has produced high qual-
ity data, it is limited in size (by survey respon-
dents) and scope (to diagnoses which have a bat-
tery amenable to administration over the internet).
In this paper we examine a range of mental
health disorders using automatically derived sam-
ples from large amounts of Twitter data. Rather
than rely on surveys, we automatically identify
self-expressions of mental illness diagnoses and
leverage these messages to construct a labeled data
set for analysis. Using this dataset, we make the
following contributions:
? We demonstrate the effectiveness of our au-
tomatically derived data by showing that sta-
tistical classifiers can differentiate users with
four different mental health disorders: de-
pression, bipolar, post traumatic stress disor-
der and seasonal affective disorder.
? We conduct a LIWC analysis of each dis-
order to measure deviations in each illness
group from a control group, replicating pre-
vious findings for depression and providing
new findings for bipolar, PTSD and SAD.
? We conduct an open-vocabulary analysis that
captures language use relevant to mental
health beyond what is captured with LIWC.
Our results open the door to a range of large scale
analysis of mental health issues using Twitter.
2 Related Work
For a good retrospective and prospective sum-
mary of the role of social media in mental health
research, we refer the reader to De Choudhury
(2013). De Choudhury identifies ways in which
NLP has and can be used on social media data to
produce what the relevant mental health literature
would predict, both at an individual level and a
population level. She proceeds to identify ways
in which these types of analyses can be used in
the near and far term to influence mental health
research and interventions alike.
Differences in language use have been observed
in the personal writing of students who score
highly on depression scales (Rude et al., 2004),
forum posts for depression (Ramirez-Esparza et
al., 2008), self narratives for PTSD (He et al.,
2012; D?Andrea et al., 2011; Alvarez-Conrad et
al., 2001), and chat rooms for bipolar (Kramer
et al., 2004). Specifically in social media, dif-
ferences have previously been observed between
depressed and control groups (as assessed by
internet-administered batteries) via LIWC: de-
pressed users more frequently use first person pro-
nouns (Chung and Pennebaker, 2007) and more
frequently use negative emotion words and anger
words on Twitter, but show no differences in posi-
tive emotion word usage (Park et al., 2012). Simi-
larly, an increase in negative emotion and first per-
son pronouns, and a decrease in third person pro-
nouns, (via LIWC) is observed, as well as many
manifestations of literature findings in the pattern
of life of depressed users (e.g., social engagement,
demographics) (De Choudhury et al., 2013d). Dif-
ferences in language use in social media via LIWC
have also been observed between PTSD and con-
trol groups (Coppersmith et al., 2014).
For population-level analysis, surveys such as
the Behavioral Risk Factor Surveillance System
(BRFSS) are conducted via telephone (Centers
for Disease Control and Prevention (CDC), 2010).
Some of these surveys cover relatively few par-
ticipants (often in the thousands), have significant
cost, and have long delays between data collec-
tion and dissemination of the findings. However,
De Choudhury et al. (2013c) presents a promising
population-level analysis of depression that high-
lights the role of NLP and social media.
3 Data
All data we obtain is public, posted between
2008 and 2013, and made available from Twitter
via their application programming interface (API).
Specifically, this does not include any data that has
52
Genuine Statements of Diagnosis
In loving memory my mom, she was only 42, I was 17 & taken away from me. I was diagnosed with having P.T.S.D LINK
So today I started therapy, she diagnosed me with anorexia, depression, anxiety disorder, post traumatic stress disorder and
wants me to
@USER The VA diagnosed me with PTSD, so I can?t go in that direction anymore
I wanted to share some things that have been helping me heal lately. I was diagnosed with severe complex PTSD and... LINK
Disingenuous Statements of Diagnosis
?I think I?m I?m diagnosed with SAD. Sexually active disorder? -anonymous
LOL omg my bro the ?psychologist? just diagnosed me with seasonal ADHD AHAHAHAAAAAAAAAAA IM DYING.
The winter blues: Yesterday I was diagnosed with seasonal affective disorder. Now, this sounds a lot more dramat... LINK
Table 1: Examples found via regular expression keyword search for diagnosis tweets.
been marked as ?private? by the author or any di-
rect messages.
Diagnosed Group We seek users who publicly
state that they have been diagnosed with various
mental illnesses. Users may make such a state-
ment to seek support from others in their social
network, to fight the taboo of mental illness, or
perhaps as an explanation of some of their behav-
ior. Tweets were obtained using regular expres-
sions on a large multi-year health related collec-
tion, e.g. ?I was diagnosed with X.? We searched
for four conditions: depression, bipolar disorder,
post traumatic stress disorder (PTSD) and sea-
sonal affective disorder (SAD). The matched diag-
nosis tweets were manually labeled as to whether
the tweet contained a genuine statement of a men-
tal health diagnosis. Table 1 shows examples of
both genuine statements of diagnosis and disin-
genuous statements (often jokes or quotes).
Next, we retrieved the most recent tweets (up
to 3200) for each user with a genuine diagnosis
tweet. We then filtered the users to remove those
with fewer than 25 tweets and those whose tweets
were not at least 75% in English (measured using
the Compact Language Detector
1
). These filter-
ing steps left us with users that were considered
positive examples. Table 2 indicates the number
of users and tweets found for each of the mental
health categories examined. We manually exam-
ined and annotated only half the diagnosis state-
ments for depression ? indicating there are likely
800-900 depression users available via these auto-
matic methods from our collection, compared to
the 117 obtained via the methods of De Choud-
hury et al. (2013d). Additionally, we emphasize
the low cost and effort of our automated effort
as compared to their crowdsourced survey meth-
1
https://code.google.com/p/cld2/
ods. The difference in collection methods also
suggests that the two have a reasonable chance of
being complementary. This is especially signif-
icant when considering disorders with lower in-
cidence rates than depression (arguably the high-
est), where respondents to crowdsourced surveys
or self-stated diagnoses alike are rare.
This method is similar in spirit to that of De
Choudhury et al. (2013c), where they inferred
a tweet-level classifier for depression from user-
level labels (specifically, tweets from the past three
months from users scoring highly on CES-D for
the positive class and conversely for the negative).
Control Group To build models for analysis
and to validate the data, we also need a sample of
the general population to use as an approximation
of community controls. We follow a similar pro-
cess: randomly select 10k usernames from a list
of Twitter users who posted to a separate random
historical collection within a selected two week
window, downloaded the 3200 most recent tweets
from these users, and apply our two filters: at least
25 tweets and 75% English. This yields a control
group of 5728 random users, whose 13.7 million
tweets were used as negative examples.
Caveats Our method for finding users with
mental health diagnoses has significant caveats: 1)
the method may only capture a subpopulation of
each disorder (i.e., those who are speaking pub-
licly about what is usually a very private mat-
ter), which may not truly represent all aspects of
the population as a whole. 2) This method in
no way verifies whether this diagnosis is genuine
(i.e., people are not always truthful in self-reports).
However, given the stigma often associated with
mental illness, it seems unlikely users would tweet
that they are diagnosed with a condition they do
not have. 3) The control group is likely contami-
53
Match Users Tweets
Bipolar 6k 394 992k
Depression 5k 441 1.0m
PTSD 477 244 573k
SAD 389 159 421k
Control 10k 5728 13.7m
Table 2: Number of users matching the diagnosis regular
expression, users labeled with genuine diagnoses and tweets
retrieved from diagnosed users for each mental health condi-
tion.
nated by the presence of users that are diagnosed
with the various conditions investigated. We make
no attempt to remove these users, and if we as-
sume that the prevalence of each disorder in the
general population is similar in our control groups,
we likely have hundreds of such diagnosed users
contaminating our control training data. 4) Twitter
users are not an entirely representative sample of
the population as a whole. Despite these caveats,
we find that this method yielded promising results
as discussed in the next sections.
Comorbidity Since some of these disorders
have high comorbidity, there are some users in
more than one class (e.g., those that state a diagno-
sis for PTSD and depression): Bipolar and depres-
sion have 19 users in common (4.8% of the bipo-
lar users, 4.3% of the depression users), PTSD and
depression share 10 (4.0% of PTSD, 2.2% of de-
pression), and bipolar and PTSD share 9 (2.2% of
bipolar, 3.6% of PTSD). Two users state diagnosis
of bipolar, PTSD and depression (less than 1% of
each set). No users stated diagnoses of both SAD
and any other condition investigated.
4 Methods
We quantify various aspects of each user?s lan-
guage usage and pattern of life via automated
methods, extracting features for subsequent ma-
chine learning. We use these to (1) replicate pre-
vious findings, (2) build classifiers to separate di-
agnosed from control users, and (3) introspect on
those classifiers. Introspection here shows us what
quantified signals in the content the classifiers base
their decision on, and thus we can gain intuition
about what signals are present in the content rele-
vant to mental health.
4.1 Linguistic Inquiry Word Count (LIWC)
LIWC provides clinicians with a tool for gather-
ing quantitative data regarding the state of a pa-
tient from the patient?s writing (Pennebaker et al.,
2007). Previous work has found signal in the ?pos-
itive affect? and ?negative affect? categories of the
LIWC when applied to social media (including
Twitter), so we examine their correlations sepa-
rately, as well as in the context of other LIWC
categories (De Choudhury et al., 2013a). In all,
we examine some of the LIWC categories directly
(Swear, Anger, PosEmo, NegEmo, Anx) and com-
bine pronoun classes by linguistic form: I and We
classes are combined to form Pro1, You becomes
Pro2 and SheHe and They become Pro3. Each of
these classes provides one feature used by subse-
quent machine learning and our other analyses.
4.2 Language Models (LMs)
Language models are commonly used to estimate
how likely a given sequence of words is. Gener-
ally, an n-gram language model refers to a model
that examines strings of up to n words long. This
is less than ideal for applications in social me-
dia: spelling errors, shortenings, space removal,
and other aspects of social media data (especially
Twitter) confounds many traditional word-based
approaches. Thus, we employ two LMs, first a
traditional 1-gram LM (ULM) that examines the
probability of each whole word. Second, a char-
acter 5-gram LM (CLM) to examine sequences of
up to 5 characters.
LMs model the likelihood of sequences from
training data. In our case, we build one of each
model from the positive class (tweets from one
class of diagnosed users ? e.g., PTSD), yield-
ing ULM
+
and CLM
+
. We also build one of
each model from the negative class (control users),
yielding ULM
?
and CLM
?
. We score each tweet
by computing these probabilities and classifying it
according to which model has a higher probability
(e.g., for a given tweet, is ULM
+
> ULM
?
?).
4.3 Pattern of Life Analytics
For brevity, we only briefly discuss the pattern of
life analytics, since they do not depend on sig-
nificant NLP. They examine how correlates found
to be significant in the mental health literature
may manifest and be measured in social media
data. These are all imperfect proxies for the find-
ings from the literature, but our experiments will
demonstrate that they do collectively provide in-
formation relevant to mental health.
For each of the following analytics we extract
one feature to use in subsequent machine learn-
ing. Social engagement has been correlated with
54
??
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?? ??
?
?
?
?
?
?
??
??
???
?
?
?
?
?
??
?
?
?
?
?
?
??
??
?
?
?
??? ??
?
? ?
?
??
?
? ?
???
?
?
????
?
???
?
?
?
???
?
?? ?
?
??
? ?
??
?
?
?
??
??
?
?
?
?
?????
?
?
?
?
?
?
?
?
??
?
?
?
??
?
?
?
?
?
?
?
?
?
???
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
0.00
0.05
0.10
0.15 Pro1* * Pro2* Pro3* Swear* Anger* PosEmo NegEmo* Anxiety****
0
0.00
5
0.01
0.01
5
Figure 1: Box and whiskers plot of proportion of tweets each user has (y-axis) matching various LIWC categories. Each
bar represents one LIWC category for one condition ? PTSD in purple, depression in blue, SAD in orange, bipolar in red and
control in gray. Anxiety occurs an order of magnitude less often than the others, so its proportion is on the right y-axis (and thus
not comparable to the others). Statistically significant deviations from control users are denoted by asterisks.
positive mental health outcomes (Greetham et al.,
2011; Berkman et al., 2000; Organization, 2001;
De Choudhury et al., 2013d), which is difficult
to measure directly so we examine various ways
in which this may be manifest in a user?s tweet
stream: Tweet rate measures how often a twit-
ter user posts (a measure of overall engagement
with this social media platform) and Proportion
of tweets with @mentions measures how often
a user posts ?in conversation? (for lack of better
terms) with other users. Number of @mentions is
a measure of how often the user in question en-
gages other users, while Number of self @men-
tions is a measure of how often the user responds
to mentions of themselves (since users rarely in-
clude their own username in a tweet). To estimate
the size of a user?s social network, we calculate
Number of unique users @mentioned and Number
of users @mentioned at least 3 times, respectively.
For each of the following analytics, we calcu-
late the proportion of a user?s tweets that the ana-
lytic finds evidence in: Insomnia and sleep distur-
bance is often a symptom of mental health disor-
ders (Weissman et al., 1996; De Choudhury et al.,
2013d), so we calculate the proportion of tweets
that a user makes between midnight and 4am ac-
cording to their local timezone. Exercise has
also been correlated with positive mental health
outcomes (Penedo and Dahn, 2005; Callaghan,
2004), so we examine tweets mentioning one of a
small set of exercise-related terms. We also use an
English sentiment analysis lexicon from Mitchell
et al. (2013) to score individual tweets according
to the presence and valence of sentiment words.
We apply no thresholds, so any tweet with a senti-
ment score above 0 was considered positive, below
0 was considered negative, and those with score 0
were considered to have no sentiment. Thus we
use the proportion of Insomnia, Exercise, Positive
Sentiment and Negative Sentiment tweets as fea-
tures in subsequent machine learning and analysis.
5 Results
We present three types of experiments to evalu-
ate the quality and character of these data, and to
demonstrate some quantifiable mental health sig-
nals in Twitter. First, we validate our method for
obtaining data by replicating previous findings us-
ing LIWC. Next, we build classifiers to distinguish
each group from the control group, demonstrating
that there is useful signal in the language of each
group, and compare these classifiers. Finally, we
analyze the correlations between our analytics and
classifiers to uncover relationships between them
and derive insight into quantifiable and relevant
mental health signals in Twitter.
Validation First, we provide some validation
for our novel method for gathering samples. We
demonstrate that language use, as measured by
LIWC, is statistically significantly different be-
tween control and diagnosed users. Figure 1
shows the proportion of tweets from each user
that scores positively on various LIWC categories
(i.e., have at least one word from that category).
Box-and-whiskers plots (Tukey, 1977)
2
summa-
rize a distribution of observations and ease com-
2
For a modern implementation see Wickham (2009).
55
False Alarm: 0.1 0.2
Bipolar 0.64 0.82
Depression 0.48 0.68
PTSD 0.67 0.81
SAD 0.42 0.65
Figure 2: ROC curves for separating diagnosed from con-
trol users, compared across disorders: bipolar in red, depres-
sion in blue, PTSD in purple, SAD in orange. The preci-
sion (diagnosed, correctly labeled) for each disorder at false
alarm (control, labeled as diagnosed) rates of 10% and 20%
are shown to the right of the ROC curve. Chance performance
is indicated by the dotted black line.
parison between them (here, each observation is
the proportion of a user?s tweets that score posi-
tively on LIWC). The median of the distribution
is the black horizontal line in the middle of the
bar, the bar covers the inter quartile range (where
50% of the observations lie), the whiskers are a
robust estimate of the extent of the data, with out-
liers plotted as circles beyond the whiskers. An
approximation of statistical significance is indi-
cated by the pinched in notches on each bar. If
the notches on the bars do not overlap, the dif-
ferences between those distributions is different
(?<0.05, 95% confidence interval). Each bar is
colored according to diagnosis, and each group
of 5 bars notes the scores for one LIWC cat-
egory. Differences that reach statistical signifi-
cance from the control group are noted with as-
terisks (e.g., Pro1, Swear, Anger, NegEmo and
Anxiety are statistically significantly different for
the depression group). Importantly, this repli-
cates previous findings of significant differences
between depressed users (according to an internet-
administered diagnostic battery): significant in-
creases are expected in NegEmo, Anger, Pro1 and
Pro3 and no change in PosEmo, given all previous
work (Park et al., 2012; Chung and Pennebaker,
2007; De Choudhury et al., 2013d). We repli-
cate all these findings except the increase in Pro3
(which only De Choudhury et al. (2013d) found),
which validates our data collection methods.
Classification We next explore the ability of
the various analytics to separate diagnosed from
control users and assess performance on a leave-
one-out cross-validation task. We train a log lin-
ear classifier on the features described in ?4 using
scikit-learn (Pedregosa et al., 2011).
Bipolar Depression
PTSD SAD
Figure 3: ROC curves of performance of individual analyt-
ics for each disorder: LIWC in blue, pattern of life in yellow,
CLM in red, ULM in green, all in black. Chance performance
is indicated by the dotted black line.
The receiver operating characteristic (ROC)
curves in Figures 2 and 3 demonstrate perfor-
mance of the various classifiers at the task of sepa-
rating diagnosed from control groups. In all cases,
the correct detections (or hits) are on the y-axis
and the false detections (or false alarms) are on
the x-axis. Figure 2 compares performance across
diagnoses, one line per disorder.
Figure 3 shows one plot per mental health con-
dition, with the performance of the various an-
alytics, individually and in concert as individual
ROC curves. A few trends emerge ? 1) All an-
alytics show some ability to separate the classes,
indicating they are finding useful signals. 2) The
LMs provide superior performance to the other an-
alytics, indicating there are more signals present
in the language than are captured by LIWC and
pattern-of-life analytics. For readability we do not
show the performance of all combinations of an-
alytics, but they perform as expected: any set of
them perform equal to or better than their indi-
vidual components. Taken together, this indicates
that there is information relevant to separating di-
agnosed users from controls in all the analytics
discussed here. Furthermore, this highlights that
there remains significant signals to be uncovered
and understood in the language of social media.
These trends also allow us to compare the dis-
orders as manifest in language usage, though this
56
tends to raise more questions than it answers. Gen-
erally, the pattern-of-life analytics and LIWC are
on par, but this is decidedly not true for depres-
sion, where pattern-of-life seems to perform espe-
cially poorly, and for SAD, where pattern-of-life
seems to perform especially well. This indicates
that the depression users have patterns-of-life that
look more similar to the controls than is the case
for the other disorders (perhaps especially surpris-
ing given the inclusion of the sentiment lexicon)
and that there may be significant correlation be-
tween pattern-of-life factors and SAD.
5.1 Analytic Introspection
To examine correlations between the analytics and
the linguistic content they depend on, we scored
a random subset of 1 million tweets from control
users with each of the linguistic analytics, and plot
their Pearson?s correlation coefficients (r) in Fig-
ure 4. A simple overlap of wordlists is not suf-
ficient to assess the true utility of these methods
since it does not take into account the frequency
of occurrence of each word, nor the correlation be-
tween these words in real data (e.g., does a classi-
fier based on the LIWC category Swear provide
redundant information to the sentiment analysis).
Each row and column in Figure 4 represents one of
the 17 analytics, in the same order. Colors denote
Bonferroni-corrected Pearson?s r for statistically
significant correlations between the analytic on the
row and column. Correlations that do not reach
statistical significance are in aquamarine (corre-
sponding to r=0). Excluded for brevity is a sanity
check of a ?
2
test between the analytics to assert
they were scoring significantly differently.
The strong correlations between the various
LIWC analytics, notably Swear, Anger and
NegEmo, likely indicates that the analytics are
triggered by the same word(s) ? in this case pro-
fanity. Similarly for LIWC?s PosEmo and the sen-
timent lexicon ? ?happy? for example. The corre-
lation between CLM for various diagnoses is par-
ticularly intriguingly, as it is in line with known
patterns of comorbidity: major depressive disor-
der, PTSD, and bipolar all have observed comor-
bidity (Brady et al., 2000; Campbell et al., 2007;
McElroy et al., 2001) while SAD is currently con-
sidered a specifier of major depressive disorder or
bipolar disorder (American Psychiatric Associa-
tion, 2013; Lurie et al., 2006), without published
findings indicating comorbidity. Indeed our small
Figure 4: Pearson?s r correlations between various analyt-
ics, color indicates the strength of statistically significant cor-
relations, or 0 (aquamarine) otherwise. Bonferroni corrected,
each comparison is significant only if ?<0.0002). Rows and
columns represent the analytics in the same order, so the di-
agonal is self-correlation.
sample dataset follows the same trends, where
we observed users with multiple diagnoses exist
within depression, PTSD, and bipolar, but none
exist with SAD. The correlation observed is too
large to be solely attributed to those users shared
between the groups, though (correlations at most
r = 0.05 would be attributable to that alone). Fur-
thermore, when taken in combination with the dif-
ferent patterns exhibited by the groups as seen in
Figure 1, this correlation is not solely attributable
to LIWC categories either. At its core, these cor-
relations seem to suggest that similar language
is employed by users diagnosed with these occa-
sionally comorbid disorders, and dissimilar lan-
guage by users with SAD. This should be taken as
merely suggestive of the type of analysis one could
do, though, since the literature does not present a
strong and clear prediction for the comorbidity and
exhibited symptoms (to include language use).
Interestingly, the lack of (or negative) correla-
tion between most of the analytics again highlights
the complexity of the mental illnesses and the di-
vergent signals it presents. Additionally, the lack
of correlation between ULM and the other models
is to be expected, since they are basing their scores
on significantly more words (or different signals as
is the case for CLM). Each one of these analytics is
highly imperfect, and often give contradictory ev-
idence, but when combined, the machine learning
algorithms are able to sort through the conflicting
signals with some success.
57
Analytic Example Tweet Text
Bipolar LM I?m insecure because being around your ex of 4 years little sister, makes me feel a slight bit uncom-
fortable. Ok.
Depression LM Pain has a weird way of working. You?re still the same person from before the pain, but that person is
underneath & doesn?t come out.
PTSD LM Don?t wanna get out my bed but I really need to get up & prepare myself for work
Sentiment(+) NAME is absolutely unbelievable, he just gets better and better every time I see him. The best play in
the world, no doubt about it.
Sentiment(-) I hate losing people in my life. I try so hard to not let it happen
PosEmo Wowee...that was a hectic day... Got more done than expected but so glad to be in bed now. Grateful
for my supportive husband & loving pooch
Functioning if i had a dollar for all the grammatical errors ive ever typed, my college tuition, book cost, and dorm
rent would be paid in full
NegEmo My tooth hurts, my neck hurts, my mouth hurts, my toungue hurts, my head hurts...kill me now.
Anx don?t stress over someone who is going to stress over you..
Anger Ugly n arrogant sums everytin up.shdnt hv ffd her seff
Table 3: Example high scoring tweets from each analytic.
6 Conclusion
We demonstrate quantifiable signals in Twitter
data relevant to bipolar disorder, major depres-
sive disorder, post-traumatic-stress disorder and
seasonal affective disorder. We introduce a novel
method for automatic data collection and validate
its veracity by 1) replicating observations of sig-
nificant differences between depressed and control
user groups and 2) constructing classifiers capa-
ble of separating diagnosed from control users for
each disorder. This data allows us to demonstrate
equivalent differences in language use (according
to LIWC) for bipolar, PTSD, and SAD. Further-
more, we provide evidence that more information
relevant to mental health is encoded in language
use in social media (above and beyond that cap-
tured by methods based on the mental health lit-
erature). By examining correlations between the
various analytics investigated, we provide some
insight into what quantifiable linguistic informa-
tion is captured by our classifiers. We finally
demonstrate the utility of examining multiple dis-
orders simultaneously and other larger analyses,
difficult or impossible with other methods.
Crucially, we expect that these novel data col-
lection methods can provide complementary infor-
mation to existing survey-based methods, rather
than supplant them. For many disorders rarer
than depression (which has comparatively high in-
cidence rates), we suspect that finding any data
will be a challenge, in which case combining
these methods with the existing survey collection
methods may be the best way to obtain sufficient
amounts of data for statistical analyses.
Since the LMs take more information into ac-
count when modeling the language usage of di-
agnosed and control users, it is unsurprising that
they outperform LIWC and pattern-of-life analy-
ses alone, but this is evidence of as-of-yet undis-
covered linguistic differences between diagnosed
and control users for all disorders investigated.
Uncovering and interpreting these signals can be
best accomplished through collaboration between
NLP and mental health researchers.
Naturally, some caveats come with these re-
sults: while identifying genuine self-statements of
diagnosis in Twitter works well for some condi-
tions, others exist for which there were few or
no diagnoses stated. For Alzheimer?s, the demo-
graphic with the majority of diagnoses does not
frequently use Twitter (or likely any social me-
dia). Eating disorders are also elusive via this
method, though related automatic methods (e.g.,
using disorder-related hashtags) may address this.
Finally, those willing to publicly reveal a mental
health diagnosis may not be representative of the
population suffering from that mental illness.
All these experiments, taken together, indicate
that there are a diverse set of quantifiable signals
relevant to mental health observable in Twitter.
They indicate that individual- and population-level
analyses can be made cheaper and more timely
than current methods, yet there remains as-of-yet
untapped information encoded in language use ?
promising a rich collaboration between the fields
of natural language processing and mental health.
Acknowledgments: The authors would like to
thank Kristy Hollingshead for thoughtful com-
ments and contributions throughout this research.
58
References
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Jennifer Alvarez-Conrad, Lori A. Zoellner, and
Edna B. Foa. 2001. Linguistic predictors of trauma
pathology and physical health. Applied Cognitive
Psychology, 15(7):S159?S170.
American Psychiatric Association. 2013. Diagnostic
Statistical Manual 5. American Psychiatric Associ-
ation.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using twitter. In Empirical Natural Lan-
guage Processing Conference (EMNLP).
John W. Ayers, Benjamin M. Althouse, Jon-Patrick
Allem, J. Niels Rosenquist, and Daniel E. Ford.
2013. Seasonality in seeking mental health infor-
mation on google. American journal of preventive
medicine, 44(5):520?525.
John W. Ayers, Benjamin M. Althouse, and Mark
Dredze. 2014. Could behavioral medicine lead the
web data revolution? Journal of the American Med-
ical Association (JAMA), February 27.
Lisa F. Berkman, Thomas Glass, Ian Brissette, and
Teresa E. Seeman. 2000. From social integration
to health: Durkheim in the new millennium? Social
Science & Medicine, 51(6):843?857, September.
Amber Boydstun, Rebecca Glazier, Timothy Jurka, and
Matthew Pietryka. 2013. Examining debate effects
in real time: A report of the 2012 React Labs: Ed-
ucate study. The Political Communication Report,
23(1), February. [Online; accessed 25-February-
2014].
Kathleen T. Brady, Therese K. Killeen, Tim Brewerton,
and Sylvia Lucerini. 2000. Comorbidity of psy-
chiatric disorders and posttraumatic stress disorder.
Journal of Clinical Psychiatry.
Patrick Callaghan. 2004. Exercise: a neglected inter-
vention in mental health care? Journal of Psychi-
atric and Mental Health Nursing, 11:476?483.
Duncan G. Campbell, Bradford L. Felker, Chuan-Fen
Liu, Elizabeth M. Yano, JoAnn E. Kirchner, Domin
Chan, Lisa V. Rubenstein, and Edmund F. Chaney.
2007. Prevalence of depression-PTSD comorbidity:
Implications for clinical practice guidelines and pri-
mary care-based interventions. Journal of General
Internal Medicine, 22(6):711?718.
Centers for Disease Control and Prevention (CDC).
2010. Behavioral risk factor surveillance system
survey data.
Cindy Chung and James Pennebaker. 2007. The psy-
chological functions of function words. Social com-
munication, pages 343?359.
Glen A. Coppersmith, Craig T. Harman, and Mark
Dredze. 2014. Measuring post traumatic stress
disorder in Twitter. In Proceedings of the Interna-
tional AAAI Conference on Weblogs and Social Me-
dia (ICWSM).
Wendy D?Andrea, Pearl H. Chiu, Brooks R. Casas,
and Patricia Deldin. 2011. Linguistic predictors of
post-traumatic stress disorder symptoms following
11 September 2001. Applied Cognitive Psychology,
26(2):316?323, October.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013a. Major life changes and behav-
ioral markers in social media: Case of childbirth. In
Proceedings of the ACM Conference on Computer
Supported Cooperative Work and Social Computing
(CSCW).
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013b. Predicting postpartum changes in
emotion and behavior via social media. In Proceed-
ings of the ACM Annual Conference on Human Fac-
tors in Computing Systems (CHI), pages 3267?3276.
ACM.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013c. Social media as a measurement
tool of depression in populations. In Proceedings of
the Annual ACM Web Science Conference.
Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013d. Predicting de-
pression via social media. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).
Munmun De Choudhury, Andres Monroy-Hernandez,
and Gloria Mark. 2014. ? narco? emotions: Affect
and desensitization in social media during the mexi-
can drug war.
Munmun De Choudhury. 2013. Role of social media
in tackling challenges in mental health. In Proceed-
ings of the 2nd International Workshop on Socially-
Aware Multimedia, pages 49?52.
Mark Dredze. 2012. How social media will change
public health. IEEE Intelligent Systems, 27(4):81?
84.
Danica Vukadinovic Greetham, Robert Hurling,
Gabrielle Osborne, and Alex Linley. 2011. Social
networks and positive and negative affect. Procedia
- Social and Behavioral Sciences, 22:4?13, January.
Carleen Hawn. 2009. Take Two Aspirin And Tweet
Me In The Morning: How Twitter, Facebook, And
Other Social Media Are Reshaping Health Care.
Health Affairs, 28(2):361?368.
59
Qiwei He, Bernard P. Veldkamp, and Theo de Vries.
2012. Screening for posttraumatic stress disorder
using verbal features in self narratives: A text min-
ing approach. Psychiatry Research.
Adam D. I. Kramer, Susan R. Fussell, and Leslie D.
Setlock. 2004. Text analysis as a tool for analyz-
ing conversation in online support groups. In Pro-
ceedings of the ACM Annual Conference on Human
Factors in Computing Systems (CHI).
Stephen J. Lurie, Barbara Gawinski, Deborah Pierce,
and Sally J. Rousseau. 2006. Seasonal affective dis-
order. American family physician, 74(9).
Susan L. McElroy, Lori L. Altshuler, Trisha Suppes,
Paul E. Keck, Mark A. Frye, Kirk D. Denicoff,
Willem A. Nolen, Ralph W. Kupka, Gabriele S. Lev-
erich, Jennifer R. Rochussen, A. John Rush Rush,
and Robert M. Post Post. 2001. Axis I psychi-
atric comorbidity and its relationship to historical ill-
ness variables in 288 patients with bipolar disorder.
American Journal of Psychiatry, 158(3):420?426.
Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open domain
targeted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654.
World Health Organization. 2001. The world health
report 2001 - Mental health: New understanding,
new hope. Technical report, Genf, Schweiz.
Minsu Park, Chiyoung Cha, and Meeyoung Cha. 2012.
Depressive moods of users portrayed in Twitter. In
Proceedings of the ACM SIGKDD Workshop on
Healthcare Informatics (HI-KDD).
Michael J. Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
and Matthieu Perrot
?
Edouard Duchesnay. 2011.
scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825?2830.
Frank J. Penedo and Jason R. Dahn. 2005. Exer-
cise and well-being: a review of mental and phys-
ical health benefits associated with physical activ-
ity. Current Opinion in Psychiatry, 18(2):189?193,
March.
James W. Pennebaker, Cindy K. Chung, Molly Ire-
land, Amy Gonzales, and Roger J. Booth. 2007.
The development and psychometric properties of
LIWC2007.
Nairan Ramirez-Esparza, Cindy K. Chung, Ewa
Kacewicz, and James W. Pennebaker. 2008. The
psychology of word use in depression forums in En-
glish and in Spanish: Testing two text analytic ap-
proaches. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM).
Philip Resnik, Anderson Garron, and Rebecca Resnik.
2013. Using topic modeling to improve prediction
of neuroticism and depression. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural, pages 1348?1353.
Stephanie S. Rude, Eva-Maria Gortner, and James W.
Pennebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133, December.
H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M.
Ramones, Megha Agrawal, Achal Shah, Michal
Kosinski, David Stillwell, Martin E. P. Seligman,
and Lyle H. Ungar. 2013. Personality, gender,
and age in the language of social media: The open-
vocabulary approach. PLOS One, 8(9).
Yla R. Tausczik and James W. Pennebaker. 2010. The
psychological meaning of words: LIWC and com-
puterized text analysis methods. Journal of Lan-
guage and Social Psychology, 29(1):24?54.
John W. Tukey. 1977. Box-and-whisker plots. Ex-
ploratory Data Analysis, pages 39?43.
Myrna M. Weissman, Roger C. Bland, Glorisa J.
Canino, Carlo Faravelli, Steven Greenwald, Hai-
Gwo Hwu, Peter R. Joyce, Eile G. Karam, Chung-
Kyoon Lee, Joseph Lellouch, Jean-Pierre L?epine,
Stephen C. Newman, Maritza Rubio-Stipec, J. Elis-
abeth Wells, Priya J. Wickramaratne, Hans-Ulrich
Wittchen, and Eng-Kung Yeh. 1996. Cross-national
epidemiology of major depression and bipolar dis-
order. Journal of the American Medical Association
(JAMA), 276(4):293?299.
Hadley Wickham. 2009. ggplot2: elegant graphics for
data analysis. Springer.
60
