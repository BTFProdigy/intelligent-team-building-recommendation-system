Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 97?101,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
HadoopPerceptron: a Toolkit for Distributed Perceptron Training and
Prediction with MapReduce
Andrea Gesmundo
Computer Science Department
University of Geneva
Geneva, Switzerland
andrea.gesmundo@unige.ch
Nadi Tomeh
LIMSI-CNRS and
Universite? Paris-Sud
Orsay, France
nadi.tomeh@limsi.fr
Abstract
We propose a set of open-source software
modules to perform structured Perceptron
Training, Prediction and Evaluation within
the Hadoop framework. Apache Hadoop
is a freely available environment for run-
ning distributed applications on a com-
puter cluster. The software is designed
within the Map-Reduce paradigm. Thanks
to distributed computing, the proposed soft-
ware reduces substantially execution times
while handling huge data-sets. The dis-
tributed Perceptron training algorithm pre-
serves convergence properties, thus guar-
anties same accuracy performances as the
serial Perceptron. The presented modules
can be executed as stand-alone software or
easily extended or integrated in complex
systems. The execution of the modules ap-
plied to specific NLP tasks can be demon-
strated and tested via an interactive web in-
terface that allows the user to inspect the
status and structure of the cluster and inter-
act with the MapReduce jobs.
1 Introduction
The Perceptron training algorithm (Rosenblatt,
1958; Freund and Schapire, 1999; Collins, 2002)
is widely applied in the Natural Language Pro-
cessing community for learning complex struc-
tured models. The non-probabilistic nature of the
perceptron parameters makes it possible to incor-
porate arbitrary features without the need to cal-
culate a partition function, which is required for
its discriminative probabilistic counterparts such
as CRFs (Lafferty et al 2001). Additionally, the
Perceptron is robust to approximate inference in
large search spaces.
Nevertheless, Perceptron training is propor-
tional to inference which is frequently non-linear
in the input sequence size. Therefore, training can
be time-consuming for complex model structures.
Furthermore, for an increasing number of tasks is
fundamental to leverage on huge sources of data
as the World Wide Web. Such difficulties render
the scalability of the Perceptron a challenge.
In order to improve scalability, Mcdonald et
al. (2010) propose a distributed training strat-
egy called iterative parameter mixing, and show
that it has similar convergence properties to the
standard perceptron algorithm; it finds a separat-
ing hyperplane if the training set is separable; it
produces models with comparable accuracies to
those trained serially on all the data; and reduces
training times significantly by exploiting comput-
ing clusters.
With this paper we present the HadoopPer-
ceptron package. It provides a freely available
open-source implementation of the iterative pa-
rameter mixing algorithm for training the struc-
tured perceptron on a generic sequence labeling
tasks. Furthermore, the package provides two ad-
ditional modules for prediction and evaluation.
The three software modules are designed within
the MapReduce programming model (Dean and
Ghemawat, 2004) and implemented using the
Apache Hadoop distributed programming Frame-
work (White, 2009; Lin and Dyer, 2010). The
presented HadoopPerceptron package reduces ex-
ecution time significantly compared to its serial
counterpart while maintaining comparable perfor-
mance.
97
PerceptronIterParamMix(T = {(xt,yt)}|T |t=1)
1. Split T into S pieces T = {T1, . . . ,TS}
2. w = 0
3. for n : 1..N
4. w(i,n) = OneEpochPerceptron(Ti ,w)
5. w =
?
i ?i,nw(i,n)
6. return w
OneEpochPerceptron(Ti ,w?)
1. w(0) = w?; k = 0
2. for n : 1..T
3. Let y? = argmaxy? w(k).f(xt,y?t)
4. if y? 6= yt
5. x(k+1) = x(k) + f(xt,yt)? f(xt,y?t)
6. k = k + 1
7. return w(k)
Figure 1: Distributed perceptron with iterative param-
eter mixing strategy. Each w(i,n) is computed in par-
allel. ?n = {?1,n, . . . , ?S,n}, ??i,n ? ?n : ?i,n ?
0 and ?n : ?i ?i,n = 1.
2 Distributed Structured Perceptron
The structured perceptron (Collins, 2002) is an
online learning algorithm that processes train-
ing instances one at a time during each training
epoch. In sequence labeling tasks, the algorithm
predicts a sequence of labels (an element from
the structured output space) for each input se-
quence. Prediction is determined by linear opera-
tions on high-dimensional feature representations
of candidate input-output pairs and an associated
weight vector. During training, the parameters are
updated whenever the prediction that employed
them is incorrect.
Unlike many batch learning algorithms that can
easily be distributed through the gradient calcula-
tion, the perceptron online training is more subtle
to parallelize. However, Mcdonald et al(2010)
present a simple distributed training through a pa-
rameter mixing scheme.
The Iterative Parameter Mixing is given in Fig-
ure 2 (Mcdonald et al 2010). First the training
data is divided into disjoint splits of example pairs
(xt,yt) where xt is the observation sequence and
yt is the associated labels. The algorithm pro-
ceeds to train a single epoch of the perceptron
algorithm for each split in parallel, and mix the
local models weights w(i,n) to produce the global
weight vector w. The mixed model is then passed
to each split to reset the perceptron local weights,
and a new iteration is started. Mcdonald et al
(2010) provide bound analysis for the algorithm
and show that it is guaranteed to converge and find
a seperation hyperplane if one exists.
3 MapReduce and Hadoop
Many algorithms need to iterate over number
of records and 1) perform some calculation on
each of them and then 2) aggregate the results.
The MapReduce programming model implements
a functional abstraction of these two operations
called respectively Map and Reduce. The Map
function takes a value-key pairs and produces a
list of key-value pairs: map(k, v) ? (k?, v?)?;
while the input the Reduce function is a key with
all the associated values produced by all the map-
pers: reduce(k?, (v?)?) ? (k??, v??)?. The model
requires that all values with the same key are re-
duced together.
Apache Hadoop is an open-source implementa-
tion of the MapReduce model on cluster of com-
puters. A cluster is composed by a set of comput-
ers (nodes) connected into a network. One node
is designated as the Master while other nodes
are referred to as Worker Nodes. Hadoop is de-
signed to scale out to large clusters built from
commodity hardware and achieves seamless scal-
ability. To allow rapid development, Hadoop
hides system-level details from the application
developer.The MapReduce runtime automatically
schedule worker assignment to mappers and re-
ducers;handles synchronization required by the
programming model including gathering, sort-
ing and shuffling of intermediate data across the
network; and provides robustness by detecting
worker failures and managing restarts. The frame-
work is built on top of he Hadoop Distributed
File System (HDFS), which allows to distribute
the data across the cluster nodes. Network traffic
is minimized by moving the process to the node
storing the data. In Hadoop terminology an entire
MapReduce program is called a job while individ-
ual mappers and reducers are called tasks.
4 HadoopPerceptron Implementation
In this section we give details on how the train-
ing, prediction and evaluation modules are im-
plemented for the Hadoop framework using the
98
Figure 2: HadoopPerceptron in MapReduce.
MapReduce programming model1.
Our implementation of the iterative parame-
ter mixing algorithm is sketched in Figure 2.
At the beginning of each iteration, the train-
ing data is split and distributed to the worker
nodes. The set of training examples in a
data split is streamed to map workers as pairs
(sentence-id, (xt,yt)). Each map worker per-
forms a standard perceptron training epoch and
outputs a pair (feature-id, wi,f ) for each feature.
The set of such pairs emitted by a map worker rep-
resents its local weight vector. After map workers
have finished, the MapReduce framework guaran-
tees that all local weights associated with a given
feature are aggregated together as input to a dis-
tinct reduce worker. Each reduce worker produces
as output the average of the associated feature
weight. At the end of each iteration, the reduce
workers outputs are aggregated into the global av-
eraged weight vector. The algorithm iterates N
times or until convergence is achieved. At the
beginning of each iteration the weight vector of
each distinct model is initialized with the global
averaged weight vector resultant from the previ-
ous iteration. Thus, for all the iterations except
for the first, the global averaged weight vector re-
sultant from the previous iteration needs to be pro-
vided the map workers. In Hadoop it is possible
to pass this information via the Distributed Cache
System.
In addition to the training module, the Hadoop-
Perceptron package provides separate modules
for prediction and evaluation both of them are
designed as MapReduce programs. The evalu-
1The Hadoop Perceptron toolkit is available from
https://github.com/agesmundo/HadoopPerceptron .
ation module output the accuracy measure com-
puted against provided gold standards. Prediction
and evaluation modules are independent from the
training modules, the weight vector given as input
could have been computed with any other system
using any other training algorithm as long as they
employ the same features.
The implementation is in Java, and we inter-
face with the Hadoop cluster via the native Java
API. It can be easily adapted to a wide range of
NLP tasks. Incorporating new features by mod-
ifying the extensible feature extractor is straight-
forward. The package includes the implementa-
tion of the basic feature set described in (Suzuki
and Isozaki, 2008).
5 The Web User Interface
Hadoop is bundled with several web interfaces
that provide concise tracking information for jobs,
tasks, data nodes, etc. as shown in Figure 3. These
web interfaces can be used to demonstrate the
HadoopPerceptron running phases and monitor
the distributed execution of the training, predic-
tion and evaluation modules for several sequence
labeling tasks including part-of-speech tagging
and named entity recognition.
6 Experiments
We investigate HadoopPerceptron training time
and prediction accuracy on a part-of-speech
(POS) task using the PennTreeBank corpus (Mar-
cus et al 1994). We use sections 0-18 of the Wall
Street Journal for training, and sections 22-24 for
testing.
We compare the regular percepton trained se-
rially on all the training data with the distributed
perceptron trained with iterative parameter mix-
ing with variable number of splits S ? {10, 20}.
For each system, we report the prediction accu-
racy measure on the final test set to determine
if any loss is observed as a consequence of dis-
tributed training.
For each system, Figure 4 plots accuracy re-
sults computed at the end of every training epoch
against consumed wall-clock time. We observe
that iterative mixing parameter achieves compa-
rable performance to its serial counterpart while
converging orders of magnitude faster.
Furthermore, we note that the distributed al-
gorithm achieves a slightly higher final accuracy
99
Figure 3: Hadoop interfaces for HadoopPerceptron.
Figure 4: Accuracy vs. training time. Each point cor-
responds to a training epoch.
than serial training. Mcdonald et al(2010) sug-
gest that this is due to the bagging effect that
the distributed training has, and due to parameter
mixing that is similar to the averaged perceptron.
We note also that increasing the number of
splits increases the number of epoch required to
attain convergence, while reducing the time re-
quired per epoch. This implies a trade-off be-
tween slower convergence and quicker epochs
when selecting a larger number of splits.
7 Conclusion
The HadoopPerceptron package provides the first
freely-available open-source implementation of
iterative parameter mixing Perceptron Training,
Prediction and Evaluation for a distributed Map-
Reduce framework. It is a versatile stand alone
software or building block, that can be easily
extended, modified, adapted, and integrated in
broader systems.
HadoopPerceptron is a useful tool for the in-
creasing number of applications that need to per-
form large-scale structured learning. This is the
first freely available implementation of an ap-
proach that has already been applied with success
in private sectors (e.g. Google Inc.). Making it
possible for everybody to fully leverage on huge
data sources as the World Wide Web, and develop
structured learning solutions that can scale keep-
ing feasible execution times and cluster-network
usage to a minimum.
Acknowledgments
This work was funded by Google and The Scot-
tish Informatics and Computer Science Alliance
(SICSA). We thank Keith Hall, Chris Dyer and
Miles Osborne for help and advice.
100
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, PA, USA.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: simplified data processing on large clusters.
In Proceedings of the 6th Symposium on Opeart-
ing Systems Design and Implementation, San Fran-
cisco, CA, USA.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. John lafferty and andrew mc-
callum and fernando pereira. In Proceedings of
the International Conference on Machine Learning,
Williamstown, MA, USA.
Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text
Processing with MapReduce. Morgan & Claypool
Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan Mcdonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In NAACL ?10: Proceedings of the 11th
Conference of the North American Chapter of the
Association for Computational Linguistics, Los An-
geles, CA, USA.
Frank Rosenblatt. 1958. The Perceptron: A proba-
bilistic model for information storage and organiza-
tion in the brain. Psychological Review, 65(6):386?
408.
Jun Suzuki and Hideki Isozaki. 2008. Semi-
supervised sequential labeling and segmentation us-
ing giga-word scale unlabeled data. In ACL ?08:
Proceedings of the 46th Conference of the Associa-
tion for Computational Linguistics, Columbus, OH,
USA.
Tom White. 2009. Hadoop: The Definitive Guide.
O?Reilly Media Inc.
101
Proceedings of NAACL-HLT 2013, pages 426?432,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Morphological Analysis and Disambiguation for Dialectal Arabic
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskander, and Nadi Tomeh
Center for Computational Learning Systems
Columbia University
{habash,ryanr,rambow,reskander,nadi}@ccls.columbia.edu
Abstract
The many differences between Dialectal Ara-
bic and Modern Standard Arabic (MSA) pose
a challenge to the majority of Arabic natural
language processing tools, which are designed
for MSA. In this paper, we retarget an exist-
ing state-of-the-art MSA morphological tag-
ger to Egyptian Arabic (ARZ). Our evalua-
tion demonstrates that our ARZ morphology
tagger outperforms its MSA variant on ARZ
input in terms of accuracy in part-of-speech
tagging, diacritization, lemmatization and to-
kenization; and in terms of utility for ARZ-to-
English statistical machine translation.
1 Introduction
Dialectal Arabic (DA) refers to the day-to-day na-
tive vernaculars spoken in the Arab World. DA
is used side by side with Modern Standard Arabic
(MSA), the official language of the media and edu-
cation (Holes, 2004). Although DAs are historically
related to MSA, there are many phonological, mor-
phological and lexical differences between them.
Unlike MSA, DAs have no standard orthographies
or language academies. Furthermore, different DAs,
such as Egyptian Arabic (henceforth, ARZ), Levan-
tine Arabic or Moroccan Arabic have important dif-
ferences among them, similar to those seen among
Romance languages (Holes, 2004; Abdel-Massih et
al., 1979). Most tools and resources developed for
natural language processing (NLP) of Arabic are de-
signed for MSA. Such resources are quite limited
when it comes to processing DA, e.g., a state-of-
the-art MSA morphological analyzer only has 60%
coverage of Levantine Arabic verb forms (Habash
and Rambow, 2006).
In this paper, we describe the process of retar-
geting an existing state-of-the-art tool for model-
ing MSA morphology disambiguation to ARZ, the
most commonly spoken DA. The MSA tool we
extend is MADA ? Morphological Analysis and
Disambiguation of Arabic (Habash and Rambow,
2005). The approach used in MADA, which was
inspired by earlier work by Hajic? (2000), disam-
biguates in context for every aspect of Arabic mor-
phology, thus solving all tasks in ?one fell swoop?.
The disadvantage of the MADA approach is its de-
pendence on two complex resources: a morpholog-
ical analyzer for the language and a large collection
of manually annotated words for all morphological
features in the same representation used by the an-
alyzer. For ARZ, such resources have recently be-
come available, with the development of the CAL-
IMA ARZ morphological analyzer (Habash et al,
2012b) and the release by the Linguistic Data Con-
sortium (LDC) of a large ARZ corpus annotated
morphologically in a manner compatible with CAL-
IMA (Maamouri et al, 2012a). In the work pre-
sented here, we utilize these new resources within
the paradigm of MADA, transforming MADA into
MADA-ARZ. The elegance of the MADA solution
makes this conceptually a simple extension.
Our evaluation demonstrates that our Egyptian
DA version of MADA, henceforth MADA-ARZ,
outperforms MADA for MSA on ARZ morpholog-
ical tagging and improves the quality of ARZ to En-
glish statistical machine translation (MT).
The rest of this paper is structured as follows:
Section 2 discusses related work. Section 3 presents
the challenges of processing Arabic dialects. Sec-
tion 4 outlines our approach. And Section 5 presents
and discusses our evaluation results.
426
2 Related Work
There has been a considerable amount of work on
MSA morphological analysis, disambiguation, part-
of-speech (POS) tagging, tokenization, lemmatiza-
tion and diacritization; for an overview, see (Habash,
2010). Most solutions target specific problems, such
as diacritization (Zitouni et al, 2006), tokenization
or POS tagging (Diab et al, 2007). In contrast,
MADA provides a solution to all of these problems
together (Habash and Rambow, 2005).
Previous work on DA morphological tagging fo-
cused on creating resources, using noisy or in-
complete annotations, and using unsupervised/semi-
supervised methods. Duh and Kirchhoff (2005)
adopt a minimally supervised approach that only re-
quires raw text data from several DAs, as well as a
MSA morphological analyzer. They report a POS
accuracy of 70.9% on a rather coarse-grained POS
tagset (17 tags).
Al-Sabbagh and Girju (2012) describe a super-
vised tagger for Egyptian Arabic social networking
corpora trained using transformation-based learning
(Brill, 1995). They report 94.5% F-measure on to-
kenization and 87.6% on POS tagging. Their tok-
enization and POS tagsets are comparable to the set
used by the Arabic Treebank (ATB). We do not com-
pare to them since their data sets are not public.
Stallard et al (2012) show that unsupervised
methods for learning DA tokenization can outper-
form MSA tokenizers on MT from Levantine Ara-
bic to English. We do not compare to them directly
since our work is on ARZ. However, we carry a sim-
ilar MT experiment in Section 5.
Mohamed et al (2012) annotated a small corpus
of Egyptian Arabic for morphological segmentation
and learned segmentation models using memory-
based learning (Daelemans and van den Bosch,
2005). Their best system achieves a 91.90% accu-
racy on the task of morpheme-segmentation. We
compare to their work and report on their test set
in Section 5.
There are some other morphological analyzers for
DA. Kilany et al (2002) worked on ARZ, but the
analyzer has very limited coverage. Their lexicon
was used as part of the development of CALIMA
(Habash et al, 2012b). Other efforts are not about
ARZ (Habash and Rambow, 2006; Salloum and
Habash, 2011).
Given the similarity between MSA and DA, there
has been some work on mapping DA to MSA to
exploit rich MSA resources (Chiang et al, 2006;
Abo Bakr et al, 2008; Salloum and Habash, 2011;
Salloum and Habash, 2013). Other researchers have
studied the value of simply combining DA and
MSA data, such as Zbib et al (2012) for DA to En-
glish MT. In our approach, we target DA directly,
and we evaluate the use of additional MSA anno-
tated resources to our training in Section 5.
3 Arabic Dialect Challenges
General Arabic Challenges Arabic, as MSA or
DA, poses many challenges for NLP. Arabic is a
morphologically complex language which includes
rich inflectional morphology and a number of cli-
tics. For example, the MSA word A? 	E?J.

J?J
?? wsyk-
tbwnhA (wa+sa+ya-ktub-uwna+hA)1 ?and they will
write it [lit. and+will+they-write-they+it]? has two
proclitics, one circumfix and one pronominal en-
clitic. Additionally, Arabic has a high degree of
ambiguity resulting from its diacritic-optional writ-
ing system and common deviation from spelling
standards (e.g., Alif and Ya variants) (Buckwalter,
2007). The Standard Arabic Morphological Ana-
lyzer for (SAMA) (Graff et al, 2009) produces 12
analyses per MSA word on average.
Differences between ARZ and MSA As men-
tioned above, most tools developed for MSA cannot
be expected to perform well on ARZ. This is due
to the numerous differences between the two vari-
ants. Lexically, the number of differences is quite
significant. For example, ARZ

?
	Q
K. Q? Trbyzh? ?table?
corresponds to MSA

???A? TAwlh?. Phonologically,
there are many important differences which relate
to orthography in DA, e.g., the MSA consonant H
/?/ is pronounced as /t/ in ARZ (or /s/ in more re-
cent borrowings from MSA); for a fuller discussion,
see (Habash, 2010; Habash et al, 2012a). Examples
of morphological differences include changes in the
1Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007): (in alphabetical or-
der) Abt?jHxd?rzs?SDTD???fqklmnhwy and the additional sym-
bols: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', h?

?, ? ?.
427
morpheme form, e.g., the MSA future proclitic +?
sa+ appears in ARZ as +? ha+. There are some
morphemes in ARZ that do not exist in MSA such
as the negation circum-clitic ?+ . . . + A? mA+ . . . +?.
And there are MSA features that are absent from
ARZ, most notably case and mood.
Since there are no orthographic standards, ARZ
words may be written in a variety of ways reflect-
ing different writing rules, e.g., phonologically or
etymologically. A conventional orthography for Di-
alectal Arabic (CODA) has been proposed and used
for writing ARZ in the context of NLP applications
(Habash et al, 2012a; Al-Sabbagh and Girju, 2012;
Eskander et al, 2013). Finally, MSA and ARZ co-
exist and are often used interchangeably, especially
in more formal settings. The CALIMA morpholog-
ical analyzer we use addresses several of these issues
by modeling both ARZ and MSA together, includ-
ing a limited set of inter-dialect morphology phe-
nomena, and by mapping ARZ words into CODA
orthography internally while accepting a wide range
of spelling variants.
4 Approach
4.1 The MADA Approach
MADA is a method for Arabic morphological anal-
ysis and disambiguation (Habash and Rambow,
2005; Roth et al, 2008). MADA uses a morpholog-
ical analyzer to produce, for each input word, a list
of analyses specifying every possible morphological
interpretation of that word, covering all morphologi-
cal features of the word (diacritization, POS, lemma,
and 13 inflectional and clitic features). MADA then
applies a set of models (support vector machines
and N-gram language models) to produce a predic-
tion, per word in-context, for different morpholog-
ical features, such as POS, lemma, gender, number
or person. A ranking component scores the analy-
ses produced by the morphological analyzer using a
tuned weighted sum of matches with the predicted
features. The top-scoring analysis is chosen as the
predicted interpretation for that word in context.
4.2 Extending MADA into MADA-ARZ
Adjusting MADA to handle DA requires a number
of modifications. The most significant change is re-
placing the MSA analyzer SAMA with the ARZ
analyzer CALIMA to address the differences out-
lined in Section 3. In addition, new feature predic-
tion models are needed; these are trained using ARZ
data sets annotated by the LDC (Maamouri et al,
2006; Maamouri et al, 2012b). The data sets were
not usable as released due to numerous annotation
inconsistencies and differences from CALIMA, as
well due to gaps in CALIMA. We synchronized the
annotations with the latest version of CALIMA fol-
lowing a technique described by Habash and Ram-
bow (2005). The result of this synchronization step
is the data we use in this study (for training, de-
velopment and testing). Our synchronized annota-
tions fully match the LDC annotations in 90% of the
words (in full morphological tag). We performed a
manual analysis on randomly chosen 100 words that
did not fully match. The choice we made is cor-
rect or acceptable in 55% of the cases of mismatch
with the LDC annotation, which means that the our
choice is accurate in over 95% of all cases.
Some of the original MADA features (which
were needed for MSA) are not used in ARZ and
so are dropped in MADA-ARZ; these features are
case, mood, the question-marking proclitic, state
and voice. Additional ARZ feature values have been
added, e.g., to handle the progressive particle and
future marker, among others. These are provided
by CALIMA and are classified and selected by
MADA-ARZ. In our current implementation, ARZ
features that are not present in MSA, such as the
negation and indirect-object enclitics, are not classi-
fied by MADA-ARZ classifiers, but since they are
provided by CALIMA they can be selected by the
whole MADA-ARZ system.
5 Evaluation
We evaluate MADA-ARZ intrinsically ? in terms
of performance on morphological disambiguation
? and extrinsically in the context of MT.
5.1 POS Tagging, Diacritization,
Lemmatization and Segmentation
Experimental Settings We use two sets of anno-
tated data from the LDC: ATB-123, which includes
parts 1, 2 and 3 of the MSA Penn Arabic Treebank
428
Development Test
MADA MADA-ARZ MADA MADA-ARZ
Train Data MSA ARZ ALL MSA ARZ ALL
Morph Tag 35.8 84.0 77.3 35.7 84.5 75.5
Penn POS 77.5 89.6 90.2 79.0 90.0 90.1
MADA POS 80.7 90.8 91.3 82.1 91.1 91.4
Diacritic 31.3 82.6 72.9 32.2 83.2 72.2
Lemma 64.0 85.2 81.6 67.1 86.3 82.8
Full 26.2 74.3 65.4 27.0 75.4 64.7
ATB Segmentation 90.6 97.4 97.6 90.5 97.4 97.5
Table 1: Evaluation metrics on the ATB-ARZ development and test sets. The best results are bolded. We compare
MADA and MADA-ARZ with different training data conditions. Definitions of metrics are in Section 5.1. MSA
training data is ATB-123. ARZ training data is ATB-ARZ. ALL training data is ATB-123 plus ATB-ARZ.
(Maamouri et al, 2004); and ATB-ARZ, the Egyp-
tian Arabic Treebank (parts 1-5) (Maamouri et al,
2012a). For ATB-123 training, we use all of parts 1
and 2 plus the training portion of ATB-3 (as defined
by Zitouni et al (2006)); for development and test,
we split Zitouni et al (2006)?s devtest set into two.
We sub-divide ATB-ARZ into development, train-
ing, and test sets (roughly a 10/80/10 split). The
ATB-ARZ training data has 134K words, and the
ATB-123 training data has 711K words.
We evaluate two systems. We used the latest re-
lease of MADA for MSA (v3.2), trained on ATB-
123 (MSA), as our baseline. For MADA-ARZ,
we compare two training settings: using ATB-ARZ
(ARZ) and combining ATB-ARZ with ATB-123
(ALL). We present our results on the ATB-ARZ
development and blind test sets (21.1K words and
20.4K words). Tuning for MADA-ARZ was done
using a random 10% of the ATB-ARZ training data,
which was later integrated back into the training set.
Metrics We use several evaluation metrics to mea-
sure the effectiveness of MADA-ARZ. Morph Tag
refers to the accuracy of correctly predicting the full
CALIMA morphological tag (i.e., not the diacritics
or the lemma). Penn POS and MADA POS are also
tag accuracy metrics. Penn POS, also known as the
Reduced Tag Set, is a tag set reduction of the full
Arabic morphological tag set, which was proposed
for MSA (Kulick et al, 2006; Diab, 2007; Habash,
2010); since it retains no MSA-specific morpholog-
ical features, it also makes sense for ARZ. MADA
POS is the small POS tag set (36 tags) MADA uses
internally. Diacritic and Lemma are the accura-
cies of the choice of diacritized form and Lemma,
respectively. Full is the harshest metric, requiring
that every morphological feature of the chosen anal-
ysis be correct. Finally, ATB Segmentation is the
percentage of words with correct ATB segmentation
(splitting off all clitics except for the determiner +?@
Al+).
Results The results are shown in Table 1.
MADA-ARZ performs much better than the
MADA baselines in all evaluation metrics. Compar-
ing the two MADA-ARZ systems, it is evident that
adding MSA data (ATB123) results in slightly better
performance only for the Penn POS, MADA POS,
and ATB Segmentation metrics. Including the MSA
data results in accuracy reductions for the other met-
rics, but the resulting system still outperforms the
MADA MSA baseline in all cases. The results are
consistent for development and blind test.
The CMUQ-ECA Test Set Mohamed et al
(2012) reported on the task of ARZ raw orthogra-
phy morph segmentation (determining the morphs
in the raw word). The CMUQ-ECA test data
comprised 36 ARZ political comments and jokes
from the Egyptian web site www.masrawy.com.
The set contains 2,445 words including punctua-
tion. Their best system gets a 91.9% word-level ac-
curacy. Since MADA-ARZ modifies the spelling
429
Tokenization OOV BLEU METEOR TER
Punct 9.2 22.1 27.2 63.2
MADA ATB 5.8 24.4 29.6 60.5
MADA-ARZ ATB 4.9 25.2 29.9 59.4
Table 2: Machine translation results on the test set. ?Punct? refers to the baseline which only tokenizes at punctuation.
of the word when it maps into CODA, we needed
a manual analysis where no exact match with the
gold occurs (11.8% of the time). We determined
MADA-ARZ?s accuracy on their test set for morph-
segmentation to be 93.2%.
5.2 Egyptian Arabic to English MT
MT Experimental Settings We use the open-
source Moses toolkit (Koehn et al, 2007) to build
a phrase-based SMT system. We use MGIZA++
for word alignment (Gao and Vogel, 2008). Phrase
translations of up to 8 words are extracted in the
phrase table. We use SRILM (Stolcke, 2002) with
modified Kneser-Ney smoothing to build two 4-
gram language models. The first model is trained
on the English side of the bitext, while the other
is trained on the English Gigaword data. Feature
weights are tuned to maximize BLEU (Papineni et
al., 2002) on a development set using Minimum Er-
ror Rate Training (Och, 2003). We perform case-
insensitive evaluation in terms of BLEU , METEOR
(Banerjee and Lavie, 2005) and TER (Snover et al,
2006) metrics.
Data We trained on DA-English parallel data
(Egyptian and Levantine) obtained from several
LDC corpora. The training data amounts to 3.8M
untokenized words on the Arabic side. The dev set,
used for tuning the parameters of the MT system,
has 15,585 untokenized Arabic words. The test set
has 12,116 untokenized Arabic words. Both dev and
test data contain two sets of reference translations.
The English data is lower-cased and tokenized using
simple punctuation-based rules.
Systems We build three translation systems which
vary in tokenization of the Arabic text. The first
system applies only simple punctuation-based rules.
The second and third systems use MADA and
MADA-ARZ, respectively, to tokenize the Arabic
text in the ATB tokenization scheme (Habash and
Sadat, 2006). The Arabic text is also Alif/Ya nor-
malized.
Results The MT results are in Table 2, which also
shows the percentage of out-of-vocabulary (OOV)
words ? test words not in the training data. MADA-
ARZ delivers the best translation performance ac-
cording to all metrics. All MADA-ARZ improve-
ments over MADA are statistically significant at
the .01 level (except in the case of METEOR). All
improvements over Punct by MADA and MADA-
ARZ are also statistically significant. For BLEU
scores, we observe 3.1% absolute improvement to
Punct (14% relative), and 0.8% absolute improve-
ment to MADA (3.3% relative). In addition to bet-
ter morphological disambiguation, MADA-ARZ
reduces the OOV ratio (16% relative to MADA),
which we suspect contributes to the observed im-
provements in MT quality.
6 Conclusion and Future Work
We have presented MADA-ARZ, a system for
morphological tagging of ARZ. We have shown
that it outperforms an state-of-the-art MSA tagger
(MADA) on ARZ text, and that it helps ARZ-to-
English machine translation more than MADA.
In the future, we intend to perform further feature
engineering to improve the results of MADA-ARZ,
and extend the system to handle other DAs.
Acknowledgments
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of the
authors and do not necessarily reflect the views of
DARPA.
430
References
Ernest T. Abdel-Massih, Zaki N. Abdel-Malek, and El-
Said M. Badawi. 1979. A Reference Grammar of
Egyptian Arabic. Georgetown University Press.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan.
2008. A Hybrid Approach for Converting Written
Egyptian Colloquial Dialect into Diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008. Cairo University.
Rania Al-Sabbagh and Roxana Girju. 2012. A super-
vised POS tagger for written Arabic social network-
ing corpora. In Jeremy Jancsary, editor, Proceedings
of KONVENS 2012, pages 39?52. ?GAI, September.
Main track: oral presentations.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
Eric Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part-of-Speech Tagging. Computational Lin-
guistics, 21(4):543?565.
Tim Buckwalter. 2007. Issues in Arabic Morphologi-
cal Analysis. In A. van den Bosch and A. Soudi, edi-
tors, Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
Dialects. In Proceedings of the European Chapter of
ACL (EACL).
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2007.
Automated methods for processing arabic text: From
tokenization to base phrase chunking. In Antal
van den Bosch and Abdelhadi Soudi, editors, Arabic
Computational Morphology: Knowledge-based and
Empirical Methods. Kluwer/Springer.
Mona Diab. 2007. Improved Arabic Base Phrase Chunk-
ing with a New Enriched POS Tag Set. In Proceedings
of the 2007 Workshop on Computational Approaches
to Semitic Languages: Common Issues and Resources,
pages 89?96, Prague, Czech Republic, June.
Kevin Duh and Katrin Kirchhoff. 2005. POS tagging of
dialectal Arabic: a minimally supervised approach. In
Proceedings of the ACL Workshop on Computational
Approaches to Semitic Languages, Semitic ?05, pages
55?62, Ann Arbor, Michigan.
Ramy Eskander, Nizar Habash, Owen Rambow, and Nadi
Tomeh. 2013. Processing Spontaneous Orthogra-
phy. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT), Atlanta, GA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?57,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the Ara-
bic Dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 681?688, Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic Prepro-
cessing Schemes for Statistical Machine Translation.
In Proceedings of the 7th Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics/Human Language Technologies Conference
(HLT-NAACL06), pages 49?52, New York, NY.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In NAACL-HLT 2012 Workshop on Com-
putational Morphology and Phonology (SIGMOR-
PHON2012), pages 1?9.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of the 1st Meeting of the
431
North American Chapter of the Association for Com-
putational Linguistics (NAACL?00), Seattle, WA.
Clive Holes. 2004. Modern Arabic: Structures, Func-
tions, and Varieties. Georgetown Classics in Ara-
bic Language and Linguistics. Georgetown University
Press.
H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-
Habashi, and C. McLemore. 2002. Egyptian
Colloquial Arabic Lexicon. LDC catalog number
LDC99L22.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguis-
tic Theories Conference, pages 31?42, Prague, Czech
Republic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank :
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, Mona
Diab, Nizar Habash, Owen Rambow, and Dalila
Tabessi. 2006. Developing and Using a Pilot Dialec-
tal Arabic Treebank. In The fifth international confer-
ence on Language Resources and Evaluation (LREC),
pages 443?448, Genoa, Italy.
Mohamed Maamouri, Ann Bies, Seth Kulick, Dalila
Tabessi, and Sondos Krouna. 2012a. Egyptian Ara-
bic Treebank Pilot.
Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,
Nadia Hamrouni, and Nizar Habash. 2012b. Egyptian
Arabic Morphological Annotation Guidelines.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Annotating and Learning Morphological Seg-
mentation of Egyptian Colloquial Arabic. In Proceed-
ings of the Language Resources and Evaluation Con-
ference (LREC), Istanbul.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of the 41st Annual Conference of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological tag-
ging, diacritization, and lemmatization using lexeme
models and feature ranking. In ACL 2008: The Con-
ference of the Association for Computational Linguis-
tics; Companion Volume, Short Papers, Columbus,
Ohio.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, pages 10?21, Edinburgh, Scotland.
Wael Salloum and Nizar Habash. 2013. Dialectal Ara-
bic to English Machine Translation: Pivoting through
Modern Standard Arabic. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), Atlanta, GA.
Matt Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Error Rate with Targeted Human An-
notation. In Proceedings of the Association for Ma-
chine Transaltion in the Americas (AMTA 2006), Cam-
bridge, Massachusetts.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012.
Unsupervised Morphology Rivals Supervised Mor-
phology for Arabic MT. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 322?327, Jeju Island, Korea.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 49?
59, Montr?al, Canada.
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006. Maximum entropy based restoration of ara-
bic diacritics. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 577?584, Sydney, Australia.
432
Proceedings of NAACL-HLT 2013, pages 585?595,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Processing Spontaneous Orthography
Ramy Eskander, Nizar Habash, Owen Rambow, and Nadi Tomeh
Center for Computational Learning Systems
Columbia University
{reskander,habash,rambow,nadi}@ccls.columbia.edu
Abstract
In cases in which there is no standard or-
thography for a language or language vari-
ant, written texts will display a variety of or-
thographic choices. This is problematic for
natural language processing (NLP) because it
creates spurious data sparseness. We study
the transformation of spontaneously spelled
Egyptian Arabic into a conventionalized or-
thography which we have previously proposed
for NLP purposes. We show that a two-stage
process can reduce divergences from this stan-
dard by 69%, making subsequent processing
of Egyptian Arabic easier.
1 Introduction
In areas with diglossia, vernacular spoken variants
(?low?) of a language family co-exist with a largely
written variant (?high?), which is often not spoken
as a native language. Traditionally, the low variants
have not been written: written language is reserved
for formal occasions and in those formal occasions
only the high variant is used. Prototypical exam-
ples of diglossia are the German speaking parts of
Switzerland, and the Arab world. The advent of the
internet has changed linguistic behavior: it is now
common to find written informal conversations, in
the form of email exchanges, text messages, Twit-
ter exchanges, and interactions on blogs and in web
forums. These written conversations are typically
written in the low variants (or in a mixture of low and
high), since conversations in the high variant seem
unnatural to the discourse participants. For natural
language processing (NLP), this poses many chal-
lenges, one of which is the fact that the low vari-
ants have not been written much in the past and
do not have a standard orthography which is gen-
erally agreed on by the linguistic community (and
perhaps sanctioned by an authoritative institution).
Instead, each discourse participant devises a spon-
taneous orthography, in which she chooses among
conventions from the high variant to render the spo-
ken language. We are thus faced with a large number
of ways to spell the same word, none of which can
be assumed as ?standard? since there is no standard.
As a result, the increased data sparseness adds to the
challenges of NLP tasks such as machine transla-
tion, compared to languages for which orthography
is standardized.
In this paper, we work on Egyptian Arabic
(EGY). We follow the conventions which we have
previously proposed for the normalized orthogra-
phy for EGY (Habash et al, 2012), called CODA
(Conventional Orthography for Dialectal Arabic). In
this paper, we investigate how easy it is to con-
vert spontaneous orthography of EGY written in
Arabic script into CODA orthography automatically.
We will refer to this process as ?normalization? or
?codafication?. We present a freely available system
called CODAFY, which we propose as a preproces-
sor for NLP modules for EGY. We show that a ?do
nothing? baseline achieves a normalization perfor-
mance of 75.5%, and CODAFY achieves a normal-
ization performance of 92.4%, an error reduction of
69.2% over this baseline on an unseen test set.
The paper is structured as follows. We first re-
view relevant linguistic facts in Section 2 and then
present the conventionalized orthography we use in
this paper. After reviewing related work in Sec-
tion 4, we present our data (Section 5), our approach
(Section 6), and our results (Section 7). We conclude
585
with a discussion of future work.
2 Linguistic Facts
2.1 Writing without a Standard Orthography
An orthography is a specification of how the words
of a language are mapped to and from a particular
script (in our case, the Arabic script). In cases when
a standard orthography is absent, writers make deci-
sions about spontaneous orthography based on vari-
ous criteria. Most prominent among them is phonol-
ogy: how can my pronunciation of the word be
rendered in the chosen writing system, given some
(language-specific) assumptions about grapheme-to-
phoneme mapping? Often, these assumptions come
from the ?high? variant of the language, or some re-
lated language. Another criterion for choosing or-
thography is a cognate in a related language or lan-
guage variant (Modern Standard Arabic or MSA,
the high variant for EGY), where a cognate pair is
a pair of words (or morphemes) in two languages
or language variants which are related by etymol-
ogy (in some unspecified manner) and which have
roughly the same meaning. Finally, the chosen
spontaneous orthography can be altered to reflect
speech effects, notably the lengthening of syllables
to represent emphasis or other effects (such as Q
J
J



J?
ktyyyyr1 ?very?).
It is important to distinguish typos from sponta-
neous orthography. We define spontaneous orthog-
raphy to be an intentional choice of graphemes to
render the words in a language or language variant.
We define a typographical error (typo) to be an un-
intended sequence of graphemes. For example, @Y?
kdA and ?Y? kdh can be intended spellings for EGY
/kida/ ?like this?, while @Q? krA is not a plausible in-
tentional spelling since it neither relates /kida/ to an
MSA cognate, nor does the sequence of graphemes
represent the phonology of EGY using standard as-
sumptions. Instead, we can explain the spelling by
assuming that the writer accidentally substituted the
grapheme P for the grapheme X, which look some-
what alike and are near each other on some Arabic
keyboard layouts. Of course, when we encounter
1Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007): (in alphabetical or-
der) Abt?jHxd?rzs?SDTD???fqklmnhwy and the additional sym-
bols: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', h?

?, ? ?.
a specific spelling in a corpus, it can be, in certain
cases, difficult to determine whether it is a conscious
choice or a typo.
2.2 Relevant Differences between EGY and
MSA
Lexical Variations Lexically, the number of differ-
ences is quite significant. For example, EGY H@Q?
mrAt ?wife [of]? corresponds to MSA

?k. ?
	P zwjh?. In
such cases of lexical difference, no cognate spelling
is available.
Phonological Variations There is an extensive
literature on the phonology of Arabic dialects (Wat-
son, 2002; Holes, 2004; Habash, 2010). Several
phonological differences exist between EGY and
MSA which relate to orthography. Here, we dis-
cuss one representative difference. The MSA con-
sonant H /?/ is pronounced as /t/ in EGY (or /s/
in more recent borrowings from MSA). For exam-
ple, MSA Q

?K
 yk?r ?increase (imperfective)? is pro-
nounced /yak?ur/ in MSA versus /yiktar/ in EGY,
giving rise to the EGY phonological spelling Q

?K

yktr.
Morphological Variations There are a lot of
morphological differences between MSA and EGY.
For orthography, two differences are most relevant.
The MSA future proclitic /sa/+ (spelled +? s+) ap-
pears in EGY as /ha/+ or /Ha/+. The two forms ap-
pear in free variation, and we have not been able
to find a variable that predicts which form is used
when. This variation is not a general phonological
variation between /h/ and /H/, we find it only in this
morpheme. Predictably, this leads to two spellings
in EGY: +h H+ and +? h+. Negation in EGY is
realized as the circum-clitic /ma?/+ . . . +/?/. The prin-
cipal orthographic question is whether the prefix is
a separate word or is part of the main word; both
variants are found.
3 CODA
CODA is a conventionalized orthography for Arabic
dialects (Habash et al, 2012). In this section, we
summarize CODA so that the reader can understand
the goals of this paper. CODA has five key proper-
ties.
1. CODA is an internally consistent and coherent
convention for writing DA: every word has a
586
single orthographic rendering.
2. CODA is created for computational purposes.
3. CODA uses the Arabic script as used for MSA,
with no extra symbols from, for example, Per-
sian or Urdu.
4. CODA is intended as a unified framework for
writing all dialects. In this paper, we only dis-
cuss the instantiation of CODA for EGY.
5. CODA aims to maintain a level of dialectal
uniqueness while using conventions based on
similarities between MSA and the dialects.
We list some features of CODA relevant to this
paper.
Phonological Spelling CODA generally uses
phonological spelling for EGY (as MSA spelling
does for MSA).
Etymologically Spelled Consonants A limited
number of consonants may be spelled differently
from their phonology if the following two conditions
are met: (1) the consonant must be an EGY root
radical and (2) the EGY root must have a cognate
MSA root. If the conditions are met, then we spell
the consonant using the corresponding radical from
the cognate MSA root of the dialectal word?s root.
One such example is the spelling of the EGY verb
pronounced /kitir/ as Q

? k?r ? it increased?.
Morphologically Faithful CODA preserves di-
alectal morphology and spells the dialectal mor-
phemes (clitics and inflections) phonologically. For
example, for the attachable future marker clitic, the
variant +h is chosen, not the MSA +?, so that
EGY /Hatiktar/ (and its variant /hatiktar/) are both
spelled Q

?

Jk Htk?r. The negation prefix and indi-
rect object pronoun (l+pronoun) suffixes are sepa-
rated, e.g., ?A?? I?

? A? mA qlt lhA? /ma?ultilha??/ ?I
did not tell her?.
Alif-Maqsura The letter ? ? is often used in
Egypt to write word-final ?


y and vice versa (even
when writing MSA). In CODA, all rules for us-
ing Alif-Maqsura are the same as MSA. For exam-
ple, EGY /maSr??/ ?Egyptian? can be seen in sponta-
neous orthography as ?Q??? mSr?, but in CODA it
is ?


Q??? mSry.
Ta-Marbuta As in MSA, the Ta-Marbuta ( ? h?) is
used morphemically in CODA. The Ta-Marbuta is
always written as

? h? in CODA, e.g., /?arba?a/ ?four?
is

??K. P

@ ?rb?h? in CODA, though it can be found as
??K. P

@ ?rb?h (or ??K. P@ Arb?h) in spontaneous orthog-
raphy.
Lexical Exceptions EGY CODA guidelines in-
clude a word list specifying ad hoc spellings of EGY
words that may be inconsistent with the default map-
ping outlined above. An example is /kida/ ?like this?,
which we find as both @Y? kdA and ?Y? kdh in spon-
taneous orthography; the CODA spelling is ?Y? kdh.
4 Related Work
To our knowledge, this paper is the first to discuss
the task of automatically providing a conventional-
ized spelling for a written Arabic dialect text. While
there is no direct precedent, we discuss here some
related research.
Our proposed work has some similarity to auto-
matic spelling correction (ASC) and related tasks
such as post editing for optical character recogni-
tion (OCR). Our task is different from ASC since
ASC work assumes a standard orthography that the
writer is also assumed to aim for. Both supervised
and unsupervised approaches to this task have been
explored. Unsupervised approaches rely on improv-
ing the fluency of the text and reducing the percent-
age of out-of-vocabulary words using NLP tools, re-
sources, and heuristics, e.g., morphological analyz-
ers, language models, and edit-distance measure, re-
spectively (Kukich, 1992; Oflazer, 1996; Ben Oth-
mane Zribi and Ben Ahmed, 2003; Shaalan et al,
2003; Haddad and Yaseen, 2007; Hassan et al,
2008; Shaalan et al, 2010; Alkanhal et al, 2012).
Supervised approaches learn models of correction
by training on paired examples of errors and their
corrections. This data is hard to come by nat-
urally, though for applications such as OCR cor-
pora can be created from the application itself (Ko-
lak and Resnik, 2002; Magdy and Darwish, 2006;
Abuhakema et al, 2008; Habash and Roth, 2011).
There has been some work on conversion of di-
alectal Arabic to MSA. Al-Gaphari and Al-Yadoumi
(2010) introduced a rule-based method to convert
Sanaani dialect to MSA, and Shaalan et al (2007)
used a rule-based lexical transfer approach to trans-
form from EGY to MSA. Similarly, both Sawaf
587
(2010) and Salloum and Habash (2011) showed
that translating dialectal Arabic to MSA can im-
prove dialectal Arabic machine translation into En-
glish by pivoting on MSA. A common feature across
these conversion efforts is the use of morphological
analysis and morphosyntactic transformation rules
(for example, Al-Gaphari and Al-Yadoumi (2010)).
While all this work is similar to ours in that dialec-
tal input is processed, our output is still dialectal,
while the work on conversion aims for a transforma-
tion into MSA.
The work most closely related to ours is that of
Dasigi and Diab (2011). They identify the spelling
variants in a given document and normalize them.
However, they do not present a system that con-
verts spontaneous spelling to a pre-existing conven-
tion such as CODA, and thus their results cannot
be directly related to ours. Furthermore, their tech-
nique is different. First, similarity metrics based on
string difference are used to identify if two strings
are similar. Also, a contextual string similarity is
used based on the fact that if two words are ortho-
graphic variants of each other, then they are bound
to appear in similar contexts. After identifying the
similar strings, the strings of interest are modeled in
a vector space and clustered according to the simi-
larity of their vectors.
5 Data
In this work, we use a manually annotated
EGY Arabic corpus, developed by the Linguis-
tic Data Consortium (LDC), and labeled as ?ARZ?
(Maamouri et al, 2012), parts 1, 2, 3, 4 and 5.
The corpus consists of about 160K words (excluding
numbers and punctuations), and follows the part-of-
speech (POS) guidelines used by the LDC for Egyp-
tian Arabic. The corpus contains a full analysis of
Egyptian Arabic text in spontaneous orthography.
The analysis includes the correct CODA orthogra-
phy of the raw text, in addition to the full morpho-
logical/POS annotations.
Data Preparation We divide the ARZ corpus into
three parts: training, development and test, which
are of about 122K, 19K and 19K words, respec-
tively. We only consider the orthographic informa-
tion in the ARZ corpus: for every word in the cor-
pus, we retain the spontaneous orthographic form
and its CODA-compliant form.
We manually checked the CODA-compliant an-
notations for about 500 words in the development
corpus. We found that the accuracy of the gold an-
notations in this subset is about 93%. We performed
next an error analysis for the erroneous gold annota-
tions. About one half of the gold errors are CODA
phonological and orthographical errors. Examples
of the CODA phonological errors include wrong ad-
ditions and deletions of @ A and ?


y, in addition to
the H/ H t/? transformations, and the transforma-
tions that correspond to the different phonological
forms of pronouncing the letter ? h. The CODA or-
thographical errors are those errors where a word or-
thography looks the same as its pronunciation, while
it should not be, such as the ?/

? h/h? transformations.
One fifth of the gold errors are annotation typos,
such as writing H@P??

? qTwrAt instead of H@PA?

?
qTArAt ?trains?. Moreover, 9% of the gold errors are
wrong merges for the negation particle A? mA and
the indirect object pronouns (l+pronouns). Since we
use the gold in our study, and given the error analy-
sis for the gold, we expect a qualitatively better gold
standard to yield better results.
Transformation Statistics We observe two types
of transformations when converting from sponta-
neous orthography to CODA: character substitu-
tions that do not affect the word length, and charac-
ter additions/deletions that change the word length.
Tables 1 and 2 show the most common character
substitution and addition/deletion transformations,
respectively, as they appear in the training corpus,
associated with their frequencies relative to the oc-
currence of transformations. The character sub-
stitutions are dominant, and constitute about 84%
of all the transformations in the training corpus.
While the classification of the character substitu-
tions is automatically generated, the classification of
the character additions/deletions is done manually
using a random sample of 400 additions/deletions
in the training corpus. This is because many addi-
tions/deletions are ambiguous.
6 Approach
We describe next the various approaches for spon-
taneous orthography codafication. Our codafication
techniques fall into two main categories: contex-
588
Transformation Frequency %
@/

@/ @/

@ A/?/A?/A? ? @/

@/ @/

@ A/?/A?/A? 38.5
?


y ? ? ? 29.7
? h ?

? h? 16.9
? h ? h H 2.5
H ?? H/? t/s 1.0
@ A ? ? h 0.7

? q ? @/

@/ @/

@/Z/ ?'/Z?' A/?/A?/A?/?/w?/y? 0.4
? w ? ? h 0.3
	
X ? ? X/ 	P d/z 0.3

? h?? H t 0.3
@ A ?

? h? 0.2
	
? D ? X/ 	P/ 	? d/z/D? 0.2
Table 1: Spontaneous to CODA character substitu-
tion transformations
Transformation Frequency %
Errors in closed class words 22.0
Missing space after A?/B/ AK
 mA/lA/yA 19.0
@ A additions & deletions 16.8
Gold errors 10.3
Speech effects 8.5
Missing space before ? l (+pron) 8.5
?/ ?/ @? w/h/wA ? ?/ ?/ @? w/h/wA 8.3
?


y additions & deletions 3.0
Table 2: Spontaneous to CODA character addi-
tion/deletion transformations
tual and non-contextual, where the non-contextual
approaches are a lot faster than the contextual ones.
6.1 Speech Effect Handling
Before applying any codafication techniques, we
perform a special preprocessing step for speech ef-
fects, which represent redundant repetitions of some
letter in sequence. Sometimes people intend these
repetitions to show affirmation or intensification.
This is simply handled by removing the repetitions
when a letter is repeated more than twice in a row,
except for some letters whose repetitions for more
than once indicates a speech effect; these letters
are @ A,

@ A?, Z ?, Z?' y?, ? ? and

? h?. Handling
speech effects on its own corrects about 2% of the
non-CODA spontaneous orthography to its CODA-
compliant form, without introducing any new errors.
In all experiments we report in this paper, we have
initially processed speech effects.
6.2 Character Edit Classification (CEC)
In this approach, a set of transformations is applied
on a character level, where a character may receive
a change or not. As a result, a word changes if one
or more of its characters is changed. The output of
these transformations is what constitutes the CODA
orthography. This is a surface modeling technique
that does not depend on the word context (though it
does depend on character context inside the word).
First, we train classifiers for the most frequent
transformations from EGY spontaneous orthogra-
phy to the corresponding CODA, listed in Tables 1
and 2 in Section 5. Second, we apply the trained
classifiers to generate the CODA output.
Training the classifiers For each transformation
listed in the data section, we train a separate classi-
fier. The classifiers are trained on our training cor-
pus using the k-nearest neighbor algorithm (k-NN)
(Wang et al, 2000), which is a method for classi-
fying objects based on the closest training examples
in the feature space. We did experiments using the
other classification methods included in the WEKA
machine learning tool (Hall et al, 2009), including
SVMs, Na?ve Bayes, and decision trees. However,
k-NN gives the best results for our problem.
In the training process, a set of nine static features
is applied, which are the character that is queried for
the transformation with its preceding and following
two characters (a special token indicates a character
position that does not exist because it is beyond the
word boundary), along with the first two and the last
two characters in the underlying word.
In this model, each data point is a character, where
a classifier determines whether a character should
receive a substitution, deletion, or addition of an-
other character. The effect of each classifier is ex-
amined separately on the development set. We then
determine for each classification whether it helps or
weakens the process by comparing its effect to the
baseline which is doing nothing, i.e., no change to
a word occurs. Those classifiers that on their own
perform worse than the baseline are eliminated. For
eliminating the classifiers, we examine them in a de-
scending order according to the frequencies of their
corresponding transformations. We now discuss the
seven classifiers we retain in our system:
589
1. The different @ A form ( @/

@/ @/

@ A/? /A?/A?) classifier.
The classifier can change any @ A form into any
other @ A form. The arbitrary selection of the
different @ A forms represents the most frequent
divergence from CODA in Arabic spontaneous
orthography.
2. The ?


/? y/? classifier. The classifier handles
transformations between ?


y and ? ? in both
directions, as their selection is mostly arbitrary
in EGY spontaneous orthography.
3. The ?/

?/? h/h?/w classifier. The classifier han-
dles transformations between ? h,

? h? and ? w
in both directions. These transformations are
likely to happen at word endings, since they
represent common misspellings in writing ? h,

? h? and ? w, where ? h is often substituted for
the graphically similar

? h?, and ? h and ? w can
both be used to represent the 3rd person mascu-
line singular accusative or genitive clitic. (Note
that in Table 1, we list transformations between
? h and

? h? as well as between ? h and ? w; the
remaining transformations are not frequent.)
4. The ?/h h/H classifier. The transformation
from ? h to h H is likely to happen at word
beginnings, since it represents a common devi-
ation in writing the h H future particle.
5. The @ A deletion classifier. The classifier han-
dles the deletion of extra @ A at some positions,
which is a common deviation in EGY sponta-
neous orthography, where the CODA orthog-
raphy requires only short vowels instead.
6. The @ A addition classifier. The classifier han-
dles the addition of @ A in some positions, where
it is mostly omitted in EGY spontaneous or-
thography, such as adding @ A after the h H fu-
ture particle and the H. b progressive particle
(when used with the 1st person singular imper-
fective), as well as the ? w plural pronoun at
word endings. When training this classifier, we
target the letter after which the @ A should be
added.
7. The space addition classifier. The classifier
handles the addition of spaces in the middle
of words, i.e., splitting a word into two words.
This is required to add spaces after A?/B/ AK

mA/lA/yA for negation and vocation, and be-
fore the indirect object l+pronoun, so that the
text becomes CODA-compliant.
Generating CODA Orthography Next, we ap-
ply the trained classifiers on the spontaneous-
orthography text. Each classifier determines a set
of character corrections, where the characters may
receive transformations corresponding to those on
which the classifier is trained. The classifiers are
independent of one another, so their order of appli-
cation is irrelevant.
By way of example, we apply the classifiers on the
word ??K. P@ Arb?h, ?four?. The first classifier, corre-
sponding to the different @ A forms, determines the
transformation of @ A to

@ ?, while the ?/

? h/h? clas-
sifier determines the correction of ? h to

? h?. The
other classifiers are either not involved since they do
not work on any of the word characters, or they de-
termine that no character transformation should hap-
pen for this word. Thus applying the CEC tech-
nique in this case changes the word ??K. P@ Arb?h to

??K. P

@ ?rb?h?, which is the correct CODA form.
6.3 Maximum Likelihood Estimate (MLE)
Another surface modeling approach for spontaneous
orthography codafication is to use a maximum like-
lihood model that operates on the word level. In this
approach, we build a unigram model that replaces
every word in the spontaneous orthography with its
most likely CODA form as seen in the training data.
This assumes that the underlying word exists in the
training corpus. For unseen words, the technique
keeps them with no change.
The MLE approach chooses the correct CODA
form for most of the words seen in training, making
this approach highly dependent on the training data.
It is efficient at correcting common misspellings in
frequent words, especially those that are from closed
classes.
6.4 Morphological Tagger
In addition to the approaches discussed above, we
use a morphological tagger, MADAARZ (Mor-
phological Analysis and Disambiguation for Egyp-
590
tian Arabic) (Habash et al, 2013). Although
MADAARZ is originally developed to work as a
morphological tagger, it still can help the codafica-
tion process, since the choice of a full morpholog-
ical analysis for a word in context determines its
CODA spelling. Therefore, MADAARZ is able to
correct many word misspellings that are common in
spontaneous orthography. These corrections include
( @/

@/ @/

@ A/?/A?/A?), ?


/? y/? and ?/

? h/h? transforma-
tions. However, MADAARZ , as a codafication tech-
nique, uses the context of the word, which makes
it a contextual modeling approach unlike CEC and
MLE. It is much slower than they are.
6.5 Combined Techniques
The CEC and MLE techniques can be applied
alone, or they can be applied together in a pipeline in
either order. This gives a total of four possible com-
binations. Next, we conducted experiments with
MADAARZ , running alone and as a pre- or postpro-
cessor for a combination of CEC and/or MLE. In
all cases, when we first apply one module and then
another on the output of the first, we train the sec-
ond module on the training corpus which has been
passed through the first module. The results of run-
ning the different codafication approaches are dis-
cussed next.
7 Evaluation
7.1 Accuracy Evaluation
The different codafication approaches, discussed in
the previous section, are tested against the develop-
ment set, which was not used as part of our train-
ing. The evaluation metric we use is a word accuracy
metric, i.e., we evaluate how well we can correctly
predict the CODA form of the input spontaneous or-
thography.
Table 3 lists the effects of using the different
codafication approaches. For each approach, two
numbers are reported; exact and normalized. In
the exact evaluation, the output of the codafica-
tion approach is exactly matched against the correct
CODA orthography, while in the normalized eval-
uation, the match is relaxed for the ( @/

@/ @/

@ A/?/A?/A?)
and ?


/? y/? alternations, i.e., these differences do
not count as errors. In many NLP applications (such
as machine translation), the input is normalized for
these two phenomena, so that the normalized evalu-
ation gives a sense of the relevance of codafication
to downstream processes which normalize.
In this evaluation we compare our different
codafication techniques, CEC, MLE, CEC+MLE
and MLE+CEC, against the baseline. We also show
the effect of using MADAARZ as a codafication sys-
tem. We see that MLE on its own outperforms CEC.
Running CEC first and then MLE gives us our best
result using surface techniques, namely 91.5%, for
an error reduction of 63.4% against the baseline.
This configuration also gives the highest normalized
accuracy of 95.2%, for an error reduction of 49.5%
against the baseline.
We now turn to deep modeling techniques.
The performance of MADAARZ on its own as a
codafication system is close to the performance of
CEC+MLE, by which it is outperformed in the
exact-match accuracy by 0.4%.
The best deep modeling (and the best overall)
performance is achieved when running MADAARZ
on top of MLE. This gives the highest accuracy
of 92.6% (exact) and 95.8% (normalized), for er-
ror reductions of 68.1% (exact) and 55.8% (nor-
malized) against the baseline, respectively. Note
that the non-contextual modeling techniques CEC
(5,584 words/sec) and MLE (6,698 words/sec)
are a lot faster than the deep modeling tech-
nique MADAARZ (53 words/sec), while their com-
bination CEC+MLE+MADAARZ is the slowest
among all the approaches, operating at a rate of 52
words/sec. Thus, a small drop in accuracy results in
a large increase in speed.
We also evaluated using MADAMSA (v 3.2)
(Morphological Analysis and Disambiguation for
MSA) (Habash and Rambow, 2005; Habash et al,
2010). MADAMSA is able to do some codafication,
but it performs far worse than our codafication ap-
proaches.
Table 4 lists the results of the best perform-
ing codafication surface approach, CEC+MLE, and
deep approach, MLE+MADAARZ , when applied
on the test set, which was not used as part of our
training or development, i.e., a completely blind
test. We see that on the test set, the addition
of MADAARZ improves results relatively more as
compared to the development set.
591
Approach
Exact Match Norm Match
w/s
Acc% ER% Acc% ER%
Baseline 76.8 90.5
MADAMSA 83.6 29.3 91.7 12.6 70
CEC 90.0 56.9 93.9 35.8 5,584
MLE 90.5 59.1 94.6 43.2 6,698
CEC+MLE 91.5 63.4 95.2 49.5 4,284
MLE+CEC 90.7 59.9 94.7 44.2 4,284
MADAARZ 91.1 61.6 95.2 49.5 53
MADAARZ+CEC 91.5 63.4 95.4 51.6 53
MADAARZ+MLE 91.9 65.1 95.8 55.8 53
CEC+MADAARZ 92.2 66.4 95.6 53.7 53
MLE+MADAARZ 92.6 68.1 95.8 55.8 53
MADAARZ+CEC+MLE 91.8 64.7 95.6 53.7 52
CEC+MLE+MADAARZ 92.0 65.5 95.8 55.8 52
Table 3: Comparison of the performance of the different codafication approaches on the development corpus.
Acc stands for Accuracy; ER is error reduction against the Baseline. w/s is speed (words/sec).
Approach
Exact Match Norm Match
Acc% ER% Acc% ER%
Baseline 75.5 89.7
CEC+MLE 91.3 64.5 94.8 49.5
MLE+MADAARZ 92.9 71.0 95.5 56.3
Table 4: Comparison of the performance of the
different codafication approaches on the test cor-
pus. Acc stands for Accuracy; ER is error reduction
against the Baseline.
7.2 Extrinsic Evaluation
Morphological Analysis We tested the effect of
codafication on morphological tagging, specifically
full POS and lemma determination in context by
the morphological tagger MADAARZ . Here, we
are evaluating MADAARZ not on its conversion
to CODA (as above), but on its core functional-
ity, namely morphological tagging. We compare
the performance of MADAARZ against running
CEC+MLE+MADAARZ . When tested on the de-
velopment set, the initial CEC+MLE codafication
step helps MADAARZ improve the identification
of the complete Arabic (Buckwalter) POS tag from
84% to 85.3%, for an error reduction of 8.1%, while
the correct lemma choice increases from 85.2% to
85.7%, for an error reduction of 3.4%. When tested
on the test set, we get improvements on the choice
of the complete Buckwalter POS tag and lemma
from 84.5% to 85.4% (5.8% error reduction) and
from 86.3% to 86.7% (2.9% error reduction), re-
spectively.
Arabic to English MT The goal of this exper-
iment is to test the effect of codafication on ma-
chine translation from dialectal Arabic to English.
We use the open-source Moses toolkit (Koehn et
al., 2007) to build a phrase-based SMT system. We
use MGIZA++ for word alignment (Gao and Vogel,
2008). Phrase translations of up to 8 words are ex-
tracted in the phrase table. We use SRILM (Stol-
cke, 2002) with modified Kneser-Ney smoothing to
build two 4-gram language models. The first model
is trained on the English side of the bitext, while
the other is trained on the English Gigaword data.
Feature weights are tuned to maximize BLEU (Pap-
ineni et al, 2002) on a development set using MERT
(Och, 2003). We perform case-insensitive evalua-
tion in terms of the BLEU metric.
We train the system on dialectal Arabic-English
parallel data, obtained from several LDC corpora,
which amounts to ?500k sentences with 3.8M unto-
kenized words on the Arabic side. The development
set, used for tuning the parameters of the MT sys-
tem, has 1,547 sentences with 15,585 untokenized
Arabic words. The test set has 1,065 sentences with
592
12,116 untokenized Arabic words. Both develop-
ment and test sets have two reference translations
each. The English data is lower-cased and tokenized
using simple punctuation-based rules.
We build two systems which vary in preprocess-
ing of the Arabic text. The baseline system ap-
plies only simple punctuation-based rules. The sec-
ond system applies our codafication in addition to
punctuation separation. The Arabic text is Alif/Ya
normalized and is kept untokenized in both set-
tings. The baseline system achieves a BLEU score
of 22.1%. The system using codafication obtains a
BLEU score of 22.6%, and outperforms the baseline
by 0.5% absolute BLEU points. This result shows
that improvements observed in intrinsic evaluation
of codafication carry on to the extrinsic task of ma-
chine translation.
7.3 Error Analysis
We conducted an error analysis for the best perform-
ing codafication approach on the development set.
The most frequent error types are listed in Table 5.
About two thirds of the errors are CODA phonolog-
ical and orthographical errors, denoted by CODA-
Phon and CODA-Orth, respectively. The wrong ad-
ditions and deletions of @ A and ?


y and the H/ H t/?
transformations are examples of CODA phonologi-
cal errors. The CODA orthographic errors include
cases such as the ?


/? y/? transformations. 21% of
the errors are not real errors in the codafication out-
put, but result from gold errors. Finally, about 13%
of the errors are wrong merges and splits for the the
negation particle A? mA, the vocative particle AK
 yA
and the indirect-object l+pronouns.
8 Conclusion and Future Work
We have presented the problem of transforming
spontaneous orthography of the Egyptian Arabic di-
alect into a conventionalized form, CODA. Our best
technique involves a combination of character trans-
formations, whole-word transformations, and the
use of a full morphological tagger. The tagger can
be omitted for a small decrease in performance and
a large increase in speed.2 In future work, we plan
to extend our approach to other Arabic dialects. We
2Our system will be freely available. Please contact the au-
thors for more information.
Error Type Description Percentage
Gold Error Annotation Error 21.0
CODA-Orth ? h ? ? h? 13.7
CODA-Phon @ A ?  8.7
Merge A? mA/NEG_PART 7.3
CODA-Phon ? ?


y 6.8
CODA-Phon ? @ A 5.9
CODA-Orth ?


y ? ? ? 4.1
Merge AK
 yA/VOC_PART 3.7
CODA-Phon ? h ? h H 3.2
CODA-Phon ?


y ? ? ? 3.2
Table 5: System Error Analysis: the most frequent
error types.
will also investigate incorporating the unsupervised
work of Dasigi and Diab (2011) into our algorithm,
as well as other unsupervised techniques.
Acknowledgment
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of the
authors and do not necessarily reflect the views of
DARPA. We thank three anonymous reviewers for
helpful comments, and Ryan Roth for help with run-
ning MADA.
593
References
G. Abuhakema, R. Faraj, A. Feldman, and E. Fitzpatrick.
2008. Annotating an Arabic Learner Corpus for Er-
ror. Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08).
G Al-Gaphari and M Al-Yadoumi. 2010. A method
to convert Sana?ani accent to Modern Standard Ara-
bic. International Journal of Information Science and
Management, pages 39?49.
Mohamed I. Alkanhal, Mohammed A. Al-Badrashiny,
Mansour M. Alghamdi, and Abdulaziz O. Al-
Qabbany. 2012. Automatic Stochastic Arabic
Spelling Correction With Emphasis on Space Inser-
tions and Deletions. IEEE Transactions on Audio,
Speech & Language Processing, 20:2111?2122.
Chiraz Ben Othmane Zribi and Mohammed Ben Ahmed.
2003. Efficient Automatic Correction of Misspelled
Arabic Words Based on Contextual Information. In
Proceedings of the Knowledge-Based Intelligent Infor-
mation and Engineering Systems Conference, Oxford,
UK.
Pradeep Dasigi and Mona Diab. 2011. CODACT: To-
wardsIdentifying Orthographic Variants in Dialectal
Arabic. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
318?326, Chaing Mai, Thailand.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?57,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Ryan Roth. 2011. Using deep mor-
phology to improve automatic error detection in ara-
bic handwriting recognition. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 875?884, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2010.
MADA+TOKAN Manual. Technical Report CCLS-
10-01, Center for Computational Learning Systems
(CCLS), Columbia University.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC), Istanbul.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskan-
der, and Nadi Tomeh. 2013. Morphological Analysis
and Disambiguation for Dialectal Arabic. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-Words in Arabic: A Hybrid
Approach. International Journal of Computer Pro-
cessing Of Languages (IJCPOL).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Ahmed Hassan, Sara Noeman, and Hany Hassan. 2008.
Language Independent Text Correction using Finite
State Automata. In Proceedings of the International
Joint Conference on Natural Language Processing
(IJCNLP 2008).
Clive Holes. 2004. Modern Arabic: Structures, Func-
tions, and Varieties. Georgetown Classics in Ara-
bic Language and Linguistics. Georgetown University
Press.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Okan Kolak and Philip Resnik. 2002. OCR error cor-
rection using a noisy channel model. In Proceedings
of the second international conference on Human Lan-
guage Technology Research.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Surveys,
24(4).
Mohamed Maamouri, Ann Bies, Seth Kulick, Dalila
Tabessi, and Sondos Krouna. 2012. Egyptian Arabic
Treebank Pilot.
Walid Magdy and Kareem Darwish. 2006. Arabic OCR
Error Correction Using Character Segment Correction,
594
Language Modeling, and Shallow Morphology. In
Proceedings of 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2006),
pages 408?414, Sydney, Austrailia.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of the 41st Annual Conference of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?90.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, pages 10?21, Edinburgh, Scotland.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in the
Americas (AMTA), Denver, Colorado.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Conference on Language Engineering, ELSE,
Cairo, Egypt.
K. Shaalan, Abo Bakr, and I. H. Ziedan. 2007. Transfer-
ring Egyptian Colloquial into Modern Standard Ara-
bic. In International Conference on Recent Advances
in Natural Language Processing (RANLP), Borovets,
Bulgaria.
K. Shaalan, R. Aref, and A. Fahmy. 2010. An approach
for analyzing and correcting spelling errors for non-
native Arabic learners. Proceedings of Informatics
and Systems (INFOS).
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Jun Wang, Zucker, and Jean-Daniel. 2000. Solving
multiple-instance problem: A lazy learning approach.
In Pat Langley, editor, 17th International Conference
on Machine Learning, pages 1119?1125.
Janet C. E. Watson. 2002. The Phonology and Morphol-
ogy of Arabic. Oxford University Press.
595
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 549?555,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reranking with Linguistic and Semantic Features
for Arabic Optical Character Recognition
Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra
Center for Computational Learning Systems, Columbia University
{nadi,habash,ryanr,noura}@ccls.columbia.edu
Pradeep Dasigi Mona Diab
Safaba Translation Solutions The George Washington University
pradeep@safaba.com mtdiab@gwu.edu
Abstract
Optical Character Recognition (OCR) sys-
tems for Arabic rely on information con-
tained in the scanned images to recognize
sequences of characters and on language
models to emphasize fluency. In this paper
we incorporate linguistically and seman-
tically motivated features to an existing
OCR system. To do so we follow an n-best
list reranking approach that exploits recent
advances in learning to rank techniques.
We achieve 10.1% and 11.4% reduction in
recognition word error rate (WER) relative
to a standard baseline system on typewrit-
ten and handwritten Arabic respectively.
1 Introduction
Optical Character Recognition (OCR) is the task
of converting scanned images of handwritten,
typewritten or printed text into machine-encoded
text. Arabic OCR is a challenging problem due
to Arabic?s connected letter forms, consonantal
diacritics and rich morphology (Habash, 2010).
Therefore only a few OCR systems have been de-
veloped (Ma?rgner and Abed, 2009). The BBN
Byblos OCR system (Natajan et al, 2002; Prasad
et al, 2008; Saleem et al, 2009), which we use
in this paper, relies on a hidden Markov model
(HMM) to recover the sequence of characters from
the image, and uses an n-gram language model
(LM) to emphasize the fluency of the output. For
an input image, the OCR decoder generates an n-
best list of hypotheses each of which is associated
with HMM and LM scores.
In addition to fluency as evaluated by LMs,
other information potentially helps in discrimi-
nating good from bad hypotheses. For example,
Habash and Roth (2011) use a variety of linguistic
(morphological and syntactic) and non-linguistic
features to automatically identify errors in OCR
hypotheses. Another example presented by De-
vlin et al (2012) shows that using a statistical ma-
chine translation system to assess the difficulty of
translating an Arabic OCR hypothesis into English
gives valuable feedback on OCR quality. There-
fore, combining additional information with the
LMs could reduce recognition errors. However,
direct integration of such information in the de-
coder is difficult.
A straightforward alternative which we advo-
cate in this paper is to use the available informa-
tion to rerank the hypotheses in the n-best lists.
The new top ranked hypothesis is considered as
the new output of the system. We propose com-
bining LMs with linguistically and semantically
motivated features using learning to rank meth-
ods. Discriminative reranking allows each hypoth-
esis to be represented as an arbitrary set of features
without the need to explicitly model their interac-
tions. Therefore, the system benefits from global
and potentially complex features which are not
available to the baseline OCR decoder. This ap-
proach has successfully been applied in numerous
Natural Language Processing (NLP) tasks includ-
ing syntactic parsing (Collins and Koo, 2005), se-
mantic parsing (Ge and Mooney, 2006), machine
translation (Shen et al, 2004), spoken language
understanding (Dinarelli et al, 2012), etc. Fur-
thermore, we propose to combine several ranking
methods into an ensemble which learns from their
predictions to further reduce recognition errors.
We describe our features and reranking ap-
proach in ?2, and we present our experiments and
results in ?3.
2 Discriminative Reranking for OCR
Each hypothesis in an n-best list {hi}ni=1 is repre-
sented by a d-dimensional feature vector xi ? Rd.
Each xi is associated with a loss li to generate a
labeled n-best list H = {(xi, li)}ni=1. The loss is
computed as the Word Error Rate (WER) of the
549
hypotheses compared to a reference transcription.
For supervised training we use a set of n-best lists
H = {H(k)}Mk=1.
2.1 Learning to rank approaches
Major approaches to learning to rank can be di-
vided into pointwise score regression, pairwise
preference satisfaction, and listwise structured
learning. See Liu (2009) for a survey. In this
paper, we explore all of the following learning to
rank approaches.
Pointwise In the pointwise approach, the rank-
ing problem is formulated as a regression, or ordi-
nal classification, for which any existing method
can be applied. Each hypothesis constitutes a
learning instance. In this category we use a regres-
sion method called Multiple Additive Regression
Trees (MART) (Friedman, 2000) as implemented
in RankLib.1 The major problem with pointwise
approaches is that the structure of the list of hy-
potheses is ignored.
Pairwise The pairwise approach takes pairs of
hypotheses as instances in learning, and formal-
izes the ranking problem as a pairwise classifica-
tion or pairwise regression. We use several meth-
ods from this category.
RankSVM (Joachims, 2002) is a method based
on Support Vector Machines (SVMs) for which
we use only linear kernels to keep complexity low.
Exact optimization of the RankSVM objective can
be computationally expensive as the number of
hypothesis pairs can be very large. Approximate
stochastic training strategies reduces complexity
and produce comparable performance. There-
fore, in addition to RankSVM, we use stochas-
tic sub-gradient descent (SGDSVM), Pegasos (Pe-
gasosSVM) and Passive-Aggressive Perceptron
(PAPSVM) as implemented in Sculley (2009).2
RankBoost (Freund et al, 2003) is a pairwise
boosting approach implemented in RankLib. It
uses a linear combination of weak rankers, each of
which is a binary function associated with a single
feature. This function is 1 when the feature value
exceeds some threshold and 0 otherwise.
RankMIRA is a ranking method presented in (Le
Roux et al, 2012).3 It uses a weighted linear
combination of features which assigns the highest
1http://people.cs.umass.edu/?vdang/
ranklib.html
2http://code.google.com/p/sofia-ml
3https://github.com/jihelhere/
adMIRAble
score to the hypotheses with the lowest loss. Dur-
ing training, the weights are updated according to
the Margin-Infused Relaxed Algorithm (MIRA),
whenever the highest scoring hypothesis differs
from the hypothesis with the lowest error rate.
In pairwise approaches, the group structure of
the n-best list is still ignored. Additionally, the
number of training pairs generated from an n-best
list depends on its size, which could result in train-
ing a model biased toward larger hypothesis lists
(Cao et al, 2006).
Listwise The listwise approach takes n-best lists
as instances in both learning and prediction. The
group structure is considered explicitly and rank-
ing evaluation measures can be directly optimized.
The listwise methods we use are implemented in
RankLib.
AdaRank (Xu and Li, 2007) is a boosting ap-
proach, similar to RankBoost, except that it opti-
mizes an arbitrary ranking metric, for which we
use Mean Average Precision (MAP).
Coordinate Ascent (CA) uses a listwise linear
model whose weights are learned by a coordinate
ascent method to optimize a ranking metric (Met-
zler and Bruce Croft, 2007). As with AdaRank we
use MAP.
ListNet (Cao et al, 2007) uses a neural network
model whose parameters are learned by gradient
descent method to optimize a listwise loss based
on a probabilistic model of permutations.
2.2 Ensemble reranking
In addition to the above mentioned approaches,
we couple simple feature selection and reranking
models combination via a straightforward ensem-
ble learning method similar to stacked general-
ization (Wolpert, 1992) and Combiner (Chan and
Stolfo, 1993). Our goal is to generate an overall
meta-ranker that outperforms all base-rankers by
learning from their predictions how they correlate
with each other.
To obtain the base-rankers, we train each of the
ranking models of ?2.1 using all the features of
?2.3 and also using each feature family added to
the baseline features separately. Then, we use the
best model for each ranking approach to make pre-
dictions on a held-out data set of n-best lists. We
can think of each base-ranker as computing one
feature for each hypothesis. Hence, the scores
generated by all the rankers for a given hypothe-
sis constitute its feature vector.
The held-out n-best lists and the predictions of
550
the base-rankers represent the training data for the
meta-ranker. We choose RankSVM4 as the meta-
ranker since it performed well as a base-ranker.
2.3 Features
Our features fall into five families.
Base features include the HMM and LM scores
produced by the OCR system. These features are
used by the baseline system5 as well as by the var-
ious reranking methods.
Simple features (?simple?) include the baseline
rank of the hypothesis and a 0-to-1 range normal-
ized version of it. We also use a hypothesis confi-
dence feature which corresponds to the average of
the confidence of individual words in the hypoth-
esis; ?confidence? for a given word is computed
as the fraction of hypotheses in the n-best list
that contain the word (Habash and Roth, 2011).
The more consensus words a hypothesis contains,
the higher its assigned confidence. We also use
the average word length and the number of con-
tent words (normalized by the hypothesis length).
We define ?content words? as non-punctuation and
non-digit words. Additionally, we use a set of bi-
nary features indicating if the hypothesis contains
a sequence of duplicated characters, a date-like se-
quence and an occurrence of a specific character
class (punctuation, alphabetic and digit).
Word LM features (?LM-word?) include the
log probabilities of the hypothesis obtained us-
ing n-gram LMs with n ? {1, . . . , 5}. Separate
LMs are trained on the Arabic Gigaword 3 corpus
(Graff, 2007), and on the reference transcriptions
of the training data (see ?3.1). The LM models
are built using the SRI Language Modeling Toolkit
(Stolcke, 2002).
Linguistic LM features (?LM-MADA?) are
similar to the word LM features except that they
are computed using the part-of-speech and the
lemma of the words instead of the actual words.6
Semantic coherence feature (?SemCoh?) is
motivated by the fact that semantic information
can be very useful in modeling the fluency of
phrases, and can augment the information pro-
vided by n-gram LMs. In modeling contextual
4RankSVM has also been shown to be a good choice for
the meta-learner in general stacking ensemble learning (Tang
et al, 2010).
5The baseline ranking is simply based on the sum of the
logs of the HMM and LM scores.
6The part-of-speech and the lemmas are obtained using
MADA 3.0, a tool for Arabic morphological analysis and
disambiguation (Habash and Rambow, 2005; Habash et al,
2009).
lexical semantic information, simple bag-of-words
models usually have a lot of noise; while more
sophisticated models considering positional infor-
mation have sparsity issues. To strike a balance
between these two extremes, we introduce a novel
model of semantic coherence that is based on a
measure of semantic relatedness between pairs of
words. We model semantic relatedness between
two words using the Information Content (IC) of
the pair in a method similar to the one used by Lin
(1997) and Lin (1998).
IC(w1,d, w2) = log
f(w1, d, w2)f(?,d, ?)
f(w1, d, ?)f(?,d, w2)
Here, d can generally represent some form of re-
lation between w1 and w2. Whereas Lin (1997)
and Lin (1998) used dependency relation between
words, we use distance. Given a sentence, the dis-
tance between w1 and w2 is one plus the number
of words that are seen after w1 and before w2 in
that sentence. Hence, f(w1, d, w2) is the number
of times w1 occurs before w2 at a distance d in
all the sentences in a corpus. ? is a placeholder
for any word, i.e., f(?, d, ?) is the frequency of all
word pairs occurring at distance d. The distances
are directional and not absolute values. A simi-
lar measure of relatedness was also used by Kolb
(2009).
We estimate the frequencies from the Arabic
Gigaword. We set the window size to 3 and cal-
culate IC values of all pairs of words occurring at
distance within the window size. Since the dis-
tances are directional, it has to be noted that given
a word, its relations with three words before it and
three words after it are modeled. During testing,
for each phrase in our test set, we measure se-
mantic relatedness of pairs of words using the IC
values estimated from the Arabic Gigaword, and
normalize their sum by the number of pairs in the
phrase to obtain a measure of Semantic Coherence
(SC) of the phrase. That is,
SC(p) = 1m ?
?
1?d?W
1?i+d<n
IC(wi,d, wi+d)
where p is the phrase being evaluated, n is the
number of words in it, d is the distance between
words, W is the window size (set to 3), and m is
the number of all possible wi, wi+d pairs in the
phrase given these conditions.
551
print hand
|H?| n |h| |H?| n |h|
Hb 1,560 62 9 2,295 225 8
Hm 1,000 76 9 1,000 225 9
Ht 1,000 64 9 1,000 227 9
Table 1: Data sets statistics. |H?| refers to the
number of n-best lists, n is the average size of the
lists, and |h| is the average length of a hypothesis.
print hand
Baseline 13.8% 35%
Oracle 9.8% 20.9%
Best result 12.4% 30.9%
Table 2: WER for baseline, oracle and best
reranked hypotheses.
3 Experiments
3.1 Data and baselines
We used two data sets derived from high-
resolution image scans of typewritten and hand-
written Arabic text along with ground truth tran-
scriptions.7 The BBN Byblos system was then
used to process these scanned images into se-
quences of segments (sentence fragments) and
generate a ranked n-best list of hypotheses for
each segment (Natajan et al, 2002; Prasad et al,
2008; Saleem et al, 2009). We divided each of the
typewritten data set (?print?) and handwritten data
set (?hand?) into three disjoint parts: a training set
for the base-rankersHb, a training set for the meta-
ranker Hm and a test set Ht. Table 1 presents
some statistics about these data sets. Our base-
line is based on the sum of the logs of the HMM
and LM scores. Table 2 presents the WER for our
baseline hypothesis, the best hypothesis in the list
(our oracle) and our best reranking results which
we describe in details in ?3.2.
For LM training we used 220M words from
Arabic Gigaword 3, and 2.4M words from each
?print? and ?hand? ground truth annotations.
Effect of n-best training size onWER The size
of the training n-best lists is crucial to the learning
of the ranking model. In particular, it determines
the number of training instances per list. To deter-
mine the optimal n to use for the rest of this pa-
per, we conducted the following experiment aims
to understand the effect of the size of n-best lists
7The Anfal data set discussed here was collected by the
Linguistic Data Consortium.
30.5 
31 
31.5 
32 
32.5 
33 
33.5 
34 
12 
12.5 
13 
13.5 
14 
14.5 
15 
5 15 25 35 45 55 
Size of each training n-best list 
WER print 
hand 
Figure 1: Effect of the size of training n-best lists
on WER. The horizontal axis represents the max-
imum size of the n-best lists and the vertical axis
represents WER, left is ?print? and right is ?hand?.
on the reranking performance for one of our best
reranking models, namely RankSVM. We trained
each model with different sizes of n-best, varying
from n = 5 to n = 60 for ?print? data, and be-
tween n = 5 and n = 150 for ?hand? data. The
top n hypotheses according to the baseline are se-
lected for each n. Figure 1 plots WER as a func-
tion of the size of the training list n for both ?print?
and ?hand? data.
The lowest WER scores are achieved for n =
10 and n = 15 for both ?print? and ?hand? data.
We note that a small number of hypotheses per list
is sufficient for RankSVM to obtain a good per-
formance, but also increasing n further seems to
increase the error rate. For the rest of this paper
we use the top 10-best hypotheses per segment.
3.2 Reranking results
The reranking results for ?print? and ?hand? are
presented in Table 3. The results are presented
as the difference in WER from the baseline WER.
See the caption in Table 3 for more information.
For ?print?, the pairwise approaches clearly out-
perform the listwise approaches and achieve the
lowest WER of 12.4% (10.1% WER reduction rel-
ative to the baseline) with 7 different combinations
of rankers and feature families. While both ap-
proaches do not minimize WER directly, the pair-
wise methods have the advantage of using objec-
tives that are simpler to optimize, and they are
trained on much larger number of examples which
may explain their superiority. RankBoost, how-
ever, is less competitive with a performance closer
to that of listwise approaches. All the methods
improved over the baseline with any feature fam-
ily, except for the pointwise approach which did
552
Pointwise Listwise Pairwise
Features MA
RT
Ad
aR
ank
Lis
tNe
t
CA Ra
nkB
oos
t
Ra
nkS
VM
SG
DS
VM
Ra
nkM
IRA
Peg
a.S
VM
PA
PS
VM
Pr
int
Base 1.1 -0.4 -1.0 -1.0 -1.0 -1.1 -1.2 -1.2 -1.3 -1.3
+simple -0.1 0.0 -0.1 -0.2 0.0 -0.1 0.1 0.0 0.1 0.0
+LM-word -1.0 -0.2 0.1 -0.1 -0.1 -0.3 -0.2 -0.1 0.0 -0.1
+LM-MADA 0.0 -0.3 0.1 -0.2 -0.1 0.0 -0.1 -0.2 -0.1 -0.1
+SemCoh 0.0 -0.4 0.0 -0.2 -0.1 -0.1 0.0 -0.1 0.0 0.1
+All 0.6 0.1 0.0 0.1 0.0 0.1 0.2 0.2 0.2 0.0
Ha
nd
Base 4.2 -3.1 -3.2 -3.4 -2.9 -3.2 -3.5 -3.8 -3.6 -3.8
+simple 0.3 -0.1 0.1 0.2 0.1 -0.1 0.2 -0.2 0.1 0.2
+LM-word 0.4 -0.1 0.1 0.8 -0.2 -0.7 -0.2 -0.1 0.0 0.1
+LM-MADA 0.0 -0.5 0.1 0.0 0.1 -0.4 -0.1 0.3 -0.2 0.1
+SemCoh 0.0 -0.1 0.0 -0.4 0.0 -0.2 -0.3 -0.2 -0.2 0.0
+All 0.2 0.4 0.0 0.4 0.2 0.4 0.2 0.1 0.2 0.0
Table 3: Reranking results for the ?print? and ?hand? data sets; the ?print? baseline WER is 13.9% and the ?hand? baseline
WER is 35.0%. The ?Base? numbers represent the difference in WER between the corresponding ranker using ?Base? features
only and the baseline, which uses the same ?Base? features. The ?+features? numbers represent additional gain (relative to
?Base?) obtained by adding the corresponding feature family. The ?+All? numbers represent the gain of using all features,
relative to the best single-family system. The actual WER of a ranker can be obtained by summing the baseline WER and the
corresponding ?Base? and ?+features? scores. Bolded values are the best performers overall.
worse than the baseline. When combined with
the ?Base? features, ?LM-words? lead to improve-
ments with 8 out of 10 rankers, and proved to be
the most helpful among feature families. ?LM-
MADA? follows with improvements with 7 out of
10 rankers. The lowest WER is achieved using
one of these two LM-based families. Combining
all feature families did not help and in many cases
resulted in a higher WER than the best family.
Similar improvements are observed for ?hand?.
The lowest achieved WER is 31% (11.4% WER
reduction relative to the baseline). Here also,
the pointwise method increased the WER by 12%
relative to the baseline (as opposed to 7% for
?print?). Again, the listwise approaches are over-
all less effective than their pairwise counterparts,
except for RankBoost which resulted in the small-
est WER reduction among all rankers. The two
best rankers correspond to RankMIRA with the
?simple? and the ?SemCoh? features. The ?Sem-
Coh? feature resulted in improvements for 6 out of
the 10 rankers, and thus was the best single feature
on average for the ?hand? data set. As observed
with ?print? data, combining all the features does
not lead to the best performance.
In an additional experiment, we selected the
best model for each ranking method and combined
them to build an ensemble as described in ?2.2.
For ?hand?, the ensemble slightly outperformed
all the individual rankers and achieved the lowest
WER of 30.9%. However, for the ?print? data, the
ensemble failed to improve over the base-rankers
and resulted in a WER of 12.4%.
The best overall results are presented in Table 2.
Our best results reduce the distance to the oracle
top line by 35% for ?print? and 29% for ?hand?.
4 Conclusion
We presented a set of experiments on incorporat-
ing features into an existing OCR system via n-
best list reranking. We compared several learn-
ing to rank techniques and combined them us-
ing an ensemble technique. We obtained 10.1%
and 11.4% reduction in WER relative to the base-
line for ?print? and ?hand? data respectively. Our
best systems used pairwise reranking which out-
performed the other methods, and used the MADA
based features for ?print? and our novel semantic
coherence feature for ?hand?.
Acknowledgment
We would like to thank Rohit Prasad and Matin
Kamali for providing the data and helpful dis-
cussions. This work was funded under DARPA
project number HR0011-08-C-0004. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this paper are those of the au-
thors and do not necessarily reflect the views of
DARPA. The last two authors, Dasigi and Diab,
worked on this project while at Columbia Univer-
sity.
553
References
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou
Huang, and Hsiao-Wuen Hon. 2006. Adapting
ranking SVM to document retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?06, pages 186?193.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24th international conference on Machine learning,
ICML ?07, pages 129?136.
Philip K. Chan and Salvatore J. Stolfo. 1993. Exper-
iments on multistrategy learning by meta-learning.
In Proceedings of the second international confer-
ence on Information and knowledge management,
CIKM ?93, pages 314?323.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Comput.
Linguist., 31(1):25?70, March.
Jacob Devlin, Matin Kamali, Krishna Subramanian,
Rohit Prasad, and Prem Natarajan. 2012. Statisti-
cal Machine Translation as a Language Model for
Handwriting Recognition. In ICFHR, pages 291?
296.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative Reranking for
Spoken Language Understanding. IEEE Transac-
tions on Audio, Speech & Language Processing,
20(2):526?539.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm
for combining preferences. J. Mach. Learn. Res.,
4:933?969, December.
Jerome H. Friedman. 2000. Greedy Function Approx-
imation: A Gradient Boosting Machine. Annals of
Statistics, 29:1189?1232.
Ruifang Ge and Raymond J. Mooney. 2006. Discrimi-
native Reranking for Semantic Parsing. In ACL.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan, June.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
Arabic handwriting recognition. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?02,
pages 133?142.
Peter Kolb. 2009. Experiments on the difference be-
tween semantic similarity and relatedness. In Pro-
ceedings of the 17th Nordic Conference of Computa-
tional Linguistics, NEALT Proceedings Series Vol.
4.
Joseph Le Roux, Benoit Favre, Alexis Nasr, and
Seyed Abolghasem Mirroshandel. 2012. Gener-
ative Constituent Parsing and Discriminative De-
pendency Reranking: Experiments on English and
French. In SP-SEM-MRL 2012.
Dekang Lin. 1997. Using syntactic dependency as
local context to resolve word sense ambiguity. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, EACL ?97, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98.
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval. Now Publishers Inc., Hanover, MA,
USA.
Volker Ma?rgner and Haikal El Abed. 2009. Ara-
bic Word and Text Recognition - Current Develop-
ments. In Khalid Choukri and Bente Maegaard, ed-
itors, Proceedings of the Second International Con-
ference on Arabic Language Resources and Tools,
Cairo, Egypt, April. The MEDAR Consortium.
Donald Metzler and W. Bruce Croft. 2007. Linear
feature-based models for information retrieval. Inf.
Retr., 10(3):257?274, June.
Premkumar Natajan, Zhidong Lu, Richard Schwartz,
Issam Bazzi, and John Makhoul. 2002. Hid-
den Markov models. chapter Multilingual machine
printed OCR, pages 43?63. World Scientific Pub-
lishing Co., Inc., River Edge, NJ, USA.
554
Rohit Prasad, Shirin Saleem, Matin Kamali, Ralf Meer-
meier, and Premkumar Natarajan. 2008. Improve-
ments in hidden Markov model based Arabic OCR.
In Proceedings of International Conference on Pat-
tern Recognition (ICPR), pages 1?4.
Shirin Saleem, Huaigu Cao, Krishna Subramanian,
Matin Kamali, Rohit Prasad, and Prem Natarajan.
2009. Improvements in BBN?s HMM-Based Of-
fline Arabic Handwriting Recognition System. In
Proceedings of the 2009 10th International Confer-
ence on Document Analysis and Recognition, IC-
DAR ?09, pages 773?777.
D. Sculley. 2009. Large scale learning to rank. In
NIPS 2009 Workshop on Advances in Ranking.
Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative Reranking for Machine Trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 177?184, Boston, Massachusetts, USA,
May.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Buzhou Tang, Qingcai Chen, Xuan Wang, and Xiao-
long Wang. 2010. Reranking for stacking ensem-
ble learning. In Proceedings of the 17th interna-
tional conference on Neural information processing:
theory and algorithms - Volume Part I, ICONIP?10,
pages 575?584.
David H. Wolpert. 1992. Original Contribution:
Stacked generalization. Neural Netw., 5(2):241?
259, February.
Jun Xu and Hang Li. 2007. AdaRank: a boosting al-
gorithm for information retrieval. In Proceedings of
the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?07, pages 391?398.
555
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161?167,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Generalized Character-Level Spelling Error Correction
Noura Farra, Nadi Tomeh?, Alla Rozovskaya, Nizar Habash
Center for Computational Learning Systems, Columbia University
{noura,alla,habash}@ccls.columbia.edu
?LIPN, Universit? Paris 13, Sorbonne Paris Cit?
nadi.tomeh@lipn.univ-paris13.fr
Abstract
We present a generalized discrimina-
tive model for spelling error correction
which targets character-level transforma-
tions. While operating at the charac-
ter level, the model makes use of word-
level and contextual information. In con-
trast to previous work, the proposed ap-
proach learns to correct a variety of er-
ror types without guidance of manually-
selected constraints or language-specific
features. We apply the model to cor-
rect errors in Egyptian Arabic dialect text,
achieving 65% reduction in word error
rate over the input baseline, and improv-
ing over the earlier state-of-the-art system.
1 Introduction
Spelling error correction is a longstanding Natural
Language Processing (NLP) problem, and it has
recently become especially relevant because of the
many potential applications to the large amount
of informal and unedited text generated online,
including web forums, tweets, blogs, and email.
Misspellings in such text can lead to increased
sparsity and errors, posing a challenge for many
NLP applications such as text summarization, sen-
timent analysis and machine translation.
In this work, we present GSEC, a Generalized
character-level Spelling Error Correction model,
which uses supervised learning to map input char-
acters into output characters in context. The ap-
proach has the following characteristics:
Character-level Corrections are learned at the
character-level
1
using a supervised sequence la-
beling approach.
Generalized The input space consists of all
characters, and a single classifier is used to learn
1
We use the term ?character? strictly in the alphabetic
sense, not the logographic sense (as in the Chinese script).
common error patterns over all the training data,
without guidance of specific rules.
Context-sensitive The model looks beyond the
context of the current word, when making a deci-
sion at the character-level.
Discriminative The model provides the free-
dom of adding a number of different features,
which may or may not be language-specific.
Language-Independent In this work, we in-
tegrate only language-independent features, and
therefore do not consider morphological or lin-
guistic features. However, we apply the model
to correct errors in Egyptian Arabic dialect text,
following a conventional orthography standard,
CODA (Habash et al, 2012).
Using the described approach, we demonstrate
a word-error-rate (WER) reduction of 65% over a
do-nothing input baseline, and we improve over
a state-of-the-art system (Eskander et al, 2013)
which relies heavily on language-specific and
manually-selected constraints. We present a de-
tailed analysis of mistakes and demonstrate that
the proposed model indeed learns to correct a
wider variety of errors.
2 Related Work
Most earlier work on automatic error correction
addressed spelling errors in English and built mod-
els of correct usage on native English data (Ku-
kich, 1992; Golding and Roth, 1999; Carlson
and Fette, 2007; Banko and Brill, 2001). Ara-
bic spelling correction has also received consider-
able interest (Ben Othmane Zribi and Ben Ahmed,
2003; Haddad and Yaseen, 2007; Hassan et al,
2008; Shaalan et al, 2010; Alkanhal et al, 2012;
Eskander et al, 2013; Zaghouani et al, 2014).
Supervised spelling correction approaches
trained on paired examples of errors and their cor-
rections have recently been applied for non-native
English correction (van Delden et al, 2004; Li et
al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012;
161
Rozovskaya and Roth, 2011). Discriminative
models have been proposed at the word-level for
error correction (Duan et al, 2012) and for error
detection (Habash and Roth, 2011).
In addition, there has been growing work on lex-
ical normalization of social media data, a some-
what related problem to that considered in this pa-
per (Han and Baldwin, 2011; Han et al, 2013;
Subramaniam et al, 2009; Ling et al, 2013).
The work of Eskander et al (2013) is the
most relevant to the present study: it presents
a character-edit classification model (CEC) using
the same dataset we use in this paper.
2
Eskan-
der et al (2013) analyzed the data to identify the
seven most common types of errors. They devel-
oped seven classifiers and applied them to the data
in succession. This makes the approach tailored to
the specific data set in use and limited to a specific
set of errors. In this work, a single model is con-
sidered for all types of errors. The model consid-
ers every character in the input text for a possible
spelling error, as opposed to looking only at cer-
tain input characters and contexts in which they
appear. Moreover, in contrast to Eskander et al
(2013), it looks beyond the boundary of the cur-
rent word.
3 The GSEC Approach
3.1 Modeling Spelling Correction at the
Character Level
We recast the problem of spelling correction into
a sequence labeling problem, where for each input
character, we predict an action label describing
how to transform it to obtain the correct charac-
ter. The proposed model therefore transforms a
given input sentence e = e
1
, . . . , e
n
of n char-
acters that possibly include errors, to a corrected
sentence c of m characters, where corrected char-
acters are produced by one of the following four
actions applied to each input character e
i
:
? ok: e
i
is passed without transformation.
? substitute ? with(c): e
i
is substituted with
a character c where c could be any character
encountered in the training data.
? delete: e
i
is deleted.
? insert(c): A character c is inserted before
e
i
. To address errors occurring at the end
2
Eskander et al (2013) also considered a slower, more
expensive, and more language-specific method using a mor-
phological tagger (Habash et al, 2013) that outperformed the
CEC model; however, we do not compare to it in this paper.
Input Action Label
k substitute-with(c)
o ok
r insert(r)
e ok
c ok
t ok
d delete
Table 1: Character-level spelling error correction process
on the input word korectd, with the reference word correct
Train Dev Test
Sentences 10.3K 1.67K 1.73K
Characters 675K 106K 103K
Words 134K 21.1K 20.6K
Table 2: ARZ Egyptian dialect corpus statistics
of the sentence, we assume the presence of a
dummy sentence-final stop character.
We use a multi-class SVM classifier to predict the
action labels for each input character e
i
? e. A
decoding process is then applied to transform the
input characters accordingly to produce the cor-
rected sentence. Note that we consider the space
character as a character like any other, which gives
us the ability to correct word merge errors with
space character insertion actions and word split er-
rors with space character deletion actions. Table 1
shows an example of the spelling correction pro-
cess.
In this paper, we only model single-edit actions
and ignore cases where a character requires mul-
tiple edits (henceforth, complex actions), such as
multiple insertions or a combination of insertions
and substitutions. This choice was motivated by
the need to reduce the number of output labels, as
many infrequent labels are generated by complex
actions. An error analysis of the training data, de-
scribed in detail in section 3.2, showed that com-
plex errors are relatively infrequent (4% of data).
We plan to address these errors in future work.
Finally, in order to generate the training data
in the described form, we require a parallel cor-
pus of erroneous and corrected reference text (de-
scribed below), which we align at the character
level. We use the alignment tool Sclite (Fiscus,
1998), which is part of the SCTK Toolkit.
3.2 Description of Data
We apply our model to correcting Egyptian Ara-
bic dialect text. Since there is no standard dialect
orthography adopted by native speakers of Ara-
bic dialects, it is common to encounter multiple
162
Action % Errors Example Error? Reference
Substitute 80.9
E
Alif A @ forms ( @/

@/ @

/

@A?/
?
A/
?
A) 33.3 AHdhm? ?Hdhm ??Yg@? ??Yg

@
E
Ya ?


/? forms ( y/?) 26.7 ?ly? ?l?
?


?
??
??
?
E
h/~ ?/

? , h/w ?/? forms 14.9 kfrh? kfr~ ?Q
	
???

?Q
	
??
E
h/H ?/h forms 2.2 ht?mlhA? Ht?mlhA A????

J?? A????

Jk
Other substitutions 3.8 AltAny~? Al?Any~

?J


	
K A

J? @?

?J


	
K A

J? @ ; dA? dh @X? ?X
Insert 10.5
EP
Insert {A} 3.0 ktbw? ktbwA ?J
.

J?? @?J
.

J?
EP
Insert {space} 2.9 mAtz?l?? mA tz?l?

???
	
Q

KA??

???
	
Q

K A?
Other insertion actions 4.4 Aly? Ally ?


?@?
?


?
? @
Delete 4.7
E
Del{A} 2.4 whmA? whm A???? ???
Other deletion actions 2.3 wfyh? wfy ?J


	
??? ?


	
??
Complex 4.0 mykwn?? mA ykwn?

?
	
??J


??

?
	
??K


A?
Table 3: Character-level distribution of correction labels. We model all types of transformations except complex actions, and
rare Insert labels with counts below a tuned threshold. The Delete label is a single label that comprises all deletion actions.
Labels modeled by Eskander et al (2013) are marked with
E
, and
EP
for cases modeled partially, for example, the Insert{A}
would only be applied at certain positions such as the end of the word.
spellings of the same word. The CODA orthogra-
phy was proposed by Habash et al (2012) in an
attempt to standardize dialectal writing, and we
use it as a reference of correct text for spelling
correction following the previous work by Eskan-
der et al (2013). We use the same corpus (la-
beled "ARZ") and experimental setup splits used
by them. The ARZ corpus was developed by
the Linguistic Data Consortium (Maamouri et al,
2012a-e). See Table 2 for corpus statistics.
Error Distribution Table 3 presents the distri-
bution of correction action labels that correspond
to spelling errors in the training data together with
examples of these errors.
3
We group the ac-
tions into: Substitute, Insert, Delete, and Complex,
and also list common transformations within each
group. We further distinguish between the phe-
nomena modeled by our system and by Eskander
et al (2013). At least 10% of all generated action
labels are not handled by Eskander et al (2013).
3.3 Features
Each input character is represented by a feature
vector. We include a set of basic features inspired
by Eskander et al (2013) in their CEC system and
additional features for further improvement.
Basic features We use a set of nine basic fea-
tures: the given character, the preceding and fol-
lowing two characters, and the first two and last
3
Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007). For more informa-
tion on Arabic orthography in NLP, see (Habash, 2010).
two characters in the word. These are the same
features used by CEC, except that CEC does
not include characters beyond the word boundary,
while we consider space characters as well as char-
acters from the previous and next words.
Ngram features We extract sequences of char-
acters corresponding to the current character and
the following and previous two, three, or four
characters. We refer to these sequences as bi-
grams, trigrams, or 4-grams, respectively. These
are an extension of the basic features and allow
the model to look beyond the context of the cur-
rent word.
3.4 Maximum Likelihood Estimate (MLE)
We implemented another approach for error cor-
rection based on a word-level maximum likeli-
hood model. The MLE method uses a unigram
model which replaces each input word with its
most likely correct word based on counts from the
training data. The intuition behind MLE is that it
can easily correct frequent errors; however, it is
quite dependent on the training data.
4 Experiments
4.1 Model Evaluation
Setup The training data was extracted to gener-
ate the form described in Section 3.1, using the
Sclite tool (Fiscus, 1998) to align the input and
reference sentences. A speech effect handling step
was applied as a preprocessing step to all models.
163
This step removes redundant repetitions of charac-
ters in sequence, e.g.,
Q



J





J






J? ktyyyyyr ?veeeeery?.
The same speech effect handling was applied by
Eskander et al (2013).
For classification, we used the SVM implemen-
tation in YamCha (Kudo and Matsumoto, 2001),
and trained with different variations of the fea-
tures described above. Default parameters were
selected for training (c=1, quadratic kernel, and
context window of +/- 2).
In all results listed below, the baseline corre-
sponds to the do-nothing baseline of the input text.
Metrics Three evaluation metrics are used. The
word-error-rate WER metric is computed by sum-
ming the total number of word-level substitution
errors, insertion errors, and deletion errors in the
output, and dividing by the number of words in the
reference. The correct-rate Corr metric is com-
puted by dividing the number of correct output
words by the total number of words in the refer-
ence. These two metrics are produced by Sclite
(Fiscus, 1998), using automatic alignment. Fi-
nally, the accuracy Acc metric, used by Eskander
et al (2013), is a simple string matching metric
which enforces a word alignment that pairs words
in the reference to those of the output. It is cal-
culated by dividing the number of correct output
words by the number of words in the input. This
metric assumes no split errors in the data (a word
incorrectly split into two words), which is the case
in the data we are working with.
Character-level Model Evaluation The per-
formance of the generalized spelling correction
model (GSEC) on the dev data is presented in the
first half of Table 4. The results of the Eskan-
der et al (2013) CEC system are also presented
for the purpose of comparison. We can see that
using a single classifier, the generalized model is
able to outperform CEC, which relies on a cascade
of classifiers (p = 0.03 for the basic model and
p < 0.0001 for the best model, GSEC+4grams).
4
Model Combination Evaluation Here we
present results on combining GSEC with the
MLE component (GSEC+MLE). We combine the
two models in cascade: the MLE component is
applied to the output of GSEC. To train the MLE
model, we use the word pairs obtained from the
original training data, rather than from the output
of GSEC. We found that this configuration allows
4
Significance results are obtained using McNemar?s test.
Approach Corr%/WER Acc%
Baseline 75.9/24.2 76.8
CEC 88.7/11.4 90.0
GSEC 89.7/10.4* 90.3*
GSEC+2grams 90.6/9.5* 91.2*
GSEC+4grams 91.0/9.2* 91.6*
MLE 89.7/10.4 90.5
CEC + MLE 90.8/9.4 91.5
GSEC+MLE 91.0/9.2 91.3
GSEC+4grams+ MLE 91.7/8.3* 92.2*
Table 4: Model Evaluation. GSEC represents the gener-
alized character-level model. CEC represents the character-
level-edit classification model of Eskander et al (2013).
Rows marked with an asterisk (*) are statistically signifi-
cant compared to CEC (for the first half of the table) or
CEC+MLE (for the second half of the table), with p < 0.05.
us to include a larger sample of word pair errors
for learning, because our model corrects many
errors, leaving fewer example pairs to train an
MLE post-processor. The results are shown in the
second half of Table 4.
We first observe that MLE improves the per-
formance of both CEC and GSEC. In fact,
CEC+MLE and GSEC+MLE perform similarly
(p = 0.36, not statistically significant). When
adding features that go beyond the word bound-
ary, we achieve an improvement over MLE,
GSEC+MLE, and CEC+MLE, all of which are
mostly restricted within the boundary of the word.
The best GSEC model outperforms CEC+MLE
(p < 0.0001), achieving a WER of 8.3%, corre-
sponding to 65% reduction compared to the base-
line. It is worth noting that adding the MLE com-
ponent allows Eskander?s CEC to recover various
types of errors that were not modeled previously.
However, the contribution of MLE is limited to
words that are in the training data. On the other
hand, because GSEC is trained on character trans-
formations, it is likely to generalize better to words
unseen in the training data.
Results on Test Data Table 5 presents the re-
sults of our best model (GSEC+4grams), and best
model+MLE. The latter achieves a 92.1% Acc
score. The Acc score reported by Eskander et al
(2013) for CEC+MLE is 91.3% . The two results
are statistically significant (p < 0.0001) with re-
spect to CEC and CEC+MLE respectively.
Approach Corr%/WER Acc%
Baseline 74.5/25.5 75.5
GSEC+4grams 90.9/9.1 91.5
GSEC+4grams+ MLE 91.8/8.3 92.1
Table 5: Evaluation on test data.
164
4.2 Error Analysis
To gain a better understanding of the performance
of the models on different types of errors and their
interaction with the MLE component, we separate
the words in the dev data into: (1) words seen in
the training data, or in-vocabulary words (IV), and
(2) out-of-vocabulary (OOV) words not seen in
the training data. Because the MLE model maps
every input word to its most likely gold word seen
in the training data, we expect the MLE compo-
nent to recover a large portion of errors in the IV
category (but not all, since an input word can have
multiple correct readings depending on the con-
text). On the other hand, the recovery of errors in
OOV words indicates how well the character-level
model is doing independently of the MLE compo-
nent. Table 6 presents the performance, using the
Acc metric, on each of these types of words. Here
our best model (GSEC+4grams) is considered.
#Inp Words Baseline CEC+MLE GSEC+MLE
OOV 3,289 (17.2%) 70.7 76.5 80.5
IV 15,832 (82.8%) 78.6 94.6 94.6
Total 19,121 (100%) 77.2 91.5 92.2
Table 6: Accuracy of character-level models shown sepa-
rately on out-of-vocabulary and in-vocabulary words.
When considering words seen in the training
data, CEC and GSEC have the same performance.
However, when considering OOV words, GSEC
performs significantly better (p < 0.0001), veri-
fying our hypothesis that a generalized model re-
duces dependency on training data. The data is
heavily skewed towards IV words (83%), which
explains the generally high performance of MLE.
We performed a manual error analysis on a sam-
ple of 50 word errors from the IV set and found
that all of the errors came from gold annotation er-
rors and inconsistencies, either in the dev or train.
We then divided the character transformations in
the OOV words into four groups: (1) characters
that were unchanged by the gold (X-X transforma-
tions), (2) character transformations modeled by
CEC (X-Y CEC), (3) character transformations not
modeled by CEC, and which include all phenom-
ena that were only partially modeled by CEC (X-Y
not CEC), and (4) complex errors. The character-
level accuracy on each of these groups is shown in
Table 7.
Both CEC and GSEC do much better on the
second group of character transformations (that
is, X-Y CEC) than on the third group (X-Y not
CEC). This is not surprising because the former
Type #Chars Example CEC GSEC
X-X 16502 m-m, space-space 99.25 99.33
X-Y 609 ~-h, h-~,
?
A-A 80.62 83.09
(CEC) A-
?
A, y-?
X-Y 161 t-? , del{w} 31.68 43.48
(not CEC) n-ins{space}
Complex 32 n-ins{A}{m} 37.5 15.63
Table 7: Character-level accuracy on different transforma-
tion types for out-of-vocabulary words. For complex trans-
formations, the accuracy represents the complex category
recognition rate, and not the actual correction accuracy.
transformations correspond to phenomena that are
most common in the training data. For GSEC,
they are learned automatically, while for CEC they
are selected and modeled explicitly. Despite this
fact, GSEC generalizes better to OOV words. As
for the third group, both CEC and GSEC per-
form more poorly, but GSEC corrects more errors
(43.48% vs. 31.68% accuracy). Finally, CEC is
better at recognizing complex errors, which, al-
though are not modeled explicitly by CEC, can
sometimes be corrected as a result of applying
multiple classifiers in cascade. Dealing with com-
plex errors, though there are few of them in this
dataset, is an important direction for future work,
and for generalizing to other datasets, e.g., (Za-
ghouani et al, 2014).
5 Conclusions
We showed that a generalized character-level
spelling error correction model can improve
spelling error correction on Egyptian Arabic data.
This model learns common spelling error patterns
automatically, without guidance of manually se-
lected or language-specific constraints. We also
demonstrate that the model outperforms existing
methods, especially on out-of-vocabulary words.
In the future, we plan to extend the model to use
word-level language models to select between top
character predictions in the output. We also plan
to apply the model to different datasets and differ-
ent languages. Finally, we plan to experiment with
more features that can also be tailored to specific
languages by using morphological and linguistic
information, which was not explored in this paper.
Acknowledgments
This publication was made possible by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors.
165
References
Mohamed I. Alkanhal, Mohammed A. Al-Badrashiny,
Mansour M. Alghamdi, and Abdulaziz O. Al-
Qabbany. 2012. Automatic Stochastic Arabic
Spelling Correction With Emphasis on Space Inser-
tions and Deletions. IEEE Transactions on Audio,
Speech & Language Processing, 20:2111?2122.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages
26?33, Toulouse, France, July.
Chiraz Ben Othmane Zribi and Mohammed Ben
Ahmed. 2003. Efficient Automatic Correction
of Misspelled Arabic Words Based on Contextual
Information. In Proceedings of the Knowledge-
Based Intelligent Information and Engineering Sys-
tems Conference, Oxford, UK.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the IEEE International Conference
on Machine Learning and Applications (ICMLA).
Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Huizhong Duan, Yanen Li, ChengXiang Zhai, and
Dan Roth. 2012. A discriminative model for
query spelling correction with latent structural svm.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1511?1521, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing spontaneous orthog-
raphy. In The Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL
HLT ?13.
Jon Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 163?171, Los
Angeles, California, June.
Andrew R. Golding and Dan Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
arabic handwriting recognition. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rambow.
2012. Conventional orthography for dialectal Ara-
bic. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet U?gur
Do?gan, Bente Maegaard, Joseph Mariani, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-Words in Arabic: A Hybrid
Approach. International Journal of Computer Pro-
cessing Of Languages (IJCPOL).
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 368?378.
Association for Computational Linguistics.
Bo Han, Paul Cook, and Timothy Baldwin. 2013.
Lexical normalization for social media text. ACM
Transactions on Intelligent Systems and Technology
(TIST), 4(1):5.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction us-
ing Finite State Automata. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2008).
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies, NAACL ?01, pages 1?
8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Sur-
veys, 24(4).
166
Yanen Li, Huizhong Duan, and ChengXiang Zhai.
2012. A generalized hidden markov model with dis-
criminative training for query spelling correction. In
Proceedings of the 35th international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?12, pages 611?620, New
York, NY, USA. ACM.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 73?84, Seattle, Washington, USA, Octo-
ber. Association for Computational Linguistics.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012a.
Egyptian Arabic Treebank DF Part 1 V2.0. LDC
catalog number LDC2012E93.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012b.
Egyptian Arabic Treebank DF Part 2 V2.0. LDC
catalog number LDC2012E98.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012c.
Egyptian Arabic Treebank DF Part 3 V2.0. LDC
catalog number LDC2012E89.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012d.
Egyptian Arabic Treebank DF Part 4 V2.0. LDC
catalog number LDC2012E99.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012e.
Egyptian Arabic Treebank DF Part 5 V2.0. LDC
catalog number LDC2012E107.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for esl correction tasks.
In Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), Portland, Oregon,
6. Association for Computational Linguistics.
Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An
approach for analyzing and correcting spelling er-
rors for non-native Arabic learners. Proceedings of
Informatics and Systems (INFOS).
L Venkata Subramaniam, Shourya Roy, Tanveer A
Faruquie, and Sumit Negi. 2009. A survey of types
of text noise and techniques to handle noisy text.
In Proceedings of The Third Workshop on Analytics
for Noisy Unstructured Text Data, pages 115?122.
ACM.
Sebastian van Delden, David B. Bracewell, and Fer-
nando Gomez. 2004. Supervised and unsupervised
automatic spelling correction algorithms. In Infor-
mation Reuse and Integration, 2004. Proceedings of
the 2004 IEEE International Conference on, pages
530?535.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale Arabic error annotation: Guidelines and
framework. In Proceedings of the 9th edition of the
Language Resources and Evaluation Conference.
167
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 400?405,
Dublin, Ireland, August 23-24, 2014.
LIPN: Introducing a new Geographical Context Similarity Measure and a
Statistical Similarity Measure Based on the Bhattacharyya Coefficient
Davide Buscaldi, Jorge J. Garc??a Flores, Joseph Le Roux, Nadi Tomeh
Laboratoire d?Informatique de Paris Nord, CNRS (UMR 7030)
Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France
{buscaldi,jgflores,joseph.le-roux,nadi.tomeh}@lipn.univ-paris13.fr
Bel
?
em Priego Sanchez
Laboratoire LDI (Lexique, Dictionnaires, Informatique)
Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France
LKE, FCC, BUAP, San Manuel, Puebla, Mexico
belemps@gmail.com
Abstract
This paper describes the system used by
the LIPN team in the task 10, Multilin-
gual Semantic Textual Similarity, at Sem-
Eval 2014, in both the English and Span-
ish sub-tasks. The system uses a sup-
port vector regression model, combining
different text similarity measures as fea-
tures. With respect to our 2013 partici-
pation, we included a new feature to take
into account the geographical context and
a new semantic distance based on the
Bhattacharyya distance calculated on co-
occurrence distributions derived from the
Spanish Google Books n-grams dataset.
1 Introduction
After our participation at SemEval 2013 with
LIPN-CORE (Buscaldi et al., 2013) we found that
geography has an important role in discriminating
the semantic similarity of sentences (especially in
the case of newswire). If two events happened in
a different location, their semantic relatedness is
usually low, no matter if the events are the same.
Therefore, we worked on a similarity measure able
to capture the similarity between the geographic
contexts of two sentences. We tried also to rein-
force the semantic similarity features by introduc-
ing a new measure that calculates word similari-
ties on co-occurrence distributions extracted from
Google Books bigrams. This measure was intro-
duced only for the Spanish runs, due to time con-
straints. The regression model used to integrate
the features was the ?-Support Vector Regression
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
model (?-SVR) (Sch?olkopf et al., 1999) imple-
mentation provided by LIBSVM (Chang and Lin,
2011), with a radial basis function kernel with the
standard parameters (? = 0.5). We describe all
the measures in Section 2; the results obtained by
the system are detailed in Section 3.
2 Similarity Measures
In this section we describe the measures used as
features in our system. The description of mea-
sures already used in our 2013 participation is less
detailed than the description of the new ones. Ad-
ditional details on the measures may be found in
(Buscaldi et al., 2013). When POS tagging and
NE recognition were required, we used the Stan-
ford CoreNLP
1
for English and FreeLing
2
3.1 for
Spanish.
2.1 WordNet-based Conceptual Similarity
This measure has been introduced in order to mea-
sure similarities between concepts with respect to
an ontology. The similarity is calculated as fol-
lows: first of all, words in sentences p and q are
lemmatised and mapped to the related WordNet
synsets. All noun synsets are put into the set of
synsets associated to the sentence, C
p
and C
q
, re-
spectively. If the synsets are in one of the other
POS categories (verb, adjective, adverb) we look
for their derivationally related forms in order to
find a related noun synset: if there exists one, we
put this synset in C
p
(or C
q
). No disambigua-
tion process is carried out, so we take all possible
meanings into account.
Given C
p
and C
q
as the sets of concepts con-
tained in sentences p and q, respectively, with
1
http://www-nlp.stanford.edu/software/corenlp.shtml
2
http://nlp.lsi.upc.edu/freeling/
400
|C
p
| ? |C
q
|, the conceptual similarity between p
and q is calculated as:
ss(p, q) =
?
c
1
?C
p
max
c
2
?C
q
s(c
1
, c
2
)
|C
p
|
where s(c
1
, c
2
) is a conceptual similarity mea-
sure. Concept similarity can be calculated in dif-
ferent ways. We used a variation of the Wu-Palmer
formula (Wu and Palmer, 1994) named ?Proxi-
Genea3?, introduced by (Dudognon et al., 2010),
which is inspired by the analogy between a family
tree and the concept hierarchy in WordNet. The
ProxiGenea3 measure is defined as:
s(c
1
, c
2
) =
1
1 + d(c
1
) + d(c
2
)? 2 ? d(c
0
)
where c
0
is the most specific concept that is
present both in the synset path of c
1
and c
2
(that is,
the Least Common Subsumer or LCS). The func-
tion returning the depth of a concept is noted with
d.
2.2 IC-based Similarity
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik?s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity met-
ric. This measure is more precise than the one
introduced in the previous subsection because it
takes into account also the importance of concepts
and not only their relative position in the hierarchy.
We refer to (Buscaldi et al., 2013) and (Mihalcea
et al., 2006) for a detailed description of the mea-
sure. The idf weights for the words were calcu-
lated using the Google Web 1T (Brants and Franz,
2006) frequency counts, while the IC values used
are those calculated by Ted Pedersen (Pedersen et
al., 2004) on the British National Corpus
3
.
2.3 Syntactic Dependencies
This measure tries to capture the syntactic simi-
larity between two sentences using dependencies.
Previous experiments showed that converting con-
stituents to dependencies still achieved best results
on out-of-domain texts (Le Roux et al., 2012), so
we decided to use a 2-step architecture to obtain
syntactic dependencies. First we parsed pairs of
sentences with the LORG parser
4
. Second we con-
3
http://www.d.umn.edu/ tpederse/similarity.html
4
https://github.com/CNGLdlab/LORG-Release
verted the resulting parse trees to Stanford depen-
dencies
5
.
Given the sets of parsed dependencies D
p
and
D
q
, for sentence p and q, a dependency d ? D
x
is a triple (l, h, t) where l is the dependency label
(for instance, dobj or prep), h the governor and
t the dependant. The similarity measure between
two syntactic dependencies d
1
= (l
1
, h
1
, t
1
) and
d
2
= (l
2
, h
2
, t
2
) is the levenshtein distance be-
tween the labels l
1
and l
2
multiplied by the aver-
age of idf
h
? s
WN
(h
1
, h
2
) and idf
t
? s
WN
(t
1
, t
2
),
where idf
h
and idf
t
are the inverse document fre-
quencies calculated on Google Web 1T for the
governors and the dependants (we retain the max-
imum for each pair), respectively, and s
WN
is cal-
culated using formula ??. NOTE: This measure
was used only in the English sub-task.
2.4 Information Retrieval-based Similarity
Let us consider two texts p and q, an IR system S
and a document collection D indexed by S. This
measure is based on the assumption that p and q
are similar if the documents retrieved by S for the
two texts, used as input queries, are ranked simi-
larly.
Let be L
p
= {d
p
1
, . . . , d
p
K
} and L
q
=
{d
q
1
, . . . , d
q
K
}, d
x
i
? D the sets of the top K
documents retrieved by S for texts p and q, respec-
tively. Let us define s
p
(d) and s
q
(d) the scores as-
signed by S to a document d for the query p and
q, respectively. Then, the similarity score is calcu-
lated as:
sim
IR
(p, q) = 1?
?
d?L
p
?L
q
?
(s
p
(d)?s
q
(d))
2
max(s
p
(d),s
q
(d))
|L
p
? L
q
|
if |L
p
? L
q
| 6= ?, 0 otherwise.
For the participation in the English sub-task we
indexed a collection composed by the AQUAINT-
2
6
and the English NTCIR-8
7
document collec-
tions, using the Lucene
8
4.2 search engine with
BM25 similarity. The Spanish index was cre-
ated using the Spanish QA@CLEF 2005 (agencia
EFE1994-95, El Mundo 1994-95) and multiUN
5
We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
6
http://www.nist.gov/tac/data/data desc.html#AQUAINT-
2
7
http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8-
databases.php
8
http://lucene.apache.org/core
401
(Eisele and Chen, 2010) collections. The K value
was set to 70 after a study detailed in (Buscaldi,
2013).
2.5 N-gram Based Similarity
This measure tries to capture the fact that similar
sentences have similar n-grams, even if they are
not placed in the same positions. The measure is
based on the Clustered Keywords Positional Dis-
tance (CKPD) model proposed in (Buscaldi et al.,
2009) for the passage retrieval task.
The similarity between a text fragment p and
another text fragment q is calculated as:
sim
ngrams
(p, q) =
?
?x?Q
h(x, P )
?
n
i=1
w
i
d(x, x
max
)
Where P is the set of the heaviest n-grams in p
where all terms are also contained in q; Q is the
set of all the possible n-grams in q, and n is the
total number of terms in the longest sentence. The
weights for each term w
i
are calculated as w
i
=
1 ?
log(n
i
)
1+log(N)
where n
i
is the frequency of term
t
i
in the Google Web 1T collection, and N is the
frequency of the most frequent term in the Google
Web 1T collection. The weight for each n-gram
(h(x, P )), with |P | = j is calculated as:
h(x, P ) =
{
?
j
k=1
w
k
if x ? P
0 otherwise
The function d(x, x
max
) determines the minimum
distance between a n-gram x and the heaviest one
x
max
as the number of words between them.
2.6 Geographical Context Similarity
We observed that in many sentences, especially
those extracted from news corpora, the compati-
bility of the geographic context between the sen-
tences is an important clue to determine if the sen-
tences are related or not. This measure tries to
measure if the two sentences refer to events that
took place in the same geographical area. We built
a database of geographically-related entities, using
geo-WordNet (Buscaldi and Rosso, 2008) and ex-
panding it with all the synsets that are related to a
geographically grounded synset. This implies that
also adjectives and verbs may be used as clues for
the identification of the geographical context of a
sentence. For instance, ?Afghan? is associated to
?Afghanistan?, ?Sovietize? to ?Soviet Union?, etc.
The Named Entities of type PER (Person) are also
used as clues: we use Yago
9
to check whether the
NE corresponds to a famous leader or not, and in
the affirmative case we include the related nation
to the geographical context of the sentence. For in-
stance, ?Merkel? is mapped to ?Germany?. Given
G
p
and G
q
the sets of places found in sentences p
and q, respectively, the geographical context simi-
larity is calculated as follows:
sim
geo
(p, q) = 1?log
K
?
?
?
1 +
?
x?G
p
min
y?G
q
d(x, y)
max(|G
p
|, |G
q
|)
?
?
?
Where d(x, y) is the spherical distance in Km. be-
tween x and y, and K is a normalization factor set
to 10000 Km. to obtain similarity values between
1 and 0.
2.7 2-grams ?Spectral? Distance
This measure is used to calculate the seman-
tic similarity of two words on the basis of their
context, according to the distributional hypothe-
sis. The measure exploits bi-grams in the Google
Books n-gram collection
10
and is based on the dis-
tributional hypothesis, that is, ?words that tend to
appear in similar contexts are supposed to have
similar meanings?. Given a word w, we calcu-
late the probability of observing a word x know-
ing that it is preceded by w as p(x|w) = p(w ?
x)/p(w) = c(?wx?)/c(?w?), where c(?wx?) is
the number of bigrams ?w x? observed in Google
Books (counting all publication years) 2-grams
and c(?w?) is the number of occurrences of w ob-
served in Google Books 1-grams. We calculate
also the probability of observing a word y know-
ing that it is followed by w as p(y|w) = p(w ?
y)/p(w) = c(?yw?)/c(?w?). In such a way, we
may obtain for a word w
i
two probability distri-
butions D
w
i
p
and D
w
i
f
that can be compared to the
distributions obtained in the same way for another
word w
j
. Therefore, we calculate the distance of
two words comparing the distribution probabilities
built in this way, using the Bhattacharyya coeffi-
cient:
9
http://www.mpi-inf.mpg.de/yago-naga/yago/
10
https://books.google.com/ngrams/datasets
402
sf
(w
i
, w
j
) = ? log
(
?
x?X
?
D
w
i
f
(x) ?D
w
j
f
(x)
)
s
p
(w
i
, w
j
) = ? log
(
?
x?X
?
D
w
i
p
(x) ?D
w
j
p
(x)
)
the resulting distance between w
i
and w
j
is cal-
culated as the average between s
f
(w
i
, w
j
) and
s
p
(w
i
, w
j
). All words in sentence p are compared
to the words of sentence q using this similarity
value. The words that are semantically closer are
paired; if a word cannot be paired (average dis-
tance with any of the words in the other sentence
> 10), then it is left unpaired. The value used as
the final feature is the averaged sum of all distance
scores.
2.8 Other Measures
In addition to the above text similarity measures,
we used also the following common measures:
Cosine
Cosine distance calculated between
p = (w
p
1
, . . . , w
p
n
) and q = (w
q
1
, . . . , w
q
n
), the
vectors of tf.idf weights associated to sentences
p and q, with idf values calculated on Google Web
1T.
Edit Distance
This similarity measure is calculated using the
Levenshtein distance on characters between the
two sentences.
Named Entity Overlap
This is a per-class overlap measure (in this way,
?France? as an Organization does not match
?France? as a Location) calculated using the Dice
coefficient between the sets of NEs found, respec-
tively, in sentences p and q.
3 Results
3.1 Spanish
In order to train the Spanish model, we trans-
lated automatically all the sentences in the English
SemEval 2012 and 2013 using Google Translate.
We also built a corpus manually using definitions
from the RAE
11
(Real Academia Espa?nola de la
Lengua). The definitions were randomly extracted
and paired at different similarity levels (taking into
11
http://www.rae.es/
account the Dice coefficient calculated on the def-
initions bag-of-words). Three annotators gave in-
dependently their similarity judgments on these
paired definitions. A total of 200 definitions were
annotated for training. The official results for the
Spanish task are shown in Table 1. In Figure 1 we
show the results obtained by taking into account
each individual feature as a measure of similarity
between texts. These results show that the combi-
nation was always better than the single features
(as expected), and the feature best able to capture
semantic similarity alone was the cosine distance.
In Table 2 we show the results of the ablation
test, which shows that the features that most con-
tributed to improve the results were the IR-based
similarity for the news dataset and the cosine dis-
tance for the Wikipedia dataset. The worst feature
was the NER overlap (not taking into account it
would have allowed us to gain 2 places in the final
rankings).
Wikipedia News Overall
LIPN-run1 0.65194 0.82554 0.75558
LIPN-run2 0.71647 0.8316 0.7852
LIPN-run3 0.71618 0.80857 0.77134
Table 1: Spanish results (Official runs).
The differences between the three submit-
ted runs are only in the training set used.
LIPN-run1 uses all the training data available
together, LIPN-run3 uses a training set com-
posed by the translated news for the news dataset
and the RAE training set for the Wikipedia dataset;
finally, the best run LIPN-run2 uses the same
training sets of run3 together to build a single
model.
3.2 English
Our participation in the English task was ham-
pered by some technical problems which did not
allow us to complete the parsing of the tweet data
in time. As a consequence of this and some er-
rors in the scripts launched to finalize the experi-
ments, the submitted results were incomplete and
we were able to detect the problem only after the
submission. We show in Table 3 the official re-
sults of run1 with the addition of the results on the
OnWN dataset calculated after the participation to
the task.
403
Figure 1: Spanish task: results taking into account the individual features as semantic similarity mea-
sures.
Ablated feature Wikipedia News Overall diff
LIPN-run2 (none) 0.7165 0.8316 0.7852 0.00%
1:CKPD 0.7216 0.8318 0.7874 0.22%
2:WN 0.7066 0.8277 0.7789 ?0.63%
3:Edit Dist 0.708 0.8242 0.7774 ?0.78%
4:Cosine 0.6849 0.8235 0.7677 ?1.75%
5:NER overlap 0.7338 0.8341 0.7937 0.85%
6:Mihalcea-JC 0.7103 0.8301 0.7818 ?0.34%
7:IRsim 0.7161 0.8026 0.7677 ?1.74%
8:geosim 0.7185 0.8325 0.7865 0.14%
9:Spect. Dist 0.7243 0.8311 0.7880 0.28%
Table 2: Spanish task: ablation test.
Dataset Correlation
Complete (official + OnWN) 0.6687
Complete (only official) 0.5083
deft-forum 0.4544
deft-news 0.6402
headlines 0.6527
images 0.8094
OnWN (unofficial) 0.8039
tweet-news 0.5507
Table 3: English results (Official run + unofficial
OnWN).
4 Conclusions and Future Work
The introduced measures were studied on the
Spanish subtask, observing a limited contribu-
tion from geographic context similarity and spec-
tral distance. The IR-based measure introduced
in 2013 proved to be an important feature for
newswire-based datasets as in the 2013 English
task, even when trained on a training set derived
from automatic translation, which include many
errors. Our participation in the English subtask
was inconclusive due to the technical faults experi-
enced to produce our results. We will nevertheless
take into account the lessons learned in this partic-
ipation for future ones.
Acknowledgements
Part of this work has been carried out with the sup-
port of LabEx-EFL (Empirical Foundation of Lin-
guistics) strand 5 (computational semantic analy-
sis). We are also grateful to CoNACyT (Consejo
NAcional de Ciencia y Tecnologia) for support to
404
this work.
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1.
Davide Buscaldi and Paolo Rosso. 2008. Geo-
WordNet: Automatic Georeferencing of WordNet.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco.
Davide Buscaldi, Paolo Rosso, Jos?e Manuel G?omez,
and Emilio Sanchis. 2009. Answering ques-
tions with an n-gram based passage retrieval engine.
Journal of Intelligent Information Systems (JIIS),
34(2):113?134.
Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flo-
res, and Adrian Popescu. 2013. Lipn-core: Seman-
tic text similarity using n-grams, wordnet, syntac-
tic analysis, esa and information retrieval based fea-
tures. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 162?168,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Davide Buscaldi. 2013. Une mesure de similarit?e
s?emantique bas?ee sur la recherche d?information. In
5`eme Atelier Recherche d?Information SEmantique
- RISE 2013, pages 81?91, Lille, France, July.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Damien Dudognon, Gilles Hubert, and Bachelin Jhonn
Victorino Ralalason. 2010. Proxig?en?ea : Une
mesure de similarit?e conceptuelle. In Proceedings of
the Colloque Veille Strat?egique Scientifique et Tech-
nologique (VSST 2010).
Andreas Eisele and Yu Chen. 2010. Multiun:
A multilingual corpus from united nation docu-
ments. In Daniel Tapias, Mike Rosner, Ste-
lios Piperidis, Jan Odjik, Joseph Mariani, Bente
Maegaard, Khalid Choukri, and Nicoletta Calzo-
lari (Conference Chair), editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation, pages 2868?2872. Euro-
pean Language Resources Association (ELRA), 5.
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proc. of the Int?l. Conf. on Research in Computa-
tional Linguistics, pages 19?33.
Joseph Le Roux, Jennifer Foster, Joachim Wagner,
Rasul Samad Zadeh Kaljahi, and Anton Bryl.
2012. DCU-Paris13 Systems for the SANCL 2012
Shared Task. In The NAACL 2012 First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL), pages 1?4, Montr?eal, Canada, June.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st national conference on Artificial intelli-
gence - Volume 1, AAAI?06, pages 775?780. AAAI
Press.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Bernhard Sch?olkopf, Peter Bartlett, Alex Smola, and
Robert Williamson. 1999. Shrinking the tube: a
new support vector regression algorithm. In Pro-
ceedings of the 1998 conference on Advances in neu-
ral information processing systems II, pages 330?
336, Cambridge, MA, USA. MIT Press.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
405
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 114?120,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Pipeline Approach to Supervised Error Correction
for the QALB-2014 Shared Task
Nadi Tomeh
?
Nizar Habash
?
Ramy Eskander
?
Joseph Le Roux
?
{nadi.tomeh,leroux}@lipn.univ-paris13.fr
?
nizar.habash@nyu.edu
?
, ramy@ccls.columbia.edu
?
?
Universit?e Paris 13, Sorbonne Paris Cit?e, LIPN, Villetaneuse, France
?
Computer Science Department, New York University Abu Dhabi
?
Center for Computational Learning Systems, Columbia University
Abstract
This paper describes our submission to
the ANLP-2014 shared task on auto-
matic Arabic error correction. We present
a pipeline approach integrating an er-
ror detection model, a combination of
character- and word-level translation mod-
els, a reranking model and a punctuation
insertion model. We achieve an F
1
score
of 62.8% on the development set of the
QALB corpus, and 58.6% on the official
test set.
1 Introduction
Devising algorithms for automatic error correction
generated considerable interest in the community
since the early 1960s (Kukich, 1992) for at least
two reasons. First, typical NLP tools lack in ro-
bustness against errors in their input. This sen-
sitivity jeopardizes their usefulness especially for
unedited text, which is prevalent on the web. Sec-
ond, automated spell and grammar checkers facil-
itate text editing and can be of great help to non-
native speakers of a language. Several resources
and shared tasks appeared recently, including the
HOO task (Dale and Kilgarriff, 2010) and the
CoNLL task on grammatical error correction (Ng
et al., 2013b). In this paper we describe our partic-
ipation to the first shared task on automatic error
correction for Arabic (Mohit et al., 2014).
While non-word errors are relatively easy to
handle, the task is more challenging for gram-
matical and semantic errors. Detecting and cor-
recting such errors require context-sensitive ap-
proaches in order to capture the dependencies be-
tween the words of a text at various lexical and se-
mantic levels. All the more so for Arabic which
brings dependence down to the morphological
level (Habash, 2010).
A particularity interesting approach to error cor-
rection relies on statistical machine translation
(SMT) (Brockett et al., 2006), due to its context-
sensitivity and data-driven aspect. Therefore, the
pipeline system which we describe in Section 2
has as its core a phrase-based SMT component
(PBSMT) (Section 2.3). Nevertheless, several fac-
tors may hinder the success of this approach, such
as data sparsity, discrepancies between transla-
tion and error correction tasks, and the difficulty
of incorporating context-sensitive features into the
SMT decoder.
We address all these issues in our system which
achieves a better correction quality than a simple
word-level PBSMT baseline on the QALB corpus
(Zaghouani et al., 2014) as we show in our exper-
iments in Section 3.
2 Pipeline Approach to Error Correction
The PBSMT system accounts for context by learn-
ing, from a parallel corpus of annotated errors,
mappings from erroneous multi-word segments of
text to their corrections, and using a language
model to help select the suitable corrections in
context when multiple alternatives are present.
Furthermore, since the SMT approach is data-
driven, it is possible to address multiple types of
errors at once, as long as examples of them appear
in the training corpus. These errors may include
non-word errors, wrong lexical choices and gram-
matical errors, and can also handle normalization
issues (Yvon, 2010).
One major issue is data sparsity, since large
amount of labeled training data is necessary to
provide reliable statistics of all error types. We ad-
114
dress this issue by backing-off the word-level PB-
SMT model with a character-level correction com-
ponent, for which richer statistics can be obtained.
Another issue may stem from the inherent dif-
ference in nature between error correction and
translation. Unlike translation, the input and out-
put vocabularies in the correction task overlap sig-
nificantly, and the majority of input words are typi-
cally correct and are copied unmodified to the out-
put. The SMT system should handle correct words
by selecting their identities from all possible op-
tions, which may fail resulting in over-correction.
To help the SMT decoder decide, we augment our
pipeline with a problem zone detection compo-
nent, which supplies prior information on which
input words need to be corrected.
The final issue concerns the difficulty of incor-
porating features that require context across phrase
boundaries into the SMT decoder. A straightfor-
ward alternative is to use such features to rerank
the hypotheses in the SMT n-best hypotheses lists.
Since punctuation is particularity noisy in Ara-
bic data, we add a specialized punctuation inser-
tion component to our pipeline, depicted in Figure
1.
2.1 Error Detection
We formalize the error detection problem as a
sequence labeling problem (Habash and Roth,
2011). Errors are classified into substitution, in-
sertion and deletion errors. Substitutions involve
an incorrect word form that should be replaced by
another correct form. Insertions are words that
are incorrectly added into the text and should be
deleted. Deletions are simply missing words that
should be added.
We group all error classes into a simple binary
problem tag: a word from the input text is tagged
as ?PROB? if it is the result of an insertion or
a substitution of a word. Deleted words, which
cannot be tagged themselves, cause their adjacent
words to be marked as PROB instead. In this way,
the subsequent components in the pipeline can be
alerted to the possibility of a missing word via its
surroundings. Any words not marked as PROB are
given an ?OK? tag.
Gold tags, necessary for training, can be gener-
ated by comparing the text to its correction using
some sequence alignment technique, for which we
use SCLITE (Fiscus, 1998).
For this task, we use Yamcha (Kudo and Mat-
sumoto, 2003) to train an SVM classifier using
morphological and lexical features. We employ
a quadratic polynomial kernel. The static feature
window context size is set to +/- 2 words; the pre-
vious two (dynamic) predicted tags are also used
as features.
The feature set includes the surface forms and
their normalization after ?Alef?, ?Ya? and digit
normalization, the POS tags and the lemmas of the
words. These morphological features are obtained
using MADA 3.0 (Habash et al., 2009).
1
We also
use a set of word, POS and lemma 3-gram lan-
guage models scores as features. These LMs are
built using SRILM (Stolcke, 2002).
The error detection component is integrated into
the pipeline by concatenating the predicted tags
with the words of the input text. The SMT model
uses this additional information to learn distinct
mappings conditional on the predicted correctness
of words.
2.2 Character-level Back-off Correction
Each word that is labeled as error (PROB) in the
output of the error detection component is mapped
to multiple possible corrections using a weighted
finite-state transducer similar to the transducers
used in speech recognition (Mohri et al., 2002).
The WFST, for which we used OpenFST (Al-
lauzen et al., 2007), operates on the character
level, and the character mapping is many-to-many
(similar to the phrase-based SMT framework).
The score of each proposed correction is a com-
bination of the scores of character mappings used
to build it. The list is filtered using WFST scores
and an additional character-level LM score. The
result is a list of error-tagged words and their cor-
rection suggestions, which constitutes a small on-
the-fly phrase table used to back-off primary PB-
SMT table.
During training, the mapping dictionary is
learned from the training after aligning it at the
character level using SCLITE. Mapping weights
are computed as their normalized frequencies in
the aligned training corpus.
2.3 Word-level PBSMT Correction
We formalize the correction process as a phrase-
based statistical machine translation problem
(Koehn et al., 2003), at the word-level, and solve
1
We did not use MADAMIRA (the newest version of
MADA) since it was not available when this component was
built.
115
Character-level 
Correction 
Error 
Detection 
Word-level 
PBSMT Correction 
N-best 
Reranking 
Punctuation 
Insertion 
,	 ? .	 ?
Input Error-tagged text 
N-best hypotheses 
Reranked best hypothesis Output 
Back-off Phrase 
tables 
Primary 
Figure 1: Input text is run through the error detection component which labels the problematic words.
The labeled text is then fed to the character-level correction components which constructs a back-off
phrase table. The PBSMT component then uses two phrase tables to generate n-best correction hy-
potheses. The reranking component selects the best hypothesis, and pass it to the punctuation insertion
component in order to produce the final output.
it using Moses, a well-known PBSMT tool (Koehn
et al., 2007). The decoder constructs a correction
hypothesis by first segmenting the input text into
phrases, and mapping each phrase into its best cor-
rection using a combination of scores including a
context-sensitive LM score.
Unlike translation, error correction is mainly
monotonic, therefore we set disallow reordering
by setting the distortion limit in Moses to 0.
2
When no mapping can be found for a given
phrase in the primary phrase table, the decoder
looks it up in the back-off model. The decoder
searches the space of all possible correction hy-
potheses, resulting from alternative segmentations
and mappings, and returns the list of n-best scor-
ing hypotheses.
2.4 N-best List Reranking
In this step, we combine LM information with lin-
guistically and semantically motivated features us-
ing learning to rank methods (Tomeh et al., 2013).
Discriminative reranking (Liu, 2009) allows each
hypothesis to be represented as an arbitrary set of
features without the need to explicitly model their
interactions. Therefore, the system benefits from
global and potentially complex features which are
not available to the baseline decoder.
Each hypothesis in an n-best list is represented
by a d-dimensional feature vector. Word error rate
(WER) is computed for each hypotheses by com-
paring it to the reference correction. The resulting
2
Only 0.14% of edits in the QALB corpus are actually
reordering.
scored n-best list is used for supervised training
of a reranking model. We employ a pairwise ap-
proach to ranking which takes pairs of hypotheses
as instances in learning, and formalizes the rank-
ing problem as pairwise classification.
For this task we use RankSVM (Joachims,
2002) which is a method based on Support Vec-
tor Machines (SVMs). We use only linear kernels
to keep complexity low. We use a rich set of fea-
tures including LM scores on surface forms, POS
tags and lemmas. We also use a feature based on a
global model of the semantic coherence of the hy-
potheses (Tomeh et al., 2013). The new top ranked
hypothesis is the output of this step which is then
fed to the next component.
2.5 Punctuation Insertion
We developed a model that predicts the occurrence
of periods and commas in a given Arabic text.
The core model is a decision tree classifier trained
on the QALB parallel training data using WEKA
(Hall et al., 2009). For each space between two
words, the classifier decides whether or not to in-
sert a punctuation mark, using a window size of
three words surrounding the underlying space.
The model uses the following features:
? A class punctuation feature, that is whether to
insert a period, a comma or none at the cur-
rent space location;
? The part-of-speech of the previous word;
? The existence of a conjunctive or connective
proclitic in the following word; that is a ?wa?
116
Precision?Recall Curve
Recall
Prec
ision
0.0 0.2 0.4 0.6 0.8 1.0
0.2
0.4
0.6
0.8
1.0
?
8.33
?
5.02
?
1.7
1.61
4.93
AUC= 0.715PRBE= 0.483, Cutoff= ?0.349Prec@rec(0.800)= 0.345, Cutoff= ?1.045
Figure 2: Evaluation of the error detection com-
ponent. AUC: Area Under the Curve, PRBE:
precision-recall break-even point. Classifier
thresholds are displayed on the right vertical axis.
or ?fa? proclitic that is either a conjunction, a
sub-conjunction or a connective particle.
We obtain POS and proclitic information using
MADAMIRA (Pasha et al., 2014). The output of
this component is the final output of the system.
3 Experiments
All the models we use in our pipeline are trained
in a supervised way using the training part of the
QALB corpus (Zaghouani et al., 2014), while we
reserve the development part of the corpus for test-
ing.
3.1 Error detection
We evaluate the error detection binary classifier in
terms of standard classification measures as shown
in Figure 2. Each point on the curve is computed
by selecting a threshold on the classifier score.
The threshold we use correspond to recall equal
to 80%, at which the precision is very low which
leaves much room for improvement in the perfor-
mance of the error detection component.
3.2 Character-level correction
We evaluate the character-level correction model
by measuring the percentage of erroneous phrases
that have been mapped to their in-context refer-
ence corrections. We found this percentage to be
41% on QALB dev data. We limit the size of
such phrases to one in order to focus on out-of-
vocabulary words.
3.3 Punctuation insertion
To evaluate the punctuation insertion indepen-
dently from the pipeline, we first remove the pe-
riods and commas from input text. Considering
only the locations where periods and commas ex-
ist, our model gives a recall of 49% and a precision
of 53%, giving an F
1
-score of 51%.
When we apply our punctuation model in the
correction pipeline, we find that it is always better
to keep the already existing periods and commas
in the input text instead of overwriting them by
the model prediction.
While developing the model, we ran experi-
ments where we train the complete list of fea-
tures produced by MADAMIRA; that is part-of-
speech, gender, number, person, aspect, voice,
case, mood, state, proclitics and enclitics. This
was done for two preceding words and two follow-
ing words. However, the results were significantly
outperformed by our final set-up.
3.4 The pipeline
The performance of the pipeline is evaluated in
terms of precision, recall and F
1
as computed by
the M
2
Scorer (Dahlmeier and Ng, 2012b). The
results presented in Table 1 show that a simple
PBSMT baseline achieves relatively good perfor-
mance compared to more sophisticated models.
The character-level back-off model helps by im-
proving recall at the expense of decreased preci-
sion. The error detection component hurts the per-
formance which could be explained by its intrin-
sic bad performance. Since more investigation is
needed to clarify on this point, we drop this com-
ponent from our submission. Both reranking and
punctuation insertion improve the performance.
Our system submission to the shared task (back-
off+PBSMT+Rank+PI) resulted in an F
1
score of
58.6% on the official test set, with a precision of
76.9% and a recall of 47.3%.
4 Related Work
Both rule-based and data-driven approaches to
error correction can be found in the literature
(Sidorov et al., 2013; Berend et al., 2013; Yi et
al., 2013) as well as hybridization of them (Putra
and Szabo, 2013). Unlike our approach, most of
117
System PR RC F
1
PBSMT 75.5 49.5 59.8
backoff+PBSMT 74.1 51.8 60.9
ED+backoff+PBSMT 61.3 45.4 52.2
backoff+PBSMT+Rank 75.7 52.1 61.7
backoff+PBSMT+Rank+PI 74.9 54.2 62.8
Table 1: Pipeline precision, recall and F
1
scores.
ED: error detection, PI: punctuation insertion.
the proposed systems build distinct models to ad-
dress individual types of errors (see the CoNLL-
2013, 2014 proceedings (Ng et al., 2013a; Ng
et al., 2014), and combine them afterwords us-
ing Integer Linear Programming for instance (Ro-
zovskaya et al., 2013). This approach is relatively
time-consuming when the number of error types
increases.
Interest in models that target all errors at once
has increased, using either multi-class classifiers
(Farra et al., 2014; Jia et al., 2013), of-the-shelf
SMT techniques (Brockett et al., 2006; Mizu-
moto et al., 2011; Yuan and Felice, 2013; Buys
and van der Merwe, 2013; Buys and van der
Merwe, 2013), or building specialized decoders
(Dahlmeier and Ng, 2012a).
Our system addresses the weaknesses of the
SMT approach using additional components in a
pipeline architecture. Similar work on word-level
and character-level model combination has been
done in the context of translation between closely
related languages (Nakov and Tiedemann, 2012).
A character-level correction model has also been
considered to reduce the out-of-vocabulary rate in
translation systems (Habash, 2008).
5 Conclusion and Future Work
We described a pipeline approach based on
phrase-based SMT with n-best list reranking. We
showed that backing-off word-level model with a
character-level model improves the performance
by ameliorating the recall of the system.
The main focus of our future work will be on
better integration of the error detection model, and
on exploring alternative methods for combining
the character and the word models.
Acknowledgments
This material is partially based on research funded
by grant NPRP-4-1058-1-168 from the Qatar Na-
tional Research Fund (a member of the Qatar
Foundation). The statements made herein are
solely the responsibility of the authors.
Nizar Habash performed most of his contri-
bution to this paper while he was at the Center
for Computational Learning Systems at Columbia
University.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In CIAA, pages 11?23.
Gabor Berend, Veronika Vincze, Sina Zarrie?, and
Rich?ard Farkas. 2013. Lfg-based features for noun
number and article grammatical errors. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning: Shared Task,
pages 62?67, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting esl errors using phrasal
smt techniques. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association
for Computational Linguistics, ACL-44, pages 249?
256, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Buys and Brink van der Merwe. 2013. A tree
transducer model for grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 43?51, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
EMNLP-CoNLL, pages 568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In HLT-
NAACL, pages 568?572.
Robert Dale and Adam Kilgarriff. 2010. Helping our
own: Text massaging for computational linguistics
as a new shared task. In INLG.
Noura Farra, Nadi Tomeh, Alla Rozovskaya, and Nizar
Habash. 2014. Generalized character-level spelling
error correction. In ACL (2), pages 161?167.
Jon Fiscus. 1998. Speech Recognition Scor-
ing Toolkit (SCTK). National Institute of Standard
Technology (NIST). http://www.itl.nist.
gov/iad/mig/tools/.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
arabic handwriting recognition. In Proceedings of
118
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-08: HLT, Short Papers, pages 57?
60, Columbus, Ohio.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Gram-
matical error correction as multiclass classification
with single model. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 74?81, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?02,
pages 133?142.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT/NAACL), pages 127?133,
Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Comput. Surv.,
24(4):377?439, December.
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval. Now Publishers Inc., Hanover, MA,
USA.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revision
log of language learning sns for automated japanese
error correction of second language learners. In IJC-
NLP, pages 147?155.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech & Language,
16(1):69?88.
Preslav Nakov and J?org Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In ACL (2), pages 301?305.
Hwee Tou Ng, Joel Tetreault, Siew Mei Wu, Yuanbin
Wu, and Christian Hadiwinoto, editors. 2013a. Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Association for Computational Linguistics, Sofia,
Bulgaria, August.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013b. The conll-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant, editors. 2014. Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task. Association for
Computational Linguistics, Baltimore, Maryland,
June.
Arfath Pasha, Mohamed Al-Badrashiny, Mona T. Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. Madamira: A fast, comprehensive tool for
morphological analysis and disambiguation of ara-
bic. In LREC, pages 1094?1101.
Desmond Darma Putra and Lili Szabo. 2013. Uds
at conll 2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 88?95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
119
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The university of illinois sys-
tem in the conll-2013 shared task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Grigori Sidorov, Anubhav Gupta, Martin Tozer, Do-
lors Catala, Angels Catena, and Sandrine Fuentes.
2013. Rule-based system for automatic grammar
correction using syntactic n-grams for english lan-
guage learning (l2). In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 96?101, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra,
Pradeep Dasigi, and Mona Diab. 2013. Reranking
with linguistic and semantic features for arabic op-
tical character recognition. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
549?555, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim.
2013. Kunlp grammatical error correction system
for conll-2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 123?127,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52?61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Franc?ois Yvon. 2010. Rewriting the orthography
of sms messages. Natural Language Engineering,
16:133?159, 3.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale arabic error annotation: Guidelines and
framework. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?14), Reykjavik, Iceland, May. Euro-
pean Language Resources Association (ELRA).
120
Proceedings of SADAATL 2014, pages 21?30,
Dublin, Ireland, August 24, 2014.
Ontology-based Technical Text Annotation
Franc?ois L
?
evy
?
Nadi Tomeh
?
Yue Ma
?
{francois.levy,nadi.tomeh}@lipn.univ-paris13.fr
?
, mayue@tcs.inf.tu-dresden.de
?
?
Universit?e Paris 13, Sorbonne Paris Cit?e, LIPN, Villetaneuse, France
?
Dresden University of Technology, Dresden, Germany
Abstract
Powerful tools could help users explore and maintain domain specific documentations, provided
that documents have been semantically annotated. For that, the annotations must be sufficiently
specialized and rich, relying on some explicit semantic model, usually an ontology, that repre-
sents the semantics of the target domain. In this paper, we learn to annotate biomedical scientific
publications with respect to a Gene Regulation Ontology. We devise a two-step approach to an-
notate semantic events and relations. The first step is recast as a text segmentation and labeling
problem and solved using machine translation tools and a CRF, the second as multi-class classi-
fication. We evaluate the approach on the BioNLP-GRO benchmark, achieving an average 61%
F-measure on the event detection by itself and 50% F-measure on biological relation annotation.
This suggests that human annotators can be supported in domain specific semantic annotation
tasks. Under different experimental settings, we also conclude some interesting observations: (1)
For event detection and compared to classical time-consuming sequence labeling approach, the
newly proposed machine translation based method performed equally well but with much less
computation resource required. (2) A highly domain specific part of the task, namely proteins
and transcription factors detection, is best performed by domain aware tools, which can be used
separately as an initial step of the pipeline.
1 Introduction
As is mostly the case with technical documents, biomedical documents, a critical resource for many
applications, are usually rich with domain knowledge. Efforts in formalizing biomedical information
have resulted in many interesting biomedical ontologies, such as Gene Ontology and SNOMED CT.
Ontology-based semantic annotation for biomedical documents is necessary to grasp important semantic
information, to enhance interoperability among systems, and to allow for semantic search instead of plain
text search (Welty and Ide, 1999; Uren et al., 2006; Nazarenko et al., 2011). Furthermore, it provides a
platform for consistency checking, decisions support, etc.
Ideal annotation should be accurate, thus requiring intensive knowledge and context awareness, and
it should be automatic at the same time, since expert work is time consuming. Many efforts have been
made in this field, from named entity recognition (NER) to information extraction (Ciravegna et al.,
2004; Kiryakov et al., 2004), both in open domain (Uren et al., 2006; Cucerzan, 2007; Mihalcea and
Csomai, 2007) and particular domains (Wang, 2009; Liu et al., 2011). Most cases of NER or information
extraction focus on a small set of categories to be annotated, such as Person, Location, Organization,
Misc, etc. Such a scenario often requires a special vocabulary, and generally benefits much from a
limited set of linguistic templates for names or verbs. These restrictions can be widened by linguistic
efforts in recognizing relevant forms, but they are the condition of accuracy.
With the increasing importance of ontologies in general or in specific domains
1
, annotating a text
regarding to a rich ontology has become necessary. For example, the BioNLP ST?11 GENIA challenge
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
For instance, the OBO site lists 130 biological ontologies. The NASA publishes SWEET, a set of 200 small ontologies
dedicated to earth and environment. The ProtegeOntology Library lists around 90 items.
21
task involved merely 10 concepts and 6 relations, but BioNLP ST?13 GRO task concerns more than 200
concepts and 10 relations. Some ontology-based annotating systems exist and include SemTag (Dill et
al., 2003), DBpediaSpotlight (Mendes et al., 2011), Wiki Machine (LiveMemories, 2010). However,
each of them is devoted to a particular ontology, for instance, Stanford TAP entity catalog (Guha and
McCool, 2003) for SemTag and DBpedia Lexicalization Dataset
2
for DBpediaSpotlight. Hence, these
existing systems cannot be directly used to reliably annotate biomedical domain, which is the case of the
present work. To this end, the challenge that we focus on is semantic annotation of texts in a particular
technical domain with regards to a rather large ontology (a large set of categories), which comes with
its technical language and involves uses of concepts or relations that are not named entities. In this kind
of use cases, one can get some manual expert annotations, but generally not in large quantity. And one
has to learn from them in order to annotate more. This paper experiments on a set of biological texts
provided for the BioNLP GRO task
3
. Since our approach is solely data-driven, it can be directly applied
to obtain helpful annotation on legal texts governing a particular activity, formalization of specifications
and requirement engineering, conformance of permanent services to their defining contracts, etc.
The task at hand is described in section 2, together with the main features of the GRO ontology used in
the experiments. We consider here a classical pipeline architecture. The subtasks are recast as machine
translation and sequence labeling problems, and standard tools are used to solve them. The first layer
is based on domain lexicons and is not our work. Our tools are applied to the detection of relations
and events
4
. Section 3 presents experiments, results and comparisons on the annotation of event terms.
Section 4 presents experiments in detecting relations and completing event terms with their arguments.
2 A Pipeline Approach to Ontology-Based Text Annotation
The GRO task (Kim et al., 2013) aims to populate the Gene Regulation Ontology (GRO) (Beisswanger
et al., 2008) with events and relations identified from text. We consider here automatically annotating
biomedical documents with respect to relations and events belonging to the GRO.
GRO has two top-level categories of concepts, Continuant and Occurrent, where the Occurrent branch
has concepts for processes that are related to the regulation of gene expression (e.g. Transcription,
RegulatoryProcess), and the Continuant branch has concepts mainly for physical entities that are involved
in those processes (e.g. Gene, Protein, Cell). It also defines semantic relations (e.g. hasAgent, locatedIn)
that link the instances of the concepts.
The representation involves three primary categories of annotation elements: entities (i.e. the instances
of Continuant concepts), events (i.e. those of Occurrent concepts) and relations. Mentions of entities in
text can be either contiguous or discontinuous spans that are assigned the most specific and appropriate
Continuant concepts (e.g. TranscriptionFactor, CellularComponent). Event annotation is associated with
the mention of a contiguous span in text (called event trigger) that explicitly suggests the annotated event
type (e.g. ?controls? - RegulatoryProcess). If a participant of an event, either an entity or another event,
can be explicitly identified with a specific mention in text, the participant is annotated with its role in the
event. In this task, only two types of roles are considered, hasAgent and hasPatient, where an agent of
an event is an entity that causes or initiates the event (e.g. a protein that causes a regulation event), and
a patient of an event is an entity on which the event is carried out (e.g. the gene that is expressed in a
gene expression event) (Dowty, 1991). Relation annotation is to annotate other semantic relations (e.g.
locatedIn, fromSpecies) between entities and/or events, i.e. those without event triggers. An example
annotation is shown in Figure 1.
The annotation of Continuant concepts has been considered for a long time and has well established
methods relying on large dictionaries. GRO task has provided these annotations and only evaluates events
and relations detection, including the triggers of events. We produce the annotation in two steps. The first
step takes as input a biological text and the corresponding Continuant concepts and produces Occurent
concepts (event triggers and their types). We provide two different formalizations of this problem: one
2
http://dbpedia.org/Lexicalizations
3
accessible on http://2013.bionlp-st.org/tasks
4
?Event? is taken here in a biological sense, which may not fit to the state-event-process distinction or other linguistic views
22
Figure 1: Example annotations from the GRO corpus (Kim et al., 2013).
as a named entity recognition problem, and the other as a machine translation problem. The second step
takes as input the text and both Continuant and Occurrent concepts (predicted in step 1) and predicts
relations between them. Relations are either: (a) an ?event argument role? relation (hasAgent, hasPa-
tient) between an Occurent concept and another concept, or (b) one of a small set of predefined relations
between two concepts that do not involve trigger words (encodes, hasFunction, locatedIn, precedes, has-
Part, resultsIn, fromSpecies, startsIn, endsIn)
5
We formalize this problem as a multi-class classification
problem and solve it using a discriminative maximum-entropy classifier.
3 Step One: Event Annotation
In this step, event triggers (continuous span of text) are identified and given a label from the Occurrent
concepts (98 label in total). We formalize this task as text segmentation and labeling, and compare two
approaches to solve it: named-entity recognition approach and machine translation approach.
3.1 Event detection as named-entity recognition
A direct formalization of the event detection task is as named-entity recognition (hence named NER4SA).
The NER task is to locate and classify elements of text into pre-defined categories. In our case, the ele-
ments are contiguous segments representing biological events, and the categories are their corresponding
ontology-based occurrent labels. Conditional random fields (CRF), which represents the state of the art
in sequence labeling, are widely used for NER (Finkel et al., 2005). This is mainly because they allow for
discriminative training benefiting from manually annotated examples, and because of their ability to take
the sequential structure into consideration through the flow of probabilistic information during inference.
Here, the input sequence x = (x
1
, ..., x
n
) represents the words, and the output sequence y = (y
1
, ..., y
n
)
represents the corresponding labels. The labels we use are the ontology-based Occurrent corresponding
to events, combined with a segmentation marker in order to capture annotations possibly spanning mul-
tiple words. These markers are ?B? for beginning of event, ?I? for inside an event and ?O? for outside an
event.
CRF is powerful in allowing for a wide range of features to be considered in the model. However,
it rapidly becomes time and memory consuming when incorporating wide-range dependencies between
labels. Therefore, in our experiment, we use a linear-chain CRF (bi-gram label dependency) with features
including the current word as well as prefix and suffix character n-grams up to length 2. We compare
two label schemes, one containing the ?B?, ?I?, and ?O? markers (called BIO) and a simpler ?I?, and ?O?
scheme (called IO).
Table 1 summarizes the results using the following settings: the training data and half of the develop-
ment data from GRO task is taken to train CRF models, and the rest half development data is taken as test.
We use the Stanford NER recognizer for the implementation
6
. The performance of the system varies
significantly from an event trigger to another. For example, ?GeneExpression? is well characterized and
relatively easily detected as indicated by an F-measure of 88%, while ?Disease? has a very bad recall
resulting in a low F-measure of 21%. The majority of triggers such as ?BindingToProtein? and ?Posi-
tiveRegulation? lie in the middle. ?RNASplicing? was not recognized at all, which is partially due to its
5
Not all these relation types are present in the training and development data.
6
http://nlp.stanford.edu/software/CRF-NER.shtml
23
Precision Recall F-measure TP FP FN
Trigger IO BIO IO BIO IO BIO IO BIO IO BIO IO BIO
BindingToProtein 0.86 0.60 0.71 18 3 12
Disease 0,67 0.13 0.21 2 1 14
GeneExpression 0.85 0.92 0.88 23 4 2
PositiveRegulation 0.79 0.61 0.69 30 8 19
RNASplicing 0.00 0.00 0.00 0 1 4
Localization 0.00 0.50 0.00 0.13 0.00 0.20 0 1 1 1 8 7
CellDeath 1.00 1.00 0.33 0.67 0.50 0.80 1 2 0 0 2 1
RegulatoryProcess 0.69 0.75 0.39 0.39 0.50 0.51 9 9 4 3 14 14
Aggregated 0,76 0.77 0,43 0.44 0.556 0.563 136 138 42 41 175 173
Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false
negative.
small number of occurrences in the data. On the aggregated class of (all) event triggers, the best result
is obtained using the BIO scheme: 56.3% F-measure with a precision of 77% but with a weaker recall
(44%). However, as given in the first block of Table 1, in most of the case IO and BIO schemes resulted
in a comparable performance for triggers such as ?BindingToProtein? and ?Disease?. But there are three
cases (second block of Table 1) where a more fine-grained representation BIO slightly outperformed the
basic IO representation. These results suggest that the segmentation scheme is of little importance for
the performance of NER4SA.
3.2 Event detection as phrase-based SMT
In this section, we model the semantic annotation of specialized documents as a phrase-based statistical
machine translation task (hence named SMT4SA). This modeling provides a potential advantage com-
pared to the CRF approach due to its capacity to recognize (possibly complex) phrases as the relevant
textual units to translate (annotate for our task). However, it is more difficult to incorporate arbitrary
features into the model. The simple idea in SMT4SA is to consider an initial unannotated text as if it was
written in a ?foreign? language, and the annotated text as the target ?translated? text. Formally speaking,
two sentences ?s
1
, s
2
? are given in two languages L
1
and L
2
: L
1
is English and L
2
= L
1
? V oc(O) is
the union of English and the vocabulary of the ontology V oc(O) used as semantic tagset.
7
We say that
s
2
is an annotated version of s
1
if it is obtained by replacing some sequences of English words in s
1
by
elements of V oc(O) as shown in the following Table 2.
Language L
1
: The corresponding gene was assigned to chromosome 14q31, the same
region where genetic alterations have been associated with several
abnormalities of thyroid hormone response.
Language L
2
: The corresponding TTGene was assigned to TTChromosome, the same
region where genetic alterations have been associated with several
abnormalities of TTOrganicChemical TEResponseProcess.
Table 2: L1 and L2 languages (TT and TE escapes mark entities and events)
Several steps are performed in order to construct a phrase-based SMT (Koehn et al., 2003a). Word
alignments are first computed from paired sentences, then phrase pairs are extracted such that no word in-
side the phrase pair is aligned to a word outside it; these extracted phrase pairs are stored in a phrase table
with a set of features quantifying their quality. Such features include the conditional translation prob-
ability typically computed as normalized phrase frequencies in the corpus. One the system is trained,
the translation process is carried out as a search for the best target sentence under a log-linear scoring
function that combines several features. The scaling parameters of this function are tuned discrimina-
7
To differentiate elements of V oc(O) and the plain English vocabulary, names from O are preceded by an escape character
sequence in V oc(O).
24
tively to optimize the translation performance on a small set of paired sentences. Given a sentence to be
translated, it has to be segmented into phrases which are then individually translated, and last reordered
to fit the typical order of the target language. Applied to semantic annotation, the translation relation is
monotonic (i.e. involves no reordering) and many elements are identical to their translation. The train-
ing data we use provides one-to-one correspondence between the words and their label which allows
us to compute exact word alignments between source and target sentences. The possibility to produce
good annotations when plain lexical information is ambiguous relies on the learning algorithm and the
projection of its results on the text, inasmuch it takes the context into account for disambiguation. Note
also that the model accounts for tokens which must not be annotated (they are learned to be identically
translated). SMT systems typically incorporate a language model (LM) which helps selecting the most
probable annotated sentence from the large set of possibilities, and the phrase table functions as a sophis-
ticated dictionary between the source and target languages. We use the KenLM language model Toolkit
(Heafield et al., 2013) to train a language model for our experiments. To construct the phrase table we
use the relatively simple but effective method defined in (Koehn et al., 2003b) but we use exact word
alignment which we compute separately. The decoding is done by a beam search as implemented by
Moses (Koehn et al., 2007). To localize the precise positions of semantic annotations predicted, we use
the translation alignment between the two texts provided at the word level in the output of Moses. For
example, giving ?15-14 16-14? in the alignment for a sentence means that the 15th and 16th words in the
original are replaced by the 14th word in the translated file. If the 14th word belongs to V oc(O), such as
TTGene, the concept Gene is the semantic label associated to the 15th and 16th words of the original
text.
3.2.1 Evaluation
We performed several experiments in order to discover which information helps obtaining the best accu-
racy. The input and output languages are called respectively L1 and L2, and varying these languages is
the mean to focus on different subsets of the annotations. Due to the presence of Continuant annotations
(c-annotations for short) in the input, the vocabulary of both L1 and L2 is extended beyond natural lan-
guage in most experiments ? this is more the case for L2 than it is for L1. ?Event trigger annotation? is
henceforth abbreviated as et-annotation. For evaluation, two measures are used, one less requiring than
the other: a positive annotation has either the same label and the same endpoints as a reference label
(exact match), or at least one of these criteria is satisfied (?AL1 match?), provided that the positions,
at least, intersect. The results are summarized in Table 3. In Table 3, ?expe1? is the main experiment,
working exactly in the conditions proposed by the reference task: L1 has c-annotations and L2 has both
c-and et-annotations. It can be compared to the aggregated results in table 1. Some variants have been
made to separate the role of different factors. In ?expe2?, L1 has no annotations at all and correspond
to the raw input text, and L2 has everything, i.e., c- and et- ones. The expe2-a line gives a global result
of evaluating the prediction of c- and et-annotations together: F-measures is 0.16 points below ?expe1?,
which is an important loss. However, computing the scores separately for the two kinds of annotation
in the L2 language refines the view : the c-annotations (expe2-c line) are much worse than the et-ones
(expe2-b line), which have only lost .03 points with respect to ?expe1?. From this, we conclude that
c-annotations in L1 (as used in ?expe1?) do not help much to learn et-annotations.
Analyzing the conditions of ?expe2?, it can be seen that including the c-annotations from the references
in L2 provide helpful information via the inverse probabilities used as a feature in the phrase table. So we
made two more experiments to check each type of annotation by itself. In ?expe3?, L1 is the unannotated
text and L2 has only c-annotations. A slight improvement is observed on the F-measure of AL1 relative
to ?expe2-c?, while the exact case gets the same score. In fact, Moses suggests 20% more annotations but
the ratio of true positive is worse. In ?expe4?, L1 is the text and L2 has only et-annotations. The results
are 0.02 points below ?expe1? and close to ?expe2-b?, which proves that knowing c-annotations does not
help us much to detect events triggers in this setting (note that c-annotations are used to detect events
arguments in the next section). It also clearly shows that c-annotations are much harder to learn and that
dictionaries or similar lexicon-based methods are more suitable.
The following experiments, namely ?exp5? and ?exp6? have no annotations in L1 compared to ?expe4?
25
#ref #mo #MP #PG #LG #PLG #AL1 FPL FAL1
expe1 314 301 250 215 209 188 236 0.61 0.77
expe2-a 1229 869 734 520 594 476 638 0.45 0.61
expe2-b 313 328 248 210 214 190 234 0.59 0.73
expe2-c 916 541 468 310 391 286 415 0.39 0.57
expe3 916 647 533 334 444 310 468 0.40 0.60
expe4 313 329 253 217 213 191 239 0.60 0.74
expe5 313 242 204 175 174 158 191 0.57 0.69
expe6 313 306 246 210 204 181 233 0.58 0.75
The headers
#ref nbr of annotations in the reference #PLG nbr of exact (pos- and lab-good) matches
#mo nbr of annotations in moses output #AL1 nbr of matches with at least one good attribute
#MP nbr of matches (meeting pairs) FPL Fmesure - exact case
#PG nbr of position-good matches FAL1 Fmesure - at least one case
#LG nbr of label-good matches
Table 3: The results of experiments on event detection as phrase-based SMT.
but only et-annotations in L2. In these experiments we use factored translation models (Koehn and
Hoang, 2007) as implemented in Moses. Factors allow for incorporating supplementary information, in
addition to the actual words, into the model. A simple analysis suggests that being an event term could
be correlated to the nature of the word (favored by being a verb) or to the kind of dependency it enters in.
We therefore added part-of-speech tags and grammatical dependency labels, computed from dependency
trees, to L1. In ?expe5?, the three L1 factors are compared altogether to L2 while in ?expe6? they are
compared independently (and successively) to ?expe6?. In the first case, the performance drops by .03 to
.06 points compared to ?expe4?. The second case has small effects on the two F-measures. Finally, using
factor models in our settings does not improve the recognition of event terms.
To summarize, using c-annotations in L1, c- and et-annotations in L2 provides the best result, slightly
better for et-annotations alone than if c-annotations are omitted. In these settings, et-annotation reaches
a precision of 62% and a recall of 59% in the exact case (78% and 75% in the approximate one). We find
60% of exact positives; nearly 40% of the obtained annotations are not exact. Among these annotations,
15% captured at least one characteristic.
The predicted annotations obtained by both NER4SA and SMT4SA are then supplied to the next step
in the pipeline. This second step in which relations and event arguments are computed is discussed in
the next section.
4 Step Two: Relations and Event Arguments Annotation
In the second step of the pipeline, we take the output of the first step, namely the detected events, and we
predict their arguments. We also predict other relations in the text.
The essential difference between the extraction of relations and that of event arguments is that relations
link exactly two locations in the text while events link a variable number of locations and are supported
by triggers. Nevertheless, we use a unified representation for both events and relations. A relation is
a labeled link between two elements in the text. Examples of relation labels include ?locatedIn? and
?fromSpecies?. An event is a set of labeled relations between the event trigger (detected in step 1 of the
pipeline) to an event argument which is another element of the text. Event-to-argument relations are
labeled either ?hasAgent? or ?hasPatient?. Therefore, the problem of relation extractions boils down to a
multi-class classification problem of candidate links. A candidate link involves two c- or et-annotations
and is labeled by the biological relation name in the first case, or by an event argument role when its
source is an event trigger. Note that the same event trigger may have several agent or patient roles.
26
4.1 A multi-class classification approach
For each candidate link between two elements of the text, we predict a label among ?none? (which
indicate no link), ?hasAgent?, ?hasPatient?, ?locatedIn?, etc. Although we use the same representation for
both event arguments annotation and relation annotation, we use two distinct multi-class classifier. The
first classifier locate the arguments of each detected event and identify their roles. Event arguments can
be Continuant concepts or other events. The second classifier extracts and label relations between any
two concepts which can be Continuant or events. We perform these two tasks independently and combine
their predictions afterward. For event arguments annotation: for each detected event, we assign one of
the labels ?hasAgent?, ?hasPatient?, ?no-relation? to all other entities. Similarly for relation annotation:
for each pair of c- or et-annotations we predict a label which is either the label of the binary relation or the
special label ?no-relation?. We use an implementation of a maximum-entropy classifier called Wapiti
8
(Lavergne et al., 2010). The set of features we used contains lexical and morpho-syntactic features
extracted from the pair of entities in question. This include their lexical identities as they appear in the
document as well as the ontology labels assigned to them. We also include the part-of-speech tags of
involved words. Additionally, we include positional features such as the distance between the words
in the document, computed as the number of words separating them, as well as their relative positions
indicating which word precedes the other in the text. Furthermore, we use compound features resulting
from combining pairs of the individual features.
4.2 Evaluation
The reference result has much more ?No? than ?Yes?, and labeling randomly while respecting the propor-
tion would give a good score for the No. So in the evaluation the numbers of true positives, false positives
and false negatives only account for ?Yes? answers. The criterion is an exact match (label and position)
at each end of the link. Table 4 gives the results for the relations appearing in our test set. The number
of occurrences of each relation in the reference is pointed out. Except for the sparse ?hasFunction?, the
precision is at least 57% and higher for relations which have the greatest number of occurrences. For
recall, however, only ?fromSpecies? relation has an important recall.The mean precision is 80% and the
mean recall is 37%, which yields a F-measure of 50%.
Relation # of occurrences Precision Recall F-measure
locatedIn 182 0.73 0.26 0.38
encodes 46 0.57 0.21 0.31
hasPart 178 0.77 0.26 0.39
fromSpecies 172 0.90 0.69 0.78
hasFunction 24 0.20 0.08 0.12
Table 4: Detection of relations
The annotation of events presents seemingly more difficulties than relations: the precision is at best
60% for a much higher number of occurrences. The recall has the same order of magnitude for the agent
role, and is better for the patient role which has twice more occurrences. The mean precision is 58%,
and the mean recall is 36%. In the pipeline evaluation presented in the next section, errors due to event
recognition will accumulate with errors proper to relation annotation.
Class # of occurrences Precision Recall F-measure
hasPatient 562 0.61 0.43 0.50
hasAgent 258 0.46 0.20 0.28
Table 5: Detecting arguments of events
8
http://wapiti.limsi.fr
27
5 Pipeline Evaluation
The pipeline evaluation compares the relations and events obtained at the end of the pipeline to the
reference. We have implemented the algorithm defined in the task description, and applied it to one
unused half of the development data. In this evaluation, the data consist in 175 documents for training
(of which 25 are reserved for Moses for tuning) and 25 for testing.
Events Relations Both
Event detection Pr Rc F1 Pr Rc F1 Pr Rc F1
NER4SA .20 .10 .13 .80 .30 .44 .44 .19 .26
SMT4SA .14 .13 .13 .80 .30 .44 .32 .21 .25
SMT4SA ? NER .16 .22 .19 .80 .30 .44 .29 .26 .27
Table 6: Pipeline precision, recall and F-measure using strict matching for the NER4SA and SMT4SA
approaches for event detection, and for their combination.
Relation detection has roughly the same figures as in table 4. The combination of event detection
and arguments annotation obtains the same F-measure for both detection methods proposed, so the 5%
point advantage of the second when tested out of the pipeline disappears here. Interestingly, using a
combination (union) of the outputs of the NER and SMT approaches results in improvements in recall
(and f1) over each approach in isolation.
6 Related work
Some effort has been dedicated to the recognition of ontology concepts in biomedical literature. This in-
cludes TextPresso (Muller et al., 2004) and GoPubMed (Doms and Schroeder, 2005). These approaches
are based on term extraction methods to find the ontology concepts occurrences, together with some
terminological variations. Systems like (Rebholz-Schuhmann et al., 2007) and FACTA (Tsuruoka et al.,
2008) collect and display co-occurrences of ontology terms. However, they do not extract events and
relations of the semantic types defined in ontologies. For event and relation extraction, (Klinger et al.,
2011) use imperatively defined factor graphs to build Markov Networks that model inter-dependencies
between mentions of events within sentences, and across sentence-boundaries. OSEE (jae Kim and
Rebholz-Schuhmann, 2011) is a pattern matching system that learns language patterns for event extrac-
tion. Most similar to our work, is the TEES 2.1 system (Bj?orne and Salakoski, 2013) which is based on
multi-step SVM classifiers that learns event annotation by first locating triggers then identifying event
arguments and finally selecting candidate events.
7 Conclusion
In this work, we have proposed a pipeline for annotating documents with domain sepcific ontologies
and tested it on the BioNLP?13 GRO task. The two-step pipeline gives a flexible modeling choice, and
is realized by different inner components. For the first step, the sequence labeling and phrase-based
statistical machine translation approaches are applied. And we conducted detailed experiments to test
different settings, from which we can conclude the following findings: (1) For the event recognition task,
NER4SA, much computationally expensive due to its model complexity, did not result in higher scores
than SMT4SA in terms of F-measure. It did give better precision, however at the expense of the recall.
This shows that SMT4SA is a good practical modeling method for the task. (2) For SMT4SA, the extra
features added by factored learning did not boost the system much, which means that a basic setting
can capture the essential quality of the system. (3) For the relation detection based on the output of the
pipeline, we obtained reasonable scores for events and relations. Interestingly, NER4SA, SMT4SA, or
their combination did affect the detection of events, but not relations which is step-one independent. And
the combination has had a better performance.
28
Acknowledgements
We are thankful to the reviewers for their comments. This work is part of the program Investissements
d?Avenir, overseen by the French National Research Agency, ANR-10-LABX-0083, (Labex EFL). We
acknowledge financial support by the DFG Research Unit FOR 1513, project B1.
References
[Beisswanger et al.2008] Elena Beisswanger, Vivian Lee, Jung jae Kim, Dietrich Rebholz-Schuhmann, Andrea
Splendiani, Olivier Dameron, Stefan Schulz, and Udo Hahn. 2008. Gene regulation ontology (gro): Design
principles and use cases. In MIE, volume 136 of Studies in Health Technology and Informatics, pages 9?14.
IOS Press.
[Bj?orne and Salakoski2013] Jari Bj?orne and Tapio Salakoski. 2013. Tees 2.1: Automated annotation scheme
learning in the bionlp 2013 shared task. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages
16?25, Sofia, Bulgaria, August. Association for Computational Linguistics.
[Chiang2007] David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33:201?228.
[Ciravegna et al.2004] Fabio Ciravegna, Sam Chapman, Alexiei Dingli, and Yorick Wilks. 2004. Learning to
harvest information for the semantic web. In Proceedings of ESWS?04.
[Cucerzan2007] Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In
Proceedings of EMNLP-CoNLL?07, pages 708?716.
[Dill et al.2003] Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl, R. Guha, Anant Jhingran, Tapas Ka-
nungo, Sridhar Rajagopalan, Andrew Tomkins, John A. Tomlin, and Jason Y. Zien. 2003. Semtag and seeker:
bootstrapping the semantic web via automated semantic annotation. In Proceedings of WWW ?03, pages 178?
186.
[Doms and Schroeder2005] Andreas Doms and Michael Schroeder. 2005. Gopubmed: exploring pubmed with the
gene ontology. Nucleic Acids Research, 33(Web-Server-Issue):783?786.
[Dowty1991] David Dowty. 1991. Thematic proto-roles and argument selection. Language, 67:547?619.
[Finkel et al.2005] Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local
information into information extraction systems by gibbs sampling. In Proceedings of ACL?05, pages 363?370.
[Guha and McCool2003] R Guha and R McCool. 2003. Tap: A semantic web test-bed. Web Semantics Science
Services and Agents on the World Wide Web, 1(1):81?87.
[Heafield et al.2013] Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable
modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 690?696, Sofia, Bulgaria, August.
[jae Kim and Rebholz-Schuhmann2011] Jung jae Kim and Dietrich Rebholz-Schuhmann. 2011. Improving the
extraction of complex regulatory events from scientific text by using ontology-based inference. J. Biomedical
Semantics, 2(S-5):S3.
[Kim et al.2013] Jung-Jae Kim, Xu Han, Vivian Lee, and Dietrich Rebholz-Schuhmann. 2013. Gro task: Popu-
lating the gene regulation ontology with events and relations. In Proceedings of the BioNLP Shared Task 2013
Workshop, pages 50?57, Sofia, Bulgaria, August. Association for Computational Linguistics.
[Kiryakov et al.2004] Atanas Kiryakov, Borislav Popov, Ivan Terziev, Dimitar Manov, and Damyan Ognyanoff.
2004. Semantic annotation, indexing, and retrieval. Journal of Web Semantics, 2:49?79.
[Klinger et al.2011] Roman Klinger, Sebastian Riedel, and Andrew McCallum. 2011. Inter-event dependencies
support event extraction from biomedical literature. Mining Complex Entities from Network and Biomedical
Data (MIND), European Conference on Machine Learning and Principles and Practice of Knowledge Discovery
in Databases (ECML PKDD).
[Koehn and Hoang2007] Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In EMNLP-CoNLL,
pages 868?876. ACL.
29
[Koehn et al.2003a] Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003a. Statistical phrase-based transla-
tion. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology - Volume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Koehn et al.2003b] Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003b. Statistical phrase-based transla-
tion. In HLT-NAACL, pages 127?133.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceed-
ings of ACL?07, pages 177?180.
[Lavergne et al.2010] Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. 2010. Practical very large scale
CRFs. In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages
504?513. Association for Computational Linguistics, July.
[Liu et al.2011] Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011. Recognizing named entities in
tweets. In Proceedings of HLT ?11, pages 359?367.
[LiveMemories2010] LiveMemories. 2010. Livememories: Second year scientific report. Technical report, Live-
Memories, December.
[Marcu and Wong2002] Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for
statistical machine translation. In Proceedings of EMNLP?02, pages 133?139.
[Mendes et al.2011] Pablo N. Mendes, Max Jakob, Andr?es Garc??a-Silva, and Christian Bizer. 2011. DBpedia
Spotlight: Shedding light on the web of documents. In Proceedings of I-Semantics?11.
[Mihalcea and Csomai2007] Rada Mihalcea and Andras Csomai. 2007. Wikify!: linking documents to encyclope-
dic knowledge. In Proceedings of CIKM?07, pages 233?242.
[Muller et al.2004] H. Muller, E. Kenny, and P. Sternberg. 2004. Textpresso: An ontology-based information
retrieval and extraction system for biological literature. PLoS Biology, 2(11):1984?1998.
[Nazarenko et al.2011] Adeline Nazarenko, Abdoulaye Guiss?e, Franois L?evy, Nouha Omrane, and Sylvie Szulman.
2011. Integrating written policies in business rule management systems. In Proceedings of RuleML?11.
[Och and Ney2003] Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical
alignment models. Computational Linguistics, pages 19?51.
[Rebholz-Schuhmann et al.2007] Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel Arregui, Sylvain Gaudan,
Mark Riethoven, and Peter Stoehr. 2007. Ebimed - text crunching to gather facts for proteins from medline.
Bioinformatics, 23(2):237?244.
[Stolcke2002] Andreas Stolcke. 2002. Srilm ? an extensible language modeling toolkit. In In Proceedings of
ICSLP?02, pages 901?904.
[Tsuruoka et al.2008] Y Tsuruoka, J Tsujii, and S Ananiadou. 2008. Facta: a text search engine for finding
associated biomedical concepts. Bioinformatics, 24(21):2559?2560, November.
[Uren et al.2006] Victoria S. Uren, Philipp Cimiano, Jos?e Iria, Siegfried Handschuh, Maria Vargas-Vera, Enrico
Motta, and Fabio Ciravegna. 2006. Semantic annotation for knowledge management: Requirements and a
survey of the state of the art. J. Web Sem., 4(1):14?28.
[Wang2009] Yefeng Wang. 2009. Annotating and recognising named entities in clinical notes. In ACL/AFNLP
(Student Workshop), pages 18?26.
[Welty and Ide1999] Christopher Welty and Nancy Ide. 1999. Using the right tools: Enhancing retrieval from
marked-up documents. In Journal Computers and the Humanities, pages 33?10.
30
