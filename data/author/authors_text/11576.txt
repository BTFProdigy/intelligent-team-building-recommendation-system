Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1086?1095,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Empirical Exploitation of Click Data for Task Specific Ranking
Anlei Dong Yi Chang Shihao Ji Ciya Liao Xin Li Zhaohui Zheng
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
{anlei,yichang,shihao,ciyaliao,xinli,zhaohui}@yahoo-inc.com
Abstract
There have been increasing needs for task
specific rankings in web search such as
rankings for specific query segments like
long queries, time-sensitive queries, navi-
gational queries, etc; or rankings for spe-
cific domains/contents like answers, blogs,
news, etc. In the spirit of ?divide-and-
conquer?, task specific ranking may have
potential advantages over generic ranking
since different tasks have task-specific fea-
tures, data distributions, as well as feature-
grade correlations. A critical problem for
the task-specific ranking is training data
insufficiency, which may be solved by us-
ing the data extracted from click log. This
paper empirically studies how to appro-
priately exploit click data to improve rank
function learning in task-specific ranking.
The main contributions are 1) the explo-
ration on the utilities of two promising ap-
proaches for click pair extraction; 2) the
analysis of the role played by the noise
information which inevitably appears in
click data extraction; 3) the appropriate
strategy for combining training data and
click data; 4) the comparison of click data
which are consistent and inconsistent with
baseline function.
1 Introduction
Learning-to-rank approaches (Liu, 2008) have
been widely applied in commercial search en-
gines, in which ranking models are learned using
labeled documents. Significant efforts have been
made in attempt to learn a generic ranking model
which can appropriately rank documents for all
queries . However, web users? query intentions are
extremely heterogeneous, which makes it difficult
for a generic ranking model to achieve best rank-
ing results for all queries. For this reason, there
have been increasing needs for task specific rank-
ings in web search such as rankings for specific
query segments like long queries, time-sensitive
queries, navigational queries, etc; or rankings
for specific domains/contents like answers, blogs,
news, etc. Therefore, a specific ranking task usu-
ally correspond to a category of queries; when
the search engine determines that a query is be-
longing to this category, it will call the ranking
function dedicated to this ranking task. The mo-
tivation of this divide-and-conquer strategy is that,
task specific ranking may have potential advan-
tages over generic ranking since different tasks
have task-specific features, data distributions, as
well as feature-grade correlations.
Such a dedicated ranking model can be trained
using the labeled data belonging to this query cat-
egory (which is called dedicated training data).
However, the amount of training data dedicated
to a specific ranking task is usually insufficient
because human labeling is expensive and time-
consuming, not to mention there are multiple rank-
ing tasks that need to be taken care of. To deal
with the training data insufficiency problem for
task-specific ranking, we propose to extract click-
through data and incorporate it with dedicated
training data to learn a dedicated model.
In order to incorporate click data to improve the
ranking for a dedicate query category, it is critical
to fully exploit click information. We empirically
explore the related approaches for the appropriate
click data exploitation in task-specific rank func-
tion learning. Figure 1 illustrates the procedures
and critical components to be studied.
1) Click data mining: the purpose is to extract
informative and reliable users? preference infor-
mation from click log. We employ two promis-
ing approaches: one is heuristic rule approach, the
other is sequential supervised learning approach.
2) Sample selection and combination: with la-
beled training data and unlabeled click data, how
1086
Generic training data
Dedicated training data
GBrank algorithm
Task-specific ranking model
Generic click data
Dedicated click data
Sample selection and combination
Click log Click data mining? Heuristic-rule-based approach
? Sequential supervised learning approach
Figure 1: Framework of incorporating click-
through data with training data to improve dedi-
cated model for task-specific ranking.
to select and combine them so that the samples
have the best utility for learning? As the data
distribution for a specific ranking task is differ-
ent from the generic data distribution, it is nat-
ural to select those labeled training samples and
unlabeled click preference pairs which belong to
this query category, so that the data distributions
of training set and testing set are consistent for
this category. On the other hand, we should keep
in mind that: a) non-dedicated data, i.e, the data
that does not belong the specific category, might
also have similar distribution as the dedicated data.
Such distribution similarity makes non-dedicated
data also useful for task-specific rank function
learning, especially for the scenario that dedicated
training samples is insufficient. b) The quality of
dedicated click data may be not as reliable as hu-
man labeled training data. In other words, there
are some extracted click preference pairs that are
inconsistent with human labeling while we regard
human labeling as correct labeling.
3) Rank function learning algorithm: we use
GBrank (Zheng et al, 2007) algorithm for rank
function learning, which has proved to be one
of the most effective up-to-date learning-to-rank
algorithms; furthermore, GBrank algorithm also
takes preference pairs as inputs, which will be il-
lustrated with more details in the paper.
2 Related work
Learning to rank has been a promising research
area which continuously improves web search rel-
evance (Burges et al, 2005) (Zha et al, 2006)
(Cao et al, 2007) (Freund et al, 1998) (Fried-
man, 2001) (Joachims, 2002) (Wang and Zhai,
2007) (Zheng et al, 2007). The ranking prob-
lem is usually formulated as learning a ranking
function from preference data. The basic idea
is to minimize the number of contradicted pairs
in the training data, and different algorithm cast
the preference learning problem from different
point of view, for example, RankSVM (Joachims,
2002) uses support vector machines; RankBoost
(Freund et al, 1998) applies the idea of boost-
ing from weak learners; GBrank (Zheng et al,
2007) uses gradient boosting with decision tree;
RankNet (Burges et al, 2005) uses gradient boost-
ing with neural net-work. In (Zha et al, 2006),
query difference is taken into consideration for
learning effective retrieval function, which leads
to a multi-task learning problem using risk mini-
mization framework.
There are a few related works to apply multi-
ple ranking models for different query categories.
However, none of them takes click-through infor-
mation into consideration. In (Kang and Kim,
2003), queries are categorized into 3 types, infor-
mational, navigational and transactional, and dif-
ferent models are applied on each query category.
a KNN method is proposed to employ different
ranking models to handle different types of queries
(Geng et al, 2008). The KNN method is unsuper-
vised, and it targets to improve the overall ranking
instead of the rank-ing for a certain query cate-
gory. In addition, the KNN method requires all
feature vector to be the same.
Quite a few research papers explore how to ob-
tain useful information from click-through data,
which could benefit search relevance (Carterette
et al, 2008) (Fox et al, 2005) (Radlinski and
Joachims, 2007) (Wang and Zhai, 2007). The in-
formation can be expressed as pair-wise prefer-
ences (Chapelle and Zhang, 2009) (Ji et al, 2009)
(Radlinski et al, 2008), or represented as rank fea-
tures (Agichtein et al, 2006). Task-specific rank-
ing relies on the accuracy of query classification.
Query classification or query intention identifica-
tion has been extensively studied in (Beitzel et al,
2007) (Lee et al, 2005) (Li et al, 2008) (Rose and
Levinson, 2004). How to combine editorial data
and click data is well discussed in (Chen et al,
2008) (Zheng et al, 2007). In addition, how to use
click data to improve ranking are also exploited
in personalized or preference-based search (Coyle
1087
Table 1: Statistics of click occurrences for heuris-
tic rule approach.
imp impression, number of occurrence of the tuple
cc number of occurrence of the tuple where two
documents both get clicked
ncc number of occurrence of the tuple where url
1
is not clicked but url
2
is clicked
cnc number of occurrence of the tuple where url
1
is clicked but url
2
is not clicked
ncnc number of occurrence of the tuple where url
1
and url
2
are not clicked
and Smyth, 2007) (Glance, 2001) (R. Jin, 2008).
3 Technical approach
This section presents the related approaches in
Figure 1. In Section 4, we will make deeper anal-
ysis based on experimental results.
3.1 Click data mining
We use two approaches for click data mining,
whose outputs are preference pairs. A preference
pair is defined as a tuple {< x
q
, y
q
> |x
q
? y
q
},
which means for the query q, the document x
q
is
more relevant than y
q
. We need to extract infor-
mative and reliable preference pairs which can be
used to improve rank function learning.
3.1.1 Heuristic rule approach
We use heuristic rules to extract skip-above pairs
and skip-next pairs, which are similar to Strategy
1 (click > skip above) and Strategy 5 (click > no-
click next) proposed in (Joachims et al, 2005). To
reduce the misleading effect of an individual click
behavior, click information from different query
sessions is aggregated before applying heuristic
rules. For a tuple (q, url
1
, url
2
, pos
1
, pos
2
) where
q is query, url
1
and url
2
are urls representing two
documents, pos
1
and pos
2
are ranking positions
for the two documents with pos
1
? pos
2
mean-
ing url
1
has higher rank than url
2
, the statistics for
this tuple are listed in Table 1.
Skip-above pair extraction: if ncc is much
larger than cnc, and
cc
imp
,
ncnc
imp
is much smaller
than 1, that means, when url
1
is ranked higher than
url
2
in query q, most users click url
2
but not click
url
1
. In this case, we extract a skip-above pair, i.e.,
url
2
is more relevant than url
1
. In order to have
highly accurate skip-above pairs, a set of thresh-
Table 2: Skip-above pairs count vs. human judge-
ments (e.g., the element in the third row and sec-
ond column means we have 40 skip-above pairs
with ?excellent? url
1
and ?perfect? url
2
). P: per-
fect; E: excellent; G: good; F: fair; B: bad.
P E G F B
P 13 13 12 4 0
E 40 44 16 2 2
G 27 53 103 29 8
F 10 15 43 27 5
B 4 4 11 20 14
Table 3: Skip-next pairs vs. human judgements
(e.g., the element in the third row and second col-
umn means we have 10 skip-next pairs with ?ex-
cellent? url
1
and ?perfect? url
2
). P: perfect; E:
excellent; G: good; F: fair; B: bad.
P E G F B
P 126 343 225 100 35
E 10 71 84 37 12
G 6 9 116 56 21
F 1 5 17 29 14
B 1 1 1 2 5
olds are applied to only extract the pairs that have
high impression and ncc is larger enough than cnc.
Skip-next pair extraction: if pos
1
= pos
2
? 1,
cnc is much larger than ncc, and
cc
imp
,
ncnc
imp
is much
smaller than 1, that means, in most of cases when
url
2
is ranked just below url
1
in query q, most
users click url
1
but not click url
2
. In this case, we
regard this tuple as a skip-next pair.
To test the accuracy of preference pairs, we
ask editors to judge some randomly selected pairs
from skip-above pairs and skip-next pairs. Edi-
tors label each query-url pair using five grades ac-
cording to relevance: perfect, excellent, good, fair,
bad. Table 2 shows skip-above pair distribution.
The diagonal elements have high values, which
are for tied pairs labeled by editors but determined
as skip-above pairs from heuristic rules. Higher
values appear in the left-bottom triangle than in
the right-top triangle, because there are more skip-
above preferences agreed with editors than dis-
agreed with editors. Summing up the tied pairs,
agreed and disagreed pairs, 44% skip-above pref-
erence judgments agree with editors, 18% skip-
above preference judgments disagree with editors,
1088
and there are 38% skip-above pairs judged as tie
pairs by editors.
Table 3 shows skip-next pair distribution. Sum-
ming up the tied pairs, agreed and disagreed pairs,
70% skip-next preference judgments agree with
editors, 4% skip-next preference judgments dis-
agree with editors, and 26% skip-next pairs judged
as tie pairs by editors.
Therefore, skip-next pairs have much higher
accuracy than skip-above. That is because in a
search engine that already has a good ranking
function, it is much easier to find a correct skip-
next pairs which are consistent with the search en-
gine than to find a correct skip-above pairs which
are contradictory to the search engine. Skip-above
and skip-next preferences provide us two kinds of
users?s feedbacks which are complementary: skip-
above preferences provide us the feedback that the
user?s vote is contradictory to the current ranking,
which implies the current relative ranking should
be reversed; skip-next preferences shows that the
user?s vote is consistent with the current ranking,
which implies the current relative ranking should
be maintained with high confidence provided by
users? vote.
3.1.2 Sequential supervised learning
The click modeling by sequential supervised
learning (SSL) was proposed in (Ji et al, 2009),
in which user?s sequential click information is
exploited to extract relevance information from
click-logs. This approach is reliable because 1)
the sequential click information embedded in an
aggregation of user clicks provides substantial rel-
evance information of the documents displayed in
the search results, and 2) the SSL is supervised
learning (i.e., human judgments are provided with
relevance labels for the training).
The SSL is formulated in the framework
of global ranking (Qin et al, 2008). Let
x
(q)
= {x
(q)
1
, x
(q)
2
, . . . , x
(q)
n
} represent the doc-
uments retrieved with a query q, and y
(q)
=
{y
(q)
1
, y
(q)
2
, . . . , y
(q)
n
} represent the relevance la-
bels assigned to the documents. Here n is the
number of documents retrieved with q. Without
loss of generality, we assume that n is fixed and
invariant with respect to different queries. The
SSL determines to find a function F in the form
of y
(q)
=F (x
(q)
) that takes all the documents as
its inputs, exploiting both local and global infor-
mation among the documents, and predict the rel-
evance labels of all the document jointly. This
is distinct to most of learning to rank methods
that optimize a ranking model defined on a sin-
gle document, i.e., in the form of y
(q)
i
=f(x
(q)
i
),
? i = 1, 2, . . . , n. This formulation of the SSL
is important in extracting relevance information
from user click data since users? click decisions
among different documents displayed in a search
session tend to rely not only on the relevance judg-
ment of a single document, but also on the relative
relevance comparison among the documents dis-
played; and the global ranking framework is well-
formulated to exploit both local and global infor-
mation from an aggregation of user clicks.
The SSL aggregates all the user sessions for
the same query into a tuple <query, n-document
list, and an aggregation of user clicks>. Fig-
ure 2 illustrates the process of feature extrac-
tion from an aggregated session, where x
(q)
=
{x
(q)
1
, x
(q)
2
, . . . , x
(q)
n
} denotes a sequence of fea-
ture vectors extracted from the aggregated ses-
sion, with x
(q)
i
representing the feature vector ex-
tracted for document i. Specifically, to form fea-
ture vector x
(q)
i
, first a feature vector x
(q)
i,j
is ex-
tracted from each user j?s click information, and
j ? {1, 2, . . . }, then x
(q)
i
is formed by averaging
over x
(q)
i,j
, ?j ? {1, 2, . . . }, i.e., x
(q)
i
is actually an
aggregated feature vector for document i. Table
4 lists all the features used in the SSL modeling.
Note that some features are statistics independent
of temporal information of the clicks, such as ?Po-
sition? and ?Frequency?, while other features re-
ply on their surrounding documents and the click
sequences. We use 90,000 query-url pairs to train
the SSL model, and 10,000 query-url pairs for best
model selection.
With the sequential click modeling discussed
above, several sequential supervised algorithms,
including the conditional random fields (CRF)
(Lafferty et al, 2001), the sliding window method
and the recurrent sliding window method (Diet-
terich, 2002), are explored to find a global ranking
function F . We omit the details but refer one to
(Ji et al, 2009). The emphasis here is on the im-
portance to adapt these algorithms to the ranking
problem.
After training, the SSL model can be used to
predict the relevance labels of all the documents in
a new aggregated session, and thus pair-wise pref-
erence data can be extracted, with the score dif-
ference representing the confidence of preference
1089
={
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 }
?
o
o
user2
doc10
odoci
oodoc2
odoc1 user1
q
?
?
Feature Extraction
x
(q)
x1
x2
xi
x10
?
?
={
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 }
y
(q)
y1
y2
yi
y10
?
?
Figure 2: An illustration of feature extraction for
an aggregated session for SSL approach. x
(q)
de-
notes an extracted sequence of feature vectors, and
y
(q)
denotes the corresponding label sequence that
is assigned by human judges for training.
Table 4: Click features used in SSL model.
Position Position of the document
in the result list
ClickRank Rank of 1st click of doc. in click seq.
Frequency Average number of clicks for this doc.
FrequencyRank Rank in the list sorted by num. of clicks
IsNextClicked 1 if next position is clicked, 0 otherwise
IsPreClicked 1 if previous position is clicked,
0 otherwise
IsAboveClicked 1 if there is a click above, 0 otherwise
IsBelowClicked 1 if there is a click below, 0 otherwise
ClickDuration Time spent on the document
prediction. For the reason of convenience, we also
call the preference pairs contradicting with pro-
duction ranking as skip-above pairs and those con-
sistent with production ranking as skip-next pairs,
so that we can analyze these two types of prefer-
ence pairs respectively.
3.2 Modeling algorithm
The basic idea of GBrank (Zheng et al, 2007)
is that if the ordering of a preference pair
by the ranking function is contradictory to this
preference, we need to modify the ranking
function along the direction by swapping this
prefence pair. Preferences pairs could be gen-
erated from labeled data, or could be extracted
from click data. For each preference pair <
x, y > in the available preference set S =
{< x
i
, y
i
> |x
i
? y
i
, i = 1, 2, ..., N}, x should
be ranked higher than y. In GBrank algorithm, the
problem of learning ranking functions is to com-
pute a ranking function h , so that h matches the
set of preference, i.e, h(x
i
) ? h(y
i
) , if x ? y,
i = 1, 2, ..., N as many as possible. The following
loss function is used to measure the risk of a given
ranking function h.
R(h) =
1
2
N
?
i=1
(max{0, h(y
i
)?h(x
i
)+?})
2
, (1)
where ? is the margin between the two documents
in the pair. To minimize the loss function, h(x) has
to be larger than h(y) with the margin ? , which can
be chosen as constant value, or as dynamic val-
ues varying with pairs. When pair-wise judgments
are extracted from editors? labels with different
grades, pair-wise judgments can include grade dif-
ference, which can further be used as margin ? .
The GBrank algorithm is illustrated in Algorithm
1, and two parameters need to be determined: the
shrinkage factor ? and the number of iteration.
Algorithm 1 GBrank algorithm.
Start with an initial guess h
0
, for m = 1, 2, ...
1. Construct a training set: for each < x
i
, y
i
>?
S, derive (x
i
,max{0, h
m?1
(y
i
) ? h
m
1
(x
i
) +
?}), and
(y
i
,?max{0, h
m?1
(y
i
)? h
m
1
(x
i
) + ?}).
2. Fit h
m
by using a base regressor with the
above training set.
3. h
m
= h
m?1
+?s
m
h
m
(x), where s
m
is found
by line search to minimize the object function,
? is shrinkage factor.
3.3 Sample selection and combination
We use a straightforward approach to learn rank-
ing model from the combined data, which is illus-
trated in Algorithm 2.
Algorithm 2 Learn ranking model by combining
editorial data and click preference pairs.
Input:
? Editorial absolute judgement data.
? Preference pairs from click data.
1. Extract preference pairs from labeled data
with absolute judgement.
2. Select and combine preference pairs from
click data and labeled data.
3. Learn GBrank model from the combined
preference pairs.
Absolute judgement on labeled data contains
(query, url) pairs with absolute grade values la-
beled by human. In Step 1, for each query with
1090
nq
query-url pairs with corresponding grades, {<
query, url
i
, grade
i
> |i = 1, 2, . . . , n
q
}, its prefer-
ence pairs are extracted as
{< query, url
i
, url
j
, grade
i
? grade
j
> |i, j =
1, 2, . . . , n
q
, i 6= j} .
When combining human-labeled pairs and click
preference pairs, we can give use different relative
weights for these two data sources. The loss func-
tion becomes
R(h) =
w
N
l
?
i?Labeled
(max{0, h(y
i
)? h(x
i
) + ?})
2
1? w
N
c
?
i?Click
(max{0, h(y
i
)? h(x
i
) + ?})
2
,(2)
where w is used to control the relative weights be-
tween labeled training data and click data, N
l
is
the number of training data pairs, and N
c
is the
number of click pairs. The margin ? can be deter-
mined as grade difference for editor pairs, and be
a constant parameter for click pairs.
Step 2 is critical for the efficacy of the approach.
A few factors need to be considered:
1) data distribution: for the application of task-
specific ranking, our purpose is to improve ranking
for the queries belonging to this category. An im-
portant observation is that the relevance patterns
for the ranking within a specific category may
have some unique characteristics, which are differ-
ent from generic relevance ranking. Thus, it is rea-
sonable to consider only using dedicated labeled
training data and dedicated click preference data
for training. The reality is that dedicated training
data is usually insufficient, while it is possible that
non-dedicated data can also help the learning.
2) click pair quality: it is inevitable there exist
some incorrect pairs in the click preference pairs.
Such incorrect pairs may mislead the learning. So
overall, can the click preference pairs still help the
learning for task-specific ranking? By our study,
skip-above pairs usually contain more incorrect
pairs compared with skip-above pairs. Does this
mean skip-next pairs are always more helpful in
improving learning than skip-above pairs?
3) click pair utility: use labeled training data as
baseline, how much complimentary information
can click pairs bring? This is determined by the
methodology of click data mining approach.
While it is possible to achieve some learning
improvement for task-specific ranking by using
click pairs by a plausible method, we attempt to
empirically explore the above interweaving fac-
tors for deeper understanding, in order to apply the
most appropriate strategy to exploit click data on
real-world applications of task-specific ranking.
4 Experiments
4.1 Data set
Query category: in the experiments, we use long
query ranking as an example of task-specific rank-
ing, because it is commonly known that long query
ranking has some unique relevance patterns com-
pared with generic ranking. We define the long
queries as the queries containing at least three to-
kens. The techniques and analysis proposed in this
paper can be applied to other ranking tasks, such
as rankings for specific query segments like time-
sensitive queries, navigational queries, or rankings
for specific domains/contents like answers, blogs,
news, as long as the tasks have their own charac-
teristics of data distributions and discriminant rank
features.
Labeled training data: we do experiments
based on a data set for a commercial search en-
gine, for which there are 16,797 query-url pairs
(with 1,123 different queries) that have been la-
beled by editors. The proportion of long queries
is about 35% of all queries. The data distribution
of such long queries may be different from gen-
eral data distribution, as it will be validated in the
experiments below.
The human labeled data is randomly split into
two sets: training set (8,831 query-url pairs, 589
queries), and testing set (7,966 query-url pairs,
534 queries). The training set will be combined
with click preference pairs for rank function learn-
ing, and the testing set will be used to evaluate the
efficacy of the ranking function. In the training set,
there are 3,842 long query-url pairs (229 queries).
At testing stage, the learned rank functions are ap-
plied only to the long queries in the testing data,
as our concern in this paper is how to improve
task-specific ranking, i.e., long query ranking in
the experiment. In the testing data, there are 3,210
query-url pairs (193 queries) are long query data,
which will be used to test rank functions.
Click preference pairs: using the two ap-
proaches of heuristic rule approach and sequen-
tial supervised approach, we extract click prefence
pairs from the click log of the search engine. Each
approach yields both skip-next and skip-above
pairs, which are sorted by confidence descending
order respectively.
1091
Table 5: Use click data by heuristic rule approach
(Data Selection: ?N?: not use; ?D?: use dedicated
data; ?G?: use generic data. Data Source: ?T?:
training data; ?C?: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7822 0.7906 (1.2%) 0.7997(2.4%)
GC 0.7834 0.7908 (1.2%) 0.7950 (1.7%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6649 0.7676 (-1.6%) 0.7748 (-0.8%)
GC 0.6792 0.7656 (-2.0%) 0.7989 (2.2%)
4.2 Setup and measurements
We try different sample selection and combination
strategies to train rank functions using GBrank al-
gorithm. For the labeled training data, we either
use generic data or dedicated data. For the click
preference pairs, we also try these two options.
Furthermore, as more click preference pairs may
bring more useful information to help the learn-
ing while on the other hand, the more incorrect
pairs may be given so that they mislead the learn-
ing, we try different amounts of these prefence
pairs: 5,000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs.
We use NDCG to evaluate ranking model,
which is defined as
NDCG
n
= Z
n
?
n
i=1
2
r(i)
?1
log(i+1)
where i is the position in the document list, r(i) is
the score of Document i, and Z
n
is a normalization
factor, which is used to make the NDCG of ideal
list be 1.
4.3 Results
Table 5 and 6 show the NDCG
5
results by using
heuristic rule approach and SSL approach respec-
tively. We do not present NDCG
1
results due to
space limitation, but NDCG
1
results have the sim-
ilar trends as NDCG
5
.
Baseline by training data: there are two base-
line functions by using training data sets 1) use
dedicated training data (DT), NDCG
5
on the test-
ing set by the rank function is 0.7736; 2) use
generic training data (GT), NDCG
5
is 0.7813. It
is reasonable that using generic training data is
Table 6: Use click data by SSL approach (Data
Selection: ?N?: not use; ?D?: use dedicated data;
?G?: use generic data. Data Source: ?T?: training
data; ?C?: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7752 0.7933 (1.5%) 0.7936 (1.5%)
GC 0.7624 0.7844 (0.4%) 0.7914 (1.2%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6756 0.7636 (-2.2%) 0.7784 (-0.3%)
GC 0.6860 0.7717 (-1.2%) 0.7774 (-0.5%)
better than only using dedicated training data, be-
cause the distributions of non-dedicated data and
dedicated data share some similarity. As the ded-
icated training data is insufficient, the adoption of
the extra non-dedicated data helps the learning.
We compare learning results with Baseline 2) (use
generic training data, the slot of NC + GT in the
tables), which is the higher baseline.
Baseline by click data: we then study the utili-
ties of click preference pairs by using them alone
for training without using labeled training data.
In Table 5 and 6, each of the NDCG
5
results us-
ing click preference pairs is the highest NDCG
5
value over the cases of using different amounts of
pairs (5000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs). The results regarding the pairs
amounts are illustrated in Figure 3, which will help
us to analyze the results more deeply.
If we only use click preference pairs for training
(the two table slots DC+NT and GC+NT, corre-
sponding to using dedicated click preference pairs
and generic click pairs respectively), the best case
is using skip-next pairs extracted by heuristic rule
approach (Table 5 (a) ). It is not surprising that
skip-next pairs outperform skip-above pairs be-
cause there are significantly lower percentage of
incorrect pairs in skip-next pairs compared with
skip-above pairs. It is a little bit surprising that
the case of DC+NT has no dominant advantage
over GC+NT as we expected. For example, in Ta-
ble 5 (a), the NDCG
5
values (0.7822 and 0.7834)
are very close to each other. However, in Figure
3, we find that with the same amount of pairs,
when we use 30,000 or fewer pairs, using dedi-
1092
1 2 3 4 5 6 7 8 9 10
x 104
0.755
0.76
0.765
0.77
0.775
0.78
0.785
0.79
0.795
0.8
click pairs
ND
CG
5
 
 
dedicate train + dedicate clickgeneric train + dedicate clickdedicate clickgeneric click
Figure 3: Incorporate different amounts of skip-
next pairs by heuristic rule approach with generic
training data.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.76
0.77
0.78
0.79
0.8
0.81
trainging sample weight
ND
CG
5
 
 
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
Figure 4: The effects of using different combin-
ing weights. Skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
cated click pairs alone is always better than using
generic click pairs alone. With more click pairs
being used (> 30, 000), the noise rates become
higher in the pairs, which makes the distribution
factor less important.
Combine training data and click data: we
compare the four table slots, DC+DT, GC+DT,
DC+GT, GC+GT, in Table 5 and 6, and there are
quite a few interesting observations:
1) Skip-next vs. skip-above: overall, incorporat-
ing skip-next pairs with training data is better than
incorporating skip-above pairs, due to the reason
that there are more incorrect pairs in skip-above
pairs, which may mislead the learning. The only
exception is the slot GC+GT in Table 5 (b), whose
NDCG
5
improvement is as high as 2.2%. We fur-
ther track this result, and find that this is the case
by using only 5,000 generic skip-above pairs. The
noise rate of these 5,000 pairs is low because they
have the highest pair extraction confidence values.
At the same time, these 5,000 pairs may provide
good complementary signals to the generic train-
ing data, so that the learning result is good. How-
ever, in general, skip-next pairs have better utilities
than skip-above pairs.
2) Dedicated training data vs. generic train-
ing data: using generic training data is gen-
erally better than only using dedicated training
data. If training data is insufficient, the extra
non-dedicated data provides useful information
for relevance pattern learning, and the distribu-
tion dissimilarity between dedicated data and non-
dedicated data is not the most important factor.
3) Dedicated click data vs. generic click data:
using dedicated click data is more effective than
using generic click data. From Figure 3, we ob-
serve that when 30,000 or fewer pairs are incorpo-
rated into training data, using dedicate click pairs
is always better than using generic click pairs.
0 0.5 1 1.5 2 2.50.784
0.786
0.788
0.79
0.792
0.794
0.796
0.798
0.8
0.802
?
ND
CG
5
 
 
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
Figure 5: The effects of using different margin
values for click preference pairs. Skip-next pairs
by heuristic rule approach are incorporated with
generic training data.
4) Heuristic rule approach vs. SSL approach:
the preference pairs extracted by heuristic rule ap-
proach have better utilities than those extracted by
SSL approach.
5) GBrank parameters for combining training
data and click pairs: the relative weight w for
combining training data and click pairs in (2) may
also affect rank function learning. Figure 4 shows
the effects of using different combining weights,
1093
for which skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
We observe that neither over-weighting training
data or over-weighting click pairs yields good re-
sults while the two data sources are best exploited
at certain weight values when there is good bal-
ance between them. Another concern is the ap-
propriate margin value ? for the click pairs in (2).
Figure 5 shows that ? = 1 consistently yields good
learning results, which suggests us that click pair
provides good information at ? = 1.
4.4 Discussions
we have defactorized the related approaches for
exploiting click data to improve task-specific rank
learning. The utility of click preference pairs de-
pends on the following factors:
1) Data distribution: if click pairs have good
quality, we should use dedicated click pairs in-
stead of generic click pairs, so that the samples
for training have similar distribution to the task of
task-specific ranking.
2) The amount of dedicated training data: the
more dedicated training data, the more reliable the
task-specific rank function is; thus, the less room
for learning improvement using click data. For the
case in the experiment that dedicated training is in-
sufficient, the non-dedicated training data can also
help the learning as non-dedicated training data
share relevance pattern similarity with the dedi-
cated data distribution.
3) The quality of click pairs: if we can extract
large amount of high-quality click pairs, the learn-
ing improvement will be significant. For example,
as shown in Figure 3, at the early stage with fewer
click pairs (5,000 and 10,000 pairs) being com-
bined with training data, the learning improvement
is best. With more click pairs are used, the noise
rate in the click pairs becomes higher so that the
learning misleading factor is more important than
information complementary factor. Thus, it is im-
portant to improve the reliability of the click pairs.
4) The utility of click pairs: by our study, the
quality of click pairs extracted by SSL approach
is comparable to those extracted by heuristic rule
approach. The possible reason that heuristic-rule-
based click pairs can bring more benefit is that
these pairs provide more complementary infor-
mation compared with SSL approach. As the
methodologies of these two click data extraction
approaches are totally different, in future we will
explore the concrete reason that causes such utility
difference.
5 Conclusions
By empirically exploring the related factors in
utilizing click-through data to improve dedicated
model learning for task-specific ranking, we have
better understood the principles of using click
preference pairs appropriately, which is impor-
tant for the real-world applications in commer-
cial search engines as using click data can sig-
nificantly save human labeling costs and makes
rank function learning more efficient. In the case
that dedicated training data is limited, while non-
dedicated training data is helpful, using dedicated
skip-next pairs is the most effective way to further
improve the learning. Heuristic rule approach pro-
vides more useful click pairs compared with se-
quential supervised learning approach. The qual-
ity of click pairs is critical for the efficacy of the
approach. Therefore, an interesting topic is how
to further reduce the inconsistency between skip-
above pairs and human labeling so that such data
may also be useful for task-specific ranking.
1094
References
E. Agichtein, E. Brill, and S. Dumais. 2006. Improv-
ing web search ranking by incorporating user behav-
ior information. Proc. of ACM SIGIR Conference.
S. M. Beitzel, E. C. Jensen, A. Chowdhury, and
O. Frieder. 2007. Varying approaches to topical
web query classification. Proceedings of ACM SI-
GIR conference.
C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
Learning to rank using gradient descent. Proc. of
Intl. Conf. on Machine Learning.
Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. 2007.
Learning to rank: From pairwise approach to list-
wise. Proceedings of ICML conference.
B. Carterette, P. N. Bennett, D. M. Chickering, and S. T.
Dumais. 2008. Here or there: preference judgments
for relevance. Proc. of ECIR.
O. Chapelle and Y. Zhang. 2009. A dynamic bayesian
network click model for web search ranking. Pro-
ceedings of the 18th International World Wide Web
Conference.
K. Chen, Y. Zhang, Z. Zheng, H. Zha, and G. Sun.
2008. Adapting ranking functions to user prefer-
ence. ICDE Workshops, pages 580?587.
M. Coyle and B. Smyth. 2007. Supporting intelligent
web search. ACM Transaction Internet Tech., 7(4).
T. G. Dietterich. 2002. Machine learning for sequen-
tial data: a review. Lecture Notes in Computer Sci-
ence, (2396):15?30.
S. Fox, K. Karnawat, M. Mydland, S. Dumias, and
T. White. 2005. Evaluating implicit measures to
improve web search. ACM Trans. on Information
Systems, 23(2):147?168.
Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer.
1998. An efficient boosting algorithm for combin-
ing preferences. Proceedings of International Con-
ference on Machine Learning.
J. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist., 29:1189?
1232.
X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and H. Shum.
2008. Query dependent ranking with k nearest
neighbor. Proceedings of ACM SIGIR Conference.
N. S. Glance. 2001. Community search assistant. In-
telligent User Interfaces, pages 91?96.
S. Ji, K. Zhou, C. Liao, Z. Zheng, G. Xue, O. Chapelle,
G. Sun, and H. Zha. 2009. Global ranking by ex-
ploiting user clicks. In SIGIR?09, Boston, USA, July
19-23.
T. Joachims, L. Granka, B. Pan, and G Gay. 2005.
Accurately interpreting clickthough data as implicit
feedback. Proc. of ACM SIGIR Conference.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
I. Kang and G. Kim. 2003. Query type classification
for web document retrieval. Proceedings of ACM
SIGIR Conference.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML, pages
282?289.
U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifi-
cation of user goals in web search. Proceedings of
International Conference on World Wide Web.
X. Li, Y.Y Wang, and A. Acero. 2008. Learning query
intent from regularized click graphs. Proceedings of
ACM SIGIR Conference.
T. Y Liu. 2008. Learning to rank for information re-
trieval. SIGIR tutorial.
T. Qin, T. Liu, X. Zhang, D. Wang, and H. Li. 2008.
Global ranking using continuous conditional ran-
dom fields. In NIPS.
H. Li R. Jin, H. Valizadegan. 2008. Ranking re-
finement and its application to information retrieval.
Proceedings of International Conference on World
Wide Web.
F. Radlinski and T. Joachims. 2007. Active exploration
for learning rankings from clickthrough data. Proc.
of ACM SIGKDD Conference.
F. Radlinski, M. Kurup, and T. Joachims. 2008. How
does clickthrough data reflect retrieval quality? Pro-
ceedings of ACM CIKM Conference.
D. E. Rose and D. Levinson. 2004. Understanding user
goals in web search. Proceedings of International
Conference on World Wide Web.
X. Wang and C. Zhai. 2007. Learn from web search
logs to organize search results. In Proceedings of
the 30th ACM SIGIR.
H. Zha, Z. Zheng, H. Fu, and G. Sun. 2006. Incor-
porating query difference for learning retrieval func-
tions in world wide web search. Proceedings of the
15th ACM Conference on Information and Knowl-
edge Management.
Z. Zheng, H. Zhang, T. Zhang, O. Chapelle, K. Chen,
and G. Sun. 2007. A general boosting method and
its application to learning ranking functions for web
search. NIPS.
1095
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1129?1139,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Learning Recurrent Event Queries for Web Search
Ruiqiang Zhang and Yuki Konda and Anlei Dong
Pranam Kolari and Yi Chang and ZhaohuiZheng
Yahoo! Inc
701 First Avenue, Sunnyvale, CA94089
Abstract
Recurrent event queries (REQ) constitute a
special class of search queries occurring at
regular, predictable time intervals. The fresh-
ness of documents ranked for such queries is
generally of critical importance. REQ forms a
significant volume, as much as 6% of query
traffic received by search engines. In this
work, we develop an improved REQ classi-
fier that could provide significant improve-
ments in addressing this problem. We ana-
lyze REQ queries, and develop novel features
from multiple sources, and evaluate them us-
ing machine learning techniques. From histor-
ical query logs, we develop features utilizing
query frequency, click information, and user
intent dynamics within a search session. We
also develop temporal features by time series
analysis from query frequency. Other gener-
ated features include word matching with re-
current event seed words and time sensitiv-
ity of search result set. We use Naive Bayes,
SVM and decision tree based logistic regres-
sion model to train REQ classifier. The re-
sults on test data show that our models outper-
formed baseline approach significantly. Ex-
periments on a commercial Web search en-
gine also show significant gains in overall rel-
evance, and thus overall user experience.
1 Introduction
REQ pertains to queries about events which oc-
cur at regular, predictable time intervals, most often
weekly, monthly, annually, bi-annually, etc. Natu-
rally, users issue REQ periodically. REQ usually re-
fer to:
Organized public events such as festivals, confer-
ences, expos, sports competitions, elections: winter
olympics, boston marathon, the International Ocean
Research Conference, oscar night.
Public holidays and other noteworthy dates: labor day,
date of Good Friday, Thanksgiving, black friday.
Products with annual model releases, such as car models:
ford explorer, prius.
Lottery drawings: California lotto results.
TV shows and programs which are currently running:
American idol, Inside Edition.
Cultural related activities: presidential election, tax re-
turn, 1040 form.
Our interest in studying REQ arises from the chal-
lenge imposed on Web search ranking. To illustrate
this, we show an example in Fig. 1 that snapshots
the real ranking results of the query, EMNLP, is-
sued in 2010 when the authors composed this pa-
per, on Google search engine. It is obvious the
ranking is not satisfactory because the page about
EMNLP2008 is on the first position in 2010. Ide-
ally, the page about EMNLP2010 on the 6th position
should be on the first position even if users don?t
explicitly issue the query, EMNLP 2010, because
EMNLP is a REQ. The query, ?EMNLP?, implic-
itly, without a year qualifier, needs to be served the
most recent pages about ?EMNLP?.
A better search ranking result cannot be achieved
if we do not categorize ?EMNLP? as a REQ, and
provide special ranking treatment to such queries.
Existing search engines adopt a fairly involved rank-
ing algorithm to order Web search results by con-
sidering many factors. Time is an important fac-
tor but not the most critical. The page?s rank-
ing score mostly depends on other features such
as tf-idf (Salton and McGill, 1983), BM25 (Jones
1129
Figure 1: A real problematic ranking result by Google for
a REQ query, ?EMNLP?. The EMNLP2010 page should
be on the 1st position.
et al, 2000), anchor text, historical clicks, pager-
ank (Brin and Page, 1998), and overall page qual-
ity. New pages about EMNLP2010 obtain less fa-
vorable feature values than the pages of 2009 earlier
in terms of anchor text, click or pagerank because
they have existed for a shorter time and haven?t ac-
cumulated sufficient popularity to make them stand
out. Without special treatment, the new pages about
?EMNLP2010? will typically not be ranked appro-
priately for the users.
Typically, a recurrent event is associated with a
root, and spawns a large set of queries. Oscar,
for instance, is a recurrent event about the annual
Academy Award. Based on this, queries like ?oscar
best actress?, ?oscar best dress?, ?oscar best movie
award?, are all recurrent event queries. As such,
REQ is a highly frequent category of query in Web
search. By Web search query log analysis, we ob-
serve that there about 5-6% queries of total query
volume belongs to this category.
In this work, we learn if a query is in the REQ
class, by effectively combining multiple features.
Our features are developed through analysis of his-
torical query logs. We discuss our approaches in de-
tail in Section 3. We then develop a REQ classi-
fier where all the features are integrated by machine
learning models. We use Naive Bayes, SVM and de-
cision tree based logistic regression models. These
models are described in Section 4. Our experiments
for REQ classifier and Web search ranking are de-
tailed in Section 5 and 6.
2 Related Work
We found our work were related to two other prob-
lems: general query classification and time-sensitive
query classification. For general query classifica-
tion, the task is to assign a Web search query to
one or more predefined categories based on its top-
ics. In the query classification contest in KDD-
CUP 2005 (Li et al, 2005), seven categories and
67 sub-categories were defined. The winning so-
lution (Shen et al, 2005) used multiple classifiers
integrated by ensemble method. The difficulties for
query classification are from short queries, lack of
labeled data, and query sense ambiguity. Most pop-
ular studies use query log, web search results, unla-
beled data to enrich query classification (Shen et al,
2006; Beitzel et al, 2005), or use document classifi-
cation to predict query classification (Broder et al, ).
General query classification is also studied for query
intent detection by (Li et al, 2008).
There are many prior works to study the time sen-
sitivity issue in web search. For example, Baeza-
Yates et al (Baeza-Yates et al, 2002) studied the re-
lation between the web dynamics, structure and page
quality, and demonstrated that PageRank is biased
against new pages. In T-Rank Light and T-Rank al-
gorithms (Berberich et al, 2005), both activity (i.e.,
update rates) and freshness (i.e., timestamps of most
recent updates) of pages and links are taken into ac-
count for link analysis. Cho et al (Cho et al, 2005)
proposed a page quality ranking function in order to
alleviate the problem of popularity-based ranking,
and they used the derivatives of PageRank to fore-
cast future PageRank values for new pages. Pandey
et al (Pandey et al, 2005) studied the tradeoff be-
tween new page exploration and high-quality page
exploitation, which is based on a ranking method to
randomly promote some new pages so that they can
accumulate links quickly.
More recently, Dong et al (Dong et al, 2010a)
1130
proposed a machine-learned framework to improve
ranking result freshness, in which novel features,
modeling algorithms and editorial guideline are used
to deal with time sensitivities of queries and doc-
uments. In another work (Dong et al, 2010b), they
use micro-blogging data (e.g., Twitter data) to detect
fresh URLs. Novel and effective features are also
extracted for fresh URLs so that ranking recency in
web search is improved.
Perhaps the most related work to this paper is
the query classification approach used in (Zhang
et al, 2009) and (Metzler et al, 2009), in which
year qualified queries (YQQs) are detected based
on heuristic rules. For example, a query contain-
ing a year stamp is an explicit YQQ; if the year
stamp is removed from this YQQ, the remaining part
of this query is also a YQQ, which is called im-
plicit YQQ. Different ranking approaches were used
in (Zhang et al, 2009) and (Metzler et al, 2009)
where (Zhang et al, 2009) boosted pages of the most
latest year while (Metzler et al, 2009) promoted
pages of the most influential years. Similarly, Nunes
et al (Nunes, 2007) applied information extraction
techniques to identify temporal expression in web
search queries, and found 1.5% of queries contain-
ing temporal expression.
Dong et al (Dong et al, 2010a) proposed a
breaking-news query classifier with high accuracy
and reasonable coverage, which works not by mod-
eling each individual topic and tracking it over time,
but by modeling each discrete time slot, and compar-
ing the models representing different time slots. The
buzziness of a query is computed as the language
model likelihood difference between different time
slots. In this approach, both query log and news
contents are exploited to compute language model
likelihood.
Diaz (Diaz, 2009) determined the newsworthiness
of a query by predicting the probability of a user
clicks on the news display of a query. In this frame-
work, the data sources of both query log and news
corpus are leveraged to compute contextual features.
Furthermore, the online click feedback also plays a
critical role for future click prediction.
Konig et al (Knig et al, 2009) estimated the
click-through rate for dedicated news search result
with a supervised model, which is to satisfy the
requirement of adapting quickly to emerging news
event. Some additional corpora such as blog crawl
and Wikipedia is used for buzziness inference. Com-
pared with (Diaz, 2009), different feature and learn-
ing algorithms are used.
Elsas et al (Elsas and Dumais, 2010) studied
improving relevance ranking by detecting document
content change to leverage temporal information.
3 Feature Generation
To better understand our work, we first introduce
three terms. We subdivide all raw queries in query
log into three categories: Explicit Timestamp, Im-
plicit Timestamp, and No Timestamp. An Explicit
Timestamp query contains at least one token being a
time indicator. For example, emnlp 2010, 2007 De-
cember holiday calendar, amsterdam weather sum-
mer 2009, Google Q1 reports 2010. These queries
are considered to conatin time indicators, because
we can regard {2010, 2007, 2009} as year indica-
tor, december as month indicator, {summer, Q1(first
quarter)} as seasonal indicator. To simplify our
work, we only consider the year indicators, 2010,
2007, 2009. Such year indicators are also the most
important and most popular indicators, as noted in
(Zhang et al, 2009). Any query containing at least
one year indicator is an Explicit Timestamp query.
Due to word sense ambiguity, some queries labeled
as Explicit Timestamp by this method may have no
connection with time such as Windows Office 2007,
2010 Sunset Boulevard, or call number 2008. In this
work, we tolerate this type of error because word
sense disambiguation is a peripheral problem for this
task.
Implicit Timestamp queries are resulted by re-
moving all year indicators from the corresponding
Explicit Timestamp queries. For example, the Im-
plicit Timestamp query of emnlp 2010 is emnlp.
All other queries are No Timestamp queries because
they have never been found together with a year in-
dicator.
Classifying queries into the above three cate-
gories depends on the used query log. A search
engine company partner provided us a query log
from 08/01/2009 to 02/29/2010 for this research.
We found the proportions of the three categories
in this query log are 13.8% (Explicit), 17.1% (Im-
plicit) and 69.1% (No Timestamp). These numbers
1131
could be slightly different depending on the source
of query logs. Note that 17.1% of Implicit Times-
tamp queries in the query log is a significant num-
ber. However, not all Implicit Timestamp queries
are REQ. Many Implicit Timestamp queries have no
time sense. They belong to Implicit Timestamp just
because users issued the query with a year indica-
tor through varied intents. For example, ?google? is
found to be an Implicit Timestamp query since there
were many ?google 2008? or ?google 2009? in the
query log.
The next few sections introduce our work in rec-
ognizing recurrent event time sense for Implicit
Timestamp queries. We first focus on features.
There are many features that were exploited in REQ
classifier. We extract these features from query log,
query session log, click log, search results, time se-
ries and NLP morphological analysis.
3.1 Query log analysis
The following features are extracted from query log
analysis:
QueryDailyFrequency: the total counts of the
query divided by the number of the days in the pe-
riod.
ExplicitQueryRatio: Ratio of number of counts
query was issued with year and number of counts
query was issued with or without year. This feature
is the method used by (Zhang et al, 2009).
UniqExplicitQueryCount: Number of uniq Ex-
plicit Timestamp queries associated with query. For
example, if a query was issued with query+2009 and
query+2008, this feature?s value is two.
ChiSquareYearDist: this feature is the distance be-
tween two distributions: one is frequency distribu-
tion over years for all REQ queries. The other is
that for single REQ query. It is calculated through
following steps: (a) Aggregate the frequencies for
all queries for all years. Suppose we observe all
years from 2001 to 2010. So we can get vector,
E = ( a f10
sum1 ,
a f09
sum1 , ...,
a f01
sum1 ) where a fi is the frequency
sum of year 20i for all REQ queries. sum1 =
a f10 + a f09 + ... + a f01, the sum of all year fre-
quency. (b) Given a query, suppose we observe
this query?s yearly frequency distribution is , Oq =
(q f10, q f09, , ..., q f01). q fi is this query?s frequency
for the year 20i. Pad the slot with zeros if no fre-
quency found. The expected distribution for this
query is, Eq = ( sum2?a f10
sum1 ,
sum2?a f09
sum1 , ...,
sum2?a f01
sum1 ),
where sum2 = q f10 + q f09 + ... + q f01 is sum of
all year frequency for the query. (d) Calculate CHI-
squared value to represent the different yearly fre-
quency distribution between Eq and Oq according to
?2 =
?N
i=1
(Oqi ?E
q
i )2
Eqi
. Using CHI square distance as a
method is widely used for statistical hypothesis test.
We found it to be a useful feature for REQ classifier.
3.2 Query reformulation
If users cannot find the newest page by issuing Im-
plicit Timestamp query, they may re-issue the query
using an Explicit Timestamp query. We can detect
this change in a search session (a 30 minutes period
for each query). By finding this kind of behavior
from users, we next extract three features.
UserSwitch: Number of unique users that switched
from Implicit Timestamp queries to Explicit Times-
tamp queries.
YearSwitch: Number of unique year-like tokens
switched by users in a query session.
NormalizedUserSwitch: Feature UserSwitch di-
vided by QueryDailyFrequency.
3.3 Click log analysis
If a query is time sensitive, users may click a
page that displays the year indicator on title or
url. An example that shows year indicator on
url is www.lsi.upc.edu/events/emnlp2010/call.html.
Search engine click log saves all users? click infor-
mation. We used click log to derive the following
features.
YearUrlTop5CTR: Aggregated click through rate
(CTR) of all top five URLs containing a year in-
dicator. CTR of an URL is defined as the number
of clicks of an URL divided by the number of page
views.
YearUrlFPCTR: Aggregated click through rate
(CTR) of all first page URLs containing a year in-
dicator.
3.4 Search engine result set
For each Implicited Timestamp query, we can scrape
the search engine to get search results. We count the
number of titles and urls that contain year indicator.
We use this number as a feature, and generate 6 fea-
tures.
1132
TitleYearTop5: the number of titles containing a
year indication on the top 5 results. This value is
4 in Fig. 1.
TitleYearTop10: the number of titles containing a
year indication on the top 10 results. This value is 6
in Fig. 1.
TitleYearTop30: the number of titles containing a
year indication on the top 30 results.
UrlYearTop5: the number of urls containing a year
indication on the top 5 results. This value is 1 in
Fig. 1.
UrlYearTop10: the number of urls containing a year
indication on the top 10 results.
UrlYearTop30: the number of titles containing a
year indication on the top 30 results.
3.5 Time series analysis
Recurrent event query has periodic occurrence pat-
tern in time series. Top graph of Figure 2 shows the
frequency change of the query, ?Oscar?. The annual
event usually starts from Oscar nomination as ear-
lier as last year December to award announcement
of February this year. So a small spike and a big
spike are observed in the graph to indicate nomina-
tion period and ceremony period. There are a period
of silence between the two periods. The frequency
pattern keeps unchanged each year. We show three
years (2007,8,9) in the graph. By making use of re-
current event queries? periodic properties, we calcu-
lated the query period as a new feature.
We use autocorrelation to calculate the period.
R(?) =
?N??
t=1 (xt ? ?)(xt+? ? ?)
{
?N??
t=1 (xt ? ?)2(xt+? ? ?)2}1/2
where x(t) is query daily frequency. N is the num-
ber of days used for this query. We can get maxi-
mum of 3 years data for some queries but only a few
months for others. R(?) is autocorrelation function.
Peaks (the local biggest R(?) given a time window)
can be detected from R(?) plot. The period T is cal-
culated as the duration between two neighbor peaks.
T = 365 for the query, ?Oscar?. The bottom graph
of Fig. 2 shows the autocorrelation function plot for
the query Oscar.
3.6 Recurrent event seed word list
Many recurrent event queries share some common
words that have recurrent time sense. We list most
new results top schedule
football festival movie world
show day best tax
result calendar honda ford
download exam nfl miss
awards toyota tour sale
american fair list pictures
election game basketball cup
Table 1: Top recurrent event seed words
frequently used recurrent seeds in Table 1. Those
seeds are likely combined with other words to form
new recurrent event queries. For example, the seed,
?new?, can be used by queries ?new bmw cars?,
?whitney houston new songs?, ?apple new iphone?,
or ?hairstyle new?.
To generate the seed list, we tokenized all the
queries from Implicit Timestamp queries and split
all the tokens. We then sort and unique all the to-
kens, and submit top tokens to professional editors
who are asked to pick 8,000 seeds from the top fre-
quent tokens. Some top tokens were removed if they
are not qualified to form recurrent event queries. The
editors took about four days to do the judgment ac-
cording to the token?s time sense and examples of
recurrent event queries. However, this is a one-time
effort. A token will be in the seed if there are many
recurrent event examples formed by this token, by
editors? judgment.
Table 1 shows 32 top seeds. Some seeds connect
with time such as, ?new, schedule, day, best, calen-
dar?; some relate to sports, ?football, game, nfl, tour,
basketball, cup?; some about cars, ?honda, ford, toy-
ota?. The reason why ?miss? is in the seeds is that
there are many annual events about beauty contest
such as ?miss america, miss california, miss korea?.
We use the seed list to generate the following
three features:
AveNumberTokensSeeds: number of tokens that is
in the seed list divided by number of tokens in the
query.
AveNumberTokensNotSeeds: number of tokens
that is not in the seed list divided by number of to-
kens in the query.
DiffNumberTokensSeeds: The difference of the
above two values.
1133
-0 .2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1 15 29 43 57 71 85 99 113 127 141 155 169 183 197 211 225 239 253 267 281 295 309 323 337 351 365 379 393 407 421 435 449 463 477 491 505 519 533 547 561 575 589 603 617 631 645 659 673 687 701 715 729 743 757 771 785 799 813 827 841 855 869 883 897
Figure 2: Frequency waveform(top) and corresponding autocorrelation curve (bottom) for query Oscar.
4 Learning Approach for REQ
The REQ classification is a typical machine learn-
ing task. Given M observed samples used for train-
ing data, {(x0, y0), (x1, y1), ? ? ? , (xM, yM)} where xi is
a feature vector we developed in last section for a
given query. yi is the observation value, {+1,?1},
indicating the class of REQ and non-REQ. The task
is to find the class probability given an unknown fea-
ture vector, x?, that is,
p(y = c|x?), c = +1,?1. (1)
There are a lot of machine learning methods ap-
plicable to implement Eq. 1. In this work, we
adopted three representative methods.
The first method is Naive Bayes method. This
method treats features independent. If x is enx-
tended into feature vector, x = {x0, x1, ? ? ? , xN} then,
p(y = c|x) = 1
Z
p(c)
i=N
?
i=0
p(xi|c)
The second method is SVM. In this work we used
the tool for our experiments, LIBSVM (Chang and
Lin, 2001). Because SVM is a well known approach
and widely used in many classification task, we skip
to describe how to use this tool. Readers can turn to
the reference for more details.
The third method is based on decision tree based
logistic regression model. The probability is given
by the formula below,
p(y = c|x) = 1
1 + e? f (x)
(2)
We employ Gradient Boosted Decision Tree algo-
rithm (Friedman, 2001) to learn the function f (X).
Gradient Boosted Decision Tree is an additive re-
gression algorithm consisting of an ensemble of
trees, fitted to current residuals, gradients of the loss
function, in a forward step-wise manner. It itera-
tively fits an additive model as
ft(x) = Tt(x;?) + ?
T
?
t=1
?tTt(x;?t)
such that certain loss function L(yi, fT (x + i)) is
minimized, where Tt(x;?t) is a tree at iteration t,
weighted by parameter ?t, with a finite number of
parameters, ?t and ? is the learning rate. At iteration
t, tree Tt(x; ?) is induced to fit the negative gradient
by least squares.
The optimal weights of trees ?t are determined by
?t = argmin?
N
?
i
L(yi, ft?1(xi) + ?T (xi, ?))
Each node in the trees represents a split on a fea-
ture. The tuneable parameters in such a machine-
learnt model include the number of leaf nodes in
each tree, the relative contribution of score from
each tree called the shrinkage, and total number of
shallow decision trees.
The relative importance of a feature S i, in such
forests of decision trees, is aggregated over all the
1134
m shallow decision trees (Breiman et al, 1984) as
follows:
S 2i =
1
M
M
?
m=1
L?1
?
n=1
wl ? wr
wl + wr
(ylyr)2I(vt = i) (3)
where vt is the feature on which a split occurs, yl
and yr are the mean regression responses from the
right, and left sub-tree, and wl and wr are the corre-
sponding weights to the means, as measured by the
number of training examples traversing the left and
right sub-trees.
5 REQ Learner Evaluation
We collected 6,000 queries labeled as either Recur-
rent or Non-recurrent by professional human edi-
tors. The 6,000 queries were sampled from Implicit
Timestamp queries according to frequency distribu-
tion to be representative. We split the queries into
5,000 for training and 1,000 for test. For each query,
we calculated features? values as described in Sec-
tion 3.
The Naive Bayes method used single Gaussian
function for each independent feature. Mean and
variance were calculated from the training data.
As for LIBSVM, we used C-SVC, linear function
as kernel and 1.0 of shrinkage.
The parameters used in the regression model were
20 of trees, 20 of nodes and 0.8 of learning rate
(shrinkage).
The test results are shown in Fig. 3, recall-
precision curve. We set a series of threshold to the
probability of c = +1 calculated by Eq. 1 so that
we can get the point values of recall and precision in
Fig. 3. For example, if we set a threshold of 0.6, a
query with a probability larger than 0.6 is classified
as REQ. Otherwise, it is non-REQ. The precision
is a measure of correctly classified REQ queries di-
vided by all classified REQ queries. The recall is a
measure of correctly classified REQ queries divided
by all REQ queries in test data.
In addition to the three plots, we also show the
results using only one feature, ExplicitQueryRatio,
for comparison with the classification method used
by (Zhang et al, 2009).All the three models us-
ing all features performed better than the existing
method using ExplicitQueryRatio. The highest im-
provement was achieved by GBDT regression tree
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
ExplicitQueryRatio
GBDTree
Naive Bayes
SVM
Figure 3: Comparison of precision and recall rate be-
tween our method and the existing method.
model. The results of Naive Bayes were lower than
SVM and GBDTree. This model is weaker because
it treats features independently. Typically SVMs and
GBDT gives comparable results on a large class of
problems. Since for this task we use features from
different sources, the feature values are designed to
have larger dynamic range, which is better handled
by GBDT.
The features? importance ranked by Equation 3
is shown in Table 2. We list the top 10 features.
The No.1 important feature is ExplicitQueryRatio.
The second and seventh features are from search ses-
sion analysis by counting users who changed queries
from Implicit Timestamp to Explicit Timestamp.
This is a strong source of features. The time se-
ries analysis feature is ranked No.3. Calculation of
this feature needs two years query log to be much
more effective, but we didn?t get so large data for
many queries. One of the features from recurrent
event seed list is ranked No.4. This is also an impor-
tant feature source. The ChiSquareYearDist feature
is ranked 5th, that proves the recurrent event query
frequency has a statistical distribution pattern over
years. TitleYearTop30 and TitleYearTop10 that are
derived from scraping results are ranked the 9th and
10th important.
Fig. 4 shows the distribution of feature values for
1135
Feature Rank Score
ExplicitQueryRatio 1 100
NormalizedUserSwitch 2 71.7
AutoCorrelation 3 54.0
AveNumberTokenSeeds 4 48.7
ChiSquareYearDist 5 36.3
YearUrlFPCTR 6 19.1
UserSwitch 7 11.7
QueryDailyFreq 8 10.7
TitleYearTop30 9 10.6
TitleYearTop10 10 5.8
Table 2: Top 10 most important features: rank and im-
portance score (100 is maximum)
1
2
3
4
5
6
7
8
9
10
Figure 4: Feature value distribution of all data
(blue=REQ, red=non-REQ)
each sample of the 6,000 data, where each point rep-
resents a query and each line represents a feature?s
value for all queries. One point is a query. The fea-
tures are ordered according to feature importance of
Table 2. The ?blue? points indicate REQ queries and
the ?red? points, non-REQ queries. Some features
are continuous like the 1st and 2nd. Some feature
values are discrete like the last two indicating Ti-
tleYearTop30 and TitleYearTop10. There are ?red?
samples in the 4th feature but overlapped with and
covered by ?blue? samples visually.
In the Table 3, we show F-Measure values as we
gradually added features from the feature, Explicit-
QueryRatio, according to feature importance in Ta-
ble 2. We listed the F-Measure values under three
threshold, 0.6, 0.7 and 0.8. Higher threshold will in-
crease classifier precision rate but reduce recall rate.
F-Measure is a metric combining precision rate and
recall rate. It is clearly observed that the classifier
performance is improved as more features are used.
Threshold
Feature 0.6 0.7 0.8
ExplicitQueryRatio 0.833 0.833 0.752
+NormalizedUserSwitch 0.840 0.837 0.791
+AutoCorrelation 0.850 0.839 0.823
+AveNumberTokenSeeds 0.857 0.854 0.834
+ChiSquareYearDist 0.857 0.864 0.839
+YearUrlFPCTR 0.869 0.867 0.837
+UserSwitch 0.862 0.862 0.846
+QueryDailyFreq 0.860 0.852 0.847
+TitleYearTop30 0.854 0.853 0.843
+TitleYearTop10 0.858 0.861 0.852
+All 0.876 0.867 0.862
Table 3: F-Measures as varying thresholds by adding top
features.
Query Probability
ncaa men?s basketball tournament 0.999
bmw 328i sedan reviews 0.999
new apple iphone release 0.932
sigir 0.920
new york weather in april 0.717
academy awards reviews 0.404
google ipo 0.120
adidas jp 0.082
Table 4: Probabilities of example queries by GBDT tree
classifier
Some query examples, and their scores from our
model are listed in Table 4. The last two exam-
ples, google ipo and adidas jp, have very low values,
and are not REQs. The first four queries are typical
REQs. They have higher values of features Explicit-
QueryRatio,Normalized UserSwitch and YearUrlF-
PCTR. Although both new apple iphone release re-
views and academy awards reviews are about re-
views, academy awards reviews has lower value
of NormalizedUserSwitch and ChiSquareYearDist
could be the reason for a lower score.
6 Web Search Ranking
In this section, we use the approach proposed
by (Zhang et al, 2009) to test the REQ classifier
for Web search ranking. In their approach, search
ranking is altered by boosting pages with most re-
cent year if the query is a REQ. The year indicator
1136
DCG@5 DCG@1
bucket #(query) Organic Our?s % over Organic Organic Ours % over Organic
[0.0,0.1] 59 6.87 6.96 1.48(-2.3) 4.08 4.19 2.69(-1.07)
[0.1,0.2] 76 5.86 6.01 2.52(0.98) 2.88 2.91 1.14(1.69)
[0.2,0.3] 85 6.33 6.41 1.24(2.12) 3.7 3.7 0.0(0.8)
[0.3,0.4] 75 5.18 5.24 1.18(-0.7) 2.92 2.95 1.14(1.37)
[0.4,0.5] 78 4.96 4.82 -2.84(-1.35) 2.5 2.42 -3.06(0)
[0.5,0.6] 84 5.4 5.37 -0.45(-0.3) 2.82 2.85 1.05(-1.5)
[0.6,0.7] 78 4.78) 5.19) 8.42(3.64) 2.56 2.83 10.75(4.1)
[0.7,0.8] 80 4.45 4.60 3.41(3.19) 2.21 2.26 1.98(2.8)
[0.8,0.9] 78 4.81 4.96 3.15(4.79) 2.32 2.33 0.55(0.65)
[0.9,1.0] 107 5.08 5.50 8.41*(4.41) 2.64 3.09 16.78*(1.36)
[0.0,1.0] 800 5.33 5.47 2.74*(2.17) 2.83 2.93 3.6*(1.26)
Table 5: REQ learner improves search engine organic results. The numbers in the brackets are by Zhang?s methods.
Direct comparison with Zhang?s method is valid only in the last line, using all queries. A sign ??? indicates statistical
significance (p-value<0.05)
can be detected either from title or URL of the re-
sult. For clarity, we re-write their ranking function
as below,
F(q, d) = R(q, d) + [e(do, dn) + k]e??(q)
where the ranking function, F(q, d), consists of
two parts: the base function R(q, d) plus boosting.
If the query q is not a REQ, boosting is set to zero.
Otherwise, boosting is decided by e(do, dn), k, ? and
?(q). e(do, dn) is the difference of base ranking score
between the oldest page and the newest page. If the
newest page has a lower ranking score than the old-
est page, then the difference is added to the newest
page to promote the ranking of the newest page.
?(q) is the confidence score of a REQ query. It is
the value of Eq. 1. ? and k are two empirical param-
eters. (Zhang et al, 2009)?s work has experimented
the effects of using different value of ? and k (? = 0
equals to no discounts for ranking adjustment). We
used ? = 0.4 and k = 0.3 which were the best con-
figuration in (Zhang et al, 2009).
For evaluating our methods, we randomly ex-
tracted 800 queries from the Implicit Timestamp
queries. We scraped a commercial search engine us-
ing the 800 queries. We extracted the top five search
results for each query under three configures: or-
ganic search engine results, (Zhang et al, 2009)?s
method and ours using REQ classifier. We asked
human editors to judge all the scraped (query, url)
pairs. Editors assign five grades according to rel-
evance between query and articles: Perfect, Excel-
lent, Good, Fair, and Bad. For example, a ?Perfect?
grade means the content of the url match exactly the
query intent.
We use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2002) at rank k as
our primary evaluation metrics to measure retrieval
performance. DCG is defined as,
DCG@k =
k
?
i=1
2r(i) ? 1
log2(1 + i)
where r(i) ? {0 . . . 4} is the relevance grade of the ith
ranked document.
The Web search ranking results are shown in Ta-
ble 5. We used GBDT tree learning methods be-
cause it achieved the best results. We divided 800
test queries into 10 buckets according to the classi-
fier probability. The bucket, [0.0,0.1], contains the
query with a classifier probability greater than 0 but
less than 0.1. Our results are compared with organic
search results, but we also show the improvements
over search organic by (Zhang et al, 2009) in the
brackets. Because Zhang?s approach output differ-
ent classifier values from Ours for the same query,
buckets of the same range in the Table contain dif-
ferent queries. Hence, it is inappropriate to compare
1137
Zhang?s with Ours for the same buckets except the
last row where we used all the queries.
Our classifier?s overall performance is much bet-
ter than the organic search results. We achieved
2.74% DCG@5 gain and 3.6% DCG@1 gain over
organic search for all queries. The gains are higher
than (Zhang et al, 2009)?s results with regards to
improvement over organic results. By direct com-
parison, Ours was 2.7% better than Zhangs signif-
icantly in terms of DCG@1 by Wilcoxon signifi-
cant test. DCG@5 is 1.1% better, but not signifi-
cant. The table also show that the higher buckets
with higher probability achieved higher DCG gain
than the lower buckets overall. Our approach ob-
served 16.78% DCG@1 gain for bucket [0.9,1.0].
This shows that our methods are very effective.
7 Conclusions
We found most of REQ are long tail queries that
pose a major challenge to Web search. We have
demonstrated learning REQ is important for Web
search. this type of queries can?t be solved in tra-
ditional ranking method. We found building a REQ
classifier was a good solution. Our work described
using machine learning method to build REQ clas-
sifier. Our proposed methods are novel compar-
ing with traditional query classification methods.
We identified and developed features from query
log, search session, click and time series analysis.
We applied several ML approaches including Naive
Bayes, SVM and GBDT tree to implement REQ
learner. Finally, we show through ranking experi-
ments that the methods we proposed are very effec-
tive and beneficial for search engine ranking.
Acknowledgements
We express our thanks to who have assisted us
to complete this work, especially, to Fumiaki Ya-
maoka, Toru Shimizu, Yoshinori Kobayashi, Mit-
suharu Makita, Garrett Kaminaga, Zhuoran Chen.
References
R. Baeza-Yates, F. Saint-Jean, and C. Castillo. 2002.
Web dynamics, age and page qualit. String Process-
ing and Information Retrieval, pages 453?461.
Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, David
Grossman, David D. Lewis, Abdur Chowdhury, and
Aleksandr Kolcz. 2005. Automatic web query classi-
fication using labeled and unlabeled training data. In
SIGIR ?05, pages 581?582.
K. Berberich, M. Vazirgiannis, and G. Weikum. 2005.
Time-aware authority rankings. Internet Math,
2(3):301?332.
L. Breiman, J. Friedman, R. Olshen, and C. Stone. 1984.
Classification and Regression Trees. Wadsworth and
Brooks, Monterey, CA.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Proceedings of
International Conference on World Wide Web.
Andrei Z. Broder, Marcus Fontoura, Evgeniy
Gabrilovich, Amruta Joshi, Vanja Josifovski, and
Tong Zhang. Robust classification of rare queries
using web knowledge. In SIGIR ?07, pages 231?238.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
J. Cho, S. Roy, and R. Adams. 2005. Page quality: In
search of an unbiased web ranking. Proc. of ACM SIG-
MOD Conference.
F. Diaz. 2009. Integration of news content into web re-
sults. Proceedings of the Second ACM International
Conference on Web Search and Data Mining (WSDM),
pages 182?191.
Anlei Dong, Yi Chang, Zhaohui Zheng, Gilad Mishne,
Jing Bai, Ruiqiang Zhang, Karolina Buchner, Ciya
Liao, and Fernando Diaz. 2010a. Towards recency
ranking in web search. Proceedings of the Third ACM
International Conference on Web Search and Data
Mining (WSDM), pages 11?20.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010b. Time is of the essence: im-
proving recency ranking using twitter data. 19th Inter-
national World Wide Web Conference (WWW), pages
331?340.
Jonathan L. Elsas and Susan T. Dumais. 2010. Lever-
aging temporal dynamics of document content in rele-
vance ranking. In WSDM, pages 1?10.
J. H. Friedman. 2001. Greedy function approximation:
A gradient boosting machine. Annals of Statistics,
29(5):1189?1232.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000.
A probabilistic model of information retrieval: devel-
opment and comparative experiments. Inf. Process.
Manage., 36(6):779?808.
A. C. Knig, M. Gamon, and Q. Wu. 2009. Click-through
prediction for news queries. Proc. of SIGIR, pages
347?354.
1138
Ying Li, Zijian Zheng, and Honghua (Kathy) Dai.
2005. Kdd cup-2005 report: facing a great challenge.
SIGKDD Explor. Newsl., 7(2):91?99.
Xiao Li, Ye yi Wang, and Alex Acero. 2008. Learning
query intent from regularized click graphs. In In SI-
GIR 2008, pages 339?346. ACM.
Donald Metzler, Rosie Jones, Fuchun Peng, and Ruiqiang
Zhang. 2009. Improving search relevance for im-
plicitly temporal queries. In SIGIR ?09: Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
pages 700?701.
S. Nunes. 2007. Exploring temporal evidence in web
information retrieval. BCS IRSG Symposium: Future
Directions in Information Access.
S. Pandey, S. Roy, C. Olston, J. Cho, and S. Chakrabarti.
2005. Shuffling a stacked deck: The case for partially
randomized ranking of search engine results. VLDB.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, NY.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng
Pan, Kangheng Wu, Jie Yin, and Qiang Yang. 2005.
Q2c@ust: our winning solution to query classification
in kddcup 2005. SIGKDD Explor. Newsl., 7(2):100?
110.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,
Kangheng Wu, Jie Yin, and Qiang Yang. 2006. Query
enrichment for web-query classification. ACM Trans.
Inf. Syst., 24(3):320?352.
Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, Donald
Metzler, and Jian-yun Nie. 2009. Search result
re-ranking by feedback control adjustment for time-
sensitive query. In HLT-NAACL ?09, pages 165?168.
1139
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 611?619,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Iterative Viterbi A* Algorithm forK-Best Sequential Decoding
Zhiheng Huang?, Yi Chang, Bo Long, Jean-Francois Crespo?,
Anlei Dong, Sathiya Keerthi and Su-Lin Wu
Yahoo! Labs
701 First Avenue, Sunnyvale
CA 94089, USA
{zhiheng huang,jfcrespo}@yahoo.com?
{yichang,bolong,anlei,selvarak,sulin}@yahoo-inc.com
Abstract
Sequential modeling has been widely used in
a variety of important applications including
named entity recognition and shallow pars-
ing. However, as more and more real time
large-scale tagging applications arise, decod-
ing speed has become a bottleneck for exist-
ing sequential tagging algorithms. In this pa-
per we propose 1-best A*, 1-best iterative A*,
k-best A* and k-best iterative Viterbi A* al-
gorithms for sequential decoding. We show
the efficiency of these proposed algorithms for
five NLP tagging tasks. In particular, we show
that iterative Viterbi A* decoding can be sev-
eral times or orders of magnitude faster than
the state-of-the-art algorithm for tagging tasks
with a large number of labels. This algorithm
makes real-time large-scale tagging applica-
tions with thousands of labels feasible.
1 Introduction
Sequence tagging algorithms including HMMs (Ra-
biner, 1989), CRFs (Lafferty et al, 2001), and
Collins?s perceptron (Collins, 2002) have been
widely employed in NLP applications. Sequential
decoding, which finds the best tag sequences for
given inputs, is an important part of the sequential
tagging framework. Traditionally, the Viterbi al-
gorithm (Viterbi, 1967) is used. This algorithm is
quite efficient when the label size of problem mod-
eled is low. Unfortunately, due to its O(TL2) time
complexity, where T is the input token size and L
is the label size, the Viterbi decoding can become
prohibitively slow when the label size is large (say,
larger than 200).
It is not uncommon that the problem modeled
consists of more than 200 labels. The Viterbi al-
gorithm cannot find the best sequences in tolerable
response time. To resolve this, Esposito and Radi-
cioni (2009) have proposed a Carpediem algorithm
which opens only necessary nodes in searching the
best sequence. More recently, Kaji et al (2010) pro-
posed a staggered decoding algorithm, which proves
to be very efficient on datasets with a large number
of labels.
What the aforementioned literature does not cover
is the k-best sequential decoding problem, which is
indeed frequently required in practice. For example
to pursue a high recall ratio, a named entity recogni-
tion system may have to adopt k-best sequences in
case the true entities are not recognized at the best
one. The k-best parses have been extensively stud-
ied in syntactic parsing context (Huang, 2005; Pauls
and Klein, 2009), but it is not well accommodated
in sequential decoding context. To our best knowl-
edge, the state-of-the-art k-best sequential decoding
algorithm is Viterbi A* 1. In this paper, we general-
ize the iterative process from the work of (Kaji et al,
2010) and propose a k-best sequential decoding al-
gorithm, namely iterative Viterbi A*. We show that
the proposed algorithm is several times or orders of
magnitude faster than the state-of-the-art in all tag-
ging tasks which consist of more than 200 labels.
Our contributions can be summarized as follows.
(1) We apply the A* search framework to sequential
decoding problem. We show that A* with a proper
heuristic can outperform the classic Viterbi decod-
ing. (2) We propose 1-best A*, 1-best iterative A*
decoding algorithms which are the second and third
fastest decoding algorithms among the five decod-
ing algorithms for comparison, although there is a
significant gap to the fastest 1-best decoding algo-
rithm. (3) We propose k-best A* and k-best iterative
Viterbi A* algorithms. The latter is several times or
orders of magnitude faster than the state-of-the-art
1Implemented in both CRFPP (http://crfpp.sourceforge.net/)
and LingPipe (http://alias-i.com/lingpipe/) packages.
611
k-best decoding algorithm. This algorithm makes
real-time large-scale tagging applications with thou-
sands of labels feasible.
2 Problem formulation
In this section, we formulate the sequential decod-
ing problem in the context of perceptron algorithm
(Collins, 2002) and CRFs (Lafferty et al, 2001). All
the discussions apply to HMMs as well. Formally, a
perceptron model is
f(y,x) =
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt), (1)
and a CRFs model is
p(y|x) =
1
Z(x)
exp{
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt)}, (2)
where x and y is an observation sequence and a la-
bel sequence respectively, t is the sequence position,
T is the sequence size, fk are feature functions and
K is the number of feature functions. ?k are the pa-
rameters that need to be estimated. They represent
the importance of feature functions fk in prediction.
For CRFs, Z(x) is an instance-specific normaliza-
tion function
Z(x) =
?
y
exp{
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt)}. (3)
If x is given, the decoding is to find the best y which
maximizes the score of f(y,x) for perceptron or the
probability of p(y|x) for CRFs. As Z(x) is a con-
stant for any given input sequence x, the decoding
for perceptron or CRFs is identical, that is,
argmax
y
f(y,x). (4)
To simplify the discussion, we divide the features
into two groups: unigram label features and bi-
gram label features. Unigram features are of form
fk(yt,xt) which are concerned with the current la-
bel and arbitrary feature patterns from input se-
quence. Bigram features are of form fk(yt, yt?1,xt)
which are concerned with both the previous and the
current labels. We thus rewrite the decoding prob-
lem as
argmax
y
T?
t=1
(
K1?
k=1
?1kf
1
k (yt,xt)+
K2?
k=1
?2kf
2
k (yt, yt?1,xt)).
(5)
For a better understanding, one can inter-
pret the term
?K1
k=1 ?
1
kf
1
k (yt,xt) as node yt?s
score at position t, and interpret the term
?K2
k=1 ?
2
kf
2
k (yt, yt?1,xt) as edge (yt?1, yt)?s
score. So the sequential decoding problem is cast as
a max score pathfinding problem2. In the discussion
hereafter, we assume scores of nodes and edges are
pre-computed (denoted as n(yt) and e(yt?1, yt)),
and we can thus focus on the analysis of different
decoding algorithms.
3 Background
We present the existing algorithms for both 1-best
and k-best sequential decoding in this section. These
algorithms serve as basis for the proposed algo-
rithms in Section 4.
3.1 1-Best Viterbi
The Viterbi algorithm is a classic dynamic program-
ming based decoding algorithm. It has the computa-
tional complexity of O(TL2), where T is the input
sequence size and L is the label size3. Formally, the
Viterbi computes ?(yt), the best score from starting
position to label yt, as follows.
max
yt?1
(?yt?1 + e(yt?1, yt)) + n(yt), (6)
where e(yt?1, yt) is the edge score between nodes
yt?1 and yt, n(yt) is the node score for yt. Note
that the terms ?yt?1 and e(yt?1, yt) take value 0 for
t = 0 at initialization. Using the recursion defined
above, we can compute the highest score at end po-
sition T ? 1 and its corresponding sequence. The
recursive computation of ?yt is denoted as forward
pass since the computing traverses the lattice from
left to right. Conversely, the backward pass com-
putes ?yt as the follows.
max
yt+1
(?yt+1 + e(yt, yt+1) + n(yt+1)). (7)
Note that ?yT?1 = 0 at initialization. The max
score can be computed using maxy0(?0 + n(y0)).
We can use either forward or backward pass to
compute the best sequence. Table 1 summarizes
the computational complexity of all decoding algo-
rithms including Viterbi, which has the complexity
of TL2 for both best and worst cases. Note that
N/A means the decoding algorithms are not applica-
ble (for example, iterative Viterbi is not applicable
to k-best decoding). The proposed algorithms (see
Section 4) are highlighted in bold.
3.2 1-Best iterative Viterbi
Kaji et al (Kaji et al, 2010) presented an efficient
sequential decoding algorithm named staggered de-
coding. We use the name iterative Viterbi to describe
2With the constraint that the path consists of one and only
one node at each position.
3We ignore the feature size terms for simplicity.
612
this algorithm for the reason that the iterative pro-
cess plays a central role in this algorithm. Indeed,
this iterative process is generalized in this paper to
handle k-best sequential decoding (see Section 4.4).
The main idea is to start with a coarse lattice
which consists of both active labels and degenerate
labels. A label is referred to as an active label if it
is not grouped (e.g., all labels in Fig. 1 (a) and la-
bel A at each position in Fig. 1 (b)), and otherwise
as an inactive label (i.e., dotted nodes). The new la-
bel, which is made by grouping the inactive labels,
is referred to as a degenerate label (i.e., large nodes
covering the dotted ones). Fig. 1 (a) shows a lattice
which consists of active labels only and (b) shows
a lattice which consists of both active and degener-
ate ones. The score of a degenerate label is the max
score of inactive labels which are included in the de-
generate label. Similarly, the edge score between a
degenerate label z and an active label y? is the max
edge score between any inactive label y ? z and y?,
and the score of two degenerate labels z and z? is the
max edge score between any inactive label y ? z
and y? ? z?. Using the above definitions, the best
sequence derived from a degenerate lattice would be
the upper bound of the sequence derived from the
original lattice. If the best sequence does not include
any degenerate labels, it is indeed the best sequence
for the original lattice.
F
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
B
C
D
E
Figure 1: (a) A lattice consisting of active labels only.
(b) A lattice consisting of both active labels and degener-
ate ones. Each position has one active label (A) and one
degenerate label (consisting of B, C. D, E, and F).
The pseudo code for this algorithm is shown in
Algorithm 1. The lattice is initialized to include one
active label and one degenerate label at each position
(see Figure 1 (b)). Note that the labels are ranked
by the probabilities estimated from the training data.
The Viterbi algorithm is applied to the lattice to find
the best sequence. If the sequence consists of ac-
tive labels only, the algorithm terminates and returns
such a sequence. Otherwise, the lower bound lb4 of
the active sequence in the lattice is updated and the
lattice is expanded. The lower bound can be initial-
ized to the best sequence score using a beam search
(with beam size being 1). After either a forward or
a backward pass, the lower bound is assigned with
4The maximum score of the active sequences found so far.
the best active sequence score best(lattice)5 if the
former is less than the latter. The expansion of lat-
tice ensures that the lattice has twice active labels
as before at a given position. Figure 2 shows the
column-wise expansion step. The number of active
labels in the column is doubled only if the best se-
quence of the degenerate lattice passes through the
degenerate label of that column.
Algorithm 1 Iterative Viterbi Algorithm
1: lb = best score from beam search
2: init lattice
3: for i=0;;i++ do
4: if i %2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: return y
11: end if
12: if lb < best(lattice) then
13: lb = best(lattice)
14: end if
15: expand lattice
16: end for
Algorithm 2 Forward
1: for i=0; i < T; i++ do
2: Compute ?(yi) and ?(yi) according to Equations (6) and (7)
3: if ?(yi) + ?(yi) < lb then
4: prune yi from the current lattice
5: end if
6: end for
7: Node b = argmaxyT?1 ?(yT?1)
8: return sequence back tracked by b
(c)
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
(a) (b)
Figure 2: Column-wise lattice expansion: (a) The best
sequence of the initial degenerate lattice, which does not
pass through the degenerate label in the first column. (b)
Column-wise expansion is performed and the best se-
quence is searched again. Notice that the active label in
the first column is not expanded. (c) The final result.
Algorithm 2 shows the forward pass in which the
node pruning is performed. That is, for any node,
if the best score of sequence which passes such a
node is less than the lower bound lb, such a node
is removed from the lattice. This removal is safe
as such a node does not have a chance to form an
optimal sequence. It is worth noting that, if a node
is removed, it can no longer be added into the lattice.
5We do not update the lower bound lb if we cannot find an
active sequence.
613
This property ensures the efficiency of the iterative
Viterbi algorithm. The backward pass is similar to
the forward one and it is thus omitted.
The alternative calls of forward and backward
passes (in Algorithm 1) ensure the alternative updat-
ing/lowering of node forward and backward scores,
which makes the node pruning in either forward pass
(see Algorithm 2) or backward pass more efficient.
The lower bound lb is updated once in each iteration
of the main loop in Algorithm 1. While the forward
and backwards scores of nodes gradually decrease
and the lower bound lb increases, more and more
nodes are pruned.
The iterative Viterbi algorithm has computational
complexity of T and TL2 for best and worst cases
respectively. This can be proved as follows (Kaji et
al., 2010). At the m-th iteration in Algorithm 1, it-
erative Viterbi decoding requires order of T4m time
because there are 2m active labels (plus one degen-
erate label). Therefore, it has
?m
i=0 T4
i time com-
plexity if it terminates at the m-th iteration. In the
best case in which m = 0, the time complexity is T .
In the worst case in which m = dlog2 Le ? 1 (d.e is
the ceiling function which maps a real number to the
smallest following integer), the time complexity is
order of TL2 because
?dlog2 Le?1
i=0 T4
i < 4/3TL2.
3.3 1-Best Carpediem
Esposito and Radicioni (2009) have proposed a
novel 1-best6 sequential decoding algorithm, Car-
pediem, which attempts to open only necessary
nodes in searching the best sequence in a given lat-
tice. Carpediem has the complexity of TL logL and
TL2 for the best and worst cases respectively. We
skip the description of this algorithm due to space
limitations. Carpediem is used as a baseline in our
experiments for decoding speed comparison.
3.4 K-Best Viterbi
In order to produce k-best sequences, it is not
enough to store 1-best label per node, as the k-
best sequences may include suboptimal labels. The
k-best sequential decoding gives up this 1-best
label memorization in the dynamic programming
paradigm. It stores up to k-best labels which are nec-
essary to form k-best sequences. The k-best Viterbi
algorithm thus has the computational complexity of
KTL2 for both best and worst cases.
Once we store the k-best labels per node in a lat-
tice, the k-best Viterbi algorithm calls either the for-
ward or the backward passes just in the same way as
the 1-best Viterbi decoding does. We can compute
6They did not provide k-best solutions.
the k highest score at the end position T ? 1 and the
corresponding k-best sequences.
3.5 K-Best Viterbi A*
To our best knowledge the most efficient k-best se-
quence algorithm is the Viterbi A* algorithm as
shown in Algorithm 3. The algorithm consists of one
forward pass and an A* backward pass. The forward
pass computes and stores the Viterbi forward scores,
which are the best scores from the start to the cur-
rent nodes. In addition, each node stores a backlink
which points to its predecessor.
The major part of Algorithm 3 describes the back-
ward A* pass. Before describing the algorithm, we
note that each node in the agenda represents a se-
quence. So the operations on nodes (push or pop)
correspond to the operations on sequences. Initially,
the L nodes at position T ? 1 are pushed to an
agenda. Each of the L nodes ni, i = 0, . . . , L ? 1,
represents a sequence. That is, node ni represents
the best sequence from the start to itself. The best of
the L sequences is the globally best sequence. How-
ever, the i-th best, i = 2, . . . , k, of the L sequence
may not be the globally i-th best sequence. The pri-
ority of each node is set as the score of the sequence
which is derived by such a node. The algorithm then
goes to a loop of k. In each loop, the best node is
popped off from the agenda and is stored in a set r.
The algorithm adds alternative candidate nodes (or
sequences) to the agenda via a double nested loop.
The idea is that, when an optimal node (or sequence)
is popped off, we have to push to the agenda all
nodes (sequences) which are slightly worse than the
just popped one. The interpretation of slightly worse
is to replace one edge from the popped node (se-
quence). The slightly worse sequences can be found
by the exact heuristic derived from the first Viterbi
forward pass.
Figure 3 shows an example of the push operations
for a lattice of T = 4, Y = 4. Suppose an optimal
node 2:B (in red, standing for node B at position 2,
representing the sequence of 0:A 1:D 2:B 3:C) is
popped off, new nodes of 1:A, 1:B, 1:C and 0:B,
0:C and 0:D are pushed to the agenda according to
the double nested for loop in Algorithm 3. Each
of the pushed nodes represents a sequence, for ex-
ample, node 1:B represents a sequence which con-
sists of three parts: Viterb sequence from start to
1:B (0:C 1:B), 2:B and forward link of 2:B (3:C
in this case). All of these pushed nodes (sequences)
are served as candidates for the next agenda pop op-
eration.
The algorithm terminates the loop once it has op-
timal k nodes. The k-best sequences can be de-
rived by the k optimal nodes. This algorithm has
614
TB
C
D
B
C
D
B
C
D
B
C
D
A A A A
31 20
Figure 3: Alternative nodes push after popping an opti-
mal node.
computation complexity of TL2 + TL for both best
and worst cases, with the first term accounting for
Viterbi forward pass and the second term account-
ing for A* backward process. The bottleneck is thus
at the Viterbi forward pass.
Algorithm 3K-Best Viterbi A* algorithm
1: forward()
2: push L best nodes to agenda q
3: c = 0
4: r = {}
5: while c < K do
6: Node n = q.pop()
7: r = r ? n
8: for i = n.t? 1; i ? 0; i?? do
9: for j = 0; j < L; j + + do
10: if j! = n.backlink.y then
11: create new node s at position i and label j
12: s.forwardlink = n
13: q.push(s)
14: end if
15: end for
16: n = n.backlink
17: end for
18: c+ +
19: end while
20: return K best sequences derived by r
4 Proposed Algorithms
In this section, we propose A* based sequen-
tial decoding algorithms that can efficiently handle
datasets with a large number of labels. In particular,
we first propose the A* and the iterative A* decod-
ing algorithm for 1-best sequential decoding. We
then extend the 1-best A* algorithm to a k-best A*
decoding algorithm. We finally apply the iterative
process to the Viterbi A* algorithm, resulting in the
iterative Viterbi A* decoding algorithm.
4.1 1-Best A*
A*(Hart et al, 1968; Russell and Norvig, 1995), as
a classic search algorithm, has been successfully ap-
plied in syntactic parsing (Klein and Manning, 2003;
Pauls and Klein, 2009). The general idea of A* is to
consider labels yt which are likely to result in the
best sequence using a score f as follows.
f(y) = g(y) + h(y), (8)
where g(y) is the score from start to the current node
and h(y) is a heuristic which estimates the score
from the current node to the target. A* uses an
agenda (based on the f score) to decide which nodes
are to be processed next. If the heuristic satisfies the
condition h(yt?1) ? e(yt?1, yt) + h(yt), then h is
called monotone or admissible. In such a case, A* is
guaranteed to find the best sequence. We start with
the naive (but admissible) heuristic as follows
h(yt) =
T?1?
i=t+1
(maxn(yi) + max e(yi?1, yi)). (9)
That is, the heuristic of node yt to the end is the sum
of max edge scores between any two positions and
max node scores per position. Similar to (Pauls and
Klein, 2009) we explore the heuristic in different
coarse levels. We apply the Viterbi backward pass
to different degenerate lattices and use the Viterbi
backward scores as different heuristics. Different
degenerate lattices are generated from different it-
erations of Algorithm 1: The m-th iteration corre-
sponds to a lattice of (2m+1)?T nodes. A largerm
indicates a more accurate heuristic, which results in
a more efficient A* search (fewer nodes being pro-
cessed). However, this efficiency comes with the
price that such an accurate heuristic requires more
computation time in the Viterbi backward pass. In
our experiments, we try the naive heuristic and the
following values of m: 0, 3, 6 and 9.
In the best case, A* expands one node per posi-
tion, and each expansion results in the push of all
nodes at next position to the agenda. The search is
similar to the beam search with beam size being 1.
The complexity is thus TL. In the worst case, A*
expands every node per position, and each expan-
sion results in the push of all nodes at next position
to the agenda. The complexity thus becomes TL2.
4.2 1-Best Iterative A*
The iterative process as described in the iterative
Viterbi decoding can be used to boost A* algorithm,
resulting in the iterative A* algorithm. For simplic-
ity, we only make use of the naive heuristic in Equa-
tion (9) in the iterative A* algorithm. We initialize
the lattice with one active label and one degenerate
label at each position (see Figure 1 (b)). We then run
A* algorithm on the degenerate lattice and get the
best sequence. If the sequence is active we return
it. Otherwise we expand the lattice in each iteration
until we find the best active sequence. Similar to
iterative Viterbi algorithm, iterative A* has the com-
plexity of T and TL2 for the best and worst cases
respectively.
4.3 K-Best A*
The extension from 1-best A* to k-best A* is again
due to the memorization of k-best labels per node.
615
Table 1: Best case and worst case computational complexity of various decoding algorithms.
1-best decoding K-best decoding
best case worst case best case worst case
beam TL TL KTL KTL
Viterbi TL2 TL2 KTL2 KTL2
iterative Viterbi T TL2 N/A N/A
Carpediem TL logL TL2 N/A N/A
A* TL TL2 KTL KTL2
iterative A* T TL2 N/A N/A
Viterbi A* N/A N/A TL2 +KTL TL2 +KTL
iterative Viterbi A* N/A N/A T +KT TL2 +KTL
We use either the naive heuristic (Equation (9)) or
different coarse level heuristics by setting m to be 0,
3, 6 or 9 (see Section 4.1). The first k nodes which
are popped off the agenda can be used to back track
the k-best sequences. The k-best A* algorithm has
the computational complexity of KTL and KTL2
for best and worst cases respectively.
4.4 K-Best Iterative Viterbi A*
We now present the k-best iterative Viterbi A* algo-
rithm (see Algorithm 4) which applies the iterative
process to k-best Viterbi A* algorithm. The major
difference between 1-best iterative Viterbi A* algo-
rithm (Algorithm 1) and this algorithm is that the
latter calls the k-best Vitebi A* (Algorithm 3) after
the best sequence is found. If the k-best sequences
are all active, we terminate the algorithm and return
the k-best sequences. If we cannot find either the
best active sequence or the k-best active sequences,
we expand the lattice to continue the search in the
next iteration.
As in the iterative Viterbi algorithm (see Section
3.2), nodes are pruned at each position in forward
or backward passes. Efficient pruning contributes
significantly to speeding up decoding. Therefore, to
have a tighter (higher) lower bound lb is important.
We initialize the lower bound lb with the k-th best
score from beam search (with beam size being k) at
line 1. Note that the beam search is performed on the
original lattice which consists of L active labels per
position. The beam search time is negligible com-
pared to the total decoding time. At line 16, we up-
date lb as follows. We enumerate the best active se-
quences backtracked by the nodes at position T ? 1.
If the current lb is less than the k-th active sequence
score, we update the lbwith the k-th active sequence
score (we do not update lb if there are less than k ac-
tive sequences). At line 19, we use the sequences
returned from Viterbi A* algorithm to update the lb
in the same manner. To enable this update, we re-
quest the Viterbi A* algorithm to return k?, k? > k,
sequences (line 10). A larger number of k? results
in a higher chance to find the k-th active sequence,
which in turn offers a tighter (higher) lb, but it comes
with the expense of additional time (the backward
A* process takes O(TL) time to return one more
sequence). In experiments, we found the lb updates
on line 1 and line 16 are essential for fast decoding.
The updating of lb using Viterbi A* sequences (line
19) can boost the decoding speed further. We exper-
imented with different k? values (k? = nk, where n
is an integer) and selected k? = 2k which results in
the largest decoding speed boost.
Algorithm 4K-Best iterative Viterbi A* algorithm
1: lb = k-th best (original lattice)
2: init lattice
3: for i = 0; ; i+ + do
4: if i%2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: ys= k-best Viterbi A* (Algorithm 3)
11: if ys consists of active sequences only then
12: return ys
13: end if
14: end if
15: if lb < k-th best(lattice) then
16: lb = k-th best(lattice)
17: end if
18: if lb < k-th best(ys) then
19: lb = k-th best(ys)
20: end if
21: expand lattice
22: end for
5 Experiments
We compare aforementioned 1-best and k-best se-
quential decoding algorithms using five datasets in
this section.
5.1 Experimental setting
We apply 1-best and k-best sequential decoding al-
gorithms to five NLP tagging tasks: Penn TreeBank
(PTB) POS tagging, CoNLL2000 joint POS tag-
ging and chunking, CoNLL 2003 joint POS tagging,
chunking and named entity tagging, HPSG supertag-
ging (Matsuzaki et al, 2007) and a search query
named entity recognition (NER) dataset. We used
616
sections 02-21 of PTB for training and section 23
for testing in POS task. As in (Kaji et al, 2010),
we combine the POS tags and chunk tags to form
joint tags for CoNLL 2000 dataset, e.g., NN|B-NP.
Similarly we combine the POS tags, chunk tags, and
named entity tags to form joint tags for CoNLL 2003
dataset, e.g., PRP$|I-NP|O. Note that by such tag
joining, we are able to offer different tag decodings
(for example, chunking and named entity tagging)
simultaneously. This indeed is one of the effective
approaches for joint tag decoding problems. The
search query NER dataset is an in-house annotated
dataset which assigns semantic labels, such as prod-
uct, business tags to web search queries.
Table 2 shows the training and test sets size (sen-
tence #), the average token length of test dataset and
the label size for the five datasets. POS and su-
pertag datasets assign tags to tokens while CoNLL
2000 , CoNLL 2003 and search query datasets as-
sign tags to phrases. We use the standard BIO en-
coding for CoNLL 2000, CoNLL 2003 and search
query datasets.
Table 2: Training and test datasets size, average token
length of test set and label size for five datasets.
training # test # token length label size
POS 39831 2415 23 45
CoNLL2000 8936 2012 23 319
CoNLL2003 14987 3684 12 443
Supertag 37806 2291 22 2602
search query 79569 6867 3 323
Due to the long CRF training time (days to weeks
even for stochastic gradient descent training) for
these large label size datasets, we choose the percep-
tron algorithm for training. The models are averaged
over 10 iterations (Collins, 2002). The training time
takes minutes to hours for all datasets. We note that
the selection of training algorithm does not affect
the decoding process: the decoding is identical for
both CRF and perceptron training algorithms. We
use the common features which are adopted in previ-
ous studies, for example (Sha and Periera, 2003). In
particular, we use the unigrams of the current and its
neighboring words, word bigrams, prefixes and suf-
fixes of the current word, capitalization, all-number,
punctuation, and tag bigrams for POS, CoNLL2000
and CoNLL 2003 datasets. For supertag dataset,
we use the same features for the word inputs, and
the unigrams and bigrams for gold POS inputs. For
search query dataset, we use the same features plus
gazetteer based features.
5.2 Results
We report the token accuracy for all datasets to facil-
itate comparison to previous work. They are 97.00,
94.70, 95.80, 90.60 and 88.60 for POS, CoNLL
2000, CoNLL 2003, supertag, and search query re-
spectively. We note that all decoding algorithms as
listed in Section 3 and Section 4 are exact. That is,
they produce exactly the same accuracy. The accu-
racy we get for the first four tasks is comparable to
the state-of-the-art. We do not have a baseline to
compare with for the last dataset as it is not pub-
licly available7. Higher accuracy may be achieved if
more task specific features are introduced on top of
the standard features. As this paper is more con-
cerned with the decoding speed, the feature engi-
neering is beyond the scope of this paper.
Table 3 shows how many iterations in average
are required for iterative Viterbi and iterative Viterbi
A* algorithms. Although the max iteration size is
bounded to dlog2 Le for each position (for exam-
ple, 9 for CoNLL 2003 dataset), the total iteration
number for the whole lattice may be greater than
dlog2 Le as different positions may not expand at
the same time. Despite the large number of itera-
tions used in iterative based algorithms (especially
iterative Viterbi A* algorithm), the algorithms are
still very efficient (see below).
Table 3: Iteration numbers of iterative Viterbi and itera-
tive Viterbi A* algorithms for five datasets.
POS CoNLL2000 CoNLL2003 Supertag search query
iter Viter 6.32 8.76 9.18 10.63 6.71
iter Viter A* 14.42 16.40 15.41 18.62 9.48
Table 4 and 5 show the decoding speed (sen-
tences per second) of 1-best and 5-best decoding al-
gorithms respectively. The proposed decoding algo-
rithms and the largest decoding speeds across differ-
ent decoding algorithms (other than beam) are high-
lighted in bold. We exclude the time for feature ex-
traction in computing the speed. The beam search
decoding is also shown as a baseline. We note that
beam decoding is the only approximate decoding al-
gorithm in this table. All other decoding algorithms
produce exactly the same accuracy, which is usually
much better than the accuracy of beam decoding.
For 1-best decoding, iterative Viterbi always out-
performs other ones. A* with a proper heuristic de-
noted as A* (best), that is, the best A* using naive
heuristic or the values of m being 0, 3, 6 or 9 (see
Section 4.1), can be the second best choice (ex-
cept for the POS task), although the gap between
iterative Viterbi and A* is significant. For exam-
ple, for CoNLL 2003 dataset, the former can de-
code 2239 sentences per second while the latter only
decodes 225 sentences per second. The iterative
process successfully boosts the decoding speed of
iterative Viterbi compared to Viterbi, but it slows
down the decoding speed of iterative A* compared
7The lower accuracy is due to the dynamic nature of queries:
many of test query tokens are unseen in the training set.
617
to A*(best). This is because in the Viterbi case,
the iterative process has a node pruning procedure,
while it does not have such pruning in A*(best)
algorithm. Take CoNLL 2003 data as an exam-
ple, the removal of the pruning slows down the 1-
best iterative Viterbi decoding from 2239 to 604
sentences/second. Carpediem algorithm performs
poorly in four out of five tasks. This can be ex-
plained as follows. The Carpediem implicitly as-
sumes that the node scores are the dominant factors
to determine the best sequence. However, this as-
sumption does not hold as the edge scores play an
important role.
For 5-best decoding, k-best Viterbi decoding is
very slow. A* with a proper heuristic is still slow.
For example, it only reaches 11 sentences per second
for CoNLL 2003 dataset. The classic Viterbi A* can
usually obtain a decent decoding speed, for example,
40 sentences per second for CoNLL 2003 dataset.
The only exception is supertag dataset, on which the
Viterbi A* decodes 0.1 sentence per second while
the A* decodes 3. This indicates the scalability is-
sue of Viterbi A* algorithm for datasets with more
than one thousand labels. The proposed iterative
Viterbi A* is clearly the winner. It speeds up the
Viterbi A* to factors of 4, 7, 360, and 3 for CoNLL
2000, CoNLL 2003, supertag and query search data
respectively. The decoding speed of iterative Viterbi
A* can even be comparable to that of beam search.
Figure 4 shows k-best decoding algorithms de-
coding speed with respect to different k values for
CoNLL 2003 data . The Viterbi A* and iterative
Viterbi A* algorithms are significantly faster than
the Viterbi and A*(best) algorithms. Although the
iterative Viterbi A* significantly outperforms the
Viterbi A* for k < 30, the speed of the former con-
verges to the latter when k becomes 90 or larger.
This is expected as the k-best sequences span over
the whole lattice: the earlier iteration in iterative
Viterbi A* algorithm cannot provide the k-best se-
quences using the degenerate lattice. The over-
head of multiple iterations slows down the decoding
speed compared to the Viterbi A* algorithm.
l l l l l l l l l l10 20 30 40 50 60 70 80 90 100020
406080
100120140
160180200
k
sentenc
es/secon
d l ViterbiA*(best)Viterbi A*iterative Viterbi A*
Figure 4: Decoding speed of k-best decoding algorithms
for various k for CoNLL 2003 dataset.
6 Related work
The Viterbi algorithm is the only exact algorithm
widely adopted in the NLP applications. Esposito
and Radicioni (2009) proposed an algorithm which
opens necessary nodes in a lattice in searching the
best sequence. The staggered decoding (Kaji et al,
2010) forms the basis for our work on iterative based
decoding algorithms. Apart from the exact decod-
ing, approximate decoding algorithms such as beam
search are also related to our work. Tsuruoka and
Tsujii (2005) proposed easiest-first deterministic de-
coding. Siddiqi and Moore (2005) presented the pa-
rameter tying approach for fast inference in HMMs.
A similar idea was applied to CRFs as well (Cohn,
2006; Jeong, 2009). We note that the exact algo-
rithm always guarantees the optimality which can-
not be attained in approximate algorithms.
In terms of k-best parsing, Huang and Chiang
(2005) proposed an efficient algorithm which is sim-
ilar to the k-best Viterbi A* algorithm presented in
this paper. Pauls and Klein (2009) proposed an algo-
rithm which replaces the Viterbi forward pass with
an A* search. Their algorithm optimizes the Viterbi
pass, while the proposed iterative Viterbi A* algo-
rithm optimizes both Viterbi and A* passes.
This paper is also related to the coarse to fine
PCFG parsing (Charniak et al, 2006) as the degen-
erate labels can be treated as coarse levels. How-
ever, the difference is that the coarse-to-fine parsing
is an approximate decoding while ours is exact one.
In terms of different coarse levels of heuristic used
in A* decoding, this paper is related to the work of
hierarchical A* framework (Raphael, 2001; Felzen-
szwalb et al, 2007). In terms of iterative process,
this paper is close to (Burkett et al, 2011) as both
exploit the search-and-expand approach.
7 Conclusions
We have presented and evaluated the A* and itera-
tive A* algorithms for 1-best sequential decoding in
this paper. In addition, we proposed A* and iterative
Viterbi A* algorithm for k-best sequential decoding.
K-best Iterative A* algorithm can be several times
or orders of magnitude faster than the state-of-the-
art k-best decoding algorithm. It makes real-time
large-scale tagging applications with thousands of
labels feasible.
Acknowledgments
We wish to thank Yusuke Miyao and Nobuhiro Kaji
for providing us the HPSG Treebank data. We are
grateful for the invaluable comments offered by the
anonymous reviewers.
618
Table 4: Decoding speed (sentences per second) of 1-best decoding algorithms for five datasets.
POS CoNLL2000 CoNLL2003 supertag query search
beam 7252 1381 1650 395 7571
Viterbi 2779 51 41 0.19 443
iterative Viterbi 5833 972 2239 213 6805
Carpediem 2638 14 20 0.15 243
A* (best) 802 131 225 8 880
iterative A* 1112 84 109 3 501
Table 5: Decoding speed (sentences per second) of 5-best decoding algorithms for five datasets.
POS CoNLL2000 CoNLL2003 supertag query search
beam 2760 461 592 75 4354
Viterbi 19 0.41 0.25 0.12 3.83
A* (best) 205 4 11 3 92
Viterbi A* 1266 47 40 0.1 357
iterative Viterbi A* 788 200 295 36 1025
References
D. Burkett, D. Hall, and D. Klein. 2011. Optimal graph
search with iterated graph cuts. Proceedings of AAAI.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D.
Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M.
Pozar, and T. Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. Proceedings of NAACL.
T. Cohn. 2006. Efficient inference in large conditional
random fields. Proceedings of ECML.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. Proceedings of EMNLP.
R. Esposito and D. P. Radicioni. 2009. Carpediem:
Optimizing the Viterbi Algorithm and Applications to
Supervised Sequential Learning. Journal of Machine
Learning Research.
P. Felzenszwalb and D. McAllester. 2007. The general-
ized A* architecture. Journal of Artificial Intelligence
Research.
P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A For-
mal Basis for the Heuristic Determination of Minimum
Cost Paths. IEEE Transactions on Systems Science
and Cybernetics.
L. Huang and D. Chiang. 2005. Better k-best parsing.
Proceedings of the International Workshops on Parsing
Technologies (IWPT).
M. Jeong, C. Y. Lin, and G. G. Lee. 2009. Efficient infer-
ence of CRFs for large-scale natural language data.
Proceedings of ACL-IJCNLP Short Papers.
N. Kaji, Y. Fujiwara, N. Yoshinaga, and M. Kitsuregawa.
2010. Efficient Staggered Decoding for Sequence La-
beling. Proceedings of ACL.
D. Klein and C. Manning. 2003. A* parsing: Fast exact
Viterbi parse selection. Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings of
ICML.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2007. Efficient
HPSG parsing with supertagging and CFG-filtering.
Proceedings of IJCAI.
A. Pauls and D. Klein. 2009. K-Best A* Parsing. Pro-
ceedings of ACL.
L. R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of The IEEE.
C. Raphael. 2001. Coarse-to-fine dynamic program-
ming. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence.
S. Russell and P. Norvig. 1995. Artificial Intelligence: A
Modern Approach.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. Proceedings of HLT-NAACL.
S. M. Siddiqi and A. Moore. 2005. Fast inference and
learning in large-state-space HMMs. Proceedings of
ICML.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. Proceedings of HLT/EMNLP.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding algo-
rithm. IEEE Transactions on Information Theory.
619
